[{"figure_path": "clTa4JFBML/figures/figures_1_1.jpg", "caption": "Figure 1: The Representation-Conditioned Generation (RCG) framework for unconditional generation. RCG consists of three parts: (a) it uses a pre-trained self-supervised encoder to map the image distribution to a representation distribution; (b) it learns a representation generator that samples from a noise distribution and generates a representation subject to the representation distribution; (c) it learns an image generator (e.g., which can be ADM [18], DiT [50], or MAGE [41]) that maps a noise distribution to the image distribution conditioned on the representation distribution.", "description": "This figure illustrates the Representation-Conditioned Generation (RCG) framework, a three-part process for unconditional image generation. First, a pre-trained self-supervised encoder maps the image distribution to a lower-dimensional representation distribution.  Second, a representation generator creates new representations based on a noise distribution, constrained by the learned representation distribution. Finally, an image generator (like ADM, DiT, or MAGE) uses these representations to generate new images, conditioned on this representation.", "section": "1 Introduction"}, {"figure_path": "clTa4JFBML/figures/figures_2_1.jpg", "caption": "Figure 2: Unconditional Image Generation can be largely improved by our RCG framework. Regardless of the specific form of the image generator (LDM [54], ADM [18], DiT [50], or MAGE [41]), RCG massively improves the unconditional generation quality. Generation quality is measured by FID on ImageNet with a 256\u00d7256 resolution. All comparisons between models without and with RCG are conducted under controlled conditions to ensure fairness. The technical details and more metrics are in Section 4.1.", "description": "The figure shows a bar chart comparing the FID scores (Fr\u00e9chet Inception Distance, a metric for evaluating the quality of generated images) for unconditional image generation using different image generators (LDM-8, ADM, DiT-XL/2, MAGE-L) with and without the RCG framework.  The RCG method significantly reduces the FID scores across all generators, demonstrating its effectiveness in improving the quality of unconditional image generation.", "section": "4.1 Observations"}, {"figure_path": "clTa4JFBML/figures/figures_3_1.jpg", "caption": "Figure 3: RCG's training framework. The pre-trained self-supervised image encoder extracts representations from images and is fixed during training. To train the representation generator, we add standard Gaussian noise to the representations and ask the network to denoise them. To train the MAGE image generator, we add random masking to the tokenized image and ask the network to reconstruct the missing tokens conditioned on the representation extracted from the same image.", "description": "This figure illustrates the training framework of the Representation-Conditioned Generation (RCG) model.  It shows two main components: a representation generator and an image generator. The representation generator takes representations from a pre-trained self-supervised encoder and adds Gaussian noise. It then trains a network to denoise these representations.  The image generator uses the same pre-trained encoder and takes the denoised representation as a condition.  It also takes a masked tokenized image as input and trains a network to reconstruct the full image based on the representation and the masked input.", "section": "3 Method"}, {"figure_path": "clTa4JFBML/figures/figures_4_1.jpg", "caption": "Figure 4: Representation generator's backbone architecture. Each \u201cLayer", "description": "This figure shows the architecture of the representation generator in RCG. The backbone consists of an input layer, N fully-connected blocks, and an output layer. Each fully-connected block contains a LayerNorm, SiLU activation, and a linear layer.  The diffusion timestep is embedded and added to each fully-connected block. This architecture generates representations without explicit conditioning.", "section": "3 Method"}, {"figure_path": "clTa4JFBML/figures/figures_6_1.jpg", "caption": "Figure 5: RCG achieves outstanding unconditional generation performance with less training cost. All numbers are reported under the unconditional generation setting. The training cost is measured using a cluster of 64 V100 GPUs. Given that the MoCo v3 ViT encoder is pre-trained and not needed for generation, its training cost is excluded. Detailed computational cost is reported in Appendix B.1.", "description": "This figure compares the training cost and the unconditional generation FID of several unconditional image generation models, including those enhanced by RCG. RCG significantly reduces the FID (Frechet Inception Distance, a metric of image generation quality) with less training time compared to the baselines. This highlights the efficiency of RCG in achieving high-quality unconditional generation.", "section": "4 Experiments"}, {"figure_path": "clTa4JFBML/figures/figures_8_1.jpg", "caption": "Figure 6: RCG can generate images with diverse appearances but similar semantics from the same representation. We extract representations from reference images and, for each representation, generate a variety of images from different random seeds.", "description": "This figure demonstrates the ability of RCG to generate multiple images from a single representation, showing that the model can produce images with diverse appearances while maintaining semantic consistency.  The images in each row share a common semantic core (as indicated by the reference image on the left), while exhibiting variations in style, pose, and other details. This highlights the model's ability to capture high-level semantic understanding while allowing for diverse low-level variations.", "section": "4.2 Qualitative Insights"}, {"figure_path": "clTa4JFBML/figures/figures_8_2.jpg", "caption": "Figure 7: RCG's results conditioned on interpolated representations from two images. The semantics of the generated images gradually transfer between the two images.", "description": "This figure shows the results of RCG when generating images using interpolated representations from two different source images.  As the interpolation weight changes from 0.0 (representing the first image) to 1.0 (representing the second image), the generated images smoothly transition semantically between the characteristics of the two source images. This demonstrates the ability of RCG to generate semantically coherent and smoothly interpolating images in its representation space.  Each row shows interpolation between a pair of images from different ImageNet classes.", "section": "4.2 Qualitative Insights"}, {"figure_path": "clTa4JFBML/figures/figures_13_1.jpg", "caption": "Figure 8: Unconditional generation results of RCG on ImageNet 256x256. RCG can generate realistic images with diverse semantics without human annotations.", "description": "This figure shows a grid of images generated by the Representation-Conditioned Generation (RCG) model on the ImageNet dataset. The images are 256x256 pixels and demonstrate the model's ability to generate high-quality, diverse images without the need for human-provided labels. This highlights a key aspect of RCG: its ability to model complex data distributions and generate realistic images, thus addressing the problem of unconditional image generation.", "section": "4.2 Qualitative Insights"}, {"figure_path": "clTa4JFBML/figures/figures_18_1.jpg", "caption": "Figure 8: Unconditional generation results of RCG on ImageNet 256x256. RCG can generate realistic images with diverse semantics without human annotations.", "description": "This figure shows a grid of 100 images generated by the Representation-Conditioned Generation (RCG) model on the ImageNet dataset.  Each image is 256x256 pixels.  The images demonstrate the model's ability to generate diverse and realistic images without relying on human-annotated labels. The variety of objects and scenes showcases the model's capacity to learn and model complex data distributions.", "section": "4.2 Qualitative Insights"}, {"figure_path": "clTa4JFBML/figures/figures_19_1.jpg", "caption": "Figure 10: RCG class-conditional image generation results on ImageNet 256x256. Classes are 874: trolleybus, 664: monitor, 249: malamute; 952: fig, 968: cup, 256: Newfoundland; 789: shoji, 659: mixing bowl, 681: notebook; 119: rock crab, 629: lipstick, 192: cairn; 359: ferret, 9: ostrich, 277: red fox.", "description": "This figure shows example images generated by the Representation-Conditioned Generation (RCG) model with class conditioning.  It demonstrates the model's ability to generate images of different classes with high fidelity and diversity. Each row represents images generated for a specific class, showcasing the model's capability to generate multiple variations of the same class, reflecting the diversity within each category.", "section": "4.2 Qualitative Insights"}, {"figure_path": "clTa4JFBML/figures/figures_20_1.jpg", "caption": "Figure 11: RCG class-conditional image generation results on ImageNet 256x256. Classes are 1: goldfish, 388: panda, 279: Arctic fox; 323: monarch butterfly, 292: tiger, 933: cheeseburger; 985: daisy, 979: valley, 992: agaric", "description": "This figure shows examples of images generated by the Representation-Conditioned Generation (RCG) model when conditioned on class labels.  It demonstrates the model's ability to generate diverse and high-quality images for various classes, showcasing its effectiveness in class-conditional generation.", "section": "4.2 Qualitative Insights"}, {"figure_path": "clTa4JFBML/figures/figures_20_2.jpg", "caption": "Figure 8: Unconditional generation results of RCG on ImageNet 256x256. RCG can generate realistic images with diverse semantics without human annotations.", "description": "This figure shows several examples of images generated by the Representation-Conditioned Generation (RCG) model on the ImageNet dataset at a resolution of 256x256 pixels.  The images demonstrate the model's ability to generate diverse and realistic images without relying on any human-provided labels or class information. The diversity in the generated images showcases the model's ability to capture the underlying data distribution effectively, generating a wide variety of images that are both realistic and semantically meaningful. The lack of human annotation highlights the model's capability to perform unconditional image generation.", "section": "4.2 Qualitative Insights"}, {"figure_path": "clTa4JFBML/figures/figures_21_1.jpg", "caption": "Figure 13: Class-unconditional image generation results on ImageNet 256\u00d7256, with or without guidance. RCG achieves strong generation performance even without guidance. Incorporating guidance further improves the generation quality.", "description": "This figure shows the results of class-unconditional image generation on ImageNet 256x256 dataset using the proposed Representation-Conditioned Generation (RCG) method. It compares the image generation results with and without classifier-free guidance (CFG).  The results demonstrate that RCG achieves strong generation quality even without CFG, and that incorporating CFG further improves the quality of generated images. Three categories of images are shown, for each category, the images in the left column are generated without guidance, and the images in the right column are generated with guidance.", "section": "4.2 Qualitative Insights"}]