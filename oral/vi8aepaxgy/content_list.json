[{"type": "text", "text": "Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shengbang Tong\u2217 Ellis Brown\u2217 Penghao $\\mathbf{W}\\mathbf{u}^{*}$ Sanghyun Woo Manoj Middepogu Sai Charitha Akula Jihan Yang Shusheng Yang Adithya Iyer Xichen Pan Austin Wang Rob Fergus Yann LeCun Saining Xie\u2020 New York University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a vision-centric approach. While stronger language models can enhance multimodal capabilities, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research. This gap hinders accurate sensory grounding in real-world scenarios. Our study uses LLMs and visual instruction tuning as an interface to evaluate various visual representations, offering new insights into different models and architectures\u2014selfsupervised, strongly supervised, or combinations thereof\u2014based on experiments with over 20 vision encoders. We critically examine existing MLLM benchmarks, addressing the difficulties involved in consolidating and interpreting results from various tasks, and introduce a new vision-centric benchmark, CV-Bench. To further improve visual grounding, we propose the Spatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that integrates high-resolution vision features with LLMs while reducing the number of tokens. Additionally, we discuss the curation of high-quality visual instruction-tuning data from publicly available sources, emphasizing the importance of data source balancing and distribution ratio. Collectively, Cambrian-1 not only achieves state-of-the-art performance but also serves as a comprehensive, open cookbook for instruction-tuned MLLMs. We provide model weights, code, supporting tools, datasets, and detailed instructiontuning and evaluation recipes. We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning. ", "page_idx": 0}, {"type": "text", "text": "Project page: https://cambrian-mllm.github.io/ ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "There is a long-standing debate in philosophy about whether understanding and meaning in language require sensory grounding. Aristotle\u2019s emphasis on acquiring knowledge through sensory experience and empirical observation was central to his ancient Peripatetic school and remains influential to this day [8]; Aquinas famously formalized these ideas in the 13th century with the Peripatetic axiom: \u201cNihil est in intellectu quod non sit prius in sensu\u201d (Nothing is in the intellect that was not first in the senses) [7]. Though many philosophers disagree [23], it is evident that having robust and highly capable sensory grounding is at least beneficial. Consider the Cambrian explosion, during which the emergence of vision is believed [105] to have been crucial for early animals to not only find food and avoid predators but also to evolve and improve. In fact, most human knowledge (and nearly all animal knowledge) is acquired through sensory experiences like sight, hearing, touch, taste, and smell, through interactions with the physical world [107]. These sensory experiences are fundamental to understanding the world around us and are crucial for real-world actions and decision-making. ", "page_idx": 0}, {"type": "image", "img_path": "Vi8AepAXGy/tmp/14460571a32553bc0dc3e12fa622f1a77fdce70de3734330359e3654f610b3cc.jpg", "img_caption": ["Figure 1: We draw parallels between traditional protocols and the use of MLLMs for evaluating visual representations. MLLMs employ visual question answering to address a diverse array of real-world perception tasks. The bottom section highlights the five key pillars studied in Cambrian-1. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Beyond philosophical debates, recent advances in multimodal large language models (MLLMs) have brought the topic of visual representation learning vs. language understanding into practical focus. Language models have shown strong scaling behaviors [55], and recent advancements in multimodal learning are largely driven by the development of better, larger LLMs [81]. On the other hand, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research. For instance, many pioneering frameworks such as LLaVA [82] use vision transformer-based CLIP models [109, 145], which are strongly supervised by language\u2021, as the vision feature extractor. While other visual representations, such as self-supervised DINO [103], are being explored [126], there is a lack of comprehensive and systematic study in this domain. This gap exists primarily because such studies are challenging: MLLMs involve a complex training and evaluation pipeline with numerous design decisions to consider. In this work, we aim to bridge the gap by exploring MLLMs from a vision-centric perspective. More specifically, we use MLLM instruction tuning as an evaluation protocol for various visual representations (illustrated in Fig. 1). ", "page_idx": 1}, {"type": "text", "text": "Our motivation for this study also stems from two potential concerns of the current multimodal learning research: 1) relying too heavily too early on language can act as a shortcut [47, 144], compensating for the deficiencies in learning effective visual representations, and 2) existing benchmarks may not provide adequate guidance for real-world scenarios\u2014where visual grounding is crucial for robust multimodal understanding. These concerns are not unfounded, as researchers have started to notice that visual grounding is becoming a bottleneck for applying MLLMs in some challenging real-world applications, despite significant progress in improving general capabilities [41, 126, 136]. ", "page_idx": 1}, {"type": "text", "text": "From another perspective, traditional evaluation protocols for visual representation learning (e.g., linear probing and end-to-end fine-tuning on datasets like ImageNet-1K [113], COCO [79], and ADE20K [154]) are becoming saturated and do not reflect the diverse perception challenges found in real-world distributions. On the other hand, using language in the form of visual question answering (VQA) offers a flexible and robust evaluation protocol. Our study aims to explore this new protocol design, setting it up to gain insights that will guide the development of better visual representations in the future. Furthermore, to better evaluate visual representations in this integrated setting, we develop a vision-centric MLLM benchmark, CV-Bench, by transforming traditional vision benchmarks into VQA format (Section 2.2). ", "page_idx": 1}, {"type": "text", "text": "Cambrian-1 is structured around five key pillars, each offering important insights into the design space of MLLMs: ", "page_idx": 1}, {"type": "text", "text": "Visual Representations: We explore various vision encoders and their combinations. $\\S2.4$ \u2022 Connector Design: We design a new dynamic and spatially-aware connector that integrates vision features with LLMs while reducing the number of tokens. \u00a73 \u2022 Instruction Tuning Data: We curate high-quality visual instruction-tuning data from public sources, emphasizing the importance of distribution balancing. $\\S4$ \u2022 Instruction Tuning Recipes: We discuss instruction tuning strategies and practices. $\\S2.3$ \u2022 Benchmarking: We analyze existing MLLM benchmarks, cluster them into 4 intuitive groups, and introduce a new vision-centric benchmark \u201cCV-Bench\u201d. $\\S2.1$ , $\\S2.2$ ", "page_idx": 1}, {"type": "image", "img_path": "Vi8AepAXGy/tmp/2591cf9afadf78c4264225ff9e6e070f412deba948317be61a453894f49bc65b.jpg", "img_caption": ["Figure 2: Examples of various vision models, objectives, and architectures studied. Image from [48]. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "We defer a detailed review of the fundamental components and methodologies that underpin MLLM research to Appendix B ", "page_idx": 2}, {"type": "text", "text": "2 Evaluating Visual Representations through MLLMs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Current MLLMs predominantly rely on CLIP [109] as the visual encoder due to its pre-alignment with language and ease of adaptation to the LLM token space. However, strong language priors can be a double-edged sword\u2014they compensate for deficiencies in learning effective visual representations [126] and diminish insights gained from extensive visual representation learning research. In this section, we systematically evaluate how various visual encoder choices (see Fig. 2) impact the multimodal capabilities of MLLMs. We also advocate for using MLLM evaluation as a robust framework for assessing visual representation methods, moving beyond traditional protocols like linear probing and end-to-end fine-tuning to more faithfully reflect the diverse perception challenges in real-world scenarios and to better guide the development of improved visual representations. ", "page_idx": 2}, {"type": "text", "text": "2.1 Analyzing the Benchmarks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To effectively evaluate visual representations and MLLMs, we first need to select benchmarks that accurately assess the multimodal capabilities of these models. We use a suite of commonly used benchmarks [24, 45, 54, 57, 83, 84, 91, 92, 96, 97, 120, 126, 137, 143], which is the intersection of those used in recent MLLM research [75, 77, 137]. To help interpret our results, we begin by analyzing the benchmarks themselves. Here, we train MLLMs with 23 different vision backbones (see Table 6) from a variety of model families (see Fig. 2) using a 2-stage instruction tuning process initially proposed in [82]: first training connector on 1.2M adapter data from ShareGPT-4V [27] followed by fine-tuning both the connector and LLM on 737K instruction tuning data (see more details in Appendices G.5 and H). Full benchmark results in Table 9. ", "page_idx": 2}, {"type": "text", "text": "Who\u2019s answering the question: the LLM or MLLM? Determining whether a benchmark truly needs visual input to be solved has been a persistent challenge in vision-language research [2, 26, 50, 94]. In this study, we compare the performance of MLLMs with and without visual input\u00a7, and also calculate the expected score via randomly guessing. These three conditions are visualized in Fig. 3-left, with benchmarks sorted by the difference between the average score with vision enabled and disabled. SQA- $\\cdot\\mathrm{I}^{\\P}$ , MMMU, MathVista, and AI2D display less than a $5\\%$ gap between vision enabled and disabled, suggesting that these benchmarks may not significantly depend on visual input and rather heavily rely on the base LLM. TextVQA and GQA both demonstrate a nearly $40\\%$ positive gap between random guessing and vision-disabled scores, implying a strong language bias in these benchmarks. On the other hand, the vision-disabled performance on benchmarks like MMVP is notably worse than random guessing, suggesting that strong visual grounding is particularly crucial. ", "page_idx": 2}, {"type": "text", "text": "Clustering the Benchmarks To better understand the different aspects of MLLM performance, we analyze the correlations between the performance of our 23 MLLMs on each benchmark. A confusion matrix (Fig. 10) reveals that certain benchmarks, such as MMMU, are largely uncorrelated with the others. We perform principal component analysis on the benchmark scores and observe the formation of clusters corresponding to \u201cGeneral,\u201d \u201cKnowledge,\u201d \u201cChart & OCR,\u201d and \u201cVision-Centric\u201d categories (Fig. 3-right). We assign MMMU to the knowledge category based on the types of questions it includes (see Appendix D). We also find that existing vision-centric benchmarks [126, 137] are of insufficient size (see Fig. 3-right), challenging the robustness of evaluating such capabilities. These benchmarks do not cover crucial visual elements such as depth and spatial awareness. ", "page_idx": 2}, {"type": "image", "img_path": "Vi8AepAXGy/tmp/32104300d1cbfd77063e9e5725ddcc6c4f7f92a5b7f3cb121018365ef6aa20bd.jpg", "img_caption": ["Figure 3: Left: Performance comparison of MLLMs with visual input enabled and disabled across various benchmarks. Benchmarks are sorted by the difference between the average score with vision enabled and disabled. Right: Principal component analysis displaying clusters of benchmarks based on performance metrics, with bubble size corresponding to benchmark size. We label the clusters as \u201cGeneral\u201d in green, \u201cKnowledge\u201d in yellow, \u201cChart & OCR\u201d in red, and \u201cVision-Centric\u201d in blue. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Finding 1: Most benchmarks do not properly measure vision-centric capabilities, and the ones that do have very few samples. ", "page_idx": 3}, {"type": "text", "text": "2.2 Cambrian Vision-Centric Benchmark (CV-Bench) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To address the limitations of existing vision-centric benchmarks, we introduce the Cambrian VisionCentric Benchmark (CV-Bench). With 2638 manually-inspected examples, CV-Bench provides significantly more examples than other vision-centric MLLM benchmarks\u2014 $3.5\\times$ more than RealWorldQA [137] and $8.8\\times$ more than MMVP [126]. By repurposing standard vision benchmarks [18, $79,154]^{\\parallel}$ , we can assess models at classic vision tasks within a multimodal context. Leveraging the rich ground truth annotations from the benchmarks, we formulate natural language questions that probe the fundamental 2D and 3D understanding of the models. ", "page_idx": 3}, {"type": "text", "text": "As visualized in Fig. 11, CV-Bench evaluates 2D understanding via spatial relationships & object counting, and 3D understanding via depth order & relative distance. We refer details to Appendix E. ", "page_idx": 3}, {"type": "text", "text": "Finding 2: Existing vision benchmarks can be effectively repurposed into VQA questions, enabling the assessment of vision-centric MLLM capabilities. ", "page_idx": 3}, {"type": "text", "text": "2.3 Instruction Tuning Recipes ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "MLLMs start with pre-trained LLM and vision backbones, connecting these modules with a connector such as a projector (MLP). The original LLaVA [80, 82] proposes a 2-stage frozen training process: first, pre-training a connector between frozen LLM and vision backbones using adapter data, and then fine-tuning both the connector and LLM with instruction tuning data while leaving the vision encoder frozen. Various studies [27, 63, 81, 98] have drawn different conclusions regarding the optimal training methodology for MLLMs. Here, we revisit this topic with extensive experiments. ", "page_idx": 3}, {"type": "text", "text": "For our experiments, we tune a set of MLLMs using Vicuna-1.5-7B as the LLM backbone and each of our 23 vision models (Table 6) as the visual encoder. We use a 737K instruction tuning data mix for all experiments here (see Appendix H). All hyperparameters are matched across each experimental setting\u2014highlighting the impact of different tuning strategies with each visual encoder. All experimental settings and results are tabulated in Appendix F.2. ", "page_idx": 3}, {"type": "image", "img_path": "Vi8AepAXGy/tmp/fc9b68efb487386861bc5669c4c129e044a47020f26ee83cd7baa20fbf7b2e32.jpg", "img_caption": ["Figure 4: Effect of Training Recipe on Model Performance. Boxplots display the distribution of benchmark scores across benchmark categories for different training recipes and types of visual encoders. The four training recipes include freezing the visual encoder with various amounts of adapter data $\\mathbf{(0M_{\\lambda}\\ast}$ , $0.5\\mathrm{M}_{\\mathrm{^3(2)}}^{\\mathrm{2}}$ , $1.2\\mathrm{M}_{\\mathrm{3}}^{\\mathrm{3}}$ as well as unfreezing it with $1.2\\mathbf{M}\\boldsymbol{\\vartheta}$ adapter data. Amount of Adapter Data: All model types show increased performance on general and vision-centric benchmarks; knowledge benchmarks show mixed results; OCR & chart benchmarks benefit from more data for language-supervised models. Unfreezing: Unfreezing the visual encoder with $1.2\\mathbf{M}\\boldsymbol{\\vartheta}$ adapter data generally benefits all categories. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "One Stage vs Two Stage Training Recent work [63] advocates for skipping connector pre-training, claiming this \u201creduces compute cost without harming downstream performance.\u201d To explore whether this claim holds\u2014especially when using non-language-supervised visual encoders\u2014we conduct experiments using 0, 0.5M, and 1.2M adapter data. Following LLaVA\u2019s recipe [82], we tune only the connector on the adapter data during this first phase, before unfreezing the LLM and connector during instruction tuning on the 737K mix. Fig. 4 shows that pre-training the connector first enhances model performance and that more adapter data further improves performance across all domains. Thus, we subsequently adopt 2-stage training with 1.2M adapter data as our standard setup. ", "page_idx": 4}, {"type": "text", "text": "Finding 3: Two-stage training is beneficial; more adapter data further improves results. ", "page_idx": 4}, {"type": "text", "text": "Freeze vs Unfreeze Vision Encoder There are also mixed practices in freezing [63, 80, 82] or unfreezing [44, 81] vision backbones during fine-tuning. Some argue that unfreezing the vision backbone significantly degrades performance [63]. Our experiments demonstrate that unfreezing beneftis performance across all benchmarks except for a marginal change in knowledge benchmarks (Fig. 4). We suspect this is due to the composition of the 737K instruction tuning data and the LLMheavy focus of these benchmarks (see Section 2.1). We note that unfreezing the vision backbone introduces additional computational overhead, which prohibits testing on some larger vision models under current sharding strategies (see more details in Appendix H). ", "page_idx": 4}, {"type": "text", "text": "Finding 4: Unfreezing the vision encoder is widely beneficial. Language-supervised models always benefit; SSL models particularly benefit on vision-centric benchmarks. ", "page_idx": 4}, {"type": "text", "text": "2.4 MLLMs as a Visual Representation Evaluator ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As discussed in earlier sections, MLLMs provide a new interface to explore aspects of vision models beyond traditional benchmarks like ImageNet-1k linear probing. We study the 2-stage instruction tuning setting using 1.2M adapter data, 737K fine-tuning data, and frozen visual encoders to allow comparison of the widest range of models. ", "page_idx": 4}, {"type": "image", "img_path": "Vi8AepAXGy/tmp/0bd88297d0102658d23762c325e51b05f0a46df2a9f3b382163d178389d4d7c9.jpg", "img_caption": ["Figure 5: Evaluating Visual Representations with MLLMs While language-supervised models outperform self-supervised or other models, a well-trained self-supervised model like DINOv2 can also achieve competitive performance on vision-centric tasks. "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "Vi8AepAXGy/tmp/64c267638824006560411004ffe3dc35edab4374714a6c1b754beae9cef89971.jpg", "img_caption": ["Figure 6: Continued Fine-Tuning Narrows the Gap Between CLIP and DINOv2. Performance is compared with $0.7\\mathbf{M}$ and 5M instruction tuning data in both frozen $(\\frac{2\\pi}{3})$ and unfrozen $(\\wp)$ settings. DINOv2 shows significant performance improvement with increased data and unfreezing\u2014surpassing the $0.7\\mathbf{M}_{\\sun}^{\\ast}$ CLIP model in several benchmarks and narrowing the gap to the $5\\mathbf{M}\\boldsymbol{\\mathrm{\\textlangle}}\\mathbf{\\lambda}$ model in knowledge and vision-centric tasks. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "We evaluate on benchmarks detailed in Section 2.1, calculating the average performance\\*\\* for each category and visualize the results in Fig. 5 (full results in Appendix F). Our findings highlight the advantages of language-supervised models over non-CLIP models across all benchmark categories, with significantly better performance on chart and OCR-related benchmarks. We hypothesize that this is due to CLIP\u2019s training data, such as LAION [115], containing abundant OCR and text-heavy data, whereas SSL and other vision models primarily train on natural images with significantly less text content. It is also noteworthy that language-supervised models are typically trained with a very large pool of data, ranging from 400 million [109] to 10 billion [28] samples, whereas the largest vision self-supervised training dataset, like DINOv2, consists of only 142 million samples [103]. ", "page_idx": 5}, {"type": "text", "text": "Additionally, we observe that higher-resolution models particularly enhance performance on chart and vision-centric benchmarks while remaining neutral on general VQA and knowledge-based VQAs. While the majority of the backbones we examine are ViT-based [39], ConvNet-based architectures (such as OpenCLIP ConvNeXt [86]) are inherently well-suited for high-resolution image processing [130] and can produce superior results on OCR & Chart and Vision-Centric benchmarks. In vision-centric benchmarks, the gap between language-supervised and other types of vision models is smaller, with a well-trained self-supervised DINOv2 model even outperforming some language-supervised models. ", "page_idx": 5}, {"type": "text", "text": "Finding 5: High-res encoders greatly enhance performance on chart & vision-centric benchmarks, and ConvNet-based architectures are inherently well-suited for such tasks. ", "page_idx": 5}, {"type": "text", "text": "Narrowing the gap between Language- and Self-Supervised models Above, we observe that DINOv2 stands midway between SSL models and language-supervised models on general and knowledge benchmarks, even outperforming some language-supervised models on vision-centric benchmarks. Here, we study whether the continued finetuning of an MLLM based on a SSL model can achieve performance similar to that of a language-supervised model. Specifically, we scale up the instruction tuning data from 737K to 5M (see more details in Appendix G.5), and instruction tune MLLMs with DINOv2 ViT-L/ $14@336$ and OpenAI CLIP ViT-L/ $14@336$ encoders in both frozen and unfrozen settings. In Fig. 6, we observe that by unfreezing the vision backbone, the DINOv2-based MLLM fine-tuned with 5M data surpasses the MLLM trained with a CLIP model on $0.7\\mathbf{M}$ data. Additionally, the gap between DINOv2 and the CLIP models is reduced under the 5M setting. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Finding 6: Language supervision offers strong advantages, but the performance gap can be narrowed with SSL methods given enough data and proper tuning. ", "page_idx": 6}, {"type": "text", "text": "2.5 Combining Multiple Vision Encoders ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As observed in Fig. 5, different vision encoders excel in different aspects of MLLM performance. In this study, we explore the potential of combining multiple vision encoders to leverage their distinctive representations, aiming to build a more capable MLLM. Given that different vision encoders use varying architectures and image resolutions, we interpolate to a fixed number of visual tokens (576) in this subsection (see details in Appendix F.3). We then concatenate these tokens along the feature dimension, following a method similar to A-MoF proposed in [126]. ", "page_idx": 6}, {"type": "text", "text": "Our study indicates that adding a non-language-supervised model (DINOv2) can improve benchmark performance, especially in vision-centric tasks. Notably, even OCR benchmarks benefit from incorporating DINOv2. This highlights the importance of self-supervised learning models in complementing language-supervised models to achieve robust multimodal understanding. Detailed results and configurations are available in Appendix F.3. ", "page_idx": 6}, {"type": "text", "text": "However, this naive strategy has two limitations: 1) it employs interpolation, which can lead to information loss, especially with vision encoders with high-resolution feature maps, and 2) it treats each model equally via simple concatenation. Therefore, we seek a more effective strategy that can more flexibly leverage model combinations with less information loss. ", "page_idx": 6}, {"type": "text", "text": "Finding 7: Combining multiple vision encoders, including SSL models, can enhance MLLM performance across various benchmarks, particularly in vision-centric tasks. ", "page_idx": 6}, {"type": "text", "text": "3 Spatial Vision Aggregator (SVA): A New Connector Design ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To effectively aggregate features from multiple vision encoders and prevent the information loss introduced by interpolation, we use a set of learnable latent queries that interact with multiple vision features via cross-attention layers [37]. In particular, our approach incorporates two new vision-centric design principles: ", "page_idx": 6}, {"type": "text", "text": "1. We introduce spatial inductive bias by explicitly defining the aggregation space for each token in the query.   \n2. We aggregate vision features multiple times across the LLM layers, enabling the model to repeatedly access and integrate necessary visual information. ", "page_idx": 6}, {"type": "text", "text": "To facilitate information aggregation via cross-attention, we create a $C$ -dimension learnable latent token $\\mathbf{x}\\in\\mathbb{R}^{C}$ that is repeated $L\\times L$ times to form a 2D grid, serving as the query $\\mathbf{X}\\in\\mathbb{R}^{L^{2}\\times C}$ . The set of visual features $\\mathbf{F}$ from $N$ vision encoders serve as the context (i.e., key and value). We ensure the output resolution of every vision encoder is a multiple of $L$ . Formally, the feature map of the $k$ -th vision encoder $(\\mathbf{F}_{k})$ has a resolution of $m_{k}L\\times m_{k}L\\times C$ , where $m_{k}$ is a positive integer multiplier, and $L$ is the height/width of the learnable 2D grid with hidden dimension $C$ . ", "page_idx": 6}, {"type": "text", "text": "Spatial inductive bias To maintain the spatial structure during cross-attention, we align each token in the query with a specific sub-region of the feature maps in all vision encoders. Formally, a token at row $i$ and column $j$ in the query ${\\bf x}_{i,j}$ corresponds to the sub-region ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{F}_{k}[m_{k}\\cdot i:m_{k}\\cdot(i+1),m_{k}\\cdot j:m_{k}\\cdot(j+1)]\\in\\mathbb{R}^{m_{k}^{2}\\times C}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "of the $k$ -th vision feature map. As a result, a token ${\\bf x}_{i,j}$ aggregates a total of $\\sum_{k}m_{k}^{2}$ features from $N$ vision encoders through cross-attention (see Fig. 7-left). ", "page_idx": 6}, {"type": "text", "text": "Specifically, the updated query vector $\\mathbf{q}_{\\textbf{i,j}}^{*}\\in\\mathbb{R}^{1\\times C}$ at position $(i,j)$ is computed as ", "page_idx": 6}, {"type": "image", "img_path": "Vi8AepAXGy/tmp/f250788b823e46d39fd9ff671fd5a90ae5324d5fd0bf6aac1d8fcb5aa46e7166.jpg", "img_caption": ["Figure 7: Spatial Vision Aggregator (SVA). We propose SVA, a dynamic and spatially-aware connector that integrates multiple vision features with LLMs while reducing the number of tokens.9 "], "img_footnote": [], "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{q}^{*}{}_{i,j}=\\mathrm{softmax}\\left(\\frac{\\mathbf{q}_{i,j}\\cdot\\left[\\mathbf{k}_{i,j,1},\\mathbf{k}_{i,j,2},\\ldots,\\mathbf{k}_{i,j,N}\\right]^{\\top}}{\\sqrt{C}}\\right)\\left[\\mathbf{v}_{i,j,1},\\mathbf{v}_{i,j,2},\\ldots,\\mathbf{v}_{i,j,N}\\right],\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf q_{i,j}=\\mathbf W^{Q}\\mathbf x_{\\mathbf i,\\mathbf j}\\in\\mathbb R^{1\\times C},}\\\\ &{\\mathbf k_{i,j,k}=\\mathbf W_{k}^{K}\\mathbf F_{k}[m_{k}\\cdot i:m_{k}\\cdot(i+1),\\,m_{k}\\cdot j:m_{k}\\cdot(j+1)]\\in\\mathbb R^{m_{k}^{2}\\times C},}\\\\ &{\\mathbf v_{i,j,k}=\\mathbf W_{k}^{V}\\mathbf F_{k}[m_{k}\\cdot i:m_{k}\\cdot(i+1),\\,m_{k}\\cdot j:m_{k}\\cdot(j+1)]\\in\\mathbb R^{m_{k}^{2}\\times C}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Here, ${\\bf q}_{i,j}$ is the query vector at position $(i,j)$ , calculated using the query projection matrix $\\mathbf{W}^{Q}\\in$ $\\mathbb{R}^{C\\times C}$ . The key vectors $\\mathbf{k}_{i,j,k}$ and value vectors $\\mathbf{v}_{i,j,k}$ are computed for each vision encoder $k$ using their respective key and value projection matrices $\\mathbf{W}_{k}^{K}\\in\\mathbb{R}^{C\\times C}$ and $\\mathbf{W}_{k}^{V}\\in\\mathbb{R}^{C\\times C}$ . Since $\\sum_{k}m_{k}^{2}$ features are aggregated into a single token, we effectively reduce the number of tokens. ", "page_idx": 7}, {"type": "text", "text": "Multi-layer vision aggregation Although our proposal effectively aggregates features from multiple vision encoders, there is still potential information loss with high-resolution input (large $m_{k}$ ) or multiple vision encoders (large $N$ ). Here, a single token would have to handle a larger amount of context information during aggregation. To prevent this, we allow cross-attention to occur multiple times by inserting our proposal throughout the LLM layers\u2014allowing consistent access to the uncompressed visual information (see Fig. 7-right). ", "page_idx": 7}, {"type": "text", "text": "Hyperparameters To flexibly modulate capacity, we introduce two hyperparameters $D$ and $G$ , which indicate the number of cross-attention layers and distinct groups of learnable queries used between the vision models and the LLM, respectively. $D$ and $G$ are always set to 1 for cross-attention layers within LLM layers. We provide ablation studies on the selection of $D$ and $G$ in Appendix $_\\mathrm{H}$ . ", "page_idx": 7}, {"type": "table", "img_path": "Vi8AepAXGy/tmp/4ad78ee6d11e5f34e8b9fbeb988ac7748d68fa692650552c80487f6ff39da64f.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 1: Comparison between our SVA and other aggregation approaches. The SVA module consistently outperforms other baselines and excels in aggregating high-resolution vision information. ", "page_idx": 7}, {"type": "text", "text": "We demonstrate the efficacy of SVA module using the best vision model combination results from the previous section and a Vicuna-1.5-7B base LLM. Specifically, we employ a combination of four vision encoders: OpenAI CLIP ViT- $.L/14@336$ , SigLIP ViT-SO400M/14@384, OpenCLIP $\\operatorname{ConvNeXt-XXL@1024}$ , and DINOv2 ViT-L/14@518. We compare our method with two strong baselines: 1) concatenation-based [126] and 2) Re-sampler [11, 72]. Here, we include two variants of our SVA module. The standard one, \u201cSVA\u201d, uses $D=3$ , $G=1$ , and inserts cross-attention blocks inside the LLM with a layer stride of 3. To isolate the advantages of spatial inductive biases, we include another SVA variant, \u201cSVA-no-multi-agg\u201d, that does not add cross-attention blocks inside the LLM and sets $D=3$ and $G=3$ . Table 1 shows that SVA outperforms both baselines, with a significant improvement in the OCR & chart category. In contrast, the Resampler\u2014which lacks spatial inductive biases\u2014struggles to condense concatenated tokens from various vision towers into a limited number of learnable queries via global cross-attention. We also compare SVA with other connectors in Appendix H and show clear advantages. ", "page_idx": 7}, {"type": "image", "img_path": "Vi8AepAXGy/tmp/760c4ba4a40350801c7ccffe3945053092c02db21088ff3d3f8498e8b6fcb913.jpg", "img_caption": ["Figure 8: Cambrian-7M: A Large-Scale Curated Instruction Tuning Dataset for MLLM. Left: The inner circle shows the original distribution of Cambrian-10M. The outer circle shows the curated Cambrian-7M. Right: All the data sources in the Cambrian dataset as well as the ones filtered in data curation. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "Vi8AepAXGy/tmp/2f8eb0839bd4ada799b118fb54736289fb01b5c80014362f522878045f4bd6ff.jpg", "img_caption": ["Table 2: t value between 250k and Figure 9: Exploring instruction tun-Table 3: Performance improves with 350k obtains better performance. ing data mixture ratios. better instruction tuning data curation. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Finding 8: Spatial inductive bias and deep interaction between LLM and vision feature help to better aggregate and condense vision features. ", "page_idx": 8}, {"type": "text", "text": "4 Instruction Tuning Data for Training MLLMs ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "4.1 Data Collection ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Collecting Instruction Tuning Data from existing data sources Unlike language data, multimodal (visual) instruction-tuning data is much rarer and harder to collect. To address this, we use existing multimodal benchmarks and datasets involving visual interaction data, such as Visual Question Answering (VQA) and OCR data. To help maintain conversational abilities [147], we also collect a small volume of high-quality language-only instruction-following data. We categorize data into General conversation, OCR, Counting, Code, Math, Science, and Language-only data. We list the data sources in Fig. 8, and the details of data preparation in Appendix G. ", "page_idx": 8}, {"type": "text", "text": "Targeted Internet Data Collection Engine As observed in Fig. 8, there is an unbalanced distribution of data. Some categories, such as science, have very few data sources, and each source has limited samples. Inspired by previous works [73], we introduce a data engine to create large-scale, reliable, high-quality knowledge-based instruction tuning data (see Fig. 15). Details are in Appendix G.3. Our data engine produces a large volume of reliable scientific data, increasing the diversity in the data pool. We generate 161k science-related data points\u2014 $400\\%$ more than the previous combined data sources. ", "page_idx": 8}, {"type": "text", "text": "Cambrian-10M We create a large pool of instruction tuning data, which we refer to as Cambrian10M. This pool contains approximately $9784\\mathrm{k}$ data points, offering a diverse range of data for our work and future research. We visualize its composition in Fig. 8. ", "page_idx": 8}, {"type": "text", "text": "4.2 Data Curation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Cambrian-10M is a large pool of instruction tuning data sourced from a variety of data sources, with an unbalanced data ratio between categories. Here, we take a preliminary step to study data curation by improving data balancing and adjusting data ratios. ", "page_idx": 8}, {"type": "table", "img_path": "Vi8AepAXGy/tmp/0dc906e851fc0945d04bdb53dc65bc6ad727c961576022c0e28576e0909b8aff.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Data Balancing We follow previous work [109, 138] to set thresholds $t$ for the number of data points from a single data source. We choose $t=150k$ , $250k$ , $350k$ , and $450k$ in this section and observe an elbow effect in Table 2\u2014finding that a threshold between $250k$ and $350k$ work the best for Cambrian-10M. We also plot in Appendix G.4 the cumulative sum of counts for entries sorted by counts from tail to head and we see this intermediate threshold prevents explosive heavy tail. ", "page_idx": 9}, {"type": "text", "text": "Data Ratio Cambrian-10M is designed for visual instruction tuning. Given the various capabilities of different types of data, it is essential to balance the ratio of these data types. We conduct pilot experiments with a fixed dataset size of $1350\\mathbf{k}$ , examining the impact of different data ratios. We visualize the results in Fig. 9 and summarize our findings as follows: (i) Balancing General, OCR and Language data is crucial. (ii) Performance on knowledge-intensive tasks is influenced by multiple factors, often requiring a mix of OCR, chart, reasoning, and general perception. ", "page_idx": 9}, {"type": "text", "text": "Cambrian-7M By applying data flitering to Cambrian-10M with our identified data ratio, we create a smaller but higher-quality dataset called Cambrian-7M. Table 3 showcases the benefits of a wellbalanced and carefully curated dataset. Despite having fewer samples, Cambrian-7M demonstrates improved performance. We additionally apply system prompts in Cambrian-7M to avoid the \"answer machine phenomenon\", see more details in Appendix G.2. ", "page_idx": 9}, {"type": "text", "text": "5 State of the Art Performance ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Finally, we leverage the insights from all our previous studies to train a family of MLLMs we call Cambrian-1. We train models using LLM backbones of various scales: LLaMA-3-Instruct-8B [4], Vicuna-1.5-13B [151], and Hermes-2-Yi-34B [139]. Our vision component combines four models\u2014 OpenAI CLIP ViT-L/14@336, SigLIP ViT-SO400M/14@384, OpenCLIP ConvNeXt-XXL $@1024$ , and DINOv2 ViT-L/14@518 (Section 2.5)\u2014via the Spatial Vision Aggregator (Section 3). We pre-train the connector using $2.5\\mathrm{M}$ adapter data and instruction tune using our Cambrian-7M data mix (Section 4.2). Our models are evaluated on the benchmarks categorized in Section 2.1, with results presented in Table 4.\u2020\u2020. ", "page_idx": 9}, {"type": "text", "text": "Cambrian-1 surpasses open-source models like LLaVA-NeXT and Mini-Gemini. Thanks to the SVA, Cambrian-1 excels in tasks requiring high-resolution image processing, even with only 576 image tokens\u2014about 1/5 of the tokens used by LLaVA-NeXT and Mini-Gemini. Cambrian-1 also achieves comparable performance to the best proprietary models, such as GPT-4V, Gemini-Pro, and MM-1, on several benchmarks. We provide model weights, open-source code, datasets, and detailed recipes for model training and evaluation. We hope our work will strengthen the open research community and accelerate research in both visual representation learning and multimodal systems. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We are grateful to LLaVA [82] for their excellent codebase, which served as the launching point for our research. Special thanks to Hexu Zhao for extensive discussions and knowledge-sharing around FSDP and large-scale training techniques, and to Jiasen Lu for helpful discussions on TPU and JAX distributed training infrastructure. We also appreciate the assistance and responses from the PyTorchXLA team via GitHub. ", "page_idx": 10}, {"type": "text", "text": "We are thankful to Kaiming He for early discussions on multi-modal large language models. We also thank Zhuang Liu, Junlin Han, Yuexiang Zhai, Tianzhe Chu, Daohan Lu, Weiyang Jin, Boyang Zhang, and Jiayi Pan for reviewing this manuscript. We also acknowledge DeepSeek [87] for the paper template inspiration. ", "page_idx": 10}, {"type": "text", "text": "This work was primarily supported by the Google TPU Research Cloud (TRC) program and the Google Cloud Research Credits program (GCP19980904). Additional support was provided by the NYU IT High Performance Computing resources, services, and staff expertise. S.X. would like to thank the OpenAI Researcher Access program, Open Path AI Foundation, and an Amazon Research award for their support. S.T. is supported by the OpenAI SuperAlignment Fellowship, and E.B. is supported by the NDSEG Fellowship. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "[1] M. Acharya, K. Kafle, and C. Kanan. \u201cTallyQA: Answering complex counting questions\u201d. In: AAAI. 2019.   \n[2] A. Agrawal et al. \u201cDon\u2019t just assume; look and answer: Overcoming priors for visual question answering\u201d. In: CVPR. 2018. [3] A. Ahmadyan et al. \u201cObjectron: A Large Scale Dataset of Object-Centric Videos in the Wild with Pose Annotations\u201d. In: CVPR (2021). [4] AI@Meta. \u201cLlama 3 Model Card\u201d. In: (2024).   \n[5] H. A. Alawwad et al. \u201cEnhancing Textbook Question Answering Task with Large Language Models and Retrieval Augmented Generation\u201d. In: arXiv preprint arXiv:2402.05128 (2024).   \n[6] J.-B. Alayrac et al. \u201cFlamingo: a visual language model for few-shot learning\u201d. In: NeurIPS. 2022.   \n[7] T. Aquinas. Quaestiones Disputatae de Veritate. q.2 a.3 arg.19, 1259.   \n[8] Aristotle. Metaphysics. Ed. by T. by W. D. Ross. The Internet Classics Archive, 350BCE.   \n[9] M. Assran et al. \u201cSelf-supervised learning from images with a joint-embedding predictive architecture\u201d. In: CVPR. 2023.   \n[10] J. Bai et al. \u201cQwen Technical Report\u201d. In: arXiv preprint arXiv:2309.16609 (2023).   \n[11] J. Bai et al. \u201cQwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond\u201d. In: (2023).   \n[12] M. E. Banani et al. \u201cProbing the 3D Awareness of Visual Foundation Models\u201d. In: arXiv preprint arXiv:2404.08636 (2024).   \n[13] G. Baruch et al. \u201cARKitScenes - A Diverse Real-World Dataset for 3D Indoor Scene Understanding Using Mobile RGB-D Data\u201d. In: NeurIPS Datasets and Benchmarks Track (Round 1). 2021.   \n[14] J. Belouadi, A. Lauscher, and S. Eger. \u201cAutomatikz: Text-guided synthesis of scientific vector graphics with tikz\u201d. In: ICLR. 2024.   \n[15] R. Birkl, D. Wofk, and M. M\u00fcller. \u201cMidas v3. 1\u2013a model zoo for robust monocular relative depth estimation\u201d. In: arXiv preprint arXiv:2307.14460 (2023).   \n[16] A. F. Biten et al. \u201cLatr: Layout-aware transformer for scene-text vqa\u201d. In: CVPR. 2022.   \n[17] A. F. Biten et al. \u201cScene text visual question answering\u201d. In: ICCV. 2019.   \n[18] G. Brazil et al. \u201cOmni3d: A large benchmark and model for 3d object detection in the wild\u201d. In: CVPR. 2023.   \n[19] J. Buchner. imagehash (fork). https://github.com/JohannesBuchner/imagehash. 2021.   \n[20] H. Caesar et al. \u201cnuscenes: A multimodal dataset for autonomous driving\u201d. In: CVPR. 2020.   \n[21] J. Cha et al. \u201cHoneybee: Locality-enhanced projector for multimodal llm\u201d. In: CVPR. 2024.   \n[22] S. Cha et al. \u201cVisually Dehallucinative Instruction Generation: Know What You Don\u2019t Know\u201d. In: arXiv preprint arXiv:2402.09717 (2024).   \n[23] D. J. Chalmers. \u201cDoes Thought Require Sensory Grounding? From Pure Thinkers to Large Language Models\u201d. In: Proceedings and Addresses of the American Philosophical Association 97 (2023), pp. 22\u201345.   \n[24] Y. Chang et al. \u201cA survey on evaluation of large language models\u201d. In: ACM Transactions on Intelligent Systems and Technology 15.3 (2024), pp. 1\u201345.   \n[25] G. H. Chen et al. \u201cALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model\u201d. In: arXiv preprint arXiv:2402.11684 (2024).   \n[26] L. Chen et al. \u201cAre We on the Right Way for Evaluating Large Vision-Language Models?\u201d In: arXiv preprint arXiv:2403.20330 (2024).   \n[27] L. Chen et al. \u201cSharegpt4v: Improving large multi-modal models with better captions\u201d. In: arXiv preprint arXiv:2311.12793 (2023).   \n[28] X. Chen et al. \u201cPali: A jointly-scaled multilingual language-image model\u201d. In: ICLR. 2023.   \n[29] X. Chen, S. Xie, and K. He. \u201cAn empirical study of training self-supervised vision transformers\u201d. In: ICCV. 2021.   \n[30] Z. Chen et al. \u201cHow far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites\u201d. In: arXiv preprint arXiv:2404.16821 (2024).   \n[31] Z. Chen et al. \u201cFinqa: A dataset of numerical reasoning over financial data\u201d. In: EMNLP. 2021.   \n[32] Z. Cheng et al. \u201cHiTab: A hierarchical table dataset for question answering and natural language generation\u201d. In: ACL. 2022.   \n[33] M. Cherti et al. \u201cReproducible scaling laws for contrastive language-image learning\u201d. In: CVPR. 2023.   \n[34] W.-L. Chiang et al. \u201cChatbot arena: An open platform for evaluating llms by human preference\u201d. In: arXiv preprint arXiv:2403.04132 (2024).   \n[35] X. Chu et al. \u201cMobilevlm v2: Faster and stronger baseline for vision language model\u201d. In: arXiv preprint arXiv:2402.03766 (2024).   \n[36] M. Conover et al. Free Dolly: Introducing the World\u2019s First Truly Open Instruction-Tuned LLM. 2023. URL: https://www.databricks.com/blog/2023/04/12/dolly-firstopen-commercially-viable-instruction-tuned-llm (visited on 06/30/2023).   \n[37] W. Dai et al. \u201cInstructblip: Towards general-purpose vision-language models with instruction tuning\u201d. In: NeurIPS. 2024.   \n[38] H. Dong et al. \u201cRlhf workflow: From reward modeling to online rlhf\u201d. In: arXiv preprint arXiv:2405.07863 (2024).   \n[39] A. Dosovitskiy et al. \u201cAn image is worth 16x16 words: Transformers for image recognition at scale\u201d. In: ICLR. 2021.   \n[40] A. Fang et al. \u201cData filtering networks\u201d. In: ICLR. 2024.   \n[41] X. Fu et al. \u201cBLINK: Multimodal Large Language Models Can See but Not Perceive\u201d. In: arXiv preprint arXiv:2404.12390 (2024).   \n[42] J. Gao et al. \u201cG-llava: Solving geometric problem with multi-modal large language model\u201d. In: arXiv preprint arXiv:2312.11370 (2023).   \n[43] P. Gao et al. \u201cLlama-adapter v2: Parameter-efficient visual instruction model\u201d. In: arXiv preprint arXiv:2304.15010 (2023).   \n[44] P. Gao et al. \u201cSPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models\u201d. In: arXiv preprint arXiv:2402.05935 (2024).   \n[45] Y. Ge et al. \u201cPlanting a seed of vision in large language model\u201d. In: arXiv preprint arXiv:2307.08041 (2023).   \n[46] A. Geiger, P. Lenz, and R. Urtasun. \u201cAre we ready for Autonomous Driving? The KITTI Vision Benchmark Suite\u201d. In: CVPR. 2012.   \n[47] R. Geirhos et al. \u201cShortcut learning in deep neural networks\u201d. In: Nature Machine Intelligence (2020).   \n[48] R. Girshick et al. \u201cRich feature hierarchies for accurate object detection and semantic segmentation\u201d. In: CVPR. 2014.   \n[49] Google. Gemini. 2023.   \n[50] Y. Goyal et al. \u201cMaking the v in vqa matter: Elevating the role of image understanding in visual question answering\u201d. In: CVPR. 2017.   \n[51] D. Gurari et al. \u201cVizwiz grand challenge: Answering visual questions from blind people\u201d. In: CVPR. 2018.   \n[52] K. He et al. \u201cMasked autoencoders are scalable vision learners\u201d. In: CVPR. 2022.   \n[53] X. He et al. \u201cPathVQA: $30000+$ Questions for Medical Visual Question Answering\u201d. In: CoRR abs/2003.10286 (2020).   \n[54] T. Hiippala et al. \u201cAI2D-RST: A multimodal corpus of 1000 primary school science diagrams\u201d. In: Language Resources and Evaluation 55 (2021), pp. 661\u2013688.   \n[55] J. Hoffmann et al. \u201cTraining compute-optimal large language models\u201d. In: NeurIPS (2023).   \n[56] Y.-C. Hsiao, F. Zubach, M. Wang, et al. \u201cScreenqa: Large-scale question-answer pairs over mobile app screenshots\u201d. In: arXiv preprint arXiv:2209.08199 (2022).   \n[57] D. A. Hudson and C. D. Manning. \u201cGQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering\u201d. In: CVPR. 2019.   \n[58] A. Jaegle et al. \u201cPerceiver: General perception with iterative attention\u201d. In: ICML. 2021.   \n[59] J. Johnson et al. \u201cClevr: A diagnostic dataset for compositional language and elementary visual reasoning\u201d. In: CVPR. 2017.   \n[60] N. Jouppi et al. \u201cTpu v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings\u201d. In: Proceedings of the 50th Annual International Symposium on Computer Architecture. 2023.   \n[61] K. Kafle et al. \u201cDvqa: Understanding data visualizations via question answering\u201d. In: CVPR. 2018.   \n[62] S. Kantharaj et al. \u201cChart-to-text: A large-scale benchmark for chart summarization\u201d. In: ACL. 2022.   \n[63] S. Karamcheti et al. \u201cPrismatic vlms: Investigating the design space of visually-conditioned language models\u201d. In: arXiv preprint arXiv:2402.07865 (2024).   \n[64] M. Kazemi et al. \u201cGeomverse: A systematic evaluation of large models for geometric reasoning\u201d. In: 2023.   \n[65] A. Kembhavi et al. \u201cA diagram is worth a dozen images\u201d. In: ECCV. 2016.   \n[66] D. Kiela et al. \u201cThe hateful memes challenge: Detecting hate speech in multimodal memes\u201d. In: NeurIPS. 2020.   \n[67] G. Kim et al. \u201cDonut: Document understanding transformer without ocr\u201d. In: ECCV. 2022.   \n[68] A. Kirillov et al. \u201cSegment anything\u201d. In: ICCV. 2023.   \n[69] R. Krishna et al. \u201cVisual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations\u201d. In: IJCV (2016).   \n[70] LAION. laion/gpt4v-dataset. 2023.   \n[71] H. Lauren\u00e7on, L. Tronchon, and V. Sanh. \u201cUnlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset\u201d. In: arXiv preprint arXiv:2403.09029 (2024).   \n[72] H. Lauren\u00e7on et al. \u201cWhat matters when building vision-language models?\u201d In: arXiv preprint arXiv:2405.02246 (2024).   \n[73] A. C. Li et al. \u201cInternet Explorer: Targeted Representation Learning on the Open Web\u201d. In: ICML. 2023.   \n[74] A. C. Li et al. \u201cYour diffusion model is secretly a zero-shot classifier\u201d. In: ICCV. 2023.   \n[75] B. Li et al. LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild. 2024.   \n[76] L. Li et al. \u201cMultimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models\u201d. In: arXiv preprint arXiv:2403.00231 (2024).   \n[77] Y. Li et al. \u201cMini-gemini: Mining the potential of multi-modality vision language models\u201d. In: arXiv preprint arXiv:2403.18814 (2024).   \n[78] W. Lian et al. OpenOrca: An Open Dataset of GPT Augmented FLAN Reasoning Traces. https://https://huggingface.co/Open-Orca/OpenOrca. 2023.   \n[79] T.-Y. Lin et al. \u201cMicrosoft coco: Common objects in context\u201d. In: ECCV. 2014.   \n[80] H. Liu et al. \u201cImproved baselines with visual instruction tuning\u201d. In: arXiv preprint arXiv:2310.03744 (2023).   \n[81] H. Liu et al. LLaVA-NeXT: Improved reasoning, OCR, and world knowledge. 2024.   \n[82] H. Liu et al. \u201cVisual Instruction Tuning\u201d. In: NeurIPS. 2023.   \n[83] Y. Liu et al. \u201cMmbench: Is your multi-modal model an all-around player?\u201d In: arXiv preprint arXiv:2307.06281 (2023).   \n[84] Y. Liu et al. \u201cOn the hidden mystery of ocr in large multimodal models\u201d. In: arXiv preprint arXiv:2305.07895 (2023).   \n[85] Z. Liu and K. He. \u201cA Decade\u2019s Battle on Dataset Bias: Are We There Yet?\u201d In: arXiv preprint arXiv:2403.08632 (2024).   \n[86] Z. Liu et al. \u201cA convnet for the 2020s\u201d. In: CVPR. 2022.   \n[87] H. Lu et al. \u201cDeepSeek-VL: towards real-world vision-language understanding\u201d. In: arXiv preprint arXiv:2403.05525 (2024).   \n[88] P. Lu et al. \u201cDynamic prompt learning via policy gradient for semi-structured mathematical reasoning\u201d. In: ICLR. 2023.   \n[89] P. Lu et al. \u201cIconqa: A new benchmark for abstract diagram understanding and visual language reasoning\u201d. In: NeurIPS. 2021.   \n[90] P. Lu et al. \u201cInter-GPS: Interpretable geometry problem solving with formal language and symbolic reasoning\u201d. In: ACL. 2021.   \n[91] P. Lu et al. \u201cLearn to explain: Multimodal reasoning via thought chains for science question answering\u201d. In: NeurIPS. 2022.   \n[92] P. Lu et al. \u201cMathvista: Evaluating mathematical reasoning of foundation models in visual contexts\u201d. In: ICLR (2023).   \n[93] Z. Luo et al. \u201cWizardcoder: Empowering code large language models with evol-instruct\u201d. In: ICLR. 2024.   \n[94] A. Majumdar et al. \u201cOpenEQA: Embodied Question Answering in the Era of Foundation Models\u201d. In: 2nd Workshop on Mobile Manipulation and Embodied Intelligence at ICRA 2024. 2024.   \n[95] K. Marino et al. \u201cOK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge\u201d. In: CVPR. 2019.   \n[96] A. Masry et al. \u201cChartqa: A benchmark for question answering about charts with visual and logical reasoning\u201d. In: ACL. 2022.   \n[97] M. Mathew, D. Karatzas, and C. Jawahar. \u201cDocvqa: A dataset for vqa on document images\u201d. In: WACV. 2021.   \n[98] B. McKinzie et al. \u201cMm1: Methods, analysis & insights from multimodal llm pre-training\u201d. In: arXiv preprint arXiv:2403.09611 (2024).   \n[99] A. Mitra et al. Orca-Math: Unlocking the potential of SLMs in Grade School Math. 2024. arXiv: 2402.14830 [cs.CL].   \n[100] \u201cOCR-VQA: Visual Question Answering by Reading Text in Images\u201d. In: 2019.   \n[101] OpenAI. ChatGPT. 2022.   \n[102] OpenAI. gpt4o. 2024.   \n[103] M. Oquab et al. \u201cDinov2: Learning robust visual features without supervision\u201d. In: TMLR (2023).   \n[104] L. Ouyang et al. \u201cTraining language models to follow instructions with human feedback\u201d. In: NeurIPS. 2022.   \n[105] A. Parker. In the blink of an eye: how vision sparked the big bang of evolution. 2003.   \n[106] P. Pasupat and P. Liang. \u201cCompositional semantic parsing on semi-structured tables\u201d. In: ACL. 2015.   \n[107] J. Piaget, M. Cook, et al. The origins of intelligence in children. Vol. 8. 5. International Universities Press New York, 1952.   \n[108] J. Pont-Tuset et al. \u201cConnecting Vision and Language with Localized Narratives\u201d. In: ECCV. 2020.   \n[109] A. Radford et al. \u201cLearning transferable visual models from natural language supervision\u201d. In: ICML. 2021.   \n[110] R. Rafailov et al. \u201cDirect preference optimization: Your language model is secretly a reward model\u201d. In: NeurIPS. 2024.   \n[111] M. Roberts et al. \u201cHypersim: A Photorealistic Synthetic Dataset for Holistic Indoor Scene Understanding\u201d. In: ICCV. 2021.   \n[112] R. Rombach et al. \u201cHigh-Resolution Image Synthesis With Latent Diffusion Models\u201d. In: CVPR. 2022.   \n[113] O. Russakovsky et al. \u201cImagenet large scale visual recognition challenge\u201d. In: IJCV (2015).   \n[114] O. Sanseviero. LLM Evals and Benchmarking. 2022.   \n[115] C. Schuhmann et al. \u201cLaion-5b: An open large-scale dataset for training next generation image-text models\u201d. In: NeurIPS. 2022.   \n[116] D. Schwenk et al. \u201cA-OKVQA: A Benchmark for Visual Question Answering using World Knowledge\u201d. In: ECCV. 2022.   \n[117] M. Shridhar et al. \u201cALFWorld: Aligning Text and Embodied Environments for Interactive Learning\u201d. In: ICLR. 2021.   \n[118] C. Si et al. \u201cDesign2Code: How Far Are We From Automating Front-End Engineering?\u201d In: arXiv preprint arXiv:2403.03163 (2024).   \n[119] O. Sidorov et al. TextCaps: a Dataset for Image Captioning with Reading Comprehension. 2020. arXiv: 2003.12462 [cs.CV].   \n[120] A. Singh et al. \u201cTowards vqa models that can read\u201d. In: CVPR. 2019.   \n[121] S. Song, S. P. Lichtenberg, and J. Xiao. \u201cSun rgb-d: A rgb-d scene understanding benchmark suite\u201d. In: CVPR. 2015.   \n[122] Q. Sun et al. \u201cEva-clip: Improved training techniques for clip at scale\u201d. In: arXiv preprint arXiv:2303.15389 (2023).   \n[123] R. Tanaka, K. Nishida, and S. Yoshida. \u201cVisualMRC: Machine Reading Comprehension on Document Images\u201d. In: AAAI. 2021.   \n[124] B. J. Tang, A. Boggust, and A. Satyanarayan. \u201cVistext: A benchmark for semantically rich chart captioning\u201d. In: arXiv preprint arXiv:2307.05356 (2023).   \n[125] S. Tong, E. Jones, and J. Steinhardt. \u201cMass-producing failures of multimodal systems with language models\u201d. In: NeurIPS. 2024.   \n[126] S. Tong et al. \u201cEyes wide shut? exploring the visual shortcomings of multimodal llms\u201d. In: CVPR. 2024.   \n[127] H. Touvron et al. \u201cLLaMA 2: Open foundation and fine-tuned chat models\u201d. In: (2023).   \n[128] H. Touvron et al. \u201cLLaMA: Open and efficient foundation language models\u201d. In: arXiv preprint arXiv:2302.13971 (2023).   \n[129] H. Tu et al. \u201cHow many unicorns are in this image? a safety evaluation benchmark for vision llms\u201d. In: arXiv preprint arXiv:2311.16101 (2023).   \n[130] K. Vishniakov, Z. Shen, and Z. Liu. \u201cConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy\u201d. In: ICML. 2024.   \n[131] J. Wang et al. \u201cTo see is to believe: Prompting gpt-4v for better visual instruction tuning\u201d. In: arXiv preprint arXiv:2311.07574 (2023).   \n[132] K. Wang et al. \u201cMeasuring Multimodal Mathematical Reasoning with MATH-Vision Dataset\u201d. In: arXiv preprint arXiv:2402.14804 (2024).   \n[133] J. Wei et al. \u201cChain-of-thought prompting elicits reasoning in large language models\u201d. In: NeurIPS. 2022.   \n[134] C. Wendler. wendlerc/RenderedText. 2023.   \n[135] H. Wu et al. \u201cQ-instruct: Improving low-level visual abilities for multi-modality foundation models\u201d. In: arXiv preprint arXiv:2311.06783 (2023).   \n[136] P. Wu and S. Xie. \u201cV\\*: Guided Visual Search as a Core Mechanism in Multimodal LLMs\u201d. In: CVPR. 2024.   \n[137] xAI. grok. 2024.   \n[138] H. Xu et al. \u201cDemystifying clip data\u201d. In: ICLR. 2024.   \n[139] A. Young et al. \u201cYi: Open foundation models by 01. ai\u201d. In: arXiv preprint arXiv:2403.04652 (2024).   \n[140] L. Yu et al. Modeling Context in Referring Expressions. 2016. arXiv: 1608.00272 [cs.CV].   \n[141] T. Yu et al. \u201cRlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback\u201d. In: arXiv preprint arXiv:2312.00849 (2023).   \n[142] X. Yue et al. \u201cMammoth: Building math generalist models through hybrid instruction tuning\u201d. In: ICLR. 2024.   \n[143] X. Yue et al. \u201cMmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi\u201d. In: CVPR. 2024.   \n[144] M. Yuksekgonul et al. \u201cWhen and why vision-language models behave like bags-of-words, and what to do about it?\u201d In: ICLR. 2022.   \n[145] X. Zhai et al. \u201cSigmoid loss for language image pre-training\u201d. In: ICCV. 2023.   \n[146] Y. Zhai et al. \u201cFine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning\u201d. In: arXiv preprint arXiv:2405.10292 (2024).   \n[147] Y. Zhai et al. \u201cInvestigating the catastrophic forgetting in multimodal large language models\u201d. In: CPAL. 2024.   \n[148] C. Zhang et al. \u201cRaven: A dataset for relational and analogical visual reasoning\u201d. In: CVPR. 2019.   \n[149] Y. Zhang et al. \u201cLlavar: Enhanced visual instruction tuning for text-rich image understanding\u201d. In: arXiv preprint arXiv:2306.17107 (2023).   \n[150] Y. Zhao et al. \u201cPytorch fsdp: experiences on scaling fully sharded data parallel\u201d. In: arXiv preprint arXiv:2304.11277 (2023).   \n[151] L. Zheng et al. \u201cJudging llm-as-a-judge with mt-bench and chatbot arena\u201d. In: NeurIPS. 2024.   \n[152] T. Zheng et al. \u201cOpenCodeInterpreter: Integrating Code Generation with Execution and Refinement\u201d. In: arXiv preprint arXiv:2402.14658 (2024).   \n[153] V. Zhong, C. Xiong, and R. Socher. \u201cSeq2sql: Generating structured queries from natural language using reinforcement learning\u201d. In: 2017.   \n[154] B. Zhou et al. \u201cSemantic understanding of scenes through the ade20k dataset\u201d. In: IJCV (2019).   \n[155] K. Zhou et al. \u201cDon\u2019t Make Your LLM an Evaluation Benchmark Cheater\u201d. In: arXiv preprint arXiv:2311.01964 (2023).   \n[156] B. Zhu et al. Starling-7b: Improving llm helpfulness & harmlessness with rlaif. 2023.   \n[157] D. Zhu et al. \u201cMinigpt-4: Enhancing vision-language understanding with advanced large language models\u201d. In: arXiv preprint arXiv:2304.10592 (2023).   \n[158] F. Zhu et al. \u201cTAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance\u201d. In: ACL. 2021.   \n[159] Y. Zhu et al. \u201cVisual7w: Grounded question answering in images\u201d. In: CVPR. 2016. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "A Broader Discussion ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We advocate for using MLLMs as an interface to evaluate visual representations, as previous benchmarks are becoming saturated and do not adequately reflect the diverse and complex perception challenges of the real world. Our work highlights the current gap between language-supervised models and self-supervised learning models and demonstrates the potential of bridging this gap. However, it is known that features of language-supervised models behave like a bag-of-words [125, 144], underscoring the need for advancements in vision-only models to ensure better visual understanding. We hope to inspire future research into developing better vision-only models intended to be adapted into the MLLM setting, that more effectively leverage large-scale datasets [85] and preserve the advantages in visual grounding [126]. ", "page_idx": 17}, {"type": "text", "text": "As we observe in Table 4, a well-trained open-source model such as Cambrian-1 can match or even outperform proprietary models on many existing benchmarks. However, the use and evaluation of MLLMs extend far beyond the current scope of benchmarks\u2014to conversational ability, creativity, reliability, and overall user experience. Developing models solely based on benchmark results can result in an \u201canswer machine\u201d, over-optimized for benchmarks but lacking in practical interaction capabilities. Therefore, the development of MLLMs that better align with human and societal needs is a continuously evolving process, both in terms of evaluation and model development. ", "page_idx": 17}, {"type": "text", "text": "Our current Cambrian-1 model uses a moderate number of visual tokens and does not adopt the anyresolution strategy [30, 77, 81] to handle ultra high-resolution images or those with extreme aspect ratios, which require a larger number of visual tokens. For specialized tasks like $\\mathrm{\\DeltaV^{*}}$ Bench [136], which require processing ultra high-resolution images, increasing the resolution and number of visual tokens could lead to an HD version of the Cambrian-1 model. ", "page_idx": 17}, {"type": "text", "text": "One promising direction for post-training alignment is through reinforcement learning rather than supervised fine-tuning. Many MLLM studies, including Cambrian, primarily focus on supervised fine-tuning. Yet, recent advancements in LLMs [38, 104, 110, 156] and some in MLLMs [141, 146] suggest that reinforcement learning from human or environmental feedback can further improve models, potentially surpassing the limits of supervised fine-tuning, especially in decision-making abilities. ", "page_idx": 17}, {"type": "text", "text": "Cambrian-10M (Fig. 8 and Section 4) provides a rich pool of data for studying data curation in fine-tuning MLLMs. Our work takes an initial step in curating higher-quality data to enable more efficient and effective instruction tuning. We believe there is room for further improvement in the data curation pipeline, and we hope this work can serve as a foundation for future research. ", "page_idx": 17}, {"type": "text", "text": "Additionally, training large-scale models requires careful design of model sharding, data sharding, and infrastructure adaptations. In this work, we train our model on TPU-V4 [60] with FSDP [150] using TorchXLA. We share our experiences, technical challenges, and solutions in Appendix C. We also open-source our implementation and provide tutorials to help the community undertake large-scale training more efficiently. ", "page_idx": 17}, {"type": "text", "text": "To conclude, Cambrian-1 introduces a family of state-of-the-art MLLM models that achieve top performance across diverse benchmarks and excel in visual-centric tasks. We provide model weights, open-source code, datasets, and detailed recipes for model training and evaluation. We hope our work will strengthen the open research community and accelerate future advancements in both visual representation learning and multimodal systems. ", "page_idx": 17}, {"type": "text", "text": "B Multimodal LLMs: Preliminaries and Related Work ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The key components of MLLM research include the Large Language Model, Visual Encoder, Multimodal Connector, Data Curation Pipeline, Instruction Tuning Strategy, and Evaluation & Benchmarking. Each component has its intricacies, and understanding their interactions presents significant challenges. Our study investigates these aspects from a vision-centric perspective. ", "page_idx": 17}, {"type": "text", "text": "Large Language Model Advanced LLMs [4, 101, 127, 128] are the foundation of an MLLM. After instruction-tuning on multimodal data, these models can be prompted to solve a variety of complex tasks and generate free-form responses leveraging input from a visual encoder. Recent MLLM research focuses on enhancing the LLM backbone [10, 75, 81], resulting in improved performance on benchmarks like MMMU [143] and AI2D [54]. However, this improvement raises the concern that our current multimodal evaluation is biased by the development of LLMs, neglecting a true assessment of visual perception. For example, some benchmarks such as MMMU [143] are dominated by LLM capabilities, underscoring the need for evaluations that genuinely assess multimodality (see Section 2.1). ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Visual Encoder Most MLLMs utilize language-supervised models like CLIP [109, 122, 145], which benefit from the massive scale of noisy web image-text data. However, there is a much broader pool of visual models that learn representations using only visual signals\u2014such as self-supervised models [9, 103], segmentation [68], depth-supervised [15], and diffusion models [74, 112] (see Fig. 2). Recent work [87, 126] advocates for incorporating these diverse vision models into MLLMs. In this study, we systematically examine the impact of various vision backbones on MLLM performance (Section 1) and explore the benefits of model ensembles (Section 2.5). ", "page_idx": 18}, {"type": "text", "text": "Multimodal Connector Representations from a visual encoder cannot be natively processed by an LLM\u2014they must be mapped into the LLM token space by a connector. There are three primary approaches to connector design: Resamplers [6], Q-Formers [11, 37], and MLP Projectors [43, 80, 82, 157]. We begin our exploration using an MLP projector, which is highly effective but presents challenges: the visual token count grows quadratically with image resolution, inhibiting scaling context length input resolution. For example, LLaVA-Next [81] requires 2880 visual tokens to process one 672px image. To address this, we explore new vision connector designs that process high-resolution images while maintaining a smaller number of visual tokens (Section 3). ", "page_idx": 18}, {"type": "text", "text": "Instruction Tuning Data Visual instruction tuning data is crucial but hard to collect, as it rarely naturally exists on the internet. Previous work [37, 80, 98] transforms existing VQA benchmarks [50, 69] into instruction tuning data, showing marked MLLM performance improvements. With this inspiration, we collect all VQA benchmarks and visual interaction data that we can find (Fig. 8), study data balancing and category mixtures (Section 4.2), and develop an internet data collection engine to fill in the gaps (Section 4.1). ", "page_idx": 18}, {"type": "text", "text": "Instruction Tuning Most current MLLMs leverage pre-trained LLMs and visual encoders, finetuning the LLM and connector using visual instruction tuning data. Some aspects of the tuning recipe are up for debate, including whether to pre-train the connector before joint fine-tuning with the LLM , and whether to freeze or unfreeze the vision encoder during fine-tuning [63, 98]. Additionally, some recent proprietary models explore end-to-end training from scratch [49, 102]. In this work, we use pre-trained models and revisit the debated recipe aspects with extensive studies, providing more insights for future MLLM research (Section 2.3). ", "page_idx": 18}, {"type": "text", "text": "Evaluation & Benchmarking There is an extensive set of benchmarks that evaluate various aspects of MLLMs, such as perception [45, 83], knowledge [91, 92], chart interpretation [84, 96], and visual capabilities [126, 136]. Instead of over-optimizing for specific benchmarks, we advocate for examining aggregates of benchmarks that focus on specific capabilities. To achieve this, we analyze existing benchmarks, categorize them, and assess the extent to which they measure multimodality (Section 2.1). Additionally, we find there are currently few benchmarks focused on vision-centric evaluation, and those that do exist contain relatively few images, leading to higher variance during evaluation. To address this issue, we propose a new vision-centric benchmark by reformulating classic vision tasks (Section 2.2). ", "page_idx": 18}, {"type": "text", "text": "C Training, Infrastructure, and Implementation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "All models in this paper were trained using TPU-V4 pods [60]; we evaluate using NVIDIA A6000, A100, and H100 cards. The experiments in Section 2.4 require less than 24 hours on a TPU-V4-128, while our final Cambrian-1 models are trained in less than 4 days on a TPU-V4-512. ", "page_idx": 18}, {"type": "text", "text": "To enable and facilitate large-scale parallel training on TPUs, we employ TorchXLA with FSDP [150] to handle training sharding and parallelism. Training a large-scale multimodal model with TorchXLA on TPU is a challenging journey, as there are no open-source codebases and many critical features are not supported in the TorchXLA or TorchXLA FSDP libraries. To provide a brief taste of the difficulties: TPUs require a static graph throughout the program, which requires ground-up rewrites of dynamically-written open-source PyTorch codebases; model resuming is not implemented in TorchXLA, which is especially crucial when training on preemptable TPUs; existing TorchXLA FSDP tutorials fail to compile due to version changes in TorchXLA, updates in Hugging Face ", "page_idx": 18}, {"type": "text", "text": "Transformers & Accelerate, or simply inherent issues with the tutorial; loading very large models (over 30 billion parameters) with the TorchXLA FSDP library is natively impossible due to the 100GB memory constraints of TPU-V4s, and requires extensive workarounds. ", "page_idx": 19}, {"type": "text", "text": "To this end, we have rewritten or developed many new functions to make this research possible. For instance, we rewrote the TorchXLA FSDP Sharding API to load very large models; we implemented model resuming on TorchXLA; we rewrote parts of the Hugging Face Transformers FSDP and gradient checkpointing implementations to enable large-scale FSDP training. We are committed to open-sourcing our codebase and publishing a comprehensive tutorial to share our insights, with the hope of inspiring and supporting future research and open-source contributions to the TPU and TorchXLA ecosystem. ", "page_idx": 19}, {"type": "text", "text": "D Analyzing the Benchmarks ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "MLLM Benchmark Performance Confusion Matrix ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We evaluate the benchmark scores for our one-stage, two-stage finetune-only and hybrid models, and then plot the correlation matrix for the pool of MLLM benchmarks. The correlation plot displays in Fig. 10. The result demonstrates that MMMU is less correlated in measuring model performance to other benchmarks. Nonetheless, we acknowledge it is widely used and therefore cluster it into knowledge-based QAs based on the nature of their questions. ", "page_idx": 19}, {"type": "image", "img_path": "Vi8AepAXGy/tmp/5c3877b89715e1967ddcae579732ab22d609862d9b06e6931e52fc98fe960c81.jpg", "img_caption": ["Figure 10: Correlation matrix for MLLM benchmarks. The correlation matrix for MLLM benchmarks with respect to different vision backbones. The correlation matrix helps us to analysis and group benchmarks. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "E Cambrian Vision-Centric Benchmark (CV-Bench) ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "CV-Bench Curation Below we describe the procedure for programmatically constructing questions for each task. To ensure reliability, we also manually inspect each question, removing those that are unclear, ambiguous, or erroneous. ", "page_idx": 19}, {"type": "text", "text": "Spatial Relationship $(2D)$ . We consider images with two distinct ground-truth object categories and use visual prompts (bounding boxes) to avoid ambiguity when multiple instances are present. In these questions, we designate an anchor object, and the question asks for the direction of the other object relative to this anchor. ", "page_idx": 19}, {"type": "text", "text": "Object Counting $(2D)$ . This tests the model\u2019s ability to count objects. When generating options for these questions, we construct multiple-choice options that are similar to the correct answer. For example, if the correct answer is 4, the options might be 2, 3, 4, 5, & 6. We also include existence check examples where the correct count is 0. ", "page_idx": 19}, {"type": "image", "img_path": "Vi8AepAXGy/tmp/b385c395240639d464eaf8de36141048581f3ef53c91bbd59b02038a3924aa56.jpg", "img_caption": ["Figure 11: Cambrian Vision-Centric Benchmark (CV-Bench). We repurpose standard vision benchmarks to evaluate the fundamental 2D and 3D visual understanding of MLLMs. See Section 2.2 for more details. "], "img_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "Vi8AepAXGy/tmp/60fabdf0b9f8e6f6feeb56f470535950e6824ed6c68d87b2fca93bb0091c4793.jpg", "table_caption": ["Table 5: Breakdown of the 2D and 3D tasks evaluated in the Cambrian Vision-Centric Benchmark (CV-Bench). The examples are sourced from ADE20K [154], COCO [79], and Omni3D [18]. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Depth Order $(3D)$ . We consider images with two distinct categories (i.e., object A and object B) and use visual prompts (e.g., bounding boxes with two different colors) to avoid ambiguity. We define \u201ccloser\u201d as follows: object A is closer to the camera than object B only if the farthest vertex of object A is closer\u2021\u2021 to the camera than the nearest vertex of object B by a specified offset. ", "page_idx": 20}, {"type": "text", "text": "Relative Distance $(3D)$ . We consider images with three distinct categories (i.e., anchor, object A, and object B), and use visual prompts (e.g., bounding boxes with three different colors) to avoid ambiguity. Object A is closer than object B only if the farthest distance from A\u2019s vertices is shorter than the shortest distance from B\u2019s vertices to the anchor object by a certain offset. ", "page_idx": 20}, {"type": "text", "text": "Curation Procedure We provide an overview of the data curation process in Fig. 12, which is conducted in a semi-automatic manner. The procedure consists of two main steps: ", "page_idx": 20}, {"type": "text", "text": "First, using the original benchmarks and their associated ground truth annotations, we generate query and answer pairs. These pairs are tailored to specific tasks: 2D-related tasks with COCO and ADE20K datasets, and 3D-related tasks with Omni3D. ", "page_idx": 20}, {"type": "text", "text": "Second, after generating the query and answer pairs, we engage human experts to manually filter out any incorrect or ambiguous queries to enhance the quality of benchmark. Each query is assigned one of three statuses: accepted (used as is), modified (where the incorrect answer is modified), and rejected (queries that are ambiguous, such as those too small or difficult to discern, even for human experts). ", "page_idx": 20}, {"type": "text", "text": "Following this two-stage process, we finalize the benchmark, which results in a total of 2638 image queries with improved accuracy and reliability. Subsequently, we will discuss the methods of human verification and the evaluation metrics used in this process. ", "page_idx": 20}, {"type": "text", "text": "Human verification ", "text_level": 1, "page_idx": 20}, {"type": "image", "img_path": "Vi8AepAXGy/tmp/aa90faceaf1094299e992b857c59feba6300cc083b380fdc5ae97abe795a29a8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 12: CV-CB Benchmark Filtering. We reformulate classic 2D and 3D CV benchmarks into Q&A questions to evaluate MLLM\u2019s visual capabilities. ", "page_idx": 21}, {"type": "text", "text": "There are multiple reasons for the above generated data to be inaccurate. One of the main reasons is sparse annotations, but occasionally there could be wrong annotations as well. ", "page_idx": 21}, {"type": "text", "text": "Thus, we need manual inspection to change/remove these examples generated. Here are a few criteria we followed while manually filtering COCO and ADE20k data. ", "page_idx": 21}, {"type": "text", "text": "For Counting question types, if all instances of a category are not annotated, the ground truth would have lower count than the actual number of instances appearing in the image. In a few cases where the image distinctly has different countable instances of the object, we change the options/answer. In case the count is ambiguous, we reject the data sample altogether. ", "page_idx": 21}, {"type": "text", "text": "For Relative Distance question types without annotation, if the question is asked about two objects A and B and if there are two instances of a specific category (say A), the relative location of A w.r.t B can be have multiple correct answers. We reject the sample in this case. We also reject cases with clear incorrect annotations. ", "page_idx": 21}, {"type": "text", "text": "Benchmark Evaluation To ensure that equal importance is given to both 2D and 3D tasks, we use an evaluation metric that is the average of the accuracies obtained from these tasks. Specifically, the overall performance is calculated as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{Accuracy}_{2D}=\\left(\\frac{\\mathrm{Accuracy}_{C O C O}+\\mathrm{Accuracy}_{A D E20k}}{2}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{Overall\\Accuracy}=\\left(\\frac{\\mathrm{Accuracy}_{2D}+\\mathrm{Accuracy}_{3D}}{2}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "F Vision Models in MLLMs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "As mentioned in Section 2.4, we use MLLM as an interface to evaluate vision model\u2019s different capabilities. Here, we list details in terms of the model selection, full results, and data split. ", "page_idx": 21}, {"type": "text", "text": "F.1 Details of Vision Models ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In our exploration of versatile vision models, we select thirteen models and group them into four categories: language-supervised models (i.e., OpenAI CLIP [109], SigLIP [145], DFN-CLIP [40], EVA-CLIP [122] and OpenCLIP [33]), self-supervised models (i.e., DINOv2 [103], I-JEPA [9], MAE [52], MoCo v3 [29]), class-supervised models (ImagetNet22K ViT [39]) and other models such as stable diffusion [112], segmentation models like SAM [68], and depth estimation models like MiDaS [15]. To provide a clear understanding of the specific variant evaluated, we meticulously detail their backbone architectures, resolution, number of tokens, and hidden dimension sizes in Table 6. For models that output a large number of patches in the last layer (e.g., SAM and ConvNeXt) we interpolate to the number of tokens specified in Table 6, and denote interpolation with I. ", "page_idx": 21}, {"type": "table", "img_path": "Vi8AepAXGy/tmp/b277d85444fc805b75159ec8e109d012059e0ee9af9cb37f2440baa627771c09.jpg", "table_caption": [], "table_footnote": ["Table 6: Catalog of all vision backbones tested. I denotes that the visual tokens have been interpolated down to the specified length. "], "page_idx": 22}, {"type": "table", "img_path": "Vi8AepAXGy/tmp/c5e3560162c1b3b9219a1634a32db73a6bbf6489d473459b620ca29a99e5788f.jpg", "table_caption": ["Table 7: Linear Probing Results of Different Vision Backbones "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "F.2 Full Results of Different Vision Backbones ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Here, we also show a ranking version of Fig. 5. We observe a clear advantage of CLIP models over non-CLIP models. We also observe that within the family of CLIP models, each model perform differently in different domains. This provide insight into both vision model development and data curation in training large vision models. ", "page_idx": 22}, {"type": "text", "text": "For the above-listed vision models in Table 6, they are integrated as the vision encoder of the MLLMs. These MLLMs are trained on various adapter adapter data splits (i.e., 0, 0.5 and 1.2 million), and subsequently fine-tuned on a 737K instruction tuning dataset provided in LLaVA-1.5[80]. For the adapter data splits, the 0M split indicates that no initial adapter pertaining phase is employed for the ", "page_idx": 22}, {"type": "table", "img_path": "Vi8AepAXGy/tmp/a015cd430dab427d41b6b24edfcecf0cd36dc0d571f46e5a2a25f59c69eaabc4.jpg", "table_caption": ["Language Supervised ", "Self-Supervised & Other "], "table_footnote": ["Table 8: Benchmark performance rankings for MLLMs built upon language-supervised and self-supervised vision encoders across all benchmarks (All), and across general (G), knowledge (K), OCR & chart (O), and vision-centric (V) benchmark categories. Full results for all models on each benchmark are tabulated in Table 11. \\*We add DINOv2 here to show its standing amongst the CLIP models. "], "page_idx": 23}, {"type": "text", "text": "MLLM. The 0.5M data split utilizes the 558K adapter data from LLaVA-1.5[80], while the 1.2M variant uses ShareGPT4V-PT dataset [27]. ", "page_idx": 23}, {"type": "text", "text": "0M Adapter Data $\\mathbf{+737K}$ Instruction Tuning Data As shown in Table 9, we provide 20 results for different variants of the above-mentioned thirteen vision backbones. Among them, languagesupervised models show superior performance. Especially, OpenCLIP ConvNeXT-XXL $@$ 1024 model surpasses all other models on DocVQA with over $12\\%$ , indicating its potential to handle OCR-related benchmarks. ", "page_idx": 23}, {"type": "table", "img_path": "Vi8AepAXGy/tmp/a8f34dc1b7a91b2dedd5d155f662f9ce795c2a5f37344f5ac9247387256804b1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 9: All Benchmark Results for 0M Adapter Data $^+$ 737K Instruction Tuning Data ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "0.5M Adapter Data $\\mathbf{+737K}$ Instruction Finetune As shown in Table 10 and Table 9, the inclusion of an alignment stage with $0.5\\mathrm{M}$ data split results in a notable increase in performance for DFN-CLIP $\\mathrm{ViT-H}/14@378$ , from 36.21 to 49.94. This substantial improvement highlights the value of the alignment stage for enhancing certain vision backbones, suggesting its importance in harnessing the full potential of vision models. ", "page_idx": 23}, {"type": "text", "text": "1.2M Adapter Data $\\mathbf{+737K}$ Instruction Finetune As we increase the amount of data in the alignment phase, we observe a consistent performance improvement for SigLIP ViT-SO400M/14@384 from 46.79 to 49.72 to 53.09 across 0M, 0.5M to 1.2M data splits as shown in Table 9, Table 10 and Table 11. ", "page_idx": 23}, {"type": "table", "img_path": "Vi8AepAXGy/tmp/e0951db7c8e78cc754fbf90e158fdf4a0f5b29a63c4cd3fb48be5d48f2e21257.jpg", "table_caption": ["Table 10: All Benchmark Results for 0.5M Adapter Data $^+$ 737K Instruction Tuning Data "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "table", "img_path": "Vi8AepAXGy/tmp/4ee7c737f8c8d9bd42c69e26e9ab17e0434246cc3b8a873152aa7c229448a191.jpg", "table_caption": ["Table 11: All Benchmark Results for 1.2M Adapter Data $^+$ 737K Instruction Tuning Data "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "1.2M Adapter Data $\\mathbf{+737K}$ Instruction Finetune with Unfrozen Vision Model Here, we present the results of different vision models trained with $1.2\\mathfrak{m}$ adapter data and 737K instruction tuning data in Appendix F.2. Comparing to Appendix I.2, we observe nearly all the models see improvement on most of the benchmarks, especially on the OCR & Chart and Vision-Centric benchmarks. ", "page_idx": 24}, {"type": "text", "text": "1.2M Adapter Data $\\mathbf{\\Gamma}+\\mathbf{5}\\mathbf{M}$ Instruction Finetune We present the results of 5M instruction tuning experiments in Fig. 6 here. In Table 12, we observe that after $5\\mathrm{m}$ instruction tuning, the gap between DINOv2 and CLIP models continue to bridge on general, knowledge and vision-centric benchmarks. ", "page_idx": 24}, {"type": "text", "text": "F.3 Model Ensemble ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Model Ensemble Details We introduce the implementation details of the model ensemble in Section 2.5. For a given image, the image passes through each vision encoder to obtain the features from the last layer. The shape of each model\u2019s output differs depending on the resolution and patch size of each vision model. To resolve these differences, we interpolate the output of each model to a fixed number of tokens, using 576 tokens in our implementation, as described in Section 2.5. Our example code for interpolation can be seen below. ", "page_idx": 24}, {"type": "table", "img_path": "Vi8AepAXGy/tmp/9f9047476caefe4216c25bd314c63947df485715f8b9d1e0e71618a62cb60e2e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "Vi8AepAXGy/tmp/703bb7e0228a7e2d5867050093cd001aaf87236237334d013ab05f3d0ca09a5e.jpg", "table_caption": ["Table 12: All Benchmark Results for 1.2M Adapter Data $^+$ 737K Instruction Tuning Data with Unfrozen vision model. ", "Table 13: All Benchmark Results for 1.2M Adapter Data $^+$ 5M Instruction Tuning Data "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "# Example code for interpolation   \nb, num_tokens, dim $=$ image_features.shape   \nif num_tokens $!=$ self.image_token_len: target_h $=$ target_w $=$ int(np.sqrt(self.image_token_len)) h $=\\texttt{w}=$ int(np.sqrt(num_tokens)) image_features $=$ image_features.view(b, h, w, dim) image_features $=$ image_features.permute(0, 3, 1, 2).contiguous() image_features $=\\textsf{F}$ .interpolate(image_features), size=(target_h, target_w), mode=\u2019bilinear\u2019, align_corners=False) image_features $=$ image_features.permute(0, 2, 3, 1).contiguous().flatten(1, 2) ", "page_idx": 25}, {"type": "text", "text": "We then concatenate the model outputs along the feature dimension and use a larger MLP to project the concatenated visual tokens into the LLM token space. ", "page_idx": 25}, {"type": "text", "text": "Full results on Model Ensemble We present all the benchmarks from the model ensemble experiment in Section 2.5 in Table 14. As discussed in Section 1 and Section 2.4, this comprehensive view of benchmarks provides a better understanding of the model\u2019s performance compared to simply averaging across benchmarks. Adding a vision-only SSL model enhances the MLLM\u2019s performance in vision-centric benchmarks while maintaining strong capabilities in other categories. ", "page_idx": 25}, {"type": "text", "text": "G Data ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "G.1 Catalog of Visual Instruction Data ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Here, we provide a comprehensive catalog of visual instruction datasets utilized in our study. The datasets are categorized based on their primary focus, including general conversation and VQA data, OCR-related data, counting data, knowledge-based data, and language-only data. Table 15 summarizes these datasets and their respective references. ", "page_idx": 25}, {"type": "image", "img_path": "Vi8AepAXGy/tmp/b19e2b710185914d46e6be49c5fec258537e57288fe3208d17feb85824217e7b.jpg", "img_caption": ["Figure 13: Percentage $(\\%)$ ) Change in Benchmark Performance ( $\\mathbf{\\{rozen\\rightarrowUnfro}}$ zen) Heatmap depicting the percentage change in performance across multiple benchmarks when visual encoders are unfrozen compared to when they are kept frozen during fine-tuning. The color gradient indicates the magnitude of the performance change after unfreezing visual encoders\u2014white indicates no change, red is a positive change, and blue is a negative change. Notably, unfreezing leads to significant gains in OCR Chart tasks for most Language-Supervised Models, as reflected by the deep red cells. ConvNeXt, in particular, shows substantial improvements, demonstrating the benefits of updating this visual encoder during fine-tuning. "], "img_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "Vi8AepAXGy/tmp/1b6ca44c54210d8593dda7113ecccb380f2ca3632c6a963421aa2a0ce71cb964.jpg", "table_caption": ["Table 14: All Benchmark Results for Model Ensemble with 1.2M Adapter Data $^+$ 737K Instruction Tuning Data. Here, \u201cSigLIP\u201d $=$ ViT-SO400M/14@384, \u201cDINOv2\u201d $=$ ViT-L/14@518, \u201cConvNext\u201d $=$ OpenCLIP ConvNeXt-XXL $@1024$ , and \u201cCLIP\u201d $=$ OpenAI CLIP ViT-L/14@336. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "G.2 Additional System Prompts used in Cambrian Data ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Here, we investigate a phenomenon we term the \u201canswer machine phenomenon\u201d. We observe that a well-trained MLLM may excel at VQA benchmarks, but lack basic conversational abilities and default to outputting short, curt responses (see examples in Fig. 14). This discrepancy arises because benchmark questions typically require responses that are limited to a single option, choice, or word\u2014 diverging from the more broad and realistic use cases of MLLMs. Similar phenomena have been discussed in other LLM studies [114, 151, 155]. ", "page_idx": 26}, {"type": "table", "img_path": "Vi8AepAXGy/tmp/7fc1a31cbc9ec7deea6e9af57fdfea504d4bd015c64f45467f438c672cba464f.jpg", "table_caption": ["Table 15: Visual Instruction-Tuning Data Catalog "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "We suspect that this issue stems from instruction tuning data containing an excessive number of short-response VQA tasks, leading to catastrophic forgetting in LLMs. To address this, we incorporate additional system prompts during training. We append prompts such as \u201cAnswer the question using a single word or phrase.\u201d before questions that generate a single word or phrase in the response. Full details of the system prompts used are provided in Appendix G.2. After integrating these system prompts, we observe that while the model\u2019s benchmark performance remains unchanged, its conversational ability improves dramatically. For example, in Fig. 14, models with system prompts produce longer and more engaging responses while answering questions correctly. The system prompts also enhance the model\u2019s performance on reasoning-related tasks, such as math problems, by encouraging a chain of thoughts [133] followed by the answer. ", "page_idx": 27}, {"type": "text", "text": "This underscores the necessity of developing evaluation protocols like the Chatbot Arena [34] for MLLMs, despite the challenges in collecting large-scale, real-world interaction data. While performing well on benchmarks is important, it is equally crucial to ensure the model can engage in meaningful and natural interactions. The overall user experience and the model\u2019s conversational abilities are paramount, as a model that excels in benchmarks but fails to converse effectively cannot meet the needs of practical applications. ", "page_idx": 27}, {"type": "text", "text": "As our Cambrian data includes instructions/questions and responses of different types and formats (e.g., Short response with a single word or regular response as a complete sentence), it is important to specify the required response format in the instruction prompt to avoid ambiguity and possible conflicts. Some of the datasets already include such prompts and we add proper prompts for the remaining datasets. The detailed response formatting prompts we additionally add are listed in Table 16. ", "page_idx": 27}, {"type": "text", "text": "G.3 Data Engine ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Comprehensive Implementation Details of the Data Engine ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The data engine is designed to generate instruction tuning data for knowledge-based fields, where previous works rarely covers and MLLMs are not reliable to distill for from. The data engine takes in a given field, such as \u201cPhysics\u201d, utilizing reliable web sources like Wikipedia. Below are the various stages involved in the process. We also visualize this process in Fig. 15: ", "page_idx": 27}, {"type": "text", "text": "Stage 1 - Topic Generation: We start by compiling a list of fields and subfields and subsequently generate topics for each field using a Large Language Model (LLM), such as GPT-4. In this stage, ", "page_idx": 27}, {"type": "image", "img_path": "Vi8AepAXGy/tmp/0251c0d3a933526f49b13eaef9d6eb712ecc7362f4deab25b8589b24e1864180.jpg", "img_caption": ["Figure 14: Incorporating System Prompt in Instruction Tuning Data alleviates the \u201cAnswer Machine Phenomenon\u201d By adding system prompts in Cambrian-7M, the model exhibits better chat ability while retaining strong question answering abilities. The model without system prompts requires additional prompting to elicit longer responses. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "Vi8AepAXGy/tmp/c31e9c2fa78ce2346e1f7fe8ffc2debfa29bd23692e7d83e6c01160a0dfbdeb7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 15: Targeted Internet Data Collection Engine. We build a targeted internet data engine to collect high-quality and large-scale multimodal instruction tuning data for domains like knowledge. ", "page_idx": 28}, {"type": "text", "text": "we processed 30 fields, resulting in 3660 topics. We then post-process the output of LLMs into json formats. For example, the topic data for Physics looks like below. ", "page_idx": 28}, {"type": "text", "text": "Physics ", "page_idx": 28}, {"type": "text", "text": "{ \"Classical Mechanics\": [ \"Newton\u2019s Laws of Motion\", \"Conservation of Energy\", \"Conservation of Momentum\", \"Harmonic Motion\", \"Rotational Dynamics\", \"Gravitation and Orbits\", \"Fluid Dynamics\", \"Elasticity and Plasticity\", \"Friction\", \"Waves and Sound\", \"Velocity and Acceleration\", \"Angular Momentum\", \"Statics and Equilibrium\", \"Kinematics of Particles\", \"Dynamics of Systems of Particles\", \"Collisions\", \"Centripetal Force and Acceleration\", \"Lagrangian and Hamiltonian Mechanics\", \"Chaos Theory\", \"Equations of Motion\" ], \"Electromagnetism\": [ \"Coulomb\u2019s Law\", \"Electric Field and Electric Potential\", \"Gauss\u2019s Law\", \"Capacitance and Dielectrics\", \"Current and Resistance\", ", "page_idx": 28}, {"type": "table", "img_path": "Vi8AepAXGy/tmp/b57d8dd79b9b68b405b8723a53dd86e9695cdbfc728df00677b69272a4459602.jpg", "table_caption": ["Table 16: Response formatting prompts for Cambrian Data "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "\"Direct Current Circuits\", \"Magnetic Fields and Magnetic Forces\", \"Ampere\u2019s Law\", \"Faraday\u2019s Law of Induction\", \"Inductance\", \"Alternating Current Circuits\", \"Electromagnetic Waves\", \"Maxwell\u2019s Equations\", \"Electromagnetic Radiation\", \"Optics and Light\", \"Quantum Electrodynamics\", \"Special Theory of Relativity Implication\", \"Magnetostatics\", \"Electrostatics\", \"Bioelectromagnetism\" ], ", "page_idx": 29}, {"type": "text", "text": "Stage 2 - Filtering Web Data: For each generated topic, we utilize search engine APIs to fetch relevant high-quality web pages. For each topic, we query for 10 relevant links. Thus, we get 36,600 webpages post this stage. Here is an example of the data retrieved for the topic \"Electric Field and Electric Potential\": ", "page_idx": 29}, {"type": "text", "text": "\"Electric Field and Electric Potential\": [ \"https://en.wikipedia.org/wiki/Electric_potential\", \"https://en.wikipedia.org/wiki/Electric_field\", \"https://en.wikipedia.org/wiki/Electric_potential_energy\", ", "page_idx": 29}, {"type": "text", "text": "\"https://en.wikipedia.org/wiki/Voltage\", \"https://en.wikipedia.org/wiki/Electricity\", \"https://en.wikipedia.org/wiki/Electrostatics\", \"https://en.wikipedia.org/wiki/Electric_dipole_moment\", \"https://en.wikipedia.org/wiki/Magnetic_vector_potential\", \"https://en.wikipedia.org/wiki/Electric-field_screening\", \"https://en.wikipedia.org/wiki/Electric_flux\" ], ", "page_idx": 30}, {"type": "text", "text": "Stage 3 - Parsing: In this stage, we parse each web page to extract image-caption-text tuples. We aim to identify the blocks containing an image, the image\u2019s caption, and relevant textual content. Below is an example of the parsed data for the same topic, \"Electric Field and Electric Potential\": ", "page_idx": 30}, {"type": "text", "text": "\"Electric Field and Electric Potential\", [ { \"section\": \"Electrostatics\", \"text\": \"An electric potential at a point r in a static electric field E is given by the line integral where C is an   \narbitrary path from some fixed reference point to r; it is uniquely determined up to a constant... The generalization of   \nelectric potential to this case is described in the section Generalization to electrodynamics.\", \"images\": [ { \"url\": https://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/VFPt_plus_thumb_potential+contour.svg/   \n142px-VFPt_plus_thumb_potential+contour.svg.png, \"caption\": \"Electric potential of separate positive and negative point charges shown as color range from magenta   \n(+), through yellow (0), to cyan (-). Circular contours are equipotential lines. Electric field lines leave the positive   \ncharge and enter the negative charge.\" }, { \"url\": https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/VFPt_charges_plus_minus_potential+contour.svg/   \n288px-VFPt_charges_plus_minus_potential+contour.svg.png, \"caption\": \"Electric potential in the vicinity of two opposite point charges.\" } ], \"link\": https://en.wikipedia.org/wiki/Electric_potential, \"title\": \"Electric potential\", \"field\": \"Physics\", \"subfield\": \"Electromagnetism\", \"topic\": \"Electric Field and Electric Potential\" }, ] ", "page_idx": 30}, {"type": "text", "text": "Stage 4 - Data Generation: We generate dataset in this stage, ensuring high quality. We first filter out data samples with fewer than 50 words in the text. Then, instead of downloading images directly from the links retrieved during web parsing, we download high-resolution images from the original sources. We then convert formats like SVG or GIF into a common standardized format, PNG. ", "page_idx": 30}, {"type": "text", "text": "Question-Answer pairs are generated by using LLM such as GPT-3.5 from the image metadata, caption, and contextual text. These Q&A pairs and the image form our VQA dataset. We generated $165\\mathrm{k}$ data samples. Here is an example of the generated data: ", "page_idx": 30}, {"type": "text", "text": "\"id\": \"96232.png\",   \n\"image_id\": \"450px-Magnetic_Vector_Potential_Circular_Toroid\",   \n\"image_url\": 11   \n\"text\": 11 ", "page_idx": 30}, {"type": "text", "text": "\"caption\": \"Representing the Coulomb gauge magnetic vector potential A, magnetic flux density B, and current density J fields around a toroidal inductor of circular cross section. Thicker lines indicate field lines of higher average intensity. Circles in the cross section of the core represent the B-field coming out of the picture, plus signs represent B-field going into the picture. $\\backslash\\mathtt{u2207}\\backslash\\mathtt{u22c5}\\texttt{A}=\\textsf{O}$ ", "page_idx": 30}, {"type": "image", "img_path": "Vi8AepAXGy/tmp/b71da959db5f0f32ad14340398125247f22e36c79bd40fee14dbed5669161530.jpg", "img_caption": ["Figure 16: Dataset Image Id: 96232.png ", "Table 17: All Benchmark Results for Data Balancing Experiments "], "img_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "Vi8AepAXGy/tmp/9d94de91bd7f6df4f6b36b285bcc7ecd7ca9ad396130e2d0b1bc2c93c103f6ef.jpg", "table_caption": [], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "has been assumed\", ", "page_idx": 31}, {"type": "text", "text": "\"section\": \"Depiction of the A-field\", ", "page_idx": 31}, {"type": "text", "text": "\"Question\": \"What do the thicker lines signify?\", \"Answer\": \"The thicker lines in the image represent paths of higher average intensity in the magnetic vector potential A field. Shorter paths have higher intensity, ensuring that the path integral remains constant. These lines are drawn to visually convey the general behavior and intensity distribution of the A-field around the toroidal inductor\" } ", "page_idx": 31}, {"type": "text", "text": "This data engine is designed for scalability and efficiency and is capable of handling extensive data generation tasks using multithreading techniques. ", "page_idx": 31}, {"type": "text", "text": "G.4 Full results on data curation experiment ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Data Balance via Fitlering $t$ As discussed in Section 4.2, if left unflitered, the data pool is dominated by noisy, unbalanced data sources such as CLEVR and DVQA, leading to pronounced exponential tails. However, as we apply different $t$ values to filter data from each source, the exponential tails become less pronounced, resulting in a more balanced dataset. We also present all the results in Table 17. $t$ value $250\\mathbf{k}$ has the highest average across all benchmarks; $250\\mathbf{k}$ and $350\\mathbf{k}$ also have the highest performance across many individual benchmarks. ", "page_idx": 31}, {"type": "text", "text": "Here, we plot the cumulative sum of counts for entries sorted by counts from tail to head. From Fig. 17, we see this intermediate threshold prevents explosive heavy tail. ", "page_idx": 31}, {"type": "text", "text": "Data Ratio Studies We present the full results of our data ratio study in Table 18. The table highlights the importance of finding an optimal data ratio that balances different aspects of MLLM. Experiment 5 achieves well-rounded performance with its selected data ratio. ", "page_idx": 31}, {"type": "image", "img_path": "Vi8AepAXGy/tmp/c0775460c510508d1c552716ec1cbdb76e6f4626be9181e4c8cfe95170ffe81f.jpg", "img_caption": ["Figure 17: Data Balancing via Applying Thresholds on Data Sources. Applying threshold $t$ alleviates the exponential tail of Cambrian-10M. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "Vi8AepAXGy/tmp/3918282c8f60a64e699a20b00177a891f8a9d2cc37bc93f8a1ca4ee1bfea2f86.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "G.5 737K and 5M Mixes ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "0.7M For the 0.7M data we used in Section 2.4, We add a small number of OCR and chart data to LLaVA 665K, specifically 15,501 AI2D, 14,999 DocVQA, and 13,000 DVQA data points. This results in a 737K mix, which covers all categories in training MLLMs. This data mix allows us to study visual representations efficiently. ", "page_idx": 32}, {"type": "text", "text": "5.0M For the 5M data mixes we use in Section 2.4, we apply data filtering discussed in Section 4.2 and apply $t{=}150\\mathrm{k}$ on all multimodal instruction data in Cambrian-10M. ", "page_idx": 32}, {"type": "text", "text": "G.6 Test Image Leakage in Visual Instruction Training Data ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "One potential concern with our targeted data engine (Section 4.1) is that instruction-tuning data collected from the open web could introduce data leakage. To address this, we systematically analyze the extent of direct image matches between our training data and our test sets. Using difference hashing (dHash) [19], we compute hashes for all images in the training data and test sets. We then compare these hash sets to determine how many test images overlap with our training data, reporting the number of collisions in Table 19. ", "page_idx": 32}, {"type": "text", "text": "Across all fifteen datasets, our targeted data engine finds only 32 test images in total, amounting to just $0.06\\%$ of the test data. This low overlap percentage dispels concerns that our data engine inadvertently targets specific test sets. When analyzing the full Cambrian10M dataset\u2014which is $15\\mathrm{x}$ larger than LLaVA-665k\u2014we observe only 6x more matching test images (7,244 compared to 1,034 in LLaVA-665k). This discrepancy suggests that Cambrian10M\u2019s scale does not inherently result in excessive overlap with test sets. Instead, any overlap likely arises from the natural reuse of training images across benchmark datasets rather than targeted duplication. ", "page_idx": 32}, {"type": "table", "img_path": "Vi8AepAXGy/tmp/13dd5ee5b4c66db252a75e48365c20820de504537469b95f85f576375e4db003.jpg", "table_caption": [], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "Table 19: Number of leaked test set images. Using image hashing, we assess the overlap of test images across three training datasets: Cambrian10M Data Engine 161k subset (\u201cData Eng.\u201d), Cambrian10M, and $L L a V\\!A{\\cdot}665k$ . We list the number of images in each test set, as well as the number of matching images and percentage of overlap for each training set in blue. Our Data Engine finds a neglible $0.06\\%$ of test images, dispelling any concerns that it is targeting the test sets. The full Cambrian10M training set contains 7,244 test set images, whereas LLaVA-665k contains 1,034. Despite being a $15\\mathrm{x}$ larger dataset, Cambrian10M only has 6x more overlapping images. Such overlap is inevitable since many test sets use validation images from standard benchmarks (like COCO). It is worth highlighting: although exact image matches are found, this does not mean that exact image-question pairs have been found. Unlike in prior unimodal paradigms of computer vision research, in the multimodal setting, a single data point is composed of an image-text (question) pair, not just the image itself. Thus, seeing a test image during training is not equivalent to \u201ctraining on the test set\u201d so long as the training image does not have the same text pair as the test data point. ", "page_idx": 33}, {"type": "text", "text": "It is important to emphasize that while some exact image matches are found, this does not imply that the exact image-question pairs have been encountered during training. Unlike in traditional unimodal computer vision research, where an image alone constitutes a data point, the multimodal paradigm treats each image-text (question-answer) pair as unique. Consequently, seeing a test image during training is not equivalent to \u201ctraining on the test set\u201d as long as the associated text (question-answer) pairs differ. This distinction ensures that Cambrian10M respects the integrity of test evaluations, even in cases where images might appear in both training and test sets. ", "page_idx": 33}, {"type": "text", "text": "We encourage future research exploring the impact of image-only leakage on the performance of MLLMs. Understanding this influence may yield insights into the boundaries of model generalization and guide future best practices for dataset construction in multimodal learning. ", "page_idx": 33}, {"type": "text", "text": "G.7 Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "We conducted a preliminary analysis of the Cambrian dataset, focusing on the distribution of male, female, and neutral pronouns. Our findings show the following distribution: $38.35\\%$ male pronouns, $17.99\\%$ female pronouns, and $43.66\\%$ neutral pronouns. ", "page_idx": 33}, {"type": "text", "text": "We recognize that training models on biased data can perpetuate these biases. Addressing bias by artificially modifying data distributions\u2014such as through rebalancing or applying fairness constraints\u2014can help mitigate this issue, but it also presents challenges. These include the potential loss of generalization and the risk of introducing new biases. Additionally, identifying and mitigating bias in Multimodal Large Language Models (MLLMs) is particularly complex, given the interaction between different data modalities. We believe that openness in model development and data curation will accelerate research aimed at understanding and mitigating these potential harms. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "H Implementation Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Cambrian Models For our final Cambrian models, we use 2.5M adapter data which is comprised of 1.2M captioning data from shareGPT4V [27] and 1.2M captioning data used in MiniGemini [77]. SVA We provide here ablation studies of SVA module. ", "page_idx": 34}, {"type": "table", "img_path": "Vi8AepAXGy/tmp/743385d671a1179af730c329061ac96a2819e4b1a3901c10da5174d13fc43b06.jpg", "table_caption": ["Table 20: Ablations on hyperparameter choices for SVA. Enlarging the model capacity of the SVA module can further improve the performance. "], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "We further conduct ablation experiments using OpenAI CLIP ViT-L/14@336 + OpenCLIP ConvNeXt$L@1024$ as our base model combination. We focus on the OCR & chart categories to assess the impact on high-resolution visual understanding. The results show that increasing capacity via $D$ or $G$ improves performance and that allowing vision aggregation across multiple layers by adding cross-attention layers within the LLM also enhances performance. ", "page_idx": 34}, {"type": "text", "text": "Compared with other spatial-based connectors like C/D-Abstractor [21] which are designed for single vision feature maps, our SVA module can dynamically combine visual features from multiple vision models with varying resolutions. Besides, our spatial inductive bias in SVA can better compress spatial information compared with such methods. To isolate the effect of spatial inductive bias, we consider the case of token reduction using a single vision encoder. Specifically, we use OpenAI CLIP ViT-L as the vision model and compress its original 576 tokens to 36 tokens using our SVA module and other connectors. We compare our SVA module with three baselines: 1) Direct interpolation $+\\operatorname{MLP},2$ ) C-Abstractor [21], and 3)LDPv2 Projector [35] (similar to C-Abstractor but more lightweight). For fair comparisons, we do not include multi-layer aggregation inside the LLM for our SVA baseline, and the results are shown in Table 21. Compared with the simple MLP baseline, C-Abstractor performs better on General and Vision-Centric tasks but inferior on Knowledge and OCR & Chart tasks. LDPv2 performs similarly to the MLP baseline. Our SVA consistently demonstrates superior performance across all categories, especially in OCR & Chart and Vision-Centric tasks, demonstrating its effectiveness in information compression. ", "page_idx": 34}, {"type": "table", "img_path": "Vi8AepAXGy/tmp/99e8ca2318afe1b9f44d5764c628ed5deaaeb417e6cc37f5150c0ce01c298c0f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "We introduce learnable $k_{m}\\times k_{m}$ positional encodings in the vision features when $k_{m}>1$ . Besides, during cross-attention, the query is augmented with a global feature obtained by global pooling over the vision features, which is concatenated with ${\\bf q}_{i,j}$ to better guide the aggregation process. In our experiments, the feature maps of all vision encoders except for ConvNext are interpolated to $576\\!\\times\\!576$ ( $m_{k}=1$ for $L=24$ ). For ConvNext, we first interpolate the feature maps from its 4 stages to $96\\times96$ $m_{k}=4$ for $L=24$ ) and then channel-wise concatenate them to form its final vision feature map similar to [77]. ", "page_idx": 34}, {"type": "text", "text": "For experiments in Section 3, we set $D=3$ , $G=1$ and add cross-attention layers between the layers of LLM with a stride equal to 3. For our final Cambrian models, we set $D=3$ , $G=1$ and insert multiple cross-attention layers in LLM considering the tradeoff between performance and efficiency. For Cambrian-8B, Cambrian-13B, and Cambrian-34B, the strides of cross-attention layers inside the LLM are 3, 4, and 9 respectively. ", "page_idx": 35}, {"type": "text", "text": "To study the importance of visual features from different vision models to different image categories, we further investigate the attention score distribution in our SVA module. We evaluate our Cambrian8b model on GQA, DocVQA, and ScienceQA (representing three different benchmark categories), and the attention distribution results are shown in Table 22. We can see that on real-world images (GQA), the contribution of different vision models is relatively uniform, in part due to the similar characteristics of SigLIP and CLIP. On document-type images (DocVQA) which are text-heavy and often high-resolution, the influence of SigLIP increases and that of ConvNext greatly increases to aid in high-resolution information processing. For scientific images (ScienceQA) composed of illustrations and diagrams about different science categories, the contribution of SigLIP is further increased while the portion of DINOv2 decreases compared to GQA. ", "page_idx": 35}, {"type": "table", "img_path": "Vi8AepAXGy/tmp/2fc3ad8f015e1f21fc48ef129a3794e345349ceef4773708c15dcefcb1e93484.jpg", "table_caption": [], "table_footnote": [], "page_idx": 35}, {"type": "text", "text": "Unfreezing While unfreezing is largely beneficial (Section 2.3 and Fig. 13), it has a significant speed drawback. Given fixed computational resources, unfreezing visual encoders slows down the fine-tuning process by approximately $50{-}55\\%$ . For initial explorations or when computational overhead is a concern, leaving the visual encoders frozen can be a practical strategy. This allows for quicker iterations and tuning, especially during early research phases, while still providing valuable insights. Ultimately, unfreezing is recommended for achieving the best performance once the setup has been optimized. ", "page_idx": 35}, {"type": "text", "text": "I Evaluation Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "I.1 System Prompts Used in Evaluation ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "To ensure the reproduction of our results, we also include the system prompts we used in this work. The system prompts for our models can be found in Table 25. Additionally, we release the prompts we used while evaluating our models on the various benchmarks in Table 26. We hope this sets a precedent for future research to improve the reproducibility of benchmark results. ", "page_idx": 35}, {"type": "text", "text": "I.2 Ablation Study on Fuzzy Matching Vs LLM Judgement ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We use fuzzy matching to evaluate responses in some benchmarks, since MLLMs can answer questions with auxillary phrases. To study the effectiveness of our fuzzy matching, we compare our model accuracy through fuzzy matching with the model accuracy obtained when we use LLM as a grader. ", "page_idx": 35}, {"type": "text", "text": "The LLM grader is sensitive to the prompt given to it while grading, and we prompt the LLM (we use OpenAI GPT-3.5-turbo and GPT-4-turbo as our graders) with few shot grading examples, which we notice significantly improves grading accuracy. An example of such a prompt is given below. ", "page_idx": 35}, {"type": "text", "text": "LLM Grader Prompt ", "text_level": 1, "page_idx": 35}, {"type": "table", "img_path": "Vi8AepAXGy/tmp/e3336e560476e43b5cdc69960b36703555d5674e357dee4d24e2a85f9feaa753.jpg", "table_caption": [], "table_footnote": [], "page_idx": 35}, {"type": "table", "img_path": "Vi8AepAXGy/tmp/ddb313c2fe8beb4dca7c8b0f2f62326ceaaf4d94c0119324b3ef653fd3a7a89d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 36}, {"type": "text", "text": "Table 23: Implementation details and hyperparameters for all experiments. $^{*}4$ encoders are: OpenAI CLIP ViT-L/14@336, SigLIP ViT-SO400M/14@384, DINOv2 ViT-L/14@518, OpenCLIP ConvNeXt-XXL@1024 ", "page_idx": 36}, {"type": "image", "img_path": "Vi8AepAXGy/tmp/7a41d3a9027aaad7f0ba29b73d4cda22040eb237f1862ed43ae6976d5305fa15.jpg", "img_caption": [], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "We conduct an ablation study on the benchmarks that require fuzzy matching and present the results in Table 24. We discover that fuzzy matching provides reliable results compared to an LLM grader. We recommend using a more capable model (such as GPT-4-turbo) for grading benchmarks that have more subjective responses (such as numbers and words). ", "page_idx": 36}, {"type": "image", "img_path": "Vi8AepAXGy/tmp/08abff6c8675c26032d905b505503931c3f59511b0f1cb56b71667255f95a77d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "Table 24: Comparison between Fuzzy Matching Accuracy and LLM Judged Accuracy. Fuzzy matching and LLM referee yield similar accuracies for the benchmarks that require matching. ", "page_idx": 37}, {"type": "table", "img_path": "Vi8AepAXGy/tmp/99796ceea1ce1406786d2c099ec875b9758d03c0a24697436320445247e9b7a2.jpg", "table_caption": ["Table 25: LLM Backbone System Prompts "], "table_footnote": [], "page_idx": 37}, {"type": "table", "img_path": "Vi8AepAXGy/tmp/be2687bb8d132c3ee9d1141bbb111d1cc8543797d1518c1f4644f91c7cda021f.jpg", "table_caption": ["Table 26: Listing the prompts used in the evaluation of each benchmark "], "table_footnote": [], "page_idx": 38}, {"type": "image", "img_path": "Vi8AepAXGy/tmp/a8b8dfb16c0c09e26453d4a3621a6adc7b2fa0bc49cf8ac6eaf06a113f56ee9a.jpg", "img_caption": ["Figure 18: Comparison of model average performances on each category. Cambrian-1 outperforms other open-source models across all sizes. The lead is especially large on OCR & Chart and Vision-Centric benchmarks, highlighting the advantage of our vision-centric design. "], "img_footnote": [], "page_idx": 39}, {"type": "table", "img_path": "Vi8AepAXGy/tmp/7058cc6a0aa96a44479138afea28a6221e73908740c5c0738097bdcd636af1af.jpg", "table_caption": [], "table_footnote": [], "page_idx": 39}, {"type": "text", "text": "J More results of Cambrian-1 Model ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "In Fig. 18, we also plot our Cambrian-1 performance as well as GPT-4 performances. In the plot, it is clear that Cambrian-1 offers competitive performance compared to proprietary models in most categories. We also showcase some examples in Fig. 19, demonstrating that the model effectively attends to details in images despite using only 576 tokens. ", "page_idx": 39}, {"type": "text", "text": "K Potential Misuse & Mitigation Strategies ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "We recognize that there are ethical concerns regarding the potential misuse of multimodal large language models like Cambrian-1, particularly in generating misleading content or spreading misinformation. Below, we outline the main risks and provide strategies to address them: ", "page_idx": 39}, {"type": "text", "text": "1. Misinformation Cambrian-1 could be used to create misleading text descriptions of images, leading to false narratives or misrepresentations. For instance, such models might be leveraged by social media bots to manipulate public opinion during elections or other critical events. ", "page_idx": 39}, {"type": "text", "text": "2. Hallucination Similar to any large language model, Cambrian-1 may produce information that is not based on facts or actual input data. This phenomenon, often called \"hallucination,\" can be dangerous if users assume the model\u2019s output is entirely accurate without verification. ", "page_idx": 39}, {"type": "text", "text": "To mitigate these risks, users should exercise caution and critical thinking when interpreting outputs generated by Cambrian-1. It is important to verify the information produced by the model, particularly if the results are intended for sensitive or high-stakes applications. Users must be aware of the potential for hallucinations, where the model produces information not grounded in facts, and take steps to cross-check and validate any critical outputs. Additionally, implementing content filtering as a safeguard can help flag potentially harmful or misleading content before it is disseminated. ", "page_idx": 39}, {"type": "image", "img_path": "Vi8AepAXGy/tmp/dd844ee118ce625c23532aff647a87974da419a51be4a317fca49e49e8d65f0f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "Figure 19: Examples of Cambrian-1-34B. Cambrian-1 showcases impressive abilities in visual intersection. The model demonstrates instruction-following ability such as output in json format, as illustrated in the bottomleft example. Cambrian-1 also demonstrates remarkable OCR ability (See model handles different Comma \u201c,\u201d in the right down example). ", "page_idx": 40}, {"type": "text", "text": "L Checklist ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: We have detailed all claims mentioned in the abstract and introduction in the remaining part of the paper and Appendix. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 41}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: We have discussed the limitations of our work in the conclusion section. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 41}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: We do not include any theoretical assumptions and proofs in our paper. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 42}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: We will publicly release all code, hyperparameters, model checkpoints, and datasets for reproducing our experimental results. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 42}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: Yes, as highlighted in the abstract, we will release our model weights, code, tools, dataset, and all recipes. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 43}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: Yes. We include all training and evaluation recipes in the main paper and Appendix, and will additionally include them in the code release. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 43}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Justification: We do report error bars and other statistical results for our experiments. Part of these results are included in Fig. 3, Fig. 4 and Fig. 6. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 44}, {"type": "text", "text": "Answer: [No] ", "page_idx": 44}, {"type": "text", "text": "Justification: We will include these in our code release. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 44}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: Our research conform NeurIPS Code of Ethics. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 44}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: Our work has no societal impact. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 44}, {"type": "text", "text": "", "page_idx": 45}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: We used pre-trained LLM, which already has safeguards. As for datasets, we have conducted filtering to avoid safety risks. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 45}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Justification: We have credited and mentioned the assets we used. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 45}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 46}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We have explained the data engine and new benchmark in our paper. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 46}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 46}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 46}]