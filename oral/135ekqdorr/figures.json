[{"figure_path": "135eKqDoRR/figures/figures_1_1.jpg", "caption": "Figure 1: Drawbacks of one-to-one LM from the perspectives of (a) individual images and (b) the entire dataset. An ImageNet-pretrained classifier is reused in downstream tasks. In (a), images \u2018Dog\u2019 and \u2018Osteospermum\u2019 from downstream tasks are mapped into only one pretrained label, respectively, ignoring other probabilities. In (b), the distribution of [predicted pretrained label ys, ground-truth downstream label yT] pairs reveals the existence of suboptimal solutions, where \u2018Automobile\u2019 cannot be paired with the optimal pretrained label \u2018Moving Van\u2019, which has already been mapped to \u2018Truck\u2019.", "description": "This figure illustrates the limitations of using a one-to-one label mapping in visual reprogramming.  Panel (a) shows that individual images may have multiple strong activations in the pretrained model's output, but only one is used in the mapping. Panel (b) shows that using a greedy one-to-one mapping may lead to suboptimal assignments across the dataset, where the best possible mapping isn't always selected for each downstream label. The examples highlight that one-to-one mappings fail to capture the rich relationships between pretrained and downstream label spaces.", "section": "1 Introduction"}, {"figure_path": "135eKqDoRR/figures/figures_4_1.jpg", "caption": "Figure 2: Learning strategy of BLM and BLM+. First, input images, incorporated with VR watermarking or padding patterns, are fed into a fixed pretrained model to obtain logits and predicted labels. Then, the true labels (of yT) and predicted labels (of ys) are used to estimate WBLM or WBLM+. Next, using WBLM or WBLM+ that reweights output logits of pretrained models for the downstream labels, the predicted results can be derived. Finally, backpropagation is performed to update the input VR.", "description": "This figure illustrates the learning process of Bayesian-guided Label Mapping (BLM) and its enhanced version BLM+. It details the four steps involved: 1) Input image with VR patterns goes into a pretrained model, generating logits and predicted pretrained labels; 2) BLM/BLM+ estimates the probabilistic label mapping matrix using ground-truth downstream labels and predicted pretrained labels; 3) BLM/BLM+ reweights the output logits of the pretrained model for the downstream labels; 4) Backpropagation updates the input visual reprogramming patterns.  This iterative process refines the label mapping and the input VR to optimize the performance on the downstream task.", "section": "4 Bayesian-guided Probabilistic Label Mapping (BLM)"}, {"figure_path": "135eKqDoRR/figures/figures_7_1.jpg", "caption": "Figure 1: Drawbacks of one-to-one LM from the perspectives of (a) individual images and (b) the entire dataset. An ImageNet-pretrained classifier is reused in downstream tasks. In (a), images \u2018Dog\u2019 and \u2018Osteospermum\u2019 from downstream tasks are mapped into only one pretrained label, respectively, ignoring other probabilities. In (b), the distribution of [predicted pretrained label ys, ground-truth downstream label yT] pairs reveals the existence of suboptimal solutions, where \u2018Automobile\u2019 cannot be paired with the optimal pretrained label \u2018Moving Van\u2019, which has already been mapped to \u2018Truck\u2019.", "description": "This figure illustrates the limitations of using a one-to-one label mapping in visual reprogramming.  Panel (a) shows that individual images may have multiple relevant pretrained labels, but only the highest-scoring one is used.  Panel (b) demonstrates how the greedy one-to-one mapping can lead to suboptimal assignments across the entire dataset, as the best pretrained label for one downstream category might already be assigned to another.", "section": "1 Introduction"}, {"figure_path": "135eKqDoRR/figures/figures_8_1.jpg", "caption": "Figure 4: Visualization of input VR and top-weighted pretrained labels applying BLM+. Training loss and weight changes (Euclidean norm) of probabilistic LM WBLM+ per iteration are plotted below. Pretrained ResNet-18 is used, and the downstream label 'Marigold' is selected as an example.", "description": "This figure visualizes the learning process of the BLM+ method.  It shows how the input visual reprogramming patterns and the top-weighted pretrained labels change over training epochs.  The decrease in training loss demonstrates the model's improvement. The Euclidean norm of the weight changes in the probabilistic label mapping matrix (WBLM+) indicates the stability of the learning process.  The example uses the 'Marigold' label from a dataset, with ResNet-18 as the pretrained model.", "section": "4.1 Method Demonstration"}, {"figure_path": "135eKqDoRR/figures/figures_9_1.jpg", "caption": "Figure 6: Accuracy (%) of methods when varying training dataset sizes n for downstream task CIFAR100, using pretrained ResNet-18.", "description": "This figure shows the accuracy of different label mapping methods (RLM, ILM, BLM, BLM+) on the CIFAR100 dataset when varying the size of the training dataset.  It demonstrates the robustness of BLM and BLM+ to smaller training datasets, maintaining comparatively high accuracy even with only 40% of the full training data compared to RLM and ILM.", "section": "5 Experiments"}, {"figure_path": "135eKqDoRR/figures/figures_9_2.jpg", "caption": "Figure 1: Drawbacks of one-to-one LM from the perspectives of (a) individual images and (b) the entire dataset. An ImageNet-pretrained classifier is reused in downstream tasks. In (a), images \u2018Dog\u2019 and \u2018Osteospermum\u2019 from downstream tasks are mapped into only one pretrained label, respectively, ignoring other probabilities. In (b), the distribution of [predicted pretrained label ys, ground-truth downstream label yT] pairs reveals the existence of suboptimal solutions, where \u2018Automobile\u2019 cannot be paired with the optimal pretrained label \u2018Moving Van\u2019, which has already been mapped to \u2018Truck\u2019.", "description": "This figure illustrates the limitations of using a one-to-one label mapping in visual reprogramming.  Panel (a) shows that using only the highest-probability pretrained label ignores other potentially relevant labels for a given downstream image. Panel (b) demonstrates that a greedy one-to-one mapping can lead to suboptimal assignments, as the best pretrained label for one downstream label might already be assigned to another.", "section": "1 Introduction"}, {"figure_path": "135eKqDoRR/figures/figures_14_1.jpg", "caption": "Figure 7: The problem setting of Visual Reprogramming. The left part shows the pretrained model and corresponding dataset, while the right part shows downstream tasks. The pretrained model is fixed, whereas the input VR and output LM modules are variable.", "description": "This figure illustrates the visual reprogramming (VR) process.  The left side shows a pretrained model trained on a large dataset. The right side shows various downstream tasks with different input images and labels.  The key idea is that the pretrained model remains fixed; however, the input data is modified using an \"input visual reprogramming\" module, and the output is adapted using an \"output label mapping\" module to produce results relevant to the downstream task.  The figure highlights the flexibility of VR in adapting pretrained models to diverse new applications without retraining.", "section": "1 Introduction"}, {"figure_path": "135eKqDoRR/figures/figures_15_1.jpg", "caption": "Figure 1: Drawbacks of one-to-one LM from the perspectives of (a) individual images and (b) the entire dataset. An ImageNet-pretrained classifier is reused in downstream tasks. In (a), images \u2018Dog\u2019 and \u2018Osteospermum\u2019 from downstream tasks are mapped into only one pretrained label, respectively, ignoring other probabilities. In (b), the distribution of [predicted pretrained label ys, ground-truth downstream label yT] pairs reveals the existence of suboptimal solutions, where \u2018Automobile\u2019 cannot be paired with the optimal pretrained label \u2018Moving Van\u2019, which has already been mapped to \u2018Truck\u2019.", "description": "This figure shows the drawbacks of using a one-to-one label mapping (LM) strategy in visual reprogramming (VR).  Subfigure (a) illustrates how this approach can overlook important relationships between pretrained and downstream labels when applied to individual images; for example, only considering the highest logit for each image, disregarding other potentially relevant labels. Subfigure (b) demonstrates this limitation at a dataset level by showing suboptimal solutions where the optimal pretrained label is already assigned to a different downstream label, leading to mismatches and reduced performance. The visualization highlights that a one-to-one mapping is insufficient for capturing the complex many-to-many relationships between pretrained and downstream labels.", "section": "1 Introduction"}, {"figure_path": "135eKqDoRR/figures/figures_24_1.jpg", "caption": "Figure 1: Drawbacks of one-to-one LM from the perspectives of (a) individual images and (b) the entire dataset. An ImageNet-pretrained classifier is reused in downstream tasks. In (a), images \u2018Dog\u2019 and \u2018Osteospermum\u2019 from downstream tasks are mapped into only one pretrained label, respectively, ignoring other probabilities. In (b), the distribution of [predicted pretrained label ys, ground-truth downstream label yT] pairs reveals the existence of suboptimal solutions, where \u2018Automobile\u2019 cannot be paired with the optimal pretrained label \u2018Moving Van\u2019, which has already been mapped to \u2018Truck\u2019.", "description": "This figure illustrates the limitations of using a one-to-one label mapping in visual reprogramming.  Panel (a) shows that using only the single most likely pretrained label ignores other potentially relevant labels. Panel (b) demonstrates that a greedy one-to-one mapping can lead to suboptimal solutions where the best pretrained label for a downstream class is already assigned to another class.  This motivates the need for a more flexible many-to-many mapping approach.", "section": "1 Introduction"}, {"figure_path": "135eKqDoRR/figures/figures_24_2.jpg", "caption": "Figure 1: Drawbacks of one-to-one LM from the perspectives of (a) individual images and (b) the entire dataset. An ImageNet-pretrained classifier is reused in downstream tasks. In (a), images \u2018Dog\u2019 and \u2018Osteospermum\u2019 from downstream tasks are mapped into only one pretrained label, respectively, ignoring other probabilities. In (b), the distribution of [predicted pretrained label ys, ground-truth downstream label yT] pairs reveals the existence of suboptimal solutions, where \u2018Automobile\u2019 cannot be paired with the optimal pretrained label \u2018Moving Van\u2019, which has already been mapped to \u2018Truck\u2019.", "description": "This figure illustrates the limitations of using a one-to-one label mapping in visual reprogramming.  Panel (a) shows that individual images can have multiple relevant pretrained labels, but only the highest-scoring one is used, ignoring potentially useful information. Panel (b) demonstrates that a greedy one-to-one mapping can lead to suboptimal assignments across the entire dataset because once a pretrained label is assigned to a downstream label, it is unavailable for other potential pairings, even if it would be a better match.", "section": "1 Introduction"}, {"figure_path": "135eKqDoRR/figures/figures_25_1.jpg", "caption": "Figure 1: Drawbacks of one-to-one LM from the perspectives of (a) individual images and (b) the entire dataset. An ImageNet-pretrained classifier is reused in downstream tasks. In (a), images \u2018Dog\u2019 and \u2018Osteospermum\u2019 from downstream tasks are mapped into only one pretrained label, respectively, ignoring other probabilities. In (b), the distribution of [predicted pretrained label ys, ground-truth downstream label yT] pairs reveals the existence of suboptimal solutions, where \u2018Automobile\u2019 cannot be paired with the optimal pretrained label \u2018Moving Van\u2019, which has already been mapped to \u2018Truck\u2019.", "description": "This figure shows the drawbacks of using a one-to-one label mapping (LM) strategy in visual reprogramming (VR).  The left subfigure (a) illustrates how individual images might be incorrectly mapped to a single pretrained label, ignoring other potentially relevant labels. The right subfigure (b) shows that the one-to-one mapping can lead to suboptimal solutions across the entire dataset, as evidenced by the frequency distribution of pretrained and downstream labels.", "section": "1 Introduction"}, {"figure_path": "135eKqDoRR/figures/figures_27_1.jpg", "caption": "Figure 1: Drawbacks of one-to-one LM from the perspectives of (a) individual images and (b) the entire dataset. An ImageNet-pretrained classifier is reused in downstream tasks. In (a), images \u2018Dog\u2019 and \u2018Osteospermum\u2019 from downstream tasks are mapped into only one pretrained label, respectively, ignoring other probabilities. In (b), the distribution of [predicted pretrained label ys, ground-truth downstream label yT] pairs reveals the existence of suboptimal solutions, where \u2018Automobile\u2019 cannot be paired with the optimal pretrained label \u2018Moving Van\u2019, which has already been mapped to \u2018Truck\u2019.", "description": "This figure illustrates the limitations of using a one-to-one label mapping approach in visual reprogramming.  Panel (a) shows that individual images might have multiple relevant pretrained labels, but only the highest-scoring one is used, ignoring potentially valuable information. Panel (b) demonstrates that a greedy one-to-one mapping can lead to suboptimal solutions across the entire dataset, where the optimal pretrained label for a downstream class is already assigned to another downstream class. This highlights the need for a more nuanced, many-to-many mapping approach.", "section": "1 Introduction"}, {"figure_path": "135eKqDoRR/figures/figures_27_2.jpg", "caption": "Figure 1: Drawbacks of one-to-one LM from the perspectives of (a) individual images and (b) the entire dataset. An ImageNet-pretrained classifier is reused in downstream tasks. In (a), images \u2018Dog\u2019 and \u2018Osteospermum\u2019 from downstream tasks are mapped into only one pretrained label, respectively, ignoring other probabilities. In (b), the distribution of [predicted pretrained label ys, ground-truth downstream label yT] pairs reveals the existence of suboptimal solutions, where \u2018Automobile\u2019 cannot be paired with the optimal pretrained label \u2018Moving Van\u2019, which has already been mapped to \u2018Truck\u2019.", "description": "This figure illustrates the limitations of using a one-to-one label mapping (LM) strategy in visual reprogramming (VR).  Panel (a) shows how a single pretrained label is assigned to multiple downstream labels, ignoring the nuances within the pretrained model's predictions. Panel (b) demonstrates that a greedy one-to-one mapping can lead to suboptimal solutions where the best pretrained label for a downstream task is already assigned to another downstream label. The figure highlights the need for a more flexible, many-to-many mapping approach.", "section": "1 Introduction"}, {"figure_path": "135eKqDoRR/figures/figures_28_1.jpg", "caption": "Figure 3: Visualization results of top weighted pretrained labels y<sup>s</sup> and weights w<sub>ys,y<sup>T</sup></sub> for some y<sup>T</sup> applying BLM and BLM+. Downstream labels \u2018Edamame\u2019, \u2018Fibrous\u2019, and \u2018Dog\u2019 are shown as examples. ResNet-18 pretrained on ImageNet is used. More results are in Appendix K.", "description": "This figure visualizes the top weighted pretrained labels and their corresponding weights for three example downstream labels using both BLM and BLM+.  It shows how the methods assign weights to various pretrained labels based on their relevance to the downstream label.  The examples used are 'Edamame', 'Fibrous', and 'Dog', highlighting the many-to-many relationships learned by the probabilistic label mapping.", "section": "5 Experiments"}, {"figure_path": "135eKqDoRR/figures/figures_28_2.jpg", "caption": "Figure 1: Drawbacks of one-to-one LM from the perspectives of (a) individual images and (b) the entire dataset. An ImageNet-pretrained classifier is reused in downstream tasks. In (a), images \u2018Dog\u2019 and \u2018Osteospermum\u2019 from downstream tasks are mapped into only one pretrained label, respectively, ignoring other probabilities. In (b), the distribution of [predicted pretrained label ys, ground-truth downstream label yT] pairs reveals the existence of suboptimal solutions, where \u2018Automobile\u2019 cannot be paired with the optimal pretrained label \u2018Moving Van\u2019, which has already been mapped to \u2018Truck\u2019.", "description": "This figure illustrates the limitations of using a one-to-one label mapping in visual reprogramming.  Panel (a) shows how individual images might be incorrectly mapped to a single pretrained label, even though other pretrained labels might be more suitable. Panel (b) demonstrates that using a greedy one-to-one mapping for the entire dataset can lead to suboptimal solutions where the best pretrained label for a downstream label is already assigned to another downstream label. These issues highlight the need for a more flexible many-to-many mapping strategy.", "section": "1 Introduction"}, {"figure_path": "135eKqDoRR/figures/figures_29_1.jpg", "caption": "Figure 1: Drawbacks of one-to-one LM from the perspectives of (a) individual images and (b) the entire dataset. An ImageNet-pretrained classifier is reused in downstream tasks. In (a), images 'Dog' and 'Osteospermum' from downstream tasks are mapped into only one pretrained label, respectively, ignoring other probabilities. In (b), the distribution of [predicted pretrained label ys, ground-truth downstream label yT] pairs reveals the existence of suboptimal solutions, where \u2018Automobile\u2019 cannot be paired with the optimal pretrained label \u2018Moving Van\u2019, which has already been mapped to \u2018Truck\u2019.", "description": "This figure illustrates the limitations of using a one-to-one label mapping (LM) strategy in visual reprogramming (VR).  Subfigure (a) shows how a single pretrained label is assigned to multiple downstream labels, ignoring the probabilistic nature of the relationship. Subfigure (b) demonstrates that a greedy one-to-one mapping can lead to suboptimal assignments due to the many-to-many nature of the actual label relationships between the pretrained model and downstream tasks.", "section": "1 Introduction"}, {"figure_path": "135eKqDoRR/figures/figures_30_1.jpg", "caption": "Figure 2: Learning strategy of BLM and BLM+. First, input images, incorporated with VR watermarking or padding patterns, are fed into a fixed pretrained model to obtain logits and predicted labels. Then, the true labels (of yT) and predicted labels (of ys) are used to estimate WBLM or WBLM+. Next, using WBLM or WBLM+ that reweights output logits of pretrained models for the downstream labels, the predicted results can be derived. Finally, backpropagation is performed to update the input VR.", "description": "This figure illustrates the step-by-step process of the Bayesian-guided Label Mapping (BLM) and BLM+ methods. It starts by inputting images with added VR patterns into a pretrained model, generating logits and predicted labels.  These are then used to estimate the probabilistic label mapping matrices WBLM and WBLM+.  Finally, these matrices are used to refine the predictions for the downstream labels, and backpropagation updates the input VR patterns.", "section": "4 Bayesian-guided Probabilistic Label Mapping (BLM)"}, {"figure_path": "135eKqDoRR/figures/figures_31_1.jpg", "caption": "Figure 1: Drawbacks of one-to-one LM from the perspectives of (a) individual images and (b) the entire dataset. An ImageNet-pretrained classifier is reused in downstream tasks. In (a), images \u2018Dog\u2019 and \u2018Osteospermum\u2019 from downstream tasks are mapped into only one pretrained label, respectively, ignoring other probabilities. In (b), the distribution of [predicted pretrained label ys, ground-truth downstream label yT] pairs reveals the existence of suboptimal solutions, where \u2018Automobile\u2019 cannot be paired with the optimal pretrained label \u2018Moving Van\u2019, which has already been mapped to \u2018Truck\u2019.", "description": "This figure illustrates the limitations of using one-to-one label mapping in visual reprogramming.  Panel (a) shows how individual images might be incorrectly mapped to a single pretrained label, even though other pretrained labels might be more appropriate. Panel (b) demonstrates how these suboptimal mappings can affect the overall performance, showing a many-to-many relationship between pretrained and downstream labels is overlooked by the one-to-one approach.", "section": "1 Introduction"}, {"figure_path": "135eKqDoRR/figures/figures_32_1.jpg", "caption": "Figure 1: Drawbacks of one-to-one LM from the perspectives of (a) individual images and (b) the entire dataset. An ImageNet-pretrained classifier is reused in downstream tasks. In (a), images \u2018Dog\u2019 and \u2018Osteospermum\u2019 from downstream tasks are mapped into only one pretrained label, respectively, ignoring other probabilities. In (b), the distribution of [predicted pretrained label ys, ground-truth downstream label yT] pairs reveals the existence of suboptimal solutions, where \u2018Automobile\u2019 cannot be paired with the optimal pretrained label \u2018Moving Van\u2019, which has already been mapped to \u2018Truck\u2019.", "description": "This figure illustrates the limitations of using a one-to-one label mapping (LM) strategy in visual reprogramming (VR).  The left subplot shows how a single pretrained label is assigned to multiple downstream labels, ignoring the nuanced relationships and probabilities within the predicted output. The right subplot shows that even when using the optimal one-to-one mapping, some downstream labels cannot be effectively mapped due to conflicts and limitations inherent in the one-to-one strategy. This highlights the need for a more flexible, many-to-many approach like the Bayesian-guided Label Mapping (BLM) proposed in the paper.", "section": "1 Introduction"}, {"figure_path": "135eKqDoRR/figures/figures_32_2.jpg", "caption": "Figure 1: Drawbacks of one-to-one LM from the perspectives of (a) individual images and (b) the entire dataset. An ImageNet-pretrained classifier is reused in downstream tasks. In (a), images \u2018Dog\u2019 and \u2018Osteospermum\u2019 from downstream tasks are mapped into only one pretrained label, respectively, ignoring other probabilities. In (b), the distribution of [predicted pretrained label ys, ground-truth downstream label yT] pairs reveals the existence of suboptimal solutions, where \u2018Automobile\u2019 cannot be paired with the optimal pretrained label \u2018Moving Van\u2019, which has already been mapped to \u2018Truck\u2019.", "description": "This figure shows the drawbacks of using a one-to-one label mapping in visual reprogramming.  Subfigure (a) demonstrates how individual images might be mislabeled because the one-to-one mapping ignores the probabilities of other relevant pretrained labels. Subfigure (b) illustrates how a greedy one-to-one mapping can lead to suboptimal solutions for the entire dataset by preventing optimal pairings between pretrained and downstream labels.", "section": "1 Introduction"}, {"figure_path": "135eKqDoRR/figures/figures_33_1.jpg", "caption": "Figure 1: Drawbacks of one-to-one LM from the perspectives of (a) individual images and (b) the entire dataset. An ImageNet-pretrained classifier is reused in downstream tasks. In (a), images \u2018Dog\u2019 and \u2018Osteospermum\u2019 from downstream tasks are mapped into only one pretrained label, respectively, ignoring other probabilities. In (b), the distribution of [predicted pretrained label ys, ground-truth downstream label yT] pairs reveals the existence of suboptimal solutions, where \u2018Automobile\u2019 cannot be paired with the optimal pretrained label \u2018Moving Van\u2019, which has already been mapped to \u2018Truck\u2019.", "description": "This figure illustrates the limitations of using a one-to-one label mapping in visual reprogramming.  Panel (a) shows how individual images can be misrepresented because the highest-probability pretrained label is selected, ignoring other potentially relevant labels. Panel (b) shows how the one-to-one mapping can lead to suboptimal assignments across the entire dataset, where some downstream labels might not be optimally mapped to any pretrained label because the best pretrained label for the downstream label was already assigned in the mapping.", "section": "1 Introduction"}, {"figure_path": "135eKqDoRR/figures/figures_33_2.jpg", "caption": "Figure 3: Visualization results of top weighted pretrained labels ys and weights wys,yt for some y applying BLM and BLM+. Downstream labels \u2018Edamame\u2019, \u2018Fibrous\u2019, and \u2018Dog\u2019 are shown as examples. ResNet-18 pretrained on ImageNet is used. More results are in Appendix K.", "description": "This figure visualizes the top weighted pretrained labels and their corresponding weights obtained from BLM and BLM+ for three downstream labels: Edamame, Fibrous, and Dog.  The weights represent the contribution of each pretrained label to the prediction of the downstream label. This visualization helps illustrate how BLM and BLM+ move beyond a one-to-one mapping between pretrained and downstream labels and instead consider multiple relationships. ResNet-18 pretrained on ImageNet is the model used.", "section": "5 Experiments"}]