[{"type": "text", "text": "Graph Diffusion Transformers for Multi-Conditional Molecular Generation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Gang Liu, Jiaxin Xu, Tengfei Luo, Meng Jiang University of Notre Dame {gliu7, jxu24, tluo, mjiang2}@nd.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Inverse molecular design with diffusion models holds great potential for advancements in material and drug discovery. Despite success in unconditional molecular generation, integrating multiple properties such as synthetic score and gas permeability as condition constraints into diffusion models remains unexplored. We present the Graph Diffusion Transformer (Graph DiT) for multi-conditional molecular generation. Graph DiT integrates an encoder to learn numerical and categorical property representations with the Transformer-based denoiser. Unlike previous graph diffusion models that add noise separately on the atoms and bonds in the forward diffusion process, Graph DiT is trained with a novel graph-dependent noise model for accurate estimation of graph-related noise in molecules. We extensively validate Graph DiT for multi-conditional polymer and small molecule generation. Results demonstrate the superiority of Graph DiT across nine metrics from distribution learning to condition control for molecular properties. A polymer inverse design task for gas separation with feedback from domain experts further demonstrates its practical utility. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models for molecular graphs are essential for inverse design of materials and drugs by generating molecules and polymers (macro-molecules) [40, 46], because the models can be effectively trained to predict discrete graph structures and atom/bond types in denoising processes [43]. Practical inverse designs consider multiple factors such as molecular synthetic score and various properties [15], known as the task of multi-conditional graph generation. ", "page_idx": 0}, {"type": "text", "text": "Existing work converted multiple conditions into a single one and solved the task as single-condition generation [5, 25]. However, multi-property relations may not be properly or explicitly defined [5]. First, the properties have diverse scales and units. For example, the synthetic complexity ranges from 1 to 5 [8], while the gas permeability varies widely, exceeding 10,000 in Barrier units [4]. This gap makes it hard for models to balance the conditions. Second, multi-conditions consist of a mix of categorical and numerical properties. The common practice of addition [47] or multiplication [25] is inadequate for combination. ", "page_idx": 0}, {"type": "text", "text": "Figure 1(a) empirically illustrates the challenges in multi-conditional generation, i.e., discovering molecules meeting multiple properties. We used a test set of 100 data points with three properties: synthesizability (Synth.) [12], $\\mathrm{O_{2}}$ and $\\ensuremath{\\mathrm{N_{2}}}$ permeability $\\mathrm{{[{O}_{2}P e r m}}$ and $\\mathrm{N_{2}P e r m}\\rangle$ [4]. A single-conditional diffusion model generated up to 30 graphs for each condition, resulting in a total of 90 graphs for three conditions. We sort the 30 graphs in each set using a polymer property Oracle (see appendix B.3). Then, we check whether a shared polymer structure that meets multi-property constraints can be identified across different condition sets. If we find the polymer, its rank $K$ (where $K$ is between 1 and 30) indicates how high it appears on the lists, considering all condition sets. If not, we set $K$ as 30. Figure 1(a) shows the frequency distribution of $K$ on the 100 test cases. The median $K$ was ", "page_idx": 0}, {"type": "image", "img_path": "cfrDLD1wfO/tmp/a12aa9994b1dbf3081cbb8f0dd42b1ab606f722c141e9dea55f3f820000ce499.jpg", "img_caption": ["Figure 1: Multi-conditional diffusion guidance in (b) generates polymers of higher property accuracy than existing work in (a). Explanations are in Section 1 and details are in appendix B.3. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "30, indicating that the multiple properties were not met on over half of the test polymers despite generating a large number of graphs. ", "page_idx": 1}, {"type": "text", "text": "To address these challenges, we project multi-properties into representations by learning, thereby guiding the diffusion process for molecule generation. We propose the Graph Diffusion Transformer (Graph DiT) for graph denoising under conditions. Graph DiT has a condition encoder for property representation learning and a graph denoiser. The condition encoder utilizes a novel clustering-based method for numerical properties and one-hot encoding for categorical ones to learn multi-property representations. The graph denoiser first integrates node and edge features into graph tokens, then denoise these tokens with adaptive layer normalization (AdaLN) in Transformer layers [19, 34]. AdaLN replaces the molecular statistics (mean and variance) in each hidden layer with those from the condition representation, effectively outperforming other predictor-based and predictor-free conditioning methods [22, 43, 34], as shown in Section 4.4. We observe that existing forward diffusion processes [43, 22] apply noise separately to atoms and bonds, which may compromise the accuracy of Graph DiT in noise estimation. Hence, we propose a novel graph-dependent noise model that effectively applies noise tailored to the dependencies between atoms and bonds within the graph. ", "page_idx": 1}, {"type": "text", "text": "Results in Figure 1(b) show that the polymers generated by Graph DiT closely align with multiproperty constraints. For each test case, we have one graph generated from Graph DiT conditional on three properties. The Oracle determines the rank of this graph among 30 single-conditionally generated graphs for each condition. We find the median ranks are 4, 9, and 11, for Synth., $\\mathrm{O_{2}}$ Perm, and $\\Nu_{2}$ Perm, respectively, all much higher than 30. Note that the ranked set of 30 graphs was very competitive because the model was trained on the specific condition dedicatedly. ", "page_idx": 1}, {"type": "text", "text": "In experiments, we evaluate model performance on one polymer and three small molecule datasets. The polymer dataset includes four numerical conditions for multi-conditional evaluation. Our model has the lowest average mean absolute error (MAE), significantly reducing the error by $17.86\\%$ compared to the best baseline. It also excels in small molecule tasks, achieving over 0.9 accuracy on task-related categorical conditions, notably surpassing the baseline accuracy of less than 0.6. We also examine the model\u2019s utility in inverse polymer designs for $\\mathrm{O_{2}/N_{2}}$ gas separation, with domain expert feedback highlighting our model\u2019s practical utility in multi-conditional molecular design. ", "page_idx": 1}, {"type": "text", "text": "2 Problem Definition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Multi-Conditional Inverse Molecular Design ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A molecular graph $G\\,=\\,(V,E)$ consists of a set of nodes (atoms) $V$ and edges (bonds) $E$ . We follow [43] and define \u201cnon-bond\u201d as a type of edge. There are $N$ atoms and each atom has a one-hot encoding, denoting the atom type. We represent it as ${\\bf X}_{V}\\in\\mathbb{R}^{N\\times F_{V}}$ , where $F_{V}$ is the total number of atom types. Similarly, the bond features are a tensor $\\mathbf{X}_{E}\\in\\mathbb{R}^{N\\times N\\times F_{E}}$ , representing both the graph structure and $F_{E}$ bond types. ", "page_idx": 2}, {"type": "text", "text": "Let $\\mathcal{C}=\\{c_{1},c_{2},\\dots,c_{M}\\}$ be a set of $M$ numerical and categorical conditions. The task is: $q(G\\mid$ $c_{1},c_{2},\\ldots,c_{M})\\propto q(G)q(c_{1},c_{2},\\ldots,c_{M}\\mid G),$ where $q$ represents observed probability. We use a model parameterized by $\\theta$ for multi-conditional molecular generation $p_{\\theta}(G^{^{\\overbar{\\mathbf{\\theta}}}}|\\mathbf{\\theta}^{C})$ . The evaluation involves both distribution learning $q(G)$ [35] and condition control $q(c_{1},c_{2},\\ldots,c_{M}\\ |\\ G)$ . We follow previous work in assuming that there exist different oracle functions $\\scriptscriptstyle\\mathcal{O}$ that can independently evaluate each conditioned property [14]: $\\begin{array}{r}{q(c_{1},c_{2},\\cdot\\cdot\\cdot,c_{M}\\mid G)=\\prod_{i=1}^{M}\\mathcal{O}_{i}(c_{i}\\mid G)}\\end{array}$ . Note that the oracles are not used in the training of $p_{\\theta}$ . ", "page_idx": 2}, {"type": "text", "text": "2.2 Diffusion Model on Graph Data ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion models consist of forward and reverse diffusion processes [17]. We refer to the forward diffusion process as the diffusion process following [17]. The diffusion process $q(G^{1:T}\\mid G^{0})=$ $\\textstyle\\prod_{t=1}^{T}q(G^{t}\\mid G^{t-1})$ corrupts molecular graph data $G^{0}=G,$ ) into noisy states $G^{t}$ . As timesteps $T\\to\\infty$ , $q(G^{T})$ converges a stationary distribution $\\pi(G)$ . The reverse Markov process $p_{\\theta}(G^{0:T})=$ $q(G^{T})\\prod_{t=1}^{T}p_{\\theta}(G^{t-1}\\mid G^{t})$ , parameterized by neural networks, gradually denoises the latent states toward the desired data distribution. ", "page_idx": 2}, {"type": "text", "text": "Diffusion Process One may perturb $G^{t}$ in a discrete state-space to capture the structural properties of molecules [43]. Two transition matrices $\\mathbf{Q}_{V}\\,\\in\\,\\mathbb{R}^{F_{V}\\times F_{V}^{\\star}}$ and $\\mathbf{Q}_{E}^{\\,\\,^{\\,^{\\mathbf{2}}}}\\in\\,\\mathbb{R}^{F_{E}\\times F_{E}}$ are defined for nodes $\\mathbf{X}_{V}$ and edges $\\mathbf{X}_{E}$ , respectively [43]. Then, each step $q(G^{t}\\mid G^{t-1},G^{0})=q(G^{t}\\mid G^{t-1})$ in the diffusion process is sampled as follows. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\left\\{q(\\mathbf{X}_{V}^{t}\\mid\\mathbf{X}_{V}^{t-1})=\\operatorname{Cat}\\left(\\mathbf{X}_{V}^{t};\\mathbf{p}=\\mathbf{X}_{V}^{t-1}\\mathbf{Q}_{V}^{t}\\right),}\\\\ {q(\\mathbf{X}_{E}^{t}\\mid\\mathbf{X}_{E}^{t-1})=\\operatorname{Cat}\\left(\\mathbf{X}_{E}^{t};\\mathbf{p}=\\mathbf{X}_{E}^{t-1}\\mathbf{Q}_{E}^{t}\\right),}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\operatorname{Cat}(\\mathbf{X};\\mathbf{p})$ denotes sampling from a categorical distribution with probability $\\mathbf{p}$ . We remove the subscript $\\left(\\nu/E\\right)$ when the description applies to both nodes and edges. It is assumed that the noise $\\mathbf{Q}^{i}$ $i\\leq t)$ is independently applied to $\\mathbf{X}$ in each step $i$ , allowing us to rewrite $q(\\mathbf{X}^{t}\\mid\\mathbf{X}^{t-1})$ as the probability of the initial state $q(\\mathbf{X}^{t}\\mid\\mathbf{X}^{0})=\\mathrm{Cat}\\left(\\mathbf{X}^{t};\\mathbf{p}=\\mathbf{X}^{0}\\bar{\\mathbf{Q}}^{t}\\right)$ , where $\\begin{array}{r}{\\bar{\\mathbf{Q}}^{t}=\\prod_{i\\leq t}\\mathbf{Q}^{i}}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "Noise Scheduling Transition matrices $\\mathbf{Q}_{V}$ and $\\mathbf{Q}_{E}$ control the noise applied to atom features and bond features, respectively. Vignac et al. [43] defined $\\pi(G)\\,=\\,\\bigl(\\mathbf{m}_{X}^{\\,\\,\\,\\star\\,}\\in\\;\\mathbb{R}^{F_{V}},\\mathbf{m}_{E}\\,\\in\\,\\mathbb{R}^{F_{E}}\\bigr)$ as the marginal distributions of atom types and bond types. The transition matrix at timestep $t$ is $\\mathbf{Q}^{t}=\\alpha^{t}\\mathbf{I}\\,\\mathbf{\\check{+}}\\,(1-\\alpha^{t})\\mathbf{1}\\mathbf{m}^{\\prime}$ for atoms or bonds, where $\\mathbf{m}^{\\prime}$ denotes the transposed row vector. Therefore, we have $\\bar{\\mathbf{Q}}^{t}=\\bar{\\alpha}^{t}\\mathbf{I}+(1-\\bar{\\alpha}^{t})\\mathbf{1m}^{\\prime}$ , where $\\begin{array}{r}{\\bar{\\alpha}^{t}=\\prod_{\\tau=1}^{t}\\alpha^{\\tau}}\\end{array}$ . The cosine schedule [32] is often chosen for $\\bar{\\alpha}^{t}=\\cos(0.5\\pi(t/T+s)/(1+s))^{2}$ . ", "page_idx": 2}, {"type": "text", "text": "Reverse Process With the initial sampling $G^{T}\\sim\\pi(G)$ , the reverse process generates $G^{0}$ iteratively in reversed steps $t=T,T-1,\\ldots,0$ . We use a neural network to predict the probability $p_{\\theta}(\\tilde{G}^{0}\\mid G^{t})$ as the product over nodes and edges [1, 43]: ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{\\theta}(\\tilde{G}^{0}\\mid G^{t})=\\prod_{v\\in V}p_{\\theta}(v^{t-1}\\mid G^{t})\\prod_{e\\in E}p_{\\theta}(e^{t-1}\\mid G^{t})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "$p_{\\theta}(\\tilde{G}^{0}\\mid G^{t})$ could be combined with $q(G^{t-1}\\mid G^{t},G^{0})$ to estimate the reverse distribution on the graph $\\overset{\\cdot}{p_{\\theta}}(G^{t-1}\\mid G^{t})$ . For example, $p_{\\theta}(v^{t-1}\\mid G^{t})$ is marginalized over predictions of node types $\\tilde{v}\\in\\tilde{\\mathbf{x}}_{v}$ , which applies similarly to edges: ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{\\boldsymbol{\\theta}}\\big(\\boldsymbol{v}^{t-1}\\mid\\boldsymbol{G}^{t}\\big)=\\sum_{\\tilde{\\boldsymbol{v}}\\in\\tilde{\\mathbf{x}}_{\\boldsymbol{v}}}q(\\boldsymbol{v}^{t-1}\\mid\\tilde{\\boldsymbol{v}},\\boldsymbol{G}^{t})p_{\\boldsymbol{\\theta}}(\\tilde{\\boldsymbol{v}}\\mid\\boldsymbol{G}^{t}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The neural network could be trained to minimize the negative log-likelihood [43]. ", "page_idx": 2}, {"type": "equation", "text": "$$\nL=\\mathbb{E}_{q(G^{0})}\\mathbb{E}_{q(G^{t}|G^{0})}\\left[-\\mathbb{E}_{\\mathbf{x}\\in G^{0}}\\log p_{\\theta}\\left(\\mathbf{x}\\mid G^{t}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "cfrDLD1wfO/tmp/d989c7564131d83465391ca883ffa7d9246a70fd5db6c05bcc9d2af3d582c447.jpg", "img_caption": ["Figure 2: Denoising framework and architectures for Graph DiT. Details are in Section 3.2. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{x}\\in G^{0}$ denotes the node or edge features. Typically, the reverse process in diffusion models does not consider molecular properties as conditions. While there have been efforts to introduce property-related guidance using additional predictors, the more promising approach of predictor-free guidance [16], particularly in multi-conditional generation, remains underexplored. ", "page_idx": 3}, {"type": "text", "text": "3 Multi-Conditional Graph Diffusion Transformers ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We present the denoising framework of Graph DiT in Figure 2. The condition encoder learns the representation of $M$ conditions. The statistics of this representation like mean and variance are used to replace the ones from the molecular representations [19] (see Section 3.2). Besides, we introduce a new noise model in the diffusion process to better fit graph-structured molecules (see Section 3.1). ", "page_idx": 3}, {"type": "text", "text": "3.1 Graph-Dependent Noise Models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The transition probability of a node or an edge should rely on the joint distribution of nodes and edges in the prior state. However, as an example shown in Eq. (1), current diffusion models [22, 43, 25] treat node and edge state transitions as independent, misaligning with the denoising process in Eq. (3). This difference between the sampling distributions of noise in the diffusion and reverse processes introduces unnecessary challenges to multi-conditional molecular generations. ", "page_idx": 3}, {"type": "text", "text": "To address this, we use a single matrix $\\mathbf{X}_{G}\\ \\in\\ \\mathbb{R}^{N\\times F_{G}}$ to represent graph tokens for $G$ , with $F_{G}=F_{V}+N\\cdot F_{E}$ . Token representations are created by concatenating the node feature matrix $\\mathbf{X}_{V}$ and the flattened edge connection matrix from $\\mathbf{X}_{E}$ . Each row vector in $\\mathbf{X}_{G}$ contains features for both nodes and edges, representing all connections and non-connections. Hence, we could design a transition matrix $\\mathbf{Q}_{G}$ considering the joint distribution of nodes and edges. $\\mathbf{Q}_{G}\\,\\in\\,\\mathbb{R}^{F_{G}\\times F_{G}}$ is constructed from four matrices $\\mathbf{Q}_{V},\\mathbf{\\check{Q}}_{E V}\\stackrel{*}{\\in}\\mathbb{R}^{F_{E}\\times F_{V}},\\mathbf{Q}_{E},\\mathbf{Q}_{V E}\\in\\mathbb{R}^{F_{V}\\times F_{E}^{\\vee}}$ , denoting the transition probability (\u201cdependent old state\u201d $\\rightarrow$ \u201ctarget new state\u201d) node $\\rightarrow$ node; edge $\\rightarrow$ node; edge $\\rightarrow$ edge; node $\\rightarrow$ edge, respectively. ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\bf Q}_{G}=\\left[\\!\\!\\begin{array}{c c}{{\\bf Q}_{V}}&{{\\bf1}_{N}^{\\prime}\\otimes{\\bf Q}_{V E}}\\\\ {{\\bf1}_{N}\\otimes{\\bf Q}_{E V}}&{{\\bf1}_{N\\times N}\\otimes{\\bf Q}_{E}}\\end{array}\\!\\!\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\otimes$ denotes the Kronecker product, $\\mathbf{1}_{N},\\mathbf{1}_{N}^{\\prime}$ , and ${\\mathbf{1}}_{N\\times N}$ represent the column vector, row vector, and matrix with all 1 elements, respectively. According to Eq. (5), the first $F_{V}$ columns in $\\mathbf{Q}_{G}$ determine the node feature transitions based on both node features (first $F_{V}$ rows) and edge features (remaining $N\\cdot F_{E}$ rows). Conversely, the remaining $N\\cdot F_{E}$ columns determine the edge feature transitions, depending on the entire graph. We introduce a new diffusion noise model: ", "page_idx": 3}, {"type": "equation", "text": "$$\nq(\\mathbf{X}_{G}^{t}\\mid\\mathbf{X}_{G}^{t-1})=\\widetilde{\\mathrm{Cat}}\\left(\\mathbf{X}_{G}^{t};\\tilde{\\mathbf{p}}=\\mathbf{X}_{G}^{t-1}\\mathbf{Q}_{G}^{t}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\tilde{\\mathbf{p}}$ is the unnormalized probability andC at denotes categorical sampling: The first $F_{V}$ columns of $\\tilde{\\mathbf{p}}$ are normalized to sample ${\\bf X}_{V}^{t}$ , while the remaining $N{\\cdot}E$ dimensions are reshaped and normalized to sample edges $\\mathbf{X}_{E}^{t}$ . These components are combined to form $\\mathbf{X}_{G}^{t}$ , completing the $\\widetilde{\\mathrm{Cat}}$ sampling. ", "page_idx": 3}, {"type": "text", "text": "Choice of $\\mathbf{Q}_{V E}$ and $\\mathbf{Q}_{E V}$ Similar to the definitions of $\\mathbf{m}_{V}$ and $\\mathbf{m}_{E}$ [43], we leverage the prior knowledge within the training data for the formulation of task-specific matrices, $\\mathbf{Q}_{E V}$ and $\\mathbf{Q}_{V E}$ . We calculate co-occurrence frequencies of atom and bond types in training molecular graphs to obtain the marginal atom-bond co-occurrence probability distribution. For each bond type, each row in $\\mathbf{m}_{E V}$ represents the probability of co-occurring atom types. $\\mathbf{m}_{V E}$ is the transpose of $\\mathbf{m}_{E V}$ and has a similar meaning. Subsequently, we define ${\\bf Q}_{E V}=\\bar{\\alpha}^{t}{\\bf I}+(1-\\bar{\\alpha}^{t}){\\bf1m}_{E V}^{\\prime}$ and $\\bar{\\mathbf{Q}}_{V E}=\\bar{\\alpha}^{t}\\mathbf{I}+(1-\\bar{\\alpha}^{t})\\mathbf{1}\\mathbf{m}_{V E}^{\\prime}$ ", "page_idx": 3}, {"type": "text", "text": "3.2 Denoising Models with Multi-Property Conditions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We present Graph DiT as the denoising model to generate molecules under multi-conditions ${\\mathcal{C}}=$ $\\{c_{1},{\\bar{c}}_{2},\\dots,c_{M}\\}$ without extra predictors. ", "page_idx": 4}, {"type": "text", "text": "Predictor-Free Guidance The predictor-free reverse process $\\hat{p}_{\\theta}(G^{t-1}\\mid G^{t},\\mathcal{C})$ aims to generate molecules with a high probability $\\stackrel{\\cdot}{q}(\\mathcal{C}\\mid G^{0})$ . This could be achieved by a linear combination of the log probability for unconditional and conditional denoising [16]: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{p}_{\\theta}(G^{t-1}\\mid G^{t},\\mathcal{C})=\\log p_{\\theta}(G^{t-1}\\mid G^{t})+s\\left(\\log p_{\\theta}(G^{t-1}\\mid G^{t},\\mathcal{C})-\\log p_{\\theta}(G^{t-1}\\mid G^{t})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $s$ denotes the scale of conditional guidance. Unlike classifier-free guidance [16], which typically predicts noise, we directly estimate $p_{\\theta}(\\tilde{G}^{0}\\mid G^{t},{\\mathcal{C}})$ . We one one denoising model $f_{\\theta}(G^{t},{\\mathcal{C}})$ for both $p_{\\theta}(\\tilde{G}^{0}\\mid G^{t})$ and $p_{\\theta}(\\tilde{G}^{0}\\mid G^{t},\\mathcal{C})$ . Here, $f_{\\theta}(G^{t},{\\mathcal{C}}\\;=\\;\\varnothing)$ computes the unconditional probability by substituting the original conditional embeddings with the null value. During training, we randomly drop the condition with a ratio, i.e., ${\\mathcal{C}}=\\varnothing$ , to learn the embedding of the null value. $f_{\\theta}(G^{t}=\\mathbf{X}_{G}^{\\dot{t}},\\mathcal{C})$ comprises two components: the condition encoder and the graph denoiser. An overview of the architecture is presented in Figure 2. ", "page_idx": 4}, {"type": "text", "text": "Condition Encoder We treat the timestep $t$ as a special condition and follow [31] to obtain a $D$ - dimensional representation t with sinusoidal encoding. For property-related numerical or categorical condition $c_{i}\\in\\mathcal{C}$ , we apply distinct encoding operations to get $D$ -dimensional representation. For a categorical condition, we use the one-hot encoding. For a numerical variable, we introduce a clustering encoding method. This defines learnable centroids, assigning $c_{i}$ to clusters, and transforming the soft assignment vector of condition values into the representation. It could be implemented using two Linear layers and a Softmax layer in the middle as: Linear (Softmax $({\\mathrm{Linear}}(c_{i})))$ ). Finally, we could obtain the representation of the condition as $\\begin{array}{r}{\\mathbf{c}=\\sum_{i=1}^{M}\\mathrm{encode}(c_{i})}\\end{array}$ , where encode is the specific encoding method based on the condition type. For numerical conditions, we evaluate our proposed clustering-based approach against alternatives like direct or interval-based encodings [28]. As noted in Section 4.4, the clustering encoding outperforms the other methods. ", "page_idx": 4}, {"type": "text", "text": "Graph Denoiser: Transformer Layers Given the noisy graph at timestep $t$ , the graph tokens are first encoded into the hidden space as $\\mathbf{H}=\\operatorname{Linear}(\\mathbf{X}_{G}^{t})$ , where $\\mathbf{H}\\in\\mathbb{R}^{N\\times D}$ . We then adapt the standard Transformer layers [42] with self-attention and multi-layer perceptrons (MLP), but replace the normalization with the adaptive layer normalization (AdaLN) controlled by the representations of the conditions [19, 34]: $\\dot{\\mathbf{H}}\\,\\bar{=}\\,\\mathrm{AdaLN}(\\mathbf{H},\\mathbf{c})$ . For each row $\\mathbf{h}$ in $\\mathbf{H}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{AdaLN}\\left(\\mathbf{h},\\mathbf{c}\\right)=\\gamma_{\\theta}(\\mathbf{c})\\odot\\frac{\\mathbf{h}-\\mu\\left(\\mathbf{h}\\right)}{\\sigma\\left(\\mathbf{h}\\right)}+\\beta_{\\theta}(\\mathbf{c}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mu(\\cdot)$ and $\\sigma(\\cdot)$ are mean and variance values. $\\odot$ indicates element-wise product. $\\gamma_{\\theta}(\\cdot)$ and $\\beta_{\\theta}(\\cdot)$ are neural network modules in $f_{\\theta}(\\cdot)$ , each of which consists of two linear layers with SiLU activation [11] in the middle. We have a gated variant $\\mathrm{AdaLN}_{g a t e}$ for residuals: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{AdaLN}_{g a t e}\\left(\\mathbf{h},\\mathbf{c}\\right)=\\alpha_{\\theta}(\\mathbf{c})\\odot\\mathrm{AdaLN}\\left(\\mathbf{h},\\mathbf{c}\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We apply the zero initialization for the first layer of $\\gamma_{\\theta}(\\cdot),\\beta_{\\theta}(\\cdot)$ , and $\\alpha_{\\theta}(\\cdot)$ [34]. There are other options to learn the structure representation from the condition [34]: In-Context conditioning adds condition representation to the structure representation at the beginning of the structure encoder, and Cross-Attention calculates cross-attention between the condition and structure representation. We observe in Section 4.4 that AdaLN performs best among them. ", "page_idx": 4}, {"type": "text", "text": "Graph Denoiser: Final MLP We have the hidden states $\\mathbf{H}$ after the final Transformer layers, the MLP is used to predict node probabilities $\\tilde{\\mathbf{X}}_{V}^{0}$ and edge probabilities $\\tilde{\\mathbf{X}}_{E}^{0}$ at $t=0$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{X}}_{G}^{0}=\\mathrm{AdaLN}(\\mathrm{MLP}(\\mathbf{H}),\\mathbf{c}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We split the output $\\mathbf{X}_{G}$ into atom and bond features $\\tilde{\\mathbf{X}}_{V}^{0},\\tilde{\\mathbf{X}}_{E}^{0}$ . The first $F_{V}$ dimensions of $\\tilde{\\mathbf{X}}_{G}^{0}$ represent node type probabilities, and the remaining $N\\cdot\\dot{F}_{E}$ dimensions cover probabilities for $N$ edge types associated with the node, as detailed in Section 3.1. ", "page_idx": 4}, {"type": "text", "text": "Generation to Molecule Conversion A common way of converting generated graphs to molecules selects only the largest connected component [43], denoted as Graph DiT-LCC in our model. For Graph DiT, we connect all components by randomly selecting atoms. It minimally alters the generated structure to more accurately reflect model performance than Graph DiT-LCC. ", "page_idx": 4}, {"type": "table", "img_path": "cfrDLD1wfO/tmp/f93a6aad734252a796a3988c055a61e42053601070079ee351e3b596dcba8da9.jpg", "table_caption": ["Table 1: Multi-Conditional Generation of 10K Polymers: Results on the synthetic score (Synth.) and three numerical properties (gas permeability for $\\mathrm{O_{2}}$ , $\\ensuremath{\\mathrm{N_{2}}}$ , $\\mathrm{CO_{2}}$ ). MAE is calculated between the input conditions and the properties of the generated polymers using Oracles. Best results are highlighted. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "RQ1: We validate the generative power of Graph DiT compared to baselines from molecular optimization and diffusion models in Section 4.2. RQ2: We study a polymer inverse design for gas separation in Section 4.3. RQ3: We conduct further analysis to examine Graph DiT in Section 4.4. ", "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We use datasets with over ten types of atoms and up to fifty nodes in a molecular graph. We include both numerical and categorical properties for drugs and materials, offering a benchmark for evaluation across diverse chemical spaces. Model performance is validated across up to nine metrics, including distribution coverage, diversity, and condition control capacity for various properties. ", "page_idx": 5}, {"type": "text", "text": "Datasets and Input Conditions We have one polymer dataset [40] for materials, featuring three numerical gas permeability conditions: $\\mathrm{O_{2}P e r m}$ , $\\mathrm{CO}_{\\mathrm{2}}\\mathrm{Perm}$ , and $\\mathrm{N_{2}P e r m}$ . For drug design, we create three class-balanced datasets from MoleculeNet [46]: HIV, BBBP, and BACE, each with a categorical property related to HIV virus replication inhibition, blood-brain barrier permeability, or human $\\beta$ - secretase 1 inhibition, respectively. We have two more numerical conditions for synthesizability from synthetic accessibility (SAS) and complexity scores (SCS) [12, 8]. ", "page_idx": 5}, {"type": "text", "text": "Evaluation We randomly split the dataset into training, validation, and testing (reference) sets in a 6:2:2 ratio. Evaluations are conducted on 10,000 generated examples with metrics [35] (1) molecular validity (Validity); (2) heavy atom type coverage (Coverage); (3) internal diversity among the generated examples (Diversity); (4) fragment-based similarity with the reference set (Similarity); (5) Fr\u00b4echet ChemNet Distance with the reference set (Distance) [36]; MAE between the generated and conditioned (6) synthetic accessibility score [12] (Synth.); $(7){\\sim}(9)$ MAE/Accuracy for the numerical/categorical task conditions (Property). The evaluation Oracle uses random forest trained on all task-related molecules [14]. Lower MAE or higher accuracy indicates stronger model controllability. ", "page_idx": 5}, {"type": "text", "text": "Baselines We select strong and popular molecular optimization baselines from recent studies [14]: Graph-GA [20], MARS [47], JTVAE [21] with Bayesian optimization (JTVAE-BO), LSTM [6] on SMILES with Hill Climbing (LSTM-HC). We include the most recent diffusion models: GDSS[22], DiGress [43], and their conditional version with extra predictors: MOOD [25], and DiGress v2 [43]. We train multi-task predictors using the same architecture for MOOD and DiGress v2 models to provide additional guidance for generation. For molecular optimization, we formulate the condition set of each test data point as a combined goal, minimizing the sum of the normalized errors between generated and input properties. We train a random forest model for each property using the training data to optimize the molecular structure. ", "page_idx": 5}, {"type": "text", "text": "4.2 RQ1: Multi-Conditional Molecular Generation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We have the observations from Table 1 and Table 2: ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Chemical Validity High validity may not accurately represent the model\u2019s generative performance if hard-coded rules are introduced in the algorithm. For example, GraphGA could eliminate non-valid molecules during mutation and crossover iterations to achieve perfect validity in the final evaluation. Without rule checking in the generation-to-molecule step, DiGress, GDSS, and MOOD show a marked performance decline, with validity often dropping from 0.99 to below 0.6. In contrast, Graph DiT often maintains over 0.8 validity without any rule-based processing. ", "page_idx": 5}, {"type": "table", "img_path": "cfrDLD1wfO/tmp/78c022d1c15a6f41cc1a9646e4118bd23f18e0ec6a8fa29da0ad9f4377f85f97.jpg", "table_caption": ["Table 2: Multi-Conditional Generation of 10K Small Molecules: Each dataset involves a numerical synthesizability score (Synth.) and a categorical task-specific property. MAE/Accuracy is calculated by comparing input conditions and generated properties. The best number per metric is highlighted. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Distribution Learning GraphGA is a simple yet effective baseline for generating in-distribution molecules, e.g., on BBBP and HIV generation datasets. Diffusion model baselines such as DiGress and MOOD could produce diverse molecules but often fail to capture the original data distribution in multi-conditional tasks. Graph DiT shows the competitive performance of diffusion models in ftiting complex molecular data distributions. Using fragment-based similarity and neural network-based distance metrics [36], we achieve the best in the polymer task and rank second in the HIV small molecule task, involving up to 11 and 29 types of heavy atoms, respectively. ", "page_idx": 6}, {"type": "text", "text": "Condition Controllability LSTM-HC surpasses many baselines, achieving lower average MAE on polymer properties and higher rankings on small molecular properties. However, its control over synthetic scores in polymer tasks is relatively poor. Conversely, MARS effectively manages synthetic scores for polymers but exhibits a larger MAE in gas permeability conditions compared to other baselines. GDSS performs well in gas permeability control but underperforms Graph GA and MARS in terms of the synthetic score condition. DiGress v2 and MOOD, although equipped with the predictor guidance, still exhibit limited condition control compared to their unconditional counterparts over polymer and small molecule tasks. These baselines struggle to balance and control multiple conditions in generation. In contrast, Graph DiT significantly improves diffusion models and achieves the best multi-conditional performance in all tasks. In polymer tasks, Graph DiT reduces MAE on all gas permeability conditions, averaging $+17.8\\%$ improvement over the best baseline LSTM-HC. For small molecule tasks, Graph DiT consistently ranks top-1 in condition controllability with over 0.9 accuracy in categorical conditions. Compared to Graph DiT-LCC, we observe that Graph DiT, which connects all generated graph components, shows better controllability performance due to minimal rule-based post-generation processing. ", "page_idx": 6}, {"type": "text", "text": "4.3 RQ2: Polymer Inverse Design for Gas Separation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We aim to design polymers with high $\\mathrm{O_{2}}$ and low $\\Nu_{2}$ permeability, demonstrating the models\u2019 precise control over related properties. Following Robeson [38]\u2019s definition of high-performance polymers based on the $\\mathrm{O_{2}/N_{2}}$ permeability ratio, we selected 16 polymers meeting this criterion from 609 examples as our test/reference set. The remaining data is used for training and validation. Subsequently, we generated 1,000 polymers conditioned on test set labels. ", "page_idx": 6}, {"type": "image", "img_path": "cfrDLD1wfO/tmp/9b959f27ebb60fed41aec44a9309de79599dcc74108b2e34020b762cfc873c15.jpg", "img_caption": ["Figure 3: Polymer Inverse Design for $\\mathrm{O_{2}/N_{2}}$ Gas Separation: Feedback from four domain experts includes an average Utility Score (UtS) for relative usefulness and an Agreement Score (AS) for generated polymers, both ranging [0, 1]. Polymers are generated conditional on $\\{\\mathrm{SAS}{=}3.8\\$ , ${\\mathrm{SCS}}{=}4.3$ , $\\mathrm{O_{2}P e r m{=}34.0}$ , $\\mathrm{N}_{2}\\mathrm{Perm}{=}5.2\\}$ . The top-3 polymers, highlighted, are all generated by Graph DiT. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "cfrDLD1wfO/tmp/e6d5e88c142895e6fcbcccd89d2468be9bcc70b58f6f7d9796e1f7a2def1c738.jpg", "img_caption": ["(a) Numerical Condition Encodings (b) Condition Architectures (c) Graph Dependent Noise Model Figure 4: Relative Performance of Different Model Designs: A higher bar indicates better performance. We use the performance of clustering-based encoding or AdaLN as the Reference Value and the current option as the Current Value. Relative performance is calculated asRCefuerrreennct eV aVlauleue Similarity and Diversity metrics, and as Reference Value for other metrics. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "In Figure 3, we present the top three polymers generated by each model for a case study with expertise. Initially, a random forest algorithm identifies the top five polymers per method based on average MAE in two gas permeability. These 25 polymers are then shuffled and evaluated by four polymer scientists, who rank them from 1 to 25 using their domain knowledge. Rankings are normalized to a Utility Score (UtS) ranging from 0 to 1, with higher scores indicating greater utility. The variance in UtS is converted into an Agreement Score (AS) for further evaluation. As shown in Figure 3, there is a high consensus among experts that the three polymers generated by Graph DiT are the most promising for successful polymer inverse design tasks. More details are in appendix D. By comparing generated examples from different models, we have further observations: ", "page_idx": 7}, {"type": "text", "text": "\u2022 DiGress and MOOD struggle to capture polymerization points, marked with asterisks $(^{\\bullet\\bullet:\\bullet})$ , which is one of the most important features that distinguish polymers from small molecules. Additionally, the two methods frequently feature excessive carbon atoms and overly large cycles. These molecular configurations with significant distortion from the canonical geometry of stable compounds may lead to poor synthesizability [35, 7]. \u2022 LSTM-HC may result in too-long carbon chains with limited diversity. MARS produces examples with asymmetrical graph structures, challenging polymer synthesis [10, 2]. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Graph DiT generates structurally diverse and symmetric polymers with two polymerization points, indicative of more valid and synthesizable polymer structures. The first two, which are polyimides, imply effective gas separation performance [24]. ", "page_idx": 8}, {"type": "text", "text": "4.4 RQ3: Ablation Studies and Model Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Model Components In light of Table 1, we analyze three components that impact our model\u2019s learning in various conditions. Our assessment of relative performance is based on the ratio between our method and comparative approaches. The first component is numerical conditional encoding. Results in Figure 4(a) highlight the superiority of clustering encoding over direct and interval-based encoding, particularly in controlling gas permeability, despite its slightly lower diversity. The second component concerns the neural architecture for conditions. As shown in Figure 4(b), similar to Figure 4(a), AdaLN surpasses both In-Context Conditioning and Cross-Attention in learning distribution with better condition controllability. The third component validates the importance of the graph-dependent noise model compared to separately applying noise to atoms and bonds. It also shows the improvement of the predictor-free Graph DiT over the predictor-guided DiGress v2, even without the graph-dependent noise model. More results on model controllability are in appendix E. ", "page_idx": 8}, {"type": "text", "text": "Oracle Selections We analyze the robustness of Oracles in evaluating six task-related properties (three gas permeability and three small molecule properties) across six conditional generation tasks. Oracles are switched from Random Forest to Gaussian Process or Support Vector Machines for ranking generative model performance. Results in Table 3 show consistent rankings (Graph DiT, LSTM-HC, MARS, JTVAE-BO, MOOD, GDSS, GraphGA). It indicates that while perfectly approximating the truth properties of generated molecules is difficult, we could effectively compare the relative performance of various models. Graph DiT consistently ranked first among baselines. ", "page_idx": 8}, {"type": "text", "text": "Table 3: Oracles for Generation Evaluation: We consider three Oracles. Generative performance is ranked on average from 1 to 9 across six properties, with various Oracles yielding similar outcomes. We highlight models with the same ranking sequence in different Oracle evaluation. ", "page_idx": 8}, {"type": "table", "img_path": "cfrDLD1wfO/tmp/f85d67b2dcba8d21bf4f06aaa8cf706993258f5d9ebd4399bd9c6d41f6bd6d38.jpg", "table_caption": [], "table_footnote": [""], "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Diffusion Models for Molecules: Score-based diffusion models applied noise and denoising in continuous space [33, 22]. DiGress [43] used discrete noise as transition matrices based on marginal distributions of atom and bond types. Extra predictor models are studied to guide the generation process in DiGress and GDSS [25]. Diffusion models could also be used for molecular property prediction [27], for conformation [48] and molecule generation with 3D atomic coordinates [18, 49, 3]. We focus on molecular graph generation, considering the high computational cost of accurate 3D coordinates for larger molecules like polymers [23]. We explore predictor-free diffusion guidance, instead of the classifier guidance [9, 44], for generating molecules under categorical and numerical conditions. It can be integrated with diffusion models for atomic coordinates in future research. ", "page_idx": 8}, {"type": "text", "text": "Molecular Optimization: Optimization algorithms could optimize molecules towards property constraints, including genetic algorithms [20], Bayesian optimization [39, 50], REINFORCE [45], and reinforcement learning [30]. Both sequential and graph-based generative models [6, 21, 30], along with diverse sampling methods [47, 13], are used in conjunction with these algorithms to produce desirable molecules. These methods have been applied to both single-objective and multi-objective optimization, the latter by manually integrating multiple property conditions into a single one [5, 25]. Several challenges in molecular optimization methods remain underexplored, including the inadequate or unclear definition of multi-property relations when integration into a single objective [5], and the inaccessibility of the oracle function for property-oriented optimization during the training phase [14]. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we solved inverse molecular design using properties as predictor-free diffusion guidance. The proposed Graph DiT performed diffusion based on the joint distribution of atoms and bonds in both forward and reverse processes. It introduced representation learning for multiple categorical and numerical properties and utilized a Transformer-based graph denoiser for conditional graph denoising. Results on multi-conditional generations and polymer inverse designs showed the remarkable generative capabilities of Graph DiT, making it suitable for designing promising molecules. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by NSF IIS-2142827, IIS-2146761, IIS-2234058, CBET-2332270, CBET2102592, and ONR N00014-22-1-2507. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981\u201317993, 2021.   \n[2] Alexandru T Balaban. Symmetry in chemical structures and reactions. In Symmetry, pages 999\u20131020. Elsevier, 1986.   \n[3] Fan Bao, Min Zhao, Zhongkai Hao, Peiyao Li, Chongxuan Li, and Jun Zhu. Equivariant energy-guided SDE for inverse molecular design. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=r0otLtOwYW.   \n[4] J Wesley Barnett, Connor R Bilchak, Yiwen Wang, Brian C Benicewicz, Laura A Murdock, Tristan Bereau, and Sanat K Kumar. Designing exceptional gas-separation polymer membranes using machine learning. Science advances, 6(20):eaaz4301, 2020.   \n[5] Camille Bilodeau, Wengong Jin, Tommi Jaakkola, Regina Barzilay, and Klavs F Jensen. Generative models for molecular discovery: Recent advances and challenges. Wiley Interdisciplinary Reviews: Computational Molecular Science, 12(5):e1608, 2022. [6] Nathan Brown, Marco Fiscato, Marwin HS Segler, and Alain C Vaucher. Guacamol: benchmarking models for de novo molecular design. Journal of chemical information and modeling, 59(3):1096\u20131108, 2019.   \n[7] Robert F Bruns and Ian A Watson. Rules for identifying potentially reactive or promiscuous compounds. Journal of medicinal chemistry, 55(22):9763\u20139772, 2012.   \n[8] Connor W Coley, Luke Rogers, William H Green, and Klavs F Jensen. Scscore: synthetic complexity learned from a reaction corpus. Journal of chemical information and modeling, 58 (2):252\u2013261, 2018.   \n[9] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:8780\u20138794, 2021.   \n[10] Jack D Dunitz. Symmetry arguments in chemistry. Proceedings of the National Academy of Sciences, 93(25):14260\u201314266, 1996.   \n[11] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural networks, 107:3\u201311, 2018.   \n[12] Peter Ertl and Ansgar Schuffenhauer. Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Journal of cheminformatics, 1:1\u201311, 2009.   \n[13] Tianfan Fu, Wenhao Gao, Cao Xiao, Jacob Yasonik, Connor W Coley, and Jimeng Sun. Differentiable scaffolding tree for molecular optimization. arXiv preprint arXiv:2109.10469, 2021.   \n[14] Wenhao Gao, Tianfan Fu, Jimeng Sun, and Connor Coley. Sample efficiency matters: a benchmark for practical molecular optimization. Advances in Neural Information Processing Systems, 35:21342\u201321357, 2022.   \n[15] Niklas WA Gebauer, Michael Gastegger, Stefaan SP Hessmann, Klaus-Robert M\u00a8uller, and Kristof T Sch\u00a8utt. Inverse design of 3d molecular structures with conditional generative neural networks. Nature communications, 13(1):973, 2022.   \n[16] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.   \n[17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[18] Emiel Hoogeboom, V\u0131ctor Garcia Satorras, Cl\u00b4ement Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3d. In International conference on machine learning, pages 8867\u20138887. PMLR, 2022.   \n[19] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In Proceedings of the IEEE international conference on computer vision, pages 1501\u20131510, 2017.   \n[20] Jan H Jensen. A graph-based genetic algorithm and generative model/monte carlo tree search for the exploration of chemical space. Chemical science, 10(12):3567\u20133572, 2019.   \n[21] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph generation. In International conference on machine learning, pages 2323\u20132332. PMLR, 2018.   \n[22] Jaehyeong Jo, Seul Lee, and Sung Ju Hwang. Score-based generative modeling of graphs via the system of stochastic differential equations. In International Conference on Machine Learning, volume 162, pages 10362\u201310383. PMLR, 2022.   \n[23] Soumil Y Joshi and Sanket A Deshmukh. A review of advancements in coarse-grained molecular dynamics simulations. Molecular Simulation, 47(10-11):786\u2013803, 2021.   \n[24] Michael Langsam. Polyimides for gas separation. In Polyimides, pages 697\u2013742. CRC Press, 2018.   \n[25] Seul Lee, Jaehyeong Jo, and Sung Ju Hwang. Exploring chemical space with score-based out-ofdistribution generation. In International Conference on Machine Learning, pages 18872\u201318892. PMLR, 2023.   \n[26] Gang Liu, Tong Zhao, Jiaxin Xu, Tengfei Luo, and Meng Jiang. Graph rationalization with environment-based augmentations. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1069\u20131078, 2022.   \n[27] Gang Liu, Eric Inae, Tong Zhao, Jiaxin Xu, Tengfei Luo, and Meng Jiang. Data-centric learning from unlabeled graphs with diffusion model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id $\\cdot^{=}$ DmakwvCJ7l.   \n[28] Gang Liu, Tong Zhao, Eric Inae, Tengfei Luo, and Meng Jiang. Semi-supervised graph imbalanced regression. In 29th SIGKDD Conference on Knowledge Discovery and Data Mining, 2023.   \n[29] Ruimin Ma and Tengfei Luo. Pi1m: a benchmark database for polymer informatics. Journal of Chemical Information and Modeling, 60(10):4684\u20134690, 2020.   \n[30] Amina Mollaysa, Brooks Paige, and Alexandros Kalousis. Goal-directed generation of discrete structures with conditional generative models. Advances in Neural Information Processing Systems, 33:21923\u201321933, 2020.   \n[31] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.   \n[32] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pages 8162\u20138171. PMLR, 2021.   \n[33] Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon. Permutation invariant graph generation via score-based generative modeling. In International Conference on Artificial Intelligence and Statistics, pages 4474\u20134484. PMLR, 2020.   \n[34] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195\u20134205, 2023.   \n[35] Daniil Polykovskiy, Alexander Zhebrak, Benjamin Sanchez-Lengeling, Sergey Golovanov, Oktai Tatanov, Stanislav Belyaev, Rauf Kurbanov, Aleksey Artamonov, Vladimir Aladinskiy, Mark Veselov, et al. Molecular sets (moses): a benchmarking platform for molecular generation models. Frontiers in pharmacology, 11:565644, 2020.   \n[36] Kristina Preuer, Philipp Renz, Thomas Unterthiner, Sepp Hochreiter, and Gunter Klambauer. Fr\u00b4echet chemnet distance: a metric for generative models for molecules in drug discovery. Journal of chemical information and modeling, 58(9):1736\u20131741, 2018.   \n[37] Philipp Renz, Dries Van Rompaey, Jo\u00a8rg Kurt Wegner, Sepp Hochreiter, and Gu\u00a8nter Klambauer. On failure modes in molecule generation and optimization. Drug Discovery Today: Technologies, 32:55\u201363, 2019.   \n[38] Lloyd M Robeson. The upper bound revisited. Journal of membrane science, 320(1-2):390\u2013400, 2008.   \n[39] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking the human out of the loop: A review of bayesian optimization. Proceedings of the IEEE, 104 (1):148\u2013175, 2015.   \n[40] A Thornton, L Robeson, B Freeman, and D Uhlmann. Polymer gas separation membrane database, 2012.   \n[41] Austin Tripp and Jos\u00b4e Miguel Hern\u00b4andez-Lobato. Genetic algorithms are strong baselines for molecule generation. arXiv preprint arXiv:2310.09267, 2023.   \n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[43] Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard. Digress: Discrete denoising diffusion for graph generation. arXiv preprint arXiv:2209.14734, 2022.   \n[44] Tomer Weiss, Eduardo Mayo Yanes, Sabyasachi Chakraborty, Luca Cosmo, Alex M Bronstein, and Renana Gershoni-Poranne. Guided diffusion for inverse molecular design. Nature Computational Science, pages 1\u201310, 2023.   \n[45] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229\u2013256, 1992.   \n[46] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513\u2013530, 2018.   \n[47] Yutong Xie, Chence Shi, Hao Zhou, Yuwei Yang, Weinan Zhang, Yong Yu, and Lei Li. Mars: Markov molecular sampling for multi-objective drug discovery. arXiv preprint arXiv:2103.10432, 2021.   \n[48] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geometric diffusion model for molecular conformation generation. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id= PzcvxEMzvQC.   \n[49] Minkai Xu, Alexander S Powers, Ron O Dror, Stefano Ermon, and Jure Leskovec. Geometric latent diffusion models for 3d molecule generation. In International Conference on Machine Learning, pages 38592\u201338610. PMLR, 2023.   \n[50] Yiheng Zhu, Jialu Wu, Chaowen Hu, Jiahuan Yan, Tingjun Hou, Jian Wu, et al. Sampleefficient multi-objective molecular optimization with gflownets. Advances in Neural Information Processing Systems, 36, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "table", "img_path": "cfrDLD1wfO/tmp/6376dbb66d1cf6da573127b285bcca5bea52ee5becc5be3283787b4e3c641541.jpg", "table_caption": ["Table 4: Dataset information for all multi-conditional generation and inverse polymer design tasks. $\\mathrm{O_{2}/C O_{2}/N_{2}P e r m}$ only denotes the data statistics considering only one permeability and the generation results are presented in Table 5. The number of task conditions shown in the table does not include the timestep condition in the diffusion model. "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "A Details on the Denoising Model Component ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Numerical Condition Encoding ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We explore several approaches for encoding numerical conditions. In addition to the clustering-based method, we consider: ", "page_idx": 12}, {"type": "text", "text": "1. The direct encoding approach, which employs a linear layer to map a continuous number into a high-dimensional space.   \n2. The interval-based approach, as described in [28], divides the label space into $N_{\\mathrm{Interval}}$ intervals. It then converts the number into an interval index, allowing us to apply one-hot encoding for the number. ", "page_idx": 12}, {"type": "text", "text": "A.2 Neural Architecture for Conditions ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Besides the AdaLN, there are two more options to integrate condition representation into molecular graph representations [34]: ", "page_idx": 12}, {"type": "text", "text": "1. The In-Context conditioning approach adds the condition representation c to each row of the molecular graph representation $\\mathbf{H}$ after mapping the $\\mathbf{X}_{G}^{t}$ into $\\mathbf{H}$ using the linear layer in the structure encoder.   \n2. The Cross-Attention approach concatenates the timestep encoding vector with the condition representation from synthesis scores or task-related properties into a two-length sequence. In each Transformer encoder layer, this is followed by a cross-attention layer at the end of the standard multi-head self-attention layer. ", "page_idx": 12}, {"type": "text", "text": "B Details on Datasets and Evaluation Methods ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "All experiments can be run on a single A6000 GPU card. ", "page_idx": 12}, {"type": "text", "text": "B.1 Datasets and Task Conditions ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "As presented in Table 4, we collect popular datasets in prediction tasks for more challenging molecular generation tasks. We include a polymer dataset [40, 26] for material design. It consists of conditions of $\\mathrm{O_{2}}$ , $\\mathrm{CO_{2}}$ , and $\\ensuremath{\\mathbf{N}}_{2}$ , which study the numerical gas permeability for oxygen, carbon dioxide, and nitrogen, respectively. Additionally, we also study the generative performance of different models separately on the polymer data with $\\mathrm{O_{2}}$ , $\\mathrm{CO_{2}}$ , or $\\Nu_{2}$ , as illustrated in Table 4. We also create three class-balanced molecule datasets from [46] for drug design: HIV, BBBP, and BACE, which study categorical properties related to the inhibition of HIV virus replication, blood-brain barrier permeability, and inhibition of human $\\beta$ -secretase 1, respectively. We aim to generate synthesizable molecules. Therefore, we add two numerical conditions for synthetic complexity scores [12, 8] for each of the tasks. For the gas separation polymer design task, we consider joint conditions for $\\mathrm{O_{2}}$ and $\\ensuremath{\\mathrm{N_{2}}}$ and measure the selectivity $\\mathrm{O_{2}/N_{2}}$ as the ratio between two gas permeability scores. All polymer gas permeabilities are scaled in the log space following previous work [29]. We focus on experiments for polymers and molecules within 50 nodes. ", "page_idx": 12}, {"type": "image", "img_path": "cfrDLD1wfO/tmp/451fc63fc11016447e655950dcba45462760e0236a60f0f3564a51e045886257.jpg", "img_caption": ["Figure 5: Histogram of Generated Distribution for Atom and Bond Types in Different Models. Results are calculated based on Table 1 for the polymer gas permeability tasks. We observe that the atom and bond type distributions from our Graph DiT\u2019s generated molecules are closer to those of the training data than other diffusion models. It indicates that Graph DiT has better capacity for learning molecular distributions. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "B.2 Evaluation and Metrics ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We randomly split the dataset into training, validation, and testing (reference) sets in a 6:2:2 ratio. We investigate more than eight metrics to systematically evaluate the generation performance. First, we assess generation validity (Validity). Second, we evaluate the distribution learning capacity of different models by measuring heavy atom type coverage (Coverage), internal diversity among the generated examples using Tanimoto similarity (Diversity), fragment-based similarity with the reference set (Similarity), and the Fre\u00b4chet ChemNet Distance with the reference set (Distance). Third, we evaluate the model\u2019s controllability by measuring the mean absolute error (MAE) between the generated condition score and the actual condition scores if the condition is numerical; otherwise, we measure the accuracy score. We follow previous work [14] to use the random forest trained on all the available data as the Oracle evaluation function. For molecular optimization algorithms, we train random forest predictors on the training set for conditional generation. For predictor-guided diffusion models, DiGress v2 and MOOD, we use the same architecture as their denoising models to train predictors for diffusion guidance. Given the conditions in the test set, we report the generation performance by generating 10,000 examples for the six multi-conditional generation tasks and 1,000 examples for the polymer inverse design problem focused on selectivity. ", "page_idx": 13}, {"type": "text", "text": "B.3 Datasets and Tasks in Figure 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Using the same dataset from the $\\mathrm{O_{2}/N_{2}}$ polymer inverse design task, we keep 100 polymers to provide condition sets for testing and split the rest into training and validation sets in a 0.65:0.35 ratio. We apply our proposed Graph DiT for both single-conditional and multi-conditional approaches, focusing on three properties: (1) Synth. score for synthesizability [12], (2) $\\mathrm{O_{2}}$ permeability, and (3) $\\Nu_{2}$ permeability. The single-conditional approach generates 30 polymers per condition for each test data point, totaling 9,000 polymers. In contrast, the multi-conditional approach generates 30 polymers for each set of conditions per test data point, resulting in 3,000 polymers. We rank these polymers based on the mean absolute error between the generated properties (evaluated by a random forest model trained on all the data to simulate the Oracle function) and the conditional property. For each test data point, we also rank the best multi-conditional polymer in different single-conditional sets. For the single-conditional approach, we identify a common polymer meeting various properties and visualize the minimum top $K$ value distribution across all 100 test points. ", "page_idx": 13}, {"type": "image", "img_path": "cfrDLD1wfO/tmp/d9f348316502b58152085e76a7d8e9795c52a13e9666ee2b982185567bb75e6d.jpg", "img_caption": ["Figure 6: We compare the average ranking of generated polymers with desirable properties. First, we generate three sets of polymers using a single-conditional approach for each condition. In (a), as shown in Figure 1, we find the ranking of the shared structure for each multi-condition requirement. In (b), polymers are generated using a multi-conditional approach, and for each, we identify the highest ranking among the three single-conditional sets. Then, we calculate the median of these maximum ranking positions, which is 16, approximately $2\\times$ better than single-conditional generation, which has a median value greater than 30. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "In addition to Figure 1, we compute the median rankings of the multi-conditionally generated polymers within the single-conditional sets. The results are shown in Figure 6. Both Figure 1 and Figure 6 demonstrate the advantages of multi-conditional generation over single-conditional generation. ", "page_idx": 14}, {"type": "text", "text": "C Details on Multi-Conditional Generation Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We show results for three polymer generation tasks in Table 1 and molecule generation tasks in Table 2. As complementary results for Table 1, we present new results on generation using only one gas permeability in Table 5. We also compare the distributions of atom and bond types between generated and training data in Figure 5. Furthermore, Figure 7 visualizes the two-dimensional molecular data distribution of both training and generated molecules across various generative models. ", "page_idx": 14}, {"type": "text", "text": "C.1 Discussion on Diffusion Model Baselines ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "While diffusion models like GDSS [22] and DiGress [43] show promise in unconditional tasks, their performance in multi-conditional generations needs improvement for fitting training distributions and achieving more controllable results. As indicated by Figure 5(a), the generation of GDSS [22] often collapses to carbon elements with Gaussian noise in the continuous diffusion state-space. MOOD [25] improves atom type coverage by adding predictor guidance and an out-of-distribution hyper-parameter, but it is hard to fti the training distribution, as visualized in Figure 7(h). DiGress and its predictor-guided variant [43] (i.e., DiGress v2), using discrete state-space and transition matrices for diffusion noise, outperform GDSS and MOOD in distribution fitting and internal diversity in Tables 1, 2 and 5. However, as indicated in Figure 7(e) and Figure 7(f), these two models still generate too many out-of-distribution examples without justification of the generalization capacity. While GDSS and MOOD show lower average MAE in polymer conditional generation tasks, their subpar distribution learning performance and the results from Table 2 suggest that this may be due to the carbon element, which may be a confounder and affect the evaluation of the correlation between the model and controllability in polymer tasks. ", "page_idx": 14}, {"type": "text", "text": "C.2 Discussion on Molecular Optimization Baselines ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Popular molecular optimization baselines are competitive in molecular generation tasks. Earlier studies have noted their strong performance: Gao et al. [14] showed their effectiveness in the standard molecular optimizations with a combined optimization target, and Tripp and Herna\u00b4ndez-Lobato [41] found that genetic algorithm often outperforms recent methods in unconditional generation. We observe their competitive performance in multi-conditional settings, characterized by high validity, good atom type coverage, and distribution similarity in Tables 1, 2 and 5. MARS [47] and JTVAE [21] perform well for controlling the synthetic accessibility score [12] but are less effective at controlling specific task properties like gas permeability, BACE, BBBP, and HIV. For example, in generation tasks with the categorical task condition, the generated examples only achieve around $50\\%$ accuracy in hitting the input condition. ", "page_idx": 14}, {"type": "table", "img_path": "cfrDLD1wfO/tmp/c3cc935f13b997a3947e5c8b6e0f46c0efc35ac502417e3dda7244ed136f4e81.jpg", "table_caption": ["Table 5: Generation of 10K Polymers: Results on a numerical synthesizability score (Synth.) and a numerical properties (gas permeability for $\\mathrm{O_{2}}$ , $\\Nu_{2}$ , or $\\mathrm{{CO}_{2}}$ ). MAE is calculated between input conditions and generated properties. Best results are highlighted. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "cfrDLD1wfO/tmp/01bd86551fb17b6d94c3d060589bcd1aa65002fb44da8ec0bf712f416c97f96c.jpg", "table_caption": ["Table 6: Complete results on 1,000 generated polymers for the inverse $\\mathrm{O_{2}/N_{2}}$ gas separation polymer design. # UB is the count of generated polymers successfully identified (by Oracle functions) as upper bound instances defined by Robeson [38]. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "image", "img_path": "cfrDLD1wfO/tmp/c6f138d3b1c1922c4e91295b6bfb3bf3e1dca5be5d98645bfb1105a878c3230c.jpg", "img_caption": ["Figure 7: Distribution of training (grey-colored) and generated (orange-colored) molecules. The generated distribution in Figure 7(i) is from Graph DiT, and the visualization shows that the generated molecular data points fti the training distribution well, with reasonable interpolation and extrapolation in the training data space. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "C.3 Discussion on Training Dynamics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Figure 8, we illustrate changes in various indicators on the validation set during model training. We note an increase in generated validity and similarity to the validation reference set, along with a decrease in distance to the reference set and errors between generated properties and conditions, indicating gradual improvement in conditional generation over epochs. However, a trade-off between distribution fitting and internal diversity is observed in our current model, suggesting that further work on enhancing generation diversity could be promising. ", "page_idx": 16}, {"type": "image", "img_path": "cfrDLD1wfO/tmp/3e304eba5b3bbc1802e2ad04ad5232d28e968ad9f1269f6e004566d5a98abc4c.jpg", "img_caption": ["Figure 8: Change of indicators on the validation set during model training. The generated validity and similarity to the validation reference set have increased, accompanied by a decrease in distance to the reference set and errors between generated properties and conditions. We also observe a trade-off between distribution fitting and internal diversity over epochs. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "C.4 Discussion on Uniqueness, and Novelty ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We evaluate the model\u2019s performance on Novelty and Uniqueness. Unlike unconditional generation, multi-conditional generation involves generating multiple possible molecules under the same condition. Therefore, we compute these metrics across different sets of conditions rather than for generated molecules under the same conditions. Results are presented in Table 7. ", "page_idx": 17}, {"type": "text", "text": "Graph DiT demonstrates reasonable performance on these metrics. However, higher Novelty and Uniqueness values do not necessarily indicate better performance, as they may not reflect the model\u2019s ability to design satisfactory molecules with desirable properties. Moreover, these values risk leading to misleading conclusions. For instance, AddCarbon achieves nearly perfect scores $g9.94\\%$ Novelty and $99.86\\%$ Uniqueness) according to [37, 41], yet it randomly adds carbon atoms to existing molecules, resulting in new molecules that are not practically useful [37]. ", "page_idx": 17}, {"type": "table", "img_path": "cfrDLD1wfO/tmp/ffbcf310787e55b6de3b164138d6cae22cecca74c85620253005dc82ce5be19d.jpg", "table_caption": ["Table 7: Comparison of Novelty and Uniqueness across different conditions "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "D Details on Polymer Inverse Design ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We aim to design polymers with high $\\mathrm{O_{2}}$ and low $\\Nu_{2}$ permeability, showing refined control of models over these properties. This is reflected in the selectivity, defined as the $\\mathrm{O_{2}/N_{2}}$ permeability ratio. Robeson [38] has identified an inherent trade-off between gas permeability and selectivity, known as the upper-bound. Ideally, high-performance polymers should fall in the above-the-bound region, demonstrating an effective combination of permeability and selectivity. We have 609 polymers with annotated permeability values for both gases. 16 above-the-bound polymers are included in the test/reference set and excluded from the training set. We generate 1,000 polymers conditional on test set labels. ", "page_idx": 18}, {"type": "text", "text": "D.1 Survey Setup on Generated Polymers ", "text_level": 1, "page_idx": 18}, {"type": "image", "img_path": "cfrDLD1wfO/tmp/88315a20c4fb70d4d8d2409ffb3e646057e00b1dbfe7639f553fb57353751295.jpg", "img_caption": ["Figure 9: The reference polymer structure in the case study has conditions $\\{\\mathrm{SAS}{=}3.8\\$ , ${\\mathrm{SCS}}{=}4.3$ , $\\mathrm{O_{2}P e r m{=}34.0}$ , $\\mathrm{N}_{2}\\mathrm{Perm}{=}5.2\\}$ . "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "We aim to gather expert evaluations on the generation performance of various methods. We conduct a study using a test data point from the $\\mathrm{O_{2}/N_{2}}$ gas separation inverse design task, taking its properties as conditions. The structure of the selected data point is presented in Figure 9. We display 25 generated polymers, each with its properties, alongside three real polymers from the training dataset as references. The first real polymer serves as the test reference, while the other two, similar in properties to the first, also aid experts in assessing the generated polymers. The properties of these generated polymers are predicted using a well-trained random forest model. Experts are asked to rank the generated polymers from 1 to 25, considering: (1) Structures of real polymers with desirable properties; (2) Predicted properties of generated polymers, displayed beneath each visualization. Here, a rank of 1 represents the best example as per domain knowledge, while 25 is the least favorable. Rankings are then converted to utility scores (UtS) ranging from 0 to 1 using $\\frac{1}{\\mathrm{ranking}}$ , allowing us to quantify the relative performance of different generation methods. The agreement score (AS) could be obtained by exp ( $-25\\times$ Variance $\\mathrm{[UtS_{1}}$ , $\\mathrm{{UtS}_{2}}$ , $\\mathrm{{Ut}S_{3}}$ , $\\mathrm{UtS}_{4})$ ), where $\\mathrm{Ut}{\\bf S}_{i}$ denotes the utility score from the $i$ -th domain expert. (3) Finally, we select top-3 polymers for each generative models and present them in Figure 3. ", "page_idx": 18}, {"type": "text", "text": "D.2 Results on Inverse Design ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We present the inverse design results of all 1,000 generated polymers in validity, distribution learning, and condition control in Table 6. # UB is the count of generated polymers successfully identified as upper bound instances. Higher # UB indicates that Graph DiT has a higher likelihood of generating candidates for excellent $\\mathrm{O_{2}}$ and $\\Nu_{2}$ gas separation. The smallest MAE across most properties and a $9.9\\%$ average MAE improvement over baselines highlight Graph DiT\u2019s superior control in generating examples closely aligned with multiple conditions. ", "page_idx": 18}, {"type": "image", "img_path": "cfrDLD1wfO/tmp/597b27feb589465f28c6b4dd68786615d7a06d65790f1f28473a12afb24bcb70.jpg", "img_caption": ["(a) Changes of Validity "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "cfrDLD1wfO/tmp/a1a14db54b7dd966f11f08fd4760042d3f69ca0b69728efbaeaf69cf22077c96.jpg", "img_caption": ["(b) Changes of MAE to the target Synthesizability "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "cfrDLD1wfO/tmp/7c0787903b4b68df88ccce7d5c52be8c01dbdd263957df99c021b33fa958f864.jpg", "img_caption": ["Figure 10: Analysis of Model Controllability when Varying $N_{2}$ Values: The true $N_{2}$ value from the test set is 213.75. We note that the controllability performance (i.e., MAE value) for $N_{2}$ and $O_{2}$ is measured in log space. ", "(c) Changes of MAE to the target $N_{2}$ "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "cfrDLD1wfO/tmp/6a7e1213f80a06b31865418180073eeb0cfb975cf631ab92ca705f31f7ee5145.jpg", "img_caption": ["(d) Changes of MAE to the target $O_{2}$ "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "E Details and More Results on Model Analysis ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "E.1 Case Studies on Generation Controllability ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We conduct a new case study on the $O_{2}/N_{2}$ polymer dataset, studying the controllability on three properties synthesizability score, $O_{2}$ and $N_{2}$ properties with varied $N_{2}$ property values. We select a polymer example from the test set and vary its $N_{2}$ while keeping other properties fixed. The $N_{2}$ property from the test polymer is 213.75, and we vary it from 0 to 1000. We sample 50 values within this range and generate 100 polymer graphs conditioned on multiple properties with each sampled value. We evaluate various metrics, including the chemical validity of the generated polymers. ", "page_idx": 19}, {"type": "text", "text": "We visualize results in Figure 10. We consistently observe that validity and controllability performance improve as the values approach 213.75, derived from a real test polymer. Conversely, performance deteriorates when the sampled $N_{2}$ values are closer to the extremes of the sampling range (0 or 1000). This observation underscores the interdependency between conditions, where less frequent combinations of different properties may be more challenging to learn. Moreover, the model performs well across a elatively large range from 0 to 1000 in terms of validity, $O_{2}$ , and synthesizability score control. This demonstrates good generalization of the proposed method in capturing complex condition interdependencies. ", "page_idx": 19}, {"type": "image", "img_path": "cfrDLD1wfO/tmp/678332812be406f6afbbe30714fa02b4b7fddddee4bcc9c1e25fff9fecc438ea.jpg", "img_caption": ["Figure 11: Ablation studies on the final MLP layer "], "img_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "cfrDLD1wfO/tmp/926fc28f2a0f55103c71523f7f3480487be74e73a68380e8d916c70963a562e1.jpg", "table_caption": ["Table 8: Training Performance of Oracle Methods: we train the models on all polymers or small molecules in a task to simulate the Oracle. Results from the random forest model are highlighted because it has the lowest training MAE and highest training AUC. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "E.2 Ablation Studies on Final MLP ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In addition to the three components related to conditioning effectiveness of Graph DiT studied in Section 4.4, we also examine the importance of the final layer MLP for conditional graph denoising. Results in Figure 11 show that MLP significantly outperforms a linear layer [34]. ", "page_idx": 20}, {"type": "text", "text": "E.3 Details on Oracle Simulation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We train three types of Oracles based on Random Forest, Gaussian Process, and Support Vector Machine on all polymers or molecules in a task to evaluate the properties of generated polymers conditional on $\\mathrm{O_{2}P e r m}$ only, $\\mathrm{N_{2}P e r m}$ only, $\\mathrm{CO}_{\\mathrm{2}}\\mathrm{Perm}$ only, BACE, BBBP, or HIV. The training performance (MAE or AUC) is presented in Table 8. Our findings show that the random forest achieves the lowest MAE and highest AUC scores, leading us to select it for simulating oracles in our generation evaluation process. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Discussions on the proposed method and empirical support can be found in sections 1, 3, and 4. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Discussions on generation diversity and Oracle evaluations are found in Sections 4.2 and 4.4. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper focuses on empirical studies. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Details are in Section 4, appendix, and supplementary code. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Code is provided in the supplementary materials. Data and code will be on Github after publication. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Details are in Section 4.1. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Improvement percentages $(>\\!10\\%$ ) are discussed in Section 4.2. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Discussions are in Appendix B. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The research conforms the NeurIPS Code of Ethics. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The work has positive societal impacts on designing molecules for drugs and materials, as discussed in Sections 1 and 6. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: ", "page_idx": 25}, {"type": "text", "text": "Justification: The work does not present issues of high-risk misuse. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All data sources and baseline models are open-sourced. They have been properly credited and mentioned, as outlined in Section 4.1. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: ", "page_idx": 26}, {"type": "text", "text": "Justification: No new assets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: ", "page_idx": 26}, {"type": "text", "text": "Justification: The experiment results are verified through a survey conducted with a small group of domain experts. However, no crowdsourcing experiments were conducted. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}]