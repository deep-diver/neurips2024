[{"figure_path": "8Fxqn1tZM1/figures/figures_23_1.jpg", "caption": "Figure 1: Norm distribution on the weights and biases in the CIFAR-10-GS-ReLU dataset.", "description": "This figure shows the distribution of the norms of weights and biases for each layer of the CNNs in the CIFAR-10-GS dataset that use ReLU activation function. The distributions are shown separately for weights and biases in each of the four layers of the CNNs. The x-axis represents the norm values, and the y-axis represents the frequency of occurrence of those norm values. The distributions suggest that weights and biases are not uniformly distributed in norm, with some norm values being more frequent than others. This information is used in the paper to understand and analyze the scaling symmetries present in the datasets.", "section": "6 Experiments"}, {"figure_path": "8Fxqn1tZM1/figures/figures_24_1.jpg", "caption": "Figure 1: Norm distribution on the weights and biases in the CIFAR-10-GS-ReLU dataset.", "description": "This figure shows the distribution of the norms of weights and biases for each layer (1-4) of a convolutional neural network trained on the CIFAR-10 dataset using ReLU activation function.  The distributions are shown separately for weights and biases, providing a visual representation of how the magnitude of these parameters vary across the layers of the network. This information is relevant to understanding the scaling symmetries of neural networks and how they may affect learning and generalization.", "section": "6 Experiments"}, {"figure_path": "8Fxqn1tZM1/figures/figures_24_2.jpg", "caption": "Figure 3: Sign distribution of the weights and biases in the CIFAR-10-GS-tanh dataset.", "description": "This figure shows the distribution of signs (+1 or -1) for weights and biases across four layers (layer 1 to layer 4) in the CIFAR-10-GS-tanh dataset.  The histograms illustrate the proportion of positive and negative values for each layer, providing insights into the symmetry characteristics of the weights and biases in this dataset. Notably, the near-even distribution of positive and negative values in each layer suggests that the weights and biases do not have an inherent positive or negative bias, which is useful information for network training and analysis. The distributions also show the degree of symmetry, as an almost uniform distribution of weights/biases could suggest a high degree of symmetry.", "section": "A.5 Additional ablation studies"}, {"figure_path": "8Fxqn1tZM1/figures/figures_25_1.jpg", "caption": "Figure 1: Norm distribution on the weights and biases in the CIFAR-10-GS-ReLU dataset.", "description": "This figure shows the distribution of the norms of weights and biases for each layer of a convolutional neural network (CNN) trained on the CIFAR-10 dataset using ReLU activation functions. The distributions are shown separately for weights and biases, and are displayed for each layer of the network. The purpose of the figure is to illustrate the distribution of the parameters to showcase the need for Scale Equivariant networks.  The distributions reveal variations across layers and whether the symmetries studied in this paper are present in the datasets used.", "section": "6 Experiments"}]