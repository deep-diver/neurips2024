[{"figure_path": "J2wI2rCG2u/figures/figures_2_1.jpg", "caption": "Figure 1: The computation graph of computing second order gradient by repeated application of backward mode AD, for a function F(\u00b7) with 4 primitives (L = 4), which computes the Hessian-vector-product. Red nodes represent the cotangent nodes in the second backward pass. With each repeated application of VJP the length of sequential computation doubles.", "description": "This figure illustrates the inefficiency of using repeated backward mode automatic differentiation (AD) for higher-order derivatives.  It shows that with each repeated application of the backward pass (VJP), the computation graph grows exponentially in length and memory usage. The red nodes highlight the accumulating cotangents in the second backward pass, emphasizing the computational cost increase.", "section": "3.2 Inefficiency of the first-order AD for high-order derivative on inputs"}, {"figure_path": "J2wI2rCG2u/figures/figures_4_1.jpg", "caption": "Figure 2: The computation graph of d\u00b2F for F with 4 primitives. Parameters \u03b8i are omitted. The first column from the left represents the input 2-jet J(t) = (x, v(1), v(2)), and d\u00b2F\u2081 pushes it forward to the 2-jet Jf\u25cbg(t) = (y1, v(1), v(1)(2)) which is the subsequent column. Each row can be computed in parallel, and no evaluate trace needs to be cached.", "description": "This figure illustrates the computation graph for calculating the second-order Fr\u00e9chet derivative (d\u00b2F) of a function F composed of four primitives (F1 to F4).  The input is a 2-jet, which contains the primal (x) and two tangents (v(1) and v(2)). Each primitive's second-order derivative is applied sequentially, pushing the 2-jet forward through the computation graph. The key point is that each row of the computation can be done in parallel, unlike traditional methods, and no evaluation trace needs to be stored, making this approach significantly more memory-efficient and computationally faster.", "section": "4.2 Estimating arbitrary differential operator by pushing forward random jets"}, {"figure_path": "J2wI2rCG2u/figures/figures_14_1.jpg", "caption": "Figure 3: The computation graph of forward mode AD (left) and backward mode AD (right) of a function F(\u00b7) with 4 primitives Fi each parameterized by \u03b8i. Nodes represent (intermediate) values, and arrows represent computation. Input nodes are colored blue; output nodes are colored green, and intermediate nodes are colored yellow.", "description": "This figure illustrates the computation graphs for both forward and backward mode automatic differentiation (AD).  The forward mode computes the Jacobian-vector product (JVP) by propagating a tangent vector through the linearized computation graph. The backward mode computes the vector-Jacobian product (VJP) by propagating a cotangent vector backward through the adjoint linearized graph.  The figure highlights the differences in computational flow and memory requirements between the two methods.", "section": "3.1 First-order auto-differentiation (AD)"}, {"figure_path": "J2wI2rCG2u/figures/figures_19_1.jpg", "caption": "Figure 4: Convolutional weight sharing in the first layer, with input dimension 9 and filter size 3.", "description": "This figure illustrates the concept of convolutional weight sharing in the first layer of a neural network.  The input has a dimension of 9. A 1D convolution with a filter size of 3 and a stride of 3 is applied. This reduces the number of parameters, since the same weights (\u03b8\u2081, \u03b8\u2082, \u03b8\u2083) are used across multiple input elements (x\u2081, x\u2082, x\u2083; x\u2084, x\u2085, x\u2086; x\u2087, x\u2088, x\u2089). The output of the convolution are three elements (y\u2081, y\u2082, y\u2083).  This technique is employed to handle high-dimensional input data efficiently, reducing the memory footprint during the training process.", "section": "4.3 Constructing STDE for high-order differential operators with sparse random jets"}, {"figure_path": "J2wI2rCG2u/figures/figures_30_1.jpg", "caption": "Figure 5: Ablation on randomization batch size with Inseparable and effectively high-dimensional PDEs, dim=100k, 5 runs with different random seeds. Model converges when the difference of L2 error is below 1e-7.", "description": "This figure displays ablation studies on the impact of randomization batch size on the performance of the proposed method (STDE) for solving three different types of PDEs: Allen-Cahn, Poisson, and Sine-Gordon.  The results are shown across various metrics including L2 relative error, residual loss, iterations per second, and convergence time.  Each sub-figure presents these metrics for a specific PDE, demonstrating how changes in batch size affect the model's convergence behavior and overall efficiency. The consistent pattern across PDE types emphasizes the impact of this hyperparameter.", "section": "L Further ablation study"}]