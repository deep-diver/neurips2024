{"references": [{"fullname_first_author": "Edward J. Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2022-04-25", "reason": "This paper introduces LoRA, a foundational parameter-efficient fine-tuning technique that is central to the work and is extensively analyzed in the current paper."}, {"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-06", "reason": "This paper is highly influential in establishing the capabilities and characteristics of large language models, which directly relates to the central theme of efficient fine-tuning discussed in the current paper."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2019-06-02", "reason": "BERT is a landmark model in NLP that is frequently used as a base model for downstream tasks, and therefore understanding its characteristics and potential improvements is relevant to the current work."}, {"fullname_first_author": "Robert A. Jacobs", "paper_title": "Adaptive mixtures of local experts", "publication_date": "1991-01-01", "reason": "This seminal paper introduces the Mixture of Experts (MoE) architecture, a key component of the proposed HydraLoRA model, making it an essential reference for understanding this key aspect of the work."}, {"fullname_first_author": "Neil Houlsby", "paper_title": "Parameter-efficient transfer learning for NLP", "publication_date": "2019-06-09", "reason": "This paper introduces the concept of Parameter-Efficient Fine-Tuning (PEFT) and establishes the importance of efficiency for adapting large language models; a core concept in relation to the current paper."}]}