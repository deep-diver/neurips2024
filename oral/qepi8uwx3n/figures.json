[{"figure_path": "qEpi8uWX3N/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of LoRA architecture changes in HydraLoRA. Only the tunable parameters are shown in this Figure. (a) LoRA architecture with matrix A to achieve low rank and matrix B to recover. (b) under the same parameter count, a monolithic LoRA is split into multiple smaller A and B matrices to avoid training interference. (c) based on (b), HydraLoRA has an asymmetric structure that has a shared A matrix and multiple B matrices.", "description": "This figure illustrates the evolution of the LoRA architecture from its original symmetric form to the asymmetric HydraLoRA.  Panel (a) shows the standard LoRA with matrices A and B. Panel (b) demonstrates how a single, larger LoRA can be broken into multiple smaller LoRA units to alleviate training interference. Panel (c) presents the final HydraLoRA architecture, which utilizes a shared A matrix across multiple B matrices for improved efficiency.", "section": "3 HydraLoRA"}, {"figure_path": "qEpi8uWX3N/figures/figures_2_1.jpg", "caption": "Figure 2: Performance impact of corpus heterogeneity on full fine-tuning vs. parameter-efficient fine-tuning. Heterogeneity signifies the diversity within the dataset, often leading to interference due to its varied content and style [2]. Parameter-efficient approaches are particularly sensitive, suffering greater performance losses in heterogeneous cases.", "description": "The figure shows two lines representing the performance of \"Full Parameter Fine-tuning\" and \"Parameter-Efficient Fine-tuning\" methods as corpus heterogeneity increases.  The line representing full fine-tuning shows a relatively small decrease in performance as heterogeneity increases, while the parameter-efficient line shows a much steeper decline. The difference between the two lines (the gap) widens as heterogeneity increases, illustrating the limitation of parameter-efficient methods when dealing with diverse datasets.", "section": "2.2 LoRA's Practical Dilemma"}, {"figure_path": "qEpi8uWX3N/figures/figures_3_1.jpg", "caption": "Figure 3: Breakdown analysis of LoRA modules. Compare fine-tuned LoRA modules of Dolly-15K [8] with three subtasks of Dolly-15K including \u201csummarization (Sum)\u201d, \u201cclosed QA (QA)\u201d and \u201cinformation extraction (IE)\u201d using t-SNE. Consider LLaMA2-7B (random seed=42), which contains 32 decoder layers, corresponding to 32 adaptive modules. Each module consists of {0: q_proj of A, 1: q_proj of B, 2: v_proj of A, 3: v_proj of B} submodules. This makes a total of 32 \u00d7 4 submodules. Left displays all submodules. Center shows all even submodules, i.e. the A matrix. Right represents all odd submodules, i.e. the B matrix. It can be seen that the differences in the fine-tuned LoRA modules for different tasks arise mainly from the B matrix.", "description": "This figure uses t-SNE to visualize the parameters of LoRA modules trained on different subtasks of the Dolly-15K dataset.  It shows that the parameters of matrix A (even submodules) are similar across different tasks, while the parameters of matrix B (odd submodules) are distinct, highlighting the role of matrix B in task-specific adaptation.", "section": "2.3 Observations"}, {"figure_path": "qEpi8uWX3N/figures/figures_4_1.jpg", "caption": "Figure 4: Architecture and workflow of HydraLoRA. During the fine-tuning stage, HydraLoRA first adaptively identifies and initializes N of intrinsic components without specific domain knowledge. It then employs a trainable MoE router that treats each intrinsic component as an expert to automatically segregate training samples into intrinsic components for fine-tuning. During the inference stage, HydraLoRA merges multiple B matrices flexibly and dynamically through a trained router.", "description": "This figure illustrates the architecture and workflow of HydraLoRA, a novel asymmetric LoRA architecture.  The fine-tuning process involves an adaptive identification and initialization of intrinsic components, followed by a training phase using a Mixture-of-Experts (MoE) router to segregate training samples. During inference, multiple B matrices are merged dynamically using a trained router. This figure shows the process of both fine-tuning and inference phases.", "section": "3 HydraLoRA"}, {"figure_path": "qEpi8uWX3N/figures/figures_7_1.jpg", "caption": "Figure 5: Energy consumption and latency during fine-tuning with different LoRA approaches (fine-tuning LLaMA2-7B with GSM-8K).", "description": "This figure compares the energy consumption (in kWh) and latency (in hours) of different LoRA approaches during the fine-tuning process of the LLaMA2-7B model on the GSM-8K dataset.  The energy consumption is broken down by CPU, GPU, and RAM usage.  The latency is shown as a single value for each approach. The different LoRA approaches compared include LoRA with ranks 8, 16, and 32, LoRA-Split (4x8), and HydraLoRA.", "section": "4.3 Energy and Throughput Analysis"}, {"figure_path": "qEpi8uWX3N/figures/figures_7_2.jpg", "caption": "Figure 6: Comparative performance of ablation study for HydraLoRA across multiple benchmarks.", "description": "The figure displays the performance comparison of HydraLoRA with ablation studies across three benchmarks: Mmlu, Medical, and Law.  It shows the performance drop when removing the MoE architecture, the gating mechanism, and the Hydra architecture itself, demonstrating the contribution of each component to the overall performance of HydraLoRA.", "section": "4.4 Ablation Study"}, {"figure_path": "qEpi8uWX3N/figures/figures_8_1.jpg", "caption": "Figure 7: Number of clusters generated by different approaches including developer-specific (static), k-means, and DBSCAN.", "description": "This figure shows the number of clusters identified by three different methods: a statically defined number of clusters (Static), the k-means clustering algorithm (K-means), and the DBSCAN density-based clustering algorithm (DBSCAN).  The x-axis represents the trial number, while the y-axis shows the number of clusters identified in each trial.  The figure illustrates the variation in the number of clusters identified by each method across multiple trials, highlighting the different behavior and sensitivity of each algorithm to data characteristics and variations across trials.", "section": "4.5 Hyper-parameter Analysis"}, {"figure_path": "qEpi8uWX3N/figures/figures_8_2.jpg", "caption": "Figure 8: The results of experiments for hyper-parameters number of clusters.", "description": "This figure shows the performance of HydraLoRA on the MMLU benchmark with different numbers of clusters (N) generated by k-means.  The x-axis represents the number of clusters (N), ranging from 1 to 5. The y-axis shows the model's performance, measured as a percentage. The figure demonstrates that the performance of HydraLoRA is relatively insensitive to the number of clusters within a reasonable range, with only a small performance drop when using 5 clusters compared to the optimal number of clusters (3 or 4).", "section": "4.5 Hyper-parameter Analysis"}, {"figure_path": "qEpi8uWX3N/figures/figures_16_1.jpg", "caption": "Figure 3: Breakdown analysis of LoRA modules. Compare fine-tuned LoRA modules of Dolly-15K [8] with three subtasks of Dolly-15K including \u201csummarization (Sum)\u201d, \u201cclosed QA (QA)\u201d and \u201cinformation extraction (IE)\u201d using t-SNE. Consider LLaMA2-7B (random seed=42), which contains 32 decoder layers, corresponding to 32 adaptive modules. Each module consists of {0: q_proj of A, 1: q_proj of B, 2: v_proj of A, 3: v_proj of B} submodules. This makes a total of 32 \u00d7 4 submodules. Left displays all submodules. Center shows all even submodules, i.e. the A matrix. Right represents all odd submodules, i.e. the B matrix. It can be seen that the differences in the fine-tuned LoRA modules for different tasks arise mainly from the B matrix.", "description": "This figure presents a t-SNE visualization of the parameters of LoRA modules fine-tuned on three different subtasks of the Dolly-15K dataset. It shows that the parameters of matrix A are similar across different tasks, while the parameters of matrix B are distinct. This observation supports the hypothesis that matrix A captures commonalities across tasks, while matrix B adapts to task-specific diversities.", "section": "2.3 Observations"}, {"figure_path": "qEpi8uWX3N/figures/figures_16_2.jpg", "caption": "Figure 9: Breakdown analysis of LoRA modules. Compare fine-tuned LoRA modules of GSM-8K [7] with its subsets using T-SNE. We employ the Independent and Identically Distributed (IID) segmentation scheme to divide GSM8K into three subsets and fine-tune them using different LoRAs. Consider LLaMA2-7B (random seed=42), which contains 32 decoder layers, corresponding to 32 adaptive modules. Each module consists of {0: q_proj_A, 1: q_proj_B, 2: v_proj_A, 3: v_proj_B} submodules. (a,b) left displays all submodules. (a,b) center shows all even submodules, i.e. the A-matrix. (a,b) right represents all odd submodules, i.e. the B-matrix. It can be seen that the differences in the fine-tuned LoRA modules for different tasks arise mainly from the B matrix.", "description": "This figure presents a breakdown analysis of LoRA modules using t-SNE visualization. It compares fine-tuned LoRA modules trained on the full GSM8K dataset and its three subsets, each fine-tuned with a different LoRA.  The visualization highlights the differences in the A and B matrices across different tasks, showing that the variations primarily stem from the B matrices. This observation supports the paper's hypothesis that a shared A matrix and multiple B matrices are more effective for efficient fine-tuning.", "section": "D Limitation"}]