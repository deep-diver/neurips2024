[{"type": "text", "text": "NeuroClips: Towards High-fidelity and Smooth fMRI-to-Video Reconstruction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zixuan $\\mathbf{Gong}^{1}$ , Guangyin $\\mathbf{Bao}^{1}$ , Qi $\\mathbf{Z}\\mathbf{hang}^{1,\\dagger}$ , Zhongwei $\\mathbf{Wan}^{2}$ , Duoqian $\\mathbf{Mia}^{1,\\dagger}$ , Shoujin Wang3, Lei $\\mathbf{Z}\\mathbf{h}\\mathbf{u}^{1}$ , Changwei Wang4, Rongtao $\\mathbf{X}\\Bar{\\mathbf{u}}^{4}$ , Liang $\\mathbf{H}\\mathbf{u}^{1}$ , Ke $\\mathbf{Liu}^{5}$ , Yu Zhang ", "page_idx": 0}, {"type": "text", "text": "1Tongji University 2Ohio State University 3University of Technology Sydney 4Chinese Academy of Sciences 5Beijing Anding Hospital {gongzx,baogy,zhangqi_cs,dqmiao,izy}@tongji.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reconstruction of static visual stimuli from non-invasion brain activity fMRI achieves great success, owning to advanced deep learning models such as CLIP and Stable Diffusion. However, the research on fMRI-to-video reconstruction remains limited since decoding the spatiotemporal perception of continuous visual experiences is formidably challenging. We contend that the key to addressing these challenges lies in accurately decoding both high-level semantics and low-level perception flows, as perceived by the brain in response to video stimuli. To the end, we propose NeuroClips, an innovative framework to decode high-fidelity and smooth video from fMRI. NeuroClips utilizes a semantics reconstructor to reconstruct video keyframes, guiding semantic accuracy and consistency, and employs a perception reconstructor to capture low-level perceptual details, ensuring video smoothness. During inference, it adopts a pre-trained T2V diffusion model injected with both keyframes and low-level perception flows for video reconstruction. Evaluated on a publicly available fMRI-video dataset, NeuroClips achieves smooth high-fidelity video reconstruction of up to 6s at 8FPS, gaining significant improvements over state-of-the-art models in various metrics, e.g., a $128\\%$ improvement in SSIM and an $81\\%$ improvement in spatiotemporal metrics. Our project is available at https://github.com/gongzix/NeuroClips. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Decoding visual stimuli from neural activity is crucial and prospective to unraveling the intricate mechanisms of the human brain. In the context of non-invasive approaches, visual reconstruction from functional magnetic resonance imaging (fMRI), such as fMRI-to-image reconstruction, shows high fidelity [1, 2, 3, 4], largely beneftiing from advanced deep learning models such as CLIP [5, 6] and Stable Diffusion [7]. This convergence of brain science and deep learning presents a promising data-driven learning paradigm to explore a comprehensive understanding of the advanced perceptual and semantic functions of the cerebral cortex. Unfortunately, fMRI-to-video reconstruction still presents significant hurdles that discourage researchers, since decoding the spatiotemporal perception of a continuous flow of scenes, motions, and objects is formidably challenging. ", "page_idx": 0}, {"type": "text", "text": "At first glance, fMRI measures blood oxygenation level-dependent (BOLD) signals by snapshotting a few seconds of brain activity, leading to differential temporal resolutions between fMRI (low) and videos (high). The previously advisable solution to address such differential granularity is to perform self-interpolation on fMRI and downsample video frames to pre-align fMRI and videos. Going further, decoding accurate high-level semantics and low-level perception flows has a more profound impact on the ability to reconstruct high-fidelity videos from brain activity. Early studies before 2022 struggled with achieving satisfactory reconstruction performance, as they failed to acquire precise semantics from powerful (pre-trained) diffusion models. The latest research MinD-Video [8] guides the diffusion model conditioned on visual fMRI features, making an initial attempt to address the semantic issue. However, it lacks a design of low-level visual detailing, so it significantly diverges from the brain\u2019s visual system, exhibiting limitations in perceiving continuous low-level visual details. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The brain\u2019s reflection of video stimuli is a crucial factor that influences and enlightens the visual decoding of fMRI-to-video reconstruction. Notably, the human brain perceives videos discreetly [9, 10] due to the persistence of vision [11, 12, 13, 14, 15] and delayed memory [16]. It is impractical to perceive every video frame, and instead, only keyframes elicit significant responses in the brain\u2019s visual system. Reconstructing keyframes from fMRI avoids the issue of differential temporal resolutions between fMRI and videos. Also, the precise semantics and perceptual details in keyframes ensure the high fidelity and smoothness of reconstructed videos, both within and across successive fMRI inputs. Accordingly, we argue that utilizing keyframe images as anchors for transitional video reconstruction aligns with the brain\u2019s cognitive mechanisms and holds greater promise. ", "page_idx": 1}, {"type": "text", "text": "However, relying solely on manipulating fMRI-to-image models, e.g., incorporated with spatiotemporal conditions for successive image reconstruction, to generate keyframes, easily yields suboptimal outcomes. Research dating back to as early as 1960 [17] has shown that the fleeting sequence of images perceived by the retina is hardly discernible during the process of perception. Instead, what emerges is a phenomenal scene or its intriguing features, which can be regarded as non-detailed, low-level images. Initially, the retina captures these low-level perceptions, and subsequently, the higher central nervous system in the brain focuses on and pursues the details, generating high-level images in the cerebral cortex [18]. This process is reflected in the fMRI signal [19, 20, 21, 22, 23, 24]. The video-to-fMRI process naturally incorporates a combination of both low-level and high-level images. Therefore, the reverse fMRI-to-video reconstruction intuitively benefits from taking into account both the high-level semantic features and the low-level perceptual features. Specifically, it is necessary to decode the low-level perception flows, such as motions and dynamic scenes, from brain activity to complement keyframes, which enhances the reconstruction of high-fidelity frames and produces smooth videos. ", "page_idx": 1}, {"type": "text", "text": "In light of the above discussion, we propose a novel fMRI-to-video reconstruction framework NeuroClips that introduces two trainable components of Perception Reconstructor and Semantics Reconstructor for reconstructing low-level perception flows and keyframes, respectively. 1) Perception Reconstructor introduces Inception Extension and Temporal Upsampling modules to adaptively align fMRI with video frames, decoding low-level perception flows, i.e., a blurry video. This blurry video ensures the smoothness and consistency of subsequent video reconstruction. 2) Semantics Reconstructor adopts a diffusion prior and multiple training strategies to concentrate quantities of high-level semantics from various modalities into fMRI embeddings. These fMRI embeddings are mapped to the CLIP image space and decoded to reconstruct high-quality keyframes. 3) During inference, NeuroClips adopts a pre-trained T2V diffusion model injected with keyframes and low-level perception flows for video reconstruction with high fidelity, smoothness, and consistency. ", "page_idx": 1}, {"type": "text", "text": "Extensive experiments have validated the superior performance of NeuroClips, which is substantially ahead of SOTA baselines in pixel-level metrics and video consistency. NeuroClips achieves a 0.219 improvement $(128\\%)$ in SSIM and a 0.330 improvement $(81\\%)$ in spatiotemporal metrics and also performs better overall on most video semantic-level metrics. Meanwhile, using multi-fMRI fusion, NeuroClips pioneers the exploration of longer video reconstruction up to 6s at 8FPS. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Visual Reconstruction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "After its initial exploration [25], static image reconstruction from fMRI has witnessed remarkable success in recent years. Due to the lack of information in fMRI adapted to deep learning models, a path has been gradually explored that aligns fMRI to specific modal representations such as the common image and text modality [26, 27, 28, 29], and CLIP\u2019s [5] rich representation of space is pretty favored. Then, the aligned representations can then be fed into the diffusion model to complete the image generation. Along this path, a large body of literature demonstrates that reconstructing images at both pixel level and semantic level achieves great results [1, 2, 4, 30]. However, the field of fMRI-to-video reconstruction remains largely unexplored. Early studies [31, 32, 33] attempted to reconstruct low-level visual content from fMRI, using the embedded fMRI as conditions to guide GANs or AEs in generating multiple static images. The reconstructed videos contained little to no clear semantics recognizable by humans. Due to the excellent performance of diffusion models, MinD-Video [8] reconstructed 3FPS videos from fMRI. Despite this success, the smoothness and semantic accuracy of these videos remain unsatisfactory, leaving substantial space for improvement. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.2 Video Diffusion Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion models for image generation have gained significant attention in research communities recently [34, 35, 36]. DALLE\u00b72 [37] improved text-image generation by leveraging the CLIP [5] joint representation space. Stable Diffusion [7] enhanced generation efficiency by moving the diffusion process to the latent space of VQVAE [38]. To achieve customized generation with trained diffusion models, many works focused on adding extra condition control networks, such as ControlNet [39] and T2I-Adapter [40]. In the realm of video generation, it is typical to extend existing diffusion models with temporal modeling [41, 42, 43]. Animatediff [44] trained a plug-and-play motion module that can be seamlessly integrated into any customized image diffusion model to form a video generator. Stable Video Diffusion [45] fine-tunes pre-trained diffusion models using high-quality video datasets from multiple views to achieve powerful generation. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The overall framework of NeuroClips is illustrated in Figure 1. NeuroClips consists of three essential components: 1) Perception Reconstructor (PR) generates the blurry but continuous rough video from the perceptual level while ensuring consistency between its consecutive frames. 2) Semantics Reconstructor (SR) reconstructs the high-quality keyframe image from the semantic level. 3) Inference Process is the fMRI-to-video reconstruction process, which employs a T2V diffusion model and combines the reconstructions from PR and SR to reconstruct the final exquisite video with high fidelity, smoothness, and consistency. Furthermore, NeuroClips also pioneers the exploration of Multi-fMRI Fusion for longer video reconstruction. ", "page_idx": 2}, {"type": "image", "img_path": "8qu52Fl1Dt/tmp/567454fbbb41a8fc1f772c1be1a39f877d94c6fb7b85aaecc45f2b487bb71681.jpg", "img_caption": ["Figure 1: The overall framework of NeuroClips. The red lines represent the infernence process. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "3.1 Perception Reconstructor ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Perception reconstruction is essential not only for video reconstruction but also for semantics reconstruction. Additionally, smoothness and consistency are crucial metrics of video quality, and Perception Reconstructor (PR) plays a key role in ensuring these attributes. ", "page_idx": 2}, {"type": "text", "text": "We split a video into several clips at two-second intervals (i.e., fMRI time resolution). For each clip $c$ , we downsample it and retain part frames at fixed intervals, resulting in a set of frames $\\mathbf{X}=[\\mathcal{X}_{1},\\mathcal{X}_{2},\\cdot\\cdot\\cdot\\,,\\mathcal{X}_{N_{f}}]$ . $\\mathcal{X}_{i}$ is the $i$ -th retained frame image, with a total of $N_{f}$ retained frames. $\\mathcal{V}_{c}$ is the corresponding fMRI signal of clip $c$ . Here we introduce Inception Extension module to extend one fMRI to $N_{f}$ fMRIs, denoted as $\\mathcal{V}_{c}\\to\\mathbf{Y}$ , $\\mathbf{Y}=[\\mathcal{Y}_{1},\\mathcal{Y}_{2},\\cdot\\cdot\\cdot,\\bar{\\mathcal{Y}}_{N_{f}}]$ . ", "page_idx": 3}, {"type": "text", "text": "Sequentially applying a simple MLP and Temporal Upsampling module to obtain $\\mathbf{Y}$ \u2019s embedding set $\\mathbf{E}_{\\mathcal{Y}}\\,=\\,[e_{\\mathcal{Y}_{1}},e_{\\mathcal{Y}_{2}},\\cdot\\cdot\\cdot\\,,e_{\\mathcal{Y}_{N_{f}}}]$ , which can be fed into the Stable Diffusion [7] VAE decoder to produce a series of blurry images. We regard this sequence of blurry images as blurry video. We expect the blurry video to lack semantic content, but to exhibit state-of-the-art perceptual metrics, such as position, shape, scene, etc. Thus, we consider using frame set $\\mathbf{X}$ to align $\\mathbf{Y}$ . ", "page_idx": 3}, {"type": "text", "text": "Training Loss. Mapping $\\mathbf{X}$ to the latent space of Stable Diffusion\u2019s VAE to obtain the perception embedding set $\\mathbf{E}_{\\mathcal{X}}=[e_{\\mathcal{X}_{1}},e_{\\mathcal{X}_{2}},\\cdot\\cdot\\cdot\\,,e_{\\mathcal{X}_{N_{f}}}]$ . We adopt mean absolute error (MAE) loss and contrastive loss to train the PR, the overall loss $\\mathcal{L}_{P R}$ of PR can be described as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\mathcal{L}_{P R}=\\displaystyle\\frac{1}{N_{f}}\\sum_{i=1}^{N_{f}}|e\\chi_{i}-e y_{i}|-\\displaystyle\\frac{1}{2N_{f}}\\sum_{j=1}^{N_{f}}\\log\\displaystyle\\frac{\\exp(s i m(e x_{j},e y_{j})/\\tau)}{\\sum_{k=1}^{N_{f}}\\exp(s i m(e x_{j},e y_{k})/\\tau)}}}\\\\ {{\\displaystyle\\qquad-\\,\\frac{1}{2N_{f}}\\sum_{j=1}^{N_{f}}\\log\\displaystyle\\frac{\\exp(s i m(e y_{j},e x_{j})/\\tau)}{\\sum_{k=1}^{N_{f}}\\exp(s i m(e y_{j},e x_{k})/\\tau)},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\tau$ is a temperature hyper-parameter. The function $s i m(,)$ is used to compute the similarity. ", "page_idx": 3}, {"type": "text", "text": "Temporal Upsampling. Due to the low signal-to-noise ratio of fMRI, the direct alignment of fMRI to VAE\u2019s pixel space is highly susceptible to overftiting noise, and the learning task is too complex to guarantee the generation of decent blurry images. A common method is aligning to a coarser-grained pixel space and then upsampling to a fine-grained pixel space. The temporal relationship of the frames also needs to be considered in the video task to maintain consistency. Therefore, to achieve consistency between the retained frames, we integrated temporal attention into the upsampling operation. The fMRI embedding $\\mathbf{E}_{\\mathcal{X}}$ has five dimensions, i.e. $\\mathbf{E}_{\\mathcal{V}}\\,\\in\\,\\mathbb{R}^{b\\times N_{f}\\times c\\times h\\times\\dot{w}}$ , where $b$ denotes the batch size, $c\\times h\\times w$ is the dimension of embedding space. The upsampling operation merely models spatial relationship, receiving reshaped embedding $\\bar{\\mathbf{E}_{\\mathcal{V}}^{\\mathrm{spat}}}\\in\\mathbb{R}^{(b\\times\\bar{N}_{f})\\times\\bar{c}\\times h\\times w}$ as input. TToh emn,o dweel  tuesem lpeoarranl arbellea timoanpsphiinp go tfo $N_{f}$ mfpMuRteI , thwee  qfuiresrt yr evsahlaupe $\\mathbf{E}_{\\mathcal{D}}$ $\\mathbf{E}_{\\mathcal{V}}^{\\mathrm{temp}}\\,\\in\\,\\mathbb{R}^{(b\\times h\\times w)\\times N_{f}\\times c}$ $Q=W^{Q}\\mathbf{E}_{\\mathcal{V}}^{\\mathrm{temp}}$ $K=W^{K}\\mathbf{E}_{\\mathcal{V}}^{\\mathrm{temp}}$ . Finally, the output of temporal attention layer is $\\begin{array}{r}{\\mathbf{E}_{\\mathcal{Y}}^{\\prime}=\\mathrm{Softmax}\\bigl(\\frac{Q^{\\top}K}{\\sqrt{c}}\\bigr)\\cdot\\mathbf{E}_{\\mathcal{Y}}^{\\mathrm{temp}}}\\end{array}$ We utilize a learnable mixing coefficient $\\eta$ to conduct residual connection: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{E}_{\\mathcal{Y}}=\\eta\\cdot\\mathbf{E}_{\\mathcal{Y}}^{t e m p}+(1-\\eta)\\cdot\\mathbf{E}_{\\mathcal{Y}}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Based on the above design, PR generates the blurry rough video with initial smoothness and great consistency, laying the foundation for subsequent video reconstruction. ", "page_idx": 3}, {"type": "text", "text": "3.2 Semantics Reconstructor ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Recent cognitive neuroscience studies [46, 47] argue that \u2019key-frames\u2019 play a crucial role in how the human brain recalls and connects relevant memories with unfolding events, and other research [48, 49] also demonstrates that video key-frames can be used as representative features of the entire video clip. Building on these conclusions, the core objective of Semantics Reconstructor (SR) is to reconstruct a high-quality keyframe image that can be used to address the issue of frame rate mismatch between visual stimuli and fMRI signals, thereby enhancing the fidelity of the final exquisite video. The existing fMRI-to-image reconstruction studies [1, 2, 4] facilitate our objective, detailed below: ", "page_idx": 3}, {"type": "text", "text": "fMRI Low-dimensional Processing. For each clip $c$ , its corresponding fMRI signal is $\\mathcal{V}_{c}$ . We use ridge regression to map $\\mathcal{V}_{c}$ to a lower-dimensional ${\\mathcal{V}}_{c}^{\\prime}$ for easier follow-up: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{Y}_{c}^{\\prime}=X(X^{T}X+\\lambda I)^{-1}X^{T}\\mathcal{Y}_{c},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $X$ is design matrix, $\\lambda$ is regularization parameter, and $I$ is identity matrix. Although the human brain processes information in a highly complex and non-linear way, empirical evidence [1, 26, 2] underscores the effectiveness and sufficiency of linear mapping for achieving desirable reconstruction, due to nonlinear models will easily overfit to fMRI noise and then lead to poor performance [50]. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Alignment of Keyframe Image with fMRI. Randomly choose one frame in the clip $c$ as its keyframe $\\scriptstyle{\\mathcal{X}}_{c}$ , and use OpenCLIP ViT-bigG/14 [51] to obtain $e_{\\mathcal{X}_{c}}$ , the embedding of keyframe image $\\scriptstyle{\\mathcal{X}}_{c}$ in CLIP image space. $e_{\\mathcal{Y}_{c}}$ is the fMRI embedding of ${\\mathcal{V}}_{c}^{\\prime}$ via another MLP. Consequently, we perform contrastive learning between $e_{\\mathcal{X}_{c}}$ and $e_{\\mathcal{Y}_{c}}$ to align the keyframe image $\\scriptstyle{\\mathcal{X}}_{c}$ and the fMRI $\\mathcal{V}_{c}$ , resulting in enhancing the semantics of $e_{\\mathcal{Y}_{c}}$ . It is worth noting that the MLP gets a bidirectional contrastive loss. Previous research [1] has demonstrated that introducing MixCo [52] data augmentation, an extension of mixup utilizing the InfoNCE loss, can effectively help model convergence, especially for scarce fMRI samples. Therefore, the bidirectional loss called BiMixCo $\\mathcal{L}_{\\mathrm{BiMixCo}}$ , which combines MixCo and contrastive loss, needs to be used for training. ", "page_idx": 4}, {"type": "text", "text": "Generation of Reconstruction-Embedding. Since the embeddings in the CLIP ViT image space are more approximate to real images compared to fMRI embeddings, transforming fMRI embedding $e_{\\mathcal{D}_{c}}$ into CLIP ViT\u2019s image embedding will significantly benefit the reconstruction quality of the keyframe. Therefore, we have to generate the reconstruction-embedding $e_{\\mathcal{X}_{c}}^{r e}$ for the keyframe image $\\scriptstyle{\\mathcal{X}}_{c}$ , essentially, which is the image embedding that will be fed to the subsequent generative model for reconstruction. Inspired by DALLE\u00b72 [37], diffusion prior is an effective approach to transforming embedding. So, we map the fMRI embedding $e_{\\mathcal{Y}_{c}}$ to the OpenCLIP ViT-bigG/14 image space to generate $e_{\\mathcal{X}_{c}}^{r e}$ . Here, we use the same prior loss $\\mathcal{L}_{\\mathrm{Prior}}$ in DALLE\u00b72 [37] for training. ", "page_idx": 4}, {"type": "text", "text": "Reconstruction Enhancement from Text Modality. Original fMRI-to-image reconstruction only relies on visual modality embedding. For instance, reconstructing images conditional on the image embeddings generated by diffusion prior. However, text is another critical modality. Incorporating text with higher semantic density can help improve the semantic content of reconstruction embedding, resulting in making semantics reconstruction more straightforward and effective. We adopt BLIP2 [53] to introduce the text modality, i.e., the caption $\\mathcal{T}_{c}$ of the keyframe images $\\scriptstyle{\\mathcal{X}}_{c}$ . Then, we embed $\\mathcal{T}_{c}$ to obtain the text embedding $e\\tau_{c}$ . Inspired by contrastive learning, we perform contrastive learning between $e_{\\mathcal{X}_{c}}^{r e}$ and $e\\tau_{c}$ to enhance reconstruction-embedding $e_{\\mathcal{X}_{c}}^{r e}$ via additional text modality. The contrastive loss serves as the training loss ${\\mathcal{L}}_{\\mathrm{Reftm}}$ of this process, similar to Equation 1, omitted here. ", "page_idx": 4}, {"type": "text", "text": "Training Loss. As discussed above, the overall training loss $\\mathcal{L}_{S R}$ in SR is composite. Therefore, We set mixing coefficients $\\delta$ and $\\mu$ to balance multiple losses: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{S R}=\\mathcal{L}_{\\mathrm{BiMixCo}}+\\delta\\mathcal{L}_{\\mathrm{Prior}}+\\mu\\mathcal{L}_{\\mathrm{Reftm}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.3 Inference Process ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The inference of NeuroClips is the process of fMRI-to-video reconstruction. We jointly utilize the blurry rough video, the high-quality keyframe image, and the additional text modality, which are $\\alpha$ , $\\beta$ , and $\\gamma$ guidance, to reconstruct the final exquisite video with high fidelity, smoothness, and consistency. And we employ a text-to-video diffusion model to help reconstruct video. ", "page_idx": 4}, {"type": "text", "text": "Text-to-video Diffusion Model. Pre-training text-to-video (T2V) diffusion models possess a significant amount of prior knowledge from the graphics, image, and video domains. However, like other diffusion models, they face huge challenges in achieving controllable generation. Therefore, directly using the text corresponding to fMRI to reconstruct videos will result in unsatisfactory outcomes, as the semantics of embeddings only originate from the text modality. We also need to enhance the embeddings with semantics from the video and image modalities to produce \"composite semantics\" embeddings, which aid in achieving controllable generation for the T2V diffusion model. ", "page_idx": 4}, {"type": "text", "text": "$\\alpha$ Guidance. We consider the blurry rough-video $\\mathbf{V}_{b l u r r y}$ output from PR as $\\alpha$ Guidance. Treating $\\mathbf{V}_{b l u r r y}$ as an intermediate noisy video between target video $\\mathbf{V}_{0}$ and noise video ${\\mathbf{V}}_{T}$ , the originally required $T$ steps for the complete forward process can now be reduced to $\\vartheta T$ steps. By applying the latent space translation and reparameterization trick, noise $z_{T}$ can be formalized as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nz_{T}=\\sqrt{\\bar{\\alpha}_{T}/\\bar{\\alpha}_{\\vartheta T}}z_{b l u r r y}+\\sqrt{1-\\bar{\\alpha}_{T}/\\bar{\\alpha}_{\\vartheta T}}\\;\\epsilon,\\quad\\bar{\\alpha}_{T}=\\prod_{t=1}^{T}\\alpha_{t},\\quad\\bar{\\alpha}_{\\vartheta T}=\\prod_{t=1}^{\\vartheta T}\\alpha_{t},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\alpha_{t}$ represents the noise schedule parameter at time step $t$ and $\\mathbf{\\boldsymbol{\\epsilon}}\\sim\\mathcal{N}(\\mathbf{\\boldsymbol{0}},\\mathbf{\\boldsymbol{1}})$ is Gaussian noise.   \nThe reverse process involves iteratively denoising the noise video from $T$ steps back to 0 steps. ", "page_idx": 4}, {"type": "text", "text": "Adopting a pretrained T2V diffusion model $p_{\\theta}$ to predict the mean and variance of the denoising distribution at each step: ", "page_idx": 5}, {"type": "equation", "text": "$$\nz_{t-1}\\sim p_{\\theta}(z_{t-1}|z_{t})=\\mathcal{N}(z_{t-1};\\mu_{\\theta}(z_{t},t),\\Sigma_{\\theta}(z_{t},t)),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $t=T,T-1,...,1$ . After translating $z_{0}$ to pixel space, the reconstructed video $\\mathbf{V}_{0}$ is obtained. ", "page_idx": 5}, {"type": "text", "text": "$\\beta$ Guidance. $\\alpha$ Guidance only directs the video generation of the T2V diffusion model at the perception level, leading to significant randomness in the semantics of the reconstructed videos. To resolve this issue, we need to incorporate keyframe images with more supplementary semantics to control the generation process, thereby enhancing the fidelity of the reconstructed videos. Compared to directly reconstructing keyframe images from fMRI embeddings, combining perception embeddings will be more beneficial for maintaining the consistency of structural and semantic information. Therefore, we select the first frame $\\protect\\nu_{1}$ of blurry $\\mathbf{V}_{b l u r r y}$ . Input $\\protect\\nu_{1}$ \u2019s embedding and fMRI embedding to SDXL unCLIP [2] (See Appendix $\\mathrm{D}$ for more discussion about SDXL unCLIP) in SR to reconstruct the keyframe image $\\chi_{k e y}$ as $\\beta$ Guidance. We employ ControlNet [54] to add $\\beta$ Guidance to the T2V diffusion model, in which the keyframes are used as the first-frame to guide video generation. ", "page_idx": 5}, {"type": "text", "text": "$\\gamma$ Guidance. The text is the necessary input for the T2V diffusion model. In order to maintain the consistency of visual semantics, we adopt BLIP-2 [53] to generate the caption for the keyframe image $\\chi_{k e y}$ , which is used as $\\gamma$ Guidance (prompt) for video reconstruction. ", "page_idx": 5}, {"type": "text", "text": "The inference process inputs $\\alpha$ , $\\beta,\\gamma$ Guidance into the T2V diffusion model, and the fMRI-to-video reconstruction can be completed, resulting in the exquisite video with high fidelity and smoothness. ", "page_idx": 5}, {"type": "text", "text": "3.4 Multi-fMRI Fusion ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "While it is important to emphasize that single-frame fMRI generates higher frame rate video, the more realistic question is how to recover longer video (longer than fMRI temporal resolution). Previous methods treat single-frame fMRI as a sample, and temporal attention is computed at the single-frame fMRI level, thus failing to generate coherent videos longer than 2s. With the help of NeuroClips\u2019 SR, we explored the generation of longer videos for the first time. Current video generative models are built on diffusion-based image generation models and attention-based transformer architectures, both of which incur significant computational overhead. As the number of frames increases, the content scales linearly, highlighting the limitations in generating long and complex videos efficiently. Therefore, we chose a more straightforward fusion strategy that does not require additional GPU training. In the inference process, we consider the semantic similarity of two reconstructed keyframes from two neighboring fMRI samples (here we directly determine whether they belong to the same class of objects, e.g., both are jellyfish). Specifically, we obtain the CLIP representations of reconstructed neighboring keyframes and train a shallow MLP based on the representations to distinguish whether two frames share the same class. If semantically similar, we replace the keyframe of the latter fMRI with the tail-frame of the former fMRI\u2019s reconstructed video, which will be taken as the first-frame of the latter fMRI to generate the video. As shown in Figure 2, with this strategy, we achieved continuous video reconstruction of up to 6s for the first time. ", "page_idx": 5}, {"type": "image", "img_path": "8qu52Fl1Dt/tmp/fac51ee208dcad12f8a2a5b939c944b8e83d8216df1e37c2f47c1ea9f1a9253c.jpg", "img_caption": ["Figure 2: Visualization of Multi-fMRI fusion. With the semantic relevance measure, we can generate video clips up to 6s long without any additional training. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Dataset and Pre-processing ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Dataset. In this study, we performed fMRI-to-video reconstruction experiments using the opensource fMRI-video dataset (cc2017 dataset1) [31]. For each subject, the training and testing video clips were presented 2 and 10 times, respectively, and the testing set was averaged across trials. The dataset consists of a training set containing 18 8-minute video clips and a test set containing 5 8-minute video clips. The MRI (T1 and T2-weighted) and fMRI data (with 2s temporal resolution) were collected using a 3-T MRI system. Thus there are 8640 training samples and 1200 testing samples of fMRI-video pairs. ", "page_idx": 6}, {"type": "text", "text": "Pre-processing. The fMRI data on the $c c2017$ were preprocessed using the minimal preprocessing pipeline [55]. The fMRI volumes underwent artifact removal, motion correction (6 DOF), registration to standard space (MNI space), and were further transformed onto the cortical surfaces, which were coregistered onto a cortical surface template [56]. We calculated the voxel-wise correlation between the fMRI voxel signals of each training movie repetition for each subject. The correlation coefficient for each voxel underwent Fisher ${\\bf Z}$ -transformation, and the average z scores across 18 training movie segments were tested using the one-sample t-test. The significant voxels (Bonferroni correction, $\\mathbf{P}<$ 0.05) were considered to be stimulus-activated voxels and used for subsequent analysis A total of 13447, 14828, and 9114 activated voxels were observed in the visual cortex for the three subjects. Following the previous works [3, 57, 58], we used the BOLD signals with a delay of 4 seconds as the movie stimulus responses to account for the hemodynamic delay. ", "page_idx": 6}, {"type": "text", "text": "4.2 Implementation Details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this paper, videos from the cc2017 dataset were downsampled from 30FPS to 3FPS to make a fair comparison with the previous methods, and the blurred video was interpolated to 8FPS to generate the final 8FPS video during inference. We split semantic reconstruction into two parts: image contrastive learning and the fine-tuning of the diffusion prior to adapting to the new image distribution space. In PR, all downsampled frames were utilized, and the inception extension was implemented by a shallow MLP. Theoretically, our approach can be used in any text-to-video diffusion model, and we choose the open-source available AnimateDiff [44] as our inference model. $\\vartheta$ is set to 0.3 in $\\alpha$ Guidance, and the inference is performed with 25 DDIM [35] steps (See Appendix A for more implementation details). All experiments were conducted using a single A100 GPU. ", "page_idx": 6}, {"type": "text", "text": "4.3 Evaluation Metrics ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conduct the quantitative assessments through frame-based and video-based metrics. Frame-based metrics evaluate each frame individually, providing a snapshot evaluation, whereas video-based metrics evaluate the quality of the video, emphasizing the consistency and smoothness of the video frame sequence. Both are used for a comprehensive analysis from a semantic or pixel perspective. ", "page_idx": 6}, {"type": "text", "text": "Frame-based Metrics We evaluate frames at the pixel level and semantic level. We use the structural similarity index measure (SSIM) and Peak Signal-to-Noise Ratio (PSNR) as the pixel-level metric and the N-way top-K accuracy classification test (total 1,000 image classes from ImageNet [59]) as the semantics-level metric. To conduct the classification test, we essentially compare the ground truth (GT) classification results with the predicted frame (PF) results using an ImageNet classifier [30, 8]. We consider the trial successful if the GT class is among the top-K probabilities in the PF classification results (We used top-1), selected from N randomly chosen classes that include the GT class. The reported success rate is based on the results of 100 repeated tests. ", "page_idx": 6}, {"type": "text", "text": "Video-based Metrics We evaluate videos at the semantic level and spatiotemporal(ST)-level. For semantic-level metrics, a similar classification test (total 400 video classes from the Kinetics-400 dataset [60]) is used above, with a video classifier based on VideoMAE [61]. For spatiotemporal-level metrics that measure video consistency, we compute CLIP image embeddings on each frame of the predicted videos and report the average cosine similarity between all pairs of adjacent video frames, which is the common metric CLIP-pcc in video editing. ", "page_idx": 6}, {"type": "image", "img_path": "8qu52Fl1Dt/tmp/9b132a052f800b7c86a8d26aead6fea91da0a8e993eb236bd4dfdb35fe4eec5a.jpg", "img_caption": ["Figure 3: Video reconstruction on the cc2017 dataset. On the left are the results of the comparison with previous studies, and on the right are additional comparisons with previous SOTA methods. Best viewed with zoom-in. As shown in the leftmost figure group, Mind-Video\u2019s reconstruction fails to go for detail consistency on the character\u2019s face, but our NeuroClips achieves an extremely high consistency. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "8qu52Fl1Dt/tmp/e1772743bcd546c5fcfdb131a7c7897ee7e998046a6acd84e4eb594a73534d2c.jpg", "table_caption": ["Table 1: Quantitative comparison of NeuroClips reconstruction performance against other methods. Bold font signifies the best performance, while underlined text indicates the second-best performance. MinD-Video and NeuroClips are both results averaged across all three subjects, and the other methods are results from subject 1. Results of baselines are quoted from [62]. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we compare NeuroClips with previous video reconstruction methods on the cc2017 dataset. We provide a visual comparison in Figure 3 and report quantitative metrics in Table 1. We purposely focus on the comparison with the previous SOTA method on the right side of Figure 3 due to the lack of obvious semantic information in the premature method. All methods report results for all test sets except Wen [31], whose results are available for only one segment. Our method generates videos at 8fps and even higher frame rates. For a fair comparison with previous studies, we downsampled the 8FPS videos to 3FPS. Unless otherwise noted, the experimental results and visualizations shown below are all at 3FPS. ", "page_idx": 7}, {"type": "text", "text": "As can be seen in Figure 3, earlier methods were unable to produce videos with complete semantics but guaranteed some of the low-level visual features. Compared to MinD-Video, our NeuroClips generates single-frame images with higher quality, more precise semantics (e.g., people, turtles, and airplanes), and smoother movements. At the same time, due to the limited data in the training set, some objects in the test set videos did not appear in the training set, such as the moon, and perfect reproduction of the semantics is difficult. However, thanks to our perception reconstructor (aka $\\alpha$ Guidance), NeuroClips basically reproduces the shape and motion trajectory of the moon, although semantically it is more similar to the aperture, demonstrating the pixel-level reconstruction ability of the video. In terms of quantitative metrics, NeuroClips significantly outperformed 5 of the 7 metrics, with a $128\\%$ improvement in SSIM performance, indicating that the PR complements the lack of pixel-level control. At the semantic level, our metrics overall outperform previous methods, demonstrating the better semantic alignment paradigm of NeuroClips. For the ST-level metric, which evaluates video smoothness, NeuroClips substantially outperforms MinD-Video because we introduce blurry rough-video (aka $\\alpha$ Guidance) for the frozen video generation model, incorporating initial smoothness for video reconstruction. In contrast, MinD-Video lacks perception control, resulting in a substantial decrease in smoothness, as can also be seen in Figure 3, where the human deformations and scene switches are too large within an fMRI frame. In addition, benefiting from our keyframes for first-frame guidance (aka $\\beta$ Guidance) and blurry videos, we can connect semantically similar videos to generate longer videos, which may be the reason for the slightly lower video 2-way metrics, as neighboring reconstructed videos are more similar after multi-fMRI fusion. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6 Ablations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we discuss the impact of three important intermediate processes on video reconstruction, including keyframes, blurry videos, and keyframe captioning. The quantitative results are in Table 2 and the visualization results are shown in Figure 4, where all the results of the ablation experiments are from subject 1. Since the keyframe captioning must be obtained through the keyframe, by default we eliminate keyframe and keyframe captioning at the same time. From the quantitative results, it can be seen that NeuroClips exhibits better pixel-level metrics without keyframes while showing better semantic-level metrics without blurry clips. This indicates a trade-off between semantic and perception reconstruction, which is similar to the results of a large body of literature on fMRI-to-image reconstruction [1]. The perception recon- Figure 4: Visualization of ablation study. struction improves the pixel-level performance and the semantic reconstruction improves the semantic metrics. In addition to this, the best ST-level results for the full model demonstrate the contribution of each module to video consistency. ", "page_idx": 8}, {"type": "image", "img_path": "8qu52Fl1Dt/tmp/52ac12670f8ace871ecfe1fef180300b91a4d688b44bf8022793e758a065c4b8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "8qu52Fl1Dt/tmp/9e9edf915bb484a84e40a8453be7ef3e0770142d8d92e4f11d0476edeaa9639d.jpg", "table_caption": ["Table 2: Ablations on the modules of NeuroClips, and all results are from subject 1. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "For the image captioning method, previous studies [2] have used the GIT [63] model to generate keyframe captions directly from fMRI embeddings in image space, and we generated it from BLIP2 [53]. Here we compare the text generation results of these two approaches as shown in the Figure 4. We found that GIT\u2019s text generation is slightly homogeneous, filled with a large number of similar descriptions, such as \u2019a large body of water\u2019, \u2019a man is standing\u2019. For the diffusion model, the influence of text is significant, so GIT\u2019s captions degrade the quality of the semantic reconstruction of the video, e.g., generating flowers on water from jellyfish keyframes. This shows that keyframe-based captioning is more flexible compared to representation-based captioning. Finally, we remove the keyframes and keyframe captions and use only blurred videos to guide the reconstruction, with the text input replaced with the generic description \u2019a smooth video\u2019. With this approach, we find that the model generates the video completely blindly, with poor semantic control, demonstrating the strong semantic support that keyframes bring to the NeuroClips generation capability. More ablation analysis can be found in Appendix C.4. ", "page_idx": 8}, {"type": "text", "text": "7 Interpretation Results ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To explore the neural interpretability of the model, we visualized voxel-level weights on a brain flat map, where we can see comprehensive structural attention throughout the whole region, as shown in Figure 5. It can be seen that the visual cortex occupies an important position regardless of the task. In addition, for semantic-level reconstruction, the weight distribution of voxels is more spread out over the higher visual cortex, indicating the semantic level of the video. For perceptual reconstruction, the weight distribution of voxels is more concentrated on the lower visual cortex, corresponding to the low-level perception of the human brain. See Appendix C.5 for more subjects\u2019 interpretation results. ", "page_idx": 9}, {"type": "image", "img_path": "8qu52Fl1Dt/tmp/79a5cfc82eea7a78433a3452d03b2dcf5b0b1771c5ba2e7e6c02f6fe0e26d614.jpg", "img_caption": ["Figure 5: Visualization of voxel weights for the first ridge regression layer for subject 1, with each voxel\u2019s weight averaged and normalized to between 0 and 1 and we set the colorbar to 0.25-0.75 for a clear comparison. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we present NeuroClips, a novel framework for fMRI-to-video reconstruction. we implement pixel-level and semantic-level visual learning of fMRI through two paths: perception reconstruction and semantic reconstruction. With the learned components, we can configure them into the latest video diffusion models to generate higher quality, higher frame rate, and longer videos without additional training. NeuroClips recovers videos with more accurate semantic-level precision and degree of pixel-level matching, establishing a new state-of-the-art in this domain. In addition to this, we visualized the neuroscience interpretability of NeuroClips with reliable biological principles. ", "page_idx": 9}, {"type": "text", "text": "9 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Although NeuroClips has achieved high-fidelity, smooth, and consistent multi-fMRI to video reconstruction, there are still slight flaws. Specifically, our framework is slightly bulky and it relies on extending keyframes to the reconstructed video. A model that can reconstruct videos from the CLIP latent space will avoid this intermediate process. Unfortunately, there is no such available model now. In addition, our method does not reconstruct cross-scene fMRI well, i.e., fMRI recorded during video clip switching. Even if such fMRI scans are in a tiny minority, this will be a direction for future research. Moreover, additional subjects and fMRI recordings should be considered in order to reflect real-world visual experiences sufficiently. However, The alleviation of these limitations will require joint advances in multiple areas and significant further effort will be required. This is because improvements in these areas need to be supported by common developments including machine learning, computer vision, brain science, and biomedicine. ", "page_idx": 9}, {"type": "text", "text": "Ethics Statements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The dataset paper [31] states that informed written consent was obtained from every study participant according to the research protocol approved by the Institutional Review Board at Purdue University. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research is supported by the National Key Research and Development Program of China (No. 2022YFB3104700), the National Natural Science Foundation of China (No. 61976158, No. 62376198), Shanghai Baiyulan Pujiang Project (No. 08002360429). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Paul Scotti, Atmadeep Banerjee, Jimmie Goode, Stepan Shabalin, Alex Nguyen, Aidan Dempster, Nathalie Verlinde, Elad Yundler, David Weisberg, Kenneth Norman, et al. Reconstructing the mind\u2019s eye: fmri-to-image with contrastive learning and diffusion priors. Advances in Neural Information Processing Systems, 36, 2024.   \n[2] Paul S Scotti, Mihir Tripathy, Cesar Kadir Torrico Villanueva, Reese Kneeland, Tong Chen, Ashutosh Narang, Charan Santhirasegaran, Jonathan Xu, Thomas Naselaris, Kenneth A Norman, et al. Mindeye2: Shared-subject models enable fmri-to-image with 1 hour of data. arXiv preprint arXiv:2403.11207, 2024.   \n[3] Shinji Nishimoto, An T Vu, Thomas Naselaris, Yuval Benjamini, Bin Yu, and Jack L Gallant. Reconstructing visual experiences from brain activity evoked by natural movies. Current biology, 21(19):1641\u20131646, 2011.   \n[4] Zixuan Gong, Qi Zhang, Guangyin Bao, Lei Zhu, Ke Liu, Liang Hu, and Duoqian Miao. Mindtuner: Cross-subject visual decoding with visual fingerprint and semantic correction. arXiv preprint arXiv:2404.12630, 2024.   \n[5] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[6] Yu Zhang, Qi Zhang, Zixuan Gong, Yiwei Shi, Yepeng Liu, Duoqian Miao, Yang Liu, Ke Liu, Kun Yi, Wei Fan, et al. Mlip: Efficient multi-perspective language-image pretraining with exhaustive data utilization. arXiv preprint arXiv:2406.01460, 2024.   \n[7] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[8] Zijiao Chen, Jiaxin Qing, and Juan Helen Zhou. Cinematic mindscapes: High-quality video reconstruction from brain activity. Advances in Neural Information Processing Systems, 36, 2024.   \n[9] Simon Scholler, Sebastian Bosse, Matthias Sebastian Treder, Benjamin Blankertz, Gabriel Curio, Klaus-Robert Muller, and Thomas Wiegand. Toward a direct measure of video quality perception using eeg. IEEE transactions on Image Processing, 21(5):2619\u20132629, 2012.   \n[10] EVE RABINOFF. Human Perception, pages 43\u201370. Northwestern University Press, 2018.   \n[11] Ervin S Ferry. Persistence of vision. American Journal of Science, 3(261):192\u2013207, 1892.   \n[12] Lin Zhu, Siwei Dong, Jianing Li, Tiejun Huang, and Yonghong Tian. Ultra-high temporal resolution visual reconstruction from a fovea-like spike camera via spiking neuron model. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(1):1233\u20131249, 2022.   \n[13] Guangyi Chen, Yifan Shen, Zhenhao Chen, Xiangchen Song, Yuewen Sun, Weiran Yao, Xiao Liu, and Kun Zhang. Caring: Learning temporal causal representation under non-invertible generation process. arXiv preprint arXiv:2401.14535, 2024.   \n[14] Aditya Patel, Ashish Kumar Khandual, Kavya Kumar, and Shreya Kumar. Persistence of vision display-a review. IOSR Journal of Electrical and Electronics Engineering (IOSR-JEEE) $e$ -ISSN, 10(4):36\u201340, 2015.   \n[15] Andreas Br\u00f8gger and Karen Newman. Persistence of vision: the interplay of vision. Vision, Memory and Media, page 13, 2010.   \n[16] Leslie G Ungerleider, Susan M Courtney, and James V Haxby. A neural system for human visual working memory. Proceedings of the National Academy of Sciences, 95(3):883\u2013890, 1998. ", "page_idx": 10}, {"type": "text", "text": "[17] James J. Gibson. Pictures, perspective, and perception. Daedalus, 89(1):216\u2013227, 1960. ", "page_idx": 11}, {"type": "text", "text": "[18] Jonathan Vacher, Aida Davila, Adam Kohn, and Ruben Coen-Cagli. Texture interpolation for probing visual perception. Advances in neural information processing systems, 33:22146\u201322157, 2020.   \n[19] Changde Du, Changying Du, Lijie Huang, and Huiguang He. Reconstructing perceived images from human brain activities with bayesian deep multiview learning. IEEE transactions on neural networks and learning systems, 30(8):2310\u20132323, 2018.   \n[20] Mo Chen, Junwei Han, Xintao Hu, Xi Jiang, Lei Guo, and Tianming Liu. Survey of encoding and decoding of visual stimulus via fmri: an image analysis perspective. Brain imaging and behavior, 8:7\u201323, 2014.   \n[21] Kendrick N Kay, Jonathan Winawer, Aviv Mezer, and Brian A Wandell. Compressive spatial summation in human visual cortex. Journal of neurophysiology, 110(2):481\u2013494, 2013.   \n[22] Panagiotis Sapountzis, Denis Schluppeck, Richard Bowtell, and Jonathan W Peirce. A comparison of fmri adaptation and multivariate pattern classification analysis in visual cortex. Neuroimage, 49(2):1632\u20131640, 2010.   \n[23] Kun Wang, Tianzi Jiang, Chunshui Yu, Lixia Tian, Jun Li, Yong Liu, Yuan Zhou, Lijuan Xu, Ming Song, and Kuncheng Li. Spontaneous activity associated with primary visual cortex: a resting-state fmri study. Cerebral cortex, 18(3):697\u2013704, 2008.   \n[24] Kalanit Grill-Spector and Rafael Malach. The human visual cortex. Annu. Rev. Neurosci., 27(1):649\u2013677, 2004.   \n[25] Tomoyasu Horikawa and Yukiyasu Kamitani. Generic decoding of seen and imagined objects using hierarchical visual features. Nature communications, 8(1):15037, 2017.   \n[26] Yu Takagi and Shinji Nishimoto. High-resolution image reconstruction with latent diffusion models from human brain activity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14453\u201314463, 2023.   \n[27] Weijian Mai and Zhijun Zhang. Unibrain: Unify image reconstruction and captioning all in one diffusion model from human brain activity. arXiv preprint arXiv:2308.07428, 2023.   \n[28] Zixuan Gong, Qi Zhang, Guangyin Bao, Lei Zhu, Yu Zhang, KE LIU, Liang Hu, and Duoqian Miao. Lite-mind: Towards efficient and robust brain representation learning. In ACM Multimedia 2024, 2024.   \n[29] Guangyin Bao, Zixuan Gong, Qi Zhang, Jialei Zhou, Wei Fan, Kun Yi, Usman Naseem, Liang Hu, and Duoqian Miao. Wills aligner: A robust multi-subject brain representation learner. arXiv preprint arXiv:2404.13282, 2024.   \n[30] Zijiao Chen, Jiaxin Qing, Tiange Xiang, Wan Lin Yue, and Juan Helen Zhou. Seeing beyond the brain: Conditional diffusion model with sparse masked modeling for vision decoding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22710\u201322720, 2023.   \n[31] Haiguang Wen, Junxing Shi, Yizhen Zhang, Kun-Han Lu, Jiayue Cao, and Zhongming Liu. Neural encoding and decoding with deep learning for dynamic natural vision. Cerebral cortex, 28(12):4136\u20134160, 2018.   \n[32] Chong Wang, Hongmei Yan, Wei Huang, Jiyi Li, Yuting Wang, Yun-Shuang Fan, Wei Sheng, Tao Liu, Rong Li, and Huafu Chen. Reconstructing rapid natural vision with fmri-conditional video generative adversarial network. Cerebral Cortex, 32(20):4502\u20134511, 2022.   \n[33] Ganit Kupershmidt, Roman Beliy, Guy Gaziv, and Michal Irani. A penny for your (visual) thoughts: Self-supervised reconstruction of natural movies from brain activity. arXiv preprint arXiv:2206.03544, 2022.   \n[34] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[35] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.   \n[36] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. Diffusion models in vision: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.   \n[37] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.   \n[38] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017.   \n[39] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836\u20133847, 2023.   \n[40] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 4296\u20134304, 2024.   \n[41] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22563\u201322575, 2023.   \n[42] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022.   \n[43] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022.   \n[44] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023.   \n[45] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023.   \n[46] Janne Kauttonen, Yevhen Hlushchuk, Iiro P J\u00e4\u00e4skel\u00e4inen, and Pia Tikka. Brain mechanisms underlying cue-based memorizing during free viewing of movie memento. NeuroImage, 172:313\u2013 325, 2018.   \n[47] Junwei Han, Kaiming Li, Ling Shao, Xintao Hu, Sheng He, Lei Guo, Jungong Han, and Tianming Liu. Video abstraction based on fmri-driven visual attention model. Information sciences, 281:781\u2013796, 2014.   \n[48] Xintao Hu, Kaiming Li, Junwei Han, Xiansheng Hua, Lei Guo, and Tianming Liu. Bridging the semantic gap via functional brain imaging. IEEE Transactions on Multimedia, 14(2):314\u2013325, 2011.   \n[49] Pulkit Narwal, Neelam Duhan, and Komal Kumar Bhatia. A comprehensive survey and mathematical insights towards video summarization. Journal of Visual Communication and Image Representation, 89:103670, 2022.   \n[50] Matteo Ferrante, Tommaso Boccato, Furkan Ozcelik, Rufin VanRullen, and Nicola Toschi. Through their eyes: multi-subject brain decoding with simple alignment techniques. Imaging Neuroscience, 2:1\u201321, 2024.   \n[51] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278\u201325294, 2022.   \n[52] Sungnyun Kim, Gihun Lee, Sangmin Bae, and Se-Young Yun. Mixco: Mix-up contrastive learning for visual representation. arXiv preprint arXiv:2010.06300, 2020.   \n[53] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730\u201319742. PMLR, 2023.   \n[54] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 3836\u20133847, October 2023.   \n[55] Matthew F Glasser, Stamatios N Sotiropoulos, J Anthony Wilson, Timothy S Coalson, Bruce Fischl, Jesper L Andersson, Junqian Xu, Saad Jbabdi, Matthew Webster, Jonathan R Polimeni, et al. The minimal preprocessing pipelines for the human connectome project. Neuroimage, 80:105\u2013124, 2013.   \n[56] Matthew F Glasser, Timothy S Coalson, Emma C Robinson, Carl D Hacker, John Harwell, Essa Yacoub, Kamil Ugurbil, Jesper Andersson, Christian F Beckmann, Mark Jenkinson, et al. A multi-modal parcellation of human cerebral cortex. Nature, 536(7615):171\u2013178, 2016.   \n[57] Kuan Han, Haiguang Wen, Junxing Shi, Kun-Han Lu, Yizhen Zhang, Di Fu, and Zhongming Liu. Variational autoencoder: An unsupervised model for encoding and decoding fmri activity in visual cortex. NeuroImage, 198:125\u2013136, 2019.   \n[58] Chong Wang, Hongmei Yan, Wei Huang, Jiyi Li, Yuting Wang, Yun-Shuang Fan, Wei Sheng, Tao Liu, Rong Li, and Huafu Chen. Reconstructing rapid natural vision with fmri-conditional video generative adversarial network. Cerebral Cortex, 32(20):4502\u20134511, 2022.   \n[59] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[60] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017.   \n[61] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. Advances in neural information processing systems, 35:10078\u201310093, 2022.   \n[62] Yizhuo Lu, Changde Du, Chong Wang, Xuanliu Zhu, Liuyun Jiang, and Huiguang He. Animate your thoughts: Decoupled reconstruction of dynamic natural vision from slow brain activity. arXiv preprint arXiv:2405.03280, 2024.   \n[63] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022.   \n[64] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[65] Leslie N Smith and Nicholay Topin. Super-convergence: Very fast training of neural networks using large learning rates. In Artificial intelligence and machine learning for multi-domain operations applications, volume 11006, pages 369\u2013386. SPIE, 2019.   \n[66] Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Sparsectrl: Adding sparse controls to text-to-video diffusion models. arXiv preprint arXiv:2311.16933, 2023. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A More Implementation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Training Details. For Semantic Reconstructor, We first train the fMRI-to-keyframe alignment for 30 epochs with a batch size of 240 and then tune the Diffusion Prior for 150 epochs with a batch size of 64. For Perceptual Reconstructor, we train it for 150 epochs and the batch size is set to 40. We use the AdamW [64] for optimization, with a learning rate set to 3e-4, to which the OneCircle learning rate schedule [65] was set. Mixing coefficients $\\delta$ and $\\mu$ are set to 30 and 1. ", "page_idx": 14}, {"type": "text", "text": "Inference Process. In the phase of inference, we use AnimateDiff v3 [44] with Stable Diffusion v1.5 based motion module. The scene model is selected as RealisticVisionV60, and keyframe control is implemented by RGB condition with SparseCtrl [66]. For the video diffusion model, the text prompt is set to the keyframe captioning $+\\ ;\\mathit{\\Omega}_{}\\mathit{\\mathcal{S}}k$ uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3\u2019 and the negative prompt is fixed to \u2019semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck\u2019. ", "page_idx": 14}, {"type": "text", "text": "Visualization Software. We use the Connectome Workbench from Human Connectome Project (HCP), and flatmap templates were selected as \u2019Q1-Q6_R440.L.flat.32k_fs_LR.surf.gii\u2019 and Q1- Q6_R440.R.flat.32k_fs_LR.surf.gii\u2019. Additionally, the cortical parcellation was manually delineated by neuroimaging specialists and neurologists, and aligned with the public templates in FreeSurfer software with verification. We normalized the voxel weights, scaling them to between 0 and 1. Finally, to show a better comparison, the Colorbar was chosen to be 0.25-0.75. ", "page_idx": 14}, {"type": "text", "text": "B More Details about Method ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 More Details about Perception Reconstructor ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We elaborate on the architecture of our Temporal Upsampling in Sec 3.1 here. As illustrated in Figure 6, the Temporal Upsampling module consists of Spatial Layer, Temporal Attention, Learnable Residual Connection, and Upsampling. ", "page_idx": 14}, {"type": "text", "text": "The input embedding $\\mathbf{E}_{\\mathcal{Y}}$ of Temporal Upsampling has five dimensions, i.e. $\\mathbf{E}_{\\mathcal{V}}\\in\\mathbb{R}^{\\check{b}\\times N_{f}\\times c\\times h^{\\ast}\\times w}$ (To express concisely, we have drawn only its $N_{f}$ , $h$ , and $w$ in Figure 6.) Before feeding $\\mathbf{E}_{\\mathcal{X}}$ into the spatial layer, we reshape it to $\\mathbf{E}_{\\mathcal{V}}^{s p a t}\\in\\mathbb{R}^{(b\\times N_{f})\\times c\\times h\\times w}$ First, we conduct modeling in spatial level, using 3D convolution and spatial attention, formalized as ${\\bf E}_{y}^{'}=\\mathrm{Spatial}({\\bf E}_{y}^{s p a t})$ . Note that the spatial layer maintains consistent input and output dimensions so that the dimensions of $\\mathbf{E}_{\\mathcal{X}}^{'}$ are exactly the same as those of ${\\bf{E}}_{\\mathcal{Y}}^{s p a t}$ . Then, applying the learnable residual connection: $\\mathbf{E}_{\\mathcal{Y}}\\leftarrow\\eta_{1}\\cdot\\mathbf{\\dot{E}}_{\\mathcal{Y}}^{s p a t}+(1-\\eta_{1})\\mathbf{E}_{\\mathcal{Y}}^{'}$ . Subsequently, we reshape $\\mathbf{E}_{\\mathcal{X}}$ as $\\mathbf{E}_{\\mathcal{V}}^{t e m p}\\in\\mathbb{R}^{(b\\times h\\times w)\\times N_{f}\\times c}$ and input $\\mathbf{E}_{\\mathcal{Y}}^{t e m p}$ to the temporal layer Y $\\vec{\\mathbf{E}_{\\mathcal{V}}^{'\\prime}}\\,=\\,\\mathrm{Temporal}(\\mathbf{E}_{\\mathcal{V}}^{t e m p})$ , which is achieved using Temporal Attention as mentioned in main text. Similarly, we apply the learnable residual connection again: $\\mathbf{E}_{\\mathcal{V}}\\leftarrow\\eta_{2}\\cdot\\dot{\\mathbf{E}_{\\mathcal{V}}^{t e m p}}+(1-\\eta_{2})\\mathbf{E}_{\\mathcal{V}}^{'\\prime}$ . Finally, we conduct upsampling to the result $\\mathbf{E}_{\\mathcal{X}}$ , formalized as EY \u2190Upsampling(EY) \u2208Rb\u00d7Nf \u00d7c\u2032\u00d7h\u2032\u00d7w\u2032. ", "page_idx": 14}, {"type": "text", "text": "We repeat employing the above module in different $(c,h,w)$ until $\\mathbf{E}_{\\mathcal{X}}$ is upsampled to the target dimensions. ", "page_idx": 14}, {"type": "image", "img_path": "8qu52Fl1Dt/tmp/0ac75b1824b6d846152065942e43661c8c134607165a0ace277eb51a7e3448d8.jpg", "img_caption": ["Figure 6: The detailed architecture of Temporal Upsampling module. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "B.2 More Details in Semantic Reconstruction ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Here we elaborate on the three loss functions in Equation 4. ", "page_idx": 14}, {"type": "text", "text": "BiMixCo Loss. BiMixCo aligns the keyframe $\\scriptstyle{\\mathcal{X}}_{c}$ and its corresponding fMRI signal $\\mathcal{V}_{c}$ using bidirectional contrastive loss and MixCo data augmentation. The MixCo needs to mix two independent fMRI signals. For each $\\mathcal{V}_{c}$ , we random sample another fMRI $\\mathcal{V}_{m_{c}}$ , which is the keyframe of the clip index by $m_{c}$ . Then, we mix $\\mathcal{V}_{c}$ and $\\mathcal{V}_{m_{c}}$ using a linear combination: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{Y}_{c}^{*}=m i x(\\mathcal{Y}_{c},\\mathcal{Y}_{m_{c}})=\\lambda_{c}\\cdot\\mathcal{Y}_{c}+(1-\\lambda_{c})\\mathcal{Y}_{m_{c}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $y_{c}^{*}$ denotes mixed fMRI signal and $\\lambda_{c}$ is a hyper-parameter sampled from Beta distribution. Then, we adapt the ridge regression to map $y_{c}^{*}$ to a lower-dimensional ${\\mathfrak{V}_{c}^{*^{\\prime}}}$ and obtain the embedding $e_{\\mathcal{D}_{c}^{\\ast}}$ via the MLP, i.e. $e y_{c}^{*}=\\mathcal{E}({y_{c}^{*}}^{\\prime})$ . Based on this, the BiMixCo loss can be formed as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{BiMixCo}}=-\\frac{1}{2N_{f}}\\underset{i=1}{\\overset{N_{f}}{\\sum}}\\lambda_{i}\\cdot\\log\\frac{\\exp(s i m(e_{y_{i}^{*}},e_{X_{i}})/\\tau)}{\\sum_{k=1}^{N_{f}}\\exp(s i m(e_{y_{i}^{*}},e_{X_{k}})/\\tau)}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ -\\frac{1}{2N_{f}}\\underset{i=1}{\\overset{N_{f}}{\\sum}}(1-\\lambda_{i})\\cdot\\log\\frac{\\exp(s i m(e_{y_{i}^{*}},e_{X_{m_{i}}})/\\tau)}{\\sum_{k=1}^{N_{f}}\\exp(s i m(e_{y_{i}^{*}},e_{X_{k}})/\\tau)}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ -\\frac{1}{2N_{f}}\\underset{j=1}{\\overset{N_{f}}{\\sum}}\\lambda_{j}\\cdot\\log\\frac{\\exp(s i m(e_{y_{j}^{*}},e_{X_{j}})/\\tau)}{\\sum_{k=1}^{N_{f}}\\exp(s i m(e_{y_{k}^{*}},e_{X_{j}})/\\tau)}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ -\\frac{1}{2N_{f}}\\underset{j=1}{\\overset{N_{f}}{\\sum}}\\underset{\\{l=m_{i}=j\\}}{\\sum}(1-\\lambda_{j})\\cdot\\log\\frac{\\exp(s i m(e_{y_{i}^{*}},e_{X_{j}})/\\tau)}{\\sum_{k=1}^{N_{f}}\\exp(s i m(e_{y_{i}^{*}},e_{X_{j}})/\\tau)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $e_{\\mathcal{X}_{c}}$ denotes the OpenCLIP embedding for keyframe $\\scriptstyle{\\mathcal{X}}_{c}$ . ", "page_idx": 15}, {"type": "text", "text": "Prior Loss. We use the Diffusion Prior to transform fMRI embedding $e_{\\mathcal{D}_{c}}$ into the reconstructed OpenCLIP embedding of keyframe $e_{\\mathcal{X}_{c}}^{r e}$ . Similar to DALLE\u00b72, Diffusion Prior predicts the target embeddings with mean-squared error (MSE) as the supervised objective: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{Prior}}=\\mathbb{E}_{e_{X_{c}},e_{y_{c}},\\epsilon\\sim\\mathcal{N}(0,1)}||\\epsilon(e_{\\mathcal{Y}_{c}})-e_{\\mathcal{X}_{c}}||.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Reftm Loss. In addition to image representation-level alignment, the assistance of text can aid in generating semantically more compatible images. Considering the inconsistency in the number and dimension of image and text tokens, the previous approaches mostly align the 257 tokens of the image with the CLS token of the text after globally averaging the pooling projection. In this paper, we ignore the CLS token and only map fMRI to 256 tokens of images, so the projection layer of the alignment needs to be adjusted. We develop the Reftm to achieve the alignment between the reconstruction-embedding erXec and its corresponding text embedding $e\\tau_{c}$ (the text $\\mathcal{T}_{c}$ is generated by BLIP2 captioning according to the GT keyframe). To align the dimension of reconstructionembedding $e_{\\mathcal{X}_{c}}^{r e}$ and that of text embedding $e\\tau_{c}$ , we add an adjusted projector $\\mathcal{P}(\\cdot)$ . The adjusted projector directly maps 256 tokens of $e_{\\mathcal{X}_{c}}^{r e}$ to the dimension of text embedding. We fine-tuned the projector for 20 epochs on the MSCOCO dataset, which consists of $73\\mathbf{k}$ images and 5 texts in each image. And in the training phase of NeuroClips, the projector was frozen. Table 3 displays the effect of fine-tuning. ", "page_idx": 15}, {"type": "table", "img_path": "8qu52Fl1Dt/tmp/3f2250a944ded7b225be002051dc7e8e1aef8716fd5af5718a9020a945326fe9.jpg", "table_caption": ["Table 3: The effect of fine-tuning on the MSCOCO 2017. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Finally, for our Semantic Reconstruction, we use CLIP contrastive loss to align reconstructionembedding $e_{\\mathcal{X}_{c}}^{r e}$ and its corresponding text embedding $e_{T_{c}}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{Reftm}}=-\\frac{1}{2N_{f}}\\sum_{i=1}^{N_{f}}\\left[\\log\\frac{\\exp(s i m(\\mathcal{P}(e_{\\mathcal{X}_{i}}^{r c}),e_{\\mathcal{T}_{i}})/\\tau)}{\\sum_{k=1}^{N_{f}}\\exp(s i m(\\mathcal{P}(e_{\\mathcal{X}_{i}}^{r e}),e_{\\mathcal{T}_{k}})/\\tau)}+\\log\\frac{\\exp(s i m(\\mathcal{P}(e_{\\mathcal{X}_{i}}^{r c}),e_{\\mathcal{T}_{i}})/\\tau)}{\\sum_{k=1}^{N_{f}}\\exp(s i m(\\mathcal{P}(e_{\\mathcal{X}_{k}}^{r e}),e_{\\mathcal{T}_{i}})/\\tau)}\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "C More Experimental Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Visualizing Reconstructed Keyframe and Text ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We display the reconstructed keyframes and their corresponding text in Figure 7. It can be observed that the reconstructed keyframes and ground truth exhibit an extremely high semantic similarity. These results demonstrate the ability of our Semantic Reconstructor to reconstruct semantic-accuracy keyframes and corresponding text descriptions. This reconstructed keyframe contains not only the correct object categories but also detailed information such as object position, color, scene structure, etc., which is crucial for reconstructing high-fidelity videos. ", "page_idx": 16}, {"type": "image", "img_path": "8qu52Fl1Dt/tmp/0f2def7f60ea64128c353fc771dae5d0a8a1009ee7654827c209efe8cfcaaa3b.jpg", "img_caption": ["Figure 7: The reconstructed keyframe and its corresponding text. Best viewed with zoom in. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "C.2 Visualizing More Successful Reconstructions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We visualized more successfully reconstructed videos in Figure 8. As it can be seen, our NeuroClips is capable of successfully reconstructing videos with correct semantics, covering various categories such as portraits, objects, animals, and natural scenes, demonstrating the effectiveness of our Semantic Reconstructor. By conducting detailed comparisons with the ground truth visual stimulus, we find that the structural information and motions in the videos could also be reconstructed, such as the size of the main objects, their positions within the scene and the direction in which a person walks. This demonstrates the critical addition of blurred video to motion and structural information. ", "page_idx": 16}, {"type": "text", "text": "We further show videos that are successfully reconstructed by all three subjects in Figure 9, which demonstrated generalization of the model. What\u2019s more, we find that these videos often consist of simple backgrounds featuring portraits, animals, or objects, as well as some natural scenes. By comparing the reconstruction results of the three subjects, we discover an interesting phenomenon: the perceived size of objects in simple scenes varies between individuals. For example, the sizes of the jellyfish and airplane reconstructed from different subjects\u2019 fMRI show significant differences. These results also suggest, to some extent, the differences in the human visual system among individuals. ", "page_idx": 16}, {"type": "text", "text": "C.3 Visualizing Incorrect Reconstructions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Incorrect results often provide more insights. Therefore, we present some of the incorrect reconstruction results generated by NeuroClips in Figure 10. Most incorrect reconstruction clips arise from semantic errors, leading to confusion between semantically similar items, such as fish and turtle, man and woman, and cat and dog. This type of error may arise from two main reasons. On the one hand, the semantic accuracy of the keyframes we reconstructed may be insufficient. We attempt to use fMRI embedding to retrieve from a pool of keyframe image representations. When the size of the retrieval pool is set to 300, the retrieval accuracy on the test set is approximately 0.22, indicating that there is still significant improvement room for visual semantics encapsulated by the fMRI embedding. On the other hand, the test set may encounter out-of-distribution issues relative to the training set, where semantic categories present in the test set are not seen in the training set, making semantic reconstruction challenging to generalize on the test set. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "image", "img_path": "8qu52Fl1Dt/tmp/f3a36622b110d75a3d9b27e04e13f3c7333b4a16cd23049b348ab46f20b310f9.jpg", "img_caption": ["Figure 8: Visualization of more successful reconstruction results. We displayed 6 frames from each 16-frame video generated by fMRI. Best viewed with zoom in. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "8qu52Fl1Dt/tmp/e4d9145e39d17c35d9f9d16266b3038b5bee7784ed1c1da576c612029ec5e1cb.jpg", "img_caption": ["Figure 9: Visualization of successful reconstruction results by three subjects. We displayed 6 frames from each 16-frame video generated by fMRI. Best viewed with zoom in. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "8qu52Fl1Dt/tmp/aa4677fb18470214482c716816559f8a40b0a846437239d3d7deb11e46077674.jpg", "img_caption": ["Figure 10: Visualization of some incorrect reconstruction results. We displayed 6 frames from each 16-frame video generated by fMRI. Best viewed with zoom in. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "C.4 More Ablation Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The first frame of the blurry video provides structural information for the reconstruction of the keyframes and motion information for the generation of the video. Here, we perform the ablation visualization of the blurry video to observe its role in perception reconstruction. Although the building can still be generated with the blurry video removed as shown in Figure 11, the shape difference from the original image is too large, indicating the structural information provided by the blur video. In addition, with the addition of the blurred video guidance, the camera view is progressively upward, similar to the original image, but with the removal of the blurred video, the camera view is encircled, suggesting that the perception reconstruction provides motion guidance. ", "page_idx": 19}, {"type": "image", "img_path": "8qu52Fl1Dt/tmp/846969efa5c171f6c9939e7cda6de21595670234690e65fcac9a1701ac020998.jpg", "img_caption": ["Figure 11: Visualization of ablation study. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "C.5 More Interpretation Results ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "8qu52Fl1Dt/tmp/68149f4a5ec78e15ab5a88eebd6e87c1372431c596c8b0fdd2490bfb379173c9.jpg", "img_caption": ["Figure 12: Additional visualization of voxel-wise weights. Best viewed with zoom in. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "In Section 7, we visualized voxel-level weights on a brain flat map for subject 1. Here we provide detailed visualization results for all three subjects from the cc2017 dataset, as shown in Figure 12. As can be seen from the Figure, the three subjects learned similar weights in the Perception Reconstructor, which echoes the subjects\u2019 comparable SSIM metrics in Table 1 and demonstrates the existence of commonality in low-level vision in humans. However, the weights learned by subject 3 in the Semantic Reconstructor are different from the other subjects, which also leads to its image retrieval accuracy being at a lower value, as shown in Table 4. This may indicate that there were differences in the understanding of the video between subjects. ", "page_idx": 19}, {"type": "text", "text": "C.6 Results of Retrieval Metrics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We further evaluate the performance of our Semantic Reconstructor using Top-1 keyframe retrieval accuracy and Top-1 fMRI retrieval accuracy. For keyframe retrieval, each test fMRI $\\mathcal{V}_{c}$ is first converted to an fMRI embedding $e_{\\mathcal{D}_{c}}$ , and we compute the cosine similarity to its respective CLIP keyframe embedding $e_{\\mathcal{X}_{c}}$ and 299 other randomly selected CLIP keyframe embedding in the test set. For each test sample, success is determined if the cosine similarity is greatest between the fMRI embedding and its respective CLIP keyframe embedding (aka top-1 retrieval performance, random chance is 1/300). The test set contains 1,200 fMRI-keyframe pairs, and we randomly divide it into 4 parts (thus each part contains 300 test pairs) for retrieval evaluation. We report the average retrieval accuracy among 4 parts. The same procedure is used for fMRI retrieval, except fMRI and keyframe are flipped. Table 4 displays the retrieval performance. ", "page_idx": 20}, {"type": "table", "img_path": "8qu52Fl1Dt/tmp/c809ba2422beecae3618ce5eaec5a581f704633a491868439f263b9766f88080.jpg", "table_caption": ["Table 4: The top-1 retrieval accuracy for each subject. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Unlike the latest techniques for fMRI image reconstruction (where retrieval accuracy can reach more than $90\\%$ ), NeuroClips\u2019 retrieval accuracy on the cc2017 dataset averages around $22\\%$ , which is at the lower end of the scale. Leaving aside the difference between fMRI in image stimuli and video stimuli, we found that the cc2017 test set had a large number of categories of objects that did not appear in the training set. This is likely to be the main reason for the low retrieval accuracy and is also the reason why most of the previous studies have focused on low-level visual reconstruction. In the future, a large-scale fMRI-video dataset more compatible with deep learning is worth looking forward to. ", "page_idx": 20}, {"type": "text", "text": "D Is NeuroClips Credible? ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "A big difference in NeuroClips\u2019 video generation inference is that we freeze the weights of the video diffusion model, while MinD-Video fine-tunes it. This raises the question of whether our generation relies exclusively on the knowledge base that the video diffusion model has been trained on, and is not consistent with the distribution of video training data related to fMRI. ", "page_idx": 20}, {"type": "text", "text": "Firstly, since our keyframes are control conditions as first frames, the content of our video generation is closely related to the keyframes, both semantically and structurally. So in NeuroClips, the video diffusion model will not rely entirely on its own semantic knowledge to generate videos, but on the keyframes. Secondly, for keyframe generation, keyframes are generated by SDXL unCLIP. The normal version of SDXL unCLIP generates images from a larger dimensional text representation space, which was fine-tuned for 6 epochs by Scotti et al. [2] on MSCOCO with image representations. The fine-tuned version of SDXL unCLIP provides a $256\\!\\times\\!1664$ large dimensional CLIP representation space, which has a strong correspondence with the pixel space of the image, as shown in Figure 13. The SDXL unCLIP we use is this fine-tuned version. The videos captured from the cc2017 dataset used in this paper are from Videoblocks (https://www.videoblocks.com) and YouTube (https://www.youtube.com). These videos are not part of the COCO dataset, but SDXL unCLIP can still restore the original images as shown in Figure 13, demonstrating the equivalence of its representation space and pixel space. Since NeuroClips trained Semantic Reconstructor to align fMRI to this representation space, this also shows that our keyframe reconstruction is closely related to fMRI training. ", "page_idx": 20}, {"type": "image", "img_path": "8qu52Fl1Dt/tmp/4ee8ec53435134b35e69d828dbb789ad7265f365c32029815d2cf6b264d337e0.jpg", "img_caption": ["Figure 13: Visualisation of the generalization ability of SDXL unCLIP. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "E Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Our research is dedicated to exploring the possibilities of neural decoding using deep learning techniques, which have a positive impact on the field of neuroscience. With the expansion of model scales and the improvement of corresponding hardware devices, this research will also make positive contributions to the field of brain-computer interfaces. However, this research also highlights the importance of personal privacy and security. Governments and research institutions should take appropriate measures to protect the privacy data collected and prevent any potential misuse. In our research, we use publicly available and de-identified datasets, so the study strictly adheres to relevant ethical requirements. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our abstract and introduction clearly state the claims made, including the contributions made in the paper. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We create a separate \"Limitations\" section in our Appendix to discuss the limitations of our works. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when the image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The novelty of our work is focused on the application level and does not involve theoretical results. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide detailed experimental details in the paper and appendix to ensure the reproducibility of our work. In addition, we will make the code and weight files public if the paper is accepted. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We use publicly available datasets and provide anonymous links to our project. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide the key experimental setup in the paper and provide the complete experimental setup in our Appendix. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We report the average and standard deviation of multiple runs to make the experimental results statistically significant. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide sufficient information on the computer resources. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our research conducted in the paper conforms with the NeurIPS Code of Ethics in every respect. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We discuss the potential societal impacts of our work in the Appendix. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 25}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our work poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The creators or original owners of assets (e.g., code, data, models), used in the paper, are properly credited. And the license and terms of use are explicitly included in our code repository. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our anonymized URL includes these new codes, new results, and related documentation. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 27}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]