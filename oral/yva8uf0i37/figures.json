[{"figure_path": "YvA8UF0I37/figures/figures_1_1.jpg", "caption": "Figure 1: WikiText-2 perplexity (left) and average zero-shot accuracy (right) of 2-bit quantized LLAMA 2 models as a function of model size (GiB). See detailed setup in Section 4.3.", "description": "This figure compares the performance of three different 2-bit quantized LLAMA 2 models (QuIP#, AQLM, and PV-Tuning) across various model sizes.  The left panel shows the perplexity on the WikiText-2 dataset, a measure of how well the model predicts the next word in a sequence. The right panel shows the average zero-shot accuracy across a set of tasks.  Lower perplexity and higher accuracy indicate better performance.  The figure demonstrates that PV-Tuning consistently outperforms the other methods across different model sizes.", "section": "4 Experiments"}, {"figure_path": "YvA8UF0I37/figures/figures_6_1.jpg", "caption": "Figure 2: (left) L2 errors for 17th layer of LLAMA 27B with different representations. Full model perplexity on WikiText-2 is reported without finetuning (middle) and with fine-tuning (right).", "description": "This figure compares several popular weight representations for quantized LLMs.  The left panel shows the L2 error for the 17th layer of a Llama 2 7B model after applying different quantization techniques. The middle panel displays the full model's perplexity on the WikiText-2 benchmark without any further fine-tuning. Finally, the right panel presents the perplexity after applying fine-tuning, demonstrating the impact of fine-tuning on each quantization method. The figure highlights the effectiveness of various representation techniques and the improvement obtained after finetuning.", "section": "Evaluating quantized representations with finetuning"}, {"figure_path": "YvA8UF0I37/figures/figures_27_1.jpg", "caption": "Figure 3: WikiText-2 perplexity of 2-bit quantized LLAMA 2 models as a function of model size (GiB) compared to a theoretical lossless 3 bit compressed model (i.e. float16 perplexity numbers paired with 3-bit model sizes).", "description": "The figure shows the perplexity on the WikiText-2 dataset for 2-bit quantized Llama 2 models with varying sizes.  It compares the perplexity of models using different quantization techniques (AQLM, PV-Tuning, QuIP#) against a theoretical lossless baseline (using 3 bits and FP16 precision).  The plot demonstrates how perplexity changes with model size and quantization method.", "section": "Additional evaluations of perplexity"}, {"figure_path": "YvA8UF0I37/figures/figures_28_1.jpg", "caption": "Figure 2: (left) L2 errors for 17th layer of LLAMA 27B with different representations. Full model perplexity on WikiText-2 is reported without finetuning (middle) and with fine-tuning (right).", "description": "This figure compares different quantization methods in terms of their accuracy-size trade-off. The leftmost plot shows the L2 error for a single layer of a Llama 2 7B model with various quantization techniques. The middle plot presents the perplexity on WikiText-2 for the full models without fine-tuning, and the rightmost plot depicts the same for models after fine-tuning. The figure highlights the performance gains achieved through PV-Tuning, especially in the low-bit regime.", "section": "Evaluating quantized representations with finetuning"}, {"figure_path": "YvA8UF0I37/figures/figures_28_2.jpg", "caption": "Figure 5: Learning curve for PV-tuning and STE algorithms, when tuning tinyllama model with 2x8g8 AQLM quantization (2 codebooks with 8 bits per code and input groupsize equal to 8).", "description": "This figure shows the learning curves of PV-Tuning (subspace 0.01) and Straight-Through Estimation (STE) when fine-tuning a TinyLlama model.  The model uses 2x8g8 AQLM quantization, meaning it has 2 codebooks with 8 bits each, and the input groups are of size 8.  The y-axis represents the perplexity on the WikiText2 dataset, and the x-axis represents the training step. The shaded areas represent the standard deviation across multiple runs.  The plot illustrates the convergence behavior of both methods and the relative performance in terms of perplexity.", "section": "Experiments"}, {"figure_path": "YvA8UF0I37/figures/figures_35_1.jpg", "caption": "Figure 1: WikiText-2 perplexity (left) and average zero-shot accuracy (right) of 2-bit quantized LLAMA 2 models as a function of model size (GiB). See detailed setup in Section 4.3.", "description": "This figure compares three different methods (QuIP#, AQLM, and PV-Tuning) for 2-bit quantization of LLAMA 2 models of varying sizes, evaluating their performance on the WikiText-2 perplexity benchmark and average zero-shot accuracy.  It visually demonstrates the relative performance gains of PV-Tuning over existing methods in terms of both perplexity (lower is better) and zero-shot accuracy (higher is better) as the model size increases.  Detailed experimental setup is described in Section 4.3 of the paper.", "section": "Experiments"}, {"figure_path": "YvA8UF0I37/figures/figures_35_2.jpg", "caption": "Figure 6: PV algorithm (1) applied on the very small dimensional (d = 6) quadratic objective (18). The starting point x\u2070 is chosen randomly using the ng algorithm (6).", "description": "This figure shows the results of applying the PV algorithm to a small-scale problem with a 6-dimensional quadratic objective function.  The algorithm's performance is evaluated for different numbers of unique values (c) in the weight vectors. The starting point for each run is randomly chosen using Algorithm 6, ensuring variation in initial conditions. Subplots illustrate the algorithm's convergence for various values of c and the effect of separate P and V steps on loss function.", "section": "Small-Scale Experiments and Interpretation"}, {"figure_path": "YvA8UF0I37/figures/figures_37_1.jpg", "caption": "Figure 1: WikiText-2 perplexity (left) and average zero-shot accuracy (right) of 2-bit quantized LLAMA 2 models as a function of model size (GiB). See detailed setup in Section 4.3.", "description": "This figure shows the performance of 2-bit quantized Llama 2 models on two tasks: WikiText-2 perplexity and average zero-shot accuracy.  The x-axis represents the model size in GiBs, demonstrating how performance changes with varying model sizes after 2-bit quantization.  The results compare three different methods: QuIP#, AQLM, and PV-Tuning.", "section": "Experiments"}, {"figure_path": "YvA8UF0I37/figures/figures_37_2.jpg", "caption": "Figure 6: PV algorithm (1) applied on the very small dimensional (d = 6) quadratic objective (18). The starting point x\u00ba is chosen randomly using the ng algorithm (6).", "description": "This figure shows the results of applying the PV algorithm to a small-scale problem (d=6, c=[1,6]) with a quadratic objective function.  The algorithm is run multiple times (r=50) with different random starting points to show its behavior and convergence properties. The subplots demonstrate: (a) convergence curves for different values of 'c', representing varying levels of compression; (b) the effect of P and V steps on the objective function for c=3. The results highlight the algorithm's convergence and the iterative nature of P and V steps, showing that both are needed to find an accurate solution.  The scale in (a) is logarithmic, explaining why the line for c=6 is not visible, as it is very close to the minimum loss (0).", "section": "Small-Scale Experiments and Interpretation"}, {"figure_path": "YvA8UF0I37/figures/figures_37_3.jpg", "caption": "Figure 8: Experiments with Linearized PV algorithm (B.1).", "description": "This figure shows the results of experiments using the Linearized PV algorithm with different numbers of iterations (T) for the linearized V-step. Subfigure (a) compares the convergence rates of the Linearized PV algorithm with different values of T, showing that increasing T leads to faster convergence but similar final accuracy. Subfigure (b) illustrates the influence of P and V steps on the loss function for a specific value of T (T=2), demonstrating the iterative improvement achieved by alternating between these steps.", "section": "Q.6 Linearized PV"}, {"figure_path": "YvA8UF0I37/figures/figures_37_4.jpg", "caption": "Figure 6: PV algorithm (1) applied on the very small dimensional (d = 6) quadratic objective (18). The starting point x\u2070 is chosen randomly using the ng algorithm (6).", "description": "This figure shows the results of applying the PV algorithm to a small-scale problem with a quadratic objective function.  The dimensionality (d) is 6, and the maximum number of unique values (c) is varied from 1 to 6. The algorithm is run 50 times with different randomly chosen starting points to demonstrate its behavior. The subplots illustrate the convergence of the algorithm for different values of c and show the impact of the P and V steps on the loss function.", "section": "Q.2 Tiny-scale experiments (d = 6, c \u2208 [1, 6])"}, {"figure_path": "YvA8UF0I37/figures/figures_38_1.jpg", "caption": "Figure 9: Comparison of the exact PV algorithm (1) with Linearized PV (B.1) and Linearized PV + sparse updates (3.3).", "description": "This figure compares three different algorithms for optimizing a quantized model: the exact PV algorithm, the linearized PV algorithm, and the linearized PV algorithm with sparse updates.  The x-axis represents the iteration number, and the y-axis shows the objective function value (loss).  The results demonstrate that the linearized PV algorithm converges to a lower accuracy compared to the exact PV algorithm. However, incorporating sparse updates into the linearized PV algorithm leads to a significant improvement in accuracy, converging to a value close to that of the exact PV algorithm.  There is also a trade-off; the convergence speed is slower with sparse updates compared to the linearized PV algorithm alone.", "section": "Q.7 Linearized PV + sparse updates"}, {"figure_path": "YvA8UF0I37/figures/figures_38_2.jpg", "caption": "Figure 9: Comparison of the exact PV algorithm (1) with Linearized PV (B.1) and Linearized PV + sparse updates (3.3).", "description": "This figure compares three different algorithms in terms of their convergence behavior and final accuracy. The algorithms are: the exact PV algorithm, the linearized PV algorithm, and the linearized PV algorithm with sparse updates. The results show that the exact PV algorithm converges to the best accuracy, while the linearized PV algorithm converges to a worse accuracy. The linearized PV algorithm with sparse updates converges to an accuracy that is in between the other two algorithms.", "section": "Q.7 Linearized PV + sparse updates"}, {"figure_path": "YvA8UF0I37/figures/figures_38_3.jpg", "caption": "Figure 10: Comparison of the exact PV algorithm (1) with Linearized PV (B.1) and Linearized PV + Sparse Updates (3.3).", "description": "This figure compares the performance of three PV algorithms: the exact PV algorithm, the linearized PV algorithm, and the linearized PV algorithm with sparse updates. The x-axis represents the iteration number, and the y-axis represents the value of L<sup>Sk</sup><sub>k</sub> (a measure of the smoothness of the objective function in the sparse subspace).  The figure shows that the exact PV algorithm converges to the best accuracy, but the linearized PV algorithm with sparse updates converges to a better accuracy than the linearized PV algorithm alone, although it takes more iterations to converge. Different methods for selecting the sparse subspace (greedy Top-K, random uniform, and random proportional) are also shown, demonstrating that the greedy Top-K method generally yields the fastest convergence.", "section": "Q.7 Linearized PV + sparse updates"}, {"figure_path": "YvA8UF0I37/figures/figures_38_4.jpg", "caption": "Figure 1: WikiText-2 perplexity (left) and average zero-shot accuracy (right) of 2-bit quantized LLAMA 2 models as a function of model size (GiB). See detailed setup in Section 4.3.", "description": "This figure compares the performance of three different 2-bit quantized LLAMA 2 models (QuIP#, AQLM, and PV-Tuning) across various model sizes.  The left panel shows the perplexity on the WikiText-2 benchmark, a measure of how well the model predicts the next word in a sequence.  The right panel displays the average zero-shot accuracy across a range of tasks. The results indicate that PV-Tuning consistently outperforms QuIP# and AQLM in terms of both perplexity and zero-shot accuracy, particularly as the model size increases.", "section": "Experiments"}]