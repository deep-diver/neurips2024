[{"type": "text", "text": "PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vladimir Malinovskii\u2020 Denis Mazur\u2020 Ivan Ilin\u2020 Denis Kuznedelev Yandex, HSE University MIPT\u22c4, SberDevices\u00b6 AI Initiative, KAUST\u2217 Yandex, Skoltech ", "page_idx": 0}, {"type": "text", "text": "Konstantin Burlachenko Kai Yi Dan Alistarh\u2021 Peter Richtarik\u2021 AI Initiative, KAUST\u2217 AI Initiative, KAUST\u2217 IST Austria, NeuralMagic AI Initiative, KAUST\u2217 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "There has been significant interest in \u201cextreme\u201d compression of large language models (LLMs), i.e., to 1-2 bits per parameter, which allows such models to be executed efficiently on resource-constrained devices. Existing work focused on improved one-shot quantization techniques and weight representations; yet, purely post-training approaches are reaching diminishing returns in terms of the accuracyvs-bit-width trade-off. State-of-the-art quantization methods such as QuIP# and AQLM include fine-tuning (part of) the compressed parameters over a limited amount of calibration data; however, such fine-tuning techniques over compressed weights often make exclusive use of straight-through estimators (STE), whose performance is not well-understood in this setting. In this work, we question the use of STE for extreme LLM compression, showing that it can be sub-optimal, and perform a systematic study of quantization-aware fine-tuning strategies for LLMs. We propose PV-Tuning \u2014 a representation-agnostic framework that generalizes and improves upon existing fine-tuning strategies, and provides convergence guarantees in restricted cases. On the practical side, when used for 1-2 bit vector quantization, PV-Tuning outperforms prior techniques for highly-performant models such as Llama and Mistral. Using PV-Tuning, we achieve the first Pareto-optimal quantization for Llama-2 family models at 2 bits per parameter. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent years have seen the development of ever more capable large language models, attracting immense interest from both researchers and industry. One of the driving factors behind progress in this area is the availability of powerful open LLMs such as Llama [69], Mistral [34, 35], or Phi [41]. The main advantage of open LLMs is that they can be run and fine-tuned locally by end users; however, as state-of-the-art LLMs grow larger, they also become harder to run on commodity hardware. For instance, in order to fit the best available Llama-3 model on a consumer GPU, the model would have to be compressed to below 2 bits per parameter1. ", "page_idx": 0}, {"type": "text", "text": "To achieve such \u201cextreme\u201d degrees of compression accurately, researchers have proposed a variety of techniques, which can be roughly categorized into i) better quantized weight representations and ii) better algorithms to learn these representations. The weight representations used for extreme quantization include group quantization [22, 20], sparse high-precision outliers [17, 32], incoherence processing of the weights [9, 70], or additive and residual quantization [21, 72]. In turn, the calibration algorithms also vary between data-free methods [20], layer-wise calibration [22, 18], block-wise or global fine-tuning [21, 71] or even quantization-aware training [78, 75]. However, the weight representation and the fine-tuning algorithm are largely orthogonal: most popular quantized representations could be obtained layer-wise in one-shot, fine-tuned layer-wise to a variety of optimization objectives, or even trained entirely from scratch. ", "page_idx": 0}, {"type": "image", "img_path": "YvA8UF0I37/tmp/f1045bb3ea27fd6d883b257dda188c93fd4507763450aa975c076962f2d68086.jpg", "img_caption": ["Figure 1: WikiText-2 perplexity (left) and average zero-shot accuracy (right) of 2-bit quantized LLAMA 2 models as a function of model size (GiB). See detailed setup in Section 4.3. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Surprisingly, there is a clear disparity between the degree of interest shown to accurate one-shot2 quantization versus accurate fine-tuning. Specifically, one-shot quantization is very well-studied, to the extent that, as shown in Figure 2, improvements in this direction are clearly saturating. At the same time, the impact of fine-tuning strategy is largely unknown: while many recent works use some form of fine-tuning [63, 21, 71], they typically consider a single fine-tuning regimen based on straight-through estimation (STE) [6, 15]. Thus, given the multitude of representations considered, it is not at all clear whether current fine-tuning strategies are optimal. ", "page_idx": 1}, {"type": "text", "text": "In this work, we analyze the problem of fine-tuning over highly-compressed weights from the optimization perspective. We begin by analyzing popular fine-tuning strategies for extreme LLM quantization. The key challenge in this context is that the quantized representations may contain both continuous and discrete variables: while continuous parameters, such as learnable scales or codebooks, can be optimized by backpropagation, the discrete parameters (e.g., integer assignments for the weights) cannot. Existing fine-tuning techniques either do not optimize over discrete parameters at all [71, 21] or fine-tune them using heuristics such as STE or stochastic rounding [3]. Unfortunately, these methods are not well-justified for weight quantization from the point of view of optimization theory, and, as we show in Section 3, can provide poor practical performance. ", "page_idx": 1}, {"type": "text", "text": "We propose an alternative solution: instead of following heuristic gradient estimates, our approach follows the actual gradient of the objective in a small subspace of optimized parameters where it can be meaningfully improved. Following this insight, we formulate the $P V.$ tuning framework for fine-tuning arbitrary quantized representations. We update both discrete and continuous components to minimize a global objective function, such as the KL divergence relative to the original model predictions. Our results show that this strategy leads to significant improvements across weight representations, achieving new state-of-the-art in compression-accuracy trade-offs. ", "page_idx": 1}, {"type": "text", "text": "The main contributions of our work can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We analyze the problem for training discrete quantized representations for better understanding of the limitations of existing optimization algorithms. We then propose a novel algorithm inspired by compressed gradient methods that addresses these limitations. When compared to straight-through estimation and stochastic rounding, our approach 1) can be shown to converge to a stable solution; and 2) this solution is significantly more accurate in practice.   \n2. We generalize the proposed algorithm into the PV-Tuning framework3, which can minimize a global objective function over a general quantized representation, by optimizing both continuous and discrete parameters via a variant of coordinate descent.   \n3. We demonstrate that PV-tuning can improve quantized model accuracy for leading existing approaches, including GPTQ and AQLM, on popular LLMs including Llama-2 & 3 and Mistral. Our procedure achieves state-of-the-art accuracy (measured through perplexity) in 1- and 2-bit quantization regimes while using the same amount of calibration data as the original algorithms. Importantly, the PV-tuned models use the same underlying weight representations, and are compatible with existing inference kernels. In terms of accuracy per model size, PV-tuning of vector quantization outperforms all prior techniques in the 1-3 bits/parameter range, and is the first to achieve Pareto-optimal quantization for Llama-2 models at around 2 bits per parameter. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Post-Training LLM Quantization (PTQ). There has been significant interest in PTQ methods [49, 25] that would scale to LLMs. Early work [17, 78, 50] used direct round-to-nearest (RTN) quantization over weight groups of well-chosen size. GPTQ [22] improved upon these results significantly via an accurate one-shot solver for minimizing layer-wise compression errors. Next, AWQ [42] improved upon these results by employing per-channel scaling to reduce the error on important weights while SqueezeLLM [36] implemented non-uniform quantization. QuIP [9] proposed a more accurate weight representation by leveraging incoherence matrices. Another line of works [18, 39] proposes an improved quantized weight representation, which saves a small fraction of outliers in full precision. Other recent works propose augmenting quantized representations with lowrank \u201cadapters\u201d that compensate quantization error [28, 84]. Recently, BiLLM [32] developed residual binarization that stores salient weights in progressively higher bitwidth, quantizing models to nearly 1 bit per parameter at non-catastrophic accuracy loss. ", "page_idx": 2}, {"type": "text", "text": "Currently, the state-of-the-art methods in terms of accuracy-vs-size are QuIP# [71] and AQLM [21]. Both methods work roughly by mapping weight groups to points on highly-dimensional lattices, which are either chosen to satisfy some optimality properties (for QuIP#) or are learned (for AQLM). Interestingly, AQLM showed that fine-tuning the continuous parameters (codebooks) can improve accuracy significantly relative to pure one-shot compression; a variant of this approach was also adopted by QuIP#. PV-Tuning is compatible with both methods: as we show, it can lead to state-ofthe-art compression results for such representations. ", "page_idx": 2}, {"type": "text", "text": "Fine-tuning over Quantized Weights. As mentioned above, the two SOTA quantization techniques apply fine-tuning, but only update continuous parameters, such as quantization scales. When optimizing over discrete parameter sets, a standard choice in deep learning is the Straight-Through Estimator (STE) [6, 15, 73]. Prior work on LLM compression proposed to update both continuous and discrete parameters, via STE, both for post-training quantization [78, 63] and for training quantized networks from scratch [32]. However, it was observed early on that STE leads to instability when fine-tuning heavily quantized LLMs [78]. While early results suggest that STE can perform well when training quantized models from scratch [44], this behavior is yet to be validated for highly-performant multi-billion-parameter models, which are the focus of our work. ", "page_idx": 2}, {"type": "text", "text": "In summary, the two standard approaches for fine-tuning quantized LLMs are 1) fine-tuning only over the continuous parameters, such as quantization scales, which heavily limits the number of trainable parameters; and 2) optimizing all parameters via the STE, which however is known to be quite noisy especially for extreme quantization. In this context, our work proposes alternative approaches in the post-training compression setting, which lead to state-of-the-art results relative to both options. ", "page_idx": 2}, {"type": "text", "text": "3 Fine-Tuning Quantized Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we study the problem of fine-tuning quantized models to minimize a global objective, such as cross-entropy. Section 3.1 formulates this problem from an optimization perspective and introduces our notation. In Section 3.2, we analyze several popular strategies for solving this problem and highlight some of their limitations. To circumvent these limitations, we propose an alternative optimization algorithm in Section 3.3 and discuss implementation details in Section 3.4. ", "page_idx": 2}, {"type": "text", "text": "3.1 Problem description ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider the problem of minimizing objective (loss) $\\phi$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}_{c}^{d}}\\phi(x),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\phi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is a differentiable function bounded from below (e.g., by zero), and $\\mathbb{R}_{c}^{d}\\subset\\mathbb{R}^{d}$ is a set of all possible quantized weights that can be represented with a given quantization method. Without loss of generality4, we first analyze the case of scalar nonlinear quantization. In this scenario, $c\\in[d]:=\\{1,\\breve{2},\\ldots,d\\}$ (typically $c\\ll d,$ ), and $\\mathbb{R}_{c}^{d}\\subset\\mathbb{R}^{d}$ is the set of all vectors in $\\mathbb{R}^{d}$ whose $d$ entries take exactly $c$ distinct values. In other words, the cardinality of the set $V(x):=\\{x_{1},\\ldots,x_{d}\\}$ is equal to $c$ , and we can therefore write $\\mathbb{R}_{c}^{d}:=\\{x\\in\\mathbb{R}^{d}\\ :\\ |V(x)|=c\\}$ . ", "page_idx": 2}, {"type": "text", "text": "Useful notation. A vector $\\boldsymbol{x}\\in\\mathbb{R}_{c}^{d}$ naturally induces a partition, which we shall call $P(x)$ , of the set $\\{1,\\ldots,d\\}$ into $c$ nonempty subsets $P_{1}(x),\\dotsc,P_{c}(x)$ characterized by ", "page_idx": 3}, {"type": "equation", "text": "$$\nx_{i}=x_{j}\\quad\\Leftrightarrow\\quad\\exists k\\,:\\,i\\in P_{k}{\\mathrm{~and~}}j\\in P_{k}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Let\u2019s denote $P(x):=\\{P_{1}(x),\\ldots,P_{c}(x)\\}$ . Moreover, we shall write $P(y)\\supseteq P(x)$ if each element of $P(x)$ is a subset of some element of $P(y)$ . For distinct $i,j\\in[d]$ , let us introduce the notation $\\delta_{i j}(x)=1$ if there exists $k$ such that $i,j\\in P_{k}(x)$ , and $\\delta_{i j}(x)=0$ otherwise. Given this notation, notice that $P(y)\\supseteq P(x)$ if and only if for all $i\\neq j$ we have $\\delta_{i j}(x)=1\\Rightarrow y_{i}=y_{j}$ . Finally, we define $\\mathbb{R}_{\\leq c}^{d}:=\\mathbb{R}_{1}^{d}\\cup\\cdot\\cdot\\cdot\\cup\\mathbb{R}_{c}^{d}$ as the set of all vectors in $\\mathbb{R}^{d}$ whose $d$ entries take at most $c$ distinct values. So, if $x\\in\\mathbb{R}_{c}^{d}$ and $P(y)\\supseteq P(x)$ , then $\\boldsymbol{y}\\in\\mathbb{R}_{\\leq c}^{d}$ . ", "page_idx": 3}, {"type": "text", "text": "PV method. Following this notation, we define an optimization algorithm that alternates between optimizing $\\phi$ with fixed $P$ or fixed $V$ . From a practitioner\u2019s point of view, these represent optimizing continuous parameters (scales, codebooks, zeros) and discrete codes (assignments), respectively. ", "page_idx": 3}, {"type": "text", "text": "$\\diamond$ The $\\mathbf{P}$ step (fixing $P$ ). Given $\\boldsymbol{x}\\in\\mathbb{R}_{c}^{d}$ c\uff0c , consider the mapping ", "page_idx": 3}, {"type": "equation", "text": "$$\nM_{P}(x)=M_{P,\\phi}(x):=\\arg\\operatorname*{min}_{y\\in\\mathbb{R}^{d}}\\{\\phi(y)\\,:\\,P(y)\\supseteq P(x)\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Notice that, necessarily, $\\boldsymbol{M}_{P}(\\boldsymbol{x})\\,\\in\\,\\mathbb{R}_{\\leq c}^{d}$ and $\\phi(M_{P}(x))\\,\\leq\\,\\phi(M_{P}(x))\\,\\leq\\,\\phi(x)$ . Evaluating $M_{P}$ amounts to solving an unconstrained optimization problem in a $c$ -dimensional space. ", "page_idx": 3}, {"type": "text", "text": "$\\diamond$ The $\\mathbf{V}$ step (fixing $V$ ). Similarly, given $y\\in\\mathbb{R}_{c}^{d}$ , we define the mapping ", "page_idx": 3}, {"type": "equation", "text": "$$\nM_{V}(y)=M_{V,\\phi}(y):=\\arg\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\{\\phi(x)\\,:\\,V(x)\\subseteq V(y)\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Likewise, $M_{V}(y)\\in\\mathbb{R}_{\\leq c}^{d}$ and $\\phi(M_{V}(y))\\leq\\phi(M_{V}(y))\\leq\\phi(y)$ . Evaluating $M_{V}$ amounts to solving difficult discrete optimization problems with a search space of size $|V(x)|^{d}\\leq c^{d}$ (exponential in $d$ ). ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 PV algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "1: Initialization: starting point $\\boldsymbol x^{0}\\in\\mathbb R_{\\leq c}^{d}$ ", "text_level": 1, "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\dot{y}^{k}=\\dot{M}_{P}(x^{k}):=\\operatorname*{arg\\,min}_{y\\in\\mathbb{R}^{d}}\\left\\{\\phi(y):P(y)\\supseteq P(x^{k})\\right\\}}&{\\qquad}&{\\mathrm{(P~step;continuous)}}\\\\ {\\dot{x}_{\\cdot}^{k+1}=M_{V}(y^{k}):=\\operatorname*{arg\\,min}_{x\\in\\mathbb{R}^{d}}\\left\\{\\phi(x):V(x)\\subseteq V(y^{k})\\right\\}}&{\\qquad}&{\\mathrm{(V~step;~discrete)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "5: end for ", "page_idx": 3}, {"type": "text", "text": "Our key algorithmic idea, in its simplest form, is to optimize $\\phi$ by alternating the $\\mathbf{P}$ and $\\mathrm{v}$ steps, i.e., iteratively applying the $M_{P}$ and $M_{V}$ operators. (We will propose several more practically-useful approximations and variations later; see Sections 3.2\u20133.3 and also Appendix B.) This resulting method, which we call the PV method, is formalized as Algorithm 1. Our key guarantee for the PV method is formalized in the next result. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1 (Convergence of the PV method). Assume $\\phi$ is bounded below, and let $x^{0}\\in\\mathbb{R}_{c}^{d}$ . Then (i) $\\boldsymbol{y}^{k}\\in\\mathbb{R}_{\\leq c}^{d}$ and $\\boldsymbol{x}^{k}\\in\\mathbb{R}_{\\leq c}^{d}$ for all $k\\geq0$ ; (ii) $\\phi(x^{k+1})\\leq\\phi(y^{k})\\leq\\phi(x^{k})$ for all $k\\geq0$ ; and (iii) the sequence $\\{\\phi({\\boldsymbol{x}}^{k})\\}_{k\\geq0}$ converges. ", "page_idx": 3}, {"type": "text", "text": "The proof can be found in Appendix A.1. Note that we do not claim that the method converges to a minimizer of $\\phi$ ; the optimization problem is too difficult for us to be able to guarantee this. However, as we shall see in the numerical results, we nevertheless obtain great empirical performance, especially when coupling the PV approach with some additional algorithmic tricks. ", "page_idx": 3}, {"type": "text", "text": "This general approach is popular in \u201cshallow\u201d machine learning problems; for instance, if $\\phi(x)=$ $\\|x-z\\|^{2}$ is the squared error with respect to some user-specified vector $z$ , then the above algorithm recovers 1-dimensional $K$ -means on the data vector $z$ . Likewise, if $\\phi(\\cdot)$ is the log-likelihood, then, depending on the choice of the set $\\mathbb{R}_{c}^{d}$ , the approach is related to the EM algorithm [16]. ", "page_idx": 3}, {"type": "text", "text": "In turn, we apply the PV method to obtaining highly-accurate quantized LLMs. Applying the PV method \u201cas is\u201d, would be infeasible in practice: computing the $\\mathbf{P}$ and $\\mathrm{v}$ mappings requires solving difficult optimization problems especially due to LLM parameter scales. However, both mappings can be approximated. The $\\mathbf{P}$ step can be reparameterized as an unconstrained optimization problem on the unique values in the weight matrix. Practically it means that the \u201ccodebooks\u201d can be optimized using an automated differentiation engine (i.e. PyTorch). However, for many quantized representations, $M_{P}(x)$ can be approximated by one or more steps of GD, directly optimizing $\\phi$ over the set $V(x)$ of its $c$ unique values. The $c$ -dimensional gradient can be computed efficiently by backprop, as described in prior works [63, 71]. On the other hand, the $\\mathrm{v}$ step $(M_{V}(\\cdot))$ is more difficult to approximate as it involves searching a discrete space of size $c^{d}$ . We dedicate the next two sections to this task. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.2 Linearized $\\mathbf{V}$ step $\\&$ gradient-based discrete updates ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The $\\mathrm{v}$ mapping (3) can be approximated by solving a discrete least squares problem using an approximation of $\\phi(x)$ around $y$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\phi(\\boldsymbol{x})\\approx\\widetilde{\\phi}_{y}(\\boldsymbol{x}):=\\phi(\\boldsymbol{y})+\\langle\\nabla\\phi(\\boldsymbol{y}),\\boldsymbol{x}-\\boldsymbol{y}\\rangle+\\frac{L}{2}\\|\\boldsymbol{x}-\\boldsymbol{y}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $L>0$ is a sufficiently large constant. Subsequently, we perform the $\\mathrm{v}$ step using the simpler convex quadratic function $\\tilde{\\phi}_{y}$ instead of the typically more complicated function $\\phi$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nM_{V,\\phi}(y)\\stackrel{(4)}{\\approx}M_{V,\\widetilde{\\phi}_{y}}(y)\\stackrel{(3)}{=}\\arg\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\left\\{\\widetilde{\\phi}_{y}(x)\\;:\\;V(x)\\subseteq V(y)\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Our first lemma shows that we can replace $\\widetilde{\\phi}_{y}$ by a more convenient function $\\widehat{\\phi}_{y}$ measuring the squared distance between $x$ and $\\begin{array}{r}{y^{+}:=y-\\frac{1}{L}\\bar{\\nabla}\\phi(y)}\\end{array}$ , the latter being the point ob tained after taking a single GD step from $y$ with learning rate $\\scriptstyle{\\frac{1}{L}}$ , disregarding the constraint: ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.2. For any $\\boldsymbol{y}\\in\\mathbb{R}_{\\leq c}^{d}$ we have $M_{V,\\tilde{\\phi}_{y}}(y)=M_{V,\\hat{\\phi}_{y}}(y)$ , where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\phi}_{y}(x):=\\left\\Vert x-\\left(y-\\textstyle\\frac{1}{L}\\nabla\\phi(y)\\right)\\right\\Vert^{2}=\\left\\Vert x-y^{+}\\right\\Vert^{2}=\\sum_{i=1}^{d}\\left(x_{i}-y_{i}^{+}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The proof can be found in Appendix A.2. To summarize, the $\\mathrm{v}$ step of the PV method (Algorithm 1), i.e., $x=M_{V,\\phi}(y)$ , can be approximated via the \u201clinearized $\\mathrm{v}$ step\u201d ", "page_idx": 4}, {"type": "equation", "text": "$$\nx:=M_{V,\\phi}(y)\\approx M_{V,\\widehat{\\phi}_{y}}(y):=\\widehat{x}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Our next lemma says that the above approximation is in a certain sense natural reasonable provided that $\\phi$ is $L$ -smooth5 on $\\mathbb{R}_{\\leq c}^{d}$ , i.e., provided that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\phi(x)\\leq\\phi(y)+\\left\\langle\\nabla\\phi(y),x-y\\right\\rangle+\\frac{L}{2}\\|x-y\\|^{2},\\qquad\\forall x,y\\in\\mathbb{R}_{\\leq c}^{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Lemma 3.3 (Monotonicity). Let $\\boldsymbol{y}\\in\\mathbb{R}_{\\leq c}^{d}$ . If $\\phi$ is $L$ -smooth on $\\mathbb{R}_{\\leq c}^{d},$ , then $\\phi\\left(M_{V,\\phi}(y)\\right)\\leq\\phi(\\hat{x})\\leq$ $\\phi(y)$ , where $\\hat{x}$ is the point obtained from $y$ by the linearized $V$ step (6). ", "page_idx": 4}, {"type": "text", "text": "Indeed, the point $\\hat{x}$ obtained via the linearized $\\mathrm{v}$ step can not have a worse loss than the previous point $y$ . Of course, one hopes that the loss will strictly decrease so that the method makes progress. From a practical perspective, the key advantage of linearized $\\mathrm{v}$ step is that it can be performed much faster compared to the vanilla $\\mathrm{v}$ step. The proof of Lemma 3.3 can be found in Appendix A.3. ", "page_idx": 4}, {"type": "text", "text": "Note that since $\\widehat{\\phi}_{y}(x)$ is separable (see (8)), each entry/weight of $x$ can be optimized independently of others. For s calar quantization, each individual problem can be solved in ${\\mathcal{O}}(\\log_{2}(c))$ time using binary search in sorted version of $V(y)$ . For vector quantization, there are specialized optimization procedures for efficiently minimizing the $L_{2}$ error (see Appendix D) ", "page_idx": 4}, {"type": "text", "text": "Key challenge. The main caveat with linearized $\\mathrm{v}$ step is that it may be impossible to make small gradient-based updates to low-bitwidth discrete weights. More specifically, in (6), one must update the discrete assignments to approximate $\\begin{array}{r}{y^{k}-\\frac{1}{L}\\nabla\\phi\\bar{(}y^{k})}\\end{array}$ . However, for low-bit weights, the desired update $\\scriptstyle{\\frac{1}{L}}\\nabla\\phi(y^{k})$ can be smaller than the lowest possible increment to obtain a quantized vector. As a result, the optimal solution to (6) is often $y^{k}$ itself. In such a situation, the algorithm will get stuck on $y^{k}$ , which is undesirable. This problem is especially pronounced in deep LLMs, where $L$ can be very large, or, from a practitioner\u2019s point of view, where one needs a small learning rate. In practice, as we explore in Section 4.2, the lowest learning rate where the algorithm makes any updates at all is already too large for optimization, leading to divergence. ", "page_idx": 4}, {"type": "table", "img_path": "YvA8UF0I37/tmp/a0c39dd5a594986c40324059e90dc5f2d73e81187795ed38315e8ea7ecea6b82.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Many popular strategies for discrete fine-tuning can be seen as attempts to reconcile coarse lowprecision weights with the need to make small updates. These include straight-through estimation, stochastic rounding, or adding regularizers that push the solution to (6) away from $y^{\\overline{{k}}}$ . We review straight-through estimation in Appendix E.1 and stochastic rounding in Appendix E.2. ", "page_idx": 5}, {"type": "text", "text": "3.3 Linearized subspace V step ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Here we ask the following question: Can we modify the PV method so as to force the $\\mathbf{V}$ step to make a larger update? In other words, we need an optimization algorithm that updates quantized weights either by a sufficiently large increment, or not at all. ", "page_idx": 5}, {"type": "text", "text": "A natural example of such an algorithm is coordinate descent (CD) [43, 58], or more generally, subspace descent [26, 38]. Instead of updating all parameters by a small margin, CD in each iteration chooses a single parameter, and makes a large update instead. This strategy can be generalized to updating more parameters at the same time, which leads to subspace descent methods.6 The parameters to be updated can be chosen either greedily, (e.g., several $i~\\in~[d]$ with the largest magnitude of the partial derivative $|\\nabla_{i}\\phi(\\cdot)|)$ , or at random, or through a variety of other means. ", "page_idx": 5}, {"type": "text", "text": "Let $S^{k}\\,\\subset\\,[d]$ be the set of parameters/weights/coordinates we wish to update at iteration $k$ . We choose $|S^{k}|=\\tau\\ll d.$ . Let $\\bar{Z}^{k}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ be the linear mapping defined as follows: $\\left(Z^{k}(x)\\right)_{i}=x_{i}$ if $i\\in S^{k}$ and $\\left(Z^{k}(x)\\right)_{i}=0$ if $i\\not\\in{\\cal S}^{k}$ . We now formulate the linearized subspace $\\mathrm{v}$ step: ", "page_idx": 5}, {"type": "equation", "text": "$$\nx^{+}:=M_{V,\\widehat{\\phi}_{y,s^{k}}}(y):=\\arg\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\left\\{\\widehat{\\phi}_{y,S^{k}}(x)\\,:\\,V(x)\\subseteq V(y)\\right\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{where}}&{{}\\widehat{\\phi}_{y,S^{k}}(x):=\\Big\\|x-\\Big(y-\\frac{1}{L_{S^{k}}}Z^{k}\\left(\\nabla\\phi(y)\\right)\\Big)\\Big\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and $L_{S^{k}}>0$ is a smoothness parameter of $\\phi$ associated with the subspace spanned by the parameters belonging to $S^{k}$ . This detail is important because $L_{S^{k}}\\ll L$ when $\\tau\\ll d$ . When estimating Lipschitz constants for real $L L M s$ , we found that it is lower by at least one order of magnitude, making it possible to train with sufficiently large step sizes (see details and $L_{S^{k}}$ estimates in Appendix F). ", "page_idx": 5}, {"type": "text", "text": "Note that, necessarily, $x_{i}^{+}=y_{i}$ for $i\\not\\in{\\cal S}^{k}$ . The remaining $\\tau$ entries of $x^{+}$ can be identified exactly by searching a discrete space of size $|V(y)|^{\\tau}$ , which is feasible if $c=\\mathcal{O}(1)$ and $\\tau=\\mathcal{O}(1)$ , for example. ", "page_idx": 5}, {"type": "text", "text": "In practice, it means that the algorithm can apply large updates to quantized LLM weights, with the caveat that should only update a fraction of them at a time. This allows us to perform the linearized V step with sufficiently large \u201clearning rate\u201d to make non-trivial (i.e., $x^{k+1}\\neq y^{k}$ ) improvements to quantized weights even without straight-through estimation or stochastic rounding. ", "page_idx": 5}, {"type": "text", "text": "We formulate the full procedure in Algorithm 2. The algorithm performs the $\\mathbf{P}$ step by directly optimizing $V(x)$ (i.e., codebooks) by backprop as described in Section 3.1. For the $\\mathrm{v}$ step, the algorithm greedily chooses a subset of $\\tau$ quantized weights for update, then updates them using Eq. (8). The arg top $\\tau$ operator finds $\\tau$ indices with the largest absolute gradient values and builds a subspace of $\\mathbb{R}_{\\leq c}^{d}$ where only these values can be changed, and the rest must be equal to $y^{k}$ . ", "page_idx": 5}, {"type": "text", "text": "3.4 Implementation details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To speed up convergence, we use adaptive learning rates for both P and $\\mathrm{v}$ steps. In Eq. 8, we replace $\\nabla\\phi(\\boldsymbol{y})$ with a single Adam [37] update, as depicted in Algorithm 3. In preliminary experiments, we found that this results in a significant convergence speedup. When choosing the subspace $S^{k}$ , we select weights based not on $|\\nabla_{i}\\phi(y)|$ , but on the magnitude of Adam update for that weight. For simplicity, we greedily choose the $\\tau$ weights with the largest update norm within each weight matrix. ", "page_idx": 6}, {"type": "text", "text": "This could be further improved through better techniques for choosing $S^{k}$ explored in Appendix Q. We also found that, despite the fact that PV-tuning by itself outperforms straight-through estimation, we could achieve slightly better accuracy by combining PV-tuning with straight-through estimation. We explore this in more detail in Section 4.2). ", "page_idx": 6}, {"type": "text", "text": "We describe our approach for preparing the calibration data in Appendix G. We found that the preprocessing used in several recent PTQ works introduce a small bias when sampling the calibration data, leading to somewhat worse fine-tuning accuracy. For fairness, we always compare representations (Section 4.1) and algorithms (Section 4.2) using the same pre-processing. ", "page_idx": 6}, {"type": "text", "text": "Fine-tuning efficiency. The most compute-intensive part of PV tuning is computing the gradients $\\nabla\\phi(\\cdot)$ , which is done through repeated forward and backward passes on an LLM. To reduce the number of gradient accumulations, we reuse gradients for $\\mathbf{P}$ and $\\mathrm{v}$ steps within one iteration. We use mixed precision, gradient checkpointing and batch accumulation to train more efficiently; for larger LLMs such as LLAMA 3 70B we also use sharding and optimizer offloading (see Appendix H). Our code can train 7B LLMs on a single GPU, while larger ones (e.g. 70B) fti into a single machine with $8\\!\\times\\!\\mathrm{Al00}$ . In terms of wall-clock time, PV-tuning takes up to $1.5\\times$ longer than the fine-tuning procedure of [71] and requires additional memory in order to hold $\\nabla\\phi({\\boldsymbol{x}})$ . ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Evaluating quantized representations with finetuning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Before evaluating PV-tuning, we need to choose the quantized representation to be fine-tuned. We therefore compare popular weight representations from recent works on LLM quantization (see Section 2). To better isolate the effect of the weight representation, we evaluate them in three configurations: i) when quantizing a single LLM layer, in terms of MSE, ii) full model quantization in terms of perplexity without finetuning and iii) with finetuning. ", "page_idx": 6}, {"type": "text", "text": "We compare several recently proposed quantized representations (see details in Appendix J): ", "page_idx": 6}, {"type": "text", "text": "1. GPTQ: scalar uniform quantization with channel-wise and block-wise scales [22],   \n2. SpQR: an extension of block-wise GPTQ with learned sparse outliers [18],   \n3. VQ: basic vector quantization with a single codebook [72] with multi-step training.   \n4. AQLM: additive vector quantization with multiple learned codebooks [21],   \n5. QuIP#: vector quantization with lattices and incoherence processing [71],   \n6. $\\mathbf{VQ}/\\mathbf{AQ}+$ outliers: vector/additive quantization with sparse outliers via pruning [66, 8],   \n7. VQ/AQ $^+$ lowrank: vector/additive quantization with Low-Rank Compensation (LoRC) [79], ", "page_idx": 6}, {"type": "image", "img_path": "YvA8UF0I37/tmp/4b262b2cf10c9ba503bfee6a90df9229a6038c90160bc0e39e1e80ca58dfb8d3.jpg", "img_caption": ["Figure 2: (left) L2 errors for 17th layer of LLAMA 2 7B with different representations. Full model perplexity on WikiText-2 is reported without finetuning (middle) and with fine-tuning (right). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "We run all three experiments on LLAMA 2 7B model [69], calibrating on the RedPajama [13] dataset that best approximates the original pre-training data. When evaluating single layer errors, we report the L2 error in attention query projection outputs of a fixed transformer block, with other blocks exhibiting similar behavior. For full model evaluation, we report quantized model perplexity on WikiText-2 [45] dataset. We use the same data splits and preprocessing as in most recent PTQ works [22, 42, 18, 70, 21, 71], including the biased preprocessing step that we mentioned in Section 3.4. For fine-tuning, we train continuous parameters only, using the approach from [71]. To compare these diverse representations, we evaluate their quantization errors as a function of average number of bits per parameter. To get a diverse set of bits per parameter, we vary the hyperparameters such as wbits, block size, codebook and group size for vector quantization and the rate of outliers. ", "page_idx": 7}, {"type": "text", "text": "Figure 2 summarizes our findings. Overall, vector quantization methods (VQ, QuIP# and AQLM) outperform their scalar counterparts. Outliers and low-rank compensation both reduce error, but this improvement comes at the cost of extra bits per parameter. Interestingly, the improvement from outliers is significantly smaller when both methods have access to fine-tuning. Likewise, the improvement from using low-rank adapters also diminishes when comparing fine-tuned models, to a point where it no longer justifies the increase in model size. We provide a more detailed breakdown of results and hyperparameter configurations in Appendix J. ", "page_idx": 7}, {"type": "text", "text": "Our main takeaway is that for sub 2 bits per parameter, the vector quantization (VQ) representation can achieve near-optimal quantization accuracy, whether or not it uses outliers, LoRC or incoherence processing. Naturally, this does not reduce the value of prior works since they were designed for different scenarios, typically with a higher number of bits per parameter. ", "page_idx": 7}, {"type": "text", "text": "4.2 Evaluating Fine-tuning Algorithms ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Next, we compare different fine-tuning strategies and ablate our PV-tuning protocol. We design our protocol to be representation-agnostic, i.e. compatible with different quantized representations. To showcase this, we pick three methods from the previous section: GPTQ, VQ and AQLM. ", "page_idx": 7}, {"type": "text", "text": "These methods differ not only in their weight representations, but also in how they search for the optimal codes. Namely, GPTQ can scale the target weight and round it to nearest 2-bit integer. In turn, VQ quantizes weights as a group and must find the nearest vector from its codebook, and AQLM uses a multi-step beam search procedure to choose the best combination of codes from both codebooks. Our PV-Tuning implementation uses these search algorithms during the subspace linearized V step (find_nearest in Alg. 3). We describe the full PV configuration for each method in Appendix K. ", "page_idx": 7}, {"type": "text", "text": "We compare PV tuning against several popular fine-tuning regimens found in the literature. Our first baseline is fine-tuning only continuous parameters, e.g., codebooks or input/output embeddings [71, 74]. The second baseline is training with Straight Through Estimation (STE) [75, 77]. We also test stochastic rounding as described in Appendix E.2. Finally, we evaluate PV tuning combined with STE, but otherwise the same configuration. We set the subspace size $\\tau$ equal to the number of weights such that the update satisfies $\\|{\\boldsymbol{x}}^{k\\mp1}-{\\boldsymbol{x}}^{k}\\|/\\|{\\boldsymbol{x}}^{k}\\|\\leq0.01$ , also known as known as trust ratio [81]. ", "page_idx": 7}, {"type": "text", "text": "The results in Table 1 show that PV-Tuning consistently finds better quantized models, with STE coming consistently second. We explore this further by combining subspace updates with STE, which leads to slightly better perplexity and accuracy in most (but not all) setups. ", "page_idx": 7}, {"type": "table", "img_path": "YvA8UF0I37/tmp/1965699435f80e704792167fccb129c5edafafe1e2c22f2c193f129467fc9410.jpg", "table_caption": ["Table 1: Comparing different fine-tuning strategies for VQ, GPTQ and AQLM on LLAMA 2 7B in terms of perplexity on WikiText-2, C4 and average zero-shot accuracy on tasks from Section 4.3. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "PV-Tuning over QuIP# In addition to these three configurations, we also apply PV-tuning to QuIP# [71] \u2014 a modification of vector quantization that applies Randomized Hadamard Transform (RHT) before quantization and uses fixed lattices instead of learned codebooks. We experiment with Llama-2 7B model quantized with QuIP# to 2 bits per weight and found that it is possible to significantly improve the model through PV-Tuning. For instance, PV-tuning improves WikiText-2 perplexity from 6.19 (QuIP# with built-in continuous fine-tuning) to 5.71 (PV-Tuning $^+$ STE). Since original 16-bit model has a perplexity of 5.13, this corresponds to almost halving the quantization error in terms of perplexity. We report additional details for QuIP# with PV-Tuning and full evaluation results in Appendix L and include it to Table 2 as \u201cQuIP#+PV\u201d. ", "page_idx": 8}, {"type": "text", "text": "On the choice of hyperparameters for 1-bit vector quantization. There are several possible hyperparameter configurations for vector quantization (VQ) that fall into 1-1.1 bit range. One can either use larger codebooks for longer groups (vectors), or smaller codebooks for shorter groups accordingly. In our main evaluations, we quantized vectors of 16 consecutive weights with 14-16 bit codebooks to fit into the desired bitwidth. However, we later found that it is more advantageous to choose smaller groups as well as codebooks. We found that 8-bit code per 8 weights outperforms 14-bit code per 16 weights despite having near-identical bitwidth (due to smaller codebooks). We report this configuration as \u201cPV (gs8)\u201d in Table 2 and provide additional experiments in Appendix M. ", "page_idx": 8}, {"type": "text", "text": "4.3 Large-scale Evaluation & Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Finally, we evaluate the resulting PV algorithm with a vector quantization backbone and KL objective on a range of popular LLM models. For this section, our goal is to evaluate our approach holistically for different models and target bit-widths, comparing against the best known baselines in common settings. To that end, we evaluate on LLAMA 2 & 3 [69], MISTRAL 7B [34] and PHI-3 Mini-4kInstruct [1] at 1\u20132.5 bits per parameter (averaged over all transformer layers). ", "page_idx": 8}, {"type": "text", "text": "We report perplexity on WikiText-2 [45] and C4 [54] validation sets, zero-shot accuracy on WinoGrande [60], PiQA [67], HellaSwag [83], ARC-easy and ARC-challenge [12] via the LM Eval Harness [24]. We follow the exact evaluation setup from GPTQ [22]. We compare against QuIP [70], BiLLM [32], PB-LLM [62], DB-LLM [10], AQLM [21], OneBit [77], QuIP# [71], the latter three using fine-tuning. For LLAMA 3, we use baselines from [33] and re-evaluate perplexity in our setup. ", "page_idx": 8}, {"type": "table", "img_path": "YvA8UF0I37/tmp/8165949803954b36c13eb0c2ac20049d2dbf2c8adf3a9e44f483746a055c8b95.jpg", "table_caption": ["Table 2: Quantized model perplexity on WikiText-2\u2193[45] & $\\mathbf{C4}\\downarrow$ [54] and the Average\u2191accuracy on 5 zero-shot tasks [24] for various models and bitwidths. Arrows $\\uparrow/\\downarrow$ mean higher / lower is better. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 2 summarizes our findings: PV-tuning with vector quantization outperforms all known methods for 1- and 2-bit per weight. The closest competitors on LLAMA 2 are QuIP#, AQLM and OneBit, all of which use fine-tuning. The improvements on LLAMA 3 are also remarkable as this model is notoriously hard to compress [33]. We report additional evaluations in Appendix N. ", "page_idx": 9}, {"type": "text", "text": "Pareto-optimality. A key practical question concerns obtaining optimal quality for the target model size, where a smaller model compressed to 3-4 bits often dominates a larger model compressed to 1-bit. The best known Pareto-optimal bit-width for Llama 2 is 2.5 [21]: compressing a larger model to less than 2.5 bits per weight is inferior to a smaller model quantized to the same total number of bytes. From this perspective, PV-tuning pushes the Pareto-optimal frontier for LLAMA 2 to 2.0 bits. This is easiest to see in Table 12: a 2-bit 13B model outperforms any 7B quantization and is comparable with the 16-bit 7B model. The same holds for the 2-bit 70B model. ", "page_idx": 9}, {"type": "text", "text": "Fine-tuning efficiency. One limitation of our algorithm is that it requires more compute and memory during the fine-tuning procedure. The 7B models can be fine-tuned on a single GPU, our 70B runs require a server with $8\\!\\times\\!A100$ or rely on RAM offloading. PV-Tuning shares this drawback with prior methods based on STE [21, 71], as both methods need gradients w.r.t. dequantized weights. Our longest training run took 2 days on 8 GPUs to outperform all baselines and 8 days to fully converge. ", "page_idx": 9}, {"type": "text", "text": "Inference speed. PV-Tuning does not change the underlying compressed representation, allowing us to reuse existing high-performance inference kernels. Specifically, $\\mathrm{VQ+PV}$ can reuse efficient kernels from [21, 71], while GPTQ $+\\mathrm{PV}$ can use ExLlamaV2 kernels [14]. We report these inference speed evaluations in Appendix O. From a practitioner\u2019s point of view, PV-Tuning can significantly improve the accuracy of extreme (1-2 bit) quantized models, making it possible to deploy large LLMs on resource-constrained devices. As a proof of concept, we developed specialized inference engines for running vector-quantized models with PV-Tuning on mobile devices7 or in the browser8. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Limitations. We focused our effort on evaluating PV-Tuning with multiple setups and models, but spent relatively little effort tuning our algorithm for each specific setup. For instance, we always use constant learning rate and $\\tau$ with no schedule, and always train on the same data. While this shows robustness of PV-Tuning, it also means that our results may be improved with better hyperparameters. For instance, Appendix M shows how PV-Tuning with 1-bit vector quantization can be improved by choosing smaller vector sizes, while Appendix L suggests that PV-Tuning can dramatically improve models quantized with QuIP# and may similarly be applied to other quantized representations. Furthermore, the algorithm could achieve better accuracy by simply training longer and on more data. ", "page_idx": 9}, {"type": "text", "text": "Future work. This work opens several new research directions. The first is about how to choose $S^{k}$ : while we found that a greedy strategy works in practice, there may be fundamentally better ways. Another direction is applying PV-Tuning to other quantization niches: our evaluation focuses on extreme weight-only quantization, but the proposed algorithm can be used in weight $^+$ activation setting or KV cache quantization. Overall, PV-Tuning shows how an insight from optimization theory can improve LLM quantization and we are excited to see how this develops in future research. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Authors would like to thank Vage Egiazarian, Andrei Panferov and Ruslan Svirschevski for their help and advice on AQLM codebase and running large-scale experiments. We also thank Philip Zmushko and Artem Fedorov for helpful discussions during the early stages of our research. The research of Kai Yi, Konstantin Burlachenko, and Peter Richt\u00e1rik reported in this publication was supported by funding from King Abdullah University of Science and Technology (KAUST) \u2013 Center of Excellence for Generative AI, under award number 5940. We would also like to thank our NeurIPS reviewers for their helpful suggestions, we specifically highlight p3Lv\u2019s suggestions to consider smaller codebook sizes and evaluate PV-Tuning with QuIP#, both of which produced interesting findings. Finally, we thank the open-source contributors from llama.cpp9 and the LocalLlama10 community for discussions and inspirations on practical use cases of quantized language models, and in particular, Yalda Shabanzadeh and Arthur Aardvark for their help with improving the codebase. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] M. Abdin, S. A. Jacobs, A. A. Awan, J. Aneja, A. Awadallah, H. Awadalla, N. Bach, A. Bahree, A. Bakhtiari, H. Behl, A. Benhaim, M. Bilenko, J. Bjorck, S. Bubeck, M. Cai, C. C. T. Mendes, W. Chen, V. Chaudhary, P. Chopra, A. D. Giorno, G. de Rosa, M. Dixon, R. Eldan, D. Iter, A. Garg, A. Goswami, S. Gunasekar, E. Haider, J. Hao, R. J. Hewett, J. Huynh, M. Javaheripi, X. Jin, P. Kauffmann, N. Karampatziakis, D. Kim, M. Khademi, L. Kurilenko, J. R. Lee, Y. T. Lee, Y. Li, C. Liang, W. Liu, E. Lin, Z. Lin, P. Madan, A. Mitra, H. Modi, A. Nguyen, B. Norick, B. Patra, D. Perez-Becker, T. Portet, R. Pryzant, H. Qin, M. Radmilac, C. Rosset, S. Roy, O. Ruwase, O. Saarikivi, A. Saied, A. Salim, M. Santacroce, S. Shah, N. Shang, H. Sharma, X. Song, M. Tanaka, X. Wang, R. Ward, G. Wang, P. Witte, M. Wyatt, C. Xu, J. Xu, S. Yadav, F. Yang, Z. Yang, D. Yu, C. Zhang, C. Zhang, J. Zhang, L. L. Zhang, Y. Zhang, Y. Zhang, Y. Zhang, and X. Zhou. Phi-3 technical report: A highly capable language model locally on your phone, 2024.   \n[2] D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic. Qsgd: Communication-efficient sgd via gradient quantization and encoding. Advances in neural information processing systems, 30, 2017.   \n[3] D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic. QSGD: Randomized quantization for communication-efficient stochastic gradient descent. In Conference on Neural Information Processing Systems (NeurIPS), 2017.   \n[4] D. Alistarh, T. Hoefler, M. Johansson, S. Khirirat, N. Konstantinov, and C. Renggli. The convergence of sparsified gradient methods. In Advances in Neural Information Processing Systems, 2018.   \n[5] A. Babenko and V. Lempitsky. Additive quantization for extreme vector compression. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 931\u2013938, 2014.   \n[6] Y. Bengio, N. L\u00e9onard, and A. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.   \n[7] A. Beznosikov, S. Horv\u00e1th, P. Richt\u00e1rik, and M. Safaryan. On biased compression for distributed learning. arXiv:2002.12410, 2020.   \n[8] V. Bo\u017ea. Fast and optimal weight update for pruned large language models, 2024.   \n[9] J. Chee, Y. Cai, V. Kuleshov, and C. D. Sa. Quip: 2-bit quantization of large language models with guarantees, 2023.   \n[10] H. Chen, C. Lv, L. Ding, H. Qin, X. Zhou, Y. Ding, X. Liu, M. Zhang, J. Guo, X. Liu, and D. Tao. Db-llm: Accurate dual-binarization for efficient llms, 2024.   \n[11] S. Chen, W. Wang, and S. J. Pan. Metaquant: Learning to quantize by learning to penetrate non-differentiable quantization. Advances in Neural Information Processing Systems, 32, 2019.   \n[12] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.   \n[13] T. Computer. Redpajama: an open dataset for training large language models, October 2023.   \n[14] E. contributors. ExLlamaV2. A fast inference library for running LLMs locally on modern consumer-class GPUs. Open-source library developed by turboderp and controbutors.   \n[15] M. Courbariaux, Y. Bengio, and J.-P. David. Training deep neural networks with low precision multiplications. arXiv preprint arXiv:1412.7024, 2014.   \n[16] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society: series B (methodological), 39(1):1\u201322, 1977.   \n[17] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. LLM.int8(): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, 2022.   \n[18] T. Dettmers, R. Svirschevski, V. Egiazarian, D. Kuznedelev, E. Frantar, S. Ashkboos, A. Borzunov, T. Hoefler, and D. Alistarh. Spqr: A sparse-quantized representation for nearlossless llm weight compression. arXiv preprint arXiv:2306.03078, 2023.   \n[19] T. Dettmers and T. von Koeller. Accessible large language models via k-bit quantization for pytorch.   \n[20] T. Dettmers and L. Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv preprint arXiv:2212.09720, 2022.   \n[21] V. Egiazarian, A. Panferov, D. Kuznedelev, E. Frantar, A. Babenko, and D. Alistarh. Extreme compression of large language models via additive quantization. arXiv preprint arXiv:2401.06118, 2024.   \n[22] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.   \n[23] Y. Freund and R. E. Schapire. Large margin classification using the perceptron algorithm. In Proceedings of the eleventh annual conference on Computational learning theory, pages 209\u2013217, 1998.   \n[24] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPof,i C. Foster, L. Golding, J. Hsu, K. McDonell, N. Muennighoff, J. Phang, L. Reynolds, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. A framework for few-shot language model evaluation, Sept. 2021.   \n[25] A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer. A survey of quantization methods for efficient neural network inference. arXiv preprint arXiv:2103.13630, 2021.   \n[26] R. M. Gower and P. Richt\u00e1rik. Stochastic dual ascent for solving linear systems. arXiv:1512.06890, 2015.   \n[27] A. Griewank and A. Walther. Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation. ACM Transactions on Mathematical Software (TOMS), 26(1):19\u201345, 2000.   \n[28] H. Guo, P. Greengard, E. P. Xing, and Y. Kim. Lq-lora: Low-rank plus quantized matrix decomposition for efficient language model finetuning, 2024.   \n[29] S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan. Deep learning with limited numerical precision. In International conference on machine learning, pages 1737\u20131746. PMLR, 2015.   \n[30] K. Helwegen, J. Widdicombe, L. Geiger, Z. Liu, K.-T. Cheng, and R. Nusselder. Latent weights do not exist: Rethinking binarized neural network optimization. Advances in neural information processing systems, 32, 2019.   \n[31] G. Hinton. Neural networks for machine learning, coursera (video lectures)., 2012.   \n[32] W. Huang, Y. Liu, H. Qin, Y. Li, S. Zhang, X. Liu, M. Magno, and X. Qi. Billm: Pushing the limit of post-training quantization for llms. arXiv preprint arXiv:2402.04291, 2024.   \n[33] W. Huang, X. Ma, H. Qin, X. Zheng, C. Lv, H. Chen, J. Luo, X. Qi, X. Liu, and M. Magno. How good are low-bit quantized llama3 models? an empirical study, 2024.   \n[34] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.   \n[35] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.   \n[36] S. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li, S. Shen, M. W. Mahoney, and K. Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023.   \n[37] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations (ICLR), 2015.   \n[38] D. Kozak, S. Becker, A. Doostan, and L. Tenorio. Stochastic subspace descent. arXiv preprint arXiv:1904.01145, 2019.   \n[39] C. Lee, J. Jin, T. Kim, H. Kim, and E. Park. Owq: Outlier-aware weight quantization for efficient fine-tuning and inference of large language models, 2024.   \n[40] S. Li, Y. Zhao, R. Varma, O. Salpekar, P. Noordhuis, T. Li, A. Paszke, J. Smith, B. Vaughan, P. Damania, and S. Chintala. Pytorch distributed: Experiences on accelerating data parallel training, 2020.   \n[41] Y. Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, and Y. T. Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023.   \n[42] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.   \n[43] Z.-Q. Luo and P. Tseng. On the convergence of the coordinate descent method for convex differentiable minimization. Journal of Optimization Theory and Applications, 72(1):7\u201335, 1992.   \n[44] S. Ma, H. Wang, L. Ma, L. Wang, W. Wang, S. Huang, L. Dong, R. Wang, J. Xue, and F. Wei. The era of 1-bit llms: All large language models are in 1.58 bits, 2024.   \n[45] S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.   \n[46] X. Miao, G. Oliaro, Z. Zhang, X. Cheng, Z. Wang, R. Y. Y. Wong, Z. Chen, D. Arfeen, R. Abhyankar, and Z. Jia. Specinfer: Accelerating generative llm serving with speculative inference and token tree verification, 2023.   \n[47] R. Mises and H. Pollaczek-Geiringer. Praktische verfahren der gleichungsaufl\u00f6sung. ZAMMJournal of Applied Mathematics and Mechanics/Zeitschrift f\u00fcr Angewandte Mathematik und Mechanik, 9(1):58\u201377, 1929.   \n[48] J. Mockus. On bayesian methods for seeking the extremum. In Optimization Techniques, 1974.   \n[49] M. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, and T. Blankevoort. Up or down? Adaptive rounding for post-training quantization. In International Conference on Machine Learning (ICML), 2020.   \n[50] G. Park, B. Park, S. J. Kwon, B. Kim, Y. Lee, and D. Lee. nuQmm: Quantized matmul for efficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557, 2022.   \n[51] J. Park and E. K. Ryu. Exact optimal accelerated complexity for fixed-point iterations. In Proceedings of the 39th International Conference on Machine Learning, 2022.   \n[52] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems (NeurIPS). Neural Information Processing Systems Foundation, 2019.   \n[53] A. Polino, R. Pascanu, and D. Alistarh. Model compression via distillation and quantization. arXiv preprint arXiv:1802.05668, 2018.   \n[54] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020.   \n[55] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: memory optimizations toward training trillion parameter models. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC \u201920. IEEE Press, 2020.   \n[56] I. Rechenberg. Evolutionsstrategie: Optimierung technischer Systeme nach Prinzipien der biologischen Evolution. Problemata (Stuttgart). Frommann-Holzboog, 1973.   \n[57] J. Ren, S. Rajbhandari, R. Y. Aminabadi, O. Ruwase, S. Yang, M. Zhang, D. Li, and Y. He. Zero-offload: Democratizing billion-scale model training, 2021.   \n[58] P. Richt\u00e1rik and M. Tak\u00e1\u02c7c. Parallel coordinate descent methods for big data optimization. Mathematical Programming, 156(1-2):433\u2013484, 2016.   \n[59] F. Rosenblatt. The perceptron - a perceiving and recognizing automaton. Technical Report 85-460-1, Cornell Aeronautical Laboratory, Ithaca, New York, January 1957.   \n[60] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: an adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99\u2013106, 2021.   \n[61] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili\u00b4c, D. Hesslow, R. Castagn\u00e9, A. S. Luccioni, F. Yvon, M. Gall\u00e9, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.   \n[62] Y. Shang, Z. Yuan, Q. Wu, and Z. Dong. Pb-llm: Partially binarized large language models, 2023.   \n[63] W. Shao, M. Chen, Z. Zhang, P. Xu, L. Zhao, Z. Li, K. Zhang, P. Gao, Y. Qiao, and P. Luo. Omniquant: Omnidirectionally calibrated quantization for large language models. arXiv preprint arXiv:2308.13137, 2023.   \n[64] A. Shekhovtsov and V. Yanush. Reintroducing straight-through estimators as principled methods for stochastic binary networks. In DAGM German Conference on Pattern Recognition, pages 111\u2013126. Springer, 2021.   \n[65] M. Spallanzani, G. P. Leonardi, and L. Benini. Training quantised neural networks with ste variants: the additive noise annealing algorithm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 470\u2013479, 2022.   \n[66] M. Sun, Z. Liu, A. Bair, and J. Z. Kolter. A simple and effective pruning approach for large language models, 2024.   \n[67] S. Tata and J. M. Patel. PiQA: An algebra for querying protein data sets. In International Conference on Scientific and Statistical Database Management, 2003.   \n[68] TII UAE. The Falcon family of large language models. https://huggingface.co/tiiuae/ falcon-40b, May 2023.   \n[69] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[70] A. Tseng, J. Chee, Q. Sun, V. Kuleshov, and C. De Sa. Quip#: Even better llm quantization with hadamard incoherence and lattice codebooks. arXiv preprint arXiv:2402.04396, 2024.   \n[71] A. Tseng, J. Chee, Q. Sun, V. Kuleshov, and C. D. Sa. Quip#: Even better llm quantization with hadamard incoherence and lattice codebooks, 2024.   \n[72] M. van Baalen, A. Kuzmin, M. Nagel, P. Couperus, C. Bastoul, E. Mahurin, T. Blankevoort, and P. Whatmough. Gptvq: The blessing of dimensionality for llm quantization. arXiv preprint arXiv:2402.15319, 2024.   \n[73] A. van den Oord, O. Vinyals, and k. kavukcuoglu. Neural discrete representation learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.   \n[74] H. Vanholder. Efficient inference with TensorRT. NVIDIA GTC On-Demand. Slides available at https://on-demand-gtc.gputechconf.com/gtcnew/sessionview.php?sessionName=23425- efficient+inference+with+tensorrt, 2017.   \n[75] H. Wang, S. Ma, L. Dong, S. Huang, H. Wang, L. Ma, F. Yang, R. Wang, Y. Wu, and F. Wei. Bitnet: Scaling 1-bit transformers for large language models. arXiv preprint arXiv:2310.11453, 2023.   \n[76] B. Widrow and M. A. Lehr. 30 years of adaptive neural networks: perceptron, madaline, and backpropagation. Proc. IEEE, 78:1415\u20131442, 1990.   \n[77] Y. Xu, X. Han, Z. Yang, S. Wang, Q. Zhu, Z. Liu, W. Liu, and W. Che. Onebit: Towards extremely low-bit large language models, 2024.   \n[78] Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.   \n[79] Z. Yao, X. Wu, C. Li, S. Youn, and Y. He. Exploring post-training quantization in llms from comprehensive study to low rank compensation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 19377\u201319385, 2024.   \n[80] P. Yin, J. Lyu, S. Zhang, S. Osher, Y. Qi, and J. Xin. Understanding straight-through estimator in training activation quantized neural nets. arXiv preprint arXiv:1903.05662, 2019.   \n[81] Y. You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli, X. Song, J. Demmel, K. Keutzer, and C.-J. Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. In International Conference on Learning Representations (ICLR), 2020.   \n[82] P. Zamirai, J. Zhang, C. R. Aberger, and C. De Sa. Revisiting bffloat16 training. 2020.   \n[83] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag: Can a machine really finish your sentence? In A. Korhonen, D. R. Traum, and L. M\u00e0rquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 4791\u20134800. Association for Computational Linguistics, 2019.   \n[84] C. Zhang, J. Cheng, G. A. Constantinides, and Y. Zhao. Lqer: Low-rank quantization error reconstruction for llms, 2024.   \n[85] P. Zhang, G. Zeng, T. Wang, and W. Lu. Tinyllama: An open-source small language model, 2024.   \n[86] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.   \n[87] S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1 Introduction 1 ", "page_idx": 15}, {"type": "text", "text": "2 Background 3 ", "page_idx": 15}, {"type": "text", "text": "3 Fine-Tuning Quantized Models 3 ", "page_idx": 15}, {"type": "text", "text": "3.1 Problem description . . 3   \n3.2 Linearized V step & gradient-based discrete updates . . 5   \n3.3 Linearized subspace V step . . . 6   \n3.4 Implementation details 7 ", "page_idx": 15}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "4.1 Evaluating quantized representations with finetuning . . 7   \n4.2 Evaluating Fine-tuning Algorithms . . . . 8   \n4.3 Large-scale Evaluation & Discussion . . 9 ", "page_idx": 15}, {"type": "text", "text": "5 Conclusions 10 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Proofs 18 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Proof of Theorem 3.1 18   \nA.2 Proof of Lemma 3.2 . 18   \nA.3 Proof of Lemma 3.3 . 18 ", "page_idx": 15}, {"type": "text", "text": "B Approximate PV Algorithm 19 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Approximate V step, variant 1 (non-accelerated) . . 19   \nB.2 Approximate V step, variant 2 (accelerated) 19 ", "page_idx": 15}, {"type": "text", "text": "C Generalization to Other Quantization Algorithms 20 ", "page_idx": 15}, {"type": "text", "text": "D Efficient Linearized V Step for Vector Quantization 20 ", "page_idx": 15}, {"type": "text", "text": "E Gradient-based Strategies for Training Quantized Models 21 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "E.1 Straight-through Gradient Estimation . . 22   \nE.2 Stochastic Rounding . . 22   \nE.3 Comparing Discrete Optimization Techniques . . 23 ", "page_idx": 15}, {"type": "text", "text": "F On $L$ -smoothness of LLM Objective in Sparse Subspaces 24 ", "page_idx": 15}, {"type": "text", "text": "G Calibration Data Matters 26 ", "page_idx": 15}, {"type": "text", "text": "H Additional Engineering Considerations 27 ", "page_idx": 15}, {"type": "text", "text": "I Additional evaluations of perplexity 28 ", "page_idx": 16}, {"type": "text", "text": "J Additional Details for Section 4.1 28 ", "page_idx": 16}, {"type": "text", "text": "K Additional Details for Section 4.2 30 ", "page_idx": 16}, {"type": "text", "text": "L PV-Tuning of QuIP# 31 ", "page_idx": 16}, {"type": "text", "text": "M On 1-bit Vector Quantization Options 31 ", "page_idx": 16}, {"type": "text", "text": "N Additional Details and Evaluations for Section 4.3 32 ", "page_idx": 16}, {"type": "text", "text": "O Inference Speed with Vector Quantization Kernels 32 ", "page_idx": 16}, {"type": "text", "text": "P The Choice of the Initial Point $x^{0}$ 35 ", "page_idx": 16}, {"type": "text", "text": "P.1 Clipping of $x^{\\star}$ 35   \nP.2 Random $x^{0}\\in\\mathbb{R}_{c}^{d}$ 35 ", "page_idx": 16}, {"type": "text", "text": "Q Small-Scale Experiments and Interpretation 35 ", "page_idx": 16}, {"type": "text", "text": "Q.1 Objective function . . 35   \nQ.2 Tiny-scale experiments ( $d=6$ , $c\\in[1,6]!$ ) 36   \nQ.3 Interpretation of $P(y)\\supseteq P(x^{k})$ 36   \nQ.4 Simple example of $P(y)\\supseteq P(x^{k})$ . 37   \nQ.5 Small-scale experiments ( $d=100$ , $c\\in[1,100])$ . . 37   \nQ.6 Linearized PV . . 38   \nQ.7 Linearized $\\mathrm{PV}+$ sparse updates . . . 39 ", "page_idx": 16}, {"type": "text", "text": "R $\\mathbf{PV}^{+}$ Algorithm 40 ", "page_idx": 16}, {"type": "text", "text": "S Broader Impact 41 ", "page_idx": 16}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A.1 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Part (i): First, by assumption, we know that $\\boldsymbol{x}^{0}\\in\\mathbb{R}_{\\leq c}^{d}$ . Assume that $\\boldsymbol{x}^{k}\\in\\mathbb{R}_{\\leq c}^{d}$ for some $k\\geq0$ . Since $P(y^{k})\\supseteq P(x^{k})$ , this implies that $\\boldsymbol{y}^{k}\\in\\mathbb{R}_{\\leq c}^{d}$ . Next, since $V(x^{k+1})\\subseteq{\\bar{V}}(y^{k})$ , we conclude that $\\boldsymbol{x}^{k+1}\\in\\mathbb{R}_{\\leq c}^{d}$ . The claim now follows by induction. ", "page_idx": 17}, {"type": "text", "text": "Part (ii): Since ", "page_idx": 17}, {"type": "equation", "text": "$$\ny^{k}=\\arg\\operatorname*{min}_{y\\in\\mathbb{R}^{d}}\\{\\phi(y)\\ :\\ P(y)\\supseteq P(x^{k})\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and because $y\\,=\\,x^{k}$ satisfies the constraint $P(y)\\,\\equiv\\,P(x^{k})$ , we conclude that $\\phi(y^{k})\\;\\leq\\;\\phi(x^{k})$ . Further, since ", "page_idx": 17}, {"type": "equation", "text": "$$\nx^{k+1}=\\arg\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\{\\phi(x)\\ :\\ V(x)\\subseteq V(y^{k})\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and because $x=y^{k}$ satisfies the constraint $V(y)\\subseteq V(y^{k})$ , we conclude that $\\phi(x^{k+1})\\leq\\phi(y^{k})$ . In summary, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\phi(x^{k+1})\\leq\\phi(y^{k})\\leq\\phi(x^{k}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Part (iii): In view of part (ii), the sequence $\\{\\phi({\\boldsymbol{x}}^{k})\\}_{k=0}^{\\infty}$ is non-increasing. By assumption, it is bounded below. Hence, it converges to its infimum: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\to\\infty}\\phi(x^{k})=\\operatorname*{inf}_{k\\in\\{0,1,\\ldots\\}}\\phi(x^{k}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A.2 Proof of Lemma 3.2 ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Delta_{V,\\hat{\\sigma}_{v}^{*}}(g)}&{:=\\,\\,\\frac{\\mathrm{arg~inin}}{\\epsilon\\sqrt{\\epsilon}\\Delta_{v}^{2}}\\,\\Big\\{\\hat{\\sigma}_{v}(x):V(x)\\subseteq V(y)\\Big\\}}\\\\ &{=\\,\\,\\frac{\\mathrm{arg~inin}}{\\epsilon\\sqrt{\\epsilon}\\Delta_{v}^{2}}\\,\\bigg\\{\\phi(y)+\\{\\nabla\\phi(y),c-y\\}+\\frac{L}{2}\\|x-y\\|^{2}:V(x)\\subseteq V(y)\\bigg\\}}\\\\ &{=\\,\\,\\frac{\\mathrm{arg~inin}}{\\epsilon\\sqrt{\\epsilon}\\Delta_{v}^{2}}\\,\\bigg\\{\\langle\\nabla\\phi(y),\\Delta_{v}\\rangle+\\frac{L}{2}\\|x-y\\|^{2}:V(x)\\subseteq V(y)\\bigg\\}}\\\\ &{=\\,\\,\\frac{\\mathrm{arg~inin}}{\\epsilon\\sqrt{\\epsilon}\\Delta_{v}^{2}}\\,\\bigg\\{2\\bigg\\langle\\frac{1}{L}\\nabla\\phi(y),\\Delta_{v}\\bigg\\rangle+\\|x-y\\|^{2}:V(x)\\subseteq V(y)\\bigg\\}}\\\\ &{=\\,\\,\\frac{\\mathrm{arg~inin}}{\\epsilon\\sqrt{\\epsilon}\\Delta_{v}^{2}}\\,,}\\\\ &{=\\,\\,\\frac{\\mathrm{arg~inin}}{\\epsilon\\sqrt{\\epsilon}\\Delta_{v}^{2}}\\,\\bigg\\{\\Big\\|x-\\Big(y-\\frac{1}{L}\\nabla\\phi(y)\\Big)\\Big\\|^{2}:V(x)\\subseteq V(y)\\bigg\\}}\\\\ &{=\\,\\,\\frac{\\mathrm{arg~inin}}{\\epsilon\\sqrt{\\epsilon}\\Delta_{v}}\\,\\bigg\\{\\hat{\\sigma}_{v}(x):V(x)\\subseteq V(y)\\bigg\\}}\\\\ &{=\\,\\,M_{V,\\hat{\\sigma}_{v}}(y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A.3 Proof of Lemma 3.3 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "First, note that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\phi(\\hat{x})\\quad}&{\\stackrel{\\mathrm{\\scriptsize(B)}}{=}}&{\\phi\\left(M_{V,\\widehat{\\phi}_{y}}(y)\\right)}\\\\ &{\\stackrel{\\mathrm{Lemma~3.2}}{=}}&{\\phi\\left(M_{V,\\widetilde{\\phi}_{y}}(y)\\right)}\\\\ &{\\stackrel{\\mathrm{\\scriptsize(B)}}{=}}&{\\underset{x\\in\\mathbb{R}^{d}}{\\operatorname*{min}}\\{\\widetilde{\\phi}_{y}(x)\\,:\\,V(x)\\subseteq V(y)\\}}\\\\ &{\\stackrel{\\mathrm{\\scriptsize(4)}}{=}}&{\\underset{x\\in\\mathbb{R}^{d}}{\\operatorname*{min}}\\left\\{\\phi(y)+\\langle\\nabla\\phi(y),x-y\\rangle+\\frac{L}{2}\\|x-y\\|^{2}\\,:\\,V(x)\\subseteq V(y)\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $y\\in\\mathbb{R}_{\\leq c}^{d}$ , any $x\\in\\mathbb{R}^{d}$ satisfying $V(x)\\subseteq V(y)$ must also satisfy $\\boldsymbol{x}\\in\\ensuremath{\\mathbb{R}}_{\\leq c}^{d}$ . So, in view of L-smoothness of \u03d5 on Rd\u2264c, we can bound the last expression in (9) from below via ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\phi(\\hat{x})\\stackrel{{\\mathrm{~\\tiny~\\?~+7~}}}{\\geq}}&{\\underset{x\\in\\mathbb{R}^{d}}{\\operatorname*{min}}\\left\\{\\phi(x)\\;:\\;V(x)\\subseteq V(y)\\right\\}}\\\\ {\\stackrel{(3)}{=}}&{\\phi\\left(M_{V,\\phi}(y)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, since $x=y$ satisfies the constraint $V(x)\\subseteq V(y)$ , we can upper bound the same expression via ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{\\phi(\\hat{x})}&{\\stackrel{(9)}{\\leq}}&{\\phi(y)+\\langle\\nabla\\phi(y),y-y\\rangle+\\displaystyle\\frac{L}{2}{\\|y-y\\|}^{2}}\\\\ &{=}&{\\phi(y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "B Approximate PV Algorithm ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We now introduce the pseudocode of an approximate PV meta-algorithm; the idea is to replace the P and $\\mathrm{v}$ steps with some approximate computations to be defined later. ", "page_idx": 18}, {"type": "table", "img_path": "YvA8UF0I37/tmp/61101ad74aaa44ec3b582a61b58204bf089f0a86d7b0be9775672b5802372ac4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Next, we describe two new approximations of the $\\mathrm{v}$ step. ", "page_idx": 18}, {"type": "text", "text": "B.1 Approximate $\\mathbf{V}$ step, variant 1 (non-accelerated) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We now describe an algorithm computing an approximation to $M_{V,\\phi}(y)$ : ", "page_idx": 18}, {"type": "text", "text": "1. Start with some $y\\in\\mathbb{R}_{c}^{d}$ and choose sufficiently large $L>0$ , number of iterations $T$   \n2. Set $z^{0}=y$   \n3. For $t=0,\\dots,T-1$ iterate: (i) Define $\\begin{array}{r}{\\widehat{\\phi}_{z^{t}}(\\cdot):=\\left\\|\\cdot-\\left(z^{t}-\\frac{1}{L}\\nabla\\phi(z^{t})\\right)\\right\\|^{2}}\\end{array}$ (ii) Set $z^{t+1}=M_{V,\\widehat{\\phi}_{z}t}\\left(z^{t}\\right)$   \n4. Output: $z^{T}$ ", "page_idx": 18}, {"type": "text", "text": "The method is constructed so that $z^{T}\\approx M_{V,\\phi}(y)$ . If use this subroutine with $T=1$ to approximate the $\\mathrm{v}$ step in the PV method, we recover what we earlier called the linearized PV method. Choosing sufficiently large $T\\geq2$ may be advantageous. ", "page_idx": 18}, {"type": "text", "text": "B.2 Approximate V step, variant 2 (accelerated) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We now describe a different algorithm for computing an approximation to $M_{V,\\phi}(y)$ : ", "page_idx": 18}, {"type": "text", "text": "1. Start with some $y\\in\\mathbb{R}_{c}^{d}$ and choose sufficiently large $L>0$ , number of iterations $T$   \n2. Choose a suitable decreasing sequence of positive scalars $\\left\\{\\alpha_{t}\\right\\}$ , with $\\alpha_{0}~=~1$ and   \n$\\operatorname*{lim}_{t\\to\\infty}\\alpha_{t}=0$   \n3. Set $z^{0}=y$   \n4. For $t=0,\\dots,T-1$ iterate: ", "page_idx": 18}, {"type": "text", "text": "(i) Define $\\begin{array}{r}{\\widehat{\\phi}_{z^{t}}(\\cdot):=\\left\\|\\cdot-\\left(z^{t}-\\frac{1}{L}\\nabla\\phi(z^{t})\\right)\\right\\|^{2}}\\end{array}$ (ii) S $\\textrm{e t}z^{t+1}=\\left(1-\\alpha_{t}\\right)M_{V,\\widehat{\\phi}_{z}t}\\left(z^{t}\\right)+\\alpha_{t}z^{0}$ 5. Output: $z^{T}$ ", "page_idx": 19}, {"type": "text", "text": "The method is constructed so that $z^{T}\\approx M_{V,\\phi}(y)$ . This approach is based on Halpern acceleration of fixed point methods [51], and as such, may be sometimes effective. ", "page_idx": 19}, {"type": "text", "text": "C Generalization to Other Quantization Algorithms ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Section 3.1, we define $\\mathbb{R}_{\\leq c}^{d}$ as a set of vectors with at most $c$ unique items. This translates to the idea of $\\mathbf{k}$ -means quantization, a scalar nonlinear quantization where each weight is rounded to one of $c$ centroids found by clustering the weights. Below, we show how this can be generalized to linear quantization, vector quantization, additive quantization, and others. ", "page_idx": 19}, {"type": "text", "text": "Linear quantization is the most basic and widely used type of quantization where weights are stored as integers, possibly multiplied by a scale and added to a zero point. The simplest way to account for this quantization type is to declare that weight values are integers up to $c$ : $V(x)=(0,1,2,...,c-1)$ . After that, one can treat scales / zero points as an extra non-quantized parameter, similar to biases or layer-normalized scales. This extra parameter interacts with weights by multiplication or addition, and hence it can be updated by backprop, similarly to other non-quantized weights. Equivalently, once can declare that $V(x)=(0,s,\\bar{2}s,...,s\\cdot(c-1))$ for arbitrary $s\\,\\in\\,{\\mathcal{R}}$ . Both options lead to equivalent fine-tuning algorithms where the $\\mathrm{v}$ step does not change and the $\\mathbf{P}$ step has an additional condition. ", "page_idx": 19}, {"type": "text", "text": "Next, let us discuss vector quantization. Consider a quantization that splits $x$ into 2-dimensional groups (non-intersecting pairs of adjacent weights) and encodes these weights as one of $c\\ 2\\cdot$ dimensional codes that form its codebook. This can be viewed as two sets of weights (odd and even) quantized with scalar quantization, except that values in the two sets have the same partitioning $P(x)$ . In other words, if two values in the odd half belong to the same partition, the corresponding values in the other half also belong to the same partition, though the values themselves can be different. Alternatively, one can simply write down a version of $\\mathbb{R}_{\\leq c}^{d}$ , where $V(\\cdot)$ is a set of 2-dimensional vectors, not scalars. Likewise, higher-dimensional vector quantization translates to higher-dimensional items in $V(\\cdot)$ . ", "page_idx": 19}, {"type": "text", "text": "Both the $\\mathbf{P}$ and $\\mathrm{v}$ steps for vector-quantized weights follow the same general principle: P-step can be approximated by backprop with slightly more trainable parameters. In turn, the $\\mathrm{v}$ step can be done by trying all values in $\\mathbf{V}(\\mathbf{x})$ and selecting the one with the lowest $\\widehat{\\phi}(\\cdot)$ . A more compute-efficient version of the $\\mathrm{v}$ step for this case is described in Appendix D. ", "page_idx": 19}, {"type": "text", "text": "RVQ and Additive Quantization can be treated as learning two separate sets of vector-quantized parameters. However, a more efficient way would be to run the $\\mathrm{v}$ step to find the best combination of codes via beam search [5]. ", "page_idx": 19}, {"type": "text", "text": "Quantization with sparse outliers [17, 18, 42] can be seen as learning two distinct matrices with different definitions of $\\mathbb{R}_{c}^{d}$ : one is quantized and the other is sparse (for outliers). Similarly, quantized weights with low-rank adapters (e.g. [28]) can treat the adapter as an additional non-quantized parameter for the $\\mathbf{P}$ step. This makes PV-tuning potentially extensible to neural network pruning. ", "page_idx": 19}, {"type": "text", "text": "D Efficient Linearized V Step for Vector Quantization ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "As we describe in Section 3.2, the linearized V step minimizes the squared error between quantized weights and the updated \u201ctarget\u201d weights. Here, we explain how one can compute and minimize the squared error efficiently in practice. To simplify the notation for this section, we define the objective as $\\|{\\boldsymbol{x}}-{\\boldsymbol{B}}\\|^{2}$ where $B$ is the target vector set by the linearized $\\mathrm{v}$ step. Following the definition of squared $L2$ norm, this objective can be re-written as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|x-B\\|^{2}=\\|x\\|^{2}-2\\langle x,B\\rangle+\\|B\\|^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Consider the first term: $\\begin{array}{r}{\\|\\boldsymbol{x}\\|^{2}=\\sum_{i=1}^{d}x_{i}^{2}}\\end{array}$ . Since $\\boldsymbol{x}\\in\\mathbb{R}_{\\leq c}^{d}$ , this term is a sum of at most $c$ unique terms. Abusing your notation, th is can be rewritten as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|x\\|^{2}=\\sum_{i}^{c}V_{i}(x)^{2}\\cdot|P_{i}(x)|,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $V_{i}(x)$ is $i$ -th unique element in $x$ and $|P_{i}(x)|$ is the number of such elements. The second term is also a sum of $c$ unique values: ", "page_idx": 20}, {"type": "equation", "text": "$$\n-2\\cdot\\left<x,B\\right>=-2\\sum_{i=1}^{d}x_{i}B_{i}=-2\\sum_{i=1}^{c}\\left[V_{i}(x)\\cdot\\sum_{i\\in P_{i}(x)}B_{i}\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The third term does not depend on $x$ . ", "page_idx": 20}, {"type": "text", "text": "If you know the objective for some given $x$ , you can efficiently compute $\\phi$ for all neighboring $\\hat{x}\\in\\mathcal{N}_{1}(x)$ where you only change one index. For the sake of formality, let us define the set of such neighboring $\\hat{x}$ as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{N}_{1}(x):=\\{\\hat{x}\\in\\mathbb{R}_{\\leq c}^{d}:V(\\hat{x})=V(x),\\:\\|x-\\hat{x}\\|_{0}=1\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Consider one $\\hat{x}\\in\\mathcal{N}_{1}(x)$ where only $k$ -th value changed (i.e. $x_{k}\\neq\\hat{x}_{k}$ ). Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\phi(\\hat{x})-\\phi(x)=\\|\\hat{x}\\|^{2}-\\|x\\|^{2}-2\\cdot\\langle\\hat{x}-x,B\\rangle+\\|B\\|^{2}-\\|B\\|^{2}}}\\\\ {{\\mathrm{}}}\\\\ {{\\phi(\\hat{x})-\\phi(x)=\\hat{x}_{k}^{2}-x_{k}^{2}-2\\cdot\\left(\\hat{x}_{k}-x_{k}\\right)\\cdot B_{k}+0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that, for any $\\forall\\hat{x}\\in\\mathcal{N}_{1}(x)$ , there are $c^{2}$ possible values for $\\hat{x}_{k}^{2}-x_{k}^{2}$ and another $c^{2}$ unique values for $2\\cdot\\left({\\hat{x}}_{k}-x_{k}\\right)\\cdot B_{k}$ , regardless of $d$ , since there are $c$ unique values in both $v$ and $\\hat{x}$ . This allows for an efficient local search algorithm: ", "page_idx": 20}, {"type": "text", "text": "1. Let $x^{0}$ be the input to $M_{P}$   \n2. Compute and save all $2c^{2}$ possible red and blue values   \n3. Compute $\\phi_{0}=\\phi(x^{0})$   \n4. for $\\mathbf{t}=0,\\,\\dots.$ :   \n5. for $\\hat{x}\\in\\mathcal{N}_{1}(x^{t})$ :   \n6. find $k:\\hat{x}_{k}\\neq x_{k}^{t}$ (there\u2019s only one such $k$ )   \n7. compute $\\phi(\\hat{x})=\\phi(x^{t})+\\hat{x}_{k}^{2}-x_{k}^{2}-2\\cdot(\\hat{x}_{k}-x_{k})\\cdot B_{k}$   \n8. $x^{t+1}:=\\operatorname*{arg\\,min}_{\\hat{x}\\in\\mathcal{N}_{1}(x^{t})}\\phi(\\hat{x})$ (minimum from array of pre-computed values) ", "page_idx": 20}, {"type": "text", "text": "In practice, this can be extended from greedy (local) search to semi-greedy beam search. These practical algorithms are described in AQ, LSQ, and ${\\mathrm{LSQ}}++$ . Algorithms for $\\|A(x-B)\\|^{2}$ are explained in AQLM and probably other works. ", "page_idx": 20}, {"type": "text", "text": "E Gradient-based Strategies for Training Quantized Models ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we overview possible solution to the general problem of training / fine-tuning neural networks with quantized weights. We focus on strategies that train by gradient descent with additional measures to deal with coarse-grained weights. ", "page_idx": 20}, {"type": "text", "text": "In principle, there are also gradient-free methods for quantized training, such as Evolution Strategies [56], Bayesian Optimization [48] and others. However, these gradient-free methods have so far not gained popularity for large language model quantization. Adapting these methods to the scale and dimensionality of LLMs would likely require extra effort. Thus, we leave these methods outside the scope of our work and focus on gradient-based optimization. ", "page_idx": 20}, {"type": "text", "text": "Reminder: gradient-based training of quantized models. We describe the general framework for training quantized weights in Sections 3.1 and 3.2. To summarize, the training algorithm computes the gradient w.r.t. de-quantized weights as though they were continuous, then uses these gradients to update continuous (P step) and discrete (V step) parameters. The core problem with this approach is that, when discrete parameters are very coarse (e.g. low-bit quantization), gradient updates are no longer large enough to make any changes and are lost to the \u201crounding error\u201d. We review two strategies for circumventing this problem: straight-through estimation and stochastic rounding. ", "page_idx": 21}, {"type": "text", "text": "E.1 Straight-through Gradient Estimation ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Straight-through gradient estimation is a technique for training neural networks with discrete components that ignores these discrete components during backpropagation. Its usage goes back to the Perceptron introduced by Rosenblatt [59]. There, the artificial neuron uses a step function as activation, but the training procedure treats this function as though it was identity. Subsequent works introduce variations of this idea, extend it to multi-layer networks [31], discuss its convergence properties [76, 23, 80]. Other works use straight-through estimation or similar techniques to training neural network with quantized weights [30, 64, 65]. ", "page_idx": 21}, {"type": "text", "text": "STE for LLM quantization. As we discuss in Section 2, straight-through estimation introduces an auxiliary non-quantized weight tensor that is updated using the gradients $\\nabla\\phi(y)$ w.r.t. quantized weights. The quantized weights are then updated to best approximate this auxiliary buffer, usually in terms of $L2$ error. As a result, if an update to $\\begin{array}{r}{y\\mathrm{~-~}\\frac{1}{L}\\dot{\\nabla}\\dot{\\phi}(y)}\\end{array}$ is not large enough to change the parameter, it is still accumulated in a straight-through \u201cbuffer\u201d. Eventually, the cumulative effect of several such updates will be large enough that $y^{k}$ will no longer be the solution to Equation (6). ", "page_idx": 21}, {"type": "text", "text": "This strategy prevents the algorithm from stalling, but it does so at the cost of convergence guarantees [80]. When applied to extreme LLM quantization (Section 4.2, straight-through estimation initially improves $y^{\\hat{k}}$ , but then stops improving and oscillates. We also tried several a variant of straight-through estimation [65] that introduce stochasticity to forward pass. When applied to extreme LLM quantization, this variant did not diverge like naive STE, but trained much slower and did not reach the same optimum as \u201cdeterministic\u201d STE. We attribute this to the fact that adding noise during training can slow down convergence, which also applies to stochastic rounding (Appendix E.2). ", "page_idx": 21}, {"type": "text", "text": "E.2 Stochastic Rounding ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Stochastic (or probabilistic) rounding [82, 87, 2, 29] is one of the techniques that can circumvent stalling when training low-precision weights. To recall, the linearized $\\mathrm{v}$ step (6) can be seen as rounding $\\begin{array}{r}{y^{+}:=y-\\frac{\\mathsf{T}}{L}\\nabla\\phi(\\dot{y})}\\end{array}$ to the nearest quantized weight in $\\mathbb{R}_{c}^{d}$ , which often happens to be $y$ itself. To circumvent the problem of rounding back to $y$ , one can instead round stochastically, to one of the two adjacent values that $y^{+}$ falls between. Let\u2019s denote these two adjacent values $x_{l}$ and $x_{r}$ for left and right. The probability of rounding is inversely proportional to the rounding error (distance), or, in terms of the objective, ", "page_idx": 21}, {"type": "equation", "text": "$$\np(\\mathrm{round\\;to}\\;x_{l})=\\frac{\\widehat{\\phi}(x_{l})^{-1/2}}{\\widehat{\\phi}(x_{l})^{-1/2}+\\widehat{\\phi}(x_{r})^{-1/2}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This way, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\underset{p(\\mathrm{round}\\,\\mathrm{to}\\,x)}{E}x=y-\\frac{1}{L}\\nabla\\phi(y).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The main drawback of stochastic rounding is t introduces noise, it changes the underlying optimization problem. Intuitively, if the optimal $x^{\\star}$ is adjacent to a significantly worse solution, the method may oscillate between rounding to either side. This rounding noise increases further as we consider lower quantization width. In Section 4.2 we exploit his phenomenon for real-world LLMs and find that stochastic rounding converges find significantly worse solutions, presumably because at every step, some portion of LLM weights will be rounded poorly. On top of that, when used for vector quantization, stochastic rounding is either intractable or biased. ", "page_idx": 21}, {"type": "text", "text": "Stochastic rounding for vector quantization. To recall, stochastic rounding for non-vector quantization needs to find two quantized values: the nearest neighbor above the solution, and the nearest neighbor below it. It will then round to either of the two values inversely proportionally to their rounding errors. ", "page_idx": 21}, {"type": "text", "text": "However, this intuition no longer works if you consider more complex quantization schemes such as vector quantization, additive quantization, quantized low-rank adapters, and others. In vector quantization, a group of weights is encoded jointly as one vector from a fixed set (usually called codebook or lattice). For simplicity, let us consider the case where the weight group size equals 2, i.e. weights are quantized in pairs. ", "page_idx": 22}, {"type": "text", "text": "For a pair of two eights, we can no longer rely on the fact that they have one neighbor from above and one from below. Instead, they may have any number of adjacent \"clusters\" they can be rounded to. Intuitively, a pair of weights is a point in 2-dimensional that can have neighbors from left, right, top, bottom, and any diagonals. Formally, to determine a full list of neighbors, we can run Delaunay triangulation on all vectors from the codebook (or lattice) plus the target vector that needs to be rounded, then find all points that share a triangle with the target vector. ", "page_idx": 22}, {"type": "text", "text": "Unfortunately, this procedure can be very expensive, especially for higher-dimensional vectors. A popular practical approximation to stochastic rounding for vector quantizations is to find K (e.g. 2) nearest vectors from the codebook, then use the probability formula from scalar stochastic rounding: ", "page_idx": 22}, {"type": "equation", "text": "$$\np(\\mathrm{round\\;to}\\;x_{i})=\\widehat{\\phi}(x_{i})^{-1/2}/(\\sum_{j}^{K}\\widehat{\\phi}(x_{j})^{-1/2})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "However, unlike the original stochastic rounding, this approximation does not guarantee that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathop{E}_{p(\\mathrm{round}\\,\\mathrm{to}\\,x_{i})}x_{i}=y-\\frac{1}{L}\\nabla\\phi(y).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For a (counter)example, if there is a high density of codes on one side of the target, all K (e.g. 2) nearest neighbors will be on the same side. As a result, this approximate stochastic rounding is biased and further changes the optimization result. ", "page_idx": 22}, {"type": "text", "text": "Stochastic rounding with temperature. When used for LLM quantization, the main problem with stochastic rounding is that it introduces noise to the training procedure. This is important because modern LLMs [69, 68, 1] typically train without dropout or similar noise layers. This is because the dataset is huge and the training suffers not from overfitting, but from not fitting the data enough. ", "page_idx": 22}, {"type": "text", "text": "Training with stochastic rounding makes optimization inherently noisy as if using dropout, making it harder to train. What is worse, low-bitwidth models produce more noise than high-bitwidth due to larger intervals between quantized values. This additional noise makes it difficult for the model to fit the training data tightly. For extreme 1-bit training, we often observed that the training objective (cross-entropy) would increase instead of decreasing due to sheer amount of rounding noise. ", "page_idx": 22}, {"type": "text", "text": "To combat this issue, we introduce stochastic rounding with temperature $\\tau$ (hyperparameter): ", "page_idx": 22}, {"type": "equation", "text": "$$\np(\\mathrm{round~to~}x_{l})=\\frac{\\widehat{\\phi}(x_{l})^{-1/(2\\tau)}}{\\widehat{\\phi}(x_{l})^{-1/(2\\tau)}+\\widehat{\\phi}(x_{r})^{-1/(2\\tau)}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Setting $\\tau<1$ results the algorithm keeping the original weights more often, which reduces the training noise at the cost of slower updates. In Table 1 (Section 4.2), we try $\\tau\\in\\{1,0.5,0.1,0.01\\}$ and choose the best result for each setup that involves stochastic rounding. It can be shown to converge as long as $\\tau$ is annealed, but it changes the underlying optimization problem similarly to dropout. In principle, it is possible to gradually anneal $\\tau$ during training to make the algorithm unbiased in the limit. ", "page_idx": 22}, {"type": "text", "text": "E.3 Comparing Discrete Optimization Techniques ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Finally, we can compare the two above strategies and our proposed approach from Section 3.3. ", "page_idx": 22}, {"type": "text", "text": "From the optimization perspective, the most popular variant of straight-through estimation is known to not converge to a stable solution. While, in practice, STE can still significantly improve model quality (see Table 1), it is still a heuristic. In turn, stochastic rounding can be seen as an additional source of noise to stochastic gradient descent which can converge to a stable solution when $\\tau$ is gradually reduced to 0. In contrast, subspace PV does not introduce noise or instability and does not require annealing. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "The most notable difference of PV-Tuning from STE is that the former tries to update all weights on every step, whereas our algorithm only updates a chosen subset. We give an explanation on why updating all weights is problematic at the end of Section 3.2. ", "page_idx": 23}, {"type": "text", "text": "From the efficiency perspective, training with straight-through estimation requires storing an additional set of buffers on device memory to accumulate weight updates. Unlike the quantized weights, these buffers need to be stored in higher precision (half or full) to accumulate smaller updates that would be lost on quantized buffers. As a result, straight-through estimation requires additional memory for fine-tuning. Stochastic rounding and subspace PV (w/o STE) do not need these buffers, but they still need to accumulate high precision gradients w.r.t. de-quantized weights and store optimizer statistics for those weights in higher precision. To summarize, all methods require significantly more memory than naive (P-only) fine-tuning, but straight-through estimation has higher memory overhead. ", "page_idx": 23}, {"type": "text", "text": "As for the computational overhead, both STE, stochastic rounding and subspace PV introduce additional computations and therefore increase step complexity. Of the three alternatives, the subspace PV algorithm is slightly faster since it only runs the discrete optimization on a small portion (subspace) of model weights per step, while stochastic rounding has somewhat higher overhead due to the complicated rounding procedure, especially for vector quantization. However, this overhead is small in practice: most of the LLM training time is spent accumulating the gradients on a large training batch, which is not affected by any of the three algorithms. In principle, it should be possible to reduce the compute / memory overhead both with technical improvements and better optimization algorithms, but we leave this investigation to future work. ", "page_idx": 23}, {"type": "text", "text": "F On $L$ -smoothness of LLM Objective in Sparse Subspaces ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The classical definition of $L$ -smoothness for differentiable function $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ represented by requirement ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\nabla f(x)-\\nabla f(y)\\|\\leq L\\|x-y\\|,\\qquad\\forall x,y\\in\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "If function $f(x)$ is twice continuously differentiable, then it is easy to show that function $f$ is $L$ - smooth if and only if $\\|\\nabla^{2}f(x)\\|\\leq L$ . If striving to find the minimum value of $L$ then via following the definition, one has to select $L$ as $L=\\operatorname*{max}_{x\\in\\mathbb{R}^{d}}\\left(\\|\\nabla^{2}f(x)\\|\\right)$ . ", "page_idx": 23}, {"type": "text", "text": "If the domain of function $f(x)$ is restricted to any subset $S\\subseteq\\mathbb{R}^{d}$ , then the global $L$ smooth constant can only decrease for a new function. It can be observed from the fact that $\\forall S\\subseteq\\mathbb{R}^{d}$ we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\nabla^{2}f(x)\\|=\\operatorname*{max}_{v\\in\\mathbb{R}^{d}\\backslash0}\\left(\\nabla^{2}f(x)\\cdot v/\\|v\\|\\right)\\geq\\operatorname*{max}_{v\\in S\\backslash0}\\left(\\nabla^{2}f(x)\\cdot v/\\|v\\|\\right),\\forall x\\in\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Sparse sub-spaces satisfy this requirement; therefore, this theoretical observation is valid in this circumstance. ", "page_idx": 23}, {"type": "text", "text": "Another observation is that the definition of $L$ smooth constant has a global notion. However, for practical purposes, for the first-order training algorithm what matters is $L$ -smoothness constant for the function $f(x)$ with the domain restricted to the trajectory of iterates only. Unfortunately, the training process iterates follow the prior unknown path in Rdspace. ", "page_idx": 23}, {"type": "text", "text": "Below we demonstrate two approximate schemas for evaluating $L$ -smoothness constant for a function with a domain (artificially) restricted to the trajectory induced by iterates generated by Gradient Descent (GD) for functions $f(x)$ with different subspace sizes and in general with different optimization trajectories, but with the same start iterate (model). We have performed 10 iterations of GD for training auto-regressive Llama-like LLama-160M [46] and TinyLlama-1.1B [85] models using automatic tokenizer from these models. We trained all linear layers in these models. The used step size for GD is $10^{-4}$ . ", "page_idx": 23}, {"type": "text", "text": "Schema I: Estimating $L$ along the trajectory of iterates without capturing local curvature. ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "After running GD for 10 iterations the $L$ smooth constant has been estimated along trajectory $s=\\{x_{1},x_{2},\\ldots,x_{10}\\}$ with approximated as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{L}:=\\operatorname*{max}_{x_{i},x_{j}\\in z,x_{i}\\neq x_{i}}\\left(\\frac{\\|\\nabla f(x_{i})-\\nabla f(x_{j})\\|}{\\|x_{i}-x_{j}\\|}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Results are presented in Tables 3, 4. From them, we can see that $\\hat{L}$ estimate along the trajectory of iterates have the same property as global $L$ , namely during restricting subspace of training variables the $L$ -the smooth constant is non-increasing, and in practice substantially decreasing. This schema exploits available information on gradient oracles in iterates in $s$ and iterates $s$ itself. This schema represents an estimation of upper bound L\u02c6 on the true value of $L$ . ", "page_idx": 24}, {"type": "table", "img_path": "YvA8UF0I37/tmp/3c0f085bc5702a01f5e44fbf77bb6a1f2f628e387679f99f6f1ef74bd24d6f96.jpg", "table_caption": ["Table 3: Estimated $L$ along the GD trajectory for LLama-160m (Schema I). "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "YvA8UF0I37/tmp/a59ee9923060a599d339664ac6c75fe1d1305894231ff32b052ee5a8eefd3708.jpg", "table_caption": ["Table 4: Estimated $L$ along the GD trajectory for TinyLlama-1.1B (Schema I). "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Schema II: Estimating $L$ along the sequence of iterates with capturing local curvature. ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The previous schema used a fixed sequence of iterates $s=\\{x_{1},x_{2},\\ldots,x_{10}\\}$ essentially estimate the $L$ -smoothness constant along the piece-wise linear interpolated path along $s$ . In the next schema, we approximate $L$ -smoothness constant as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{L}=\\operatorname*{max}_{x_{i}\\in s}\\left(\\|\\nabla^{2}f(x_{i})\\|\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore this schema exploits a sequence of points $s$ and selects the maximum in absolute values eigenvalue for all matrices $\\nabla^{2}f(x_{i})$ . Computing any spectral information for a matrix with big dimensions can be challenging. ", "page_idx": 24}, {"type": "text", "text": "The approximate schema that we have used to compute $\\Vert\\nabla^{2}f(x)\\Vert$ leverages several observable facts. First, $\\|\\nabla^{2}f(x)\\|\\,=\\,\\operatorname*{max}(|\\lambda_{i}(\\nabla^{2}f(x))|)$ , where $\\lambda_{i}$ is $i-t h$ eigenvalue for $\\nabla^{2}f(x)$ . Second, to identify the maximum eigenvalue in the absolute value we can use the normalized Power Iteration algorithm [47], which requires execution only the hessian-vector product. Third, we can use Taylor expansion and forward difference approximation for $\\nabla f(x+r)$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\nabla f(x+r)-\\nabla f(x)=\\nabla^{2}f(x)\\cdot r+\\mathcal{O}\\left(\\|r\\|^{2}\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The $K$ approximate hessian-vector product can be accomplished with $K+1$ gradient oracle calls. For the experiment, we run Power Iteration for a prior known number of iterations equal to 10. In fact Power Iteration does not converge in case of degeneracy such as a situation when the matrix has two maximum eigenvalues in absolute values but with opposite signs, and the convergence rate is determined by the absolute value of the ratio of the second-largest-magnitude eigenvalue to the first. We ignore these aspects in our heuristic approximate Algorithm 5. ", "page_idx": 25}, {"type": "text", "text": "Algorithm 5 Approximate Matrix-free Algorithm for Computing $\\|\\nabla f(x)\\|$   \n1: Parameters: Point $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , fixed $\\gamma\\in\\mathbb{R}$ such as $\\gamma=10^{-5}\\cdot x$ for numerical stability.   \n2: $r^{0}\\sim_{\\mathrm{u.a.r}}\\mathbb{R}^{d}$   \n3: $g=\\nabla f(x)$   \n4: for $k=0,1,\\ldots,K$ do   \n5: $\\hat{r^{k}}=r^{k}/\\lVert r^{k}\\rVert$   \n6: $r^{k+1}=1/_{\\gamma}\\left(\\nabla f(x+\\gamma r^{k})-g\\right)/_{\\mu}$ / Approximate computation of $\\boldsymbol{r}^{k+1}\\approx\\nabla^{2}(f)\\cdot\\boldsymbol{\\hat{r}}^{k}$ .   \n7: end for   \n8: Output: Approximate eigenvector ${r}^{K+1}/||{r}^{K+1}||$ corresponding to $|\\lambda_{\\operatorname*{max}}|\\approx\\|r^{K+1}\\|$ . ", "page_idx": 25}, {"type": "text", "text": "Results are presented in Tables 5, 6. From them, we can see that also this notion of $\\hat{L}$ estimate along the set of iterates has the same property as global $L$ , namely during restricting subspace of training variables the $L$ -the smooth constant is non-increasing similar to previous estimation method. ", "page_idx": 25}, {"type": "table", "img_path": "YvA8UF0I37/tmp/5c0b47daed4eaf6cb38991685daf683cf04aafa72475a1c9ae1ab03d527d1df8.jpg", "table_caption": ["Table 5: Estimated $L$ along the GD iterates for LLama-160m with local curvature (Schema II). "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "YvA8UF0I37/tmp/6ff614b78663edb6a0a63702302bcbc0ad0349afd5151c53cd7c6e5a2442c865.jpg", "table_caption": ["Table 6: Estimated $L$ along the GD iterates for TinyLlama-1.1B with local curvature (Schema II). "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "G Calibration Data Matters ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "For a fair comparison, we run our algorithm using the same calibration data as the baseline algorithms, typically a sample from RedPajama [13]. However, the way we handle this calibration data differs from most baselines [21, 18, 71]. ", "page_idx": 25}, {"type": "text", "text": "When analyzing their codebase, we found that these algorithms resample calibration data by taking a random excerpt of a fixed length from a random document in the calibration data, both sampled uniformly. However, with this approach, the data from longer documents (e.g. books) are underrepresented compared to shorter ones (e.g. news articles), which biases the calibration data. ", "page_idx": 26}, {"type": "text", "text": "Upon further investigation, we believe that new methods blindly copied this code from each other, going back to GPTQ [22] and possibly further. This choice was harmless for GPTQ since it requires relatively little calibration data; however, full model fine-tuning like in QuIP# [9] and AQLM [21], works better on unbiased data. ", "page_idx": 26}, {"type": "text", "text": "To remove the bias, we use standard11 LM preprocessing that concatenates all documents, then splits them into evenly sized chunks that become training sequences. The benefits from debiasing range from insignificant to as large as 0.15 perplexity for some models. To compensate for that, we run experiments with the same preprocessing protocol unless explicitly stated otherwise. ", "page_idx": 26}, {"type": "text", "text": "H Additional Engineering Considerations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "When done naively, the longest operation is the discrete update (8) that runs discrete optimization on all LLM parameters. For scalar quantization, this step does simple rounding and runs nearly instantly. In turn, applying it to vector quantization requires solving a discrete optimization algorithm (e.g. beam search) for every group of weights. However, since equation (8) can only update weights within $S^{k}$ , we can skip discrete optimization for any weight that was not among the chosen few. As a result, when training models with up to 70 billion parameters, we search less than one billion times per step. ", "page_idx": 26}, {"type": "text", "text": "The next longest operation is computing the gradients $\\nabla\\phi(\\cdot)$ , needed both for $\\mathbf{P}$ and $\\mathrm{v}$ steps. This involves running LLM multiple forward and backward passes on batches of texts and accumulating the gradients. To reduce the overhead from gradient computation, we compute the gradients once using mixed precision, then reuse these gradients for one $\\mathbf{P}$ and one $\\mathrm{v}$ step, respectively. In other words, we switch from alternating $\\mathbf{P}$ and $\\mathrm{v}$ steps to performing these steps simultaneously. We also reuse these gradients to update any parameters not affected by quantization: input embeddings, normalization layers, biases, etc. ", "page_idx": 26}, {"type": "text", "text": "To limit VRAM usage, we use gradient checkpointing [27], batch accumulation. For larger models, we also use parameter sharding12[55] and optimizer offloading [57]. We need these techniques so that smaller 7B LLMs can be trained on a single GPU and larger ones with 70B parameters fit into a single machine with $8\\!\\times\\!\\mathrm{Al00}$ . Still, PV-tuning takes up to $1.5\\times$ longer than tuning continuous parameters and uses more memory (during training) to hold the gradients w.r.t. dequantized weights. ", "page_idx": 26}, {"type": "image", "img_path": "YvA8UF0I37/tmp/9d171f0b7ad479e30907435cf021213083452a477ad4efd1a700d5e9d1c4f3f3.jpg", "img_caption": ["Figure 3: WikiText-2 perplexity of 2-bit quantized LLAMA 2 models as a function of model size (GiB) compared to a theoretical lossless 3 bit compressed model (i.e. float16 perplexity numbers paired with 3-bit model sizes). "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "J Additional Details for Section 4.1 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Here, we describe some of the implementation details we used to optimize different quantized representations. For every optimizations, we check that this optimization improves the algorithm in both MSE and perplexity and does not require additional resources that would result in unfair comparison. ", "page_idx": 27}, {"type": "text", "text": "Vector Quantziation. The original algorithm quantizes all weights a single pass over input channels. We found that it works slightly better if we make multiple such passes and, between passes, update codes by Adam to minimize the same objective [72]. This is resembles QuIP# with no RHT & lattices or AQLM with no additive quantization. For simplicity, we also use a single shared codebook (e.g. instead of groupwise codebooks). ", "page_idx": 27}, {"type": "text", "text": "$\\mathbf{VQ+}$ outliers To select outlier coordinates, we use https://github.com/fmfi-compbio/ admm-pruning that outperforms the $\\operatorname{SpQR}$ outlier criterion [18] in both L2 error and perplexity (when both criteria are used alongside vector quantization). We re-run the ADMM procedure multiple times during optimization, resulting in an EM-like algorithm. ", "page_idx": 27}, {"type": "text", "text": "VQ+lowrank. We experimented with two different initializations for low-rank correction: a) quantizing weight matrix, then training LoRC on quantization error, as in [79] and b) initializing LoRC to approximate the reference weight matrix, the applying vector quantization to LoRC errors. Of these two approaches, we found that the latter one produces a (slightly) better solution in both MSE and perplexity. ", "page_idx": 27}, {"type": "text", "text": "Additional representation evaluations. Below, we report some additional quantized representation configurations that extend our evaluations from Section 4.1. For convenience, we report them both as per-method perplexity values in Table 7 and the combined plots in Figure 4. ", "page_idx": 27}, {"type": "text", "text": "Table 7: Comparison of WikiText-2 Perplexity for each method with and without fine-tuning for quantizing Llama 2 7B model. For each method, PPL no FT denotes its perplexity without finetuning, whereas PPL w/ FT is perplexity with fine-tuning. We use the same setup as in Section 4.1. ", "page_idx": 28}, {"type": "table", "img_path": "YvA8UF0I37/tmp/52d0bf86f7212f7e824939908e9db0868cca9e4455b0ce5c85f2e8c88058df22.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "YvA8UF0I37/tmp/5284d1916f3a618629544a463f10bdcca9a3884b99faa4689b40fb0b2adcdb09.jpg", "img_caption": ["Figure 4: Llama2 7B perplexity on WikiText-2 after compression without fine-tuning (left) and with fine-tuning (middle). On the (right), there is a plot that combines the first two, allowing for a better comparison of each method with and without fine-tuning. Compression algorithms without fine-tuning are represented with dashed lines, while algorithms with fine-tuning are represented with continuous lines. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 8: Evaluation of quantized LLAMA 2 models for 2.x bits per weight. We use the same setup as in the main paper (Section 4.2 with an extra baseline). As requested, we finetune the model in 16-bit precision for the same number of steps, then quantize it with AQLM, reported as $\\mathbf{\\cdot}\\mathbf{FT+AOLM^{\\ast}}$ . Finally, the \u201cFinetuned\u201d row corresponds to an uncompressed 16-bit model finetuned without quantization. We hypothesize that fine-tuning the model has little effect since we train on a dataset resembling its original pretraining data. ", "page_idx": 28}, {"type": "table", "img_path": "YvA8UF0I37/tmp/59c71dc4f33f7e20adcaedc67fe885b1745a20410cc893dfc23de868e37f25f4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "YvA8UF0I37/tmp/e5c9bd9227385d45cf8e3a85665a65cf027c34ccd6606cbc19627da892166ea1.jpg", "img_caption": ["Figure 5: Learning curve for PV-tuning and STE algorithms, when tuning tinyllama model with 2x8g8 AQLM quantization (2 codebooks with 8 bits per code and input groupsize equal to 8). "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "K Additional Details for Section 4.2 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "VQ: vector quantization as a simple near-optimal algorithm. We use a single 16-bit codebook with group size 16 (over input dimension) and per-channel trainable scales over output dimension. During P step, we update the codebook, scales and non-quantized model layers by backprop. During V step, we try every code in the codebook and choose the one that minimizes (6). ", "page_idx": 29}, {"type": "text", "text": "GPTQ: scalar quantization with block-wise scales and zero points. We use 2-bit base codes and block size 128. During P step, we update the scales and non-quantized parameters by backprop. In turn, V step performs simple rounding to nearest (scaled) integer.13. ", "page_idx": 29}, {"type": "text", "text": "AQLM: we perform scalar quantization with two 8-bit codebooks and group size 8 and channel-wise steps. This was originally published in [21] as the \u201cspeed-optimized\u201d configuration capable of fast lookup-based inference. During P step, we update both codebooks, as well as scales an non-quantized parameters by backprop. On V step, we run beam search with beam size 8 with gradient-updated dequantized weight as target. ", "page_idx": 29}, {"type": "text", "text": "Training. We minimize Kullback\u2013Leibler divergence as our loss function for all three representations. More specifically, we fine-tune the quantized \u201cstudent\u201d model to approximate the predictions (logits) of a \u201cteacher\u201d \u2014 the same model prior to quantization. We fine-tune on the same RedPajama sample as in calibration. More specifically, we use the official one-billion-token sample14provided by the dataset authors [13]. We use a batch size of $2^{20}$ $\\langle\\approx1\\mathrm{M})$ tokens, split into batches of model-specific sequence length (e.g. 4096 tokens for LLAMA 2, 8192 for LLAMA 3). In early experiments, we found PV to be resilient to the choice of batch size, consistently training with up to $4\\times$ smaller batches. ", "page_idx": 29}, {"type": "text", "text": "Hyperparameter tuning: we tune the hyperparameters for each method individually. For all algorithms, we tune learning rate on a logarithmic scale out of (1e-4, 3e-4, 1e-3, 3e-3, 1e-2). For methods involving discrete optimization, we tune learning rate for $\\mathbf{P}$ and $\\mathrm{v}$ step separately. The optimal configuration for STE and stochastic rounding is to use learning rate 3e-4 for both codes and codebooks. The optimal configuration for subspace linearized PV and the same with STE is to use learning rate 3e-4 for P step and 3e-3 for codes. Curiously, the subspace methods are stable even with larger step sizes for codes, e.g. 1e-2, whereas unrestricted methods (e.g. pure STE) are not. ", "page_idx": 29}, {"type": "text", "text": "For stochastic rounding, we found that the unbiased rounding [53] causes the model quality to quickly degrade, likely due to the fact that the algorithm makes too many weight changes due to rounding. The results we reported in Table 1 use stochastic rounding with temperature 0.2. In other words, we measure the distances to 2 nearest bins and round to each bin proportionally to distance $^{-5}$ . We also tried gradually annealing the rounding temperature to 0, but achieved only insignificant improvements in accuracy and perplexity $(<\\!0.01)$ . To simplify evaluation, we do not use annealing in Table1. ", "page_idx": 29}, {"type": "text", "text": "For PV-tuning and PV-tuning with STE, we always set $\\tau$ to maximum number of weights such that updating them satisfies $||x^{k\\mp1}-x^{k}||/||x^{k}||<\\bar{0.01}$ . We implement this by trying to update large portions of weights (in our implementation, we update 0.01 of all weights at a time) until the total change exceeds the constraint. Once it does, we rollback weights in the last chunk to $x^{k}$ until the constraint is satisfied. As an implementation quirk, we always allow at least one weight to change even if changing just one weight already exceeds $0.01\\cdot||x^{k}||$ . However, we didn\u2019t see this in practice. ", "page_idx": 29}, {"type": "text", "text": "We also found that Lamb15 [81] is more stable when training with large batch sizes, but converges to approximately the same accuracy. We use $\\beta_{1}{=}0.9$ and $\\beta_{2}{=}0.95$ , same as in most LLM training configurations [69, 86, 61]. We do not use learning rate decay for simplicity. It is likely possibly to improve our results by annealing the learning rates during training or using a warmup. We intentionally avoid this to reduce the number of \u201cmoving parts\u201d and simplify evaluation. Overall, we found that PV-tuning is about as sensitive to hyperparameters as continuous-only LLM fine-tuning [71, 21, 63]. ", "page_idx": 29}, {"type": "text", "text": "In the codebase, we release several additional PV-tuned models with a more careful choice of training hyperparameters and calibration datasets: for instance, when quantizing instruction tuned models, we found that calibrating the model on chat-like data results in better accuracy. ", "page_idx": 29}, {"type": "text", "text": "L PV-Tuning of QuIP# ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "QuIP# is a popular modification of classical vector quantization for neural network compression. Unlike standard VQ, QuIP# does not quantize weights directly, but instead applies a Randomized Hadamard Transform (RHT) and quantizes the projected weight representations. This is done by applying the Hadamard transform to the rows and columns of the $m\\times n$ matrix and multiplying them by random vectors $S_{V}\\sim\\mathcal{U}\\{\\pm1\\}^{n}$ , $S_{U}\\sim\\mathcal{U}\\{\\pm1\\}^{m}$ after each transformation. These vectors do not need to be stored, but may be generated on the fly from a random seed. ", "page_idx": 30}, {"type": "text", "text": "The main objective of this transformation is to ensure that the Hadamard-transformed weight matrix adheres to a normal distribution. With this in mind, QuIP# no longer needs to learn the optimal quantization \u201ccodebook\u201d, but may instead use the optimal lattice for quantizing normal distribution. As a result, QuIP# does not need to store the codebooks in memory, thereby reducing its overall bitwidth. For instance, for Llama-2 7B vector quantization with group size 8 and 16-bit codes, VQ produces a 2.29 bit model while QuIP# has slightly less than 2.01 bits per parameter. ", "page_idx": 30}, {"type": "text", "text": "We apply PV-Tuning to QuIP# in a manner similar to Vector Quantization. We start with a model already quantized using the original QuIP# algorithm, specifically employing the official 2-bit quantization of the Llama 2 7B model16. We then optimize this model using the same hyperparameters as those for Vector Quantization (see Appendix K). ", "page_idx": 30}, {"type": "text", "text": "During $\\mathbf{P}$ step, we optimize quantization scales (SU, SV), as well as any non-quantized model parameters (embeddings, LM head, normalization scales and biases), but not the codebook, since QuIP# relies on a predefined lattice structure that cannot be trained directly. During V step, we update the discrete codes responsible for choosing a vector out of the said lattice using the same procedure as in standard vector quantization. ", "page_idx": 30}, {"type": "text", "text": "We compare PV-Tuning against several other fine-tuning strategies: no fine-tuning, the built-in finetuning procedure proposed in [71], as well as the technically improved version of that procedure using the observations described in in G (denoted as \u201cimproved FT\u201d). We report the resulting perplexities and zero-shot accuracies in Table 9 and include them in other relevant tables as \u201cQuIP#+PV\u201d. ", "page_idx": 30}, {"type": "table", "img_path": "YvA8UF0I37/tmp/e5f9d4c37fcadac6cb37a8aeadf9ea7229e20fd6a17e15dea7ebb190d6e29e63.jpg", "table_caption": ["Table 9: Evaluation of LLAMA-2 7B quantized using QuIP# with various fine-tuning strategies. We report perplexity on WikiText-2 [45] & C4 [54] and zero-shot accuracy. The Average is the mean accuracy of 5 zero-shot tasks. Primary metrics are Wiki2 (PPL), C4 (PPL) and Average (Accuracy). "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "In general, we found that QuIP# shows similar if not better improvements from PV-Tuning as VQ and AQLM. Furthermore, when compared to only fine-tuning continuous parameters, QuIP# shows better relative improvement from V steps than traditional vector quantization. We attribute this to the fact that VQ can fine-tune its codebook even without PV-Tuning, whereas QuIP# relies on fixed codebooks that cannot be learned. As a result, discrete parameters (codes) take up a larger fraction of the total model size in QuIP#, allowing V steps to make more significant improvements. The detailed instructions for running these experiments can be found in our official implementation17, in a separate subsection of the common README file. ", "page_idx": 30}, {"type": "text", "text": "M On 1-bit Vector Quantization Options ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "As discussed above, there are several ways to achieve the same number of bits per parameter with vector quantization. In this section, we explore the effect of varying the group size: either quantizing groups of 16 weights with larger codebooks, or groups of 8 weights with smaller ones. The results with different group and codebook sizes are reported in Table 10. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "table", "img_path": "YvA8UF0I37/tmp/7f03058f1abc0ff9ebe998405bc1adf2299dcf7758073e1841357f392af85aa5.jpg", "table_caption": ["Table 10: Evaluation of LLAMA-2 7B quantized using $\\mathrm{VQ+PV}$ with different group size (GS, also known as vector dimension) and code bits (CB, s.t. codebook size is $2^{C B}$ ). We report perplexity on WikiText-2 [45] & C4 [54] and zero-shot accuracy. The Average is the mean accuracy of 5 zero-shot tasks. Primary metrics are Wiki2 (PPL), C4 (PPL) and Average (Accuracy). "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "Overall, we figure out that smaller groups provide better performance for the same model size. We hypothesize that PV-Tuning is better able to deal with these configurations because with smaller group size, discrete codes constitute a larger fraction of model parameters, which allows the V step of our algorithm to achieve more significant improvements. In the initial version of manuscript we performed all 1-bit experiments with suboptimal group size 16. More recent results for group size 8 are provided under a separate name \u201cPV (gs 8)\u201d in Table 2. ", "page_idx": 31}, {"type": "text", "text": "N Additional Details and Evaluations for Section 4.3 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we report additional results for LLAMA 2 & 3, MISTRAL and PHI-3 and discuss baselines. In this section, we always evaluate PV-tuning for vector quantization, using 14-16 bits per codebook for a group of 8 or 16 weights, with each combination fitting a particualr niche. For instance, 16 bits per 8 weights is slightly over 2 bits per weight, whereas 14 bits per 16 weights is either at or below 1 bit per weight, depending on the model size. ", "page_idx": 31}, {"type": "text", "text": "We use LLAMA 2 models as our main benchmark as they are well studied in the PTQ community. Here, we gather the latest state-of-the-art algorithms at the time of publication and group them according to their target number of bits, roughly 1-1.7 bits per weight (Table 11) and 2-2.5 bits per weight (Table 12). ", "page_idx": 31}, {"type": "text", "text": "Both our training runs and almost all baselines use the same sample of RedPajama data from previous sections18. The only exception to this is OneBit that uses a corpora of LLM outputs gathered specifically for that paper [77]. ", "page_idx": 31}, {"type": "text", "text": "The source code for this method was unavailable until very recently. The official link https://github.com/xuyuzhuang11/OneBit used to point to an empty repository until the code was released in a commit https://github.com/xuyuzhuang11/OneBit/commit/ 380a6aedc3c060993056ff50b79065e893be99ae on May 10th. Thus, unfortunately, we did not have time to make OneBit compatible with models except LLAMA 2 7B and 13B that were featured in the original paper. ", "page_idx": 31}, {"type": "text", "text": "For LLAMA 3, we evaluate PV-tuning of vector quantization against the baselines introduced in [33]. Curiously, their paper seems to compute perplexity differently than our paper. Since our protocol matches with most prior works [22, 18, 21, 71], we chose to re-evaluate the results from [33] with our perplexity code and not the other way around. We calibrate using the official code 19 and reuse published models where available. ", "page_idx": 31}, {"type": "text", "text": "O Inference Speed with Vector Quantization Kernels ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we demonstrate that PV-Tuning can achieve speedups by using fast inference kernels from the underlying quantized representation. Since our main experiments use vector quantization, ", "page_idx": 31}, {"type": "text", "text": "Table 11: Evaluation of quantized LLAMA 2 models for 1-1.7 bits per weight, grouped by bitwidth. We report perplexity on WikiText-2 [45] & C4 [54] and zero-shot accuracy. The Average is the mean accuracy of 5 zero-shot tasks. Primary metrics are Wiki2 (PPL), C4 (PPL) and Average (Accuracy). ", "page_idx": 32}, {"type": "table", "img_path": "YvA8UF0I37/tmp/3b67bc7e366c5e8138e98bc0ef030ef3f93af8372c91cfed4479df72eff18f84.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "Table 12: Evaluation of quantized LLAMA 2 models for 2-2.3 bits per weight, grouped by bitwidth. We report perplexity on WikiText-2 [45] & C4 [54] and zero-shot accuracy. The Average is the mean accuracy of 5 zero-shot tasks. Primary metrics are Wiki2 (PPL), C4 (PPL) and Average (Accuracy). ", "page_idx": 32}, {"type": "table", "img_path": "YvA8UF0I37/tmp/65be3550d0f974cf426aeecf0659f67deaed254bc79b2ecae0c58ba364c64416.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "we adopt simplified version of AQLM inference kernels with one codebook of size 16 for groups of 8 consecutive weights. This kernel is not written by us: it was added to the official AQLM implementation by an open-source contributor. ", "page_idx": 32}, {"type": "text", "text": "We adapt this inference code to our codebase and switch it to using group size 16 to support out 1.1- 1.58 bit models. We evaluate inference speeds on a single Nvidia RTX 3090 GPU using transformers with cuda graphs20. ", "page_idx": 32}, {"type": "text", "text": "Table 13: Evaluation of quantized LLAMA 3 models for 1-2.3 bits per weight, grouped by bitwidth. We report perplexity on WikiText-2 [45] & C4 [54] and zero-shot accuracy. The Average is the mean accuracy of 5 zero-shot tasks. Primary metrics are Wiki2 (PPL), C4 (PPL) and Average (Accuracy). ", "page_idx": 33}, {"type": "table", "img_path": "YvA8UF0I37/tmp/fd961643a37989b8aa2fd9e1b58d79666b4add250e3906952b52bffe28c43fa8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "Table 14: Evaluation of quantized MISTRAL V0.1 7B (A) and PHI 3-MINI-4K-INSTRUCT 3.8B (B) models for 1-2.3 bits per weight, grouped by bitwidth. We report perplexity on WikiText-2 [45] & C4 [54] and zero-shot accuracy. The Average is the mean accuracy of 5 zero-shot tasks. Primary metrics are Wiki2 (PPL), C4 (PPL) and Average (Accuracy). ", "page_idx": 33}, {"type": "table", "img_path": "YvA8UF0I37/tmp/12dbfa7e4148ef2df418353196097759c34445b4a15b045237d3ac85a9002726.jpg", "table_caption": [], "table_footnote": ["We were able to acheve 47.4 tokens per second for 7B model, 32.8 tokens per second for 13B model and 7.2 tokens per second for LLAMA 2 70B model. Compared to 16-bit inference code, this results in speedups of $14\\%$ , $22\\%$ and $28\\%$ respectively. Note that PV-Tuning does not make the model inherently faster than, for instance, AQLM. Instead, it can achieve better quality with lower bit models, allowing practitioners to run smaller and faster models for the same accuracy target. "], "page_idx": 33}, {"type": "text", "text": "P The Choice of the Initial Point $x^{0}$ ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Algorithm 1 converges from any initial point $x^{0}\\in\\mathbb{R}_{c}^{d}$ (3.1), but it might converge to a different final point ${\\hat{x}}(x^{0})$ . In this section, we discuss two possible variants of instantiating the initial point. ", "page_idx": 34}, {"type": "text", "text": "P.1 Clipping of $x^{\\star}$ ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Notation: $[d]:=\\{1,\\cdot\\cdot\\cdot,d\\}$ is the set of $d$ distinct natural numbers from 1 to $d$ . ", "page_idx": 34}, {"type": "text", "text": "Assume we have the vector $x$ with dimensionality $d$ and let $c\\in[d]$ . Let us define the clipping operator $C(\\boldsymbol{x}):\\mathbb{R}^{d}\\rightarrow\\mathbb{R}_{\\leq c}^{d}$ in the following way: ", "page_idx": 34}, {"type": "equation", "text": "$$\nC(x)={\\tilde{x}}:\\quad V({\\tilde{x}})\\subseteq V(x)\\quad{\\mathrm{and}}\\quad|V({\\tilde{x}})|\\leq c\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We can come up with different variants of clipping operators $C(x)$ , but in our experiments, we choose $C(x)$ such that $V(\\tilde{x})$ consists of the smallest distinct elements from $V(x)$ . We will call this clipping operator by $C_{-}(x)$ . ", "page_idx": 34}, {"type": "text", "text": "Example P.1. If $x\\ =\\ \\{1,1,3,5,9\\}$ , then having $c\\ =\\ 2$ the clipping operator $C_{-}(x)\\ =\\ \\tilde{x}\\ =$ $\\{1,1,3,3,3\\}$ . So, $\\tilde{x}$ consists of smallest 2 elements from $V(x)=\\{1,3,5,9\\}$ . ", "page_idx": 34}, {"type": "text", "text": "P.2 Random $x^{0}\\in\\mathbb{R}_{c}^{d}$ ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Now let us define the algorithm for generating random points from $\\mathbb{R}_{c}^{d}$ (Alg. 6). ", "page_idx": 34}, {"type": "table", "img_path": "YvA8UF0I37/tmp/66278df18d80848618793fea29c189fec1a258a512dc313e094323c302d40a9a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "Different random $x^{0}\\in\\mathbb{R}_{c}^{d}$ provide different loss functions. To get more stable and smoothed results we ran the algorithm (1) with different initial points, generated by algorithm (6). We will denote $r$ as the number of runs with different random initialization. ", "page_idx": 34}, {"type": "text", "text": "Q Small-Scale Experiments and Interpretation ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Q.1 Objective function ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Let $c\\in[d]$ and consider the problem ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}_{\\leq c}^{d}}\\phi(x),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\phi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ , and $\\mathbb{R}_{\\leq c}^{d}\\subset\\mathbb{R}^{d}$ is the set of all vectors in $\\mathbb{R}^{d}$ whose $d$ entries take at most $c$ distinct values. In other words, the cardinality of the set ", "page_idx": 34}, {"type": "equation", "text": "$$\nV(x):=\\{x_{1},\\cdot\\cdot\\cdot\\,,x_{d}\\}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "is at most $c$ . For small experiments, we aim to minimize the following objective ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\phi(x)=\\sum_{i=1}^{d}a_{i}(x_{i}-x_{i}^{\\star})^{2}=(x-x^{\\star})^{T}\\Lambda(x-x^{\\star}),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $a_{i}\\in\\mathbb{R}^{d}$ , $x^{\\star}$ is a unique optimal point: $\\nabla\\phi(\\ensuremath{\\boldsymbol{{x}}}^{\\star})=0$ and ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\Lambda=\\mathrm{diag}(a_{1},\\cdot\\cdot\\cdot\\cdot,a_{d})=\\left[\\begin{array}{c c c}{{a_{1}}}&{{\\cdot\\cdot\\cdot}}&{{0}}\\\\ {{\\vdots}}&{{\\ddots}}&{{0}}\\\\ {{0}}&{{0}}&{{a_{d}}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We set $a_{i}=i/d$ for $i\\in[d]$ . Hence, we have the Lipschitz continuity for (18) with $L=1$ . Note that the Algorithm 1 does not guarantee to converge to $x^{\\star}$ . Let $\\hat{x}$ denote the vector to which the Algorithm 1 converges. ", "page_idx": 34}, {"type": "text", "text": "We applied PV algorithm to the problem (16) with $\\phi(x)$ being (18) (Fig. 6). Number of runs with different random initial points $r=50$ . ", "page_idx": 35}, {"type": "image", "img_path": "YvA8UF0I37/tmp/eae780833e335ac5f7037b255f022c7c43a3c9856865fe99be3365abdf75dd3d.jpg", "img_caption": ["(a) PV algorithm with different values of $c\\ \\in$ $[1,6]$ , $d=6$ . "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "YvA8UF0I37/tmp/f58cf3d6dee7a087ea3e19deb58267d7dc1f93c58ee74e311e72b33a070446ed.jpg", "img_caption": ["(b) The influence of $\\mathbf{P}$ and $\\mathrm{v}$ steps on the loss function $\\phi(x)$ , $c=3$ . "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Figure 6: PV algorithm (1) applied on the very small dimensional $[d=6]$ ) quadratic objective (18).   \nThe starting point $x^{0}$ is chosen randomly using the ng algorithm (6). ", "page_idx": 35}, {"type": "text", "text": "When $c=1$ we converge just in one step of PV (red line on Fig.6a) because we have only one degree of freedom to play with and we fully utilize it on the first step. One can show that the solution for the case $c=1$ will be $\\begin{array}{r}{\\hat{x}=\\left(\\sum_{i=1}^{d}a_{i}\\right)^{-1}\\sum_{i=1}^{d}a_{i}x_{i}^{\\star},}\\end{array}$ . ", "page_idx": 35}, {"type": "text", "text": "Note that as we increase the maximum number of unique elements $c$ , the final loss $\\phi(\\hat{x})$ decreases up until the moment when $c=d$ , when the loss is zero (the plot 6a is in logarithmic scale and that is why we cannot see the last line). ", "page_idx": 35}, {"type": "text", "text": "We run PV algorithm (1) with different random starting points $x^{0}\\in\\mathbb{R}_{c}^{d}$ (6). Different starting points can lead to different local optimum. That is why we ran the algorithm (1) several times with different random initial points $x^{0}$ . A number of runs $r=50$ , we used this value for all further experiments. ", "page_idx": 35}, {"type": "text", "text": "Each of the two steps of the PV algorithm contributes to the convergence. To observe this we plotted the loss function over the iterates and explicitly marked the progress of both $\\mathbf{P}$ and $\\mathrm{v}$ steps (Fig. 6b). We can see that we have progress during each of these steps and one single $\\mathbf{P}$ and $\\mathrm{v}$ step is not enough to obtain a solution even in this very small and simple case. ", "page_idx": 35}, {"type": "text", "text": "These simple experiments demonstrate that ", "page_idx": 35}, {"type": "text", "text": "1. larger $c$ (smaller compress ratio) provides better final accuracy   \n2. several $\\mathbf{P}$ and $\\mathrm{v}$ steps are needed to converge to the solution ", "page_idx": 35}, {"type": "text", "text": "Q.3 Interpretation of $P(y)\\supseteq P(x^{k})$ ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "As we mentioned before, we select the initial point $x^{0}\\in\\mathbb{R}_{c}^{d}$ randomly such that it has $c$ unique elements in its linear shell $V(x^{0})$ . To understand the notation $\\bar{P}(y)\\supseteq P(x^{k})$ , defined in the chapter (3.1), let us consider the first step of the algorithm, specifically the $\\mathbf{P}$ step. ", "page_idx": 35}, {"type": "text", "text": "On the first step of the algorithm we select a new point by solving the following optimization problem: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{y^{0}=\\underset{y\\in\\mathbb{R}_{\\leq c}^{d}}{\\arg\\operatorname*{min}}\\{\\phi(y)\\,:\\,P(y)\\supseteq P(x^{0})\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Here we have a random point $x^{0}$ , with some partition $P(x^{0})=\\{P_{1}(x^{0}),\\cdot\\cdot\\cdot,P_{c}(x^{0})\\}-c$ sets of equal elements from $x^{0}$ . We find $y^{0}$ such that minimizes $\\phi(y^{0})$ and have the same or smaller number of partitions: $P(y^{0})=\\{P_{1}(y^{0}),\\dot{\\cdot}\\cdot\\cdot,P_{s}(y^{0})\\}$ , where $s\\leq c$ . ", "page_idx": 35}, {"type": "text", "text": "Strict equality of two sets, $P(x^{0})$ and $P(y^{0})$ would mean that the linear shell \u2013 the number of unique elements of the vector $x^{0}$ is equal to the number of unique elements from $y^{0}$ ( $\\mathit{\\omega}_{c}=s_{,}$ . In our PV algorithm we allow $y^{0}$ to have smaller number of unique elements than $x^{0}$ , so we possibly merge some partitions of $x^{0}$ . ", "page_idx": 36}, {"type": "text", "text": "It is worth to mention that in real experiments we observe that the number of unique elements did not change or the change is not significant. ", "page_idx": 36}, {"type": "text", "text": "Q.4 Simple example of $P(y)\\supseteq P(x^{k})$ ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Let $d=8$ and $x=(1,3,1,4,4,5,1,3)$ . Then $x\\in R_{c}^{d}$ , where $c=4$ since $x$ consists of 4 unique floats: $1,3,4$ and 5. We have $P_{1}(x)=\\{1,3,7\\}$ , $P_{2}(x)\\bar{=}\\{2,8\\},P_{3}(x)=\\{4,5\\}$ and $P_{4}(x)=\\{6\\}$ . So, $P(x)=\\{P_{1}(x),P_{2}(x),P_{3}(x),P_{4}(\\bar{x})\\}=\\left\\{\\{1,3,7\\},\\{2,8\\},\\{4,5\\},\\{6\\}\\right\\}$ . ${\\dot{P}}(x)$ is a set whose elements are four sets, forming a partition of $\\{1,2,\\cdots,8\\}$ . ", "page_idx": 36}, {"type": "text", "text": "Let $y\\,=\\,\\{1,1,1,4,4,4,1,1\\}$ . Then $y\\in R_{c}^{d}$ , where $c\\,=\\,2$ . We have $P_{1}(y)\\,=\\,\\{1,2,3,7,8\\}$ and $P_{2}(y)=\\{4,5,6\\}$ . So, $P(y)\\,=\\,\\{\\{1,2,3,7,8\\},\\{4,5,6\\}\\}$ . Notice that each element of $P(x)$ is a subset of some element of $P(y)$ . For example, $\\{2,8\\}$ is a subset of $\\{1,2,3,7,8\\}$ and $\\{6\\}$ is a subset of $\\{4,5,6\\}$ . Because of this, by our definition, $P(y)\\supseteq P(x)$ . ", "page_idx": 36}, {"type": "text", "text": "Q.5 Small-scale experiments $(d=100,c\\in[1,100])$ ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "The problem of the algorithm (1) is that the $\\mathrm{v}$ step requires the full parameter search which gives us the complexity $O(c^{d})$ . Even for small tasks, this becomes unpractical to solve. ", "page_idx": 36}, {"type": "text", "text": "Example Q.1. Let us take $d=100$ and $c=10$ , then the complexity of one $\\mathrm{v}$ step will be $\\mathcal{O}(10^{100})$ . Modern computers can make roughly 10 petaflops or $10^{16}$ calculations per second, hence we will have to wait $\\therefore10^{76}$ years to make a single $\\mathrm{v}$ step. ", "page_idx": 36}, {"type": "text", "text": "Let us consider special sets of function $\\phi(x)$ that we will call separable functions. This class of functions should satisfy the assumption (Q.2). ", "page_idx": 36}, {"type": "text", "text": "Assumption Q.2 (Separable function). The function $\\phi(x):\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ can be written in the form $\\begin{array}{r}{\\phi(x)=\\sum_{i=1}^{d}\\phi_{i}(x_{i})}\\end{array}$ , where $\\phi_{i}(\\cdot)$ is a mapping $\\phi_{i}(\\cdot):\\mathbb{R}\\rightarrow\\mathbb{R}$ . ", "page_idx": 36}, {"type": "text", "text": "Example Q.3. The objective function (18) is a sum of squares that can be written in the following form: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\phi(x)=\\sum_{i=1}^{d}a_{i}(x_{i}-x_{i}^{\\star})^{2}=\\sum_{i=1}^{d}\\phi_{i}(x_{i}),\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\phi_{i}(x_{i})=a_{i}(x_{i}-x_{i}^{\\star})^{2}$ . Hence, the objective (18) is a separable function. ", "page_idx": 36}, {"type": "text", "text": "One can show that for separable functions (Q.2) the algorithm (1) can be written in the form (7). Hence, for separable functions, we can compute the $\\mathrm{v}$ step in $O(c\\cdot d)$ operations (instead of $O(c^{d}))$ , which makes the algorithm (1) practical to use. ", "page_idx": 36}, {"type": "table", "img_path": "YvA8UF0I37/tmp/24a5623c89e80d002560b752e60df85a83f529c7e1c0cee6a2be3eb47ee842e6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 36}, {"type": "text", "text": "Then we ran the optimized PV algorithm (7) on the quadratic objective (18). The results are presented in (Figure 7). Number of runs with different random initial points $r=50$ . ", "page_idx": 36}, {"type": "text", "text": "For these experiments, we see the same results as for tiny scale with $d=6$ (Q.2): larger $c$ (smaller compress ratio) provides better final accuracy, and several $\\mathbf{P}$ and $\\mathrm{v}$ steps have to be done to obtain better solution. In addition, we observe that the PV algorithm does not decrease the dimensionality (number of unique elements) of $V(x)$ . ", "page_idx": 36}, {"type": "text", "text": "As it was mentioned before, the algorithm (1) is conceptual only and cannot be used in the raw form in practice. That is why we need to move to the experiments with linearized $\\mathrm{v}$ step. ", "page_idx": 37}, {"type": "image", "img_path": "YvA8UF0I37/tmp/68371be02f1b2ce5f8d86fc254895fec01f9c6be8a6530e6880cadc8491c294d.jpg", "img_caption": ["(a) PV algorithm with different values of $c\\ \\in$ [10, 100], $d=100$ . "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "YvA8UF0I37/tmp/dc5648cde35e34c059df6aab64c8d471ea0c28a57675f38aa40b443cdead4845.jpg", "img_caption": ["(b) The influence of $\\mathbf{P}$ and $\\mathrm{v}$ steps on the loss function $\\phi(x)$ , $c=10$ . "], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "Figure 7: Optimized PV algorithm (7) applied on the quadratic objective (18), $d=100$ . Number of runs with different random initial points $r=50$ . ", "page_idx": 37}, {"type": "text", "text": "Q.6 Linearized PV ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Linearized $\\mathrm{v}$ step (B.1) allows us to greatly reduce the cost of one $\\mathrm{v}$ step. We ran Linearized PV on the quadratic objective (18) (Fig. 8). ", "page_idx": 37}, {"type": "image", "img_path": "YvA8UF0I37/tmp/291e5c57975fb62c5ad8d1e4290b73a3c8cfd0eb9e75fc7f9fd02e836349ebc6.jpg", "img_caption": ["(a) Linearized PV algorithm (B.1) with different $T\\in[1,5]$ . Larger $T$ provides faster convergence rates, but the same final accuracy. ", "Figure 8: Experiments with Linearized PV algorithm (B.1). "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "YvA8UF0I37/tmp/9337a494504bd4b5922dd92ddc848c975fb9d54abdbe4c3585be16413ea39427.jpg", "img_caption": ["(b) The influence of $\\mathbf{P}$ and $\\mathrm{v}$ steps on the loss function $\\phi(x)$ , $c=10$ , $T=2$ . We need to make 2 linearized $\\mathrm{v}$ steps on each iteration. "], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "From the first experiment (Fig. 8a) we can see that ", "page_idx": 37}, {"type": "text", "text": "1. increasing $T$ provides slightly better convergence rates and approximately the same final accuracy. We saw the similar results on large-scale experiments. Hence, we can use $T=1$ to save computations.   \n2. Linearized PV algorithm (B.1) converges to a worse accuracy than the exact PV (1).   \n3. Linearized PV has to make more iterations than PV to converge ", "page_idx": 37}, {"type": "text", "text": "The second plot (Fig.8b) demonstrates the effect of multiple Linearized V step. We can see that the largest effect comes from the first $\\mathrm{v}$ step. ", "page_idx": 37}, {"type": "text", "text": "Q.7 Linearized $\\mathbf{PV+}$ sparse updates ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "In previous section (Q.6) we have seen that Linearized PV algorithm converges to a worse accuracy than the exact PV (1). ", "page_idx": 38}, {"type": "text", "text": "Linearized PV (B.1) with combination of sparse updates (3.3) is intended to mitigate this issue. ", "page_idx": 38}, {"type": "text", "text": "On (Fig. 9a) we can see comparison of three methods: exact PV (1) \u2013 red line, Linearized PV (B.1) \u2013 blue line and Linearized PV with sparse updates (3.3) \u2013 green line. ", "page_idx": 38}, {"type": "text", "text": "From this experiment we observe ", "page_idx": 38}, {"type": "text", "text": "1. Linearized PV with sparse updates converges to a better accuracy than Linearized PV 2. inearized $\\mathrm{PV}+$ sparse updates has to make more iterations than Linearized PV and exact PV to converge ", "page_idx": 38}, {"type": "text", "text": "Hence, this approach helps us to converge to a better accuracy, but with a price of larger number of iterations to converge. ", "page_idx": 38}, {"type": "image", "img_path": "YvA8UF0I37/tmp/235ed69dd7f8c841c21d751c99e1463a64065547d046aa7605cfd9bccbd1bc73.jpg", "img_caption": [], "img_footnote": [], "page_idx": 38}, {"type": "image", "img_path": "YvA8UF0I37/tmp/6881c397a3ef86d0f0f4eb43c9010844d10dea7ec77dcf49d4aeeb228eafdc8e.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "Figure 9: Comparison of the exact PV algorithm (1) with Linearized PV (B.1) and Linearized $\\mathrm{PV}+$ sparse updates (3.3). ", "page_idx": 38}, {"type": "text", "text": "We can use different rules for choosing the subspace $S^{k}$ which produce different convergence rates and the final accuracy levels (9b). ", "page_idx": 38}, {"type": "image", "img_path": "YvA8UF0I37/tmp/955c5e3c5ea701f7182c850502303ef7b2337eff5c5ab31f9976571d5065a28e.jpg", "img_caption": ["(a) The behaviour of $L_{S^{k}}$ in Linearized $\\mathrm{PV}~+$ sparce updates. "], "img_footnote": [], "page_idx": 38}, {"type": "image", "img_path": "YvA8UF0I37/tmp/d3f207bf5f12e413a299d69915406ababd5ab7ced0ad1eb776fc7350e4f325a7.jpg", "img_caption": ["(b) Degradation of dimensionality of $V(x)$ for Linearized PV algorithm (B.1). "], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "Figure 10: Comparison of the exact PV algorithm (1) with Linearized PV (B.1) and Linearized $\\mathrm{PV}+$ Sparse Updates (3.3). ", "page_idx": 38}, {"type": "text", "text": "In large scale experiments we used sparse updates with $S^{k}$ being chosen greedily based on the absolute values of the entries of gradient vector $|\\nabla_{i}\\phi(y^{k})|-$ green line. Here we demonstrated that other strategies based on random uniform sampling or random proportional to $\\left|\\nabla_{i}\\phi(y^{k})\\right|$ sampling can be even more advanced providing better final accuracy at cost of larger number of iterations to converge \u2013 purple and orange lines (Fig. 9b). ", "page_idx": 39}, {"type": "text", "text": "We can see that Linearized PV with sparse updates can converge to even a better accuracy than the exact PV algorithm. The problem of Linearized PV algorithm is that we come to the local minimum and cannot get out of that minima because of the small value of the gradient (we are near to a real solution) and small stepsize. ", "page_idx": 39}, {"type": "text", "text": "Linearized PV with sparse updates allows us to mitigate this problem by reducing the subspace from $\\mathbb{R}^{d}$ to $S^{k}$ in which we are solving this optimization problem. This allows us to greatly reduce the local Lipschitz constant from $L$ to $L_{S^{k}}$ and hence significantly increase the stepsize $\\gamma=1/L_{S^{k}}$ . We demonstrated how $L_{S^{k}}$ changes for Linearized PV with different sparse updates sampling methods (Fig. 10a). ", "page_idx": 39}, {"type": "text", "text": "R $\\mathbf{PV}^{+}$ Algorithm ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "We can have degradation of $|V(x^{k})|$ during both the $\\mathbf{P}$ and $\\mathrm{v}$ steps. Here are examples for both of them with optimizing simple quadratic objective $\\begin{array}{r}{\\phi(\\boldsymbol{x})=\\sum_{i=1}^{d}\\left(x_{i}-x_{i}^{\\star}\\right)}\\end{array}$ (case when all $a_{i}$ , $i\\in[d]$ in (18) are equal to one). ", "page_idx": 39}, {"type": "text", "text": "1. Degradation during the P step: Let $x^{\\star}=\\{0,2,1\\}$ , $x^{0}=\\{x,x,y\\}$ , so $|V(x^{0})|=2$ , then after the $\\mathbf{P}$ step we will have $\\bar{y^{0}}=\\{1,1,1\\}$ , hence $|V(y^{0})|=1$ . ", "page_idx": 39}, {"type": "text", "text": "2. Degradation during the $\\mathbf{V}$ step: Let $x^{\\star}=\\{2,10,0,11\\}$ , $x^{0}=\\{x,y,y,z\\}$ , so $|V(x^{0})|=$ 3, then after the $\\mathbf{P}$ step we will have $y^{0}=\\{2,5,5,11\\}$ . Finally, after the $\\mathrm{v}$ step we have $x^{1}=\\{2,11,2,11\\}$ , so $|V(x^{1})|=2$ . ", "page_idx": 39}, {"type": "text", "text": "In real experiments we observe decreasing of $|V(x)|$ . You can observe this phenomena with Linearized PV algorithm (10b). As we can see we have a big decreasing of $|V(x)|$ during the first several iterations. We can use this gap to find even better solution with improved final accuracy and faster convergence rate. ", "page_idx": 39}, {"type": "text", "text": "To add additional unique element in $V(x)$ we considered the modification of $\\mathrm{v}$ step: $\\mathrm{V}^{+}$ step, where we allow $|V(x)|$ to become larger up to some upper bound. ", "page_idx": 39}, {"type": "text", "text": "Let $\\phi(\\cdot)$ is a mapping $\\phi(\\cdot):\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ and the vector $x^{\\star}\\in\\mathbb{R}^{d}$ be the optimal point: $\\nabla\\phi(\\boldsymbol{x}^{\\star})=0$ . Let the vector $x\\in\\mathbb{R}_{<c}^{d}$ and define the set $W(x,x^{\\star},c)$ such that it contains at most $c-|V(x)|$ unique elements from $V\\bar{(}x^{\\star})$ without elements from $V(x)$ : ", "page_idx": 39}, {"type": "equation", "text": "$$\nW(x,x^{\\star},c)=V(x^{\\star})\\setminus V(x):\\quad|W(x,x^{\\star},c)|=c-|V(x)|\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Algorithm 8 PV+ Algorithm ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "1: Parameters: starting point $\\boldsymbol x^{0}\\in\\mathbb R_{\\leq c}^{d}$ , the optimal point $x^{\\star}$ , maximal number of distinct values $c$ c, ", "page_idx": 39}, {"type": "text", "text": "2: for $k=0,1,\\dots$ do   \n3: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall^{k}=\\arg\\operatorname*{min}_{y\\in\\mathbb{R}^{d}}\\left\\{\\phi(y):P(y)\\supseteq P(x^{k-1})\\right\\}}\\\\ &{x^{k+1}=\\arg\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\left\\{\\phi(x):V(x)\\subseteq V(y^{k})\\bigcup W(y^{k},x^{\\star},c)\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "5: end for ", "page_idx": 39}, {"type": "text", "text": "Theorem R.1. Assume $\\phi$ is bounded below, and let $\\boldsymbol{x}^{0}\\in\\mathbb{R}_{\\leq\\hat{c}}^{d}$ . Then the algorithm $P V^{+}$ has the following guarantees ", "page_idx": 39}, {"type": "equation", "text": "$$\n(i)\\ y^{k},x^{k}\\in\\mathbb{R}_{\\leq\\hat{c}}^{d}\\quad f o r\\,a l l\\,k\\geq0,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "(iii) the sequence $\\{\\phi(\\boldsymbol{x}^{k})\\}_{k\\geq0}$ converges to some value, which is smaller or equal to one produced by $P V$ algorithm ", "page_idx": 39}, {"type": "text", "text": "Proof. Part (ii): Since ", "page_idx": 40}, {"type": "equation", "text": "$$\nx^{k+1}=\\arg\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\left\\{\\phi(x):V(x)\\subseteq V(y^{k})\\bigcup W(y^{k},x^{\\star},c)\\right\\}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and because $x\\;=\\;y^{k}$ satisfies the constraint $V(x)\\;\\subseteq\\;V(y^{k})\\bigcup W(y^{k},x^{\\star},c)$ , we conclude that $\\phi(x^{k+1})\\leq\\phi(y^{k})$ . ", "page_idx": 40}, {"type": "text", "text": "The rest of the proof is identical to (A.1). ", "page_idx": 40}, {"type": "text", "text": "S Broader Impact ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "The main impact of our work, both positive and negative, is in the ability to deploy higher-quality LLMs to run on memory-limited devices like desktops, laptops, and phones. On the positive side, this would allow practitioners to develop offilne LLM applications (e.g. translate service), lower-latency chat assistants that are not dependant on network latency, or privacy-sensitive LLM applications where the user\u2019s private data never leaves their device. Furthermore, this can facilitate the creation of free open-source software based on LLMs by eliminating the need to maintain costly inference servers on the backend. Since phones are everywhere and LLMs are powerful general-purpose tools, PV-tuned models could significantly impact how the general population uses LLMs to complete tasks. ", "page_idx": 40}, {"type": "text", "text": "However, LLMs are still a dual-use technology with the potential for significant beneftis and serious harm. Risks range from deliberate misuse (e.g. spam generation) and accidental misuse to negative economic side-effects. An upper bound on these risks is that PV tuning does not create new (potentially risky) LLM capabilities, merely making existing ones more accessible. ", "page_idx": 40}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: Our main claims are that the PV-tuning algorithm 1) achievstate-of-the-artart quantized LLM quality for 1-2 bits per parameter (backed by Section 4.3), 2) Pareto optimal at around 2 bits (also Section 4.3), 3) and is compatible with various methods (Section 4.2). In the introduction, we also claim that the advanced quantized representations, such as those having sparse outliers, do not give significant benefit on top of simple vector quantization with fine-tuning: this part is backed by 4.1. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 41}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: We discuss the methodological limitations of our study near the end, after Section 4.3. We also explain limitations for practitioners in Section 3.4. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when the image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to addressing problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 41}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: We carefully introduced the assumptions (e.g. that $\\phi(\\cdot)$ is $\\mathrm{L}$ -smooth) and provided proofs in appendix. To the best of our knowledge, these proofs are both correct and complete. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided the in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 42}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: We train open-access LLMs on open datasets and release our full training code. We do our best to provide instructions and hyperparameters in the code, though running our algorithm in different conditions may require basic tuning. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 42}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 43}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: As we state above, we release the full implementation for the PV algorithm with requisite instructions. We do not introduce new datasets and use openly available ones. We also plan to release the main quantized models in the non-anonymized version of the paper, since it would be impractical to upload them with the supplementary zip archive. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means the paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 43}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Justification: We describe our setup in Sections 3.4 and 4, with additional hyperparameters baked into the supplementary code. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, the in appendix, or as supplemental material. ", "page_idx": 43}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [No] ", "page_idx": 43}, {"type": "text", "text": "Justification: We report error bars for small-scale experiments Q. For full fine-tuning runs, we do not include error bars since running those would be prohibitively costly for us. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar and then state that they have a $96\\%C1$ if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 44}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type computing workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We report the hardware setting, calibration setting, time, and memory requirements in Section 4, which is sufficient for practitioners to reproduce our results. We omit some details, e.g. which runs were restarted due to unrelated server infrastructure issues. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute worker CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required mocomputingute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 44}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: Our research is focused on the base capability and accessibility of LLMs. While working on LLMs always has potential externalities, our specific work adheres to the ethics guidelines. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 44}, {"type": "text", "text": "The authors should make sure to preserve anonymity (eg. if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 45}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: While our work is more concerned with fundamental matters of discrete optimization and LLM quantization, we provide a brief overview of its societal impacts in Appendix S. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact on the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 45}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pre-trained language models, image generators, or scraped datasets)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: Our work does not release any newer models, and quantizing existing models typically results in a less capable (and therefore less risky) model. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make the best faith effort. ", "page_idx": 45}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited, and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We use academically published artifacts (datasets, models, etc) and cite their respective authors. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 46}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We release the code and provide documentation in the form of README and detailed docstrings. Both are included in the supplementary archive. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 46}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: We do not use human subjects in our experiments. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 46}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 46}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: We did not conduct any research on human subjects. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 47}]