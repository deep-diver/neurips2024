[{"type": "text", "text": "Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Matthew Zurek ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yudong Chen ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Computer Sciences University of Wisconsin-Madison matthew.zurek@wisc.edu ", "page_idx": 0}, {"type": "text", "text": "Department of Computer Sciences University of Wisconsin-Madison yudong.chen@wisc.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the sample complexity of learning an $\\varepsilon$ -optimal policy in an averagereward Markov decision process (MDP) under a generative model. For weakly communicating MDPs, we establish the complexity bound $\\widetilde{O}\\left(S A\\frac{\\mathsf{H}}{\\varepsilon^{2}}\\right)$ , where $\\mathsf{H}$ the span of the bias function of the optimal policy and $S A$ is the cardinality of the state-action space. Our result is the first that is minimax optimal (up to log factors) in all parameters $S,A,\\mathsf{H}$ , and $\\varepsilon$ , improving on existing work that either assumes uniformly bounded mixing times for all policies or has suboptimal dependence on the parameters. We also initiate the study of sample complexity in general (multichain) average-reward MDPs. We argue a new transient time parameter $\\textsf{B}$ is necessary, establish an $\\begin{array}{r}{\\widetilde{O}\\left(S A\\frac{\\mathsf{B}+\\mathsf{H}}{\\varepsilon^{2}}\\right)}\\end{array}$ complexity bound, and prove a matching (up to log factors) minimax lower bound. Both results are based on reducing the average-reward MDP to a discounted MDP, which requires new ideas in the general setting. To optimally analyze this reduction, we develop improved bounds for $\\gamma$ discounted MDPs swing hat $\\begin{array}{r}{\\tilde{O}\\bigl(S A\\frac{\\mathsf{H}}{(1-\\gamma)^{2}\\varepsilon^{2}}\\bigr)}\\end{array}$ and $\\begin{array}{r}{\\widetilde{O}\\big(S A\\frac{{\\sf B}+{\\sf H}}{(1-\\gamma)^{2}\\varepsilon^{2}}\\big)}\\end{array}$ 1-)e2 ) samples suffice to learn $\\varepsilon$ -optimal policies in weakly communicating and in general MDPs, respectively. Both these results circumvent the well-known minimax lower bound () ) for -discounted MDPs, and establish a quadraticrther than cubic horizon dependence for a fixed MDP instance. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The paradigm of Reinforcement learning (RL) has demonstrated remarkable successes in various sequential learning and decision-making problems. Empirical successes have motivated extensive theoretical study of RL algorithms and their fundamental limits. The RL environment is commonly modeled as a Markov decision process (MDP), where the objective is to find a policy $\\pi$ that maximizes the expected cumulative rewards. Different reward criteria are considered, such as the finite horizon total reward $\\mathbb{E}^{\\pi}\\big[\\sum_{t=0}^{T}R_{t}\\big]$ and the infnite horizontl discounted reward $\\mathbb{E}^{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]$ witha discount factor $\\gamma<1$ The finite horizon criterion only measures performance for $T$ steps, and the discounted criterion is dominated by rewards from the first 1 time steps. In many situations where the long-term performance of the policy $\\pi$ is of interest, we may prefer to evaluate policies by their long-run average reward $\\begin{array}{r}{\\operatorname*{lim}_{T\\rightarrow\\infty}(1/T)\\mathbb{E}^{\\pi}\\left[\\sum_{t=0}^{T-1}R_{t}\\right]}\\end{array}$ ", "page_idx": 0}, {"type": "text", "text": "A foundational theoretical problem in RL is the sample complexity for learning a near-optimal policy using a generative model of the MDP [10], meaning the ability to obtain independent samples of the next state given any initial state and action. For the finite horizon and discounted reward criteria, the sample complexity of this task has been thoroughly studied (e.g., [2, 3, 15, 19, 1, 12]). However, despite significant effort (reviewed in Section 1.1), the sample complexity of the average reward setting is unresolved in existing literature. ", "page_idx": 0}, {"type": "text", "text": "Our contributions _ In this paper, we resolve the sample complexity of weakly communicating Average-Reward MDPs (AMDP) in terms of $\\mathsf{H}:=\\|h^{\\star}\\|_{\\mathrm{span}}$ , the span of the bias (a.k.a. relative value function) of the optimal policy. We show that ${\\widetilde{O}}(S A\\mathsf{H}/\\varepsilon^{2})$ samples suffice to find an $\\varepsilon$ -optimal policy of a weakly communicating MDP with $S$ states and $A$ actions. This bound, presented in Theorem 2, is the first that matches the minimax lower bound $\\widetilde{\\Omega}(S A\\mathsf{H}/\\varepsilon^{2})$ up to log factors. ", "page_idx": 1}, {"type": "text", "text": "Furthermore, we initiate the study of sample complexity for average-reward general $M D P s$ which refers to the class of all finite-space MDPs without any restrictions [14]. General MDPs are not necessarily weakly communicating and all their optimal policies may be multichain. In this general setting, we demonstrate the span H alone cannot characterize the sample complexity, as the lower bound in Theorem 4 exhibits instances which require $\\gg{\\mathsf{H}}S A/\\varepsilon^{2}$ samples. This observation motivates our introduction of a new transient time bound parameter B, which in conjunction with $\\mathsf{H}$ captures the sample complexity of general average-reward MDPs. Specifically, our Theorem 8 shows that $\\begin{array}{r}{\\widetilde{O}\\left(S A\\frac{\\mathsf{\\tilde{B}+H}}{\\varepsilon^{2}}\\right)}\\end{array}$ samples sfieto larm an $\\varepsilon,$ opmalplyadhrmvid minimax lower bound of $\\Omega\\left(S A\\frac{{\\sf B}+{\\sf H}}{\\varepsilon^{2}}\\right)$ We remark that it istrivially impossible to achieve low regret in standard online settings of general MDPs, since the agent may become trapped in a closed class of low reward states [4]. The simulator setting is natural for studying general MDPs since it avoids this fatal issue, although the existence of multiple closed classes with different long-run rewards still plays a fundamental role in the minimax sample complexity, as refected in the dependence on B. ", "page_idx": 1}, {"type": "text", "text": "To establish the above upper bounds, we adopt the reduction-to-discounted-MDP approach [9, 20], and improve on prior work by developing enhanced sample complexity bounds for $\\gamma$ -discounted MDPs (DMDPs). We improve the analysis of variance parameters related to DMDPs using a new multistep variance Bellman equation, which is applied in a recursive manner to bound the variance of near-optimal policies. For general (multichain) MDPs, we further utilize law-of-totalvariance ideas to bound the total variance contribution from transient states, which present new challenges significantly different to their behavior in the weakly communicating setting. Our averageto-discounted reduction also requires new techniques, because many structural properties used in earlier reduction arguments no longer hold for general MDPs. Our analysis leads to DMDP sample complexities of O(SA(1)2 (1-)2=2 ) and O(SA B+H) to learn $\\varepsilon$ -optimal policies in weakly communicating and general MDPs, respectively. Notably, the latter bound, valid for all MDPs, cireumvents the existing lower bound $\\tilde{\\tilde{\\Omega}}\\Bigl(\\frac{S A}{(1\\!-\\!\\gamma)^{3}\\varepsilon^{2}}\\Bigr)$ [3, 15]. Whereas this minimax lower bound alowsthedearytchetansioni $P$ based on $\\gamma$ with $\\begin{array}{r}{\\textsf{B}\\approx\\frac{1}{1-\\gamma}}\\end{array}$ [3, Theorem 3], our result reflects the complexity of a fixed MDP $P$ through its parameters $\\mathsf{H},\\mathsf{B}$ and a quadratic dependence on the effctivehorizn $\\frac{1}{1-\\gamma}$ . This fixed- $P$ complexity is essential for our particular algorithmic approach, where the reduction discount $\\gamma$ is chosen depending on $P$ . It is also a more relevant framework in general for many RL problems where the discount factor is tuned for best performance on a particular instance. ", "page_idx": 1}, {"type": "text", "text": "1.1 Comparison with related work on average-reward MDPs ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We summarize in Table 1 existing sample complexity results for average reward MDPs. ", "page_idx": 1}, {"type": "text", "text": "Various parameters have been used to characterize the sample complexity of average reward MDPs, including the diameter $D$ of the MDP, the uniform mixing time bound $\\tau_{\\mathrm{unif}}$ for all policies, and the span $\\mathsf{H}$ of the optimal bias; formal definitions are provided in Section 2. All sample complexity upper bounds involving $\\tau_{\\mathrm{unif}}$ require the strong assumption that all stationary deterministic policies have finite mixing times. Otherwise, $\\tau_{\\mathrm{unif}}\\,=\\,\\infty$ by definition, which for example occurs if some policy induces a periodic Markov chain. It is also possible to have $D=\\infty$ , while H and our newly introduced B are always finite for finite state-action spaces. As shown in [20], there is generally no relationship between $D$ and $\\tau_{\\mathrm{unif}}$ ; they can each be arbitrarily larger than the other. On the other hand, it has been shown that $\\mathsf{H}\\leq D$ [4] and that $\\mathsf{H}\\leq8\\tau_{\\mathrm{unif}}$ [20]. Therefore, either of the first two minimax lower bounds in Table 1 (which both use hard instances that are weakly communicating) imply a lower bound of $\\widetilde{\\Omega}\\left(S A_{\\varepsilon^{2}}^{\\mathsf{H}}\\right)$ and thus the minimax optimality of our Theorem 2. ", "page_idx": 1}, {"type": "text", "text": "To the best of our knowledge, no prior work has considered the average-reward sample complexity of general (potentially multichain) MDPs. Existing results make assumptions at least as strong as weakly communicating or uniformly bounded mixing times. ", "page_idx": 1}, {"type": "table", "img_path": "pGEY8JQ3qx/tmp/9dccaaf6bddab4c338d1cf7d086d66b87d3d5d07e7efee14bb9e6e7b2e4d5597.jpg", "table_caption": [], "table_footnote": ["Table 1: Algorithms and sample complexity bounds for average reward MDPs with $S$ states and $A$ actions. The goal is finding an $\\varepsilon,$ optimal policy under a generative model. Here $\\mathsf{H}:=\\|h^{\\star}\\|_{\\mathrm{span}}$ .s. the span of the optimal bias, $\\tau_{\\mathrm{unif}}$ is a uniform upper bound on mixing times of all policies, and $D$ is the MDP diameter, with the relationships $\\mathsf{H}\\leq8\\tau_{\\mathrm{unif}}$ and $\\mathsf{H}\\leq D$ . B is the transient time parameter. "], "page_idx": 2}, {"type": "text", "text": "The work [9] was the first to develop an algorithm based on reduction to a discounted MDP with a discount factor of $\\begin{array}{r}{\\gamma\\,=\\,1\\,-\\,\\frac{\\varepsilon}{\\tau_{\\mathrm{unif}}}}\\end{array}$ . Their argument was improved in [20], which improved the uniform mixing assumption to only assuming a weakly communicating MDP, and used a smaller discount factor $\\begin{array}{r}{\\gamma=1-\\frac{\\varepsilon}{\\mathsf{H}}}\\end{array}$ . These arguments both make essential use of the fact that the optimal gain is independent of the starting state, which does not hold for general MDPs. After analyzing the reductions, both [9] and [20] then solved the discounted MDPs by appealing to the algorithm from [12]. To the best of our knowledge, the algorithm of [12] is the only known algorithm for discounted MDPswhildwrkwitretn,asthdtioahrq $\\frac{\\varepsilon}{1\\!-\\!\\gamma}$ optimal plicey from the discounted MDP, and other known algorithms for discounted MDPs do not permit such large suboptimality levels. (We discuss algorithms for discounted MDPs in more detail below.) Other algorithms for average-reward MDPs are considered in [9, 13, 26]. The above results fall short of matching the minimax lower bounds. ", "page_idx": 2}, {"type": "text", "text": "While preparing this manuscript, we became aware of [22], which considers the uniform mixing setting and obtains a minimax optimal sample complexity $\\begin{array}{r}{\\widetilde{O}\\left(S A\\frac{\\tau_{\\mathrm{unif}}}{\\varepsilon^{2}}\\right)}\\end{array}$ in terms of $\\tau_{\\mathrm{unif}}$ Although developed independently, their work and ours have several similarities. We both utilize discounted reductions and observe that it is possible to improve the sample complexity of the resulting DMDP task by improving the analysis of variance parameters. They accomplish the improvement by leveraging the uniform mixing assumption, whereas we make use of the low span of the optimal policy. Note that $\\mathsf{H}\\leq8\\tau_{\\mathrm{unif}}$ holds in general and there exist MDPs with $\\mathsf{H}\\ll\\tau_{\\mathrm{unif}}=\\infty$ , so our Theorem 2 is strictly stronger than the result of [22]. ", "page_idx": 2}, {"type": "text", "text": "1.2 Comparison with related work on discounted MDPs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We discuss a subset of results for discounted MDPs in the generative setting. Several works [15, 19, 1, 12] obtain the minimax optimal sample complexity of $\\begin{array}{r}{\\tilde{\\tilde{O}}\\big(S A\\frac{1}{(1-\\gamma)^{3}\\varepsilon^{2}}\\big)}\\end{array}$ A(-)\\*gza) for finding an -optimal policy w.r.t. the discounted reward. However, only [12] is able to show this bound for the full range Oof $\\begin{array}{r}{\\varepsilon\\in(0,\\frac{1}{1-\\gamma}]}\\end{array}$ . As mentioned, the reduction from average reward MDPs requires a large $\\varepsilon$ in the resulting discounted MDP, making it unsurprising that all of [9, 20, 22] as well as our Algorithm 1 essentially use their algorithm. The matching lower bound is established in [15, 3]. ", "page_idx": 2}, {"type": "text", "text": "As mentioned earlier, both we and the authors of [22, 21] independently observed that the $\\begin{array}{r}{\\widetilde\\Omega\\Bigl(S A\\frac{1}{(1-\\gamma)^{3}\\varepsilon^{2}}\\Bigr)}\\end{array}$ sample complexitylowerbound can beireumventedi thesetinsthat aise under the average-to-discounted reductions. The authors of [22, 21] assume uniform mixing and obtain a discounted MDP sample complexity of A)g), frst in [21 by modifying the algorithm of [19], and then in [22] under a wider range of $\\varepsilon$ by instead modifying the analysis of [12]. The work [21] also proves a matching lower bound. Our Theorem 1 for discounted MDPs attains a sample complexity of $\\begin{array}{r}{\\tilde{O}\\bigl(S A\\frac{\\mathsf{H}}{(1-\\gamma)^{2}\\varepsilon^{2}}\\bigr)}\\end{array}$ A(l-)-z ) assuming only that the MDP is weakly communicating. Again, in light of the relationship that $\\mathsf{H}\\leq8\\tau_{\\mathrm{unif}}$ , our results are strictly better (ignoring constants), and their lower bound also establishes the optimality of our Theorem 1. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "2  Problem setup and preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A Markov decision process (MDP) is given by a tuple $(S,{\\mathcal{A}},P,r)$ , where $\\boldsymbol{S}$ is the finite set of states, $\\boldsymbol{\\mathcal{A}}$ is the finite set of actions, $P:S\\times A\\to\\Delta(S)$ is the transition kernel with $\\Delta(S)$ denoting the probability simplex over $\\boldsymbol{S}$ , and $r:S\\times A\\to[0,1]$ is the reward function. Let $S:=|S|$ and $A:=|{\\mathcal{A}}|$ denote the cardinality of the state and action spaces, respectively. Unless otherwise noted, all policies considered are stationary Markovian policies of the form $\\pi^{\\prime}\\colon S\\ \\rightarrow\\ \\Delta(A)$ . For any initial state $s_{0}\\in\\mathcal S$ and policy $\\pi$ , we let $\\mathbb{E}_{s_{0}}^{\\pi}$ denote the expectation with respect to the probability distribution over trajectories $(S_{0},A_{0},S_{1},A_{1},\\ldots)$ where $S_{0}\\;=\\;s_{0}$ $A_{t}\\,\\sim\\,\\pi(S_{t})$ , and $S_{t+1}\\,\\sim\\,P(\\cdot\\,\\mid\\,S_{t},A_{t})$ Equivalently, this is the expectation with respect to the Markov chain induced by $\\pi$ starting in state $s_{0}$ , with the transition probability matrix $P_{\\pi}$ given by $\\begin{array}{r}{\\textstyle(P_{\\pi})_{s,s^{\\prime}}:=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)P(s^{\\prime}\\mid s,a)}\\end{array}$ .We also define $\\textstyle(r_{\\pi})_{s}:=\\sum_{a\\in{\\mathcal{A}}}\\pi(a|s)r(s,a)$ . We occasionally treat $P$ as an $(S\\times A)$ -by- $\\mathcal{S}$ matrix where $P_{s a,s^{\\prime}}=P(s,a,s^{\\prime})$ . We also let $P_{s a}$ denote the row vector such that $P_{s a}(s^{\\prime})=P(s,a,s^{\\prime})$ For any $s\\,\\in\\,S$ and any bounded function $X$ of the trajectory, we define the variance $\\mathbb{V}_{s}^{\\pi}\\left[X\\right]:=$ $\\mathbb{E}_{s}^{\\pi}\\left(X-\\mathbb{E}_{s}^{\\pi}\\left[X\\right]\\right)^{2}$ , with its vector version $\\mathbb{V}^{\\pi}\\left[X\\right]\\in\\mathbb{R}^{S}$ given by $(\\mathbb{V}^{\\pi}\\left[X\\right])_{s}=\\mathbb{V}_{s}^{\\pi}\\left[X\\right]$ . For $s\\in S$ let $\\boldsymbol{e}_{s}\\in\\mathbb{R}^{S}$ be the vector that is all O except for a $1$ in entry $s$ Let $\\mathbf{1}\\in\\mathbb{R}^{S}$ be the all-one vector. For each $\\boldsymbol{v}\\in\\mathbb{R}^{S}$ , define the span semi-norm $\\begin{array}{r}{\\|v\\|_{\\mathrm{span}}:=\\operatorname*{max}_{s\\in S}v(s)-\\operatorname*{min}_{s\\in S}v(s)}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "Discounted reward criterion  A discounted MDP is a tuple. $(S,{\\mathcal{A}},P,r,\\gamma)$ , where $\\gamma\\in(0,1)$ is the discount factor. For a stationary policy $\\pi$ , the (discounted) value function $V_{\\gamma}^{\\pi}:{\\cal S}\\rightarrow[0,\\infty)$ is defined, for each $s\\ \\in\\ S$ as $\\begin{array}{r}{V_{\\gamma}^{\\pi}(s)\\,:=\\,\\mathbb{E}_{s}^{\\pi}\\,[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}]}\\end{array}$ where $R_{t}\\,=\\,r(S_{t},\\dot{A_{t}})$ is the reward received at time $t$ It is well-known that there exists an optimal policy $\\pi_{\\gamma}^{\\star}$ that is deterministic and satisfies $V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}(s)=V_{\\gamma}^{\\star}(s):=\\operatorname*{sup}_{\\pi}V_{\\gamma}^{\\pi}(s)$ for all $s\\in S$ [14]. In discounted MDPs the goal is to compute an $\\varepsilon$ -optimal policy, which we define as a policy $\\pi$ satisfying $\\left\\|V_{\\gamma}^{\\pi}-V_{\\gamma}^{\\star}\\right\\|_{\\infty}\\leq\\varepsilon$ We define one more variance parameter $\\mathbb{V}_{P_{\\pi}}\\,\\left[V_{\\gamma}^{\\pi}\\right]\\in\\mathbb{R}^{S}$ , specific to a given policy $\\pi$ , by $\\widetilde{\\big(}\\mathbb{V}_{P_{\\pi}}\\,\\left[V_{\\gamma}^{\\pi}\\right]\\big)_{s}:=$ $\\begin{array}{r}{\\sum_{s^{\\prime}\\in S}\\left(P_{\\pi}\\right)_{s,s^{\\prime}}\\left[V_{\\gamma}^{\\pi}(s^{\\prime})-\\sum_{s^{\\prime\\prime}}\\left(P_{\\pi}\\right)_{s,s^{\\prime\\prime}}V_{\\gamma}^{\\pi}(s^{\\prime\\prime})\\right]^{2}}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "Average-reward criterion  In an MDP $(S,{\\mathcal{A}},P,r)$ , the average reward per stage or the gain of a policy $\\pi$ startin from state $s$ is defined as $\\begin{array}{r}{\\rho^{\\pi}(s):=\\operatorname*{lim}_{T\\rightarrow\\infty}\\frac{1}{T}\\mathbb{E}_{s}^{\\pi}\\big[\\sum_{t=0}^{T-1}R_{t}\\big]}\\end{array}$ The bias function of anystationarypliey $\\pi$ ish $\\begin{array}{r}{h^{\\pi}(s):=\\mathrm{C}\\mathrm{-}\\!\\operatorname*{lim}_{T\\rightarrow\\infty}\\mathbb{E}_{s}^{\\pi}\\big[\\sum_{t=0}^{T-1}\\big(R_{t}-\\rho^{\\pi}(S_{t})\\big)\\big]}\\end{array}$ WwhereC-lin dentes the Cesaro limit. When the Markov chain induced by $P_{\\pi}$ is aperiodic, C-lim can be replaced with the usual limit. For any policy $\\pi$ , its $\\rho^{\\pi}$ and $h^{\\pi}$ satisfy $\\rho^{\\pi}=P_{\\pi}\\rho^{\\pi}$ and $\\rho^{\\pi}+h^{\\pi}=r_{\\pi}+P_{\\pi}h^{\\pi}$ ", "page_idx": 3}, {"type": "text", "text": "A policy $\\pi^{\\star}$ is Blackwell-optimal if there exists some discount factor $\\bar{\\gamma}\\in(0,1)$ such that for all $\\gamma\\geq\\bar{\\gamma}$ we have $V_{\\gamma}^{\\pi^{\\star}}\\geq V_{\\gamma}^{\\pi}$ for all policies $\\pi$ . Henceforth we let $\\pi^{\\star}$ denote some fixed Blackwell-optimal policy, which is guaranteed to exist when $S$ and $A$ are finite [14]. We define the optimal gain $\\rho^{\\star}\\in\\mathbb{R}^{S}$ by $\\rho^{\\star}(s)=\\operatorname*{sup}_{\\pi}\\rho^{\\pi}(s)$ and note that we have $\\rho^{\\star}=\\rho^{\\pi^{\\star}}$ . For all $s\\in S$ $\\rho^{\\star}(s)\\geq\\operatorname*{max}_{a\\in\\mathcal{A}}P_{s a}\\rho^{\\star}$ or equivalently $\\rho^{\\star}\\geq P_{\\pi}\\rho^{\\star}$ for all policies $\\pi$ (and this maximum is achieved by $\\pi^{\\star}$ ). We also define $h^{\\star}=$ $h^{\\bar{\\pi}^{\\star}}$ (and we note that this definition does not depend on which Blackwell-optimal $\\pi^{\\star}$ is used, if there are multiple). For all $s\\in S$ $\\rho^{\\star}$ and $h^{\\star}$ satisfy $\\begin{array}{r}{\\rho^{\\star}(s)+h^{\\star}(s)=\\operatorname*{max}_{a\\in\\mathcal{A}:P_{s a}\\rho^{\\star}=\\rho^{\\star}(s)}r_{s a}+P_{s a}h^{\\star},}\\end{array}$ known as the (unmodified) Bellman equation. ", "page_idx": 3}, {"type": "text", "text": "A weakly communicating MDP is such that the states can be partitioned into two disjoint subsets $S=S_{1}\\cup S_{2}$ such that all states in $S_{1}$ are transient under any stationary policy and within $S_{2}$ any state is reachable from any other state under some stationary policy. In weakly communicating MDPs $\\rho^{\\star}$ is a constant vector (all entries are equal), and thus $(\\rho^{\\star},h^{\\star})$ are also a solution to the modified Bellman equation $\\begin{array}{r}{\\rho^{\\star}(s)+h^{\\star}(s)=\\operatorname*{max}_{a\\in\\mathcal{A}}r_{s a}+P_{s a}h^{\\star}}\\end{array}$ When discussing weakly communicating MDPs we occasionally abuse notation and treat $\\rho^{\\star}$ as a scalar. A stationary policy is multichain if it induces multiple closed irreducible recurrent classes, and an MDP is called multichain if it contains such a policy. Weakly-communicating MDPs always contain some gain-optimal policy which is unichain (not multichain), but in general MDPs, all gain-optimal policies may be multichain and $\\rho^{\\star}$ may not be a constant vector. All uniformly mixing MDPs are weakly communicating. In the average reward setting, our goal is find an $\\varepsilon$ -optimal policy, defined as a policy $\\pi$ such that $\\|\\bar{\\rho}^{\\star}-\\rho^{\\pi}\\|_{\\infty}\\leq\\varepsilon$ ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Complexity parameters  Our most important complexity parameter is the span of the optimal bias function $\\mathsf{H}:=\\|h^{\\star}\\|_{\\mathrm{span}}$ . In addition, for general MDPs we introduce a new transient time parameter B, defined as follows. Let $\\Pi$ be the set of deterministic stationary policies. For each $\\pi\\in\\Pi$ , let $\\mathcal{R}^{\\pi}$ be the set of states which are recurrent in the Markov chain $P_{\\pi}$ , and let $\\mathcal{T}^{\\pi}=\\mathcal{S}\\setminus\\mathcal{R}^{\\pi}$ be the set of transient states. Let $T_{\\mathcal{R}^{\\pi}}=\\operatorname*{inf}\\left\\{t:S_{t}\\in\\mathcal{R}^{\\pi}\\right\\}$ be the first hitting time of a state which is recurrent under $\\pi$ We say an MDP satisfies the bounded transient time property with parameter $\\textsf{B}$ if for all policies $\\pi$ and states $s\\in S$ we have ${\\mathbb E}_{s}^{\\pi}\\left[{T_{\\mathcal R^{\\pi}}}\\right]\\leq{\\tt B}$ , or in words, the expected time spent in transient states (with respect to the Markov chain induced by $\\pi$ ) is bounded by $\\textsf{B}$ ", "page_idx": 4}, {"type": "text", "text": "We recall several other parameters used in the literature to characterize sample complexity. The diameter is defined as $D:=\\operatorname*{max}_{s_{1}\\neq s_{2}}\\operatorname*{inf}_{\\pi\\in\\Pi}\\mathbb{E}_{s_{1}}^{\\pi}\\left[\\eta_{s_{2}}\\right]$ where $\\eta_{s}$ denotes the hitting time of a state $s\\in S$ . For each policy $\\pi$ , if the Markov chain induced by $P_{\\pi}$ has a unique stationary distribution $\\nu_{\\pi}$ , we define the mixing time of $\\pi$ as $\\begin{array}{r}{\\tau_{\\pi}:=\\operatorname*{inf}\\left\\{t\\geq1:\\operatorname*{max}_{s\\in S}\\left\\|e_{s}^{\\top}\\left(P_{\\pi}\\right)^{t}-\\nu_{\\pi}^{\\top}\\right\\|_{1}\\leq\\frac{1}{2}\\right\\}}\\end{array}$ If all policies $\\pi\\in\\Pi$ satisfy this assumption, we define the uniform mixing time $\\tau_{\\mathrm{unif}}:=\\operatorname*{sup}_{\\pi\\in\\Pi}\\tau_{\\pi}$ Note that $D$ and $\\tau_{\\mathrm{unif}}$ are generally incomparable [20], while we always have $\\mathsf{H}\\leq D$ [4] and $\\mathsf{H}\\leq8\\tau_{\\mathrm{unif}}$ [20]. It is possible for $\\tau_{\\mathrm{unif}}=\\infty$ , for instance if there are any policies which induce periodic Markov chains. Also, $D=\\infty$ if there are any states which are transient under all policies. However, $\\mathsf{H}$ and B are finite in any MDP with $S,A<\\infty$ Also if $\\tau_{\\mathrm{unif}}$ is finite, Lemma 27 shows $\\mathsf{B}\\leq4\\tau_{\\mathrm{unif}}$ ", "page_idx": 4}, {"type": "text", "text": "We assume access to a generative model [10], also known as a simulator. This means we can obtain independent samples from $P(\\cdot\\mid s,a)$ for any given $s\\in{\\mathcal{S}},a\\in{\\mathcal{A}}$ ,but $P$ itself is unknown. We assume the reward function $r$ is deterministic and known, which is standard in generative settings (e.g., [1, 12]) since otherwise estimating the mean rewards is relatively easy. Specifically, to learn an $\\varepsilon$ -optimal policy for the discounted MDP, we would need to estimate each entry of $r$ to accuracy O((1 - )e), which requires a lower order number of samples O((\u00b1- (a). For this reason we assume (as in [20]) that $\\mathsf{H}\\geq1$ . Using samples from the generative model, our Algorithm 1 constructs an empirical transition kernel $\\widehat{P}$ . For a policy $\\pi$ , we use $\\bar{\\widehat V}_{\\gamma}^{\\pi}(s)$ to denote the value function computed with respect to the Markov chain with transition matrix ${\\widehat{P}}_{\\pi}$ (as opposed to $P_{\\pi}$ ). Our Algorithm 1 also utilizes a perturbed reward function $\\widetilde r$ , and we use the notation $V_{\\gamma,\\mathrm{p}}^{\\pi}(s)$ to denote a value function computed using this reward (and $P_{\\pi}$ ); more concretely, we replace $R_{t}$ with $\\widetilde{R}_{t}=\\widetilde{r}(S_{t},A_{t})$ in the definition above of $V_{\\gamma}^{\\pi}$ . We use the notation $\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\pi}$ when using $\\widehat{P}$ and $\\widetilde{r}$ simultaneously. ", "page_idx": 4}, {"type": "text", "text": "3   Main results for weakly communicating MDPs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our approach is based on reducing the average-reward problem to a discounted problem. We first present our algorithm and guarantees for the discounted MDP setting. As discussed in Subsection 1.1, our algorithm of choice, Algorithm 1, is essentially the same as the one presented in [12], with a slightly different perturbation level $\\xi$ . Algorithm 1 constructs an empirical transition kernel P using $n$ samples per state-action pair from the generative model, and then solves the resulting empirical (perturbed) MDP $(\\widehat{P},\\widetilde{r},\\gamma)$ As noted i 12, the perturbation ensures $\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}$ can be computed exactly in poly $\\textstyle\\bigl(\\frac{1}{1-\\gamma},S,A,\\log(1/\\delta\\varepsilon)\\bigr)$ time by multiple standard MDP solvers. We remark in passing that the $S A$ -by- $S$ transition matrix $\\widehat{P}$ has at most $n S A$ nonzero entries. ", "page_idx": 4}, {"type": "text", "text": "Our Theorem 1 provides an improved sample complexity bound for Algorithm 1 under the setting that the MDP is weakly communicating. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Sample Complexity of Weakly Communicating DMDP). Suppose the discounted MDP $(P,r,\\gamma)$ is weakly communicating, $\\begin{array}{r}{\\mathsf{H}\\le\\frac{1}{1-\\gamma}}\\end{array}$ and $\\varepsilon\\le{\\mathsf{H}}$ There exists a constant $C_{2}>0$ such that, for any $\\delta\\in(0,1)$ $\\begin{array}{r}{\\dot{n}\\geq C_{2}\\frac{\\mathsf{H}}{(1-\\gamma)^{2}\\varepsilon^{2}}\\log\\left(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\right)}\\end{array}$ then with probability at least $1-\\delta$ the policy $\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}$ output by Algorithm $^{\\,l}$ satisfies $\\left\\|V_{\\gamma}^{\\star}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\|_{\\infty}\\leq\\varepsilon$ ", "page_idx": 4}, {"type": "text", "text": "nput: Sample size per state-action pair $n$ , target accuracy $\\varepsilon$ , discount factor $\\gamma$   \n1: for each state-action pair $(s,a)\\in S\\times A$ do   \n2:  Collect $n$ samples $S_{s,a}^{1},\\ldots,S_{s,a}^{n}$ from $P(\\cdot\\mid s,a)$   \n3:  Form the empirical transition kermel $\\begin{array}{r}{\\widehat{P}(s^{\\prime}\\mid s,a)=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{I}\\{S_{s,a}^{i}=s^{\\prime}\\}}\\end{array}$ for all $s^{\\prime}\\in\\mathcal{S}$   \n4: end for   \n5: Set perturbation level $\\xi=(1-\\gamma)\\varepsilon/6$   \n6: Form perturbed reward $\\widetilde{r}=r+Z$ where $Z(s,a)\\overset{\\mathrm{i.i.d.}}{\\sim}\\mathrm{Unif}(0,\\xi)$   \n7: Compute a policy $\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}$ which is optimal for the perturbed empirical discounted MDP $(\\widehat{P},\\widetilde{r},\\gamma)$   \n8: return \u516d, ", "page_idx": 5}, {"type": "text", "text": "Since we observe $n$ samples for each state-action pair, Theorem 1 shows that a total number of $\\begin{array}{r}{\\widetilde{O}\\big(\\frac{\\mathsf{H}S A}{(1-\\gamma)^{2}\\varepsilon^{2}}\\big)}\\end{array}$ $\\varepsilon$ l $\\begin{array}{r}{\\widetilde{O}\\big(\\frac{S A}{(1-\\gamma)^{3}\\varepsilon^{2}}\\big)}\\end{array}$ complexity bound from [12] when the span is no larger than the effective horizon $\\textstyle{\\frac{1}{1-\\gamma}}$ This assumption holds in many situations, as can be seen by using the relationships $\\mathsf{H}\\leq D$ or $\\mathsf{H}\\leq8\\tau_{\\mathrm{unif}}$ On the other hand, in the regime with H > 1-'? , the existing bound (), also achieved by Algorithm 1, is superior. In this regime, the discounting effectively truncates the MDP at a short horizon 1 before the long-run behavior of the optimal policy (as captured by $\\mathsf{H}$ ) kicks in. ", "page_idx": 5}, {"type": "text", "text": "Proof highlights for Theorem $^{\\,I}$ . The key to obtaining this improved complexity is a careful analysiso certai instance-specie variance paramters. I sufice to bound $\\left\\|\\hat{V}_{\\gamma,\\mathrm{p}}^{\\pi_{\\gamma}^{\\star}}-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\|_{\\infty}$ and $\\left\\|\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\|_{\\infty}$ $O(\\varepsilon)$ $\\frac{S A}{(1\\!-\\!\\gamma)^{3}\\varepsilon^{2}}$ known law-of-total-variance argument [3, 1, 12], which ultimately yields a sample complexity like $\\begin{array}{r}{\\widetilde{O}\\left(\\sqrt{\\frac{S A}{(1-\\gamma)\\varepsilon^{2}}\\left\\|\\mathbb{V}^{\\pi_{\\gamma}^{\\star}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]\\right\\|_{\\infty}}\\right)}\\end{array}$ to bound $\\left\\|\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\pi_{\\gamma}^{\\star}}-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\|_{\\infty}\\leq O(\\varepsilon)$ From here, th variance of the cumulative discounted reward $\\begin{array}{r}{\\left|\\left|\\mathbb{V}^{\\pi_{\\gamma}^{\\star}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]\\right|\\right|_{\\infty}}\\end{array}$ is bounded by $\\frac{1}{(1\\!-\\!\\gamma)^{2}}$ sinethe total reward in a trajectory is within $[0,\\frac{1}{1-\\gamma}]$ . We insead seek to bound $\\begin{array}{r}{\\left\\|\\mathbb{V}^{\\pi_{\\gamma}^{\\star}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]\\right\\|_{\\infty}\\leq\\,O\\left(\\frac{\\sf H}{1-\\gamma}\\right)}\\end{array}$ Assume $\\mathsf{H}$ is an integer. The frst step is to decompose $\\mathbb{V}^{\\pi_{\\gamma}^{\\star}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]$ recursively like ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{V}^{\\pi_{\\gamma}^{\\star}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]=\\mathbb{V}^{\\pi_{\\gamma}^{\\star}}\\left[\\sum_{t=0}^{\\mathsf{H}-1}\\gamma^{t}R_{t}+\\gamma^{\\mathsf{H}}V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}(S_{\\mathsf{H}})\\right]+\\gamma^{2\\mathsf{H}}\\left(P_{\\pi_{\\gamma}^{\\star}}\\right)^{\\mathsf{H}}\\mathbb{V}^{\\pi_{\\gamma}^{\\star}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "(see our Lemma 13). This is a multi-step version of the standard variance Bellman equation (e.g., [16, Theorem 1]). Ordinarily an $\\mathsf{H}.$ -step expansion would not be useful, since the term $V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}(S_{\\mathsf{H}})$ by itself appears tohavefuctuationsonthedef $\\textstyle{\\frac{1}{1-\\gamma}}$ in the worst case depending on $S_{\\mathsf{H}}$ (note $S_{\\mathsf{H}}$ is the random state encunterd at tmeH).owever, inour setting, we should have $\\begin{array}{r}{V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}(S_{\\mathsf{H}})\\approx\\frac{1}{1-\\gamma}\\rho^{\\star}+}\\end{array}$ $h^{\\star}(S_{\\mathsf{H}})$ , reducing the magnitude of the random fluctuations to order $\\mathsf{H}=\\|h^{\\star}\\|_{\\mathrm{span}}$ (See Lemma 11 for a formalization of this approximation which first appeared in [23].) Therefore expansion to $\\mathsf{H}$ steps achieves the optimal tradeof betwen maintaining $\\begin{array}{r}{\\overleftarrow{\\mathbb{V}^{\\pi_{\\gamma}^{\\star}}}\\left[\\sum_{t=0}^{\\mathsf{H}-1}\\gamma^{t}R_{t}+\\gamma^{\\mathsf{H}}V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}(S_{\\mathsf{H}})\\right]\\leq O\\left(\\mathsf{H}^{2}\\right)}\\end{array}$ and minimizing $\\gamma^{2\\mathsf{H}}$ As desred thi ields $\\begin{array}{r}{\\left\\|\\mathbb{V}^{\\pi_{\\gamma}^{\\star}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]\\right\\|_{\\infty}\\leq O\\Big(\\frac{\\mathsf{H}^{2}}{1-\\gamma^{2\\mathsf{H}}}\\Big)=O\\Big(\\frac{\\mathsf{H}}{1-\\gamma}\\Big)}\\end{array}$ where $\\begin{array}{r}{\\frac{1}{1-\\gamma^{2\\mathsf{H}}}\\le O\\Bigl(\\frac{1}{\\mathsf{H}(1-\\gamma)}\\Bigr)}\\end{array}$ requires $\\begin{array}{r}{\\frac{1}{1-\\gamma}\\geq{\\sf H}}\\end{array}$ Se Lemma 15 for the complete argument. ", "page_idx": 5}, {"type": "text", "text": "We would like to use a similar argument as above to bound the second term $\\left\\|\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\|_{\\infty},$ which is the \u201cevaluation error' of the empirically optimal policy $\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}$ However, applying the same argument would give a bound in terms of $\\left\\|V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\|_{\\mathrm{span}}$ , which, unlike for the analogous term involving the true optimal policy $\\pi_{\\gamma}^{\\star}$ , is not a priori bounded in terms of $H$ . (If we instead assumed uniform mixing, we could immediately bound this by $O(\\tau_{\\mathrm{unif}}).$ ) Thus, to control the variance associated with evaluating $\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}$ , we are able to recursively bound $\\begin{array}{r}{\\left\\|V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\|_{\\mathrm{span}}\\leq O\\big(H+\\left\\|\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\|_{\\infty}\\big)}\\end{array}$ , which can be shown to yield the desired sample complexity. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "Now we present our main result for the average-reward problem in the weakly communicating setting. Applied in this setting with a DMDP target accuracy of $\\overline{{\\varepsilon}}=\\mathsf{H}$ our Algorithm 2 reduces the problem ${\\overline{\\gamma}}.$ discounted MDP with $\\begin{array}{r}{\\overline{{\\gamma}}=1-\\frac{\\varepsilon}{12\\mathsf{H}}}\\end{array}$ and then all Algorithm 1 with target accuracy $\\mathsf{H}$ ", "page_idx": 6}, {"type": "table", "img_path": "pGEY8JQ3qx/tmp/b0fd8daa8a5441f428fc0f15833db4562192d16842b6e26932b16d81712ed48c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "We have the following sample complexity bound for Algorithm 2 ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 (Sample Complexity of Weakly Communicating AMDP). Suppose the MDP $(P,r)$ is weakly communicating.There exists a constant $C_{1}~>~0$ such that for any $\\delta,\\varepsilon~\\in~(0,\\dot{1})$ $i f$ $\\begin{array}{r}{n\\geq C_{1}\\frac{\\sf H}{\\varepsilon^{2}}\\log\\left(\\frac{S A\\sf H}{\\delta\\varepsilon}\\right)}\\end{array}$ and we callAlgorithm2with $\\overline{{\\varepsilon}}=\\mathsf{H}$ then with,probailit a least $1-\\delta$ the output policy $\\widehat{\\pi}^{\\star}$ satisfies the elementwiseinequality $\\rho^{\\star}-\\rho^{\\widehat{\\pi}^{\\star}}\\leq\\varepsilon\\mathbf{1}$ ", "page_idx": 6}, {"type": "text", "text": "Again,sice we observe $n$ samples for achstate-action par thisresit shows that $\\widetilde{O}\\left(\\frac{\\mathsf{H}S A}{\\varepsilon^{2}}\\right)$ total samples suffice to learn an $\\varepsilon$ -optimal policy for the average reward MDP. This bound matches the minimax lower bound in [20] and is superior to existing results for weakly communicating MDPs (see Table 1). We note that the proof o Theorem 1 works so long as $\\mathsf{H}$ is any upper bound of $\\bar{\\|h^{\\star}\\|_{\\mathrm{span}}}$ hence Algorithm 2 also only needs an upper bound for $\\|h^{\\star}\\|_{\\mathrm{span}}$ ", "page_idx": 6}, {"type": "text", "text": "We show in the following theorem that it is in general impossible to obtain a useful upper bound on $\\|h^{\\star}\\|_{\\mathrm{span}}$ with a sample complexity that is a function of only $\\|h^{\\star}\\|_{\\mathrm{span}}$ . This suggests that it is not easy to remove the need for knowledge o Ih\\*Ilsan'? ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. For any given $n,T\\geq1$ , there exist two MDPs $\\mathcal{M}_{0}$ and $\\mathcal{M}_{1}$ with $S=4$ $A=1$ such that $\\mathcal{M}_{\\mathrm{0}}$ has optimal bias span 1, $\\mathcal{M}_{1}$ has optimal bias span $T$ .and it is impossible to distinguish between $\\mathbf{\\mathcal{M}}_{0}$ and $\\mathcal{M}_{1}$ with probability $\\geq\\frac{3}{4}$ with $n$ samples from each state-action pair. ", "page_idx": 6}, {"type": "text", "text": "Thus even for an MDP with a small span, there exists another MDP that has an arbitrarily large span and is arbitrarily statistically close (that is, cannot be distinguished even with a large sample size $n]$ 0. We emphasize that all previous algorithms in Table 1 also require knowledge of their respective complexity parameters, and such assumptions are pervasive throughout the literature on averagereward RL. The only exception of which we are aware is the contemporaneous work [7], which achieves a suboptimal $\\tilde{O}(S A\\frac{\\tau_{\\mathrm{unif}}^{8}}{\\varepsilon^{8}})$ sample complexity without knowledge of $\\tau_{\\mathrm{unif}}$ in the uniformly mixing setting. It is unclear if $\\mathsf{H}$ -based sample complexities are possible without knowing H. Besides the evidence offered by Theorem 3, in the online setting, it has been conjectured that knowledge of $\\mathsf{H}$ is necessary to obtain an H-dependent regret bound [6, 5, 25]. Moreover, even with knowledge of $\\mathsf{H}$ the only known online algorithm with optimal regret is computationally inefficient [25], making it somewhat surprising that our Theorem 2 uses a simple and efficient algorithm. ", "page_idx": 6}, {"type": "text", "text": "Nevertheless, when $\\mathsf{H}$ is unknown, one can replace H with the diameter $D$ (since $\\mathsf{H}\\leq D,$ 0.The diameter is known to be estimable [25, 17] and is often a more refined complexity parameter than $\\tau_{\\mathrm{unif}}$ Our Theorem 2 is the first to imply the optimal diameter-based complexity $\\tilde{\\widetilde O}(\\frac{S A D}{\\varepsilon^{2}})$ given knowledge of $D$ or using a constant-factor upper bound obtained from some estimation procedure. ", "page_idx": 6}, {"type": "text", "text": "4  Main results for general MDPs ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our starting point for general MDPs is that unlike the weakly communicating setting, their complexity cannot be captured solely by $\\|h^{\\star}\\|_{\\mathrm{span}}$ . We first argue this point informally using the simple example in Figure 1, which is parameterized by a value $T>1$ . Only state 1 contains multiple actions, and action 2 is optimal since it leads to state 2 which collects reward 0.5 forever, while taking action 1 will always eventually lead to state 3 where the reward is 0 forever. We thus have $\\rho^{\\star}=[0.5,0.5,0]^{\\top}$ and $\\|h^{\\star}\\|_{\\mathrm{span}}=0$ However, cearly $\\Omega(T)$ samples are required toeven oberve a transition $1\\rightarrow3$ so the sample complexity must depend on $T\\gg\\mathsf{H}$ (without observing a transition $1\\rightarrow3$ , we cannot determine that action 1 is not optimal). Taking action 1 leads to a large reward of 1 in the short term (for $T$ steps in expectation), so even if we had perfect knowledge of the environment, the optimal $\\gamma$ -discounted policy would not choose the optimal action $a=2$ until the effective horizon ${\\dot{\\frac{1}{1-\\gamma}}}\\,\\geq\\,\\Omega(T)$ Thus $\\frac{1}{1-\\gamma}\\overset{\\cdot}{\\approx}{\\sf H}$ is insuffcient for th redution todisouted MDP Nte that ths instance has its bounded transient time parameter $\\mathsf{B}=T$ This example reflects that transient states play a categorically different role in general MDPs: in the weakly communicating setting, states which are transient under all policies can be completely ignored, whereas in this example our action at state 1 fully determines our reward even though state 1 is transient under all policies. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "pGEY8JQ3qx/tmp/762152a5527a26648f0e55dfee73acdab5d07493e2dc4b77d71c59ddc126a916.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 1: A general MDP where $\\gamma$ discounted approximation fails unless $\\begin{array}{r}{\\frac{1}{1-\\gamma}=\\Omega(T)\\gg\\|h^{\\star}\\|_{\\mathrm{span}}}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "The statistical hardness is formally captured by the following theorem, which uses improved instances to obtain the correct dependence on $\\varepsilon$ ", "page_idx": 7}, {"type": "text", "text": "Theorem 4 (Lower Bound for General AMDPs). For any $\\varepsilon\\ \\in\\ (0,1/4)$ \uff0c $B~\\ge~1$ $A\\,\\geq\\,4$ and $S\\in8\\mathbb{N}$ for any algorithm $\\pmb{{\\cal A}}\\,\\!\\!\\stackrel{\\triangledown}{\\boldsymbol{\\mathit{l o}}}\\!\\!\\!g$ whichisguaranteedtoreturn an $\\varepsilon/3$ optimalpolicyforanyinput average-rewardMDPwith probability at least $\\frac{3}{4}$ there exists an MDP $\\mathcal{M}=(P,r)$ such that: ", "page_idx": 7}, {"type": "text", "text": "1. $\\mathcal{M}$ has $S$ statesand $A$ actions.   \n2. Letting $h^{\\star}$ be the bias of the Blackwell-optimal policy for $\\mathcal{M}$ we have $\\Vert h^{\\star}\\Vert_{\\mathrm{span}}=0$   \n3. $\\mathcal{M}$ satisfies the bounded transient time assumption with parameter $B$   \n4. Alg requires S2( Blog(SA)) samples per state-action pair on $\\mathcal{M}$ ", "page_idx": 7}, {"type": "text", "text": "A similar minimax lower bound holds for the discounted setting. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5 (Lower Bound for General DMDP). For any $\\varepsilon\\in(0,1/4)$ \uff0c $B\\geq1$ $A\\geq4$ and $S\\in8\\mathbb{N}$ for any algorithm $\\b{A}\\b{\\ l}_{\\b{9}}$ whichisguaranteed toreturnan $\\varepsilon/3$ -optimalpolicyforanyinputdiscounted MDP with probability at least $\\textstyle{\\frac{3}{4}}$ thereexistsadiscountedMDP $\\mathcal{M}=(P,r,\\gamma)$ such that: ", "page_idx": 7}, {"type": "text", "text": "1. $\\mathcal{M}$ has $S$ states and $A$ actions.   \n2. $\\mathcal{M}$ satisfies the bounded transient time assumption with parameter $B$   \n3. Alg requires 2( Bl-ssA2 samples per state-action pair on $\\mathcal{M}$ ", "page_idx": 7}, {"type": "text", "text": "The lower bounds of $\\widetilde{\\cal O}\\left(\\frac{\\sf H}{\\varepsilon^{2}}\\right)$ from the weakly communicating setting still apply in the general setting.   \nTogether with Theorem 4 they imply a $\\begin{array}{r}{\\widetilde{O}\\left(\\frac{\\mathsf{H}+\\mathsf{B}}{\\varepsilon^{2}}\\right)}\\end{array}$ lower bound for general average-reward MDPs. ", "page_idx": 7}, {"type": "text", "text": "Figure 1 demonstrats that, unlik the weakly communicating seting, discounted reductio with $\\textstyle{\\frac{1}{1-\\gamma}}$ set in terms of only H cannot succeed for general MDPs. (Contrast with Lemma 9 for the analogous theorem from [20] for weakly communicating MDPs.) We remedy this issue and lay the foundation for our matching upper bound by proving a new reduction theorem in terms of $\\mathsf{H}$ and B; in particular, B measures how much farther ahead we must look in order to determine which closed communicating class will be reached. By Lemma $27\\;\\mathsf{B}\\leq4\\tau_{\\mathrm{unif}}$ , although B is always finite unlike $\\tau_{\\mathrm{unif}}$ ", "page_idx": 7}, {"type": "text", "text": "Theorem 6 (Average-to-Discount Reduction for General MDP). Suppose $(P,r)$ is a general MDP, has an optimal bias function $h^{\\star}$ satisfying $\\|h^{\\star}\\|_{\\mathrm{span}}\\leq\\mathsf{H}_{\\rho}$ and satisfies the bounded transient time assumption with parameter B. Fix $\\varepsilon\\in(0,1]$ and set $\\begin{array}{r}{\\gamma=1-\\frac{\\varepsilon}{\\mathsf{B}+\\mathsf{H}}}\\end{array}$ . For any $\\varepsilon_{\\gamma}\\in[0,\\frac{1}{1-\\gamma}]$ f $\\pi$ any $\\varepsilon_{\\gamma}$ -optimal policy for the discounted MDP $(P,r,\\gamma)$ then $\\begin{array}{r}{\\rho^{\\star}-\\rho^{\\pi}\\leq\\Big(3+2\\frac{\\varepsilon_{\\gamma}}{8+\\mathsf{H}}\\Big)\\varepsilon\\mathbf{1}}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "Proof highlights. Letting $\\pi_{\\gamma}^{\\star}$ be the optimal policy for the $\\gamma.$ discounted MDP, our first key observation is that $\\rho^{\\star}$ is constant within any irreducible closed recurrent block of the Markov chain $P_{\\pi_{\\gamma}^{\\star}}$ ,essentially because all states in this block must be reachable from each other with probability one (see Lemma 17). Leveraging the optimality of $\\pi_{\\gamma}^{\\star}$ , this enables us to bound both $\\begin{array}{r}{|V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}(s)-\\frac{1}{1-\\gamma}\\rho^{\\star}(s)|}\\end{array}$ and $\\begin{array}{r}{\\bigl|V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}(s)-\\frac{1}{1-\\gamma}\\rho^{\\pi_{\\gamma}^{\\star}}(s)\\bigr|}\\end{array}$ by $O\\big(\\|h^{\\star}\\|_{\\mathrm{span}}\\big)$ for any $s$ which is recurrent under $\\pi_{\\gamma}^{\\star}$ which when combined demonstrate that the gain $\\rho^{\\pi_{\\gamma}^{\\star}}(s)$ of $\\pi_{\\gamma}^{\\star}$ is near-optimal for its recurrent states. See Lemma 21. We then leverage the bounded transient time assumption to guarantee that for transient $s$ $V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}(s)$ is dominated by the expected returns from recurrent states, since at most $O(\\mathsf{B})$ time is spent in transient states. We complete the proof of Theorem 6 by combining these facts, as well as extending them to accommodate approximately optimal policies. \u53e3 ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Next we establish an improved sample complexity for the discounted problem in the setting relevant to this reduction. This bound matches the lower bound in Theorem 5 up to log factors. ", "page_idx": 8}, {"type": "text", "text": "Theorem 7 Sample Complexity of General DMDP). Suppose $\\begin{array}{r}{\\mathsf{B}+\\mathsf{H}\\le\\frac{1}{1-\\gamma}}\\end{array}$ and $\\varepsilon\\leq\\mathsf{B}+\\mathsf{H}.$ There $C_{3}>0$ $\\delta\\in(0,1).$ $\\begin{array}{r}{\\dot{n}\\geq C_{3}\\frac{\\mathsf{B}+\\mathsf{H}}{(1-\\gamma)^{2}\\varepsilon^{2}}\\log\\Big(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\Big),}\\end{array}$ thnwith probability $1-\\delta$ the policy $\\widehat{\\pi}_{\\gamma,\\mathbf{p}}^{\\star}$ output by Algorithm 1 satisfies $\\begin{array}{r}{\\left\\|V_{\\gamma}^{\\star}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\|_{\\infty}\\leq\\varepsilon}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "Finally, we present our result for the sample complexity of general average-reward MDPs, matching the lower bound in Theorem 4 up to log factors. We again use the reduction Algorithm 2, this time Wwith the largerDMDP targtacuraey $\\overline{{\\varepsilon}}=\\mathsf{B}+\\mathsf{H}$ ladingtoa discoun factorof $\\begin{array}{r}{\\overline{{\\gamma}}=1-\\frac{\\varepsilon}{12(\\mathsf{B}+\\mathsf{H})}}\\end{array}$ Theorem 8 (Sample Complexity of General AMDP). There exists a constant $C_{4}>0$ such that for any $\\delta,\\varepsilon\\in(0,1)$ $\\begin{array}{r}{\\epsilon_{n}\\geq C_{4}\\frac{\\mathsf{B}+\\mathsf{H}}{\\varepsilon^{2}}\\log\\left(\\frac{S A(\\mathsf{B}+\\mathsf{H})}{\\delta\\varepsilon}\\right)}\\end{array}$ and we call Algorithm 2 with $\\varepsilon=\\mathsf{B}+\\mathsf{H},$ then with probability at least $1-\\delta$ the output policy $\\widehat{\\pi}^{\\star}$ satisfies the elementwise inequality $\\rho^{\\star}-\\rho^{\\widehat{\\pi}^{\\star}}\\leq\\varepsilon{\\bf1}$ ", "page_idx": 8}, {"type": "text", "text": "Proof highlights. Similarly to Theorem 2, we seek to bound certain variance parameters, and this time it would sufice to bound the variance of the cumulative discounted reward starting from any state $s$ like $\\begin{array}{r}{\\left|\\mathbb{V}_{s}^{\\pi_{\\gamma}^{\\star}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]\\right|\\leq O\\big(\\frac{\\sf H+B}{1-\\gamma}\\big)}\\end{array}$ Such abound inded holds for states $s$ that are recurent under $\\pi_{\\gamma}^{\\star}$ , because $\\rho^{\\star}(S_{t})$ will remain constant to $\\rho^{\\star}(s)$ for all $t$ , since, as mentioned above, $\\rho^{\\star}$ is constant on closed irreducible recurrent blocks, and all $(S_{t})_{t\\geq0}$ will stay in the same block as $s$ Therefore, we can almost reuse our argument from the weakly communicating case. However, if $s$ is transient, it is easy to see that $\\begin{array}{r l}{\\left|\\mathbb{V}_{s}^{\\pi_{\\gamma}^{\\star}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]\\right|=\\Omega\\Big(\\big(\\frac{1}{1-\\gamma}\\big)^{2}\\Big)}&{{}}\\end{array}$ in general (even under the bounded transient time assumption), as we can consider an example where from $s$ we transition to either an absorbing reward 1 state or an absorbing reward 0 state. Thus, when $s$ is transient, instead of bounding $\\left|{\\mathbb{V}}_{s}^{\\pi_{\\gamma}^{\\star}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]\\right|$ we directly work with the sharper variance parameter $\\left|e_{s}^{\\top}(I-\\gamma P_{\\pi_{\\gamma}^{\\star}})^{-1}\\sqrt{\\mathbb{V}_{P_{\\pi_{\\gamma}^{\\star}}}[V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}]}\\right|$ , which is also common to the analysis of DMDPs [3, 1, 12] (and in these previous works is bounded in terms of $\\begin{array}{r}{\\left|\\left|\\mathbb{V}^{\\pi_{\\gamma}^{\\star}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]\\right|\\right|_{\\infty}}\\end{array}$ ; see Lemma 12 for this relationship). We instead develop a novel law-of-total-variance-style argument which limits the total contribution of transient states to this sharper variance parameter. See Lemma 26 for details. \u53e3 ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper we obtained optimal sample complexities for weakly communicating and general average reward MDPs by improving the analysis of discounted MDPs, revealing a quadratic rather than cubic dependence on the effective horizon for a fixed instance. A limitation of our results (as well as of all previous results) is that the average-to-discounted reduction requires prior knowledge of parameters for optimal complexity, and an interesting open question is whether it is possible to remove this assumption. In conclusion, we believe our results shed greater light on the relationship between the discounted and average reward settings as well as the fundamental complexity of the discounted setting, and we hope that our technical developments can be useful in future work, such as leading to efficient optimal algorithms in the online setting. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Y. Chen and M. Zurek were supported in part by National Science Foundation CCF-2233152 and DMS-2023239. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1]  Alekh Agarwal, Sham Kakade, and Lin F. Yang. Model-Based Reinforcement Learning with a Generative Model is Minimax Optimal, April 2020. arXiv:1906.03804 [cs, math, stat] version: 3. [2]  Mohammad Gheshlaghi Azar, Remi Munos, and Bert Kappen. On the Sample Complexity of Reinforcement Learning with a Generative Model, June 2012. arXiv:1206.6461 [cs, stat].   \n[3] Mohammad Gheshlaghi Azar, Remi Munos, and Hilbert J. Kappen. Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model. Machine Learning, 91(3):325-349, June 2013.   \n[4]  Peter L. Bartlett and Ambuj Tewari. REGAL: A Regularization based Algorithm for Reinforcement Learning in Weakly Communicating MDPs, May 2012. arXiv:1205.2661.   \n[5]  Ronan Fruit, Matteo Pirotta, and Alessandro Lazaric. Near Optimal Exploration-Exploitation in Non-Communicating Markov Decision Processes, March 2019. arXiv:1807.02373 [cs, stat]. [6]  Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, and Ronald Ortner. Efficient Bias-SpanConstrained Exploration-Exploitation in Reinforcement Learning, July 2018. arXiv:1802.04020 [cs, stat].   \n[7]  Ying Jin, Ramki Gummadi, Zhengyuan Zhou, and Jose Blanchet. Feasible $\\mathbb{S}{\\sf O}\\mathbb{S}$ -Learning for Average Reward Reinforcement Learning. In Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, pages 1630-1638. PMLR, April 2024. ISSN: 2640-3498.   \n[8]  Yujia Jin and Aaron Sidford. Effciently Solving MDPs with Stochastic Mirror Descent, August 2020. arXiv:2008.12776.   \n[9]  Yujia Jin and Aaron Sidford. Towards Tight Bounds on the Sample Complexity of Averagereward MDPs, June 2021. arXiv:2106.07046 [cs, math].   \n[10] Michael Kearns and Satinder Singh. Finite-Sample Convergence Rates for Q-Learning and Indirect Algorithms. In Advances in Neural Information Processing Systems, volume 11. MIT Press, 1998.   \n[11]  David A. Levin and Yuval Peres. Markov Chains and Mixing Times. American Mathematical Soc., October 2017.   \n[12] Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Breaking the Sample Size Barrier in Model-Based Reinforcement Learning with a Generative Model. In Advances in Neural Information Processing Systems, volume 33, pages 12861-12872. Curran Associates, Inc., 2020.   \n[13]  Tianjiao Li, Feiyang Wu, and Guanghui Lan. Stochastic first-order methods for average-reward Markov decision processes, September 2024. arXiv:2205.05800.   \n[14] Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, August 2014.   \n[15]  Aaron Sidford, Mengdi Wang, Xian Wu, Lin Yang, and Yinyu Ye. Near-Optimal Time and Sample Complexities for Solving Markov Decision Processes with a Generative Model. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.   \n[16]  Matthew J. Sobel. The variance of discounted Markov decision processes. Journal of Applied Probability, 19(4):794-802, December 1982. Publisher: Cambridge University Press.   \n[17] Jean Tarbouriech, Matteo Pirotta, Michal Valko, and Alessandro Lazaric. A Provably Efficient Sample Collection Strategy for Reinforcement Learning, November 2021. arXiv:2007.06437 [cs, stat].   \n[18]  Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge University Press, 1 edition, February 2019.   \n[19]  Martin J. Wainwright.  Variance-reduced $\\mathbb{S}{\\sf O}\\mathbb{S}$ -learning is minimax optimal, August 2019. arXiv:1906.04697 [cs, math, stat].   \n[20] Jinghan Wang, Mengdi Wang, and Lin F. Yang. Near Sample-Optimal Reduction-based Policy Learning for Average Reward MDP, December 2022. arXiv:2212.00603 [cs].   \n[21] Shengbo Wang, Jose Blanchet, and Peter Glynn. Optimal Sample Complexity of Reinforcement Learning for Mixing Discounted Markov Decision Processes, September 2023. arXiv:2302.07477.   \n[22]  Shengbo Wang, Jose Blanchet, and Peter Glynn. Optimal Sample Complexity for Average Reward Markov Decision Processes, February 2024. arXiv:2310.08833.   \n[23] Chen- Yu Wei, Mehdi Jafarnia-Jahromi, Haipeng Luo, Hiteshi Sharma, and Rahul Jain. Modelfree Reinforcement Learning in Infinite-horizon Average-reward Markov Decision Processes, February 2020. arXiv:1910.07072 [cs, stat].   \n[24] Bin Yu. Assouad, Fano, and Le Cam. In Festschrift for Lucien Le Cam, pages 423-435. Springer, 1997.   \n[25]  Zihan Zhang and Xiangyang Ji. Regret Minimization for Reinforcement Learning by Evaluating the Optimal Bias Function, December 2019. arXiv:1906.05110 [cs, stat] version: 3.   \n[26] Zihan Zhang and Qiaomin Xie. Sharper Model-free Reinforcement Learning for Average-reward Markov Decision Processes, June 2023. arXiv:2306.16394 [cs]. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "A Proofs for weakly communicating MDPs ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "In this section, we provide the proofs for our main results in Section 3 for weakly communicating MDPs. Before beginning, we note that given that $\\mathsf{H}\\geq1$ ,wemay assume that $\\mathsf{H}$ is an integer by setting $\\mathsf{H}\\gets\\lceil\\mathsf{H}\\rceil$ , which only affects the sample complexity by a constant multiple $<2$ relative to the original parameter H. Let $\\begin{array}{r}{\\|M\\|_{\\infty\\to\\infty}:=\\operatorname*{sup}_{v:\\|v\\|_{\\infty}\\leq1}\\|M v\\|_{\\infty}}\\end{array}$ denote the $\\ell_{\\infty}$ operator norm of a matrix $M$ We record thestandard and usful fat that $\\begin{array}{r}{\\left\\|(I-\\gamma P^{\\prime})^{-1}\\right\\|_{\\infty\\to\\infty}\\le\\frac{1}{1-\\gamma}}\\end{array}$ for any trasition probability matrix $P^{\\prime}$ , which follows from the Neumann series $\\begin{array}{r}{(I-\\gamma P^{\\prime})^{-1}=\\sum_{t\\geq0}\\left(\\gamma P^{\\prime}\\right)^{t}}\\end{array}$ and the elementary fact that $\\|P^{\\prime}\\|_{\\infty\\to\\infty}\\le1$ ", "page_idx": 11}, {"type": "text", "text": "A.1 Technical lemmas ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "First we formally state the main theorem from [20], which gives a reduction from weakly communicating average-reward problems to discounted problems. ", "page_idx": 11}, {"type": "text", "text": "Lemma 9.Suppose $(P,r)$ is an MDP which is weakly communicating and has an optimal bias function $h^{\\star}$ satisfying $\\begin{array}{r}{\\|h^{\\star}\\|_{\\mathrm{span}}\\leq\\mathsf{H}}\\end{array}$ Fix $\\varepsilon\\in(0,1]$ and set $\\begin{array}{r}{\\gamma=1-\\frac{\\varepsilon}{\\mathsf{H}}}\\end{array}$ For any $\\begin{array}{r}{\\varepsilon_{\\gamma}\\in[0,\\frac{\\mathbf{\\bar{\\alpha}}_{1}}{1-\\gamma}].}\\end{array}$ f\u03c0is any $\\varepsilon_{\\gamma}$ -optimalpolicyforthediscountedMDP $(P,r,\\gamma)$ ,then ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\rho^{\\star}-\\rho^{\\pi}\\leq\\left(8+3\\frac{\\varepsilon_{\\gamma}}{\\mathsf{H}}\\right)\\varepsilon\\mathbf{1}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "From here, we will first establish lemmas which are useful for proving Theorem 1 on discounted MDPs, and then we will apply the reduction approach of Lemma 9 to prove Theorem 2 on averagereward MDPs. As mentioned in the introduction, a key technical component of our approach is to establish superior bounds on a certain instance-dependent variance quantity which replace a factor of $\\textstyle{\\frac{1}{1-\\gamma}}$ withafactor of $\\mathsf{H}$ Before reaching this step however, to make use of such a bound, we require an algorithm for discounted MDPs which enjoys a variance-dependent guarantee. ", "page_idx": 11}, {"type": "text", "text": "The work [12] obtains bounds with variance dependence that suffice for our purposes. However, they do not directly present said variance-dependent bounds, so we must slightly repackage their arguments in the form we require. ", "page_idx": 11}, {"type": "text", "text": "Lemma 10. There exist absolute constants $c_{1},c_{2}$ such that for any $\\delta\\in\\mathrm{~\\mathsf~{\\Omega}~}(0,1)$ f $n~~\\ge$ $\\begin{array}{r}{\\frac{c_{2}}{1-\\gamma}\\log\\left(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\right)}\\end{array}$ then with probability at least $1-\\delta$ after running Algorithm $^{\\,I}$ we have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\widehat{V}_{\\gamma,\\overline{{\\mathbf{p}}}}^{\\pi_{\\gamma}^{*}}-V_{\\gamma}^{\\pi_{\\gamma}^{*}}\\right\\|_{\\infty}\\leq\\gamma\\sqrt{\\frac{c_{1}\\log\\Big(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\Big)}{n}}\\left\\|(I-\\gamma P_{\\pi_{\\gamma}^{*}})^{-1}\\sqrt{\\mathbb{V}_{P_{\\pi_{\\gamma}^{*}}}\\left[V_{\\gamma}^{\\pi_{\\gamma}^{*}}\\right]}\\right\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad+\\left.c_{1}\\gamma\\frac{\\log\\Big(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\Big)}{(1-\\gamma)n}\\left\\|V_{\\gamma}^{\\pi_{\\gamma}^{*}}\\right\\|_{\\infty}+\\frac{\\varepsilon}{6}}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "and ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\widehat{V}_{\\gamma,\\mathbf{p}}^{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}^{\\star}}\\right\\|_{\\infty}\\leq\\gamma\\sqrt{\\frac{c_{1}\\log\\Big(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\Big)}{n}}\\left\\|(I-\\gamma P_{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}})^{-1}\\sqrt{\\mathbb{V}_{P_{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}}}\\left[V_{\\gamma,\\mathbf{p}}^{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}}\\right]}\\right\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left.c_{1}\\gamma\\frac{\\log\\Big(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\Big)}{(1-\\gamma)n}\\left\\|V_{\\gamma,\\mathbf{p}}^{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}^{\\star}}\\right\\|_{\\infty}+\\frac{\\varepsilon}{6}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Proof. First we establish equation (1). The proof of [12, Lemma 1] shows that when $n~\\geq$ $\\textstyle{\\frac{16e^{2}}{1-\\gamma}}2\\log\\left({\\frac{4S\\log{\\frac{e}{1-\\gamma}}}{\\delta}}\\right)$ , with probability at least $1-\\delta$ wehave ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\widehat{V}_{\\gamma}^{\\pi_{\\gamma}^{\\star}}-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\|_{\\infty}\\leq4\\gamma\\sqrt{\\frac{2\\log\\left(\\frac{4S\\log\\frac{e}{1-\\gamma}}{\\delta}\\right)}{n}}\\left\\|(I-\\gamma P_{\\pi_{\\gamma}^{\\star}})^{-1}\\sqrt{\\mathbb{V}_{P_{\\pi_{\\gamma}^{\\star}}}\\left[V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right]}\\right\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left.\\gamma\\frac{2\\log\\left(\\frac{4S\\log\\frac{e}{1-\\gamma}}{\\delta}\\right)}{(1-\\gamma)n}\\left\\|V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Now since ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\widehat{V}_{\\gamma,\\mathbb{p}}^{\\pi_{\\gamma}^{\\star}}-\\widehat{V}_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\|_{\\infty}=\\left\\|(I-\\gamma\\widehat{P}_{\\pi_{\\gamma}^{\\star}})^{-1}\\widetilde{r}_{\\pi_{\\gamma}^{\\star}}-(I-\\gamma\\widehat{P}_{\\pi_{\\gamma}^{\\star}})^{-1}r_{\\pi_{\\widehat{\\gamma}}^{\\star}}\\right\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left\\|(I-\\gamma\\widehat{P}_{\\pi_{\\gamma}^{\\star}})^{-1}\\right\\|_{\\infty\\to\\infty}\\|\\widetilde{r}-r\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\xi}{1-\\gamma}=\\frac{\\varepsilon}{6},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "we can obtain equation (1) by triangle inequality (although we will choose the constant $c_{1}$ below). Next we establish equation (2). Using [12, Lemma 6], with probability at least $1-\\delta$ we have that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left|\\widehat{Q}_{\\gamma,\\mathrm{p}}^{\\star}(s,\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}(s))-\\widehat{Q}_{\\gamma,\\mathrm{p}}^{\\star}(s,a)\\right|>\\frac{\\xi\\delta(1-\\gamma)}{3S A^{2}}=\\frac{\\varepsilon\\delta(1-\\gamma)^{2}}{18S A^{2}}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "uniformly over all $s$ and all $a\\neq\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}(s)$ . From this separation condition (4), the assumptions of [12, Lemma 5] hold (with $\\begin{array}{r}{\\omega=\\frac{\\varepsilon\\delta(1-\\gamma)^{2}}{18S A^{2}}}\\end{array}$ in their notation) for the MDP with the perturbed reward $\\widetilde r$ The proof of [12, Lemma 5] shows that under the event (4) holds, the conditions for [12, Lemma 2] are satishfed with n thir notation,. $\\begin{array}{r}{\\beta_{1}=2\\log\\left(\\frac{32}{(1-\\gamma)^{2}\\omega\\delta}S A\\log\\frac{e}{1-\\gamma}\\right)=2\\log\\left(\\frac{576S^{2}A^{3}}{(1-\\gamma)^{4}\\delta^{2}\\varepsilon}\\log\\frac{e}{1-\\gamma}\\right))}\\end{array}$ with additional failure probability $\\le\\delta$ . The proof of [12, Lemma 2] then shows that, assuming $\\begin{array}{r}{n>\\frac{16e^{2}}{1-\\gamma}2\\log\\left(\\frac{576S^{2}A^{3}}{(1-\\gamma)^{4}\\delta^{2}\\varepsilon}\\log\\frac{e}{1-\\gamma}\\right)}\\end{array}$ log 1- ), we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left\\|\\widehat{V}_{\\gamma,\\mathbb{P}}^{\\widehat{\\pi}_{\\gamma,\\mathbb{P}}^{*}}-V_{\\gamma,\\mathbb{P}}^{\\widehat{\\pi}_{\\gamma,\\mathbb{P}}^{*}}\\right\\|_{\\infty}\\leq4\\gamma\\sqrt{\\frac{\\beta_{1}}{n}}\\left\\|(I-\\gamma P_{\\widehat{\\pi}_{\\gamma,\\mathbb{P}}})^{-1}\\sqrt{\\mathbb{V}_{P\\widehat{\\pi}_{\\gamma,\\mathbb{P}}}}\\left[V_{\\gamma,\\mathbb{P}}^{\\widehat{\\pi}_{\\gamma,\\mathbb{P}}^{*}}\\right]\\right\\|_{\\infty}+\\frac{\\gamma\\beta_{1}}{(1-\\gamma)n}\\left\\|V_{\\gamma,\\mathbb{P}}^{\\widehat{\\pi}_{\\gamma,\\mathbb{P}}^{*}}\\right\\|_{\\infty}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where we abbreviated $\\begin{array}{r}{\\beta_{1}=2\\log\\left(\\frac{576S^{2}A^{3}}{(1-\\gamma)^{4}\\delta^{2}\\varepsilon}\\log\\frac{e}{1-\\gamma}\\right)}\\end{array}$ ", "page_idx": 12}, {"type": "text", "text": "We can again calculate that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{V_{\\gamma,\\mathbf{p}}^{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}^{\\star}}}\\right\\|_{\\infty}=\\left\\|{(I-\\gamma P_{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}^{\\star}})^{-1}\\widetilde{r}_{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}^{\\star}}-(I-\\gamma P_{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}^{\\star}})^{-1}r_{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}^{\\star}}}\\right\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left\\|{(I-\\gamma P_{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}^{\\star}})^{-1}}\\right\\|_{\\infty\\to\\infty}\\left\\|{\\widetilde{r}-r}\\right\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\xi}{1-\\gamma}=\\frac{\\varepsilon}{6},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "SO $\\begin{array}{r}{\\left\\lVert\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\rVert_{\\infty}\\leq\\left\\lVert\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\rVert_{\\infty}+\\frac{\\varepsilon}{6}}\\end{array}$ ", "page_idx": 12}, {"type": "text", "text": "Finally, to choose the constants $c_{1}$ and $c_{2}$ , we first note that $\\begin{array}{r l r}{2\\log\\Big(\\frac{4S\\log\\frac{e}{1-\\gamma}}{\\delta}\\Big)}&{{}\\leq}&{\\beta_{1}\\;\\;<}\\end{array}$ $\\begin{array}{r}{c_{1}^{\\prime}\\log\\left(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\right)}\\end{array}$ for some absolute constant $c_{1}^{\\prime}$ , and therefore also all our requirements on $n$ are $\\begin{array}{r}{\\stackrel{\\cdot}{n}\\,\\geq\\,\\frac{16e^{2}}{1-\\gamma}c_{1}^{\\prime}\\log\\left(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\right)\\,=\\,\\frac{c_{2}^{\\prime}}{1-\\gamma}\\log\\left(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\right)}\\end{array}$ for another absolute constant $c_{2}^{\\prime}$ Lastly we note that by the union bound the total failure probability is at most $3\\delta$ , so to obtain a failure probability of $\\delta^{\\prime}$ we may set $\\delta=\\delta^{\\prime}/3$ and absorb the additional constant when defining $c_{1},c_{2}$ in terms of $c_{1}^{\\prime},c_{2}^{\\prime}$ , and we also then increase $c_{1}$ by a factor of 4 to absorb the factor of 4 appearing in the first terms within (3) and (5). \u53e3 ", "page_idx": 12}, {"type": "text", "text": "Now we can analyze the variance parameters ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\lVert(I-\\gamma P_{\\pi_{\\gamma}^{\\star}})^{-1}\\sqrt{\\mathbb{V}_{P_{\\pi_{\\gamma}^{\\star}}}\\left[V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right]}\\right\\rVert_{\\infty}\\quad\\mathrm{~and~}\\quad\\left\\lVert(I-\\gamma P_{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}})^{-1}\\sqrt{\\mathbb{V}_{P_{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}}\\left[V_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right]}\\right\\rVert_{\\infty},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "which appear in the error bounds in Lemma 10. We begin by reproducing the following inequality from [23, Lemma 2]. ", "page_idx": 12}, {"type": "text", "text": "Lemma 11. In a weakly communicating MDP, for all $\\gamma\\in[0,1)$ ,itholdsthat ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{s}\\left\\vert V_{\\gamma}^{\\pi^{\\star}}(s)-\\frac{1}{1-\\gamma}\\rho^{\\star}\\right\\vert\\le\\mathsf{H}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The following relates the variance parameter of interest to another parameter, the variance of the total discounted rewards. This result essentially appears in [1, Lemma 4] (which was in turn inspired by [3, Lemma 8]), but since their result pertains to objects slightly different than $P_{\\pi}$ and $\\mathbb{V}_{P_{\\pi}}\\,\\left[\\dot{V}_{\\gamma}^{\\pi}\\right]$ ,we provide the full argument for completeness. ", "page_idx": 13}, {"type": "text", "text": "Lemma 12. For any deterministic stationary policy $\\pi$ wehave ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\gamma\\left\\|(I-\\gamma P_{\\pi})^{-1}\\sqrt{\\mathbb{V}_{P_{\\pi}}\\left[V_{\\gamma}^{\\pi}\\right]}\\right\\|_{\\infty}\\leq\\sqrt{\\frac{2}{1-\\gamma}}\\sqrt{\\left\\|\\mathbb{V}^{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]\\right\\|_{\\infty}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. First we note the well-known variance Bellman equation (see for instance [16, Theorem 1]): ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{V}^{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]=\\gamma^{2}\\mathbb{V}_{P_{\\pi}}\\left[V_{\\gamma}^{\\pi}\\right]+\\gamma^{2}P_{\\pi}\\mathbb{V}^{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Now we can basically identically follow the argument of [1, Lemma 4]. The matrix $(1\\!-\\!\\gamma)(I\\!-\\!\\gamma P_{\\pi})^{-1}$ has rows which are each probability distributions (are non-negative and sum to 1). Therefore, by Jensen's inequality and the concavity of the function $x\\mapsto{\\sqrt{x}}$ ,foreachrow $s\\in S$ wehave ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|(1-\\gamma)e_{s}^{\\top}(I-\\gamma P_{\\pi})^{-1}\\sqrt{\\mathbb{V}_{P_{\\pi}}\\left[V_{\\gamma}^{\\pi}\\right]}\\right|\\leq\\sqrt{\\left|(1-\\gamma)e_{s}^{\\top}(I-\\gamma P_{\\pi})^{-1}\\mathbb{V}_{P_{\\pi}}\\left[V_{\\gamma}^{\\pi}\\right]\\right|}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Using this fact we can calculate that, abbreviating $v=\\mathbb{V}_{P_{\\pi}}\\left[V_{\\gamma}^{\\pi}\\right]$ \uff0c ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\gamma\\left\\lVert(I-\\gamma P_{\\pi})^{-1}\\sqrt{v}\\right\\rVert_{\\infty}=\\gamma\\frac{1}{1-\\gamma}\\left\\lVert(1-\\gamma)(I-\\gamma P_{\\pi})^{-1}\\sqrt{v}\\right\\rVert_{\\infty}}&{}\\\\ {\\leq\\gamma\\frac{1}{1-\\gamma}\\sqrt{\\|(1-\\gamma)(I-\\gamma P_{\\pi})^{-1}v\\|_{\\infty}}}&{}\\\\ {=\\gamma\\frac{1}{\\sqrt{1-\\gamma}}\\sqrt{\\|(I-\\gamma P_{\\pi})^{-1}v\\|_{\\infty}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In order to relate $\\left\\|(I-\\gamma P_{\\pi})^{-1}v\\right\\|_{\\infty}$ to $\\left\\|(I-\\gamma^{2}P_{\\pi})^{-1}v\\right\\|_{\\infty}$ in order to apply the variance Bellman equation (6), we calculate ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|(I-\\gamma P_{\\pi})^{-1}v\\right|_{\\infty}=\\left\\|(I-\\gamma P_{\\pi})^{-1}(I-\\gamma^{2}P_{\\pi})(I-\\gamma^{2}P_{\\pi})^{-1}v\\right\\|_{\\infty}}&{}\\\\ {=\\left\\|(I-\\gamma P_{\\pi})^{-1}\\left((1-\\gamma)I+\\gamma(I-\\gamma P_{\\pi})\\right)(I-\\gamma^{2}P_{\\pi})^{-1}v\\right\\|_{\\infty}}&{}\\\\ {=\\left\\|\\left((1-\\gamma)(I-\\gamma P_{\\pi})^{-1}+\\gamma I\\right)(I-\\gamma^{2}P_{\\pi})^{-1}v\\right\\|_{\\infty}}&{}\\\\ {\\ \\leq\\left\\|(1-\\gamma)(I-\\gamma P_{\\pi})^{-1}(I-\\gamma^{2}P_{\\pi})^{-1}v\\right\\|_{\\infty}+\\gamma\\left\\|(I-\\gamma^{2}P_{\\pi})^{-1}v\\right\\|_{\\infty}}&{}\\\\ {\\ \\leq(1-\\gamma)\\left\\|(I-\\gamma P_{\\pi})^{-1}\\right\\|_{\\infty\\to\\infty}\\left\\|(I-\\gamma^{2}P_{\\pi})^{-1}v\\right\\|_{\\infty}+\\gamma\\left\\|(I-\\gamma^{2}P_{\\pi})^{-1}v\\right\\|_{\\infty}}&{}\\\\ {\\leq(1+\\gamma)\\left\\|(I-\\gamma^{2}P_{\\pi})^{-1}v\\right\\|_{\\infty}}&{}\\\\ {\\ \\leq2\\left\\|(I-\\gamma^{2}P_{\\pi})^{-1}v\\right\\|_{\\infty}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Combining these calculations with the variance Bellman equation (6), we conclude that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\gamma\\left\\|(I-\\gamma P_{\\pi})^{-1}\\sqrt{v}\\right\\|_{\\infty}\\leq\\gamma\\frac{1}{\\sqrt{1-\\gamma}}\\sqrt{2\\left\\|(I-\\gamma^{2}P_{\\pi})^{-1}v\\right\\|_{\\infty}}\\leq\\sqrt{\\frac{2}{1-\\gamma}}\\sqrt{\\left\\|\\mathbb{V}^{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]\\right\\|_{\\infty}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "as desired. ", "page_idx": 13}, {"type": "text", "text": "The following is a multi-step version of the variance Bellman equation, which we will later apply With $T=\\mathsf{H}$ but holds for arbitrary $T$ ", "page_idx": 13}, {"type": "text", "text": "Lemma 13. For any integer $T\\geq1$ for any deterministic stationary policy $\\pi$ ,we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{V}^{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]=\\mathbb{V}^{\\pi}\\left[\\sum_{t=0}^{T-1}\\gamma^{t}R_{t}+\\gamma^{T}V_{\\gamma}^{\\pi}(S_{T})\\right]+\\gamma^{2T}P_{\\pi}^{T}\\mathbb{V}^{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and consequently ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{V}^{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]\\right\\|_{\\infty}\\leq\\frac{\\left\\|\\mathbb{V}^{\\pi}\\left[\\sum_{t=0}^{T-1}\\gamma^{t}R_{t}+\\gamma^{T}V_{\\gamma}^{\\pi}(S_{T})\\right]\\right\\|_{\\infty}}{1-\\gamma^{2T}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Fix a state $s_{0}\\in\\mathcal{S}$ . Letting $\\mathcal{F}_{T}$ be the $\\sigma$ -algebra generated by $(S_{1},\\bot...,S_{T})$ , we calculate that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int_{s_{0}}^{\\pi}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]=\\mathbb{E}_{s_{0}}^{\\pi}\\left(\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}-V_{\\gamma}^{\\pi}(s_{0})\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{s_{0}}^{\\pi}\\left(\\displaystyle\\sum_{t=0}^{T-1}\\gamma^{t}R_{t}+\\gamma^{T}V_{\\gamma}^{\\pi}(S_{T})-V_{\\gamma}^{\\pi}(s_{0})+\\displaystyle\\sum_{t=T}^{\\infty}\\gamma^{t}R_{t}-\\gamma^{T}V_{\\gamma}^{\\pi}(S_{T})\\right)^{2}}\\\\ &{\\displaystyle=\\mathbb{E}_{s_{0}}^{\\pi}\\left[\\mathbb{E}_{s_{0}}^{\\pi}\\left[\\left(\\displaystyle\\sum_{t=0}^{T-1}\\gamma^{t}R_{t}+\\gamma^{T}V_{\\gamma}^{\\pi}(S_{T})-V_{\\gamma}^{\\pi}(s_{0})+\\displaystyle\\sum_{t=T}^{\\infty}\\gamma^{t}R_{t}-\\gamma^{T}V_{\\gamma}^{\\pi}(S_{T})\\right)^{2}\\right]\\bigg\\rvert\\mathcal{F}_{T}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Using the above shorthands and opening the square, we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\gamma_{\\infty}^{\\pi}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]=\\mathbb{E}_{s_{0}}^{\\pi}\\left[\\mathbb{E}_{s_{0}}^{\\pi}\\left[A^{2}+B^{2}+2A B|\\mathcal{F}_{T}\\right]\\right]}\\\\ &{=\\mathbb{E}_{s_{0}}^{\\pi}\\left[A^{2}+\\mathbb{E}_{s_{0}}^{\\pi}\\left[B^{2}|\\mathcal{F}_{T}\\right]+2A\\mathbb{E}_{s_{0}}^{\\pi}\\left[B|\\mathcal{F}_{T}\\right]\\right]}\\\\ &{=\\mathbb{E}_{s_{0}}^{\\pi}\\left[A^{2}+\\mathbb{E}_{s_{0}}^{\\pi}\\left[B^{2}\\right]\\right]}\\\\ &{=\\mathbb{E}_{s_{0}}^{\\pi}\\left[\\displaystyle\\sum_{t=0}^{T-1}\\gamma^{t}R_{t}+\\gamma^{T}V_{\\gamma}^{\\pi}(S_{T})-V_{\\gamma}^{\\pi}(s_{0})\\right)^{2}+\\mathbb{E}_{s_{T}}^{\\pi}\\left[\\left(\\displaystyle\\sum_{t=T}^{\\infty}\\gamma^{t}R_{t}-\\gamma^{T}V_{\\gamma}^{\\pi}(S_{T})\\right)\\right.}\\\\ &{\\quad\\left.=\\mathbb{E}_{s_{0}}^{\\pi}\\left[\\displaystyle\\sum_{t=0}^{T-1}\\gamma^{t}R_{t}+\\gamma^{T}V_{\\gamma}^{\\pi}(S_{T})-V_{\\gamma}^{\\pi}(s_{0})\\right)^{2}+\\gamma^{2}\\mathbb{E}_{s_{T}}^{\\pi}\\left[\\left(\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}-V_{\\gamma}^{\\pi}(S_{T})\\right)\\right]}\\\\ &{=\\mathbb{V}_{s_{0}}^{\\pi}\\left[\\displaystyle\\sum_{t=0}^{T-1}\\gamma^{t}R_{t}+\\gamma^{T}V_{\\gamma}^{\\pi}(S_{T})\\right]+\\gamma^{2}F_{\\sigma_{0}}^{\\pi}P_{T}^{T}\\mathbb{V}^{\\pi}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we used the tower property, the Markov property, and the fact that $\\mathbb{E}_{s_{0}}^{\\pi}\\left[B|\\mathcal{F}_{T}\\right]=0$ (which is immediate from the definition of $V_{\\gamma}^{\\pi}$ ).Since $e_{s_{0}}^{\\top}P_{\\pi}^{T}$ is a probability distribution, it follows from Holder's inequality that $\\begin{array}{r}{\\left|e_{s_{0}}^{\\top}P_{\\pi}^{T}\\mathbb{V}^{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]\\right|\\leq\\left\\|\\mathbb{V}^{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]\\right\\|_{\\infty}.}\\end{array}$ Therefore, it holds that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{V}_{s_{0}}^{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]\\right\\|_{\\infty}\\leq\\left\\|\\mathbb{V}^{\\pi}\\left[\\sum_{t=0}^{T-1}\\gamma^{t}R_{t}+\\gamma^{T}V_{\\gamma}^{\\pi}(S_{T})\\right]\\right\\|_{\\infty}+\\gamma^{2T}\\left\\|\\mathbb{V}_{s_{0}}^{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]\\right\\|_{\\infty}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and we can obtain the desired conclusion after rearranging terms. ", "page_idx": 14}, {"type": "text", "text": "We also need the following elemetary inequality. ", "page_idx": 14}, {"type": "text", "text": "Lemma 14. $\\begin{array}{r}{I f\\gamma\\geq1-\\frac{1}{T}}\\end{array}$ for some integer $T\\geq1$ then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{1-\\gamma^{2T}}{1-\\gamma}\\geq\\left(1-\\frac{1}{e^{2}}\\right)T\\geq\\frac{4}{5}T.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Fixing $T\\geq1$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\frac{1-\\gamma^{2T}}{1-\\gamma}}=1+\\gamma+\\gamma^{2}+\\cdot\\cdot\\cdot+\\gamma^{2T-1}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is increasing in , so inf >1-\u2192 is attained at $\\textstyle\\gamma=1-{\\frac{1}{T}}$ . Now allowing $T\\geq1$ to be arbitrary, note ()T(1-(1-1)\uff09soitsufiestosowhat1-(1-)\u22651-e2 for all $T\\geq1$ . By computing the derivative, one finds that $\\textstyle1-\\left(1-{\\frac{1}{T}}\\right)^{2T}$ is monotonically decreasing, ", "page_idx": 15}, {"type": "equation", "text": "$$\n1-\\left(1-\\frac{1}{T}\\right)^{2T}\\geq\\operatorname*{lim}_{T\\to\\infty}1-\\left(1-\\frac{1}{T}\\right)^{2T}=1-\\frac{1}{e^{2}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We can now provide a bound on the variance of the total discounted rewards under $\\pi_{\\gamma}^{\\star}$ ", "page_idx": 15}, {"type": "text", "text": "Lemma 15. Letting $\\pi_{\\gamma}^{\\star}$ be the optialpolicyfortheweaklycommunicating discounted MDP $(P,r,\\gamma)$ $\\begin{array}{r}{i f\\gamma\\geq1-\\frac{1}{\\mathsf{H}}}\\end{array}$ we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{V}^{\\pi_{\\gamma}^{\\star}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]\\right\\|_{\\infty}\\leq5\\frac{\\mathsf{H}}{1-\\gamma}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. By using the multi-step variance Bellman equation in Lemma 13, it suffices to bound the quantity $\\begin{array}{r}{\\left\\|\\mathbb{V}^{\\pi_{\\gamma}^{\\star}}\\left[\\sum_{t=0}^{\\mathsf{H}-1}\\gamma^{t}R_{t}+\\gamma^{\\mathsf{H}}V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}(S_{\\mathsf{H}})\\right]\\right\\|_{\\infty}.}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Fixing a state $s_{0}\\in\\mathcal S$ \uff0c ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\quad\\Psi_{s_{0}}^{\\pi^{*}}\\left[\\displaystyle\\sum_{t=0}^{\\mu-1}\\gamma^{t}R_{t}+\\gamma^{\\mathrm{q}}V_{\\gamma}^{\\pi^{*}}(S_{\\mathrm{t}})\\right]=\\Psi_{s_{0}}^{\\pi^{*}}\\left[\\displaystyle\\sum_{t=0}^{1-1}\\gamma^{t}R_{t}+\\gamma^{\\mathrm{q}}\\left(V_{\\gamma}^{\\pi^{*}}(S_{\\mathrm{H}})-\\displaystyle\\frac{1}{1-\\gamma}\\rho^{*}\\right)\\right]}&{}\\\\ {\\leq\\mathbb{E}_{s_{0}}^{\\pi^{*}}\\left[\\displaystyle\\sum_{t=0}^{\\mu-1}\\gamma^{t}R_{t}+\\gamma^{\\mathrm{q}}\\left(V_{\\gamma}^{\\pi^{*}}(S_{\\mathrm{H}})-\\displaystyle\\frac{1}{1-\\gamma}\\rho^{*}\\right)\\right]^{2}}&{}\\\\ {\\leq2\\mathbb{E}_{s_{0}}^{\\pi^{*}}\\left[\\displaystyle\\sum_{t=0}^{1-1}\\gamma^{t}R_{t}\\right]^{2}+2\\mathbb{E}_{s_{0}}^{\\pi^{*}}\\left|\\gamma^{\\mathrm{q}}\\left(V_{\\gamma}^{\\pi^{*}}(S_{\\mathrm{H}})-\\displaystyle\\frac{1}{1-\\gamma}\\rho^{*}\\right)\\right|^{2}}&{}\\\\ {\\leq2\\mathbb{H}^{2}+2\\operatorname*{sup}_{s}\\left(V_{\\gamma}^{\\pi^{*}}(s)-\\displaystyle\\frac{1}{1-\\gamma}\\rho^{*}\\right)^{2}}&{}\\\\ {\\leq4\\mathbb{H}^{2}}&{\\leq2\\operatorname*{sup}_{s}\\left(V_{\\gamma}^{\\pi^{*}}(s)-\\displaystyle\\frac{1}{1-\\gamma}\\rho^{*}\\right)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where in the final inequality we used Lemma 11. Taking the maximum over all states $s$ andcombining with Lemma 13 we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{V}^{\\pi_{\\gamma}^{\\star}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]\\right\\|_{\\infty}\\leq\\frac{4\\mathsf{H}^{2}}{1-\\gamma^{2\\mathsf{H}}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining this bound with the elementary inequality in Lemma 14, which can be rearranged to show $\\begin{array}{r}{\\frac{1}{1-\\gamma^{2\\sf H}}\\leq\\frac{5}{4}\\frac{1}{(1-\\gamma)\\sf H}}\\end{array}$ wecompletetheprof. ", "page_idx": 15}, {"type": "text", "text": "We also need to control the variance under $\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}$ , which requires additional stes. This is done in the following lemma. ", "page_idx": 15}, {"type": "text", "text": "Lemma 16. We have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{V}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\widetilde{R}_{t}\\right]\\right\\|_{\\infty}\\leq15\\frac{\\mathsf{H}^{2}+\\left\\|V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}-\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\|_{\\infty}^{2}+\\left\\|V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}-\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\pi_{\\gamma}^{\\star}}\\right\\|_{\\infty}^{2}}{\\mathsf{H}(1-\\gamma)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. In light of the multi-step variance Bellman equation in Lemma 13, it suffices to give a bound on $\\begin{array}{r}{\\left\\|\\overline{{\\mathbb{V}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\left[\\sum_{t=0}^{\\mathsf{H}-1}\\gamma^{t}\\widetilde{R}_{t}+\\gamma^{\\mathsf{H}}\\bar{V}_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}(S_{\\mathsf{H}})\\right]}\\right\\|_{\\infty}}\\end{array}$ .We have for any state $s_{0}$ that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{\\sigma_{k}}^{\\varepsilon,\\frac{1}{\\varepsilon}}\\Bigg[\\underset{t=0}{\\overset{...}{\\prod}}\\Bigg\\lbrace\\overline{{\\gamma}}_{k}^{t}+\\gamma\\mathfrak{F}_{k}^{t}\\mathfrak{L}_{k}^{\\varepsilon}(\\xi_{0})}\\\\ &{=\\mathbb{P}_{\\sigma_{k}}^{\\varepsilon,\\frac{1}{\\varepsilon}}\\Bigg[\\underset{t=0}{\\overset{...}{\\prod}}\\mathfrak{F}_{k}^{t}+\\gamma\\mathfrak{F}_{k}^{t}\\mathfrak{L}_{k}^{\\varepsilon}(\\xi_{0})-\\frac{\\gamma\\mathfrak{F}_{k}}{1-\\gamma^{\\varepsilon}}\\mathfrak{L}_{k}^{\\varepsilon}\\Bigg]}\\\\ &{\\leq\\mathbb{E}_{\\sigma_{k}}^{\\varepsilon,\\frac{1}{\\varepsilon}}\\Bigg(\\frac{\\sqrt{\\varepsilon_{0}^{2}}}{2\\sqrt{\\varepsilon_{0}^{2}}}\\sqrt{\\mathfrak{F}_{k}^{\\varepsilon}}+\\gamma\\mathfrak{F}_{k}^{t}\\mathfrak{L}_{k}^{\\varepsilon}(\\xi_{0})-\\gamma\\mathfrak{F}_{k}^{\\varepsilon}(\\xi_{0})\\Bigg)^{2}}\\\\ &{=\\mathbb{E}_{\\sigma_{k}}^{\\varepsilon,\\frac{1}{\\varepsilon}}\\Bigg(\\frac{\\sqrt{\\varepsilon_{0}^{2}}}{2\\sqrt{\\varepsilon_{0}^{2}}}\\sqrt{\\mathfrak{F}_{k}}+\\gamma\\mathfrak{F}_{k}^{\\varepsilon}\\mathfrak{L}_{k}^{\\varepsilon}(\\xi_{0})(\\xi_{0})-\\gamma\\mathfrak{F}_{k}^{\\varepsilon}(\\xi_{0})+\\gamma\\mathfrak{F}_{k}^{\\varepsilon}(\\xi_{0})-\\frac{1}{1-\\gamma^{\\varepsilon}}\\mathfrak{F}_{k}^{\\varepsilon}\\Bigg)\\Bigg)^{2}}\\\\ &{\\leq\\mathbb{S}_{\\sigma_{k}}^{\\varepsilon,\\frac{1}{\\varepsilon}}\\Bigg(\\frac{\\sqrt{\\varepsilon_{0}^{2}}}{2\\sqrt{\\varepsilon_{0}^{2}}}\\sqrt{\\mathfrak{F}_{k}^{\\varepsilon}\\mathfrak{L}_{k}^{\\varepsilon}}\\Bigg)^{2}+\\gamma\\mathfrak{F}_{k}^{\\varepsilon,\\frac{1}{\\varepsilon}}\\mathfrak{L}_{k}^{\\varepsilon,\\frac{1}{\\varepsilon}}\\left(\\mathbb{S}_{k}^{\\varepsilon,\\frac{1}{\\varepsilon}}(\\xi_{0})-\\gamma\\mathfrak{F}_{k}^{\\varepsilon\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we have used triangle inequality and the inequalities $(a+b)^{2}\\leq2a^{2}+2b^{2}$ and $(a+b+c)^{2}\\leq$ $3a^{2}+3b^{2}+3c^{2}$ . Now we bound each term of (7). First, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\langle\\mathbb{E}_{s_{0}}^{\\hat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\left(\\sum_{t=0}^{\\mathsf{H}-1}\\gamma^{t}\\widetilde{R}_{t}\\right)^{2}\\leq3\\left(\\mathsf{H}\\left\\Vert\\widetilde{r}\\right\\Vert_{\\infty}\\right)^{2}\\leq3\\mathsf{H}^{2}(\\Vert r\\Vert_{\\infty}+\\xi)^{2}\\leq6\\mathsf{H}^{2}\\left(1+\\left(\\frac{\\left(1-\\gamma\\right)\\varepsilon}{6}\\right)^{2}\\right)\\leq6\\mathsf{H}^{2}\\left(\\frac{7}{6}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we had $\\begin{array}{r}{\\frac{(1-\\gamma)\\varepsilon}{6}\\leq\\frac{\\varepsilon}{6\\mathsf{H}}\\leq\\frac{1}{6}}\\end{array}$ because $\\frac{1}{1-\\gamma}\\geq{\\sf H}$ and $\\varepsilon\\le{\\mathsf{H}}$ Clearly t holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n6\\gamma^{2\\mathbb{H}}\\mathbb{E}_{s_{0}}^{\\widehat\\pi_{\\gamma,\\mathbb{p}}^{\\star}}\\left(V_{\\gamma}^{\\widehat\\pi_{\\gamma,\\mathbb{p}}^{\\star}}(S_{\\mathsf{H}})-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}(S_{\\mathsf{H}})\\right)^{2}\\leq6\\left\\lVert V_{\\gamma}^{\\widehat\\pi_{\\gamma,\\mathbb{p}}^{\\star}}-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\rVert_{\\infty}^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By an argument identical to those used in the proof of the error bounds in Lemma 10, we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\|V_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\|_{\\infty}\\leq\\frac{1}{1-\\gamma}\\xi=\\frac{\\varepsilon}{6},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$\\begin{array}{r}{6\\gamma^{2\\mathsf{H}}\\left\\|V_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\|_{\\infty}^{2}\\leq\\frac{\\varepsilon^{2}}{6}\\leq\\frac{\\mathsf{H}^{2}}{6}}\\end{array}$ sine $\\varepsilon\\le{\\mathsf{H}}$ Fially uing Lema e bain ", "page_idx": 16}, {"type": "equation", "text": "$$\n3\\gamma^{2\\mathsf{H}}\\mathbb{E}_{s_{0}}^{\\widehat\\pi_{\\gamma,\\mathtt{p}}^{\\star}}\\left(V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}(S_{\\mathsf{H}})-\\frac{1}{1-\\gamma}\\rho^{\\star}\\right)^{2}\\leq3\\operatorname*{sup}_{s}\\left\\vert V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}(S_{\\mathsf{H}})-\\frac{1}{1-\\gamma}\\rho^{\\star}\\right\\vert^{2}\\leq3\\mathsf{H}^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using all these bounds in (7), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\nabla_{x_{0}}^{\\widehat{\\pi}_{y}^{*}}\\left[\\sum_{t=0}^{H-1}\\gamma^{t}\\widetilde{R}_{t}+\\gamma^{t}\\mathcal{W}_{\\gamma_{y}^{\\widehat{\\gamma}},\\mathbf{p}}^{\\widehat{\\pi}_{y}^{*}}(S_{\\mathbf{H}})\\right]}\\\\ &{\\le3\\mathbb{E}_{s_{0}}^{\\widehat{\\pi}_{y}^{*}}\\left(\\sum_{t=0}^{H-1}\\gamma^{t}\\widetilde{R}_{t}\\right)^{2}+6\\gamma^{2t}\\mathbb{E}_{\\mathbb{S}_{0}^{\\gamma_{x}}}^{\\widehat{\\pi}_{y}^{*},\\widehat{\\pi}}\\left(V_{\\gamma}^{\\widehat{\\pi}_{y}^{*},\\nu}(S_{\\mathbf{H}})-V_{\\gamma}^{\\pi_{y}^{*}}(S_{\\mathbf{H}})\\right)^{2}+6\\gamma^{2t}\\left\\lVert\\left\\vert V_{\\gamma,\\mathbf{p}}^{\\widehat{\\pi}_{y}^{*},\\nu}-V_{\\gamma}^{\\widehat{\\pi}_{y}^{*},\\nu}\\right\\vert\\right\\vert_{\\infty}^{2}}\\\\ &{\\quad\\quad+\\left.3\\gamma^{2t}\\mathbb{E}_{s_{0}}^{\\widehat{\\pi}_{y}^{*},\\nu}\\left(V_{\\gamma}^{\\pi_{y}^{*}}(S_{\\mathbf{H}})-\\frac{1}{1-\\gamma}\\rho^{*}\\right)^{2}\\right.}\\\\ &{\\le\\left.\\left(\\frac{49}{6}+\\frac{1}{6}+3\\right)\\mathbb{H}^{2}+6\\left\\lVert V_{\\gamma}^{\\widehat{\\pi}_{y}^{*},\\nu}-V_{\\gamma}^{\\pi_{y}^{*}}\\right\\rVert_{\\infty}^{2}}\\\\ &{\\le12\\mathbb{H}^{2}+6\\left\\lVert V_{\\gamma}^{\\widehat{\\pi}_{y}^{*},\\nu}-V_{\\gamma}^{\\pi_{y}^{*}}\\right\\rVert_{\\infty}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Finally, we use the elementwise inequality ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\geq V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}^{\\star}}}\\\\ &{\\phantom{\\leq}\\geq\\widehat{V}_{\\gamma,\\mathbf{p}}^{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}^{\\star}}-\\left\\lVert\\widehat{V}_{\\gamma,\\mathbf{p}}^{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}^{\\star}}\\right\\rVert_{\\infty}\\mathbf{1}}\\\\ &{\\phantom{\\leq}\\geq\\widehat{V}_{\\gamma,\\mathbf{p}}^{\\pi_{\\gamma}^{\\star}}-\\left\\lVert\\widehat{V}_{\\gamma,\\mathbf{p}}^{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}^{\\star}}\\right\\rVert_{\\infty}\\mathbf{1}}\\\\ &{\\phantom{\\leq}\\geq V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}-\\left\\lVert\\widehat{V}_{\\gamma,\\mathbf{p}}^{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}^{\\star}}\\right\\rVert_{\\infty}\\mathbf{1}-\\left\\lVert\\widehat{V}_{\\gamma,\\mathbf{p}}^{\\pi_{\\gamma}^{\\star}}-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\rVert_{\\infty}\\mathbf{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "from which it follows that $\\left\\|V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\|_{\\infty}\\leq\\left\\|\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\|_{\\infty}+\\left\\|\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\pi_{\\gamma}^{\\star}}-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\|_{\\infty}$ . Combining this with (8), we conclude ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{V}_{s_{0}}^{\\hat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\left[\\sum_{t=0}^{\\mathsf{H}-1}\\gamma^{t}\\widetilde{R}_{t}+\\gamma^{\\mathsf{H}}V_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}(S_{\\mathsf{H}})\\right]\\leq12\\mathsf{H}^{2}+12\\left\\lVert\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\rVert_{\\infty}^{2}+12\\left\\lVert\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\pi_{\\gamma}^{\\star}}-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\rVert_{\\infty}^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now combining with Lemma 13 and then using Lemma 14, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\nabla^{\\hat{\\pi}_{\\gamma,p}^{*}}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\tilde{R_{t}}\\right]\\right\\|_{\\infty}\\leq\\frac{\\left\\|\\nabla^{\\hat{\\pi}_{\\gamma,p}^{*}}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\tilde{R_{t}}+\\gamma^{t}V_{\\gamma}^{\\hat{\\pi}_{\\gamma,p}^{*}}(S_{t})\\right]\\right\\|_{\\infty}}{1-\\gamma^{2H}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq12\\frac{\\mathbf{H}^{2}+\\left\\|V_{\\gamma}^{\\hat{\\pi}_{\\gamma,p}^{*}}-\\hat{V}_{\\gamma,p}^{\\hat{\\pi}_{\\gamma,p}^{*}}\\right\\|_{\\infty}^{2}+\\left\\|V_{\\gamma}^{\\pi_{\\gamma}^{*}}-\\hat{V}_{\\gamma,p}^{\\pi_{\\gamma}^{*}}\\right\\|_{\\infty}^{2}}{1-\\gamma^{2H}}}\\\\ &{\\qquad\\qquad\\qquad\\leq12\\frac{5}{4}\\frac{\\mathbf{H}^{2}+\\left\\|V_{\\gamma}^{\\hat{\\pi}_{\\gamma,p}^{*}}-\\hat{V}_{\\gamma,p}^{\\hat{\\pi}_{\\gamma,p}^{*}}\\right\\|_{\\infty}^{2}+\\left\\|V_{\\gamma}^{\\pi_{\\gamma}^{*}}-\\hat{V}_{\\gamma,p}^{\\pi_{\\gamma}^{*}}\\right\\|_{\\infty}^{2}}{\\mathbb{H}(1-\\gamma)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times12^{2}+\\left\\|V_{\\gamma}^{\\hat{\\pi}_{\\gamma,p}^{*}}-\\hat{V}_{\\gamma,p}^{\\hat{\\pi}_{\\gamma,p}^{*}}\\right\\|_{\\infty}^{2}+\\left\\|V_{\\gamma}^{\\pi_{\\gamma}^{*}}-\\hat{V}_{\\gamma,p}^{\\pi_{\\gamma}^{*}}\\right\\|_{\\infty}^{2}}\\\\ &{\\qquad\\qquad\\qquad=15\\frac{\\mathbf{H}^{2}+\\left\\|V_{\\gamma}^{\\hat{\\pi}_{\\gamma,p}^{*}}-\\hat{V}_{\\gamma,p}^{\\pi_{\\gamma}^{*}}\\right\\|_{\\infty}^{2}+\\left\\|V_{\\gamma}^{\\pi_{\\gamma}^{*}}-\\hat{V}_{\\gamma,p}^{\\pi_ \n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "as desired. ", "page_idx": 17}, {"type": "text", "text": "A.2  Proofs of Theorems 1 and 2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "With the above lemmas we can complete the proof of Theorem 1 on discounted MDPs. ", "page_idx": 17}, {"type": "text", "text": "Proof of Theorem 1. Our approach will be to utilize our variance bounds within the error bounds from Lemma 10. We will find a value for $n$ whichguaranteesthat $\\left\\|\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\pi_{\\gamma}^{\\star}}-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\|_{\\infty}$ and $\\left\\|\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma}^{\\star},\\mathrm{p}}\\right\\|_{\\infty}$ are both $\\leq\\varepsilon/2$ , which guarantees that $\\left\\|V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\|_{\\infty}\\leq\\varepsilon$ ", "page_idx": 17}, {"type": "text", "text": "First we note thathe conclusions ofLemma 10 require n\u2265  log (se sowe assume $n$ is large enough that this holds. ", "page_idx": 18}, {"type": "text", "text": "Nowwebound $\\left\\|\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\pi_{\\gamma}^{\\star}}-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\|_{\\infty}$ : Starting with inequality (I fom Lemma I and the aplying ur variance bounds through Lemma 12 and then Lemma 15, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\hat{V}_{\\tau}^{x,\\xi}-V_{\\tau}^{x,\\xi}\\right\\|_{\\infty}}\\\\ &{\\quad\\leq\\gamma\\sqrt{\\frac{c_{1}\\log\\left(\\frac{S\\,A}{(1-\\gamma)k\\varepsilon}\\right)}{n}}\\left\\|(I-\\gamma P_{x,\\xi})^{-1}\\sqrt{\\nabla{P_{\\tau,\\xi}}\\left[V_{\\tau}^{x,\\xi}\\right]}\\right\\|_{\\infty}+c_{1}\\gamma\\frac{\\log\\left(\\frac{S\\,A}{(1-\\gamma)k\\varepsilon}\\right)}{(1-\\gamma)n}\\left\\|{V_{\\tau}^{x,\\xi}}\\right\\|_{\\infty}+\\frac{\\varepsilon}{6}}\\\\ &{\\quad\\leq\\sqrt{\\frac{c_{1}\\log\\left(\\frac{S\\,A}{(1-\\gamma)k\\varepsilon}\\right)}{n}}\\sqrt{\\frac{2}{1-\\gamma}}\\sqrt{\\left\\|\\nabla{\\tau^{*}}\\left[\\sum_{\\tau=0}^{\\infty}\\gamma^{t}R_{t}\\right]\\right\\|_{\\infty}}+c_{1}\\gamma\\frac{\\log\\left(\\frac{S\\,A}{(1-\\gamma)k\\varepsilon}\\right)}{(1-\\gamma)n}\\left\\|{V_{\\tau}^{x,\\xi}}\\right\\|_{\\infty}+\\frac{\\varepsilon}{6}}\\\\ &{\\quad\\leq\\sqrt{\\frac{c_{1}\\log\\left(\\frac{S\\,A}{(1-\\gamma)k\\varepsilon}\\right)}{n}}\\sqrt{\\frac{2}{1-\\gamma}}\\sqrt{5\\frac{\\mathbb H}{1-\\gamma}}+c_{1}\\gamma\\frac{\\log\\left(\\frac{S\\,A}{(1-\\gamma)k\\varepsilon}\\right)}{(1-\\gamma)n}\\left\\|{V_{\\tau}^{x,\\xi}}\\right\\|_{\\infty}+\\frac{\\varepsilon}{6}}\\\\ &{\\quad\\leq\\sqrt{\\frac{c_{1}\\log\\left(\\frac{S\\,A}{(1-\\gamma)k\\varepsilon}\\right)}{n}}\\sqrt{\\operatorname*{lom}\\left(\\frac{M}{(1-\\gamma)^{2}}\\right)}+c_{1}\\frac{\\log\\left(\\frac{S\\,A}{(1-\\gamma)k\\varepsilon}\\right)}{(1-\\gamma)^{2}n}+\\frac{\\varepsilon}{6}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where in the last inequality we used the facts that $\\begin{array}{r}{\\left\\|V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\|_{\\infty}\\leq\\frac{1}{1-\\gamma}}\\end{array}$ and $\\gamma\\le1$ . Now if we assume $\\begin{array}{r}{n\\geq360c_{1}\\frac{\\mathsf{H}}{(1-\\gamma)^{2}\\varepsilon^{2}}\\log\\left(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\right)}\\end{array}$ w have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\widehat{V}_{\\gamma,\\mathbf{p}}^{\\pi_{\\gamma}^{\\star}}-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\|_{\\infty}\\leq\\sqrt{\\frac{c_{1}\\log\\left(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\right)}{n}}\\sqrt{10\\frac{\\mathsf{H}}{(1-\\gamma)^{2}}}+c_{1}\\frac{\\log\\left(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\right)}{(1-\\gamma)^{2}n}+\\frac{\\varepsilon}{6}}\\\\ &{\\qquad\\qquad\\leq\\frac{1}{6}\\sqrt{\\varepsilon^{2}}+\\frac{1}{6}\\frac{\\varepsilon^{2}}{\\mathsf{H}}+\\frac{\\varepsilon}{6}}\\\\ &{\\qquad\\qquad\\leq\\varepsilon/2}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "due to the fact that $\\varepsilon\\leq{\\mathsf{H}}$ ", "page_idx": 18}, {"type": "text", "text": "Next, to bound $\\left\\|\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\|_{\\infty}$ , starting from inequality (2) in Lemma 10 and then analogously applying Lemma 12 and then Lemma 16, we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\hat{V}_{\\gamma,\\gamma^{*}}^{\\frac{\\alpha_{1}}{\\alpha_{2}},\\gamma}\\!\\!\\right\\|_{\\infty}}\\\\ &{\\leq\\gamma\\sqrt{\\frac{C_{1}\\log\\left(\\frac{S_{4}}{(1\\!-\\!\\gamma)\\delta\\varepsilon}\\right)}{n}}\\left\\|(I-\\gamma P_{\\varepsilon;\\varepsilon_{\\varepsilon}})^{-1}\\sqrt{\\nabla{P_{\\varepsilon;\\varepsilon_{\\varepsilon}}}\\left[V_{\\gamma,\\gamma}^{\\frac{\\alpha_{\\varepsilon}}{\\gamma},\\gamma}\\right]}\\right\\|_{\\infty}+c_{1}\\gamma\\frac{\\log\\left(\\frac{S_{4}}{(1\\!-\\!\\gamma)\\delta\\varepsilon}\\right)}{(1-\\gamma)n}\\left\\|V_{\\gamma,\\gamma^{*}}^{\\frac{\\alpha_{\\varepsilon}}{\\gamma},\\gamma}\\right\\|_{\\infty}+\\frac{\\varepsilon^{2}}{\\bar{\\zeta}}}\\\\ &{\\leq\\sqrt{\\frac{C_{1}\\log\\left(\\frac{S_{4}}{(1\\!-\\!\\gamma)\\delta\\varepsilon}\\right)}{n}}\\sqrt{\\frac{2}{1-\\gamma}}\\sqrt{\\left\\|\\nabla^{*}\\gamma_{,\\gamma}^{*}\\left[\\displaystyle\\sum_{\\ell=0}^{\\infty}\\gamma^{\\ell}\\tilde{R}_{\\mu}\\right]\\right\\|_{\\infty}}+c_{1}\\gamma\\frac{\\log\\left(\\frac{S_{4}}{(1\\!-\\!\\gamma)\\delta\\varepsilon}\\right)}{(1-\\gamma)n}\\left\\|V_{\\gamma,\\gamma^{*}}^{\\frac{\\alpha_{\\varepsilon}}{\\gamma},\\gamma}\\right\\|_{\\infty}+\\frac{\\varepsilon}{6}}\\\\ &{\\leq\\sqrt{\\frac{c_{1}\\log\\left(\\frac{S_{4}}{(1\\!-\\!\\gamma)\\delta\\varepsilon}\\right)}{n}}\\sqrt{\\frac{2}{1\\!-\\gamma}}\\sqrt{\\frac{\\|\\nabla\\xi^{\\frac{\\alpha_{\\varepsilon}}{\\gamma},\\gamma}-\\bar{V}_{\\gamma,\\gamma^{\\prime}}^{\\frac{\\alpha_{\\varepsilon}}{\\gamma},\\gamma}\\|_{\\infty}^{2}}{\\|\\nabla\\bar{\\xi}^{\\frac{\\alpha_{\\varepsilon}}{\\gamma},\\gamma}\\|_{\\infty}}+\\left\\|V_{\\gamma}^{\\frac{\\alpha_{\\varepsilon}}{\\gamma}}-\\bar{V}_{\\gamma,\\gamma}^{\\frac{\\alpha_{\\varepsilon}}{\\gamma}}\\right\\|_{\\infty}^{2}}}\\\\ &{\\qquad+c_{1}\\gamma\\frac{\\log\\left(\\frac{S_{4}\\varepsilon}{\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining with the fact from above that $\\begin{array}{r}{\\left\\|\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\pi_{\\gamma}^{\\star}}-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\|_{\\infty}\\leq\\frac{\\mathsf{H}}{2}}\\end{array}$ , as well s the faetsthat $\\left\\|V_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\|_{\\infty}\\leq$ $\\textstyle{\\frac{1}{1-\\gamma}}$ $\\gamma\\le1$ , and ${\\sqrt{a+b}}\\leq{\\sqrt{a}}+{\\sqrt{b}},$ wehave ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|\\bar{V}_{\\gamma,\\theta^{\\star}}^{\\star}-V_{\\gamma,\\theta^{\\star}}^{\\star}\\right\\|_{\\infty}\\leq\\sqrt{\\frac{c_{1}\\log\\left(\\frac{c_{3}A}{(1-\\gamma)\\theta^{\\star}}\\right)}{(1-\\gamma)\\theta^{\\star}}}\\sqrt{\\frac{2}{1-\\gamma}}\\sqrt{\\frac{1}{15}\\frac{\\sin^{2}\\left(1\\left|V_{\\gamma}^{\\star,\\star}-V_{\\gamma,\\theta^{\\star}}^{\\star,\\star}\\right|\\right)^{2}}{\\theta^{\\star}}}}\\\\ {+\\frac{\\log\\left(\\frac{c_{3}A}{(1-\\gamma)\\theta^{\\star}}\\right)}{(1-\\gamma)^{2}\\theta^{\\star}}+\\frac{\\varepsilon}{6}}\\\\ {\\leq\\sqrt{\\frac{c_{1}\\log\\left(\\frac{c_{3}A}{(1-\\gamma)\\theta^{\\star}}\\right)}{(1-\\gamma)\\theta^{\\star}}}\\sqrt{\\frac{3}{\\pi(1-\\gamma)^{2}}}\\left(\\sqrt{\\frac{5}{4}\\Psi}+\\sqrt{\\left\\|V_{\\gamma}^{\\star,\\star}-\\hat{V}_{\\gamma,\\theta^{\\star}}^{\\star,\\star}\\right\\|_{\\infty}^{2}}\\right)}\\\\ {+c_{1}\\frac{\\log\\left(\\frac{c_{3}A}{(1-\\gamma)\\theta^{\\star}}\\right)}{(1-\\gamma)^{2}\\theta^{\\star}}+\\frac{\\varepsilon}{6}}\\\\ {-\\sqrt{\\frac{c_{1}\\log\\left(\\frac{c_{3}A}{(1-\\gamma)\\theta^{\\star}}\\right)}{(1-\\gamma)\\theta^{\\star}}}\\sqrt{\\frac{3}{\\pi(1-\\gamma)^{2}}}\\left(\\sqrt{\\frac{5}{4}\\Psi}+\\left\\|V_{\\gamma}^{\\star,\\star}-\\hat{V}_{\\gamma,\\theta^{\\star}}^{\\star,\\star}\\right\\|_{\\infty}\\right)}\\\\ {+c_{1}\\frac{\\log\\left(\\frac{c_{3}A}{(1-\\gamma)\\theta^{\\star}}\\right)}{(1-\\gamma)^{2}\\theta^{\\star}}+\\frac{\\varepsilon}{6}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Rearranging terms gives ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(1-\\sqrt{\\frac{c_{1}\\log\\Big(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\Big)}{n}}\\sqrt{\\frac{30}{\\mathsf{H}(1-\\gamma)^{2}}}\\right)\\left\\Vert\\widehat{V}_{\\gamma,\\mathsf{p}}^{\\hat{\\pi}_{\\gamma,\\mathsf{p}}^{\\star}}-V_{\\gamma}^{\\hat{\\pi}_{\\gamma,\\mathsf{p}}^{\\star}}\\right\\Vert_{\\infty}}\\\\ &{\\leq\\sqrt{\\frac{c_{1}\\log\\Big(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\Big)}{n}}\\sqrt{\\frac{75\\mathsf{H}/2}{(1-\\gamma)^{2}}}+c_{1}\\frac{\\log\\Big(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\Big)}{(1-\\gamma)^{2}n}+\\frac{\\varepsilon}{6}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Assuming $\\begin{array}{r}{n\\geq120c_{1}\\frac{\\mathsf{H}}{(1-\\gamma)^{2}\\varepsilon^{2}}\\log\\left(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\right)}\\end{array}$ ((1)se ), we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n1-\\sqrt{\\frac{c_{1}\\log\\Big(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\Big)}{n}}\\sqrt{\\frac{30}{\\mathsf{H}(1-\\gamma)^{2}}}\\geq1-\\frac{1}{2}\\sqrt{\\frac{\\varepsilon^{2}(1-\\gamma)^{2}}{\\mathsf{H}}\\frac{1}{\\mathsf{H}(1-\\gamma)^{2}}}=1-\\frac{1}{2}\\frac{\\varepsilon}{\\mathsf{H}}\\geq\\frac{1}{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "$\\varepsilon\\le{\\mathsf{H}}$ $\\begin{array}{r}{n\\geq(75/2)\\cdot24^{2}c_{1}\\frac{\\mathsf{H}}{(1-\\gamma)^{2}\\varepsilon^{2}}\\log\\left(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\right)}\\end{array}$ that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{\\frac{c_{1}\\log\\Big(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\Big)}{n}}\\sqrt{\\frac{75\\mathsf{H}/2}{(1-\\gamma)^{2}}}+c_{1}\\frac{\\log\\Big(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\Big)}{(1-\\gamma)^{2}n}+\\frac{\\varepsilon}{6}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{1}{24}\\sqrt{\\frac{(1-\\gamma)^{2}\\varepsilon^{2}}{\\mathsf{H}}\\frac{\\mathsf{H}}{(1-\\gamma)^{2}}}+\\frac{1}{24}\\frac{(1-\\gamma)^{2}\\varepsilon^{2}}{\\mathsf{H}}\\frac{1}{(1-\\gamma)^{2}}+\\frac{\\varepsilon}{6}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{\\varepsilon}{24}+\\frac{\\varepsilon}{24}+\\frac{\\varepsilon}{6}=\\frac{\\varepsilon}{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining these two calculations, we have $\\begin{array}{r}{\\frac{1}{2}\\left\\lVert\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\rVert_{\\infty}\\leq\\frac{\\varepsilon}{4}}\\end{array}$ as desired. ", "page_idx": 19}, {"type": "text", "text": "Since we have established that $\\begin{array}{r}{\\left\\|\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\pi_{\\gamma}^{\\star}}-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\|_{\\infty},\\left\\|\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\|_{\\infty}\\leq\\frac{\\varepsilon}{2}}\\end{array}$ \u2264, since also V, $\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\geq\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\pi_{\\gamma}^{\\star}}$ we can conclude that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\leq\\left\\lVert\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\pi_{\\gamma}^{\\star}}-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\rVert_{\\infty}\\mathbf{1}+\\left\\lVert\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\rVert_{\\infty}\\mathbf{1}\\leq\\varepsilon\\mathbf{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "that is that $\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}$ .s $\\varepsilon$ optimal for the discounted MDP $(P,r,\\gamma)$ ", "page_idx": 20}, {"type": "text", "text": "We finally note that all our requirements on the size of $n$ can be satisfied by requiring ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{n\\geq C_{2}\\frac{\\mathsf{H}}{(1-\\gamma)^{2}\\varepsilon^{2}}\\log\\left(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\right)}\\\\ &{\\phantom{\\geq}:=\\operatorname*{max}\\left\\{\\frac{c_{2}\\mathsf{H}}{(1-\\gamma)^{2}\\varepsilon^{2}},\\frac{360c_{1}\\mathsf{H}}{(1-\\gamma)^{2}\\varepsilon^{2}},\\frac{(75/2)24^{2}c_{1}\\mathsf{H}}{(1-\\gamma)^{2}\\varepsilon^{2}}\\right\\}\\log\\left(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\right)}\\\\ &{\\phantom{\\geq}\\operatorname*{max}\\left\\{\\frac{c_{2}}{1-\\gamma},\\frac{360c_{1}\\mathsf{H}}{(1-\\gamma)^{2}\\varepsilon^{2}},\\frac{(75/2)24^{2}c_{1}\\mathsf{H}}{(1-\\gamma)^{2}\\varepsilon^{2}}\\right\\}\\log\\left(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we used that $\\begin{array}{r}{\\frac{\\mathsf{H}}{(1-\\gamma)^{2}\\varepsilon^{2}}\\geq\\frac{\\mathsf{H}^{2}}{(1-\\gamma)\\varepsilon^{2}}\\geq\\frac{1}{1-\\gamma}}\\end{array}$ (since $\\begin{array}{r}{\\frac{1}{1-\\gamma}\\geq{\\sf H}}\\end{array}$ and $\\mathsf{H}\\geq\\varepsilon]$ ", "page_idx": 20}, {"type": "text", "text": "We next use Theorem 1 to prove Theorem 2 on average-reward MDPs. ", "page_idx": 20}, {"type": "text", "text": "ProofofTheorm2UingTheorm withtet acracy $\\mathsf{H}$ and discount faetor $\\begin{array}{r}{\\overline{{\\gamma}}=1-\\frac{\\varepsilon}{12\\mathsf{H}}}\\end{array}$ we obtain a $\\mathsf{H}$ -optimal policy for the discounted MDP $(P,r,\\overline{{\\gamma}})$ with probability at least $1-\\delta$ as long as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{n\\geq C_{2}\\frac{\\mathsf{H}}{(1-\\overline{{\\gamma}})^{2}\\mathsf{H}^{2}}\\log\\left(\\frac{S A}{(1-\\overline{{\\gamma}})\\delta\\varepsilon}\\right)}}\\\\ {\\displaystyle{=12^{2}C_{2}\\frac{\\mathsf{H}}{\\mathsf{H}^{2}}\\frac{\\mathsf{H}^{2}}{\\varepsilon^{2}}\\log\\left(\\frac{12\\mathsf{H}}{\\varepsilon}\\frac{S A}{\\delta\\varepsilon}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which is satisfied when $\\begin{array}{r}{n\\geq C_{1}\\frac{\\sf H}{\\varepsilon^{2}}\\log\\left(\\frac{S A\\sf H}{\\delta\\varepsilon}\\right)}\\end{array}$ for sufficiently large $C_{1}$ ", "page_idx": 20}, {"type": "text", "text": "ApplyingLma9 wiheror $\\frac{\\varepsilon}{12}$ since we have chosen $\\begin{array}{r}{\\overline{{\\gamma}}=1-\\frac{\\varepsilon/12}{\\mathsf{H}})}\\end{array}$ , we ae that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\rho^{\\star}-\\rho^{\\widehat{\\pi}^{\\star}}\\leq\\left(8+3\\frac{\\mathsf{H}}{\\mathsf{H}}\\right)\\frac{\\varepsilon}{12}\\leq\\varepsilon{\\bf1}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "as desired. ", "page_idx": 20}, {"type": "text", "text": "A.3 Proof of Theorem 3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof of Theorem 3. Fix $T,n\\geq1$ . First we define the instances $\\mathcal{M}_{0}$ and $\\mathcal{M}_{1}$ , which have parameters $B$ and $\\varepsilon$ which we will choose later, using Figure 2. Note that in both MDPs, all states have only one action. The only difference is in the state transition distribution at state 1: For $\\mathbf{\\mathcal{M}}_{0}$ this is a $\\operatorname{Cat}({\\frac{1}{2}},{\\frac{1}{2}})$ distribution and for $\\mathcal{M}_{1}$ this is a $\\textstyle\\operatorname{Cat}({\\frac{1}{2}}+\\varepsilon,{\\frac{1}{2}}-\\varepsilon)$ distribtion, where $\\mathrm{Cat}(p_{1},p_{2})$ denotes the categorical distribution with event probabilities $p_{1}$ and $p_{2}=1-p_{1}$ ", "page_idx": 20}, {"type": "image", "img_path": "pGEY8JQ3qx/tmp/c461b1947abb812d82d35f5eda28be4e1022a75d8e5840a78b97203a19079222.jpg", "img_caption": ["Figure 2: MDPs used in Theorem 3 "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Now we calculate the bias of instance $\\mathcal{M}_{1}$ . It is easy to check the stationary distribution is $\\mu=$ $[\\textstyle{\\frac{1}{2}},{\\frac{1}{4}}+{\\frac{\\varepsilon}{2}},{\\frac{1}{4}}-{\\frac{\\varepsilon}{2}},0]$ Therefore it has optimalgain $\\begin{array}{r}{\\rho^{\\star}\\dot{=}\\frac{1}{2}\\frac{1}{2}+\\frac{1}{4}+\\frac{\\varepsilon}{2}=\\frac{1}{2}+\\frac{\\dot{\\varepsilon}}{2}}\\end{array}$ Nowweclaim that the optimal bias is ", "page_idx": 21}, {"type": "equation", "text": "$$\nh^{\\star}=\\left[\\begin{array}{c}{-\\varepsilon/2}\\\\ {\\frac{1}{2}-\\varepsilon/2}\\\\ {-\\frac{1}{2}-\\varepsilon/2}\\\\ {-(B+1)\\frac{\\varepsilon}{2}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We can check this by showing that $\\mu h^{\\star}=0$ and that $\\rho^{\\star}{\\bf1}+h^{\\star}=r+P h^{\\star}$ where $P$ is the transition matrix of the above MDP (again, note that each state has only one action, so there is only one policy, and we use this policy to induce the markov chain with transition matrix $P$ ).First, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mu h^{\\star}=-\\frac{\\varepsilon}{4}+\\frac{1}{8}+\\frac{\\varepsilon}{4}-\\frac{\\varepsilon}{8}-\\frac{\\varepsilon^{2}}{4}-\\frac{1}{8}+\\frac{\\varepsilon}{4}-\\frac{\\varepsilon}{8}+\\frac{\\varepsilon^{2}}{4}=0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It is also easy to check the first three rows of the equality $\\rho^{\\star}{\\bf1}+h^{\\star}=r+P h^{\\star}$ . For the fourth row, wehave ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{h^{\\star}(4)+\\displaystyle\\frac{1}{2}+\\displaystyle\\frac{\\varepsilon}{2}=\\displaystyle\\frac{1}{2}+\\displaystyle\\frac{1}{B}h^{\\star}(1)+\\left(1-\\displaystyle\\frac{1}{B}\\right)h^{\\star}(4)}}\\\\ {{\\Longleftrightarrow\\displaystyle\\frac{1}{B}h^{\\star}(4)=\\displaystyle\\frac{-\\varepsilon}{2B}-\\displaystyle\\frac{\\varepsilon}{2}}}\\\\ {{\\Longleftrightarrow h^{\\star}(4)=\\displaystyle\\frac{\\varepsilon}{2}(B+1).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus $\\begin{array}{r}{\\|h^{\\star}\\|_{\\mathrm{span}}=\\frac{1}{2}\\!-\\!\\varepsilon/2\\!-\\!\\left(\\!-(B+1)\\frac{\\varepsilon}{2}\\right)=\\frac{1}{2}(B\\varepsilon\\!+\\!1)}\\end{array}$ If we set $\\begin{array}{r}{B=\\frac{2T}{\\varepsilon}\\!-\\!\\frac{1}{2}}\\end{array}$ , we have $\\Vert h^{\\star}\\Vert_{\\mathsf{s p a n}}=T$ Also note that the calculation for $h^{\\star}$ holds for any $\\varepsilon$ , so the optimal bias span of $\\mathbf{\\mathcal{M}}_{0}$ .s $[0,\\frac{1}{2},-\\frac{1}{2},0]^{\\top}$ \uff0c and thus $\\mathcal{M}_{0}$ has optimal bias span 1. ", "page_idx": 21}, {"type": "text", "text": "Finally, to distinguish between the two MDPs $\\mathcal{M}_{\\mathrm{0}}$ and $\\mathcal{M}_{1}$ , we must be able to determine the next-stedistributionofstaeI, that is,to distnguish between the two hypotheses $\\begin{array}{r}{Q_{1}=\\mathrm{Cat}(\\frac{1}{2},\\frac{1}{2})}\\end{array}$ and $\\begin{array}{r}{Q_{2}=\\mathsf{C a t}(\\frac{1}{2}+\\varepsilon,\\frac{1}{2}-\\varepsilon)}\\end{array}$ . Given $n$ i.i.d. bservations from the transition distribution of state 1, this is a binary hypothesis testing problem between the product distributions $Q_{1}^{n}$ and $Q_{2}^{n}$ . By Le ", "page_idx": 21}, {"type": "text", "text": "Cam's bound [24], the testing failure probability is lower bounded by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{2}\\left(1-\\Vert Q_{1}^{n}-Q_{2}^{n}\\Vert_{\\mathrm{TV}}\\right)\\ge\\frac{1}{2}\\left(1-\\sqrt{\\frac{1}{2}\\mathrm{D}_{\\mathrm{KL}}(Q_{1}^{n}|Q_{2}^{n})}\\right)}\\\\ {=\\frac{1}{2}\\left(1-\\sqrt{\\frac{n}{2}\\mathrm{D}_{\\mathrm{KL}}(Q_{1}|Q_{2})}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\|Q_{1}^{n}-Q_{2}^{n}\\|_{\\mathrm{TV}}$ and $\\mathsf{D}_{\\mathrm{KL}}\\big(Q_{1}^{n}|Q_{2}^{n}\\big)$ denote the total variation distance and Kullback-Leibler (KL) divergence between $Q_{1}^{n}$ and $Q_{2}^{n}$ , respectively, and the last two (in)equalities follow from Pinsker's inequality and tensorization of KL divergence. By direct calculation, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{{\\mathrm{D}}_{\\mathrm{KL}}(Q_{1}|Q_{2})={\\frac{1}{2}}\\log{\\frac{1}{1+2\\varepsilon}}+{\\frac{1}{2}}\\log{\\frac{1}{1-2\\varepsilon}}}\\\\ &{\\qquad\\qquad\\leq{\\frac{1}{2}}\\cdot{\\frac{-2\\varepsilon}{1+2\\varepsilon}}+{\\frac{1}{2}}\\cdot{\\frac{2\\varepsilon}{1-2\\varepsilon}}}\\\\ &{\\qquad\\qquad={\\frac{4\\varepsilon^{2}}{1-4\\varepsilon^{2}}}}\\\\ &{\\qquad\\qquad\\leq8\\varepsilon^{2}}\\end{array}}\\qquad\\qquad{\\mathrm{log}}(1+x)\\leq x,{\\forall x>-1}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combining the last two equations, we see that the testing failure probabilityisat least $\\textstyle{\\frac{1}{2}}\\left(1-{\\sqrt{4n\\varepsilon^{2}}}\\right)$ Thus, if we set $\\textstyle\\varepsilon={\\frac{1}{4{\\sqrt{n}}}}$ , the failure probability is at least $\\textstyle{\\frac{1}{4}}$ \u53e3 ", "page_idx": 22}, {"type": "text", "text": "B Proofs for general MDPs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we provide the proofs for our main results in Section 4 for general MDPs. Again, we can assume that $\\mathsf{H}+\\mathsf{B}$ is an integer, which only affects the sample complexity by a constant multiple $<2$ ", "page_idx": 22}, {"type": "text", "text": "First we develop more notation which will be useful in the setting of general MDPs. Recall we defined, for any policy $\\pi$ ,that $\\mathcal{R}^{\\pi}$ is the set of states which are recurrent in the Markov chain $P_{\\pi}$ and $\\mathcal{T}^{\\pi}=\\mathcal{S}\\setminus\\dot{\\mathcal{R}^{\\pi}}$ is the set of transient states. We now present a standard decomposition of Markov chains [14, Appendix A]. For any policy $\\pi$ , possibly after reordering states so that the recurrent states appear first (and are grouped into disjoint irreducible closed sets), we can decompose ", "page_idx": 22}, {"type": "equation", "text": "$$\nP_{\\pi}=\\left[{X_{\\pi}\\atop Y_{\\pi}}\\quad{0}\\atop{Z_{\\pi}}\\right]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "such that $X_{\\pi}$ are probabilities of transitions between states which are recurrent under $\\pi$ \uff0c $Y_{\\pi}$ are probabilities of transitions from $\\mathcal{T}^{\\pi}$ into $\\mathcal{R}^{\\pi}$ , and $Z_{\\pi}$ are probabilities of transitions between states within $\\mathcal{T}^{\\pi}$ . Furthermore, supposing there are $k$ irreducible closed blocks within $\\mathcal{R}^{\\pi}$ \uff0c $X_{\\pi}$ is blockdiagonal of the form ", "page_idx": 22}, {"type": "equation", "text": "$$\nX_{\\pi}=\\left[\\begin{array}{c c c c}{{X_{\\pi,1}}}&{{0}}&{{\\cdots}}&{{0}}\\\\ {{0}}&{{X_{\\pi,2}}}&{{\\cdots}}&{{0}}\\\\ {{\\vdots}}&{{\\vdots}}&{{\\ddots}}&{{\\vdots}}\\\\ {{0}}&{{0}}&{{\\cdots}}&{{X_{\\pi,k}}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The limiting matrix of the Markov chain induced by policy $\\pi$ is defined as the matrix ", "page_idx": 22}, {"type": "equation", "text": "$$\nP_{\\pi}^{\\infty}=\\underset{T\\rightarrow\\infty}{\\mathrm{C-lim}}\\,P_{\\pi}^{T}=\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}\\,\\frac{1}{T}\\sum_{t=0}^{T-1}P_{\\pi}^{t}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "$P_{\\pi}^{\\infty}$ is a stochastic matrix (all rows positive and sum to 1) since $\\boldsymbol{S}$ is finite. We alsohave $P_{\\pi}P_{\\pi}^{\\infty}=$ $P_{\\pi}^{\\infty}={P_{\\pi}^{\\infty}}P_{\\pi}$ .Additionally, $\\rho^{\\pi}=P_{\\pi}^{\\infty}r_{\\pi}$ . In terms of our decomposition, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\nP_{\\pi}^{\\infty}=\\left[\\!\\!\\begin{array}{l l}{X_{\\pi}^{\\infty}}&{0}\\\\ {Y_{\\pi}^{\\infty}}&{0}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ", "page_idx": 23}, {"type": "equation", "text": "$$\nX_{\\pi}^{\\infty}=\\left[\\begin{array}{c c c c}{X_{\\pi,1}^{\\infty}}&{0}&{\\cdots}&{0}\\\\ {0}&{X_{\\pi,2}^{\\infty}}&{\\cdots}&{0}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {0}&{0}&{\\cdots}&{X_{\\pi,k}^{\\infty}}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "each $X_{\\pi,i}^{\\infty}=\\mathbf{1}x_{\\pi,i}^{\\top}$ for some stochastic row vector $x_{\\pi,i}^{\\top}$ , and $Y_{\\pi}^{\\infty}=(I-Z_{\\pi})^{-1}Y_{\\pi}X_{\\pi}^{\\infty}$ . Also we have $\\textstyle(I-Z_{\\pi})^{-1}=\\sum_{t=0}^{\\infty}Z_{\\pi}^{t}$ and $\\textstyle\\sum_{t=0}^{\\infty}Z_{\\pi}^{t}Y_{\\pi}=(I-Z_{\\pi})^{-1}Y_{\\pi}$ has sochastic rows each row isa probability distribution, that is all entries are positive and sum to 1). ", "page_idx": 23}, {"type": "text", "text": "With the same arrangement of states as within the above decomposition of $P_{\\pi}$ (10), let ", "page_idx": 23}, {"type": "equation", "text": "$$\nV_{\\gamma}^{\\pi}=\\left[\\stackrel{\\overline{{V}}_{\\gamma}^{\\pi}}{V_{\\gamma}^{\\pi}}\\right]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "decompose $V_{\\gamma}^{\\pi}$ into recurrent and transient states, and generally we use this same notation for any vector $x\\in\\mathbb{R}^{s}$ : we let $\\overline{{x}}$ list the values of $x_{s}$ for recurrent $x\\in{\\mathcal{R}}^{\\pi}$ \uff0c $\\underline{{x}}$ contain $x_{s}$ for $s\\in\\mathcal{T}^{\\pi}$ , and we assume the entire $x$ has been rearranged so that $x=[\\overline{{x}}\\,\\underline{{x}}]^{\\top}$ . Note that the rearrangement of states depends on the policy $\\pi$ so this notation has potential for confusion if applied to objects relating to multiple policies at once, but the policy determining the rearrangement will always be clear from context in our arguments. ", "page_idx": 23}, {"type": "text", "text": "The main reason we decompose $P_{\\pi}$ into recurrent and transient states is the following key observation. Lemma 17. For any policy $\\pi$ $i f\\,s,s^{\\prime}$ are in the same recurrent block of the Markov chain with transition matrix $P_{\\pi}$ ,then $\\rho^{\\star}(s)=\\rho^{\\star}(s^{\\prime})$ ", "page_idx": 23}, {"type": "text", "text": "Proof. Define the history-dependent policy $\\tilde{\\pi}$ which follows $\\pi$ until its history first contains $s^{\\prime}$ , after which point it follows $\\pi^{\\star}$ . Since $\\rho^{\\star}(s)$ is the optimal gain achievable starting at $s$ by following any history-dependent policy [14], we have $\\begin{array}{r}{\\rho^{\\star}(s)\\,\\geq\\,\\rho^{\\tilde{\\pi}}(s)\\,:=\\,\\operatorname*{lim}_{T\\rightarrow\\infty}\\frac{1}{T}\\mathbb{E}_{s}^{\\tilde{\\pi}}\\sum_{t=0}^{T-1}R_{t}}\\end{array}$ (where $\\mathbb{E}_{s}^{\\tilde{\\pi}}$ is defined in the natural way from the distribution over trajectories $(S_{0},A_{0},\\ldots)$ where $A_{t}\\,\\sim$ $\\tilde{\\pi}\\big(S_{0},A_{0},\\ldots,S_{t}\\big)$ and $S_{t+1}\\sim\\dot{P}(\\cdot\\mid S_{t},A_{t}))$ . Let $T_{s^{\\prime}}=\\operatorname*{inf}\\{t\\stackrel{}{\\geq}1:S_{t}=s^{\\prime}\\}$ be the hitting time of state $s^{\\prime}$ and let ${\\mathcal{F}}_{T_{s^{\\prime}}}$ be the stopped $\\sigma$ algebra (with respect to the fltration where for all nonnegative integers $t$ $\\mathcal{F}_{t}$ is the $\\sigma$ -algebra generated by $S_{0},A_{0},...,S_{t},A_{t})$ . Then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{1}{T}\\mathbb{E}_{s}^{\\frac{T-1}{\\delta}}R_{t}=\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{1}{T}\\mathbb{E}_{s}^{\\frac{T}{\\delta}}\\left[\\mathbb{E}_{s}^{s}\\left[\\frac{T^{-1}}{\\sum R_{t}}R_{t}\\right]\\mathcal{F}_{T,r}\\right]\\Bigg]}\\\\ &{=\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{1}{T}\\mathbb{E}_{s}^{\\frac{T}{\\delta}}\\left[\\underset{t=0}{\\overset{T}{\\sum}}\\underset{R_{t}+1}{\\overset{T}{\\sum}}\\left[\\underset{t=T,r}{\\sum}R_{t}\\bigg|\\mathcal{F}_{T,r}\\right]\\right]}\\\\ &{=\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{1}{T}\\mathbb{E}_{s}^{\\frac{T}{\\delta}}\\left[\\underset{t=0}{\\overset{T}{\\sum}}R_{t}+g(T,T_{s})\\right]}\\\\ &{=\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{1}{T}\\mathbb{E}_{s}^{\\frac{T}{\\delta}}\\left[\\underset{t=0}{\\overset{T}{\\sum}}R_{t}+g(T,T_{s})\\right]}\\\\ &{\\geq\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{1}{T}\\mathbb{E}_{s}^{\\frac{T}{\\delta}}\\left[\\underset{t=0}{\\overset{T}{\\sum}}R_{t}+g(T,T_{s})\\right]}\\\\ &{\\geq\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{1}{T}\\mathbb{E}_{s}^{\\frac{T}{\\delta}}\\left[\\mathcal{G}(T,T_{s})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $g(T,k)\\;:=\\;\\mathbb{E}_{s^{\\prime}}^{\\pi^{\\star}}\\left[\\sum_{t=0}^{T-k-1}R_{t}\\right]$ and we used the tower property. $\\mathcal{F}_{T_{s^{\\prime}}}$ -measurabity of $\\sum_{t=0}^{T_{s^{\\prime}}-1}R_{t}$ $\\tilde{\\pi}$ tf ${\\cal T}_{s^{\\prime}}\\,<\\,\\infty$ $s$ $s^{\\prime}$ $\\{T_{s^{\\prime}}=k\\}$ number $k$ , we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\frac{1}{T}g(T,k)=\\operatorname*{lim}_{T\\to\\infty}\\frac{1}{T}\\mathbb{E}_{s^{\\prime}}^{\\pi^{\\star}}\\left[\\sum_{t=0}^{T-k-1}R_{t}\\right]=\\rho^{\\star}(s^{\\prime})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "because we can bound ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\mathbb{E}_{s^{\\prime}}^{\\pi^{\\star}}\\left[\\sum_{t=0}^{T-1}R_{t}\\right]-\\frac{k}{T}\\leq\\frac{1}{T}\\mathbb{E}_{s^{\\prime}}^{\\pi^{\\star}}\\left[\\sum_{t=0}^{T-k-1}R_{t}\\right]\\leq\\frac{1}{T}\\mathbb{E}_{s^{\\prime}}^{\\pi^{\\star}}\\left[\\sum_{t=0}^{T-1}R_{t}\\right]\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and bothside converge to $\\rho^{\\star}\\big(s^{\\prime}\\big)$ Therefore $\\frac{g(T,T_{s^{\\prime}})}{T}$ converge almot srly t theconstant $\\rho^{\\star}\\!\\left(s^{\\prime}\\right)$ and also this random variable is bounded by 1, so by the dominated convergence theorem we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\frac{1}{T}\\mathbb{E}_{s}^{\\pi}\\left[g(T,T_{s^{\\prime}})\\right]=\\mathbb{E}_{s}^{\\pi}\\left[\\operatorname*{lim}_{T\\to\\infty}\\frac{1}{T}g(T,T_{s^{\\prime}})\\right]=\\rho^{\\star}(s^{\\prime}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus we have shown that $\\rho^{\\star}(s)\\geq\\rho^{\\star}(s^{\\prime})$ . Since $s$ and $s^{\\prime}$ were arbitrary states in the same recurrent block we also have $\\rho^{\\star}(s^{\\prime})\\geq\\rho^{\\star}(s)$ , and thus $\\rho^{\\star}(s)=\\rho^{\\star}(s^{\\prime})$ as desired. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Lemma 18. For any state s which is transient under a policy $\\pi$ if the MDP satisfies the bounded transienttimeassumptionwithparameter $\\textsf{B}$ wehave ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{t=0}^{\\infty}e_{s}^{\\top}Z_{\\pi}^{t}\\right\\|_{1}\\leq\\mathsf{B}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Let $T=\\operatorname*{inf}\\{t:S_{t}\\in\\mathcal{R}^{\\pi}\\}$ . Notice that $\\left\\lVert e_{s}^{\\top}Z_{\\pi}^{t}\\right\\rVert_{1}=\\mathbb{P}_{s}^{\\pi}(T>t)$ . Therefore, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left\\|\\sum_{t=0}^{\\infty}e_{s}^{\\top}Z_{\\pi}^{t}\\right\\|_{1}\\leq\\sum_{t=0}^{\\infty}\\left\\|e_{s}^{\\top}Z_{\\pi}^{t}\\right\\|_{1}}}\\\\ &{}&{=\\displaystyle\\sum_{t=0}^{\\infty}\\mathbb{P}_{s}^{\\pi}(T>t)}\\\\ &{}&{=\\mathbb{E}_{s}^{\\pi}\\left[T\\right]}\\\\ &{}&{\\leq\\mathrm{B},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we used a well-known formula for the expectation of nonnegative-integer-valued random variables, and the bounded transient time assumption. ", "page_idx": 24}, {"type": "text", "text": "Lemma 19. Let s be a transient state under $P_{\\pi}$ .Then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{e_{s}^{\\top}(I-\\gamma P_{\\pi})^{-1}=\\left[e_{s}^{\\top}\\sum_{k=1}^{\\infty}\\gamma^{k}Z_{\\pi}^{k-1}Y_{\\pi}(I-\\gamma X_{\\pi})^{-1}\\right.\\ \\left.\\underline{{e_{s}}}^{\\top}\\sum_{t=0}^{\\infty}\\gamma^{t}Z_{\\pi}^{t}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Using the decomposition of $P_{\\pi}$ , we can calculate for any integer $t\\geq1$ that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P_{\\pi}^{t}=\\left[{\\underset{\\sum_{k=1}^{t}}{X_{\\pi}^{t}}}{X_{\\pi}^{t}}\\right.}\\\\ {P_{\\pi}^{t}=\\left[{\\sum_{k=1}^{t}{Z_{\\pi}^{k-1}Y_{\\pi}}X_{\\pi}^{t-k}}\\quad\\left.Z_{\\pi}^{t}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{e_{s}^{\\top}(I-\\gamma P_{\\pi})^{-1}=e_{s}^{\\top}\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}P_{\\pi}^{t}}\\\\ &{\\qquad\\qquad\\qquad=\\left[e_{s}^{\\top}\\sum_{t=0}^{\\infty}\\gamma^{t}\\sum_{k=1}^{t}Z_{\\pi}^{k-1}Y_{\\pi}X_{\\pi}^{t-k}\\right.\\ \\left.\\underbrace{e_{s}^{\\top}\\sum_{t=0}^{\\infty}\\gamma^{t}Z_{\\pi}^{t}}_{=\\mathrm{~}\\left[e_{s}^{\\top}\\right.\\sum_{k=1}^{\\infty}\\gamma^{t}Z_{\\pi}^{k-1}Y_{\\pi}\\sum_{t=0}^{k}\\gamma^{t-k}\\right.}\\ \\underbrace{e_{s}^{\\top}}_{=\\mathrm{~}\\left[e_{s}^{\\top}\\right.\\sum_{t=0}^{\\infty}\\gamma^{t}Z_{\\pi}^{t}}\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\left.\\left[e_{s}^{\\top}\\sum_{k=1}^{\\infty}\\gamma^{k}Z_{\\pi}^{k-1}Y_{\\pi}\\sum_{t=k}^{\\infty}\\gamma^{t-k}X_{\\pi}^{t-k}\\right.\\underbrace{e_{s}^{\\top}\\sum_{t=0}^{\\infty}\\gamma^{t}Z_{\\pi}^{t}}_{=\\mathrm{~}\\left[e_{s}^{\\top}\\right]^{2}\\sum_{t=0}^{\\infty}\\gamma^{t}Z_{\\pi}^{t}}\\right]}\\\\ &{\\qquad\\qquad=\\left.\\left[e_{s}^{\\top}\\sum_{k=1}^{\\infty}\\gamma^{k}Z_{\\pi}^{k-1}Y_{\\pi}(I-\\gamma X_{\\pi})^{-1}\\right.\\right.\\ \\left.e_{s}^{\\top}\\right\\lbrack\\sum_{t=0}^{\\infty}\\gamma^{t}Z_{\\pi}^{t}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that we are able to rearrange the order of the summation in the third equality because all summands are (elementwise) positive. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "B.1 Proof of Theorem 6 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Theorem 6, our result which helps reduce general average reward MDPs to discounted MDPs, is proven as a straightforward consequence of the following sequence of lemmas, some of which will also be needed for the proof of our discounted MDP sample complexity bound Theorem 7. ", "page_idx": 25}, {"type": "text", "text": "Lemma 20. We have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\|V_{\\gamma}^{\\pi^{\\star}}-\\frac{1}{1-\\gamma}\\rho^{\\star}\\right\\|_{\\infty}\\leq\\|h^{\\star}\\|_{\\mathrm{span}}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. We begin by observing that $\\pi^{\\star}$ satisfies ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\rho^{\\star}+h^{\\star}=r_{\\pi^{\\star}}+P_{\\pi^{\\star}}h^{\\star}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{V_{\\gamma}^{\\pi^{\\star}}=(I-\\gamma P_{\\pi^{\\star}})^{-1}r_{\\pi^{\\star}}}}\\\\ &{\\qquad=(I-\\gamma P_{\\pi^{\\star}})^{-1}\\left(\\rho^{\\star}+h^{\\star}-P_{\\pi^{\\star}}h^{\\star}\\right)}\\\\ &{\\qquad=(I-\\gamma P_{\\pi^{\\star}})^{-1}\\rho^{\\star}+(I-\\gamma P_{\\pi^{\\star}})^{-1}\\left(I-P_{\\pi^{\\star}}\\right)h^{\\star}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $P_{\\pi^{\\star}}\\rho^{\\star}=\\rho^{\\star}$ , we can calculate that ", "page_idx": 25}, {"type": "equation", "text": "$$\n(I-\\gamma P_{\\pi^{\\star}})^{-1}\\rho^{\\star}=\\sum_{t\\geq0}\\gamma^{t}P_{\\pi^{\\star}}^{t}\\rho^{\\star}=\\sum_{t\\geq0}\\gamma^{t}\\rho^{\\star}={\\frac{1}{1-\\gamma}}\\rho^{\\star}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It also holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(I-\\gamma P_{\\pi^{\\star}}\\right)^{-1}(I-P_{\\pi^{\\star}})=\\displaystyle\\sum_{t\\geq0}\\gamma^{t}P_{\\pi^{\\star}}^{t}(I-P_{\\pi^{\\star}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{t\\geq0}\\gamma^{t}P_{\\pi^{\\star}}^{t}-\\displaystyle\\sum_{t\\geq0}\\gamma^{t}P_{\\pi^{\\star}}^{t+1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=P_{\\pi^{\\star}}+\\displaystyle\\sum_{t\\geq0}(\\gamma^{t+1}-\\gamma^{t})P_{\\pi^{\\star}}^{t+1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and $\\begin{array}{r}{\\sum_{t\\geq0}\\gamma^{t+1}-\\gamma^{t}=\\left(\\gamma-1\\right)\\sum_{t\\geq0}\\gamma^{t}=-1}\\end{array}$ Therefore (12) is the difference of two stochastic matrices, and so it follows that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\|(I-\\gamma P_{\\pi^{\\star}})^{-1}\\left(I-P_{\\pi^{\\star}}\\right)h^{\\star}\\right\\|_{\\infty}\\leq\\left\\|h^{\\star}\\right\\|_{\\mathrm{span}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma 21. If $\\pi_{\\gamma}^{\\star}$ is optimal for the discounted MDP $(P,r,\\gamma)$ and $s$ is recurrent under $\\pi_{\\gamma}^{\\star}$ then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}(s)-\\frac{1}{1-\\gamma}\\rho^{\\star}(s)\\right|\\leq\\|h^{\\star}\\|_{\\mathrm{span}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}(s)-\\frac{1}{1-\\gamma}\\rho^{\\pi_{\\gamma}^{\\star}}(s)\\right|\\leq2\\left\\|h^{\\star}\\right\\|_{\\mathrm{span}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "These facts can be written as $\\begin{array}{r}{\\left\\|\\overline{{V_{\\gamma}^{\\pi_{\\gamma}^{*}}}}-\\frac{1}{1-\\gamma}\\overline{{\\rho^{*}}}\\right\\|_{\\infty}\\leq\\|h^{\\star}\\|_{\\mathrm{span}}\\,a n d\\left\\|\\overline{{V_{\\gamma}^{\\pi_{\\gamma}^{*}}}}-\\frac{1}{1-\\gamma}\\overline{{\\rho^{\\pi_{\\gamma}^{\\star}}}}\\right\\|_{\\infty}\\leq2\\left\\|h^{\\star}\\right\\|_{\\mathrm{span}}}\\end{array}$ respectively. ", "page_idx": 25}, {"type": "text", "text": "Proof. First note that if $s$ is recurrent for the Markov chain $P_{\\pi_{\\gamma}^{\\star}}$ , then all states in the support of $e_{s}^{\\top}P_{\\pi_{\\gamma}^{\\star}}$ arein the same recurrent block as state $s$ and $\\rho^{\\star}$ is constant and equal to $\\rho^{\\star}(s)$ within this recurrent block by Lemma 17. The (unmodified) Bellman equation states that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\rho^{\\star}(s)+h^{\\star}(s)=\\operatorname*{max}_{a:P_{s a}\\rho^{\\star}=\\rho^{\\star}(s)}r_{s a}+P_{s a}h^{\\star}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since we established that $e_{s}^{\\top}P_{\\pi_{\\gamma}^{\\star}}\\rho^{\\star}\\;=\\;\\rho^{\\star}(s)$ , all actions $a$ in the support of $\\pi_{\\gamma}^{\\star}(a\\;\\;|\\;\\;s)$ satisfy $P_{s a}\\rho^{\\star}=\\rho^{\\star}(s)$ , and therefore ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho^{\\star}(s)+h^{\\star}(s)=\\underset{a:P_{s a}\\rho^{\\star}=\\rho^{\\star}(s)}{\\operatorname*{max}}\\,r_{s a}+P_{s a}h^{\\star}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\displaystyle\\sum_{a\\in\\mathcal{A}}\\pi_{\\gamma}^{\\star}(a\\mid s)\\left(r_{s a}+P_{s a}h^{\\star}\\right)}\\\\ &{\\qquad\\qquad\\qquad=e_{s}^{\\top}\\left(r_{\\pi_{\\gamma}^{\\star}}+P_{\\pi_{\\gamma}^{\\star}}h^{\\star}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since this holds for all $s\\in\\mathcal{R}^{\\pi_{\\gamma}^{\\star}}$ , we can rearrange to obtain that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{r_{\\pi_{\\gamma}^{\\star}}}}\\le\\overline{{\\rho^{\\star}}}+\\overline{{h^{\\star}}}-\\overline{{P_{\\pi_{\\gamma}^{\\star}}h^{\\star}}}=\\overline{{\\rho^{\\star}}}+\\overline{{h^{\\star}}}-X_{\\pi_{\\gamma}^{\\star}}\\overline{{h^{\\star}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Now we can follow an argument which is similar to that of [23, Lemma 2]. We have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}=\\overline{{(I-\\gamma P_{\\pi_{\\gamma}^{\\star}})^{-1}r_{\\pi_{\\gamma}^{\\star}}}}}\\\\ &{\\qquad=(I-X_{\\pi_{\\gamma}^{\\star}})^{-1}\\overline{{r_{\\pi_{\\gamma}^{\\star}}}}}\\\\ &{\\qquad\\leq(I-X_{\\pi_{\\gamma}^{\\star}})^{-1}\\left(\\overline{{\\rho^{\\star}}}+\\overline{{h^{\\star}}}-X_{\\pi_{\\gamma}^{\\star}}\\overline{{h^{\\star}}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "using monotonicity of $(I-X_{\\pi_{\\gamma}^{\\star}})^{-1}$ in the final inequality. Due to the observation above that for all $s\\in\\mathcal{R}^{\\pi_{\\gamma}^{\\star}}$ , all actions $a$ in the support of $\\pi_{\\gamma}^{\\star}(a\\mid s)$ satisfy $P_{s a}\\rho^{\\star}=\\rho^{\\star}(s)$ , we have $X_{\\pi_{\\gamma}^{\\star}}\\overline{{\\rho^{\\star}}}=\\overline{{\\rho^{\\star}}}$ Therefore we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n(I-X_{\\pi_{\\gamma}^{\\star}})^{-1}\\overline{{\\rho^{\\star}}}=\\sum_{t=0}^{\\infty}\\gamma^{t}X_{\\pi_{\\gamma}^{\\star}}\\overline{{\\rho^{\\star}}}=\\sum_{t=0}^{\\infty}\\gamma^{t}\\overline{{\\rho^{\\star}}}=\\frac{1}{1-\\gamma}\\overline{{\\rho^{\\star}}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For the second term, by using an argument which is completely analogous to that used in Lemma 20 wehave $\\left\\|(I-X_{\\pi_{\\gamma}^{\\star}})^{-1}\\left(\\overline{{h^{\\star}}}-X_{\\pi_{\\gamma}^{\\star}}\\overline{{h^{\\star}}}\\right)\\right\\|_{\\infty}\\leq\\|h^{\\star}\\|_{\\mathrm{span}}$ . Combining these steps we obtain that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\overline{{V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}}}-\\frac{1}{1-\\gamma}\\overline{{\\rho^{\\star}}}\\leq\\|h^{\\star}\\|_{\\mathrm{span}}\\,{\\bf1}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "To obtain a lower bound, we can combine the optimality of $\\pi_{\\gamma}^{\\star}$ for the $\\gamma.$ -discounted problem with Lemma 20 to obtain the bound ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\overline{{V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}}}-\\frac{1}{1-\\gamma}\\overline{{\\rho^{\\star}}}\\geq\\overline{{V_{\\gamma}^{\\pi^{\\star}}}}-\\frac{1}{1-\\gamma}\\overline{{\\rho^{\\star}}}\\geq\\|h^{\\star}\\|_{\\mathrm{span}}\\,\\mathbf{1}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore we can conclude that $\\begin{array}{r}{\\left\\|\\overline{{V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}}}-\\frac{1}{1-\\gamma}\\overline{{\\rho^{\\star}}}\\right\\|_{\\infty}\\leq\\|h^{\\star}\\|_{\\mathrm{span}}.}\\end{array}$ ", "page_idx": 26}, {"type": "text", "text": "For the second bound in the lemma statement, we first note that, as observed in [20], ", "page_idx": 26}, {"type": "equation", "text": "$$\nP_{\\pi_{\\gamma}^{\\star}}^{\\infty}V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}=P_{\\pi_{\\gamma}^{\\star}}^{\\infty}\\sum_{t=0}^{\\infty}\\gamma^{t}P_{\\pi_{\\gamma}^{\\star}}^{t}r_{\\pi_{\\gamma}^{\\star}}=\\sum_{t=0}^{\\infty}\\gamma^{t}P_{\\pi_{\\gamma}^{\\star}}^{\\infty}r_{\\pi_{\\gamma}^{\\star}}=\\frac{1}{1-\\gamma}\\rho^{\\pi_{\\gamma}^{\\star}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Also, as discussed previously, if $s\\in\\mathcal{R}^{\\pi_{\\gamma}^{\\star}}$ then $e_{s}^{\\top}P_{\\pi_{\\gamma}^{\\star}}\\rho^{\\star}=\\rho^{\\star}(s)$ so then we also have $e_{s}^{\\top}P_{\\pi_{\\gamma}^{\\star}}\\rho^{\\star}=$ $\\rho^{\\star}(s)$ (whichcaedirelyf thednnof ten x $P_{\\pi_{\\gamma}^{\\star}}^{\\infty}$ .Equivalenly, $e_{s}^{\\top}(I-P_{\\pi_{\\gamma}^{\\star}}^{\\infty})\\rho^{\\star}=0$ . Using both of these two observations, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}(s)-\\displaystyle\\frac{1}{1-\\gamma}\\rho^{\\pi_{\\gamma}^{\\star}}(s)=e_{s}^{\\top}(I-P_{\\pi_{\\gamma}^{\\star}}^{\\infty})V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=e_{s}^{\\top}(I-P_{\\pi_{\\gamma}^{\\star}}^{\\infty})(V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}-\\displaystyle\\frac{1}{1-\\gamma}\\rho^{\\star})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\overline{{e_{s}}}^{\\top}(I-X_{\\pi_{\\gamma}^{\\star}}^{\\infty})(\\overline{{V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}}}-\\displaystyle\\frac{1}{1-\\gamma}\\overline{{\\rho^{\\star}}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, we obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|\\overline{{V_{\\gamma}^{\\pi_{\\gamma}^{*}}}}-\\frac{1}{1-\\gamma}\\overline{{\\rho^{\\pi_{\\gamma}^{*}}}}\\right\\|_{\\infty}\\leq\\left\\|(I-X_{\\pi_{\\gamma}^{*}}^{\\infty})(\\overline{{V_{\\gamma}^{\\pi_{\\gamma}^{*}}}}-\\frac{1}{1-\\gamma}\\overline{{\\rho^{\\star}}})\\right\\|_{\\infty}}&{}\\\\ {\\leq\\left\\|\\overline{{V_{\\gamma}^{\\pi_{\\gamma}^{*}}}}-\\frac{1}{1-\\gamma}\\overline{{\\rho^{\\star}}}\\right\\|_{\\mathrm{span}}}&{}\\\\ {\\leq2\\left\\|\\overline{{V_{\\gamma}^{\\pi_{\\gamma}^{*}}}}-\\frac{1}{1-\\gamma}\\overline{{\\rho^{\\star}}}\\right\\|_{\\infty}}&{}\\\\ {\\leq2\\left\\|h^{\\star}\\right\\|_{\\mathrm{span}}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "using the first bound from the lemma statement in the final inequality. ", "page_idx": 27}, {"type": "text", "text": "Lemma 22. We have ", "text_level": 1, "page_idx": 27}, {"type": "equation", "text": "$$\n\\left\\|V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}-\\frac{1}{1-\\gamma}\\rho^{\\star}\\right\\|_{\\infty}\\leq\\mathsf{B}+\\|h^{\\star}\\|_{\\mathrm{span}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left\\|V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}-\\frac{1}{1-\\gamma}\\rho^{\\pi_{\\gamma}^{\\star}}\\right\\|_{\\infty}\\leq\\mathsf{B}+2\\left\\|h^{\\star}\\right\\|_{\\mathrm{span}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Note that by combining with Lemma 21, it suffices to prove for any transient state $s\\in\\tau^{\\pi_{\\gamma}^{\\star}}$ that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left|V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}(s)-\\frac{1}{1-\\gamma}\\rho^{\\star}(s)\\right|\\leq\\mathsf{B}+\\|h^{\\star}\\|_{\\mathrm{span}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left|V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}(s)-\\frac{1}{1-\\gamma}\\rho^{\\pi_{\\gamma}^{\\star}}(s)\\right|\\le\\mathsf{B}+2\\left\\|h^{\\star}\\right\\|_{\\mathrm{span}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Let $s$ be transient under $\\pi_{\\gamma}^{\\star}$ . Then starting by using Lemma 19, we can calculate ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{\\gamma}^{\\pi_{*}^{*}}(s)=e_{s}^{\\top}(I-\\gamma P_{\\pi_{\\gamma}^{*}})^{-1}r_{\\pi_{\\gamma}^{*}}}\\\\ &{\\qquad\\quad=\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\underline{{e}}_{s}^{\\top}Z_{\\pi_{\\gamma}^{*}}^{t}\\underline{{r}}_{\\underline{{\\pi}}_{\\gamma}^{*}}+\\gamma\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\underline{{e}}_{s}^{\\top}Z_{\\pi_{\\gamma}^{*}}^{t}Y_{\\pi_{\\gamma}^{*}}(I-\\gamma X_{\\pi_{\\gamma}^{*}})^{-1}\\overline{{r}}_{\\pi_{\\gamma}^{*}}}\\\\ &{\\qquad=\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\underline{{e}}_{s}^{\\top}Z_{\\pi_{\\gamma}^{*}}^{t}\\underline{{r}}_{\\underline{{\\pi}}_{\\gamma}^{*}}+\\gamma\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\underline{{e}}_{s}^{\\top}Z_{\\pi_{\\gamma}^{*}}^{t}Y_{\\pi_{\\gamma}^{*}}\\overline{{V_{\\gamma}^{\\pi_{\\gamma}^{*}}}}}\\\\ &{\\qquad\\le\\displaystyle\\sum_{t=0}^{\\infty}\\underline{{e}}_{s}^{\\top}Z_{\\pi_{\\gamma}^{*}}^{t}\\underline{{r}}_{\\pi_{\\gamma}^{*}}+\\left(\\displaystyle\\sum_{t=0}^{\\infty}\\underline{{e}}_{s}^{\\top}Z_{\\pi_{\\gamma}^{*}}^{t}Y_{\\pi_{\\gamma}^{*}}\\right)\\overline{{V_{\\gamma}^{\\pi_{\\gamma}^{*}}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By Lemma 18 we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{\\infty}\\underline{{e_{s}}}^{\\top}Z_{\\pi_{\\gamma}^{\\star}}^{t}\\underline{{r}}_{\\pi_{\\gamma}^{\\star}}\\leq\\left\\|\\sum_{t=0}^{\\infty}\\underline{{e_{s}}}^{\\top}Z_{\\pi_{\\gamma}^{\\star}}^{t}\\right\\|_{1}\\left\\|\\underline{{r_{\\pi_{\\gamma}^{\\star}}}}\\right\\|_{\\infty}\\leq\\mathsf{B}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Now we can obtain the two bounds in the lemma statement by bounding the second term of (13) in two different ways. For the first bound in the lemma statement, we can use the first bound in Lemma ", "page_idx": 27}, {"type": "text", "text": "21 to calculate that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=m}^{1}\\sum_{c,s=1\\atop b\\geq2}^{n}\\sum_{t=1}^{r_{t}}\\frac{1}{r_{s,t}}\\left[\\frac{\\sum_{c^{\\prime}=2}^{b}C_{2,s^{\\prime}}^{t}\\gamma_{\\tau,s^{\\prime}}}{\\left(\\sum_{s=0}^{b}\\sum_{c^{\\prime}=2}^{b}C_{2,s^{\\prime}}^{t}\\gamma_{\\tau,s^{\\prime}}\\right)\\frac{1}{1-\\gamma^{\\prime}}\\rho^{\\prime}}+\\left(\\displaystyle\\sum_{c^{\\prime}=2}^{b}C_{2,s^{\\prime}}^{t}\\gamma_{\\tau,s^{\\prime}}\\right)\\right]\\left\\vert\\Gamma_{{\\gamma}^{\\prime}}^{\\prime\\prime}-\\frac{1}{1-\\gamma^{\\prime}}\\rho^{\\prime}\\right\\vert_{\\infty}^{2}}&{{}}\\\\ {\\displaystyle=\\left(\\displaystyle\\sum_{s=0}^{b}C_{2,s^{\\prime}}^{t}\\gamma_{\\tau,s^{\\prime}}\\right)\\frac{1}{1-\\gamma^{\\prime}}\\rho^{\\prime}+\\left\\vert\\Gamma_{{\\gamma}^{\\prime}}^{\\prime\\prime}-\\frac{1}{1-\\gamma^{\\prime}}\\rho^{\\prime}\\right\\vert_{\\infty}}&{{}}\\\\ {\\displaystyle\\leq\\left(\\displaystyle\\sum_{s=0}^{b}C_{2,s^{\\prime}}^{t}\\gamma_{\\tau,s^{\\prime}}\\right)\\frac{1}{1-\\gamma^{\\prime}}\\rho^{\\prime}+\\left\\vert\\Gamma^{\\prime}\\right\\vert_{{\\infty}}}&{{}}\\\\ {\\displaystyle=\\left(\\displaystyle\\sum_{s=0}^{b}C_{2,s^{\\prime}}^{t}\\gamma_{\\tau,s^{\\prime}}\\right)\\frac{1}{1-\\gamma^{\\prime}}\\rho^{\\prime}+\\left\\vert\\Gamma^{\\prime}\\right\\vert_{{\\infty}}}&{{}}\\\\ {\\displaystyle=\\left(\\displaystyle\\sum_{s=0}^{b}C_{2,s^{\\prime}}^{t}\\gamma_{\\tau,s^{\\prime}}\\gamma_{\\tau,s^{\\prime}}\\right)\\frac{1}{1-\\gamma^{\\prime}}\\rho^{\\prime}+\\left\\vert\\Gamma^{\\prime}\\right\\vert_{{\\infty}}}&{{}}\\\\ {\\displaystyle=\\left(\\displaystyle\\sum_{s=0}^{b}\\sum_{c^{\\prime}=2}^{b}\\gamma_{\\tau,s^{\\prime}}\\right)\\frac{1}{b}-\\left\\vert\\Gamma^{ \n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we used the fact that $X_{\\pi_{\\gamma}^{\\star}}^{\\infty}\\overline{{\\rho^{\\star}}}=\\overline{{\\rho^{\\star}}}$ and then that $e_{s}^{\\top}P_{\\pi_{\\gamma}^{\\star}}^{\\infty}\\rho^{\\star}\\le\\rho^{\\star}(s)$ This gives an upper bound of ", "page_idx": 28}, {"type": "equation", "text": "$$\nV_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\leq\\frac{1}{1-\\gamma}\\rho^{\\star}(s)+\\mathsf{B}+\\|h^{\\star}\\|_{\\mathrm{span}}\\,.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Combining with the lower bound ", "page_idx": 28}, {"type": "equation", "text": "$$\nV_{\\gamma}^{\\pi_{\\gamma}^{\\star}}(s)\\geq V_{\\gamma}^{\\pi^{\\star}}(s)\\geq\\frac{1}{1-\\gamma}\\rho^{\\star}(s)-\\|h^{\\star}\\|_{\\mathrm{span}}\\,,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "we obtain that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\|V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}-\\frac{1}{1-\\gamma}\\rho^{\\star}\\right\\|_{\\infty}\\leq\\mathsf{B}+\\|h^{\\star}\\|_{\\mathrm{span}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which is the first bound in the lemma statement. ", "page_idx": 28}, {"type": "text", "text": "To obtain the second bound in the lemma statement, using the second bound from Lemma 21, we can calculate for the second term in (13) that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i=0}^{N}\\mathbb{E}_{x}^{\\top}\\mathbb{E}_{x}^{\\top}\\mathbb{E}_{y}\\Bigg\\rvert_{*}^{*}\\Bigg\\lvert\\nabla_{x}^{2}\\times\\Bigg(\\displaystyle\\sum_{s=0}^{N}\\mathbb{E}_{x}^{\\top}\\mathbb{E}_{x}^{s}\\Bigg)\\frac{1}{s}\\frac{\\overline{{\\eta}}^{*}}{1-\\overline{{\\eta}}^{*}}+\\displaystyle\\left(\\displaystyle\\sum_{s=0}^{N}\\mathbb{E}_{x}^{\\top}\\mathbb{E}_{x}^{s}\\mathbb{E}_{y}^{s}\\mathbb{E}_{x}^{s}\\Bigg)\\left\\lVert\\overline{{\\nu}}_{x}^{\\top}\\right\\rVert_{*}^{*}\\Bigg-\\displaystyle\\frac{1}{1-\\overline{{\\eta}}^{*}}\\overline{{\\eta}}^{*}}\\\\ &{\\qquad\\qquad\\qquad\\quad\\times\\left(\\displaystyle\\sum_{s=0}^{N}\\mathbb{E}_{x}^{\\top}\\mathbb{E}_{x}^{s}\\mathbb{E}_{y}^{s}\\right)\\frac{1}{s}\\frac{\\overline{{\\eta}}^{*}}{1-\\overline{{\\eta}}^{*}}\\overline{{\\eta}}^{*}+\\left\\lVert\\overline{{\\nu}}_{x}^{\\top}\\right\\rVert_{*}^{*}\\frac{1}{1-\\overline{{\\eta}}^{*}}\\overline{{\\eta}}^{*}\\Bigg\\rVert_{*}^{2}}\\\\ {\\displaystyle}\\\\ &{\\qquad\\quad\\leq\\left(\\displaystyle\\sum_{s\\leq0}^{N}\\mathbb{E}_{x}^{\\top}\\mathbb{E}_{x}^{s}\\mathbb{E}_{x}^{s}\\right)\\frac{1}{1-\\overline{{\\eta}}^{*}}\\overline{{\\eta}}^{*}+2\\left\\lVert\\mathbf{k}^{\\top}\\right\\rVert_{*}}\\\\ &{\\qquad\\quad=\\left(\\displaystyle\\sum_{s\\leq0}^{N}\\mathbb{E}_{x}^{\\top}\\mathbb{E}_{x}^{s}\\mathbb{E}_{x}^{s}\\right)\\frac{1}{1-\\overline{{\\eta}}^{*}}\\overline{{\\eta}}^{*}\\mathbb{E}_{x}^{s}\\mathbb{E}_{y}^{s}+2\\left\\lVert\\mathbf{k}^{\\top}\\right\\rVert_{*}}\\\\ &{\\qquad\\quad=\\left(\\displaystyle\\sum_{s\\leq0}^{N}\\mathbb{E}_{x}^{s}\\mathbb{E}_{x}^{s}\\mathbb{E}_{x}^{s}\\right)\\frac{1}{1-\\\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "$\\left(\\sum_{t=0}^{\\infty}\\underline{{e_{s}^{\\top}}}Z_{\\pi_{\\gamma}^{\\star}}^{t}Y_{\\pi_{\\gamma}^{\\star}}\\right)$ andinthealsweusdtedempsitf $P_{\\pi_{\\gamma}^{\\star}}^{\\infty}$ and the fact that $\\rho^{\\pi_{\\gamma}^{\\star}}=P_{\\pi_{\\gamma}^{\\star}}^{\\infty}r_{\\pi_{\\gamma}^{\\star}}$ ", "page_idx": 29}, {"type": "text", "text": "Therefore by combining these steps we obtain that ", "page_idx": 29}, {"type": "equation", "text": "$$\nV_{\\gamma}^{\\pi_{\\gamma}^{\\star}}(s)\\leq\\mathsf{B}+2\\left\\|h^{\\star}\\right\\|_{\\mathrm{span}}+\\frac{1}{1-\\gamma}\\rho^{\\pi_{\\gamma}^{\\star}}(s).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Combining with the lower bound ", "page_idx": 29}, {"type": "equation", "text": "$$\nV_{\\gamma}^{\\pi_{\\gamma}^{\\star}}(s)\\geq V_{\\gamma}^{\\pi^{\\star}}(s)\\geq\\frac{1}{1-\\gamma}\\rho^{\\star}(s)-\\|h^{\\star}\\|_{\\mathrm{span}}\\geq\\frac{1}{1-\\gamma}\\rho^{\\pi_{\\gamma}^{\\star}}(s)-\\|h^{\\star}\\|_{\\mathrm{span}}\\,,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "we obtain the desired bound ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left|V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}(s)-\\frac{1}{1-\\gamma}\\rho^{\\pi_{\\gamma}^{\\star}}(s)\\right|\\le\\mathsf{B}+2\\left\\|h^{\\star}\\right\\|_{\\mathrm{span}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Lemma 23. If $\\pi$ satisfies $V_{\\gamma}^{\\pi}\\geq V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}-\\delta{\\bf1}$ then ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|V_{\\gamma}^{\\pi}-\\frac{1}{1-\\gamma}\\rho^{\\pi}\\right\\|_{\\infty}\\leq3\\mathsf{B}+2\\left\\|h^{\\star}\\right\\|_{\\mathrm{span}}+\\delta.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. Similar to the proof of Lemmas 21 and 22, we will first establish a bound for the states which are recurrent under $\\pi$ . Specifically, we will first show that if $s$ is recurrent under $\\pi$ wehave ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left|V_{\\gamma}^{\\pi}(s)-\\frac{1}{1-\\gamma}\\rho^{\\pi}(s)\\right|\\leq2\\mathsf{B}+2\\left\\|h^{\\star}\\right\\|_{\\mathrm{span}}+\\delta.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Letting $s\\in\\mathcal{R}^{\\pi}$ , following steps which are similar to the proof of the second part of Lemma 21, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{\\gamma}^{\\pi}(s)-\\displaystyle\\frac{1}{1-\\gamma}\\rho^{\\pi}(s)=e_{s}^{\\top}(I-P_{\\pi}^{\\infty})V_{\\gamma}^{\\pi}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=e_{s}^{\\top}(I-P_{\\pi}^{\\infty})(V_{\\gamma}^{\\pi}-\\displaystyle\\frac{1}{1-\\gamma}\\rho^{\\star})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=e_{s}^{\\top}(I-P_{\\pi}^{\\infty})(V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}-\\displaystyle\\frac{1}{1-\\gamma}\\rho^{\\star})+e_{s}^{\\top}(I-P_{\\pi}^{\\infty})(V_{\\gamma}^{\\pi}-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "using the fact discussed in Lemma 21 that $e_{s}^{\\top}(I-P_{\\pi}^{\\infty})\\rho^{\\star}=0$ since $s$ is recurrent under $\\pi$ . Then by triangle inequality, we obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|V_{\\gamma}^{\\pi}(s)-\\displaystyle\\frac{1}{1-\\gamma}\\rho^{\\pi}(s)\\right|\\leq\\left|e_{s}^{\\top}(I-P_{\\pi}^{\\infty})(V_{\\gamma}^{\\pi_{\\gamma}^{*}}-\\displaystyle\\frac{1}{1-\\gamma}\\rho^{\\star})\\right|+\\left|e_{s}^{\\top}(I-P_{\\pi}^{\\infty})(V_{\\gamma}^{\\pi}-V_{\\gamma}^{\\pi_{\\gamma}^{*}})\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\left\\|V_{\\gamma}^{\\pi_{\\gamma}^{*}}-\\displaystyle\\frac{1}{1-\\gamma}\\rho^{\\star}\\right\\|_{\\mathrm{span}}+\\left\\|V_{\\gamma}^{\\pi}-V_{\\gamma}^{\\pi_{\\gamma}^{*}}\\right\\|_{\\mathrm{span}}}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\left\\|V_{\\gamma}^{\\pi_{\\gamma}^{*}}-\\displaystyle\\frac{1}{1-\\gamma}\\rho^{\\star}\\right\\|_{\\mathrm{\\infty}}+\\delta}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\mathsf{B}+2\\left\\|h^{\\star}\\right\\|_{\\mathrm{span}}+\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where we used the facts that $\\|\\cdot\\|_{\\mathrm{span}}\\leq2\\left\\|\\cdot\\right\\|_{\\infty}$ and that $V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\geq V_{\\gamma}^{\\pi}\\geq V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}-\\delta{\\bf1}$ ", "page_idx": 30}, {"type": "text", "text": "Having established (14), we now extend to transient states using arguments similar to those for the second bound of Lemma 22. Let $s$ be transient under $\\pi$ . Then starting by using Lemma 19, we can calculate ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{\\gamma}^{\\pi}(s)=c_{\\gamma}^{\\pi}(I-\\gamma P_{\\alpha})^{-1}r_{\\pi}}\\\\ &{\\qquad=\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}e_{s}^{\\tau}Z_{\\pi_{\\geq}^{t}\\pi_{\\geq}^{t}}+\\gamma\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}e_{s}^{\\tau}Z_{\\pi_{\\geq}^{t}}^{\\tau}Y_{\\pi}(I-\\gamma X_{\\pi})^{-1}\\overline{{r_{\\pi}}}}\\\\ &{\\qquad=\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}e_{s}^{\\tau}Z_{\\pi_{\\geq}^{t}\\pi_{\\geq}^{t}}+\\gamma\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}e_{s}^{\\tau}Z_{\\pi}^{t}Y_{\\pi}^{\\tau}\\overline{{Y}}_{\\pi}^{\\tau}}\\\\ &{\\qquad\\le\\displaystyle\\sum_{t=0}^{\\infty}c_{\\gamma}^{\\tau}Z_{\\pi_{\\geq}^{t}\\pi_{\\geq}^{t}}+\\left(\\displaystyle\\sum_{t=0}^{\\infty}e_{s}^{\\tau}Z_{\\pi}^{t}Y_{\\pi}\\right)\\overline{{V_{\\pi}^{\\tau}}}}\\\\ &{\\qquad\\le\\displaystyle\\left\\lVert\\sum_{t=0}^{\\infty}e_{s}^{\\tau}Z_{\\pi}^{\\tau}\\right\\rVert_{1}\\left\\lVert\\Gamma_{\\pi}\\right\\rVert_{\\infty}+\\left(\\displaystyle\\sum_{t=0}^{\\infty}e_{s}^{\\tau}Z_{\\pi}^{t}Y_{\\pi}\\right)\\overline{{V_{\\pi}^{\\tau}}}}\\\\ &{\\qquad\\le\\mathrm{B}+\\left(\\displaystyle\\sum_{t=0}^{\\infty}e_{s}^{\\tau}Z_{\\pi}^{t}Y_{\\pi}\\right)\\overline{{V_{\\pi}^{\\tau}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "using the bounded transient time assumption via Lemma 18 in the final step. Then we can calculate ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bigg(\\displaystyle\\sum_{i=0}^{\\infty}c_{i}^{2}z_{i}^{r}\\nu_{i}^{r}\\bigg)\\nabla_{j}^{\\top}\\leq\\bigg(\\displaystyle\\sum_{i=0}^{\\infty}c_{i}^{2}z_{i}^{r}\\nu_{j}\\bigg)\\frac{1}{1-\\gamma}\\overline{{\\rho^{r}}}^{r}+\\bigg(\\displaystyle\\sum_{i=0}^{\\infty}c_{i}^{2}z_{i}^{r}\\nu_{j}^{r}\\bigg)\\left\\|\\nabla_{j}^{\\top}-\\frac{1}{1-\\gamma}\\overline{{\\rho^{r}}}\\right\\|_{\\infty}}&{{}}\\\\ {\\qquad\\qquad\\qquad\\quad=\\bigg(\\displaystyle\\sum_{i=0}^{\\infty}c_{i}^{2}z_{i}^{r}\\nu_{i}^{r}\\bigg)\\frac{1}{1-\\gamma}\\overline{{\\rho^{r}}}^{r}+\\left\\|\\nabla_{j}^{\\top}-\\frac{1}{1-\\gamma}\\overline{{\\rho^{r}}}\\right\\|_{\\infty}}&{{}}\\\\ {\\qquad\\qquad\\quad\\leq\\bigg(\\displaystyle\\sum_{i=0}^{\\infty}c_{i}^{2}z_{i}^{r}\\nu_{i}^{r}\\bigg)\\frac{1}{1-\\gamma}\\overline{{\\rho^{r}}}^{r}+2\\delta+2\\left\\|k^{\\star}\\right\\|_{\\infty}+\\delta}&{{}}\\\\ {\\qquad\\qquad\\quad=\\bigg(\\displaystyle\\sum_{i=0}^{\\infty}c_{i}^{2}z_{i}^{r}\\nu_{i}^{r}\\bigg)\\frac{1}{1-\\gamma}\\overline{{\\rho^{r}}}^{r}\\nu_{i}^{r}+2\\delta+2\\left\\|k^{\\star}\\right\\|_{\\infty}+\\delta}&{{}}\\\\ {\\qquad\\qquad\\quad=\\bigg(\\displaystyle\\sum_{i=0}^{\\infty}c_{i}^{2}z_{i}^{r}\\nu_{i}^{r}\\bigg)\\displaystyle\\frac{1}{1-\\gamma}\\overline{{\\rho^{r}}}^{r}\\nu_{i}^{r}-2\\delta+2\\left\\|k^{\\star}\\right\\|_{\\infty}+\\delta}&{{}}\\\\ {\\qquad\\qquad\\quad=\\displaystyle\\sum_{i=0}^{\\infty}c_{i}^{2}\\nu_{i}^{r}\\nu_{i}^{r}+2\\delta+2\\left\\|k^{\\star}\\right\\|_{\\infty}+\\delta}&{{}}\\\\ {\\qquad\\qquad\\quad=\\displaystyle\\frac{1}{1-\\gamma-\\gamma}\\overline{{\\rho^{r}}}\\nu \n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Where in the first equality we used the fat that $\\left(\\sum_{t=0}^{\\infty}e_{s}^{\\top}Z_{\\pi}^{t}Y_{\\pi}\\right)$ is a probability disribution, in the second inequality we used the bound (14), and in the final steps we used the decomposition of $P_{\\pi}^{\\infty}$ and the fact that $\\rho^{\\pi}=P_{\\pi}^{\\infty}r_{\\pi}$ ", "page_idx": 31}, {"type": "text", "text": "Therefore by combining this last bound with the bound (15), we have ", "page_idx": 31}, {"type": "equation", "text": "$$\nV_{\\gamma}^{\\pi}(s)\\leq38+2\\left\\|h^{\\star}\\right\\|_{\\mathrm{span}}+\\delta+\\frac{1}{1-\\gamma}\\rho^{\\pi}(s).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Combining with the lower bound ", "page_idx": 31}, {"type": "equation", "text": "$$\nV_{\\gamma}^{\\pi}(s)\\geq V_{\\gamma}^{\\pi^{*}}-\\delta\\geq V_{\\gamma}^{\\pi^{*}}(s)-\\delta\\geq\\frac{1}{1-\\gamma}\\rho^{\\star}(s)-\\|h^{\\star}\\|_{\\mathrm{span}}-\\delta\\geq\\frac{1}{1-\\gamma}\\rho^{\\pi}(s)-\\|h^{\\star}\\|_{\\mathrm{span}}-\\delta,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "we conclude that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left|V_{\\gamma}^{\\pi}(s)-{\\frac{1}{1-\\gamma}}\\rho^{\\pi}(s)\\right|\\leq38+2\\left\\|h^{\\star}\\right\\|_{\\mathrm{span}}+\\delta\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "as desired. ", "page_idx": 31}, {"type": "text", "text": "Proof of Theorem $6$ .Suppose $\\pi$ is $\\varepsilon_{\\gamma}$ -optimal for the discounted MDP $(P,r,\\gamma)$ . We can calculate that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{1-\\gamma}\\rho^{\\pi}\\geq V_{\\gamma}^{\\pi}-\\left(3\\mathbf{B}+2\\left\\|h^{\\star}\\right\\|_{\\mathrm{span}}+\\varepsilon_{\\gamma}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\geq V_{\\gamma}^{\\pi\\star}-\\left(3\\mathbf{B}+2\\left\\|h^{\\star}\\right\\|_{\\mathrm{span}}+2\\varepsilon_{\\gamma}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\geq V_{\\gamma}^{\\pi^{\\star}}-\\left(3\\mathbf{B}+2\\left\\|h^{\\star}\\right\\|_{\\mathrm{span}}+2\\varepsilon_{\\gamma}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\geq\\displaystyle\\frac{1}{1-\\gamma}\\rho^{\\star}-\\left(3\\mathbf{B}+3\\left\\|h^{\\star}\\right\\|_{\\mathrm{span}}+2\\varepsilon_{\\gamma}\\right)\\!,}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where in the first inequality we used Lemma 23, in the second inequality we used the fact that $\\pi$ is $\\varepsilon_{\\gamma}$ -optimal, in the third inequality we used the optimality of $\\pi_{\\gamma}^{\\star}$ for the discounted MDP, and in the final inequality we used Lemma 20. Therefore by mulitplying both sides by $1-\\gamma$ ,we have that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\rho^{\\pi}\\geq\\rho^{\\star}-\\frac{\\varepsilon}{\\mathsf{B}+\\mathsf{H}}\\big(3\\mathsf{B}+3\\left\\|h^{\\star}\\right\\|_{\\mathrm{span}}+2\\varepsilon_{\\gamma}\\big)\\geq\\rho^{\\star}-\\left(3\\varepsilon+2\\frac{\\varepsilon_{\\gamma}}{\\mathsf{B}+\\mathsf{H}}\\right)\\varepsilon.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "B.2Proof of Theorem 7 (Discounted MDP Bounds) ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we provide our main result on the sample complexity of general discounted MDPs. ", "page_idx": 31}, {"type": "text", "text": "Our proof relies on three lemmas that provide bounds on relevant variance parameters. The first lemma controls the variance for $\\pi_{\\gamma}^{\\star}$ on recurrent states. ", "page_idx": 31}, {"type": "text", "text": "Lemma 24. Leting $\\pi_{\\gamma}^{\\star}$ beoli $\\begin{array}{r}{(P,r,\\gamma),\\,i f\\,\\gamma\\geq1-\\frac{1}{\\mathsf{B}+\\mathsf{H}}}\\end{array}$ we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{s\\in\\mathcal{R}^{\\pi_{\\gamma}^{\\star}}}\\gamma\\left|e_{s}^{\\top}(I-\\gamma P_{\\pi_{\\gamma}^{\\star}})^{-1}\\sqrt{\\mathbb{V}_{P_{\\pi_{\\gamma}^{\\star}}}\\left[V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right]}\\right|\\leq\\sqrt{\\frac{32}{5}\\frac{8+\\mathsf{H}}{(1-\\gamma)^{2}}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. First, using the decomposition (10), we can calculate for any $s\\in\\mathcal{R}^{\\pi_{\\gamma}^{\\star}}$ that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{e_{s}^{\\top}(I-\\gamma P_{\\pi_{\\gamma}^{\\star}})^{-1}\\sqrt{\\mathbb{V}_{P_{\\pi_{\\gamma}^{\\star}}}\\left[V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right]}=\\overline{{e_{s}}}^{\\top}(I-\\gamma X_{\\pi_{\\gamma}^{\\star}})^{-1}\\sqrt{\\mathbb{V}_{P_{\\pi_{\\gamma}^{\\star}}}\\left[V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right]}}\\\\ {=\\overline{{e_{s}}}^{\\top}(I-\\gamma X_{\\pi_{\\gamma}^{\\star}})^{-1}\\sqrt{\\mathbb{V}_{X_{\\pi_{\\gamma}^{\\star}}}\\left[\\overline{{V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}}}\\right]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Also due to the decomposition, notice that set $\\mathcal{R}^{\\pi_{\\gamma}^{\\star}}$ is a closed set for the Markov chain with transition matrix $P_{\\pi_{\\gamma}^{\\star}}$ , and furthermore when restricting to the entries corresponding to this closed set we obtain the transition matrix $X_{\\pi_{\\gamma}^{\\star}}$ . Therefore we can apply Lemma 12 to this subchain to obtain that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\gamma\\left\\|(I-\\gamma X_{\\pi_{\\gamma}^{\\star}})^{-1}\\sqrt{\\mathbb{V}_{X_{\\pi_{\\gamma}^{\\star}}}\\left[\\overline{{V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}}}\\right]}\\right\\|_{\\infty}\\leq\\sqrt{\\frac{2}{1-\\gamma}}\\sqrt{\\left\\|\\overline{{\\mathbb{V}^{\\pi_{\\gamma}^{\\star}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]}}\\right\\|_{\\infty}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Abbreviating $L=\\mathsf{B}+\\mathsf{H}$ , we can also then apply Lemma 13 to bound ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left\\|\\overline{{\\mathbb{V}^{\\pi_{\\gamma}^{\\star}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]}}\\right\\|_{\\infty}\\leq\\frac{\\left\\|\\overline{{\\mathbb{V}^{\\pi_{\\gamma}^{\\star}}\\left[\\sum_{t=0}^{L-1}\\gamma^{t}R_{t}+\\gamma^{L}V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}(S_{L})\\right]}}\\right\\|_{\\infty}}{1-\\gamma^{2L}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We can repeat a similar argument as within Lemma 15 to bound this term. Fixing an initial state $s_{0}\\in\\mathcal{R}^{\\pi_{\\gamma}^{\\star}}$ , the key observation is that $\\rho^{\\star}$ is constant on the recurrent block of $X_{\\pi_{\\gamma}^{\\star}}$ containing $s_{0}$ and therefore any state trajectory ${\\cal S}_{0}\\,=\\,s_{0},S_{1},S_{2},..\\,.$ under the transition matrix $P_{\\pi_{\\gamma}^{\\star}}$ will have $\\rho^{\\star}(S_{\\cal L})=\\rho^{\\star}(s_{0})$ . Therefore for this fixed $s_{0}$ we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\gamma_{\\infty}^{\\prime\\prime}\\left[\\displaystyle\\sum_{i=0}^{k-1}\\gamma^{i}R_{i}+\\gamma^{i}V_{\\gamma}^{*}(S_{U})\\right]=\\mathbb{V}_{\\infty}^{*}\\left[\\displaystyle\\sum_{i=0}^{k-1}\\gamma^{i}R_{i}+\\gamma^{L}\\left(V_{\\gamma}^{**}(S_{U})-\\displaystyle\\frac{1}{1-\\gamma}\\rho^{*}(s_{0})\\right)\\right]}&{}\\\\ {\\leq\\mathbb{E}_{\\omega}^{*}\\left[\\displaystyle\\sum_{i=0}^{k-1}\\gamma^{i}R_{i}+\\gamma^{L}\\left(V_{\\gamma}^{**}(S_{U})-\\displaystyle\\frac{1}{1-\\gamma}\\rho^{*}(s_{0})\\right)\\right]^{2}}&{}\\\\ {\\leq2\\mathbb{E}_{\\omega}^{*}\\left[\\displaystyle\\sum_{i=0}^{k-1}\\gamma^{i}R_{i}\\right]^{2}+2\\mathbb{E}_{\\omega}^{*}\\left[\\gamma^{i}\\left(V_{\\gamma}^{*}(S_{U})-\\displaystyle\\frac{1}{1-\\gamma}\\rho^{*}(s_{0})\\right)\\right]^{2}}&{}\\\\ {=2\\mathbb{E}_{\\omega}^{*}\\left[\\displaystyle\\sum_{i=0}^{k-1}\\gamma^{i}R_{i}\\right]^{2}+2\\mathbb{E}_{\\omega}^{*}\\left[\\gamma^{i}\\left(V_{\\gamma}^{*}(S_{U})-\\displaystyle\\frac{1}{1-\\gamma}\\rho^{*}(S_{L})\\right)\\right]^{2}}&{}\\\\ {\\leq2L^{2}+2\\exp\\left(V_{\\gamma}^{**}(s)-\\displaystyle\\frac{1}{1-\\gamma}\\rho^{*}(s_{0})\\right)^{2}}&{}\\\\ {\\leq2L^{2}+\\epsilon\\mathbb{E}_{\\omega}^{*}\\times\\left(V_{\\gamma}^{**}(s)-\\displaystyle\\frac{1}{1-\\gamma}\\rho^{*}(s_{0})\\right)^{2}}&{}\\\\ {\\leq2L^{2}+2\\epsilon^{2}}&{}\\\\ {\\leq4L^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where we used Lemma 21 in the penultimate inequality. Applying this argument to all $s_{0}\\in\\mathcal{R}^{\\pi_{\\gamma}^{\\star}}$ we obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left\\|\\overline{{\\mathbb{V}^{\\pi_{\\gamma}^{\\star}}\\left[\\sum_{t=0}^{L-1}\\gamma^{t}R_{t}+\\gamma^{L}V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}(S_{L})\\right]}}\\right\\|_{\\infty}\\le4L^{2}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore by combining with our initial bounds we have that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{*\\in\\mathbb{R}^{+}}{\\operatorname*{max}}\\left\\gamma\\left|e_{s}^{\\top}(I-\\gamma P_{*})^{-1}\\sqrt{\\operatorname*{VP}_{P_{*}}\\left\\{V_{*}^{*}\\right\\}}\\right|\\leq\\sqrt{\\frac{2}{1-\\gamma}}\\sqrt{\\left\\|\\frac{\\ln^{2}\\left[\\underset{s=0}{\\operatorname*{max}}\\right]}{\\log^{2}\\left[\\underset{s=0}{\\operatorname*{max}}\\right]}\\right\\|_{\\infty}}}&{}\\\\ {\\leq\\sqrt{\\frac{2}{1-\\gamma}}\\sqrt{\\frac{\\left[\\coth^{2}\\left[\\underset{s=0}{\\operatorname*{max}}\\right]\\cdot\\left[B_{1}+\\gamma L_{Y}^{*}(S_{L})\\right]\\right]}{1-\\gamma^{2L}}}\\underset{s=0}{\\operatorname*{max}}}\\\\ &{\\leq\\sqrt{\\frac{2}{1-\\gamma}}\\sqrt{\\frac{4L^{2}}{1-\\gamma^{2L}}}}\\\\ &{\\leq\\sqrt{\\frac{2}{1-\\gamma}}\\sqrt{\\frac{16L^{2}}{\\operatorname*{Var}\\left(1-\\gamma\\right)}}}\\\\ &{\\leq\\sqrt{\\frac{2}{5}\\frac{L}{\\left(1-\\gamma\\right)^{2}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "wheqal $\\begin{array}{r}{\\frac{1}{1-\\gamma^{2L}}\\leq\\frac{5}{4}\\frac{1}{(1-\\gamma)L}}\\end{array}$ ", "page_idx": 33}, {"type": "text", "text": "The next lemma controls the variance for $\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}$ on recurrent states. ", "page_idx": 33}, {"type": "text", "text": "Lema 25. Leting $\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}$ bthoi $\\begin{array}{r}{(\\widehat{P},\\widetilde{r},\\gamma),\\,i f\\gamma\\geq1-\\frac{1}{\\mathsf{B}+\\mathsf{H}},}\\end{array}$ we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\epsilon\\mathcal{R}^{\\tilde{\\pi}_{\\gamma,\\mathrm{p}}}}{\\operatorname*{max}}\\gamma\\left|e_{s}^{\\top}(I-\\gamma P_{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}})^{-1}\\sqrt{\\mathbb{V}_{P_{\\tilde{\\pi}_{\\gamma,\\mathrm{p}}}}\\left[V_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right]}\\right|}\\\\ &{\\phantom{\\sum_{\\epsilon\\mathcal{R}^{\\tilde{\\pi}_{\\gamma,\\mathrm{p}}}}\\gamma}\\leq\\sqrt{2\\mathfrak{P}\\frac{\\mathsf{B}+\\mathsf{H}}{(1-\\gamma)^{2}}}+\\sqrt{\\frac{15}{\\mathsf{B}+\\mathsf{H}}}\\frac{\\left\\|\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\|_{\\infty}+\\left\\|\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\pi_{\\gamma}^{\\star}}-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\|_{\\infty}}{1-\\gamma}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. Let $L=\\mathsf{B}+\\mathsf{H}$ . By the same arguments as in the beginning of the proof of Lemma 24, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\epsilon\\mathcal{R}^{\\tilde{\\pi}_{\\gamma,\\mathrm{p}}}}{\\operatorname*{max}}\\gamma\\left|e_{s}^{\\top}(I-\\gamma P_{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}})^{-1}\\sqrt{\\mathbb{V}_{P_{P_{\\tilde{\\tau},\\mathrm{p}}}}\\left[V_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{*}}\\right]}\\right|\\leq\\sqrt{\\frac{2}{1-\\gamma}}\\sqrt{\\left\\|\\overline{{\\mathbb{V}^{\\tilde{\\pi}_{\\gamma,\\mathrm{p}}^{*}}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\widetilde{R}_{t}\\right]}}\\right\\|_{\\infty}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\sqrt{\\frac{2}{1-\\gamma}}\\sqrt{\\frac{\\left\\|\\overline{{\\mathbb{V}^{\\tilde{\\pi}_{\\gamma,\\mathrm{p}}^{*}}\\left[\\displaystyle\\sum_{t=0}^{L-1}\\gamma^{t}\\widetilde{R}_{t}+\\gamma^{L}V_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{*}}(S_{L})\\right]}}{1-\\gamma^{2L}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "so it again suffices to bound $\\mathbb{V}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\left[\\sum_{t=0}^{L-1}\\gamma^{t}\\widetilde{R}_{t}+\\gamma^{L}V_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}(S_{L})\\right]$ Fix $s_{0}\\,\\in\\,\\mathcal{R}^{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}^{\\star}}$ .Again, as observed in Lemma 24, $\\rho^{\\star}$ is constant on the recurrent block of $X_{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}$ containing $s_{0}$ , so we will have ", "page_idx": 33}, {"type": "text", "text": "$\\rho^{\\star}(S_{\\cal L})=\\rho^{\\star}(s_{0})$ with probability one. Therefore (mostly following the steps of Lemma 16) ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta_{k}}^{\\theta_{k}}\\bigg[\\frac{\\mathrm{d}}{\\mathrm{d}t}\\bigg(\\frac{\\mathrm{d}}{\\mathrm{d}t}\\bigg)^{\\theta_{k}}+\\gamma\\nabla_{\\theta_{k}}^{\\theta_{k}}(\\theta_{k})^{\\theta_{k}}(\\theta_{k})}\\\\ &{=\\nabla_{\\theta_{k}}^{\\theta_{k}}\\bigg[\\frac{\\mathrm{d}}{\\mathrm{d}t}\\bigg(\\sum_{i=1}^{n}\\gamma_{i}^{\\theta_{k}}+\\gamma\\nabla_{\\theta_{k}}^{\\theta_{k}}(\\theta_{k})-\\mu\\frac{\\mathrm{d}}{1-\\gamma}\\frac{\\mathrm{d}}{\\mathrm{d}t}\\bigg)^{\\theta_{k}}\\bigg]}\\\\ &{\\le\\nabla_{\\theta_{k}}^{\\theta_{k}}\\bigg[\\bigg(\\frac{\\mathrm{d}}{\\mathrm{d}t}\\bigg)^{\\theta_{k}}\\bigg(\\frac{\\mathrm{d}}{\\mathrm{d}t}-\\gamma_{\\theta_{k}}^{\\theta_{k}}\\bigg)^{\\theta_{k}}\\bigg(\\mu_{k}-\\mu\\frac{\\mathrm{d}}{1-\\gamma}\\frac{\\mathrm{d}}{\\mathrm{d}t}\\bigg)^{\\theta_{k}}\\bigg]^{2}}\\\\ &{=\\nabla_{\\theta_{k}}^{\\theta_{k}}\\bigg[\\frac{\\mathrm{d}}{\\mathrm{d}t}\\bigg(\\sum_{i=1}^{n}\\gamma_{i}^{\\theta_{k}}+\\mu\\big(\\frac{\\mathrm{d}}{\\mathrm{d}t}\\big(\\frac{\\mathrm{d}}{\\mathrm{d}t}\\big)^{\\theta_{k}}(\\theta_{k})-\\mu\\frac{\\mathrm{d}}{1-\\gamma}^{\\theta_{k}}(\\theta_{k})\\big)+\\mu\\left(\\nu_{\\theta}^{\\star}+\\mu\\frac{\\mathrm{d}}{1-\\gamma}\\frac{1}{\\mu}\\rho^{\\star}(\\delta_{k})\\right)\\bigg)^{2}}\\\\ &{\\le\\nabla_{\\theta_{k}}^{\\theta_{k}}\\bigg[\\bigg(\\frac{\\mathrm{d}}{\\mathrm{d}t}\\bigg)^{\\theta_{k}}\\bigg(\\frac{\\mathrm{d}}{\\mathrm{d}t}\\bigg)^{2}+3\\mu\\frac{\\mathrm{d}}{\\mathrm{d}t}\\frac{\\mathrm{d}}{\\mathrm{d}t}\\bigg(\\sum_{i=1}^{n}\\gamma_{i}^{\\theta_{k}}(\\theta_{k})-\\mu\\frac{\\mathrm{d}}{\\mathrm{d}t}\\bigg(\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "using the inequalities $(a+b+c)^{2}\\leq3a^{2}+3b^{2}+3c^{2}$ and $(a+b)^{2}\\leq2a^{2}+2b^{2}$ .Nowwebound each term of (16) analogously to the steps of Lemma 16. For the first term of (16), ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{\\langle\\mathbb{R}_{s_{0}}^{\\hat{\\pi}_{\\gamma,\\mathbb{P}}^{\\star}}\\left(\\sum_{t=0}^{L-1}\\gamma^{t}\\widetilde{R}_{t}\\right)^{2}\\rangle}{\\langle\\mathbb{I}_{s}^{\\dagger}\\rangle}\\overset{\\ge}3\\left(L\\left\\Vert\\widetilde{r}\\right\\Vert_{\\infty}\\right)^{2}\\le3L^{2}(\\left\\Vert r\\right\\Vert_{\\infty}+\\xi)^{2}\\le6L^{2}\\left(1+\\left(\\frac{\\left(1-\\gamma\\right)\\varepsilon}{6}\\right)^{2}\\right)\\le6L^{2}\\left(\\frac{7}{6}\\right)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where we had $\\begin{array}{r}{\\frac{(1-\\gamma)\\varepsilon}{6}\\leq\\frac{\\varepsilon}{6L}\\leq\\frac{1}{6}}\\end{array}$ because $\\begin{array}{r}{\\frac{1}{1-\\gamma}\\geq L}\\end{array}$ and $\\varepsilon\\leq L$ For thescond tem of (16), ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{6\\gamma^{2L}\\mathbb{E}_{s_{0}}^{\\widehat\\pi_{\\gamma,\\mathrm{p}}^{\\star}}\\left(V_{\\gamma}^{\\widehat\\pi_{\\gamma,\\mathrm{p}}^{\\star}}(S_{L})-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}(S_{L})\\right)^{2}\\leq6\\left\\|V_{\\gamma}^{\\widehat\\pi_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\|_{\\infty}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq6\\left(\\left\\|\\widehat V_{\\gamma,\\mathrm{p}}^{\\widehat\\pi_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma}^{\\widehat\\pi_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\|_{\\infty}+\\left\\|\\widehat V_{\\gamma,\\mathrm{p}}^{\\pi_{\\gamma}^{\\star}}-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\|_{\\infty}\\right)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where we used $(a+b)^{2}\\leq2a^{2}+2b^{2}$ and the faet that $\\left\\lVert V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\rVert_{\\infty}\\leq\\left\\lVert\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\rVert_{\\infty}+$ $\\left\\|\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\pi_{\\gamma}^{\\star}}-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\|_{\\infty}$ which was shown in Lemma 16. For the third term of (16), ", "page_idx": 34}, {"type": "equation", "text": "$$\n6\\gamma^{2L}\\left\\|V_{\\gamma,\\mathbb{p}}^{\\widehat{\\pi}_{\\gamma,\\mathbb{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathbb{p}}^{\\star}}\\right\\|_{\\infty}^{2}\\leq6\\left\\|V_{\\gamma,\\mathbb{p}}^{\\widehat{\\pi}_{\\gamma,\\mathbb{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathbb{p}}^{\\star}}\\right\\|_{\\infty}^{2}\\leq6\\left(\\frac{\\xi}{1-\\gamma}\\right)^{2}=6\\left(\\frac{\\varepsilon}{6}\\right)^{2}\\leq\\frac{L^{2}}{6}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the fact that $\\begin{array}{r}{\\left\\|V_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\|_{\\infty}\\leq\\frac{\\xi}{1-\\gamma}}\\end{array}$ is identical to the arguments used in the proof of Lemma 10, and the final inequality is due to the assumption that $\\varepsilon\\leq L$ . For the fourth term of (16), ", "page_idx": 34}, {"type": "equation", "text": "$$\n3\\gamma^{2L}\\mathbb{E}_{s_{0}}^{\\widehat\\pi_{\\gamma,\\mathbf{p}}^{\\star}}\\left(V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}(S_{L})-\\frac{1}{1-\\gamma}\\rho^{\\star}(S_{L})\\right)^{2}\\le3\\left\\lVert V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}-\\frac{1}{1-\\gamma}\\rho^{\\star}\\right\\rVert_{\\infty}^{2}\\le3L^{2}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "using Lemma 22 for the second inequality. Using all these bounds in (16), we obtain ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\lceil\\sum_{s_{0}=\\atop r_{0}}^{\\hat{\\pi}_{\\gamma,\\gamma}^{*}}\\left[\\sum_{t=0}^{L-1}\\gamma^{t}\\widetilde{R}_{t}+\\gamma^{L}V_{\\gamma,\\mathbb{P}}^{\\widehat{\\pi}_{\\gamma,\\mathbb{P}}^{*}}(S_{L})\\right]\\leq\\left(\\frac{49}{6}+\\frac{1}{6}+3\\right)L^{2}+6\\left(\\left\\|\\widehat{V}_{\\gamma,\\mathbb{P}}^{\\widehat{\\pi}_{\\gamma,\\mathbb{P}}^{*}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathbb{P}}^{*}}\\right\\|_{\\infty}+\\left\\|\\widehat{V}_{\\gamma,\\mathbb{P}}^{\\pi_{\\gamma}^{*}}-V_{\\gamma}^{\\pi_{\\gamma}^{*}}\\right\\|_{\\infty}\\right)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and so (since this holds for arbitrary $s_{0}\\in\\mathcal{R}^{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}^{\\star}}.$ ,wehave ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{V}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\left[\\sum_{t=0}^{L-1}\\gamma^{t}\\widetilde{R}_{t}+\\gamma^{L}V_{\\gamma,\\mathbb{P}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}(S_{L})\\right]\\leq\\frac{68}{6}L^{2}+6\\left(\\left\\lVert\\widehat{V}_{\\gamma,\\mathbb{P}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\rVert_{\\infty}+\\left\\lVert\\widehat{V}_{\\gamma,\\mathbb{P}}^{\\pi_{\\gamma}^{\\star}}-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\rVert_{\\infty}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Therefore, combining with our initial arguments, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{c>c}{\\operatorname*{max}}_{c>0}\\bigg|e^{\\tau_{c}}(I-P r_{\\mathrm{t_{c}}})^{-1}\\sqrt{\\gamma_{\\mathrm{Pr_{\\mathrm{t_{c}}}}}\\left[\\nu_{\\mathrm{Pr_{\\mathrm{t_{c}}}}}^{2}\\nu_{\\mathrm{o}}^{2}\\right]}\\;\\bigg|}&{}\\\\ {\\;\\;\\;\\leq\\sqrt{\\frac{2}{1-\\gamma}}\\sqrt{\\frac{\\sqrt{8}\\pi^{4}\\,\\gamma\\left(\\sum_{j=0}^{k-1}\\sqrt{B_{j}+1+\\gamma_{\\mathrm{t_{c}},j}^{2}\\nu_{\\mathrm{o}}^{2}\\left(X_{j}\\right)}\\right)}{1-\\gamma^{2}}}\\;\\bigg|}&{}\\\\ {\\;\\;\\;\\leq\\sqrt{\\frac{2}{1-\\gamma}}\\sqrt{\\frac{8}{1-\\gamma}}L^{-4}\\left(\\frac{\\left\\Vert\\tilde{V}_{\\mathrm{t_{c}}}^{2}-\\nu_{\\mathrm{o}}^{2}\\nu_{\\mathrm{o}}^{2}\\right\\Vert_{\\infty}+\\left\\Vert\\tilde{V}_{\\mathrm{t_{c}}}^{2}-\\nu_{\\mathrm{o}}^{2}\\nu_{\\mathrm{o}}^{2}\\right\\Vert_{\\infty}\\right)^{2}}{\\sqrt{1-\\gamma^{2}}}}}\\\\ {\\;\\;\\;}&{\\leq\\sqrt{\\frac{2}{1-\\gamma}}\\sqrt{\\frac{8}{1-\\gamma}}L^{-4}\\sqrt{6\\left(\\left\\Vert\\tilde{V}_{\\mathrm{t_{c}}}^{2}-\\nu_{\\mathrm{o}}^{2}\\nu_{\\mathrm{o}}^{2}\\right\\Vert_{\\infty}+\\left\\Vert\\tilde{V}_{\\mathrm{t_{c}}}^{2}-\\nu_{\\mathrm{o}}^{2}\\nu_{\\mathrm{o}}^{2}\\right\\Vert_{\\infty}\\right)^{2}}}\\\\ {\\;\\;\\;}&{\\leq\\sqrt{\\frac{2}{1-\\gamma}}\\sqrt{\\frac{8}{1-\\gamma}{2}}L^{-4}\\sqrt{6\\left(\\left\\Vert\\tilde{V}_{\\mathrm{t_{c}}}^{2}-\\nu_{\\mathrm{o}}^{2}\\nu_{\\mathrm{o}}^{2}\\right\\Vert_{\\infty}+\\left\\Vert\\tilde{V}_{\\mathrm{t_{c}}}^{2}-\\nu_{\\mathrm{o}}^{2}\\nu_{\\mathrm{o}}^{2}\\right\\Vert_{\\infty}\\right)^{2}}}\\\\ {\\;\\;\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where we used Lemma 14 to bound 1-2L $\\begin{array}{r}{\\frac{1}{1-\\gamma^{2L}}\\leq\\frac{5}{4}\\frac{1}{(1-\\gamma)L}}\\end{array}$ ", "page_idx": 35}, {"type": "text", "text": "The next lemma controls the variance on all states. ", "page_idx": 35}, {"type": "text", "text": "Lemma 26. Under the settings of Lemmas 24 and 25, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\gamma\\left\\|(I-\\gamma P_{\\pi_{\\gamma}^{\\star}})^{-1}\\sqrt{\\mathbb{V}_{P_{\\pi_{\\gamma}^{\\star}}}\\left[V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right]}\\right\\|_{\\infty}\\leq4\\sqrt{\\frac{\\mathsf{B}+\\mathsf{H}}{(1-\\gamma)^{2}}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left\\langle\\left\\lVert(I-\\gamma P_{\\widehat{\\tau}_{,\\mathrm{P}}^{\\star},\\mathrm{P}})^{-1}\\sqrt{\\mathbb{V}_{P\\widehat{\\tau}_{,\\mathrm{P}}^{\\star}}\\left[V_{\\gamma,\\mathrm{P}}^{\\widehat{\\tau}_{,\\mathrm{P}}^{\\star}}\\right]}\\right\\rVert_{\\infty}\\leq8\\sqrt{\\frac{\\mathbb{B}+\\mathsf{H}}{(1-\\gamma)^{2}}}+\\sqrt{\\frac{15}{\\mathsf{B}+\\mathsf{H}}}\\frac{\\left\\lVert\\widehat{V}_{\\gamma,\\mathrm{P}}^{\\widehat{\\tau}_{,\\mathrm{P}}^{\\star}}-V_{\\gamma}^{\\widehat{\\tau}_{,\\mathrm{P}}^{\\star}}\\right\\rVert_{\\infty}+\\left\\lVert\\widehat{V}_{\\gamma,\\mathrm{P}}^{\\pi_{,\\mathrm{P}}^{\\star}}-V_{\\gamma}^{\\widehat{\\tau}_{,\\mathrm{P}}^{\\star}}\\right\\rVert_{\\infty}}{1-\\gamma}\\mathrm{~,~}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. First we establish the first bound in the lemma statement. As we have already bounded the entries corresponding to the recurrent states of $\\pi_{\\gamma}^{\\star}$ by Lemma 24, it remains to bound the transient states. Let $s\\in\\tau^{\\pi_{\\gamma}^{\\star}}$ be an arbitrary transient state. Using Lemma 19, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{e_{s}^{\\top}\\gamma(I-\\gamma P_{\\pi_{\\gamma}^{*}})^{-1}\\sqrt{\\mathbb{V}_{P_{\\pi_{\\gamma}^{*}}}\\left[V_{\\gamma}^{\\pi_{\\gamma}^{*}}\\right]}=\\gamma\\underline{{e_{s}}}^{\\top}\\displaystyle\\sum_{k=1}^{\\infty}\\gamma^{k}Z_{\\pi_{\\gamma}^{*}}^{k-1}Y_{\\pi_{\\gamma}^{*}}(I-\\gamma X_{\\pi_{\\widetilde{\\gamma}}})^{-1}\\sqrt{\\mathbb{V}_{P_{\\pi_{\\widetilde{\\gamma}}}}\\left[V_{\\gamma}^{\\pi_{\\widetilde{\\gamma}}^{*}}\\right]}}\\\\ {+\\gamma\\underline{{e_{s}}}^{\\top}\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}Z_{\\pi_{\\widetilde{\\gamma}}^{*}}^{t}\\sqrt{\\underline{{\\mathbb{V}_{P_{\\pi_{\\widetilde{\\gamma}}}}}}\\left[V_{\\gamma}^{\\pi_{\\widetilde{\\gamma}}^{*}}\\right]}.\\eqnoindent}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Now we bound each of the terms in (17). For the first term, we can calculate ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\overbrace{\\mathscr{t}_{\\infty}}^{\\infty}\\overbrace{\\sum_{k=1}^{\\infty}}^{\\infty}\\gamma^{k}Z_{\\pi_{\\gamma}^{k}}^{k-1}Y_{\\pi_{\\gamma}^{k}}(I-\\gamma X_{\\pi_{\\gamma}^{k}})^{-1}\\sqrt{\\nabla_{P_{T_{\\gamma}^{k}}}\\left[V_{\\gamma}^{\\pi_{\\gamma}^{k}}\\right]}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\gamma\\underline{{e_{s}}}^{\\top}\\sum_{k=1}^{\\infty}Z_{\\pi_{\\gamma}^{k}}^{k-1}Y_{\\pi_{\\gamma}^{k}}(I-\\gamma X_{\\pi_{\\gamma}^{k}})^{-1}\\sqrt{\\nabla_{P_{T_{\\pi_{\\gamma}^{k}}}}\\left[V_{\\gamma}^{\\pi_{\\gamma}^{k}}\\right]}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\left\\lVert\\underline{{e_{s}}}^{\\top}\\sum_{k=1}^{\\infty}Z_{\\pi_{\\gamma}^{k-1}}^{k-1}Y_{\\pi_{\\gamma}^{k}}\\right\\rVert_{1}\\gamma\\left\\lVert(I-\\gamma X_{\\pi_{\\gamma}^{k}})^{-1}\\sqrt{\\nabla_{P_{T_{\\pi_{\\gamma}^{k}}}}\\left[V_{\\gamma}^{\\pi_{\\gamma}^{k}}\\right]}\\right\\rVert_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\sqrt{\\frac{32}{5}\\frac{\\textrm{B+H}}{(1-\\gamma)^{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where we used the fact that es T =1 Z is a probability distribution and Lemma 24. For the second term of (17), we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\gamma\\underline{{e_{s}}}^{\\top}\\sum_{t=0}^{\\infty}\\gamma^{t}Z_{\\pi;\\widetilde{\\gamma}}^{t}\\sqrt{\\underline{{\\Psi_{P\\pi_{\\widetilde{\\gamma}}}}}\\left[V_{\\gamma}^{\\pi_{\\widetilde{\\gamma}}^{*}}\\right]}=\\gamma\\left\\lVert\\underline{{e_{s}}}^{\\top}\\sum_{t=0}^{\\infty}\\gamma^{t}Z_{\\pi_{\\widetilde{\\gamma}}^{t}}^{t}\\right\\rVert_{1}\\underset{t=0}{\\overset{\\infty}{\\sum}}\\frac{\\gamma^{t}\\underline{{e_{s}}}^{\\top}Z_{\\pi_{\\widetilde{\\gamma}}^{t}}^{t}}{\\left\\lVert\\underline{{e_{s}}}^{\\top}\\sum_{t=0}^{\\infty}\\gamma^{t}Z_{\\pi;\\widetilde{\\gamma}}^{t}\\right\\rVert_{1}}\\sqrt{\\underline{{\\Psi_{P\\pi_{\\widetilde{\\gamma}}}}}\\left[V_{\\gamma}^{\\pi_{\\widetilde{\\gamma}}^{*}}\\right]}}\\\\ &{}&{\\le\\gamma\\left\\lVert\\underline{{e_{s}}}^{\\top}\\sum_{t=0}^{\\infty}\\gamma^{t}Z_{\\pi_{\\widetilde{\\gamma}}^{t}}^{t}\\right\\rVert_{1}\\sqrt{\\displaystyle{\\sum_{t=0}^{\\infty}\\frac{\\gamma^{t}\\underline{{e_{s}}}^{\\top}Z_{\\pi_{\\widetilde{\\gamma}}^{t}}^{t}}{\\left\\lVert\\underline{{e_{s}}}^{\\top}\\sum_{t=0}^{\\infty}\\gamma^{t}Z_{\\pi_{\\widetilde{\\gamma}}^{t}}^{t}\\right\\rVert_{1}^{\\frac{\\mathbb{V}_{P\\pi_{\\widetilde{\\gamma}}^{*}}}{\\gamma}}\\left[V_{\\gamma}^{\\pi_{\\widetilde{\\gamma}}^{*}}\\right]}}}\\\\ &{}&{=\\sqrt{\\left\\lVert\\underline{{e_{s}}}^{\\top}\\sum_{t=0}^{\\infty}\\gamma^{t}Z_{\\pi_{\\widetilde{\\gamma}}^{t}}^{t}\\right\\rVert_{1}}\\sqrt{\\gamma^{2}\\displaystyle{\\sum_{t=0}^{\\infty}\\gamma^{t}\\underline{{e_{s}}}^{\\top}Z_{\\pi}^{t}\\underline{{\\Psi_{P\\pi_{\\widetilde{\\gamma}}}}}\\left[V_{\\gamma}^{\\pi_{\\widetilde{\\gamma}}^{*}}\\right]}}\\qquad(18).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where we used Jensen's inequality since $x\\mapsto{\\sqrt{x}}$ isconcave and $\\frac{\\sum_{t=0}^{\\infty}\\gamma^{t}{e_{s}}^{\\top}Z_{\\pi_{\\gamma}^{\\star}}^{t}}{\\left\\|{e_{s}}^{\\top}\\sum_{t=0}^{\\infty}\\gamma^{t}Z_{\\pi_{\\gamma}^{\\star}}^{t}\\right\\|_{1}}$ isa probabity distribution (all entries of this row vector are positive and they sum to 1 due to our normalization). Now we bound each factor in (18). Using Lemma 18, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sqrt{\\left\\|\\frac{e_{s}\\top}{e_{\\bar{s}}\\top}\\sum_{t=0}^{\\infty}\\gamma^{t}Z_{\\pi_{\\gamma}^{\\star}}^{t}\\right\\|_{1}}\\leq\\sqrt{\\left\\|\\frac{e_{s}\\top}{t}\\sum_{t=0}^{\\infty}Z_{\\pi_{\\gamma}^{\\star}}^{t}\\right\\|_{1}}\\leq\\sqrt{\\mathsf{B}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "For the second factor in (18), we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\underline{{e_{s}}}^{\\top}Z_{\\pi_{\\gamma}^{t}}^{t}\\underline{{\\mathbb{V}_{P_{\\pi_{\\gamma}^{t}}}}}\\left[V_{\\gamma}^{\\pi_{\\gamma}^{*}}\\right]\\leq\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\underline{{e_{s}}}^{\\top}Z_{\\pi_{\\gamma}^{t}}^{t}\\underline{{\\mathbb{V}_{P_{\\pi_{\\gamma}^{t}}}}}\\left[V_{\\gamma}^{\\pi_{\\gamma}^{*}}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\underline{{e_{s}}}^{\\top}\\displaystyle\\sum_{k=1}^{\\infty}\\gamma^{k}Z_{\\pi_{\\gamma}^{k}}^{k-1}Y_{\\pi_{\\gamma}^{*}}(I-\\gamma X_{\\pi_{\\gamma}^{*}})^{-1}\\overline{{\\mathbb{V}_{P_{\\pi_{\\gamma}^{*}}}\\left[V_{\\gamma}^{\\pi_{\\gamma}^{*}}\\right]}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=e_{s}^{\\top}(I-\\gamma P_{\\pi_{\\gamma}^{*}})^{-1}\\mathbb{V}_{P_{\\pi_{\\gamma}^{*}}}\\left[V_{\\gamma}^{\\pi_{\\gamma}^{*}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the equality step is due to Lemma 19. Now we can apply two steps which are used within Lemma 12 to obtain the desired bound on this term. Abbreviating $\\boldsymbol{v}=\\bar{\\mathbb{V}}_{P_{\\pi_{\\gamma}^{\\star}}}\\left[V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right]$ itis shown within Lemma12 that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\gamma^{2}\\left\\|(I-\\gamma P_{\\pi_{\\gamma}^{\\star}})^{-1}v\\right\\|_{\\infty}\\leq2\\gamma^{2}\\left\\|(I-\\gamma^{2}P_{\\pi_{\\gamma}^{\\star}})^{-1}v\\right\\|_{\\infty}\\leq2\\left\\|\\mathbb{V}^{\\pi_{\\gamma}^{\\star}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]\\right\\|_{\\infty}\\leq\\frac{2}{(1-\\gamma)^{2}}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "(where the final inequality is because the total discounted return is within $[0,\\frac{1}{1-\\gamma}])$ . Therefore we can bound the second factor in (18) as ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sqrt{\\gamma^{2}\\sum_{t=0}^{\\infty}\\gamma^{t}\\underline{{e_{s}}}^{\\top}Z_{\\pi_{\\gamma}^{\\star}}^{t}\\mathbb{V}_{P_{\\pi_{\\gamma}^{\\star}}}\\left[V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right]}\\leq\\sqrt{\\frac{2}{(1-\\gamma)^{2}}}=\\frac{\\sqrt{2}}{1-\\gamma}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Combining all of these bounds back into (17), we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{e_{s}^{\\top}\\gamma(I-\\gamma P_{\\pi_{\\gamma}^{\\star}})^{-1}\\sqrt{\\mathbb{V}_{P_{\\pi_{\\gamma}^{\\star}}}\\left[V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right]}\\leq\\sqrt{\\frac{32}{5}\\frac{\\mathsf{B}+\\mathsf{H}}{(1-\\gamma)^{2}}}+\\sqrt{\\mathsf{B}}\\frac{\\sqrt{2}}{1-\\gamma}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad<4\\sqrt{\\frac{\\mathsf{B}+\\mathsf{H}}{(1-\\gamma)^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Thus we have established the first inequality from the lemma statement. ", "page_idx": 37}, {"type": "text", "text": "For the second inequality, the argument is entirely analogous, except that we use Lemma 25 instead of Lemma 24, and also in the MDP with the perturbed reward $\\widetilde r$ wehavethebound ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\Vert\\mathbb{V}^{\\pi_{\\gamma}^{\\star}}\\left[\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}R_{t}\\right]\\right\\Vert_{\\infty}\\leq\\left(\\frac{\\|\\widetilde{r}\\|_{\\infty}}{1-\\gamma}\\right)^{2}\\leq\\left(\\frac{\\|r\\|_{\\infty}+\\xi}{1-\\gamma}\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{1}{(1-\\gamma)^{2}}\\left(1+\\frac{(1-\\gamma)\\varepsilon}{6}\\right)^{2}\\leq\\frac{1}{(1-\\gamma)^{2}}\\left(\\frac{7}{6}\\right)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Wwhere e usd thefaethat $\\begin{array}{r}{\\frac{(1-\\gamma)\\varepsilon}{6}\\leq\\frac{\\varepsilon}{6(\\mathsf{B}+\\mathsf{H})}\\leq\\frac{1}{6}}\\end{array}$ because $\\begin{array}{r}{\\frac{1}{1-\\gamma}\\geq\\mathsf{B}+\\mathsf{H}}\\end{array}$ $\\varepsilon\\leq\\mathsf{B}+\\mathsf{H}$ Thus we can obtain the bound ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma\\left\\|(I-\\gamma P_{\\overline{{\\pi}}_{\\gamma,\\mathbf{p}}^{*}})^{-1}\\sqrt{\\mathbb{V}_{P\\varrho_{\\gamma,\\mathbf{p}}^{*}}\\left[V_{\\overline{{\\gamma}},\\mathbf{p}}^{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}^{*}}\\right]}\\right\\|_{\\infty}}\\\\ &{\\leq\\sqrt{2\\mathfrak{L}\\frac{\\mathbb{B}+\\mathbb{H}}{(1-\\gamma)^{2}}}+\\sqrt{\\frac{15}{\\mathbb{B}+\\mathbb{H}}}\\frac{\\left\\|\\widehat{V}_{\\gamma,\\mathbf{p}}^{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}^{*}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}^{*}}\\right\\|_{\\infty}+\\left\\|\\widehat{V}_{\\gamma,\\mathbf{p}}^{\\pi_{\\gamma}^{*}}-V_{\\gamma}^{\\pi_{\\gamma}^{*}}\\right\\|_{\\infty}}{1-\\gamma}}\\\\ &{\\qquad+\\sqrt{\\mathbb{B}}\\frac{7\\sqrt{2}}{6(1-\\gamma)}}\\\\ &{\\leq8\\sqrt{\\frac{\\mathbb{B}+\\mathbb{H}}{(1-\\gamma)^{2}}}+\\sqrt{\\frac{15}{\\mathbb{B}+\\mathbb{H}}}\\frac{\\left\\|\\widehat{V}_{\\gamma,\\mathbf{p}}^{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}^{*}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathbf{p}}^{*}}\\right\\|_{\\infty}+\\left\\|\\widehat{V}_{\\gamma,\\mathbf{p}}^{\\pi_{\\gamma}^{*}}-V_{\\gamma}^{\\pi_{\\gamma}^{*}}\\right\\|_{\\infty}}{1-\\gamma}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "This completes the proof of the lemma. ", "page_idx": 37}, {"type": "text", "text": "We are now ready to prove Theorem 7 on the sample complexity of general discounted MDPs. ", "page_idx": 37}, {"type": "text", "text": "Proof of Theorem 7. To prove Theorem 7 we will combine our bounds of the variance parameters in Lemma 26 with Lemma 10. First, starting with (1) from Lemma 10 and combining with the first bound from Lemma 26, we have that there exist absolute constants $c_{1},c_{2}$ such that for any $\\delta\\in(0,1)$ ", "page_idx": 37}, {"type": "text", "text": "$\\begin{array}{r}{n\\geq\\frac{c_{2}}{1-\\gamma}\\log\\left(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\right)}\\end{array}$ , then with probability at least $1-\\delta$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\widehat{V}_{\\gamma,\\gamma}^{\\star\\star}-V_{\\gamma^{\\star}}^{\\star}\\right\\|_{\\infty}\\leq\\gamma\\sqrt{\\frac{c_{1}\\log\\left(\\frac{N\\delta}{(1-\\gamma)\\delta}\\right)}{n}}\\left\\|(I-\\gamma\\mathcal{P}_{\\varepsilon_{\\varepsilon}})^{-1}\\sqrt{\\nabla\\mathcal{P}_{\\gamma,\\gamma}\\left[V_{\\gamma^{\\star}}^{\\star}\\right]}\\right\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+c_{1}\\frac{\\log\\left(\\frac{N\\delta}{(1-\\gamma)\\delta}\\right)}{(1-\\gamma)n}\\left\\|\\big\\vert V_{\\gamma^{\\star}}^{\\star}\\right\\|_{\\infty}+\\frac{\\varepsilon}{6}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\sqrt{\\frac{c_{1}\\log\\left(\\frac{N\\delta}{(1-\\gamma)\\delta}\\right)}{(1-\\gamma)^{2}}}+c_{1}\\frac{\\log\\left(\\frac{N\\delta}{(1-\\gamma)\\delta}\\right)}{(1-\\gamma)n}\\left\\|\\big\\vert V_{\\gamma^{\\star}}^{\\star}\\right\\|_{\\infty}+\\frac{\\varepsilon}{6}}\\\\ &{\\leq\\sqrt{\\frac{c_{1}\\log\\left(\\frac{N\\delta}{(1-\\gamma)\\delta}\\right)}{n}}4\\sqrt{\\frac{8+\\gamma}{(1-\\gamma)^{2}}}+c_{1}\\frac{\\log\\left(\\frac{N\\delta}{(1-\\gamma)\\delta}\\right)}{(1-\\gamma)^{2}n}+\\frac{\\varepsilon}{6}}\\\\ &{\\leq\\sqrt{\\frac{c_{1}}{n}}\\frac{1}{(1-\\gamma)^{2}}\\frac{\\varepsilon^{2}}{4}\\sqrt{\\frac{8+\\gamma}{(1-\\gamma)^{2}}}+c_{1}\\frac{\\log\\left(\\frac{5\\delta}{(1-\\gamma)\\delta}\\right)}{(1-\\gamma)^{2}n}+\\frac{\\varepsilon}{6}}\\\\ &{\\leq\\frac{\\varepsilon}{6}+\\frac{1}{16}\\frac{\\varepsilon^{2}}{(1-\\gamma)^{2}}\\frac{\\varepsilon^{2}}{8}+\\frac{\\varepsilon}{6}}\\\\ &{\\leq\\frac{\\varepsilon}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "$\\begin{array}{r}{n\\geq16\\cdot6^{2}c_{1}\\frac{{\\sf B}+{\\sf H}}{\\varepsilon^{2}(1-\\gamma)^{2}}\\log\\left(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\right)}\\end{array}$ and the final inequality makes use of the fact that $\\varepsilon\\leq\\mathsf{B}+\\mathsf{H}$ ", "page_idx": 38}, {"type": "text", "text": "Next, still using Lemma 10, under the same event, we also have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{\\sqrt{s}}{s}}\\cdots\\lefteqn{\\mathrm{F}_{\\infty}^{s+}\\bigg[\\int_{\\infty}^{\\infty}\\int_{\\infty}^{\\infty}\\bigg]_{s}}}\\\\ &{\\leq\\mathrm{var}\\left[\\frac{\\sqrt{s+\\frac{q(\\mathbf{v}-\\mathbf{u}_{0}^{\\perp})}{2}}}{N}\\left\\|(-\\gamma^{\\mathbb{R}_{n+}})^{-1}\\sqrt{\\gamma^{\\mathbb{R}_{n+}}\\left[\\int_{0}^{\\kappa_{p}^{\\perp}}\\right]_{s}}\\right\\|_{s}\\right]}\\\\ &{\\quad+\\mathrm{c}\\frac{\\mathrm{lear}\\left(\\frac{\\sqrt{s+\\frac{q(\\mathbf{v}-\\mathbf{u}_{0}^{\\perp})}{2N}}}{1-\\gamma}\\right)_{s}}{(1-\\gamma^{\\mathbb{R}_{n+}})^{2}}\\mathrm{F}_{\\infty}^{s}\\bigg[\\mathrm{s}_{0}^{\\perp}-\\frac{\\kappa}{6}}\\\\ &{\\leq\\sqrt{\\frac{\\mathrm{tear}\\left(\\frac{\\sqrt{s+\\frac{q(\\mathbf{v}-\\mathbf{u}_{0}^{\\perp})}{2N}}}{N}\\right)}{N}}\\left(\\sqrt{\\gamma^{\\mathbb{R}_{n+}}\\left[\\mathbf{1}-\\eta^{\\mathbb{R}_{n}^{\\perp}}\\right]^{2}}+\\sqrt{\\frac{\\|\\mathbf{v}\\|_{0}^{\\kappa_{p}^{\\perp}}}{N}}\\frac{\\|\\mathbf{F}_{\\infty}^{s}-\\mathbf{F}_{\\infty}^{\\perp}\\|_{s+}}{1-\\gamma^{\\mathbb{R}_{n}^{\\perp}}}\\mathrm{L}\\right)\\bigg]}\\\\ &{\\quad+\\mathrm{c}\\frac{\\mathrm{lear}\\left(\\frac{\\sqrt{s+\\mathbf{u}_{0}^{\\perp}}}{2N}\\right)}{(1-\\gamma^{\\mathbb{R}_{n+}})^{2}}\\left\\|\\mathrm{v}\\right\\|_{s}^{s}\\mathrm{f}_{s}^{*}\\right\\|_{s}+\\frac{\\varepsilon}{6}}\\\\ &{\\leq\\sqrt{\\frac{\\mathrm{tear}\\left(\\frac{\\sqrt{s+\\mathbf{u}_{0}^{\\perp}}}{2N}\\right)}{N}}\\left(\\sqrt{\\gamma^{\\mathbb{R}_{n+}}\\left[\\mathbf{1}-\\eta^{\\mathbb{R}_{n+}^{\\perp}}\\right]^{2}}+\\sqrt{\\frac{\\|\\mathbf{v}\\|_{s}^{\\kappa_{p \n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "using the second inequality from Lemma 26 for the second inequality, and then we use the fact that $\\begin{array}{r}{\\left\\|V_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\|_{\\infty}\\le\\ \\frac{7}{6}\\frac{1}{1-\\gamma}}\\end{array}$ , which was argued in Lemma 26, as well as the fact from above that ", "page_idx": 38}, {"type": "text", "text": "$\\left\\|\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\pi_{\\gamma}^{\\star}}-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\|_{\\infty}\\leq\\varepsilon/2\\leq(\\mathsf{B}+\\mathsf{H})/2$ . After rearranging, we obtain that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(1-\\sqrt{\\frac{c_{1}\\log\\Big(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\Big)}{n}}\\sqrt{\\frac{15}{8+\\mathsf{H}}}\\frac1{1-\\gamma}\\right)\\left\\Vert\\widehat{V}_{\\gamma,\\mathtt{p}}^{\\widehat{\\pi}_{\\mathtt{y},\\mathtt{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathtt{p}}^{\\star}}\\right\\Vert_{\\infty}}\\\\ &{\\quad\\leq\\sqrt{\\frac{c_{1}\\log\\Big(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\Big)}{n}}\\left(8\\sqrt{\\frac{\\mathsf{B}+\\mathsf{H}}{(1-\\gamma)^{2}}}+\\sqrt{\\frac{15}{8+\\mathsf{H}}}\\frac{(\\mathsf{B}+\\mathsf{H})/2}{1-\\gamma}\\right)+c_{1}\\frac{\\log\\Big(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\Big)}{(1-\\gamma)^{2}n}\\frac{7}{6}+\\frac{\\varepsilon}{6}}\\\\ &{\\quad\\leq\\sqrt{\\frac{c_{1}\\log\\Big(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\Big)}{n}}10\\sqrt{\\frac{\\mathsf{B}+\\mathsf{H}}{(1-\\gamma)^{2}}}+c_{1}\\frac{\\log\\Big(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\Big)}{(1-\\gamma)^{2}n}\\frac{7}{6}+\\frac{\\varepsilon}{6}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "$\\begin{array}{r}{n\\geq6^{2}\\cdot10^{2}c_{1}\\frac{{\\mathsf{B}}+{\\mathsf{H}}}{\\varepsilon^{2}(1-\\gamma)^{2}}\\log\\left(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\right)}\\end{array}$ then the RHS of (19) is bounded by ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{\\varepsilon}{6}+\\frac{7}{6}\\frac{\\varepsilon^{2}}{8+\\mathsf{H}}\\frac{1}{6^{2}\\cdot10^{2}}+\\frac{\\varepsilon}{6}\\leq\\left(\\frac{1}{6}+\\frac{1}{6^{2}\\cdot10^{2}}+\\frac{1}{6}\\right)\\varepsilon\\leq0.4\\varepsilon\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "using the assumption that $\\varepsilon\\leq\\mathsf{B}+\\mathsf{H}$ . Under the same condition on $n$ , we also have that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(1-\\sqrt{\\frac{c_{1}\\log\\left(\\frac{S A}{(1-\\gamma)^{\\delta}}\\varepsilon\\right)}{n}}\\sqrt{\\frac{15}{8+\\mathsf{H}}}\\frac{1}{1-\\gamma}\\right)\\left\\Vert\\widehat{V}_{\\gamma,\\mathrm{P}}^{\\widehat{n}_{\\gamma,\\mathrm{P}}^{\\star}}-V_{\\gamma}^{\\widehat{n}_{\\gamma,\\mathrm{P}}^{\\star}}\\right\\Vert_{\\infty}}\\\\ &{\\quad\\ge\\left(1-\\sqrt{\\frac{\\varepsilon^{2}}{(8+\\mathsf{H})^{2}}}\\sqrt{\\frac{15}{6^{2}\\cdot10^{2}}}\\right)\\left\\Vert\\widehat{V}_{\\gamma,\\mathrm{P}}^{\\widehat{n}_{\\gamma,\\mathrm{P}}^{\\star}}-V_{\\gamma}^{\\widehat{n}_{\\gamma,\\mathrm{P}}^{\\star}}\\right\\Vert_{\\infty}}\\\\ &{\\quad\\ge\\left(1-\\sqrt{\\frac{15}{6^{2}\\cdot10^{2}}}\\right)\\left\\Vert\\widehat{V}_{\\gamma,\\mathrm{P}}^{\\widehat{n}_{\\gamma,\\mathrm{P}}^{\\star}}-V_{\\gamma}^{\\widehat{n}_{\\gamma,\\mathrm{P}}^{\\star}}\\right\\Vert_{\\infty}}\\\\ &{\\quad\\ge0.9\\left\\Vert\\widehat{V}_{\\gamma,\\mathrm{P}}^{\\widehat{n}_{\\gamma,\\mathrm{P}}^{\\star}}-V_{\\gamma}^{\\widehat{n}_{\\gamma,\\mathrm{P}}^{\\star}}\\right\\Vert_{\\infty}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where again we used the assumption that $\\varepsilon\\;\\leq\\;{\\mathsf{B}}+{\\mathsf{H}}$ . Combining these two bounds with the inequality (19), we obtain that ", "page_idx": 39}, {"type": "equation", "text": "$$\n0.9\\left\\|\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\|_{\\infty}\\leq0.4\\varepsilon\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "which implies that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left\\|\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\|_{\\infty}\\leq\\frac{0.4}{0.9}\\varepsilon<\\frac{\\varepsilon}{2}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Since we have established hat $\\begin{array}{r}{\\left\\|\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\pi_{\\gamma}^{\\star}}-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\|_{\\infty}\\leq\\frac{\\varepsilon}{2}}\\end{array}$ and that $\\begin{array}{r}{\\left\\|\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\|_{\\infty}\\leq\\frac{\\varepsilon}{2}}\\end{array}$ , since also $\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\geq\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\pi_{\\gamma}^{\\star}}$ V,p, we can conclude that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\leq\\left\\lVert\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\pi_{\\gamma}^{\\star}}-V_{\\gamma}^{\\pi_{\\gamma}^{\\star}}\\right\\rVert_{\\infty}\\mathbf{1}+\\left\\lVert\\widehat{V}_{\\gamma,\\mathrm{p}}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}-V_{\\gamma}^{\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}}\\right\\rVert_{\\infty}\\mathbf{1}\\leq\\varepsilon\\mathbf{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "that is that $\\widehat{\\pi}_{\\gamma,\\mathrm{p}}^{\\star}$ .s $\\varepsilon$ optimal. ", "page_idx": 39}, {"type": "text", "text": "Finally, we check that all of our conditions on $n$ can be satisfied if ", "page_idx": 39}, {"type": "equation", "text": "$$\nn\\geq\\operatorname*{max}\\left\\{6^{2}\\cdot10^{2}c_{1}\\frac{\\mathsf{B}+\\mathsf{H}}{\\varepsilon^{2}(1-\\gamma)^{2}},6^{2}\\cdot16c_{1}\\frac{\\mathsf{B}+\\mathsf{H}}{\\varepsilon^{2}(1-\\gamma)^{2}},\\frac{c_{2}}{1-\\gamma}\\right\\}\\log\\left(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\right),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "andsince $\\begin{array}{r}{\\frac{1}{1-\\gamma}\\,\\geq\\,\\mathsf{B}+\\mathsf{H}}\\end{array}$ and $\\textsf{B}+\\textsf{H}\\geq\\varepsilon$ wehave $\\begin{array}{r}{\\frac{\\mathsf{B}+\\mathsf{H}}{\\varepsilon^{2}(1-\\gamma)^{2}}\\,\\geq\\,\\frac{(\\mathsf{B}+\\mathsf{H})^{2}}{\\varepsilon^{2}(1-\\gamma)}\\,\\geq\\,\\frac{1}{1-\\gamma}}\\end{array}$ so the above is guaranteed if we set Cs = max[62 102c1, C2} and require n \u2265 C3) $\\begin{array}{r}{n\\geq C_{3}\\frac{{\\sf B}+{\\sf H}}{\\varepsilon^{2}(1-\\gamma)^{2}}\\log\\left(\\frac{S A}{(1-\\gamma)\\delta\\varepsilon}\\right)}\\end{array}$ \u53e3 ", "page_idx": 39}, {"type": "text", "text": "B.3 Proof of Theorem 8 (General Average-Reward MDP Bounds) ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "In this section, we prove our main result on the sample complexity of general average-reward MDPs. ", "page_idx": 40}, {"type": "text", "text": "Proof of Theorem 8. We can combine our bound for discounted MDPs, Theorem 7, with our reduction from average-reward MDPs to discounted MDPs, Theorem 6. ", "page_idx": 40}, {"type": "text", "text": "Using Theorem 7 with target ccuracy $\\mathsf{B}+\\mathsf{H}$ and discoun factor $\\begin{array}{r}{\\overline{{\\gamma}}=1-\\frac{\\varepsilon}{12(\\mathsf{B}+\\mathsf{H})}}\\end{array}$ , we obain a $(\\mathsf{B}+\\mathsf{H})$ -optimal policy for the discounted MDP $(P,r,\\overline{{\\gamma}})$ with probability at least $1-\\delta$ as long as ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{n\\geq C_{3}\\frac{\\mathsf{B}+\\mathsf{H}}{(1-\\overline{{\\gamma}})^{2}(\\mathsf{B}+\\mathsf{H})^{2}}\\log\\left(\\frac{S A}{(1-\\overline{{\\gamma}})\\delta\\varepsilon}\\right)}}\\\\ {\\displaystyle{=12^{2}C_{3}\\frac{\\mathsf{B}+\\mathsf{H}}{(\\mathsf{B}+\\mathsf{H})^{2}}\\frac{(\\mathsf{B}+\\mathsf{H})^{2}}{\\varepsilon^{2}}\\log\\left(\\frac{12(\\mathsf{B}+\\mathsf{H})}{\\varepsilon}\\frac{S A}{\\delta\\varepsilon}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "which is satisied when $\\begin{array}{r}{n\\geq C_{4}\\frac{\\mathsf{B}+\\mathsf{H}}{\\varepsilon^{2}}\\log\\left(\\frac{S A(\\mathsf{B}+\\mathsf{H})}{\\delta\\varepsilon}\\right)}\\end{array}$ (SA(B+H))for sufciently large C4. ", "page_idx": 40}, {"type": "text", "text": "Applying Theorem 6 (with error parameter ${\\frac{\\varepsilon}{12}}\\,,$ ), we obtain ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\rho^{\\star}-\\rho^{\\widehat\\pi^{\\star}}\\leq\\left(3+2\\frac{\\mathsf{B}+\\mathsf{H}}{\\mathsf{B}+\\mathsf{H}}\\right)\\frac{\\varepsilon}{12}\\leq\\varepsilon\\mathbf{1}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "as desired. ", "page_idx": 40}, {"type": "text", "text": "B.4 Proof of Theorems 4 and 5 (Lower Bounds) ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "In this section, we prove our minimax lower bounds on the sample complexity of general averagereward MDPs (Theorem 4) and discounted MDPs (Theorem 5). ", "page_idx": 40}, {"type": "text", "text": "Proof of Theorem 4. First consider the MDP instances $\\mathcal{M}_{a^{\\star}}$ indexed by $a^{\\star}\\,\\in\\,\\{1,\\ldots,A\\}$ shown in Figure 3. In all instances, states 2, 3 and 4 are absorbing states, and state 1 is a transient state. State 1 has $A$ actions and is the only state with multiple actions. At state 1, taking action $a=1$ will take the agent to state 4 deterministically; taking action 2 will take the agent back to state 1 with probability $\\begin{array}{r}{P(1|1,2)=1-\\frac{1}{T}}\\end{array}$ to state 2 with probability $P(2|1,2)$ , and to state 3 with probability $P(3|1,2)\\,=\\,1\\,-\\,P(1|1,2)\\,\\stackrel{\\textstyle.}{-}\\,P(2|1,2)$ . The instances differ only in the values of $P(2|1,a)$ and $P(3|1,a)$ , which are shown in Figure 3 along with the reward $R$ for each state-action pair. ", "page_idx": 40}, {"type": "text", "text": "For the MDP instance $\\mathcal{M}_{1}$ , the optimal policy is taking action $a=1$ at state 1, leading to an average reward of $1/2$ inanylaad $\\textstyle{\\frac{1-2\\varepsilon}{2}}$ Similaly.for the instance $\\mathcal{M}_{a^{\\star}}$ with , the optimal action is with average reward $\\textstyle{\\frac{1+2\\varepsilon}{2}}$ \uff0c the action $a=1$ has average reward $\\frac{1}{2}$ , and all other actions have average reward $\\frac{1-2\\varepsilon}{2}$ . By direct calculation, we find that the span of the optimal poliey is $\\Vert h^{\\star}\\Vert_{\\mathrm{span}}=0$ in all instances. Moreover, by taking any action $a\\ne1$ , the agent will stay in state 1 for $B$ steps in expectation before transitioning to state 2 or 3, so the bounded transient time is satisfied with parameter $B$ ", "page_idx": 40}, {"type": "text", "text": "We next define $(A{-}1)S/4$ master MDPs $\\overline{{\\mathcal{M}}}_{s^{\\star},a^{\\star}}$ indexed by $s^{\\star}\\in\\{1,\\ldots,S/4\\}$ and $a^{\\star}\\in\\{2,\\ldots,A\\}$ as follows. Each master MDP $\\overline{{\\mathcal{M}}}_{s^{\\star},a^{\\star}}$ has $S/4$ copies of sub-MDPs such that the $s^{\\star}$ th sub-MDP is equal to $\\mathcal{M}_{a^{\\star}}$ and all other sub-MDPs are equal to $\\mathcal{M}_{1}$ . We rename the states so that the states of the $s$ th sub-MDP has states $4s+1,4s+2,4s+3,4s+4$ corresponding to states $1,2,3,4$ of the instances shown in Figure 3. Note each of these master MDPs has $S$ states and $A$ actions, satisfies the bounded transient time property with parameter $B$ , and has the span of the bias of its Blackwell optimal policy equal to O. Note that for a given policy $\\pi$ to be $\\varepsilon/3$ -average optimal in master MDP $\\overline{{\\mathcal{M}}}_{s^{\\star},a^{\\star}}$ , it must take action $a^{\\star}$ in state $4s^{\\star}+1$ with probability at least $2/3$ , and it must take action 1 in states $4s+1$ for $s\\in\\{1,\\cdot\\cdot\\cdot,S/4\\}\\setminus\\{s^{\\star}\\}$ with probability at least $2/3$ ", "page_idx": 40}, {"type": "text", "text": "Thus, for an algorithm ${\\tt A l g}$ to output an $\\varepsilon/3$ -average optimal policy $\\pi$ , it must identify the master MDP instance $\\overline{{\\mathcal{M}}}_{s^{\\star},a^{\\star}}$ (equivalently, the values of $s^{\\star}$ and $a^{\\star}$ ), in the sense that there must be exactly one state $4s+1$ where an action $a\\ne1$ is taken with probability $\\geq2/3$ . Therefore it suffices to lower bound the failure probability of any algorithm ${\\tt A l g}$ for this $(A-1)S/4$ -way testing problem. By construction, for any two distinct index pairs $(s_{1}^{\\star},a_{1}^{\\star})$ and $(s_{2}^{\\star},a_{2}^{\\star})$ , the master MDPs $\\overline{{\\mathcal{M}}}_{s_{1}^{\\star},a_{1}^{\\star}}$ and $\\overline{{\\mathcal{M}}}_{s_{2}^{\\star},a_{2}^{\\star}}$ differonlyin the state-actiopas $(4s_{1}^{\\star},a_{1}^{\\star})$ and $(4s_{2}^{\\star},a_{2}^{\\star})$ , and we have ", "page_idx": 40}, {"type": "image", "img_path": "pGEY8JQ3qx/tmp/2a5a02e00d181852edf458f368ae3f41f78dce0f77695c9c9ae1df638176c14c.jpg", "img_caption": ["Instance $\\mathcal{M}_{1}$ "], "img_footnote": [], "page_idx": 41}, {"type": "image", "img_path": "pGEY8JQ3qx/tmp/7237d07e1495d12f0e00c26772b241f43d4f3193ed5964d9c95c6a07b36bc428.jpg", "img_caption": ["Figure 3: MDP Instances Used in the Proof of Lower Bound in Theorem 4 ", "Instance $\\mathcal{M}_{a^{\\star}}$ , for $a^{\\star}\\in\\{2,\\ldots,A\\}$ "], "img_footnote": [], "page_idx": 41}, {"type": "text", "text": "", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P_{\\overline{{\\mathcal{M}}}_{s_{1}^{\\star},a_{1}^{\\star}}}(\\cdot\\mid4s_{1}^{\\star},a_{1}^{\\star})=\\mathrm{Cat}\\left(1-\\displaystyle\\frac{1}{B},\\frac{1-2\\varepsilon}{2B},\\frac{1+2\\varepsilon}{2B}\\right)=:Q_{1},}\\\\ {P_{\\overline{{\\mathcal{M}}}_{s_{2}^{\\star},a_{2}^{\\star}}}(\\cdot\\mid4s_{1}^{\\star},a_{1}^{\\star})=\\mathrm{Cat}\\left(1-\\displaystyle\\frac{1}{B},\\frac{1+2\\varepsilon}{2B},\\frac{1-2\\varepsilon}{2B}\\right)=:Q_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $\\mathrm{Cat}(p_{1},p_{2},p_{3})$ denotes the categorical distribution with event probabilities $p_{i}$ 's (and vice versa for the distributions of the state action pair $(4s_{2}^{\\star},a_{2}^{\\star}))$ ", "page_idx": 41}, {"type": "text", "text": "Now we use Fano's method [18] to lower bound this failure probability. Choose an index $J$ uniformly at random from the set $\\mathcal{J}:=\\{1,...\\,,S/4\\}\\times\\{2,...\\,,A\\}$ and suppose that we draw $n$ id samples $\\boldsymbol{X}=(X_{1},\\ldots,X_{n})$ from the master MDP ${\\overline{{\\mathcal{M}}}}_{J}$ ; note that under the generative model, each random variable $X_{i}$ represents an $(S\\times A)$ -by- $S$ transition matrix with exactly one nonzero entry in each row. Letting $\\operatorname{I}(J;X)$ denote the mutual information between $J$ and $X$ , Fano's inequality yields that the failure probability is lower bounded by ", "page_idx": 41}, {"type": "equation", "text": "$$\n1-\\frac{\\operatorname{I}(J;X)+\\log2}{\\log((A-1)S/4)}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We can calculate using the fact that the $P_{i}$ 's are i.i.d., the chain rule of mutual information, and the form of the construction that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname{I}(J;X)=n\\mathrm{I}(J;X_{1})}\\\\ &{\\qquad\\quad\\leq n\\operatorname*{max}_{(s_{1}^{\\star},a_{1}^{\\star}),(s_{2}^{\\star},a_{2}^{\\star})\\in\\mathcal{I}:}\\operatorname{D}_{\\mathrm{KL}}\\left(P_{\\overline{{\\mathcal{M}}}_{s_{1}^{\\star},a_{1}^{\\star}}}\\,\\big|\\,P_{\\overline{{\\mathcal{M}}}_{s_{2}^{\\star},a_{2}^{\\star}}}\\right)}\\\\ &{\\qquad\\qquad\\quad(s_{1}^{\\star},a_{1}^{\\star})\\overline{{\\varkappa}}(s_{2}^{\\star},a_{2}^{\\star})}\\\\ &{\\qquad\\quad=n\\big(\\mathrm{D}_{\\mathrm{KL}}(Q_{1}\\,|\\,Q_{2})+\\mathrm{D}_{\\mathrm{KL}}(Q_{2}\\,|\\,Q_{1})\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "By direct calculation, we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{D}_{\\mathrm{KL}}(Q_{1}|Q_{2})=\\cfrac{1-2\\varepsilon}{2B}\\log\\cfrac{1-2\\varepsilon}{1+2\\varepsilon}+\\cfrac{1+2\\varepsilon}{2B}\\log\\cfrac{1+2\\varepsilon}{1-2\\varepsilon}}\\\\ &{\\qquad\\qquad\\leq\\cfrac{1-2\\varepsilon}{2B}\\cdot\\cfrac{-4\\varepsilon}{1+2\\varepsilon}+\\cfrac{1+2\\varepsilon}{2B}\\cdot\\cfrac{4\\varepsilon}{1-2\\varepsilon}\\qquad\\qquad\\log(1+x)\\leq x,\\forall x>-1}\\\\ &{\\qquad\\qquad=\\cfrac{16\\varepsilon^{2}}{B(1+2\\varepsilon)(1-2\\varepsilon)}}\\\\ &{\\qquad\\qquad\\leq\\cfrac{32\\varepsilon^{2}}{B}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Also note that $\\mathsf{D}_{\\mathrm{KL}}(Q_{2}|Q_{1})=\\mathsf{D}_{\\mathrm{KL}}(Q_{1}|Q_{2})$ in this case. Therefore the failure probability is at least ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{1-\\frac{\\operatorname{I}(J;P^{n})+\\log2}{\\log((A-1)S/4)}\\geq1-\\frac{n\\frac{64\\varepsilon^{2}}{B}+\\log2}{\\log((A-1)S/4)}}}\\\\ &{}&{\\geq\\frac{1}{2}-\\frac{n\\frac{64\\varepsilon^{2}}{B}}{\\log((A-1)S/4)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where in the second inequality we assumed $A$ and $S$ are at least a sufficiently large constant. For the above RHS to be smalle than $1/4$ , we therefore require $n\\geq\\Omega{\\bigl(}{\\frac{B\\log(S A)}{\\varepsilon^{2}}}{\\bigr)}$ \u53e3 ", "page_idx": 42}, {"type": "text", "text": "Proof of Theorem 5. The desired DMDP lower bound follows from combining our AMDP lower bound Theorem 4 with the average-to-discount reduction in Theorem 6. \u53e3 ", "page_idx": 42}, {"type": "text", "text": "B.5   Relationship between transient time and mixing time ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Lemma 27. In any uniformly mixing MDP, we have $\\mathsf{B}\\leq4\\tau_{\\mathrm{unif}}$ ", "page_idx": 42}, {"type": "text", "text": "Proof. Fix a deterministic stationary policy $\\pi$ . Notice that since all states in the support of the stationary distribution $\\nu_{\\pi}$ are recurrent, for any $s\\in S$ wehave ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{P}_{s}^{\\pi}\\left(S_{t}\\mathrm{~is~transient}\\right)=\\sum_{s^{\\prime}\\in T^{\\pi}}\\mathbb{P}_{s}^{\\pi}\\left(S_{t}=s^{\\prime}\\right)}}\\\\ &{\\le\\sum_{s^{\\prime}\\in T^{\\pi}}\\mathbb{P}_{s}^{\\pi}\\left(S_{t}=s^{\\prime}\\right)+\\sum_{s^{\\prime}\\in\\mathcal{R}^{\\pi}}\\big|\\mathbb{P}_{s}^{\\pi}\\left(S_{t}=s^{\\prime}\\right)-\\nu^{\\pi}(s^{\\prime})\\big|}\\\\ &{=\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}}\\big|\\mathbb{P}_{s}^{\\pi}\\left(S_{t}=s^{\\prime}\\right)-\\nu^{\\pi}(s^{\\prime})\\big|}\\\\ &{\\le2\\operatorname*{max}\\frac{1}{s}\\left\\|e_{s}^{\\top}P_{\\pi}^{t}-\\nu^{\\pi}\\right\\|_{1}}\\\\ &{\\le2\\cdot2^{-|t^{\\prime}|\\tau_{\\operatorname*{min}}}\\mathrm{.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where the final inequality uses standard properties of mixing [11, Chapter 4]. Now define $T=\\operatorname*{inf}\\{t:$ $S_{t}\\in\\mathcal{R}^{\\pi}\\}$ . Then, using a standard formula for the expectation of nonnegative-integer-values random ", "page_idx": 42}, {"type": "text", "text": "variables, we have for any $s\\in S$ that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}_{s}^{\\pi}\\left[T\\right]=\\sum_{t=0}^{\\infty}\\displaystyle\\mathbb{P}_{s}^{\\pi}(T>t)}\\\\ {\\displaystyle=\\sum_{t=0}^{\\infty}\\mathbb{P}_{s}^{\\pi}\\left(S_{t}\\mathrm{~is~transient}\\right)}\\\\ {\\displaystyle\\le2\\sum_{t=0}^{\\infty}2^{-|t/\\tau_{\\mathrm{mif}}|}}\\\\ {\\displaystyle=2\\sum_{\\ell=0}^{\\infty}\\tau_{\\mathrm{mif}}2^{-\\ell}}\\\\ {\\displaystyle=4\\tau_{\\mathrm{mif}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Since this bound holds for all $s\\in S$ and all deterministic stationary policies $\\pi$ , we conclude that B\u2264 4Tunif\u00b7 \u53e3 ", "page_idx": 43}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: All claims made in the abstract and introduction match the theoretical results provided in the main results Sections 3 and 4. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 44}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Justification: The conclusion (Section 5) mentions the main limitation, of the necessity of knowledge of H/B for the optimal average-reward complexity results to hold, and this point is elaborated upon in Section 3. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 44}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: All assumptions are provided with their respective theorems and within the problem setup Section 2, and formal proofs of all results are provided in Appendices A and B. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 45}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 45}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: The paper does not include experiments. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 46}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 46}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 46}, {"type": "text", "text": "", "page_idx": 47}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 47}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: Our research does not involve any human subjects or datasets, and as a foundational theoretical paper it does not have any direct potentially harmful societal consequences. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 47}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: Our work is foundational research on the sample complexity of average-reward and discounted MDPs, and thus is not directly tied to any negative applications. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed. \u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 47}, {"type": "text", "text": "\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 48}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: This paper does not provide any data nor models. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 48}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: The paper does not use any code, model, nor data assets. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 48}, {"type": "text", "text": "", "page_idx": 49}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 49}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 49}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 49}]