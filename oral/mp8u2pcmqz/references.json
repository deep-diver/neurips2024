{"references": [{"fullname_first_author": "Song Han", "paper_title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "publication_date": "2015-10-01", "reason": "This paper is foundational to the field of neural network quantization, introducing techniques that are relevant to the current research on quantizing LLMs."}, {"fullname_first_author": "Elias Frantar", "paper_title": "GPTQ: Accurate post-training quantization for generative pre-trained transformers", "publication_date": "2022-10-26", "reason": "This paper introduced GPTQ, a highly effective post-training quantization method for LLMs, which is a common baseline in many subsequent works and a major point of comparison in this paper."}, {"fullname_first_author": "Guangxuan Xiao", "paper_title": "SmoothQuant: Accurate and efficient post-training quantization for large language models", "publication_date": "2023-07-01", "reason": "This paper introduced SmoothQuant, a state-of-the-art post-training quantization method that directly addresses activation outliers, which are the primary focus of this paper's proposed method."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "LLaMA: Open and efficient foundation language models", "publication_date": "2023-02-24", "reason": "This paper introduced the LLaMA family of LLMs, which are used extensively as the experimental subject in this paper to test the performance of the proposed quantization method."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "publication_date": "2020-01-01", "reason": "This paper introduced the MMLU benchmark, a widely used and important evaluation metric for LLMs, which is used in this paper to evaluate the performance of the quantized models."}]}