[{"figure_path": "mp8u2Pcmqz/tables/tables_6_1.jpg", "caption": "Table 1: Perplexity (\u2193) results under 4-bit weight-activation quantization. The results for W6A6 can be found in Table D8. Atom and OmniQuant unprocessed group-query attention for LLaMA2-70B.", "description": "This table presents the perplexity scores achieved by different quantization methods (SmoothQuant, OmniQuant, AffineQuant, QLLM, Atom, DuQuant, and DuQuant+LWC) on the WikiText2 and C4 datasets using 4-bit weight-activation quantization.  Lower perplexity scores indicate better performance. The table compares results across various sizes of LLMs (1-7B to 2-70B parameters).  Note that Atom and OmniQuant did not process group-query attention for the LLaMA2-70B model.  The results for 6-bit weight-activation quantization (W6A6) are available in Table D8 in the appendix.", "section": "4 Experiment"}, {"figure_path": "mp8u2Pcmqz/tables/tables_6_2.jpg", "caption": "Table 2: Zero-shot QA (\u2191) results of LLaMA1 models under 4-bit weight-activation quantization. The results for LLaMA2 models and W6A6 quantization can be found in Table D1 D9, and D10.", "description": "This table presents the zero-shot results for several question answering tasks using the LLaMA1 model with 4-bit weight-activation quantization.  It shows the performance of different quantization methods (FP16, SmoothQuant, OS+, OmniQuant, AffineQuant, QLLM, Atom, DuQuant, and DuQuant+LWC) across various datasets (PIQA, ARC-E, ARC-C, BoolQ, HellaSwag, and WinoGrande).  The table highlights the performance improvements achieved by DuQuant compared to the baselines.", "section": "4 Experiment"}, {"figure_path": "mp8u2Pcmqz/tables/tables_7_1.jpg", "caption": "Table 3: Zero-shot and five-shot results on the MMLU benchmark for Vicuna-v1.5-13B under 4-bit weight-activation quantization. The results for Vicuna-v1.5-7b can be found in Table D2.", "description": "This table shows the zero-shot and five-shot performance of the Vicuna-v1.5-13B language model on the MMLU benchmark after applying 4-bit weight-activation quantization using the DuQuant method.  It compares the results to several baselines (SmoothQuant, OmniQuant, Atom), showing the effectiveness of DuQuant on this instruction-tuned model.  The results are broken down by category (STEM, Hums, Social, Others) for both zero-shot and five-shot settings.", "section": "4.1 Main Results"}, {"figure_path": "mp8u2Pcmqz/tables/tables_7_2.jpg", "caption": "Table 4: Long-context generation results for 4-bit Vicuna models on the LongBench benchmark.", "description": "This table presents the results of long-context generation experiments using 4-bit quantized Vicuna models.  It shows the performance of different quantization methods (SmoothQuant, OmniQuant, Atom, and DuQuant) compared to the full-precision (FP16) model on various long-context generation tasks from the LongBench benchmark.  The tasks cover different aspects of long-form text generation, including question answering, summarization, and code generation. The scores for each task provide a comprehensive evaluation of the models' abilities to generate high-quality text in long-context scenarios.", "section": "4 Experiment"}, {"figure_path": "mp8u2Pcmqz/tables/tables_7_3.jpg", "caption": "Table 1: Perplexity (\u2193) results under 4-bit weight-activation quantization. The results for W6A6 can be found in Table D8. Atom and OmniQuant unprocessed group-query attention for LLaMA2-70B.", "description": "This table presents the perplexity scores achieved by different quantization methods on the WikiText2 and C4 datasets using 4-bit weight and activation quantization. Lower perplexity values indicate better performance.  The table compares DuQuant and DuQuant+LWC against several baselines (SmoothQuant, OmniQuant, AffineQuant, QLLM, Atom) across various LLM sizes (1-7B, 1-13B, 1-30B, 1-65B, 2-7B, 2-13B, 2-70B).  Note that Atom and OmniQuant results are incomplete for the LLaMA2-70B model.", "section": "4 Experiment"}, {"figure_path": "mp8u2Pcmqz/tables/tables_8_1.jpg", "caption": "Table 6: Influence of different components in DuQuant under 4-bit weight-activation quantization.", "description": "This table presents the ablation study of different components in the DuQuant model. By removing or adding different components (smooth, rotation 1, permutation, rotation 2), the table shows the effect of each component on the final performance (WikiText2 and C4 perplexity) of the model using 4-bit weight-activation quantization.  It demonstrates the incremental improvement of the model's performance by adding these components.", "section": "4.2 Ablation Study"}, {"figure_path": "mp8u2Pcmqz/tables/tables_8_2.jpg", "caption": "Table 7: Outliers impact on quantization. We only apply the smooth technique on Normal and Massive outliers for W4A4 quantization.", "description": "This table presents the results of an ablation study evaluating the impact of different outlier types (Normal and Massive) on quantization performance when only using the smoothing technique.  It shows the perplexity scores (lower is better) on the WikiText2 and C4 datasets for LLaMA2-7B and LLaMA2-13B models under different outlier handling scenarios.  The results highlight that Massive outliers have a significantly more negative impact on quantization accuracy than Normal outliers.", "section": "4.2 Ablation Study"}, {"figure_path": "mp8u2Pcmqz/tables/tables_8_3.jpg", "caption": "Table 1: Perplexity (\u2193) results under 4-bit weight-activation quantization. The results for W6A6 can be found in Table D8. Atom and OmniQuant unprocessed group-query attention for LLaMA2-70B.", "description": "This table presents the perplexity scores achieved by different quantization methods (SmoothQuant, OmniQuant, AffineQuant, QLLM, Atom, DuQuant, and DuQuant+LWC) on the WikiText2 and C4 datasets using 4-bit weight-activation quantization.  Lower perplexity scores indicate better performance.  The table compares results across various LLM sizes (1-7B, 1-13B, 1-30B, 1-65B, 2-7B, 2-13B, 2-70B), providing a comprehensive evaluation of each method's effectiveness in handling low-bit quantization.", "section": "4.1 Main Results"}, {"figure_path": "mp8u2Pcmqz/tables/tables_9_1.jpg", "caption": "Table 9: Layer-wise speedup during pre-filling stage for 4-bit weight-activation quantization.", "description": "This table presents the layer-wise speedup achieved by DuQuant during the pre-filling stage for 4-bit weight-activation quantization.  It shows the speedup factor obtained for different batch sizes (1, 4, and 16) on two different models, LLaMA2-7B and LLaMA2-13B. The results highlight the significant performance improvement gained by using DuQuant during the pre-filling phase of LLM inference.", "section": "4 Experiment"}, {"figure_path": "mp8u2Pcmqz/tables/tables_9_2.jpg", "caption": "Table 1: Perplexity (\u2193) results under 4-bit weight-activation quantization. The results for W6A6 can be found in Table D8. Atom and OmniQuant unprocessed group-query attention for LLaMA2-70B.", "description": "This table presents the perplexity scores achieved by different methods for quantizing LLMs using 4-bit weight-activation quantization.  Lower perplexity indicates better performance. The table compares DuQuant and DuQuant+LWC against several state-of-the-art baseline methods across various LLM sizes (7B, 13B, 30B, 65B) from LLaMA and LLaMA2.  Results are shown for WikiText2 and C4 datasets. Note that Atom and OmniQuant did not process group-query attention for LLaMA2-70B.", "section": "4 Experiment"}, {"figure_path": "mp8u2Pcmqz/tables/tables_9_3.jpg", "caption": "Table 12: Quantization runtime on one NVIDIA A100.", "description": "This table presents the runtime comparison of different quantization methods (OmniQuant, AffineQuant, QLLM, Atom, and DuQuant) for three different LLaMA2 models (7B, 13B, and 70B) on a single NVIDIA A100 GPU.  The results highlight DuQuant's significant speed advantage over other methods, showing its efficiency in the quantization process.", "section": "4 Experiment"}, {"figure_path": "mp8u2Pcmqz/tables/tables_17_1.jpg", "caption": "Table 1: Perplexity (\u2193) results under 4-bit weight-activation quantization. The results for W6A6 can be found in Table D8. Atom and OmniQuant unprocessed group-query attention for LLaMA2-70B.", "description": "This table presents the perplexity scores achieved by different quantization methods (SmoothQuant, OmniQuant, AffineQuant, QLLM, Atom, DuQuant, and DuQuant+LWC) on the WikiText2 and C4 datasets, using 4-bit weight-activation quantization.  Lower perplexity indicates better performance.  The table compares results across different sizes of LLaMA and LLaMA2 language models.  DuQuant+LWC represents DuQuant with learnable weight clipping.", "section": "4.1 Main Results"}, {"figure_path": "mp8u2Pcmqz/tables/tables_18_1.jpg", "caption": "Table 3: Zero-shot and five-shot results on the MMLU benchmark for Vicuna-v1.5-13B under 4-bit weight-activation quantization. The results for Vicuna-v1.5-7b can be found in Table D2.", "description": "This table presents the zero-shot and five-shot results of the Vicuna-v1.5-13B model on the MMLU benchmark using 4-bit weight-activation quantization.  It compares the performance of different quantization methods (FP16, SmoothQuant, OmniQuant, Atom, DuQuant, and DuQuant+LWC) across different subcategories of the MMLU benchmark (STEM, Hums, Social, Others) and provides the average performance across all subcategories. The table shows that DuQuant achieves competitive results compared to the full precision (FP16) model, particularly in the five-shot setting.", "section": "4.1 Main Results"}, {"figure_path": "mp8u2Pcmqz/tables/tables_18_2.jpg", "caption": "Table 2: Zero-shot QA (\u2191) results of LLaMA1 models under 4-bit weight-activation quantization. The results for LLaMA2 models and W6A6 quantization can be found in Table D1 D9, and D10.", "description": "This table presents the zero-shot results on several question answering datasets for different sizes of LLaMA1 models using 4-bit weight and activation quantization.  It compares the performance of DuQuant against other state-of-the-art quantization methods (SmoothQuant, OS+, OmniQuant, AffineQuant, QLLM, Atom). The table shows the accuracy scores for each model on different datasets (PIQA, ARC-E, ARC-C, BoolQ, HellaSwag, WinoGrande) and the average accuracy across all datasets.", "section": "4 Experiment"}, {"figure_path": "mp8u2Pcmqz/tables/tables_18_3.jpg", "caption": "Table 2: Zero-shot QA (\u2191) results of LLaMA1 models under 4-bit weight-activation quantization. The results for LLaMA2 models and W6A6 quantization can be found in Table D1 D9, and D10.", "description": "This table presents the zero-shot results for several common sense question answering tasks on the LLaMA1 model with 4-bit weight-activation quantization.  It shows the performance of different quantization methods (SmoothQuant, OS+, OmniQuant, AffineQuant, QLLM, Atom, DuQuant, and DuQuant+LWC) compared to the full precision (FP16) baseline.  The results are given for different model sizes (7B, 13B, 30B, and 65B parameters).  Additional results for LLaMA2 models and 6-bit quantization are available in the supplementary materials.", "section": "4 Experiment"}, {"figure_path": "mp8u2Pcmqz/tables/tables_18_4.jpg", "caption": "Table 3: Zero-shot and five-shot results on the MMLU benchmark for Vicuna-v1.5-13B under 4-bit weight-activation quantization. The results for Vicuna-v1.5-7b can be found in Table D2.", "description": "This table shows the zero-shot and five-shot results of the Vicuna-v1.5-13B model on the MMLU benchmark under 4-bit weight-activation quantization.  It compares the performance of different quantization methods (FP16, SmoothQuant, OmniQuant, Atom, DuQuant, DuQuant+LWC) across various sub-categories of the MMLU benchmark (STEM, Hums, Social, Others).  The results highlight the relative performance gains of DuQuant compared to other state-of-the-art quantization techniques.", "section": "4.1 Main Results"}, {"figure_path": "mp8u2Pcmqz/tables/tables_19_1.jpg", "caption": "Table D6: Perplexity results of Mistral-7B and Phi2-2.8B under 4-bit weight-activation quantization.", "description": "This table shows the perplexity results on WikiText2 and C4 datasets for Mistral-7B and Phi2-2.8B models under 4-bit weight-activation quantization.  It compares the performance of several different quantization methods (FP16, RTN, SmoothQuant, OmniQuant, Atom, and DuQuant) to highlight the effectiveness of the DuQuant method, particularly in handling the challenges posed by massive outliers present in these models.", "section": "4.1 Main Results"}, {"figure_path": "mp8u2Pcmqz/tables/tables_19_2.jpg", "caption": "Table 1: Perplexity (\u2193) results under 4-bit weight-activation quantization. The results for W6A6 can be found in Table D8. Atom and OmniQuant unprocessed group-query attention for LLaMA2-70B.", "description": "This table presents the perplexity scores achieved by various LLMs using different quantization methods. Lower perplexity values indicate better performance.  The table compares the performance of DuQuant against several state-of-the-art baseline methods for 4-bit weight-activation quantization across different sizes of LLMs. Results are shown for WikiText2 and C4 datasets. Note that Atom and OmniQuant did not process the group-query attention for LLaMA2-70B.", "section": "4 Experiment"}, {"figure_path": "mp8u2Pcmqz/tables/tables_19_3.jpg", "caption": "Table 1: Perplexity (\u2193) results under 4-bit weight-activation quantization. The results for W6A6 can be found in Table D8. Atom and OmniQuant unprocessed group-query attention for LLaMA2-70B.", "description": "This table presents the perplexity scores achieved by different quantization methods (SmoothQuant, OmniQuant, AffineQuant, QLLM, Atom, DuQuant, and DuQuant+LWC) on the WikiText2 and C4 datasets using 4-bit weight-activation quantization.  Lower perplexity scores indicate better performance.  The table compares these methods against a floating-point (FP16) baseline across various sizes of LLaMA and LLaMA2 models.  Note that Atom and OmniQuant did not process group-query attention for LLaMA2-70B, and the W6A6 results are in Table D8.", "section": "4 Experiment"}, {"figure_path": "mp8u2Pcmqz/tables/tables_20_1.jpg", "caption": "Table 2: Zero-shot QA (\u2191) results of LLaMA1 models under 4-bit weight-activation quantization. The results for LLaMA2 models and W6A6 quantization can be found in Table D1 D9, and D10.", "description": "This table shows the zero-shot results of the LLaMA1 model using 4-bit weight-activation quantization on several question answering tasks.  It compares the performance of different quantization methods (SmoothQuant, OS+, OmniQuant, AffineQuant, QLLM, Atom, DuQuant, and DuQuant+LWC) against the full precision floating point model (FP16). The results are presented as the accuracy achieved on each task (PIQA, ARC-E, ARC-C, BoolQ, HellaSwag, WinoGrande), and an average accuracy across all tasks.  The table also indicates that similar results for LLaMA2 models and using 6-bit quantization can be found in other tables within the appendix.", "section": "4 Experiment"}, {"figure_path": "mp8u2Pcmqz/tables/tables_20_2.jpg", "caption": "Table 2: Zero-shot QA (\u2191) results of LLaMA1 models under 4-bit weight-activation quantization. The results for LLaMA2 models and W6A6 quantization can be found in Table D1 D9, and D10.", "description": "This table presents the results of zero-shot question answering (QA) experiments conducted on several LLaMA1 models using 4-bit weight-activation quantization.  It compares the performance of different quantization methods (FP16, SmoothQuant, OS+, OmniQuant, AffineQuant, QLLM, Atom, DuQuant, and DuQuant+LWC) across six different QA datasets (PIQA, ARC-E, ARC-C, BoolQ, HellaSwag, and WinoGrande).  The table shows the average accuracy across all datasets for each method and model.  Additional results for LLaMA2 models and using 6-bit quantization are available in supplementary tables.", "section": "4 Experiment"}, {"figure_path": "mp8u2Pcmqz/tables/tables_21_1.jpg", "caption": "Table E11: End-to-end pre-filling speedup on LLaMA2-7B model.", "description": "This table presents the end-to-end pre-filling speedup results on the LLaMA2-7B model.  It shows the time taken for pre-filling using FP16 and DuQuant at different batch sizes (1, 2, and 3). The speedup is calculated as the ratio of FP16 time to DuQuant time for each batch size.  The results demonstrate the efficiency gains achieved by DuQuant in the pre-filling phase of LLM inference.", "section": "E.1 Time Speedup and Memory Saving"}, {"figure_path": "mp8u2Pcmqz/tables/tables_21_2.jpg", "caption": "Table E12: Peak memory usage during pre-filling phase of LLaMA2-7B model.", "description": "This table shows the peak memory usage (in GB) for the LLaMA2-7B model during the pre-filling phase under different batch sizes (1, 2, and 3). It compares the memory usage of the FP16 model with the DuQuant quantized model.  The \"Saving Factor\" column indicates the reduction in memory usage achieved by DuQuant compared to FP16 for each batch size.  The results highlight the significant memory savings offered by DuQuant, particularly at smaller batch sizes.", "section": "E.1 Time Speedup and Memory Saving"}, {"figure_path": "mp8u2Pcmqz/tables/tables_21_3.jpg", "caption": "Table E13: Decoding phase results of one LLaMA2-7B layer with a batch size of 64.", "description": "This table presents the results of a decoding phase experiment on a single LLaMA2-7B layer using a batch size of 64. It compares the time taken and memory usage of different quantization methods: FP16 (full precision), SmoothQuant, QLLM, QuaRot, and DuQuant. The time is measured in milliseconds (ms), and the memory is in gigabytes (GB).  The table also shows the saving factor for time and memory usage compared to the FP16 baseline.  The OOM (Out Of Memory) entry for QLLM indicates that this method exceeded the available memory.  The results illustrate the relative efficiency of different quantization approaches during the decoding phase of LLM inference.", "section": "E.1 Time Speedup and Memory Saving"}, {"figure_path": "mp8u2Pcmqz/tables/tables_22_1.jpg", "caption": "Table E14: Impact of rotation block size.", "description": "This table presents the results of an ablation study on the impact of rotation block size on the performance of the quantized models. The experiment was conducted on LLaMA2-7B and LLaMA2-13B models. The table shows that increasing block size generally improves model performance, likely due to more efficient transformations during the reshaping of original activation/weight matrices.  The perplexity on WikiText2 and C4 datasets, and the runtime are shown for different block sizes (4, 8, 16, 32, 64, 128).", "section": "E.2 Effects of Rotation Matrix"}, {"figure_path": "mp8u2Pcmqz/tables/tables_22_2.jpg", "caption": "Table E15: Impact of rotation times.", "description": "This table presents the results of an ablation study on the number of rotation times used in the DuQuant method. The study was conducted on LLaMA2-7B and LLaMA2-13B models using different rotation times (1, 4, 16, 64, 256, 1024). The table shows the perplexity on WikiText2 and C4 datasets, as well as the time taken for each setting. The results indicate that increasing the number of rotations initially improves performance, but excessive rotations can lead to overfitting.", "section": "E.2 Effects of Rotation Matrix"}, {"figure_path": "mp8u2Pcmqz/tables/tables_22_3.jpg", "caption": "Table E16: Impact of channel permutation algorithm.", "description": "This table presents a comparison of different permutation algorithms used in the DuQuant method.  It shows the WikiText2 and C4 perplexity scores, the variance of activation magnitudes across blocks, and the computation time for each algorithm (w.o. Permutation, Random, Simulated Annealing, Zigzag). The results demonstrate the effectiveness of the Zigzag permutation in reducing variance while maintaining computational efficiency.", "section": "E.3 Effects of Permutation Algorithm"}, {"figure_path": "mp8u2Pcmqz/tables/tables_23_1.jpg", "caption": "Table E18: Calibration-free quantization, where we generate random data within vocabulary range.", "description": "This table presents the results of applying the DuQuant method with randomly generated calibration data instead of using actual data from WikiText2 and C4 datasets. This tests the robustness of DuQuant against varying calibration settings, demonstrating the method's adaptability and performance even without specific calibration data.", "section": "E.4 Effects of Calibration Datasets"}, {"figure_path": "mp8u2Pcmqz/tables/tables_23_2.jpg", "caption": "Table E18: Calibration-free quantization, where we generate random data within vocabulary range.", "description": "This table presents the results of applying the DuQuant method to the LLaMA2-7B and LLaMA2-13B models using randomly generated calibration data instead of data from WikiText2.  It demonstrates the robustness of DuQuant, showing that it achieves comparable performance even without using specific calibration data.", "section": "E.4 Effects of Calibration Datasets"}, {"figure_path": "mp8u2Pcmqz/tables/tables_23_3.jpg", "caption": "Table E19: Ablation of different numbers in the calibration dataset.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of varying the number of calibration samples used in the DuQuant quantization method on the LLaMA2-7B model.  The study explores how changing the number of samples (16, 32, 64, 128, and 256) affects the performance of the quantized model, measured in terms of perplexity on the WikiText2 and C4 datasets. The results show that the quantization performance is relatively insensitive to the number of calibration samples used, indicating that the averaging process inherent to DuQuant reduces the influence of individual samples on the final results.  This robustness is a key advantage of the approach.", "section": "E.4 Effects of Calibration Datasets"}, {"figure_path": "mp8u2Pcmqz/tables/tables_24_1.jpg", "caption": "Table F20: Comparison of quantization settings between QuaRot and DuQuant.", "description": "This table compares the quantization settings used in the QuaRot and DuQuant methods.  It shows that QuaRot uses per-channel symmetric quantization for weights and per-token symmetric quantization for activations, while keeping query inputs in FP16 precision. In contrast, DuQuant employs per-channel asymmetric quantization for weights, per-token asymmetric quantization for activations, and per-token asymmetric quantization for query inputs. This highlights a key difference in the approaches taken by the two methods.", "section": "F Detailed Comparison with QuaRot"}, {"figure_path": "mp8u2Pcmqz/tables/tables_24_2.jpg", "caption": "Table 1: Perplexity (\u2193) results under 4-bit weight-activation quantization. The results for W6A6 can be found in Table D8. Atom and OmniQuant unprocessed group-query attention for LLaMA2-70B.", "description": "This table presents the perplexity scores achieved by different quantization methods (SmoothQuant, OmniQuant, AffineQuant, QLLM, Atom, DuQuant, and DuQuant+LWC) on the WikiText2 and C4 datasets using 4-bit weight-activation quantization.  Lower perplexity scores indicate better performance. The table compares the performance across various sizes of LLaMA and LLaMA2 models. Note that Atom and OmniQuant's results for the LLaMA2-70B model are incomplete due to unprocessed group-query attention.", "section": "4.1 Main Results"}, {"figure_path": "mp8u2Pcmqz/tables/tables_24_3.jpg", "caption": "Table 2: Zero-shot QA (\u2191) results of LLaMA1 models under 4-bit weight-activation quantization. The results for LLaMA2 models and W6A6 quantization can be found in Table D1 D9, and D10.", "description": "This table presents the results of zero-shot question answering experiments conducted on four different sizes of the LLaMA1 large language model, each quantized using a 4-bit weight-activation method.  The table shows the performance of the models on six different tasks (PIQA, ARC-E, ARC-C, BoolQ, HellaSwag, WinoGrande), along with an average score across all six tasks.  The results are compared to a floating-point (FP16) baseline, highlighting the effectiveness of the quantization technique. The table also notes that results for LLaMA2 models and using a 6-bit weight-activation method are available in other tables within the paper's supplementary material.", "section": "4 Experiment"}, {"figure_path": "mp8u2Pcmqz/tables/tables_24_4.jpg", "caption": "Table F23: Matrices comparison between DuQuant and QuaRot under W4A4 quantization.", "description": "This table compares the performance of DuQuant and QuaRot on the WikiText2 and C4 datasets for the LLaMA2-7B and LLaMA2-13B models using W4A4 (4-bit weight and activation) quantization.  It highlights the perplexity scores achieved by each method, offering a direct comparison of the two approaches on these benchmark datasets. The table demonstrates that DuQuant is superior to QuaRot in terms of achieving lower perplexity, suggesting a more effective quantization strategy.", "section": "F Detailed Comparison with QuaRot"}, {"figure_path": "mp8u2Pcmqz/tables/tables_24_5.jpg", "caption": "Table 12: Quantization runtime on one NVIDIA A100.", "description": "This table presents a comparison of the quantization runtime for different models (LLaMA2-7B, LLaMA2-13B, and LLaMA2-70B) using various quantization methods (OmniQuant, AffineQuant, QLLM, Atom, and DuQuant) on a single NVIDIA A100 GPU.  The results highlight the significant speedup achieved by DuQuant compared to other methods.", "section": "4 Experiment"}]