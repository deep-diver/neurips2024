[{"type": "text", "text": "Questioning the Survey Responses of Large Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ricardo Dominguez-Olmedo1,2 Moritz Hardt1,2 Celestine Mendler-D\u00fcnner1,2,3 ", "page_idx": 0}, {"type": "text", "text": "1Max-Planck Institute for Intelligent Systems, T\u00fcbingen 2T\u00fcbingen AI Center 3ELLIS Institute T\u00fcbingen {rdo,hardt,cmendler}@tuebingen.mpg.de ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Surveys have recently gained popularity as a tool to study large language models. By comparing survey responses of models to those of human reference populations, researchers aim to infer the demographics, political opinions, or values best represented by current language models. In this work, we critically examine this methodology on the basis of the well-established American Community Survey by the U.S. Census Bureau. Evaluating 43 different language models using de-facto standard prompting methodologies, we establish two dominant patterns. First, models\u2019 responses are governed by ordering and labeling biases, for example, towards survey responses labeled with the letter \u2018A\u2019. Second, when adjusting for these systematic biases through randomized answer ordering, models across the board trend towards uniformly random survey responses, irrespective of model size or pre-training data. As a result, in contrast to conjectures from prior work, survey-derived alignment measures often permit a simple explanation: models consistently appear to better represent subgroups whose aggregate statistics are closest to uniform for any survey under consideration. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Surveys have a long tradition in social science research as a means for gathering statistical information about the characteristics, values, and opinions of human populations [Groves et al., 2009]. Insights from surveys inform policy interventions, business decisions, and science across various domains. Surveys typically consist of a series of well-curated questions in a multiple-choice format, with unambiguous framing and a set of answer choices carefully selected by domain experts. Surveys are then presented to groups of individuals and their answers are aggregated to gain statistical insights about the populations that these groups of individuals represent. ", "page_idx": 0}, {"type": "text", "text": "Many established survey questionnaires together with the carefully collected answer statistics are publicly available. Machine learning researchers have identified the potential beneftis of building on this valuable data resource to study large language models (LLMs). Survey questions offer a way to systematically prompt LLMs, and the aggregate statistics over answers collected by surveying human populations serve as a reference point for evaluation. As a result, the use of surveys has recently gained popularity for studying LLMs\u2019 biases [Santurkar et al., 2023, Durmus et al., 2023]. Also prompting LLMs with survey questions, researchers in the social sciences have explored using LLMs to emulate the survey responses of human populations [Argyle et al., 2023, Lee et al., 2023]. If effective proxies, simulated responses could augment or replace the expensive data collection process involving human subjects and provide insights into subpopulations that are otherwise hard to reach. ", "page_idx": 0}, {"type": "image", "img_path": "Oo7dlLgqQX/tmp/50974441a1303514b57306768b112e099e0a24fe091ba145f136c6af19d5a638.jpg", "img_caption": ["Figure 1: We prompt language models with questions from the American Community Survey (ACS). We systematically compare models\u2019 survey responses to those of the U.S. Census. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "It is tempting to prompt LLMs with survey questions, due to their syntactic similarity to question answering tasks [Brown et al., 2020, Liang et al., 2022]. However, it is a priori unclear how to interpret their answers. Rather than knowledge testing, surveys seek to elicit aggregate statistics over individuals, providing an unbiased view on the properties of the population they are targeting. The quality of survey data hinges on the validity and robustness of the conclusions that can be drawn from it. Clearly, running a survey on LLMs is different from interrogating humans and thus it comes with distinct challenges. While much research has gone into carefully designing surveys to ensure faithful human responses, it is unclear whether prompting LLMs with the same surveys satisfies similar premises out-of-the-box. We devote this work to gain systematic insights into the survey responses of LLMs, what we can expect to learn from them, and to what extent they resemble those of human populations. ", "page_idx": 1}, {"type": "text", "text": "1.1 Our work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The basis of our investigation is the American Community Survey1 (ACS), a demographic survey conducted by the U.S. Census Bureau at a national level, on a yearly basis. We curate a questionnaire composing of 25 multiple choice questions from the 2019 ACS. We prompt 43 language models of varying size with these questions, individually and in sequence, and we record their probability distribution over answers. Based on the collected data, we investigate the following two questions: What can we infer about LLMs, and the data they have been trained on, from their survey responses? Does the data generated by prompting models to answer the ACS questionnaire qualitatively resemble the census data collected by surveying the U.S. population? See Figure 1. ", "page_idx": 1}, {"type": "text", "text": "We start by inspecting models\u2019 distributions over answers to individual survey questions when the questions are asked independently. We observe that the entropy of response distributions differs substantially across models of varying size. Entropy tends to increase log linearly with model size, and it is preserved across different questions asked. We find that this differences arise because strong ordering and labeling biases confound models\u2019 answers. In fact, after adjusting for such systematic biases through randomized choice ordering, we find that response distributions are very similar across models and tend to correspond to highly balanced answers. ", "page_idx": 1}, {"type": "text", "text": "Comparing models\u2019 adjusted responses to those of the U.S. census population, we find that natural variations in entropy across questions are not reflected in the responses. Instead, on average across questions, models\u2019 responses are no closer to the census population, or the population of any state within the US, than to a fixed uniform baseline. This qualitative difference between model responses and human data puts into question the insights that can be gained from such comparisons. We find that even after instruction-tuning this trend persists, and model responses have consistently higher entropy than any human population we compare to, independent of the survey used. Only for models of size larger than 70 billion parameters we can recognize a trend that the divergence between model responses and the census data decreases after instruction-tuning. ", "page_idx": 1}, {"type": "text", "text": "With these insights in mind, we inspect conjectures from prior work related to survey derived alignment metrics, that is, that differences in similarity between models\u2019 and populations\u2019 responses might be attributable to certain demographics being better represented in the training data. Instead, our results suggest a much simpler explanation: the relative alignment of model responses with different demographic subgroups can be explained by the entropy of the subgroups\u2019 responses, irrespective of the data or training procedure employed to train the model. We demonstrate this beyond the ACS on other surveys considered by prior work. As such, our findings provide important context to prior studies that employ surveys to examine the biases of LLMs. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "More broadly, our findings suggest caution when treating language models\u2019 survey responses as a faithful representation of any human population, at least a present time, as it could lead to potentially misguided conclusions about alignment. ", "page_idx": 2}, {"type": "text", "text": "1.2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Despite the syntactical similarities, there are important differences between evaluating LLMs on the basis of their survey responses and traditional question answering evaluations [Liang et al., 2022]. Question answering (QA) tasks predominantly serve the purpose of knowledge testing [e.g., Kwiatkowski et al., 2019, Rajpurkar et al., 2016, Talmor et al., 2019, Mihaylov et al., 2018]. In such setting, a language model\u2019s answer to some unambiguous input question is extracted by computing its most likely completion. Similarly, for questions that lack a clear answer (e.g., \u201cAngela and Patrick are sitting together. Who is an entrepreneur?\u201d) models\u2019 most likely response have been used to investigate various biases of LLMs [Li et al., 2020, Mao et al., 2021, Perez et al., 2023, Abid et al., 2021, Jiang et al., 2022]. ", "page_idx": 2}, {"type": "text", "text": "When evaluating LLMs on the basis of survey questions, the focus is not on the model\u2019s most likely completion but rather on the probability distribution that the model assigns to various answer choices. For example, not whether the model is more likely to answer \u201cYes\u201d than \u201cNo\u201d to a given survey question, but the normalized probability assigned to each of the two answer choices. See Figure 1. More concretely, Santurkar et al. [2023] study LLMs\u2019 answer distributions for multiple-choice opinion polling questions, measuring their similarity to those of various U.S. demographic groups. They extract models\u2019 answer distributions from the next token probabilities corresponding to each answer choice. Subsequent works employ a similar methodology but instead consider transnational opinion surveys [Durmus et al., 2023, AlKhamissi et al., 2024] and moral beliefs surveys [Scherrer et al., 2024]. We adopt this popular methodology to systematically investigate the properties of models\u2019 answer distributions on the basis of a well-established demographic survey. ", "page_idx": 2}, {"type": "text", "text": "Instead of asking questions individually, Hartmann et al. [2023], Rutinowski et al. [2023], Motoki et al. [2023], Feng et al. [2023] sequentially prompt language models to answer entire political compass or voting advice questionnaires. Rather than aggregating answers into a political affinity score, our focus is instead on examining whether models\u2019 responses qualitatively resemble those of human populations. We discuss this sequential generation setting in detail in Appendix F. ", "page_idx": 2}, {"type": "text", "text": "Lastly, there is an emerging body of research that integrates LLMs into computational social science [Ziems et al., 2024]. This includes tasks such as taxonomic labeling, where language models are employed for tasks such as opinion prediction [Kim and Lee, 2023, Mellon et al., 2022], and free-form coding, where language models are used to generate explanations for social science constructs [Nelson et al., 2021]. Recent studies have also investigated the feasibility of using LLMs to simulate human participants in psychological, psycholinguistic, and social psychology experiments [Dillion et al., 2023, Aher et al., 2023], or as proxies for specific human populations in social science research [Argyle et al., 2023, Lee et al., 2023, Sanders et al., 2023] and economics [Brand et al., 2023, Horton, 2023]. Within this context, our work suggests caution in relying on the survey responses of LLMs to elicit synthetic responses that resemble those of human populations and highlights potential pitfalls. ", "page_idx": 2}, {"type": "text", "text": "2 Surveying language models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We employ the de-facto standard methodology to survey language models introduced by Santurkar et al. [2023]. For every survey question, we generate a prompt containing the multiple-choice question and we collect language models\u2019 probability distribution over answer choices. Formally, for a given model $m$ and survey question $q$ we define the model\u2019s survey response as a categorical random variable $R_{q}^{m}$ which can take on $k_{q}$ values corresponding to the number of answer choices to question $q$ . The respective answer distributions are then contrasted with those of human populations align various dimensions. The overall setup is illustrated in Figure 1. ", "page_idx": 2}, {"type": "text", "text": "1. We construct an input prompt of the form \u201cQuestion: <question> \\n A. <choice $\\mathtt{1}>\\mathtt{N n}$ B. <choice $\\scriptstyle2>\\textsf{V n}$ ... <choice $k_{q}>\\textsf{V}$ Answer:\u201d.   \n2. We query language models with the input prompt and obtain their output distribution over next-token probabilities. We select the $k_{q}$ output probabilities corresponding to each answer choice (e.g., the tokens \u201cA\u201d, \u201cB\u201d, etc.), and we renormalize to obtain the probability distribution over survey answers. 2. ", "page_idx": 3}, {"type": "text", "text": "The chosen style of prompt is standard for question answering tasks [Hendrycks et al., 2021], used in OpinionQA [Santurkar et al., 2023], and follows the best practices for social science research recommended by Ziems et al. [2024]. For completeness we perform several prompt ablations, including the prompt variations used by Argyle et al. [2023], Santurkar et al. [2023] and Durmus et al. [2023]. We find our take-aways to be robust to such changes, see Appendix D. However, note that our goal is not to engineer better prompts, but to critically examine popular scientific practices. ", "page_idx": 3}, {"type": "text", "text": "Survey questions. We use a representative subset of 25 multiple-choice questions from the 2019 ACS questionnaire. We denote the set of questions by $Q$ . The questions cover basic demographic information, education attainment, healthcare coverage, disability status, family status, veteran status, employment status, and income. We generally consider the questions and answers as they appear in the ACS questionnaire. Figure 1 depicts an example question. We refer to Appendix A.1 for our list of questions and the exact framing we used for each question. ", "page_idx": 3}, {"type": "text", "text": "Models surveyed. We survey 43 language models of size varying from 110M to 175B parameters: the base models GPT-2 [Radford et al., 2019], GPT-Neo [Black et al., 2021], Pythia [Biderman et al., 2023], MPT [MosaicML, 2023], Llama 2 [Touvron et al., 2023],Llama 3 [Dubey et al., 2024] and GPT-3 [Brown et al., 2020]; as well as the instruct variants of MPT 7B and GPT NeoX 20B, the Dolly fine-tune of Pythia 12B [Databricks, 2023], Llama 2 Chat, Llama 3 Instruct, the text-davinci variants of GPT-3 [Ouyang et al., 2022], and GPT-4 [OpenAI, 2023]. ", "page_idx": 3}, {"type": "text", "text": "Reference data & evaluation. We use the responses collected by the U.S. Census Bureau when surveying the U.S. population as our reference data. In particular, we use the 2019 ACS public use microdata sample3(henceforth census data). The data contains the anonymized responses of around 3.2 million individuals in the United States. For each survey question $q\\in Q$ , we denote the census\u2019 population-level response as a categorical random variable $C_{q}$ whose event probabilities are the relative frequency of each answer choice among survey respondents. We use $U_{q}$ to denote the uniform distribution over answers. Given these two reference points, we evaluate language models\u2019 responses $R_{q}^{m}$ along two dimensions: ", "page_idx": 3}, {"type": "text", "text": "\u2022 We use entropy to measure the degree of variation in models\u2019 responses. We denote the entropy of a random variable $R$ as $H(R)$ . To meaningfully compare the entropy of responses across questions with varying number of choices $k_{q}$ , we report normalized entropy, that is, the entropy relative to the uniform distribution. $H(R_{q}^{m^{*}})=1$ implies that model $m$ \u2019s survey response to question $q$ is uniformly distributed (i.e., $H(U_{q})=1)$ ).   \n\u2022 We use the Kullback\u2013Leibler $(K L)$ divergence to measure the \u201csimilarity\u201d between two distributions over answers. We write $\\mathrm{KL}(R_{q}^{m}\\parallel C_{q})$ for the KL divergence between the response distribution $R_{q}^{m}$ of model $m$ to question $q$ and the corresponding aggregate response distribution $C_{q}$ observed in the census data. The larger the KL distance between two distributions, the more dissimilar the two distributions are. ", "page_idx": 3}, {"type": "text", "text": "Note that the KL divergence between any distribution and the uniform distribution corresponds to the entropy difference. For normalized entropy this yields $\\mathrm{KL}(C_{q}\\parallel U_{q})=k_{q}(1-H(C_{q}))$ . ", "page_idx": 3}, {"type": "image", "img_path": "Oo7dlLgqQX/tmp/eee13f14960d990d2c65330022387813c49e0ece02847ad915a4bb10b385907c.jpg", "img_caption": ["(a) Entropy of base models\u2019 responses, for five of the ACS questions. "], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "Oo7dlLgqQX/tmp/66df5df25b057c9c1c5aa909711f6679e7e112c4d2cd19baa444e40d7eb9b5f6.jpg", "img_caption": ["(b) Entropy of base models\u2019 responses to the ACS, ordered by model size. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: Entropy of model responses across the ACS questions for naive prompting. Entropy of models\u2019 responses $\\left(\\diamond\\right)$ tends to increase log-linearly with model size, irrespective of the underlying response entropy observed in the U.S. census $(-)$ . ", "page_idx": 4}, {"type": "text", "text": "Randomized choice ordering. For several investigations we survey models under randomized choice ordering. This means, for a given question $q$ , we prompt models with different permutations of the answer choice ordering, i.e., the assignment of answers (e.g., \u201cmale\u201d, \u201cfemale\u201d) to choice labels (\u201cA\u201d, \u201cB\u201d, etc), while the choice labels are kept in alphabetic order. We evaluate models\u2019 survey responses under all possible choice orderings and we use $\\bar{R}_{q}^{m}$ to denote the expected distribution over answers and $\\bar{O}_{q}^{m}$ to denote the expected distribution over selected choice labels. For questions with more than 6 answers we evaluate a maximum of 5000 permutations. For OpenAI\u2019s models we evaluate up to 50 permutations due to the costs of querying the OpenAI API. This distinction serves to decouple a model\u2019s tendency towards picking a particular answer from its tendency towards picking a particular choice label. In the following we refer to the expected survey response $\\check{R}_{q}^{m}$ under uniformly distributed choice ordering as the adjusted survey response. We will come back to this in Section 4. ", "page_idx": 4}, {"type": "text", "text": "3 Systematic biases in models\u2019 survey responses ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We start by surveying the base pre-trained models. We present survey questions independently of one another, showing the answer choices in the same order as the ACS. ", "page_idx": 4}, {"type": "text", "text": "For a first investigation, we consider the normalized entropy of models\u2019 responses to the \u201cSEX\", \u201cHICOV\u201d, and \u201cFER\" questions. The SEX question inquiries about the person\u2019s sex, encoded as male female, the HICOV question inquiries whether the person is currently covered by any health insurance plan, and the FER question inquires whether the person has given birth in the past 12 months. When surveying the U.S. population, these three questions elicit responses with very different entropy; responses to the SEX question are almost uniformly distributed, whereas most people answer \u201cNo\u201d to the FER question. In contrast, as shown in Figure 2(a), the entropy of models\u2019 responses to these three questions are surprisingly similar. In particular, we find that the entropy of models\u2019 responses tends to increase log-linearly with model size, independent of the question asked. This trend is consistent across all ACS survey questions, see Figure 8 in Appendix B.1. ", "page_idx": 4}, {"type": "text", "text": "For a broader picture, we illustrate models\u2019 response entropy across all survey questions in Figure 2(b). The blue dots represent models\u2019 responses to individual questions, and the green dots represent the entropy of the responses of the U.S. census. We order models by size. We observe that the entropy of responses of the U.S. census greatly varies across questions. In contrast, for any given model, the entropy of its responses varies substantially less so. ", "page_idx": 4}, {"type": "image", "img_path": "Oo7dlLgqQX/tmp/4dbce8334a0a57507b5d56ed1cae45e05bced54b9ca18e5ad303ec4d70e3d4e7.jpg", "img_caption": ["Figure 3: A-bias of in model responses across ACS questions. Each dot corresponds to one of the 25 questions. Models are ordered by size. As a reference, the extreme points illustrate A-bias for a model that always answers \u2019A\u2019 and a model that never answers \u2019A\u2019. All models suffer from substantial A-bias. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Overall, we find that models\u2019 response distributions seem to be widely independent of the survey question asked, and variations across models are much larger than variations across questions. This lead us to suspect that observed differences across models might arise mostly due to systematic biases. ", "page_idx": 5}, {"type": "text", "text": "3.1 Testing for systematic biases: A-bias ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "It is well-known that language models\u2019 most likely answer to multiple-choice questions can change depending on seemingly minor factors such as the ordering of few-shot examples [Zhao et al., 2021, Lu et al., 2022] or the ordering of answer choices [Robinson and Wingate, 2023a]. We are interested in the extent to which changes in choice ordering affect a model\u2019s output distribution over answers. ", "page_idx": 5}, {"type": "text", "text": "We start by measuring A-bias: the tendency of a model towards picking the answer choice labeled \u201cA\". In particular, we seek to study the extent to which the strength of this bias explains the differences in responses observed across models. For an unbiased model that outputs the same answer distribution irrespective of choice ordering, the expected choice distribution $\\bar{O}_{q}^{m}$ under randomized choice ordering would match precisely the uniform distribution (e.g., $\\mathrm{P(^{\\ast}A^{\\ast})}\\,\\mathrm{\\dot{=}\\,P(^{\\ast}B^{\\ast})}=0.5)$ . We define a model\u2019s A-bias as its absolute deviation from this unbiased baseline: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{Abias}_{q}^{m}:=\\mathrm{P}(\\bar{O}_{q}^{m}={}^{\\ast}A^{\\ast})-1/k_{q}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We measure A-bias for each question $q$ and model $m$ . Results are illustrated in Figure 3. We again sort models by their size. We observe all models exhibit substantial A-bias. However, models in the order of a few billion parameters or fewer consistently exhibit particularly strong A-bias, and tend towards mono answers. We additionally observe that the strength of A-bias in instruction or RLHF tuned models is similar to that of base models, see Appendix B.2. A plausible explanation for small models exhibiting strong A-bias is that the ability to answer MMLU-style multiple-choice questions only emerges for models of sufficient scale [Dominguez-Olmedo et al., 2024]. ", "page_idx": 5}, {"type": "text", "text": "We investigate other types of labeling and position bias (e.g., last-choice bias) in Appendix C. Overall, we find a strong tendency of LLMs to pick up on spurious signals in the way that answers are ordered and labeled, rather than their semantic meaning. Notably, in contrast to the primacy bias observed in humans [Groves et al., 2009], we find that models exhibit substantial A-bias even when randomizing the position of the \u201cA\u201d choice. Our findings are consistent with the concurrent work of Tjuatja et al. [2023], which similarly finds that models\u2019 response biases to multiple-choice survey questions are generally not human-like. The orthogonal work of Wang et al. [2024] additionally shows that models\u2019 responses to multiple-choice survey questions may not consistently reflect their free-form outputs. ", "page_idx": 5}, {"type": "text", "text": "In summary, we find that systematic biases confound models\u2019 answer distributions. This makes it challenging to draw robust conclusions about inherent properties of LLMs, such as the opinions or populations they best represent. For example, simply reversing the order of answers to the \u201cSEX\u201d question could lead to GPT-2 seemingly representing a population where females are significantly over-represented, whereas a reverse conclusion would be drawn when using the standard answer order. While much research went into designing the ACS to elicit faithful answers and eliminate systematic biases when surveying human populations, simply using the same question framing does not protect against the systematic response biases that language models exhibit. ", "page_idx": 5}, {"type": "image", "img_path": "Oo7dlLgqQX/tmp/c05f402775eadd09156a1b1e466d3033b9a61832fdde6bb6a9c2fd0a432ddffc.jpg", "img_caption": ["Figure 4: Entropy of model responses after adjustment. (top) Illustration of how adjustment is performed. We average models\u2019 responses over all possible answer orderings. (bottom) Entropy of models\u2019 responses after adjustment. Entropy of base models\u2019 responses is close to 1 (i.e., uniform). Instruction tuned-models exhibit substantially higher variations in entropy across questions. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4 Inspecting adjusted responses ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To eliminate confounding due to labeling and ordering biases, we survey models under randomized choice ordering, borrowing an established methodology to adjust for ordering biases of all kinds in survey research [Groves et al., 2009]. Also a recent work in LLM research adopts this methodology [Robinson and Wingate, 2023b]. In the following, we refer to the expected response after answer choice randomization as the adjusted response. ", "page_idx": 6}, {"type": "text", "text": "In Figure 4 we plot the normalized entropy of models\u2019 adjusted responses for the ACS questions considered. First focusing on base models, and comparing the results to Figure 2(b) we find that after adjustment, 1) the variations in responses\u2019 entropy across survey questions are very small, 2) we no longer observe the trend of the entropy of model responses increasing log-linearly with model size. In fact, models\u2019 survey responses have a normalized entropy of approximately 1 irrespective of model size or survey question asked. This validates our initial hypothesis that, without adjustment, variations in responses across base models arise predominantly due to systematic biases such as A-bias, rather than the content of the survey questions asked. ", "page_idx": 6}, {"type": "text", "text": "4.1 Effect of instruction tuning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now evaluate language models that have been fine-tuned with instructions and/or human preferences, henceforth \u201cinstruction-tuned models\u201d. In the right plot of Figure 4 we show the normalized entropy of instruction-tuned models\u2019 ACS survey responses after adjustment. We observe that instruction tuned-models all exhibit substantially higher variations in entropy across questions compared to base models. But in general, the entropy of their responses remains higher than the entropy of the census responses. Interestingly, as we will see, although deviating more from uniform, model responses do not tend to be closer to the U.S. census responses. ", "page_idx": 6}, {"type": "image", "img_path": "Oo7dlLgqQX/tmp/8828fb39eae7ca8007365d5bafa2d04bd03b59c9561c381aecac260b63d42fc3.jpg", "img_caption": ["Figure 5: Divergence between adjusted model responses and different baselines: the overall U.S. census $(\\pmb{\\mathscr{s}})$ , individual U.S. states (\u25cf), and a uniform baseline $(\\star)$ . Smaller means more similar. Model responses are by far more similar to the uniform baseline than to any human reference population. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2 Comparing model responses to the U.S. census ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now investigate the similarity of language models\u2019 adjusted responses to the census data. To do so, we consider the overall U.S. census population, as well as 50 census subgroups corresponding to every state in the United States. This leads to different human reference populations. ", "page_idx": 7}, {"type": "text", "text": "Inspired by the alignment measures proposed by Santurkar et al. [2023] and Durmus et al. [2023], we investigate the similarity of model responses to the census data by evaluating the average divergence across questions between model responses and the census statistics.4 As we focus on categorical questions, we evaluate average KL divergence between each language model $m$ and each reference population Ref, as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\bar{\\mathrm{KL}}(m,\\mathrm{Ref})=\\frac{1}{|Q|}\\sum_{q\\in Q}\\mathrm{KL}(\\bar{R}_{q}^{m}||\\mathrm{Ref}_{q}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Results are depicted in Figure 5. For each model we plot the divergence to the census in black, the divergence to the different subgroups in blue, and the divergence to a uniform baseline with balanced responses in red. We observe that models are strikingly more similar to the uniform baseline than to any of the populations considered. For base models, this result is unsurprising, since in the previous section we established that base models\u2019 responses are essentially uniform after adjustment. ", "page_idx": 7}, {"type": "text", "text": "Looking at Figure 5 we find no consistent trend that instruction-tuning would move responses closer to the census, despite the increased deviation from uniform and the larger variations in entropy (recall Figure 4). Only for larger models the divergence seems to clearly decrease with instruction-tuning. However, all models\u2019 responses still remain significantly closer to the uniform baseline than to the U.S. census. For instance, for the GPT-4 model whose answers exhibit the highest similarity to the human reference populations, only 6 out of 25 questions $(24\\%)$ are closer to the U.S. census than to the uniform baseline. Given these results, drawing conclusions about the relative alignment of models with subgroups is prone to resulting in brittle conclusions. ", "page_idx": 7}, {"type": "text", "text": "5 Implications for survey-based alignment metrics ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our findings add important context to previous works studying the alignment of language models with different human subpopulations. In particular, we highlighted the tendency of models towards balanced answers. Due to varying entropy in the responses of subgroups this leads to a strong correlation between model alignment and the reference population\u2019s entropy. The linear trend in Figure 6 visualizes this. For any given model, it consistently appears to be more \u201caligned\u201d with the subpopulations exhibiting high entropy in their answers. Interestingly, we find that this trend also holds pre-adjustment, suggesting that the transformation of the response through randomized choice ordering is orthogonal to differentiating aspects of any specific population. In contrast, when comparing different models in Figure 6, we can see how adjustment has a large influence on their relative order. Differences across models that we see under naive prompting disappear after adjustment, which means that they should largely be attributed to systematic biases, rather than inherent properties of the model. ", "page_idx": 7}, {"type": "image", "img_path": "Oo7dlLgqQX/tmp/b3b47f749b0281bd74f15ba3cdb3c1cbea2ad13e473d876e161d70bd7dae1b24.jpg", "img_caption": ["(c) MPT, Pythia, GPT-NeoX and its instruction variants. (d) Llama and its instruction and chat variants. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 6: Alignment of models with different census subgroups. All models tend to exhibit similar relative alignment, and the divergence metric decreases with the entropy of the subgroups\u2019 responses. ", "page_idx": 8}, {"type": "text", "text": "Taken together our findings imply that the survey-derived alignment measure is more informative of differences in the reference populations rather than the language models is aims to evaluate. Model particularities, such as the pre-training data used, instruction tuning or the use of reinforcement learning with human feedback, seem to have little impact on which population is best represented. ", "page_idx": 8}, {"type": "text", "text": "5.1 Beyond the ACS ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To inspect whether this trend changes with the content of the questions asked, we reproduce our experiments with additional surveys. We use the American Trends Panel (ATP) opinion surveys considered by Santurkar et al. [2023], and the Pew Research\u2019s Global Attitudes Surveys (GAS) and World Values Surveys (WVS) considered by Durmus et al. [2023]. These surveys encompass around 1500 questions and 60 U.S. demographic subgroups, and around 2300 questions and 60 national populations, respectively. We adopt the alignment metrics considered by the aforementioned works. We find that our insights gained from the ACS also hold for the ATP and GAS/WVS surveys. In particular, we similarly find a linear trend between the alignment metrics and subgroups\u2019 entropy of responses, in particular after adjustment, see Figure 7. Note here that alignment and divergence are negatively correlated by definition. Interestingly, this observation explains some of the findings in prior works. For example, Santurkar et al. [2023] find that \u201call the base models share striking similarities\u2013e.g., being most aligned with lower income, moderate, and Protestant or Roman Catholic groups\u201d and \u201cour analysis [...] surfaces groups whose opinions are poorly reflected by current LLMs (e.g., $^{65+}$ and widowed individuals)\u201d. For the ATP surveys considered, low income, moderate, and Protestant/Catholic are precisely the demographic subgroups with responses closest to uniformly random among the income, political ideology, and religion demographic subgroups; whereas age $65+$ and widowed are the demographic subgroups with responses furthest from uniform among the age and marital status demographic subgroups. Further, Santurkar et al. [2023] observe that RLHF can result in a \u201csubstantial shift [...] towards more liberal, educated, and wealthy [demographic groups]\u201d. Our results suggest that this could be an artifact of systematic biases. For the ATP surveys, we observe three outliers for which its alignment before adjustment is not correlated with the entropy of subgroup\u2019s responses: Llama 2 70B Chat and the two Llama 3 Instruct models. These are the models with largest pre-training compute considered. However, after adjustment, the alignment trends of Llama 2 70B Chat and the Llama 3 Instruct models are remarkably similar to that of their corresponding base models and all other LLMs. ", "page_idx": 8}, {"type": "image", "img_path": "Oo7dlLgqQX/tmp/c6467af12ab7344f40380b4e01ea05df07448fd625a223ef8df6e0288973c29c.jpg", "img_caption": ["Figure 7: Alignment beyond ACS for selected models. We adopt the measures of Santurkar et al. [2023] and Durmus et al. [2023] on ATP and GAS/VVS opinion surveys. Again, the alignment between models and a given subpopulation correlates with the entropy of the subpopulations\u2019 responses. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We used a popular methodology to elicit LLMs\u2019 answer distributions to survey questions and closely examined the responses on the basis of the prime US demographic survey. We found that model responses are dominated by systematic ordering biases and do not exhibit the natural variations in entropy found in the human reference data collected by the US census. Even after adjusting for ordering biases, LLMs\u2019 responses still do not resemble those of human populations. Instead, they exhibit consistently high entropy, independent of the question asked. This holds true irrespective of model size or fine-tuning with human preferences. ", "page_idx": 9}, {"type": "text", "text": "These findings have important implications for insights gained from survey-derived alignment metrics. In particular, it explains why models of varying size all exhibit the same trend: they are most aligned with subgroups who happen to have balanced answers for the survey questions under consideration. For all models and surveys considered, alignment appears to be a proxy for the entropy of subgroups, rather than an inherent property of the model, or its training data. ", "page_idx": 9}, {"type": "text", "text": "We want to reiterate that our focus lies on questioning a popular methodology of eliciting survey responses from large language models using multiple choice prompting. At the example of this methodology our results highlight an important pitfall and suggest caution to expect robust insights when comparing such responses against those of human populations. The robustness and quality of an established survey does not seamlessly translate from the results obtained by surveying human populations to the logits output by LLMs. More research is urgently needed to design methodologies for getting insights into the inherent biases of LLMs and the population they might represent. Here public surveys and their accompanying data offer exciting potential and the could play an important role as a benchmarking tool for systematic evaluations of LLMs, see [Cruz et al., 2024] as an example. Although the use of survey data for LLM research has recently gained popularity, it still remains a widely under explored data source. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors would like to thank Frauke Kreuter and the Social Data Science and AI Lab at LudwigMaximilians-Universit\u00e4t Munich for inspiring discussions on an earlier version of this manuscript. Celestine Mendler-D\u00fcnner acknowledges financial support from the Hector Foundation. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Abubakar Abid, Maheen Farooqi, and James Zou. Persistent anti-muslim bias in large language models. In AAAI/ACM Conference on AI, Ethics, and Society, pages 298\u2013306, 2021. ", "page_idx": 10}, {"type": "text", "text": "Gati V Aher, Rosa I Arriaga, and Adam Tauman Kalai. Using large language models to simulate multiple humans and replicate human subject studies. In International Conference on Machine Learning, pages 337\u2013371, 2023. ", "page_idx": 10}, {"type": "text", "text": "Badr AlKhamissi, Muhammad ElNokrashy, Mai Alkhamissi, and Mona Diab. Investigating cultural alignment of large language models. In Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12404\u201312422, 2024. ", "page_idx": 10}, {"type": "text", "text": "Lisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua R Gubler, Christopher Rytting, and David Wingate. Out of one, many: Using language models to simulate human samples. Political Analysis, 31(3):337\u2013351, 2023. ", "page_idx": 10}, {"type": "text", "text": "Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar Van Der Wal. Pythia: a suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, 2023. ", "page_idx": 10}, {"type": "text", "text": "Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, 2021. ", "page_idx": 10}, {"type": "text", "text": "James Brand, Ayelet Israeli, and Donald Ngwe. Using GPT for Market Research. Harvard Business School Marketing Unit Working Paper No. 23-062, 2023. ", "page_idx": 10}, {"type": "text", "text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, pages 1877\u20131901, 2020. ", "page_idx": 10}, {"type": "text", "text": "Andr\u00e9 F Cruz, Moritz Hardt, and Celestine Mendler-D\u00fcnner. Evaluating language models as risk scores. ArXiv preprint arXiv:2407.14614, 2024. ", "page_idx": 10}, {"type": "text", "text": "Databricks. Dolly 12b, 2023. URL https://github.com/databrickslabs/dolly. ", "page_idx": 10}, {"type": "text", "text": "Danica Dillion, Niket Tandon, Yuling Gu, and Kurt Gray. Can AI language models replace human participants? Trends in Cognitive Sciences, 2023. ", "page_idx": 10}, {"type": "text", "text": "Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring adult: New datasets for fair machine learning. Advances in Neural Information Processing Systems, 2021. ", "page_idx": 10}, {"type": "text", "text": "Ricardo Dominguez-Olmedo, Florian E Dorner, and Moritz Hardt. Training on the test task confounds evaluation and emergence. ArXiv preprint arXiv:2407.07890, 2024. ", "page_idx": 10}, {"type": "text", "text": "Florian Dorner, Tom S\u00fchr, Samira Samadi, and Augustin Kelava. Do personality tests generalize to large language models? In NeurIPS Workshop on Socially Responsible Language Modelling Research, 2023. ", "page_idx": 10}, {"type": "text", "text": "Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. ArXiv preprint arXiv:2407.21783, 2024. ", "page_idx": 10}, {"type": "text", "text": "Esin Durmus, Karina Nyugen, Thomas I Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, et al. Towards measuring the representation of subjective global opinions in language models. ArXiv preprint arXiv:2306.16388, 2023. ", "page_idx": 10}, {"type": "text", "text": "Shangbin Feng, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov. From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models. Findings of the Association for Computational Linguistics, 2023.   \nR.M. Groves, F.J. Fowler, M.P. Couper, J.M. Lepkowski, E. Singer, and R. Tourangeau. Survey Methodology. Wiley, 2009.   \nJochen Hartmann, Jasper Schwenzow, and Maximilian Witte. The political ideology of conversational AI: Converging evidence on ChatGPT\u2019s pro-environmental, left-libertarian orientation. ArXiv preprint arXiv:2301.01768, 2023.   \nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021.   \nJohn J Horton. Large language models as simulated economic agents: What can we learn from homo silicus? NBER Working Paper, 2023.   \nHang Jiang, Doug Beeferman, Brandon Roy, and Deb Roy. CommunityLM: Probing Partisan Worldviews from Language Models. In International Conference on Computational Linguistics, 2022.   \nJunsol Kim and Byungkyu Lee. AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys. ArXiv preprint arxiv:2305.09620, 2023.   \nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452\u2013466, 2019.   \nSanguk Lee, Tai-Quan Peng, Matthew H Goldberg, Seth A Rosenthal, John E Kotcher, Edward W Maibach, and Anthony Leiserowitz. Can large language models capture public opinion about global warming? an empirical assessment of algorithmic fidelity and bias. ArXiv preprint arXiv:2311.00217, 2023.   \nTao Li, Daniel Khashabi, Tushar Khot, Ashish Sabharwal, and Vivek Srikumar. Unqovering stereotyping biases via underspecified questions. In Findings of the Association for Computational Linguistics, pages 3475\u20133489, 2020.   \nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher R\u00e9, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models. ArXiv preprint arxiv:2211.09110, 2022.   \nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Annual Meeting of the Association for Computational Linguistics, volume 1, pages 8086\u20138098, 2022.   \nAndrew Mao, Naveen Raman, Matthew Shu, Eric Li, Franklin Yang, and Jordan Boyd-Graber. Eliciting bias in question answering models through ambiguity. In Workshop on Machine Reading for Question Answering, pages 92\u201399, 2021.   \nJonathan Mellon, Jack Bailey, Ralph Scott, James Breckwoldt, Marta Miori, and Phillip Schmedeman. Do ais know what the most important issue is? using language models to code open-text social survey responses at scale. SSRN Electronic Journal, 2022. ", "page_idx": 11}, {"type": "text", "text": "Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In Conference on Empirical Methods in Natural Language Processing, pages 2381\u20132391, 2018. ", "page_idx": 12}, {"type": "text", "text": "MosaicML. Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs, 2023. URL www.mosaicml.com/blog/mpt-7b. ", "page_idx": 12}, {"type": "text", "text": "Fabio Motoki, Valdemar Pinho Neto, and Victor Rodrigues. More human than human: Measuring chatgpt political bias. Available at SSRN 4372349, 2023. ", "page_idx": 12}, {"type": "text", "text": "Laura K Nelson, Derek Burk, Marcel Knudsen, and Leslie McCall. The future of coding: A comparison of hand-coding and three types of computer-assisted text analysis methods. Sociological Methods & Research, 50(1):202\u2013237, 2021. ", "page_idx": 12}, {"type": "text", "text": "OpenAI. Gpt-4 technical report. ArXiv preprint arXiv:2303.08774, 2023. ", "page_idx": 12}, {"type": "text", "text": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 2022. ", "page_idx": 12}, {"type": "text", "text": "Ethan Perez, Sam Ringer, Kamile Lukosiute, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, Andy Jones, Anna Chen, Benjamin Mann, Brian Israel, Bryan Seethor, Cameron McKinnon, Christopher Olah, Da Yan, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Guro Khundadze, Jackson Kernion, James Landis, Jamie Kerr, Jared Mueller, Jeeyoon Hyun, Joshua Landau, Kamal Ndousse, Landon Goldberg, Liane Lovitt, Martin Lucas, Michael Sellitto, Miranda Zhang, Neerav Kingsland, Nelson Elhage, Nicholas Joseph, Noemi Mercado, Nova DasSarma, Oliver Rausch, Robin Larson, Sam McCandlish, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Jack Clark, Samuel R. Bowman, Amanda Askell, Roger Grosse, Danny Hernandez, Deep Ganguli, Evan Hubinger, Nicholas Schiefer, and Jared Kaplan. Discovering language model behaviors with model-written evaluations. In Findings of the Association for Computational Linguistics, pages 13387\u201313434, 2023. ", "page_idx": 12}, {"type": "text", "text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. ", "page_idx": 12}, {"type": "text", "text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: $100{,}000{+}$ questions for machine comprehension of text. In Conference on Empirical Methods in Natural Language Processing, 2016. ", "page_idx": 12}, {"type": "text", "text": "Joshua Robinson and David Wingate. Leveraging large language models for multiple choice question answering. In International Conference on Learning Representations, 2023a. ", "page_idx": 12}, {"type": "text", "text": "Joshua Robinson and David Wingate. Leveraging large language models for multiple choice question answering. In International Conference on Learning Representations, 2023b. ", "page_idx": 12}, {"type": "text", "text": "J\u00e9r\u00f4me Rutinowski, Sven Franke, Jan Endendyk, Ina Dormuth, and Markus Pauly. The SelfPerception and Political Biases of ChatGPT. ArXiv preprint arXiv:2304.07333, 2023. ", "page_idx": 12}, {"type": "text", "text": "Nathan E Sanders, Alex Ulinich, and Bruce Schneier. Demonstrations of the potential of ai-based political issue polling. ArXiv preprint arXiv:2307.04781, 2023. ", "page_idx": 12}, {"type": "text", "text": "Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4149\u20134158, 2019.   \nLindia Tjuatja, Valerie Chen, Sherry Tongshuang Wu, Ameet Talwalkar, and Graham Neubig. Do llms exhibit human-like response biases? a case study in survey design. ArXiv preprint arXiv:2311.04076, 2023.   \nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. ArXiv preprint arXiv:2307.09288, 2023.   \nXinpeng Wang, Bolei Ma, Chengzhi Hu, Leon Weber-Genzel, Paul R\u00f6ttger, Frauke Kreuter, Dirk Hovy, and Barbara Plank. \"my answer is C\": First-token probabilities do not match text answers in instruction-tuned language models. arXiv preprint arXiv:2402.14499, 2024.   \nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697\u201312706, 2021.   \nCaleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi Yang. Can large language models transform computational social science? Computational Linguistics, 50(1): 237\u2013291, 2024. ", "page_idx": 13}, {"type": "text", "text": "A Experimental details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We use the American Community Survey (ACS) Public Use Microdata Sample (PUMS) files made available by the U.S. Census Bureau.5 The data itself is governed by the terms of use provided by the Census Bureau.6 We download the data directly from the U.S. Census using the Folktables Python package [Ding et al., 2021]. We download the files corresponding to the year 2019. ", "page_idx": 14}, {"type": "text", "text": "We downloaded the publicly available language model weights from their respective official HuggingFace repositories. We run the models in an internal cluster. The total number of GPU hours needed to complete all experiments is approximately 1500 (NVIDIA A100). The budget spent querying the OpenAI models was approximately $\\mathbb{S}200$ . ", "page_idx": 14}, {"type": "text", "text": "We open source the code to replicate all experiments.7 ", "page_idx": 14}, {"type": "text", "text": "In addition, the repository contains notebooks to visualize the results of our investigations under different prompt ablations. ", "page_idx": 14}, {"type": "text", "text": "A.1 Survey questionnaire used ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The exact questionnaire used in our experiments can be retrieved from our Github repository. We consider 25 questions from the 2019 ACS questionnaire corresponding to the following variables in the Public Use Microdata Sample: SEX, AGEP, HISP, RAC1P, NATIVITY, CIT, SCH, SCHL, LANX, ENG, HICOV, DEAR, DEYE, MAR, FER, GCL, MIL, WRK, ESR, JWTRNS, WKL, WKWN, WKHP, COW, PINCP. We take all questions as they appear in the ACS, with the exceptions: ", "page_idx": 14}, {"type": "text", "text": "\u2022 HISP: The ACS contains 5 answer choices corresponding to different Hispanic, Latino, and Spanish origins, and respondents are instructed to write down their origin if their origin is not among the choices provided. We instead provide two choices: \u201cYes\u201d and \u201cNo\". \u2022 RAC1P: The ACS contains 15 answer choices, allows for selecting multiple choices, and respondents are instructed to write down their race if not among those in the multiple choice. The PUMS then provides up to 170 race codes (RAC2P and RAC3P). We instead present 9 choices, corresponding to the race codes of the RAC1P varible in the PUMS data dictionary. ", "page_idx": 14}, {"type": "text", "text": "Additionally, the variables ESR and COW are not directly associated with any single question in the ACS, but rather aggregate employment information. We formulate them as questions by taking the PUMS data dictionary\u2019s variable and codes descriptions. Lastly, for the questions corresponding to the variables AGE, WKWN, WKHP, and PINCP, respondents are asked to write down an integer number. We convert such questions to multiple-choice via binning. ", "page_idx": 14}, {"type": "text", "text": "B Detailed experimental results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Model responses across questions before and after adjusting for A-bias ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The results in this section complement Section 3, and pertain non-instruction-tuned language models. When surveying models without choice order randomization, we observe that the entropy of model responses tends to increase log-linearly with model size, often matching the entropy of the uniform distribution for the larger models. This trend is consistent across survey questions, irrespective of the question\u2019s distribution over responses observed in the U.S. census (Figure 8). ", "page_idx": 14}, {"type": "text", "text": "B.2 A-bias of instruction-tuned models ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The results in this section complement Section 3.1, and pertain instruction-tuned language models as well as language models fine-tuned with reinforcement learning with human feedback (RLHF). We observe that the strength of A-bias for these models, plotted in Figure 9, is comparable to that of base pre-trained models, plotted in Figure 3. This motivates the use of choice-order randomization in order to eliminate confounding due to labeling biases in models\u2019 responses. ", "page_idx": 14}, {"type": "image", "img_path": "Oo7dlLgqQX/tmp/a20bee61addddbf85de10d9d469ea8d01250f4ff6a859ee54f3fb2e4e1e15f52.jpg", "img_caption": ["Figure 8: Normalized entropy of survey responses for individual questions (without adjustment). "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "Oo7dlLgqQX/tmp/93e2700269004968af9bc8592726bd46074a9d9964d0f6601e2f0d5150490627.jpg", "img_caption": ["Figure 9: A-bias of instruction-tuned models. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "B.3 Relative alignment across demographic subgroups ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The results presented here complement those of Section 5. We plot the average KL divergence between each language model and each demopgrahic subpopulation (U.S. state) against the average entropy of the subgroup\u2019s responses. For readability, we split models into GPT-2 and GPT-Neo (Figure 6(a)), OpenAI\u2019s API models (Figure 6(b)), MPT, Pythia, GPT-NeoX and its instruction variants (Figure 6(c)), and LLaMA, Llama 2 and its instruction and chat variants (Figure 6(d)). ", "page_idx": 15}, {"type": "text", "text": "C Ordering bias: further experiments ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We conduct additional randomization experiments pertaining to answer choice position and labeling bias, complimenting Section 3. We consider the GPT-2, GPT Neo, MPT, Pythia, and LLaMA models. The experiments follow a consistent setup: ", "page_idx": 15}, {"type": "text", "text": "1. We randomize both the order in which choices are presented and the label (i.e., letter) assigned to each answer choice. For example, for the \"sex\" question, the possible combinations are \u201cA. Male B. Female\u201d, \u201cA. Female B. Male\u201d, \u201cB. Male A. Female\u201d, and \u201cB. Female A. Male\u201d. Note that in the experiments presented in Section 3.1 we only randomized over the order in which choices are presented (i.e., the \u201cA\u201d choice was always presented first). 2. We compute the output distribution over responses for choice position (the probability assigned to the first, second, etc., answer choice presented) and letter assignment (the probability assigned to the answer choice assigned \u201cA\u201d, \u201cB\u201d, etc.). ", "page_idx": 15}, {"type": "text", "text": "For each model and survey question, we estimate the expected distribution over responses for both choice position and letter assignment by collecting 3,000 responses (step 2) under different randomizations of choice position and letter assignment (step 1). A model with no position and labeling biases would assign the same probability distribution to answer choices (e.g., \u201cmale\u201d and \u201cfemale\u201d) regardless of position or letter assignment, and therefore the expected distributions over position (e.g., selecting the first choice) and letter assignment (e.g., selecting \u201cA\u201d) would be uniform. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "C.1 Disentangling ordering bias into positioning bias and labeling bias ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We perform chi-square tests to determine whether language models\u2019 output responses distributions over position and letter assignment significantly deviate from the uniform distribution (i.e., if there exists statistically significant bias in position or letter assignment). Since we collect 3,000 response distributions under randomized choice position and letter assignment, we ensure a high test power $\\geq$ 0.98) in detecting small effect sizes (0.1) at a significance level of 0.05. ", "page_idx": 16}, {"type": "text", "text": "We find that models exhibit significant positioning and labelling for most survey questions, see Figure 10. We observe that labelling is more prevalent that positioning bias. While both tend to decrease with model size, order bias decreases more significantly with model size, whereas labeling bias tends to be very prevalent across all model sizes. In Figure 11 we plot both the strength of A-bias and first-choice bias across survey questions. The strength of A-bias tends to be greater than that of first-choice bias, particularly for the smaller models. ", "page_idx": 16}, {"type": "image", "img_path": "Oo7dlLgqQX/tmp/b4190fdf0804425cfc5293f65b80b9811b6e2929bcad5884b7560d532b9fd527.jpg", "img_caption": ["Figure 10: All models exhibit statistically significant letter and ordering bias for most survey questions. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "Oo7dlLgqQX/tmp/559bdfe4c4dfe079eeb3b440a49665a4d9d8215cf10e205f01b7b2a7693a2c74.jpg", "img_caption": ["Figure 11: Models, particularly those with less than a few billion parameters, tend to exhibit stronger A-bias than first-choice bias. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "C.2 I-bias ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We hypothesize that A-bias is prevalent because the single character \u201cA\u201d is relatively frequent as the starting word of a sentence in written English. We test this hypothesis by replacing the character \u201cB\u201d with \u201cI\u201d when presenting the survey questions, since the character \u201cI\u201d is even more frequent as the starting word of a sentence in written English. We randomize over choice ordering and label assignment as in the previous evaluation. We find that, when presenting both \u201cA\u201d and \u201cI\u201d, small models then exhibit I-bias rather than A-bias (Figure 12), supporting our initial hypothesis. ", "page_idx": 16}, {"type": "text", "text": "C.3 Using letters with similar frequency in written English ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Motivated by the I-bias experiment, we now examine whether labeling bias can be mitigated by using letters that have similar frequency in written English. Therefore, instead of assigning to choices the ", "page_idx": 16}, {"type": "image", "img_path": "Oo7dlLgqQX/tmp/d0daae4f3dc2ff58afe74b89f8f1f87358227a7de1f6ef2a94ac767ffa410d12.jpg", "img_caption": ["(a) A-bias in the \u201cA\u201d, \u201cI\u201d randomization experiment. (b) I-bias in the \u201cA\u201d, \u201cI\u201d randomization experiment. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 12: When both \u201cA\u201d and \u201cI\u201d are present, small models exhibit I-bias rather than A-bias. ", "page_idx": 17}, {"type": "image", "img_path": "Oo7dlLgqQX/tmp/0fcb08d242596338edd6f0f682cf6a1d1452fd4419637d7904c65837f1063c74.jpg", "img_caption": ["Figure 13: \u201cR\u201d, \u201cS\u201d, \u201cN\u201d, etc. randomization experiment. All models, irrespective of size, exhibit statistically significant letter and positioning bias for most survey questions. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "labels \u201cA\u201d, \u201cB\u201d, etc. we assign the following labels: \u201cR\u201d, \u201cS\u201d, \u201cN\u201d, \u201cL\u201d, \u201cO\u201d, \u201cT\u201d, \u201cM\u201d, \u201cP\u201d, \u201cW\u201d, \u201cU\u201d, \u201cY\u201d, \u201cV\u201d. We find that, compared to the \u201cA\u201d, \u201cB\u201d, etc. randomization experiment, the percentage of questions for which models exhibit significant labeling bias somewhat decreases (Figure 13). However, models tend to exhibit substantially more position bias. This indicates that, in the absence of a label that provides a strong signal (e.g., \u201cA\u201d or \u201cI\u201d), models tend to exhibit significantly higher choice-ordering bias, irrespective of model size. ", "page_idx": 17}, {"type": "text", "text": "D Prompt ablations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We reproduce our experiments using different prompts to query the model. Due to the cost of querying OpenAI\u2019s models, we only perform these ablations for models with publicly available weights. The notebooks with all figures can be retrieved from our Github repository. 8 ", "page_idx": 17}, {"type": "text", "text": "Overall, the prompt ablation results are very consistent with the findings presented in the main text of the paper. In the following we provide an overview over the different ablations performed. ", "page_idx": 17}, {"type": "text", "text": "D.1 System rompt used for GPT-3.5 and GPT-4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "When querying GPT-3.5, GPT-4, and GPT-4 Turbo, we use the system prompt Please respond with a single letter., as otherwise for most questions none of the top-5 logits correspond to answer choice labels (e.g., \u201cA\u201d, \u201cB\u201d). Note that this problematic arises due to the fact that the OpenAI API only allows access to the top 5 logits. We adapt the system prompt used by Dorner et al. [2023] in the context of surveying GPT-4 with standarized personality tests. ", "page_idx": 17}, {"type": "text", "text": "D.2 Individual survey questions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "First, we use different styles to prompt individual survey questions. We enumerate the prompt styles as (P1)-(P8). ", "page_idx": 17}, {"type": "text", "text": "Additional context. We first explore whether including additional context signaling that the questions presented are from the American Community Survey, or that they are to be answered by ", "page_idx": 17}, {"type": "text", "text": "U.S. households. Keeping identical survey questions, we append at the start of the prompt one of the following sentences: ", "page_idx": 18}, {"type": "text", "text": "(P1) Bellow is a question from the American Community Survey. (P2) Answer the following question from the American Community Survey. (P3) Answer the following question as if you lived at a household in the United States. ", "page_idx": 18}, {"type": "text", "text": "Asking questions in the second person. We change the framing of the questions. ", "page_idx": 18}, {"type": "text", "text": "(P4) We modify the survey questionnaire such that questions are formulated in the second person rather than the third person (e.g., \u201cWhat is your sex?\u201d instead of \u201cWhat is this person\u2019s sex?\u201d). ", "page_idx": 18}, {"type": "text", "text": "Including instructions. Following the prompt ablation of Santurkar et al. [2023], we append at the start of the prompt one of the following instructions: ", "page_idx": 18}, {"type": "text", "text": "(P5) Please read the following multiple-choice question carefully and select ONE of the listed options.   \n(P6) Please read the multiple-choice question below carefully and select ONE of the listed options. Here is an example of the format:\\nQuestion: Question $1\\backslash\\mathtt{n A}$ . Option 1\\nB. Option $\\scriptstyle2\\setminus\\mathtt{n}$ C. Option 3\\nAnswer: C ", "page_idx": 18}, {"type": "text", "text": "Chat-style prompt. We consider the prompt used by Durmus et al. [2023]: ", "page_idx": 18}, {"type": "text", "text": "(P7) Human: {question}\\nHere are the options:\\n{options}\\n Assistant: If had to select one of the options, my answer would be ", "page_idx": 18}, {"type": "text", "text": "Interview-style prompt. We consider the prompt used by Argyle et al. [2023]: ", "page_idx": 18}, {"type": "text", "text": "(P8) Interviewer: {question}\\n{options}\\nMe: ", "page_idx": 18}, {"type": "text", "text": "D.3 Sequential generation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We use different prompts to integrate a model\u2019s previous responses when prompting subsequent survey questions. Instead of summarizing previous responses using bullet points as in Section 5, we keep previous questions and answers in-context. ", "page_idx": 18}, {"type": "text", "text": "Question answering. Keeping questions and answers in-context resembles the typical few-shot Q&A setting. For instance, prompting for the third question in the questionnaire corresponds to ", "page_idx": 18}, {"type": "text", "text": "Question: {question 1}\\n{options 1}\\nAnswer:{answer 1}\\n Question: {question 2}\\n{options 2}\\nAnswer:{answer 2}\\n Question: {question 3}\\n{options 3}\\nAnswer: ", "page_idx": 18}, {"type": "text", "text": "Interview-style prompt. We consider the prompting style used by Argyle et al. [2023]. For instance, prompting for the third question in the questionnaire corresponds to ", "page_idx": 18}, {"type": "text", "text": "Interviewer: {question 1}\\n{options 1}\\nMe:{answer 1}\\n Interviewer: {question 2}\\n{options 2}\\nMe:{answer 2}\\n Interviewer: {question 3}\\n{options 3}\\nMe: ", "page_idx": 18}, {"type": "image", "img_path": "Oo7dlLgqQX/tmp/c4570136ae41bb4be1c4c8b499ddac389ec9da60621d07e6f1e54d7a1a458f09.jpg", "img_caption": ["Figure 14: Reproduction of the experiments in Sections 3 and 4 for the ATP surveys. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "E Results for ATP, GAS, WVS, and ANES surveys ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We reproduce the experiments of Sections 3 and 4 using the ATP, and GAS/WVS used by Santurkar et al. [2023] and Durmus et al. [2023], where questions are presented individually of one another. We additionally reproduce the experiments of Section 5 using the 2016 ANES questionnaire considered by Argyle et al. [2023], where questions are presented in sequence. We do not consider OpenAI\u2019s models as the cost to reproduce the experiments via the OpenAI API exceeds our budget. We obtain very similar results to those of the ACS presented in the main text of the paper. The notebooks with all figures can be retrieved from our Github repository. 9 ", "page_idx": 19}, {"type": "text", "text": "E.1 ATP surveys ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We obtain the ATP survey questions and their corresponding human responses from the OpinionsQA repository.10 We present all answer choices when querying the models, but exclude the answer choices corresponding to refusals from our analysis similarly to Santurkar et al. [2023]. When comparing the similarity of models\u2019 responses to different demographic subgroups, we use the demographic subgroups and the alignment metric considered by Santurkar et al. [2023]. For such metric, higher values of alignment indicate that models\u2019 responses are more similar to the reference demographic group. We find that all models are more \u201caligned\u201d with the uniformly random baseline than with any of the demographic subgroups, see Figure 14. ", "page_idx": 19}, {"type": "text", "text": "E.2 GAS and WVS surveys ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We obtain the ATP survey questions and their corresponding human responses from the GlobalOpinionsQA repository.11 When comparing the similarity of models\u2019 responses to the population-level survey responses of different countries, we use the countries and the similarity metric considered by Durmus et al. [2023]. We find that all models produce survey responses that are more similar to those of the uniformly random baseline than to those of any of the demographic subgroups, see Figure 15. ", "page_idx": 19}, {"type": "image", "img_path": "Oo7dlLgqQX/tmp/1a697ea704865456bb2f126768cfb4ac4aafc56ed268d988a2f346d6534028ce.jpg", "img_caption": ["Figure 15: Reproduction of the experiments in Sections 3 and 4 for the GAS/WVS surveys. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "Oo7dlLgqQX/tmp/c2f9a916d3ee277231e29d1613184e9e6d7a41e7d433bc91ed7e1a540003b6ad.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 16: The discriminator test performed on datasets generated using the 2016 ANES survey questionnaire (with choice randomization). ", "page_idx": 20}, {"type": "text", "text": "E.3 Relative alignment for ATP and GAS/WVS surveys ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We consider the alignment measures proposed by Santurkar et al. [2023] and Durmus et al. [2023] on ATP and GAS/VVS opinion surveys for the largest base / instruct models considered. We find that, similarly to our observations for the ACS, the alignment between models and a given subpopulation is highly correlated with the entropy of the subpopulations\u2019 responses. ", "page_idx": 20}, {"type": "text", "text": "E.4 ANES survey ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We present questions in the multiple-choice format described in Section 2, using the Interviewer:, Me: prompt style described by Argyle et al. [2023]. We retrieve the 2016 ANES data from the official website12, and process it such that it matches in form the questionnaire designed by Argyle et al. [2023]. We find that the trained classifiers can discriminate between the model-generated data and the ANES data with very high accuracy $(\\ge\\!99\\%)$ , see Figure 16. ", "page_idx": 20}, {"type": "text", "text": "F Sequential sampling of responses ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Motivated by recent findings of Argyle et al. [2023] we conducted an additional investigation where we seek to flil entire ACS questionnaires in a sequential manner, in order to generate for each language model a synthetic dataset of responses. This data emulates in form the ACS dataset collected by the ", "page_idx": 20}, {"type": "image", "img_path": "Oo7dlLgqQX/tmp/1698d1c01717f0595d6eac6938280c30c752d3e172537eac5ca07c65e08a0c34.jpg", "img_caption": ["Figure 17: Methodology and prompt template used to sequentially sample models\u2019 responses to entire survey questionnaires. We provide the answers to previous question in context when prompting subsequent questions. The output is a tabular dataset of responses. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "U.S. Census Bureau. We then study the extent to which such synthetic datasets resemble the ACS dataset. ", "page_idx": 21}, {"type": "text", "text": "F.1 Methodology ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We present survey questions in the same order as in the ACS questionnaire. When querying a model to answer survey question $q$ , we include a summary of the $q-1$ previously sampled answers in context. 13 We then sample from the model\u2019s output probability distribution over answers, and continue to the next question. We illustrate this sequential process in Figure 17. We refer to Appendix D.3 for results collected with different variations of how a model\u2019s previous answers are integrated into the prompt. We find our results to be robust to these prompt variations. ", "page_idx": 21}, {"type": "text", "text": "For each language model we sample $N{=}100{,}000$ model-generated responses to the ACS. Due to the cost of querying OpenAI\u2019s models, we only survey GPT-4 and sample $N=500$ responses. As a result, we generate for each language model a tabular dataset similar in form to the ACS data, with $N$ rows corresponding to each filled questionnaire and 25 columns corresponding to each question. ", "page_idx": 21}, {"type": "text", "text": "F.2 The discriminator test ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We investigate whether the model-generated datasets resemble the U.S. census data by constructing a binary prediction task aiming to discriminate synthetic responses from census responses. Intuitively, if the two datasets were very dissimilar, then a classifier would be able to achieve high accuracy. Formally, let $\\mathcal{F}$ be class of binary prediction functions mapping each data point (i.e., a row in the tabular dataset) to $\\{0,1\\}$ , then the accuracy of the best $f\\in\\mathcal F$ on the discriminator task provides a lower bound on the total variation (TV) distance between the two empirical data distributions. ", "page_idx": 21}, {"type": "text", "text": "Hence, we train a predictor $f$ to discriminate between the model-generated data and the census data in order to obtain an empirical lower bound on the distance between the two datasets. Specifically, we concatenate to each model-generated dataset a random sample of $N$ individuals from the ACS census data, and introduce a binary label indicating whether each row of the concatenated dataset was model-generated or not. We then train an XGBoost classifier in this binary prediction task. As an additional point of reference, we also consider the accuracy in discriminating between the census data of any given U.S. state and an equally-sized sample of the ACS data of all other U.S. states. ", "page_idx": 21}, {"type": "text", "text": "We report mean test accuracy in Figure 18. We consider 100 different random seeds. We find that the trained classifiers can differentiate between model-generated data and census data with very high accuracy $(>90\\%)$ ) in all cases. Therefore, the empirical distributions corresponding to the modelgenerated data and the census data have TV distance larger than 0.9. These stark results indicate that data generated by sequentially prompting language models with the ACS survey questionnaire bears little similarity with the data collected by surveying the U.S. population. ", "page_idx": 21}, {"type": "text", "text": "F.3 Contrast with silicon samples ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Argyle et al. [2023] propose \u201csilicon sampling\u201d, a methodology to produce synthetic survey respondents using LLMs by conditioning on actual survey respondents. They focus on a subset of ", "page_idx": 21}, {"type": "table", "img_path": "Oo7dlLgqQX/tmp/351f1684a8e157fe53f3d86af089b5645b4fec1025c66616df9d1cb3ff07fba3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 18: Accuracy of the discriminator test. For all language models, it is possible to discriminate with very high accuracy between the ACS census data and model-generated data, $(\\ast)$ before adjustment and $(\\ast)$ after adjustment. We contrast this against the accuracy value of discriminating between the ACS data of any given U.S. state and the rest of the ACS census data $(-)$ . ", "page_idx": 22}, {"type": "text", "text": "12 questions from the 2016 American National Election Studies (ANES) survey. For every human respondent, they construct a corresponding \u201csilicon individual\u201d by querying GPT-3 to predict the ANES respondent\u2019s answer to each survey question given the respondent\u2019s answers to all other questions. Their results indicate that, for the 2016 ANES survey, GPT-3 can be a fairly calibrated predictor of an individual\u2019s answer to some survey question conditioned on the respondent\u2019s answers to all other survey questions.14 ", "page_idx": 22}, {"type": "text", "text": "However, Argyle et al. [2023] emphasize that important insights can be gained by emulating the survey responses of human populations \u201cprior to or in the absence of human data\u201d. In this work we have considered precisely the setting where models\u2019 responses are obtained in the absence of human data.15 To investigate how our findings transfer to the ANES, we reproduce the experiments of Section F using the 2016 ANES survey questionnaire considered by Argyle et al. [2023] and their \u201cinterview-style\u201d prompt. We apply the discriminator test, and find that the trained classifiers can discriminate between the model-generated data and the ANES data with accuracy $>99\\,\\%$ (see Appendix E), indicating that models\u2019 responses are markedly different to those in the ANES data. ", "page_idx": 22}, {"type": "text", "text": "Thus, the fact that models may perform reasonably well at feature imputation tasks (e.g., predicting an individual\u2019s answer to some question given their answers to all other questions) does not imply that models can generate synthetic respondents that resemble the responses obtained by surveying human populations. This suggests caution when using LLMs to emulate human populations at present time, in particular in the absence of human data. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The claims in the abstract and introduction directly refer to the experiments and results detailed in the paper\u2019s main body. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: See Section 2. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Yes, see Section 2 and Appendix A. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/pu blic/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: See Appendix A and the code release. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The main figures contain exact measures. We conduct significance tests on Section B. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Yes, see Appendix A. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We confirm that the research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We argue throughout the paper that current evaluation practices might result in misleading claims regarding what subgroups current models best represent. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: We do not release any models. We release models\u2019 survey responses, which pose no risk of misuse. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We properly credit the authors of the models considered, as well as the sources of the surveys considered. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]