[{"figure_path": "Y8YVCOMEpz/figures/figures_3_1.jpg", "caption": "Figure 1: General Form of LinFormer/SSM/LinRNN Mechanisms. The general form equips with two modes of parallel and recurrent computation which enjoys both training and inference efficiency.", "description": "This figure shows the unified model of LinFormer, SSM, and LinRNN.  It highlights that these seemingly different models share a common underlying structure, differing mainly in how they maintain the hidden state and the specific functions used for query, key, and value mappings. The figure illustrates both parallel and recurrent computation modes, showcasing the flexibility and efficiency of the unified model.", "section": "3 General Form of LinFormer/SSM/LinRNN Mechanisms"}, {"figure_path": "Y8YVCOMEpz/figures/figures_6_1.jpg", "caption": "Figure 2: Recurrent form of MetaLA. We mark all three enhancements in red.", "description": "This figure shows the recurrent form of the MetaLA (Meta Linear Attention) model.  The diagram illustrates the flow of information through the model, highlighting three key enhancements made to improve performance: (1) Removal of unnecessary Key matrices, (2) Self-augmentation to enhance a token's attention to itself (avoiding attention dilution), and (3) The use of short convolutions to improve local interactions.  These three key enhancements are marked in red in the diagram. The diagram shows the input (xt), the hidden state (St-1), the updated hidden state (St), the output (yt), and several intermediate components involved in calculations for Query (qt), Value (vt), decay (\u03b1t), output gate (gt), and augmented output (ot).", "section": "5 MetaLA Transformer"}, {"figure_path": "Y8YVCOMEpz/figures/figures_6_2.jpg", "caption": "Figure 3: Accuracy (%) on the synthetic MQAR task.", "description": "This figure shows the accuracy achieved on a synthetic Multi-Query Associative Recall (MQAR) task, comparing MetaLA against several other linear attention models (Base, GLA, RWKV, Mamba).  The results are shown for both sequence lengths of 256 and 512, and across varying model dimensions (64, 128, 256, 512).  It demonstrates the relative performance of MetaLA compared to other approaches, highlighting its superior accuracy, particularly at higher model dimensions and sequence length.", "section": "6 Experiments"}, {"figure_path": "Y8YVCOMEpz/figures/figures_23_1.jpg", "caption": "Figure 1: General Form of LinFormer/SSM/LinRNN Mechanisms. The general form equips with two modes of parallel and recurrent computation which enjoys both training and inference efficiency.", "description": "This figure illustrates the general form of LinFormer, SSM, and LinRNN mechanisms, unifying their recurrent and parallel computation modes.  The unified form reveals shared components, including query, key, and value matrices, despite the differences in their origins and forms.  The recurrent form maintains a hidden state which is updated to maintain history information, similar to how softmax attention uses a KV cache. The parallel form computes the attention mechanism in parallel but still demonstrates a relationship to the hidden state.  This unification facilitates a deeper understanding of these models and their relationship to softmax attention.", "section": "3 General Form of LinFormer/SSM/LinRNN Mechanisms"}, {"figure_path": "Y8YVCOMEpz/figures/figures_25_1.jpg", "caption": "Figure 1: General Form of LinFormer/SSM/LinRNN Mechanisms. The general form equips with two modes of parallel and recurrent computation which enjoys both training and inference efficiency.", "description": "This figure illustrates the unified form of LinFormer, SSM, and LinRNN mechanisms. It shows that these seemingly different models can be represented by a common structure encompassing Query, Key, and Value matrices, along with parallel and recurrent computation modes.  This unification highlights the key design differences between these linear models, mainly focusing on hidden state size and maintenance, as well as how they map parameters, and facilitates understanding their relationship to softmax attention.", "section": "3 General Form of LinFormer/SSM/LinRNN Mechanisms"}]