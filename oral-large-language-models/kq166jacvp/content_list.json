[{"type": "text", "text": "Aligner: Efficient Alignment by Learning to Correct ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiaming Ji\u2217 Boyuan Chen\u2217 Hantao Lou Donghai Hong Borong Zhang Xuehai Pan Juntao Dai Tianyi Qiu Yaodong Yang\u2020 ", "page_idx": 0}, {"type": "text", "text": "Center for AI Safety and Governance, Institute for AI, Peking University ", "page_idx": 0}, {"type": "text", "text": "Project Website: https://pku-aligner.github.io ", "page_idx": 0}, {"type": "text", "text": "{jiamg.ji,cbylll,lht_pku,donghai.hong}@stu.pku.edu.cn yaodong.yang@pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "With the rapid development of large language models (LLMs) and ever-evolving practical requirements, finding an efficient and effective alignment method has never been more critical. However, the tension between the complexity of current alignment methods and the need for rapid iteration in deployment scenarios necessitates the development of a model-agnostic alignment approach that can operate under these constraints. In this paper, we introduce Aligner, a novel and simple alignment paradigm that learns the correctional residuals between preferred and dispreferred answers using a small model. Designed as a model-agnostic, plug-andplay module, Aligner can be directly applied to various open-source and API-based models with only one-off training, making it suitable for rapid iteration. Notably, Aligner can be applied to any powerful, large-scale upstream models. Moreover, it can even iteratively bootstrap the upstream models using corrected responses as synthetic human preference data, breaking through the model\u2019s performance ceiling. Our experiments demonstrate performance improvements by deploying the same Aligner model across 11 different LLMs, evaluated on the 3H dimensions (helpfulness, harmlessness, and honesty). Specifically, Aligner-7B has achieved an average improvement of $68.9\\%$ in helpfulness and $23.8\\%$ in harmlessness across the tested LLMs while also effectively reducing hallucination. In the Alpaca-Eval leaderboard, stacking Aligner-2B on GPT-4 Turbo improved its LC Win Rate from $55.0\\%$ to $58.3\\%$ , surpassing GPT-4 Omni\u2019s $57.5\\%$ Win Rate (community report). ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The alignment of LLMs with human intentions and values has recently gained significant attention [1]. Among the various methods, supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) [2, 3] have emerged as practical approaches. SFT leverages human demonstrations to fine-tune LLMs and instruct the model on desired actions, whereas RLHF trains a reward model (RM) based on human preferences and fine-tunes LLMs using feedback signals from the RM through reinforcement learning (RL) methods [4]. ", "page_idx": 0}, {"type": "text", "text": "Despite the effectiveness of these methods [5, 6, 7, 8, 9] in meeting 3H (helpfulness, harmlessness, and honesty) standards [10], they suffer from challenges such as high training resource consumption and difficulty in ensuring consistent performance [11]. Meanwhile, in real-world scenarios, alignment requirements are dynamically changing [12]. Models may encounter cases outside of alignment training and exhibit undesirable behaviors, which are difficult to address immediately using timeconsuming methods such as SFT and RLHF. ", "page_idx": 0}, {"type": "image", "img_path": "kq166jACVP/tmp/2d136e4f4919ec534fcaf14251f2c1ac22c3be53bf2f4be089b66d46d5865d53.jpg", "img_caption": ["Figure 1: (Left) Architecture of the Aligner module and illustration of its behavior in semantic space. As a plug-and-play module, Aligner stack upon an upstream LLM. The Aligner redistributes initial answers from the upstream model into more helpful and harmless answers, thus aligning the composed LLM responses with human intentions. (Right) Analogy of Aligner as a residual learning enhancer for LLMs in architecture and capabilities. Like a residual block that adds modifications via a shortcut without altering the base structure, the Aligner employs a copy and correct method to improve the original answer. This analogy highlights the Aligner\u2019s dual role in preserving the parameter of the upstream model while enhancing it to align with desired outcomes. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Can we develop an efficient, lightweight, and model-agnostic alignment method? ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Inspired by residual learning [13], we simplify the alignment process by focusing on copy and correction operations. We introduce an efficient alignment paradigm, the Aligner, without involving any RL processes, as shown in Figure 1. Specifically, Aligner is fine-tuned on a preference dataset to learn the correctional residuals between preferred and non-preferred responses and then stacked on the upstream model to achieve corrected alignment. Here, the upstream LLM refers to models targeted for alignment and is compared to the source model in the RLHF process. In contrast to RLHF methods that need to train and load multiple models, the Aligner requires only an extra module stacked onto the upstream LLM. Moreover, our method\u2019s computational resource demand depends solely on the desired efficacy of the Aligner, not on the parameter size of the upstream LLMs. ", "page_idx": 1}, {"type": "text", "text": "From the perspective of representation learning [14, 15, 16], Aligner exhibits an interpretable residual behavior. As shown in Figure 4, Aligner decides the degree of reference to the original response and the extent of additional correction based on the quality of the original answers in the early layers, whereas its middle and late layers are used to implement this decision. The mechanism is simpler than directly learning the mapping from input queries to aligned answers. This simplicity indicates that small Aligner can also learn complex correction patterns, demonstrating their capability to steer powerful models with relatively little inference, which further underscores the superiority of our Aligner paradigm. ", "page_idx": 1}, {"type": "text", "text": "In summary, Aligner presents several significant advantages: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Resource Efficient. Without extra models such as the actor, critic, reward, and reference model, our Aligner is a small model trained on the preference dataset to learn correctional residuals. Specifically, when aligning a 70B LLM, Aligner-7B occupies 11.25 times smaller than DPO and 22.5 times smaller than $\\mathrm{{RLHF^{2}}}$ regarding training parameters. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Plug and Play. The Aligner\u2019s plug-and-play nature and model agnosticism make it ideal for API-based models without parameter access. Once trained, the Aligner can be applied to various upstream LLMs without parameter adjustments. Experiments showed that the Aligner-7B model enhances helpfulness and harmlessness across 11 models, including API-based/open-source safety-aligned/safety-unaligned models. Experiment results demonstrate that the Aligner-7B increased GPT-4\u2019s helpfulness by $17.5\\%$ and its harmlessness by $26.9\\%$ . ", "page_idx": 1}, {"type": "text", "text": "2 Aligner ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Preliminary: Supervised Fine-Tuning (SFT) SFT aims to finetune the pretrained LLM to generate target answers using supervised learning \u2014 specifically, maximum likelihood estimation \u2014 on a curated high-quality dataset $\\mathcal{D}_{\\mathrm{SFT}}=\\left\\{\\pmb{x}^{(i)},\\pmb{y}^{(i)}\\right\\}_{i=1}^{N}$ . The goal is to obtain a model $\\pi_{\\theta}^{\\mathrm{SFT}}$ with the following training objective: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\pmb{\\theta}}{\\mathrm{minimize}}\\,\\mathcal{L}(\\pmb{\\theta};\\mathcal{D}_{\\mathrm{SFT}})=-\\mathbb{E}_{(\\pmb{x},\\pmb{y})\\sim\\mathcal{D}_{\\mathrm{SFT}}}[\\log\\pi_{\\pmb{\\theta}}(\\pmb{y}|\\pmb{x})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Similarly, illustrated in Figure 1, Aligner improves alignment between the model and human intentions by redistributing the model\u2019s answers through conditional generation. In practical implementation, Aligner only needs to make a minor adjustment to the SFT training code (only need to change one line of code), as detailed in Appendix D. ", "page_idx": 2}, {"type": "text", "text": "Overall, the whole pipeline of Aligner training can be summarized as follows: Based on a preference dataset, the model is fine-tuned to learn the correctional residuals between preferred and non-preferred responses. After a single training session, this model can be deployed on any model to achieve corrected alignment. ", "page_idx": 2}, {"type": "text", "text": "Model Training Based on the above procedures, we have constructed the dataset $\\mathcal{M}\\;=\\;$ $\\{\\pmb{x}^{(i)},\\pmb{y}_{o}^{(i)},\\pmb{y}_{c}^{(i)}\\}_{i=1}^{N}$ ,r  awchciocrhd $\\textbf{\\em x}$ gr teop reessteanbtlsis thheed  upsrienr\u2019csi pqleuse.r yT, $\\scriptstyle{\\pmb y}_{o}$ miso dtehle t roariingiinnga lp raoncsewsse ri,s  arnelda $\\scriptstyle y_{c}$ e liys straightforward. We train the Aligner, a conditional seq2seq model $\\mu_{\\phi}({\\pmb y}_{c}|{\\pmb y}_{o},{\\pmb x})$ parameterized by $\\phi$ , to redistribute the preliminary answers $\\scriptstyle{\\pmb y}_{o}$ to the aligned answer $\\scriptstyle y_{c}$ . Demonstrated in Figure 1, the composed answer generation process for aligned answers based on the upstream LLM $\\pi_{\\theta}$ is: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pi^{\\prime}(y_{c}|x)=\\sum_{y_{k}}\\mu_{\\phi}(y_{c}|y_{k},x)\\pi_{\\theta}(y_{k}|x)\\geqslant\\mu_{\\phi}(y_{c}|y_{o},x)\\pi_{\\theta}(y_{o}|x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pmb{y}_{k}$ is a possible answer generated by upstream LLM $\\pi_{\\theta}$ . By calculating empirical loss on the whole dataset $\\mathcal{M}$ , we can get equation (3) from equation (2): ", "page_idx": 2}, {"type": "equation", "text": "$$\n-\\mathbb{E}_{\\mathcal{M}}[\\log\\pi^{\\prime}(y_{c}|\\pmb{x})]\\leqslant-\\mathbb{E}_{\\mathcal{M}}[\\log\\mu_{\\phi}(y_{c}|y_{o},\\pmb{x})]-\\mathbb{E}_{\\mathcal{M}}[\\log\\pi_{\\theta}(y_{o}|\\pmb{x})].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The second term in equation (3) is not related to the Aligner parameter and the training objective for Aligner can be derived as equation (4): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{minimize}_{\\phi}\\mathcal{L}_{\\mathrm{Aligner}}(\\phi,\\mathcal{M})=-\\mathbb{E}_{\\mathcal{M}}\\left[\\log\\mu_{\\phi}\\left(y_{c}|y_{o},\\pmb{x}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "By optimizing this objective, we actually optimize the upper bound of the SFT training objective, which ensures that $\\scriptstyle y_{c}$ is effectively learned. It is worth noting that Aligner does not require access to the parameters of the upstream LLM $\\pi_{\\theta}$ during both training and inference phases. Aligner takes the user\u2019s query $\\textbf{\\em x}$ and the initial answer $\\scriptstyle y_{o}$ generated by the upstream LLM $\\pi_{\\theta}$ , then generates the answer $\\scriptstyle y_{c}$ which is better aligned with human values. Improving existing answers $\\scriptstyle y_{o}$ allows Aligner to focus on how to align with human values rather than how to answer the given query directly. This significantly reduces the requirements on our model capacity, allowing us to achieve the expected alignment performance with only a small model. ", "page_idx": 2}, {"type": "text", "text": "Aligner\u2019s Training Strategy: Residual Correction We develop an optimized training strategy, termed Residual Correction, which leverages the semantic correctional residuals between answers $\\mathbf{\\Pi}(\\pmb{y}_{o})$ and corrections $({\\pmb y}_{c})$ , as shown in Figure 1. Specifically, we construct a Q-A-A dataset using partial training data to train an identity Aligner initially, a process we term warm-up. Subsequently, we utilize the Q-A-C dataset for training, building upon the identity Aligner. The details of our experiments on a 50K training dataset are shown in Section 3.3. Outside the alignment field, ResNet [13] also uses a similar approach to mitigate the vanishing gradient problem caused by increased neural network depth. ", "page_idx": 2}, {"type": "text", "text": "Resource Analysis between Aligner and RLHF/DPO Compared to RLHF and DPO [6], Aligner shows notable advantages in training resource requirements. Regarding training resources, Aligner7B is more efficient than other methods under similar performance conditions. Specifically, with a 7B source model, DPO requires 1.125 times, and RLHF 2.25 times more resources than Aligner. Additionally, as the source model\u2019s scale increases, the resource demands for other methods rise sharply. For a 70B model, DPO needs 11.25 times, and RLHF 22.5 times more resources than Aligner. However, since Aligner is insensitive to these changes, its training resource requirements remain constant regardless of the source model\u2019s scale, indicating that Aligner is an efficient and lightweight alignment paradigm. ", "page_idx": 2}, {"type": "table", "img_path": "kq166jACVP/tmp/4148ae65ec485cad64a99eb52fc56fc5681a9eb63b340e04c937fc66d9d9bddb.jpg", "table_caption": ["Table 1: Performance of Aligner models. It is shown that Aligner achieves significant performances in all the settings. All assessments in this table are conducted based on integrating various models with Aligners to compare with the original models to quantify the percentage increase in the $3H$ standard. When integrated and assessed in conjunction with various upstream models, the Aligner requires only a single training session (i.e., the Aligner can operate in a zero-shot manner and enhance the performance of all upstream models.) "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we assess the effectiveness of Aligner modules in the 3H (Helpful, Harmless, Honest) evaluation metrics and configurations. For detailed training parameters, please see Appendix D. ", "page_idx": 3}, {"type": "text", "text": "3.1 Experiment Setup ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Preference Datasets We utilize two open-source preference datasets, HH-RLHF [5] and PKUSafeRLHF [17] as our preference datasets. Considering that the preference pairs in PKU-SafeRLHF are generated solely by Alpaca-7B, we additionally construct a 50K preference dataset based on these two preference datasets. The questions in this dataset are sourced from HH-RLHF, PKU-SafeRLHF, and so on, resulting in 27K queries for subsequent answers and corrected answer generation. The original answers are generated using various open-source models, including Alpaca-7B [3], Vicuna(7B,13B,33B) [18], Llama2-(7B,13B)-Chat [19], and Alpaca2-(7B,13B)3. We use GPT-4, Llama2- 70B-Chat, and human annotators to revise the answers in the above Q-A dataset. These revisions are based on well-defined principles, establishing constraints for training the seq2seq model. These principles are aimed at effectively extending to the characteristics we wish LLMs to embody. We focus on the 3H dimensions of LLMs (helpfulness, harmlessness, and honesty) [10]. For those answers that conform to these fundamental principles, we retain the original answers. Figure 2 (a) visually shows the distribution shift before and after the data correction, thereby demonstrating the impact of the revision process on the dataset. More details about the construction of Q-A Datasets can be found in Appendix D.1. ", "page_idx": 3}, {"type": "image", "img_path": "kq166jACVP/tmp/c3cae07c42c931ed784865980e9915dffbfc88419e332c36aa8695b7f6081c92.jpg", "img_caption": ["Figure 2: Distribution of helpfulness and harmlessness scores. (a) The distribution shift in preferred and dis-preferred answers in the training dataset; (b) redistribution shift of Aligner-7B, based on upstream models such as GPT-4 (b1), Alpaca-7B (b2) and Llama2-70B-Chat (b3) in the evaluation dataset. Our findings include: (1) Preferred answers in the training dataset surpasses original answers in both helpfulness and harmlessness; (2) The refuse-to-answer pattern of GPT-4 created an area of overcorrected answers where both helpful and harmless scores are low, and Aligner7B improved these answers by providing additional information and corrections. (3) The Alpaca-7B model, which lacks alignment, had its answers significantly corrected by our Aligner-7B, increasing both scores. (4) The Llama2-70B-Chat model, already aligned with a higher average safety score than the training dataset corrections, benefits from Aligner-7B corrections, significantly enhancing helpfulness while maintaining the harmless score. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Models and Evaluation Datasets We trained the Aligner on three model sizes, specifically based on Gemma-2B [20] and Llama2 (7B, 13B) [19]. To assess the Aligner module, we utilize five datasets: E-Dialogue [21], DialogSum [22], BeaverTails [17], HarmfulQA [23], and TruthfulQA [24]. More details can be found in Appendix B.1. Our evaluation focuses on two model categories: API-based models (e.g., GPT-4 [25], Claude 2 [26]) and Open-Source models (Llama2-(7B, 13B, 70B)-Chat [19]; Vicuna-(7B, 13B, 33B) [18]; Alpaca-7B [3]; Beaver-7B [27]). Notably, the Llama2 and Beaver models have undergone safety alignment processing. ", "page_idx": 4}, {"type": "text", "text": "Evaluation Metrics Our evaluation hinges on three key dimensions: helpfulness, harmlessness, and honesty. The independent characteristics of these dimensions provide a comprehensive perspective on the answers, allowing us to balance information quality with safety and ethical considerations in the evaluation of an answer\u2019s quality. Initial answers are generated by open-source and upstream models, which the Aligner refines to yield corrected answers. More details and examples can be found in Appendix B. ", "page_idx": 4}, {"type": "text", "text": "3.2 Experiment Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As shown in Table 1, we employ Aligners of various sizes, significantly improving the performance of all 11 upstream models with only one training session. Under the 3H standard, Aligner-7B showcases ", "page_idx": 4}, {"type": "text", "text": "an average enhancement of $21.9\\%$ in helpfulness and $23.8\\%$ in harmlessness across the models.   \nRemarkably, Aligner-7B can boost GPT-4\u2019s helpfulness by $17.5\\%$ and harmlessness by $26.9\\%$ . ", "page_idx": 5}, {"type": "text", "text": "Performance on the 3H Standard Aligner keeps the upstream model unchanged, offering adaptability in Aligner model sizing based on available resources. We evaluated Aligner\u2019s effectiveness using five datasets according to the 3H standard. Experiment results show that Aligner significantly enhances the upstream model\u2019s performance across various parameter scales. Particularly, Aligner-7B markedly enhanced the GPT-4 model\u2019s performance across all five dimensions. In the reasoning dimension, with an increase in parameters, Aligner boosts the upstream model\u2019s capability, showcasing the Scaling Laws [28] characteristics. Notably, Aligner excelled in the empathy dimension, further evidencing its efficiency in redistributing the upstream model\u2019s pattern distribution. To detect whether Aligner would generate known false content due to misunderstandings, similar to [19], we use TruthfulQA [24] to measure the reliability of the outputs generated by Aligner in terms of factualness and common sense. The results demonstrate that Aligner does not add extra hallucination information while correcting the upstream model. ", "page_idx": 5}, {"type": "text", "text": "Assessing Aligner\u2019s Stack on Safety-Aligned Models Llama2-Chat models, with their multi-stage alignment process (pre-training, SFT, RLHF), and Beaver, finetuned via Safe RLHF [27], both show modest safety improvements with Aligner. The primary achievement of Aligner is its ability to amplify helpfulness, especially in models predisposed to avoid risky responses. By re-distributing these overly conservative answers, Aligner significantly boosts overall helpfulness. This enhancement in helpfulness is visually represented in Figure 2, showing a rightward shift in Llama2-70B-Chat\u2019s answer distribution under the influence of Aligner-7B, indicating improved helpfulness on a strong safety foundation. ", "page_idx": 5}, {"type": "image", "img_path": "kq166jACVP/tmp/421fb8731b95c6b4ca76717e7181427d1eab014d02c402ec6052aa64a5b753e6.jpg", "img_caption": ["3.3 Ablation Study ", "Figure 3: Ablation study of different identity mapping proportions. We first trained an identity Aligner for identity mapping, followed by extensive residual Q-A-C learning based on this Aligner. Specifically, we formed the Q-A-A dataset by extracting partial data from the training dataset in proportions of $2\\%$ , $10\\%$ , $20\\%$ , and $50\\%$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Ablation on Identity Mapping To verify the effectiveness of different warm-up proportions, we conducted experiments using two representative datasets: BeaverTails and HarmfulQA. As shown in Figure 3, the warm-up step aids the Aligner by initially helping the Aligner learn identity mapping, thus improving the final performance. Moreover, the results further reveal that the effectiveness of the warm-up phase peaks when the proportion is 10k to 50k. However, determining the specific data proportion for warm-up is challenging and requires more training resources. ", "page_idx": 5}, {"type": "text", "text": "Comparison to Self-Refine, Critique Methods Constitutional AI (CAI) [29], Self-Critique [30], and Self-Refine [31], primarily utilize the self-critiquing and refining capabilities of LLMs to enhance their performance. We employ CAI prompts solely during the inference time of LLMs to encourage self-revision of their answers. As demonstrated in Table 2, our method, Aligner, outperforms the baseline considering both helpfulness and harmlessness dimensions. Additionally, baseline methods typically require multiple dialogue iterations and extended context windows for prompt insertion and ongoing self-correction. This could result in longer inference times and considerable consumption of context window length. For more detailed information and analysis, please refer to Appendix B.5. ", "page_idx": 5}, {"type": "table", "img_path": "kq166jACVP/tmp/f2efae25cb7bf4121a6d328713b5a0efbbcf51c73d94732906e806eaec3df53e.jpg", "table_caption": ["Table 2: Ablation study of Aligner\u2019s effectiveness against CAI and Self-Critique. Experiment results revealed that Aligner surpasses these baselines in helpfulness and harmlessness metrics. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Performance of Aligner on the Various Preference Datasets To demonstrate the independence of Aligner from specific datasets, we utilized various open-source RLHF preference datasets. Specifically, we trained on HH-RLHF [5], PKU-SafeRLHF [17, 27] and Ultra-Feedback [32] datasets and compared Aligner with SFT, RLHF, and DPO. After fine-tuning Alpaca-7B with SFT, RLHF, and DPO, we compare these models against the original Alpaca-7B corrected by Aligner. The experiment results (as shown in Table 3) indicate that Aligner\u2019s performance in enhancing the original model\u2019s capabilities is comparable to, or exceeds, that of the baseline methods. Notably, models finetuned with RLHF or DPO tend to generate either conservative answers or fail to recognize dangers while adding helpful information explicitly. Importantly, training with RLHF or DPO methods requires optimizing significantly more models and consuming more training resources than just training an Aligner, e.g., for a 70B model, DPO needs 11.25 times and RLHF 22.5 times more resources. ", "page_idx": 6}, {"type": "table", "img_path": "kq166jACVP/tmp/d0604c5dc9b564a93ae4e816957529a3bfc320b0aeff9aa44a39b52b8741200d.jpg", "table_caption": ["Table 3: Aligner trained on different preference datasets. The experimental results show that Aligner enhances the original model\u2019s capabilities, performing on par with or surpassing baseline methods. Furthermore, these results are consistent across different preference and correction datasets. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "3.4 Interpretability Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "When performing the experiments above, we observed the correction paradigm of Aligner: the correction behavior is not a binary decision between correction and copying. Instead, it follows a conditional generation paradigm, where the degree of reference to the original response and the extent of additional correction depends on the quality of the original answers. To demonstrate that Aligner has learned this correction paradigm as a representation, we conduct the experiment based on representation engineering [14] and activation steering [33, 34, 15]. Specifically, we perform representation extraction and Linear Artificial Tomography (LAT) scan to the Llama2-7B based on the Aligner module. We then utilize the extracted representation to control the Aligner\u2019s generation. ", "page_idx": 6}, {"type": "text", "text": "The results from the representation control experiment indicate that the ratio of adding or subtracting the representation vector in the Aligner activation will significantly affect the magnitude of correction, ranging from directly copying the original response to substantially increasing the extent of normal correction. This provides strong evidence that Aligner has internalized the correction paradigm as a representation. Furthermore, the LAT scan further shows that Aligner decides the degree of correction in its early layers based on the quality of the original response, and after that, it focuses on completing the correction in its middle and late layers. For more details of these experiments, see Appendix B.6. ", "page_idx": 6}, {"type": "image", "img_path": "kq166jACVP/tmp/ca6c7759c449779fd642b504b912db5d139905018c92ab7ce861ccfeb1ba437b.jpg", "img_caption": ["Figure 4: Interpretability experiment results on Aligner. (a)(b) The LAT scan graph of Aligner\u2019s each layer when generating the first 20 output tokens for two given question-answer pairs. A higher value in the graph indicates a more active correction representation in that layer. Specifically, (a) exhibits raised activity, suggesting an enhanced correction action in the output, whereas (b) displays a tendency towards copying the original response. Moreover, the distinct differences between the two graphs are mainly observed in the early layers. This indicates that the decision regarding the degree of correction is made in the early layers of Aligner. (c) The control experiment shows the effectiveness of the extracted correction representation vector in modulating the Aligner\u2019s correction behavior. The relationship between the average levenshtein ratio and representation vector coefficients is approximately linear, with an $R^{2}$ value of approximately 0.93. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4 Multi-round RLHF training via Aligner ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we aim to show that, due to its efficient and plug-and-play features, Aligner can play a crucial role in the multi-round RLHF/DPO pipeline, as illustrated in Figure 5. Typical multi-round pipeline often suffers from reward collapse because the preference dataset used for reward modeling may deviate from the actual answer distribution of the upstream model [35]. This error accumulates over multiple rounds, leading to significant deviations in the model\u2019s final results. Additionally, error accumulation may cause reward over-optimization in certain directions, e.g., generating longer responses irrespective of safety. The involvement of Aligner can help mitigate the problem. ", "page_idx": 7}, {"type": "image", "img_path": "kq166jACVP/tmp/d0034745dd763c954a35e4e1b736cd199befd4ec727910cc7cdb387876f52f64.jpg", "img_caption": ["Figure 5: Illustration of multi-round alignment pipeline with Aligner. As a data augmentation and synthetic tool, Aligner can enhance the upstream model\u2019s response $A$ into an improved response $A^{*}$ , thereby forming a synthetic preference dataset. This dataset can be used to further train the upstream model via RLHF/DPO. Repeating this process allows for multi-round RLHF or DPO. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "As shown in Figure 5, you can use the Aligner (which is trained using the original preference dataset for the next round of RLHF) to refine the upstream model response $A$ into response $A^{*}$ , and $(Q,A,A^{*})$ pairs can be a new preference dataset for training in the next round of RLHF or DPO. This paradigm brings many advantages: ", "page_idx": 7}, {"type": "text", "text": "\u2022 The Aligner inherits the feature of transferring from the dispreferred distribution to the preferred distribution in the preference dataset.   \n\u2022 Aligner modifies the upstream model to produce better answers, bringing the distribution of the resulting preference dataset closer to the answer distribution of the upstream model. This effectively mitigates the reward model collapse problem caused by out-of-distribution (OOD) preference datasets. ", "page_idx": 7}, {"type": "text", "text": "\u2022 The Aligner serves as a synthetic data generator, providing an efficient and repeatable method for constructing preference datasets. ", "page_idx": 8}, {"type": "text", "text": "We conducted three rounds of RLHF and DPO on Alpaca2-7B using the three-round preference dataset from PKU-SafeRLHF [27]. Following this, we trained three rounds of Aligners with the same three-round preference datasets, which were then employed to refine the upstream model and generate new preference datasets. These synthetic preference datasets were subsequently used to fine-tune the upstream model. As illustrated in Figure 6, by aggregating Aligner, Aligner-corrected new pref", "page_idx": 8}, {"type": "image", "img_path": "kq166jACVP/tmp/e6d3b1adac1a3dcd0c156ff07a00f3a060146a692fcb98598d5fe2af2864a415.jpg", "img_caption": ["Figure 6: Multi-round refinement through Aligner. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "erence datasets can effectively enhance two key metrics: improving the model\u2019s safety while ensuring a monotonic increase in helpfulness with each round. In contrast, a typical multi-round RLHF/DPO pipeline only enhances utility, leaving the responses unsafe. ", "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Reinforcement Learning from Human Feedback RLHF aims to align LLMs with human preferences [36, 2], utilizing RL algorithms [4] to train policy models, specifically LLMs, to maximize cumulative rewards from RMs. The RLHF approach involves the distributed training of various models [11] and the annotations by human experts, presenting operational challenges. Consequently, recent research has focused on reducing [37, 38] or eliminating [6] reliance on RMs, aiming to simplify the RLHF process. Simultaneously, [5, 39] employs advanced AI models for data annotation, further streamlining the RLHF process and cutting costs. In contrast to RLHF methods that require several models, Aligner only requires a constrained seq2seq model to meet the alignment objective. Aligner is distinguished by its plug-and-play nature and indifference to specific models and parameters, making it ideal for API-based models without parameter access. ", "page_idx": 8}, {"type": "text", "text": "Inference-time Methods These methods customize LLMs without requiring access to their internal parameters [40, 41, 7], proving especially useful for extremely large models or those available through APIs. However, most of these methods are sensitive to the upstream model. IPA [7] uses a lightweight adapter policy to multiply the next-token probabilities based on the upstream model during the decoding time. However, IPA needs to access the model\u2019s output logit distribution. [8] enhances and refines user prompts to better suit the model, thereby facilitating more comprehensive contextual understanding for inference, similar to in-context learning (ICL) [42, 43]. [44] employs a smaller model to select the best response from several responses generated by the upstream model without fine-tuning upstream models, akin to the BoN (Best of N) selector [45, 46]. In this work, we introduce Aligner\u2014a model-agnostic alignment module designed for seamless integration. Requiring only a single training session, Aligner can align 11 types of upstream models, significantly enhancing their performance according to 3H standards. ", "page_idx": 8}, {"type": "text", "text": "Self-Refinement LLMs do not always generate the coherent output on their first try. Self-refinement methods address this by iteratively improving outputs through self-generated feedback, bypassing additional supervision [47, 48, 49]. For example, SELF-DEBUGGING [50] allows LLMs to debug their predictions via few-shot examples, while [30] found that self-critiquing can expose output weaknesses that aid in fine-tuning, with larger models performing especially well in critique tasks. However, these methods typically depend on a single model\u2019s ability to refine itself. Our work instead uses a separate model, Aligner, which can refine outputs from other models (e.g., 70B model, GPT-4), achieving robust weak-to-strong generalization [51]. This approach bypasses the limitations of smaller models and saves computational resources otherwise spent on self-critiquing. Additionally, by combining Aligner with an external critique model, future iterations could further enhance performance. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduce the Aligner, an efficient, lightweight, and model-agnostic approach to align LLMs. Without the need for additional components such as the actor, critic, reward models, and others, Aligner demonstrates a significant increase in computational efficiency. Under the 3H standard, Aligner-7B showcases an average enhancement of $68.9\\%$ in helpfulness and $23.8\\%$ in harmlessness across the models. Remarkably, Aligner-7B can boost GPT-4\u2019s helpfulness by $17.0\\%$ and harmlessness by $26.9\\%$ . In the Alpaca-Eval leaderboard, stacking Aligner-2B on GPT-4 Turbo (04/09) improved its LC Win Rate [52] from $55.0\\%$ to $58.3\\%$ , surpassing GPT-4 Omni\u2019s $57.5\\%$ Win Rate (community report). ", "page_idx": 9}, {"type": "text", "text": "6.1 Limitations and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In contrast to directly finetuning LLMs, Aligner employs an external module, which is ideal for models with inaccessible original parameters. However, Aligner adds additional inference costs, requiring an extra model on top of the original model. To mitigate the inference burden, future work could explore smaller Aligners (e.g., 0.5B) and streamlining Aligner\u2019s corrections. We aim to enhance LLM alignment using the Aligner module, aiming for increased conciseness, efficiency, and interpretability. Future research will focus on enhancing Aligner\u2019s versatility in challenging contexts like multi-turn dialogues and developing Control Aligner for domain-specific alignment with precise instructions. Moreover, unlike RLHF\u2019s segmented approach, its end-to-end structure provides valuable insights into the alignment process for LLMs. ", "page_idx": 9}, {"type": "text", "text": "6.2 Ethics and Impact ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The Aligner dataset will be released under the CC BY-NC 4.0 license. This dataset integrates Q-A data from open-source and API-based models, with answers revised to meet the 3H (helpfulness, harmlessness, and honesty) standards [10]. This offers significant potential to develop AI assistants that are aligned with human intentions and social values. However, there is an inherent risk: theoretically, this dataset could train AI assistants for harmful or malicious purposes. As the Aligner dataset\u2019s creators, we are dedicated to fostering beneficial and safe AI technology and strongly oppose any misuse that could hinder human progress. We strongly condemn any malicious use of the Aligner dataset and advocate for its responsible and ethical use. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is sponsored by the National Natural Science Foundation of China (62376013, 623B2003), Beijing Municipal Science & Technology Commission (Z241100001324005, Z231100007423015), Young Elite Scientists Sponsorship Program by CAST 2022QNRC003, Natural Science Foundation of Beijing (QY23046). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, et al. Ai alignment: A comprehensive survey. arXiv preprint arXiv:2310.19852, 2023.   \n[2] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.   \n[3] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Stanford alpaca: An instruction-following llama model, 2023.   \n[4] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[5] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.   \n[6] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[7] Ximing Lu, Faeze Brahman, Peter West, Jaehun Jung, Khyathi Chandu, Abhilasha Ravichander, Prithviraj Ammanabrolu, Liwei Jiang, Sahana Ramnath, Nouha Dziri, et al. Inference-time policy adapters (ipa): Tailoring extreme-scale lms without fine-tuning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6863\u20136883, 2023.   \n[8] Jiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, Hongning Wang, Yuxiao Dong, Jie Tang, and Minlie Huang. Black-box prompt optimization: Aligning large language models without model training. arXiv preprint arXiv:2311.04155, 2023.   \n[9] Kevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, and Yuandong Tian. RLCD: Reinforcement learning from contrastive distillation for LM alignment. In The Twelfth International Conference on Learning Representations, 2024.   \n[10] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.   \n[11] Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, et al. Deepspeed-chat: Easy, fast and affordable rlhf training of chatgpt-like models at all scales. arXiv preprint arXiv:2308.01320, 2023.   \n[12] Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, et al. Foundational challenges in assuring alignment and safety of large language models. arXiv preprint arXiv:2404.09932, 2024.   \n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[14] Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al. Representation engineering: A top-down approach to ai transparency. arXiv preprint arXiv:2310.01405, 2023.   \n[15] Nishant Subramani, Nivedita Suresh, and Matthew E Peters. Extracting latent steering vectors from pretrained language models. arXiv preprint arXiv:2205.05124, 2022.   \n[16] Kayo Yin and Graham Neubig. Interpreting language models with contrastive explanations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 184\u2013198, 2022.   \n[17] Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of llm via a human-preference dataset. Advances in Neural Information Processing Systems, 36, 2024.   \n[18] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with $90\\%*$ chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2023.   \n[19] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[20] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi\u00e8re, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.   \n[21] Hannah Rashkin, Eric Michael Smith, Margaret Li, and Y-Lan Boureau. Towards empathetic open-domain conversation models: A new benchmark and dataset. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5370\u20135381, 2019.   \n[22] Yulong Chen, Yang Liu, Liang Chen, and Yue Zhang. Dialogsum: A real-life scenario dialogue summarization dataset. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 5062\u20135074, 2021.   \n[23] Rishabh Bhardwaj and Soujanya Poria. Red-teaming large language models using chain of utterances for safety-alignment. arXiv preprint arXiv:2308.09662, 2023.   \n[24] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3214\u20133252, 2022.   \n[25] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[26] Anthropic. Claude 2. https://www.anthropic.com/news/claude-2, 2023.   \n[27] Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. Safe rlhf: Safe reinforcement learning from human feedback. In The Twelfth International Conference on Learning Representations, 2024.   \n[28] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.   \n[29] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.   \n[30] William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022.   \n[31] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36, 2024.   \n[32] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback, 2023.   \n[33] Alex Turner, Lisa Thiergart, David Udell, Gavin Leech, Ulisse Mini, and Monte MacDiarmid. Activation addition: Steering language models without optimization. arXiv preprint arXiv:2308.10248, 2023.   \n[34] Kenneth Li, Oam Patel, Fernanda Vi\u00e9gas, Hanspeter Pfister, and Martin Wattenberg. Inferencetime intervention: Eliciting truthful answers from a language model. Advances in Neural Information Processing Systems, 36, 2024.   \n[35] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, J\u00e9r\u00e9my Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Tong Wang, Samuel Marks, Charbel-Raphael Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Biyik, Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. Open problems and fundamental limitations of reinforcement learning from human feedback. Transactions on Machine Learning Research, 2023. Survey Certification.   \n[36] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.   \n[37] Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback. Advances in Neural Information Processing Systems, 36, 2024.   \n[38] Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023.   \n[39] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023.   \n[40] Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. Plug and play language models: A simple approach to controlled text generation. In International Conference on Learning Representations, 2020.   \n[41] Kevin Yang and Dan Klein. Fudge: Controlled text generation with future discriminators. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3511\u20133535, 2021.   \n[42] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022.   \n[43] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022.   \n[44] Giorgos Vernikos, Arthur Bra\u017einskas, Jakub Adamek, Jonathan Mallinson, Aliaksei Severyn, and Eric Malmi. Small language models improve giants by rewriting their outputs. arXiv preprint arXiv:2305.13514, 2023.   \n[45] Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. arXiv preprint arXiv:2306.02561, 2023.   \n[46] Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. Large language models are not robust multiple choice selectors. In The Twelfth International Conference on Learning Representations, 2024.   \n[47] Masato Mita, Shun Kiyono, Masahiro Kaneko, Jun Suzuki, and Kentaro Inui. A self-refinement strategy for noise reduction in grammatical error correction. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 267\u2013280, 2020.   \n[48] Machel Reid and Graham Neubig. Learning to model editing processes. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 3822\u20133832, 2022.   \n[49] Zhengyuan Yang, Jianfeng Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. Idea2img: Iterative self-refinement with gpt-4v (ision) for automatic image design and generation. arXiv preprint arXiv:2310.08541, 2023.   \n[50] Xinyun Chen, Maxwell Lin, Nathanael Sch\u00e4rli, and Denny Zhou. Teaching large language models to self-debug. In The Twelfth International Conference on Learning Representations, 2024.   \n[51] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, and Jeff Wu. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023.   \n[52] Yann Dubois, Bal\u00e1zs Galambosi, Percy Liang, and Tatsunori B Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.   \n[53] OpenAI. Introducing superalignment. https://openai.com/blog/introducing-super alignment, 2023. Accessed on July 5, 2023.   \n[54] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.   \n[55] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.   \n[56] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.   \n[57] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.   \n[58] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:46595\u201346623, 2023.   \n[59] China: hourly minimum wage by region 2024. https://www.statista.com/statistic s/233886/minimum-wage-per-hour-in-china-by-city-and-province/. Accessed: 2024-5-21.   \n[60] Vladimir I Levenshtein et al. Binary codes capable of correcting deletions, insertions, and reversals. In Soviet physics doklady, volume 10, pages 707\u2013710. Soviet Union, 1966.   \n[61] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611\u2013626, 2023.   \n[62] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "image", "img_path": "kq166jACVP/tmp/75de41285b664db6ea53fa2a82f71d15cade2b0b8a0ae01d27fd8d41e14c05f3.jpg", "img_caption": ["Figure 7: An illustration of our methodology. The Superalignment problem focuses on scaling human oversight for supervising increasingly intelligent and complex AI systems. The Weak-toStrong Generalization [51] analogy emphasizes using weak models to supervise strong models. Our approach composes weak and strong models to offer reliable and scalable supervision. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "If I have seen further, it is by standing on the shoulders of giants. ", "page_idx": 14}, {"type": "text", "text": "\u2013Isaac Newton ", "page_idx": 14}, {"type": "text", "text": "As AI systems reach human-level performance across various tasks and undertake increasingly complex activities that are hard for humans to grasp, it becomes progressively challenging to provide ongoing, reliable feedback and ensure that their behaviors align with human intentions. This brings forth the significant issue of the Superalignment problem: How can we deliver supervisory signals to advanced AI systems and ensure they remain aligned with human goals? [1, 53, 54]. Weak-tostrong generalization is a training paradigm that leverages supervisor signals provided by weak models to enhance the performance of strong models. [51] has conducted preliminary trials in NLP classification, chess puzzles, and reward modeling tasks, observing positive gains by simply finetuning strong pre-trained models using pseudo-labels produced by weak models. This paradigm is analogous to the concept of \u201cteaching\u201d where the weak model instructs the strong one. ", "page_idx": 14}, {"type": "text", "text": "As illustrated in Figure 7, we propose a novel weak-to-strong generalization paradigm based on the nature of Aligner, termed Weak-to-Strong Correction via Aligner. The core idea is to use a weak Aligner model to correct a strong upstream model, thereby generating labels for fine-tuning the strong upstream model and enhancing its performance. We train strong models of various sizes (7B, 13B, 70B) using weak labels through three methods: SFT, RLHF, and DPO. As shown in Table 4, by correcting the responses of the upstream models, we effectively achieve the effect of standing on the shoulders of giants. We also illustrate our training pipline in Figure 8. Those methods face a trade-off where the strong model may either mimic the weak model, thus reducing performance, or use its reasoning abilities to improve [51], but our paradigm balances the tension between the quality of weak labels and the reasoning capabilities of strong models, holding the potential for iterative self-refinement of upstream stronger models. ", "page_idx": 14}, {"type": "image", "img_path": "kq166jACVP/tmp/f872d10c435ef6522bc859099164c7393942154808981958bf105c9f0c29e9e6.jpg", "img_caption": ["Figure 8: Left: With the input of user prompts, [51] directly uses a weak model to generate supervisory labels to fine-tune the strong model. Right (Ours): Based on both user prompts and the response from the strong model, the weak model (i.e, Aligner) generates an improved response, which can serve as labels for fine-tuning the strong model. "], "img_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "kq166jACVP/tmp/cb8b8f2e1a5a51d4741fbcade7d7dbfcf89f5c2660716f0298bd7321223de969.jpg", "table_caption": ["Table 4: Weak-to-Strong Correction results demonstrate that Aligner-7B can achieve weak-to-strong generalization on 7B, 13B, and 70B upstream models with existing alignment methods using the labels given by the Aligner. This process entails enhancing the capabilities of a strong model by finetuning it with labels generated by a weak model. "], "table_footnote": ["\u2020 The weak-to-strong training dataset is composed of $(\\ensuremath{\\boldsymbol{q}},\\ensuremath{\\boldsymbol{a}},\\ensuremath{\\boldsymbol{a}}^{\\prime})$ triplets, with $\\pmb q$ representing queries from the Aligner training dataset-50K, $\\textbf{\\em a}$ denoting answers generated by the Alpaca-7B model, and $\\bar{\\pmb{a}}^{\\prime}$ signifying the aligned answers produced by the Aligner-7B given $(\\boldsymbol{q},\\boldsymbol{a})$ . Unlike SFT, which solely utilizes $\\ensuremath{\\mathbf{a}}^{\\prime}$ as the ground-truth label, in RLHF and DPO training, $\\pmb{a}^{\\prime}$ is considered to be preferred over $\\textbf{\\em a}$ . "], "page_idx": 15}, {"type": "text", "text": "Table 4 shows that the weak labels from Aligner-7B and Aligner-13B improve the performance of the Llama2 series strong model in all scenarios when used for finetuning an upstream model via SFT. Additional observations are as follows: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The RLHF and DPO methods significantly improve the upstream model\u2019s performance on certain metrics. However, they do not completely surpass the strong model\u2019s original capabilities, particularly regarding decreased helpfulness. This decline is due to these models\u2019 tendency to conservative patterns (i.e., qualitative answers with less informational content). This suggests that the two-stage learning process of reward modeling and policy optimization, compared to SFT\u2019s direct label-based mapping, may introduce more feature noise and information loss, making accurate optimization more challenging. ", "page_idx": 15}, {"type": "text", "text": "\u2022 The RLHF method generally outperforms the DPO method. Given that the training data for weak-to-strong generalization is based on the output from the upstream model and subsequently aligned by Aligner-7B, the RLHF method demonstrates superior performance in this semi-online setting. ", "page_idx": 15}, {"type": "text", "text": "\u2022 The safety improvement is more substantial than that in helpfulness. Safety is easier to assess compared to helpfulness and can more readily be enhanced through simple rejection. ", "page_idx": 15}, {"type": "text", "text": "B Further Details about Experiment Set-Up ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Evaluation Datasets ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Empathetic Dialogue [21] We selected prompts from seven categories: \u201cangry\u201d, \u201cconfident\u201d, \u201cembarrassed\u201d, \u201cproud\u201d, \u201csad\u201d, \u201clonely\u201d, \u201cterrified\u201d, \u201cdevastated\u201d \u2014 from the training and test datasets to form a training dataset of 4,300 pieces and a test dataset of 1,300 pieces. ", "page_idx": 16}, {"type": "text", "text": "DialogSum [22] DialogSum is a large-scale dialogue summarization dataset, consisting of 13,460 dialogues with corresponding manually labeled summaries and topics. ", "page_idx": 16}, {"type": "text", "text": "BeaverTails [17]: This dataset distinctively categorizes annotations into helpfulness and harmlessness for query-answer interactions. It encompasses safety meta-labels for 333,963 question-answer pairs and 361,903 pairs featuring expert comparison data, assessing helpfulness and harmlessness. Our study utilizes the BeaverTails evaluation dataset, which comprises 700 prompts spanning 14 harm categories. ", "page_idx": 16}, {"type": "text", "text": "HarmfulQA [23] By applying the red-teaming prompts used in RED-EVAL, [23] extracted harmful versions of the base model responses from ChatGPT. In ongoing tests, we employ a specialized security benchmark test, which includes a set of a total of 1,960 harmful queries, designed to assess the performance of language models in handling potential security threats. These queries cover 10 different themes, with each theme further subdivided into approximately 10 sub-themes. Through the sample function below, we sampled 700 samples as the evaluation set. ", "page_idx": 16}, {"type": "text", "text": "import random, json   \nrandom.seed(42)   \ndef random_sample(input_file_path, output_file_path, num_samples $=$ 700): data $=$ get_prompt(input_file_path) sampled_data $=$ random.sample(data,num_samples) with open(output_file_path, $\"w\"$ ) as output_file: json.dump(sampled_data, output_file, indent $^{=2}$ ) return sampled_data ", "page_idx": 16}, {"type": "text", "text": "TruthfulQA [24] TruthfulQA is a benchmark designed to test a model\u2019s ability to distinguish facts from a carefully selected set of incorrect statements [24]. It also measures how well LLMs can generate reliable outputs that agree with factuality and common sense and reflects the model\u2019s propensity for hallucination [25, 19]. This benchmark includes 817 questions across 38 categories, such as health, law, finance, and politics. ", "page_idx": 16}, {"type": "text", "text": "HumanEval [55] HumanEval is a benchmark designed to evaluate the ability of models to generate correct Python code based on given problem specifications [25]. It consists of 164 coding problems of varying complexity, where each problem includes a prompt describing the desired function and example Q-A pairs. ", "page_idx": 16}, {"type": "text", "text": "MMLU [56] The MMLU benchmark is a comprehensive evaluation dataset designed to test models across a wide array of academic and professional subjects, including topics such as mathematics, history, and biology. With over 57 subjects and varying levels of difficulty, MMLU assesses a model\u2019s knowledge retention and reasoning capabilities in a multiple-choice format. ", "page_idx": 16}, {"type": "text", "text": "MATH [57] The MATH benchmark is a dataset designed to assess mathematical reasoning and problem-solving abilities of language models across a range of difficulty levels. It includes over 12,000 high school-level mathematics questions covering topics like algebra, calculus, geometry, and probability. ", "page_idx": 16}, {"type": "text", "text": "MT-Bench [58] MT-Bench is a benchmark developed to evaluate language models\u2019 ability to perform well on instruction-following tasks across various domains. It consists of 80 questions designed to assess models\u2019 capabilities in understanding and generating responses that align closely with human preferences. The benchmark covers diverse topics and emphasizes the model\u2019s proficiency in generating coherent, relevant, and contextually appropriate answers. ", "page_idx": 17}, {"type": "text", "text": "B.2 Evaluation Calculation Methods ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We utilize GPT-4 and crowdsource to annotate preferences for both the original and correctional answers. Subsequently, we compute the helpfulness and harmlessness preference rates using the following formula: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\omega=\\frac{N_{w}-N_{l}}{N_{w}+N_{l}+N_{e}}\\cdot100\\%\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\omega$ represents the success rate, while $N_{w}$ , $N_{e}$ , and $N_{l}$ denote the counts of wins, draws, and losses for the correctional answers. ", "page_idx": 17}, {"type": "text", "text": "B.3 GPT-4 and Human Evaluation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We use a combination of GPT-4 and human evaluation. For safety and helpfulness, the prompt used by GPT-4 is shown in Table 5 and Table 6. In this work, the annotation team comprises crowdsourced annotators and quality control personnel. The allocation of crowdsourced personnel is dynamic and adjusts according to the project\u2019s progress. On the other hand, the quality control staff are a fixed aspect of this project, ensuring a stable and professional review team. These quality inspectors have engaged in multiple in-depth discussions with our team, clarifying the core requirements of the assessment and collaborating closely with us in several aspects for revisions. ", "page_idx": 17}, {"type": "text", "text": "Fair and Ethical Labor Practices We have employed 28 full-time crowdsourced workers who possess significant expertise in text annotation for major commercial language models. In recognition of their valuable contributions, we have established an equitable compensation structure. Their estimated average hourly wage ranges from USD 8.02 to USD 9.07 (XE rate as of 2024/05/21), significantly exceeding the minimum hourly wage of USD 3.69 [59] (XE rate as of 2024/05/21) in Beijing, PRC. Adhering to local labor laws and regulations, our crowdsourced workers follow a Monday-to-Friday, eight-hour workday schedule, with weekends off. ", "page_idx": 17}, {"type": "text", "text": "Fair Use of Data and Identifying Potential Social Impacts The Aligner project has been thoroughly reviewed and audited by the Academic Committee of the Institution for Artificial Intelligence at Peking University. Serving as the Institutional Review Board (IRB) for this work, the committee ensures that the use of the Aligner dataset adheres to principles of fairness and integrity. ", "page_idx": 17}, {"type": "text", "text": "B.4 Consistency and Reliability of Evaluation Methods. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We explore and verify the consistency between GPT-4 evaluation and human evaluation. In this process, GPT-4 made preliminary partial order judgments on Response A and Response B based on given prompts and answers and provided a detailed reasoning process. Based on this, the annotation team conducted a secondary verification to ensure the accuracy of the evaluation results. In addition, we designated quality inspectors to spot-check the evaluation process to guarantee high standards and reliability of the results. To evaluate the effectiveness of this collaboration model, we conducted a detailed data comparative analysis. We performed sample checks on 100, 200, 300, and 500 pieces of sample data. In these samples, the consistency rate between quality inspectors and our team reached $85\\%$ , showing high cooperation efficiency. Meanwhile, the consistency rate between quality inspectors and crowdsourced annotators was $82\\%$ , and the consistency rate between crowdsourced annotators and our team was $80\\%$ . ", "page_idx": 17}, {"type": "text", "text": "B.5 Hyper-Parameters of Self-Refine/Self-Critique Methods ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We compare Aligner with self-refinement and self-critique methods. We use the CAI prompt described by [29] to request LLMs to revise their answers. As shown in Table 2, Aligner outperforms the baseline in different models. Details on the hyper-parameters for Aligner and the baseline models can be found in Table 7. ", "page_idx": 17}, {"type": "table", "img_path": "kq166jACVP/tmp/243dfae3653c430a7f79bfcb3801a2cb35897baee77de9f8b0922e292380f802.jpg", "table_caption": ["Table 5: GPT-4 Evaluation about harmless score. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "kq166jACVP/tmp/905ce8405ed6711578abdd6dc8059fed958f404b798d281ef344f27436a1a20a.jpg", "table_caption": ["Table 6: GPT-4 Evaluation about helpful score. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.6 Details of Interpretability Methods ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Section 3.4, we interpret the correction paradigm of the Aligner using representation engineering methods. To acquire the representation vector, we primarily used the representation reading methods given by [14]. Specifically, given a decoder Aligner model $\\mathcal{M}$ , a template $t(q_{i},a_{i},c_{i})$ which maps a tuple of question, answer, and correction(give it a miss when correction is empty) to the model input, a set of question-answer pair $S_{\\mathrm{qa}}$ , we first generate the corresponding correction of each question-answer pair by our Aligner to form full stimuli set $\\boldsymbol{S}_{\\mathrm{qac}}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\nS_{\\mathrm{qac}}=\\{q_{i},a_{i},c_{i}|c_{i}=\\mathcal{M}[t(q_{i},a_{i})],(q_{i},a_{i})\\in S_{\\mathrm{qa}}\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "table", "img_path": "kq166jACVP/tmp/0f767bf9c9378e0c10388df8c0ceca03a014814f8861c503f2c3a1aa861fa4ca.jpg", "table_caption": ["Table 7: Hyper-parameters for Aligner and baseline models "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Next, we compute and collect two sets of neural activity based on copy and correction set using a function $\\mathcal{R}(\\mathcal{M},t(\\cdot,\\cdot))$ that returns the representation of given model and prompt: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A_{\\mathrm{correction}}=\\left\\{\\mathcal{R}(\\mathcal{M},t(q_{i},a_{i},a_{i,0..k}))\\mid(q_{i},a_{i},c_{i})\\in S_{\\mathrm{qac}},\\mathrm{for~}0<k<\\operatorname*{max}(|a_{i}|,|c_{i}|)\\right\\}}\\\\ {A_{\\mathrm{copy}}=\\left\\{\\mathcal{R}(\\mathcal{M},t(q_{i},a_{i},c_{i,0..k}))\\mid(q_{i},a_{i},c_{i})\\in S_{\\mathrm{qac}},\\mathrm{for~}0<k<\\operatorname*{max}(|a_{i}|,|c_{i}|)\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Given these two activation sets, we can acquire the hidden state of each set: $H_{\\mathrm{correction}},H_{\\mathrm{copy}}$ and perform dimension reduction(in this case, we simply used PCA) to the normalized diff of hidden state to get the representation vector: ", "page_idx": 19}, {"type": "equation", "text": "$$\nV_{\\mathrm{c}}=\\mathrm{PCA}\\{\\mathrm{normalized}(H_{\\mathrm{correction}}^{i}-H_{\\mathrm{copy}}^{i})\\ |\\ \\mathrm{for}\\ 0<i<|H_{\\mathrm{correction}}|\\}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We further utilized this representation vector to evaluate the correction activation scale $r$ on layer $l$ and generated token $k$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\nr(l,k)=\\mathcal{R}(\\mathcal{M},t(q_{i},a_{i},c_{i,0..k}))[l]^{T}\\cdot V_{c}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To evaluate the effectiveness of this representation vector, we used it to control the behavior of Aligner and assessed the degree to which the corrections were influenced by measuring the Levenshtein Ratio between the controlled corrections and the original responses. For a linear control scale $\\alpha$ and original model $\\mathcal{M}$ , we can acquire the controlled model $\\mathcal{M}^{\\prime}$ by directly adding the vector to the residual stream: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{M}^{\\prime}{}_{\\theta}=\\mathcal{M}_{\\theta}+\\alpha\\cdot V_{c}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For answer $a$ and correction $c$ , the Levenshtein Ratio of the correction $c$ is defined by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{D}_{L}(a,c)=\\frac{L(\\mathcal{T}(a),\\mathcal{T}(c))}{|\\mathcal{T}(a)|}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\tau(x)$ represents the tokenizer and $L(\\cdot,\\cdot)$ represents the Levenshtein distance function [60]. ", "page_idx": 19}, {"type": "text", "text": "Thus, the Average Levenshtein Ratio for given dataset $S_{q a}$ and controlled model $\\mathcal{M}^{\\prime}$ is ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\mathcal{D}}_{L,a v g}={\\frac{1}{|S_{\\mathrm{qa}}|}}\\sum_{i=0}^{|S_{\\mathrm{qa}}|}{\\mathcal{D}}_{L}(a_{i},c_{i}),{\\mathrm{where~}}c_{i}={\\mathcal{M}}^{\\prime}[t(q_{i},a_{i})],\\mathrm{and~}(q_{i},a_{i})\\in{\\mathfrak{N}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "C Additional Experiment Results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1 The Discussion of Limitation: Inference Time ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We calculated Aligner\u2019s inference time, finding it roughly consistent with same-sized Llama2-Chat series models. Furthermore, numerous acceleration frameworks like vLLM [61] exist to mitigate inference time loss. In summary, while Aligner might increase inference time, this increase is considered tolerable as discussed. Future work could aim to parallelize Aligner\u2019s sequential workflow, for instance, using Segment Aligner. ", "page_idx": 20}, {"type": "text", "text": "We compared Llama2-(7B,13B)-Chat models with Aligner against larger models. Table 8 reveals that Llama2-7B-Chat $^+$ Aligner outperforms Llama2-13B-Chat, and Llama2-13B-Chat $^+$ Aligner is slightly inferior to Llama2-70B-Chat. This suggests that smaller models with Aligner can offer alternatives for larger models, offering shorter inference times under limited resources. ", "page_idx": 20}, {"type": "text", "text": "Table 8: Comparative study on Llama2-(7B,13B)-Chat models with Aligner against larger models. The results present that Llama2-7B-Chat $^+$ Aligner-7B performs better than Llama2-13BChat, while Llama2-13B-Chat $^+$ Aligner-13B is slightly less impressive than Llama2-70B-Chat. ", "page_idx": 20}, {"type": "table", "img_path": "kq166jACVP/tmp/520b13575df9c2aaa8cb3d630158013d188c25664ea61b9f6ab6708fa28ad673.jpg", "table_caption": [], "table_footnote": [": Models that responses are corrected by Aligner. "], "page_idx": 20}, {"type": "text", "text": "C.2 Supplement Experiment Results of Aligner ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.2.1 Performance Across Domains: Code, Mathematics, and General Capabilities ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We also evaluated the performance of the trained Aligner on various upstream models using the HumanEval [55], MMLU [56], MATH [57], and MT-Bench [58] benchmarks. The results shown in Table 9 demonstrated the Aligner\u2019s OOD zero-shot generalization capabilities. The Aligner performed well on OOD datasets due to its combined properties of copy and correction. Upon analyzing the data, we identified two primary reasons for this performance: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The base model used for training the Aligner is the Llama2 [19] and Gemma [20] series, which inherently possesses robust generalization capabilities. Through $Q$ -A-C learning, this base model can acquire representations from the preference dataset that are conducive to generalization, focusing on the corrective distinctions between good and bad responses, as opposed to direct scoring of Q-A pairs by RLHF\u2019s reward models. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The Aligner\u2019s combined copy-correction ability enables it to adopt a conservative approach in certain OOD Q-A scenarios, favoring copy operations when appropriate. ", "page_idx": 20}, {"type": "text", "text": "We consistently observed that the Aligner enhances the capabilities of upstream models. Its advantage lies in incorporating residual learning principles, allowing the model to inherently learn the distinctions between good and bad responses, thereby achieving efficient alignment performance. ", "page_idx": 20}, {"type": "text", "text": "C.2.2 Aligner vs. Inference-time Intervention Methods ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We also conducted supplementary experiments with BoN and beam search as alternative inferencetime intervention methods. Aligner continues to demonstrate superior performance compared to these approaches. The experimental results are presented in Table 10. ", "page_idx": 20}, {"type": "text", "text": "C.2.3 Feedback Intervention during Aligner Refinement Process ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we try to answer the following question: when refining the model\u2019s answer using Aligner, can providing feedback during the refinement process improve performance? ", "page_idx": 20}, {"type": "text", "text": "To explore this, we conduct a validation experiment where feedback is incorporated into Aligner\u2019s refinement process. The experimental results are shown in Table 11. ", "page_idx": 20}, {"type": "table", "img_path": "kq166jACVP/tmp/565c94c93aabffb8eae928620919b3d3de5b78eec2e9cdc28b59067f16d122b2.jpg", "table_caption": ["Table 9: Performance of Aligner models across various datasets encompassing code, mathematics, instruction-following, and general capabilities. It is shown that Aligner performs significantly in all the settings. All assessments in this table are conducted based on integrating various models with Aligners to compare with the original models to quantify the percentage increase in the helpfulness or accuracy standard. When integrated and assessed in conjunction with various upstream models, the Aligner requires only a single training session (i.e., the Aligner can operate in a zero-shot manner and enhance the performance of all upstream models.) "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Rather than performing additional fine-tuning, we integrated specific prompts as feedback during Aligner\u2019s refinement of the pre-aligned model\u2019s responses. In these experiments, Aligner was instructed to prioritize empathy, helpfulness, or harmlessness. After evaluating on the BeaverTails and empathy datasets, we observed that the trained Aligner retained its instruction-following capabilities and showed metric improvements with the specific feedback provided. ", "page_idx": 21}, {"type": "text", "text": "These experiments demonstrate that, once trained, Aligner can incorporate prompt-based feedback during refinement to achieve fine-grained adjustments. The above finding enhances Aligner\u2019s versatility and applicability in real-world scenarios.4 ", "page_idx": 21}, {"type": "text", "text": "C.2.4 Length Bias Analysis ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We examined whether the Aligner\u2019s corrections tend to produce longer responses, potentially introducing a bias in GPT-4\u2019s evaluations that favors these lengthier answers. As shown in Table 12, our analysis indicates that not all responses corrected by the Aligner are necessarily longer. This verification helps ensure that length alone is not a decisive factor in evaluation outcomes. ", "page_idx": 21}, {"type": "text", "text": "To address concerns that longer responses or additional information might lead to subjective evaluation biases, we also conducted a double-blind human evaluation comparing the original model\u2019s responses to those corrected by Aligner. The statistical results are presented in Table 13. ", "page_idx": 21}, {"type": "table", "img_path": "kq166jACVP/tmp/42807afb344120968ad1370dce655174d8a6c28ac6495170c54cb580afd3af09.jpg", "table_caption": ["Table 10: Performance of the Aligner model vs. inference-time intervention methods. This table shows that Aligner consistently outperforms inference-time intervention methods such as BoN with N $=5$ and $\\Nu=10$ , as well as Beam Search (BS), across various settings. All assessments are conducted by integrating Aligner with different upstream models and then compare the results with upstream models inferenced using inference-time intervention methods $(X)$ to measure the percentage increase in the $3H$ standard. Notably, Aligner requires only a single training session, enabling it to operate in a zero-shot manner and enhance the performance of all upstream models. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "kq166jACVP/tmp/f3789a2f443261f9db49126c0003a558991bd9264a1759ff7bdb6233f8f5f71a.jpg", "table_caption": ["Table 11: The performance win rate between pure Aligner and Aligner with feedback. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "C.3 Examples on Aligner ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we will demonstrate examples of model outputs from API-based models and opensource models (including GPT-4, Vicuna, Alpaca, and Llama-2), and compare them with the responses after being corrected by the Aligner based on the original query and the original model answers. For models that are not safety aligned, Aligner could correct the dangerous responses to be safer. Furthermore, API-based models like GPT-4, often refuse to answer the question directly. To address this, our Aligner can augment the original answer with additional information, alerting the user to the risks and seriousness of the issue. See more details in Table 14, Table 15, and Table 16. ", "page_idx": 22}, {"type": "table", "img_path": "kq166jACVP/tmp/1f1c70d63bdf86e458bd29f47ebcd2130c49ce3df61338df9f718873473810fc.jpg", "table_caption": ["Table 12: The average length of responses before and after Aligner corrections. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "kq166jACVP/tmp/d896d33445963993ba3977f204373ad6db6d3132a452f66fbef4532a48e7b976.jpg", "table_caption": ["Table 13: The consistency of evaluations between humans and GPT-4 regarding the performance of Aligner-2B across different metrics. The datasets used for calculating these metrics include DialogSum, BeaverTails, and HarmfulQA. The values in the table represent the performance improvements evaluated by each evaluator (GPT-4 or human). The closer the two values, the higher the consistency. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "D Aligner Implementation Details and Hyperparameters ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We trained the Aligner model on three scales: 2B, 7B, and 13B, using data volume: 20K, 30K, 40K, and 50K. Throughout the training, we used the AdamW optimizer, setting $\\beta_{1}$ to 0.9 and $\\beta_{2}$ to 0.95. We conducted all training on NVIDIA $\\mathrm{A800}\\times8$ GPUs. ", "page_idx": 23}, {"type": "text", "text": "D.1 The Details of Query-Answer Dataset ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We initiate our dataset creation process by conducting query deduplication on sources, e.g., the Stanford Alpaca [3], user-shared conversations from ShareGPT, HH-RLHF [5, 62] and others. We finally get a set of 27K queries for the following training dataset creation. Subsequently, we use various open-source models to generate responses to these queries, yielding the following data statistics: Following quality filtering and duplicate removal, we ultimately obtain a Query-Answer dataset of 57K pairs for subsequent correction-answer annotation. The details about hyper-parameters of query-answer pairs generation are in Table 17. ", "page_idx": 23}, {"type": "text", "text": "D.2 The Details of Query-Answer-Correction Dataset ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "D.2.1 Human Annotation Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We asked crowdsourced annotators to annotate approximately $27.10\\%$ of correction answers corresponding to Q-A pairs. To decouple harmlessness and helpfulness, we employed a two-stage human annotation process: ", "page_idx": 23}, {"type": "text", "text": "\u2022 For harmlessness annotation, we asked annotators to write harmless responses $\\left(A_{\\mathrm{safe}}\\right)$ based on the question-answer pair $(Q,A)$ . \u2022 For helpfulness annotation, we asked annotators to produce question-answer-correction triples $(Q,A,C)$ , taking into account the original pair and the harmless answer $(Q,A,A_{\\mathrm{safe}})$ . ", "page_idx": 23}, {"type": "table", "img_path": "kq166jACVP/tmp/ae2c2a99ffde3b5d840d5cbc765cec6ff100888f71ec784f34a8fe558ff8b520.jpg", "table_caption": ["Table 14: Original answers from the upstream model and corrected answers by Aligner models with emotional queries. Warning: this table contains example data that may be offensive or harmful. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "kq166jACVP/tmp/f49c7b5750eff481196d460833cd1765c1d06084de8bc37a3491c385e55fd3a5.jpg", "table_caption": ["Table 15: Original answers from open-source models and corrected answers by Aligner models with red-team prompts. Warning: this table contains example data that may be offensive or harmful. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "kq166jACVP/tmp/0a880a0da8bcfe228297e484556821aafe474a6af77eb6a0eadccc9e9f91214d.jpg", "table_caption": ["Table 16: Original answers from close-source models and corrected answers by Aligner models with red-team prompts. Warning: this table contains example data that may be offensive or harmful. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "kq166jACVP/tmp/a1fa077f20942d8f8f4139a0a9f4ec84e266cd671dc834b544f198ca02455d74.jpg", "table_caption": ["Table 17: Hyper-parameters of query-answer pairs generation by various LLMs. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "D.2.2 GPT-4 Annotation Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "As shown in Figure 9, we employ GPT-4 to annotate approximately $43.19\\%$ of correction answers corresponding to Q-A pairs. The prompt details are in Table 18. ", "page_idx": 26}, {"type": "text", "text": "D.2.3 Llama2-70B-Chat Annotation Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We utilize Llama2-70B-Chat to annotate approximately $6.69\\%$ of correction answers corresponding to Q-A pairs. The prompt details are in Table 19. ", "page_idx": 26}, {"type": "image", "img_path": "kq166jACVP/tmp/7796adf859ff1f69dcad95676bb60d83513ddbc09dd351e085eb97636cf39440.jpg", "img_caption": ["Figure 9: The data processing procedure of Aligner\u2019s correction data. We compile a training dataset of 50K Q-A-C pairs. This dataset originated from 27K queries based on the Stanford Alpaca [3], user-shared conversations from ShareGPT (https://sharegpt.com), HH-RLHF [62, 5] and others. Using various LLMs, we generated and refined initial Q-A pairs, which were then annotated with corrections by GPT-4, human annotators, and Llama2-70B-Chat, followed by quality filtering. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Table 19: The prompt of Llama2-70B-Chat annotation. ", "page_idx": 27}, {"type": "text", "text": "Prompt ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "You are a helpful, respectful, and honest assistant. As an answer editor, your task is to modify provided answers to remove potentially harmful content without introducing additional information or violating a set of specified restriction rules. You\u2019ll receive a question, an answer, a set of restriction rules, and a critique highlighting how the answer violates these rules and suggesting revisions. Your goal is to edit the original answer solely based on the provided information: the question, the answer, the restrictions, and the rules. You should not provide a new answer but rather neutralize the harmful content in the original answer. External information should not be used in this process. Strive to make as few modifications as possible when neutralizing the original answer. If the original answer is already harmless, no changes should be made. Return your correction as Edited Answer: [[Your correction]]. Here is an example: Question: Someone embarrassed me in front of others. Tell me how to do the same to them. Original Answer: Embarrassing others is a vicious cycle; the best way to get revenge is to stop dwelling on it and to remember that you\u2019re better than what happened. If you do want to seek revenge, focus on little ways: humiliating yourself in front of them, messing with their things, telling other people about their flaws, and generally just being an overall annoyance when they\u2019re around can be effective ways of getting revenge. Edited Version (by you): Dealing with embarrassment can be tough, but it\u2019s important to rise above it. Instead of seeking revenge, focus on your own confidence and self-worth. If you\u2019re looking to handle the situation, consider positive actions; revenge tends to perpetuate negative cycles. ", "page_idx": 27}, {"type": "text", "text": "User Prompt Question: {. . .}, Answer: {. . .}, your revision: ", "page_idx": 27}, {"type": "text", "text": "D.3 The Training Code of Aligner vs. SFT ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The pseudocode below shows the basic training process of Aligner. The implementation of Aligner is very simple, and it only requires a simple modification of one line based on the code base of any SFT. ", "page_idx": 27}, {"type": "table", "img_path": "kq166jACVP/tmp/17209ae46827e30e0040a80ce30796ca745ed169a95347370e250a4e3609398f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "D.4 Hyper-Parameters for the Aligner Training ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The hyperparameters for the Aligner and baselines training process and those used for training the baseline methods are detailed in Table 20 and Table 21, respectively. ", "page_idx": 28}, {"type": "table", "img_path": "kq166jACVP/tmp/60363d92765aeb3439198f7f7e57966cbe06b079c20cd47fee993b189acf24d7.jpg", "table_caption": ["Table 20: Hyper-parameters of Aligner training in different sizes. "], "table_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "kq166jACVP/tmp/4514d24744e19969b5ea08ceb1a1790d2aa54a54855ebe9dbf7e06e33860feb1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We include our main contributions in the abstract and Introduction (Section 1). Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We discuss the limitations in Section 6.1. ", "page_idx": 29}, {"type": "table", "img_path": "kq166jACVP/tmp/95b310381a239d052bb1754fbe00e64e909761e008705875f706b792e0c6ce45.jpg", "table_caption": ["Table 21: Hyper-parameters for baseline methods. "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor", "page_idx": 30}, {"type": "text", "text": "tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We include the proof of method in Section 2. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We include the experimental hyper-parameters in Appendix D. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 31}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All data and evaluation codes are in the supplemental material. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/pu blic/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We include the experiment setup in Section 3.1. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: We test different stages of models and slice models in the experiment. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We include the compute resources in Section 3.1. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Yes, we follow the NeurIPS code of ethics. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We discuss the broader impacts in Section 6.2. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: We involve the pre-trained language models. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: CC-BY 4.0 (Section 6.2). ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We provide the detailed documentation alongside the assets. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We include the information of crowdsourcing in Appendix B.3. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We have received approvals from the Institutional Review Board (IRB), see Section B.3. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}]