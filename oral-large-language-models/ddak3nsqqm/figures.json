[{"figure_path": "Ddak3nSqQM/figures/figures_1_1.jpg", "caption": "Figure 1: (A-B) A comparison between the problem of policy learning from tutorial books (PLfB) and from data; (C) An illustration of the understanding, rehearsal, and introspection for PLfB.", "description": "This figure compares policy learning from real-world data (A) with policy learning from tutorial books (B).  Panel (A) shows a traditional RL approach where a policy network is trained using data collected from interactions in the real world. Panel (B) shows the PLfB approach, which leverages information from tutorial books to directly learn a policy network. Panel (C) details the three-stage learning methodology for PLfB: Understanding (extracting knowledge from the tutorial book), Rehearsing (generating imaginary trajectories), and Introspecting (distilling a policy network from the imaginary data).", "section": "1 Introduction"}, {"figure_path": "Ddak3nSqQM/figures/figures_3_1.jpg", "caption": "Figure 2: The URI pipeline consists of three major stages: (A) Understanding: The knowledge extractor and aggregator modules process paragraphs from books to form a structured knowledge database organized as pseudo-code. (B) Rehearsing: Using the knowledge database, the simulator generates and iterates through imagined states, actions, and rewards to create an extensive imaginary dataset. (C) Introspecting: The introspection module refines the policy network by evaluating and correcting errors in the generated states, actions, and rewards to ensure accurate and effective policy implementation. The pseudocode of the pipeline is in Appendix F.8.", "description": "This figure illustrates the three main stages of the URI (Understanding, Rehearsing, and Introspecting) framework for Policy Learning from Tutorial Books (PLfB).  Stage 1 (Understanding) shows how knowledge is extracted from tutorial books and organized into a pseudo-code knowledge database.  Stage 2 (Rehearsing) depicts the use of this database to generate an imaginary dataset via simulated interactions using LLMs to model the policy, dynamics, and reward functions.  Finally, Stage 3 (Introspecting) demonstrates how offline reinforcement learning is used to refine the policy based on the imaginary dataset, addressing inaccuracies in the simulated data.", "section": "5.2 Book Content Understanding"}, {"figure_path": "Ddak3nSqQM/figures/figures_7_1.jpg", "caption": "Figure 3: Knowledge Segment Aggregation.", "description": "This figure shows the number of code segments at each aggregation round during the knowledge aggregation process for both Tic-Tac-Toe and Football games.  It illustrates how the iterative aggregation process effectively consolidates the initial large number of code segments into a smaller, more concise representation of the knowledge.  The reduction in the number of segments highlights the effectiveness of the aggregation process in refining and summarizing the extracted knowledge from tutorial books.", "section": "5.2 Book Content Understanding"}, {"figure_path": "Ddak3nSqQM/figures/figures_8_1.jpg", "caption": "Figure 2: The URI pipeline consists of three major stages: (A) Understanding: The knowledge extractor and aggregator modules process paragraphs from books to form a structured knowledge database organized as pseudo-code. (B) Rehearsing: Using the knowledge database, the simulator generates and iterates through imagined states, actions, and rewards to create an extensive imaginary dataset. (C) Introspecting: The introspection module refines the policy network by evaluating and correcting errors in the generated states, actions, and rewards to ensure accurate and effective policy implementation. The pseudocode of the pipeline is in Appendix F.8.", "description": "This figure illustrates the three-stage learning methodology for Policy Learning from Tutorial Books (PLfB) proposed in the paper.  Stage 1 (Understanding) extracts knowledge from tutorial books and structures it into a pseudo-code knowledge database. Stage 2 (Rehearsing) uses this database to generate imaginary datasets by simulating decision-making trajectories. Finally, Stage 3 (Introspecting) refines the policy network by learning from the imaginary data, correcting any inconsistencies or errors.", "section": "5.2 Book Content Understanding"}, {"figure_path": "Ddak3nSqQM/figures/figures_8_2.jpg", "caption": "Figure 2: The URI pipeline consists of three major stages: (A) Understanding: The knowledge extractor and aggregator modules process paragraphs from books to form a structured knowledge database organized as pseudo-code. (B) Rehearsing: Using the knowledge database, the simulator generates and iterates through imagined states, actions, and rewards to create an extensive imaginary dataset. (C) Introspecting: The introspection module refines the policy network by evaluating and correcting errors in the generated states, actions, and rewards to ensure accurate and effective policy implementation. The pseudocode of the pipeline is in Appendix F.8.", "description": "This figure illustrates the three-stage learning methodology for Policy Learning from Tutorial Books (PLfB). Stage 1 (Understanding) extracts knowledge from tutorial books and organizes it into a structured knowledge database. Stage 2 (Rehearsing) uses this database to generate imagined decision-making trajectories with the help of LLMs.  Finally, Stage 3 (Introspecting) uses these trajectories to refine a policy network for decision-making.", "section": "5.2 Book Content Understanding"}, {"figure_path": "Ddak3nSqQM/figures/figures_9_1.jpg", "caption": "Figure 6: Visualization of the projected distributions for real and imaginary datasets in the football environment.", "description": "This figure visualizes the results of t-SNE dimensionality reduction applied to real and imaginary datasets from the Google Research Football environment.  The real data points represent trajectories collected from a rule-based policy. The imaginary data points are generated by the URI method and are further categorized into \"low-uncertainty\" and \"high-uncertainty\" subsets based on uncertainty estimates (RT and RR).  The figure shows a 2D projection of the high-dimensional data, highlighting the similarity in distribution between real and imaginary data, while also identifying areas where the imaginary data deviates significantly, indicating uncertainty.", "section": "6.8 Imaginary Dataset Visualization"}, {"figure_path": "Ddak3nSqQM/figures/figures_17_1.jpg", "caption": "Figure 1: (A-B) A comparison between the problem of policy learning from tutorial books (PLfB) and from data; (C) An illustration of the understanding, rehearsal, and introspection for PLfB.", "description": "This figure compares policy learning from real-world data (using reinforcement learning) with policy learning from tutorial books.  Panel (A) shows the traditional RL approach of collecting data from real-world interactions to train a policy network. Panel (B) illustrates the proposed approach, where a policy network is learned directly from tutorial books.  Panel (C) details the three-stage learning methodology: understanding (extracting knowledge from the books), rehearsing (generating imaginary trajectories using the knowledge), and introspecting (distilling a policy network from the imaginary data).", "section": "1 Introduction"}, {"figure_path": "Ddak3nSqQM/figures/figures_19_1.jpg", "caption": "Figure 1: (A-B) A comparison between the problem of policy learning from tutorial books (PLfB) and from data; (C) An illustration of the understanding, rehearsal, and introspection for PLfB.", "description": "This figure compares traditional policy learning from real-world data with the proposed method, Policy Learning from tutorial Books (PLfB).  (A) shows the standard RL approach of collecting data from real-world interactions to train a policy network. (B) shows the PLfB approach which utilizes tutorial books to derive a policy network. (C) details the three stages of the PLfB method: Understanding, Rehearsing, and Introspecting.  This framework mimics the human learning process where knowledge is extracted from books (understanding), imaginary scenarios are played out (rehearsing), and the policy is refined based on those scenarios (introspecting).", "section": "1 Introduction"}, {"figure_path": "Ddak3nSqQM/figures/figures_22_1.jpg", "caption": "Figure 1: (A-B) A comparison between the problem of policy learning from tutorial books (PLfB) and from data; (C) An illustration of the understanding, rehearsal, and introspection for PLfB.", "description": "This figure compares policy learning from real-world data and policy learning from tutorial books. (A) shows the traditional approach of collecting real-world data, training a policy network, and applying it to the real world. (B) shows the proposed method of using tutorial books to generate a policy network. (C) details the three stages of the proposed approach: understanding the information in the books, rehearsing decision-making, and introspecting to improve the network.", "section": "1 Introduction"}, {"figure_path": "Ddak3nSqQM/figures/figures_23_1.jpg", "caption": "Figure 1: (A-B) A comparison between the problem of policy learning from tutorial books (PLfB) and from data; (C) An illustration of the understanding, rehearsal, and introspection for PLfB.", "description": "This figure compares policy learning from real-world data with policy learning from tutorial books.  Panel (A) shows the traditional approach using real-world interaction and RL to generate a policy network. Panel (B) illustrates the novel approach of Policy Learning from Tutorial Books (PLfB), using tutorial books as input. Panel (C) details the three-stage learning methodology of PLfB, which involves understanding the content from the books, rehearsing imaginary decision-making trajectories, and introspecting over those to distill a final policy network.", "section": "1 Introduction"}, {"figure_path": "Ddak3nSqQM/figures/figures_28_1.jpg", "caption": "Figure 1: (A-B) A comparison between the problem of policy learning from tutorial books (PLfB) and from data; (C) An illustration of the understanding, rehearsal, and introspection for PLfB.", "description": "This figure compares policy learning from real-world data and policy learning from tutorial books.  Panel (A) shows traditional policy learning using reinforcement learning (RL) with real-world interactions and data collection. Panel (B) shows policy learning from tutorial books using the proposed method, bypassing real-world interaction. Finally, Panel (C) details the three-stage framework (Understanding, Rehearsing, and Introspecting) used in the proposed method for policy learning from tutorial books.", "section": "1 Introduction"}, {"figure_path": "Ddak3nSqQM/figures/figures_29_1.jpg", "caption": "Figure 1: (A-B) A comparison between the problem of policy learning from tutorial books (PLfB) and from data; (C) An illustration of the understanding, rehearsal, and introspection for PLfB.", "description": "This figure compares traditional policy learning from real-world data with the proposed method of policy learning from tutorial books (PLfB).  Panel (A) shows the standard RL approach where a policy network is trained using data collected from real-world interactions. Panel (B) depicts the PLfB approach where a policy network is learned using only knowledge extracted from tutorial books. Panel (C) illustrates the three-stage learning framework of PLfB: Understanding, Rehearsing, and Introspecting. The diagram shows how knowledge is extracted from the books, used to generate imaginary datasets, and finally distilled into a policy network.", "section": "1 Introduction"}, {"figure_path": "Ddak3nSqQM/figures/figures_30_1.jpg", "caption": "Figure 1: (A-B) A comparison between the problem of policy learning from tutorial books (PLfB) and from data; (C) An illustration of the understanding, rehearsal, and introspection for PLfB.", "description": "This figure compares policy learning from real-world data using reinforcement learning (RL) with policy learning from tutorial books using the proposed method.  Panel (A) shows the traditional RL approach, where data from real-world interactions is used to train a policy network.  Panel (B) illustrates the proposed approach, where knowledge from tutorial books is used.  Panel (C) details the three-stage learning methodology (Understanding, Rehearsing, Introspecting) used to derive a policy network from the tutorial books.", "section": "1 Introduction"}, {"figure_path": "Ddak3nSqQM/figures/figures_31_1.jpg", "caption": "Figure 1: (A-B) A comparison between the problem of policy learning from tutorial books (PLfB) and from data; (C) An illustration of the understanding, rehearsal, and introspection for PLfB.", "description": "Figure 1 compares policy learning from real-world data and policy learning from tutorial books. (A) shows traditional policy learning from real-world data through reinforcement learning (RL), where an agent interacts with an environment and learns a policy. (B) shows the proposed policy learning from tutorial books (PLfB), where an agent learns a policy directly from tutorial books. (C) illustrates the proposed three-stage framework for PLfB: Understanding, Rehearsing, and Introspecting.", "section": "1 Introduction"}, {"figure_path": "Ddak3nSqQM/figures/figures_36_1.jpg", "caption": "Figure 1: (A-B) A comparison between the problem of policy learning from tutorial books (PLfB) and from data; (C) An illustration of the understanding, rehearsal, and introspection for PLfB.", "description": "This figure compares policy learning from real-world data with policy learning from tutorial books.  Panel (A) shows a typical reinforcement learning setup using real-world interactions to learn a policy network.  Panel (B) illustrates the proposed approach, which leverages tutorial books to learn a policy network without the need for real-world interaction. Panel (C) details the three-stage learning methodology of the proposed approach, including the Understanding, Rehearsing, and Introspecting stages.", "section": "1 Introduction"}, {"figure_path": "Ddak3nSqQM/figures/figures_44_1.jpg", "caption": "Figure 6: Visualization of the projected distributions for real and imaginary datasets in the football environment.", "description": "This figure visualizes the results of t-SNE dimensionality reduction applied to real and imaginary datasets from the Google Research Football environment.  The \"real data\" points represent data collected from a rule-based policy.  The imaginary data is further split into \"low-unc. data\" (low uncertainty) and \"high-unc. data\" (high uncertainty) based on uncertainty scores from the model. The visualization shows that the imaginary data generally follows a similar distribution to the real data, indicating that the model successfully generates realistic data.  Yellow dashed circles highlight areas where the imaginary data deviates from the real data, indicating where the model's uncertainty is highest.", "section": "6.8 Imaginary Dataset Visualization"}]