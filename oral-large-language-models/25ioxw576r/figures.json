[{"figure_path": "25Ioxw576r/figures/figures_2_1.jpg", "caption": "Figure 1: Overview of the decoder-decoder architecture. Self-decoder generates the global KV cache. Then cross-decoder employs cross-attention to reuse the shared KV caches. Both self-decoder and cross-decoder use causal masking. The overall architecture behaves like a decoder-only Transformer, autoregressively generating tokens.", "description": "This figure illustrates the YOCO architecture, a decoder-decoder model for language generation.  It highlights the two main components: the self-decoder and the cross-decoder. The self-decoder efficiently creates a global key-value (KV) cache. This cache is then reused by the cross-decoder via cross-attention, allowing for efficient long-context processing while maintaining a decoder-only behavior. Both decoders utilize causal masking, preventing information leakage from future tokens during autoregressive generation.", "section": "3 You Only Cache Once (YOCO)"}, {"figure_path": "25Ioxw576r/figures/figures_3_1.jpg", "caption": "Figure 2: YOCO Inference. Prefill: encode input tokens in parallel. Generation: decode output tokens one by one. The computation flow enables prefilling to early exit without changing the final output, thereby significantly speeding up the prefill stage.", "description": "The figure illustrates the two-stage inference process of the YOCO model.  The *Prefilling* stage encodes the input tokens in parallel using only the self-decoder. The *Generation* stage then generates output tokens one by one using both the self- and cross-decoders.  The key point is that the prefilling stage can stop early before fully completing all layers of the self-decoder, significantly speeding up the overall process without altering the final output.", "section": "3.3 Inference Advantages"}, {"figure_path": "25Ioxw576r/figures/figures_6_1.jpg", "caption": "Figure 3: LM loss decreases along with scaling up the model size (ranging from 160M to 13B).", "description": "This figure displays the relationship between the number of parameters in a language model and its loss.  It shows that as the model size increases (from 160 million to 13 billion parameters), the loss consistently decreases, indicating improved performance.  Three model architectures are compared: the standard Transformer, YOCOSWA (You Only Cache Once with Sliding-Window Attention), and YOCOgRet (You Only Cache Once with Gated Retention).  YOCOgRet shows the lowest loss across all model sizes, suggesting its superior efficiency and performance.", "section": "5.2 Scalability Compared with Transformers"}, {"figure_path": "25Ioxw576r/figures/figures_7_1.jpg", "caption": "Figure 5: Cumulative average negative log-likelihood on book and repository-level code. We filter the validation examples that are longer than 1M tokens. YOCO achieves improved performance with longer context, i.e., utilizing long-distance information for language modeling.", "description": "This figure shows two line graphs, one for book data and one for repository-level code data, illustrating the cumulative average negative log-likelihood (NLL) as a function of sequence length.  The graphs demonstrate that the NLL generally decreases with longer sequence length, indicating improved performance of the YOCO model in capturing long-range dependencies within text.  The filtering of validation examples longer than 1M tokens suggests a focus on evaluating the model's performance on very long sequences.", "section": "5.3 Long-Context Evaluation"}, {"figure_path": "25Ioxw576r/figures/figures_7_2.jpg", "caption": "Figure 6: Breakdown memory consumption in 1M context length.", "description": "The figure shows the breakdown of GPU memory usage for both Transformer and YOCO models with a context length of 1M tokens.  The Transformer model's memory is dominated by KV Cache, while YOCO significantly reduces the KV Cache memory usage.  This illustrates the main memory saving advantage of the proposed YOCO architecture.", "section": "5.4 Inference Advantages"}, {"figure_path": "25Ioxw576r/figures/figures_8_1.jpg", "caption": "Figure 7: Inference memory of Transformer and YOCO across various lengths.", "description": "This figure compares the GPU memory usage of the Transformer and YOCO models across different context lengths (32K, 64K, 128K, 256K, 512K, and 1M tokens).  It visually demonstrates that YOCO's memory consumption remains relatively constant regardless of the context length, while the Transformer's memory usage increases dramatically.  The inset shows a zoomed-in view of the memory usage for shorter context lengths (32K, 64K, and 128K tokens).  The red arrows highlight the fold increase in memory consumption for Transformer compared to YOCO at each context length.  The results underscore YOCO's significant advantage in memory efficiency, especially when handling long sequences.", "section": "5.4 Inference Advantages"}, {"figure_path": "25Ioxw576r/figures/figures_8_2.jpg", "caption": "Figure 8: GPU memory of KV cache for each token with different model size.", "description": "The figure compares the GPU memory usage of key-value (KV) caches per token for Transformer and YOCO models of various sizes.  The Y-axis represents the KV cache memory in kilobytes per token, and the X-axis shows the model size in billions of parameters.  It demonstrates that YOCO's KV cache memory usage remains relatively constant across different model sizes, while the Transformer's KV cache memory usage increases significantly with model size.  The red arrows indicate the magnitude of the memory reduction achieved by YOCO compared to Transformer at each model size.", "section": "5.4 Inference Advantages"}, {"figure_path": "25Ioxw576r/figures/figures_8_3.jpg", "caption": "Figure 9: Prefilling latency for different lengths. Transformer's time grows quadratically while YOCO's grows linearly.", "description": "This figure compares the prefilling latency (time taken to prepare the model for text generation) of Transformer and YOCO models for various sequence lengths (32K to 1M tokens).  The key takeaway is that the Transformer's prefilling time increases quadratically with the sequence length, while YOCO's prefilling time increases linearly. This illustrates a significant advantage of YOCO in terms of efficiency and speed when handling long sequences.", "section": "Inference Advantages"}, {"figure_path": "25Ioxw576r/figures/figures_8_4.jpg", "caption": "Figure 10: Inference throughput of Transformer and YOCO varying the context length.", "description": "The bar chart compares the throughput (tokens/second) of the Transformer and YOCO models for different context lengths (32K, 64K, 128K, 256K, and 512K).  YOCO demonstrates significantly higher throughput than Transformer across all context lengths, with the improvement increasing as context length increases. The figure highlights the superior efficiency of YOCO in processing long sequences.", "section": "5.4 Inference Advantages"}, {"figure_path": "25Ioxw576r/figures/figures_9_1.jpg", "caption": "Figure 11: Long sequence task perplexity decreases along with the increasing input length.", "description": "This figure shows the results of long sequence task perplexity on four different datasets (GovReport, QMSum, Qasper, NarrativeQA) with different context lengths (4K, 8K, 12K, 16K).  It compares the performance of several models: Mamba, Sparse TRM, Hybrid H3, Transformer, and YOCOgRet. The graph illustrates how the perplexity (a measure of how well a model predicts a sequence) changes as the context length increases.  Generally, lower perplexity indicates better performance. The graph visually demonstrates the trend of decreasing perplexity as context length increases for all models, highlighting the impact of context length on language modeling performance.", "section": "5.3 Long-Context Evaluation"}, {"figure_path": "25Ioxw576r/figures/figures_13_1.jpg", "caption": "Figure 1: Overview of the decoder-decoder architecture. Self-decoder generates the global KV cache. Then cross-decoder employs cross-attention to reuse the shared KV caches. Both self-decoder and cross-decoder use causal masking. The overall architecture behaves like a decoder-only Transformer, autoregressively generating tokens.", "description": "This figure illustrates the YOCO architecture, a decoder-decoder model. The self-decoder layer efficiently encodes the global key-value (KV) cache which is then reused by the cross-decoder layer through cross-attention.  Both layers utilize causal masking. The result is a model that functions like a decoder-only Transformer but with the memory efficiency of only caching KV pairs once.", "section": "3 You Only Cache Once (YOCO)"}]