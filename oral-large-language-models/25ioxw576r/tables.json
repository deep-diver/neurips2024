[{"figure_path": "25Ioxw576r/tables/tables_3_1.jpg", "caption": "Table 1: Inference memory complexity of KV caches. N, L, D are the sequence length, number of layers, and hidden dimension.", "description": "This table compares the memory complexity of key-value (KV) caches between the Transformer and YOCO architectures.  It shows that Transformer's memory usage grows linearly with the sequence length (N), number of layers (L), and hidden dimension (D), while YOCO's memory usage has a much lower complexity, growing only linearly with N and D, due to its efficient caching mechanism that reuses KV pairs only once.", "section": "3.3 Inference Advantages"}, {"figure_path": "25Ioxw576r/tables/tables_3_2.jpg", "caption": "Table 2: Prefilling time complexity of attention modules. N, L, D are the same as above.", "description": "This table compares the time complexity of the attention modules in Transformer and YOCO models during the pre-filling stage.  It shows that Transformer's pre-filling time is proportional to the square of the sequence length (N), while YOCO's is linear with respect to N, indicating a significant improvement in efficiency for longer sequences.", "section": "3.3 Inference Advantages"}, {"figure_path": "25Ioxw576r/tables/tables_5_1.jpg", "caption": "Table 3: Eval Harness [10] accuracy compared with well-trained Transformer language models. We scale the 3B model to 1.6 trillion training tokens. The 1T and 1.6T results of StableLM-3B-4E1T are taken from its technical report [39]. YOCO-3B-1M is extended to the context length of 1M tokens.", "description": "This table compares the performance of the YOCO-3B model against other well-trained Transformer language models on the Eval Harness benchmark.  The comparison is done for various training token sizes (1T and 1.6T) and context lengths (up to 1M tokens).  It demonstrates that YOCO-3B achieves competitive performance compared to existing large language models, even when scaled to large training datasets and long contexts.", "section": "5.1 Language Modeling Evaluation"}, {"figure_path": "25Ioxw576r/tables/tables_6_1.jpg", "caption": "Table 4: Multi-needle retrieval accuracy. N indicates the number of needles. N = 1 is single-needle retrieval used as a reference, and N > 1 indicates the multi-needle test. The evaluation is conducted in 128K length, because most previous long-context models are tuned with this length.", "description": "This table presents the multi-needle retrieval accuracy of several long-context language models, including YOCO-3B-1M, on a 128K sequence length. The accuracy is measured by the number of correctly retrieved needles (N) out of a total number of needles, with N ranging from 1 to 8.  The results show YOCO's strong performance even compared to larger models.", "section": "5.3 Long-Context Evaluation"}, {"figure_path": "25Ioxw576r/tables/tables_9_1.jpg", "caption": "Table 5: Fine-grained LM perplexity results.", "description": "This table presents the fine-grained Language Model perplexity results for various models including Mamba, RetNet, Hybrid H3, gRetNet, Transformer, YOCOSWA, and YOCOgRet.  The perplexity is broken down into \"AR-Hit\", which measures the model's ability to recall previously seen bigrams, and \"First-Occur\", which measures the perplexity of tokens not previously seen.  Lower perplexity values indicate better performance.", "section": "5.5 Comparisons with Transformer Variants"}, {"figure_path": "25Ioxw576r/tables/tables_9_2.jpg", "caption": "Table 6: Fine-grained LM perplexity results. \"[s:c]\" is the ratio of self-decoder to cross-decoder layers.", "description": "This table presents the results of a fine-grained language model perplexity evaluation.  It compares different configurations of the YOCO model, varying the ratio of self-decoder to cross-decoder layers.  The metrics used are AR-Hit (autoregressive hit rate) and First-Occur (first occurrence rate), indicating the model's ability to recall previously seen tokens and handle novel tokens respectively. The table shows the impact of the layer ratio on the model's performance.", "section": "5.6 Ablation Studies"}, {"figure_path": "25Ioxw576r/tables/tables_14_1.jpg", "caption": "Table 3: Eval Harness [10] accuracy compared with well-trained Transformer language models. We scale the 3B model to 1.6 trillion training tokens. The 1T and 1.6T results of StableLM-3B-4E1T are taken from its technical report [39]. YOCO-3B-1M is extended to the context length of 1M tokens.", "description": "This table compares the performance of the YOCO-3B model with other well-trained Transformer language models on the Eval Harness benchmark.  It shows accuracy results for various tasks across three different model configurations: the 3B model trained on 1T tokens, the 3B model trained on 1.6T tokens, and the 3B model trained on 1.6T tokens with a context length extended to 1M.  The results demonstrate the performance of YOCO-3B, and how it scales up with increased training tokens and context length.", "section": "5.1 Language Modeling Evaluation"}, {"figure_path": "25Ioxw576r/tables/tables_15_1.jpg", "caption": "Table 7: Hyperparamters used for the YOCO-3B model in Section 5.1.", "description": "This table shows the hyperparameters used for training the YOCO-3B language model, which is the main model evaluated in Section 5.1 of the paper.  The hyperparameters cover various aspects of the training process, including the model architecture (number of layers, hidden size, FFN size, number of heads, etc.), the optimizer used (AdamW, along with its beta values), the learning rate, the batch size, the warmup steps, and the weight decay.  These parameters were chosen to achieve the reported results in Section 5.1. This model is trained and evaluated with one trillion tokens (1T).", "section": "5.1 Language Modeling Evaluation"}, {"figure_path": "25Ioxw576r/tables/tables_16_1.jpg", "caption": "Table 8: Model size and hyper-parameters used for scaling curves in Section 5.2.", "description": "This table shows the different model sizes that were used in the scaling curve experiments of Section 5.2 of the paper.  The table lists the number of parameters (size), hidden dimension, the number of layers, and the number of heads for each of the models used in the experiment. These parameters were varied to show how YOCO scales with respect to model size.", "section": "5.2 Scalability Compared with Transformers"}, {"figure_path": "25Ioxw576r/tables/tables_16_2.jpg", "caption": "Table 9: Hyperparamters used for length extension in Section 5.3.", "description": "This table shows the hyperparameters used for extending the context length to 1M tokens in Section 5.3 of the paper.  Specifically, it details the learning rate, RoPE \u03b8 (Rotary Position Embedding parameter), and the total number of training tokens used at each stage of the length extension schedule (64K, 256K, and 1M tokens).  These parameters were adjusted progressively as the model's context length increased.", "section": "5.3 Long-Context Evaluation"}]