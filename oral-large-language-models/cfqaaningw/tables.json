[{"figure_path": "cFqAANINgW/tables/tables_4_1.jpg", "caption": "Table 1: Experiment results on code generation benchmarks. We report Pass@1 as evaluate metric. Results from the original paper are underlined, and the best results are bold.", "description": "This table presents the performance of different code generation methods (Standard, CodeT, Reflexion, LDB, Parsel, MetaGPT, and FUNCODER) on three benchmarks (HumanEval, MBPP, and xCodeEval) using two different language models (GPT-3.5 and GPT-4).  For each method and benchmark, the table shows the Pass@1 score (the percentage of times the method generated a correct program on the first attempt), and the improvement over the standard method (\u0394\u2191). The best results for each benchmark are highlighted in bold.", "section": "3.1 Code Generation"}, {"figure_path": "cFqAANINgW/tables/tables_6_1.jpg", "caption": "Table 1: Experiment results on code generation benchmarks. We report Pass@1 as evaluate metric. Results from the original paper are underlined, and the best results are bold.", "description": "This table presents the performance of different code generation methods (Standard, CodeT, Reflexion, LDB, Parsel, MetaGPT, and FUNCODER) on various benchmarks (HumanEval, MBPP, xCodeEval) using two different large language models (GPT-3.5 and GPT-4).  Pass@1 represents the percentage of test cases where the generated code passed on the first attempt. The table shows the improvement of FUNCODER over existing state-of-the-art methods, highlighting its superior performance across multiple benchmarks and models.", "section": "3.1 Code Generation"}, {"figure_path": "cFqAANINgW/tables/tables_7_1.jpg", "caption": "Table 1: Experiment results on code generation benchmarks. We report Pass@1 as evaluate metric. Results from the original paper are underlined, and the best results are bold.", "description": "This table presents the results of experiments conducted on code generation benchmarks using various models and methods.  The Pass@1 metric, representing the percentage of test cases passed on the first attempt, is used to evaluate performance.  The table compares the performance of FUNCODER against several baseline methods, including standard prompting, CodeT, Reflexion, and MetaGPT, across benchmarks like HumanEval, MBPP, and xCodeEval.  The results for different models (GPT-3.5, GPT-4, Llama, StableCode, and CodeLlama) are shown, highlighting FUNCODER's improvements in accuracy.", "section": "3.1 Code Generation"}, {"figure_path": "cFqAANINgW/tables/tables_8_1.jpg", "caption": "Table 1: Experiment results on code generation benchmarks. We report Pass@1 as evaluate metric. Results from the original paper are underlined, and the best results are bold.", "description": "This table presents the performance comparison of different code generation methods (Standard, CodeT, Reflexion, LDB, Parsel, MetaGPT, and FUNCODER) using various language models (GPT-3.5, GPT-4, Llama3, StableCode, and CodeLlama) across four code generation benchmarks (HumanEval, MBPP, xCodeEval, and MATH).  Pass@1, representing the percentage of test cases where the model generates correct code on the first attempt, is used as the evaluation metric.  Results from the original paper are underlined for easy comparison, and the best results for each benchmark and model are highlighted in bold.", "section": "3.1 Code Generation"}, {"figure_path": "cFqAANINgW/tables/tables_15_1.jpg", "caption": "Table 1: Experiment results on code generation benchmarks. We report Pass@1 as evaluate metric. Results from the original paper are underlined, and the best results are bold.", "description": "This table presents the results of code generation experiments using various models and methods on three benchmarks: HumanEval, MBPP, and xCodeEval.  The Pass@1 metric indicates the percentage of test cases where the generated code passed on the first attempt.  The table compares the performance of FUNCODER against several baseline methods, including standard prompting, CodeT, and Reflexion.  Results are shown for both GPT-3.5 and GPT-4, as well as several open-source models.  Underlined values represent results from the original papers, while bolded values indicate the best results achieved in each category.", "section": "3.1 Code Generation"}, {"figure_path": "cFqAANINgW/tables/tables_17_1.jpg", "caption": "Table 1: Experiment results on code generation benchmarks. We report Pass@1 as evaluate metric. Results from the original paper are underlined, and the best results are bold.", "description": "This table presents the performance comparison of different code generation methods (Standard, CodeT, Reflexion, LDB, Parsel, MetaGPT, and FUNCODER) on various models (GPT-3.5, GPT-4, Llama38b, StableCode36, and CodeLlama346) across three benchmarks (HumanEval, MBPP, and xCodeEval).  For each model and method, the Pass@1 score (percentage of correctly generated code) is given, along with the improvement (\u0394\u2191) compared to the standard method.  The best results for each benchmark are highlighted in bold.", "section": "3.1 Code Generation"}, {"figure_path": "cFqAANINgW/tables/tables_18_1.jpg", "caption": "Table 1: Experiment results on code generation benchmarks. We report Pass@1 as evaluate metric. Results from the original paper are underlined, and the best results are bold.", "description": "This table presents the results of code generation experiments using various models and methods.  It compares the performance of FUNCODER against several baselines across four benchmarks: HumanEval, MBPP, xCodeEval, and a combined 'All' score. Pass@1 represents the percentage of test cases where the model correctly generates code on the first attempt.  The table shows the improvement of FUNCODER over existing methods.  Results from the original paper are highlighted, and the best-performing results are bolded for easy comparison.", "section": "3.1 Code Generation"}, {"figure_path": "cFqAANINgW/tables/tables_19_1.jpg", "caption": "Table 1: Experiment results on code generation benchmarks. We report Pass@1 as evaluate metric. Results from the original paper are underlined, and the best results are bold.", "description": "This table presents the performance of different code generation methods (Standard, CodeT, Reflexion, LDB, Parsel, MetaGPT, and FUNCODER) on various models (GPT-3.5, GPT-4, Llama, StableCode, and CodeLlama) across three benchmarks (HumanEval, MBPP, and xCodeEval).  The Pass@1 metric indicates the percentage of correctly generated programs.  The table highlights the improvements achieved by FUNCODER compared to other methods on each benchmark and model.  Results from the original paper are underlined for comparison, and the best-performing method for each row is shown in bold.", "section": "3.1 Code Generation"}, {"figure_path": "cFqAANINgW/tables/tables_23_1.jpg", "caption": "Table 1: Experiment results on code generation benchmarks. We report Pass@1 as evaluate metric. Results from the original paper are underlined, and the best results are bold.", "description": "This table presents the performance comparison of different code generation methods (Standard, CodeT, Reflexion, LDB, Parsel, MetaGPT, and FUNCODER) on various benchmarks (HumanEval, MBPP, xCodeEval) using two different Language Models (GPT-3.5 and GPT-4).  It shows the Pass@1 score (percentage of correctly generated programs) and the improvement (\u0394\u2191) achieved by each method compared to the standard approach for each benchmark.  The best result for each model/benchmark combination is shown in bold. Results from other studies are underlined for easy comparison.", "section": "3.1 Code Generation"}, {"figure_path": "cFqAANINgW/tables/tables_24_1.jpg", "caption": "Table 1: Experiment results on code generation benchmarks. We report Pass@1 as evaluate metric. Results from the original paper are underlined, and the best results are bold.", "description": "This table presents the performance comparison of different code generation methods (Standard, CodeT, Reflexion, LDB, Parsel, MetaGPT, and FUNCODER) on various benchmarks (HumanEval, MBPP, xCodeEval) using two large language models (GPT-3.5 and GPT-4).  For each model and method, it shows the Pass@1 score (percentage of correctly generated programs), along with the improvement (\u0394\u2191) compared to the standard method. The table highlights the best-performing method for each benchmark and model in bold, illustrating the effectiveness of FUNCODER compared to existing techniques.", "section": "3.1 Code Generation"}, {"figure_path": "cFqAANINgW/tables/tables_25_1.jpg", "caption": "Table 1: Experiment results on code generation benchmarks. We report Pass@1 as evaluate metric. Results from the original paper are underlined, and the best results are bold.", "description": "This table presents the performance comparison of different code generation methods (Standard, CodeT, Reflexion, LDB, Parsel, MetaGPT, and FUNCODER) on various models (GPT-3.5, GPT-4, Llama3, StableCode36, and CodeLlama346) across multiple benchmarks (HumanEval, MBPP, and xCodeEval).  Pass@1 represents the percentage of test cases where the generated code correctly solves the problem on the first attempt.  The table shows the improvement of FUNCODER over the baselines across all benchmarks and models.  The best performance for each setting is highlighted in bold.", "section": "3.1 Code Generation"}, {"figure_path": "cFqAANINgW/tables/tables_26_1.jpg", "caption": "Table 1: Experiment results on code generation benchmarks. We report Pass@1 as evaluate metric. Results from the original paper are underlined, and the best results are bold.", "description": "This table presents the performance comparison of different code generation methods (Standard, CodeT, Reflexion, LDB, Parsel, MetaGPT, and FUNCODER) on various models (GPT-3.5, GPT-4, Llama, StableCode, and CodeLlama) across three benchmarks (HumanEval, MBPP, and xCodeEval).  The Pass@1 metric indicates the percentage of correctly generated programs.  The table highlights FUNCODER's superior performance compared to other methods, especially on more complex tasks.", "section": "3.1 Code Generation"}, {"figure_path": "cFqAANINgW/tables/tables_27_1.jpg", "caption": "Table 1: Experiment results on code generation benchmarks. We report Pass@1 as evaluate metric. Results from the original paper are underlined, and the best results are bold.", "description": "This table presents the results of various code generation methods on different benchmarks (HumanEval, MBPP, xCodeEval).  It compares the performance of different models (GPT-3.5, GPT-4, Llama, StableCode, CodeLlama) using the Pass@1 metric (the percentage of times the top-ranked generated code passes all the tests).  The table highlights the improvement achieved by the proposed FUNCODER method compared to existing state-of-the-art methods.  Underlined values indicate results from the original papers being referenced, while bold values are the best results in the table.", "section": "3.1 Code Generation"}]