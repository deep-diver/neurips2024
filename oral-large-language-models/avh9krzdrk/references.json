{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational for the concept of in-context learning, a central theme of the current research."}, {"fullname_first_author": "Alethea Power", "paper_title": "Grokking: Generalization beyond overfitting on small algorithmic datasets", "publication_date": "2022-01-01", "reason": "This paper introduces the phenomenon of \"grokking,\" which is directly investigated and extended in the current work."}, {"fullname_first_author": "Kwangjun Ahn", "paper_title": "Transformers learn to implement preconditioned gradient descent for in-context learning", "publication_date": "2023-12-01", "reason": "This paper provides a theoretical framework for understanding in-context learning in transformers, which is relevant to the current study's mechanistic investigation."}, {"fullname_first_author": "Ekin Aky\u00fcrek", "paper_title": "What learning algorithm is in-context learning? Investigations with linear models", "publication_date": "2023-12-01", "reason": "This paper explores the algorithmic nature of in-context learning, a key aspect of the current research."}, {"fullname_first_author": "Johannes von Oswald", "paper_title": "Transformers learn in-context by gradient descent", "publication_date": "2023-12-01", "reason": "This paper offers another perspective on the optimization dynamics underlying in-context learning, complementing the current study"}]}