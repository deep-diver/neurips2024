[{"figure_path": "4NJBV6Wp0h/tables/tables_6_1.jpg", "caption": "Table 1: Correlation (Kendall's T) between the LLM's confidence in recognizing its summary and its confidence in preferring the same summary in pairs of examples.", "description": "This table shows the correlation between an LLM's ability to recognize its own summaries and its tendency to prefer those summaries, measured using Kendall's Tau correlation coefficient.  The results are broken down by model (GPT-3.5 and Llama 2), fine-tuning configuration (number of examples used for fine-tuning), and dataset (XSUM and CNN/DailyMail).  Higher correlation values indicate a stronger link between self-recognition and self-preference.", "section": "3 Measuring correlation between self-preference and self-recognition"}, {"figure_path": "4NJBV6Wp0h/tables/tables_14_1.jpg", "caption": "Table 6: Self-preference scores with correct and incorrect labels.", "description": "This table presents the self-preference scores obtained from three different LLMs (GPT-4, GPT-3.5, and Llama) on two datasets (XSUM and CNN).  The scores are broken down into two conditions: one where the source of the summary is correctly labeled, and one where the source is incorrectly labeled. This allows for an assessment of how much the models' preferences are influenced by knowing the source of the text.", "section": "3 Measuring correlation between self-preference and self-recognition"}, {"figure_path": "4NJBV6Wp0h/tables/tables_15_1.jpg", "caption": "Table 7: Pairwise results (self-recognition and self-preference scores) on the XSUM and CNN datasets.", "description": "This table presents the results of pairwise experiments evaluating self-recognition and self-preference on two datasets, XSUM and CNN.  It shows the performance of three LLMs (GPT-4, GPT-3.5, and Llama-2-7b) with and without fine-tuning on self-recognition, and also with fine-tuning on control tasks (always 1, random, readability, length, and vowel count).  The scores represent the LLM's confidence in identifying its own output and its preference for its own output compared to others' outputs.", "section": "Pairwise-setting experiments"}, {"figure_path": "4NJBV6Wp0h/tables/tables_16_1.jpg", "caption": "Table 8: Frequency of ambiguous and unambiguous pairwise results on the XSUM dataset.", "description": "This table presents the frequency of ambiguous and unambiguous results for self-recognition and self-preference tasks using pairwise comparisons on the XSUM dataset.  Ambiguous results are those where the LLM's preference reverses when the order of options is swapped. The table breaks down the results for each LLM model (GPT-4, GPT-3.5, Llama-2-7b) and further separates the results into those with no fine-tuning and those with fine-tuning using various control tasks (e.g., self-recognition, always 1, random, readability, length, vowel count). For self-preference, the results are categorized as self-preference, other-preference.  It helps to analyze the consistency and reliability of LLMs in their self-evaluations and preference judgments.", "section": "3 Measuring correlation between self-preference and self-recognition"}, {"figure_path": "4NJBV6Wp0h/tables/tables_17_1.jpg", "caption": "Table 7: Pairwise results (self-recognition and self-preference scores) on the XSUM and CNN datasets.", "description": "This table presents the results of pairwise experiments evaluating both self-recognition and self-preference.  It shows the scores for different models (GPT-4, GPT-3.5, Llama-2-7b) with and without fine-tuning on both XSUM and CNN datasets.  The scores reflect the models' ability to correctly identify their own outputs (self-recognition) and their tendency to prefer their own outputs (self-preference) compared to those generated by others.  Different fine-tuning tasks (self-recognition, always 1, random, readability, length, vowel count) are included to investigate the impact of these factors.", "section": "3.2 Fine-tuning results"}, {"figure_path": "4NJBV6Wp0h/tables/tables_19_1.jpg", "caption": "Table 11: Self-Recognition confidence scores in the individual setting, evaluated on the XSUM dataset.", "description": "This table presents the self-recognition confidence scores obtained from three large language models (LLMs): GPT-4, GPT-3.5, and Llama-2-7b.  The evaluation was conducted using an individual setting, where each LLM was given a single summary and asked to determine if the summary was generated by itself.  The table shows the confidence score for each LLM in recognizing its own summary against summaries generated by other LLMs (GPT-4, GPT-3.5, Llama) and humans. The scores indicate the LLMs' confidence level in identifying their own outputs. The experiment was performed on the XSUM dataset.  The rows show the model used to evaluate (evaluator) and the columns show the origin of the summary being evaluated.  In addition to the out-of-the-box performance, it also shows the performance after fine-tuning on self-recognition tasks.", "section": "3.2 Fine-tuning results"}, {"figure_path": "4NJBV6Wp0h/tables/tables_19_2.jpg", "caption": "Table 11: Self-Recognition confidence scores in the individual setting, evaluated on the XSUM dataset.", "description": "This table presents the self-recognition confidence scores achieved by different LLMs (GPT-4, GPT-3.5, Llama-2-7b) in an individual setting, where each model is given one summary at a time and asked to identify if it was generated by itself. The evaluation is performed on the XSUM dataset, with scores broken down by the source of the given summaries (GPT-4, GPT-3.5, Llama, Human, and Claude-2).  The table also shows the results from fine-tuning experiments using different numbers of examples and control tasks (Always 1, Random, Readability, Length, and Vowel Count). This allows for an analysis of how fine-tuning and control tasks influence the models' ability to recognize their own outputs.", "section": "3.2 Fine-tuning results"}, {"figure_path": "4NJBV6Wp0h/tables/tables_19_3.jpg", "caption": "Table 11: Self-Recognition confidence scores in the individual setting, evaluated on the XSUM dataset.", "description": "This table presents the self-recognition confidence scores obtained from three different LLMs (GPT-4, GPT-3.5, and Llama-2-7b) when evaluated individually on the XSUM dataset.  The scores reflect the LLM's confidence in identifying its own generated summaries compared to summaries generated by other LLMs or humans. Different fine-tuning configurations (2, 10, and 500 examples) for self-recognition were used, and the scores are shown for each configuration along with those for control tasks (Always 1, Random, Readability, Length, and Vowel Count).", "section": "3.2 Fine-tuning results"}, {"figure_path": "4NJBV6Wp0h/tables/tables_19_4.jpg", "caption": "Table 11: Self-Recognition confidence scores in the individual setting, evaluated on the XSUM dataset.", "description": "This table presents the self-recognition confidence scores obtained from three large language models (LLMs): GPT-4, GPT-3.5, and Llama-2-7b.  The scores are evaluated in an individual setting, where each model is presented with a single summary and asked whether it generated the summary. The summaries are from four sources: GPT-4, GPT-3.5, Llama-2, and humans. The table also includes results from control experiments, where models are fine-tuned on tasks unrelated to self-recognition (Always 1, Random, Readability, Length, Vowel Count). This allows for a comparison of self-recognition capabilities under various conditions and fine-tuning strategies.", "section": "3.2 Fine-tuning results"}, {"figure_path": "4NJBV6Wp0h/tables/tables_19_5.jpg", "caption": "Table 11: Self-Recognition confidence scores in the individual setting, evaluated on the XSUM dataset.", "description": "This table presents the self-recognition confidence scores obtained from different LLM evaluator models in an individual setting, specifically evaluated on the XSUM dataset.  The scores represent the LLMs' confidence in identifying whether a given summary was generated by itself or another source (including GPT-4, GPT-3.5, Llama 2, and human). Different fine-tuning configurations (2, 10, and 500 examples) for self-recognition, along with control tasks (Always 1, Random, Readability, and Vowel count), are included for comparison to assess the impact of fine-tuning on self-recognition capability.", "section": "3.2 Fine-tuning results"}, {"figure_path": "4NJBV6Wp0h/tables/tables_20_1.jpg", "caption": "Table 12: Self-preference scores in the individual setting, evaluated on the XSUM dataset.", "description": "This table presents self-preference scores obtained from three large language models (LLMs): GPT-4, GPT-3.5, and Llama 2-7b.  The scores reflect the models' tendency to rate their own generated summaries higher than summaries generated by other LLMs or human annotators. The evaluation is performed in an individual setting, meaning each LLM rates one summary at a time without comparing it to other options.  The table shows the results before any fine-tuning and after fine-tuning on several different tasks (self-recognition and control tasks using in-domain and out-of-domain data).", "section": "3.2 Fine-tuning results"}, {"figure_path": "4NJBV6Wp0h/tables/tables_20_2.jpg", "caption": "Table 11: Self-Recognition confidence scores in the individual setting, evaluated on the XSUM dataset.", "description": "This table presents the self-recognition confidence scores achieved by different LLMs (GPT-4, GPT-3.5, Llama-2-7b) in an individual setting, where each model is given a single summary and asked to determine if it generated the summary. The evaluation is performed on the XSUM dataset.  Scores are provided for each model and are broken down by the source of the summary (GPT-4, GPT-3.5, Llama, Human, Claude-2).  Additionally, results are shown for models fine-tuned on self-recognition tasks with varying numbers of examples (2, 10, 500), as well as control groups that always respond with '1', respond randomly, or are fine-tuned on length, readability, and vowel count.", "section": "3.2 Fine-tuning results"}, {"figure_path": "4NJBV6Wp0h/tables/tables_20_3.jpg", "caption": "Table 14: Self-recognition confidence scores in the individual setting, evaluated on the CNN dataset.", "description": "This table presents self-recognition confidence scores obtained from individual setting evaluations performed on the CNN dataset.  It shows the scores for various models (GPT-4, GPT-3.5, Llama-2-7b) before and after fine-tuning on different tasks (self-recognition with varying numbers of examples, always predicting 1, random prediction, readability, length, and vowel count). The scores represent the model's confidence in determining whether a given summary was generated by itself. The target source represents the true origin of the summaries (GPT-4, GPT-3.5, Llama, Human, Claude-2).", "section": "3.2 Fine-tuning results"}, {"figure_path": "4NJBV6Wp0h/tables/tables_20_4.jpg", "caption": "Table 11: Self-Recognition confidence scores in the individual setting, evaluated on the XSUM dataset.", "description": "This table presents the self-recognition confidence scores achieved by different LLMs (GPT-4, GPT-3.5, Llama-2-7b) in an individual setting. The scores are evaluated on the XSUM dataset and broken down by the source of the summary (GPT-4, GPT-3.5, Llama, Human, Claude-2).  The table also includes results for models fine-tuned on self-recognition tasks with varying numbers of examples (2, 10, 500), as well as control models (Always 1, Random, Readability, Length, Vowel count).  These control models help isolate the impact of the fine-tuning on self-recognition scores.", "section": "3.2 Fine-tuning results"}, {"figure_path": "4NJBV6Wp0h/tables/tables_20_5.jpg", "caption": "Table 7: Pairwise results (self-recognition and self-preference scores) on the XSUM and CNN datasets.", "description": "This table presents the results of pairwise experiments evaluating self-recognition and self-preference on two summarization datasets: XSUM and CNN/DailyMail.  It shows the scores for three different LLMs (GPT-4, GPT-3.5, and Llama-2-7b), both with and without fine-tuning for self-recognition on each dataset.  Fine-tuning was performed using different amounts of training examples (2, 10, and 500) and also included control tasks such as always outputting \"1\", a random response, based on readability scores, length, and vowel counts. The table provides a comparison of self-recognition and self-preference scores for each model and condition on both datasets to analyze the relationship between the two.", "section": "3.2 Fine-tuning results"}, {"figure_path": "4NJBV6Wp0h/tables/tables_21_1.jpg", "caption": "Table 11: Self-Recognition confidence scores in the individual setting, evaluated on the XSUM dataset.", "description": "This table presents the self-recognition confidence scores obtained from three large language models (GPT-4, GPT-3.5, and Llama-2-7b) in an individual setting.  Each model was tasked with identifying whether a given summary was generated by itself or another source (another LLM or human).  The table shows the confidence scores for each model in identifying its own summaries, along with additional scores for various fine-tuning scenarios and control experiments. These scenarios help to isolate the effect of self-recognition and determine its relation to other factors.", "section": "3.2 Fine-tuning results"}, {"figure_path": "4NJBV6Wp0h/tables/tables_21_2.jpg", "caption": "Table 13: Self-recognition confidence scores in the individual setting, evaluated on the CNN dataset.", "description": "This table presents self-recognition confidence scores obtained from individual setting experiments conducted on the CNN dataset. The scores are categorized by evaluator model (GPT-4, GPT-3.5, Llama-2-7b), fine-tuning configuration (number of examples), and target source (GPT-4, GPT-3.5, Llama, Human, Claude-2).  The results show the confidence of each model in identifying its own generated summaries among those from different sources in an individual setting.  Different fine-tuning scenarios are applied to understand their impact on self-recognition capability.", "section": "3.2 Fine-tuning results"}, {"figure_path": "4NJBV6Wp0h/tables/tables_21_3.jpg", "caption": "Table 7: Pairwise results (self-recognition and self-preference scores) on the XSUM and CNN datasets.", "description": "This table presents the results of pairwise experiments evaluating self-recognition and self-preference.  It shows the scores for GPT-4, GPT-3.5, and Llama-2-7b models on two datasets (XSUM and CNN), both before and after fine-tuning on self-recognition tasks with varying numbers of training examples (2, 10, and 500). It also includes results for control tasks (Always 1, Random, Readability, Length, Vowel count) to assess the impact of fine-tuning on unrelated properties.", "section": "3.2 Fine-tuning results"}, {"figure_path": "4NJBV6Wp0h/tables/tables_21_4.jpg", "caption": "Table 13: Self-recognition confidence scores in the individual setting, evaluated on the CNN dataset.", "description": "This table presents the self-recognition confidence scores obtained from different LLM evaluator models (GPT-4, GPT-3.5, Llama-2-7b) in an individual setting.  The evaluation was performed on the CNN dataset.  Scores are shown for different target sources (GPT-4, GPT-3.5, Llama, Human, Claude-2), and for various fine-tuning configurations (different numbers of examples for fine-tuning on self-recognition, along with control fine-tuning tasks: Always 1, Random, Readability, Length, Vowel count).  It helps to understand the impact of different fine-tuning strategies on the ability of LLMs to correctly identify their own generations.", "section": "3.2 Fine-tuning results"}, {"figure_path": "4NJBV6Wp0h/tables/tables_21_5.jpg", "caption": "Table 11: Self-Recognition confidence scores in the individual setting, evaluated on the XSUM dataset.", "description": "This table presents the self-recognition confidence scores obtained from different LLM evaluator models in an individual setting, using the XSUM dataset.  The scores represent the LLM's confidence in determining whether a given summary was generated by itself.  Results are shown for various models (GPT-4, GPT-3.5, Llama-2-7b), with and without fine-tuning on self-recognition tasks using different numbers of training examples (2, 10, 500).  Control experiments (Always 1, Random) and fine-tuning on unrelated tasks (Readability, Length, Vowel count) are also included for comparison.", "section": "3.2 Fine-tuning results"}, {"figure_path": "4NJBV6Wp0h/tables/tables_22_1.jpg", "caption": "Table 11: Self-Recognition confidence scores in the individual setting, evaluated on the XSUM dataset.", "description": "This table presents the self-recognition confidence scores obtained from three different LLMs (GPT-4, GPT-3.5, and Llama-2-7b) in an individual setting.  The scores represent the LLMs' confidence in identifying their own generated summaries among other summaries from various sources, including those generated by other LLMs and humans.  The table is organized to show the confidence scores for each evaluator LLM when presented with summaries generated by each of the target sources, including itself. Fine-tuning runs were conducted on both in-domain and out-of-domain datasets for improved self-recognition abilities. The results provide insights into the level of accuracy LLMs possess at self-recognition. ", "section": "3.2 Fine-tuning results"}, {"figure_path": "4NJBV6Wp0h/tables/tables_22_2.jpg", "caption": "Table 11: Self-Recognition confidence scores in the individual setting, evaluated on the XSUM dataset.", "description": "This table presents the self-recognition confidence scores obtained from three LLMs (GPT-4, GPT-3.5, and Llama-2-7b) in an individual setting, where each model is presented with a single summary and asked to determine if it generated the summary itself.  The evaluation is performed on the XSUM dataset. The table shows the confidence scores for each model when evaluating summaries generated by itself, the other two LLMs, humans, and Claude-2.  It also includes scores for control fine-tuning experiments (Always 1, Random, Readability, Length, Vowel count) to assess how these factors influence the self-recognition ability. The scores represent the model's confidence (ranging from 0 to 1) in its judgment.", "section": "3.2 Fine-tuning results"}, {"figure_path": "4NJBV6Wp0h/tables/tables_22_3.jpg", "caption": "Table 13: Self-recognition confidence scores in the individual setting, evaluated on the CNN dataset.", "description": "This table presents the self-recognition confidence scores obtained from individual setting experiments conducted on the CNN dataset.  The results are broken down by the model used (GPT-4, GPT-3.5, Llama-2-7b), the source of the summary (GPT-4, GPT-3.5, Llama, Human, Claude-2), and the number of fine-tuning examples used (2, 10, 500).  It also includes results for control tasks: 'Always 1', 'Random', 'Readability', 'Length', and 'Vowel count'.  The scores represent the LLM's confidence in correctly identifying its own summaries.", "section": "3.2 Fine-tuning results"}, {"figure_path": "4NJBV6Wp0h/tables/tables_22_4.jpg", "caption": "Table 11: Self-Recognition confidence scores in the individual setting, evaluated on the XSUM dataset.", "description": "This table presents the self-recognition confidence scores obtained from three different LLMs (GPT-4, GPT-3.5, Llama-2-7b) in an individual setting, where each model is presented with a single summary and asked to determine if it generated the summary itself. The scores are evaluated on the XSUM dataset and broken down by target source (GPT-4, GPT-3.5, Llama, Human, Claude-2).  The table also includes results for fine-tuned models on both in-domain and out-of-domain data, for various control tasks (Always 1, Random, Readability, Length, Vowel count).", "section": "3.2 Fine-tuning results"}, {"figure_path": "4NJBV6Wp0h/tables/tables_22_5.jpg", "caption": "Table 11: Self-Recognition confidence scores in the individual setting, evaluated on the XSUM dataset.", "description": "This table presents the self-recognition confidence scores obtained from different LLMs in an individual setting, using the XSUM dataset.  The scores represent the LLM's confidence in identifying its own generated summaries among summaries from other sources, including GPT-4, GPT-3.5, Llama 2, and human-generated summaries. The results are also categorized based on different fine-tuning configurations and control tasks (Always 1, Random, Readability, Length, Vowel Count) to analyze the impact of fine-tuning on self-recognition ability. The scores range from 0.494 to 0.896 indicating varied degrees of self-recognition accuracy across models and settings.", "section": "3.2 Fine-tuning results"}]