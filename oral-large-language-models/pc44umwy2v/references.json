{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper introduces the foundational large language model (LLM) architecture that underpins many of the models discussed and analyzed in the current paper."}, {"fullname_first_author": "Rohan Anil", "paper_title": "Palm 2 technical report", "publication_date": "2023-05-10", "reason": "This paper introduces PaLM 2, a state-of-the-art LLM, whose capabilities are relevant to the context of chain-of-thought (CoT) reasoning, which is the central topic of the current paper."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-12-01", "reason": "This paper is foundational to the current paper as it introduces and popularizes the chain-of-thought (CoT) prompting technique which is the core focus of the current paper's analysis."}, {"fullname_first_author": "Aman Madaan", "paper_title": "What makes chain-of-thought prompting effective? a counterfactual study", "publication_date": "2023-12-01", "reason": "This paper directly addresses the mechanisms underlying CoT prompting, offering insights into its effectiveness which are directly relevant to the analysis in the current paper."}, {"fullname_first_author": "Zhilin Yang", "paper_title": "Hotpotqa: A dataset for diverse, explainable multi-hop question answering", "publication_date": "2018-11-01", "reason": "This paper introduces a benchmark dataset for multi-hop question answering, a task relevant to the capabilities of LLMs and their use of CoT reasoning, hence important for evaluating the proposed methods."}]}