<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Natural Language Processing on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/natural-language-processing/</link><description>Recent content in Natural Language Processing on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/natural-language-processing/index.xml" rel="self" type="application/rss+xml"/><item><title>$ extit{Read-ME}$: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design</title><link>https://deep-diver.github.io/neurips2024/posters/i8jaxy7tdi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/i8jaxy7tdi/</guid><description>Read-ME refactors pre-trained dense LLMs into efficient, router-decoupled Mixture-of-Experts (MoEs) via activation sparsity, achieving up to 10.1% improvement on MMLU and 6.1% reduction in latency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/i8jaxy7tdi/cover.png"/></item><item><title>$ extit{Trans-LoRA}$: towards data-free Transferable Parameter Efficient Finetuning</title><link>https://deep-diver.github.io/neurips2024/posters/c3pakdyi3t/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/c3pakdyi3t/</guid><description>Trans-LoRA enables near data-free transfer of fine-tuned LLMs across models!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/c3pakdyi3t/cover.png"/></item><item><title>$eta$-DPO: Direct Preference Optimization with Dynamic $eta$</title><link>https://deep-diver.github.io/neurips2024/posters/zfbuhze556/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zfbuhze556/</guid><description>Î²-DPO dynamically adjusts a key parameter in Direct Preference Optimization, significantly improving LLM alignment with human preferences.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zfbuhze556/cover.png"/></item><item><title>3-in-1: 2D Rotary Adaptation for Efficient Finetuning, Efficient Batching and Composability</title><link>https://deep-diver.github.io/neurips2024/posters/ryjywum6yh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ryjywum6yh/</guid><description>RoAd: a novel parameter-efficient finetuning method uses 2D rotation to adapt LLMs, enabling efficient batching, composability, and improved interpretability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ryjywum6yh/cover.png"/></item><item><title>A Critical Evaluation of AI Feedback for Aligning Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/fzqyfmsmx9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fzqyfmsmx9/</guid><description>Contrary to popular belief, simple supervised fine-tuning with strong language models outperforms complex reinforcement learning in aligning large language models, significantly improving efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fzqyfmsmx9/cover.png"/></item><item><title>A distributional simplicity bias in the learning dynamics of transformers</title><link>https://deep-diver.github.io/neurips2024/posters/ggv6ucziwm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ggv6ucziwm/</guid><description>Transformers learn increasingly complex language patterns sequentially, starting with simpler interactions before mastering higher-order ones.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ggv6ucziwm/cover.png"/></item><item><title>A Full-duplex Speech Dialogue Scheme Based On Large Language Model</title><link>https://deep-diver.github.io/neurips2024/posters/yawxy6mwik/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yawxy6mwik/</guid><description>This paper introduces a novel full-duplex speech dialogue system based on LLMs, achieving significantly reduced response latency and higher interruption precision compared to half-duplex systems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yawxy6mwik/cover.png"/></item><item><title>A Gradient Accumulation Method for Dense Retriever under Memory Constraint</title><link>https://deep-diver.github.io/neurips2024/posters/qdg2q5myhv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qdg2q5myhv/</guid><description>CONTACCUM: Stable, efficient memory reduction for dense retrievers using dual memory banks, surpassing high-resource baselines.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qdg2q5myhv/cover.png"/></item><item><title>A Polar coordinate system represents syntax in large language models</title><link>https://deep-diver.github.io/neurips2024/posters/x2780vcmoi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x2780vcmoi/</guid><description>LLMs spontaneously encode syntax using a polar coordinate system, representing syntactic relations via relative direction and distance of word embeddings.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x2780vcmoi/cover.png"/></item><item><title>A Prompt-Based Knowledge Graph Foundation Model for Universal In-Context Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/vqyb9lkmuh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vqyb9lkmuh/</guid><description>KG-ICL, a novel prompt-based knowledge graph foundation model, achieves universal in-context reasoning by leveraging in-context learning and a unified tokenizer, outperforming various baselines on 43 &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vqyb9lkmuh/cover.png"/></item><item><title>A teacher-teacher framework for clinical language representation learning</title><link>https://deep-diver.github.io/neurips2024/posters/zdad8zv8tg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zdad8zv8tg/</guid><description>A lightweight knowledge alignment module enables two pre-trained LLMs to mutually learn and improve clinical language representation, exceeding individual model performance on various downstream tasks&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zdad8zv8tg/cover.png"/></item><item><title>A Theoretical Perspective for Speculative Decoding Algorithm</title><link>https://deep-diver.github.io/neurips2024/posters/wsqpnemvlu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wsqpnemvlu/</guid><description>This paper theoretically analyzes speculative decoding, revealing its optimality and providing formulas for expected rejections, paving the way for more efficient large language model inference.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wsqpnemvlu/cover.png"/></item><item><title>A Theoretical Understanding of Self-Correction through In-context Alignment</title><link>https://deep-diver.github.io/neurips2024/posters/otvnltwyww/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/otvnltwyww/</guid><description>LLMs improve through self-correction, but the mechanisms are unclear. This paper provides a theoretical framework and empirical evidence demonstrating that self-correction arises from in-context align&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/otvnltwyww/cover.png"/></item><item><title>Abrupt Learning in Transformers: A Case Study on Matrix Completion</title><link>https://deep-diver.github.io/neurips2024/posters/o9rzaep34l/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/o9rzaep34l/</guid><description>Transformers exhibit abrupt learning: training loss plateaus, then suddenly drops. This study uses matrix completion to demonstrate this phenomenon, providing insights into the model&amp;rsquo;s algorithmic sh&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/o9rzaep34l/cover.png"/></item><item><title>Accelerating Blockwise Parallel Language Models with Draft Refinement</title><link>https://deep-diver.github.io/neurips2024/posters/kt6f5sw0eg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kt6f5sw0eg/</guid><description>Boost LLM inference speed by 3x! This paper refines blockwise parallel decoding (BPD) by cleverly refining draft predictions, resulting in faster text generation for large language models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kt6f5sw0eg/cover.png"/></item><item><title>Accelerating Greedy Coordinate Gradient and General Prompt Optimization via Probe Sampling</title><link>https://deep-diver.github.io/neurips2024/posters/cmgxaarqzh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cmgxaarqzh/</guid><description>Probe sampling accelerates Greedy Coordinate Gradient (GCG) and other prompt optimization methods by up to 5.6x, achieving equal or better attack success rates, making LLM safety research faster and m&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cmgxaarqzh/cover.png"/></item><item><title>Accuracy is Not All You Need</title><link>https://deep-diver.github.io/neurips2024/posters/qvg7j29sta/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qvg7j29sta/</guid><description>LLM compression accuracy hides crucial behavioral changes; use % flips and KL-divergence for better evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qvg7j29sta/cover.png"/></item><item><title>Ad Auctions for LLMs via Retrieval Augmented Generation</title><link>https://deep-diver.github.io/neurips2024/posters/ujo8v7ixmr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ujo8v7ixmr/</guid><description>This paper introduces segment auctions, maximizing logarithmic social welfare, for integrating ads into LLM outputs via Retrieval Augmented Generation, balancing ad revenue and output quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ujo8v7ixmr/cover.png"/></item><item><title>AdaNovo: Towards Robust mph{De Novo} Peptide Sequencing in Proteomics against Data Biases</title><link>https://deep-diver.github.io/neurips2024/posters/0zfuisx5si/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0zfuisx5si/</guid><description>AdaNovo tackles data biases in de novo peptide sequencing by using Conditional Mutual Information, significantly improving PTM identification and overall accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0zfuisx5si/cover.png"/></item><item><title>Adaptable Logical Control for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/58x9v92zrd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/58x9v92zrd/</guid><description>Ctrl-G: A neuro-symbolic framework enables adaptable control of LLM generation by combining any LLM with a Hidden Markov Model (HMM), ensuring outputs adhere to logical constraints specified as determ&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/58x9v92zrd/cover.png"/></item><item><title>Adaptive Layer Sparsity for Large Language Models via Activation Correlation Assessment</title><link>https://deep-diver.github.io/neurips2024/posters/jup0qzxh7u/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jup0qzxh7u/</guid><description>Adaptive Layer Sparsity (ALS) revolutionizes large language model (LLM) compression by intelligently pruning less important layers, achieving significant size reduction without performance loss. It o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jup0qzxh7u/cover.png"/></item><item><title>Advancing Cross-domain Discriminability in Continual Learning of Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/bogxvywzeq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bogxvywzeq/</guid><description>RAIL, a novel continual learning method for vision-language models, tackles catastrophic forgetting and maintains zero-shot abilities without domain-identity hints or reference data. Using a recursiv&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bogxvywzeq/cover.png"/></item><item><title>Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees</title><link>https://deep-diver.github.io/neurips2024/posters/zipdu0chyu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zipdu0chyu/</guid><description>TP-LLaMA boosts tool-augmented LLMs by optimizing inference trajectories using preference learning from both successful and failed attempts, achieving superior performance and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zipdu0chyu/cover.png"/></item><item><title>Adversarial Moment-Matching Distillation of Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/0vescjrdby/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0vescjrdby/</guid><description>Boosting LLM efficiency, this study introduces adversarial moment-matching distillation, outperforming existing methods by matching action-value moments for superior knowledge transfer and achieving s&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0vescjrdby/cover.png"/></item><item><title>Adversarial Representation Engineering: A General Model Editing Framework for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/dq9ji8e9qq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dq9ji8e9qq/</guid><description>Adversarial Representation Engineering (ARE) offers a unified, interpretable approach for editing large language models (LLMs) by using a representation sensor as an editing oracle, enhancing model sa&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dq9ji8e9qq/cover.png"/></item><item><title>Agent Planning with World Knowledge Model</title><link>https://deep-diver.github.io/neurips2024/posters/j6kjss9o6i/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/j6kjss9o6i/</guid><description>This paper introduces a parametric World Knowledge Model (WKM) to improve AI agent planning by integrating both global task knowledge and dynamic state knowledge, thereby overcoming current LLMs&amp;rsquo; limi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/j6kjss9o6i/cover.png"/></item><item><title>AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases</title><link>https://deep-diver.github.io/neurips2024/posters/y841brw9ry/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y841brw9ry/</guid><description>AGENTPOISON: A novel backdoor attack compromises LLM agents by poisoning their memory or knowledge bases, achieving high success rates with minimal performance impact.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y841brw9ry/cover.png"/></item><item><title>AGILE: A Novel Reinforcement Learning Framework of LLM Agents</title><link>https://deep-diver.github.io/neurips2024/posters/ul3ldyo3xq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ul3ldyo3xq/</guid><description>AGILE, a novel reinforcement learning framework, significantly enhances LLM agents&amp;rsquo; performance on complex conversational tasks by integrating memory, tools, expert interactions, and reflection, outpe&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ul3ldyo3xq/cover.png"/></item><item><title>AlchemistCoder: Harmonizing and Eliciting Code Capability by Hindsight Tuning on Multi-source Data</title><link>https://deep-diver.github.io/neurips2024/posters/saqxbnvv4t/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/saqxbnvv4t/</guid><description>AlchemistCoder enhances code LLMs by pioneering hindsight tuning on multi-source data, harmonizing conflicting styles via AlchemistPrompts, and achieving state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/saqxbnvv4t/cover.png"/></item><item><title>Algorithmic Capabilities of Random Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/plh8gw7tpq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/plh8gw7tpq/</guid><description>Randomly initialized transformers, with only embedding layers optimized, surprisingly excel at various algorithmic tasks, revealing inherent capabilities even before training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/plh8gw7tpq/cover.png"/></item><item><title>Algorithmic progress in language models</title><link>https://deep-diver.github.io/neurips2024/posters/5qpmqtfvhy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/5qpmqtfvhy/</guid><description>Language model algorithms have improved drastically, halving compute needs every 8 months since 2012, surpassing Moore&amp;rsquo;s Law; however, compute scaling, not algorithms, drove most recent performance ga&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/5qpmqtfvhy/cover.png"/></item><item><title>ALI-Agent: Assessing LLMs' Alignment with Human Values via Agent-based Evaluation</title><link>https://deep-diver.github.io/neurips2024/posters/kzrfbtrpey/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kzrfbtrpey/</guid><description>ALI-Agent uses LLM-powered agents for in-depth, adaptive assessment of LLMs&amp;rsquo; alignment with human values, overcoming limitations of existing static benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kzrfbtrpey/cover.png"/></item><item><title>Aligning Large Language Models with Representation Editing: A Control Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/yttomsjssw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yttomsjssw/</guid><description>RE-Control: Aligning LLMs via dynamic representation editing using optimal control theory, achieving superior alignment with significantly fewer resources than fine-tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yttomsjssw/cover.png"/></item><item><title>Aligning LLM Agents by Learning Latent Preference from User Edits</title><link>https://deep-diver.github.io/neurips2024/posters/dlyngpcuwa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dlyngpcuwa/</guid><description>PRELUDE, a novel framework, leverages user edits of LLM outputs to learn latent preferences, improving agent alignment and minimizing edit costs. CIPHER, its efficient algorithm, infers preferences f&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dlyngpcuwa/cover.png"/></item><item><title>Aligning to Thousands of Preferences via System Message Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/recsheq7e8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/recsheq7e8/</guid><description>JANUS, a 7B LLM, achieves high alignment to thousands of user preferences by generalizing from diverse system messages, outperforming existing LLMs on various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/recsheq7e8/cover.png"/></item><item><title>Alignment at Pre-training! Towards Native Alignment for Arabic LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/worfmnjilp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/worfmnjilp/</guid><description>This study introduces &amp;rsquo;native alignment&amp;rsquo; for Arabic LLMs, achieving state-of-the-art results by aligning models during pre-training, rather than post-training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/worfmnjilp/cover.png"/></item><item><title>Alignment for Honesty</title><link>https://deep-diver.github.io/neurips2024/posters/67k3xlvw8l/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/67k3xlvw8l/</guid><description>This paper introduces a novel framework for aligning LLMs with honesty, proposing new metrics and training techniques to make LLMs more truthful and less prone to confidently incorrect responses.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/67k3xlvw8l/cover.png"/></item><item><title>AlphaMath Almost Zero: Process Supervision without Process</title><link>https://deep-diver.github.io/neurips2024/posters/vaxnxq3uko/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vaxnxq3uko/</guid><description>AlphaMath: LLMs excel at math reasoning without human-annotated process supervision, using Monte Carlo Tree Search.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vaxnxq3uko/cover.png"/></item><item><title>AlphaPruning: Using Heavy-Tailed Self Regularization Theory for Improved Layer-wise Pruning of Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/fhq4x2yxvv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fhq4x2yxvv/</guid><description>AlphaPruning leverages Heavy-Tailed Self-Regularization theory to allocate optimal layer-wise sparsity ratios in LLMs, achieving 80% sparsity in LLaMA-7B with reasonable perplexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fhq4x2yxvv/cover.png"/></item><item><title>ALPINE: Unveiling The Planning Capability of Autoregressive Learning in Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/wfbzusv14e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wfbzusv14e/</guid><description>ALPINE reveals how Transformer-based LLMs learn planning by embedding graph information into their weights, but also highlights their inability to handle transitive relationships.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wfbzusv14e/cover.png"/></item><item><title>ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/0lbx844upd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0lbx844upd/</guid><description>ALPS: An optimization-based framework achieves state-of-the-art one-shot LLM pruning, significantly reducing test perplexity and improving zero-shot performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0lbx844upd/cover.png"/></item><item><title>AmoebaLLM: Constructing Any-Shape Large Language Models for Efficient and Instant Deployment</title><link>https://deep-diver.github.io/neurips2024/posters/g0yxfmp87g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/g0yxfmp87g/</guid><description>AmoebaLLM: Instantly create optimally-sized LLMs for any platform!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/g0yxfmp87g/cover.png"/></item><item><title>AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback</title><link>https://deep-diver.github.io/neurips2024/posters/jimxgqemx3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jimxgqemx3/</guid><description>AMOR: Adaptable Modular knowledge agent using LLMs, excels with FSM-based reasoning and process feedback, enabling human supervision and domain adaptation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jimxgqemx3/cover.png"/></item><item><title>An Efficient Recipe for Long Context Extension via Middle-Focused Positional Encoding</title><link>https://deep-diver.github.io/neurips2024/posters/anheqfms0n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/anheqfms0n/</guid><description>Extend LLMs context via a simple, training-efficient positional encoding method, CREAM, outperforming existing methods by focusing on crucial mid-context information.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/anheqfms0n/cover.png"/></item><item><title>ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/nrwaskgm7a/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nrwaskgm7a/</guid><description>ANAH-v2 tackles LLM hallucination by introducing a self-training framework that iteratively scales annotation datasets and improves annotator accuracy, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nrwaskgm7a/cover.png"/></item><item><title>Analysing the Generalisation and Reliability of Steering Vectors</title><link>https://deep-diver.github.io/neurips2024/posters/v8x70gtodr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v8x70gtodr/</guid><description>Steering vectors, while promising for controlling LLMs, show unreliable in- and out-of-distribution performance, highlighting crucial limitations for real-world applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v8x70gtodr/cover.png"/></item><item><title>Analyzing &amp; Reducing the Need for Learning Rate Warmup in GPT Training</title><link>https://deep-diver.github.io/neurips2024/posters/zgdnrps46k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zgdnrps46k/</guid><description>This study reveals that modifying optimizers to normalize updates based on angular changes and gradient signal-to-noise ratio significantly reduces the need for learning rate warmup in GPT training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zgdnrps46k/cover.png"/></item><item><title>AP-Adapter: Improving Generalization of Automatic Prompts on Unseen Text-to-Image Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/posters/46v9axmouu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/46v9axmouu/</guid><description>AP-Adapter boosts text-to-image diffusion model generalization by using a two-stage prompt optimization method that leverages large language models and inter-model differences.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/46v9axmouu/cover.png"/></item><item><title>Apathetic or Empathetic? Evaluating LLMs' Emotional Alignments with Humans</title><link>https://deep-diver.github.io/neurips2024/posters/pwrvgrwtgg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pwrvgrwtgg/</guid><description>LLMs&amp;rsquo; emotional alignment with humans is assessed using emotion appraisal theory, revealing that while LLMs respond appropriately in some cases, they lack alignment with human emotional behaviors and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pwrvgrwtgg/cover.png"/></item><item><title>Approaching Human-Level Forecasting with Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/flcdw7npry/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/flcdw7npry/</guid><description>Language models (LMs) can now forecast future events as accurately as expert human forecasters! This groundbreaking research unveils a retrieval-augmented LM system surpassing human forecasters in spe&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/flcdw7npry/cover.png"/></item><item><title>Are More LLM Calls All You Need? Towards the Scaling Properties of Compound AI Systems</title><link>https://deep-diver.github.io/neurips2024/posters/m5106rrlgx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/m5106rrlgx/</guid><description>More LM calls don&amp;rsquo;t always mean better results for compound AI; this study reveals performance can initially increase then decrease, highlighting the importance of optimal call number prediction.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/m5106rrlgx/cover.png"/></item><item><title>ArkVale: Efficient Generative LLM Inference with Recallable Key-Value Eviction</title><link>https://deep-diver.github.io/neurips2024/posters/4oat5l4lye/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4oat5l4lye/</guid><description>ARKVALE boosts LLM inference efficiency by intelligently evicting and recalling key-value pairs from cache, improving latency and throughput without significant accuracy loss.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4oat5l4lye/cover.png"/></item><item><title>Artemis: Towards Referential Understanding in Complex Videos</title><link>https://deep-diver.github.io/neurips2024/posters/fanhyxy6y1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fanhyxy6y1/</guid><description>Artemis: A new MLLM excels at video-based referential understanding, accurately describing targets within complex videos using natural language questions and bounding boxes, surpassing existing models&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fanhyxy6y1/cover.png"/></item><item><title>Ask, Attend, Attack: An Effective Decision-Based Black-Box Targeted Attack for Image-to-Text Models</title><link>https://deep-diver.github.io/neurips2024/posters/9umjecuekk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/9umjecuekk/</guid><description>This paper introduces AAA, a novel three-stage decision-based black-box targeted attack against image-to-text models. AAA efficiently generates semantically consistent adversarial examples by asking &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/9umjecuekk/cover.png"/></item><item><title>Autoformalize Mathematical Statements by Symbolic Equivalence and Semantic Consistency</title><link>https://deep-diver.github.io/neurips2024/posters/8ihvbypmv4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8ihvbypmv4/</guid><description>Boosting AI&amp;rsquo;s math skills, this paper introduces a novel framework for autoformalizing mathematical statements, improving accuracy by 0.22-1.35x via symbolic equivalence and semantic consistency check&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8ihvbypmv4/cover.png"/></item><item><title>AutoGuide: Automated Generation and Selection of Context-Aware Guidelines for Large Language Model Agents</title><link>https://deep-diver.github.io/neurips2024/posters/mriqz8zd6o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mriqz8zd6o/</guid><description>AutoGuide: Automated generation of context-aware guidelines significantly improves LLM agent performance in unfamiliar domains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mriqz8zd6o/cover.png"/></item><item><title>AutoManual: Generating Instruction Manuals by LLM Agents via Interactive Environmental Learning</title><link>https://deep-diver.github.io/neurips2024/posters/pwl9n4zlf5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pwl9n4zlf5/</guid><description>LLM agents can now autonomously build environmental understanding via interactive learning, generating human-readable instruction manuals that boost task success rates.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pwl9n4zlf5/cover.png"/></item><item><title>AutoMix: Automatically Mixing Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/e6wrwivgzx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/e6wrwivgzx/</guid><description>AutoMix intelligently routes queries to different-sized LLMs based on a smaller model&amp;rsquo;s self-verification, minimizing cost while maintaining performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/e6wrwivgzx/cover.png"/></item><item><title>Autonomous Agents for Collaborative Task under Information Asymmetry</title><link>https://deep-diver.github.io/neurips2024/posters/mp6owpdijc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mp6owpdijc/</guid><description>iAgents: a novel multi-agent system leveraging LLMs, overcomes information asymmetry by mirroring human social networks to enable effective collaboration in complex tasks, achieving high accuracy in d&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mp6owpdijc/cover.png"/></item><item><title>AutoPSV: Automated Process-Supervised Verifier</title><link>https://deep-diver.github.io/neurips2024/posters/eoapwwogs9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/eoapwwogs9/</guid><description>AutoPSV automates process annotation for LLMs, improving reasoning by detecting confidence shifts in reasoning steps, thus efficiently enhancing model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/eoapwwogs9/cover.png"/></item><item><title>AutoSurvey: Large Language Models Can Automatically Write Surveys</title><link>https://deep-diver.github.io/neurips2024/posters/fexx8pmrdt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fexx8pmrdt/</guid><description>AutoSurvey automates comprehensive literature survey creation using LLMs, overcoming challenges of context limitations and knowledge constraints via a novel, efficient, and rigorously evaluated method&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fexx8pmrdt/cover.png"/></item><item><title>AutoTimes: Autoregressive Time Series Forecasters via Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/fovzztnp1h/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fovzztnp1h/</guid><description>AutoTimes repurposes LLMs as autoregressive time series forecasters, achieving state-of-the-art results with minimal trainable parameters and faster training/inference.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fovzztnp1h/cover.png"/></item><item><title>AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/n4qurxe19p/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/n4qurxe19p/</guid><description>AVATAR: A novel automated framework optimizes LLM agents for effective tool usage via contrastive reasoning, significantly boosting performance on complex tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/n4qurxe19p/cover.png"/></item><item><title>B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory</title><link>https://deep-diver.github.io/neurips2024/posters/rnqdry1h5v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rnqdry1h5v/</guid><description>B&amp;rsquo;MOJO: A novel hybrid architecture for foundation models enhances transductive inference by dynamically balancing eidetic and fading memory, leading to efficient and accurate processing of long seque&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rnqdry1h5v/cover.png"/></item><item><title>BackdoorAlign: Mitigating Fine-tuning based Jailbreak Attack with Backdoor Enhanced Safety Alignment</title><link>https://deep-diver.github.io/neurips2024/posters/1pcj5evta7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1pcj5evta7/</guid><description>BackdoorAlign defends against fine-tuning-based LLM jailbreaks using a &amp;lsquo;backdoor trigger&amp;rsquo; to enforce safety alignment during inference, effectively mitigating risks with minimal additional safety exam&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1pcj5evta7/cover.png"/></item><item><title>BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/0uxtfk5knj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0uxtfk5knj/</guid><description>BAdam: A memory-efficient optimization method enabling full parameter fine-tuning of large language models using a block coordinate descent framework with Adam&amp;rsquo;s update rule, achieving comparable or s&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0uxtfk5knj/cover.png"/></item><item><title>BAM! Just Like That: Simple and Efficient Parameter Upcycling for Mixture of Experts</title><link>https://deep-diver.github.io/neurips2024/posters/bdrwqtrfyi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bdrwqtrfyi/</guid><description>BAM! Efficiently upcycles pre-trained models into powerful Mixture-of-Experts (MoE) models, achieving state-of-the-art performance with reduced computational costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bdrwqtrfyi/cover.png"/></item><item><title>Base of RoPE Bounds Context Length</title><link>https://deep-diver.github.io/neurips2024/posters/eiielh2t7s/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/eiielh2t7s/</guid><description>LLM long-context ability is fundamentally limited by RoPE&amp;rsquo;s base parameter, which determines an absolute lower bound for achievable context length.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/eiielh2t7s/cover.png"/></item><item><title>Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/dylsyafmws/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dylsyafmws/</guid><description>Goldfish Loss: A novel training method for LLMs dramatically reduces memorization without impacting performance, addressing key safety, privacy, and copyright concerns.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dylsyafmws/cover.png"/></item><item><title>BERTs are Generative In-Context Learners</title><link>https://deep-diver.github.io/neurips2024/posters/bca9nmzkls/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bca9nmzkls/</guid><description>Masked language models can perform in-context learning, challenging the dominance of causal models in this area.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bca9nmzkls/cover.png"/></item><item><title>Beyond Accuracy: Ensuring Correct Predictions With Correct Rationales</title><link>https://deep-diver.github.io/neurips2024/posters/adv0pzi3ol/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/adv0pzi3ol/</guid><description>This research introduces a novel two-phase approach to improve AI model trustworthiness by ensuring both correct predictions and correct rationales. A new dataset with structured rationales and a rat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/adv0pzi3ol/cover.png"/></item><item><title>Bias Amplification in Language Model Evolution: An Iterated Learning Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/bsyn7ah4kx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bsyn7ah4kx/</guid><description>LLMs&amp;rsquo; iterative interactions amplify subtle biases; this paper uses a Bayesian Iterated Learning framework to explain this phenomenon and offers strategies to guide LLM evolution.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bsyn7ah4kx/cover.png"/></item><item><title>Bileve: Securing Text Provenance in Large Language Models Against Spoofing with Bi-level Signature</title><link>https://deep-diver.github.io/neurips2024/posters/vjcfnytg67/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vjcfnytg67/</guid><description>Bileve: a novel bi-level signature secures text provenance in LLMs against spoofing, enhancing detectability and reliability via fine-grained integrity checks and coarse-grained source tracing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vjcfnytg67/cover.png"/></item><item><title>BiScope: AI-generated Text Detection by Checking Memorization of Preceding Tokens</title><link>https://deep-diver.github.io/neurips2024/posters/hew2jsdycr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hew2jsdycr/</guid><description>BISCOPE: AI-generated text detection using a novel bidirectional method that outperforms existing techniques by leveraging both prediction and memorization of preceding tokens.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hew2jsdycr/cover.png"/></item><item><title>BitDelta: Your Fine-Tune May Only Be Worth One Bit</title><link>https://deep-diver.github.io/neurips2024/posters/xuwwq3gy7w/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xuwwq3gy7w/</guid><description>BitDelta drastically shrinks fine-tuned LLMs by quantizing their weight deltas to just one bit, achieving 10x memory reduction and latency improvements without sacrificing performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xuwwq3gy7w/cover.png"/></item><item><title>BLoB: Bayesian Low-Rank Adaptation by Backpropagation for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/madykgj4ru/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/madykgj4ru/</guid><description>BLoB: Bayesian Low-Rank Adaptation by Backpropagation enhances LLMs by jointly tuning mean and covariance of parameters during fine-tuning, improving uncertainty estimation and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/madykgj4ru/cover.png"/></item><item><title>BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling</title><link>https://deep-diver.github.io/neurips2024/posters/haskmlrbx5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/haskmlrbx5/</guid><description>BoNBON alignment optimizes large language model (LLM) outputs towards human preferences using best-of-n sampling, maximizing win-rate against base models with minimal off-target impact.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/haskmlrbx5/cover.png"/></item><item><title>Boosting Semi-Supervised Scene Text Recognition via Viewing and Summarizing</title><link>https://deep-diver.github.io/neurips2024/posters/7nryncn2be/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7nryncn2be/</guid><description>ViSu boosts semi-supervised scene text recognition by using an online generation strategy for diverse synthetic data and a novel character alignment loss to improve model generalization and robustness&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7nryncn2be/cover.png"/></item><item><title>Boosting the Potential of Large Language Models with an Intelligent Information Assistant</title><link>https://deep-diver.github.io/neurips2024/posters/ozy4a11sug/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ozy4a11sug/</guid><description>Boosting LLMs with an intelligent information assistant, ASSISTRAG, significantly improves accuracy and reasoning, especially for less advanced models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ozy4a11sug/cover.png"/></item><item><title>Boosting Weakly Supervised Referring Image Segmentation via Progressive Comprehension</title><link>https://deep-diver.github.io/neurips2024/posters/mxdygxok9h/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mxdygxok9h/</guid><description>PCNet boosts weakly-supervised referring image segmentation by progressively processing textual cues, mimicking human comprehension, and significantly improving target localization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mxdygxok9h/cover.png"/></item><item><title>Breaking Determinism: Fuzzy Modeling of Sequential Recommendation Using Discrete State Space Diffusion Model</title><link>https://deep-diver.github.io/neurips2024/posters/h3bdt2umwq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/h3bdt2umwq/</guid><description>DDSR: a novel sequential recommendation model uses fuzzy sets and discrete diffusion to capture user behavior randomness, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/h3bdt2umwq/cover.png"/></item><item><title>Bridge the Modality and Capability Gaps in Vision-Language Model Selection</title><link>https://deep-diver.github.io/neurips2024/posters/01qa1zjs65/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/01qa1zjs65/</guid><description>SWAB bridges modality and capability gaps in Vision-Language Model selection using optimal transport, enabling accurate prediction of VLM performance without images.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/01qa1zjs65/cover.png"/></item><item><title>Bridge-IF: Learning Inverse Protein Folding with Markov Bridges</title><link>https://deep-diver.github.io/neurips2024/posters/q8yfhrbbd8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/q8yfhrbbd8/</guid><description>Bridge-IF, a novel generative diffusion model, excels at inverse protein folding by learning probabilistic dependencies between protein structures and sequences, significantly outperforming existing m&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/q8yfhrbbd8/cover.png"/></item><item><title>Bridging semantics and pragmatics in information-theoretic emergent communication</title><link>https://deep-diver.github.io/neurips2024/posters/2wlnniqcb7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2wlnniqcb7/</guid><description>AI agents learn human-like communication, combining semantic categorization and pragmatic context-sensitive reasoning, through a novel information-theoretic framework.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2wlnniqcb7/cover.png"/></item><item><title>Building on Efficient Foundations: Effective Training of LLMs with Structured Feedforward Layers</title><link>https://deep-diver.github.io/neurips2024/posters/wxlvyzbiew/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wxlvyzbiew/</guid><description>Training large language models efficiently is key; this paper shows how using structured feedforward layers and a novel training regime significantly reduces computational costs and improves training &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wxlvyzbiew/cover.png"/></item><item><title>Cal-DPO: Calibrated Direct Preference Optimization for Language Model Alignment</title><link>https://deep-diver.github.io/neurips2024/posters/57oqxxbtby/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/57oqxxbtby/</guid><description>Cal-DPO calibrates implicit rewards in contrastive preference learning, dramatically improving large language model alignment with human preferences.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/57oqxxbtby/cover.png"/></item><item><title>Calibrating Reasoning in Language Models with Internal Consistency</title><link>https://deep-diver.github.io/neurips2024/posters/udzkvmpf3s/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/udzkvmpf3s/</guid><description>LLMs&amp;rsquo; reasoning can be improved by using internal consistency to calibrate their outputs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/udzkvmpf3s/cover.png"/></item><item><title>Can Graph Learning Improve Planning in LLM-based Agents?</title><link>https://deep-diver.github.io/neurips2024/posters/bmos6ggw4j/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bmos6ggw4j/</guid><description>GNNs enhance LLM-based task planning by improving the ability to process task graphs, surpassing existing solutions even without training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bmos6ggw4j/cover.png"/></item><item><title>Can Language Models Learn to Skip Steps?</title><link>https://deep-diver.github.io/neurips2024/posters/w4antvxao9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w4antvxao9/</guid><description>Language models learn to skip steps in reasoning, improving efficiency and generalization, showcasing emergent human-like cognitive abilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w4antvxao9/cover.png"/></item><item><title>Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?</title><link>https://deep-diver.github.io/neurips2024/posters/fbuodm02ra/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fbuodm02ra/</guid><description>LLMs struggle with noisy rationales in chain-of-thought prompting. This paper introduces the NoRa dataset, showing that existing methods struggle. A new method, CD-CoT, significantly improves accura&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fbuodm02ra/cover.png"/></item><item><title>Can Large Language Model Agents Simulate Human Trust Behavior?</title><link>https://deep-diver.github.io/neurips2024/posters/ceowahuqic/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ceowahuqic/</guid><description>LLM agents surprisingly exhibit human-like trust behavior, especially GPT-4, paving the way for simulating complex human interactions in various applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ceowahuqic/cover.png"/></item><item><title>Can large language models explore in-context?</title><link>https://deep-diver.github.io/neurips2024/posters/owpzhvqiux/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/owpzhvqiux/</guid><description>LLMs struggle with in-context exploration, needing substantial prompt engineering or training interventions to effectively explore multi-armed bandit environments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/owpzhvqiux/cover.png"/></item><item><title>Can LLMs Implicitly Learn Numeric Parameter Constraints in Data Science APIs?</title><link>https://deep-diver.github.io/neurips2024/posters/lfc5rujstk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lfc5rujstk/</guid><description>LLMs struggle to reliably generate valid data science code due to a lack of true understanding of numerical constraints in APIs, despite seemingly mastering common patterns through extensive training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lfc5rujstk/cover.png"/></item><item><title>Can LLMs Learn by Teaching for Better Reasoning? A Preliminary Study</title><link>https://deep-diver.github.io/neurips2024/posters/0zzmujzjyf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0zzmujzjyf/</guid><description>LLMs can improve reasoning by teaching weaker models, a process called Learning by Teaching (LbT), as shown in this preliminary study. LbT enhances not just student models, but also the teacher model&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0zzmujzjyf/cover.png"/></item><item><title>Can Models Learn Skill Composition from Examples?</title><link>https://deep-diver.github.io/neurips2024/posters/1sldprsbmk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1sldprsbmk/</guid><description>Smaller language models can learn skill composition from limited examples, substantially improving their ability to combine skills in novel ways through fine-tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1sldprsbmk/cover.png"/></item><item><title>Cascade Speculative Drafting for Even Faster LLM Inference</title><link>https://deep-diver.github.io/neurips2024/posters/lzy9u0ijp7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lzy9u0ijp7/</guid><description>Cascade Speculative Drafting (CS Drafting) dramatically speeds up large language model inference by using a multi-stage drafting process, optimizing both time allocation and autoregressive generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lzy9u0ijp7/cover.png"/></item><item><title>Causal language modeling can elicit search and reasoning capabilities on logic puzzles</title><link>https://deep-diver.github.io/neurips2024/posters/i5poejmwoc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/i5poejmwoc/</guid><description>LLMs surprisingly master complex logic puzzles like Sudoku and Zebra puzzles after training on strategically ordered solution steps, revealing hidden reasoning abilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/i5poejmwoc/cover.png"/></item><item><title>Chain of Agents: Large Language Models Collaborating on Long-Context Tasks</title><link>https://deep-diver.github.io/neurips2024/posters/luclf4bjsr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/luclf4bjsr/</guid><description>Chain-of-Agents (CoA) framework uses multi-agent collaboration to efficiently process long contexts for LLMs, significantly improving performance on various tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/luclf4bjsr/cover.png"/></item><item><title>Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/2cczgofmp4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2cczgofmp4/</guid><description>Chain of Preference Optimization (CPO) dramatically improves LLM reasoning by leveraging ToT&amp;rsquo;s search tree for efficient fine-tuning, achieving similar or better performance with significantly reduced&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2cczgofmp4/cover.png"/></item><item><title>Chain of Thoughtlessness? An Analysis of CoT in Planning</title><link>https://deep-diver.github.io/neurips2024/posters/kpbeazu5nm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kpbeazu5nm/</guid><description>Chain of Thought prompting in LLMs offers limited generalizability, providing performance gains only when prompts are highly specific to problem types; highlighting a critical trade-off between perfor&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kpbeazu5nm/cover.png"/></item><item><title>Chain-of-Thought Reasoning Without Prompting</title><link>https://deep-diver.github.io/neurips2024/posters/4zt7s0b0jp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4zt7s0b0jp/</guid><description>LLMs can reason effectively without prompting by simply adjusting the decoding process to reveal inherent chain-of-thought paths.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4zt7s0b0jp/cover.png"/></item><item><title>Chat-Scene: Bridging 3D Scene and Large Language Models with Object Identifiers</title><link>https://deep-diver.github.io/neurips2024/posters/t3bhmwazhv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/t3bhmwazhv/</guid><description>Chat-Scene: Bridging 3D scenes and LLMs using object identifiers for efficient, object-level interaction and improved scene comprehension.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/t3bhmwazhv/cover.png"/></item><item><title>ChatQA: Surpassing GPT-4 on Conversational QA and RAG</title><link>https://deep-diver.github.io/neurips2024/posters/bkuvkpkafq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bkuvkpkafq/</guid><description>ChatQA, a new suite of models, outperforms GPT-4 in conversational QA and RAG by using a two-stage instruction tuning method and a cost-effective dense retriever.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bkuvkpkafq/cover.png"/></item><item><title>ChatTracker: Enhancing Visual Tracking Performance via Chatting with Multimodal Large Language Model</title><link>https://deep-diver.github.io/neurips2024/posters/hzanl2uncb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hzanl2uncb/</guid><description>ChatTracker boosts visual tracking by intelligently using a large language model to refine object descriptions, achieving performance on par with state-of-the-art methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hzanl2uncb/cover.png"/></item><item><title>Cherry on Top: Parameter Heterogeneity and Quantization in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/qaiklacrkj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qaiklacrkj/</guid><description>CherryQ, a novel quantization method, leverages parameter heterogeneity in LLMs to achieve superior performance by selectively quantizing less critical parameters while preserving essential ones.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qaiklacrkj/cover.png"/></item><item><title>CigTime: Corrective Instruction Generation Through Inverse Motion Editing</title><link>https://deep-diver.github.io/neurips2024/posters/gkta1qycj9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gkta1qycj9/</guid><description>CigTime generates corrective motion instructions from motion pairs using motion editing and large language models. This innovative approach improves upon baselines by leveraging motion triplets for f&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gkta1qycj9/cover.png"/></item><item><title>CLUES: Collaborative Private-domain High-quality Data Selection for LLMs via Training Dynamics</title><link>https://deep-diver.github.io/neurips2024/posters/ou1uqd1vyw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ou1uqd1vyw/</guid><description>CLUES: Collaborative learning selects high-quality private data for LLM fine-tuning via training dynamics, significantly boosting performance in diverse domains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ou1uqd1vyw/cover.png"/></item><item><title>Code Repair with LLMs gives an Exploration-Exploitation Tradeoff</title><link>https://deep-diver.github.io/neurips2024/posters/o863gx6dxa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/o863gx6dxa/</guid><description>New program synthesis method, REX, leverages Thompson Sampling to balance exploration and exploitation in iterative LLM code refinement, solving more problems with fewer model calls.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/o863gx6dxa/cover.png"/></item><item><title>CodeRosetta: Pushing the Boundaries of Unsupervised Code Translation for Parallel Programming</title><link>https://deep-diver.github.io/neurips2024/posters/v6hrg4o9gg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v6hrg4o9gg/</guid><description>Code Rosetta pushes the boundaries of unsupervised code translation by introducing the first encoder-decoder model that efficiently translates between programming languages and their parallel HPC exte&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v6hrg4o9gg/cover.png"/></item><item><title>Coevolving with the Other You: Fine-Tuning LLM with Sequential Cooperative Multi-Agent Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/ooocozfvk3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ooocozfvk3/</guid><description>CORY: a novel multi-agent RL framework boosts LLM fine-tuning!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ooocozfvk3/cover.png"/></item><item><title>COLD: Causal reasOning in cLosed Daily activities</title><link>https://deep-diver.github.io/neurips2024/posters/7mo1noosnt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7mo1noosnt/</guid><description>COLD framework rigorously evaluates LLMs&amp;rsquo; causal reasoning in everyday scenarios using 9 million causal queries derived from human-generated scripts of daily activities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7mo1noosnt/cover.png"/></item><item><title>Combining Observational Data and Language for Species Range Estimation</title><link>https://deep-diver.github.io/neurips2024/posters/iokluxb05h/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/iokluxb05h/</guid><description>LE-SINR combines Wikipedia species descriptions with citizen science observations to create accurate species range maps, even with limited data, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/iokluxb05h/cover.png"/></item><item><title>CoMERA: Computing- and Memory-Efficient Training via Rank-Adaptive Tensor Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/hxglvysg2c/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hxglvysg2c/</guid><description>CoMERA achieves 2-3x faster AI model training via rank-adaptive tensor optimization, significantly improving both computing and memory efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hxglvysg2c/cover.png"/></item><item><title>Compact Language Models via Pruning and Knowledge Distillation</title><link>https://deep-diver.github.io/neurips2024/posters/9u0nlnnmj7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/9u0nlnnmj7/</guid><description>MINITRON: Efficiently creating smaller, high-performing LLMs via pruning &amp;amp; distillation, slashing training costs by up to 40x!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/9u0nlnnmj7/cover.png"/></item><item><title>Compositional 3D-aware Video Generation with LLM Director</title><link>https://deep-diver.github.io/neurips2024/posters/oqdy2efrja/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oqdy2efrja/</guid><description>LLM-directed compositional 3D-aware video generation (C3V) achieves high-fidelity video generation with diverse motion and flexible concept control by decomposing prompts, generating 3D concepts, and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oqdy2efrja/cover.png"/></item><item><title>Compressing Large Language Models using Low Rank and Low Precision Decomposition</title><link>https://deep-diver.github.io/neurips2024/posters/lkx3opcqsz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lkx3opcqsz/</guid><description>CALDERA: a new post-training LLM compression algorithm achieving state-of-the-art zero-shot performance using low-rank, low-precision decomposition.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lkx3opcqsz/cover.png"/></item><item><title>Concentrate Attention: Towards Domain-Generalizable Prompt Optimization for Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/zoarr5qmfx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zoarr5qmfx/</guid><description>Boost language model performance across domains with &amp;lsquo;Concentration&amp;rsquo;: a new prompt optimization objective that prioritizes stable, deep-layer attention.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zoarr5qmfx/cover.png"/></item><item><title>Confidence Regulation Neurons in Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/0og7nmvdbe/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0og7nmvdbe/</guid><description>LLMs regulate uncertainty via specialized &amp;rsquo;entropy&amp;rsquo; and &amp;rsquo;token frequency&amp;rsquo; neurons, impacting prediction confidence without directly altering logits.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0og7nmvdbe/cover.png"/></item><item><title>Conformal Alignment: Knowing When to Trust Foundation Models with Guarantees</title><link>https://deep-diver.github.io/neurips2024/posters/yzycejlv9z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yzycejlv9z/</guid><description>Conformal Alignment certifies trustworthy foundation model outputs by guaranteeing a user-specified fraction meet alignment criteria, regardless of the model or data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yzycejlv9z/cover.png"/></item><item><title>Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data</title><link>https://deep-diver.github.io/neurips2024/posters/7fokmz6u8n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7fokmz6u8n/</guid><description>LLMs surprisingly infer censored knowledge from implicit training data hints, posing safety challenges.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7fokmz6u8n/cover.png"/></item><item><title>ConStat: Performance-Based Contamination Detection in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/alispmdpcq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/alispmdpcq/</guid><description>ConStat: Exposing hidden LLM contamination!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/alispmdpcq/cover.png"/></item><item><title>Construction and Application of Materials Knowledge Graph in Multidisciplinary Materials Science via Large Language Model</title><link>https://deep-diver.github.io/neurips2024/posters/gb5a0rryuv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gb5a0rryuv/</guid><description>Researchers created a Materials Knowledge Graph (MKG) using large language models to efficiently organize and integrate knowledge from a decade of high-quality materials science research, enhancing da&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gb5a0rryuv/cover.png"/></item><item><title>ContextCite: Attributing Model Generation to Context</title><link>https://deep-diver.github.io/neurips2024/posters/7cmnsqszjt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7cmnsqszjt/</guid><description>CONTEXTCITE pinpoints which parts of a given context led a language model to generate a specific statement, improving model verification and response quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7cmnsqszjt/cover.png"/></item><item><title>Continual Learning with Global Alignment</title><link>https://deep-diver.github.io/neurips2024/posters/4vp0edvy4o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4vp0edvy4o/</guid><description>Researchers developed a novel continual learning method achieving state-of-the-art performance by aligning data representations across tasks using pre-trained tokens, eliminating the need for experien&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4vp0edvy4o/cover.png"/></item><item><title>Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents</title><link>https://deep-diver.github.io/neurips2024/posters/0zwzjj6lo3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0zwzjj6lo3/</guid><description>LLMs struggle to cooperate sustainably; GOVSIM reveals this, showing communication and &amp;lsquo;universalization&amp;rsquo; reasoning improve outcomes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0zwzjj6lo3/cover.png"/></item><item><title>CorDA: Context-Oriented Decomposition Adaptation of Large Language Models for Task-Aware Parameter-Efficient Fine-tuning</title><link>https://deep-diver.github.io/neurips2024/posters/gi00nvru6n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gi00nvru6n/</guid><description>CorDA: Context-oriented weight decomposition enhances large language model fine-tuning by integrating task context, improving efficiency and mitigating catastrophic forgetting.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gi00nvru6n/cover.png"/></item><item><title>Cost-efficient Knowledge-based Question Answering with Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/pje1y71jad/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pje1y71jad/</guid><description>Coke: A cost-efficient KBQA strategy using LLMs and KGMs, maximizing accuracy while minimizing GPT-4 fees by up to 20.89%</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pje1y71jad/cover.png"/></item><item><title>CoVoMix: Advancing Zero-Shot Speech Generation for Human-like Multi-talker Conversations</title><link>https://deep-diver.github.io/neurips2024/posters/vnbqbv658b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vnbqbv658b/</guid><description>CoVoMix: Generating human-like, multi-speaker conversations with zero-shot speech synthesis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vnbqbv658b/cover.png"/></item><item><title>Crafting Interpretable Embeddings for Language Neuroscience by Asking LLMs Questions</title><link>https://deep-diver.github.io/neurips2024/posters/mxmvwwybwe/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mxmvwwybwe/</guid><description>LLM-based text embeddings are powerful but lack interpretability. This paper introduces QA-Emb, a novel method that uses an LLM to answer yes/no questions about a text, thereby producing an interpreta&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mxmvwwybwe/cover.png"/></item><item><title>CriticEval: Evaluating Large-scale Language Model as Critic</title><link>https://deep-diver.github.io/neurips2024/posters/zsxz65yql1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zsxz65yql1/</guid><description>CRITICEVAL: A new benchmark reliably evaluates LLMs&amp;rsquo; ability to identify and correct flaws in their responses, addressing limitations of existing methods by offering comprehensive and reliable evaluat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zsxz65yql1/cover.png"/></item><item><title>Cross-model Control: Improving Multiple Large Language Models in One-time Training</title><link>https://deep-diver.github.io/neurips2024/posters/ypqhstsofs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ypqhstsofs/</guid><description>One-time training improves multiple LLMs using a tiny portable model, drastically reducing costs and resource needs for model enhancement.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ypqhstsofs/cover.png"/></item><item><title>CultureLLM: Incorporating Cultural Differences into Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/sisbokqmbl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sisbokqmbl/</guid><description>CultureLLM, a new approach, effectively incorporates cultural nuances into LLMs using semantic data augmentation, significantly outperforming existing models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sisbokqmbl/cover.png"/></item><item><title>CulturePark: Boosting Cross-cultural Understanding in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/bifhhf2rod/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bifhhf2rod/</guid><description>CulturePark, a novel multi-agent communication framework, generates high-quality cross-cultural data to fine-tune LLMs, significantly reducing cultural bias and boosting cross-cultural understanding.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bifhhf2rod/cover.png"/></item><item><title>Customizing Language Models with Instance-wise LoRA for Sequential Recommendation</title><link>https://deep-diver.github.io/neurips2024/posters/isz8xre3de/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/isz8xre3de/</guid><description>Instance-wise LoRA (iLoRA) boosts LLM sequential recommendation accuracy by customizing model parameters for each user, mitigating negative transfer and improving performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/isz8xre3de/cover.png"/></item><item><title>D-CPT Law: Domain-specific Continual Pre-Training Scaling Law for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/jzkfn5fwok/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jzkfn5fwok/</guid><description>New D-CPT Law optimizes continual pre-training for LLMs by predicting optimal data mixture ratios, drastically cutting training costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jzkfn5fwok/cover.png"/></item><item><title>D-LLM: A Token Adaptive Computing Resource Allocation Strategy for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/uiojgtkhqg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uiojgtkhqg/</guid><description>D-LLM dynamically allocates computing resources during LLM token processing, reducing computational costs and memory usage by up to 50% without sacrificing accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uiojgtkhqg/cover.png"/></item><item><title>DAGER: Exact Gradient Inversion for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/cradax7h23/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cradax7h23/</guid><description>DAGER: Exact gradient inversion for LLMs; recovers full input text batches precisely.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cradax7h23/cover.png"/></item><item><title>DAPE: Data-Adaptive Positional Encoding for Length Extrapolation</title><link>https://deep-diver.github.io/neurips2024/posters/rnueubrxvu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rnueubrxvu/</guid><description>DAPE: A novel data-adaptive positional encoding method dynamically adjusts positional information based on input context, improving transformer performance and length generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rnueubrxvu/cover.png"/></item><item><title>DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph</title><link>https://deep-diver.github.io/neurips2024/posters/5ifecna7zr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/5ifecna7zr/</guid><description>DARG dynamically evaluates LLMs via adaptive reasoning graphs, revealing performance drops with increased complexity and exposing model biases.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/5ifecna7zr/cover.png"/></item><item><title>DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving</title><link>https://deep-diver.github.io/neurips2024/posters/zlu21oqjd5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zlu21oqjd5/</guid><description>DART-Math tackles LLM limitations in mathematical problem-solving by introducing Difficulty-Aware Rejection Tuning, a novel method that generates high-quality, bias-reduced datasets, resulting in supe&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zlu21oqjd5/cover.png"/></item><item><title>Data Mixture Inference Attack: BPE Tokenizers Reveal Training Data Compositions</title><link>https://deep-diver.github.io/neurips2024/posters/ehxyeimux0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ehxyeimux0/</guid><description>Researchers uncover hidden training data secrets of large language models by analyzing their byte-pair encoding tokenizers, revealing the proportions of different languages and domains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ehxyeimux0/cover.png"/></item><item><title>Data-Efficient Learning with Neural Programs</title><link>https://deep-diver.github.io/neurips2024/posters/qxqy58xu25/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qxqy58xu25/</guid><description>ISED: a novel, data-efficient algorithm learns neural programs by sampling from neural predictions to estimate gradients of black-box components, outperforming baselines on various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qxqy58xu25/cover.png"/></item><item><title>Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum</title><link>https://deep-diver.github.io/neurips2024/posters/r8m9sfymdi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/r8m9sfymdi/</guid><description>This paper introduces dataset decomposition (DD), a novel approach to accelerate LLM training while enhancing performance. DD significantly reduces training time by decomposing datasets into buckets &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/r8m9sfymdi/cover.png"/></item><item><title>DDK: Distilling Domain Knowledge for Efficient Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/xgiuruq0ss/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xgiuruq0ss/</guid><description>DDK: Dynamically Distilling Domain Knowledge for efficient LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xgiuruq0ss/cover.png"/></item><item><title>Decision-Making Behavior Evaluation Framework for LLMs under Uncertain Context</title><link>https://deep-diver.github.io/neurips2024/posters/re0ly2ylcu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/re0ly2ylcu/</guid><description>New framework reveals LLMs&amp;rsquo; human-like decision-making tendencies but highlights significant variations and biases influenced by demographic factors, underscoring ethical deployment needs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/re0ly2ylcu/cover.png"/></item><item><title>Decoding-Time Language Model Alignment with Multiple Objectives</title><link>https://deep-diver.github.io/neurips2024/posters/3csul7tvpv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3csul7tvpv/</guid><description>Multi-objective decoding (MOD) efficiently aligns language models to diverse user needs by decoding the next token from a weighted combination of predictions from multiple base models trained on indiv&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3csul7tvpv/cover.png"/></item><item><title>Deep Bayesian Active Learning for Preference Modeling in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/tadtt9ughn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tadtt9ughn/</guid><description>BAL-PM, a novel active learning approach, drastically reduces human feedback in LLM preference modeling by leveraging both model uncertainty and prompt distribution diversity, achieving 33%-68% fewer &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tadtt9ughn/cover.png"/></item><item><title>DeiSAM: Segment Anything with Deictic Prompting</title><link>https://deep-diver.github.io/neurips2024/posters/cmsnx47aeh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cmsnx47aeh/</guid><description>DeiSAM uses large language models and differentiable logic to achieve highly accurate image segmentation using complex, context-dependent descriptions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cmsnx47aeh/cover.png"/></item><item><title>Delta-CoMe: Training-Free Delta-Compression with Mixed-Precision for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/cr5eqrjlrn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cr5eqrjlrn/</guid><description>Delta-CoMe: Training-free mixed-precision delta compression boosts LLM deployment efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cr5eqrjlrn/cover.png"/></item><item><title>Delving into the Reversal Curse: How Far Can Large Language Models Generalize?</title><link>https://deep-diver.github.io/neurips2024/posters/1wxfznqwhp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1wxfznqwhp/</guid><description>Large language models struggle to generalize knowledge when facing seemingly simple reversals, a phenomenon termed the &amp;lsquo;reversal curse.&amp;rsquo; This study reveals that this limitation is strongly linked to t&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1wxfznqwhp/cover.png"/></item><item><title>DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging</title><link>https://deep-diver.github.io/neurips2024/posters/kmnoh7cxrq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kmnoh7cxrq/</guid><description>DenseFormer enhances transformers by adding a depth-weighted averaging step, improving data efficiency and outperforming baselines in memory and inference time without increasing model size.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kmnoh7cxrq/cover.png"/></item><item><title>DETAIL: Task DEmonsTration Attribution for Interpretable In-context Learning</title><link>https://deep-diver.github.io/neurips2024/posters/4jrnkah15k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4jrnkah15k/</guid><description>DETAIL: A novel attribution method reveals the impact of individual demonstrations in in-context learning, boosting interpretability and improving transformer-based model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4jrnkah15k/cover.png"/></item><item><title>DeTeCtive: Detecting AI-generated Text via Multi-Level Contrastive Learning</title><link>https://deep-diver.github.io/neurips2024/posters/cdtttjfje3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cdtttjfje3/</guid><description>DeTeCtive: a novel multi-task contrastive learning framework, achieves state-of-the-art AI-generated text detection by distinguishing diverse writing styles instead of simple binary classification.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cdtttjfje3/cover.png"/></item><item><title>DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via Adaptive Heads Fusion</title><link>https://deep-diver.github.io/neurips2024/posters/g92nu7knrq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/g92nu7knrq/</guid><description>Decoupled-Head Attention (DHA) drastically cuts LLM inference costs by adaptively sharing key/value heads, achieving 97.6% of original performance with only 0.25% pre-training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/g92nu7knrq/cover.png"/></item><item><title>Diff-eRank: A Novel Rank-Based Metric for Evaluating Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/nvn80cscvm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nvn80cscvm/</guid><description>Diff-eRank: A novel rank-based metric assessing LLMs&amp;rsquo; efficiency in eliminating redundant information during training, showing improved correlation with model size and performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nvn80cscvm/cover.png"/></item><item><title>DiffNorm: Self-Supervised Normalization for Non-autoregressive Speech-to-speech Translation</title><link>https://deep-diver.github.io/neurips2024/posters/tg2evad7vf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tg2evad7vf/</guid><description>DIFFNORM boosts non-autoregressive speech-to-speech translation by normalizing speech data with a diffusion model and classifier-free guidance, achieving significant quality improvements.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tg2evad7vf/cover.png"/></item><item><title>Diffusion of Thought: Chain-of-Thought Reasoning in Diffusion Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/g0v0txx01n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/g0v0txx01n/</guid><description>Diffusion-of-Thought (DoT) boosts reasoning in diffusion language models by enabling parallel reasoning steps, outperforming larger autoregressive models in speed and accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/g0v0txx01n/cover.png"/></item><item><title>Discovering Preference Optimization Algorithms with and for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/erjqdj0z9l/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/erjqdj0z9l/</guid><description>LLMs discover novel offline preference optimization algorithms, achieving state-of-the-art performance on various tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/erjqdj0z9l/cover.png"/></item><item><title>Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/rgtryvc9n4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rgtryvc9n4/</guid><description>DSA, a novel automated framework, discovers optimal sparsity allocation for layer-wise LLM pruning, achieving significant performance gains across various models and tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rgtryvc9n4/cover.png"/></item><item><title>Discovery of the Hidden World with Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/w50icqc6qj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w50icqc6qj/</guid><description>COAT leverages LLMs to identify high-level causal factors from unstructured data, enabling causal discovery in real-world scenarios where well-defined variables are lacking.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w50icqc6qj/cover.png"/></item><item><title>Discrete Modeling via Boundary Conditional Diffusion Processes</title><link>https://deep-diver.github.io/neurips2024/posters/7awmtpmzes/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7awmtpmzes/</guid><description>Bridging the gap between continuous diffusion models and discrete data, this work introduces a novel boundary-conditional approach achieving superior performance in language modeling and image generat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7awmtpmzes/cover.png"/></item><item><title>DISP-LLM: Dimension-Independent Structural Pruning for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/yxay6thgg0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yxay6thgg0/</guid><description>DISP-LLM: A novel dimension-independent structural pruning method for LLMs achieves accuracy similar to semi-structural pruning while improving flexibility and efficiency, outperforming state-of-the-a&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yxay6thgg0/cover.png"/></item><item><title>Distributional Preference Alignment of LLMs via Optimal Transport</title><link>https://deep-diver.github.io/neurips2024/posters/2lctgfn6ty/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2lctgfn6ty/</guid><description>LLMs are aligned to human preferences distributionally using Optimal Transport, achieving state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2lctgfn6ty/cover.png"/></item><item><title>Divergences between Language Models and Human Brains</title><link>https://deep-diver.github.io/neurips2024/posters/dpp5f3ufkw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dpp5f3ufkw/</guid><description>Language models struggle with social/emotional intelligence and physical commonsense, unlike human brains. Fine-tuning models on these aspects improves their brain response prediction accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dpp5f3ufkw/cover.png"/></item><item><title>DLAD: Improving Logits-based Detector without Logits from Black-box LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/heksssv5q9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/heksssv5q9/</guid><description>DALD: A novel framework for black-box LLM text detection, achieving state-of-the-art performance without relying on source model logits, by aligning surrogate model distributions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/heksssv5q9/cover.png"/></item><item><title>Do LLMs Build World Representations? Probing Through the Lens of State Abstraction</title><link>https://deep-diver.github.io/neurips2024/posters/lzfzjyuwgy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lzfzjyuwgy/</guid><description>LLMs prioritize task completion over full world-state understanding by using goal-oriented abstractions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lzfzjyuwgy/cover.png"/></item><item><title>Do LLMs dream of elephants (when told not to)? Latent concept association and associative memory in transformers</title><link>https://deep-diver.github.io/neurips2024/posters/wj04zx8txm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wj04zx8txm/</guid><description>LLMs&amp;rsquo; fact retrieval is easily manipulated by context, highlighting their associative memory behavior; this paper studies this with transformers, showing how self-attention and value matrices support &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wj04zx8txm/cover.png"/></item><item><title>Does Reasoning Emerge? Examining the Probabilities of Causation in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/b1ylcyjazk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/b1ylcyjazk/</guid><description>LLMs&amp;rsquo; reasoning abilities are assessed via a novel framework that leverages probabilities of causation, revealing that while capable, their understanding of causality falls short of human-level reason&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/b1ylcyjazk/cover.png"/></item><item><title>DoFIT: Domain-aware Federated Instruction Tuning with Alleviated Catastrophic Forgetting</title><link>https://deep-diver.github.io/neurips2024/posters/fdfrpugkgu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fdfrpugkgu/</guid><description>DoFIT: A novel domain-aware framework significantly reduces catastrophic forgetting in federated instruction tuning by finely aggregating overlapping weights and using a proximal perturbation initiali&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fdfrpugkgu/cover.png"/></item><item><title>Doing Experiments and Revising Rules with Natural Language and Probabilistic Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/hxdafk488a/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hxdafk488a/</guid><description>This paper introduces ActiveACRE, a model that uses LLMs and probabilistic inference to infer natural language rules through online experimentation, demonstrating higher accuracy than existing methods&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hxdafk488a/cover.png"/></item><item><title>DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation</title><link>https://deep-diver.github.io/neurips2024/posters/x4eotqw7ka/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x4eotqw7ka/</guid><description>DropBP: Accelerate LLM fine-tuning by 44% while preserving accuracy!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x4eotqw7ka/cover.png"/></item><item><title>Dual-Personalizing Adapter for Federated Foundation Models</title><link>https://deep-diver.github.io/neurips2024/posters/nkwpibsw1f/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nkwpibsw1f/</guid><description>Federated Dual-Personalizing Adapter (FedDPA) tackles test-time distribution shifts and personalization in federated foundation models using a global and local adapter co-working mechanism, achieving &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nkwpibsw1f/cover.png"/></item><item><title>EAI: Emotional Decision-Making of LLMs in Strategic Games and Ethical Dilemmas</title><link>https://deep-diver.github.io/neurips2024/posters/8aaayewnr4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8aaayewnr4/</guid><description>LLMs&amp;rsquo; emotional decision-making is assessed using a novel framework, EAI, showing that &lt;strong>emotions significantly alter ethical and strategic choices&lt;/strong> in games. This reveals crucial biases, necessitati&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8aaayewnr4/cover.png"/></item><item><title>Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision</title><link>https://deep-diver.github.io/neurips2024/posters/qwgfh2fttn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qwgfh2fttn/</guid><description>AI alignment beyond human supervision is achieved via easy-to-hard generalization: training reward models on easy tasks to effectively evaluate and improve generators on harder tasks, achieving superh&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qwgfh2fttn/cover.png"/></item><item><title>Edit Distance Robust Watermarks via Indexing Pseudorandom Codes</title><link>https://deep-diver.github.io/neurips2024/posters/fz45kf5pia/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fz45kf5pia/</guid><description>This paper presents a novel watermarking scheme for language models that is both undetectable and robust to a constant fraction of adversarial edits (insertions, deletions, substitutions).</description></item><item><title>Efficient Contextual LLM Cascades through Budget-Constrained Policy Learning</title><link>https://deep-diver.github.io/neurips2024/posters/adqlaz09ds/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/adqlaz09ds/</guid><description>TREACLE: a reinforcement learning policy efficiently selects LLMs and prompts, achieving up to 85% cost savings while maintaining high accuracy in answering reasoning questions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/adqlaz09ds/cover.png"/></item><item><title>Efficient Large Multi-modal Models via Visual Context Compression</title><link>https://deep-diver.github.io/neurips2024/posters/5ujp72ciyb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/5ujp72ciyb/</guid><description>LLaVolta significantly boosts multi-modal LLMs by using visual context compression, achieving substantial training cost reduction and enhanced inference efficiency without performance loss.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/5ujp72ciyb/cover.png"/></item><item><title>Efficient LLM Jailbreak via Adaptive Dense-to-sparse Constrained Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/bn5pa3hho8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bn5pa3hho8/</guid><description>Adaptive Dense-to-sparse Constrained Optimization (ADC) efficiently jailbreaks LLMs by transforming discrete token optimization into a continuous process, achieving higher success rates than existing &amp;hellip;</description></item><item><title>Efficient LLM Scheduling by Learning to Rank</title><link>https://deep-diver.github.io/neurips2024/posters/wlljyl0gi6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wlljyl0gi6/</guid><description>Learning to rank request outputs improves LLM scheduling, resulting in 2.8x lower chatbot latency and 6.5x higher synthetic data generation throughput.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wlljyl0gi6/cover.png"/></item><item><title>Efficient Minimum Bayes Risk Decoding using Low-Rank Matrix Completion Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/8ipobekuua/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8ipobekuua/</guid><description>Fast approximation of Minimum Bayes Risk (MBR) decoding achieved using low-rank matrix completion algorithms, drastically reducing computational cost without sacrificing translation quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8ipobekuua/cover.png"/></item><item><title>Efficient multi-prompt evaluation of LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/jzkpwcj200/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jzkpwcj200/</guid><description>PromptEval efficiently estimates LLM performance across many prompts, providing robust performance metrics and enabling reliable LLM comparisons.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jzkpwcj200/cover.png"/></item><item><title>Efficient Multi-task LLM Quantization and Serving for Multiple LoRA Adapters</title><link>https://deep-diver.github.io/neurips2024/posters/hfpv6u0kbx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hfpv6u0kbx/</guid><description>LoRA-Inlaid: a novel multi-task LLM serving system boosts throughput by 1.58x, latency by 1.76x, and job completion time by 2x, while improving SLO attainment by 10x, all while maintaining model quali&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hfpv6u0kbx/cover.png"/></item><item><title>Efficient Prompt Optimization Through the Lens of Best Arm Identification</title><link>https://deep-diver.github.io/neurips2024/posters/flnnlfbgmo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/flnnlfbgmo/</guid><description>TRIPLE: Efficient prompt optimization using fixed-budget best-arm identification.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/flnnlfbgmo/cover.png"/></item><item><title>Efficient Sketches for Training Data Attribution and Studying the Loss Landscape</title><link>https://deep-diver.github.io/neurips2024/posters/8jycrgxor5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8jycrgxor5/</guid><description>Novel sketching algorithms enable scalable gradient and Hessian analysis for large language models, revealing insights into their intrinsic dimensionality and challenging existing assumptions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8jycrgxor5/cover.png"/></item><item><title>EffiLearner: Enhancing Efficiency of Generated Code via Self-Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/khwoub0fs9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/khwoub0fs9/</guid><description>EFFI-LEARNER: A novel self-optimization framework dramatically improves the efficiency of LLM-generated code by iteratively refining code based on execution profiles.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/khwoub0fs9/cover.png"/></item><item><title>Elo Uncovered: Robustness and Best Practices in Language Model Evaluation</title><link>https://deep-diver.github.io/neurips2024/posters/pc9lljtl5f/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pc9lljtl5f/</guid><description>Elo rating&amp;rsquo;s reliability for LLM evaluation is challenged, revealing inconsistencies and suggesting new, more robust methods are needed for accurate model ranking.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pc9lljtl5f/cover.png"/></item><item><title>Embedding Trajectory for Out-of-Distribution Detection in Mathematical Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/hymxyeyec5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hymxyeyec5/</guid><description>Novel trajectory volatility score (TV Score) significantly improves out-of-distribution detection in mathematical reasoning by leveraging dynamic embedding trajectories, outperforming existing GLM met&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hymxyeyec5/cover.png"/></item><item><title>Embedding-Aligned Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/wsu1ppi2up/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wsu1ppi2up/</guid><description>EAGLE: Guiding LLMs using latent embeddings for controlled text generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wsu1ppi2up/cover.png"/></item><item><title>End-to-End Ontology Learning with Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/uqvehancjc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uqvehancjc/</guid><description>OLLM: An end-to-end LLM method builds ontologies from scratch, outperforming subtask approaches and improving semantic accuracy with novel evaluation metrics.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uqvehancjc/cover.png"/></item><item><title>Enhancing In-Context Learning Performance with just SVD-Based Weight Pruning: A Theoretical Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/wt6ghk5shc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wt6ghk5shc/</guid><description>SVD-based weight pruning surprisingly boosts in-context learning in large language models, especially when applied to deeper layers, offering a novel approach to model compression and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wt6ghk5shc/cover.png"/></item><item><title>Enhancing Large Language Models through Adaptive Tokenizers</title><link>https://deep-diver.github.io/neurips2024/posters/3h1wqedk4z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3h1wqedk4z/</guid><description>Adaptive tokenizers enhance LLMs by dynamically optimizing vocabulary during training, improving accuracy without increasing vocabulary size.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3h1wqedk4z/cover.png"/></item><item><title>Enhancing Large Vision Language Models with Self-Training on Image Comprehension</title><link>https://deep-diver.github.io/neurips2024/posters/fzw7ctyjm3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fzw7ctyjm3/</guid><description>Self-Training on Image Comprehension (STIC) significantly boosts Large Vision Language Model (LVLM) performance using unlabeled image data. STIC generates a preference dataset for image descriptions &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fzw7ctyjm3/cover.png"/></item><item><title>Enhancing LLMâs Cognition via Structurization</title><link>https://deep-diver.github.io/neurips2024/posters/q5ckneun6k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/q5ckneun6k/</guid><description>LLMs struggle with complex, long-form text. This paper introduces &amp;lsquo;context structurization,&amp;rsquo; transforming unstructured text into a structured format to enhance LLM comprehension. Experiments across &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/q5ckneun6k/cover.png"/></item><item><title>Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation Control</title><link>https://deep-diver.github.io/neurips2024/posters/askckanxno/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/askckanxno/</guid><description>Boosting LLM trustworthiness, researchers introduce Sparse Activation Control, a training-free method that concurrently enhances safety, factuality, and bias mitigation by selectively controlling atte&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/askckanxno/cover.png"/></item><item><title>Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus</title><link>https://deep-diver.github.io/neurips2024/posters/mljduaqpln/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mljduaqpln/</guid><description>Boosting AI reasoning! New research enhances LLMs&amp;rsquo; logical abilities via a principled synthetic logic corpus, achieving substantial improvements across logic, math, and coding benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mljduaqpln/cover.png"/></item><item><title>Entity Alignment with Noisy Annotations from Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/qfcq54ztx1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qfcq54ztx1/</guid><description>LLM4EA: A novel framework efficiently merges knowledge graphs using LLMs, overcoming noisy annotations and high costs via active learning and unsupervised label refinement, boosting accuracy and effic&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qfcq54ztx1/cover.png"/></item><item><title>ESPACE: Dimensionality Reduction of Activations for Model Compression</title><link>https://deep-diver.github.io/neurips2024/posters/hacaanqnmk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hacaanqnmk/</guid><description>ESPACE: A novel LLM compression technique achieving 50% model size reduction with minimal accuracy loss by cleverly projecting activations onto principal components.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hacaanqnmk/cover.png"/></item><item><title>Estimating the Hallucination Rate of Generative AI</title><link>https://deep-diver.github.io/neurips2024/posters/lzl8qjyxv5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lzl8qjyxv5/</guid><description>New method estimates hallucination rates in generative AI&amp;rsquo;s in-context learning, improving model reliability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lzl8qjyxv5/cover.png"/></item><item><title>Evaluation of Text-to-Video Generation Models: A Dynamics Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/tmx1aumkl6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tmx1aumkl6/</guid><description>DEVIL: a novel text-to-video evaluation protocol focusing on video dynamics, resulting in more realistic video generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tmx1aumkl6/cover.png"/></item><item><title>Explaining Datasets in Words: Statistical Models with Natural Language Parameters</title><link>https://deep-diver.github.io/neurips2024/posters/u5bkogwwzw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/u5bkogwwzw/</guid><description>This paper introduces a model-agnostic algorithm that uses natural language predicates to make statistical model parameters directly interpretable, significantly improving explainability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/u5bkogwwzw/cover.png"/></item><item><title>Exploiting Activation Sparsity with Dense to Dynamic-k Mixture-of-Experts Conversion</title><link>https://deep-diver.github.io/neurips2024/posters/38ufpdt3tr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/38ufpdt3tr/</guid><description>D2DMoE boosts Transformer efficiency by up to 60% via smart activation sparsity and dynamic expert selection, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/38ufpdt3tr/cover.png"/></item><item><title>Exploiting LLM Quantization</title><link>https://deep-diver.github.io/neurips2024/posters/isa7mme7vg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/isa7mme7vg/</guid><description>LLM quantization, while improving efficiency, creates a security risk: attackers can craft seemingly benign models that exhibit malicious behavior only when quantized.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/isa7mme7vg/cover.png"/></item><item><title>Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/posters/7b2dribgzz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7b2dribgzz/</guid><description>LLM-Infused Diffuser boosts text-to-image generation by smartly integrating LLMs, surpassing existing models in prompt understanding and image quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7b2dribgzz/cover.png"/></item><item><title>Fast Best-of-N Decoding via Speculative Rejection</title><link>https://deep-diver.github.io/neurips2024/posters/348hfcprus/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/348hfcprus/</guid><description>Speculative Rejection: A novel algorithm boosts Large Language Model (LLM) alignment by speeding up inference-time alignment by 16-32x!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/348hfcprus/cover.png"/></item><item><title>Fast Sampling via Discrete Non-Markov Diffusion Models with Predetermined Transition Time</title><link>https://deep-diver.github.io/neurips2024/posters/kkyzmepjhn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kkyzmepjhn/</guid><description>Accelerated discrete diffusion model sampling is achieved via novel discrete non-Markov diffusion models (DNDM) with predetermined transition times, enabling a training-free algorithm that significant&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kkyzmepjhn/cover.png"/></item><item><title>FASTopic: Pretrained Transformer is a Fast, Adaptive, Stable, and Transferable Topic Model</title><link>https://deep-diver.github.io/neurips2024/posters/7t6aq0fa9d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7t6aq0fa9d/</guid><description>FASTopic: a pretrained transformer-based topic model achieving superior speed, adaptivity, stability, and transferability compared to existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7t6aq0fa9d/cover.png"/></item><item><title>Federated Fine-tuning of Large Language Models under Heterogeneous Tasks and Client Resources</title><link>https://deep-diver.github.io/neurips2024/posters/gkozohbxuw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gkozohbxuw/</guid><description>FlexLoRA: Efficient Federated Fine-tuning of LLMs for Heterogeneous Tasks and Resources.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gkozohbxuw/cover.png"/></item><item><title>Fight Back Against Jailbreaking via Prompt Adversarial Tuning</title><link>https://deep-diver.github.io/neurips2024/posters/nrdst1qifj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nrdst1qifj/</guid><description>Prompt Adversarial Tuning (PAT) defends against LLM jailbreaking by training a protective prompt prefix. PAT uses adversarial and benign prompts to optimize this prefix, significantly reducing succes&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nrdst1qifj/cover.png"/></item><item><title>Fine-grained Analysis of In-context Linear Estimation: Data, Architecture, and Beyond</title><link>https://deep-diver.github.io/neurips2024/posters/lypaymfqqm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lypaymfqqm/</guid><description>Researchers crack the code of in-context learning in Transformers, revealing how architecture, low-rank parameters, and data correlations influence model optimization and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lypaymfqqm/cover.png"/></item><item><title>FLAME : Factuality-Aware Alignment for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/zwuhsialbh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zwuhsialbh/</guid><description>FLAME: A novel alignment method enhances large language model factuality by addressing hallucination in supervised fine-tuning and reinforcement learning, resulting in more accurate and helpful AI ass&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zwuhsialbh/cover.png"/></item><item><title>FLoRA: Federated Fine-Tuning Large Language Models with Heterogeneous Low-Rank Adaptations</title><link>https://deep-diver.github.io/neurips2024/posters/tccorxxnjq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tccorxxnjq/</guid><description>FLORA enables efficient &amp;amp; private federated fine-tuning of LLMs via novel stacking-based heterogeneous low-rank adaptation, surpassing existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tccorxxnjq/cover.png"/></item><item><title>FlowLLM: Flow Matching for Material Generation with Large Language Models as Base Distributions</title><link>https://deep-diver.github.io/neurips2024/posters/0bfxbemz8e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0bfxbemz8e/</guid><description>FlowLLM revolutionizes material design by cleverly merging large language models and Riemannian flow matching, yielding a 300% boost in stable material generation!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0bfxbemz8e/cover.png"/></item><item><title>FM-Delta: Lossless Compression for Storing Massive Fine-tuned Foundation Models</title><link>https://deep-diver.github.io/neurips2024/posters/emstukr5j4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/emstukr5j4/</guid><description>FM-Delta: Lossless compression halves cloud storage for massive fine-tuned language models, saving costs without sacrificing accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/emstukr5j4/cover.png"/></item><item><title>fMRI predictors based on language models of increasing complexity recover brain left lateralization</title><link>https://deep-diver.github.io/neurips2024/posters/xf1jpo5k6l/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xf1jpo5k6l/</guid><description>Larger language models better predict brain activity in fMRI studies, with left-hemisphere prediction significantly increasing as model complexity scales up, reconciling classic aphasia findings with &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xf1jpo5k6l/cover.png"/></item><item><title>Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding</title><link>https://deep-diver.github.io/neurips2024/posters/fpmscvb1td/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fpmscvb1td/</guid><description>Ms-PoE, a simple plug-and-play positional encoding, significantly improves LLMs&amp;rsquo; ability to utilize long contexts by mitigating the &amp;rsquo;lost-in-the-middle&amp;rsquo; problem and enhancing the capacity to capture i&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fpmscvb1td/cover.png"/></item><item><title>Fractal Patterns May Illuminate the Success of Next-Token Prediction</title><link>https://deep-diver.github.io/neurips2024/posters/clafyreaye/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/clafyreaye/</guid><description>LLMs&amp;rsquo; success is explained by the self-similar, long-range dependent fractal structure of language; small-scale patterns reflect larger ones.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/clafyreaye/cover.png"/></item><item><title>From Instance Training to Instruction Learning: Task Adapters Generation from Instructions</title><link>https://deep-diver.github.io/neurips2024/posters/cluvzbfrjj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cluvzbfrjj/</guid><description>TAGI, a novel method, generates task-specific adapters from instructions, enhancing LLM cross-task generalization by using knowledge distillation and a two-stage hypernetwork training process.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cluvzbfrjj/cover.png"/></item><item><title>From Unstructured Data to In-Context Learning: Exploring What Tasks Can Be Learned and When</title><link>https://deep-diver.github.io/neurips2024/posters/x9efgahvbi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x9efgahvbi/</guid><description>LLMs&amp;rsquo; in-context learning surprisingly arises from simple co-occurrence patterns in unstructured data, but positional information is key for complex tasks; ICL fails when patterns are unseen or fixed.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x9efgahvbi/cover.png"/></item><item><title>Fundamental Limits of Prompt Compression: A Rate-Distortion Framework for Black-Box Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/tebkvfhp2m/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tebkvfhp2m/</guid><description>This paper introduces a rate-distortion framework for prompt compression in LLMs, bridging the gap between existing methods and optimal performance. By formulating prompt compression as a linear progr&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tebkvfhp2m/cover.png"/></item><item><title>G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering</title><link>https://deep-diver.github.io/neurips2024/posters/mpj3oxttzl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mpj3oxttzl/</guid><description>G-Retriever: a novel RAG approach enables conversational interaction with textual graphs, improving graph understanding and question answering efficiency while mitigating hallucination.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mpj3oxttzl/cover.png"/></item><item><title>Gated Slot Attention for Efficient Linear-Time Sequence Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/jy4phqibmg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jy4phqibmg/</guid><description>Gated Slot Attention (GSA) enhances linear Transformers for efficient, real-time sequence modeling. GSA uses a two-layer gated linear attention structure linked by softmax, enabling improved memory ca&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jy4phqibmg/cover.png"/></item><item><title>General Detection-based Text Line Recognition</title><link>https://deep-diver.github.io/neurips2024/posters/kxerljsz84/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kxerljsz84/</guid><description>A novel detection-based approach (DTLR) achieves state-of-the-art text line recognition across diverse scripts (Latin, Chinese, ciphers), overcoming challenges of character-level annotation and comple&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kxerljsz84/cover.png"/></item><item><title>Generative Hierarchical Materials Search</title><link>https://deep-diver.github.io/neurips2024/posters/pspr4noirc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pspr4noirc/</guid><description>Generative Hierarchical Materials Search (GenMS) uses AI to design novel crystal structures from natural language descriptions, outperforming prior methods in both fulfilling user requests and finding&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pspr4noirc/cover.png"/></item><item><title>Geometric-Averaged Preference Optimization for Soft Preference Labels</title><link>https://deep-diver.github.io/neurips2024/posters/3hpcvzv9it/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3hpcvzv9it/</guid><description>Improving LLM alignment, this paper introduces soft preference labels &amp;amp; geometric averaging in Direct Preference Optimization, consistently improving performance on standard benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3hpcvzv9it/cover.png"/></item><item><title>Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment</title><link>https://deep-diver.github.io/neurips2024/posters/orxqccn8fm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/orxqccn8fm/</guid><description>Reward learning from human demonstrations enhances supervised fine-tuning (SFT) for better LLM alignment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/orxqccn8fm/cover.png"/></item><item><title>Global Lyapunov functions: a long-standing open problem in mathematics, with symbolic transformers</title><link>https://deep-diver.github.io/neurips2024/posters/komrm4zj3m/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/komrm4zj3m/</guid><description>AI-powered sequence-to-sequence transformers surpass human and algorithmic abilities in discovering Lyapunov functions for dynamical systems, solving a long-standing open problem in mathematics.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/komrm4zj3m/cover.png"/></item><item><title>Gorilla: Large Language Model Connected with Massive APIs</title><link>https://deep-diver.github.io/neurips2024/posters/tbrnc6yemy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tbrnc6yemy/</guid><description>Gorilla: a fine-tuned LLaMA model surpasses GPT-4 in generating accurate API calls by using Retriever Aware Training (RAT) to adapt to changing APIs and reduce hallucinations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tbrnc6yemy/cover.png"/></item><item><title>Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes</title><link>https://deep-diver.github.io/neurips2024/posters/vi1wqfn15v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vi1wqfn15v/</guid><description>Gradient Cuff: A novel defense mechanism against LLM jailbreaks, leveraging refusal loss landscapes for improved malicious query rejection without harming model performance on benign inputs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vi1wqfn15v/cover.png"/></item><item><title>Grammar-Aligned Decoding</title><link>https://deep-diver.github.io/neurips2024/posters/5g7ve8e1lu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/5g7ve8e1lu/</guid><description>Adaptive Sampling with Approximate Expected Futures (ASAp) ensures LLMs generate grammatically correct outputs that closely match the model&amp;rsquo;s original probability distribution.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/5g7ve8e1lu/cover.png"/></item><item><title>Graph Convolutions Enrich the Self-Attention in Transformers!</title><link>https://deep-diver.github.io/neurips2024/posters/ffnrpcbpi6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ffnrpcbpi6/</guid><description>Graph Filter-based Self-Attention (GFSA) enhances Transformers by addressing oversmoothing, boosting performance across various tasks with minimal added parameters.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ffnrpcbpi6/cover.png"/></item><item><title>GraphVis: Boosting LLMs with Visual Knowledge Graph Integration</title><link>https://deep-diver.github.io/neurips2024/posters/havpmn8ugi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/havpmn8ugi/</guid><description>GraphVis boosts LLMs by visualizing knowledge graphs, improving accuracy in textual and visual question answering.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/havpmn8ugi/cover.png"/></item><item><title>Grokking of Implicit Reasoning in Transformers: A Mechanistic Journey to the Edge of Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/d4qgswxiob/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/d4qgswxiob/</guid><description>Transformers can learn implicit reasoning through &amp;lsquo;grokking&amp;rsquo;, achieving high accuracy in composition and comparison tasks; however, generalization varies across reasoning types.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/d4qgswxiob/cover.png"/></item><item><title>Group Robust Preference Optimization in Reward-free RLHF</title><link>https://deep-diver.github.io/neurips2024/posters/prasjrmxxk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/prasjrmxxk/</guid><description>Group Robust Preference Optimization (GRPO) enhances reward-free RLHF by aligning LLMs to diverse group preferences, maximizing worst-case performance, and significantly improving fairness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/prasjrmxxk/cover.png"/></item><item><title>GTBench: Uncovering the Strategic Reasoning Capabilities of LLMs via Game-Theoretic Evaluations</title><link>https://deep-diver.github.io/neurips2024/posters/ypggxvwiv2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ypggxvwiv2/</guid><description>GTBENCH reveals LLMs&amp;rsquo; strategic reasoning weaknesses via game-theoretic evaluations, showing strengths in probabilistic scenarios but struggles with deterministic ones; code-pretraining helps.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ypggxvwiv2/cover.png"/></item><item><title>HAWK: Learning to Understand Open-World Video Anomalies</title><link>https://deep-diver.github.io/neurips2024/posters/vbkoez1pg3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vbkoez1pg3/</guid><description>HAWK: a novel framework leveraging interactive VLMs and motion modality achieves state-of-the-art performance in open-world video anomaly understanding, generating descriptions and answering questions&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vbkoez1pg3/cover.png"/></item><item><title>HENASY: Learning to Assemble Scene-Entities for Interpretable Egocentric Video-Language Model</title><link>https://deep-diver.github.io/neurips2024/posters/7uwzogn4kv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7uwzogn4kv/</guid><description>HENASY, a novel egocentric video-language model, uses a compositional approach to assemble scene entities for improved interpretability and performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7uwzogn4kv/cover.png"/></item><item><title>HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/hkujvapvsg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hkujvapvsg/</guid><description>HippoRAG, a neurobiologically inspired framework, dramatically improves LLM long-term memory and multi-hop question answering by synergistically orchestrating LLMs, knowledge graphs, and the Personali&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hkujvapvsg/cover.png"/></item><item><title>HLM-Cite: Hybrid Language Model Workflow for Text-based Scientific Citation Prediction</title><link>https://deep-diver.github.io/neurips2024/posters/ov8yuk151r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ov8yuk151r/</guid><description>HLM-Cite: A hybrid language model workflow boosts scientific citation prediction accuracy by 17.6% and scales to 100K candidate papers, surpassing existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ov8yuk151r/cover.png"/></item><item><title>HonestLLM: Toward an Honest and Helpful Large Language Model</title><link>https://deep-diver.github.io/neurips2024/posters/f7tgq7b10q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/f7tgq7b10q/</guid><description>HonestLLM boosts LLM honesty &amp;amp; helpfulness by 65.3% (Llama3-8b) and 124.7% (Mistral-7b) using training-free and fine-tuning methods, establishing principles and a new dataset (HONESET) for honesty eva&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/f7tgq7b10q/cover.png"/></item><item><title>How Do Large Language Models Acquire Factual Knowledge During Pretraining?</title><link>https://deep-diver.github.io/neurips2024/posters/tydzj1evbp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tydzj1evbp/</guid><description>LLMs&amp;rsquo; factual knowledge acquisition during pretraining is surprisingly non-linear: more data doesn&amp;rsquo;t guarantee better knowledge retention, and forgetting follows a power law.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tydzj1evbp/cover.png"/></item><item><title>How do Large Language Models Handle Multilingualism?</title><link>https://deep-diver.github.io/neurips2024/posters/ctxyooagry/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ctxyooagry/</guid><description>LLMs surprisingly process multilingual queries via an English-centric intermediate stage before generating responses in the original language, a phenomenon explained by the proposed MWork framework an&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ctxyooagry/cover.png"/></item><item><title>How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider and MoE Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/67trrjgzsh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/67trrjgzsh/</guid><description>Pre-trained language models&amp;rsquo; base capabilities are significantly influenced by architecture, not just scale; a novel Combination Enhanced Architecture (CEA) improves performance by addressing FFN-Wide&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/67trrjgzsh/cover.png"/></item><item><title>How Far Can Transformers Reason? The Globality Barrier and Inductive Scratchpad</title><link>https://deep-diver.github.io/neurips2024/posters/fogwifxzun/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fogwifxzun/</guid><description>Transformers struggle with complex reasoning tasks. This paper introduces &amp;lsquo;globality degree&amp;rsquo; to measure task difficulty and shows that high globality hinders efficient learning. However, using &amp;lsquo;induc&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fogwifxzun/cover.png"/></item><item><title>HuRef: HUman-REadable Fingerprint for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/rlzgnezsoh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rlzgnezsoh/</guid><description>HuRef: Generate unique, human-readable fingerprints for LLMs to protect copyright without exposing model parameters or impeding training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rlzgnezsoh/cover.png"/></item><item><title>Hydra: Bidirectional State Space Models Through Generalized Matrix Mixers</title><link>https://deep-diver.github.io/neurips2024/posters/preo49p1vy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/preo49p1vy/</guid><description>Hydra: Bidirectional sequence modeling redefined with quasiseparable matrix mixers, outperforming existing models on various benchmarks!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/preo49p1vy/cover.png"/></item><item><title>HYDRA: Model Factorization Framework for Black-Box LLM Personalization</title><link>https://deep-diver.github.io/neurips2024/posters/ckgngkmhyp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ckgngkmhyp/</guid><description>HYDRA, a novel model factorization framework, significantly improves black-box LLM personalization by capturing both user-specific behavior and shared knowledge, achieving a 9.01% average relative imp&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ckgngkmhyp/cover.png"/></item><item><title>HYSYNTH: Context-Free LLM Approximation for Guiding Program Synthesis</title><link>https://deep-diver.github.io/neurips2024/posters/5jt0zsa6co/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/5jt0zsa6co/</guid><description>HYSYNTH: A hybrid approach uses LLMs to create context-free surrogate models that guide efficient program synthesis, outperforming LLMs alone and existing synthesizers across multiple domains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/5jt0zsa6co/cover.png"/></item><item><title>I Don't Know: Explicit Modeling of Uncertainty with an [IDK] Token</title><link>https://deep-diver.github.io/neurips2024/posters/wc0vlquolb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wc0vlquolb/</guid><description>Boosting LLM accuracy, a new calibration method using a special [IDK] token explicitly models uncertainty, mitigating hallucinations, and improving factual precision while maintaining knowledge retent&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wc0vlquolb/cover.png"/></item><item><title>I2EBench: A Comprehensive Benchmark for Instruction-based Image Editing</title><link>https://deep-diver.github.io/neurips2024/posters/1dpmeh6iha/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1dpmeh6iha/</guid><description>I2EBench: a new benchmark for Instruction-based Image Editing provides a comprehensive evaluation framework using 16 dimensions, aligned with human perception, to evaluate IIE models objectively.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1dpmeh6iha/cover.png"/></item><item><title>IDGen: Item Discrimination Induced Prompt Generation for LLM Evaluation</title><link>https://deep-diver.github.io/neurips2024/posters/zv4uiszzp5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zv4uiszzp5/</guid><description>IDGen synthesizes LLM evaluation prompts using Item Discrimination theory, creating a more challenging and discriminative dataset than previous methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zv4uiszzp5/cover.png"/></item><item><title>IF-Font: Ideographic Description Sequence-Following Font Generation</title><link>https://deep-diver.github.io/neurips2024/posters/ciwocmo8cc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ciwocmo8cc/</guid><description>IF-Font: Revolutionary font generation using Ideographic Description Sequences (IDS) to surpass state-of-the-art methods in style transfer, especially for unique styles.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ciwocmo8cc/cover.png"/></item><item><title>Image-aware Evaluation of Generated Medical Reports</title><link>https://deep-diver.github.io/neurips2024/posters/ecpig6o84z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ecpig6o84z/</guid><description>VLScore: a novel image-aware metric revolutionizes medical report evaluation by jointly assessing textual and visual similarities, significantly improving alignment with radiologist assessments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ecpig6o84z/cover.png"/></item><item><title>Imitating Language via Scalable Inverse Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/5d2escrirc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/5d2escrirc/</guid><description>This study presents a novel Inverse Reinforcement Learning (IRL) approach for fine-tuning large language models, offering improved performance and generation diversity compared to standard methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/5d2escrirc/cover.png"/></item><item><title>Implicit Optimization Bias of Next-token Prediction in Linear Models</title><link>https://deep-diver.github.io/neurips2024/posters/xszio6gqgg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xszio6gqgg/</guid><description>Researchers reveal implicit optimization biases in next-token prediction for language models, showing how gradient descent selects solutions based on data sparsity and a novel margin concept, impactin&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xszio6gqgg/cover.png"/></item><item><title>Implicit Regularization of Sharpness-Aware Minimization for Scale-Invariant Problems</title><link>https://deep-diver.github.io/neurips2024/posters/osovme9kl2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/osovme9kl2/</guid><description>Boosting deep learning generalization, this work unveils SAM&amp;rsquo;s implicit regularization using &amp;lsquo;balancedness&amp;rsquo;, a new metric. A resource-efficient variant, BAR, achieves 95% computational savings with i&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/osovme9kl2/cover.png"/></item><item><title>Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses</title><link>https://deep-diver.github.io/neurips2024/posters/zmnd0jucef/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zmnd0jucef/</guid><description>Improved few-shot jailbreaking techniques efficiently circumvent aligned language models and their defenses, achieving high success rates even against advanced protection methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zmnd0jucef/cover.png"/></item><item><title>Improved Generation of Adversarial Examples Against Safety-aligned LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/8hbc843g1p/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8hbc843g1p/</guid><description>Researchers developed novel methods to improve the generation of adversarial examples against safety-aligned LLMs, achieving significantly higher attack success rates compared to existing techniques.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8hbc843g1p/cover.png"/></item><item><title>Improving Context-Aware Preference Modeling for Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/52r4xjyzjg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/52r4xjyzjg/</guid><description>Context-aware preference modeling improves language model alignment by resolving ambiguity through a two-step process: context selection followed by context-specific preference evaluation. The approa&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/52r4xjyzjg/cover.png"/></item><item><title>Improving Gloss-free Sign Language Translation by Reducing Representation Density</title><link>https://deep-diver.github.io/neurips2024/posters/ftzlbgohw2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ftzlbgohw2/</guid><description>SignCL, a novel contrastive learning strategy, significantly boosts gloss-free sign language translation by mitigating representation density, achieving substantial performance gains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ftzlbgohw2/cover.png"/></item><item><title>Improving Sparse Decomposition of Language Model Activations with Gated Sparse Autoencoders</title><link>https://deep-diver.github.io/neurips2024/posters/zlblin2zvw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zlblin2zvw/</guid><description>Gated Sparse Autoencoders (GSAEs) achieve Pareto improvement over baseline SAEs for unsupervised feature discovery in language models, resolving the shrinkage bias of L1 penalty by separating feature &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zlblin2zvw/cover.png"/></item><item><title>In-Context Learning of a Linear Transformer Block: Benefits of the MLP Component and One-Step GD Initialization</title><link>https://deep-diver.github.io/neurips2024/posters/thou1rkdpz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/thou1rkdpz/</guid><description>Linear Transformer Blocks (LTBs) achieve near-optimal in-context learning (ICL) for linear regression by effectively implementing one-step gradient descent with learnable initialization, a significant&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/thou1rkdpz/cover.png"/></item><item><title>In-Context Learning State Vector with Inner and Momentum Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/gnnmb7y0xx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gnnmb7y0xx/</guid><description>This paper introduces inner and momentum optimization to enhance the state vector for in-context learning, improving performance and scalability in LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gnnmb7y0xx/cover.png"/></item><item><title>In-Context Learning with Representations: Contextual Generalization of Trained Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/ik37kkxkbm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ik37kkxkbm/</guid><description>Transformers learn contextual information for generalization to unseen examples and tasks, even with limited training data, converging linearly to a global minimum.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ik37kkxkbm/cover.png"/></item><item><title>Incentivizing Quality Text Generation via Statistical Contracts</title><link>https://deep-diver.github.io/neurips2024/posters/wzgw4crxwk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wzgw4crxwk/</guid><description>Cost-robust contracts, inspired by statistical hypothesis tests, incentivize quality in LLM text generation, overcoming the moral hazard of pay-per-token models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wzgw4crxwk/cover.png"/></item><item><title>INDICT: Code Generation with Internal Dialogues of Critiques for Both Security and Helpfulness</title><link>https://deep-diver.github.io/neurips2024/posters/jcmyiuwprx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jcmyiuwprx/</guid><description>INDICT, a novel framework, empowers LLMs with internal dialogues of critiques to enhance code generation, prioritizing both safety and helpfulness, resulting in +10% absolute improvement across variou&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jcmyiuwprx/cover.png"/></item><item><title>Inevitable Trade-off between Watermark Strength and Speculative Sampling Efficiency for Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/6ykmbuiisg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/6ykmbuiisg/</guid><description>Injecting watermarks into LLM outputs while speeding up generation is impossible; this paper proves this trade-off and offers methods prioritizing either watermark strength or speed.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/6ykmbuiisg/cover.png"/></item><item><title>InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory</title><link>https://deep-diver.github.io/neurips2024/posters/bthfrqhasy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bthfrqhasy/</guid><description>InfLLM: Training-free long-context extrapolation for LLMs via efficient context memory.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bthfrqhasy/cover.png"/></item><item><title>InfoRM: Mitigating Reward Hacking in RLHF via Information-Theoretic Reward Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/3xnbvk9sd6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3xnbvk9sd6/</guid><description>InfoRM tackles reward hacking in RLHF using an information-theoretic approach, enhancing generalizability and enabling overoptimization detection.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3xnbvk9sd6/cover.png"/></item><item><title>Information Re-Organization Improves Reasoning in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/sciwuypng0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sciwuypng0/</guid><description>InfoRE: A novel method improving large language models&amp;rsquo; reasoning by reorganizing information to highlight logical relationships, resulting in a 4% average accuracy boost across various tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sciwuypng0/cover.png"/></item><item><title>Initialization is Critical to Whether Transformers Fit Composite Functions by Reasoning or Memorizing</title><link>https://deep-diver.github.io/neurips2024/posters/yobgdvayts/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yobgdvayts/</guid><description>Transformer model initialization dramatically affects whether it reasons or memorizes, impacting performance on compositional tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yobgdvayts/cover.png"/></item><item><title>Instance-adaptive Zero-shot Chain-of-Thought Prompting</title><link>https://deep-diver.github.io/neurips2024/posters/31xwlidxtm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/31xwlidxtm/</guid><description>Instance-adaptive prompting significantly improves zero-shot Chain-of-Thought reasoning in LLMs by dynamically selecting prompts tailored to each instance, leading to consistent performance gains acro&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/31xwlidxtm/cover.png"/></item><item><title>Instruction Tuning With Loss Over Instructions</title><link>https://deep-diver.github.io/neurips2024/posters/gczgo9ffgt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gczgo9ffgt/</guid><description>Boost LLM performance with INSTRUCTION MODELLING: a simple yet effective instruction tuning method that improves model outputs by over 100% in some cases by applying loss to both instructions and outp&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gczgo9ffgt/cover.png"/></item><item><title>Interpreting Learned Feedback Patterns in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/xuongr1byy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xuongr1byy/</guid><description>Researchers developed methods to measure and interpret the divergence between learned feedback patterns (LFPs) in LLMs and human preferences, helping minimize discrepancies between LLM behavior and tr&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xuongr1byy/cover.png"/></item><item><title>Invariant Tokenization of Crystalline Materials for Language Model Enabled Generation</title><link>https://deep-diver.github.io/neurips2024/posters/18fgrnd0wz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/18fgrnd0wz/</guid><description>Mat2Seq revolutionizes crystal structure generation using language models by creating unique, invariant 1D sequences from 3D crystal structures, enabling accurate and efficient crystal discovery with &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/18fgrnd0wz/cover.png"/></item><item><title>InversionView: A General-Purpose Method for Reading Information from Neural Activations</title><link>https://deep-diver.github.io/neurips2024/posters/cldghpx2la/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cldghpx2la/</guid><description>InversionView unveils neural network inner workings by decoding information from activations. It identifies inputs producing similar activations, revealing the information content. Case studies on v&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cldghpx2la/cover.png"/></item><item><title>IQA-EVAL: Automatic Evaluation of Human-Model Interactive Question Answering</title><link>https://deep-diver.github.io/neurips2024/posters/mzm99vv5rx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mzm99vv5rx/</guid><description>IQA-EVAL: An automatic evaluation framework uses LLMs to simulate human-AI interactions and evaluate interactive question answering, achieving high correlation with human judgments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mzm99vv5rx/cover.png"/></item><item><title>IRCAN: Mitigating Knowledge Conflicts in LLM Generation via Identifying and Reweighting Context-Aware Neurons</title><link>https://deep-diver.github.io/neurips2024/posters/zfxraqbbkx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zfxraqbbkx/</guid><description>IRCAN tackles LLM knowledge conflicts by identifying and reweighting context-aware neurons, significantly improving context-sensitive outputs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zfxraqbbkx/cover.png"/></item><item><title>Is Programming by Example solved by LLMs?</title><link>https://deep-diver.github.io/neurips2024/posters/xqc8yyhscl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xqc8yyhscl/</guid><description>Large Language Models (LLMs) surprisingly improve the challenging task of Programming by Example (PBE) when fine-tuned on problem-specific data, outperforming classic symbolic methods and even surpass&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xqc8yyhscl/cover.png"/></item><item><title>Is the MMI Criterion Necessary for Interpretability? Degenerating Non-causal Features to Plain Noise for Self-Rationalization</title><link>https://deep-diver.github.io/neurips2024/posters/eaqcvzx30k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/eaqcvzx30k/</guid><description>New criterion maximizes remaining discrepancy after rationale removal, treating spurious features as noise, improving rationale extraction.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/eaqcvzx30k/cover.png"/></item><item><title>Iteration Head: A Mechanistic Study of Chain-of-Thought</title><link>https://deep-diver.github.io/neurips2024/posters/qbcxwpot5w/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qbcxwpot5w/</guid><description>Researchers reveal how Chain-of-Thought reasoning emerges in transformers via specialized &amp;lsquo;iteration heads&amp;rsquo;, improving LLM performance and offering insights into mechanistic interpretability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qbcxwpot5w/cover.png"/></item><item><title>Iterative Reasoning Preference Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/4xikfvnyvx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4xikfvnyvx/</guid><description>Iterative Reasoning Preference Optimization boosts large language model reasoning by iteratively refining preferences between generated reasoning steps, achieving significant accuracy gains on benchma&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4xikfvnyvx/cover.png"/></item><item><title>Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters</title><link>https://deep-diver.github.io/neurips2024/posters/acblttkk5q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/acblttkk5q/</guid><description>New benchmark and jailbreak method exposes vulnerabilities of LLM moderation, achieving significantly higher success rates than existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/acblttkk5q/cover.png"/></item><item><title>JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models</title><link>https://deep-diver.github.io/neurips2024/posters/ujdkxwtbjx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ujdkxwtbjx/</guid><description>JiuZhang3.0 efficiently enhances LLMs&amp;rsquo; mathematical reasoning by training a small model to synthesize high-quality training data, drastically reducing costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ujdkxwtbjx/cover.png"/></item><item><title>Kangaroo: Lossless Self-Speculative Decoding for Accelerating LLMs via Double Early Exiting</title><link>https://deep-diver.github.io/neurips2024/posters/lt3oc04mdp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lt3oc04mdp/</guid><description>Kangaroo: Double early exiting boosts LLM speed!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lt3oc04mdp/cover.png"/></item><item><title>KG-FIT: Knowledge Graph Fine-Tuning Upon Open-World Knowledge</title><link>https://deep-diver.github.io/neurips2024/posters/rdopmodpki/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rdopmodpki/</guid><description>KG-FIT boosts knowledge graph embedding by smartly integrating open-world knowledge from LLMs, achieving significant performance gains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rdopmodpki/cover.png"/></item><item><title>KnowGPT: Knowledge Graph based Prompting for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/pacbluo5m7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pacbluo5m7/</guid><description>KnowGPT: A novel framework boosts Large Language Model accuracy by intelligently integrating knowledge graphs, significantly reducing factual errors and achieving near-human performance on benchmark d&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pacbluo5m7/cover.png"/></item><item><title>Knowledge Circuits in Pretrained Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/yvxzznxcag/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yvxzznxcag/</guid><description>Researchers unveil &amp;lsquo;knowledge circuits&amp;rsquo; within LLMs, revealing how knowledge is collaboratively encoded and utilized, leading to improved LLM design and interpretations of model behavior.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yvxzznxcag/cover.png"/></item><item><title>KptLLM: Unveiling the Power of Large Language Model for Keypoint Comprehension</title><link>https://deep-diver.github.io/neurips2024/posters/gwd3mqufgp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gwd3mqufgp/</guid><description>KptLLM: A novel multimodal model leverages LLMs for superior keypoint comprehension, outperforming existing methods in various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gwd3mqufgp/cover.png"/></item><item><title>Kraken: Inherently Parallel Transformers For Efficient Multi-Device Inference</title><link>https://deep-diver.github.io/neurips2024/posters/jrtxzzk0a6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jrtxzzk0a6/</guid><description>Kraken: A new Transformer architecture boosts multi-device inference speed by 35.6% by cleverly overlapping communication with computation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jrtxzzk0a6/cover.png"/></item><item><title>KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization</title><link>https://deep-diver.github.io/neurips2024/posters/pnnvzqss4p/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pnnvzqss4p/</guid><description>Boost LLM inference speed 1.4-3.5x by using Coupled Quantization (CQ) to compress KV cache down to 1 bit per channel, while preserving model accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pnnvzqss4p/cover.png"/></item><item><title>KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization</title><link>https://deep-diver.github.io/neurips2024/posters/0lxotew9du/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0lxotew9du/</guid><description>KVQuant achieves &amp;lt;0.1 perplexity degradation with 3-bit quantization in LLMs by using per-channel key quantization, pre-RoPE quantization, and non-uniform quantization, enabling 10M context length inf&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0lxotew9du/cover.png"/></item><item><title>LACIE: Listener-Aware Finetuning for Calibration in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/rnvgyd9rah/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rnvgyd9rah/</guid><description>LACIE: Listener-aware finetuning improves LLM confidence calibration, reducing incorrect answers accepted by human listeners by 47% while maintaining correct answer acceptance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rnvgyd9rah/cover.png"/></item><item><title>Lambda: Learning Matchable Prior For Entity Alignment with Unlabeled Dangling Cases</title><link>https://deep-diver.github.io/neurips2024/posters/awfryojagi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/awfryojagi/</guid><description>Lambda: A novel framework tackles entity alignment challenges with unlabeled dangling entities using GNN-based encoding, spectral contrastive learning, and an iterative PU learning algorithm, achievin&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/awfryojagi/cover.png"/></item><item><title>Language Grounded Multi-agent Reinforcement Learning with Human-interpretable Communication</title><link>https://deep-diver.github.io/neurips2024/posters/duhx779c5q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/duhx779c5q/</guid><description>LangGround: MARL agents learn human-interpretable communication via LLM-grounded training, enabling effective human-agent collaboration.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/duhx779c5q/cover.png"/></item><item><title>Language Models as Hierarchy Encoders</title><link>https://deep-diver.github.io/neurips2024/posters/gjmyvwzje1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gjmyvwzje1/</guid><description>Language models struggle with hierarchical information. This work introduces Hierarchy Transformer Encoders (HITs), a novel method to retrain transformer encoders using hyperbolic geometry and special&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gjmyvwzje1/cover.png"/></item><item><title>Language Models as Zero-shot Lossless Gradient Compressors: Towards General Neural Parameter Prior Models</title><link>https://deep-diver.github.io/neurips2024/posters/77kcjzvpoa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/77kcjzvpoa/</guid><description>Large language models (LLMs) achieve lossless gradient compression, surpassing existing methods by up to 17.2%, thereby advancing distributed learning efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/77kcjzvpoa/cover.png"/></item><item><title>Large Language Model Unlearning</title><link>https://deep-diver.github.io/neurips2024/posters/8dy42thone/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8dy42thone/</guid><description>This paper presents a novel method for large language model (LLM) unlearning, enabling LLMs to &amp;lsquo;forget&amp;rsquo; undesirable behaviors by using only negative examples. This computationally efficient approach o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8dy42thone/cover.png"/></item><item><title>Large Language Model Unlearning via Embedding-Corrupted Prompts</title><link>https://deep-diver.github.io/neurips2024/posters/e5icsxbd8q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/e5icsxbd8q/</guid><description>ECO prompts enable efficient LLM unlearning by corrupting prompts flagged for forgetting, achieving promising results across various LLMs and tasks with minimal side effects.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/e5icsxbd8q/cover.png"/></item><item><title>Large language model validity via enhanced conformal prediction methods</title><link>https://deep-diver.github.io/neurips2024/posters/jd3nypeq3r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jd3nypeq3r/</guid><description>New conformal inference methods enhance LLM validity by providing adaptive validity guarantees and improving the quality of LLM outputs, addressing prior methods&amp;rsquo; limitations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jd3nypeq3r/cover.png"/></item><item><title>Large Language Models Must Be Taught to Know What They Donât Know</title><link>https://deep-diver.github.io/neurips2024/posters/qzvwyggryb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qzvwyggryb/</guid><description>Teach LLMs uncertainty for reliable high-stakes predictions: Fine-tuning with graded examples significantly improves LLM&amp;rsquo;s uncertainty calibration and generalizes well.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qzvwyggryb/cover.png"/></item><item><title>Large Language Models-guided Dynamic Adaptation for Temporal Knowledge Graph Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/cw7agrr8gj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cw7agrr8gj/</guid><description>LLM-DA dynamically adapts LLM-generated rules for accurate, interpretable temporal knowledge graph reasoning, significantly improving accuracy without fine-tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cw7agrr8gj/cover.png"/></item><item><title>Latent Paraphrasing: Perturbation on Layers Improves Knowledge Injection in Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/t1lfrywtf7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/t1lfrywtf7/</guid><description>LaPael improves LLM knowledge injection by applying learned noise to early layers, enabling diverse and efficient knowledge updates without repeated external model usage.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/t1lfrywtf7/cover.png"/></item><item><title>LCGen: Mining in Low-Certainty Generation for View-consistent Text-to-3D</title><link>https://deep-diver.github.io/neurips2024/posters/4wgzkayi2d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4wgzkayi2d/</guid><description>LCGen: A novel method for view-consistent text-to-3D generation, resolving the &amp;lsquo;Janus Problem&amp;rsquo; by strategically using low-certainty priors to align viewpoints and optimize the generation process.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4wgzkayi2d/cover.png"/></item><item><title>Learn more, but bother less: parameter efficient continual learning</title><link>https://deep-diver.github.io/neurips2024/posters/zxtanh5uyb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zxtanh5uyb/</guid><description>LB-CL: A novel parameter-efficient continual learning method for LLMs that boosts performance and reduces forgetting by leveraging parametric knowledge transfer and maintaining orthogonal low-rank sub&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zxtanh5uyb/cover.png"/></item><item><title>Learnability Matters: Active Learning for Video Captioning</title><link>https://deep-diver.github.io/neurips2024/posters/4gp7s7u0lj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4gp7s7u0lj/</guid><description>Active learning for video captioning is enhanced by a novel algorithm that prioritizes &amp;rsquo;learnability&amp;rsquo;, diversity, and uncertainty to address annotation inconsistency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4gp7s7u0lj/cover.png"/></item><item><title>Learning and Transferring Sparse Contextual Bigrams with Linear Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/pukavawybo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pukavawybo/</guid><description>Linear transformers efficiently learn sparse contextual bigrams by leveraging both in-context and global information, achieving polynomial sample complexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pukavawybo/cover.png"/></item><item><title>Learning Complete Protein Representation by Dynamically Coupling of Sequence and Structure</title><link>https://deep-diver.github.io/neurips2024/posters/0e5uoajxo1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0e5uoajxo1/</guid><description>CoupleNet dynamically links protein sequences and structures for improved representations, surpassing state-of-the-art methods in function prediction, particularly for uncommon proteins.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0e5uoajxo1/cover.png"/></item><item><title>Learning Goal-Conditioned Representations for Language Reward Models</title><link>https://deep-diver.github.io/neurips2024/posters/swh8lxuyca/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/swh8lxuyca/</guid><description>Goal-conditioned contrastive learning boosts language reward model performance and enables better control of language model generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/swh8lxuyca/cover.png"/></item><item><title>Learning to Discuss Strategically: A Case Study on One Night Ultimate Werewolf</title><link>https://deep-diver.github.io/neurips2024/posters/1f82rnwcbl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1f82rnwcbl/</guid><description>RL-instructed language models excel at strategic communication in One Night Ultimate Werewolf, demonstrating the importance of discussion tactics in complex games.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1f82rnwcbl/cover.png"/></item><item><title>Learning to Reason via Program Generation, Emulation, and Search</title><link>https://deep-diver.github.io/neurips2024/posters/te6vagjf6g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/te6vagjf6g/</guid><description>Language models excel at generating programs for algorithmic tasks, but struggle with soft reasoning. COGEX leverages pseudo-programs and program emulation to tackle these tasks, while COTACS searches&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/te6vagjf6g/cover.png"/></item><item><title>LeDex: Training LLMs to Better Self-Debug and Explain Code</title><link>https://deep-diver.github.io/neurips2024/posters/d1xrz4einv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/d1xrz4einv/</guid><description>LEDEX: A novel training framework significantly boosts LLMs&amp;rsquo; code self-debugging by using automated data collection, supervised fine-tuning, and reinforcement learning, leading to more accurate code a&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/d1xrz4einv/cover.png"/></item><item><title>LESS: Label-Efficient and Single-Stage Referring 3D Segmentation</title><link>https://deep-diver.github.io/neurips2024/posters/hrqaot0nzf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hrqaot0nzf/</guid><description>LESS achieves state-of-the-art Referring 3D Segmentation using only binary masks, significantly reducing labeling effort and improving efficiency with a novel single-stage pipeline.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hrqaot0nzf/cover.png"/></item><item><title>Leveraging Environment Interaction for Automated PDDL Translation and Planning with Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/rzlcqnncqv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rzlcqnncqv/</guid><description>This paper presents a fully automated method for PDDL translation and planning using LLMs and environment interaction, achieving a 66% success rate on challenging PDDL domains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rzlcqnncqv/cover.png"/></item><item><title>Limits of Transformer Language Models on Learning to Compose Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/x7ad0343jz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x7ad0343jz/</guid><description>Large Language Models struggle with compositional tasks, requiring exponentially more data than expected for learning compared to learning sub-tasks individually. This paper reveals surprising sample &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x7ad0343jz/cover.png"/></item><item><title>Linguistic Collapse: Neural Collapse in (Large) Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/g0lfcmirkc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/g0lfcmirkc/</guid><description>Scaling causal language models reveals a connection between neural collapse properties, model size, and improved generalization, highlighting NC&amp;rsquo;s broader relevance to LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/g0lfcmirkc/cover.png"/></item><item><title>Linking In-context Learning in Transformers to Human Episodic Memory</title><link>https://deep-diver.github.io/neurips2024/posters/aydbfxnon4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/aydbfxnon4/</guid><description>Transformers&amp;rsquo; in-context learning mirrors human episodic memory, with specific attention heads acting like the brain&amp;rsquo;s contextual maintenance and retrieval system.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/aydbfxnon4/cover.png"/></item><item><title>LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning</title><link>https://deep-diver.github.io/neurips2024/posters/l8ifdx5xnq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/l8ifdx5xnq/</guid><description>LISA, a layerwise importance sampling method, dramatically improves memory-efficient large language model fine-tuning, outperforming existing methods while using less GPU memory.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/l8ifdx5xnq/cover.png"/></item><item><title>Lisa: Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning Attack</title><link>https://deep-diver.github.io/neurips2024/posters/rpchapuxlc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rpchapuxlc/</guid><description>Lisa: a novel lazy safety alignment method safeguards LLMs against harmful fine-tuning attacks by introducing a proximal term to constrain model drift, significantly improving alignment performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rpchapuxlc/cover.png"/></item><item><title>LIVE: Learnable In-Context Vector for Visual Question Answering</title><link>https://deep-diver.github.io/neurips2024/posters/qhremvrzbg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qhremvrzbg/</guid><description>LIVE, a novel learnable in-context vector, significantly improves visual question answering by reducing computational costs and enhancing accuracy compared to traditional ICL methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qhremvrzbg/cover.png"/></item><item><title>LLaMo: Large Language Model-based Molecular Graph Assistant</title><link>https://deep-diver.github.io/neurips2024/posters/wktndu155n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wktndu155n/</guid><description>LLaMo, a novel Large Language Model-based Molecular Graph Assistant, uses multi-level graph projection and instruction tuning to achieve superior performance on diverse molecular tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wktndu155n/cover.png"/></item><item><title>LLaNA: Large Language and NeRF Assistant</title><link>https://deep-diver.github.io/neurips2024/posters/exeiyx6u0z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/exeiyx6u0z/</guid><description>LLaNA: A novel Multimodal Large Language Model directly processes NeRF weights to enable NeRF captioning and Q&amp;amp;A, outperforming traditional 2D/3D-based methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/exeiyx6u0z/cover.png"/></item><item><title>LLM Circuit Analyses Are Consistent Across Training and Scale</title><link>https://deep-diver.github.io/neurips2024/posters/3ds5vnudie/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3ds5vnudie/</guid><description>LLM circuit analyses remain consistent across model scales and extensive training, enabling more efficient interpretability research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3ds5vnudie/cover.png"/></item><item><title>LLM Dataset Inference: Did you train on my dataset?</title><link>https://deep-diver.github.io/neurips2024/posters/fr9d1umc37/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fr9d1umc37/</guid><description>LLM dataset inference reliably detects if a dataset was used in training, overcoming limitations of existing membership inference attacks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fr9d1umc37/cover.png"/></item><item><title>LLM Processes: Numerical Predictive Distributions Conditioned on Natural Language</title><link>https://deep-diver.github.io/neurips2024/posters/hshs7q1njh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hshs7q1njh/</guid><description>LLM Processes leverage LLMs to create probabilistic regression models guided by natural language, enabling seamless integration of expert knowledge and improving prediction accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hshs7q1njh/cover.png"/></item><item><title>LLM-Check: Investigating Detection of Hallucinations in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/lyx4w3cagy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lyx4w3cagy/</guid><description>LLM-Check efficiently detects LLM hallucinations in a single response, using internal model analysis, improving real-time applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lyx4w3cagy/cover.png"/></item><item><title>LLMDFA: Analyzing Dataflow in Code with Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/qz2d8e8whu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qz2d8e8whu/</guid><description>LLMDFA: A novel LLM-powered framework performs compilation-free and customizable dataflow analysis, achieving high accuracy in bug detection by decomposing the task into sub-problems and mitigating L&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qz2d8e8whu/cover.png"/></item><item><title>LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings</title><link>https://deep-diver.github.io/neurips2024/posters/32g9bwtndc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/32g9bwtndc/</guid><description>TEA-GLM leverages LLMs for zero-shot graph learning by aligning GNN representations with LLM token embeddings, achieving state-of-the-art performance on unseen datasets and tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/32g9bwtndc/cover.png"/></item><item><title>Local to Global: Learning Dynamics and Effect of Initialization for Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/ox4yll3x53/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ox4yll3x53/</guid><description>Transformers&amp;rsquo; learning dynamics depend heavily on initialization and Markovian data properties, leading to either global or local minima; this paper proves this, offers initialization guidelines, and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ox4yll3x53/cover.png"/></item><item><title>LoFiT: Localized Fine-tuning on LLM Representations</title><link>https://deep-diver.github.io/neurips2024/posters/dfixfbecsz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dfixfbecsz/</guid><description>LOFIT: Localized fine-tuning boosts LLMs&amp;rsquo; performance by selectively training only a small subset of attention heads, achieving comparable accuracy to other methods while using significantly fewer par&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dfixfbecsz/cover.png"/></item><item><title>Loki: Low-rank Keys for Efficient Sparse Attention</title><link>https://deep-diver.github.io/neurips2024/posters/raabeiv71j/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/raabeiv71j/</guid><description>Loki: Low-rank Keys for Efficient Sparse Attention accelerates attention mechanisms in LLMs by exploiting the low-dimensionality of key vectors. It dynamically selects key tokens based on approximate&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/raabeiv71j/cover.png"/></item><item><title>Long-form factuality in large language models</title><link>https://deep-diver.github.io/neurips2024/posters/4m9f8vmt2c/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4m9f8vmt2c/</guid><description>LLMs often generate factually inaccurate long-form text. This work introduces LongFact, a new benchmark dataset of 2280 fact-seeking prompts, and SAFE, a novel automated evaluation method that outperf&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4m9f8vmt2c/cover.png"/></item><item><title>Look, Listen, and Answer: Overcoming Biases for Audio-Visual Question Answering</title><link>https://deep-diver.github.io/neurips2024/posters/twppd9umun/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/twppd9umun/</guid><description>New dataset MUSIC-AVQA-R and a multi-faceted cycle collaborative debiasing strategy significantly improve audio-visual question answering robustness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/twppd9umun/cover.png"/></item><item><title>LoQT: Low-Rank Adapters for Quantized Pretraining</title><link>https://deep-diver.github.io/neurips2024/posters/pnv8c0bu9t/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pnv8c0bu9t/</guid><description>LoQT enables efficient large language model training on consumer hardware via quantized weights and low-rank weight updates, overcoming memory limitations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pnv8c0bu9t/cover.png"/></item><item><title>LoRA-GA: Low-Rank Adaptation with Gradient Approximation</title><link>https://deep-diver.github.io/neurips2024/posters/valawrlhjv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/valawrlhjv/</guid><description>LoRA-GA: A novel initialization method dramatically speeds up low-rank adaptation (LoRA) for LLMs, achieving convergence rates comparable to full fine-tuning while improving performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/valawrlhjv/cover.png"/></item><item><title>LSH-MoE: Communication-efficient MoE Training via Locality-Sensitive Hashing</title><link>https://deep-diver.github.io/neurips2024/posters/bjfhvbky5a/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bjfhvbky5a/</guid><description>LSH-MoE accelerates Mixture-of-Experts training by 1.28x-2.2x via Locality-Sensitive Hashing, significantly reducing communication costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bjfhvbky5a/cover.png"/></item><item><title>LT-Defense: Searching-free Backdoor Defense via Exploiting the Long-tailed Effect</title><link>https://deep-diver.github.io/neurips2024/posters/jdcmwf06c6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jdcmwf06c6/</guid><description>LT-Defense: a searching-free backdoor defense for language models leveraging the long-tailed effect of poisoned data. It achieves 98% accuracy across 1440 models with less than 1% time cost of existin&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jdcmwf06c6/cover.png"/></item><item><title>MACM: Utilizing a Multi-Agent System for Condition Mining in Solving Complex Mathematical Problems</title><link>https://deep-diver.github.io/neurips2024/posters/vr2rdsxtzs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vr2rdsxtzs/</guid><description>Multi-Agent System for Condition Mining (MACM) dramatically boosts large language model accuracy in complex math problem-solving, exceeding existing methods by achieving higher accuracy and better gen&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vr2rdsxtzs/cover.png"/></item><item><title>MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution</title><link>https://deep-diver.github.io/neurips2024/posters/qevq3fz63j/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qevq3fz63j/</guid><description>MAGIS: A novel LLM-based multi-agent framework significantly boosts GitHub issue resolution by leveraging agent collaboration for planning and coding, achieving an eight-fold performance increase comp&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qevq3fz63j/cover.png"/></item><item><title>MAGNET: Improving the Multilingual Fairness of Language Models with Adaptive Gradient-Based Tokenization</title><link>https://deep-diver.github.io/neurips2024/posters/1e3mowhsix/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1e3mowhsix/</guid><description>MAGNET, a novel adaptive gradient-based tokenization method, tackles multilingual language model bias by employing language-specific boundary predictors to achieve equitable segmentation across divers&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1e3mowhsix/cover.png"/></item><item><title>MagR: Weight Magnitude Reduction for Enhancing Post-Training Quantization</title><link>https://deep-diver.github.io/neurips2024/posters/uartfgktqw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uartfgktqw/</guid><description>MagR: a novel preprocessing technique boosts post-training quantization of LLMs by reducing weight magnitudes without inference overhead, achieving state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uartfgktqw/cover.png"/></item><item><title>Make Your LLM Fully Utilize the Context</title><link>https://deep-diver.github.io/neurips2024/posters/ygtvembxtv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ygtvembxtv/</guid><description>FILM-7B, trained with Information-Intensive (IN2) training, significantly overcomes the &amp;rsquo;lost-in-the-middle&amp;rsquo; problem in long-context LLMs, enabling robust information retrieval from all context positi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ygtvembxtv/cover.png"/></item><item><title>MAmmoTH2: Scaling Instructions from the Web</title><link>https://deep-diver.github.io/neurips2024/posters/yvu5dnplqa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yvu5dnplqa/</guid><description>MAmmoTH2: Harvesting 10M web instructions for enhanced LLM reasoning!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yvu5dnplqa/cover.png"/></item><item><title>Many-shot Jailbreaking</title><link>https://deep-diver.github.io/neurips2024/posters/cw5mgd71jw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cw5mgd71jw/</guid><description>Long-context attacks easily manipulate LLMs by feeding hundreds of harmful examples, highlighting a critical vulnerability amplified by larger context windows.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cw5mgd71jw/cover.png"/></item><item><title>Masked Hard-Attention Transformers Recognize Exactly the Star-Free Languages</title><link>https://deep-diver.github.io/neurips2024/posters/fbmsbdh0yz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fbmsbdh0yz/</guid><description>Masked hard-attention transformers, with strict masking, precisely capture star-free languages, matching the expressive power of linear temporal logic.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fbmsbdh0yz/cover.png"/></item><item><title>MatFormer: Nested Transformer for Elastic Inference</title><link>https://deep-diver.github.io/neurips2024/posters/fya6ezmxd5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fya6ezmxd5/</guid><description>MatFormer: Train one universal model, extract hundreds of accurate submodels for elastic inference!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fya6ezmxd5/cover.png"/></item><item><title>Meaningful Learning: Enhancing Abstract Reasoning in Large Language Models via Generic Fact Guidance</title><link>https://deep-diver.github.io/neurips2024/posters/tihifqgoyc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tihifqgoyc/</guid><description>Boosting LLMs&amp;rsquo; abstract reasoning via &amp;lsquo;Meaningful Learning&amp;rsquo;: A new dataset and learning paradigm significantly enhance LLMs&amp;rsquo; capacity for abstract reasoning, moving beyond simple memorization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tihifqgoyc/cover.png"/></item><item><title>Measuring Progress in Dictionary Learning for Language Model Interpretability with Board Game Models</title><link>https://deep-diver.github.io/neurips2024/posters/scedogghcw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/scedogghcw/</guid><description>New metrics and p-annealing improve sparse autoencoder training for better language model interpretability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/scedogghcw/cover.png"/></item><item><title>MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive Clinical Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/w4pibq7bai/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w4pibq7bai/</guid><description>MEDIQ benchmark revolutionizes LLM evaluation by shifting from static to interactive clinical reasoning, revealing LLMs&amp;rsquo; struggles with proactive information-seeking and highlighting the importance of&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w4pibq7bai/cover.png"/></item><item><title>Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length</title><link>https://deep-diver.github.io/neurips2024/posters/xlabmzu4bo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xlabmzu4bo/</guid><description>MEGALODON: A new neural architecture for LLMs, enabling unlimited context length with improved efficiency and accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xlabmzu4bo/cover.png"/></item><item><title>Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration</title><link>https://deep-diver.github.io/neurips2024/posters/pawqvrforj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pawqvrforj/</guid><description>SPV-MIA, a novel membership inference attack, significantly improves the accuracy of identifying training data in fine-tuned LLMs by using self-prompt calibration and probabilistic variation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pawqvrforj/cover.png"/></item><item><title>Memory-Efficient LLM Training with Online Subspace Descent</title><link>https://deep-diver.github.io/neurips2024/posters/p8rtct6g45/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/p8rtct6g45/</guid><description>Online Subspace Descent: a novel memory-efficient LLM training algorithm guaranteed to converge, closing the performance gap with full-rank methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/p8rtct6g45/cover.png"/></item><item><title>MemoryFormer : Minimize Transformer Computation by Removing Fully-Connected Layers</title><link>https://deep-diver.github.io/neurips2024/posters/04ec4znzjj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/04ec4znzjj/</guid><description>MemoryFormer drastically cuts large language model computation by replacing fully-connected layers with memory-efficient hashing, enabling faster and more scalable AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/04ec4znzjj/cover.png"/></item><item><title>Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/zaxumqoaf4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zaxumqoaf4/</guid><description>Mesa-Extrapolation enhances LLM extrapolation using a novel weave position encoding method, boosting performance while significantly reducing memory and inference time.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zaxumqoaf4/cover.png"/></item><item><title>Meta-Diffu$B$: A Contextualized Sequence-to-Sequence Text Diffusion Model with Meta-Exploration</title><link>https://deep-diver.github.io/neurips2024/posters/ntwxvvixjm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ntwxvvixjm/</guid><description>Meta-DiffuB enhances sequence-to-sequence text diffusion models by using meta-exploration to learn a contextualized noise schedule, resulting in state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ntwxvvixjm/cover.png"/></item><item><title>MetaAligner: Towards Generalizable Multi-Objective Alignment of Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/divb5c0qff/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/divb5c0qff/</guid><description>MetaAligner: a novel, policy-agnostic, and generalizable method for efficiently aligning LLMs to multiple objectives, even unseen ones, achieving significant and balanced improvements while saving up &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/divb5c0qff/cover.png"/></item><item><title>Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving</title><link>https://deep-diver.github.io/neurips2024/posters/d19uyp4hyk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/d19uyp4hyk/</guid><description>LLMs gain math skills via prompt-guided skill labeling and exemplar selection, significantly boosting accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/d19uyp4hyk/cover.png"/></item><item><title>MicroAdam: Accurate Adaptive Optimization with Low Space Overhead and Provable Convergence</title><link>https://deep-diver.github.io/neurips2024/posters/tck41rangk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tck41rangk/</guid><description>MICROADAM: A new Adam optimizer variant dramatically cuts memory usage for training large language models without compromising accuracy or provable convergence.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tck41rangk/cover.png"/></item><item><title>Microstructures and Accuracy of Graph Recall by Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/tnhwg9u767/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tnhwg9u767/</guid><description>LLMs struggle with graph recall, exhibiting biases like favoring triangles and underperforming compared to humans; advanced models show striking domain dependence.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tnhwg9u767/cover.png"/></item><item><title>Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/cej1mypgww/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cej1mypgww/</guid><description>LLMs&amp;rsquo; spatial reasoning abilities are boosted by visualizing their thought processes via &amp;lsquo;Visualization-of-Thought&amp;rsquo; prompting, significantly improving performance on navigation and tiling tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cej1mypgww/cover.png"/></item><item><title>MindMerger: Efficiently Boosting LLM Reasoning in non-English Languages</title><link>https://deep-diver.github.io/neurips2024/posters/oq32ylaou2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oq32ylaou2/</guid><description>MindMerger efficiently boosts LLM reasoning in non-English languages by merging LLMs with external multilingual language understanding capabilities, achieving significant accuracy improvements, especi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oq32ylaou2/cover.png"/></item><item><title>Mini-Sequence Transformers: Optimizing Intermediate Memory for Long Sequences Training</title><link>https://deep-diver.github.io/neurips2024/posters/2kuzhyykkq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2kuzhyykkq/</guid><description>MINI-SEQUENCE TRANSFORMER (MST) drastically reduces memory usage in LLM training by processing mini-sequences iteratively, enabling training with 12-24x longer sequences than conventional methods with&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2kuzhyykkq/cover.png"/></item><item><title>MiniCache: KV Cache Compression in Depth Dimension for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/sgvojdqumt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sgvojdqumt/</guid><description>MiniCache: A novel approach to drastically reduce LLM KV cache memory footprint.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sgvojdqumt/cover.png"/></item><item><title>Mitigating Reward Overoptimization via Lightweight Uncertainty Estimation</title><link>https://deep-diver.github.io/neurips2024/posters/kyio3xh6eb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kyio3xh6eb/</guid><description>ADVPO, a novel method, tackles reward overoptimization in RLHF via a lightweight uncertainty quantification approach, resulting in enhanced LLM performance and alignment with human values.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kyio3xh6eb/cover.png"/></item><item><title>MixEval: Deriving Wisdom of the Crowd from LLM Benchmark Mixtures</title><link>https://deep-diver.github.io/neurips2024/posters/6a29luzhfv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/6a29luzhfv/</guid><description>MixEval revolutionizes LLM benchmarking by blending real-world user queries with existing datasets, creating a cost-effective, unbiased, and dynamic evaluation method.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/6a29luzhfv/cover.png"/></item><item><title>Mixture of Demonstrations for In-Context Learning</title><link>https://deep-diver.github.io/neurips2024/posters/uqxslocw3k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uqxslocw3k/</guid><description>MoD, a novel Mixture of Demonstrations framework, enhances in-context learning by partitioning demonstration pools and employing expert-wise training, achieving state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uqxslocw3k/cover.png"/></item><item><title>Mixture of In-Context Experts Enhance LLMs' Long Context Awareness</title><link>https://deep-diver.github.io/neurips2024/posters/rcphboficn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rcphboficn/</guid><description>MoICE, a novel plug-in, significantly enhances LLMs&amp;rsquo; long context awareness by dynamically routing attention using multiple RoPE angles, achieving superior performance with high inference efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rcphboficn/cover.png"/></item><item><title>Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/pgobeycxzs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pgobeycxzs/</guid><description>BinaryMoS: a novel token-adaptive binarization method that boosts LLM accuracy and efficiency by dynamically merging multiple scaling experts for each token.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pgobeycxzs/cover.png"/></item><item><title>Mixture of Tokens: Continuous MoE through Cross-Example Aggregation</title><link>https://deep-diver.github.io/neurips2024/posters/0zfvhmbzhj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0zfvhmbzhj/</guid><description>Mixture of Tokens (MoT) achieves 3x faster LLM training than dense Transformers and matches state-of-the-art MoE performance via continuous token mixing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0zfvhmbzhj/cover.png"/></item><item><title>Mobility-LLM: Learning Visiting Intentions and Travel Preference from Human Mobility Data with Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/0fejeykdrx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0fejeykdrx/</guid><description>Mobility-LLM leverages LLMs to analyze human mobility data from check-in sequences, significantly outperforming existing models in location prediction, user identification, and time prediction tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0fejeykdrx/cover.png"/></item><item><title>Model Decides How to Tokenize: Adaptive DNA Sequence Tokenization with MxDNA</title><link>https://deep-diver.github.io/neurips2024/posters/aq1umql7dz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/aq1umql7dz/</guid><description>MxDNA: Model learns optimal DNA tokenization via gradient descent, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/aq1umql7dz/cover.png"/></item><item><title>MoEUT: Mixture-of-Experts Universal Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/zxvrkm7bjl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zxvrkm7bjl/</guid><description>MoEUT: Mixture-of-Experts Universal Transformers significantly improves the compute efficiency of Universal Transformers, making them competitive with standard Transformers in large-scale language mod&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zxvrkm7bjl/cover.png"/></item><item><title>MoGU: A Framework for Enhancing Safety of LLMs While Preserving Their Usability</title><link>https://deep-diver.github.io/neurips2024/posters/srfbgijb53/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/srfbgijb53/</guid><description>MoGU: A framework dynamically balances safety and usability in LLMs by routing benign and malicious instructions to different LLM variants, leading to safer, more useful responses.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/srfbgijb53/cover.png"/></item><item><title>MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts</title><link>https://deep-diver.github.io/neurips2024/posters/y929escznj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y929escznj/</guid><description>MomentumSMoE boosts Sparse Mixture of Experts&amp;rsquo; (SMoE) performance by integrating momentum, resulting in more stable training and robust models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y929escznj/cover.png"/></item><item><title>MR-Ben: A Meta-Reasoning Benchmark for Evaluating System-2 Thinking in LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/gn2qbxzlni/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gn2qbxzlni/</guid><description>MR-Ben: A new benchmark reveals LLMs&amp;rsquo; meta-reasoning flaws, pushing the boundaries of AI evaluation beyond simple accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gn2qbxzlni/cover.png"/></item><item><title>Multi-Head Mixture-of-Experts</title><link>https://deep-diver.github.io/neurips2024/posters/dyz8gjzjtx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dyz8gjzjtx/</guid><description>Multi-Head Mixture-of-Experts (MH-MoE) drastically boosts large language model efficiency by activating almost all expert networks, achieving superior performance compared to existing Sparse Mixture-o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dyz8gjzjtx/cover.png"/></item><item><title>Multi-language Diversity Benefits Autoformalization</title><link>https://deep-diver.github.io/neurips2024/posters/2jjfrm2r6d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2jjfrm2r6d/</guid><description>Researchers created MMA, a large multilingual dataset of informal-formal mathematical pairs, leveraging a language model for reverse translation. Fine-tuned models achieved significantly improved aut&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2jjfrm2r6d/cover.png"/></item><item><title>Multi-LLM Debate: Framework, Principals, and Interventions</title><link>https://deep-diver.github.io/neurips2024/posters/sy7esexdpc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sy7esexdpc/</guid><description>Boosting LLM collaboration, this research introduces a novel theoretical framework for multi-LLM debate, revealing key principles like the effect of similar models and interventions to enhance accurac&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sy7esexdpc/cover.png"/></item><item><title>Multi-Object 3D Grounding with Dynamic Modules and Language-Informed Spatial Attention</title><link>https://deep-diver.github.io/neurips2024/posters/jfwl9ewz7z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jfwl9ewz7z/</guid><description>D-LISA: Dynamic modules &amp;amp; language-informed spatial attention revolutionizes multi-object 3D grounding, surpassing state-of-the-art accuracy by 12.8%.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jfwl9ewz7z/cover.png"/></item><item><title>Multi-turn Reinforcement Learning with Preference Human Feedback</title><link>https://deep-diver.github.io/neurips2024/posters/rvsc3hizs4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rvsc3hizs4/</guid><description>Multi-turn RLHF surpasses single-turn methods by aligning LLMs with human preferences across entire conversations, not just individual turns. A novel mirror-descent algorithm, MTPO, is introduced, pr&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rvsc3hizs4/cover.png"/></item><item><title>Multivariate Stochastic Dominance via Optimal Transport and Applications to Models Benchmarking</title><link>https://deep-diver.github.io/neurips2024/posters/ncx3kgb1nh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ncx3kgb1nh/</guid><description>This paper introduces an efficient multivariate stochastic dominance test using optimal transport, enabling robust model benchmarking by considering metric dependencies.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ncx3kgb1nh/cover.png"/></item><item><title>MutaPLM: Protein Language Modeling for Mutation Explanation and Engineering</title><link>https://deep-diver.github.io/neurips2024/posters/yppclfezgy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yppclfezgy/</guid><description>MutaPLM: a novel protein language model, provides human-understandable mutation explanations and designs novel mutations with desirable properties using a unique protein delta network and chain-of-tho&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yppclfezgy/cover.png"/></item><item><title>MUVERA: Multi-Vector Retrieval via Fixed Dimensional Encoding</title><link>https://deep-diver.github.io/neurips2024/posters/x3ydkrcqr6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x3ydkrcqr6/</guid><description>MUVERA: Revolutionizing multi-vector retrieval with single-vector speed and accuracy!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x3ydkrcqr6/cover.png"/></item><item><title>Navigating Extremes: Dynamic Sparsity in Large Output Spaces</title><link>https://deep-diver.github.io/neurips2024/posters/ra6rzoj2zi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ra6rzoj2zi/</guid><description>SPARTEX achieves memory-efficient extreme multi-label classification by integrating dynamic sparse training with an auxiliary loss function, enabling end-to-end training with millions of labels on com&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ra6rzoj2zi/cover.png"/></item><item><title>Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/gznsqbwhag/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gznsqbwhag/</guid><description>Researchers discover &amp;lsquo;safety basins&amp;rsquo; in LLMs, proposing a new metric (VISAGE) to quantify finetuning risks and visualize how these basins protect against safety compromise during model training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gznsqbwhag/cover.png"/></item><item><title>Nearest Neighbor Speculative Decoding for LLM Generation and Attribution</title><link>https://deep-diver.github.io/neurips2024/posters/ni9kebsstt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ni9kebsstt/</guid><description>NEST, a novel semi-parametric language model, significantly boosts LLM generation quality, provides accurate source attribution, and achieves a 1.8x speedup in inference time by cleverly incorporating&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ni9kebsstt/cover.png"/></item><item><title>Neuro-Symbolic Data Generation for Math Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/cicmzglyzw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cicmzglyzw/</guid><description>Neuro-symbolic framework generates high-quality mathematical datasets, enhancing LLMs&amp;rsquo; mathematical reasoning capabilities and surpassing state-of-the-art counterparts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cicmzglyzw/cover.png"/></item><item><title>No Free Lunch in LLM Watermarking: Trade-offs in Watermarking Design Choices</title><link>https://deep-diver.github.io/neurips2024/posters/riol7kbskv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/riol7kbskv/</guid><description>LLM watermarking faces inherent trade-offs; this paper reveals simple attacks exploiting common design choices, proposing guidelines and defenses for more secure systems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/riol7kbskv/cover.png"/></item><item><title>Noise Contrastive Alignment of Language Models with Explicit Rewards</title><link>https://deep-diver.github.io/neurips2024/posters/kwrldkyvol/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kwrldkyvol/</guid><description>This paper introduces InfoNCA and NCA, novel frameworks for language model alignment using noise contrastive estimation, enabling direct optimization from both explicit rewards and pairwise preference&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kwrldkyvol/cover.png"/></item><item><title>NoiseGPT: Label Noise Detection and Rectification through Probability Curvature</title><link>https://deep-diver.github.io/neurips2024/posters/vrrvjnxgqe/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vrrvjnxgqe/</guid><description>NoiseGPT uses multi-modal LLMs to detect &amp;amp; fix noisy image labels by identifying probability curvature differences between clean and noisy examples.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vrrvjnxgqe/cover.png"/></item><item><title>NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention</title><link>https://deep-diver.github.io/neurips2024/posters/4xdxvqhsbz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4xdxvqhsbz/</guid><description>NoMAD-Attention achieves up to 2x speedup in 4-bit quantized LLaMA inference on CPUs by replacing computationally expensive multiply-add operations with ultra-low-latency in-register lookups.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4xdxvqhsbz/cover.png"/></item><item><title>Normalization Layer Per-Example Gradients are Sufficient to Predict Gradient Noise Scale in Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/s7thlpvh8i/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s7thlpvh8i/</guid><description>By cleverly integrating per-example gradient norm calculations during the backward pass of LayerNorm layers, this research enables efficient and accurate gradient noise scale estimation in Transformer&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/s7thlpvh8i/cover.png"/></item><item><title>OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step</title><link>https://deep-diver.github.io/neurips2024/posters/vaogapvgyr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vaogapvgyr/</guid><description>OccamLLM: LLMs now perform accurate arithmetic in a single step!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vaogapvgyr/cover.png"/></item><item><title>On Affine Homotopy between Language Encoders</title><link>https://deep-diver.github.io/neurips2024/posters/ftpowiawuz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ftpowiawuz/</guid><description>This paper introduces a novel notion of intrinsic similarity between language encoders, based on affine homotopy, and demonstrates its strong correlation with extrinsic similarity (downstream task per&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ftpowiawuz/cover.png"/></item><item><title>On Giant's Shoulders: Effortless Weak to Strong by Dynamic Logits Fusion</title><link>https://deep-diver.github.io/neurips2024/posters/rmfiqfwawg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rmfiqfwawg/</guid><description>Effortlessly boost large language model performance by dynamically fusing knowledge from smaller, task-specific models â achieving near full fine-tuning results with minimal computational cost!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rmfiqfwawg/cover.png"/></item><item><title>On scalable oversight with weak LLMs judging strong LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/o1fp9nvraj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/o1fp9nvraj/</guid><description>Weak LLMs can accurately supervise strong LLMs via debate, outperforming simpler consultancy methods, especially in information-asymmetric tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/o1fp9nvraj/cover.png"/></item><item><title>On Softmax Direct Preference Optimization for Recommendation</title><link>https://deep-diver.github.io/neurips2024/posters/qp5vbgtam0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qp5vbgtam0/</guid><description>Softmax-DPO boosts LM-based recommender performance by directly optimizing for personalized ranking using a novel loss function that incorporates multiple negative samples, significantly outperforming&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qp5vbgtam0/cover.png"/></item><item><title>On the Inductive Bias of Stacking Towards Improving Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/3zaffoacui/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3zaffoacui/</guid><description>MIDAS: A novel training method improves language model reasoning by efficiently stacking middle layers, surprisingly boosting downstream task performance without increasing pretraining perplexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3zaffoacui/cover.png"/></item><item><title>On the Power of Decision Trees in Auto-Regressive Language Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/nge5derseh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nge5derseh/</guid><description>Auto-Regressive Decision Trees (ARDTs) surprisingly outperform Transformers on language tasks!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nge5derseh/cover.png"/></item><item><title>On the Worst Prompt Performance of Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/mi853qajx6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mi853qajx6/</guid><description>LLMs&amp;rsquo; performance drastically varies depending on prompt phrasing; this paper introduces ROBUSTAL-PACAEVAL to evaluate lower-bound performance via worst-case prompt analysis, revealing model inconsist&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mi853qajx6/cover.png"/></item><item><title>Once Read is Enough: Domain-specific Pretraining-free Language Models with Cluster-guided Sparse Experts for Long-tail Domain Knowledge</title><link>https://deep-diver.github.io/neurips2024/posters/manhbkpiw6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/manhbkpiw6/</guid><description>This research introduces Cluster-guided Sparse Experts (CSE), enabling pretrained language models to effectively learn long-tail domain knowledge without domain-specific pretraining, thus achieving su&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/manhbkpiw6/cover.png"/></item><item><title>One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos</title><link>https://deep-diver.github.io/neurips2024/posters/bqmevgcyvm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bqmevgcyvm/</guid><description>VideoLISA: A video-based multimodal large language model enabling precise, language-instructed video object segmentation with superior performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bqmevgcyvm/cover.png"/></item><item><title>OneBit: Towards Extremely Low-bit Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/zwig9kjfhv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zwig9kjfhv/</guid><description>OneBit achieves surprisingly good performance in 1-bit quantized LLMs by using a novel 1-bit parameter representation method and an effective parameter initialization method.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zwig9kjfhv/cover.png"/></item><item><title>Online Adaptation of Language Models with a Memory of Amortized Contexts</title><link>https://deep-diver.github.io/neurips2024/posters/rifgkckntu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rifgkckntu/</guid><description>MAC: Efficiently updates large language models (LLMs) using a memory of compressed contexts for improved real-time knowledge retention and adaptation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rifgkckntu/cover.png"/></item><item><title>Online Iterative Reinforcement Learning from Human Feedback with General Preference Model</title><link>https://deep-diver.github.io/neurips2024/posters/twdx1w3m6s/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/twdx1w3m6s/</guid><description>This paper proposes a novel, reward-free RLHF framework using a general preference oracle, surpassing existing reward-based approaches in efficiency and generalizability.</description></item><item><title>Open LLMs are Necessary for Current Private Adaptations and Outperform their Closed Alternatives</title><link>https://deep-diver.github.io/neurips2024/posters/jf40h5prw0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jf40h5prw0/</guid><description>Open LLMs outperform closed alternatives for private data adaptation, offering superior privacy, performance, and lower costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jf40h5prw0/cover.png"/></item><item><title>Optimized Feature Generation for Tabular Data via LLMs with Decision Tree Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/apsbwumopo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/apsbwumopo/</guid><description>LLMs boost tabular data prediction by generating optimized features via decision tree reasoning, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/apsbwumopo/cover.png"/></item><item><title>Orchid: Flexible and Data-Dependent Convolution for Sequence Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/a75f45dbhk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/a75f45dbhk/</guid><description>Orchid: a novel deep learning architecture using data-dependent convolution achieves quasilinear scalability and outperforms attention-based models on various sequence modeling tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/a75f45dbhk/cover.png"/></item><item><title>Order-Independence Without Fine Tuning</title><link>https://deep-diver.github.io/neurips2024/posters/lq45ar8l7d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lq45ar8l7d/</guid><description>Set-Based Prompting guarantees order-independent LLM outputs by modifying input representations, eliminating unwanted inconsistencies without fine-tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lq45ar8l7d/cover.png"/></item><item><title>Over-parameterized Student Model via Tensor Decomposition Boosted Knowledge Distillation</title><link>https://deep-diver.github.io/neurips2024/posters/ft1rkagrc3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ft1rkagrc3/</guid><description>Over-parameterized Distillation Framework (OPDF) boosts knowledge distillation by efficiently over-parameterizing student models via tensor decomposition, significantly improving performance without i&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ft1rkagrc3/cover.png"/></item><item><title>PaCE: Parsimonious Concept Engineering for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/lomht16t8r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lomht16t8r/</guid><description>PaCE, a novel activation engineering framework, efficiently aligns LLMs by removing undesirable concepts from activations using sparse coding, achieving state-of-the-art performance while preserving l&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lomht16t8r/cover.png"/></item><item><title>PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition</title><link>https://deep-diver.github.io/neurips2024/posters/vjw4tif8bo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vjw4tif8bo/</guid><description>PaDeLLM-NER massively accelerates LLM-based NER inference by up to 10x, enabling near real-time performance without accuracy loss.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vjw4tif8bo/cover.png"/></item><item><title>Panacea: Pareto Alignment via Preference Adaptation for LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/gl5nt4y8fn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gl5nt4y8fn/</guid><description>Panacea: a novel LLM alignment method achieving Pareto optimality via online preference adaptation using a single model.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gl5nt4y8fn/cover.png"/></item><item><title>Paralinguistics-Aware Speech-Empowered Large Language Models for Natural Conversation</title><link>https://deep-diver.github.io/neurips2024/posters/njewxjudyq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/njewxjudyq/</guid><description>Unified Spoken Dialog Model (USDM) directly generates coherent spoken responses with natural prosody, surpassing cascaded baselines and enhancing natural conversation in speech-enabled LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/njewxjudyq/cover.png"/></item><item><title>Parallelizing Linear Transformers with the Delta Rule over Sequence Length</title><link>https://deep-diver.github.io/neurips2024/posters/y8rm4vnrph/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y8rm4vnrph/</guid><description>DeltaNet, a linear transformer boosting associative recall, now trains efficiently via a novel algorithm, scaling to large language models and outperforming existing linear baselines.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y8rm4vnrph/cover.png"/></item><item><title>Parameter Competition Balancing for Model Merging</title><link>https://deep-diver.github.io/neurips2024/posters/l5sbrtvsrs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/l5sbrtvsrs/</guid><description>PCB-MERGING: A training-free model merging technique boosts performance by intelligently balancing parameter competition across multiple tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/l5sbrtvsrs/cover.png"/></item><item><title>PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications</title><link>https://deep-diver.github.io/neurips2024/posters/wvokwq12x5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wvokwq12x5/</guid><description>PediatricsGPT: a novel Chinese pediatric LLM assistant trained on a large, high-quality dataset (PedCorpus) outperforms existing models, paving the way for improved pediatric healthcare.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wvokwq12x5/cover.png"/></item><item><title>Perception of Knowledge Boundary for Large Language Models through Semi-open-ended Question Answering</title><link>https://deep-diver.github.io/neurips2024/posters/li9ythoitp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/li9ythoitp/</guid><description>This study reveals that large language models struggle with semi-open-ended questions, often hallucinating or providing insufficient answers. Researchers explored this by creating a new dataset of su&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/li9ythoitp/cover.png"/></item><item><title>Perplexity-aware Correction for Robust Alignment with Noisy Preferences</title><link>https://deep-diver.github.io/neurips2024/posters/ouxnnpjzxj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ouxnnpjzxj/</guid><description>PerpCorrect: Robust LLM alignment despite noisy human preferences, achieved via perplexity-based noisy preference detection and correction.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ouxnnpjzxj/cover.png"/></item><item><title>Personalized Steering of Large Language Models: Versatile Steering Vectors Through Bi-directional Preference Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/7qjfkuzdyo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7qjfkuzdyo/</guid><description>Bi-directional Preference Optimization (BiPO) generates superior steering vectors for personalized LLM control, improving upon existing methods by directly influencing the generation probability of hu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7qjfkuzdyo/cover.png"/></item><item><title>PhyloGen: Language Model-Enhanced Phylogenetic Inference via Graph Structure Generation</title><link>https://deep-diver.github.io/neurips2024/posters/gxvdsfarxy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gxvdsfarxy/</guid><description>PhyloGen uses a genomic language model to generate and optimize phylogenetic trees, offering faster and more accurate evolutionary analysis than traditional methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gxvdsfarxy/cover.png"/></item><item><title>Pipeline Parallelism with Controllable Memory</title><link>https://deep-diver.github.io/neurips2024/posters/vvcnqs8091/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vvcnqs8091/</guid><description>New pipeline parallelism framework achieves up to 55% higher throughput and 50% less memory usage in large language model training by systematically controlling activation memory.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vvcnqs8091/cover.png"/></item><item><title>Plan-on-Graph: Self-Correcting Adaptive Planning of Large Language Model on Knowledge Graphs</title><link>https://deep-diver.github.io/neurips2024/posters/cwcuer6wo5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cwcuer6wo5/</guid><description>Plan-on-Graph (PoG) revolutionizes KG-augmented LLMs with a self-correcting adaptive planning paradigm, enabling more efficient and accurate reasoning over knowledge graphs by dynamically adjusting ex&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cwcuer6wo5/cover.png"/></item><item><title>Policy Improvement using Language Feedback Models</title><link>https://deep-diver.github.io/neurips2024/posters/fvgcwcwpjw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fvgcwcwpjw/</guid><description>Boosting AI instruction following, Language Feedback Models (LFMs) leverage Large Language Models (LLMs) to identify desirable behaviors from visual trajectories, significantly improving task completi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fvgcwcwpjw/cover.png"/></item><item><title>Pre-trained Large Language Models Use Fourier Features to Compute Addition</title><link>https://deep-diver.github.io/neurips2024/posters/i4mutm2tzb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/i4mutm2tzb/</guid><description>Pre-trained LLMs surprisingly use Fourier features to perform addition, with MLP layers approximating magnitude and attention layers handling modular arithmetic; this mechanism requires pre-training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/i4mutm2tzb/cover.png"/></item><item><title>Predicting the Performance of Foundation Models via Agreement-on-the-Line</title><link>https://deep-diver.github.io/neurips2024/posters/ajx9onwsr4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ajx9onwsr4/</guid><description>Foundation model OOD performance prediction is reliably achieved via ensemble diversity, especially through random linear head initialization, enabling precise estimations without extensive OOD labels&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ajx9onwsr4/cover.png"/></item><item><title>Prediction-Powered Ranking of Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/7v62sq5jra/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7v62sq5jra/</guid><description>This paper presents a novel statistical framework for ranking LLMs using pairwise comparisons, accounting for the uncertainty introduced when using an LLM instead of human preferences. The framework &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7v62sq5jra/cover.png"/></item><item><title>Predictor-Corrector Enhanced Transformers with Exponential Moving Average Coefficient Learning</title><link>https://deep-diver.github.io/neurips2024/posters/ur9f4hnipn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ur9f4hnipn/</guid><description>PCformer boosts Transformer performance by using a predictor-corrector learning framework and exponential moving average coefficient learning for high-order prediction, achieving state-of-the-art resu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ur9f4hnipn/cover.png"/></item><item><title>Preference Learning Algorithms Do Not Learn Preference Rankings</title><link>https://deep-diver.github.io/neurips2024/posters/ykj5buexdd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ykj5buexdd/</guid><description>Despite common belief, state-of-the-art preference learning algorithms for LLMs achieve surprisingly low ranking accuracy, highlighting significant flaws in current alignment techniques.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ykj5buexdd/cover.png"/></item><item><title>Pretrained Transformer Efficiently Learns Low-Dimensional Target Functions In-Context</title><link>https://deep-diver.github.io/neurips2024/posters/uhcg5y6fdb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uhcg5y6fdb/</guid><description>Pretrained transformers surprisingly learn low-dimensional nonlinear functions efficiently from few in-context examples, outperforming baseline algorithms.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uhcg5y6fdb/cover.png"/></item><item><title>Probing Social Bias in Labor Market Text Generation by ChatGPT: A Masked Language Model Approach</title><link>https://deep-diver.github.io/neurips2024/posters/mp7j58lbwo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mp7j58lbwo/</guid><description>ChatGPT amplifies gender bias in job applications, revealing AI&amp;rsquo;s potential to worsen labor market inequality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mp7j58lbwo/cover.png"/></item><item><title>Probing the Decision Boundaries of In-context Learning in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/fbxqrfkvty/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fbxqrfkvty/</guid><description>LLMs&amp;rsquo; in-context learning, though effective, exhibits surprisingly irregular decision boundaries, hindering generalization; this paper reveals this issue and proposes methods to improve smoothness via&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fbxqrfkvty/cover.png"/></item><item><title>Prompt Tuning Strikes Back: Customizing Foundation Models with Low-Rank Prompt Adaptation</title><link>https://deep-diver.github.io/neurips2024/posters/symhgilvcv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/symhgilvcv/</guid><description>LoPA: a novel parameter-efficient fine-tuning method matches state-of-the-art performance while requiring no server-side adapters, improving upon traditional prompt tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/symhgilvcv/cover.png"/></item><item><title>Protecting Your LLMs with Information Bottleneck</title><link>https://deep-diver.github.io/neurips2024/posters/u9shp64fjv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/u9shp64fjv/</guid><description>IBProtector shields LLMs from harmful outputs via prompt compression, selectively preserving essential information using a trainable extractor.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/u9shp64fjv/cover.png"/></item><item><title>ProtGO: Function-Guided Protein Modeling for Unified Representation Learning</title><link>https://deep-diver.github.io/neurips2024/posters/0ouutv92yf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0ouutv92yf/</guid><description>ProtGO: A novel unified framework integrating protein sequence, structure &amp;amp; function for superior representation learning, significantly outperforming current methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0ouutv92yf/cover.png"/></item><item><title>Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer</title><link>https://deep-diver.github.io/neurips2024/posters/2cq3lphkeo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2cq3lphkeo/</guid><description>RLHF&amp;rsquo;s overoptimization problem is mitigated by RPO, a novel algorithm that uses SFT loss as an implicit adversarial regularizer, ensuring efficient and effective LLM alignment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2cq3lphkeo/cover.png"/></item><item><title>Provably Transformers Harness Multi-Concept Word Semantics for Efficient In-Context Learning</title><link>https://deep-diver.github.io/neurips2024/posters/57c9mszjj3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/57c9mszjj3/</guid><description>Transformers excel at in-context learning (ICL), solving new tasks with just prompts. This paper provides a mathematical explanation, showing how transformers use multi-concept word semantics to achie&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/57c9mszjj3/cover.png"/></item><item><title>QBB: Quantization with Binary Bases for LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/kw6mrgfx0r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kw6mrgfx0r/</guid><description>QBB: A novel post-training quantization method for LLMs dramatically improves efficiency by replacing multiplications with summations, achieving state-of-the-art results with minimal accuracy loss.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kw6mrgfx0r/cover.png"/></item><item><title>QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation</title><link>https://deep-diver.github.io/neurips2024/posters/efpznpkrm2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/efpznpkrm2/</guid><description>QuanTA: Quantum-inspired Tensor Adaptation efficiently fine-tunes LLMs with high-rank updates, surpassing low-rank methods like LoRA for complex tasks while minimizing additional parameters.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/efpznpkrm2/cover.png"/></item><item><title>Quantifying and Optimizing Global Faithfulness in Persona-driven Role-playing</title><link>https://deep-diver.github.io/neurips2024/posters/bzpmjmiaz8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bzpmjmiaz8/</guid><description>New APC metric precisely quantifies &amp;amp; optimizes global faithfulness in persona-driven role-playing, offering a fine-grained, explainable evaluation and improving AI character consistency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bzpmjmiaz8/cover.png"/></item><item><title>Quantifying the Gain in Weak-to-Strong Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/myvyh5jo1l/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/myvyh5jo1l/</guid><description>Weakly supervised strong models outperform weak models; this gain is precisely quantified by the strong model&amp;rsquo;s misfit error on weak labels.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/myvyh5jo1l/cover.png"/></item><item><title>QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/dfqsw38v1x/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dfqsw38v1x/</guid><description>QuaRot: Revolutionizing 4-bit LLM inference with lossless quantization via rotation!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dfqsw38v1x/cover.png"/></item><item><title>Query-Based Adversarial Prompt Generation</title><link>https://deep-diver.github.io/neurips2024/posters/jbf3eiyd2x/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jbf3eiyd2x/</guid><description>Researchers developed a query-based attack that generates adversarial prompts, fooling language models into producing harmful outputs with significantly higher success rates than previous methods, eff&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jbf3eiyd2x/cover.png"/></item><item><title>QUEST: Quality-Aware Metropolis-Hastings Sampling for Machine Translation</title><link>https://deep-diver.github.io/neurips2024/posters/dlnduwgtb4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dlnduwgtb4/</guid><description>QUEST, a novel Metropolis-Hastings sampling method, generates high-quality &amp;amp; diverse machine translations by using quality metrics as energy functions, overcoming limitations of likelihood-based and r&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dlnduwgtb4/cover.png"/></item><item><title>Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts</title><link>https://deep-diver.github.io/neurips2024/posters/fcsevamorw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fcsevamorw/</guid><description>Rainbow Teaming: a novel black-box approach generates diverse adversarial prompts to enhance LLM robustness and safety, achieving over 90% attack success rate across various models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fcsevamorw/cover.png"/></item><item><title>RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/s1fc92uemc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s1fc92uemc/</guid><description>RankRAG: One LLM, dual-purpose instruction-tuning for superior RAG!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/s1fc92uemc/cover.png"/></item><item><title>Realizable $H$-Consistent and Bayes-Consistent Loss Functions for Learning to Defer</title><link>https://deep-diver.github.io/neurips2024/posters/oco2xakuuk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oco2xakuuk/</guid><description>New surrogate loss functions for learning-to-defer achieve Bayes-consistency, realizable H-consistency, and H-consistency bounds simultaneously, resolving open questions and improving L2D performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oco2xakuuk/cover.png"/></item><item><title>Reasons and Solutions for the Decline in Model Performance after Editing</title><link>https://deep-diver.github.io/neurips2024/posters/xjxygdfm5m/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xjxygdfm5m/</guid><description>Boosting large language model performance after knowledge editing: A new method (D4S) minimizes model damage by regulating the explosive growth of parameter layers, enabling multiple effective edits.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xjxygdfm5m/cover.png"/></item><item><title>Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training</title><link>https://deep-diver.github.io/neurips2024/posters/yss1z5udby/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yss1z5udby/</guid><description>Overparameterized neural networks surprisingly recover from catastrophic interference when trained cyclically on repeated data sequences, exhibiting anticipatory knowledge reactivation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yss1z5udby/cover.png"/></item><item><title>REBORN: Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR</title><link>https://deep-diver.github.io/neurips2024/posters/v3qzcm1aqv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v3qzcm1aqv/</guid><description>REBORN: An iterative training framework significantly improves unsupervised ASR by learning optimal speech segment boundaries using reinforcement learning, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v3qzcm1aqv/cover.png"/></item><item><title>Recursive Introspection: Teaching Language Model Agents How to Self-Improve</title><link>https://deep-diver.github.io/neurips2024/posters/drc9pzwbwr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/drc9pzwbwr/</guid><description>RISE: Recursive Introspection teaches LLMs to iteratively improve their responses, enabling self-correction and enhanced performance on challenging reasoning tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/drc9pzwbwr/cover.png"/></item><item><title>Reducing Transformer Key-Value Cache Size with Cross-Layer Attention</title><link>https://deep-diver.github.io/neurips2024/posters/m2uzlroqic/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/m2uzlroqic/</guid><description>Cross-Layer Attention (CLA) shrinks Transformer Key-Value cache 2x, improving LLMs&amp;rsquo; memory efficiency without accuracy loss.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/m2uzlroqic/cover.png"/></item><item><title>Reference Trustable Decoding: A Training-Free Augmentation Paradigm for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/qhrlfdhklu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qhrlfdhklu/</guid><description>Reference Trustable Decoding (RTD) revolutionizes large language model adaptation by offering a training-free method, enabling efficient and cost-effective task adaptation without parameter adjustment&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qhrlfdhklu/cover.png"/></item><item><title>Reflective Multi-Agent Collaboration based on Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/wwiar5mqxq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wwiar5mqxq/</guid><description>COPPER enhances LLM-based multi-agent collaboration via a self-reflection mechanism and counterfactual PPO. It improves reflection quality, alleviates credit assignment issues, and shows strong perfo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wwiar5mqxq/cover.png"/></item><item><title>Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/jwh9mhefmy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jwh9mhefmy/</guid><description>Regularizing hidden states improves reward model generalization in RLHF for LLMs, boosting accuracy and mitigating over-optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jwh9mhefmy/cover.png"/></item><item><title>Reinforcing LLM Agents via Policy Optimization with Action Decomposition</title><link>https://deep-diver.github.io/neurips2024/posters/hz6csigmyu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hz6csigmyu/</guid><description>POAD enhances LLM agents by decomposing language agent optimization to the token level, achieving finer-grained credit assignment and improved learning efficiency and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hz6csigmyu/cover.png"/></item><item><title>ReMoDetect: Reward Models Recognize Aligned LLM's Generations</title><link>https://deep-diver.github.io/neurips2024/posters/pw9jwim918/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pw9jwim918/</guid><description>ReMoDetect leverages reward models to identify and classify LLM-generated text. By using continual preference fine-tuning and incorporating human/LLM mixed text, ReMoDetect achieves state-of-the-art p&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pw9jwim918/cover.png"/></item><item><title>Representation Noising: A Defence Mechanism Against Harmful Finetuning</title><link>https://deep-diver.github.io/neurips2024/posters/ep9auejqfg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ep9auejqfg/</guid><description>RepNoise: a novel defense against harmful fine-tuning of LLMs by removing information about harmful representations, generalizing across different harmful tasks, and maintaining LLM capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ep9auejqfg/cover.png"/></item><item><title>Repurposing Language Models into Embedding Models: Finding the Compute-Optimal Recipe</title><link>https://deep-diver.github.io/neurips2024/posters/kvl5rvkqgg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kvl5rvkqgg/</guid><description>This research unveils a compute-optimal recipe for fine-tuning language models into high-quality text embedding models, offering practical guidance and scaling laws for resource-constrained settings.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kvl5rvkqgg/cover.png"/></item><item><title>ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search</title><link>https://deep-diver.github.io/neurips2024/posters/8rcfoqeud5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8rcfoqeud5/</guid><description>ReST-MCTS*: A novel LLM self-training method using process reward guided tree search, outperforming existing methods by generating higher-quality reasoning traces for improved model accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8rcfoqeud5/cover.png"/></item><item><title>Rethinking LLM Memorization through the Lens of Adversarial Compression</title><link>https://deep-diver.github.io/neurips2024/posters/kfmrmvzazy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kfmrmvzazy/</guid><description>Researchers propose Adversarial Compression Ratio (ACR) to assess LLM memorization, offering an adversarial, flexible, and computationally efficient method for monitoring data misuse and compliance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kfmrmvzazy/cover.png"/></item><item><title>Rethinking Memory and Communication Costs for Efficient Data Parallel Training of Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/4un2td9bne/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4un2td9bne/</guid><description>PaRO boosts LLM training speed by up to 266% through refined model state partitioning and optimized communication.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4un2td9bne/cover.png"/></item><item><title>Reversing the Forget-Retain Objectives: An Efficient LLM Unlearning Framework from Logit Difference</title><link>https://deep-diver.github.io/neurips2024/posters/tydr1ltwqh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tydr1ltwqh/</guid><description>Reverse the forget-retain objectives for efficient LLM unlearning!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tydr1ltwqh/cover.png"/></item><item><title>Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy</title><link>https://deep-diver.github.io/neurips2024/posters/7jb4njs8yk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7jb4njs8yk/</guid><description>Richelieu: a self-evolving LLM-based AI agent masters Diplomacy, a complex game requiring strategic planning and negotiation, without human data, by integrating self-play for continuous improvement.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7jb4njs8yk/cover.png"/></item><item><title>Risk-Averse Fine-tuning of Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/1bzkqzphsw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1bzkqzphsw/</guid><description>Risk-Averse RLHF fine-tunes LLMs to minimize toxic outputs while maintaining performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1bzkqzphsw/cover.png"/></item><item><title>RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold</title><link>https://deep-diver.github.io/neurips2024/posters/9m87e9keq1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/9m87e9keq1/</guid><description>Leveraging model-generated synthetic data for LLM finetuning significantly improves efficiency when using both positive and strategically constructed negative examples, resulting in an eight-fold incr&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/9m87e9keq1/cover.png"/></item><item><title>RouterDC: Query-Based Router by Dual Contrastive Learning for Assembling Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/7rqvjayhrm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7rqvjayhrm/</guid><description>RouterDC: A query-based router trained via dual contrastive learning assembles multiple LLMs, significantly outperforming individual LLMs and existing routing methods on both in- and out-of-distributi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7rqvjayhrm/cover.png"/></item><item><title>RSA: Resolving Scale Ambiguities in Monocular Depth Estimators through Language Descriptions</title><link>https://deep-diver.github.io/neurips2024/posters/vh7gcadhao/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vh7gcadhao/</guid><description>RSA: Language unlocks metric depth from single images!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vh7gcadhao/cover.png"/></item><item><title>S-STE: Continuous Pruning Function for Efficient 2:4 Sparse Pre-training</title><link>https://deep-diver.github.io/neurips2024/posters/8abncvjs2j/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8abncvjs2j/</guid><description>S-STE achieves efficient 2:4 sparse pre-training by introducing a novel continuous pruning function, overcoming the limitations of previous methods and leading to improved accuracy and speed.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8abncvjs2j/cover.png"/></item><item><title>S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by Structured Sparsity</title><link>https://deep-diver.github.io/neurips2024/posters/leule8s4xq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/leule8s4xq/</guid><description>S2FT: Structured Sparse Fine-Tuning achieves state-of-the-art LLM fine-tuning performance, training efficiency, and inference scalability by selecting sparsely and computing densely.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/leule8s4xq/cover.png"/></item><item><title>SafeWorld: Geo-Diverse Safety Alignment</title><link>https://deep-diver.github.io/neurips2024/posters/vzqmiodgbg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vzqmiodgbg/</guid><description>SAFEWORLD: a new benchmark reveals and fixes LLMs&amp;rsquo; struggle with diverse safety standards.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vzqmiodgbg/cover.png"/></item><item><title>SaulLM-54B &amp; SaulLM-141B: Scaling Up Domain Adaptation for the Legal Domain</title><link>https://deep-diver.github.io/neurips2024/posters/nluyz4zqnq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nluyz4zqnq/</guid><description>SaulLM-54B &amp;amp; SaulLM-141B achieve state-of-the-art performance on legal tasks by scaling up model size, employing a specialized instruction-following protocol, and aligning model outputs with human pre&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nluyz4zqnq/cover.png"/></item><item><title>Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/pf4oujyn4q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pf4oujyn4q/</guid><description>Direct Alignment Algorithms (DAAs) for LLM alignment suffer from over-optimization, even without explicit reward models; this paper empirically demonstrates this and proposes scaling laws to understan&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pf4oujyn4q/cover.png"/></item><item><title>Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies</title><link>https://deep-diver.github.io/neurips2024/posters/skckpr8crl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/skckpr8crl/</guid><description>Boosting LLM performance: This research shows how larger language models need bigger vocabularies for optimal efficiency and performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/skckpr8crl/cover.png"/></item><item><title>Scaling Retrieval-Based Language Models with a Trillion-Token Datastore</title><link>https://deep-diver.github.io/neurips2024/posters/iakhpz7qt3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/iakhpz7qt3/</guid><description>Massive language models improve with bigger datastores at inference time. A 1.4 trillion-token datastore, MASSIVEDS, shows that retrieval-based LMs outperform larger, solely-trained models on knowled&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/iakhpz7qt3/cover.png"/></item><item><title>Scaling Sign Language Translation</title><link>https://deep-diver.github.io/neurips2024/posters/m80wgio2lb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/m80wgio2lb/</guid><description>Researchers dramatically improved sign language translation by scaling up data, model size, and the number of languages, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/m80wgio2lb/cover.png"/></item><item><title>SDP4Bit: Toward 4-bit Communication Quantization in Sharded Data Parallelism for LLM Training</title><link>https://deep-diver.github.io/neurips2024/posters/peeqnxlsck/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/peeqnxlsck/</guid><description>SDP4Bit achieves up to 4.08x speedup in LLM training by quantizing weight differences and gradients to ~4 bits, maintaining accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/peeqnxlsck/cover.png"/></item><item><title>Search for Efficient Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/lxsmlxlvks/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lxsmlxlvks/</guid><description>Training-free architecture search finds optimal subnets in LLMs, boosting inference speed and slashing memory needs without retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lxsmlxlvks/cover.png"/></item><item><title>Segmenting Watermarked Texts From Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/faufpgelmx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/faufpgelmx/</guid><description>This paper presents novel statistical methods to reliably watermark and segment LLMs-generated text, ensuring source traceability even after user modifications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/faufpgelmx/cover.png"/></item><item><title>SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware Self-Reflection</title><link>https://deep-diver.github.io/neurips2024/posters/qnieopt4fg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qnieopt4fg/</guid><description>SelectIT leverages LLMs&amp;rsquo; intrinsic uncertainty to efficiently select high-quality instruction tuning data, enhancing model performance without extra resources.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qnieopt4fg/cover.png"/></item><item><title>Selective Attention: Enhancing Transformer through Principled Context Control</title><link>https://deep-diver.github.io/neurips2024/posters/qbqlcwmxff/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qbqlcwmxff/</guid><description>Enhance Transformer models via Selective Self-Attention (SSA), a principled context control method that boosts accuracy and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qbqlcwmxff/cover.png"/></item><item><title>SELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures</title><link>https://deep-diver.github.io/neurips2024/posters/brovxhmzyk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/brovxhmzyk/</guid><description>LLMs self-discover optimal reasoning structures for complex problems, boosting performance by up to 32% compared to existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/brovxhmzyk/cover.png"/></item><item><title>Self-Guiding Exploration for Combinatorial Problems</title><link>https://deep-diver.github.io/neurips2024/posters/bgogknwhbi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bgogknwhbi/</guid><description>LLMs excel at reasoning tasks, but their application to combinatorial problems (CPs) is underexplored. This paper introduces Self-Guiding Exploration (SGE), a novel prompting strategy that significan&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bgogknwhbi/cover.png"/></item><item><title>Self-playing Adversarial Language Game Enhances LLM Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/ocgksh7ys2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ocgksh7ys2/</guid><description>Self-play adversarial language game boosts LLM reasoning!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ocgksh7ys2/cover.png"/></item><item><title>Self-Retrieval: End-to-End Information Retrieval with One Large Language Model</title><link>https://deep-diver.github.io/neurips2024/posters/h3at5y8vfw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/h3at5y8vfw/</guid><description>Self-Retrieval revolutionizes information retrieval by unifying indexing, retrieval, and reranking within a single large language model, achieving significantly improved performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/h3at5y8vfw/cover.png"/></item><item><title>Self-Supervised Alignment with Mutual Information: Learning to Follow Principles without Preference Labels</title><link>https://deep-diver.github.io/neurips2024/posters/uvbpbehgaw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uvbpbehgaw/</guid><description>SAMI: Self-Supervised Alignment with Mutual Information, effectively teaches language models to follow principles without human preference labels by maximizing the mutual information between principle&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uvbpbehgaw/cover.png"/></item><item><title>Self-Taught Recognizer: Toward Unsupervised Adaptation for Speech Foundation Models</title><link>https://deep-diver.github.io/neurips2024/posters/oloqhrbxye/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oloqhrbxye/</guid><description>STAR, a novel unsupervised adaptation framework, drastically improves automatic speech recognition (ASR) robustness across diverse domains using only unlabeled data and outperforms existing self-train&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oloqhrbxye/cover.png"/></item><item><title>SelfCodeAlign: Self-Alignment for Code Generation</title><link>https://deep-diver.github.io/neurips2024/posters/xxrnuu7xtl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xxrnuu7xtl/</guid><description>SelfCodeAlign is a novel self-alignment method for code generation LLMs that surpasses existing methods by avoiding reliance on expensive human annotation or proprietary LLMs. The method achieves thi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xxrnuu7xtl/cover.png"/></item><item><title>Semantic Routing via Autoregressive Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/jvlrufjmbi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jvlrufjmbi/</guid><description>Learning-based semantic routing, a scalable approach to route planning using rich user queries, is introduced, accompanied by a large-scale public benchmark and a proof-of-concept model demonstrating &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jvlrufjmbi/cover.png"/></item><item><title>Semantics and Spatiality of Emergent Communication</title><link>https://deep-diver.github.io/neurips2024/posters/me1mpmenpw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/me1mpmenpw/</guid><description>Emergent communication protocols are surprisingly inconsistent; this paper proves reconstruction-based objectives yield semantically consistent protocols, unlike discrimination-based ones, highlightin&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/me1mpmenpw/cover.png"/></item><item><title>SemCoder: Training Code Language Models with Comprehensive Semantics Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/pnlchqrm69/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pnlchqrm69/</guid><description>SEMCODER: A novel 6.7B parameter code LLM surpasses GPT-3.5-turbo&amp;rsquo;s performance on code generation and execution reasoning by employing &amp;lsquo;monologue reasoning&amp;rsquo;âtraining the model to verbally explain cod&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pnlchqrm69/cover.png"/></item><item><title>Separations in the Representational Capabilities of Transformers and Recurrent Architectures</title><link>https://deep-diver.github.io/neurips2024/posters/6hujod3wtj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/6hujod3wtj/</guid><description>Transformers and RNNs show contrasting representational capabilities: Transformers excel at tasks requiring associative recall, while RNNs are better suited for hierarchical language processing. This &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/6hujod3wtj/cover.png"/></item><item><title>SGLang: Efficient Execution of Structured Language Model Programs</title><link>https://deep-diver.github.io/neurips2024/posters/vqkakqibpq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vqkakqibpq/</guid><description>SGLang: A new system boosts LLM program execution speed by up to 6.4x, simplifying complex LLM application programming.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vqkakqibpq/cover.png"/></item><item><title>SHED: Shapley-Based Automated Dataset Refinement for Instruction Fine-Tuning</title><link>https://deep-diver.github.io/neurips2024/posters/gqou8prgwq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gqou8prgwq/</guid><description>SHED, a Shapley value-based framework, efficiently refines instruction-tuning datasets for LLMs, producing high-performing subsets, only 10% of original size, that transfer well across different model&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gqou8prgwq/cover.png"/></item><item><title>ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization</title><link>https://deep-diver.github.io/neurips2024/posters/jnl6h3u3ow/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jnl6h3u3ow/</guid><description>ShiftAddLLM accelerates pretrained LLMs via post-training, multiplication-less reparameterization, achieving significant memory and energy reductions with comparable or better accuracy than existing m&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jnl6h3u3ow/cover.png"/></item><item><title>Should We Really Edit Language Models? On the Evaluation of Edited Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/m0ds4oomsy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/m0ds4oomsy/</guid><description>Language model editing&amp;rsquo;s limitations exposed: Scaling current methods leads to knowledge loss and compromised safety, urging research into more robust techniques.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/m0ds4oomsy/cover.png"/></item><item><title>SILENCE: Protecting privacy in offloaded speech understanding on resource-constrained devices</title><link>https://deep-diver.github.io/neurips2024/posters/tkulgndwwn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tkulgndwwn/</guid><description>SILENCE, a novel lightweight system, protects user privacy in offloaded speech understanding on resource-constrained devices by selectively masking short-term audio details without impacting long-term&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tkulgndwwn/cover.png"/></item><item><title>Simple and Effective Masked Diffusion Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/l4uaar4arm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/l4uaar4arm/</guid><description>Simple masked discrete diffusion models achieve state-of-the-art language modeling results, closing the performance gap with autoregressive methods by using a novel training recipe and a Rao-Blackwell&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/l4uaar4arm/cover.png"/></item><item><title>Simplified and Generalized Masked Diffusion for Discrete Data</title><link>https://deep-diver.github.io/neurips2024/posters/xcqsofht4g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xcqsofht4g/</guid><description>Simplified and generalized masked diffusion models achieve state-of-the-art results in discrete data generation, surpassing previous methods in text and image modeling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xcqsofht4g/cover.png"/></item><item><title>SimPO: Simple Preference Optimization with a Reference-Free Reward</title><link>https://deep-diver.github.io/neurips2024/posters/3tzcot1lkb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3tzcot1lkb/</guid><description>SimPO: a simpler, reference-free reward algorithm significantly outperforming existing offline preference optimization methods, achieving higher accuracy and efficiency in aligning LLMs with human pre&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3tzcot1lkb/cover.png"/></item><item><title>SIRIUS : Contexual Sparisty with Correction for Efficient LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/5br2l1b2eh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/5br2l1b2eh/</guid><description>SIRIUS: A novel correction mechanism boosts the efficiency of contextually sparse LLMs for complex reasoning tasks, achieving significant latency reduction.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/5br2l1b2eh/cover.png"/></item><item><title>SLED: Self Logits Evolution Decoding for Improving Factuality in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/t7wvjstsiv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/t7wvjstsiv/</guid><description>Self Logits Evolution Decoding (SLED) boosts LLM factuality by up to 20% without extra data or fine-tuning!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/t7wvjstsiv/cover.png"/></item><item><title>SlimGPT: Layer-wise Structured Pruning for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/mxf0ikjtkw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mxf0ikjtkw/</guid><description>SlimGPT: Achieve near-optimal LLM structured pruning via Batched Greedy Pruning and Incremental Pruning Ratio, improving efficiency without sacrificing accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mxf0ikjtkw/cover.png"/></item><item><title>SlowFocus: Enhancing Fine-grained Temporal Understanding in Video LLM</title><link>https://deep-diver.github.io/neurips2024/posters/fokkndty5b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fokkndty5b/</guid><description>SlowFocus significantly improves fine-grained temporal understanding in video LLMs by using mixed-frequency sampling and a novel multi-frequency attention mechanism.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fokkndty5b/cover.png"/></item><item><title>SLTrain: a sparse plus low rank approach for parameter and memory efficient pretraining</title><link>https://deep-diver.github.io/neurips2024/posters/mxze4h7opg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mxze4h7opg/</guid><description>SLTrain: Sparsity+low-rank pretraining boosts LLM efficiency by up to 73% memory reduction without performance loss!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mxze4h7opg/cover.png"/></item><item><title>SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models</title><link>https://deep-diver.github.io/neurips2024/posters/k9iglmqpif/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/k9iglmqpif/</guid><description>SMALLTOLARGE (S2L) revolutionizes large language model (LLM) fine-tuning by using a small model to summarize training loss trajectories, enabling efficient data selection for larger models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/k9iglmqpif/cover.png"/></item><item><title>Smoothie: Label Free Language Model Routing</title><link>https://deep-diver.github.io/neurips2024/posters/ppswhsgqrp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ppswhsgqrp/</guid><description>SMOOTHIE: Label-free LLM routing achieves up to 10% accuracy gains by using a latent variable model to estimate LLM quality without labeled data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ppswhsgqrp/cover.png"/></item><item><title>SnapKV: LLM Knows What You are Looking for Before Generation</title><link>https://deep-diver.github.io/neurips2024/posters/poe54goq2l/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/poe54goq2l/</guid><description>SnapKV: Slashing LLM memory usage &amp;amp; boosting speed via smart KV cache compression!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/poe54goq2l/cover.png"/></item><item><title>Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space</title><link>https://deep-diver.github.io/neurips2024/posters/clxclpfarc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/clxclpfarc/</guid><description>Open-source LLMs are vulnerable to embedding space attacks, which efficiently bypass safety mechanisms and enable data extraction, even after unlearning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/clxclpfarc/cover.png"/></item><item><title>Soft-Label Integration for Robust Toxicity Classification</title><link>https://deep-diver.github.io/neurips2024/posters/iykhthixg1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/iykhthixg1/</guid><description>Boosting toxicity classification robustness, this paper introduces a novel bi-level optimization framework integrating crowdsourced soft-labels and GroupDRO to enhance resistance against out-of-distri&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/iykhthixg1/cover.png"/></item><item><title>Source Code Foundation Models are Transferable Binary Analysis Knowledge Bases</title><link>https://deep-diver.github.io/neurips2024/posters/qppvdzphsl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qppvdzphsl/</guid><description>ProRec, a novel framework, bridges the binary-source semantic gap by using a binary-source encoder-decoder model and LLMs, achieving significant improvements in zero-shot binary summarization and func&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qppvdzphsl/cover.png"/></item><item><title>SpaceByte: Towards Deleting Tokenization from Large Language Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/kee4iup20i/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kee4iup20i/</guid><description>SpaceByte: A novel byte-level decoder architecture achieving near-tokenized-model performance without tokenization!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kee4iup20i/cover.png"/></item><item><title>SparseLLM: Towards Global Pruning of Pre-trained Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/oxhyyhp4zb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oxhyyhp4zb/</guid><description>SparseLLM globally prunes large language models efficiently by decomposing the problem into manageable subproblems, achieving significant performance improvements, especially at high sparsity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oxhyyhp4zb/cover.png"/></item><item><title>Speaking Your Language: Spatial Relationships in Interpretable Emergent Communication</title><link>https://deep-diver.github.io/neurips2024/posters/vip8iwmzln/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vip8iwmzln/</guid><description>AI agents developed a communication system using spatial relationships, achieving over 90% accuracy in conveying relative positions of objects within a scene.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vip8iwmzln/cover.png"/></item><item><title>SpecExec: Massively Parallel Speculative Decoding For Interactive LLM Inference on Consumer Devices</title><link>https://deep-diver.github.io/neurips2024/posters/jahnsz9dvg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jahnsz9dvg/</guid><description>SpecExec achieves massively parallel speculative decoding, enabling interactive 50B+ parameter LLM inference on consumer devices at 4-6 tokens/second.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jahnsz9dvg/cover.png"/></item><item><title>Spectral Adapter: Fine-Tuning in Spectral Space</title><link>https://deep-diver.github.io/neurips2024/posters/uoxuaogv6b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uoxuaogv6b/</guid><description>Spectral Adapter boosts parameter-efficient fine-tuning by incorporating pretrained weight matrices&amp;rsquo; spectral information, enhancing efficiency and multi-adapter fusion.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uoxuaogv6b/cover.png"/></item><item><title>Spectral Editing of Activations for Large Language Model Alignment</title><link>https://deep-diver.github.io/neurips2024/posters/pqyceea87j/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pqyceea87j/</guid><description>Spectral Editing of Activations (SEA) improves large language model truthfulness and fairness by projecting input representations to maximize covariance with positive demonstrations while minimizing c&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pqyceea87j/cover.png"/></item><item><title>Speculative Decoding with CTC-based Draft Model for LLM Inference Acceleration</title><link>https://deep-diver.github.io/neurips2024/posters/pgeacyhnn5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pgeacyhnn5/</guid><description>Boosting LLM inference speed, a CTC-based draft model significantly improves speculative decoding&amp;rsquo;s acceptance rate, leading to faster inference.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pgeacyhnn5/cover.png"/></item><item><title>SpeechAlign: Aligning Speech Generation to Human Preferences</title><link>https://deep-diver.github.io/neurips2024/posters/skcbzr8pyd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/skcbzr8pyd/</guid><description>SpeechAlign: Iteratively aligning speech generation models to human preferences via preference optimization, bridging distribution gaps for improved speech quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/skcbzr8pyd/cover.png"/></item><item><title>SpeedLoader: An I/O efficient scheme for heterogeneous and distributed LLM operation</title><link>https://deep-diver.github.io/neurips2024/posters/y2i0fy4sm7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y2i0fy4sm7/</guid><description>SpeedLoader: A groundbreaking I/O efficient scheme dramatically boosts LLM training &amp;amp; inference speed on diverse hardware, even with limited resources!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y2i0fy4sm7/cover.png"/></item><item><title>SpikedAttention: Training-Free and Fully Spike-Driven Transformer-to-SNN Conversion with Winner-Oriented Spike Shift for Softmax Operation</title><link>https://deep-diver.github.io/neurips2024/posters/fs28jccjj5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fs28jccjj5/</guid><description>SpikedAttention: Training-free transformer-to-SNN conversion achieving state-of-the-art accuracy and 42% energy reduction!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fs28jccjj5/cover.png"/></item><item><title>SS1: Accelerating Inference with Fast and Expressive Sketch Structured Transform</title><link>https://deep-diver.github.io/neurips2024/posters/nrgyogu7zp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nrgyogu7zp/</guid><description>SS1: A novel GPU-friendly operator accelerates deep learning inference by leveraging structured parameter sharing, achieving superior quality-efficiency tradeoffs compared to existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nrgyogu7zp/cover.png"/></item><item><title>SSDM: Scalable Speech Dysfluency Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/ixehb4ncvy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ixehb4ncvy/</guid><description>SSDM: Scalable Speech Dysfluency Modeling tackles challenges in speech dysfluency analysis by using articulatory gestures for scalable alignment, a connectionist subsequence aligner for efficient dysf&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ixehb4ncvy/cover.png"/></item><item><title>Star-Agents: Automatic Data Optimization with LLM Agents for Instruction Tuning</title><link>https://deep-diver.github.io/neurips2024/posters/jxsxgt80sv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jxsxgt80sv/</guid><description>Star-Agents automates data optimization for instruction-tuned LLMs via multi-agent collaboration, achieving a 12% average performance boost.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jxsxgt80sv/cover.png"/></item><item><title>Stealth edits to large language models</title><link>https://deep-diver.github.io/neurips2024/posters/qap6ryyijc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qap6ryyijc/</guid><description>Researchers unveil stealth edits for large language models, offering a new metric to assess editability and reveal vulnerability to malicious attacks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qap6ryyijc/cover.png"/></item><item><title>StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving</title><link>https://deep-diver.github.io/neurips2024/posters/ukxjd64mki/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ukxjd64mki/</guid><description>StrategyLLM uses four LLM agents to generate consistent, generalizable few-shot prompts, significantly improving LLM problem-solving performance across various tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ukxjd64mki/cover.png"/></item><item><title>Stratified Prediction-Powered Inference for Effective Hybrid Evaluation of Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/8cbcddqfdq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8cbcddqfdq/</guid><description>Stratified Prediction-Powered Inference (StratPPI) significantly improves language model evaluation by combining human and automated ratings, using stratified sampling for enhanced accuracy and tighte&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8cbcddqfdq/cover.png"/></item><item><title>Streaming Long Video Understanding with Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/axx62cqjpa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/axx62cqjpa/</guid><description>VideoStreaming, a novel vision-language model, enables efficient and accurate understanding of arbitrarily long videos using a constant number of tokens via streaming encoding and adaptive memory sele&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/axx62cqjpa/cover.png"/></item><item><title>StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses</title><link>https://deep-diver.github.io/neurips2024/posters/envvjpx97o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/envvjpx97o/</guid><description>StreamingDialogue revolutionizes prolonged dialogue learning by compressing long contexts into conversational attention sinks, minimizing information loss and achieving a 4x speedup with 18x less memo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/envvjpx97o/cover.png"/></item><item><title>Stress-Testing Capability Elicitation With Password-Locked Models</title><link>https://deep-diver.github.io/neurips2024/posters/zzooqd6r1b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zzooqd6r1b/</guid><description>Fine-tuning, even on a single demonstration, effectively uncovers hidden LLM capabilities, surpassing simple prompting methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zzooqd6r1b/cover.png"/></item><item><title>Superposed Decoding: Multiple Generations from a Single Autoregressive Inference Pass</title><link>https://deep-diver.github.io/neurips2024/posters/ksokkhm9i7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ksokkhm9i7/</guid><description>Generate multiple text drafts from a single language model pass with Superposed Decoding, significantly boosting efficiency!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ksokkhm9i7/cover.png"/></item><item><title>SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors</title><link>https://deep-diver.github.io/neurips2024/posters/us0pwibzc0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/us0pwibzc0/</guid><description>SVFT: a novel parameter-efficient fine-tuning method achieves near full fine-tuning accuracy using only 0.006% to 0.25% of parameters, significantly outperforming existing techniques.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/us0pwibzc0/cover.png"/></item><item><title>SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention</title><link>https://deep-diver.github.io/neurips2024/posters/80ssl69gaz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/80ssl69gaz/</guid><description>SwitchHead: A novel MoE attention mechanism accelerates Transformers by significantly reducing computation and memory, matching baseline performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/80ssl69gaz/cover.png"/></item><item><title>SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents</title><link>https://deep-diver.github.io/neurips2024/posters/9y8zuo11eq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/9y8zuo11eq/</guid><description>SWT-Bench, a new benchmark, reveals that LLMs excel at generating tests for real-world bug fixes, surpassing dedicated test generation systems and significantly improving code-fix precision.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/9y8zuo11eq/cover.png"/></item><item><title>Symbolic Regression with a Learned Concept Library</title><link>https://deep-diver.github.io/neurips2024/posters/b7s4jjglvl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/b7s4jjglvl/</guid><description>LASR, a novel symbolic regression method, uses zero-shot LLM queries to discover and evolve abstract concepts, substantially outperforming state-of-the-art approaches and discovering a new LLM scaling&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/b7s4jjglvl/cover.png"/></item><item><title>Synatra: Turning Indirect Knowledge into Direct Demonstrations for Digital Agents at Scale</title><link>https://deep-diver.github.io/neurips2024/posters/kjnezwriqn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kjnezwriqn/</guid><description>Synatra synthesizes high-quality digital agent training data from online tutorials and web pages, significantly improving agent performance on complex web-based tasks at a fraction of the cost of huma&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kjnezwriqn/cover.png"/></item><item><title>Synthesize, Partition, then Adapt: Eliciting Diverse Samples from Foundation Models</title><link>https://deep-diver.github.io/neurips2024/posters/sp8whisnu9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sp8whisnu9/</guid><description>The Synthesize-Partition-Adapt (SPA) framework leverages synthetic data to generate diverse, high-quality responses from foundation models, enriching user experience.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sp8whisnu9/cover.png"/></item><item><title>Synthetic Programming Elicitation for Text-to-Code in Very Low-Resource Programming and Formal Languages</title><link>https://deep-diver.github.io/neurips2024/posters/kqpzfiwviu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kqpzfiwviu/</guid><description>LLMs struggle with very low-resource programming languages. SPEAC, a novel synthetic programming elicitation and compilation approach, uses an intermediate language to enable LLMs to generate syntact&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kqpzfiwviu/cover.png"/></item><item><title>TableRAG: Million-Token Table Understanding with Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/41lovpoco5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/41lovpoco5/</guid><description>TableRAG, a novel Retrieval-Augmented Generation framework, achieves state-of-the-art performance in large-scale table understanding by efficiently integrating schema and cell retrieval with language &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/41lovpoco5/cover.png"/></item><item><title>TAIA: Large Language Models are Out-of-Distribution Data Learners</title><link>https://deep-diver.github.io/neurips2024/posters/xxsme6ge1g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xxsme6ge1g/</guid><description>LLMs struggle with downstream tasks using mismatched data. TAIA, a novel inference-time method, solves this by selectively using only attention parameters during inference after training all parameter&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xxsme6ge1g/cover.png"/></item><item><title>Talking Heads: Understanding Inter-Layer Communication in Transformer Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/lusx0chtsl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lusx0chtsl/</guid><description>Transformer Language Models&amp;rsquo; (LMs) sensitivity to seemingly arbitrary prompt changes is explained by identifying low-rank communication channels between layers. By decomposing attention heads, resear&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lusx0chtsl/cover.png"/></item><item><title>Teach Better or Show Smarter? On Instructions and Exemplars in Automatic Prompt Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/idtojvwvnx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/idtojvwvnx/</guid><description>Smart prompt engineering is key to unlocking LLMs&amp;rsquo; full potential. This paper reveals that cleverly selecting examples (exemplar optimization) can outperform optimizing instructions alone, even with S&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/idtojvwvnx/cover.png"/></item><item><title>Temporal Sentence Grounding with Relevance Feedback in Videos</title><link>https://deep-diver.github.io/neurips2024/posters/eoonmxzzno/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/eoonmxzzno/</guid><description>RaTSG network tackles Temporal Sentence Grounding with Relevance Feedback (TSG-RF) by discerning query relevance at multiple granularities before selectively grounding segments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/eoonmxzzno/cover.png"/></item><item><title>Text2NKG: Fine-Grained N-ary Relation Extraction for N-ary relational Knowledge Graph Construction</title><link>https://deep-diver.github.io/neurips2024/posters/v2mbwyxp63/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v2mbwyxp63/</guid><description>Text2NKG: a novel framework for building N-ary relational knowledge graphs by performing fine-grained n-ary relation extraction, supporting multiple schemas, and achieving state-of-the-art accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v2mbwyxp63/cover.png"/></item><item><title>The Closeness of In-Context Learning and Weight Shifting for Softmax Regression</title><link>https://deep-diver.github.io/neurips2024/posters/sfaeenfeyw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sfaeenfeyw/</guid><description>Softmax regression reveals in-context learning&amp;rsquo;s surprising similarity to gradient descent in self-attention Transformers, showing the models&amp;rsquo; remarkable learning capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sfaeenfeyw/cover.png"/></item><item><title>The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains</title><link>https://deep-diver.github.io/neurips2024/posters/qart6qtiqj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qart6qtiqj/</guid><description>Transformers learn to perform in-context learning of Markov chains hierarchically, progressing from simpler unigram strategies to more complex bigram solutions, with the presence of simpler solutions &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qart6qtiqj/cover.png"/></item><item><title>The Expressive Capacity of State Space Models: A Formal Language Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/ev5yirjpdy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ev5yirjpdy/</guid><description>State-space models (SSMs) rival transformers in language modeling, but their capabilities remain unclear; this paper rigorously analyzes SSM expressivity, revealing unique strengths and limitations, i&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ev5yirjpdy/cover.png"/></item><item><title>The Factorization Curse: Which Tokens You Predict Underlie the Reversal Curse and More</title><link>https://deep-diver.github.io/neurips2024/posters/f70e6yyfhf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/f70e6yyfhf/</guid><description>Large language models (LLMs) struggle with factual inconsistencies (&amp;lsquo;hallucinations&amp;rsquo;) and the &amp;lsquo;reversal curse,&amp;rsquo; where information recall depends heavily on the input order. This work reframes the cur&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/f70e6yyfhf/cover.png"/></item><item><title>The Fine-Grained Complexity of Gradient Computation for Training Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/up4twnwrol/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/up4twnwrol/</guid><description>New research precisely defines the computational limits of training large language models, revealing a sharp threshold based on parameter matrix entries, paving the way for faster algorithms.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/up4twnwrol/cover.png"/></item><item><title>The Impact of Initialization on LoRA Finetuning Dynamics</title><link>https://deep-diver.github.io/neurips2024/posters/sn3uryritk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sn3uryritk/</guid><description>LoRA&amp;rsquo;s initialization significantly impacts finetuning; initializing matrix A randomly and B to zero yields better performance than vice-versa due to enabling larger learning rates.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sn3uryritk/cover.png"/></item><item><title>The Importance of Online Data: Understanding Preference Fine-tuning via Coverage</title><link>https://deep-diver.github.io/neurips2024/posters/hbj86rmdz8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hbj86rmdz8/</guid><description>Hybrid Preference Optimization (HyPO) outperforms existing offline methods for fine-tuning LLMs by leveraging both offline and online data, achieving better performance and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hbj86rmdz8/cover.png"/></item><item><title>The Mamba in the Llama: Distilling and Accelerating Hybrid Models</title><link>https://deep-diver.github.io/neurips2024/posters/uazhodjalu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uazhodjalu/</guid><description>This research dramatically accelerates and improves hybrid language models by distilling large Transformers into linear RNNs, achieving performance comparable to the original Transformer with signific&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uazhodjalu/cover.png"/></item><item><title>The Representation Landscape of Few-Shot Learning and Fine-Tuning in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/nmukwoohfo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nmukwoohfo/</guid><description>LLMs use different internal structures for few-shot learning and fine-tuning, showing a transition in the middle network layers that impacts information encoding and task solving strategies.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nmukwoohfo/cover.png"/></item><item><title>Thinking Forward: Memory-Efficient Federated Finetuning of Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/dgqtja9x2c/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dgqtja9x2c/</guid><description>SPRY: A memory-efficient federated learning algorithm for finetuning LLMs on resource-constrained devices, achieving high accuracy and speed.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dgqtja9x2c/cover.png"/></item><item><title>Thought of Search: Planning with Language Models Through The Lens of Efficiency</title><link>https://deep-diver.github.io/neurips2024/posters/lncsya5us1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lncsya5us1/</guid><description>This paper introduces &amp;lsquo;Thought of Search,&amp;rsquo; a novel, efficient planning approach using LLMs that prioritizes soundness and completeness. It leverages LLMs to generate Python code for search components,&amp;hellip;</description></item><item><title>To Believe or Not to Believe Your LLM: IterativePrompting for Estimating Epistemic Uncertainty</title><link>https://deep-diver.github.io/neurips2024/posters/k6iyufwdi9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/k6iyufwdi9/</guid><description>This paper introduces an innovative iterative prompting method for estimating epistemic uncertainty in LLMs, enabling reliable detection of hallucinations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/k6iyufwdi9/cover.png"/></item><item><title>Token Merging for Training-Free Semantic Binding in Text-to-Image Synthesis</title><link>https://deep-diver.github.io/neurips2024/posters/trrwoa9e80/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/trrwoa9e80/</guid><description>ToMe: a novel training-free method dramatically improves semantic binding in text-to-image synthesis by intelligently merging related tokens, ensuring accurate alignment between generated images and t&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/trrwoa9e80/cover.png"/></item><item><title>Toward Efficient Inference for Mixture of Experts</title><link>https://deep-diver.github.io/neurips2024/posters/stxtbqytwx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/stxtbqytwx/</guid><description>Unlocking the speed and efficiency of Mixture-of-Expert models, this research unveils novel optimization techniques, achieving dramatic improvements in inference throughput and resource usage.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/stxtbqytwx/cover.png"/></item><item><title>Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing</title><link>https://deep-diver.github.io/neurips2024/posters/tpdj2qhkob/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tpdj2qhkob/</guid><description>ALPHALLM boosts LLM performance in complex reasoning tasks by using imagination, search, and criticism to create a self-improving loop, eliminating the need for extra training data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tpdj2qhkob/cover.png"/></item><item><title>Towards a Theoretical Understanding of the 'Reversal Curse' via Training Dynamics</title><link>https://deep-diver.github.io/neurips2024/posters/qowf3lo6m7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qowf3lo6m7/</guid><description>LLMs struggle with simple logical reasoning due to the &amp;lsquo;reversal curse.&amp;rsquo; This paper reveals that weight asymmetry during training is the culprit, offering a new theoretical perspective and potential s&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qowf3lo6m7/cover.png"/></item><item><title>Towards a theory of how the structure of language is acquired by deep neural networks</title><link>https://deep-diver.github.io/neurips2024/posters/nacxcukihh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nacxcukihh/</guid><description>Deep learning models learn language structure through next-token prediction, but the data requirements remain unclear. This paper reveals that the effective context window, determining learning capaci&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nacxcukihh/cover.png"/></item><item><title>Towards Neuron Attributions in Multi-Modal Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/jmjvfp4bh6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jmjvfp4bh6/</guid><description>NAM: a novel neuron attribution method for MLLMs, revealing modality-specific semantic knowledge and enabling multi-modal knowledge editing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jmjvfp4bh6/cover.png"/></item><item><title>Towards Robust Multimodal Sentiment Analysis with Incomplete Data</title><link>https://deep-diver.github.io/neurips2024/posters/myejc7qgra/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/myejc7qgra/</guid><description>Robust Multimodal Sentiment Analysis (MSA) model, Language-dominated Noise-resistant Learning Network (LNLN), handles incomplete data by correcting dominant modality (language) and using a multimodal &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/myejc7qgra/cover.png"/></item><item><title>Towards Understanding How Transformers Learn In-context Through a Representation Learning Lens</title><link>https://deep-diver.github.io/neurips2024/posters/db6gwsdxkl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/db6gwsdxkl/</guid><description>Transformers&amp;rsquo; in-context learning (ICL) is explained using representation learning, revealing its ICL process as gradient descent on a dual model and offering modifiable attention layers for enhanced &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/db6gwsdxkl/cover.png"/></item><item><title>Train-Attention: Meta-Learning Where to Focus in Continual Knowledge Learning</title><link>https://deep-diver.github.io/neurips2024/posters/pwldvyimrf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pwldvyimrf/</guid><description>Train-Attention (TAALM) tackles catastrophic forgetting in LLMs by dynamically weighting tokens during training, boosting learning efficiency and knowledge retention, outperforming existing methods on&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pwldvyimrf/cover.png"/></item><item><title>Training Dynamics of Transformers to Recognize Word Co-occurrence via Gradient Flow Analysis</title><link>https://deep-diver.github.io/neurips2024/posters/w6q46islsr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w6q46islsr/</guid><description>Researchers reveal how transformers learn word co-occurrence using a novel gradient flow analysis, uncovering a two-phase training process that leads to near-minimum loss and improved model performanc&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w6q46islsr/cover.png"/></item><item><title>Transformers Can Do Arithmetic with the Right Embeddings</title><link>https://deep-diver.github.io/neurips2024/posters/aiynlwxudo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/aiynlwxudo/</guid><description>Researchers enhanced transformer performance on arithmetic tasks by introducing Abacus Embeddings, which encode each digit&amp;rsquo;s position, enabling improved generalization and unlocking multi-step reasoni&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/aiynlwxudo/cover.png"/></item><item><title>Transformers need glasses! Information over-squashing in language tasks</title><link>https://deep-diver.github.io/neurips2024/posters/93hce8vtye/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/93hce8vtye/</guid><description>Large language models (LLMs) suffer from information loss due to representational collapse and over-squashing, causing failures in simple tasks; this paper provides theoretical analysis and practical &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/93hce8vtye/cover.png"/></item><item><title>Transformers Represent Belief State Geometry in their Residual Stream</title><link>https://deep-diver.github.io/neurips2024/posters/yib7rel8uc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yib7rel8uc/</guid><description>Transformers encode information beyond next-token prediction by linearly representing belief state geometry in their residual stream, even with complex fractal structures.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yib7rel8uc/cover.png"/></item><item><title>Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models</title><link>https://deep-diver.github.io/neurips2024/posters/fjlrszbmcd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fjlrszbmcd/</guid><description>MOHAWK: Distilling Transformers&amp;rsquo; quadratic knowledge into faster subquadratic SSMs, achieving state-of-the-art performance with &amp;lt;1% of training data!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fjlrszbmcd/cover.png"/></item><item><title>TransVIP: Speech to Speech Translation System with Voice and Isochrony Preservation</title><link>https://deep-diver.github.io/neurips2024/posters/zpvtrqvx5b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zpvtrqvx5b/</guid><description>TransVIP: groundbreaking speech-to-speech translation system preserving voice &amp;amp; isochrony, outperforming current state-of-the-art models!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zpvtrqvx5b/cover.png"/></item><item><title>Tree of Attacks: Jailbreaking Black-Box LLMs Automatically</title><link>https://deep-diver.github.io/neurips2024/posters/som3vngoh5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/som3vngoh5/</guid><description>TAP: automated jailbreaking of black-box LLMs with high success rates, using fewer queries than previous methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/som3vngoh5/cover.png"/></item><item><title>Truth is Universal: Robust Detection of Lies in LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/1fc2xa2cdk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1fc2xa2cdk/</guid><description>LLM lie detectors fail to generalize; this paper presents a robust method achieving 94% accuracy by identifying a universal two-dimensional truth subspace, separating true/false statements across vari&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1fc2xa2cdk/cover.png"/></item><item><title>TSDS: Data Selection for Task-Specific Model Finetuning</title><link>https://deep-diver.github.io/neurips2024/posters/wjbthluszu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wjbthluszu/</guid><description>TSDS: A novel framework selects optimal training data for efficient large language model finetuning using only a few examples, boosting performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wjbthluszu/cover.png"/></item><item><title>Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging</title><link>https://deep-diver.github.io/neurips2024/posters/81yit63ttn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/81yit63ttn/</guid><description>Twin-Merging dynamically merges modular model expertise, significantly improving multitask performance without retraining, and adapting to diverse data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/81yit63ttn/cover.png"/></item><item><title>Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/cvpuve1n22/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cvpuve1n22/</guid><description>Uncertainty of Thoughts (UoT) algorithm significantly boosts LLMs&amp;rsquo; information-seeking abilities, leading to substantial performance gains across diverse tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cvpuve1n22/cover.png"/></item><item><title>Unchosen Experts Can Contribute Too: Unleashing MoE Modelsâ Power by Self-Contrast</title><link>https://deep-diver.github.io/neurips2024/posters/c1d3vvfdvg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/c1d3vvfdvg/</guid><description>Self-Contrast Mixture-of-Experts (SCMoE) boosts MoE model reasoning by cleverly using &amp;lsquo;unchosen&amp;rsquo; experts during inference. This training-free method contrasts outputs from strong and weak expert acti&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/c1d3vvfdvg/cover.png"/></item><item><title>Uncovering Safety Risks of Large Language Models through Concept Activation Vector</title><link>https://deep-diver.github.io/neurips2024/posters/uymv9thb50/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uymv9thb50/</guid><description>Researchers developed SCAV, a novel framework to effectively reveal safety risks in LLMs by accurately interpreting their safety mechanisms. SCAV-guided attacks significantly improve attack success r&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uymv9thb50/cover.png"/></item><item><title>Understanding and Minimising Outlier Features in Transformer Training</title><link>https://deep-diver.github.io/neurips2024/posters/npjq6qs4bg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/npjq6qs4bg/</guid><description>New methods minimize outlier features in transformer training, improving quantization and efficiency without sacrificing convergence speed.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/npjq6qs4bg/cover.png"/></item><item><title>Understanding Emergent Abilities of Language Models from the Loss Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/35daviqmfo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/35daviqmfo/</guid><description>Language model emergent abilities aren&amp;rsquo;t exclusive to large models; they emerge when pre-training loss falls below a threshold, irrespective of model or data size.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/35daviqmfo/cover.png"/></item><item><title>Understanding Information Storage and Transfer in Multi-Modal Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/s63dtq0mwa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s63dtq0mwa/</guid><description>Researchers unveil how multi-modal LLMs process information, revealing that early layers are key for storage, and introduce MULTEDIT, a model-editing algorithm for correcting errors and inserting new &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/s63dtq0mwa/cover.png"/></item><item><title>Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/1v4gksygfe/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1v4gksygfe/</guid><description>Linear probing then fine-tuning (LP-FT) significantly improves language model fine-tuning; this paper uses Neural Tangent Kernel (NTK) theory to explain why.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1v4gksygfe/cover.png"/></item><item><title>Understanding Scaling Laws with Statistical and Approximation Theory for Transformer Neural Networks on Intrinsically Low-dimensional Data</title><link>https://deep-diver.github.io/neurips2024/posters/n2wypmpifa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/n2wypmpifa/</guid><description>Deep learning scaling laws are explained by novel approximation and estimation theories for transformers on low-dimensional data, resolving discrepancies between theory and practice.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/n2wypmpifa/cover.png"/></item><item><title>Understanding the Differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/if7mnxnxrw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/if7mnxnxrw/</guid><description>Unifying framework reveals hidden connections between attention, recurrent, and state-space models, boosting foundation model efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/if7mnxnxrw/cover.png"/></item><item><title>Understanding Transformer Reasoning Capabilities via Graph Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/afzbdw6dsp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/afzbdw6dsp/</guid><description>Transformers excel at graph reasoning, with logarithmic depth proving necessary and sufficient for parallelizable tasks; single-layer transformers solve retrieval tasks efficiently.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/afzbdw6dsp/cover.png"/></item><item><title>Understanding Transformers via N-Gram Statistics</title><link>https://deep-diver.github.io/neurips2024/posters/wcc440cuhx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wcc440cuhx/</guid><description>LLMs&amp;rsquo; inner workings remain elusive. This study uses N-gram statistics to approximate transformer predictions, revealing how LLMs learn from simple to complex statistical rules, and how model variance&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wcc440cuhx/cover.png"/></item><item><title>UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and FFN Manipulation</title><link>https://deep-diver.github.io/neurips2024/posters/luqivmnvix/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/luqivmnvix/</guid><description>UniBias unveils and mitigates LLM bias by identifying and eliminating biased internal components (FFN vectors and attention heads), significantly improving in-context learning performance and robustne&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/luqivmnvix/cover.png"/></item><item><title>Universal In-Context Approximation By Prompting Fully Recurrent Models</title><link>https://deep-diver.github.io/neurips2024/posters/gproasyzk5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gproasyzk5/</guid><description>Fully recurrent neural networks can be universal in-context approximators, achieving the same capabilities as transformer models by cleverly using prompts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gproasyzk5/cover.png"/></item><item><title>Unleashing Region Understanding in Intermediate Layers for MLLM-based Referring Expression Generation</title><link>https://deep-diver.github.io/neurips2024/posters/168nlztpw8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/168nlztpw8/</guid><description>Unlocking intermediate layers in MLLMs improves referring expression generation by enhancing accuracy and detail while reducing hallucinations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/168nlztpw8/cover.png"/></item><item><title>Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback</title><link>https://deep-diver.github.io/neurips2024/posters/jmbwtlazjw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jmbwtlazjw/</guid><description>This study disentangles best practices for learning from preference feedback in LLMs, revealing that data quality, algorithm choice, and reward model significantly impact performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jmbwtlazjw/cover.png"/></item><item><title>Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?</title><link>https://deep-diver.github.io/neurips2024/posters/1iu3p8vdbn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1iu3p8vdbn/</guid><description>LLMs struggle with genuine causal reasoning; new benchmark CausalProbe-2024 reveals limitations, and G2-Reasoner method improves causal reasoning by integrating general knowledge and goal-oriented pro&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1iu3p8vdbn/cover.png"/></item><item><title>Unveiling Induction Heads: Provable Training Dynamics and Feature Learning in Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/4fn2res0ma/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4fn2res0ma/</guid><description>Transformers learn complex tasks surprisingly well through in-context learning, but the mechanism remains unclear. This paper proves that a two-layer transformer trained on n-gram Markov chain data co&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4fn2res0ma/cover.png"/></item><item><title>Unveiling LoRA Intrinsic Ranks via Salience Analysis</title><link>https://deep-diver.github.io/neurips2024/posters/vu512k8vrr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vu512k8vrr/</guid><description>SalientLoRA unveils optimal LoRA ranks by analyzing rank salience via time-series analysis, improving fine-tuning efficiency and performance significantly.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vu512k8vrr/cover.png"/></item><item><title>UQE: A Query Engine for Unstructured Databases</title><link>https://deep-diver.github.io/neurips2024/posters/t7sgov5w5z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/t7sgov5w5z/</guid><description>UQE: A novel query engine uses LLMs for efficient and accurate unstructured data analytics, surpassing existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/t7sgov5w5z/cover.png"/></item><item><title>Vaccine: Perturbation-aware Alignment for Large Language Models against Harmful Fine-tuning Attack</title><link>https://deep-diver.github.io/neurips2024/posters/lpxdzkiant/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lpxdzkiant/</guid><description>Vaccine: a novel technique safeguards LLMs against harmful fine-tuning attacks by creating invariant hidden embeddings.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lpxdzkiant/cover.png"/></item><item><title>VB-LoRA: Extreme Parameter Efficient Fine-Tuning with Vector Banks</title><link>https://deep-diver.github.io/neurips2024/posters/kucy0mw4q3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kucy0mw4q3/</guid><description>VB-LoRA achieves extreme parameter efficiency in fine-tuning LLMs by sharing parameters globally via a vector bank, outperforming state-of-the-art PEFT methods while maintaining comparable or better p&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kucy0mw4q3/cover.png"/></item><item><title>VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections</title><link>https://deep-diver.github.io/neurips2024/posters/bfoqxd7uls/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bfoqxd7uls/</guid><description>VeLoRA: Train massive LLMs efficiently by compressing intermediate activations!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bfoqxd7uls/cover.png"/></item><item><title>Verified Code Transpilation with LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/spwe9slrfg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/spwe9slrfg/</guid><description>LLMLIFT: An LLM-powered approach builds verified lifting tools for DSLs, outperforming prior symbolic methods in benchmark transpilation and requiring less development effort.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/spwe9slrfg/cover.png"/></item><item><title>WAGLE: Strategic Weight Attribution for Effective and Modular Unlearning in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/vzogndjmgh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vzogndjmgh/</guid><description>WAGLE: A novel weight attribution-guided LLM unlearning framework boosts unlearning performance by strategically identifying and manipulating influential model weights, achieving a better balance betw&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vzogndjmgh/cover.png"/></item><item><title>Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents</title><link>https://deep-diver.github.io/neurips2024/posters/nf4mhf1pi5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nf4mhf1pi5/</guid><description>LLM-based agents are vulnerable to diverse backdoor attacks that manipulate their reasoning and outputs, highlighting the urgent need for targeted defenses.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nf4mhf1pi5/cover.png"/></item><item><title>WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off</title><link>https://deep-diver.github.io/neurips2024/posters/hjekhxk2vh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hjekhxk2vh/</guid><description>WaterMax: a novel LLM watermarking scheme achieving high detectability and preserving text quality by cleverly generating multiple texts and selecting the most suitable one.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hjekhxk2vh/cover.png"/></item><item><title>Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with Situation Puzzles</title><link>https://deep-diver.github.io/neurips2024/posters/h024lpf3bz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/h024lpf3bz/</guid><description>SPLAT, a new benchmark using situation puzzles, effectively evaluates and elicits lateral thinking in LLMs through a multi-turn player-judge framework, revealing significant performance improvements o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/h024lpf3bz/cover.png"/></item><item><title>Weak-to-Strong Search: Align Large Language Models via Searching over Small Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/doj6cqwdf1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/doj6cqwdf1/</guid><description>Align LLMs efficiently via test-time search using smaller models!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/doj6cqwdf1/cover.png"/></item><item><title>What Makes and Breaks Safety Fine-tuning? A Mechanistic Study</title><link>https://deep-diver.github.io/neurips2024/posters/jeflv4nrlh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jeflv4nrlh/</guid><description>Safety fine-tuning for LLMs is shown to minimally transform weights, clustering inputs based on safety, but is easily bypassed by adversarial attacks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jeflv4nrlh/cover.png"/></item><item><title>What Rotary Position Embedding Can Tell Us: Identifying Query and Key Weights Corresponding to Basic Syntactic or High-level Semantic Information</title><link>https://deep-diver.github.io/neurips2024/posters/e5mv7iwfvw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/e5mv7iwfvw/</guid><description>LLM fine-tuning made easy! This paper reveals how analyzing weight vector angles in RoPE positional embeddings helps optimize LLMs, reducing parameter count and improving efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/e5mv7iwfvw/cover.png"/></item><item><title>When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search</title><link>https://deep-diver.github.io/neurips2024/posters/fffcdndnol/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fffcdndnol/</guid><description>RLbreaker uses deep reinforcement learning to efficiently create highly effective jailbreaking prompts, outperforming existing methods against multiple state-of-the-art LLMs and defenses.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fffcdndnol/cover.png"/></item><item><title>Where does In-context Learning \ Happen in Large Language Models?</title><link>https://deep-diver.github.io/neurips2024/posters/llusjg59an/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/llusjg59an/</guid><description>LLMs learn tasks via in-context learning, but the task recognition location is unknown. This paper reveals that LLMs transition from task recognition to task performance at specific layers, enabling s&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/llusjg59an/cover.png"/></item><item><title>WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/vjmyofjvc2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vjmyofjvc2/</guid><description>WISE, a novel dual-memory architecture, solves the impossible triangle of reliability, generalization, and locality in lifelong LLM editing by employing a side memory for knowledge updates and a route&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vjmyofjvc2/cover.png"/></item><item><title>WizardArena: Post-training Large Language Models via Simulated Offline Chatbot Arena</title><link>https://deep-diver.github.io/neurips2024/posters/vhva3d836i/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vhva3d836i/</guid><description>WizardArena simulates offline chatbot arena battles to efficiently post-train LLMs, dramatically reducing costs and improving model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vhva3d836i/cover.png"/></item><item><title>WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment</title><link>https://deep-diver.github.io/neurips2024/posters/qgjsxmhval/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qgjsxmhval/</guid><description>WorldCoder: an LLM agent builds world models via code generation and interaction, proving highly sample-efficient and enabling knowledge transfer.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qgjsxmhval/cover.png"/></item><item><title>YouDream: Generating Anatomically Controllable Consistent Text-to-3D Animals</title><link>https://deep-diver.github.io/neurips2024/posters/rh7tfqhizy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rh7tfqhizy/</guid><description>YOUDREAM generates anatomically consistent, high-quality 3D animal models from text and 2D pose priors, pushing creative boundaries in text-to-3D generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rh7tfqhizy/cover.png"/></item><item><title>Zero-Shot Tokenizer Transfer</title><link>https://deep-diver.github.io/neurips2024/posters/rwbobrsizc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rwbobrsizc/</guid><description>Zero-Shot Tokenizer Transfer (ZeTT) detaches language models from their tokenizers via a hypernetwork, enabling efficient on-the-fly tokenizer swapping without retraining, significantly improving LLM &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rwbobrsizc/cover.png"/></item><item><title>ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification</title><link>https://deep-diver.github.io/neurips2024/posters/5t4zakpijs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/5t4zakpijs/</guid><description>ZipCache: Efficient KV cache quantization for LLMs using salient token identification, achieving 4.98x compression with minimal accuracy loss!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/5t4zakpijs/cover.png"/></item><item><title>Zipfian Whitening</title><link>https://deep-diver.github.io/neurips2024/posters/pasjxzmjb7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pasjxzmjb7/</guid><description>Zipfian Whitening: Weighting PCA whitening by word frequency dramatically improves NLP task performance, surpassing established baselines and providing a theoretical framework for existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pasjxzmjb7/cover.png"/></item></channel></rss>