<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Natural Language Processing on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/natural-language-processing/</link><description>Recent content in Natural Language Processing on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/natural-language-processing/index.xml" rel="self" type="application/rss+xml"/><item><title>$ extit{Read-ME}$: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design</title><link>https://deep-diver.github.io/neurips2024/posters/i8jaxy7tdi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/i8jaxy7tdi/</guid><description>Read-ME refactors pre-trained dense LLMs into efficient, router-decoupled Mixture-of-Experts (MoEs) via activation sparsity, achieving up to 10.1% improvement on MMLU and 6.1% reduction in latency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/i8jaxy7tdi/cover.png"/></item><item><title>$eta$-DPO: Direct Preference Optimization with Dynamic $eta$</title><link>https://deep-diver.github.io/neurips2024/posters/zfbuhze556/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zfbuhze556/</guid><description>β-DPO dynamically adjusts a key parameter in Direct Preference Optimization, significantly improving LLM alignment with human preferences.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zfbuhze556/cover.png"/></item><item><title>3-in-1: 2D Rotary Adaptation for Efficient Finetuning, Efficient Batching and Composability</title><link>https://deep-diver.github.io/neurips2024/posters/ryjywum6yh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ryjywum6yh/</guid><description>RoAd: a novel parameter-efficient finetuning method uses 2D rotation to adapt LLMs, enabling efficient batching, composability, and improved interpretability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ryjywum6yh/cover.png"/></item><item><title>A Full-duplex Speech Dialogue Scheme Based On Large Language Model</title><link>https://deep-diver.github.io/neurips2024/posters/yawxy6mwik/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yawxy6mwik/</guid><description>This paper introduces a novel full-duplex speech dialogue system based on LLMs, achieving significantly reduced response latency and higher interruption precision compared to half-duplex systems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yawxy6mwik/cover.png"/></item><item><title>A Gradient Accumulation Method for Dense Retriever under Memory Constraint</title><link>https://deep-diver.github.io/neurips2024/posters/qdg2q5myhv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qdg2q5myhv/</guid><description>CONTACCUM: Stable, efficient memory reduction for dense retrievers using dual memory banks, surpassing high-resource baselines.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qdg2q5myhv/cover.png"/></item><item><title>A Polar coordinate system represents syntax in large language models</title><link>https://deep-diver.github.io/neurips2024/posters/x2780vcmoi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x2780vcmoi/</guid><description>LLMs spontaneously encode syntax using a polar coordinate system, representing syntactic relations via relative direction and distance of word embeddings.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x2780vcmoi/cover.png"/></item><item><title>A Prompt-Based Knowledge Graph Foundation Model for Universal In-Context Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/vqyb9lkmuh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vqyb9lkmuh/</guid><description>KG-ICL, a novel prompt-based knowledge graph foundation model, achieves universal in-context reasoning by leveraging in-context learning and a unified tokenizer, outperforming various baselines on 43 &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vqyb9lkmuh/cover.png"/></item><item><title>A teacher-teacher framework for clinical language representation learning</title><link>https://deep-diver.github.io/neurips2024/posters/zdad8zv8tg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zdad8zv8tg/</guid><description>A lightweight knowledge alignment module enables two pre-trained LLMs to mutually learn and improve clinical language representation, exceeding individual model performance on various downstream tasks&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zdad8zv8tg/cover.png"/></item><item><title>A Theoretical Perspective for Speculative Decoding Algorithm</title><link>https://deep-diver.github.io/neurips2024/posters/wsqpnemvlu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wsqpnemvlu/</guid><description>This paper theoretically analyzes speculative decoding, revealing its optimality and providing formulas for expected rejections, paving the way for more efficient large language model inference.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wsqpnemvlu/cover.png"/></item><item><title>A Theoretical Understanding of Self-Correction through In-context Alignment</title><link>https://deep-diver.github.io/neurips2024/posters/otvnltwyww/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/otvnltwyww/</guid><description>LLMs improve through self-correction, but the mechanisms are unclear. This paper provides a theoretical framework and empirical evidence demonstrating that self-correction arises from in-context align&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/otvnltwyww/cover.png"/></item><item><title>Accelerating Blockwise Parallel Language Models with Draft Refinement</title><link>https://deep-diver.github.io/neurips2024/posters/kt6f5sw0eg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kt6f5sw0eg/</guid><description>Boost LLM inference speed by 3x! This paper refines blockwise parallel decoding (BPD) by cleverly refining draft predictions, resulting in faster text generation for large language models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kt6f5sw0eg/cover.png"/></item><item><title>Accelerating Greedy Coordinate Gradient and General Prompt Optimization via Probe Sampling</title><link>https://deep-diver.github.io/neurips2024/posters/cmgxaarqzh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cmgxaarqzh/</guid><description>Probe sampling accelerates Greedy Coordinate Gradient (GCG) and other prompt optimization methods by up to 5.6x, achieving equal or better attack success rates, making LLM safety research faster and m&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cmgxaarqzh/cover.png"/></item><item><title>Accuracy is Not All You Need</title><link>https://deep-diver.github.io/neurips2024/posters/qvg7j29sta/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qvg7j29sta/</guid><description>LLM compression accuracy hides crucial behavioral changes; use % flips and KL-divergence for better evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qvg7j29sta/cover.png"/></item><item><title>Adaptable Logical Control for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/58x9v92zrd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/58x9v92zrd/</guid><description>Ctrl-G: A neuro-symbolic framework enables adaptable control of LLM generation by combining any LLM with a Hidden Markov Model (HMM), ensuring outputs adhere to logical constraints specified as determ&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/58x9v92zrd/cover.png"/></item><item><title>Adaptive Layer Sparsity for Large Language Models via Activation Correlation Assessment</title><link>https://deep-diver.github.io/neurips2024/posters/jup0qzxh7u/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jup0qzxh7u/</guid><description>Adaptive Layer Sparsity (ALS) revolutionizes large language model (LLM) compression by intelligently pruning less important layers, achieving significant size reduction without performance loss. It o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jup0qzxh7u/cover.png"/></item><item><title>Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees</title><link>https://deep-diver.github.io/neurips2024/posters/zipdu0chyu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zipdu0chyu/</guid><description>TP-LLaMA boosts tool-augmented LLMs by optimizing inference trajectories using preference learning from both successful and failed attempts, achieving superior performance and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zipdu0chyu/cover.png"/></item><item><title>Adversarial Representation Engineering: A General Model Editing Framework for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/dq9ji8e9qq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dq9ji8e9qq/</guid><description>Adversarial Representation Engineering (ARE) offers a unified, interpretable approach for editing large language models (LLMs) by using a representation sensor as an editing oracle, enhancing model sa&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dq9ji8e9qq/cover.png"/></item><item><title>AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases</title><link>https://deep-diver.github.io/neurips2024/posters/y841brw9ry/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y841brw9ry/</guid><description>AGENTPOISON: A novel backdoor attack compromises LLM agents by poisoning their memory or knowledge bases, achieving high success rates with minimal performance impact.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y841brw9ry/cover.png"/></item><item><title>AlchemistCoder: Harmonizing and Eliciting Code Capability by Hindsight Tuning on Multi-source Data</title><link>https://deep-diver.github.io/neurips2024/posters/saqxbnvv4t/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/saqxbnvv4t/</guid><description>AlchemistCoder enhances code LLMs by pioneering hindsight tuning on multi-source data, harmonizing conflicting styles via AlchemistPrompts, and achieving state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/saqxbnvv4t/cover.png"/></item><item><title>ALI-Agent: Assessing LLMs' Alignment with Human Values via Agent-based Evaluation</title><link>https://deep-diver.github.io/neurips2024/posters/kzrfbtrpey/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kzrfbtrpey/</guid><description>ALI-Agent uses LLM-powered agents for in-depth, adaptive assessment of LLMs&amp;rsquo; alignment with human values, overcoming limitations of existing static benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kzrfbtrpey/cover.png"/></item><item><title>Aligning Large Language Models with Representation Editing: A Control Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/yttomsjssw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yttomsjssw/</guid><description>RE-Control: Aligning LLMs via dynamic representation editing using optimal control theory, achieving superior alignment with significantly fewer resources than fine-tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yttomsjssw/cover.png"/></item><item><title>Aligning LLM Agents by Learning Latent Preference from User Edits</title><link>https://deep-diver.github.io/neurips2024/posters/dlyngpcuwa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dlyngpcuwa/</guid><description>PRELUDE, a novel framework, leverages user edits of LLM outputs to learn latent preferences, improving agent alignment and minimizing edit costs. CIPHER, its efficient algorithm, infers preferences f&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dlyngpcuwa/cover.png"/></item><item><title>Aligning to Thousands of Preferences via System Message Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/recsheq7e8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/recsheq7e8/</guid><description>JANUS, a 7B LLM, achieves high alignment to thousands of user preferences by generalizing from diverse system messages, outperforming existing LLMs on various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/recsheq7e8/cover.png"/></item><item><title>Alignment at Pre-training! Towards Native Alignment for Arabic LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/worfmnjilp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/worfmnjilp/</guid><description>This study introduces &amp;rsquo;native alignment&amp;rsquo; for Arabic LLMs, achieving state-of-the-art results by aligning models during pre-training, rather than post-training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/worfmnjilp/cover.png"/></item><item><title>AlphaMath Almost Zero: Process Supervision without Process</title><link>https://deep-diver.github.io/neurips2024/posters/vaxnxq3uko/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vaxnxq3uko/</guid><description>AlphaMath: LLMs excel at math reasoning without human-annotated process supervision, using Monte Carlo Tree Search.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vaxnxq3uko/cover.png"/></item><item><title>AlphaPruning: Using Heavy-Tailed Self Regularization Theory for Improved Layer-wise Pruning of Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/fhq4x2yxvv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fhq4x2yxvv/</guid><description>AlphaPruning leverages Heavy-Tailed Self-Regularization theory to allocate optimal layer-wise sparsity ratios in LLMs, achieving 80% sparsity in LLaMA-7B with reasonable perplexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fhq4x2yxvv/cover.png"/></item><item><title>ALPINE: Unveiling The Planning Capability of Autoregressive Learning in Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/wfbzusv14e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wfbzusv14e/</guid><description>ALPINE reveals how Transformer-based LLMs learn planning by embedding graph information into their weights, but also highlights their inability to handle transitive relationships.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wfbzusv14e/cover.png"/></item><item><title>AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback</title><link>https://deep-diver.github.io/neurips2024/posters/jimxgqemx3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jimxgqemx3/</guid><description>AMOR: Adaptable Modular knowledge agent using LLMs, excels with FSM-based reasoning and process feedback, enabling human supervision and domain adaptation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jimxgqemx3/cover.png"/></item><item><title>An Efficient Recipe for Long Context Extension via Middle-Focused Positional Encoding</title><link>https://deep-diver.github.io/neurips2024/posters/anheqfms0n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/anheqfms0n/</guid><description>Extend LLMs context via a simple, training-efficient positional encoding method, CREAM, outperforming existing methods by focusing on crucial mid-context information.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/anheqfms0n/cover.png"/></item><item><title>Analysing the Generalisation and Reliability of Steering Vectors</title><link>https://deep-diver.github.io/neurips2024/posters/v8x70gtodr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v8x70gtodr/</guid><description>Steering vectors, while promising for controlling LLMs, show unreliable in- and out-of-distribution performance, highlighting crucial limitations for real-world applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v8x70gtodr/cover.png"/></item><item><title>Analyzing &amp; Reducing the Need for Learning Rate Warmup in GPT Training</title><link>https://deep-diver.github.io/neurips2024/posters/zgdnrps46k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zgdnrps46k/</guid><description>This study reveals that modifying optimizers to normalize updates based on angular changes and gradient signal-to-noise ratio significantly reduces the need for learning rate warmup in GPT training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zgdnrps46k/cover.png"/></item><item><title>AP-Adapter: Improving Generalization of Automatic Prompts on Unseen Text-to-Image Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/posters/46v9axmouu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/46v9axmouu/</guid><description>AP-Adapter boosts text-to-image diffusion model generalization by using a two-stage prompt optimization method that leverages large language models and inter-model differences.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/46v9axmouu/cover.png"/></item><item><title>Are More LLM Calls All You Need? Towards the Scaling Properties of Compound AI Systems</title><link>https://deep-diver.github.io/neurips2024/posters/m5106rrlgx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/m5106rrlgx/</guid><description>More LM calls don&amp;rsquo;t always mean better results for compound AI; this study reveals performance can initially increase then decrease, highlighting the importance of optimal call number prediction.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/m5106rrlgx/cover.png"/></item><item><title>ArkVale: Efficient Generative LLM Inference with Recallable Key-Value Eviction</title><link>https://deep-diver.github.io/neurips2024/posters/4oat5l4lye/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4oat5l4lye/</guid><description>ARKVALE boosts LLM inference efficiency by intelligently evicting and recalling key-value pairs from cache, improving latency and throughput without significant accuracy loss.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4oat5l4lye/cover.png"/></item><item><title>AutoGuide: Automated Generation and Selection of Context-Aware Guidelines for Large Language Model Agents</title><link>https://deep-diver.github.io/neurips2024/posters/mriqz8zd6o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mriqz8zd6o/</guid><description>AutoGuide: Automated generation of context-aware guidelines significantly improves LLM agent performance in unfamiliar domains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mriqz8zd6o/cover.png"/></item><item><title>AutoManual: Generating Instruction Manuals by LLM Agents via Interactive Environmental Learning</title><link>https://deep-diver.github.io/neurips2024/posters/pwl9n4zlf5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pwl9n4zlf5/</guid><description>LLM agents can now autonomously build environmental understanding via interactive learning, generating human-readable instruction manuals that boost task success rates.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pwl9n4zlf5/cover.png"/></item><item><title>AutoMix: Automatically Mixing Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/e6wrwivgzx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/e6wrwivgzx/</guid><description>AutoMix intelligently routes queries to different-sized LLMs based on a smaller model&amp;rsquo;s self-verification, minimizing cost while maintaining performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/e6wrwivgzx/cover.png"/></item><item><title>AutoPSV: Automated Process-Supervised Verifier</title><link>https://deep-diver.github.io/neurips2024/posters/eoapwwogs9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/eoapwwogs9/</guid><description>AutoPSV automates process annotation for LLMs, improving reasoning by detecting confidence shifts in reasoning steps, thus efficiently enhancing model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/eoapwwogs9/cover.png"/></item><item><title>B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory</title><link>https://deep-diver.github.io/neurips2024/posters/rnqdry1h5v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rnqdry1h5v/</guid><description>B&amp;rsquo;MOJO: A novel hybrid architecture for foundation models enhances transductive inference by dynamically balancing eidetic and fading memory, leading to efficient and accurate processing of long seque&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rnqdry1h5v/cover.png"/></item><item><title>BAM! Just Like That: Simple and Efficient Parameter Upcycling for Mixture of Experts</title><link>https://deep-diver.github.io/neurips2024/posters/bdrwqtrfyi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bdrwqtrfyi/</guid><description>BAM! Efficiently upcycles pre-trained models into powerful Mixture-of-Experts (MoE) models, achieving state-of-the-art performance with reduced computational costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bdrwqtrfyi/cover.png"/></item><item><title>BERTs are Generative In-Context Learners</title><link>https://deep-diver.github.io/neurips2024/posters/bca9nmzkls/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bca9nmzkls/</guid><description>Masked language models can perform in-context learning, challenging the dominance of causal models in this area.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bca9nmzkls/cover.png"/></item><item><title>Bias Amplification in Language Model Evolution: An Iterated Learning Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/bsyn7ah4kx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bsyn7ah4kx/</guid><description>LLMs&amp;rsquo; iterative interactions amplify subtle biases; this paper uses a Bayesian Iterated Learning framework to explain this phenomenon and offers strategies to guide LLM evolution.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bsyn7ah4kx/cover.png"/></item><item><title>Bileve: Securing Text Provenance in Large Language Models Against Spoofing with Bi-level Signature</title><link>https://deep-diver.github.io/neurips2024/posters/vjcfnytg67/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vjcfnytg67/</guid><description>Bileve: a novel bi-level signature secures text provenance in LLMs against spoofing, enhancing detectability and reliability via fine-grained integrity checks and coarse-grained source tracing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vjcfnytg67/cover.png"/></item><item><title>BiScope: AI-generated Text Detection by Checking Memorization of Preceding Tokens</title><link>https://deep-diver.github.io/neurips2024/posters/hew2jsdycr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hew2jsdycr/</guid><description>BISCOPE: AI-generated text detection using a novel bidirectional method that outperforms existing techniques by leveraging both prediction and memorization of preceding tokens.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hew2jsdycr/cover.png"/></item><item><title>BitDelta: Your Fine-Tune May Only Be Worth One Bit</title><link>https://deep-diver.github.io/neurips2024/posters/xuwwq3gy7w/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xuwwq3gy7w/</guid><description>BitDelta drastically shrinks fine-tuned LLMs by quantizing their weight deltas to just one bit, achieving 10x memory reduction and latency improvements without sacrificing performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xuwwq3gy7w/cover.png"/></item><item><title>Boosting the Potential of Large Language Models with an Intelligent Information Assistant</title><link>https://deep-diver.github.io/neurips2024/posters/ozy4a11sug/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ozy4a11sug/</guid><description>Boosting LLMs with an intelligent information assistant, ASSISTRAG, significantly improves accuracy and reasoning, especially for less advanced models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ozy4a11sug/cover.png"/></item><item><title>Bridge-IF: Learning Inverse Protein Folding with Markov Bridges</title><link>https://deep-diver.github.io/neurips2024/posters/q8yfhrbbd8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/q8yfhrbbd8/</guid><description>Bridge-IF, a novel generative diffusion model, excels at inverse protein folding by learning probabilistic dependencies between protein structures and sequences, significantly outperforming existing m&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/q8yfhrbbd8/cover.png"/></item><item><title>Building on Efficient Foundations: Effective Training of LLMs with Structured Feedforward Layers</title><link>https://deep-diver.github.io/neurips2024/posters/wxlvyzbiew/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wxlvyzbiew/</guid><description>Training large language models efficiently is key; this paper shows how using structured feedforward layers and a novel training regime significantly reduces computational costs and improves training &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wxlvyzbiew/cover.png"/></item><item><title>Cal-DPO: Calibrated Direct Preference Optimization for Language Model Alignment</title><link>https://deep-diver.github.io/neurips2024/posters/57oqxxbtby/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/57oqxxbtby/</guid><description>Cal-DPO calibrates implicit rewards in contrastive preference learning, dramatically improving large language model alignment with human preferences.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/57oqxxbtby/cover.png"/></item><item><title>Calibrating Reasoning in Language Models with Internal Consistency</title><link>https://deep-diver.github.io/neurips2024/posters/udzkvmpf3s/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/udzkvmpf3s/</guid><description>LLMs&amp;rsquo; reasoning can be improved by using internal consistency to calibrate their outputs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/udzkvmpf3s/cover.png"/></item><item><title>Can Graph Learning Improve Planning in LLM-based Agents?</title><link>https://deep-diver.github.io/neurips2024/posters/bmos6ggw4j/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bmos6ggw4j/</guid><description>GNNs enhance LLM-based task planning by improving the ability to process task graphs, surpassing existing solutions even without training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bmos6ggw4j/cover.png"/></item><item><title>Can Language Models Learn to Skip Steps?</title><link>https://deep-diver.github.io/neurips2024/posters/w4antvxao9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w4antvxao9/</guid><description>Language models learn to skip steps in reasoning, improving efficiency and generalization, showcasing emergent human-like cognitive abilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w4antvxao9/cover.png"/></item><item><title>Can Large Language Model Agents Simulate Human Trust Behavior?</title><link>https://deep-diver.github.io/neurips2024/posters/ceowahuqic/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ceowahuqic/</guid><description>LLM agents surprisingly exhibit human-like trust behavior, especially GPT-4, paving the way for simulating complex human interactions in various applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ceowahuqic/cover.png"/></item><item><title>Can LLMs Implicitly Learn Numeric Parameter Constraints in Data Science APIs?</title><link>https://deep-diver.github.io/neurips2024/posters/lfc5rujstk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lfc5rujstk/</guid><description>LLMs struggle to reliably generate valid data science code due to a lack of true understanding of numerical constraints in APIs, despite seemingly mastering common patterns through extensive training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lfc5rujstk/cover.png"/></item><item><title>Causal language modeling can elicit search and reasoning capabilities on logic puzzles</title><link>https://deep-diver.github.io/neurips2024/posters/i5poejmwoc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/i5poejmwoc/</guid><description>LLMs surprisingly master complex logic puzzles like Sudoku and Zebra puzzles after training on strategically ordered solution steps, revealing hidden reasoning abilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/i5poejmwoc/cover.png"/></item><item><title>Chain of Agents: Large Language Models Collaborating on Long-Context Tasks</title><link>https://deep-diver.github.io/neurips2024/posters/luclf4bjsr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/luclf4bjsr/</guid><description>Chain-of-Agents (CoA) framework uses multi-agent collaboration to efficiently process long contexts for LLMs, significantly improving performance on various tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/luclf4bjsr/cover.png"/></item><item><title>Chain of Thoughtlessness? An Analysis of CoT in Planning</title><link>https://deep-diver.github.io/neurips2024/posters/kpbeazu5nm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kpbeazu5nm/</guid><description>Chain of Thought prompting in LLMs offers limited generalizability, providing performance gains only when prompts are highly specific to problem types; highlighting a critical trade-off between perfor&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kpbeazu5nm/cover.png"/></item><item><title>Chain-of-Thought Reasoning Without Prompting</title><link>https://deep-diver.github.io/neurips2024/posters/4zt7s0b0jp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4zt7s0b0jp/</guid><description>LLMs can reason effectively without prompting by simply adjusting the decoding process to reveal inherent chain-of-thought paths.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4zt7s0b0jp/cover.png"/></item><item><title>Chat-Scene: Bridging 3D Scene and Large Language Models with Object Identifiers</title><link>https://deep-diver.github.io/neurips2024/posters/t3bhmwazhv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/t3bhmwazhv/</guid><description>Chat-Scene: Bridging 3D scenes and LLMs using object identifiers for efficient, object-level interaction and improved scene comprehension.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/t3bhmwazhv/cover.png"/></item><item><title>ChatTracker: Enhancing Visual Tracking Performance via Chatting with Multimodal Large Language Model</title><link>https://deep-diver.github.io/neurips2024/posters/hzanl2uncb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hzanl2uncb/</guid><description>ChatTracker boosts visual tracking by intelligently using a large language model to refine object descriptions, achieving performance on par with state-of-the-art methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hzanl2uncb/cover.png"/></item><item><title>Cherry on Top: Parameter Heterogeneity and Quantization in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/qaiklacrkj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qaiklacrkj/</guid><description>CherryQ, a novel quantization method, leverages parameter heterogeneity in LLMs to achieve superior performance by selectively quantizing less critical parameters while preserving essential ones.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qaiklacrkj/cover.png"/></item><item><title>CigTime: Corrective Instruction Generation Through Inverse Motion Editing</title><link>https://deep-diver.github.io/neurips2024/posters/gkta1qycj9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gkta1qycj9/</guid><description>CigTime generates corrective motion instructions from motion pairs using motion editing and large language models. This innovative approach improves upon baselines by leveraging motion triplets for f&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gkta1qycj9/cover.png"/></item><item><title>Code Repair with LLMs gives an Exploration-Exploitation Tradeoff</title><link>https://deep-diver.github.io/neurips2024/posters/o863gx6dxa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/o863gx6dxa/</guid><description>New program synthesis method, REX, leverages Thompson Sampling to balance exploration and exploitation in iterative LLM code refinement, solving more problems with fewer model calls.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/o863gx6dxa/cover.png"/></item><item><title>Coevolving with the Other You: Fine-Tuning LLM with Sequential Cooperative Multi-Agent Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/ooocozfvk3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ooocozfvk3/</guid><description>CORY: a novel multi-agent RL framework boosts LLM fine-tuning!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ooocozfvk3/cover.png"/></item><item><title>Combining Observational Data and Language for Species Range Estimation</title><link>https://deep-diver.github.io/neurips2024/posters/iokluxb05h/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/iokluxb05h/</guid><description>LE-SINR combines Wikipedia species descriptions with citizen science observations to create accurate species range maps, even with limited data, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/iokluxb05h/cover.png"/></item><item><title>CoMERA: Computing- and Memory-Efficient Training via Rank-Adaptive Tensor Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/hxglvysg2c/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hxglvysg2c/</guid><description>CoMERA achieves 2-3x faster AI model training via rank-adaptive tensor optimization, significantly improving both computing and memory efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hxglvysg2c/cover.png"/></item><item><title>Compositional 3D-aware Video Generation with LLM Director</title><link>https://deep-diver.github.io/neurips2024/posters/oqdy2efrja/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oqdy2efrja/</guid><description>LLM-directed compositional 3D-aware video generation (C3V) achieves high-fidelity video generation with diverse motion and flexible concept control by decomposing prompts, generating 3D concepts, and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oqdy2efrja/cover.png"/></item><item><title>Concentrate Attention: Towards Domain-Generalizable Prompt Optimization for Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/zoarr5qmfx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zoarr5qmfx/</guid><description>Boost language model performance across domains with &amp;lsquo;Concentration&amp;rsquo;: a new prompt optimization objective that prioritizes stable, deep-layer attention.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zoarr5qmfx/cover.png"/></item><item><title>Conformal Alignment: Knowing When to Trust Foundation Models with Guarantees</title><link>https://deep-diver.github.io/neurips2024/posters/yzycejlv9z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yzycejlv9z/</guid><description>Conformal Alignment certifies trustworthy foundation model outputs by guaranteeing a user-specified fraction meet alignment criteria, regardless of the model or data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yzycejlv9z/cover.png"/></item><item><title>Continual Learning with Global Alignment</title><link>https://deep-diver.github.io/neurips2024/posters/4vp0edvy4o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4vp0edvy4o/</guid><description>Researchers developed a novel continual learning method achieving state-of-the-art performance by aligning data representations across tasks using pre-trained tokens, eliminating the need for experien&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4vp0edvy4o/cover.png"/></item><item><title>CoVoMix: Advancing Zero-Shot Speech Generation for Human-like Multi-talker Conversations</title><link>https://deep-diver.github.io/neurips2024/posters/vnbqbv658b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vnbqbv658b/</guid><description>CoVoMix: Generating human-like, multi-speaker conversations with zero-shot speech synthesis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vnbqbv658b/cover.png"/></item><item><title>Crafting Interpretable Embeddings for Language Neuroscience by Asking LLMs Questions</title><link>https://deep-diver.github.io/neurips2024/posters/mxmvwwybwe/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mxmvwwybwe/</guid><description>LLM-based text embeddings are powerful but lack interpretability. This paper introduces QA-Emb, a novel method that uses an LLM to answer yes/no questions about a text, thereby producing an interpreta&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mxmvwwybwe/cover.png"/></item><item><title>CriticEval: Evaluating Large-scale Language Model as Critic</title><link>https://deep-diver.github.io/neurips2024/posters/zsxz65yql1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zsxz65yql1/</guid><description>CRITICEVAL: A new benchmark reliably evaluates LLMs&amp;rsquo; ability to identify and correct flaws in their responses, addressing limitations of existing methods by offering comprehensive and reliable evaluat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zsxz65yql1/cover.png"/></item><item><title>Cross-model Control: Improving Multiple Large Language Models in One-time Training</title><link>https://deep-diver.github.io/neurips2024/posters/ypqhstsofs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ypqhstsofs/</guid><description>One-time training improves multiple LLMs using a tiny portable model, drastically reducing costs and resource needs for model enhancement.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ypqhstsofs/cover.png"/></item><item><title>CultureLLM: Incorporating Cultural Differences into Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/sisbokqmbl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sisbokqmbl/</guid><description>CultureLLM, a new approach, effectively incorporates cultural nuances into LLMs using semantic data augmentation, significantly outperforming existing models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sisbokqmbl/cover.png"/></item><item><title>CulturePark: Boosting Cross-cultural Understanding in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/bifhhf2rod/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bifhhf2rod/</guid><description>CulturePark, a novel multi-agent communication framework, generates high-quality cross-cultural data to fine-tune LLMs, significantly reducing cultural bias and boosting cross-cultural understanding.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bifhhf2rod/cover.png"/></item><item><title>D-CPT Law: Domain-specific Continual Pre-Training Scaling Law for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/jzkfn5fwok/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jzkfn5fwok/</guid><description>New D-CPT Law optimizes continual pre-training for LLMs by predicting optimal data mixture ratios, drastically cutting training costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jzkfn5fwok/cover.png"/></item><item><title>DAGER: Exact Gradient Inversion for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/cradax7h23/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cradax7h23/</guid><description>DAGER: Exact gradient inversion for LLMs; recovers full input text batches precisely.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cradax7h23/cover.png"/></item><item><title>DAPE: Data-Adaptive Positional Encoding for Length Extrapolation</title><link>https://deep-diver.github.io/neurips2024/posters/rnueubrxvu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rnueubrxvu/</guid><description>DAPE: A novel data-adaptive positional encoding method dynamically adjusts positional information based on input context, improving transformer performance and length generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rnueubrxvu/cover.png"/></item><item><title>DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph</title><link>https://deep-diver.github.io/neurips2024/posters/5ifecna7zr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/5ifecna7zr/</guid><description>DARG dynamically evaluates LLMs via adaptive reasoning graphs, revealing performance drops with increased complexity and exposing model biases.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/5ifecna7zr/cover.png"/></item><item><title>DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving</title><link>https://deep-diver.github.io/neurips2024/posters/zlu21oqjd5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zlu21oqjd5/</guid><description>DART-Math tackles LLM limitations in mathematical problem-solving by introducing Difficulty-Aware Rejection Tuning, a novel method that generates high-quality, bias-reduced datasets, resulting in supe&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zlu21oqjd5/cover.png"/></item><item><title>Data-Efficient Learning with Neural Programs</title><link>https://deep-diver.github.io/neurips2024/posters/qxqy58xu25/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qxqy58xu25/</guid><description>ISED: a novel, data-efficient algorithm learns neural programs by sampling from neural predictions to estimate gradients of black-box components, outperforming baselines on various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qxqy58xu25/cover.png"/></item><item><title>Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum</title><link>https://deep-diver.github.io/neurips2024/posters/r8m9sfymdi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/r8m9sfymdi/</guid><description>This paper introduces dataset decomposition (DD), a novel approach to accelerate LLM training while enhancing performance. DD significantly reduces training time by decomposing datasets into buckets &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/r8m9sfymdi/cover.png"/></item><item><title>DDK: Distilling Domain Knowledge for Efficient Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/xgiuruq0ss/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xgiuruq0ss/</guid><description>DDK: Dynamically Distilling Domain Knowledge for efficient LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xgiuruq0ss/cover.png"/></item><item><title>Decision-Making Behavior Evaluation Framework for LLMs under Uncertain Context</title><link>https://deep-diver.github.io/neurips2024/posters/re0ly2ylcu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/re0ly2ylcu/</guid><description>New framework reveals LLMs&amp;rsquo; human-like decision-making tendencies but highlights significant variations and biases influenced by demographic factors, underscoring ethical deployment needs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/re0ly2ylcu/cover.png"/></item><item><title>Deep Bayesian Active Learning for Preference Modeling in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/tadtt9ughn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tadtt9ughn/</guid><description>BAL-PM, a novel active learning approach, drastically reduces human feedback in LLM preference modeling by leveraging both model uncertainty and prompt distribution diversity, achieving 33%-68% fewer &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tadtt9ughn/cover.png"/></item><item><title>DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging</title><link>https://deep-diver.github.io/neurips2024/posters/kmnoh7cxrq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kmnoh7cxrq/</guid><description>DenseFormer enhances transformers by adding a depth-weighted averaging step, improving data efficiency and outperforming baselines in memory and inference time without increasing model size.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kmnoh7cxrq/cover.png"/></item><item><title>DETAIL: Task DEmonsTration Attribution for Interpretable In-context Learning</title><link>https://deep-diver.github.io/neurips2024/posters/4jrnkah15k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4jrnkah15k/</guid><description>DETAIL: A novel attribution method reveals the impact of individual demonstrations in in-context learning, boosting interpretability and improving transformer-based model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4jrnkah15k/cover.png"/></item><item><title>DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via Adaptive Heads Fusion</title><link>https://deep-diver.github.io/neurips2024/posters/g92nu7knrq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/g92nu7knrq/</guid><description>Decoupled-Head Attention (DHA) drastically cuts LLM inference costs by adaptively sharing key/value heads, achieving 97.6% of original performance with only 0.25% pre-training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/g92nu7knrq/cover.png"/></item><item><title>Diff-eRank: A Novel Rank-Based Metric for Evaluating Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/nvn80cscvm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nvn80cscvm/</guid><description>Diff-eRank: A novel rank-based metric assessing LLMs&amp;rsquo; efficiency in eliminating redundant information during training, showing improved correlation with model size and performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nvn80cscvm/cover.png"/></item><item><title>Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/rgtryvc9n4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rgtryvc9n4/</guid><description>DSA, a novel automated framework, discovers optimal sparsity allocation for layer-wise LLM pruning, achieving significant performance gains across various models and tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rgtryvc9n4/cover.png"/></item><item><title>Discovery of the Hidden World with Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/w50icqc6qj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w50icqc6qj/</guid><description>COAT leverages LLMs to identify high-level causal factors from unstructured data, enabling causal discovery in real-world scenarios where well-defined variables are lacking.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w50icqc6qj/cover.png"/></item><item><title>DISP-LLM: Dimension-Independent Structural Pruning for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/yxay6thgg0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yxay6thgg0/</guid><description>DISP-LLM: A novel dimension-independent structural pruning method for LLMs achieves accuracy similar to semi-structural pruning while improving flexibility and efficiency, outperforming state-of-the-a&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yxay6thgg0/cover.png"/></item><item><title>Divergences between Language Models and Human Brains</title><link>https://deep-diver.github.io/neurips2024/posters/dpp5f3ufkw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dpp5f3ufkw/</guid><description>Language models struggle with social/emotional intelligence and physical commonsense, unlike human brains. Fine-tuning models on these aspects improves their brain response prediction accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dpp5f3ufkw/cover.png"/></item><item><title>DLAD: Improving Logits-based Detector without Logits from Black-box LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/heksssv5q9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/heksssv5q9/</guid><description>DALD: A novel framework for black-box LLM text detection, achieving state-of-the-art performance without relying on source model logits, by aligning surrogate model distributions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/heksssv5q9/cover.png"/></item><item><title>Do LLMs Build World Representations? Probing Through the Lens of State Abstraction</title><link>https://deep-diver.github.io/neurips2024/posters/lzfzjyuwgy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lzfzjyuwgy/</guid><description>LLMs prioritize task completion over full world-state understanding by using goal-oriented abstractions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lzfzjyuwgy/cover.png"/></item><item><title>Do LLMs dream of elephants (when told not to)? Latent concept association and associative memory in transformers</title><link>https://deep-diver.github.io/neurips2024/posters/wj04zx8txm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wj04zx8txm/</guid><description>LLMs&amp;rsquo; fact retrieval is easily manipulated by context, highlighting their associative memory behavior; this paper studies this with transformers, showing how self-attention and value matrices support &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wj04zx8txm/cover.png"/></item><item><title>Does Reasoning Emerge? Examining the Probabilities of Causation in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/b1ylcyjazk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/b1ylcyjazk/</guid><description>LLMs&amp;rsquo; reasoning abilities are assessed via a novel framework that leverages probabilities of causation, revealing that while capable, their understanding of causality falls short of human-level reason&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/b1ylcyjazk/cover.png"/></item><item><title>Doing Experiments and Revising Rules with Natural Language and Probabilistic Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/hxdafk488a/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hxdafk488a/</guid><description>This paper introduces ActiveACRE, a model that uses LLMs and probabilistic inference to infer natural language rules through online experimentation, demonstrating higher accuracy than existing methods&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hxdafk488a/cover.png"/></item><item><title>DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation</title><link>https://deep-diver.github.io/neurips2024/posters/x4eotqw7ka/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x4eotqw7ka/</guid><description>DropBP: Accelerate LLM fine-tuning by 44% while preserving accuracy!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x4eotqw7ka/cover.png"/></item><item><title>Dual-Personalizing Adapter for Federated Foundation Models</title><link>https://deep-diver.github.io/neurips2024/posters/nkwpibsw1f/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nkwpibsw1f/</guid><description>Federated Dual-Personalizing Adapter (FedDPA) tackles test-time distribution shifts and personalization in federated foundation models using a global and local adapter co-working mechanism, achieving &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nkwpibsw1f/cover.png"/></item><item><title>Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision</title><link>https://deep-diver.github.io/neurips2024/posters/qwgfh2fttn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qwgfh2fttn/</guid><description>AI alignment beyond human supervision is achieved via easy-to-hard generalization: training reward models on easy tasks to effectively evaluate and improve generators on harder tasks, achieving superh&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qwgfh2fttn/cover.png"/></item><item><title>Efficient Contextual LLM Cascades through Budget-Constrained Policy Learning</title><link>https://deep-diver.github.io/neurips2024/posters/adqlaz09ds/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/adqlaz09ds/</guid><description>TREACLE: a reinforcement learning policy efficiently selects LLMs and prompts, achieving up to 85% cost savings while maintaining high accuracy in answering reasoning questions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/adqlaz09ds/cover.png"/></item><item><title>Efficient LLM Jailbreak via Adaptive Dense-to-sparse Constrained Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/bn5pa3hho8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bn5pa3hho8/</guid><description>Adaptive Dense-to-sparse Constrained Optimization (ADC) efficiently jailbreaks LLMs by transforming discrete token optimization into a continuous process, achieving higher success rates than existing &amp;hellip;</description></item><item><title>Efficient LLM Scheduling by Learning to Rank</title><link>https://deep-diver.github.io/neurips2024/posters/wlljyl0gi6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wlljyl0gi6/</guid><description>Learning to rank request outputs improves LLM scheduling, resulting in 2.8x lower chatbot latency and 6.5x higher synthetic data generation throughput.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wlljyl0gi6/cover.png"/></item><item><title>Efficient multi-prompt evaluation of LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/jzkpwcj200/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jzkpwcj200/</guid><description>PromptEval efficiently estimates LLM performance across many prompts, providing robust performance metrics and enabling reliable LLM comparisons.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jzkpwcj200/cover.png"/></item><item><title>Efficient Multi-task LLM Quantization and Serving for Multiple LoRA Adapters</title><link>https://deep-diver.github.io/neurips2024/posters/hfpv6u0kbx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hfpv6u0kbx/</guid><description>LoRA-Inlaid: a novel multi-task LLM serving system boosts throughput by 1.58x, latency by 1.76x, and job completion time by 2x, while improving SLO attainment by 10x, all while maintaining model quali&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hfpv6u0kbx/cover.png"/></item><item><title>EffiLearner: Enhancing Efficiency of Generated Code via Self-Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/khwoub0fs9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/khwoub0fs9/</guid><description>EFFI-LEARNER: A novel self-optimization framework dramatically improves the efficiency of LLM-generated code by iteratively refining code based on execution profiles.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/khwoub0fs9/cover.png"/></item><item><title>Elo Uncovered: Robustness and Best Practices in Language Model Evaluation</title><link>https://deep-diver.github.io/neurips2024/posters/pc9lljtl5f/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pc9lljtl5f/</guid><description>Elo rating&amp;rsquo;s reliability for LLM evaluation is challenged, revealing inconsistencies and suggesting new, more robust methods are needed for accurate model ranking.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pc9lljtl5f/cover.png"/></item><item><title>Embedding-Aligned Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/wsu1ppi2up/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wsu1ppi2up/</guid><description>EAGLE: Guiding LLMs using latent embeddings for controlled text generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wsu1ppi2up/cover.png"/></item><item><title>Enhancing In-Context Learning Performance with just SVD-Based Weight Pruning: A Theoretical Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/wt6ghk5shc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wt6ghk5shc/</guid><description>SVD-based weight pruning surprisingly boosts in-context learning in large language models, especially when applied to deeper layers, offering a novel approach to model compression and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wt6ghk5shc/cover.png"/></item><item><title>Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation Control</title><link>https://deep-diver.github.io/neurips2024/posters/askckanxno/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/askckanxno/</guid><description>Boosting LLM trustworthiness, researchers introduce Sparse Activation Control, a training-free method that concurrently enhances safety, factuality, and bias mitigation by selectively controlling atte&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/askckanxno/cover.png"/></item><item><title>Entity Alignment with Noisy Annotations from Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/qfcq54ztx1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qfcq54ztx1/</guid><description>LLM4EA: A novel framework efficiently merges knowledge graphs using LLMs, overcoming noisy annotations and high costs via active learning and unsupervised label refinement, boosting accuracy and effic&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qfcq54ztx1/cover.png"/></item><item><title>Estimating the Hallucination Rate of Generative AI</title><link>https://deep-diver.github.io/neurips2024/posters/lzl8qjyxv5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lzl8qjyxv5/</guid><description>New method estimates hallucination rates in generative AI&amp;rsquo;s in-context learning, improving model reliability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lzl8qjyxv5/cover.png"/></item><item><title>Evaluation of Text-to-Video Generation Models: A Dynamics Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/tmx1aumkl6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tmx1aumkl6/</guid><description>DEVIL: a novel text-to-video evaluation protocol focusing on video dynamics, resulting in more realistic video generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tmx1aumkl6/cover.png"/></item><item><title>Explaining Datasets in Words: Statistical Models with Natural Language Parameters</title><link>https://deep-diver.github.io/neurips2024/posters/u5bkogwwzw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/u5bkogwwzw/</guid><description>This paper introduces a model-agnostic algorithm that uses natural language predicates to make statistical model parameters directly interpretable, significantly improving explainability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/u5bkogwwzw/cover.png"/></item><item><title>Exploiting LLM Quantization</title><link>https://deep-diver.github.io/neurips2024/posters/isa7mme7vg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/isa7mme7vg/</guid><description>LLM quantization, while improving efficiency, creates a security risk: attackers can craft seemingly benign models that exhibit malicious behavior only when quantized.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/isa7mme7vg/cover.png"/></item><item><title>Fast Sampling via Discrete Non-Markov Diffusion Models with Predetermined Transition Time</title><link>https://deep-diver.github.io/neurips2024/posters/kkyzmepjhn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kkyzmepjhn/</guid><description>Accelerated discrete diffusion model sampling is achieved via novel discrete non-Markov diffusion models (DNDM) with predetermined transition times, enabling a training-free algorithm that significant&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kkyzmepjhn/cover.png"/></item><item><title>Fight Back Against Jailbreaking via Prompt Adversarial Tuning</title><link>https://deep-diver.github.io/neurips2024/posters/nrdst1qifj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nrdst1qifj/</guid><description>Prompt Adversarial Tuning (PAT) defends against LLM jailbreaking by training a protective prompt prefix. PAT uses adversarial and benign prompts to optimize this prefix, significantly reducing succes&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nrdst1qifj/cover.png"/></item><item><title>Fine-grained Analysis of In-context Linear Estimation: Data, Architecture, and Beyond</title><link>https://deep-diver.github.io/neurips2024/posters/lypaymfqqm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lypaymfqqm/</guid><description>Researchers crack the code of in-context learning in Transformers, revealing how architecture, low-rank parameters, and data correlations influence model optimization and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lypaymfqqm/cover.png"/></item><item><title>FLAME : Factuality-Aware Alignment for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/zwuhsialbh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zwuhsialbh/</guid><description>FLAME: A novel alignment method enhances large language model factuality by addressing hallucination in supervised fine-tuning and reinforcement learning, resulting in more accurate and helpful AI ass&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zwuhsialbh/cover.png"/></item><item><title>FLoRA: Federated Fine-Tuning Large Language Models with Heterogeneous Low-Rank Adaptations</title><link>https://deep-diver.github.io/neurips2024/posters/tccorxxnjq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tccorxxnjq/</guid><description>FLORA enables efficient &amp;amp; private federated fine-tuning of LLMs via novel stacking-based heterogeneous low-rank adaptation, surpassing existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tccorxxnjq/cover.png"/></item><item><title>fMRI predictors based on language models of increasing complexity recover brain left lateralization</title><link>https://deep-diver.github.io/neurips2024/posters/xf1jpo5k6l/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xf1jpo5k6l/</guid><description>Larger language models better predict brain activity in fMRI studies, with left-hemisphere prediction significantly increasing as model complexity scales up, reconciling classic aphasia findings with &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xf1jpo5k6l/cover.png"/></item><item><title>Fractal Patterns May Illuminate the Success of Next-Token Prediction</title><link>https://deep-diver.github.io/neurips2024/posters/clafyreaye/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/clafyreaye/</guid><description>LLMs&amp;rsquo; success is explained by the self-similar, long-range dependent fractal structure of language; small-scale patterns reflect larger ones.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/clafyreaye/cover.png"/></item><item><title>From Instance Training to Instruction Learning: Task Adapters Generation from Instructions</title><link>https://deep-diver.github.io/neurips2024/posters/cluvzbfrjj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cluvzbfrjj/</guid><description>TAGI, a novel method, generates task-specific adapters from instructions, enhancing LLM cross-task generalization by using knowledge distillation and a two-stage hypernetwork training process.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cluvzbfrjj/cover.png"/></item><item><title>From Unstructured Data to In-Context Learning: Exploring What Tasks Can Be Learned and When</title><link>https://deep-diver.github.io/neurips2024/posters/x9efgahvbi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x9efgahvbi/</guid><description>LLMs&amp;rsquo; in-context learning surprisingly arises from simple co-occurrence patterns in unstructured data, but positional information is key for complex tasks; ICL fails when patterns are unseen or fixed.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x9efgahvbi/cover.png"/></item><item><title>Generative Hierarchical Materials Search</title><link>https://deep-diver.github.io/neurips2024/posters/pspr4noirc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pspr4noirc/</guid><description>Generative Hierarchical Materials Search (GenMS) uses AI to design novel crystal structures from natural language descriptions, outperforming prior methods in both fulfilling user requests and finding&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pspr4noirc/cover.png"/></item><item><title>Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment</title><link>https://deep-diver.github.io/neurips2024/posters/orxqccn8fm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/orxqccn8fm/</guid><description>Reward learning from human demonstrations enhances supervised fine-tuning (SFT) for better LLM alignment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/orxqccn8fm/cover.png"/></item><item><title>Gorilla: Large Language Model Connected with Massive APIs</title><link>https://deep-diver.github.io/neurips2024/posters/tbrnc6yemy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tbrnc6yemy/</guid><description>Gorilla: a fine-tuned LLaMA model surpasses GPT-4 in generating accurate API calls by using Retriever Aware Training (RAT) to adapt to changing APIs and reduce hallucinations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tbrnc6yemy/cover.png"/></item><item><title>Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes</title><link>https://deep-diver.github.io/neurips2024/posters/vi1wqfn15v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vi1wqfn15v/</guid><description>Gradient Cuff: A novel defense mechanism against LLM jailbreaks, leveraging refusal loss landscapes for improved malicious query rejection without harming model performance on benign inputs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vi1wqfn15v/cover.png"/></item><item><title>Grammar-Aligned Decoding</title><link>https://deep-diver.github.io/neurips2024/posters/5g7ve8e1lu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/5g7ve8e1lu/</guid><description>Adaptive Sampling with Approximate Expected Futures (ASAp) ensures LLMs generate grammatically correct outputs that closely match the model&amp;rsquo;s original probability distribution.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/5g7ve8e1lu/cover.png"/></item><item><title>Graph Convolutions Enrich the Self-Attention in Transformers!</title><link>https://deep-diver.github.io/neurips2024/posters/ffnrpcbpi6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ffnrpcbpi6/</guid><description>Graph Filter-based Self-Attention (GFSA) enhances Transformers by addressing oversmoothing, boosting performance across various tasks with minimal added parameters.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ffnrpcbpi6/cover.png"/></item><item><title>GraphVis: Boosting LLMs with Visual Knowledge Graph Integration</title><link>https://deep-diver.github.io/neurips2024/posters/havpmn8ugi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/havpmn8ugi/</guid><description>GraphVis boosts LLMs by visualizing knowledge graphs, improving accuracy in textual and visual question answering.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/havpmn8ugi/cover.png"/></item><item><title>Grokking of Implicit Reasoning in Transformers: A Mechanistic Journey to the Edge of Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/d4qgswxiob/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/d4qgswxiob/</guid><description>Transformers can learn implicit reasoning through &amp;lsquo;grokking&amp;rsquo;, achieving high accuracy in composition and comparison tasks; however, generalization varies across reasoning types.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/d4qgswxiob/cover.png"/></item><item><title>Group Robust Preference Optimization in Reward-free RLHF</title><link>https://deep-diver.github.io/neurips2024/posters/prasjrmxxk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/prasjrmxxk/</guid><description>Group Robust Preference Optimization (GRPO) enhances reward-free RLHF by aligning LLMs to diverse group preferences, maximizing worst-case performance, and significantly improving fairness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/prasjrmxxk/cover.png"/></item><item><title>GTBench: Uncovering the Strategic Reasoning Capabilities of LLMs via Game-Theoretic Evaluations</title><link>https://deep-diver.github.io/neurips2024/posters/ypggxvwiv2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ypggxvwiv2/</guid><description>GTBENCH reveals LLMs&amp;rsquo; strategic reasoning weaknesses via game-theoretic evaluations, showing strengths in probabilistic scenarios but struggles with deterministic ones; code-pretraining helps.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ypggxvwiv2/cover.png"/></item><item><title>HAWK: Learning to Understand Open-World Video Anomalies</title><link>https://deep-diver.github.io/neurips2024/posters/vbkoez1pg3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vbkoez1pg3/</guid><description>HAWK: a novel framework leveraging interactive VLMs and motion modality achieves state-of-the-art performance in open-world video anomaly understanding, generating descriptions and answering questions&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vbkoez1pg3/cover.png"/></item><item><title>HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/hkujvapvsg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hkujvapvsg/</guid><description>HippoRAG, a neurobiologically inspired framework, dramatically improves LLM long-term memory and multi-hop question answering by synergistically orchestrating LLMs, knowledge graphs, and the Personali&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hkujvapvsg/cover.png"/></item><item><title>How Do Large Language Models Acquire Factual Knowledge During Pretraining?</title><link>https://deep-diver.github.io/neurips2024/posters/tydzj1evbp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tydzj1evbp/</guid><description>LLMs&amp;rsquo; factual knowledge acquisition during pretraining is surprisingly non-linear: more data doesn&amp;rsquo;t guarantee better knowledge retention, and forgetting follows a power law.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tydzj1evbp/cover.png"/></item><item><title>How do Large Language Models Handle Multilingualism?</title><link>https://deep-diver.github.io/neurips2024/posters/ctxyooagry/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ctxyooagry/</guid><description>LLMs surprisingly process multilingual queries via an English-centric intermediate stage before generating responses in the original language, a phenomenon explained by the proposed MWork framework an&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ctxyooagry/cover.png"/></item><item><title>HuRef: HUman-REadable Fingerprint for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/rlzgnezsoh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rlzgnezsoh/</guid><description>HuRef: Generate unique, human-readable fingerprints for LLMs to protect copyright without exposing model parameters or impeding training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rlzgnezsoh/cover.png"/></item><item><title>HYDRA: Model Factorization Framework for Black-Box LLM Personalization</title><link>https://deep-diver.github.io/neurips2024/posters/ckgngkmhyp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ckgngkmhyp/</guid><description>HYDRA, a novel model factorization framework, significantly improves black-box LLM personalization by capturing both user-specific behavior and shared knowledge, achieving a 9.01% average relative imp&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ckgngkmhyp/cover.png"/></item><item><title>I Don't Know: Explicit Modeling of Uncertainty with an [IDK] Token</title><link>https://deep-diver.github.io/neurips2024/posters/wc0vlquolb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wc0vlquolb/</guid><description>Boosting LLM accuracy, a new calibration method using a special [IDK] token explicitly models uncertainty, mitigating hallucinations, and improving factual precision while maintaining knowledge retent&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wc0vlquolb/cover.png"/></item><item><title>IDGen: Item Discrimination Induced Prompt Generation for LLM Evaluation</title><link>https://deep-diver.github.io/neurips2024/posters/zv4uiszzp5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zv4uiszzp5/</guid><description>IDGen synthesizes LLM evaluation prompts using Item Discrimination theory, creating a more challenging and discriminative dataset than previous methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zv4uiszzp5/cover.png"/></item><item><title>Image-aware Evaluation of Generated Medical Reports</title><link>https://deep-diver.github.io/neurips2024/posters/ecpig6o84z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ecpig6o84z/</guid><description>VLScore: a novel image-aware metric revolutionizes medical report evaluation by jointly assessing textual and visual similarities, significantly improving alignment with radiologist assessments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ecpig6o84z/cover.png"/></item><item><title>Implicit Optimization Bias of Next-token Prediction in Linear Models</title><link>https://deep-diver.github.io/neurips2024/posters/xszio6gqgg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xszio6gqgg/</guid><description>Researchers reveal implicit optimization biases in next-token prediction for language models, showing how gradient descent selects solutions based on data sparsity and a novel margin concept, impactin&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xszio6gqgg/cover.png"/></item><item><title>Implicit Regularization of Sharpness-Aware Minimization for Scale-Invariant Problems</title><link>https://deep-diver.github.io/neurips2024/posters/osovme9kl2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/osovme9kl2/</guid><description>Boosting deep learning generalization, this work unveils SAM&amp;rsquo;s implicit regularization using &amp;lsquo;balancedness&amp;rsquo;, a new metric. A resource-efficient variant, BAR, achieves 95% computational savings with i&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/osovme9kl2/cover.png"/></item><item><title>Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses</title><link>https://deep-diver.github.io/neurips2024/posters/zmnd0jucef/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zmnd0jucef/</guid><description>Improved few-shot jailbreaking techniques efficiently circumvent aligned language models and their defenses, achieving high success rates even against advanced protection methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zmnd0jucef/cover.png"/></item><item><title>Improving Context-Aware Preference Modeling for Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/52r4xjyzjg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/52r4xjyzjg/</guid><description>Context-aware preference modeling improves language model alignment by resolving ambiguity through a two-step process: context selection followed by context-specific preference evaluation. The approa&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/52r4xjyzjg/cover.png"/></item><item><title>Improving Sparse Decomposition of Language Model Activations with Gated Sparse Autoencoders</title><link>https://deep-diver.github.io/neurips2024/posters/zlblin2zvw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zlblin2zvw/</guid><description>Gated Sparse Autoencoders (GSAEs) achieve Pareto improvement over baseline SAEs for unsupervised feature discovery in language models, resolving the shrinkage bias of L1 penalty by separating feature &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zlblin2zvw/cover.png"/></item><item><title>In-Context Learning State Vector with Inner and Momentum Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/gnnmb7y0xx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gnnmb7y0xx/</guid><description>This paper introduces inner and momentum optimization to enhance the state vector for in-context learning, improving performance and scalability in LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gnnmb7y0xx/cover.png"/></item><item><title>In-Context Learning with Representations: Contextual Generalization of Trained Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/ik37kkxkbm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ik37kkxkbm/</guid><description>Transformers learn contextual information for generalization to unseen examples and tasks, even with limited training data, converging linearly to a global minimum.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ik37kkxkbm/cover.png"/></item><item><title>Incentivizing Quality Text Generation via Statistical Contracts</title><link>https://deep-diver.github.io/neurips2024/posters/wzgw4crxwk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wzgw4crxwk/</guid><description>Cost-robust contracts, inspired by statistical hypothesis tests, incentivize quality in LLM text generation, overcoming the moral hazard of pay-per-token models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wzgw4crxwk/cover.png"/></item><item><title>INDICT: Code Generation with Internal Dialogues of Critiques for Both Security and Helpfulness</title><link>https://deep-diver.github.io/neurips2024/posters/jcmyiuwprx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jcmyiuwprx/</guid><description>INDICT, a novel framework, empowers LLMs with internal dialogues of critiques to enhance code generation, prioritizing both safety and helpfulness, resulting in +10% absolute improvement across variou&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jcmyiuwprx/cover.png"/></item><item><title>Information Re-Organization Improves Reasoning in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/sciwuypng0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sciwuypng0/</guid><description>InfoRE: A novel method improving large language models&amp;rsquo; reasoning by reorganizing information to highlight logical relationships, resulting in a 4% average accuracy boost across various tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sciwuypng0/cover.png"/></item><item><title>Initialization is Critical to Whether Transformers Fit Composite Functions by Reasoning or Memorizing</title><link>https://deep-diver.github.io/neurips2024/posters/yobgdvayts/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yobgdvayts/</guid><description>Transformer model initialization dramatically affects whether it reasons or memorizes, impacting performance on compositional tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yobgdvayts/cover.png"/></item><item><title>Interpreting Learned Feedback Patterns in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/xuongr1byy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xuongr1byy/</guid><description>Researchers developed methods to measure and interpret the divergence between learned feedback patterns (LFPs) in LLMs and human preferences, helping minimize discrepancies between LLM behavior and tr&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xuongr1byy/cover.png"/></item><item><title>IRCAN: Mitigating Knowledge Conflicts in LLM Generation via Identifying and Reweighting Context-Aware Neurons</title><link>https://deep-diver.github.io/neurips2024/posters/zfxraqbbkx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zfxraqbbkx/</guid><description>IRCAN tackles LLM knowledge conflicts by identifying and reweighting context-aware neurons, significantly improving context-sensitive outputs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zfxraqbbkx/cover.png"/></item><item><title>Is Programming by Example solved by LLMs?</title><link>https://deep-diver.github.io/neurips2024/posters/xqc8yyhscl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xqc8yyhscl/</guid><description>Large Language Models (LLMs) surprisingly improve the challenging task of Programming by Example (PBE) when fine-tuned on problem-specific data, outperforming classic symbolic methods and even surpass&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xqc8yyhscl/cover.png"/></item><item><title>Is the MMI Criterion Necessary for Interpretability? Degenerating Non-causal Features to Plain Noise for Self-Rationalization</title><link>https://deep-diver.github.io/neurips2024/posters/eaqcvzx30k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/eaqcvzx30k/</guid><description>New criterion maximizes remaining discrepancy after rationale removal, treating spurious features as noise, improving rationale extraction.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/eaqcvzx30k/cover.png"/></item><item><title>Iteration Head: A Mechanistic Study of Chain-of-Thought</title><link>https://deep-diver.github.io/neurips2024/posters/qbcxwpot5w/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qbcxwpot5w/</guid><description>Researchers reveal how Chain-of-Thought reasoning emerges in transformers via specialized &amp;lsquo;iteration heads&amp;rsquo;, improving LLM performance and offering insights into mechanistic interpretability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qbcxwpot5w/cover.png"/></item><item><title>Iterative Reasoning Preference Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/4xikfvnyvx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4xikfvnyvx/</guid><description>Iterative Reasoning Preference Optimization boosts large language model reasoning by iteratively refining preferences between generated reasoning steps, achieving significant accuracy gains on benchma&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4xikfvnyvx/cover.png"/></item><item><title>Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters</title><link>https://deep-diver.github.io/neurips2024/posters/acblttkk5q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/acblttkk5q/</guid><description>New benchmark and jailbreak method exposes vulnerabilities of LLM moderation, achieving significantly higher success rates than existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/acblttkk5q/cover.png"/></item><item><title>JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models</title><link>https://deep-diver.github.io/neurips2024/posters/ujdkxwtbjx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ujdkxwtbjx/</guid><description>JiuZhang3.0 efficiently enhances LLMs&amp;rsquo; mathematical reasoning by training a small model to synthesize high-quality training data, drastically reducing costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ujdkxwtbjx/cover.png"/></item><item><title>Kangaroo: Lossless Self-Speculative Decoding for Accelerating LLMs via Double Early Exiting</title><link>https://deep-diver.github.io/neurips2024/posters/lt3oc04mdp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lt3oc04mdp/</guid><description>Kangaroo: Double early exiting boosts LLM speed!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lt3oc04mdp/cover.png"/></item><item><title>KG-FIT: Knowledge Graph Fine-Tuning Upon Open-World Knowledge</title><link>https://deep-diver.github.io/neurips2024/posters/rdopmodpki/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rdopmodpki/</guid><description>KG-FIT boosts knowledge graph embedding by smartly integrating open-world knowledge from LLMs, achieving significant performance gains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rdopmodpki/cover.png"/></item><item><title>KnowGPT: Knowledge Graph based Prompting for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/pacbluo5m7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pacbluo5m7/</guid><description>KnowGPT: A novel framework boosts Large Language Model accuracy by intelligently integrating knowledge graphs, significantly reducing factual errors and achieving near-human performance on benchmark d&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pacbluo5m7/cover.png"/></item><item><title>Knowledge Circuits in Pretrained Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/yvxzznxcag/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yvxzznxcag/</guid><description>Researchers unveil &amp;lsquo;knowledge circuits&amp;rsquo; within LLMs, revealing how knowledge is collaboratively encoded and utilized, leading to improved LLM design and interpretations of model behavior.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yvxzznxcag/cover.png"/></item><item><title>KptLLM: Unveiling the Power of Large Language Model for Keypoint Comprehension</title><link>https://deep-diver.github.io/neurips2024/posters/gwd3mqufgp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gwd3mqufgp/</guid><description>KptLLM: A novel multimodal model leverages LLMs for superior keypoint comprehension, outperforming existing methods in various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gwd3mqufgp/cover.png"/></item><item><title>Kraken: Inherently Parallel Transformers For Efficient Multi-Device Inference</title><link>https://deep-diver.github.io/neurips2024/posters/jrtxzzk0a6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jrtxzzk0a6/</guid><description>Kraken: A new Transformer architecture boosts multi-device inference speed by 35.6% by cleverly overlapping communication with computation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jrtxzzk0a6/cover.png"/></item><item><title>KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization</title><link>https://deep-diver.github.io/neurips2024/posters/pnnvzqss4p/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pnnvzqss4p/</guid><description>Boost LLM inference speed 1.4-3.5x by using Coupled Quantization (CQ) to compress KV cache down to 1 bit per channel, while preserving model accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pnnvzqss4p/cover.png"/></item><item><title>LACIE: Listener-Aware Finetuning for Calibration in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/rnvgyd9rah/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rnvgyd9rah/</guid><description>LACIE: Listener-aware finetuning improves LLM confidence calibration, reducing incorrect answers accepted by human listeners by 47% while maintaining correct answer acceptance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rnvgyd9rah/cover.png"/></item><item><title>Language Grounded Multi-agent Reinforcement Learning with Human-interpretable Communication</title><link>https://deep-diver.github.io/neurips2024/posters/duhx779c5q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/duhx779c5q/</guid><description>LangGround: MARL agents learn human-interpretable communication via LLM-grounded training, enabling effective human-agent collaboration.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/duhx779c5q/cover.png"/></item><item><title>Large Language Model Unlearning via Embedding-Corrupted Prompts</title><link>https://deep-diver.github.io/neurips2024/posters/e5icsxbd8q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/e5icsxbd8q/</guid><description>ECO prompts enable efficient LLM unlearning by corrupting prompts flagged for forgetting, achieving promising results across various LLMs and tasks with minimal side effects.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/e5icsxbd8q/cover.png"/></item><item><title>Large language model validity via enhanced conformal prediction methods</title><link>https://deep-diver.github.io/neurips2024/posters/jd3nypeq3r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jd3nypeq3r/</guid><description>New conformal inference methods enhance LLM validity by providing adaptive validity guarantees and improving the quality of LLM outputs, addressing prior methods&amp;rsquo; limitations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jd3nypeq3r/cover.png"/></item><item><title>Large Language Models Must Be Taught to Know What They Don’t Know</title><link>https://deep-diver.github.io/neurips2024/posters/qzvwyggryb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qzvwyggryb/</guid><description>Teach LLMs uncertainty for reliable high-stakes predictions: Fine-tuning with graded examples significantly improves LLM&amp;rsquo;s uncertainty calibration and generalizes well.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qzvwyggryb/cover.png"/></item><item><title>Large Language Models-guided Dynamic Adaptation for Temporal Knowledge Graph Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/cw7agrr8gj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cw7agrr8gj/</guid><description>LLM-DA dynamically adapts LLM-generated rules for accurate, interpretable temporal knowledge graph reasoning, significantly improving accuracy without fine-tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cw7agrr8gj/cover.png"/></item><item><title>Latent Paraphrasing: Perturbation on Layers Improves Knowledge Injection in Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/t1lfrywtf7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/t1lfrywtf7/</guid><description>LaPael improves LLM knowledge injection by applying learned noise to early layers, enabling diverse and efficient knowledge updates without repeated external model usage.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/t1lfrywtf7/cover.png"/></item><item><title>LCGen: Mining in Low-Certainty Generation for View-consistent Text-to-3D</title><link>https://deep-diver.github.io/neurips2024/posters/4wgzkayi2d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4wgzkayi2d/</guid><description>LCGen: A novel method for view-consistent text-to-3D generation, resolving the &amp;lsquo;Janus Problem&amp;rsquo; by strategically using low-certainty priors to align viewpoints and optimize the generation process.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4wgzkayi2d/cover.png"/></item><item><title>Learn more, but bother less: parameter efficient continual learning</title><link>https://deep-diver.github.io/neurips2024/posters/zxtanh5uyb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zxtanh5uyb/</guid><description>LB-CL: A novel parameter-efficient continual learning method for LLMs that boosts performance and reduces forgetting by leveraging parametric knowledge transfer and maintaining orthogonal low-rank sub&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zxtanh5uyb/cover.png"/></item><item><title>Learnability Matters: Active Learning for Video Captioning</title><link>https://deep-diver.github.io/neurips2024/posters/4gp7s7u0lj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4gp7s7u0lj/</guid><description>Active learning for video captioning is enhanced by a novel algorithm that prioritizes &amp;rsquo;learnability&amp;rsquo;, diversity, and uncertainty to address annotation inconsistency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4gp7s7u0lj/cover.png"/></item><item><title>Learning and Transferring Sparse Contextual Bigrams with Linear Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/pukavawybo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pukavawybo/</guid><description>Linear transformers efficiently learn sparse contextual bigrams by leveraging both in-context and global information, achieving polynomial sample complexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pukavawybo/cover.png"/></item><item><title>Learning Goal-Conditioned Representations for Language Reward Models</title><link>https://deep-diver.github.io/neurips2024/posters/swh8lxuyca/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/swh8lxuyca/</guid><description>Goal-conditioned contrastive learning boosts language reward model performance and enables better control of language model generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/swh8lxuyca/cover.png"/></item><item><title>Learning to Reason via Program Generation, Emulation, and Search</title><link>https://deep-diver.github.io/neurips2024/posters/te6vagjf6g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/te6vagjf6g/</guid><description>Language models excel at generating programs for algorithmic tasks, but struggle with soft reasoning. COGEX leverages pseudo-programs and program emulation to tackle these tasks, while COTACS searches&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/te6vagjf6g/cover.png"/></item><item><title>LESS: Label-Efficient and Single-Stage Referring 3D Segmentation</title><link>https://deep-diver.github.io/neurips2024/posters/hrqaot0nzf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hrqaot0nzf/</guid><description>LESS achieves state-of-the-art Referring 3D Segmentation using only binary masks, significantly reducing labeling effort and improving efficiency with a novel single-stage pipeline.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hrqaot0nzf/cover.png"/></item><item><title>Leveraging Environment Interaction for Automated PDDL Translation and Planning with Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/rzlcqnncqv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rzlcqnncqv/</guid><description>This paper presents a fully automated method for PDDL translation and planning using LLMs and environment interaction, achieving a 66% success rate on challenging PDDL domains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rzlcqnncqv/cover.png"/></item><item><title>Limits of Transformer Language Models on Learning to Compose Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/x7ad0343jz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x7ad0343jz/</guid><description>Large Language Models struggle with compositional tasks, requiring exponentially more data than expected for learning compared to learning sub-tasks individually. This paper reveals surprising sample &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x7ad0343jz/cover.png"/></item><item><title>LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning</title><link>https://deep-diver.github.io/neurips2024/posters/l8ifdx5xnq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/l8ifdx5xnq/</guid><description>LISA, a layerwise importance sampling method, dramatically improves memory-efficient large language model fine-tuning, outperforming existing methods while using less GPU memory.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/l8ifdx5xnq/cover.png"/></item><item><title>Lisa: Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning Attack</title><link>https://deep-diver.github.io/neurips2024/posters/rpchapuxlc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rpchapuxlc/</guid><description>Lisa: a novel lazy safety alignment method safeguards LLMs against harmful fine-tuning attacks by introducing a proximal term to constrain model drift, significantly improving alignment performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rpchapuxlc/cover.png"/></item><item><title>LIVE: Learnable In-Context Vector for Visual Question Answering</title><link>https://deep-diver.github.io/neurips2024/posters/qhremvrzbg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qhremvrzbg/</guid><description>LIVE, a novel learnable in-context vector, significantly improves visual question answering by reducing computational costs and enhancing accuracy compared to traditional ICL methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qhremvrzbg/cover.png"/></item><item><title>LLaMo: Large Language Model-based Molecular Graph Assistant</title><link>https://deep-diver.github.io/neurips2024/posters/wktndu155n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wktndu155n/</guid><description>LLaMo, a novel Large Language Model-based Molecular Graph Assistant, uses multi-level graph projection and instruction tuning to achieve superior performance on diverse molecular tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wktndu155n/cover.png"/></item><item><title>LLM Processes: Numerical Predictive Distributions Conditioned on Natural Language</title><link>https://deep-diver.github.io/neurips2024/posters/hshs7q1njh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hshs7q1njh/</guid><description>LLM Processes leverage LLMs to create probabilistic regression models guided by natural language, enabling seamless integration of expert knowledge and improving prediction accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hshs7q1njh/cover.png"/></item><item><title>LLM-Check: Investigating Detection of Hallucinations in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/lyx4w3cagy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lyx4w3cagy/</guid><description>LLM-Check efficiently detects LLM hallucinations in a single response, using internal model analysis, improving real-time applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lyx4w3cagy/cover.png"/></item><item><title>LLMDFA: Analyzing Dataflow in Code with Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/qz2d8e8whu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qz2d8e8whu/</guid><description>LLMDFA: A novel LLM-powered framework performs compilation-free and customizable dataflow analysis, achieving high accuracy in bug detection by decomposing the task into sub-problems and mitigating L&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qz2d8e8whu/cover.png"/></item><item><title>Loki: Low-rank Keys for Efficient Sparse Attention</title><link>https://deep-diver.github.io/neurips2024/posters/raabeiv71j/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/raabeiv71j/</guid><description>Loki: Low-rank Keys for Efficient Sparse Attention accelerates attention mechanisms in LLMs by exploiting the low-dimensionality of key vectors. It dynamically selects key tokens based on approximate&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/raabeiv71j/cover.png"/></item><item><title>Long-form factuality in large language models</title><link>https://deep-diver.github.io/neurips2024/posters/4m9f8vmt2c/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4m9f8vmt2c/</guid><description>LLMs often generate factually inaccurate long-form text. This work introduces LongFact, a new benchmark dataset of 2280 fact-seeking prompts, and SAFE, a novel automated evaluation method that outperf&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4m9f8vmt2c/cover.png"/></item><item><title>Look, Listen, and Answer: Overcoming Biases for Audio-Visual Question Answering</title><link>https://deep-diver.github.io/neurips2024/posters/twppd9umun/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/twppd9umun/</guid><description>New dataset MUSIC-AVQA-R and a multi-faceted cycle collaborative debiasing strategy significantly improve audio-visual question answering robustness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/twppd9umun/cover.png"/></item><item><title>LoQT: Low-Rank Adapters for Quantized Pretraining</title><link>https://deep-diver.github.io/neurips2024/posters/pnv8c0bu9t/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pnv8c0bu9t/</guid><description>LoQT enables efficient large language model training on consumer hardware via quantized weights and low-rank weight updates, overcoming memory limitations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pnv8c0bu9t/cover.png"/></item><item><title>LoRA-GA: Low-Rank Adaptation with Gradient Approximation</title><link>https://deep-diver.github.io/neurips2024/posters/valawrlhjv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/valawrlhjv/</guid><description>LoRA-GA: A novel initialization method dramatically speeds up low-rank adaptation (LoRA) for LLMs, achieving convergence rates comparable to full fine-tuning while improving performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/valawrlhjv/cover.png"/></item><item><title>LT-Defense: Searching-free Backdoor Defense via Exploiting the Long-tailed Effect</title><link>https://deep-diver.github.io/neurips2024/posters/jdcmwf06c6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jdcmwf06c6/</guid><description>LT-Defense: a searching-free backdoor defense for language models leveraging the long-tailed effect of poisoned data. It achieves 98% accuracy across 1440 models with less than 1% time cost of existin&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jdcmwf06c6/cover.png"/></item><item><title>MACM: Utilizing a Multi-Agent System for Condition Mining in Solving Complex Mathematical Problems</title><link>https://deep-diver.github.io/neurips2024/posters/vr2rdsxtzs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vr2rdsxtzs/</guid><description>Multi-Agent System for Condition Mining (MACM) dramatically boosts large language model accuracy in complex math problem-solving, exceeding existing methods by achieving higher accuracy and better gen&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vr2rdsxtzs/cover.png"/></item><item><title>Make Your LLM Fully Utilize the Context</title><link>https://deep-diver.github.io/neurips2024/posters/ygtvembxtv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ygtvembxtv/</guid><description>FILM-7B, trained with Information-Intensive (IN2) training, significantly overcomes the &amp;rsquo;lost-in-the-middle&amp;rsquo; problem in long-context LLMs, enabling robust information retrieval from all context positi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ygtvembxtv/cover.png"/></item><item><title>MAmmoTH2: Scaling Instructions from the Web</title><link>https://deep-diver.github.io/neurips2024/posters/yvu5dnplqa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yvu5dnplqa/</guid><description>MAmmoTH2: Harvesting 10M web instructions for enhanced LLM reasoning!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yvu5dnplqa/cover.png"/></item><item><title>MatFormer: Nested Transformer for Elastic Inference</title><link>https://deep-diver.github.io/neurips2024/posters/fya6ezmxd5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fya6ezmxd5/</guid><description>MatFormer: Train one universal model, extract hundreds of accurate submodels for elastic inference!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fya6ezmxd5/cover.png"/></item><item><title>Meaningful Learning: Enhancing Abstract Reasoning in Large Language Models via Generic Fact Guidance</title><link>https://deep-diver.github.io/neurips2024/posters/tihifqgoyc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tihifqgoyc/</guid><description>Boosting LLMs&amp;rsquo; abstract reasoning via &amp;lsquo;Meaningful Learning&amp;rsquo;: A new dataset and learning paradigm significantly enhance LLMs&amp;rsquo; capacity for abstract reasoning, moving beyond simple memorization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tihifqgoyc/cover.png"/></item><item><title>Measuring Progress in Dictionary Learning for Language Model Interpretability with Board Game Models</title><link>https://deep-diver.github.io/neurips2024/posters/scedogghcw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/scedogghcw/</guid><description>New metrics and p-annealing improve sparse autoencoder training for better language model interpretability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/scedogghcw/cover.png"/></item><item><title>MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive Clinical Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/w4pibq7bai/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w4pibq7bai/</guid><description>MEDIQ benchmark revolutionizes LLM evaluation by shifting from static to interactive clinical reasoning, revealing LLMs&amp;rsquo; struggles with proactive information-seeking and highlighting the importance of&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w4pibq7bai/cover.png"/></item><item><title>Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length</title><link>https://deep-diver.github.io/neurips2024/posters/xlabmzu4bo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xlabmzu4bo/</guid><description>MEGALODON: A new neural architecture for LLMs, enabling unlimited context length with improved efficiency and accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xlabmzu4bo/cover.png"/></item><item><title>Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration</title><link>https://deep-diver.github.io/neurips2024/posters/pawqvrforj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pawqvrforj/</guid><description>SPV-MIA, a novel membership inference attack, significantly improves the accuracy of identifying training data in fine-tuned LLMs by using self-prompt calibration and probabilistic variation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pawqvrforj/cover.png"/></item><item><title>Memory-Efficient LLM Training with Online Subspace Descent</title><link>https://deep-diver.github.io/neurips2024/posters/p8rtct6g45/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/p8rtct6g45/</guid><description>Online Subspace Descent: a novel memory-efficient LLM training algorithm guaranteed to converge, closing the performance gap with full-rank methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/p8rtct6g45/cover.png"/></item><item><title>Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/zaxumqoaf4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zaxumqoaf4/</guid><description>Mesa-Extrapolation enhances LLM extrapolation using a novel weave position encoding method, boosting performance while significantly reducing memory and inference time.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zaxumqoaf4/cover.png"/></item><item><title>Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving</title><link>https://deep-diver.github.io/neurips2024/posters/d19uyp4hyk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/d19uyp4hyk/</guid><description>LLMs gain math skills via prompt-guided skill labeling and exemplar selection, significantly boosting accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/d19uyp4hyk/cover.png"/></item><item><title>MicroAdam: Accurate Adaptive Optimization with Low Space Overhead and Provable Convergence</title><link>https://deep-diver.github.io/neurips2024/posters/tck41rangk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tck41rangk/</guid><description>MICROADAM: A new Adam optimizer variant dramatically cuts memory usage for training large language models without compromising accuracy or provable convergence.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tck41rangk/cover.png"/></item><item><title>Microstructures and Accuracy of Graph Recall by Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/tnhwg9u767/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tnhwg9u767/</guid><description>LLMs struggle with graph recall, exhibiting biases like favoring triangles and underperforming compared to humans; advanced models show striking domain dependence.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tnhwg9u767/cover.png"/></item><item><title>Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/cej1mypgww/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cej1mypgww/</guid><description>LLMs&amp;rsquo; spatial reasoning abilities are boosted by visualizing their thought processes via &amp;lsquo;Visualization-of-Thought&amp;rsquo; prompting, significantly improving performance on navigation and tiling tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cej1mypgww/cover.png"/></item><item><title>MindMerger: Efficiently Boosting LLM Reasoning in non-English Languages</title><link>https://deep-diver.github.io/neurips2024/posters/oq32ylaou2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oq32ylaou2/</guid><description>MindMerger efficiently boosts LLM reasoning in non-English languages by merging LLMs with external multilingual language understanding capabilities, achieving significant accuracy improvements, especi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oq32ylaou2/cover.png"/></item><item><title>MiniCache: KV Cache Compression in Depth Dimension for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/sgvojdqumt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sgvojdqumt/</guid><description>MiniCache: A novel approach to drastically reduce LLM KV cache memory footprint.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sgvojdqumt/cover.png"/></item><item><title>Mitigating Reward Overoptimization via Lightweight Uncertainty Estimation</title><link>https://deep-diver.github.io/neurips2024/posters/kyio3xh6eb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kyio3xh6eb/</guid><description>ADVPO, a novel method, tackles reward overoptimization in RLHF via a lightweight uncertainty quantification approach, resulting in enhanced LLM performance and alignment with human values.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kyio3xh6eb/cover.png"/></item><item><title>Mixture of Demonstrations for In-Context Learning</title><link>https://deep-diver.github.io/neurips2024/posters/uqxslocw3k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uqxslocw3k/</guid><description>MoD, a novel Mixture of Demonstrations framework, enhances in-context learning by partitioning demonstration pools and employing expert-wise training, achieving state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uqxslocw3k/cover.png"/></item><item><title>Mixture of In-Context Experts Enhance LLMs' Long Context Awareness</title><link>https://deep-diver.github.io/neurips2024/posters/rcphboficn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rcphboficn/</guid><description>MoICE, a novel plug-in, significantly enhances LLMs&amp;rsquo; long context awareness by dynamically routing attention using multiple RoPE angles, achieving superior performance with high inference efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rcphboficn/cover.png"/></item><item><title>Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/pgobeycxzs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pgobeycxzs/</guid><description>BinaryMoS: a novel token-adaptive binarization method that boosts LLM accuracy and efficiency by dynamically merging multiple scaling experts for each token.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pgobeycxzs/cover.png"/></item><item><title>MoEUT: Mixture-of-Experts Universal Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/zxvrkm7bjl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zxvrkm7bjl/</guid><description>MoEUT: Mixture-of-Experts Universal Transformers significantly improves the compute efficiency of Universal Transformers, making them competitive with standard Transformers in large-scale language mod&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zxvrkm7bjl/cover.png"/></item><item><title>MoGU: A Framework for Enhancing Safety of LLMs While Preserving Their Usability</title><link>https://deep-diver.github.io/neurips2024/posters/srfbgijb53/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/srfbgijb53/</guid><description>MoGU: A framework dynamically balances safety and usability in LLMs by routing benign and malicious instructions to different LLM variants, leading to safer, more useful responses.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/srfbgijb53/cover.png"/></item><item><title>MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts</title><link>https://deep-diver.github.io/neurips2024/posters/y929escznj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y929escznj/</guid><description>MomentumSMoE boosts Sparse Mixture of Experts&amp;rsquo; (SMoE) performance by integrating momentum, resulting in more stable training and robust models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y929escznj/cover.png"/></item><item><title>Multi-Head Mixture-of-Experts</title><link>https://deep-diver.github.io/neurips2024/posters/dyz8gjzjtx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dyz8gjzjtx/</guid><description>Multi-Head Mixture-of-Experts (MH-MoE) drastically boosts large language model efficiency by activating almost all expert networks, achieving superior performance compared to existing Sparse Mixture-o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dyz8gjzjtx/cover.png"/></item><item><title>Multi-LLM Debate: Framework, Principals, and Interventions</title><link>https://deep-diver.github.io/neurips2024/posters/sy7esexdpc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sy7esexdpc/</guid><description>Boosting LLM collaboration, this research introduces a novel theoretical framework for multi-LLM debate, revealing key principles like the effect of similar models and interventions to enhance accurac&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sy7esexdpc/cover.png"/></item><item><title>Multi-turn Reinforcement Learning with Preference Human Feedback</title><link>https://deep-diver.github.io/neurips2024/posters/rvsc3hizs4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rvsc3hizs4/</guid><description>Multi-turn RLHF surpasses single-turn methods by aligning LLMs with human preferences across entire conversations, not just individual turns. A novel mirror-descent algorithm, MTPO, is introduced, pr&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rvsc3hizs4/cover.png"/></item><item><title>MutaPLM: Protein Language Modeling for Mutation Explanation and Engineering</title><link>https://deep-diver.github.io/neurips2024/posters/yppclfezgy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yppclfezgy/</guid><description>MutaPLM: a novel protein language model, provides human-understandable mutation explanations and designs novel mutations with desirable properties using a unique protein delta network and chain-of-tho&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yppclfezgy/cover.png"/></item><item><title>MUVERA: Multi-Vector Retrieval via Fixed Dimensional Encoding</title><link>https://deep-diver.github.io/neurips2024/posters/x3ydkrcqr6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x3ydkrcqr6/</guid><description>MUVERA: Revolutionizing multi-vector retrieval with single-vector speed and accuracy!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x3ydkrcqr6/cover.png"/></item><item><title>Navigating Extremes: Dynamic Sparsity in Large Output Spaces</title><link>https://deep-diver.github.io/neurips2024/posters/ra6rzoj2zi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ra6rzoj2zi/</guid><description>SPARTEX achieves memory-efficient extreme multi-label classification by integrating dynamic sparse training with an auxiliary loss function, enabling end-to-end training with millions of labels on com&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ra6rzoj2zi/cover.png"/></item><item><title>Neuro-Symbolic Data Generation for Math Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/cicmzglyzw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cicmzglyzw/</guid><description>Neuro-symbolic framework generates high-quality mathematical datasets, enhancing LLMs&amp;rsquo; mathematical reasoning capabilities and surpassing state-of-the-art counterparts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cicmzglyzw/cover.png"/></item><item><title>No Free Lunch in LLM Watermarking: Trade-offs in Watermarking Design Choices</title><link>https://deep-diver.github.io/neurips2024/posters/riol7kbskv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/riol7kbskv/</guid><description>LLM watermarking faces inherent trade-offs; this paper reveals simple attacks exploiting common design choices, proposing guidelines and defenses for more secure systems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/riol7kbskv/cover.png"/></item><item><title>Noise Contrastive Alignment of Language Models with Explicit Rewards</title><link>https://deep-diver.github.io/neurips2024/posters/kwrldkyvol/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kwrldkyvol/</guid><description>This paper introduces InfoNCA and NCA, novel frameworks for language model alignment using noise contrastive estimation, enabling direct optimization from both explicit rewards and pairwise preference&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kwrldkyvol/cover.png"/></item><item><title>NoiseGPT: Label Noise Detection and Rectification through Probability Curvature</title><link>https://deep-diver.github.io/neurips2024/posters/vrrvjnxgqe/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vrrvjnxgqe/</guid><description>NoiseGPT uses multi-modal LLMs to detect &amp;amp; fix noisy image labels by identifying probability curvature differences between clean and noisy examples.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vrrvjnxgqe/cover.png"/></item><item><title>NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention</title><link>https://deep-diver.github.io/neurips2024/posters/4xdxvqhsbz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4xdxvqhsbz/</guid><description>NoMAD-Attention achieves up to 2x speedup in 4-bit quantized LLaMA inference on CPUs by replacing computationally expensive multiply-add operations with ultra-low-latency in-register lookups.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4xdxvqhsbz/cover.png"/></item><item><title>Normalization Layer Per-Example Gradients are Sufficient to Predict Gradient Noise Scale in Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/s7thlpvh8i/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s7thlpvh8i/</guid><description>By cleverly integrating per-example gradient norm calculations during the backward pass of LayerNorm layers, this research enables efficient and accurate gradient noise scale estimation in Transformer&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/s7thlpvh8i/cover.png"/></item><item><title>OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step</title><link>https://deep-diver.github.io/neurips2024/posters/vaogapvgyr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vaogapvgyr/</guid><description>OccamLLM: LLMs now perform accurate arithmetic in a single step!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vaogapvgyr/cover.png"/></item><item><title>On Giant's Shoulders: Effortless Weak to Strong by Dynamic Logits Fusion</title><link>https://deep-diver.github.io/neurips2024/posters/rmfiqfwawg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rmfiqfwawg/</guid><description>Effortlessly boost large language model performance by dynamically fusing knowledge from smaller, task-specific models – achieving near full fine-tuning results with minimal computational cost!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rmfiqfwawg/cover.png"/></item><item><title>On Softmax Direct Preference Optimization for Recommendation</title><link>https://deep-diver.github.io/neurips2024/posters/qp5vbgtam0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qp5vbgtam0/</guid><description>Softmax-DPO boosts LM-based recommender performance by directly optimizing for personalized ranking using a novel loss function that incorporates multiple negative samples, significantly outperforming&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qp5vbgtam0/cover.png"/></item><item><title>On the Power of Decision Trees in Auto-Regressive Language Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/nge5derseh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nge5derseh/</guid><description>Auto-Regressive Decision Trees (ARDTs) surprisingly outperform Transformers on language tasks!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nge5derseh/cover.png"/></item><item><title>Once Read is Enough: Domain-specific Pretraining-free Language Models with Cluster-guided Sparse Experts for Long-tail Domain Knowledge</title><link>https://deep-diver.github.io/neurips2024/posters/manhbkpiw6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/manhbkpiw6/</guid><description>This research introduces Cluster-guided Sparse Experts (CSE), enabling pretrained language models to effectively learn long-tail domain knowledge without domain-specific pretraining, thus achieving su&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/manhbkpiw6/cover.png"/></item><item><title>One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos</title><link>https://deep-diver.github.io/neurips2024/posters/bqmevgcyvm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bqmevgcyvm/</guid><description>VideoLISA: A video-based multimodal large language model enabling precise, language-instructed video object segmentation with superior performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bqmevgcyvm/cover.png"/></item><item><title>OneBit: Towards Extremely Low-bit Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/zwig9kjfhv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zwig9kjfhv/</guid><description>OneBit achieves surprisingly good performance in 1-bit quantized LLMs by using a novel 1-bit parameter representation method and an effective parameter initialization method.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zwig9kjfhv/cover.png"/></item><item><title>Online Adaptation of Language Models with a Memory of Amortized Contexts</title><link>https://deep-diver.github.io/neurips2024/posters/rifgkckntu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rifgkckntu/</guid><description>MAC: Efficiently updates large language models (LLMs) using a memory of compressed contexts for improved real-time knowledge retention and adaptation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rifgkckntu/cover.png"/></item><item><title>Open LLMs are Necessary for Current Private Adaptations and Outperform their Closed Alternatives</title><link>https://deep-diver.github.io/neurips2024/posters/jf40h5prw0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jf40h5prw0/</guid><description>Open LLMs outperform closed alternatives for private data adaptation, offering superior privacy, performance, and lower costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jf40h5prw0/cover.png"/></item><item><title>Orchid: Flexible and Data-Dependent Convolution for Sequence Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/a75f45dbhk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/a75f45dbhk/</guid><description>Orchid: a novel deep learning architecture using data-dependent convolution achieves quasilinear scalability and outperforms attention-based models on various sequence modeling tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/a75f45dbhk/cover.png"/></item><item><title>Order-Independence Without Fine Tuning</title><link>https://deep-diver.github.io/neurips2024/posters/lq45ar8l7d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lq45ar8l7d/</guid><description>Set-Based Prompting guarantees order-independent LLM outputs by modifying input representations, eliminating unwanted inconsistencies without fine-tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lq45ar8l7d/cover.png"/></item><item><title>PaCE: Parsimonious Concept Engineering for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/lomht16t8r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lomht16t8r/</guid><description>PaCE, a novel activation engineering framework, efficiently aligns LLMs by removing undesirable concepts from activations using sparse coding, achieving state-of-the-art performance while preserving l&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lomht16t8r/cover.png"/></item><item><title>PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition</title><link>https://deep-diver.github.io/neurips2024/posters/vjw4tif8bo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vjw4tif8bo/</guid><description>PaDeLLM-NER massively accelerates LLM-based NER inference by up to 10x, enabling near real-time performance without accuracy loss.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vjw4tif8bo/cover.png"/></item><item><title>Panacea: Pareto Alignment via Preference Adaptation for LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/gl5nt4y8fn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gl5nt4y8fn/</guid><description>Panacea: a novel LLM alignment method achieving Pareto optimality via online preference adaptation using a single model.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gl5nt4y8fn/cover.png"/></item><item><title>Parallelizing Linear Transformers with the Delta Rule over Sequence Length</title><link>https://deep-diver.github.io/neurips2024/posters/y8rm4vnrph/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y8rm4vnrph/</guid><description>DeltaNet, a linear transformer boosting associative recall, now trains efficiently via a novel algorithm, scaling to large language models and outperforming existing linear baselines.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y8rm4vnrph/cover.png"/></item><item><title>PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications</title><link>https://deep-diver.github.io/neurips2024/posters/wvokwq12x5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wvokwq12x5/</guid><description>PediatricsGPT: a novel Chinese pediatric LLM assistant trained on a large, high-quality dataset (PedCorpus) outperforms existing models, paving the way for improved pediatric healthcare.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wvokwq12x5/cover.png"/></item><item><title>Perception of Knowledge Boundary for Large Language Models through Semi-open-ended Question Answering</title><link>https://deep-diver.github.io/neurips2024/posters/li9ythoitp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/li9ythoitp/</guid><description>This study reveals that large language models struggle with semi-open-ended questions, often hallucinating or providing insufficient answers. Researchers explored this by creating a new dataset of su&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/li9ythoitp/cover.png"/></item><item><title>Pipeline Parallelism with Controllable Memory</title><link>https://deep-diver.github.io/neurips2024/posters/vvcnqs8091/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vvcnqs8091/</guid><description>New pipeline parallelism framework achieves up to 55% higher throughput and 50% less memory usage in large language model training by systematically controlling activation memory.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vvcnqs8091/cover.png"/></item><item><title>Plan-on-Graph: Self-Correcting Adaptive Planning of Large Language Model on Knowledge Graphs</title><link>https://deep-diver.github.io/neurips2024/posters/cwcuer6wo5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cwcuer6wo5/</guid><description>Plan-on-Graph (PoG) revolutionizes KG-augmented LLMs with a self-correcting adaptive planning paradigm, enabling more efficient and accurate reasoning over knowledge graphs by dynamically adjusting ex&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cwcuer6wo5/cover.png"/></item><item><title>Predicting the Performance of Foundation Models via Agreement-on-the-Line</title><link>https://deep-diver.github.io/neurips2024/posters/ajx9onwsr4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ajx9onwsr4/</guid><description>Foundation model OOD performance prediction is reliably achieved via ensemble diversity, especially through random linear head initialization, enabling precise estimations without extensive OOD labels&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ajx9onwsr4/cover.png"/></item><item><title>Preference Learning Algorithms Do Not Learn Preference Rankings</title><link>https://deep-diver.github.io/neurips2024/posters/ykj5buexdd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ykj5buexdd/</guid><description>Despite common belief, state-of-the-art preference learning algorithms for LLMs achieve surprisingly low ranking accuracy, highlighting significant flaws in current alignment techniques.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ykj5buexdd/cover.png"/></item><item><title>Pretrained Transformer Efficiently Learns Low-Dimensional Target Functions In-Context</title><link>https://deep-diver.github.io/neurips2024/posters/uhcg5y6fdb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uhcg5y6fdb/</guid><description>Pretrained transformers surprisingly learn low-dimensional nonlinear functions efficiently from few in-context examples, outperforming baseline algorithms.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uhcg5y6fdb/cover.png"/></item><item><title>Prompt Tuning Strikes Back: Customizing Foundation Models with Low-Rank Prompt Adaptation</title><link>https://deep-diver.github.io/neurips2024/posters/symhgilvcv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/symhgilvcv/</guid><description>LoPA: a novel parameter-efficient fine-tuning method matches state-of-the-art performance while requiring no server-side adapters, improving upon traditional prompt tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/symhgilvcv/cover.png"/></item><item><title>Protecting Your LLMs with Information Bottleneck</title><link>https://deep-diver.github.io/neurips2024/posters/u9shp64fjv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/u9shp64fjv/</guid><description>IBProtector shields LLMs from harmful outputs via prompt compression, selectively preserving essential information using a trainable extractor.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/u9shp64fjv/cover.png"/></item><item><title>Provably Transformers Harness Multi-Concept Word Semantics for Efficient In-Context Learning</title><link>https://deep-diver.github.io/neurips2024/posters/57c9mszjj3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/57c9mszjj3/</guid><description>Transformers excel at in-context learning (ICL), solving new tasks with just prompts. This paper provides a mathematical explanation, showing how transformers use multi-concept word semantics to achie&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/57c9mszjj3/cover.png"/></item><item><title>QBB: Quantization with Binary Bases for LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/kw6mrgfx0r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kw6mrgfx0r/</guid><description>QBB: A novel post-training quantization method for LLMs dramatically improves efficiency by replacing multiplications with summations, achieving state-of-the-art results with minimal accuracy loss.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kw6mrgfx0r/cover.png"/></item><item><title>Quantifying and Optimizing Global Faithfulness in Persona-driven Role-playing</title><link>https://deep-diver.github.io/neurips2024/posters/bzpmjmiaz8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bzpmjmiaz8/</guid><description>New APC metric precisely quantifies &amp;amp; optimizes global faithfulness in persona-driven role-playing, offering a fine-grained, explainable evaluation and improving AI character consistency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bzpmjmiaz8/cover.png"/></item><item><title>Query-Based Adversarial Prompt Generation</title><link>https://deep-diver.github.io/neurips2024/posters/jbf3eiyd2x/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jbf3eiyd2x/</guid><description>Researchers developed a query-based attack that generates adversarial prompts, fooling language models into producing harmful outputs with significantly higher success rates than previous methods, eff&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jbf3eiyd2x/cover.png"/></item><item><title>RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/s1fc92uemc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s1fc92uemc/</guid><description>RankRAG: One LLM, dual-purpose instruction-tuning for superior RAG!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/s1fc92uemc/cover.png"/></item><item><title>Reasons and Solutions for the Decline in Model Performance after Editing</title><link>https://deep-diver.github.io/neurips2024/posters/xjxygdfm5m/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xjxygdfm5m/</guid><description>Boosting large language model performance after knowledge editing: A new method (D4S) minimizes model damage by regulating the explosive growth of parameter layers, enabling multiple effective edits.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xjxygdfm5m/cover.png"/></item><item><title>Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training</title><link>https://deep-diver.github.io/neurips2024/posters/yss1z5udby/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yss1z5udby/</guid><description>Overparameterized neural networks surprisingly recover from catastrophic interference when trained cyclically on repeated data sequences, exhibiting anticipatory knowledge reactivation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yss1z5udby/cover.png"/></item><item><title>Recursive Introspection: Teaching Language Model Agents How to Self-Improve</title><link>https://deep-diver.github.io/neurips2024/posters/drc9pzwbwr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/drc9pzwbwr/</guid><description>RISE: Recursive Introspection teaches LLMs to iteratively improve their responses, enabling self-correction and enhanced performance on challenging reasoning tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/drc9pzwbwr/cover.png"/></item><item><title>Reducing Transformer Key-Value Cache Size with Cross-Layer Attention</title><link>https://deep-diver.github.io/neurips2024/posters/m2uzlroqic/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/m2uzlroqic/</guid><description>Cross-Layer Attention (CLA) shrinks Transformer Key-Value cache 2x, improving LLMs&amp;rsquo; memory efficiency without accuracy loss.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/m2uzlroqic/cover.png"/></item><item><title>Reference Trustable Decoding: A Training-Free Augmentation Paradigm for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/qhrlfdhklu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qhrlfdhklu/</guid><description>Reference Trustable Decoding (RTD) revolutionizes large language model adaptation by offering a training-free method, enabling efficient and cost-effective task adaptation without parameter adjustment&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qhrlfdhklu/cover.png"/></item><item><title>Reflective Multi-Agent Collaboration based on Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/wwiar5mqxq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wwiar5mqxq/</guid><description>COPPER enhances LLM-based multi-agent collaboration via a self-reflection mechanism and counterfactual PPO. It improves reflection quality, alleviates credit assignment issues, and shows strong perfo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wwiar5mqxq/cover.png"/></item><item><title>Reinforcing LLM Agents via Policy Optimization with Action Decomposition</title><link>https://deep-diver.github.io/neurips2024/posters/hz6csigmyu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hz6csigmyu/</guid><description>POAD enhances LLM agents by decomposing language agent optimization to the token level, achieving finer-grained credit assignment and improved learning efficiency and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hz6csigmyu/cover.png"/></item><item><title>Representation Noising: A Defence Mechanism Against Harmful Finetuning</title><link>https://deep-diver.github.io/neurips2024/posters/ep9auejqfg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ep9auejqfg/</guid><description>RepNoise: a novel defense against harmful fine-tuning of LLMs by removing information about harmful representations, generalizing across different harmful tasks, and maintaining LLM capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ep9auejqfg/cover.png"/></item><item><title>Rethinking LLM Memorization through the Lens of Adversarial Compression</title><link>https://deep-diver.github.io/neurips2024/posters/kfmrmvzazy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kfmrmvzazy/</guid><description>Researchers propose Adversarial Compression Ratio (ACR) to assess LLM memorization, offering an adversarial, flexible, and computationally efficient method for monitoring data misuse and compliance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kfmrmvzazy/cover.png"/></item><item><title>Rethinking Memory and Communication Costs for Efficient Data Parallel Training of Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/4un2td9bne/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4un2td9bne/</guid><description>PaRO boosts LLM training speed by up to 266% through refined model state partitioning and optimized communication.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4un2td9bne/cover.png"/></item><item><title>Reversing the Forget-Retain Objectives: An Efficient LLM Unlearning Framework from Logit Difference</title><link>https://deep-diver.github.io/neurips2024/posters/tydr1ltwqh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tydr1ltwqh/</guid><description>Reverse the forget-retain objectives for efficient LLM unlearning!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tydr1ltwqh/cover.png"/></item><item><title>RSA: Resolving Scale Ambiguities in Monocular Depth Estimators through Language Descriptions</title><link>https://deep-diver.github.io/neurips2024/posters/vh7gcadhao/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vh7gcadhao/</guid><description>RSA: Language unlocks metric depth from single images!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vh7gcadhao/cover.png"/></item><item><title>S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by Structured Sparsity</title><link>https://deep-diver.github.io/neurips2024/posters/leule8s4xq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/leule8s4xq/</guid><description>S2FT: Structured Sparse Fine-Tuning achieves state-of-the-art LLM fine-tuning performance, training efficiency, and inference scalability by selecting sparsely and computing densely.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/leule8s4xq/cover.png"/></item><item><title>SafeWorld: Geo-Diverse Safety Alignment</title><link>https://deep-diver.github.io/neurips2024/posters/vzqmiodgbg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vzqmiodgbg/</guid><description>SAFEWORLD: a new benchmark reveals and fixes LLMs&amp;rsquo; struggle with diverse safety standards.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vzqmiodgbg/cover.png"/></item><item><title>Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/pf4oujyn4q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pf4oujyn4q/</guid><description>Direct Alignment Algorithms (DAAs) for LLM alignment suffer from over-optimization, even without explicit reward models; this paper empirically demonstrates this and proposes scaling laws to understan&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pf4oujyn4q/cover.png"/></item><item><title>Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies</title><link>https://deep-diver.github.io/neurips2024/posters/skckpr8crl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/skckpr8crl/</guid><description>Boosting LLM performance: This research shows how larger language models need bigger vocabularies for optimal efficiency and performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/skckpr8crl/cover.png"/></item><item><title>SDP4Bit: Toward 4-bit Communication Quantization in Sharded Data Parallelism for LLM Training</title><link>https://deep-diver.github.io/neurips2024/posters/peeqnxlsck/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/peeqnxlsck/</guid><description>SDP4Bit achieves up to 4.08x speedup in LLM training by quantizing weight differences and gradients to ~4 bits, maintaining accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/peeqnxlsck/cover.png"/></item><item><title>Search for Efficient Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/lxsmlxlvks/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lxsmlxlvks/</guid><description>Training-free architecture search finds optimal subnets in LLMs, boosting inference speed and slashing memory needs without retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lxsmlxlvks/cover.png"/></item><item><title>SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware Self-Reflection</title><link>https://deep-diver.github.io/neurips2024/posters/qnieopt4fg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qnieopt4fg/</guid><description>SelectIT leverages LLMs&amp;rsquo; intrinsic uncertainty to efficiently select high-quality instruction tuning data, enhancing model performance without extra resources.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qnieopt4fg/cover.png"/></item><item><title>Selective Attention: Enhancing Transformer through Principled Context Control</title><link>https://deep-diver.github.io/neurips2024/posters/qbqlcwmxff/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qbqlcwmxff/</guid><description>Enhance Transformer models via Selective Self-Attention (SSA), a principled context control method that boosts accuracy and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qbqlcwmxff/cover.png"/></item><item><title>SELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures</title><link>https://deep-diver.github.io/neurips2024/posters/brovxhmzyk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/brovxhmzyk/</guid><description>LLMs self-discover optimal reasoning structures for complex problems, boosting performance by up to 32% compared to existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/brovxhmzyk/cover.png"/></item><item><title>Self-Guiding Exploration for Combinatorial Problems</title><link>https://deep-diver.github.io/neurips2024/posters/bgogknwhbi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bgogknwhbi/</guid><description>LLMs excel at reasoning tasks, but their application to combinatorial problems (CPs) is underexplored. This paper introduces Self-Guiding Exploration (SGE), a novel prompting strategy that significan&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bgogknwhbi/cover.png"/></item><item><title>Self-playing Adversarial Language Game Enhances LLM Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/ocgksh7ys2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ocgksh7ys2/</guid><description>Self-play adversarial language game boosts LLM reasoning!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ocgksh7ys2/cover.png"/></item><item><title>Self-Taught Recognizer: Toward Unsupervised Adaptation for Speech Foundation Models</title><link>https://deep-diver.github.io/neurips2024/posters/oloqhrbxye/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oloqhrbxye/</guid><description>STAR, a novel unsupervised adaptation framework, drastically improves automatic speech recognition (ASR) robustness across diverse domains using only unlabeled data and outperforms existing self-train&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oloqhrbxye/cover.png"/></item><item><title>SelfCodeAlign: Self-Alignment for Code Generation</title><link>https://deep-diver.github.io/neurips2024/posters/xxrnuu7xtl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xxrnuu7xtl/</guid><description>SelfCodeAlign is a novel self-alignment method for code generation LLMs that surpasses existing methods by avoiding reliance on expensive human annotation or proprietary LLMs. The method achieves thi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xxrnuu7xtl/cover.png"/></item><item><title>Semantic Routing via Autoregressive Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/jvlrufjmbi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jvlrufjmbi/</guid><description>Learning-based semantic routing, a scalable approach to route planning using rich user queries, is introduced, accompanied by a large-scale public benchmark and a proof-of-concept model demonstrating &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jvlrufjmbi/cover.png"/></item><item><title>SemCoder: Training Code Language Models with Comprehensive Semantics Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/pnlchqrm69/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pnlchqrm69/</guid><description>SEMCODER: A novel 6.7B parameter code LLM surpasses GPT-3.5-turbo&amp;rsquo;s performance on code generation and execution reasoning by employing &amp;lsquo;monologue reasoning&amp;rsquo;—training the model to verbally explain cod&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pnlchqrm69/cover.png"/></item><item><title>SGLang: Efficient Execution of Structured Language Model Programs</title><link>https://deep-diver.github.io/neurips2024/posters/vqkakqibpq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vqkakqibpq/</guid><description>SGLang: A new system boosts LLM program execution speed by up to 6.4x, simplifying complex LLM application programming.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vqkakqibpq/cover.png"/></item><item><title>ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization</title><link>https://deep-diver.github.io/neurips2024/posters/jnl6h3u3ow/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jnl6h3u3ow/</guid><description>ShiftAddLLM accelerates pretrained LLMs via post-training, multiplication-less reparameterization, achieving significant memory and energy reductions with comparable or better accuracy than existing m&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jnl6h3u3ow/cover.png"/></item><item><title>SILENCE: Protecting privacy in offloaded speech understanding on resource-constrained devices</title><link>https://deep-diver.github.io/neurips2024/posters/tkulgndwwn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tkulgndwwn/</guid><description>SILENCE, a novel lightweight system, protects user privacy in offloaded speech understanding on resource-constrained devices by selectively masking short-term audio details without impacting long-term&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tkulgndwwn/cover.png"/></item><item><title>Simple and Effective Masked Diffusion Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/l4uaar4arm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/l4uaar4arm/</guid><description>Simple masked discrete diffusion models achieve state-of-the-art language modeling results, closing the performance gap with autoregressive methods by using a novel training recipe and a Rao-Blackwell&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/l4uaar4arm/cover.png"/></item><item><title>Simplified and Generalized Masked Diffusion for Discrete Data</title><link>https://deep-diver.github.io/neurips2024/posters/xcqsofht4g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xcqsofht4g/</guid><description>Simplified and generalized masked diffusion models achieve state-of-the-art results in discrete data generation, surpassing previous methods in text and image modeling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xcqsofht4g/cover.png"/></item><item><title>SLED: Self Logits Evolution Decoding for Improving Factuality in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/t7wvjstsiv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/t7wvjstsiv/</guid><description>Self Logits Evolution Decoding (SLED) boosts LLM factuality by up to 20% without extra data or fine-tuning!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/t7wvjstsiv/cover.png"/></item><item><title>SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models</title><link>https://deep-diver.github.io/neurips2024/posters/k9iglmqpif/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/k9iglmqpif/</guid><description>SMALLTOLARGE (S2L) revolutionizes large language model (LLM) fine-tuning by using a small model to summarize training loss trajectories, enabling efficient data selection for larger models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/k9iglmqpif/cover.png"/></item><item><title>Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space</title><link>https://deep-diver.github.io/neurips2024/posters/clxclpfarc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/clxclpfarc/</guid><description>Open-source LLMs are vulnerable to embedding space attacks, which efficiently bypass safety mechanisms and enable data extraction, even after unlearning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/clxclpfarc/cover.png"/></item><item><title>Source Code Foundation Models are Transferable Binary Analysis Knowledge Bases</title><link>https://deep-diver.github.io/neurips2024/posters/qppvdzphsl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qppvdzphsl/</guid><description>ProRec, a novel framework, bridges the binary-source semantic gap by using a binary-source encoder-decoder model and LLMs, achieving significant improvements in zero-shot binary summarization and func&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qppvdzphsl/cover.png"/></item><item><title>SpaceByte: Towards Deleting Tokenization from Large Language Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/kee4iup20i/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kee4iup20i/</guid><description>SpaceByte: A novel byte-level decoder architecture achieving near-tokenized-model performance without tokenization!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kee4iup20i/cover.png"/></item><item><title>SparseLLM: Towards Global Pruning of Pre-trained Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/oxhyyhp4zb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oxhyyhp4zb/</guid><description>SparseLLM globally prunes large language models efficiently by decomposing the problem into manageable subproblems, achieving significant performance improvements, especially at high sparsity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oxhyyhp4zb/cover.png"/></item><item><title>Speaking Your Language: Spatial Relationships in Interpretable Emergent Communication</title><link>https://deep-diver.github.io/neurips2024/posters/vip8iwmzln/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vip8iwmzln/</guid><description>AI agents developed a communication system using spatial relationships, achieving over 90% accuracy in conveying relative positions of objects within a scene.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vip8iwmzln/cover.png"/></item><item><title>SpecExec: Massively Parallel Speculative Decoding For Interactive LLM Inference on Consumer Devices</title><link>https://deep-diver.github.io/neurips2024/posters/jahnsz9dvg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jahnsz9dvg/</guid><description>SpecExec achieves massively parallel speculative decoding, enabling interactive 50B+ parameter LLM inference on consumer devices at 4-6 tokens/second.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jahnsz9dvg/cover.png"/></item><item><title>Spectral Editing of Activations for Large Language Model Alignment</title><link>https://deep-diver.github.io/neurips2024/posters/pqyceea87j/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pqyceea87j/</guid><description>Spectral Editing of Activations (SEA) improves large language model truthfulness and fairness by projecting input representations to maximize covariance with positive demonstrations while minimizing c&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pqyceea87j/cover.png"/></item><item><title>SpeechAlign: Aligning Speech Generation to Human Preferences</title><link>https://deep-diver.github.io/neurips2024/posters/skcbzr8pyd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/skcbzr8pyd/</guid><description>SpeechAlign: Iteratively aligning speech generation models to human preferences via preference optimization, bridging distribution gaps for improved speech quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/skcbzr8pyd/cover.png"/></item><item><title>SpeedLoader: An I/O efficient scheme for heterogeneous and distributed LLM operation</title><link>https://deep-diver.github.io/neurips2024/posters/y2i0fy4sm7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y2i0fy4sm7/</guid><description>SpeedLoader: A groundbreaking I/O efficient scheme dramatically boosts LLM training &amp;amp; inference speed on diverse hardware, even with limited resources!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y2i0fy4sm7/cover.png"/></item><item><title>SpikedAttention: Training-Free and Fully Spike-Driven Transformer-to-SNN Conversion with Winner-Oriented Spike Shift for Softmax Operation</title><link>https://deep-diver.github.io/neurips2024/posters/fs28jccjj5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fs28jccjj5/</guid><description>SpikedAttention: Training-free transformer-to-SNN conversion achieving state-of-the-art accuracy and 42% energy reduction!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fs28jccjj5/cover.png"/></item><item><title>SS1: Accelerating Inference with Fast and Expressive Sketch Structured Transform</title><link>https://deep-diver.github.io/neurips2024/posters/nrgyogu7zp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nrgyogu7zp/</guid><description>SS1: A novel GPU-friendly operator accelerates deep learning inference by leveraging structured parameter sharing, achieving superior quality-efficiency tradeoffs compared to existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nrgyogu7zp/cover.png"/></item><item><title>SSDM: Scalable Speech Dysfluency Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/ixehb4ncvy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ixehb4ncvy/</guid><description>SSDM: Scalable Speech Dysfluency Modeling tackles challenges in speech dysfluency analysis by using articulatory gestures for scalable alignment, a connectionist subsequence aligner for efficient dysf&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ixehb4ncvy/cover.png"/></item><item><title>Star-Agents: Automatic Data Optimization with LLM Agents for Instruction Tuning</title><link>https://deep-diver.github.io/neurips2024/posters/jxsxgt80sv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jxsxgt80sv/</guid><description>Star-Agents automates data optimization for instruction-tuned LLMs via multi-agent collaboration, achieving a 12% average performance boost.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jxsxgt80sv/cover.png"/></item><item><title>Stealth edits to large language models</title><link>https://deep-diver.github.io/neurips2024/posters/qap6ryyijc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qap6ryyijc/</guid><description>Researchers unveil stealth edits for large language models, offering a new metric to assess editability and reveal vulnerability to malicious attacks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qap6ryyijc/cover.png"/></item><item><title>Streaming Long Video Understanding with Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/axx62cqjpa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/axx62cqjpa/</guid><description>VideoStreaming, a novel vision-language model, enables efficient and accurate understanding of arbitrarily long videos using a constant number of tokens via streaming encoding and adaptive memory sele&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/axx62cqjpa/cover.png"/></item><item><title>StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses</title><link>https://deep-diver.github.io/neurips2024/posters/envvjpx97o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/envvjpx97o/</guid><description>StreamingDialogue revolutionizes prolonged dialogue learning by compressing long contexts into conversational attention sinks, minimizing information loss and achieving a 4x speedup with 18x less memo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/envvjpx97o/cover.png"/></item><item><title>Stress-Testing Capability Elicitation With Password-Locked Models</title><link>https://deep-diver.github.io/neurips2024/posters/zzooqd6r1b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zzooqd6r1b/</guid><description>Fine-tuning, even on a single demonstration, effectively uncovers hidden LLM capabilities, surpassing simple prompting methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zzooqd6r1b/cover.png"/></item><item><title>Superposed Decoding: Multiple Generations from a Single Autoregressive Inference Pass</title><link>https://deep-diver.github.io/neurips2024/posters/ksokkhm9i7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ksokkhm9i7/</guid><description>Generate multiple text drafts from a single language model pass with Superposed Decoding, significantly boosting efficiency!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ksokkhm9i7/cover.png"/></item><item><title>SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors</title><link>https://deep-diver.github.io/neurips2024/posters/us0pwibzc0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/us0pwibzc0/</guid><description>SVFT: a novel parameter-efficient fine-tuning method achieves near full fine-tuning accuracy using only 0.006% to 0.25% of parameters, significantly outperforming existing techniques.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/us0pwibzc0/cover.png"/></item><item><title>Symbolic Regression with a Learned Concept Library</title><link>https://deep-diver.github.io/neurips2024/posters/b7s4jjglvl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/b7s4jjglvl/</guid><description>LASR, a novel symbolic regression method, uses zero-shot LLM queries to discover and evolve abstract concepts, substantially outperforming state-of-the-art approaches and discovering a new LLM scaling&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/b7s4jjglvl/cover.png"/></item><item><title>Synatra: Turning Indirect Knowledge into Direct Demonstrations for Digital Agents at Scale</title><link>https://deep-diver.github.io/neurips2024/posters/kjnezwriqn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kjnezwriqn/</guid><description>Synatra synthesizes high-quality digital agent training data from online tutorials and web pages, significantly improving agent performance on complex web-based tasks at a fraction of the cost of huma&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kjnezwriqn/cover.png"/></item><item><title>Synthesize, Partition, then Adapt: Eliciting Diverse Samples from Foundation Models</title><link>https://deep-diver.github.io/neurips2024/posters/sp8whisnu9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sp8whisnu9/</guid><description>The Synthesize-Partition-Adapt (SPA) framework leverages synthetic data to generate diverse, high-quality responses from foundation models, enriching user experience.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sp8whisnu9/cover.png"/></item><item><title>TableRAG: Million-Token Table Understanding with Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/41lovpoco5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/41lovpoco5/</guid><description>TableRAG, a novel Retrieval-Augmented Generation framework, achieves state-of-the-art performance in large-scale table understanding by efficiently integrating schema and cell retrieval with language &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/41lovpoco5/cover.png"/></item><item><title>TAIA: Large Language Models are Out-of-Distribution Data Learners</title><link>https://deep-diver.github.io/neurips2024/posters/xxsme6ge1g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xxsme6ge1g/</guid><description>LLMs struggle with downstream tasks using mismatched data. TAIA, a novel inference-time method, solves this by selectively using only attention parameters during inference after training all parameter&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xxsme6ge1g/cover.png"/></item><item><title>Talking Heads: Understanding Inter-Layer Communication in Transformer Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/lusx0chtsl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lusx0chtsl/</guid><description>Transformer Language Models&amp;rsquo; (LMs) sensitivity to seemingly arbitrary prompt changes is explained by identifying low-rank communication channels between layers. By decomposing attention heads, resear&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lusx0chtsl/cover.png"/></item><item><title>Teach Better or Show Smarter? On Instructions and Exemplars in Automatic Prompt Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/idtojvwvnx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/idtojvwvnx/</guid><description>Smart prompt engineering is key to unlocking LLMs&amp;rsquo; full potential. This paper reveals that cleverly selecting examples (exemplar optimization) can outperform optimizing instructions alone, even with S&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/idtojvwvnx/cover.png"/></item><item><title>Temporal Sentence Grounding with Relevance Feedback in Videos</title><link>https://deep-diver.github.io/neurips2024/posters/eoonmxzzno/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/eoonmxzzno/</guid><description>RaTSG network tackles Temporal Sentence Grounding with Relevance Feedback (TSG-RF) by discerning query relevance at multiple granularities before selectively grounding segments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/eoonmxzzno/cover.png"/></item><item><title>The Closeness of In-Context Learning and Weight Shifting for Softmax Regression</title><link>https://deep-diver.github.io/neurips2024/posters/sfaeenfeyw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sfaeenfeyw/</guid><description>Softmax regression reveals in-context learning&amp;rsquo;s surprising similarity to gradient descent in self-attention Transformers, showing the models&amp;rsquo; remarkable learning capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sfaeenfeyw/cover.png"/></item><item><title>The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains</title><link>https://deep-diver.github.io/neurips2024/posters/qart6qtiqj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qart6qtiqj/</guid><description>Transformers learn to perform in-context learning of Markov chains hierarchically, progressing from simpler unigram strategies to more complex bigram solutions, with the presence of simpler solutions &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qart6qtiqj/cover.png"/></item><item><title>The Expressive Capacity of State Space Models: A Formal Language Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/ev5yirjpdy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ev5yirjpdy/</guid><description>State-space models (SSMs) rival transformers in language modeling, but their capabilities remain unclear; this paper rigorously analyzes SSM expressivity, revealing unique strengths and limitations, i&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ev5yirjpdy/cover.png"/></item><item><title>The Fine-Grained Complexity of Gradient Computation for Training Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/up4twnwrol/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/up4twnwrol/</guid><description>New research precisely defines the computational limits of training large language models, revealing a sharp threshold based on parameter matrix entries, paving the way for faster algorithms.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/up4twnwrol/cover.png"/></item><item><title>The Impact of Initialization on LoRA Finetuning Dynamics</title><link>https://deep-diver.github.io/neurips2024/posters/sn3uryritk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sn3uryritk/</guid><description>LoRA&amp;rsquo;s initialization significantly impacts finetuning; initializing matrix A randomly and B to zero yields better performance than vice-versa due to enabling larger learning rates.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sn3uryritk/cover.png"/></item><item><title>The Importance of Online Data: Understanding Preference Fine-tuning via Coverage</title><link>https://deep-diver.github.io/neurips2024/posters/hbj86rmdz8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hbj86rmdz8/</guid><description>Hybrid Preference Optimization (HyPO) outperforms existing offline methods for fine-tuning LLMs by leveraging both offline and online data, achieving better performance and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hbj86rmdz8/cover.png"/></item><item><title>The Mamba in the Llama: Distilling and Accelerating Hybrid Models</title><link>https://deep-diver.github.io/neurips2024/posters/uazhodjalu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uazhodjalu/</guid><description>This research dramatically accelerates and improves hybrid language models by distilling large Transformers into linear RNNs, achieving performance comparable to the original Transformer with signific&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uazhodjalu/cover.png"/></item><item><title>The Representation Landscape of Few-Shot Learning and Fine-Tuning in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/nmukwoohfo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nmukwoohfo/</guid><description>LLMs use different internal structures for few-shot learning and fine-tuning, showing a transition in the middle network layers that impacts information encoding and task solving strategies.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nmukwoohfo/cover.png"/></item><item><title>Thinking Forward: Memory-Efficient Federated Finetuning of Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/dgqtja9x2c/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dgqtja9x2c/</guid><description>SPRY: A memory-efficient federated learning algorithm for finetuning LLMs on resource-constrained devices, achieving high accuracy and speed.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dgqtja9x2c/cover.png"/></item><item><title>Thought of Search: Planning with Language Models Through The Lens of Efficiency</title><link>https://deep-diver.github.io/neurips2024/posters/lncsya5us1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lncsya5us1/</guid><description>This paper introduces &amp;lsquo;Thought of Search,&amp;rsquo; a novel, efficient planning approach using LLMs that prioritizes soundness and completeness. It leverages LLMs to generate Python code for search components,&amp;hellip;</description></item><item><title>To Believe or Not to Believe Your LLM: IterativePrompting for Estimating Epistemic Uncertainty</title><link>https://deep-diver.github.io/neurips2024/posters/k6iyufwdi9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/k6iyufwdi9/</guid><description>This paper introduces an innovative iterative prompting method for estimating epistemic uncertainty in LLMs, enabling reliable detection of hallucinations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/k6iyufwdi9/cover.png"/></item><item><title>Token Merging for Training-Free Semantic Binding in Text-to-Image Synthesis</title><link>https://deep-diver.github.io/neurips2024/posters/trrwoa9e80/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/trrwoa9e80/</guid><description>ToMe: a novel training-free method dramatically improves semantic binding in text-to-image synthesis by intelligently merging related tokens, ensuring accurate alignment between generated images and t&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/trrwoa9e80/cover.png"/></item><item><title>Toward Efficient Inference for Mixture of Experts</title><link>https://deep-diver.github.io/neurips2024/posters/stxtbqytwx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/stxtbqytwx/</guid><description>Unlocking the speed and efficiency of Mixture-of-Expert models, this research unveils novel optimization techniques, achieving dramatic improvements in inference throughput and resource usage.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/stxtbqytwx/cover.png"/></item><item><title>Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing</title><link>https://deep-diver.github.io/neurips2024/posters/tpdj2qhkob/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tpdj2qhkob/</guid><description>ALPHALLM boosts LLM performance in complex reasoning tasks by using imagination, search, and criticism to create a self-improving loop, eliminating the need for extra training data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tpdj2qhkob/cover.png"/></item><item><title>Towards a Theoretical Understanding of the 'Reversal Curse' via Training Dynamics</title><link>https://deep-diver.github.io/neurips2024/posters/qowf3lo6m7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qowf3lo6m7/</guid><description>LLMs struggle with simple logical reasoning due to the &amp;lsquo;reversal curse.&amp;rsquo; This paper reveals that weight asymmetry during training is the culprit, offering a new theoretical perspective and potential s&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qowf3lo6m7/cover.png"/></item><item><title>Towards Neuron Attributions in Multi-Modal Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/jmjvfp4bh6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jmjvfp4bh6/</guid><description>NAM: a novel neuron attribution method for MLLMs, revealing modality-specific semantic knowledge and enabling multi-modal knowledge editing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jmjvfp4bh6/cover.png"/></item><item><title>Towards Understanding How Transformers Learn In-context Through a Representation Learning Lens</title><link>https://deep-diver.github.io/neurips2024/posters/db6gwsdxkl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/db6gwsdxkl/</guid><description>Transformers&amp;rsquo; in-context learning (ICL) is explained using representation learning, revealing its ICL process as gradient descent on a dual model and offering modifiable attention layers for enhanced &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/db6gwsdxkl/cover.png"/></item><item><title>Training Dynamics of Transformers to Recognize Word Co-occurrence via Gradient Flow Analysis</title><link>https://deep-diver.github.io/neurips2024/posters/w6q46islsr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w6q46islsr/</guid><description>Researchers reveal how transformers learn word co-occurrence using a novel gradient flow analysis, uncovering a two-phase training process that leads to near-minimum loss and improved model performanc&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w6q46islsr/cover.png"/></item><item><title>Transformers Can Do Arithmetic with the Right Embeddings</title><link>https://deep-diver.github.io/neurips2024/posters/aiynlwxudo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/aiynlwxudo/</guid><description>Researchers enhanced transformer performance on arithmetic tasks by introducing Abacus Embeddings, which encode each digit&amp;rsquo;s position, enabling improved generalization and unlocking multi-step reasoni&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/aiynlwxudo/cover.png"/></item><item><title>Transformers Represent Belief State Geometry in their Residual Stream</title><link>https://deep-diver.github.io/neurips2024/posters/yib7rel8uc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yib7rel8uc/</guid><description>Transformers encode information beyond next-token prediction by linearly representing belief state geometry in their residual stream, even with complex fractal structures.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yib7rel8uc/cover.png"/></item><item><title>TransVIP: Speech to Speech Translation System with Voice and Isochrony Preservation</title><link>https://deep-diver.github.io/neurips2024/posters/zpvtrqvx5b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zpvtrqvx5b/</guid><description>TransVIP: groundbreaking speech-to-speech translation system preserving voice &amp;amp; isochrony, outperforming current state-of-the-art models!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zpvtrqvx5b/cover.png"/></item><item><title>Tree of Attacks: Jailbreaking Black-Box LLMs Automatically</title><link>https://deep-diver.github.io/neurips2024/posters/som3vngoh5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/som3vngoh5/</guid><description>TAP: automated jailbreaking of black-box LLMs with high success rates, using fewer queries than previous methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/som3vngoh5/cover.png"/></item><item><title>TSDS: Data Selection for Task-Specific Model Finetuning</title><link>https://deep-diver.github.io/neurips2024/posters/wjbthluszu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wjbthluszu/</guid><description>TSDS: A novel framework selects optimal training data for efficient large language model finetuning using only a few examples, boosting performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wjbthluszu/cover.png"/></item><item><title>Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/cvpuve1n22/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cvpuve1n22/</guid><description>Uncertainty of Thoughts (UoT) algorithm significantly boosts LLMs&amp;rsquo; information-seeking abilities, leading to substantial performance gains across diverse tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cvpuve1n22/cover.png"/></item><item><title>Unchosen Experts Can Contribute Too: Unleashing MoE Models’ Power by Self-Contrast</title><link>https://deep-diver.github.io/neurips2024/posters/c1d3vvfdvg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/c1d3vvfdvg/</guid><description>Self-Contrast Mixture-of-Experts (SCMoE) boosts MoE model reasoning by cleverly using &amp;lsquo;unchosen&amp;rsquo; experts during inference. This training-free method contrasts outputs from strong and weak expert acti&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/c1d3vvfdvg/cover.png"/></item><item><title>Understanding and Minimising Outlier Features in Transformer Training</title><link>https://deep-diver.github.io/neurips2024/posters/npjq6qs4bg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/npjq6qs4bg/</guid><description>New methods minimize outlier features in transformer training, improving quantization and efficiency without sacrificing convergence speed.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/npjq6qs4bg/cover.png"/></item><item><title>Understanding Information Storage and Transfer in Multi-Modal Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/s63dtq0mwa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s63dtq0mwa/</guid><description>Researchers unveil how multi-modal LLMs process information, revealing that early layers are key for storage, and introduce MULTEDIT, a model-editing algorithm for correcting errors and inserting new &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/s63dtq0mwa/cover.png"/></item><item><title>Understanding the Differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/if7mnxnxrw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/if7mnxnxrw/</guid><description>Unifying framework reveals hidden connections between attention, recurrent, and state-space models, boosting foundation model efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/if7mnxnxrw/cover.png"/></item><item><title>Understanding Transformer Reasoning Capabilities via Graph Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/afzbdw6dsp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/afzbdw6dsp/</guid><description>Transformers excel at graph reasoning, with logarithmic depth proving necessary and sufficient for parallelizable tasks; single-layer transformers solve retrieval tasks efficiently.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/afzbdw6dsp/cover.png"/></item><item><title>Understanding Transformers via N-Gram Statistics</title><link>https://deep-diver.github.io/neurips2024/posters/wcc440cuhx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wcc440cuhx/</guid><description>LLMs&amp;rsquo; inner workings remain elusive. This study uses N-gram statistics to approximate transformer predictions, revealing how LLMs learn from simple to complex statistical rules, and how model variance&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wcc440cuhx/cover.png"/></item><item><title>UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and FFN Manipulation</title><link>https://deep-diver.github.io/neurips2024/posters/luqivmnvix/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/luqivmnvix/</guid><description>UniBias unveils and mitigates LLM bias by identifying and eliminating biased internal components (FFN vectors and attention heads), significantly improving in-context learning performance and robustne&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/luqivmnvix/cover.png"/></item><item><title>Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback</title><link>https://deep-diver.github.io/neurips2024/posters/jmbwtlazjw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jmbwtlazjw/</guid><description>This study disentangles best practices for learning from preference feedback in LLMs, revealing that data quality, algorithm choice, and reward model significantly impact performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jmbwtlazjw/cover.png"/></item><item><title>Unveiling Induction Heads: Provable Training Dynamics and Feature Learning in Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/4fn2res0ma/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4fn2res0ma/</guid><description>Transformers learn complex tasks surprisingly well through in-context learning, but the mechanism remains unclear. This paper proves that a two-layer transformer trained on n-gram Markov chain data co&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4fn2res0ma/cover.png"/></item><item><title>Unveiling LoRA Intrinsic Ranks via Salience Analysis</title><link>https://deep-diver.github.io/neurips2024/posters/vu512k8vrr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vu512k8vrr/</guid><description>SalientLoRA unveils optimal LoRA ranks by analyzing rank salience via time-series analysis, improving fine-tuning efficiency and performance significantly.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vu512k8vrr/cover.png"/></item><item><title>UQE: A Query Engine for Unstructured Databases</title><link>https://deep-diver.github.io/neurips2024/posters/t7sgov5w5z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/t7sgov5w5z/</guid><description>UQE: A novel query engine uses LLMs for efficient and accurate unstructured data analytics, surpassing existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/t7sgov5w5z/cover.png"/></item><item><title>Vaccine: Perturbation-aware Alignment for Large Language Models against Harmful Fine-tuning Attack</title><link>https://deep-diver.github.io/neurips2024/posters/lpxdzkiant/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lpxdzkiant/</guid><description>Vaccine: a novel technique safeguards LLMs against harmful fine-tuning attacks by creating invariant hidden embeddings.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lpxdzkiant/cover.png"/></item><item><title>VB-LoRA: Extreme Parameter Efficient Fine-Tuning with Vector Banks</title><link>https://deep-diver.github.io/neurips2024/posters/kucy0mw4q3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kucy0mw4q3/</guid><description>VB-LoRA achieves extreme parameter efficiency in fine-tuning LLMs by sharing parameters globally via a vector bank, outperforming state-of-the-art PEFT methods while maintaining comparable or better p&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kucy0mw4q3/cover.png"/></item><item><title>VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections</title><link>https://deep-diver.github.io/neurips2024/posters/bfoqxd7uls/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bfoqxd7uls/</guid><description>VeLoRA: Train massive LLMs efficiently by compressing intermediate activations!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bfoqxd7uls/cover.png"/></item><item><title>Verified Code Transpilation with LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/spwe9slrfg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/spwe9slrfg/</guid><description>LLMLIFT: An LLM-powered approach builds verified lifting tools for DSLs, outperforming prior symbolic methods in benchmark transpilation and requiring less development effort.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/spwe9slrfg/cover.png"/></item><item><title>WAGLE: Strategic Weight Attribution for Effective and Modular Unlearning in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/vzogndjmgh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vzogndjmgh/</guid><description>WAGLE: A novel weight attribution-guided LLM unlearning framework boosts unlearning performance by strategically identifying and manipulating influential model weights, achieving a better balance betw&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vzogndjmgh/cover.png"/></item><item><title>WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off</title><link>https://deep-diver.github.io/neurips2024/posters/hjekhxk2vh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hjekhxk2vh/</guid><description>WaterMax: a novel LLM watermarking scheme achieving high detectability and preserving text quality by cleverly generating multiple texts and selecting the most suitable one.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hjekhxk2vh/cover.png"/></item><item><title>Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with Situation Puzzles</title><link>https://deep-diver.github.io/neurips2024/posters/h024lpf3bz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/h024lpf3bz/</guid><description>SPLAT, a new benchmark using situation puzzles, effectively evaluates and elicits lateral thinking in LLMs through a multi-turn player-judge framework, revealing significant performance improvements o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/h024lpf3bz/cover.png"/></item><item><title>Weak-to-Strong Search: Align Large Language Models via Searching over Small Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/doj6cqwdf1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/doj6cqwdf1/</guid><description>Align LLMs efficiently via test-time search using smaller models!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/doj6cqwdf1/cover.png"/></item><item><title>What Makes and Breaks Safety Fine-tuning? A Mechanistic Study</title><link>https://deep-diver.github.io/neurips2024/posters/jeflv4nrlh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jeflv4nrlh/</guid><description>Safety fine-tuning for LLMs is shown to minimally transform weights, clustering inputs based on safety, but is easily bypassed by adversarial attacks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jeflv4nrlh/cover.png"/></item><item><title>What Rotary Position Embedding Can Tell Us: Identifying Query and Key Weights Corresponding to Basic Syntactic or High-level Semantic Information</title><link>https://deep-diver.github.io/neurips2024/posters/e5mv7iwfvw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/e5mv7iwfvw/</guid><description>LLM fine-tuning made easy! This paper reveals how analyzing weight vector angles in RoPE positional embeddings helps optimize LLMs, reducing parameter count and improving efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/e5mv7iwfvw/cover.png"/></item><item><title>Where does In-context Learning \ Happen in Large Language Models?</title><link>https://deep-diver.github.io/neurips2024/posters/llusjg59an/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/llusjg59an/</guid><description>LLMs learn tasks via in-context learning, but the task recognition location is unknown. This paper reveals that LLMs transition from task recognition to task performance at specific layers, enabling s&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/llusjg59an/cover.png"/></item><item><title>WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment</title><link>https://deep-diver.github.io/neurips2024/posters/qgjsxmhval/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qgjsxmhval/</guid><description>WorldCoder: an LLM agent builds world models via code generation and interaction, proving highly sample-efficient and enabling knowledge transfer.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qgjsxmhval/cover.png"/></item><item><title>YouDream: Generating Anatomically Controllable Consistent Text-to-3D Animals</title><link>https://deep-diver.github.io/neurips2024/posters/rh7tfqhizy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rh7tfqhizy/</guid><description>YOUDREAM generates anatomically consistent, high-quality 3D animal models from text and 2D pose priors, pushing creative boundaries in text-to-3D generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rh7tfqhizy/cover.png"/></item><item><title>Zero-Shot Tokenizer Transfer</title><link>https://deep-diver.github.io/neurips2024/posters/rwbobrsizc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rwbobrsizc/</guid><description>Zero-Shot Tokenizer Transfer (ZeTT) detaches language models from their tokenizers via a hypernetwork, enabling efficient on-the-fly tokenizer swapping without retraining, significantly improving LLM &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rwbobrsizc/cover.png"/></item><item><title>Zipfian Whitening</title><link>https://deep-diver.github.io/neurips2024/posters/pasjxzmjb7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pasjxzmjb7/</guid><description>Zipfian Whitening: Weighting PCA whitening by word frequency dramatically improves NLP task performance, surpassing established baselines and providing a theoretical framework for existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pasjxzmjb7/cover.png"/></item></channel></rss>