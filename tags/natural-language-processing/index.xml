<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Natural Language Processing on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/natural-language-processing/</link><description>Recent content in Natural Language Processing on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/natural-language-processing/index.xml" rel="self" type="application/rss+xml"/><item><title>A teacher-teacher framework for clinical language representation learning</title><link>https://deep-diver.github.io/neurips2024/posters/zdad8zv8tg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zdad8zv8tg/</guid><description>A lightweight knowledge alignment module enables two pre-trained LLMs to mutually learn and improve clinical language representation, exceeding individual model performance on various downstream tasks&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zdad8zv8tg/cover.png"/></item><item><title>A Theoretical Perspective for Speculative Decoding Algorithm</title><link>https://deep-diver.github.io/neurips2024/posters/wsqpnemvlu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wsqpnemvlu/</guid><description>This paper theoretically analyzes speculative decoding, revealing its optimality and providing formulas for expected rejections, paving the way for more efficient large language model inference.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wsqpnemvlu/cover.png"/></item><item><title>Adversarial Representation Engineering: A General Model Editing Framework for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/dq9ji8e9qq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dq9ji8e9qq/</guid><description>Adversarial Representation Engineering (ARE) offers a unified, interpretable approach for editing large language models (LLMs) by using a representation sensor as an editing oracle, enhancing model sa&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dq9ji8e9qq/cover.png"/></item><item><title>Aligning to Thousands of Preferences via System Message Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/recsheq7e8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/recsheq7e8/</guid><description>JANUS, a 7B LLM, achieves high alignment to thousands of user preferences by generalizing from diverse system messages, outperforming existing LLMs on various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/recsheq7e8/cover.png"/></item><item><title>Boosting the Potential of Large Language Models with an Intelligent Information Assistant</title><link>https://deep-diver.github.io/neurips2024/posters/ozy4a11sug/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ozy4a11sug/</guid><description>Boosting LLMs with an intelligent information assistant, ASSISTRAG, significantly improves accuracy and reasoning, especially for less advanced models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ozy4a11sug/cover.png"/></item><item><title>Calibrating Reasoning in Language Models with Internal Consistency</title><link>https://deep-diver.github.io/neurips2024/posters/udzkvmpf3s/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/udzkvmpf3s/</guid><description>LLMs&amp;rsquo; reasoning can be improved by using internal consistency to calibrate their outputs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/udzkvmpf3s/cover.png"/></item><item><title>Can Graph Learning Improve Planning in LLM-based Agents?</title><link>https://deep-diver.github.io/neurips2024/posters/bmos6ggw4j/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bmos6ggw4j/</guid><description>GNNs enhance LLM-based task planning by improving the ability to process task graphs, surpassing existing solutions even without training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bmos6ggw4j/cover.png"/></item><item><title>Can Language Models Learn to Skip Steps?</title><link>https://deep-diver.github.io/neurips2024/posters/w4antvxao9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w4antvxao9/</guid><description>Language models learn to skip steps in reasoning, improving efficiency and generalization, showcasing emergent human-like cognitive abilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w4antvxao9/cover.png"/></item><item><title>Code Repair with LLMs gives an Exploration-Exploitation Tradeoff</title><link>https://deep-diver.github.io/neurips2024/posters/o863gx6dxa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/o863gx6dxa/</guid><description>New program synthesis method, REX, leverages Thompson Sampling to balance exploration and exploitation in iterative LLM code refinement, solving more problems with fewer model calls.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/o863gx6dxa/cover.png"/></item><item><title>Compositional 3D-aware Video Generation with LLM Director</title><link>https://deep-diver.github.io/neurips2024/posters/oqdy2efrja/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oqdy2efrja/</guid><description>LLM-directed compositional 3D-aware video generation (C3V) achieves high-fidelity video generation with diverse motion and flexible concept control by decomposing prompts, generating 3D concepts, and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oqdy2efrja/cover.png"/></item><item><title>DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving</title><link>https://deep-diver.github.io/neurips2024/posters/zlu21oqjd5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zlu21oqjd5/</guid><description>DART-Math tackles LLM limitations in mathematical problem-solving by introducing Difficulty-Aware Rejection Tuning, a novel method that generates high-quality, bias-reduced datasets, resulting in supe&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zlu21oqjd5/cover.png"/></item><item><title>Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum</title><link>https://deep-diver.github.io/neurips2024/posters/r8m9sfymdi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/r8m9sfymdi/</guid><description>This paper introduces dataset decomposition (DD), a novel approach to accelerate LLM training while enhancing performance. DD significantly reduces training time by decomposing datasets into buckets &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/r8m9sfymdi/cover.png"/></item><item><title>Discovery of the Hidden World with Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/w50icqc6qj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w50icqc6qj/</guid><description>COAT leverages LLMs to identify high-level causal factors from unstructured data, enabling causal discovery in real-world scenarios where well-defined variables are lacking.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w50icqc6qj/cover.png"/></item><item><title>Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision</title><link>https://deep-diver.github.io/neurips2024/posters/qwgfh2fttn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qwgfh2fttn/</guid><description>AI alignment beyond human supervision is achieved via easy-to-hard generalization: training reward models on easy tasks to effectively evaluate and improve generators on harder tasks, achieving superh&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qwgfh2fttn/cover.png"/></item><item><title>Efficient LLM Jailbreak via Adaptive Dense-to-sparse Constrained Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/bn5pa3hho8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bn5pa3hho8/</guid><description>Adaptive Dense-to-sparse Constrained Optimization (ADC) efficiently jailbreaks LLMs by transforming discrete token optimization into a continuous process, achieving higher success rates than existing &amp;hellip;</description></item><item><title>Efficient LLM Scheduling by Learning to Rank</title><link>https://deep-diver.github.io/neurips2024/posters/wlljyl0gi6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wlljyl0gi6/</guid><description>Learning to rank request outputs improves LLM scheduling, resulting in 2.8x lower chatbot latency and 6.5x higher synthetic data generation throughput.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wlljyl0gi6/cover.png"/></item><item><title>Enhancing In-Context Learning Performance with just SVD-Based Weight Pruning: A Theoretical Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/wt6ghk5shc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wt6ghk5shc/</guid><description>SVD-based weight pruning surprisingly boosts in-context learning in large language models, especially when applied to deeper layers, offering a novel approach to model compression and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wt6ghk5shc/cover.png"/></item><item><title>Entity Alignment with Noisy Annotations from Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/qfcq54ztx1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qfcq54ztx1/</guid><description>LLM4EA: A novel framework efficiently merges knowledge graphs using LLMs, overcoming noisy annotations and high costs via active learning and unsupervised label refinement, boosting accuracy and effic&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qfcq54ztx1/cover.png"/></item><item><title>Fight Back Against Jailbreaking via Prompt Adversarial Tuning</title><link>https://deep-diver.github.io/neurips2024/posters/nrdst1qifj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nrdst1qifj/</guid><description>Prompt Adversarial Tuning (PAT) defends against LLM jailbreaking by training a protective prompt prefix. PAT uses adversarial and benign prompts to optimize this prefix, significantly reducing succes&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nrdst1qifj/cover.png"/></item><item><title>FLAME : Factuality-Aware Alignment for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/zwuhsialbh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zwuhsialbh/</guid><description>FLAME: A novel alignment method enhances large language model factuality by addressing hallucination in supervised fine-tuning and reinforcement learning, resulting in more accurate and helpful AI ass&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zwuhsialbh/cover.png"/></item><item><title>Fractal Patterns May Illuminate the Success of Next-Token Prediction</title><link>https://deep-diver.github.io/neurips2024/posters/clafyreaye/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/clafyreaye/</guid><description>LLMs&amp;rsquo; success is explained by the self-similar, long-range dependent fractal structure of language; small-scale patterns reflect larger ones.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/clafyreaye/cover.png"/></item><item><title>From Unstructured Data to In-Context Learning: Exploring What Tasks Can Be Learned and When</title><link>https://deep-diver.github.io/neurips2024/posters/x9efgahvbi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x9efgahvbi/</guid><description>LLMs&amp;rsquo; in-context learning surprisingly arises from simple co-occurrence patterns in unstructured data, but positional information is key for complex tasks; ICL fails when patterns are unseen or fixed.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x9efgahvbi/cover.png"/></item><item><title>Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment</title><link>https://deep-diver.github.io/neurips2024/posters/orxqccn8fm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/orxqccn8fm/</guid><description>Reward learning from human demonstrations enhances supervised fine-tuning (SFT) for better LLM alignment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/orxqccn8fm/cover.png"/></item><item><title>Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes</title><link>https://deep-diver.github.io/neurips2024/posters/vi1wqfn15v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vi1wqfn15v/</guid><description>Gradient Cuff: A novel defense mechanism against LLM jailbreaks, leveraging refusal loss landscapes for improved malicious query rejection without harming model performance on benign inputs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vi1wqfn15v/cover.png"/></item><item><title>GTBench: Uncovering the Strategic Reasoning Capabilities of LLMs via Game-Theoretic Evaluations</title><link>https://deep-diver.github.io/neurips2024/posters/ypggxvwiv2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ypggxvwiv2/</guid><description>GTBENCH reveals LLMs&amp;rsquo; strategic reasoning weaknesses via game-theoretic evaluations, showing strengths in probabilistic scenarios but struggles with deterministic ones; code-pretraining helps.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ypggxvwiv2/cover.png"/></item><item><title>How do Large Language Models Handle Multilingualism?</title><link>https://deep-diver.github.io/neurips2024/posters/ctxyooagry/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ctxyooagry/</guid><description>LLMs surprisingly process multilingual queries via an English-centric intermediate stage before generating responses in the original language, a phenomenon explained by the proposed MWork framework an&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ctxyooagry/cover.png"/></item><item><title>IDGen: Item Discrimination Induced Prompt Generation for LLM Evaluation</title><link>https://deep-diver.github.io/neurips2024/posters/zv4uiszzp5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zv4uiszzp5/</guid><description>IDGen synthesizes LLM evaluation prompts using Item Discrimination theory, creating a more challenging and discriminative dataset than previous methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zv4uiszzp5/cover.png"/></item><item><title>Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses</title><link>https://deep-diver.github.io/neurips2024/posters/zmnd0jucef/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zmnd0jucef/</guid><description>Improved few-shot jailbreaking techniques efficiently circumvent aligned language models and their defenses, achieving high success rates even against advanced protection methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zmnd0jucef/cover.png"/></item><item><title>Improving Sparse Decomposition of Language Model Activations with Gated Sparse Autoencoders</title><link>https://deep-diver.github.io/neurips2024/posters/zlblin2zvw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zlblin2zvw/</guid><description>Gated Sparse Autoencoders (GSAEs) achieve Pareto improvement over baseline SAEs for unsupervised feature discovery in language models, resolving the shrinkage bias of L1 penalty by separating feature &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zlblin2zvw/cover.png"/></item><item><title>Incentivizing Quality Text Generation via Statistical Contracts</title><link>https://deep-diver.github.io/neurips2024/posters/wzgw4crxwk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wzgw4crxwk/</guid><description>Cost-robust contracts, inspired by statistical hypothesis tests, incentivize quality in LLM text generation, overcoming the moral hazard of pay-per-token models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wzgw4crxwk/cover.png"/></item><item><title>Interpreting Learned Feedback Patterns in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/xuongr1byy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xuongr1byy/</guid><description>Researchers developed methods to measure and interpret the divergence between learned feedback patterns (LFPs) in LLMs and human preferences, helping minimize discrepancies between LLM behavior and tr&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xuongr1byy/cover.png"/></item><item><title>Is Programming by Example solved by LLMs?</title><link>https://deep-diver.github.io/neurips2024/posters/xqc8yyhscl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xqc8yyhscl/</guid><description>Large Language Models (LLMs) surprisingly improve the challenging task of Programming by Example (PBE) when fine-tuned on problem-specific data, outperforming classic symbolic methods and even surpass&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xqc8yyhscl/cover.png"/></item><item><title>Is the MMI Criterion Necessary for Interpretability? Degenerating Non-causal Features to Plain Noise for Self-Rationalization</title><link>https://deep-diver.github.io/neurips2024/posters/eaqcvzx30k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/eaqcvzx30k/</guid><description>New criterion maximizes remaining discrepancy after rationale removal, treating spurious features as noise, improving rationale extraction.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/eaqcvzx30k/cover.png"/></item><item><title>KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization</title><link>https://deep-diver.github.io/neurips2024/posters/pnnvzqss4p/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pnnvzqss4p/</guid><description>Boost LLM inference speed 1.4-3.5x by using Coupled Quantization (CQ) to compress KV cache down to 1 bit per channel, while preserving model accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pnnvzqss4p/cover.png"/></item><item><title>Large Language Model Unlearning via Embedding-Corrupted Prompts</title><link>https://deep-diver.github.io/neurips2024/posters/e5icsxbd8q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/e5icsxbd8q/</guid><description>ECO prompts enable efficient LLM unlearning by corrupting prompts flagged for forgetting, achieving promising results across various LLMs and tasks with minimal side effects.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/e5icsxbd8q/cover.png"/></item><item><title>Learning to Reason via Program Generation, Emulation, and Search</title><link>https://deep-diver.github.io/neurips2024/posters/te6vagjf6g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/te6vagjf6g/</guid><description>Language models excel at generating programs for algorithmic tasks, but struggle with soft reasoning. COGEX leverages pseudo-programs and program emulation to tackle these tasks, while COTACS searches&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/te6vagjf6g/cover.png"/></item><item><title>Limits of Transformer Language Models on Learning to Compose Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/x7ad0343jz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x7ad0343jz/</guid><description>Large Language Models struggle with compositional tasks, requiring exponentially more data than expected for learning compared to learning sub-tasks individually. This paper reveals surprising sample &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x7ad0343jz/cover.png"/></item><item><title>Look, Listen, and Answer: Overcoming Biases for Audio-Visual Question Answering</title><link>https://deep-diver.github.io/neurips2024/posters/twppd9umun/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/twppd9umun/</guid><description>New dataset MUSIC-AVQA-R and a multi-faceted cycle collaborative debiasing strategy significantly improve audio-visual question answering robustness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/twppd9umun/cover.png"/></item><item><title>MAmmoTH2: Scaling Instructions from the Web</title><link>https://deep-diver.github.io/neurips2024/posters/yvu5dnplqa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yvu5dnplqa/</guid><description>MAmmoTH2: Harvesting 10M web instructions for enhanced LLM reasoning!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yvu5dnplqa/cover.png"/></item><item><title>Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/zaxumqoaf4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zaxumqoaf4/</guid><description>Mesa-Extrapolation enhances LLM extrapolation using a novel weave position encoding method, boosting performance while significantly reducing memory and inference time.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zaxumqoaf4/cover.png"/></item><item><title>Microstructures and Accuracy of Graph Recall by Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/tnhwg9u767/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tnhwg9u767/</guid><description>LLMs struggle with graph recall, exhibiting biases like favoring triangles and underperforming compared to humans; advanced models show striking domain dependence.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tnhwg9u767/cover.png"/></item><item><title>Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/pgobeycxzs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pgobeycxzs/</guid><description>BinaryMoS: a novel token-adaptive binarization method that boosts LLM accuracy and efficiency by dynamically merging multiple scaling experts for each token.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pgobeycxzs/cover.png"/></item><item><title>MutaPLM: Protein Language Modeling for Mutation Explanation and Engineering</title><link>https://deep-diver.github.io/neurips2024/posters/yppclfezgy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yppclfezgy/</guid><description>MutaPLM: a novel protein language model, provides human-understandable mutation explanations and designs novel mutations with desirable properties using a unique protein delta network and chain-of-tho&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yppclfezgy/cover.png"/></item><item><title>No Free Lunch in LLM Watermarking: Trade-offs in Watermarking Design Choices</title><link>https://deep-diver.github.io/neurips2024/posters/riol7kbskv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/riol7kbskv/</guid><description>LLM watermarking faces inherent trade-offs; this paper reveals simple attacks exploiting common design choices, proposing guidelines and defenses for more secure systems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/riol7kbskv/cover.png"/></item><item><title>On Softmax Direct Preference Optimization for Recommendation</title><link>https://deep-diver.github.io/neurips2024/posters/qp5vbgtam0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qp5vbgtam0/</guid><description>Softmax-DPO boosts LM-based recommender performance by directly optimizing for personalized ranking using a novel loss function that incorporates multiple negative samples, significantly outperforming&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qp5vbgtam0/cover.png"/></item><item><title>One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos</title><link>https://deep-diver.github.io/neurips2024/posters/bqmevgcyvm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bqmevgcyvm/</guid><description>VideoLISA: A video-based multimodal large language model enabling precise, language-instructed video object segmentation with superior performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bqmevgcyvm/cover.png"/></item><item><title>Parallelizing Linear Transformers with the Delta Rule over Sequence Length</title><link>https://deep-diver.github.io/neurips2024/posters/y8rm4vnrph/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y8rm4vnrph/</guid><description>DeltaNet, a linear transformer boosting associative recall, now trains efficiently via a novel algorithm, scaling to large language models and outperforming existing linear baselines.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y8rm4vnrph/cover.png"/></item><item><title>Protecting Your LLMs with Information Bottleneck</title><link>https://deep-diver.github.io/neurips2024/posters/u9shp64fjv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/u9shp64fjv/</guid><description>IBProtector shields LLMs from harmful outputs via prompt compression, selectively preserving essential information using a trainable extractor.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/u9shp64fjv/cover.png"/></item><item><title>Quantifying and Optimizing Global Faithfulness in Persona-driven Role-playing</title><link>https://deep-diver.github.io/neurips2024/posters/bzpmjmiaz8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bzpmjmiaz8/</guid><description>New APC metric precisely quantifies &amp;amp; optimizes global faithfulness in persona-driven role-playing, offering a fine-grained, explainable evaluation and improving AI character consistency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bzpmjmiaz8/cover.png"/></item><item><title>Reasons and Solutions for the Decline in Model Performance after Editing</title><link>https://deep-diver.github.io/neurips2024/posters/xjxygdfm5m/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xjxygdfm5m/</guid><description>Boosting large language model performance after knowledge editing: A new method (D4S) minimizes model damage by regulating the explosive growth of parameter layers, enabling multiple effective edits.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xjxygdfm5m/cover.png"/></item><item><title>Representation Noising: A Defence Mechanism Against Harmful Finetuning</title><link>https://deep-diver.github.io/neurips2024/posters/ep9auejqfg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ep9auejqfg/</guid><description>RepNoise: a novel defense against harmful fine-tuning of LLMs by removing information about harmful representations, generalizing across different harmful tasks, and maintaining LLM capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ep9auejqfg/cover.png"/></item><item><title>Reversing the Forget-Retain Objectives: An Efficient LLM Unlearning Framework from Logit Difference</title><link>https://deep-diver.github.io/neurips2024/posters/tydr1ltwqh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tydr1ltwqh/</guid><description>Reverse the forget-retain objectives for efficient LLM unlearning!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tydr1ltwqh/cover.png"/></item><item><title>Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/pf4oujyn4q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pf4oujyn4q/</guid><description>Direct Alignment Algorithms (DAAs) for LLM alignment suffer from over-optimization, even without explicit reward models; this paper empirically demonstrates this and proposes scaling laws to understan&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pf4oujyn4q/cover.png"/></item><item><title>Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies</title><link>https://deep-diver.github.io/neurips2024/posters/skckpr8crl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/skckpr8crl/</guid><description>Boosting LLM performance: This research shows how larger language models need bigger vocabularies for optimal efficiency and performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/skckpr8crl/cover.png"/></item><item><title>Self-playing Adversarial Language Game Enhances LLM Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/ocgksh7ys2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ocgksh7ys2/</guid><description>Self-play adversarial language game boosts LLM reasoning!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ocgksh7ys2/cover.png"/></item><item><title>Self-Taught Recognizer: Toward Unsupervised Adaptation for Speech Foundation Models</title><link>https://deep-diver.github.io/neurips2024/posters/oloqhrbxye/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oloqhrbxye/</guid><description>STAR, a novel unsupervised adaptation framework, drastically improves automatic speech recognition (ASR) robustness across diverse domains using only unlabeled data and outperforms existing self-train&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oloqhrbxye/cover.png"/></item><item><title>Simplified and Generalized Masked Diffusion for Discrete Data</title><link>https://deep-diver.github.io/neurips2024/posters/xcqsofht4g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xcqsofht4g/</guid><description>Simplified and generalized masked diffusion models achieve state-of-the-art results in discrete data generation, surpassing previous methods in text and image modeling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xcqsofht4g/cover.png"/></item><item><title>Source Code Foundation Models are Transferable Binary Analysis Knowledge Bases</title><link>https://deep-diver.github.io/neurips2024/posters/qppvdzphsl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qppvdzphsl/</guid><description>ProRec, a novel framework, bridges the binary-source semantic gap by using a binary-source encoder-decoder model and LLMs, achieving significant improvements in zero-shot binary summarization and func&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qppvdzphsl/cover.png"/></item><item><title>Speaking Your Language: Spatial Relationships in Interpretable Emergent Communication</title><link>https://deep-diver.github.io/neurips2024/posters/vip8iwmzln/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vip8iwmzln/</guid><description>AI agents developed a communication system using spatial relationships, achieving over 90% accuracy in conveying relative positions of objects within a scene.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vip8iwmzln/cover.png"/></item><item><title>Spectral Editing of Activations for Large Language Model Alignment</title><link>https://deep-diver.github.io/neurips2024/posters/pqyceea87j/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pqyceea87j/</guid><description>Spectral Editing of Activations (SEA) improves large language model truthfulness and fairness by projecting input representations to maximize covariance with positive demonstrations while minimizing c&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pqyceea87j/cover.png"/></item><item><title>Stealth edits to large language models</title><link>https://deep-diver.github.io/neurips2024/posters/qap6ryyijc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qap6ryyijc/</guid><description>Researchers unveil stealth edits for large language models, offering a new metric to assess editability and reveal vulnerability to malicious attacks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qap6ryyijc/cover.png"/></item><item><title>Stress-Testing Capability Elicitation With Password-Locked Models</title><link>https://deep-diver.github.io/neurips2024/posters/zzooqd6r1b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zzooqd6r1b/</guid><description>Fine-tuning, even on a single demonstration, effectively uncovers hidden LLM capabilities, surpassing simple prompting methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zzooqd6r1b/cover.png"/></item><item><title>Synthesize, Partition, then Adapt: Eliciting Diverse Samples from Foundation Models</title><link>https://deep-diver.github.io/neurips2024/posters/sp8whisnu9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sp8whisnu9/</guid><description>The Synthesize-Partition-Adapt (SPA) framework leverages synthetic data to generate diverse, high-quality responses from foundation models, enriching user experience.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sp8whisnu9/cover.png"/></item><item><title>The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains</title><link>https://deep-diver.github.io/neurips2024/posters/qart6qtiqj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qart6qtiqj/</guid><description>Transformers learn to perform in-context learning of Markov chains hierarchically, progressing from simpler unigram strategies to more complex bigram solutions, with the presence of simpler solutions &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qart6qtiqj/cover.png"/></item><item><title>The Expressive Capacity of State Space Models: A Formal Language Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/ev5yirjpdy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ev5yirjpdy/</guid><description>State-space models (SSMs) rival transformers in language modeling, but their capabilities remain unclear; this paper rigorously analyzes SSM expressivity, revealing unique strengths and limitations, i&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ev5yirjpdy/cover.png"/></item><item><title>The Fine-Grained Complexity of Gradient Computation for Training Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/up4twnwrol/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/up4twnwrol/</guid><description>New research precisely defines the computational limits of training large language models, revealing a sharp threshold based on parameter matrix entries, paving the way for faster algorithms.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/up4twnwrol/cover.png"/></item><item><title>The Impact of Initialization on LoRA Finetuning Dynamics</title><link>https://deep-diver.github.io/neurips2024/posters/sn3uryritk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sn3uryritk/</guid><description>LoRA&amp;rsquo;s initialization significantly impacts finetuning; initializing matrix A randomly and B to zero yields better performance than vice-versa due to enabling larger learning rates.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sn3uryritk/cover.png"/></item><item><title>Thinking Forward: Memory-Efficient Federated Finetuning of Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/dgqtja9x2c/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dgqtja9x2c/</guid><description>SPRY: A memory-efficient federated learning algorithm for finetuning LLMs on resource-constrained devices, achieving high accuracy and speed.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dgqtja9x2c/cover.png"/></item><item><title>Token Merging for Training-Free Semantic Binding in Text-to-Image Synthesis</title><link>https://deep-diver.github.io/neurips2024/posters/trrwoa9e80/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/trrwoa9e80/</guid><description>ToMe: a novel training-free method dramatically improves semantic binding in text-to-image synthesis by intelligently merging related tokens, ensuring accurate alignment between generated images and t&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/trrwoa9e80/cover.png"/></item><item><title>Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing</title><link>https://deep-diver.github.io/neurips2024/posters/tpdj2qhkob/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tpdj2qhkob/</guid><description>ALPHALLM boosts LLM performance in complex reasoning tasks by using imagination, search, and criticism to create a self-improving loop, eliminating the need for extra training data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tpdj2qhkob/cover.png"/></item><item><title>Towards Understanding How Transformers Learn In-context Through a Representation Learning Lens</title><link>https://deep-diver.github.io/neurips2024/posters/db6gwsdxkl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/db6gwsdxkl/</guid><description>Transformers&amp;rsquo; in-context learning (ICL) is explained using representation learning, revealing its ICL process as gradient descent on a dual model and offering modifiable attention layers for enhanced &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/db6gwsdxkl/cover.png"/></item><item><title>Training Dynamics of Transformers to Recognize Word Co-occurrence via Gradient Flow Analysis</title><link>https://deep-diver.github.io/neurips2024/posters/w6q46islsr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w6q46islsr/</guid><description>Researchers reveal how transformers learn word co-occurrence using a novel gradient flow analysis, uncovering a two-phase training process that leads to near-minimum loss and improved model performanc&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w6q46islsr/cover.png"/></item><item><title>Verified Code Transpilation with LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/spwe9slrfg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/spwe9slrfg/</guid><description>LLMLIFT: An LLM-powered approach builds verified lifting tools for DSLs, outperforming prior symbolic methods in benchmark transpilation and requiring less development effort.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/spwe9slrfg/cover.png"/></item><item><title>Weak-to-Strong Search: Align Large Language Models via Searching over Small Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/doj6cqwdf1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/doj6cqwdf1/</guid><description>Align LLMs efficiently via test-time search using smaller models!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/doj6cqwdf1/cover.png"/></item><item><title>Zipfian Whitening</title><link>https://deep-diver.github.io/neurips2024/posters/pasjxzmjb7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pasjxzmjb7/</guid><description>Zipfian Whitening: Weighting PCA whitening by word frequency dramatically improves NLP task performance, surpassing established baselines and providing a theoretical framework for existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pasjxzmjb7/cover.png"/></item></channel></rss>