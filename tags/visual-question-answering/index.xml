<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Visual Question Answering on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/visual-question-answering/</link><description>Recent content in Visual Question Answering on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/visual-question-answering/index.xml" rel="self" type="application/rss+xml"/><item><title>Beyond the Doors of Perception: Vision Transformers Represent Relations Between Objects</title><link>https://deep-diver.github.io/neurips2024/posters/8puv3c9cpg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8puv3c9cpg/</guid><description>Vision transformers surprisingly struggle with visual relations; this study reveals ViTs use distinct perceptual and relational processing stages to solve same/different tasks, highlighting a previous&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8puv3c9cpg/cover.png"/></item><item><title>EMVP: Embracing Visual Foundation Model for Visual Place Recognition with Centroid-Free Probing</title><link>https://deep-diver.github.io/neurips2024/posters/v6w7keotqn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v6w7keotqn/</guid><description>EMVP: A novel PEFT pipeline boosts Visual Place Recognition accuracy by 97.6% using Centroid-Free Probing &amp;amp; Dynamic Power Normalization, saving 64.3% of parameters.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v6w7keotqn/cover.png"/></item><item><title>Learning to Edit Visual Programs with Self-Supervision</title><link>https://deep-diver.github.io/neurips2024/posters/uziwqrzjep/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uziwqrzjep/</guid><description>AI learns to edit visual programs more accurately using a self-supervised method that combines one-shot program generation with iterative local edits, significantly boosting performance, especially wi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uziwqrzjep/cover.png"/></item><item><title>Neural Concept Binder</title><link>https://deep-diver.github.io/neurips2024/posters/yppzyflbys/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yppzyflbys/</guid><description>The Neural Concept Binder (NCB) framework learns expressive, inspectable, and revisable visual concepts unsupervised, integrating both continuous and discrete representations for seamless use in neura&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yppzyflbys/cover.png"/></item><item><title>Parallel Backpropagation for Shared-Feature Visualization</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/oqzcsb6fbl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/oqzcsb6fbl/</guid><description>Researchers visualized shared visual features driving responses of body-selective neurons to non-body objects, revealing object parts resembling macaque body parts, thus explaining neural preferences.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/oqzcsb6fbl/cover.png"/></item><item><title>Visual Prompt Tuning in Null Space for Continual Learning</title><link>https://deep-diver.github.io/neurips2024/posters/8premr5kei/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8premr5kei/</guid><description>This paper presents NSP², a novel method for visual prompt tuning in continual learning that leverages orthogonal projection to prevent catastrophic forgetting by tuning prompts orthogonal to previous&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8premr5kei/cover.png"/></item><item><title>VLG-CBM: Training Concept Bottleneck Models with Vision-Language Guidance</title><link>https://deep-diver.github.io/neurips2024/posters/jm2ak3sdjd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jm2ak3sdjd/</guid><description>VLG-CBM enhances concept bottleneck models with vision-language guidance for faithful interpretability and improved accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jm2ak3sdjd/cover.png"/></item><item><title>Web-Scale Visual Entity Recognition: An LLM-Driven Data Approach</title><link>https://deep-diver.github.io/neurips2024/posters/vikufblow1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vikufblow1/</guid><description>LLM-powered data curation boosts web-scale visual entity recognition!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vikufblow1/cover.png"/></item></channel></rss>