<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Few-Shot Learning on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/few-shot-learning/</link><description>Recent content in Few-Shot Learning on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2025 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/few-shot-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>A Closer Look at the CLS Token for Cross-Domain Few-Shot Learning</title><link>https://deep-diver.github.io/neurips2024/posters/qikylfdzai/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qikylfdzai/</guid><description>Leaving the CLS token of a Vision Transformer randomly initialized during cross-domain few-shot learning consistently improves performance; a novel method leveraging this phenomenon achieves state-of-&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qikylfdzai/cover.png"/></item><item><title>An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning</title><link>https://deep-diver.github.io/neurips2024/posters/dqdffx3bs5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dqdffx3bs5/</guid><description>Mecoin: a novel memory module for efficient graph few-shot class-incremental learning, tackles catastrophic forgetting by employing structured memory units and a memory representation adaptation modul&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dqdffx3bs5/cover.png"/></item><item><title>ARC: A Generalist Graph Anomaly Detector with In-Context Learning</title><link>https://deep-diver.github.io/neurips2024/posters/idivfzjpk4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/idivfzjpk4/</guid><description>ARC: a novel generalist graph anomaly detector leveraging in-context learning for efficient, one-for-all anomaly detection across various datasets without retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/idivfzjpk4/cover.png"/></item><item><title>Attention Temperature Matters in ViT-Based Cross-Domain Few-Shot Learning</title><link>https://deep-diver.github.io/neurips2024/posters/o8m4rm5mbk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/o8m4rm5mbk/</guid><description>Boosting Vision Transformer&amp;rsquo;s transferability in cross-domain few-shot learning is achieved by a simple yet effective method: strategically adjusting attention temperature to remedy ineffective target&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/o8m4rm5mbk/cover.png"/></item><item><title>Be Confident in What You Know: Bayesian Parameter Efficient Fine-Tuning of Vision Foundation Models</title><link>https://deep-diver.github.io/neurips2024/posters/loqck0qruu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/loqck0qruu/</guid><description>Bayesian-PEFT boosts vision model accuracy and confidence in few-shot learning by integrating Bayesian components into PEFT, solving the underconfidence problem.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/loqck0qruu/cover.png"/></item><item><title>Breaking Long-Tailed Learning Bottlenecks: A Controllable Paradigm with Hypernetwork-Generated Diverse Experts</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/wppnvpaeyv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/wppnvpaeyv/</guid><description>Controllable long-tailed learning achieved via hypernetwork-generated diverse experts, adapting to user preferences and distribution shifts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/wppnvpaeyv/cover.png"/></item><item><title>D2R2: Diffusion-based Representation with Random Distance Matching for Tabular Few-shot Learning</title><link>https://deep-diver.github.io/neurips2024/posters/ls9e36lkxg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ls9e36lkxg/</guid><description>D2R2: A novel diffusion-based model for tabular few-shot learning, achieves state-of-the-art results by leveraging semantic knowledge and distance matching.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ls9e36lkxg/cover.png"/></item><item><title>EPIC: Effective Prompting for Imbalanced-Class Data Synthesis in Tabular Data Classification via Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/d5ckdhcrfj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/d5ckdhcrfj/</guid><description>EPIC: Effective prompting makes LLMs generate high-quality synthetic tabular data, significantly boosting imbalanced-class classification.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/d5ckdhcrfj/cover.png"/></item><item><title>Fast Graph Sharpness-Aware Minimization for Enhancing and Accelerating Few-Shot Node Classification</title><link>https://deep-diver.github.io/neurips2024/posters/af32gbuupc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/af32gbuupc/</guid><description>Fast Graph Sharpness-Aware Minimization (FGSAM) accelerates few-shot node classification by cleverly combining GNNs and MLPs for efficient, high-performing training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/af32gbuupc/cover.png"/></item><item><title>Few-Shot Diffusion Models Escape the Curse of Dimensionality</title><link>https://deep-diver.github.io/neurips2024/posters/jrranaazm5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jrranaazm5/</guid><description>Few-shot diffusion models efficiently generate customized images; this paper provides the first theoretical explanation, proving improved approximation and optimization bounds, escaping the curse of d&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jrranaazm5/cover.png"/></item><item><title>Few-Shot Task Learning through Inverse Generative Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/atie6npr5a/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/atie6npr5a/</guid><description>Few-shot task learning through inverse generative modeling (FTL-IGM) enables AI agents to quickly master new tasks from minimal data by leveraging invertible generative models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/atie6npr5a/cover.png"/></item><item><title>Generate Universal Adversarial Perturbations for Few-Shot Learning</title><link>https://deep-diver.github.io/neurips2024/posters/qlro8o4bol/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qlro8o4bol/</guid><description>Researchers developed FSAFW, a novel framework generating universal adversarial perturbations effective against various Few-Shot Learning paradigms, surpassing baseline methods by over 16%.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qlro8o4bol/cover.png"/></item><item><title>How Transformers Utilize Multi-Head Attention in In-Context Learning? A Case Study on Sparse Linear Regression</title><link>https://deep-diver.github.io/neurips2024/posters/fg8tukixa5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fg8tukixa5/</guid><description>Multi-head transformers utilize distinct attention patterns across layers—multiple heads are essential for initial data preprocessing, while a single head suffices for subsequent optimization steps, o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fg8tukixa5/cover.png"/></item><item><title>Knowledge Composition using Task Vectors with Learned Anisotropic Scaling</title><link>https://deep-diver.github.io/neurips2024/posters/g9ojugko4b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/g9ojugko4b/</guid><description>aTLAS: a novel parameter-efficient fine-tuning method using learned anisotropic scaling of task vectors for enhanced knowledge composition and transfer.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/g9ojugko4b/cover.png"/></item><item><title>Meta-Exploiting Frequency Prior for Cross-Domain Few-Shot Learning</title><link>https://deep-diver.github.io/neurips2024/posters/2nisrxmmqr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2nisrxmmqr/</guid><description>Meta-Exploiting Frequency Prior enhances cross-domain few-shot learning by leveraging image frequency decomposition and consistency priors to improve model generalization and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2nisrxmmqr/cover.png"/></item><item><title>Mind the Gap Between Prototypes and Images in Cross-domain Finetuning</title><link>https://deep-diver.github.io/neurips2024/posters/jwlik3kkwq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jwlik3kkwq/</guid><description>CoPA improves cross-domain few-shot learning by adapting separate transformations for prototype and image embeddings, significantly enhancing performance and revealing better representation clusters.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jwlik3kkwq/cover.png"/></item><item><title>Mixture of Adversarial LoRAs: Boosting Robust Generalization in Meta-Tuning</title><link>https://deep-diver.github.io/neurips2024/posters/hxgdbamyyr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hxgdbamyyr/</guid><description>Boosting Robust Few-Shot Learning with Adversarial Meta-Tuning!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hxgdbamyyr/cover.png"/></item><item><title>OTTER: Effortless Label Distribution Adaptation of Zero-shot Models</title><link>https://deep-diver.github.io/neurips2024/posters/rsawwsbcs7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rsawwsbcs7/</guid><description>OTTER effortlessly adapts zero-shot models to new tasks by adjusting predictions using optimal transport, improving accuracy significantly without extra training data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rsawwsbcs7/cover.png"/></item><item><title>Pin-Tuning: Parameter-Efficient In-Context Tuning for Few-Shot Molecular Property Prediction</title><link>https://deep-diver.github.io/neurips2024/posters/859dtlwnad/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/859dtlwnad/</guid><description>Pin-Tuning: A parameter-efficient method for few-shot molecular property prediction that significantly improves accuracy with fewer trainable parameters via in-context tuning and Bayesian weight cons&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/859dtlwnad/cover.png"/></item><item><title>Pretraining Codomain Attention Neural Operators for Solving Multiphysics PDEs</title><link>https://deep-diver.github.io/neurips2024/posters/wspiduxzyx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wspiduxzyx/</guid><description>CoDA-NO, a novel neural operator, revolutionizes multiphysics PDE solving via codomain tokenization, enabling efficient self-supervised pretraining and few-shot learning for superior generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wspiduxzyx/cover.png"/></item><item><title>Prospective Representation Learning for Non-Exemplar Class-Incremental Learning</title><link>https://deep-diver.github.io/neurips2024/posters/ztdarpmbun/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ztdarpmbun/</guid><description>Prospective Representation Learning (PRL) revolutionizes non-exemplar class-incremental learning by proactively reserving embedding space for new classes and minimizing the shock of new data on previo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ztdarpmbun/cover.png"/></item><item><title>Stepping Forward on the Last Mile</title><link>https://deep-diver.github.io/neurips2024/posters/ych1z6dcto/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ych1z6dcto/</guid><description>On-device training with fixed-point forward gradients enables efficient model personalization on resource-constrained edge devices, overcoming backpropagation&amp;rsquo;s memory limitations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ych1z6dcto/cover.png"/></item><item><title>Test-Time Adaptation Induces Stronger Accuracy and Agreement-on-the-Line</title><link>https://deep-diver.github.io/neurips2024/posters/gixux4vh9t/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gixux4vh9t/</guid><description>Test-time adaptation strengthens the linear correlation between in- and out-of-distribution accuracy, enabling precise OOD performance prediction and hyperparameter optimization without labeled OOD da&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gixux4vh9t/cover.png"/></item><item><title>Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series</title><link>https://deep-diver.github.io/neurips2024/posters/3o5ycewetq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3o5ycewetq/</guid><description>Tiny Time Mixers (TTMs) achieve state-of-the-art zero/few-shot multivariate time series forecasting, outperforming existing benchmarks while drastically reducing computational requirements.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3o5ycewetq/cover.png"/></item><item><title>Transformers Learn to Achieve Second-Order Convergence Rates for In-Context Linear Regression</title><link>https://deep-diver.github.io/neurips2024/posters/l8h6cozcbn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/l8h6cozcbn/</guid><description>Transformers surprisingly learn second-order optimization methods for in-context linear regression, achieving exponentially faster convergence than gradient descent!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/l8h6cozcbn/cover.png"/></item><item><title>Tri-Level Navigator: LLM-Empowered Tri-Level Learning for Time Series OOD Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/a6hzeu4kpo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/a6hzeu4kpo/</guid><description>LLM-powered Tri-level learning framework enhances time series OOD generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/a6hzeu4kpo/cover.png"/></item></channel></rss>