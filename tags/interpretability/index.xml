<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Interpretability on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/interpretability/</link><description>Recent content in Interpretability on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/interpretability/index.xml" rel="self" type="application/rss+xml"/><item><title>2D-OOB: Attributing Data Contribution Through Joint Valuation Framework</title><link>https://deep-diver.github.io/neurips2024/posters/vbxeeh1x4y/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vbxeeh1x4y/</guid><description>2D-OOB: a novel framework for jointly attributing data values to individual features, enabling fine-grained outlier detection and improved model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vbxeeh1x4y/cover.png"/></item><item><title>A theoretical design of concept sets: improving the predictability of concept bottleneck models</title><link>https://deep-diver.github.io/neurips2024/posters/otv6qa12g0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/otv6qa12g0/</guid><description>Boosting concept bottleneck model predictability, this paper introduces a theoretical framework linking concept set properties to model performance, proposing a method for effective concept identifica&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/otv6qa12g0/cover.png"/></item><item><title>Almost-Linear RNNs Yield Highly Interpretable Symbolic Codes in Dynamical Systems Reconstruction</title><link>https://deep-diver.github.io/neurips2024/posters/sepsxteekj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sepsxteekj/</guid><description>Almost-linear RNNs (AL-RNNs) offer highly interpretable symbolic codes for dynamical systems reconstruction, simplifying the analysis of complex systems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sepsxteekj/cover.png"/></item><item><title>Auditing Local Explanations is Hard</title><link>https://deep-diver.github.io/neurips2024/posters/ybmrn4tdn0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ybmrn4tdn0/</guid><description>Auditing local explanations is surprisingly hard: proving explanation trustworthiness requires far more data than previously thought, especially in high dimensions, challenging current AI explainabil&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ybmrn4tdn0/cover.png"/></item><item><title>B-cosification: Transforming Deep Neural Networks to be Inherently Interpretable</title><link>https://deep-diver.github.io/neurips2024/posters/ta5zpfh8ii/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ta5zpfh8ii/</guid><description>B-cosification: cheaply transform any pre-trained deep neural network into an inherently interpretable model.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ta5zpfh8ii/cover.png"/></item><item><title>Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?</title><link>https://deep-diver.github.io/neurips2024/posters/kyhma7hzjr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kyhma7hzjr/</guid><description>This paper presents a novel method to make black box neural networks intervenable using only a small validation set with concept labels, improving the effectiveness of concept-based interventions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kyhma7hzjr/cover.png"/></item><item><title>Causal Dependence Plots</title><link>https://deep-diver.github.io/neurips2024/posters/pu0z2snm1m/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pu0z2snm1m/</guid><description>Causal Dependence Plots (CDPs) visualize how machine learning model predictions causally depend on input features, overcoming limitations of existing methods that ignore causal relationships.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pu0z2snm1m/cover.png"/></item><item><title>Compact Proofs of Model Performance via Mechanistic Interpretability</title><link>https://deep-diver.github.io/neurips2024/posters/2zwbzx50mh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2zwbzx50mh/</guid><description>Researchers developed a novel method using mechanistic interpretability to create compact formal proofs for AI model performance, improving AI safety and reliability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2zwbzx50mh/cover.png"/></item><item><title>Data-faithful Feature Attribution: Mitigating Unobservable Confounders via Instrumental Variables</title><link>https://deep-diver.github.io/neurips2024/posters/jzv9a8tg9p/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jzv9a8tg9p/</guid><description>Data-faithful feature attribution tackles misinterpretations from unobservable confounders by using instrumental variables to train confounder-free models, leading to more robust and accurate feature &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jzv9a8tg9p/cover.png"/></item><item><title>Dual-Perspective Activation: Efficient Channel Denoising via Joint Forward-Backward Criterion for Artificial Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/ku35qkpveg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ku35qkpveg/</guid><description>Dual-Perspective Activation (DPA) efficiently denoises ANN channels by jointly using forward and backward propagation criteria, improving sparsity and accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ku35qkpveg/cover.png"/></item><item><title>Explanations that reveal all through the deﬁnition of encoding</title><link>https://deep-diver.github.io/neurips2024/posters/mkw6x0oexg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mkw6x0oexg/</guid><description>New method, STRIPE-X, powerfully detects &amp;rsquo;encoding&amp;rsquo; in AI explanations—a sneaky phenomenon where explanations predict outcomes better than their constituent parts alone would suggest.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mkw6x0oexg/cover.png"/></item><item><title>Finding Transformer Circuits With Edge Pruning</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/8osy3ra9jy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/8osy3ra9jy/</guid><description>Edge Pruning efficiently discovers sparse, yet accurate, computational subgraphs (circuits) in large language models via gradient-based edge pruning, advancing mechanistic interpretability research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/8osy3ra9jy/cover.png"/></item><item><title>GraphTrail: Translating GNN Predictions into Human-Interpretable Logical Rules</title><link>https://deep-diver.github.io/neurips2024/posters/fzlmza6drz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fzlmza6drz/</guid><description>GRAPHTRAIL unveils the first end-to-end global GNN explainer, translating black-box GNN predictions into easily interpretable boolean formulas over subgraph concepts, achieving significant improvement&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fzlmza6drz/cover.png"/></item><item><title>Interpretable Concept-Based Memory Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/willwyvmp8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/willwyvmp8/</guid><description>CMR: A novel Concept-Based Memory Reasoner delivers human-understandable, verifiable AI task predictions by using a neural selection mechanism over a set of human-understandable logic rules, achievin&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/willwyvmp8/cover.png"/></item><item><title>Interpretable Generalized Additive Models for Datasets with Missing Values</title><link>https://deep-diver.github.io/neurips2024/posters/souxmwl5ak/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/souxmwl5ak/</guid><description>M-GAM: Interpretable additive models handling missing data with superior accuracy &amp;amp; sparsity!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/souxmwl5ak/cover.png"/></item><item><title>Interpretable Mesomorphic Networks for Tabular Data</title><link>https://deep-diver.github.io/neurips2024/posters/pmlty7todm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pmlty7todm/</guid><description>Interpretable Mesomorphic Neural Networks (IMNs) achieve accuracy comparable to black-box models while offering free-lunch explainability for tabular data through instance-specific linear models gener&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pmlty7todm/cover.png"/></item><item><title>Learning Discrete Concepts in Latent Hierarchical Models</title><link>https://deep-diver.github.io/neurips2024/posters/bo5buxvh6m/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bo5buxvh6m/</guid><description>This paper introduces a novel framework for learning discrete concepts from high-dimensional data, establishing theoretical conditions for identifying underlying hierarchical causal structures and pro&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bo5buxvh6m/cover.png"/></item><item><title>Learning to Understand: Identifying Interactions via the Möbius Transform</title><link>https://deep-diver.github.io/neurips2024/posters/glgexu1zg4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/glgexu1zg4/</guid><description>Unlocking complex models&amp;rsquo; secrets: New algorithm identifies input interactions using the Möbius Transform, boosting interpretability with surprising speed and accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/glgexu1zg4/cover.png"/></item><item><title>MambaLRP: Explaining Selective State Space Sequence Models</title><link>https://deep-diver.github.io/neurips2024/posters/2n1ysn1edl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2n1ysn1edl/</guid><description>MambaLRP enhances explainability of Mamba sequence models by ensuring faithful relevance propagation, achieving state-of-the-art explanation performance, and uncovering model biases.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2n1ysn1edl/cover.png"/></item><item><title>Measuring Per-Unit Interpretability at Scale Without Humans</title><link>https://deep-diver.github.io/neurips2024/posters/oyyesvz6dx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oyyesvz6dx/</guid><description>New scalable method measures per-unit interpretability in vision DNNs without human evaluation, revealing anti-correlation between model performance and interpretability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oyyesvz6dx/cover.png"/></item><item><title>Measuring Progress in Dictionary Learning for Language Model Interpretability with Board Game Models</title><link>https://deep-diver.github.io/neurips2024/posters/scedogghcw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/scedogghcw/</guid><description>New metrics and p-annealing improve sparse autoencoder training for better language model interpretability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/scedogghcw/cover.png"/></item><item><title>Most Influential Subset Selection: Challenges, Promises, and Beyond</title><link>https://deep-diver.github.io/neurips2024/posters/qwi33ppecc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qwi33ppecc/</guid><description>Adaptive greedy algorithms significantly improve the accuracy of identifying the most influential subset of training data, overcoming limitations of existing methods that fail to capture complex inter&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qwi33ppecc/cover.png"/></item><item><title>Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization</title><link>https://deep-diver.github.io/neurips2024/posters/bia03matxq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bia03matxq/</guid><description>Multilinear Mixture of Experts (μMoE) achieves scalable expert specialization in deep neural networks through tensor factorization, enabling efficient fine-tuning and interpretable model editing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bia03matxq/cover.png"/></item><item><title>On Neural Networks as Infinite Tree-Structured Probabilistic Graphical Models</title><link>https://deep-diver.github.io/neurips2024/posters/kcmhsrhzjb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kcmhsrhzjb/</guid><description>DNNs are powerful but lack the clear semantics of PGMs. This paper innovatively constructs infinite tree-structured PGMs that exactly correspond to DNNs, revealing that DNN forward propagation approxi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kcmhsrhzjb/cover.png"/></item><item><title>Optimal ablation for interpretability</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/opt72tyzwz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/opt72tyzwz/</guid><description>Optimal ablation (OA) improves model interpretability by precisely measuring component importance, outperforming existing methods. OA-based importance shines in circuit discovery, factual recall, and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/opt72tyzwz/cover.png"/></item><item><title>RegExplainer: Generating Explanations for Graph Neural Networks in Regression Tasks</title><link>https://deep-diver.github.io/neurips2024/posters/ejwvcpluwu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ejwvcpluwu/</guid><description>RegExplainer unveils a novel method for interpreting graph neural networks in regression tasks, bridging the explanation gap by addressing distribution shifts and tackling continuously ordered decisio&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ejwvcpluwu/cover.png"/></item><item><title>Stochastic Amortization: A Unified Approach to Accelerate Feature and Data Attribution</title><link>https://deep-diver.github.io/neurips2024/posters/zdwtn2hoie/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zdwtn2hoie/</guid><description>Stochastic Amortization accelerates feature and data attribution by training amortized models using noisy, yet unbiased, labels, achieving order-of-magnitude speedups over existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zdwtn2hoie/cover.png"/></item><item><title>The Intelligible and Effective Graph Neural Additive Network</title><link>https://deep-diver.github.io/neurips2024/posters/sky1scutwa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sky1scutwa/</guid><description>GNAN: a novel interpretable graph neural network achieving accuracy comparable to black-box models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sky1scutwa/cover.png"/></item><item><title>Towards the Dynamics of a DNN Learning Symbolic Interactions</title><link>https://deep-diver.github.io/neurips2024/posters/dihxwkjxre/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dihxwkjxre/</guid><description>DNNs learn interactions in two phases: initially removing complex interactions, then gradually learning higher-order ones, leading to overfitting.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dihxwkjxre/cover.png"/></item><item><title>Training for Stable Explanation for Free</title><link>https://deep-diver.github.io/neurips2024/posters/hya3eu8scg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hya3eu8scg/</guid><description>R2ET: training for robust ranking explanations by an effective regularizer.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hya3eu8scg/cover.png"/></item><item><title>Using Noise to Infer Aspects of Simplicity Without Learning</title><link>https://deep-diver.github.io/neurips2024/posters/b172ac0r4l/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/b172ac0r4l/</guid><description>Noise in data surprisingly simplifies machine learning models, improving their interpretability without sacrificing accuracy; this paper quantifies this effect across various hypothesis spaces.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/b172ac0r4l/cover.png"/></item><item><title>What makes unlearning hard and what to do about it</title><link>https://deep-diver.github.io/neurips2024/posters/qabhlbf72k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qabhlbf72k/</guid><description>Researchers developed RUM, a refined unlearning meta-algorithm, that significantly improves existing unlearning methods by strategically refining forget sets and employing appropriate unlearning algor&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qabhlbf72k/cover.png"/></item><item><title>Zipper: Addressing Degeneracy in Algorithm-Agnostic Inference</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ahvohpkkmx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ahvohpkkmx/</guid><description>Zipper: A novel statistical device resolves the degeneracy issue in algorithm-agnostic inference, enabling reliable goodness-of-fit tests with enhanced power.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ahvohpkkmx/cover.png"/></item></channel></rss>