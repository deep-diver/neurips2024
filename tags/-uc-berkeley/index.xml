<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ UC Berkeley on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-uc-berkeley/</link><description>Recent content in üè¢ UC Berkeley on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-uc-berkeley/index.xml" rel="self" type="application/rss+xml"/><item><title>Enhancing Efficiency of Safe Reinforcement Learning via Sample Manipulation</title><link>https://deep-diver.github.io/neurips2024/posters/opfjhl6dpr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/opfjhl6dpr/</guid><description>ESPO enhances safe RL efficiency by dynamically manipulating sample size based on reward-safety gradient conflicts, ensuring faster training and superior performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/opfjhl6dpr/cover.png"/></item><item><title>Fair Allocation in Dynamic Mechanism Design</title><link>https://deep-diver.github.io/neurips2024/posters/beungps83o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/beungps83o/</guid><description>This paper presents optimal fair mechanisms for dynamic auction design, maximizing seller revenue while guaranteeing minimum allocations to multiple buyer groups.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/beungps83o/cover.png"/></item><item><title>Is Value Learning Really the Main Bottleneck in Offline RL?</title><link>https://deep-diver.github.io/neurips2024/posters/nyp59a31ju/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nyp59a31ju/</guid><description>Offline RL&amp;rsquo;s performance often lags behind imitation learning, but this paper reveals that policy learning and generalization, not value function learning, are often the main bottlenecks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nyp59a31ju/cover.png"/></item><item><title>Med-Real2Sim: Non-Invasive Medical Digital Twins using Physics-Informed Self-Supervised Learning</title><link>https://deep-diver.github.io/neurips2024/posters/xcuxjqqysd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xcuxjqqysd/</guid><description>Med-Real2Sim uses physics-informed self-supervised learning to build non-invasive medical digital twins, enabling in-silico clinical trials and unsupervised disease detection.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xcuxjqqysd/cover.png"/></item><item><title>The Impact of Initialization on LoRA Finetuning Dynamics</title><link>https://deep-diver.github.io/neurips2024/posters/sn3uryritk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sn3uryritk/</guid><description>LoRA&amp;rsquo;s initialization significantly impacts finetuning; initializing matrix A randomly and B to zero yields better performance than vice-versa due to enabling larger learning rates.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sn3uryritk/cover.png"/></item><item><title>Truthfulness of Calibration Measures</title><link>https://deep-diver.github.io/neurips2024/posters/cda8hftygc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cda8hftygc/</guid><description>Researchers developed Subsampled Smooth Calibration Error (SSCE), a new truthful calibration measure for sequential prediction, solving the problem of existing measures being easily gamed.</description></item><item><title>Verified Code Transpilation with LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/spwe9slrfg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/spwe9slrfg/</guid><description>LLMLIFT: An LLM-powered approach builds verified lifting tools for DSLs, outperforming prior symbolic methods in benchmark transpilation and requiring less development effort.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/spwe9slrfg/cover.png"/></item></channel></rss>