<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Generalization on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/generalization/</link><description>Recent content in Generalization on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/generalization/index.xml" rel="self" type="application/rss+xml"/><item><title>A Comprehensive Analysis on the Learning Curve in Kernel Ridge Regression</title><link>https://deep-diver.github.io/neurips2024/posters/imldpzmlnl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/imldpzmlnl/</guid><description>This study provides a unified theory for kernel ridge regression&amp;rsquo;s learning curve, improving existing bounds and validating the Gaussian Equivalence Property under minimal assumptions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/imldpzmlnl/cover.png"/></item><item><title>A generalized neural tangent kernel for surrogate gradient learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/kfdexqu6mc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/kfdexqu6mc/</guid><description>Researchers introduce a generalized neural tangent kernel for analyzing surrogate gradient learning in neural networks with non-differentiable activation functions, providing a strong theoretical foun&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/kfdexqu6mc/cover.png"/></item><item><title>Almost Surely Asymptotically Constant Graph Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/dn68qdftry/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dn68qdftry/</guid><description>Many graph neural networks (GNNs) surprisingly converge to constant outputs with increasing graph size, limiting their expressiveness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dn68qdftry/cover.png"/></item><item><title>Back to the Continuous Attractor</title><link>https://deep-diver.github.io/neurips2024/posters/fvg6zhrh0b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fvg6zhrh0b/</guid><description>Despite their brittleness, continuous attractors remain functionally robust analog memory models due to persistent slow manifolds surviving bifurcations, enabling accurate approximation and generaliza&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fvg6zhrh0b/cover.png"/></item><item><title>Benign overfitting in leaky ReLU networks with moderate input dimension</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/88tzdgypt6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/88tzdgypt6/</guid><description>Leaky ReLU networks exhibit benign overfitting under surprisingly relaxed conditions: input dimension only needs to linearly scale with sample size, challenging prior assumptions in the field.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/88tzdgypt6/cover.png"/></item><item><title>Bridging Multicalibration and Out-of-distribution Generalization Beyond Covariate Shift</title><link>https://deep-diver.github.io/neurips2024/posters/bos6wpv0jf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bos6wpv0jf/</guid><description>New model-agnostic framework for out-of-distribution generalization uses multicalibration across overlapping groups, showing improved robustness and prediction under various distribution shifts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bos6wpv0jf/cover.png"/></item><item><title>Can neural operators always be continuously discretized?</title><link>https://deep-diver.github.io/neurips2024/posters/cyjxphdw3b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cyjxphdw3b/</guid><description>Neural operators&amp;rsquo; continuous discretization is proven impossible in general Hilbert spaces, but achievable using strongly monotone operators, opening new avenues for numerical methods in scientific ma&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cyjxphdw3b/cover.png"/></item><item><title>Compositional PAC-Bayes: Generalization of GNNs with persistence and beyond</title><link>https://deep-diver.github.io/neurips2024/posters/zncjtnn3e8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zncjtnn3e8/</guid><description>Novel compositional PAC-Bayes framework delivers data-dependent generalization bounds for persistence-enhanced Graph Neural Networks, improving model design and performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zncjtnn3e8/cover.png"/></item><item><title>Continual learning with the neural tangent ensemble</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/qosfijdvkz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/qosfijdvkz/</guid><description>Neural networks, viewed as Bayesian ensembles of fixed classifiers, enable continual learning without forgetting; posterior updates mirror stochastic gradient descent, offering insights into optimizat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/qosfijdvkz/cover.png"/></item><item><title>Controlling Multiple Errors Simultaneously with a PAC-Bayes Bound</title><link>https://deep-diver.github.io/neurips2024/posters/lwpfh9wvko/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lwpfh9wvko/</guid><description>New PAC-Bayes bound controls multiple error types simultaneously, providing richer generalization guarantees.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lwpfh9wvko/cover.png"/></item><item><title>Credal Learning Theory</title><link>https://deep-diver.github.io/neurips2024/posters/ah5kwussln/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ah5kwussln/</guid><description>Credal Learning Theory uses convex sets of probabilities to model data distribution variability, providing theoretical risk bounds for machine learning models in dynamic environments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ah5kwussln/cover.png"/></item><item><title>Deep Homomorphism Networks</title><link>https://deep-diver.github.io/neurips2024/posters/kxuijdmfdg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kxuijdmfdg/</guid><description>Deep Homomorphism Networks (DHNs) boost graph neural network (GNN) expressiveness by efficiently detecting subgraph patterns using a novel graph homomorphism layer.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kxuijdmfdg/cover.png"/></item><item><title>Dimension-free deterministic equivalents and scaling laws for random feature regression</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/fbljifw64d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/fbljifw64d/</guid><description>This work delivers dimension-free deterministic equivalents for random feature regression, revealing sharp excess error rates and scaling laws.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/fbljifw64d/cover.png"/></item><item><title>Dissecting the Failure of Invariant Learning on Graphs</title><link>https://deep-diver.github.io/neurips2024/posters/7efs8azham/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7efs8azham/</guid><description>Cross-environment Intra-class Alignment (CIA) and its label-free variant, CIA-LRA, significantly improve node-level OOD generalization on graphs by aligning representations and eliminating spurious fe&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7efs8azham/cover.png"/></item><item><title>Drift-Resilient TabPFN: In-Context Learning Temporal Distribution Shifts on Tabular Data</title><link>https://deep-diver.github.io/neurips2024/posters/p3tsefmwpg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/p3tsefmwpg/</guid><description>Drift-Resilient TabPFN masters temporal data shifts!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/p3tsefmwpg/cover.png"/></item><item><title>Explicit Eigenvalue Regularization Improves Sharpness-Aware Minimization</title><link>https://deep-diver.github.io/neurips2024/posters/jfuhby34sc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jfuhby34sc/</guid><description>Eigen-SAM significantly boosts generalization in deep learning by directly addressing SAM&amp;rsquo;s limitations through explicit top Hessian eigenvalue regularization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jfuhby34sc/cover.png"/></item><item><title>Generalizablity of Memorization Neural Network</title><link>https://deep-diver.github.io/neurips2024/posters/sabwo1ztfi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sabwo1ztfi/</guid><description>Unlocking deep learning&amp;rsquo;s generalization mystery, this research pioneers a theoretical understanding of memorization neural network generalizability, revealing critical network structural requirements&amp;hellip;</description></item><item><title>Generalization Bounds via Conditional $f$-Information</title><link>https://deep-diver.github.io/neurips2024/posters/ocxvxe5xn1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ocxvxe5xn1/</guid><description>New information-theoretic generalization bounds, based on conditional f-information, improve existing methods by addressing unboundedness and offering a generic approach applicable to various loss fun&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ocxvxe5xn1/cover.png"/></item><item><title>Generalization Error Bounds for Two-stage Recommender Systems with Tree Structure</title><link>https://deep-diver.github.io/neurips2024/oral-ai-theory/m1a4crrjr7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-ai-theory/m1a4crrjr7/</guid><description>Two-stage recommender systems using tree structures achieve better generalization with more branches and harmonized training data distributions across stages.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-ai-theory/m1a4crrjr7/cover.png"/></item><item><title>Generalization of Hamiltonian algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/0csq1sg7db/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0csq1sg7db/</guid><description>New, tighter generalization bounds are derived for a class of stochastic learning algorithms that generate absolutely continuous probability distributions; enhancing our understanding of their perform&amp;hellip;</description></item><item><title>Graph Neural Networks and Arithmetic Circuits</title><link>https://deep-diver.github.io/neurips2024/posters/0zeonp33f0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0zeonp33f0/</guid><description>Graph Neural Networks&amp;rsquo; (GNNs) computational power precisely mirrors that of arithmetic circuits, as proven via a novel C-GNN model; this reveals fundamental limits to GNN scalability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0zeonp33f0/cover.png"/></item><item><title>Implicit Regularization Paths of Weighted Neural Representations</title><link>https://deep-diver.github.io/neurips2024/posters/oxcmwwkqtz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oxcmwwkqtz/</guid><description>Weighted pretrained features implicitly regularize models, and this paper reveals equivalent paths between weighting schemes and ridge regularization, enabling efficient hyperparameter tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oxcmwwkqtz/cover.png"/></item><item><title>Improving Adaptivity via Over-Parameterization in Sequence Models</title><link>https://deep-diver.github.io/neurips2024/posters/uflh4t676k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uflh4t676k/</guid><description>Over-parameterized gradient descent dynamically adapts to signal structure, improving sequence model generalization and outperforming fixed-kernel methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uflh4t676k/cover.png"/></item><item><title>Information-theoretic Generalization Analysis for Expected Calibration Error</title><link>https://deep-diver.github.io/neurips2024/posters/yltjalwtw9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yltjalwtw9/</guid><description>New theoretical analysis reveals optimal binning strategies for minimizing bias in expected calibration error (ECE), improving machine learning model calibration evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yltjalwtw9/cover.png"/></item><item><title>Least Squares Regression Can Exhibit Under-Parameterized Double Descent</title><link>https://deep-diver.github.io/neurips2024/posters/gzh9ntutsy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gzh9ntutsy/</guid><description>Under-parameterized linear regression models can surprisingly exhibit double descent, contradicting traditional bias-variance assumptions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gzh9ntutsy/cover.png"/></item><item><title>Model Collapse Demystified: The Case of Regression</title><link>https://deep-diver.github.io/neurips2024/posters/biohntrnqk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/biohntrnqk/</guid><description>Training AI models on AI-generated data leads to performance degradation, known as model collapse. This paper offers analytical formulas that precisely quantify this effect in high-dimensional regress&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/biohntrnqk/cover.png"/></item><item><title>Neural network learns low-dimensional polynomials with SGD near the information-theoretic limit</title><link>https://deep-diver.github.io/neurips2024/posters/qk4is49kdm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qk4is49kdm/</guid><description>SGD can train neural networks to learn low-dimensional polynomials near the information-theoretic limit, surpassing previous correlational statistical query lower bounds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qk4is49kdm/cover.png"/></item><item><title>No Free Delivery Service: Epistemic limits of passive data collection in complex social systems</title><link>https://deep-diver.github.io/neurips2024/posters/xz0fpoakeb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xz0fpoakeb/</guid><description>Passive data collection in complex social systems invalidates standard AI model validation; new methods are needed.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xz0fpoakeb/cover.png"/></item><item><title>On Feature Learning in Structured State Space Models</title><link>https://deep-diver.github.io/neurips2024/posters/aqv5abn1wf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/aqv5abn1wf/</guid><description>Unlocking the scaling secrets of structured state-space models, this research identifies novel scaling rules for improved stability, generalization, and hyperparameter transferability, revolutionizing&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/aqv5abn1wf/cover.png"/></item><item><title>On Statistical Rates and Provably Efficient Criteria of Latent Diffusion Transformers (DiTs)</title><link>https://deep-diver.github.io/neurips2024/posters/cv2lkbdlz4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cv2lkbdlz4/</guid><description>Latent Diffusion Transformers (DiTs) achieve almost-linear time training and inference through low-rank gradient approximations and efficient criteria, overcoming high dimensionality challenges.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cv2lkbdlz4/cover.png"/></item><item><title>On the Expressivity and Sample Complexity of Node-Individualized Graph Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/8appyps0yn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8appyps0yn/</guid><description>Boosting GNN expressivity and generalization: Novel node individualization schemes lower sample complexity, improving substructure identification.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8appyps0yn/cover.png"/></item><item><title>On the Impacts of the Random Initialization in the Neural Tangent Kernel Theory</title><link>https://deep-diver.github.io/neurips2024/posters/ni3ud2bv3g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ni3ud2bv3g/</guid><description>Standard initialization in neural networks negatively impacts generalization ability under Neural Tangent Kernel theory, contradicting real-world performance, urging the development of improved theore&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ni3ud2bv3g/cover.png"/></item><item><title>On the Saturation Effects of Spectral Algorithms in Large Dimensions</title><link>https://deep-diver.github.io/neurips2024/posters/kjzeclysri/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kjzeclysri/</guid><description>High-dimensional spectral algorithms show saturation effects: Kernel Ridge Regression underperforms optimal algorithms like gradient flow when regression functions are very smooth.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kjzeclysri/cover.png"/></item><item><title>Overfitting Behaviour of Gaussian Kernel Ridgeless Regression: Varying Bandwidth or Dimensionality</title><link>https://deep-diver.github.io/neurips2024/posters/7sh0xkn1ks/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7sh0xkn1ks/</guid><description>Ridgeless regression, surprisingly, generalizes well even with noisy data if dimension scales sub-polynomially with sample size.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7sh0xkn1ks/cover.png"/></item><item><title>PAC-Bayes-Chernoff bounds for unbounded losses</title><link>https://deep-diver.github.io/neurips2024/posters/cyzzend3lb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cyzzend3lb/</guid><description>New PAC-Bayes oracle bound extends CramÃ©r-Chernoff to unbounded losses, enabling exact parameter optimization and richer assumptions for tighter generalization bounds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cyzzend3lb/cover.png"/></item><item><title>Partial observation can induce mechanistic mismatches in data-constrained models of neural dynamics</title><link>https://deep-diver.github.io/neurips2024/posters/lcegp7ir6k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lcegp7ir6k/</guid><description>Partially observing neural circuits during experiments can create misleading models, even if single neuron activity matches; researchers need better validation methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lcegp7ir6k/cover.png"/></item><item><title>Partial Transportability for Domain Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/2v5ltfhcfd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2v5ltfhcfd/</guid><description>This paper introduces a novel technique to bound prediction risks in new domains using causal diagrams, enabling reliable AI performance guarantees.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2v5ltfhcfd/cover.png"/></item><item><title>Position Coupling: Improving Length Generalization of Arithmetic Transformers Using Task Structure</title><link>https://deep-diver.github.io/neurips2024/posters/5cirdgm1ug/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/5cirdgm1ug/</guid><description>Position coupling, a novel method, enhances the length generalization ability of arithmetic Transformers by directly embedding task structures into positional encodings. This simple technique enables&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/5cirdgm1ug/cover.png"/></item><item><title>Provable Tempered Overfitting of Minimal Nets and Typical Nets</title><link>https://deep-diver.github.io/neurips2024/posters/qyr1dndxrp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qyr1dndxrp/</guid><description>Deep learning&amp;rsquo;s generalization ability defies conventional wisdom; this paper proves that overfitting in deep neural networks is &amp;rsquo;tempered&amp;rsquo;, neither catastrophic nor perfectly benign, for both minimal&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qyr1dndxrp/cover.png"/></item><item><title>Sketchy Moment Matching: Toward Fast and Provable Data Selection for Finetuning</title><link>https://deep-diver.github.io/neurips2024/posters/yaaqwbmgit/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yaaqwbmgit/</guid><description>Sketchy Moment Matching (SkMM) is a fast and theoretically sound data selection method for deep learning finetuning. By controlling variance-bias tradeoffs in high dimensions, SkMM drastically reduces&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yaaqwbmgit/cover.png"/></item><item><title>Stability and Generalization of Asynchronous SGD: Sharper Bounds Beyond Lipschitz and Smoothness</title><link>https://deep-diver.github.io/neurips2024/posters/bhp9hx4svi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bhp9hx4svi/</guid><description>Sharper ASGD generalization bounds achieved by leveraging on-average model stability, even without Lipschitz and smoothness assumptions; validated with diverse machine learning models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bhp9hx4svi/cover.png"/></item><item><title>Stable Minima Cannot Overfit in Univariate ReLU Networks: Generalization by Large Step Sizes</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/7swrtm9qsp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/7swrtm9qsp/</guid><description>Deep ReLU networks trained with large, constant learning rates avoid overfitting in univariate regression due to minima stability, generalizing well even with noisy labels.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/7swrtm9qsp/cover.png"/></item><item><title>Testably Learning Polynomial Threshold Functions</title><link>https://deep-diver.github.io/neurips2024/posters/5g0z6pdogj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/5g0z6pdogj/</guid><description>Testably learning polynomial threshold functions efficiently, matching agnostic learning&amp;rsquo;s best guarantees, is achieved, solving a key problem in robust machine learning.</description></item><item><title>The Power of Hard Attention Transformers on Data Sequences: A formal language theoretic perspective</title><link>https://deep-diver.github.io/neurips2024/posters/nbq1vmfp4x/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nbq1vmfp4x/</guid><description>Hard attention transformers show surprisingly greater power when processing numerical data sequences, exceeding capabilities on string data; this advancement is theoretically analyzed via circuit comp&amp;hellip;</description></item><item><title>Theoretical Analysis of Weak-to-Strong Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/hosh0skkle/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hosh0skkle/</guid><description>Strong student models can learn from weaker teachers, even correcting errors and generalizing beyond the teacher&amp;rsquo;s expertise. This paper provides new theoretical bounds explaining this &amp;lsquo;weak-to-strong&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hosh0skkle/cover.png"/></item><item><title>Theoretical Foundations of Deep Selective State-Space Models</title><link>https://deep-diver.github.io/neurips2024/posters/3szrqwupux/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3szrqwupux/</guid><description>Deep learning&amp;rsquo;s sequence modeling is revolutionized by selective state-space models (SSMs)! This paper provides theoretical grounding for their superior performance, revealing the crucial role of gati&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3szrqwupux/cover.png"/></item><item><title>Theoretical guarantees in KL for Diffusion Flow Matching</title><link>https://deep-diver.github.io/neurips2024/posters/ia4wucwha9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ia4wucwha9/</guid><description>Novel theoretical guarantees for Diffusion Flow Matching (DFM) models are established, bounding the KL divergence under mild assumptions on data and base distributions.</description></item><item><title>Topological Generalization Bounds for Discrete-Time Stochastic Optimization Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/6u5fchiwoc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/6u5fchiwoc/</guid><description>New topology-based complexity measures reliably predict deep learning model generalization, outperforming existing methods and offering practical computational efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/6u5fchiwoc/cover.png"/></item><item><title>Transcendence: Generative Models Can Outperform The Experts That Train Them</title><link>https://deep-diver.github.io/neurips2024/posters/ejg9udqcy9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ejg9udqcy9/</guid><description>Generative models can outperform their human trainers: A groundbreaking study shows how autoregressive transformers, trained on chess game data, can achieve higher game ratings than any of the human &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ejg9udqcy9/cover.png"/></item><item><title>Transformation-Invariant Learning and Theoretical Guarantees for OOD Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/u2gzfxrlan/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/u2gzfxrlan/</guid><description>This paper introduces a novel theoretical framework for robust machine learning under distribution shifts, offering learning rules and guarantees, highlighting the game-theoretic viewpoint of distribu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/u2gzfxrlan/cover.png"/></item><item><title>Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/0o7rd5jngv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0o7rd5jngv/</guid><description>This work systematically investigates the approximation properties of Transformer networks for sequence modeling, revealing the distinct roles of key components (self-attention, positional encoding, f&amp;hellip;</description></item><item><title>When are dynamical systems learned from time series data statistically accurate?</title><link>https://deep-diver.github.io/neurips2024/posters/4t3ox9hj3z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4t3ox9hj3z/</guid><description>Learned dynamical systems often fail to capture true physical behavior; this work introduces an ergodic theoretic approach to improve statistical accuracy by incorporating Jacobian information during &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4t3ox9hj3z/cover.png"/></item></channel></rss>