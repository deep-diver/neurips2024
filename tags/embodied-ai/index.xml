<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Embodied AI on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/embodied-ai/</link><description>Recent content in Embodied AI on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/embodied-ai/index.xml" rel="self" type="application/rss+xml"/><item><title>Any2Policy: Learning Visuomotor Policy with Any-Modality</title><link>https://deep-diver.github.io/neurips2024/posters/8lcw9ltjx9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8lcw9ltjx9/</guid><description>Any2Policy: a unified multi-modal system enabling robots to perform tasks using diverse instruction and observation modalities (text, image, audio, video, point cloud).</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8lcw9ltjx9/cover.png"/></item><item><title>Architect: Generating Vivid and Interactive 3D Scenes with Hierarchical 2D Inpainting</title><link>https://deep-diver.github.io/neurips2024/posters/jhg9enuw6p/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jhg9enuw6p/</guid><description>ARCHITECT: Generating realistic 3D scenes using hierarchical 2D inpainting!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jhg9enuw6p/cover.png"/></item><item><title>Exploratory Retrieval-Augmented Planning For Continual Embodied Instruction Following</title><link>https://deep-diver.github.io/neurips2024/posters/lpxv29ggl3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lpxv29ggl3/</guid><description>ExRAP: A novel framework boosts embodied AI&amp;rsquo;s continual instruction following by cleverly combining environment exploration with LLM-based planning, leading to significantly improved task success and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lpxv29ggl3/cover.png"/></item><item><title>GenRL: Multimodal-foundation world models for generalization in embodied agents</title><link>https://deep-diver.github.io/neurips2024/posters/za9jx8yqua/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/za9jx8yqua/</guid><description>GenRL: Learn diverse embodied tasks from vision &amp;amp; language, without reward design, using multimodal imagination!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/za9jx8yqua/cover.png"/></item><item><title>Grounding Multimodal Large Language Models in Actions</title><link>https://deep-diver.github.io/neurips2024/posters/0gl5wxy6es/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0gl5wxy6es/</guid><description>Researchers unveil unified architecture for grounding multimodal large language models in actions, showing superior performance with learned tokenization for continuous actions and semantic alignment &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0gl5wxy6es/cover.png"/></item><item><title>MO-DDN: A Coarse-to-Fine Attribute-based Exploration Agent for Multi-Object Demand-driven Navigation</title><link>https://deep-diver.github.io/neurips2024/posters/mztdzhmjec/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mztdzhmjec/</guid><description>MO-DDN: A new benchmark and coarse-to-fine exploration agent boosts embodied AI&amp;rsquo;s ability to handle multi-object, preference-based task planning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mztdzhmjec/cover.png"/></item><item><title>SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation</title><link>https://deep-diver.github.io/neurips2024/posters/hmcmxbcpp2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hmcmxbcpp2/</guid><description>SG-Nav achieves state-of-the-art zero-shot object navigation by leveraging a novel 3D scene graph to provide rich context for LLM-based reasoning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hmcmxbcpp2/cover.png"/></item><item><title>Trajectory Diffusion for ObjectGoal Navigation</title><link>https://deep-diver.github.io/neurips2024/posters/1gpy0hsv2w/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1gpy0hsv2w/</guid><description>Trajectory Diffusion (T-Diff) significantly improves object goal navigation by learning sequential planning through trajectory diffusion, resulting in more accurate and efficient navigation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1gpy0hsv2w/cover.png"/></item></channel></rss>