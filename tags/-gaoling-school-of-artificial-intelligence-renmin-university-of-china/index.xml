<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ Gaoling School of Artificial Intelligence, Renmin University of China on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-gaoling-school-of-artificial-intelligence-renmin-university-of-china/</link><description>Recent content in üè¢ Gaoling School of Artificial Intelligence, Renmin University of China on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-gaoling-school-of-artificial-intelligence-renmin-university-of-china/index.xml" rel="self" type="application/rss+xml"/><item><title>Are High-Degree Representations Really Unnecessary in Equivariant Graph Neural Networks?</title><link>https://deep-diver.github.io/neurips2024/posters/m0ncnvugyn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/m0ncnvugyn/</guid><description>High-degree representations significantly boost the expressiveness of E(3)-equivariant GNNs, overcoming limitations of lower-degree models on symmetric structures, as demonstrated theoretically and em&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/m0ncnvugyn/cover.png"/></item><item><title>Exploring Context Window of Large Language Models via Decomposed Positional Vectors</title><link>https://deep-diver.github.io/neurips2024/spotlight-large-language-models/zeyyq0gpxo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-large-language-models/zeyyq0gpxo/</guid><description>Researchers extended large language models&amp;rsquo; context windows by training-free methods via analyzing and manipulating positional vectors, improving long-text processing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-large-language-models/zeyyq0gpxo/cover.png"/></item><item><title>FineCLIP: Self-distilled Region-based CLIP for Better Fine-grained Understanding</title><link>https://deep-diver.github.io/neurips2024/posters/nexi4fukwd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nexi4fukwd/</guid><description>FineCLIP boosts fine-grained image understanding by combining real-time self-distillation with semantically rich regional contrastive learning, significantly outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nexi4fukwd/cover.png"/></item><item><title>Lower Bounds of Uniform Stability in Gradient-Based Bilevel Algorithms for Hyperparameter Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/u3mzzd0pdx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/u3mzzd0pdx/</guid><description>This paper establishes tight lower bounds for the uniform stability of gradient-based bilevel programming algorithms used for hyperparameter optimization, resolving a key open problem regarding the ti&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/u3mzzd0pdx/cover.png"/></item><item><title>Mixture of In-Context Experts Enhance LLMs' Long Context Awareness</title><link>https://deep-diver.github.io/neurips2024/posters/rcphboficn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rcphboficn/</guid><description>MoICE, a novel plug-in, significantly enhances LLMs&amp;rsquo; long context awareness by dynamically routing attention using multiple RoPE angles, achieving superior performance with high inference efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rcphboficn/cover.png"/></item><item><title>Reflective Multi-Agent Collaboration based on Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/wwiar5mqxq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wwiar5mqxq/</guid><description>COPPER enhances LLM-based multi-agent collaboration via a self-reflection mechanism and counterfactual PPO. It improves reflection quality, alleviates credit assignment issues, and shows strong perfo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wwiar5mqxq/cover.png"/></item><item><title>StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses</title><link>https://deep-diver.github.io/neurips2024/posters/envvjpx97o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/envvjpx97o/</guid><description>StreamingDialogue revolutionizes prolonged dialogue learning by compressing long contexts into conversational attention sinks, minimizing information loss and achieving a 4x speedup with 18x less memo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/envvjpx97o/cover.png"/></item></channel></rss>