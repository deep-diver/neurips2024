<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Optimization on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/optimization/</link><description>Recent content in Optimization on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/optimization/index.xml" rel="self" type="application/rss+xml"/><item><title>4+3 Phases of Compute-Optimal Neural Scaling Laws</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/avsxwicpak/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/avsxwicpak/</guid><description>Researchers discovered four distinct compute-optimal phases for training neural networks, offering new predictions for resource-efficient large model training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/avsxwicpak/cover.png"/></item><item><title>A Combinatorial Algorithm for the Semi-Discrete Optimal Transport Problem</title><link>https://deep-diver.github.io/neurips2024/posters/xq0jwbczkn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xq0jwbczkn/</guid><description>A new combinatorial algorithm dramatically speeds up semi-discrete optimal transport calculations, offering an efficient solution for large datasets and higher dimensions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xq0jwbczkn/cover.png"/></item><item><title>A Neural Network Approach for Efficiently Answering Most Probable Explanation Queries in Probabilistic Models</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ufppf9ghzp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ufppf9ghzp/</guid><description>A novel neural network efficiently answers arbitrary Most Probable Explanation (MPE) queries in large probabilistic models, eliminating the need for slow inference algorithms.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ufppf9ghzp/cover.png"/></item><item><title>A Primal-Dual-Assisted Penalty Approach to Bilevel Optimization with Coupled Constraints</title><link>https://deep-diver.github.io/neurips2024/posters/uzi7h5ac0x/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uzi7h5ac0x/</guid><description>BLOCC, a novel first-order algorithm, efficiently solves bilevel optimization problems with coupled constraints, offering improved scalability and convergence for machine learning applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uzi7h5ac0x/cover.png"/></item><item><title>A provable control of sensitivity of neural networks through a direct parameterization of the overall bi-Lipschitzness</title><link>https://deep-diver.github.io/neurips2024/posters/ww62xltefb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ww62xltefb/</guid><description>New framework directly controls neural network sensitivity by precisely parameterizing overall bi-Lipschitzness, offering improved robustness and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ww62xltefb/cover.png"/></item><item><title>A Separation in Heavy-Tailed Sampling: Gaussian vs. Stable Oracles for Proximal Samplers</title><link>https://deep-diver.github.io/neurips2024/posters/zuwlghgxtq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zuwlghgxtq/</guid><description>Stable oracles outperform Gaussian oracles in high-accuracy heavy-tailed sampling, overcoming limitations of Gaussian-based proximal samplers.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zuwlghgxtq/cover.png"/></item><item><title>A Simple and Adaptive Learning Rate for FTRL in Online Learning with Minimax Regret of Î˜(T^{2/3}) and its Application to Best-of-Both-Worlds</title><link>https://deep-diver.github.io/neurips2024/posters/xlvuz9f50g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xlvuz9f50g/</guid><description>A new adaptive learning rate for FTRL achieves minimax regret of O(TÂ²/Â³) in online learning, improving existing best-of-both-worlds algorithms for various hard problems.</description></item><item><title>A Simple and Optimal Approach for Universal Online Learning with Gradient Variations</title><link>https://deep-diver.github.io/neurips2024/posters/yo5dvychzr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yo5dvychzr/</guid><description>A novel universal online learning algorithm achieves optimal gradient-variation regret across diverse function curvatures, boasting efficiency with only one gradient query per round.</description></item><item><title>Accelerated Regularized Learning in Finite N-Person Games</title><link>https://deep-diver.github.io/neurips2024/posters/lw2zyqm0ox/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lw2zyqm0ox/</guid><description>Accelerated learning in games achieved! FTXL algorithm exponentially speeds up convergence to Nash equilibria in finite N-person games, even under limited feedback.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lw2zyqm0ox/cover.png"/></item><item><title>Accelerating ERM for data-driven algorithm design using output-sensitive techniques</title><link>https://deep-diver.github.io/neurips2024/posters/yw3tlswusb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yw3tlswusb/</guid><description>Accelerating ERM for data-driven algorithm design using output-sensitive techniques achieves computationally efficient learning by scaling with the actual number of pieces in the dual loss function, n&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yw3tlswusb/cover.png"/></item><item><title>Accelerating Nash Equilibrium Convergence in Monte Carlo Settings Through Counterfactual Value Based Fictitious Play</title><link>https://deep-diver.github.io/neurips2024/posters/efd9n5zdfc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/efd9n5zdfc/</guid><description>MCCFVFP, a novel Monte Carlo-based algorithm, accelerates Nash equilibrium convergence in large-scale games by combining CFR&amp;rsquo;s counterfactual value calculations with fictitious play&amp;rsquo;s best response st&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/efd9n5zdfc/cover.png"/></item><item><title>Acceleration Exists! Optimization Problems When Oracle Can Only Compare Objective Function Values</title><link>https://deep-diver.github.io/neurips2024/posters/kxbsnewb42/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kxbsnewb42/</guid><description>Accelerated gradient-free optimization is achieved using only function value comparisons, significantly improving black-box optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kxbsnewb42/cover.png"/></item><item><title>Adam with model exponential moving average is effective for nonconvex optimization</title><link>https://deep-diver.github.io/neurips2024/posters/v416yloquu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v416yloquu/</guid><description>Clipped Adam with EMA achieves optimal convergence rates for smooth and non-smooth nonconvex optimization, particularly when scales vary across different coordinates.</description></item><item><title>Adaptive Proximal Gradient Method for Convex Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/qlh21ig1ic/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qlh21ig1ic/</guid><description>Adaptive gradient descent methods are improved by leveraging local curvature information for entirely adaptive algorithms without added computational cost, proving convergence with only local Lipschit&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qlh21ig1ic/cover.png"/></item><item><title>Adaptive Sampling for Efficient Softmax Approximation</title><link>https://deep-diver.github.io/neurips2024/posters/xsna2b8gpz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xsna2b8gpz/</guid><description>AdaptiveSoftmax: Achieve 10x+ speedup in softmax computation via adaptive sampling!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xsna2b8gpz/cover.png"/></item><item><title>Adaptive Variance Reduction for Stochastic Optimization under Weaker Assumptions</title><link>https://deep-diver.github.io/neurips2024/posters/tmqh8prqlc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tmqh8prqlc/</guid><description>Adaptive STORM achieves optimal convergence rates for stochastic optimization of non-convex functions under weaker assumptions, eliminating the need for bounded gradients or function values and removi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tmqh8prqlc/cover.png"/></item><item><title>Almost Free: Self-concordance in Natural Exponential Families and an Application to Bandits</title><link>https://deep-diver.github.io/neurips2024/posters/lkwvyvx66i/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lkwvyvx66i/</guid><description>Generalized linear bandits with subexponential reward distributions are self-concordant, enabling second-order regret bounds free of exponential dependence on problem parameters.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lkwvyvx66i/cover.png"/></item><item><title>An Accelerated Gradient Method for Convex Smooth Simple Bilevel Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/afodln7jbv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/afodln7jbv/</guid><description>Accelerated Gradient Method for Bilevel Optimization (AGM-BiO) achieves state-of-the-art convergence rates for simple bilevel optimization problems, requiring fewer iterations than existing methods to&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/afodln7jbv/cover.png"/></item><item><title>An Improved Empirical Fisher Approximation for Natural Gradient Descent</title><link>https://deep-diver.github.io/neurips2024/posters/lmjlrhvcmg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lmjlrhvcmg/</guid><description>Improved Empirical Fisher (iEF) approximation significantly boosts the performance of Natural Gradient Descent (NGD) optimizers, offering superior convergence and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lmjlrhvcmg/cover.png"/></item><item><title>Approximating the Top Eigenvector in Random Order Streams</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/gitgmieinf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/gitgmieinf/</guid><description>Random-order stream data necessitates efficient top eigenvector approximation; this paper presents novel algorithms with improved space complexity, achieving near-optimal bounds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/gitgmieinf/cover.png"/></item><item><title>Approximation-Aware Bayesian Optimization</title><link>https://deep-diver.github.io/neurips2024/spotlight-optimization/t7euv5dl5m/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-optimization/t7euv5dl5m/</guid><description>Approximation-Aware Bayesian Optimization (AABO) boosts high-dimensional Bayesian optimization by jointly optimizing model approximation and data acquisition, achieving superior efficiency and perform&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-optimization/t7euv5dl5m/cover.png"/></item><item><title>Are Graph Neural Networks Optimal Approximation Algorithms?</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/sxrblm9ams/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/sxrblm9ams/</guid><description>Graph Neural Networks (GNNs) learn optimal approximation algorithms for combinatorial optimization problems, achieving high-quality solutions for Max-Cut, Min-Vertex-Cover, and Max-3-SAT, while also p&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/sxrblm9ams/cover.png"/></item><item><title>Asymptotics of Alpha-Divergence Variational Inference Algorithms with Exponential Families</title><link>https://deep-diver.github.io/neurips2024/posters/hfqf8lolhs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hfqf8lolhs/</guid><description>This paper rigorously analyzes alpha-divergence variational inference, proving its convergence and providing convergence rates, thereby advancing the theoretical foundations of this increasingly impor&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hfqf8lolhs/cover.png"/></item><item><title>Autobidder's Dilemma: Why More Sophisticated Autobidders Lead to Worse Auction Efficiency</title><link>https://deep-diver.github.io/neurips2024/posters/hqjksiskaa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hqjksiskaa/</guid><description>More sophisticated autobidders surprisingly worsen online auction efficiency; a fine-grained analysis reveals that less powerful, uniform bidders lead to better market outcomes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hqjksiskaa/cover.png"/></item><item><title>Automatic Outlier Rectification via Optimal Transport</title><link>https://deep-diver.github.io/neurips2024/posters/udxhmgjvjb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/udxhmgjvjb/</guid><description>This study presents a novel single-step outlier rectification method using optimal transport with a concave cost function, surpassing the limitations of conventional two-stage approaches by jointly op&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/udxhmgjvjb/cover.png"/></item><item><title>Average gradient outer product as a mechanism for deep neural collapse</title><link>https://deep-diver.github.io/neurips2024/posters/vtrotud539/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vtrotud539/</guid><description>Deep Neural Collapse (DNC) explained via Average Gradient Outer Product (AGOP).</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vtrotud539/cover.png"/></item><item><title>Axioms for AI Alignment from Human Feedback</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/cmbjkpruvw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/cmbjkpruvw/</guid><description>This paper revolutionizes AI alignment by applying social choice theory axioms to RLHF, exposing flaws in existing methods and proposing novel, axiomatically guaranteed reward learning rules.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/cmbjkpruvw/cover.png"/></item><item><title>Bandits with Preference Feedback: A Stackelberg Game Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/wie991zhxh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wie991zhxh/</guid><description>MAXMINLCB, a novel game-theoretic algorithm, efficiently solves bandit problems with preference feedback over continuous domains, providing anytime-valid, rate-optimal regret guarantees.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wie991zhxh/cover.png"/></item><item><title>Barely Random Algorithms and Collective Metrical Task Systems</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/oajhfvrtbq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/oajhfvrtbq/</guid><description>Randomness-efficient algorithms are developed for online decision making, requiring only 2log n random bits and achieving near-optimal competitiveness for metrical task systems.</description></item><item><title>Batched Energy-Entropy acquisition for Bayesian Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/wqijnypent/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wqijnypent/</guid><description>BEEBO: a novel acquisition function for Bayesian Optimization, offering superior explore-exploit balance and handling large batches efficiently, even with noisy data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wqijnypent/cover.png"/></item><item><title>Bayes-optimal learning of an extensive-width neural network from quadratically many samples</title><link>https://deep-diver.github.io/neurips2024/posters/r8znyrjxj3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/r8znyrjxj3/</guid><description>This study solves a key challenge in neural network learning, deriving a closed-form expression for the Bayes-optimal test error of extensive-width networks with quadratic activation functions from qu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/r8znyrjxj3/cover.png"/></item><item><title>Bayesian Optimization of Functions over Node Subsets in Graphs</title><link>https://deep-diver.github.io/neurips2024/posters/kxjgi1krbi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kxjgi1krbi/</guid><description>GraphComBO efficiently optimizes functions defined on node subsets within graphs using Bayesian Optimization. It tackles challenges posed by combinatorial complexity and computationally expensive fun&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kxjgi1krbi/cover.png"/></item><item><title>Bayesian Strategic Classification</title><link>https://deep-diver.github.io/neurips2024/posters/sadbrpog2k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sadbrpog2k/</guid><description>Learners can improve accuracy in strategic classification by selectively revealing partial classifier information to agents, strategically guiding agent behavior and maximizing accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sadbrpog2k/cover.png"/></item><item><title>Binary Search with Distributional Predictions</title><link>https://deep-diver.github.io/neurips2024/posters/jekxtljeiq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jekxtljeiq/</guid><description>This paper presents a novel algorithm for binary search using distributional predictions, achieving optimal query complexity O(H(p) + log n) and demonstrating enhanced robustness against prediction er&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jekxtljeiq/cover.png"/></item><item><title>Boundary Decomposition for Nadir Objective Vector Estimation</title><link>https://deep-diver.github.io/neurips2024/posters/f829mkqmug/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/f829mkqmug/</guid><description>BDNE: a novel boundary decomposition method accurately estimates the nadir objective vector in complex multi-objective optimization problems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/f829mkqmug/cover.png"/></item><item><title>Challenges of Generating Structurally Diverse Graphs</title><link>https://deep-diver.github.io/neurips2024/posters/bbgpol1nlo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bbgpol1nlo/</guid><description>Researchers developed novel algorithms to generate structurally diverse graphs, improving graph algorithm testing and neural network evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bbgpol1nlo/cover.png"/></item><item><title>Collaboration! Towards Robust Neural Methods for Routing Problems</title><link>https://deep-diver.github.io/neurips2024/posters/yfqa78gefa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yfqa78gefa/</guid><description>A novel Collaborative Neural Framework (CNF) enhances the robustness of neural vehicle routing methods against adversarial attacks by collaboratively training multiple models and intelligently distrib&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yfqa78gefa/cover.png"/></item><item><title>Communication Bounds for the Distributed Experts Problem</title><link>https://deep-diver.github.io/neurips2024/posters/hyxjsi3szf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hyxjsi3szf/</guid><description>This paper presents communication-efficient protocols for the distributed experts problem, achieving near-optimal regret with theoretical and empirical validation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hyxjsi3szf/cover.png"/></item><item><title>Communication Efficient Distributed Training with Distributed Lion</title><link>https://deep-diver.github.io/neurips2024/posters/wdircetioz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wdircetioz/</guid><description>Distributed Lion: Training large AI models efficiently by communicating only binary or low-precision vectors between workers and a server, significantly reducing communication costs and maintaining co&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wdircetioz/cover.png"/></item><item><title>Conformal Inverse Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/y2nwklrdrx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y2nwklrdrx/</guid><description>Conformal inverse optimization learns uncertainty sets for parameters in optimization models, then solves a robust optimization model for high-quality, human-aligned decisions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y2nwklrdrx/cover.png"/></item><item><title>Constrained Binary Decision Making</title><link>https://deep-diver.github.io/neurips2024/posters/ntv5xzfzek/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ntv5xzfzek/</guid><description>This paper presents a unified framework for solving binary statistical decision-making problems, enabling efficient derivation of optimal strategies for diverse applications like OOD detection and sel&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ntv5xzfzek/cover.png"/></item><item><title>Constrained Sampling with Primal-Dual Langevin Monte Carlo</title><link>https://deep-diver.github.io/neurips2024/posters/o6hk6vld20/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/o6hk6vld20/</guid><description>Constrained sampling made easy! Primal-Dual Langevin Monte Carlo efficiently samples from complex probability distributions while satisfying statistical constraints.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/o6hk6vld20/cover.png"/></item><item><title>Contextual Decision-Making with Knapsacks Beyond the Worst Case</title><link>https://deep-diver.github.io/neurips2024/posters/dgt6sh2ruq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dgt6sh2ruq/</guid><description>This work unveils a novel algorithm for contextual decision-making with knapsacks, achieving significantly improved regret bounds beyond worst-case scenarios, thereby offering a more practical and eff&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dgt6sh2ruq/cover.png"/></item><item><title>Contextual Linear Optimization with Bandit Feedback</title><link>https://deep-diver.github.io/neurips2024/posters/lodbhkqzrh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lodbhkqzrh/</guid><description>This paper introduces induced empirical risk minimization for contextual linear optimization with bandit feedback, providing theoretical guarantees and computationally tractable solutions for improved&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lodbhkqzrh/cover.png"/></item><item><title>Convergence of $ ext{log}(1/psilon)$ for Gradient-Based Algorithms in Zero-Sum Games without the Condition Number: A Smoothed Analysis</title><link>https://deep-diver.github.io/neurips2024/posters/hovxlc8vqu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hovxlc8vqu/</guid><description>Gradient-based methods for solving large zero-sum games achieve polynomial smoothed complexity, demonstrating efficiency even in high-precision scenarios without condition number dependence.</description></item><item><title>Cost-aware Bayesian Optimization via the Pandora's Box Gittins Index</title><link>https://deep-diver.github.io/neurips2024/posters/ouc1f0sfb7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ouc1f0sfb7/</guid><description>Cost-aware Bayesian optimization gets a boost with the Pandora&amp;rsquo;s Box Gittins Index, a novel acquisition function that efficiently balances exploration and exploitation while considering evaluation cos&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ouc1f0sfb7/cover.png"/></item><item><title>CSPG: Crossing Sparse Proximity Graphs for Approximate Nearest Neighbor Search</title><link>https://deep-diver.github.io/neurips2024/posters/ohvxbipv7e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ohvxbipv7e/</guid><description>CSPG: a novel framework boosting Approximate Nearest Neighbor Search speed by 1.5-2x, using sparse proximity graphs and efficient two-staged search.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ohvxbipv7e/cover.png"/></item><item><title>Decision-Focused Learning with Directional Gradients</title><link>https://deep-diver.github.io/neurips2024/posters/g8kflzdcax/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/g8kflzdcax/</guid><description>New Perturbation Gradient losses connect expected decisions with directional derivatives, enabling Lipschitz continuous surrogates for predict-then-optimize, asymptotically yielding best-in-class poli&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/g8kflzdcax/cover.png"/></item><item><title>Derandomizing Multi-Distribution Learning</title><link>https://deep-diver.github.io/neurips2024/posters/twye75mnkt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/twye75mnkt/</guid><description>Derandomizing multi-distribution learning is computationally hard, but a structural condition allows efficient black-box conversion of randomized predictors to deterministic ones.</description></item><item><title>Distributed Least Squares in Small Space via Sketching and Bias Reduction</title><link>https://deep-diver.github.io/neurips2024/posters/rkuvyost2c/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rkuvyost2c/</guid><description>Researchers developed a novel sparse sketching method for distributed least squares regression, achieving near-unbiased estimates with optimal space and time complexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rkuvyost2c/cover.png"/></item><item><title>Distribution Learning with Valid Outputs Beyond the Worst-Case</title><link>https://deep-diver.github.io/neurips2024/posters/l7i5fjgkjc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/l7i5fjgkjc/</guid><description>Generative models often produce invalid outputs; this work shows that ensuring validity is easier than expected when using log-loss and carefully selecting model classes and data distributions.</description></item><item><title>Distributional regression: CRPS-error bounds for model fitting, model selection and convex aggregation</title><link>https://deep-diver.github.io/neurips2024/posters/csfxzcozpu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/csfxzcozpu/</guid><description>This paper provides the first statistical learning guarantees for distributional regression using CRPS, offering concentration bounds for model fitting, selection, and convex aggregation, applicable t&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/csfxzcozpu/cover.png"/></item><item><title>DistrictNet: Decision-aware learning for geographical districting</title><link>https://deep-diver.github.io/neurips2024/posters/njwybfau8e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/njwybfau8e/</guid><description>DISTRICTNET: A novel decision-aware learning approach drastically cuts geographical districting costs by integrating combinatorial optimization and graph neural networks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/njwybfau8e/cover.png"/></item><item><title>Drago: Primal-Dual Coupled Variance Reduction for Faster Distributionally Robust Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/ujk0xrntqz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ujk0xrntqz/</guid><description>DRAGO: A novel primal-dual algorithm delivers faster, state-of-the-art convergence for distributionally robust optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ujk0xrntqz/cover.png"/></item><item><title>Efficient Combinatorial Optimization via Heat Diffusion</title><link>https://deep-diver.github.io/neurips2024/posters/psdrko9v1d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/psdrko9v1d/</guid><description>Heat Diffusion Optimization (HeO) framework efficiently solves combinatorial optimization problems by enabling information propagation through heat diffusion, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/psdrko9v1d/cover.png"/></item><item><title>Efficient Graph Matching for Correlated Stochastic Block Models</title><link>https://deep-diver.github.io/neurips2024/posters/nbhficdnrp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nbhficdnrp/</guid><description>Efficient algorithm achieves near-perfect graph matching in correlated stochastic block models, resolving a key open problem and enabling improved community detection.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nbhficdnrp/cover.png"/></item><item><title>Efficient Sign-Based Optimization: Accelerating Convergence via Variance Reduction</title><link>https://deep-diver.github.io/neurips2024/posters/uanzvf1vfe/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uanzvf1vfe/</guid><description>Sign-based optimization gets a speed boost! This paper introduces new algorithms that significantly accelerate convergence in distributed optimization by cleverly using variance reduction and enhanced&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uanzvf1vfe/cover.png"/></item><item><title>Energy-Guided Continuous Entropic Barycenter Estimation for General Costs</title><link>https://deep-diver.github.io/neurips2024/spotlight-optimization/jzhfrloqdq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-optimization/jzhfrloqdq/</guid><description>New algorithm approximates continuous Entropic Optimal Transport (EOT) barycenters for any cost function, offering quality bounds and seamless EBM integration.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-optimization/jzhfrloqdq/cover.png"/></item><item><title>Entropy testing and its application to testing Bayesian networks</title><link>https://deep-diver.github.io/neurips2024/posters/bmsxealci4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bmsxealci4/</guid><description>This paper presents near-optimal algorithms for entropy identity testing, significantly improving Bayesian network testing efficiency.</description></item><item><title>Entrywise error bounds for low-rank approximations of kernel matrices</title><link>https://deep-diver.github.io/neurips2024/posters/ziyc4fhrnr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ziyc4fhrnr/</guid><description>This paper provides novel entrywise error bounds for low-rank kernel matrix approximations, showing how many data points are needed to get statistically consistent results for low-rank approximations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ziyc4fhrnr/cover.png"/></item><item><title>Estimating Generalization Performance Along the Trajectory of Proximal SGD in Robust Regression</title><link>https://deep-diver.github.io/neurips2024/posters/ntf7d8talq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ntf7d8talq/</guid><description>New consistent estimators precisely track generalization error during robust regression&amp;rsquo;s iterative model training, enabling optimal stopping iteration for minimized error.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ntf7d8talq/cover.png"/></item><item><title>Exact, Tractable Gauss-Newton Optimization in Deep Reversible Architectures Reveal Poor Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/p37nlki9vl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/p37nlki9vl/</guid><description>Exact Gauss-Newton optimization in deep reversible networks surprisingly reveals poor generalization, despite faster training, challenging existing deep learning optimization theories.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/p37nlki9vl/cover.png"/></item><item><title>Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport</title><link>https://deep-diver.github.io/neurips2024/spotlight-optimization/4da5vaphfb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-optimization/4da5vaphfb/</guid><description>ENOT, a new Neural Optimal Transport training method, achieves 3x quality and 10x speed improvements by using expectile regularization to stabilize the learning process.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-optimization/4da5vaphfb/cover.png"/></item><item><title>Exploring Jacobian Inexactness in Second-Order Methods for Variational Inequalities: Lower Bounds, Optimal Algorithms and Quasi-Newton Approximations</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/uvfdaefr9x/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/uvfdaefr9x/</guid><description>VIJI, a novel second-order algorithm, achieves optimal convergence rates for variational inequalities even with inexact Jacobian information, bridging the gap between theory and practice in machine le&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/uvfdaefr9x/cover.png"/></item><item><title>Fast Rates in Stochastic Online Convex Optimization by Exploiting the Curvature of Feasible Sets</title><link>https://deep-diver.github.io/neurips2024/posters/y58t1mqhh6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y58t1mqhh6/</guid><description>This paper introduces a novel approach for fast rates in online convex optimization by exploiting the curvature of feasible sets, achieving logarithmic regret bounds under specific conditions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y58t1mqhh6/cover.png"/></item><item><title>Fast T2T: Optimization Consistency Speeds Up Diffusion-Based Training-to-Testing Solving for Combinatorial Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/xdrkzozeoc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xdrkzozeoc/</guid><description>Fast T2T: Optimization Consistency Boosts Diffusion-Based Combinatorial Optimization!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xdrkzozeoc/cover.png"/></item><item><title>Faster Accelerated First-order Methods for Convex Optimization with Strongly Convex Function Constraints</title><link>https://deep-diver.github.io/neurips2024/posters/pg380vlyru/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pg380vlyru/</guid><description>Faster primal-dual algorithms achieve order-optimal complexity for convex optimization with strongly convex constraints, improving convergence rates and solving large-scale problems efficiently.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pg380vlyru/cover.png"/></item><item><title>Feedback control guides credit assignment in recurrent neural networks</title><link>https://deep-diver.github.io/neurips2024/posters/xavwvnjtst/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xavwvnjtst/</guid><description>Brain-inspired recurrent neural networks learn efficiently by using feedback control to approximate optimal gradients, enabling rapid movement corrections and efficient adaptation to persistent errors&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xavwvnjtst/cover.png"/></item><item><title>First-Order Methods for Linearly Constrained Bilevel Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/encyptcghr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/encyptcghr/</guid><description>First-order methods conquer linearly constrained bilevel optimization, achieving near-optimal convergence rates and enhancing high-dimensional applicability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/encyptcghr/cover.png"/></item><item><title>From Linear to Linearizable Optimization: A Novel Framework with Applications to Stationary and Non-stationary DR-submodular Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/dgamsmeef8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dgamsmeef8/</guid><description>A novel framework extends optimization algorithms from linear/quadratic functions to a broader class of &amp;lsquo;upper-linearizable&amp;rsquo; functions, providing a unified approach for concave and DR-submodular optim&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dgamsmeef8/cover.png"/></item><item><title>FUGAL: Feature-fortified Unrestricted Graph Alignment</title><link>https://deep-diver.github.io/neurips2024/posters/sdlos1fr4h/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sdlos1fr4h/</guid><description>FUGAL: a groundbreaking graph alignment method surpassing state-of-the-art accuracy without compromising efficiency by directly aligning adjacency matrices.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sdlos1fr4h/cover.png"/></item><item><title>Functionally Constrained Algorithm Solves Convex Simple Bilevel Problem</title><link>https://deep-diver.github.io/neurips2024/posters/paighjppam/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/paighjppam/</guid><description>Near-optimal algorithms solve convex simple bilevel problems by reformulating them into functionally constrained problems, achieving near-optimal convergence rates.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/paighjppam/cover.png"/></item><item><title>Fundamental Convergence Analysis of Sharpness-Aware Minimization</title><link>https://deep-diver.github.io/neurips2024/posters/puxyi4hoqu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/puxyi4hoqu/</guid><description>This research establishes fundamental convergence properties for the widely-used SAM optimization algorithm, significantly advancing our theoretical understanding and practical applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/puxyi4hoqu/cover.png"/></item><item><title>General bounds on the quality of Bayesian coresets</title><link>https://deep-diver.github.io/neurips2024/posters/sazeqv2ptt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sazeqv2ptt/</guid><description>New theoretical bounds on Bayesian coreset approximation errors enable efficient large-scale Bayesian inference, overcoming prior limitations and improving coreset construction methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sazeqv2ptt/cover.png"/></item><item><title>Get rich quick: exact solutions reveal how unbalanced initializations promote rapid feature learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/enm94i7r3a/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/enm94i7r3a/</guid><description>Unbalanced initializations dramatically accelerate neural network feature learning by modifying the geometry of learning trajectories, enabling faster feature extraction and improved generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/enm94i7r3a/cover.png"/></item><item><title>Gliding over the Pareto Front with Uniform Designs</title><link>https://deep-diver.github.io/neurips2024/posters/woexvqchfw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/woexvqchfw/</guid><description>UMOD: a novel multi-objective optimization algorithm efficiently generates uniformly distributed Pareto-optimal solutions by maximizing minimal pairwise distances, providing high-quality representatio&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/woexvqchfw/cover.png"/></item><item><title>GLinSAT: The General Linear Satisfiability Neural Network Layer By Accelerated Gradient Descent</title><link>https://deep-diver.github.io/neurips2024/posters/m1pvjnhvtp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/m1pvjnhvtp/</guid><description>GLinSAT: A novel neural network layer efficiently solves general linear constraint satisfaction problems via accelerated gradient descent, enabling differentiable backpropagation and improved GPU perf&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/m1pvjnhvtp/cover.png"/></item><item><title>Gradient Guidance for Diffusion Models: An Optimization Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/x1qeuybxke/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x1qeuybxke/</guid><description>This paper provides a novel optimization framework for guided diffusion models, proving Ã•(1/K) convergence for concave objective functions and demonstrating structure-preserving guidance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x1qeuybxke/cover.png"/></item><item><title>Gradient Methods for Online DR-Submodular Maximization with Stochastic Long-Term Constraints</title><link>https://deep-diver.github.io/neurips2024/posters/ptxrruephq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ptxrruephq/</guid><description>Novel gradient-based algorithms achieve O(âˆšT) regret and O(T3/4) constraint violation for online DR-submodular maximization with stochastic long-term constraints.</description></item><item><title>Guided Trajectory Generation with Diffusion Models for Offline Model-based Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/iokqzb8smr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/iokqzb8smr/</guid><description>GTG, a novel conditional generative modeling approach, leverages diffusion models to generate high-scoring design trajectories for offline model-based optimization, outperforming existing methods on b&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/iokqzb8smr/cover.png"/></item><item><title>HardCore Generation: Generating Hard UNSAT Problems for Data Augmentation</title><link>https://deep-diver.github.io/neurips2024/posters/njvpjg0bfk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/njvpjg0bfk/</guid><description>HardCore: Fast generation of hard, realistic UNSAT problems for improved SAT solver runtime prediction.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/njvpjg0bfk/cover.png"/></item><item><title>High-probability complexity bounds for stochastic non-convex minimax optimization</title><link>https://deep-diver.github.io/neurips2024/posters/xmqtnzlgtj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xmqtnzlgtj/</guid><description>First high-probability complexity guarantees for solving stochastic nonconvex minimax problems using a single-loop method are established.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xmqtnzlgtj/cover.png"/></item><item><title>How does Gradient Descent Learn Features --- A Local Analysis for Regularized Two-Layer Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/xyw051zmun/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xyw051zmun/</guid><description>Neural networks learn features effectively through gradient descent, not just at the beginning, but also at the end of training, even with carefully regularized objectives.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xyw051zmun/cover.png"/></item><item><title>Identification of Analytic Nonlinear Dynamical Systems with Non-asymptotic Guarantees</title><link>https://deep-diver.github.io/neurips2024/posters/nf34qxcy0b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nf34qxcy0b/</guid><description>This paper proves that non-active exploration suffices for identifying linearly parameterized nonlinear systems with real-analytic features, providing non-asymptotic guarantees for least-squares and s&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nf34qxcy0b/cover.png"/></item><item><title>Identifying Equivalent Training Dynamics</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/boyvesx7pk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/boyvesx7pk/</guid><description>New framework uses Koopman operator theory to identify equivalent training dynamics in deep neural networks, enabling quantitative comparison of different architectures and optimization methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/boyvesx7pk/cover.png"/></item><item><title>Implicit Bias of Mirror Flow on Separable Data</title><link>https://deep-diver.github.io/neurips2024/posters/wimaws0fwb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wimaws0fwb/</guid><description>Mirror descent&amp;rsquo;s implicit bias on separable data is formally characterized, revealing convergence towards a maximum margin classifier determined by the potential&amp;rsquo;s &amp;lsquo;horizon function&amp;rsquo;.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wimaws0fwb/cover.png"/></item><item><title>Improved Guarantees for Fully Dynamic $k$-Center Clustering with Outliers in General Metric Spaces</title><link>https://deep-diver.github.io/neurips2024/posters/otycp1yfbx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/otycp1yfbx/</guid><description>A novel fully dynamic algorithm achieves a (4+Îµ)-approximate solution for the k-center clustering problem with outliers in general metric spaces, boasting an efficient update time.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/otycp1yfbx/cover.png"/></item><item><title>Improved Regret for Bandit Convex Optimization with Delayed Feedback</title><link>https://deep-diver.github.io/neurips2024/posters/ar9jvkogjm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ar9jvkogjm/</guid><description>A novel algorithm, D-FTBL, achieves improved regret bounds for bandit convex optimization with delayed feedback, tightly matching existing lower bounds in worst-case scenarios.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ar9jvkogjm/cover.png"/></item><item><title>Improved Sample Complexity for Multiclass PAC Learning</title><link>https://deep-diver.github.io/neurips2024/posters/l2yvtrz3on/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/l2yvtrz3on/</guid><description>This paper significantly improves our understanding of multiclass PAC learning by reducing the sample complexity gap and proposing two novel approaches to fully resolve the optimal sample complexity.</description></item><item><title>Incorporating Surrogate Gradient Norm to Improve Offline Optimization Techniques</title><link>https://deep-diver.github.io/neurips2024/posters/ag7piyoyut/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ag7piyoyut/</guid><description>IGNITE improves offline optimization by incorporating surrogate gradient norm to reduce model sharpness, boosting performance up to 9.6%</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ag7piyoyut/cover.png"/></item><item><title>Inexact Augmented Lagrangian Methods for Conic Optimization: Quadratic Growth and Linear Convergence</title><link>https://deep-diver.github.io/neurips2024/posters/sj8g020adl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sj8g020adl/</guid><description>This paper proves that inexact ALMs applied to SDPs achieve &lt;strong>linear convergence for both primal and dual iterates&lt;/strong>, contingent solely on strict complementarity and a bounded solution set, thus resol&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sj8g020adl/cover.png"/></item><item><title>Information-theoretic Limits of Online Classification with Noisy Labels</title><link>https://deep-diver.github.io/neurips2024/posters/ke3msp8nr6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ke3msp8nr6/</guid><description>This paper unveils the information-theoretic limits of online classification with noisy labels, showing that the minimax risk is tightly characterized by the Hellinger gap of noisy label distributions&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ke3msp8nr6/cover.png"/></item><item><title>Invariant subspaces and PCA in nearly matrix multiplication time</title><link>https://deep-diver.github.io/neurips2024/posters/wyp8vsl9de/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wyp8vsl9de/</guid><description>Generalized eigenvalue problems get solved in nearly matrix multiplication time, providing new, faster PCA algorithms!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wyp8vsl9de/cover.png"/></item><item><title>Is Knowledge Power? On the (Im)possibility of Learning from Strategic Interactions</title><link>https://deep-diver.github.io/neurips2024/posters/dlm6z1rrjv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dlm6z1rrjv/</guid><description>In strategic settings, repeated interactions alone may not enable uninformed players to achieve optimal outcomes, highlighting the persistent impact of information asymmetry.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dlm6z1rrjv/cover.png"/></item><item><title>Is Score Matching Suitable for Estimating Point Processes?</title><link>https://deep-diver.github.io/neurips2024/posters/hqghcvzihw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hqghcvzihw/</guid><description>Weighted score matching offers a consistent, efficient solution for estimating parameters in point processes, overcoming the limitations of previous methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hqghcvzihw/cover.png"/></item><item><title>Iterative Methods via Locally Evolving Set Process</title><link>https://deep-diver.github.io/neurips2024/posters/wt2kheb97a/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wt2kheb97a/</guid><description>This paper proposes a novel framework, the locally evolving set process, to develop faster localized iterative methods for solving large-scale graph problems, achieving significant speedup over existi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wt2kheb97a/cover.png"/></item><item><title>John Ellipsoids via Lazy Updates</title><link>https://deep-diver.github.io/neurips2024/posters/lcj0rvr4d6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lcj0rvr4d6/</guid><description>Faster John ellipsoid computation achieved via lazy updates and fast matrix multiplication, improving efficiency and enabling low-space streaming algorithms.</description></item><item><title>Latent Neural Operator for Solving Forward and Inverse PDE Problems</title><link>https://deep-diver.github.io/neurips2024/posters/vlw8zykfcm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vlw8zykfcm/</guid><description>Latent Neural Operator (LNO) dramatically improves solving PDEs by using a latent space, boosting accuracy and reducing computation costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vlw8zykfcm/cover.png"/></item><item><title>Learning diffusion at lightspeed</title><link>https://deep-diver.github.io/neurips2024/oral-ai-theory/y10avdrfnk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-ai-theory/y10avdrfnk/</guid><description>JKOnet* learns diffusion processes at unprecedented speed and accuracy by directly minimizing a simple quadratic loss function, bypassing complex bilevel optimization problems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-ai-theory/y10avdrfnk/cover.png"/></item><item><title>Learning Elastic Costs to Shape Monge Displacements</title><link>https://deep-diver.github.io/neurips2024/posters/aauvnpqvbz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/aauvnpqvbz/</guid><description>Learn optimal transport maps with structured displacements using elastic costs and a novel bilevel loss function!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/aauvnpqvbz/cover.png"/></item><item><title>Learning Generalized Linear Programming Value Functions</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/vxijl0ioid/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/vxijl0ioid/</guid><description>Learn optimal LP values faster with a novel neural network method!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/vxijl0ioid/cover.png"/></item><item><title>Learning Optimal Tax Design in Nonatomic Congestion Games</title><link>https://deep-diver.github.io/neurips2024/posters/qdprhde3jb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qdprhde3jb/</guid><description>AI learns optimal taxes for congestion games, maximizing social welfare with limited feedback, via a novel algorithm.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qdprhde3jb/cover.png"/></item><item><title>Learning Social Welfare Functions</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/7o6ktaar8n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/7o6ktaar8n/</guid><description>Learning social welfare functions from past decisions is possible! This paper shows how to efficiently learn power mean functions, a widely used family, using both cardinal and pairwise welfare compar&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/7o6ktaar8n/cover.png"/></item><item><title>Learning the Expected Core of Strictly Convex Stochastic Cooperative Games</title><link>https://deep-diver.github.io/neurips2024/posters/zryfftr4xn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zryfftr4xn/</guid><description>A novel Common-Points-Picking algorithm efficiently learns stable reward allocations (expected core) in strictly convex stochastic cooperative games with unknown reward distributions, achieving high p&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zryfftr4xn/cover.png"/></item><item><title>Learning to compute GrÃ¶bner bases</title><link>https://deep-diver.github.io/neurips2024/posters/zrz7xlxbzq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zrz7xlxbzq/</guid><description>AI learns to compute GrÃ¶bner bases, solving a notorious computational algebra problem efficiently via Transformers and novel algebraic techniques.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zrz7xlxbzq/cover.png"/></item><item><title>Learning to Handle Complex Constraints for Vehicle Routing Problems</title><link>https://deep-diver.github.io/neurips2024/posters/ktx95zurjp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ktx95zurjp/</guid><description>Proactive Infeasibility Prevention (PIP) framework significantly improves neural methods for solving complex Vehicle Routing Problems by proactively preventing infeasible solutions and enhancing const&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ktx95zurjp/cover.png"/></item><item><title>Learning to Mitigate Externalities: the Coase Theorem with Hindsight Rationality</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/omyzrkacme/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/omyzrkacme/</guid><description>Economists learn to resolve externalities efficiently even when players lack perfect information, maximizing social welfare by leveraging bargaining and online learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/omyzrkacme/cover.png"/></item><item><title>Learning to Solve Quadratic Unconstrained Binary Optimization in a Classification Way</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/p43obiwjfw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/p43obiwjfw/</guid><description>Researchers developed Value Classification Model (VCM), a neural solver that swiftly solves quadratic unconstrained binary optimization (QUBO) problems by directly generating solutions using a classif&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/p43obiwjfw/cover.png"/></item><item><title>Learning-Augmented Approximation Algorithms for Maximum Cut and Related Problems</title><link>https://deep-diver.github.io/neurips2024/posters/mirkqqx6po/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mirkqqx6po/</guid><description>This paper shows how noisy predictions about optimal solutions can improve approximation algorithms for NP-hard problems like MAX-CUT, exceeding classical hardness bounds.</description></item><item><title>Learning-Augmented Dynamic Submodular Maximization</title><link>https://deep-diver.github.io/neurips2024/posters/sty80vvbs8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sty80vvbs8/</guid><description>Leveraging predictions, this paper presents a novel algorithm for dynamic submodular maximization achieving significantly faster update times (O(poly(log n, log w, log k)) amortized) compared to exist&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sty80vvbs8/cover.png"/></item><item><title>Light Unbalanced Optimal Transport</title><link>https://deep-diver.github.io/neurips2024/posters/co8kzws1yk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/co8kzws1yk/</guid><description>LightUnbalancedOptimalTransport: A fast, theoretically-justified solver for continuous unbalanced optimal transport problems, enabling efficient analysis of large datasets with imbalanced classes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/co8kzws1yk/cover.png"/></item><item><title>Linear Transformers are Versatile In-Context Learners</title><link>https://deep-diver.github.io/neurips2024/posters/p1ft33mu3j/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/p1ft33mu3j/</guid><description>Linear transformers surprisingly learn intricate optimization algorithms, even surpassing baselines on noisy regression problems, showcasing their unexpected learning capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/p1ft33mu3j/cover.png"/></item><item><title>Log-concave Sampling from a Convex Body with a Barrier: a Robust and Unified Dikin Walk</title><link>https://deep-diver.github.io/neurips2024/posters/xkrsb5a79f/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xkrsb5a79f/</guid><description>This paper introduces robust Dikin walks for log-concave sampling, achieving faster mixing times and lower iteration costs than existing methods, particularly for high-dimensional settings.</description></item><item><title>Lookback Prophet Inequalities</title><link>https://deep-diver.github.io/neurips2024/posters/cg1vwt5xou/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cg1vwt5xou/</guid><description>This paper enhances prophet inequalities by allowing lookback, improving competitive ratios and providing algorithms for diverse observation orders, thereby bridging theory and real-world online selec&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cg1vwt5xou/cover.png"/></item><item><title>Loss Landscape Characterization of Neural Networks without Over-Parametrization</title><link>https://deep-diver.github.io/neurips2024/posters/h0a3p5wtxu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/h0a3p5wtxu/</guid><description>Deep learning optimization is revolutionized by a new function class, enabling convergence guarantees without over-parameterization and accommodating saddle points.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/h0a3p5wtxu/cover.png"/></item><item><title>Low-Rank Optimal Transport through Factor Relaxation with Latent Coupling</title><link>https://deep-diver.github.io/neurips2024/posters/hggkdff2hr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hggkdff2hr/</guid><description>FRLC: a novel algorithm for low-rank optimal transport using latent coupling, enabling faster computation and better interpretability for diverse applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hggkdff2hr/cover.png"/></item><item><title>Lower Bounds and Optimal Algorithms for Non-Smooth Convex Decentralized Optimization over Time-Varying Networks</title><link>https://deep-diver.github.io/neurips2024/posters/iukff7nymw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/iukff7nymw/</guid><description>First optimal algorithms matching lower bounds for non-smooth convex decentralized optimization over time-varying networks are presented, substantially improving theoretical performance.</description></item><item><title>Lower Bounds of Uniform Stability in Gradient-Based Bilevel Algorithms for Hyperparameter Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/u3mzzd0pdx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/u3mzzd0pdx/</guid><description>This paper establishes tight lower bounds for the uniform stability of gradient-based bilevel programming algorithms used for hyperparameter optimization, resolving a key open problem regarding the ti&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/u3mzzd0pdx/cover.png"/></item><item><title>Mean-Field Analysis for Learning Subspace-Sparse Polynomials with Gaussian Input</title><link>https://deep-diver.github.io/neurips2024/posters/una5hxin6v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/una5hxin6v/</guid><description>Researchers establish basis-free conditions for SGD learnability in two-layer neural networks learning subspace-sparse polynomials with Gaussian input, offering insights into training dynamics.</description></item><item><title>Mean-Field Langevin Dynamics for Signed Measures via a Bilevel Approach</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/oo7hy9kmk6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/oo7hy9kmk6/</guid><description>This paper presents a novel bilevel approach to extend mean-field Langevin dynamics to solve convex optimization problems over signed measures, achieving stronger guarantees and faster convergence rat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/oo7hy9kmk6/cover.png"/></item><item><title>Mechanism design augmented with output advice</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ajgks7qozm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ajgks7qozm/</guid><description>Mechanism design enhanced with output advice improves approximation guarantees by using imperfect predictions of the output, not agent types, offering robust, practical solutions.</description></item><item><title>Metric Transforms and Low Rank Representations of Kernels for Fast Attention</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/k9pxsryuwg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/k9pxsryuwg/</guid><description>Researchers unveil novel linear-algebraic tools revealing the limits of fast attention, classifying positive definite kernels for Manhattan distance, and fully characterizing metric transforms for Man&amp;hellip;</description></item><item><title>MILP-StuDio: MILP Instance Generation via Block Structure Decomposition</title><link>https://deep-diver.github.io/neurips2024/posters/w433ri0vu4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w433ri0vu4/</guid><description>MILP-StuDio generates high-quality mixed-integer linear programming instances by preserving crucial block structures, significantly improving learning-based solver performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w433ri0vu4/cover.png"/></item><item><title>Minimum Entropy Coupling with Bottleneck</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ylmym7shde/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ylmym7shde/</guid><description>A novel lossy compression framework, Minimum Entropy Coupling with Bottleneck (MEC-B), extends existing methods by integrating a bottleneck for controlled stochasticity, enhancing performance in scen&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ylmym7shde/cover.png"/></item><item><title>Mirror and Preconditioned Gradient Descent in Wasserstein Space</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/n12b6wva55/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/n12b6wva55/</guid><description>This paper presents novel mirror and preconditioned gradient descent algorithms for optimizing functionals over Wasserstein space, offering improved convergence and efficiency for various machine lear&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/n12b6wva55/cover.png"/></item><item><title>Motif-oriented influence maximization for viral marketing in large-scale social networks</title><link>https://deep-diver.github.io/neurips2024/posters/uyztzchaqb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uyztzchaqb/</guid><description>Motif-oriented influence maximization tackles viral marketing&amp;rsquo;s challenge of reaching groups by proving a greedy algorithm with guaranteed approximation ratio and near-linear time complexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uyztzchaqb/cover.png"/></item><item><title>Multi-Label Learning with Stronger Consistency Guarantees</title><link>https://deep-diver.github.io/neurips2024/posters/zauerb1kgx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zauerb1kgx/</guid><description>Novel surrogate losses with label-independent H-consistency bounds enable stronger guarantees for multi-label learning.</description></item><item><title>Multi-Winner Reconfiguration</title><link>https://deep-diver.github.io/neurips2024/posters/kzfxicbxd1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kzfxicbxd1/</guid><description>This paper introduces a novel model for multi-winner reconfiguration, analyzing the computational complexity of transitioning between committees using four approval-based voting rules, demonstrating b&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kzfxicbxd1/cover.png"/></item><item><title>Multiclass Transductive Online Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3erevfwalz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3erevfwalz/</guid><description>Unbounded label spaces conquered! New algorithm achieves optimal mistake bounds in multiclass transductive online learning.</description></item><item><title>Nature-Inspired Local Propagation</title><link>https://deep-diver.github.io/neurips2024/posters/ds6xmv3yvv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ds6xmv3yvv/</guid><description>Inspired by nature, researchers introduce a novel spatiotemporal local algorithm for machine learning that outperforms backpropagation in online learning scenarios with limited data or long video stre&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ds6xmv3yvv/cover.png"/></item><item><title>Nearly Minimax Optimal Regret for Multinomial Logistic Bandit</title><link>https://deep-diver.github.io/neurips2024/posters/q4nwfstqvf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/q4nwfstqvf/</guid><description>This paper presents OFU-MNL+, a constant-time algorithm achieving nearly minimax optimal regret for contextual multinomial logistic bandits, closing the gap between existing upper and lower bounds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/q4nwfstqvf/cover.png"/></item><item><title>Nearly Minimax Optimal Submodular Maximization with Bandit Feedback</title><link>https://deep-diver.github.io/neurips2024/posters/vn0fwrimra/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vn0fwrimra/</guid><description>This research establishes the first minimax optimal algorithm for submodular maximization with bandit feedback, achieving a regret bound matching the lower bound.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vn0fwrimra/cover.png"/></item><item><title>Nearly Optimal Approximation of Matrix Functions by the Lanczos Method</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3s8v8qp9xv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3s8v8qp9xv/</guid><description>Lanczos-FA, a simple algorithm for approximating matrix functions, surprisingly outperforms newer methods; this paper proves its near-optimality for rational functions, explaining its practical succes&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3s8v8qp9xv/cover.png"/></item><item><title>Neur2BiLO: Neural Bilevel Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/esvleaqkrc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/esvleaqkrc/</guid><description>NEUR2BILO: a neural network-based heuristic solves mixed-integer bilevel optimization problems extremely fast, achieving high-quality solutions for diverse applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/esvleaqkrc/cover.png"/></item><item><title>Neural Combinatorial Optimization for Robust Routing Problem with Uncertain Travel Times</title><link>https://deep-diver.github.io/neurips2024/posters/doewnm2ut3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/doewnm2ut3/</guid><description>Neural networks efficiently solve robust routing problems with uncertain travel times, minimizing worst-case deviations from optimal routes under the min-max regret criterion.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/doewnm2ut3/cover.png"/></item><item><title>Neural Pfaffians: Solving Many Many-Electron SchrÃ¶dinger Equations</title><link>https://deep-diver.github.io/neurips2024/oral-ai-theory/hrknicwm3e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-ai-theory/hrknicwm3e/</guid><description>Neural Pfaffians revolutionize many-electron SchrÃ¶dinger equation solutions by using fully learnable neural wave functions based on Pfaffians, achieving unprecedented accuracy and generalizability acr&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-ai-theory/hrknicwm3e/cover.png"/></item><item><title>Newton Losses: Using Curvature Information for Learning with Differentiable Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/vjaorqq71s/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vjaorqq71s/</guid><description>Newton Losses enhance training of neural networks with complex objectives by using second-order information from loss functions, achieving significant performance gains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vjaorqq71s/cover.png"/></item><item><title>No-regret Learning in Harmonic Games: Extrapolation in the Face of Conflicting Interests</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/hw9s9vy5gz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/hw9s9vy5gz/</guid><description>Extrapolated FTRL ensures Nash equilibrium convergence in harmonic games, defying standard no-regret learning limitations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/hw9s9vy5gz/cover.png"/></item><item><title>Non-asymptotic Global Convergence Analysis of BFGS with the Armijo-Wolfe Line Search</title><link>https://deep-diver.github.io/neurips2024/spotlight-optimization/mkzpn2t87c/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-optimization/mkzpn2t87c/</guid><description>BFGS algorithm achieves global linear and superlinear convergence rates with inexact Armijo-Wolfe line search, even without precise Hessian knowledge.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-optimization/mkzpn2t87c/cover.png"/></item><item><title>Non-geodesically-convex optimization in the Wasserstein space</title><link>https://deep-diver.github.io/neurips2024/posters/lgg1iqhbor/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lgg1iqhbor/</guid><description>A novel semi Forward-Backward Euler scheme provides convergence guarantees for non-geodesically-convex optimization in Wasserstein space, advancing both sampling and optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lgg1iqhbor/cover.png"/></item><item><title>On Convergence of Adam for Stochastic Optimization under Relaxed Assumptions</title><link>https://deep-diver.github.io/neurips2024/posters/x7usmidzxj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x7usmidzxj/</guid><description>Adam optimizer achieves near-optimal convergence in non-convex scenarios with unbounded gradients and relaxed noise assumptions, improving its theoretical understanding and practical application.</description></item><item><title>On the Expressive Power of Tree-Structured Probabilistic Circuits</title><link>https://deep-diver.github.io/neurips2024/posters/suyaaoi5bd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/suyaaoi5bd/</guid><description>Tree-structured probabilistic circuits are surprisingly efficient: this paper proves a quasi-polynomial upper bound on their size, showing they&amp;rsquo;re almost as expressive as more complex DAG structures.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/suyaaoi5bd/cover.png"/></item><item><title>On the Optimal Time Complexities in Decentralized Stochastic Asynchronous Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/ixra8admhx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ixra8admhx/</guid><description>Fragile SGD &amp;amp; Amelie SGD achieve near-optimal speed in decentralized asynchronous optimization, handling diverse worker &amp;amp; communication speeds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ixra8admhx/cover.png"/></item><item><title>On the Sparsity of the Strong Lottery Ticket Hypothesis</title><link>https://deep-diver.github.io/neurips2024/posters/abmesb1ajx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/abmesb1ajx/</guid><description>Researchers rigorously prove the Strong Lottery Ticket Hypothesis, offering the first theoretical guarantees on the sparsity of winning neural network subnetworks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/abmesb1ajx/cover.png"/></item><item><title>One-Layer Transformer Provably Learns One-Nearest Neighbor In Context</title><link>https://deep-diver.github.io/neurips2024/posters/wdx45lnzxe/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wdx45lnzxe/</guid><description>One-layer transformers provably learn the one-nearest neighbor prediction rule, offering theoretical insights into their in-context learning capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wdx45lnzxe/cover.png"/></item><item><title>Online Bayesian Persuasion Without a Clue</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/xnpvz8e1ty/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/xnpvz8e1ty/</guid><description>Researchers developed a novel online Bayesian persuasion algorithm that achieves sublinear regret without prior knowledge of the receiver or the state distribution, providing tight theoretical guarant&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/xnpvz8e1ty/cover.png"/></item><item><title>Online Budgeted Matching with General Bids</title><link>https://deep-diver.github.io/neurips2024/posters/vtxy8wfptj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vtxy8wfptj/</guid><description>MetaAd, a novel meta-algorithm, achieves provable competitive ratios for online budgeted matching with general bids, removing prior restrictive assumptions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vtxy8wfptj/cover.png"/></item><item><title>Online Consistency of the Nearest Neighbor Rule</title><link>https://deep-diver.github.io/neurips2024/posters/eox0smruv7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/eox0smruv7/</guid><description>The 1-nearest neighbor rule achieves online consistency under surprisingly broad conditions: measurable label functions and mild assumptions on instance generation in doubling metric spaces.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/eox0smruv7/cover.png"/></item><item><title>Online Convex Optimisation: The Optimal Switching Regret for all Segmentations Simultaneously</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/u6xxyud3ro/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/u6xxyud3ro/</guid><description>Algorithm RESET achieves optimal switching regret simultaneously across all segmentations, offering efficiency and parameter-free operation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/u6xxyud3ro/cover.png"/></item><item><title>Online Estimation via Offline Estimation: An Information-Theoretic Framework</title><link>https://deep-diver.github.io/neurips2024/posters/sks7x4i8bh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sks7x4i8bh/</guid><description>This paper introduces a novel information-theoretic framework, showing how to convert offline into online estimation algorithms efficiently, impacting interactive decision-making.</description></item><item><title>Online Learning of Delayed Choices</title><link>https://deep-diver.github.io/neurips2024/posters/gc3bznwqqp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gc3bznwqqp/</guid><description>New algorithms conquer delayed feedback in online choice modeling, achieving optimal decision-making even with unknown customer preferences and delayed responses.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gc3bznwqqp/cover.png"/></item><item><title>Online Weighted Paging with Unknown Weights</title><link>https://deep-diver.github.io/neurips2024/posters/ctxty3vggq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ctxty3vggq/</guid><description>First algorithm for online weighted paging that learns page weights from samples, achieving optimal O(log k) competitiveness and sublinear regret.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ctxty3vggq/cover.png"/></item><item><title>Only Strict Saddles in the Energy Landscape of Predictive Coding Networks?</title><link>https://deep-diver.github.io/neurips2024/posters/etu6kvrksq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/etu6kvrksq/</guid><description>Predictive coding networks learn faster than backpropagation by changing the loss landscape&amp;rsquo;s geometry, making saddles easier to escape and improving robustness to vanishing gradients.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/etu6kvrksq/cover.png"/></item><item><title>Optimal Algorithms for Augmented Testing of Discrete Distributions</title><link>https://deep-diver.github.io/neurips2024/posters/talmacqk9s/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/talmacqk9s/</guid><description>Leveraging predictions, this research presents novel algorithms for uniformity, identity, and closeness testing of discrete distributions, achieving information-theoretically optimal sample complexity&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/talmacqk9s/cover.png"/></item><item><title>Optimal Algorithms for Learning Partitions with Faulty Oracles</title><link>https://deep-diver.github.io/neurips2024/posters/ygdl8q02ga/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ygdl8q02ga/</guid><description>Optimal algorithms for learning partitions are designed, achieving minimum query complexity even with up to l faulty oracle responses.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ygdl8q02ga/cover.png"/></item><item><title>Optimal Algorithms for Online Convex Optimization with Adversarial Constraints</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/txffvjmnby/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/txffvjmnby/</guid><description>Optimal algorithms for online convex optimization with adversarial constraints are developed, achieving O(âˆšT) regret and Ã•(âˆšT) constraint violationâ€”a breakthrough in the field.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/txffvjmnby/cover.png"/></item><item><title>Optimal Hypothesis Selection in (Almost) Linear Time</title><link>https://deep-diver.github.io/neurips2024/posters/skv26jtefz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/skv26jtefz/</guid><description>This paper presents the first almost linear-time algorithm achieving the optimal accuracy parameter for hypothesis selection, solving a decades-long open problem.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/skv26jtefz/cover.png"/></item><item><title>Optimal Parallelization of Boosting</title><link>https://deep-diver.github.io/neurips2024/oral-ai-theory/rtz4df9if1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-ai-theory/rtz4df9if1/</guid><description>This paper closes the performance gap in parallel boosting algorithms by presenting improved lower bounds and a novel algorithm matching these bounds, settling the parallel complexity of sample-optima&amp;hellip;</description></item><item><title>Optimization Algorithm Design via Electric Circuits</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/9jmt1eer9p/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/9jmt1eer9p/</guid><description>Design provably convergent optimization algorithms swiftly using electric circuit analogies; a novel methodology automating discretization for diverse algorithms.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/9jmt1eer9p/cover.png"/></item><item><title>Optimization Can Learn Johnson Lindenstrauss Embeddings</title><link>https://deep-diver.github.io/neurips2024/posters/w3jctbrduf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w3jctbrduf/</guid><description>Optimization can learn optimal Johnson-Lindenstrauss embeddings, avoiding the limitations of randomized methods and achieving comparable theoretical guarantees.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w3jctbrduf/cover.png"/></item><item><title>Overcoming Brittleness in Pareto-Optimal Learning Augmented Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/estpcujzhe/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/estpcujzhe/</guid><description>This research introduces a novel framework that overcomes the brittleness of Pareto-optimal learning-augmented algorithms by enforcing smoothness in performance using user-specified profiles and devel&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/estpcujzhe/cover.png"/></item><item><title>Parameter Symmetry and Noise Equilibrium of Stochastic Gradient Descent</title><link>https://deep-diver.github.io/neurips2024/posters/uhki1re2nz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uhki1re2nz/</guid><description>SGD&amp;rsquo;s dynamics are precisely characterized by the interplay of noise and symmetry in loss functions, leading to unique, initialization-independent fixed points.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uhki1re2nz/cover.png"/></item><item><title>Parameter-free Clipped Gradient Descent Meets Polyak</title><link>https://deep-diver.github.io/neurips2024/posters/sgcnphyoeq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sgcnphyoeq/</guid><description>Parameter-free optimization is revolutionized! Inexact Polyak Stepsize achieves the same convergence rate as clipped gradient descent but without any hyperparameter tuning, saving time and computatio&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sgcnphyoeq/cover.png"/></item><item><title>Paths to Equilibrium in Games</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/lxxiiinmuf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/lxxiiinmuf/</guid><description>In n-player games, a satisficing path always exists leading from any initial strategy profile to a Nash equilibrium by allowing unsatisfied players to explore suboptimal strategies.</description></item><item><title>Penalty-based Methods for Simple Bilevel Optimization under HÃ¶lderian Error Bounds</title><link>https://deep-diver.github.io/neurips2024/posters/oq1zj9ih88/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oq1zj9ih88/</guid><description>This paper proposes penalty-based methods for simple bilevel optimization, achieving (Îµ, ÎµÎ²)-optimal solutions with improved complexity under HÃ¶lderian error bounds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oq1zj9ih88/cover.png"/></item><item><title>Piecewise-Stationary Bandits with Knapsacks</title><link>https://deep-diver.github.io/neurips2024/posters/haa457jwjw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/haa457jwjw/</guid><description>A novel inventory reserving algorithm achieves near-optimal performance for bandit problems with knapsacks in piecewise-stationary settings, offering a competitive ratio of O(log(nmax/min)).</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/haa457jwjw/cover.png"/></item><item><title>Precise asymptotics of reweighted least-squares algorithms for linear diagonal networks</title><link>https://deep-diver.github.io/neurips2024/posters/nv7ox1vd3q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nv7ox1vd3q/</guid><description>New analysis reveals how reweighted least-squares algorithms for linear diagonal networks achieve favorable performance in high-dimensional settings, improving upon existing theoretical guarantees and&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nv7ox1vd3q/cover.png"/></item><item><title>Pretrained Optimization Model for Zero-Shot Black Box Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/fwqhxdeusg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fwqhxdeusg/</guid><description>Pretrained Optimization Model (POM) excels at zero-shot black-box optimization, outperforming existing methods, especially in high dimensions, through direct application or few-shot fine-tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fwqhxdeusg/cover.png"/></item><item><title>Principled Bayesian Optimization in Collaboration with Human Experts</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/idn9sikgly/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/idn9sikgly/</guid><description>COBOL: a novel Bayesian Optimization algorithm leverages human expert advice via binary labels, achieving both fast convergence and robustness to noisy input, while guaranteeing minimal expert effort.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/idn9sikgly/cover.png"/></item><item><title>Provable Acceleration of Nesterov's Accelerated Gradient for Asymmetric Matrix Factorization and Linear Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/x44oawaq7b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x44oawaq7b/</guid><description>This paper proves Nesterov&amp;rsquo;s Accelerated Gradient achieves faster convergence for rectangular matrix factorization and linear neural networks, using a novel unbalanced initialization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x44oawaq7b/cover.png"/></item><item><title>Proving Theorems Recursively</title><link>https://deep-diver.github.io/neurips2024/posters/yaa5l92ttq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yaa5l92ttq/</guid><description>POETRY: a recursive neural theorem prover achieving 5.1% higher success rate and solving substantially longer proofs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yaa5l92ttq/cover.png"/></item><item><title>Putting Gale &amp; Shapley to Work: Guaranteeing Stability Through Learning</title><link>https://deep-diver.github.io/neurips2024/posters/ivjs67xa44/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ivjs67xa44/</guid><description>Researchers improve two-sided matching market algorithms by prioritizing stability through novel bandit-learning algorithms, providing theoretical bounds on sample complexity and demonstrating intrigu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ivjs67xa44/cover.png"/></item><item><title>Quadratic Quantum Variational Monte Carlo</title><link>https://deep-diver.github.io/neurips2024/posters/ldtabi541u/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ldtabi541u/</guid><description>Q2VMC, a novel quantum chemistry algorithm, drastically boosts the efficiency and accuracy of solving the SchrÃ¶dinger equation using a quadratic update mechanism and neural network ansatzes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ldtabi541u/cover.png"/></item><item><title>Quantum Algorithms for Non-smooth Non-convex Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/wsgzvhnoax/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wsgzvhnoax/</guid><description>Quantum algorithms achieve speedups in non-smooth, non-convex optimization, outperforming classical methods by a factor of Îµâ»Â²/Â³ in query complexity for finding (Î´,Îµ)-Goldstein stationary points.</description></item><item><title>Query-Efficient Correlation Clustering with Noisy Oracle</title><link>https://deep-diver.github.io/neurips2024/posters/wrcfuoiz1h/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wrcfuoiz1h/</guid><description>Novel algorithms for query-efficient correlation clustering with noisy oracles achieve a balance between query complexity and solution quality, offering theoretical guarantees and outperforming baseli&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wrcfuoiz1h/cover.png"/></item><item><title>Random Cycle Coding: Lossless Compression of Cluster Assignments via Bits-Back Coding</title><link>https://deep-diver.github.io/neurips2024/posters/xkvnqpdfqv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xkvnqpdfqv/</guid><description>Random Cycle Coding (RCC) optimally compresses cluster assignments in large datasets, saving up to 70% storage in vector databases by eliminating the need for integer IDs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xkvnqpdfqv/cover.png"/></item><item><title>Random Function Descent</title><link>https://deep-diver.github.io/neurips2024/posters/xzcubjhqbs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xzcubjhqbs/</guid><description>Random Function Descent (RFD) replaces the classical convex function framework with a random function approach, providing a scalable gradient descent method with inherent scale invariance and a theore&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xzcubjhqbs/cover.png"/></item><item><title>Randomized Strategic Facility Location with Predictions</title><link>https://deep-diver.github.io/neurips2024/posters/yvoen0kuzt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yvoen0kuzt/</guid><description>Randomized strategies improve truthful learning-augmented mechanisms for strategic facility location, achieving better approximations than deterministic methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yvoen0kuzt/cover.png"/></item><item><title>Reliable Learning of Halfspaces under Gaussian Marginals</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/0lb8vzt1db/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/0lb8vzt1db/</guid><description>New algorithm reliably learns Gaussian halfspaces with significantly improved sample and computational complexity compared to existing methods, offering strong computational separation from standard a&amp;hellip;</description></item><item><title>ReLIZO: Sample Reusable Linear Interpolation-based Zeroth-order Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/yzvianpvu6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yzvianpvu6/</guid><description>ReLIZO boosts zeroth-order optimization by cleverly reusing past queries, drastically cutting computation costs while maintaining gradient estimation accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yzvianpvu6/cover.png"/></item><item><title>Rethinking Parity Check Enhanced Symmetry-Preserving Ansatz</title><link>https://deep-diver.github.io/neurips2024/posters/aiubyryhhv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/aiubyryhhv/</guid><description>Enhanced VQAs via Hamming Weight Preserving ansatz and parity checks achieve superior performance on quantum chemistry and combinatorial problems, showcasing quantum advantage potential in NISQ era.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/aiubyryhhv/cover.png"/></item><item><title>RoPINN: Region Optimized Physics-Informed Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/wzigmvfurk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wzigmvfurk/</guid><description>ROPINN: Revolutionizing Physics-Informed Neural Networks with Region Optimization</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wzigmvfurk/cover.png"/></item><item><title>S-SOS: Stochastic Sum-Of-Squares for Parametric Polynomial Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/ichqijtjhb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ichqijtjhb/</guid><description>S-SOS: A new algorithm solves complex, parameterized polynomial problems with provable convergence, enabling efficient solutions for high-dimensional applications like sensor network localization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ichqijtjhb/cover.png"/></item><item><title>Safe Exploitative Play with Untrusted Type Beliefs</title><link>https://deep-diver.github.io/neurips2024/posters/qztj22aov4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qztj22aov4/</guid><description>This paper characterizes the fundamental tradeoff between trusting and distrusting learned type beliefs in games, establishing upper and lower bounds for optimal strategies in both normal-form and sto&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qztj22aov4/cover.png"/></item><item><title>SAMPa: Sharpness-aware Minimization Parallelized</title><link>https://deep-diver.github.io/neurips2024/posters/ign0ktydwv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ign0ktydwv/</guid><description>SAMPa: Parallelizing gradient computations in Sharpness-Aware Minimization (SAM) achieves a 2x speedup and superior generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ign0ktydwv/cover.png"/></item><item><title>Sample Complexity of Algorithm Selection Using Neural Networks and Its Applications to Branch-and-Cut</title><link>https://deep-diver.github.io/neurips2024/posters/uovrwvw1ya/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uovrwvw1ya/</guid><description>Neural networks enhance algorithm selection in branch-and-cut, significantly reducing tree sizes and improving efficiency for mixed-integer optimization, as proven by rigorous theoretical bounds and e&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uovrwvw1ya/cover.png"/></item><item><title>Sample Complexity of Posted Pricing for a Single Item</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ek1tyhcb3w/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ek1tyhcb3w/</guid><description>This paper reveals how many buyer samples are needed to set near-optimal posted prices for a single item, resolving a fundamental problem in online markets and offering both theoretical and practical &amp;hellip;</description></item><item><title>Sample-efficient Bayesian Optimisation Using Known Invariances</title><link>https://deep-diver.github.io/neurips2024/posters/rerls4opnm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rerls4opnm/</guid><description>Boost Bayesian Optimization&amp;rsquo;s efficiency by leveraging known invariances in objective functions for faster, more effective solutions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rerls4opnm/cover.png"/></item><item><title>Sample-Efficient Geometry Reconstruction from Euclidean Distances using Non-Convex Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/yu7h8zoui2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yu7h8zoui2/</guid><description>Reconstructing geometry from minimal Euclidean distance samples: A novel algorithm achieves state-of-the-art data efficiency with theoretical guarantees.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yu7h8zoui2/cover.png"/></item><item><title>Scaling Laws in Linear Regression: Compute, Parameters, and Data</title><link>https://deep-diver.github.io/neurips2024/posters/ph7sdeanxp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ph7sdeanxp/</guid><description>Deep learning&amp;rsquo;s neural scaling laws defy conventional wisdom; this paper uses infinite-dimensional linear regression to theoretically explain this phenomenon, showing that implicit regularization of S&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ph7sdeanxp/cover.png"/></item><item><title>Semi-Random Matrix Completion via Flow-Based Adaptive Reweighting</title><link>https://deep-diver.github.io/neurips2024/posters/xzp1up0hh2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xzp1up0hh2/</guid><description>New nearly-linear time algorithm achieves high-accuracy semi-random matrix completion, overcoming previous limitations on accuracy and noise tolerance.</description></item><item><title>Semidefinite Relaxations of the Gromov-Wasserstein Distance</title><link>https://deep-diver.github.io/neurips2024/posters/rm3ffh1mqk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rm3ffh1mqk/</guid><description>This paper introduces a novel, tractable semidefinite program (SDP) relaxation for the Gromov-Wasserstein distance, enabling the computation of globally optimal transportation plans.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rm3ffh1mqk/cover.png"/></item><item><title>Sequential Probability Assignment with Contexts: Minimax Regret, Contextual Shtarkov Sums, and Contextual Normalized Maximum Likelihood</title><link>https://deep-diver.github.io/neurips2024/posters/urntypkf3v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/urntypkf3v/</guid><description>This paper introduces contextual Shtarkov sums, a new complexity measure characterizing minimax regret in sequential probability assignment with contexts, and derives the minimax optimal algorithm, co&amp;hellip;</description></item><item><title>SGD vs GD: Rank Deficiency in Linear Networks</title><link>https://deep-diver.github.io/neurips2024/posters/tsaieshx3j/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tsaieshx3j/</guid><description>SGD surprisingly diminishes network rank, unlike GD, due to a repulsive force between eigenvalues, offering insights into deep learning generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tsaieshx3j/cover.png"/></item><item><title>Shaping the distribution of neural responses with interneurons in a recurrent circuit model</title><link>https://deep-diver.github.io/neurips2024/posters/ojlieq0j9t/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ojlieq0j9t/</guid><description>Researchers developed a recurrent neural circuit model that efficiently transforms sensory signals into neural representations by dynamically adjusting interneuron connectivity and activation function&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ojlieq0j9t/cover.png"/></item><item><title>Sharpness-Aware Minimization Activates the Interactive Teaching's Understanding and Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/prw98p1nv0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/prw98p1nv0/</guid><description>Sharpness Reduction Interactive Teaching (SRIT) boosts interactive teaching&amp;rsquo;s performance by integrating SAM&amp;rsquo;s generalization capabilities, leading to improved model accuracy and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/prw98p1nv0/cover.png"/></item><item><title>SkipPredict: When to Invest in Predictions for Scheduling</title><link>https://deep-diver.github.io/neurips2024/posters/kvuw8vzsqz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kvuw8vzsqz/</guid><description>SkipPredict optimizes scheduling by prioritizing cheap predictions and using expensive ones only when necessary, achieving cost-effective performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kvuw8vzsqz/cover.png"/></item><item><title>Small coresets via negative dependence: DPPs, linear statistics, and concentration</title><link>https://deep-diver.github.io/neurips2024/spotlight-optimization/jd3mshmttl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-optimization/jd3mshmttl/</guid><description>DPPs create smaller, more accurate coresets than existing methods, improving machine learning efficiency without sacrificing accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-optimization/jd3mshmttl/cover.png"/></item><item><title>Solving Inverse Problems via Diffusion Optimal Control</title><link>https://deep-diver.github.io/neurips2024/posters/wqlc4g1gn3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wqlc4g1gn3/</guid><description>Revolutionizing inverse problem solving, this paper introduces diffusion optimal control, a novel framework converting signal recovery into a discrete optimal control problem, surpassing limitations o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wqlc4g1gn3/cover.png"/></item><item><title>Statistical and Geometrical properties of the Kernel Kullback-Leibler divergence</title><link>https://deep-diver.github.io/neurips2024/posters/rxqoiekea2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rxqoiekea2/</guid><description>Regularized Kernel Kullback-Leibler divergence solves the original KKL&amp;rsquo;s disjoint support limitation, enabling comparison of any probability distributions with a closed-form solution and efficient gra&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rxqoiekea2/cover.png"/></item><item><title>Statistical Estimation in the Spiked Tensor Model via the Quantum Approximate Optimization Algorithm</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/wtlvxdzhmp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/wtlvxdzhmp/</guid><description>Quantum Approximate Optimization Algorithm (QAOA) achieves weak recovery in spiked tensor models matching classical methods, but with potential constant factor advantages for certain parameters.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/wtlvxdzhmp/cover.png"/></item><item><title>Statistical-Computational Trade-offs for Density Estimation</title><link>https://deep-diver.github.io/neurips2024/posters/ptd4azpzcr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ptd4azpzcr/</guid><description>Density estimation algorithms face inherent trade-offs: reducing sample needs often increases query time. This paper proves these trade-offs are fundamental, showing limits to how much improvement is&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ptd4azpzcr/cover.png"/></item><item><title>Stochastic Optimal Control and Estimation with Multiplicative and Internal Noise</title><link>https://deep-diver.github.io/neurips2024/posters/mzhbkbywtp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mzhbkbywtp/</guid><description>A novel algorithm significantly improves stochastic optimal control by accurately modeling sensorimotor noise, achieving substantially lower costs than current state-of-the-art solutions, particularly&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mzhbkbywtp/cover.png"/></item><item><title>Stochastic Optimal Control Matching</title><link>https://deep-diver.github.io/neurips2024/posters/wfu2cdgmwt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wfu2cdgmwt/</guid><description>Stochastic Optimal Control Matching (SOCM) significantly reduces errors in stochastic optimal control by learning a matching vector field using a novel iterative diffusion optimization technique.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wfu2cdgmwt/cover.png"/></item><item><title>Stochastic Zeroth-Order Optimization under Strongly Convexity and Lipschitz Hessian: Minimax Sample Complexity</title><link>https://deep-diver.github.io/neurips2024/posters/jtyjwrplz5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jtyjwrplz5/</guid><description>Stochastic zeroth-order optimization of strongly convex functions with Lipschitz Hessian achieves optimal sample complexity, as proven by matching upper and lower bounds with a novel two-stage algorit&amp;hellip;</description></item><item><title>Stopping Bayesian Optimization with Probabilistic Regret Bounds</title><link>https://deep-diver.github.io/neurips2024/posters/cm2gu9xgti/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cm2gu9xgti/</guid><description>This paper presents a novel probabilistic regret bound (PRB) framework for Bayesian optimization, replacing the traditional fixed-budget stopping rule with a criterion based on the probability of find&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cm2gu9xgti/cover.png"/></item><item><title>Strategic Linear Contextual Bandits</title><link>https://deep-diver.github.io/neurips2024/posters/apphmfe63y/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/apphmfe63y/</guid><description>Strategic agents gaming recommender systems is solved by a novel mechanism that incentivizes truthful behavior while minimizing regret, offering a solution to a key challenge in online learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/apphmfe63y/cover.png"/></item><item><title>SymILO: A Symmetry-Aware Learning Framework for Integer Linear Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/rtmytziw6l/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rtmytziw6l/</guid><description>SymILO: A novel symmetry-aware learning framework dramatically improves integer linear program (ILP) solutions by addressing data variability caused by ILP symmetry.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rtmytziw6l/cover.png"/></item><item><title>Symmetries in Overparametrized Neural Networks: A Mean Field View</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/l86glqncuj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/l86glqncuj/</guid><description>Overparametrized neural networks&amp;rsquo; learning dynamics are analyzed under data symmetries using mean-field theory, revealing that data augmentation, feature averaging, and equivariant architectures asymp&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/l86glqncuj/cover.png"/></item><item><title>The Bayesian sampling in a canonical recurrent circuit with a diversity of inhibitory interneurons</title><link>https://deep-diver.github.io/neurips2024/posters/vnmi0fhn6z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vnmi0fhn6z/</guid><description>Diverse inhibitory neurons in brain circuits enable faster Bayesian computation via Hamiltonian sampling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vnmi0fhn6z/cover.png"/></item><item><title>The Empirical Impact of Neural Parameter Symmetries, or Lack Thereof</title><link>https://deep-diver.github.io/neurips2024/posters/pcvxyw6fkg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pcvxyw6fkg/</guid><description>Breaking neural network parameter symmetries leads to faster training, better generalization, and improved loss landscape behavior, as demonstrated by novel asymmetric network architectures.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pcvxyw6fkg/cover.png"/></item><item><title>The Implicit Bias of Adam on Separable Data</title><link>https://deep-diver.github.io/neurips2024/posters/xrqxan3wkm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xrqxan3wkm/</guid><description>Adam&amp;rsquo;s implicit bias revealed: On separable data, Adam converges towards the maximum lâˆž-margin solution, a finding contrasting with gradient descent&amp;rsquo;s l2-margin preference. This polynomial-time conver&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xrqxan3wkm/cover.png"/></item><item><title>The Implicit Bias of Heterogeneity towards Invariance: A Study of Multi-Environment Matrix Sensing</title><link>https://deep-diver.github.io/neurips2024/posters/pmpbxmf8t3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pmpbxmf8t3/</guid><description>Leveraging data heterogeneity, this study reveals that standard SGD implicitly learns invariant features across multiple environments, achieving robust generalization without explicit regularization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pmpbxmf8t3/cover.png"/></item><item><title>The Many Faces of Optimal Weak-to-Strong Learning</title><link>https://deep-diver.github.io/neurips2024/posters/z7h7zmgypj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z7h7zmgypj/</guid><description>A new, surprisingly simple boosting algorithm achieves provably optimal sample complexity and outperforms existing algorithms on large datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/z7h7zmgypj/cover.png"/></item><item><title>The Minimax Rate of HSIC Estimation for Translation-Invariant Kernels</title><link>https://deep-diver.github.io/neurips2024/posters/kyno0n1bj9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kyno0n1bj9/</guid><description>Researchers found the minimax optimal rate of HSIC estimation for translation-invariant kernels is O(nâ»Â¹/Â²), settling a two-decade-old open question and validating many existing HSIC estimators.</description></item><item><title>The Reliability of OKRidge Method in Solving Sparse Ridge Regression Problems</title><link>https://deep-diver.github.io/neurips2024/posters/r3ruv1gf8r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/r3ruv1gf8r/</guid><description>OKRidge&amp;rsquo;s reliability for solving sparse ridge regression problems is rigorously proven through theoretical error analysis, enhancing its applicability in machine learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/r3ruv1gf8r/cover.png"/></item><item><title>The Road Less Scheduled</title><link>https://deep-diver.github.io/neurips2024/oral-others/0xenkkenui/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-others/0xenkkenui/</guid><description>Revolutionizing machine learning, Schedule-Free optimization achieves state-of-the-art results without needing learning rate schedules, simplifying training and improving efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-others/0xenkkenui/cover.png"/></item><item><title>The Secretary Problem with Predicted Additive Gap</title><link>https://deep-diver.github.io/neurips2024/posters/lbuxdzg1pd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lbuxdzg1pd/</guid><description>Beat the 1/e barrier in the secretary problem using only an additive gap prediction!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lbuxdzg1pd/cover.png"/></item><item><title>The Space Complexity of Approximating Logistic Loss</title><link>https://deep-diver.github.io/neurips2024/posters/vdlj3vee9a/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vdlj3vee9a/</guid><description>This paper proves fundamental space complexity lower bounds for approximating logistic loss, revealing that existing coreset constructions are surprisingly optimal.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vdlj3vee9a/cover.png"/></item><item><title>Theoretical Characterisation of the Gauss Newton Conditioning in Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/fponumjlio/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fponumjlio/</guid><description>New theoretical bounds reveal how neural network architecture impacts the Gauss-Newton matrix&amp;rsquo;s conditioning, paving the way for improved optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fponumjlio/cover.png"/></item><item><title>Toward Global Convergence of Gradient EM for Over-Paramterized Gaussian Mixture Models</title><link>https://deep-diver.github.io/neurips2024/posters/zv9gyc3xgf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zv9gyc3xgf/</guid><description>Gradient EM for over-parameterized Gaussian Mixture Models globally converges with a sublinear rate, solving a longstanding open problem in machine learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zv9gyc3xgf/cover.png"/></item><item><title>Trace is the Next AutoDiff: Generative Optimization with Rich Feedback, Execution Traces, and LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/rys2dmn9td/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rys2dmn9td/</guid><description>Trace: Automating AI workflow design with LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rys2dmn9td/cover.png"/></item><item><title>Trade-Offs of Diagonal Fisher Information Matrix Estimators</title><link>https://deep-diver.github.io/neurips2024/posters/tvbckaqod8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tvbckaqod8/</guid><description>This paper examines the trade-offs between two popular diagonal Fisher Information Matrix (FIM) estimators in neural networks, deriving variance bounds and highlighting the importance of considering e&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tvbckaqod8/cover.png"/></item><item><title>Trading off Consistency and Dimensionality of Convex Surrogates for Multiclass Classification</title><link>https://deep-diver.github.io/neurips2024/posters/xcibvuxwpm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xcibvuxwpm/</guid><description>Researchers achieve a balance between accuracy and efficiency in multiclass classification by introducing partially consistent surrogate losses and novel methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xcibvuxwpm/cover.png"/></item><item><title>Transductive Learning is Compact</title><link>https://deep-diver.github.io/neurips2024/posters/ywtpmlktmj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ywtpmlktmj/</guid><description>Supervised learning&amp;rsquo;s sample complexity is compact: a hypothesis class is learnable if and only if all its finite projections are learnable, simplifying complexity analysis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ywtpmlktmj/cover.png"/></item><item><title>Truthfulness of Calibration Measures</title><link>https://deep-diver.github.io/neurips2024/posters/cda8hftygc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cda8hftygc/</guid><description>Researchers developed Subsampled Smooth Calibration Error (SSCE), a new truthful calibration measure for sequential prediction, solving the problem of existing measures being easily gamed.</description></item><item><title>Understanding the Gains from Repeated Self-Distillation</title><link>https://deep-diver.github.io/neurips2024/posters/gmqakjcocb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gmqakjcocb/</guid><description>Repeated self-distillation significantly reduces excess risk in linear regression, achieving up to a &amp;rsquo;d&amp;rsquo; factor improvement over single-step methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gmqakjcocb/cover.png"/></item><item><title>Universal Online Convex Optimization with $1$ Projection per Round</title><link>https://deep-diver.github.io/neurips2024/posters/xnncvkbwws/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xnncvkbwws/</guid><description>This paper introduces a novel universal online convex optimization algorithm needing only one projection per round, achieving optimal regret bounds for various function types, including general convex&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xnncvkbwws/cover.png"/></item><item><title>Universality of AdaGrad Stepsizes for Stochastic Optimization: Inexact Oracle, Acceleration and Variance Reduction</title><link>https://deep-diver.github.io/neurips2024/posters/rniiavjhi5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rniiavjhi5/</guid><description>Adaptive gradient methods using AdaGrad stepsizes achieve optimal convergence rates for convex composite optimization problems, handling inexact oracles, acceleration, and variance reduction without n&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rniiavjhi5/cover.png"/></item><item><title>Unraveling the Gradient Descent Dynamics of Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/xswqeljjo5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xswqeljjo5/</guid><description>This paper reveals how large embedding dimensions and appropriate initialization guarantee convergence in Transformer training, highlighting Gaussian attention&amp;rsquo;s superior landscape over Softmax.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xswqeljjo5/cover.png"/></item><item><title>Unveiling User Satisfaction and Creator Productivity Trade-Offs in Recommendation Platforms</title><link>https://deep-diver.github.io/neurips2024/posters/igan7rldcf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/igan7rldcf/</guid><description>Recommendation algorithms on UGC platforms face a critical trade-off: prioritizing user satisfaction reduces creator engagement, jeopardizing long-term content diversity. This research introduces a ga&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/igan7rldcf/cover.png"/></item><item><title>User-Creator Feature Polarization in Recommender Systems with Dual Influence</title><link>https://deep-diver.github.io/neurips2024/posters/ywq89o19wf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ywq89o19wf/</guid><description>Recommender systems, when influenced by both users and creators, inevitably polarize; however, prioritizing efficiency through methods like top-k truncation can surprisingly enhance diversity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ywq89o19wf/cover.png"/></item><item><title>Validating Climate Models with Spherical Convolutional Wasserstein Distance</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/mmsffib6pi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/mmsffib6pi/</guid><description>Researchers developed Spherical Convolutional Wasserstein Distance (SCWD) to more accurately validate climate models by considering spatial variability and local distributional differences.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/mmsffib6pi/cover.png"/></item><item><title>Variance estimation in compound decision theory under boundedness</title><link>https://deep-diver.github.io/neurips2024/posters/hvcppndykt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hvcppndykt/</guid><description>Unlocking the optimal variance estimation rate in compound decision theory under bounded means, this paper reveals a surprising (log log n/log n)Â² rate and introduces a rate-optimal cumulant-based est&amp;hellip;</description></item><item><title>Warm-starting Push-Relabel</title><link>https://deep-diver.github.io/neurips2024/posters/yyy5lze547/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yyy5lze547/</guid><description>This research introduces the first theoretical guarantees for warm-starting the celebrated Push-Relabel network flow algorithm, improving its speed using a predicted flow, while maintaining worst-case&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yyy5lze547/cover.png"/></item><item><title>What type of inference is planning?</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/txsrgrzicz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/txsrgrzicz/</guid><description>Planning is redefined as a distinct inference type within a variational framework, enabling efficient approximate planning in complex environments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/txsrgrzicz/cover.png"/></item><item><title>When Is Inductive Inference Possible?</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/2agcshccuv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/2agcshccuv/</guid><description>This paper provides a tight characterization of inductive inference, proving it&amp;rsquo;s possible if and only if the hypothesis class is a countable union of online learnable classes, resolving a long-standi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/2agcshccuv/cover.png"/></item><item><title>Why Do We Need Weight Decay in Modern Deep Learning?</title><link>https://deep-diver.github.io/neurips2024/posters/yraxxsckm2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yraxxsckm2/</guid><description>Weight decay&amp;rsquo;s role in modern deep learning is surprisingly multifaceted, impacting optimization dynamics rather than solely regularization, improving generalization and training stability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yraxxsckm2/cover.png"/></item><item><title>Why the Metric Backbone Preserves Community Structure</title><link>https://deep-diver.github.io/neurips2024/posters/kx8i0rp7w2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kx8i0rp7w2/</guid><description>Metric backbone graph sparsification surprisingly preserves community structure, offering an efficient and robust method for analyzing large networks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kx8i0rp7w2/cover.png"/></item><item><title>Why Transformers Need Adam: A Hessian Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/x6rqepbnj3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x6rqepbnj3/</guid><description>Adam&amp;rsquo;s superiority over SGD in Transformer training is explained by the &amp;lsquo;block heterogeneity&amp;rsquo; of the Hessian matrix, highlighting the need for adaptive learning rates.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x6rqepbnj3/cover.png"/></item></channel></rss>