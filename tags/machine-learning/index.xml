<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine Learning on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/machine-learning/</link><description>Recent content in Machine Learning on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>A Nearly Optimal and Low-Switching Algorithm for Reinforcement Learning with General Function Approximation</title><link>https://deep-diver.github.io/neurips2024/posters/s3iczc2nlq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s3iczc2nlq/</guid><description>MQL-UCB: Near-optimal reinforcement learning with low policy switching cost, solving the exploration-exploitation dilemma for complex models.</description></item><item><title>Adaptive Variance Reduction for Stochastic Optimization under Weaker Assumptions</title><link>https://deep-diver.github.io/neurips2024/posters/tmqh8prqlc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tmqh8prqlc/</guid><description>Adaptive STORM achieves optimal convergence rates for stochastic optimization of non-convex functions under weaker assumptions, eliminating the need for bounded gradients or function values and removi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tmqh8prqlc/cover.png"/></item><item><title>Advancing Training Efficiency of Deep Spiking Neural Networks through Rate-based Backpropagation</title><link>https://deep-diver.github.io/neurips2024/posters/wlcm21c4nk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wlcm21c4nk/</guid><description>Rate-based backpropagation boosts deep spiking neural network training efficiency by leveraging rate coding, achieving comparable performance to BPTT with reduced complexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wlcm21c4nk/cover.png"/></item><item><title>Amortized Bayesian Experimental Design for Decision-Making</title><link>https://deep-diver.github.io/neurips2024/posters/zbg7wogavm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zbg7wogavm/</guid><description>Amortized Decision-Aware BED prioritizes maximizing downstream decision utility by instantly proposing informative experimental designs and inferring decisions, facilitating accurate decision-making.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zbg7wogavm/cover.png"/></item><item><title>An Accelerated Algorithm for Stochastic Bilevel Optimization under Unbounded Smoothness</title><link>https://deep-diver.github.io/neurips2024/posters/v7vyvvmfru/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v7vyvvmfru/</guid><description>AccBO: A new accelerated algorithm achieves O(ε⁻³) oracle complexity for stochastic bilevel optimization with unbounded smoothness, significantly improving upon existing O(ε⁻⁴) methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v7vyvvmfru/cover.png"/></item><item><title>An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning</title><link>https://deep-diver.github.io/neurips2024/posters/dqdffx3bs5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dqdffx3bs5/</guid><description>Mecoin: a novel memory module for efficient graph few-shot class-incremental learning, tackles catastrophic forgetting by employing structured memory units and a memory representation adaptation modul&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dqdffx3bs5/cover.png"/></item><item><title>An exactly solvable model for emergence and scaling laws in the multitask sparse parity problem</title><link>https://deep-diver.github.io/neurips2024/posters/cuwsr25bbi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cuwsr25bbi/</guid><description>A novel multilinear model analytically explains the emergence and scaling laws of skills in the multitask sparse parity problem, accurately predicting skill emergence in neural networks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cuwsr25bbi/cover.png"/></item><item><title>Approximately Equivariant Neural Processes</title><link>https://deep-diver.github.io/neurips2024/posters/dqt9mc5nql/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dqt9mc5nql/</guid><description>Boosting meta-learning, this paper introduces a novel, flexible approach to create approximately equivariant neural processes that outperform both non-equivariant and strictly equivariant counterparts&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dqt9mc5nql/cover.png"/></item><item><title>Artificial Generational Intelligence: Cultural Accumulation in Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/pmacrgu8gv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pmacrgu8gv/</guid><description>Reinforcement learning agents achieve emergent cultural accumulation by balancing social and independent learning, outperforming single-lifetime agents.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pmacrgu8gv/cover.png"/></item><item><title>Avoiding Undesired Future with Minimal Cost in Non-Stationary Environments</title><link>https://deep-diver.github.io/neurips2024/posters/yhd2khhntb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yhd2khhntb/</guid><description>AUF-MICNS: A novel sequential method efficiently solves the avoiding undesired future problem by dynamically updating influence relations in non-stationary environments while minimizing action costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yhd2khhntb/cover.png"/></item><item><title>Bandits with Preference Feedback: A Stackelberg Game Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/wie991zhxh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wie991zhxh/</guid><description>MAXMINLCB, a novel game-theoretic algorithm, efficiently solves bandit problems with preference feedback over continuous domains, providing anytime-valid, rate-optimal regret guarantees.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wie991zhxh/cover.png"/></item><item><title>Beyond Redundancy: Information-aware Unsupervised Multiplex Graph Structure Learning</title><link>https://deep-diver.github.io/neurips2024/posters/xaqpakjnas/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xaqpakjnas/</guid><description>InfoMGF, a novel framework, tackles the limitations of unsupervised multiplex graph learning by refining graph structures, maximizing task-relevant information (both shared and unique), and achieving &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xaqpakjnas/cover.png"/></item><item><title>Breaking the curse of dimensionality in structured density estimation</title><link>https://deep-diver.github.io/neurips2024/posters/dwwin2ugye/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dwwin2ugye/</guid><description>Researchers break the curse of dimensionality in structured density estimation using graph resilience, a novel graphical parameter that effectively reduces the sample complexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dwwin2ugye/cover.png"/></item><item><title>Bridging Geometric States via Geometric Diffusion Bridge</title><link>https://deep-diver.github.io/neurips2024/posters/zcepob9rcr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zcepob9rcr/</guid><description>Geometric Diffusion Bridge (GDB) accurately predicts geometric state evolution in complex systems by leveraging a probabilistic approach and equivariant diffusion processes, surpassing existing deep l&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zcepob9rcr/cover.png"/></item><item><title>Bridging Model-Based Optimization and Generative Modeling via Conservative Fine-Tuning of Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/posters/zir2qju4hl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zir2qju4hl/</guid><description>BRAID: A novel, conservative fine-tuning method surpasses offline design optimization by cleverly combining generative diffusion models with reward models, preventing over-optimization and generating &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zir2qju4hl/cover.png"/></item><item><title>Bridging OOD Detection and Generalization: A Graph-Theoretic View</title><link>https://deep-diver.github.io/neurips2024/posters/qzwag8qxi1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qzwag8qxi1/</guid><description>A novel graph-theoretic framework bridges OOD detection &amp;amp; generalization, offering theoretical error bounds and competitive empirical performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qzwag8qxi1/cover.png"/></item><item><title>Carrot and Stick: Eliciting Comparison Data and Beyond</title><link>https://deep-diver.github.io/neurips2024/posters/ofjtu2ktxo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ofjtu2ktxo/</guid><description>Truthful comparison data is hard to obtain without ground truth. This paper presents novel peer prediction mechanisms using bonus-penalty payments that incentivize truthful comparisons, even in networ&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ofjtu2ktxo/cover.png"/></item><item><title>Causal Contrastive Learning for Counterfactual Regression Over Time</title><link>https://deep-diver.github.io/neurips2024/posters/bkozybje4z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bkozybje4z/</guid><description>Causal CPC: a novel method for accurate and efficient counterfactual regression over time using RNNs, CPC, and InfoMax, achieving state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bkozybje4z/cover.png"/></item><item><title>Changing the Training Data Distribution to Reduce Simplicity Bias Improves In-distribution Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/yyspldusu2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yyspldusu2/</guid><description>Boosting in-distribution generalization is achieved by strategically altering the training data distribution to reduce simplicity bias and promote uniform feature learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yyspldusu2/cover.png"/></item><item><title>Combining Statistical Depth and Fermat Distance for Uncertainty Quantification</title><link>https://deep-diver.github.io/neurips2024/posters/xexrhtumcf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xexrhtumcf/</guid><description>Boosting neural network prediction reliability, this research ingeniously combines statistical depth and Fermat distance for superior uncertainty quantification, eliminating the need for distributiona&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xexrhtumcf/cover.png"/></item><item><title>Communication Efficient Distributed Training with Distributed Lion</title><link>https://deep-diver.github.io/neurips2024/posters/wdircetioz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wdircetioz/</guid><description>Distributed Lion: Training large AI models efficiently by communicating only binary or low-precision vectors between workers and a server, significantly reducing communication costs and maintaining co&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wdircetioz/cover.png"/></item><item><title>Communication-Efficient Federated Group Distributionally Robust Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/xnzejfe0mh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xnzejfe0mh/</guid><description>Communication-efficient algorithms for federated group distributionally robust optimization (FGDRO) are introduced, achieving lower communication complexity and superior performance on real-world task&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xnzejfe0mh/cover.png"/></item><item><title>Confidence Calibration of Classifiers with Many Classes</title><link>https://deep-diver.github.io/neurips2024/posters/ebbnkvxmcz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ebbnkvxmcz/</guid><description>Boost multi-class classifier calibration by cleverly transforming the problem into a single binary calibration task!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ebbnkvxmcz/cover.png"/></item><item><title>Conformalized Credal Set Predictors</title><link>https://deep-diver.github.io/neurips2024/posters/vbah12uvbd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vbah12uvbd/</guid><description>Conformal prediction empowers robust credal set predictions, handling aleatoric and epistemic uncertainties in classification, guaranteed to be valid with high probability!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vbah12uvbd/cover.png"/></item><item><title>Constrained Latent Action Policies for Model-Based Offline Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/pehvscmsgg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pehvscmsgg/</guid><description>Constrained Latent Action Policies (C-LAP) revolutionizes offline reinforcement learning by jointly modeling state-action distributions, implicitly constraining policies to improve efficiency and redu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pehvscmsgg/cover.png"/></item><item><title>Context-Aware Testing: A New Paradigm for Model Testing with Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/d75qczb7tx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/d75qczb7tx/</guid><description>Context-Aware Testing (CAT) revolutionizes ML model testing by using contextual information to identify relevant failures, surpassing traditional data-only methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/d75qczb7tx/cover.png"/></item><item><title>Controlled maximal variability along with reliable performance in recurrent neural networks</title><link>https://deep-diver.github.io/neurips2024/posters/yxw2dctqdi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yxw2dctqdi/</guid><description>NeuroMOP, a novel neural principle, maximizes neural variability while ensuring reliable performance in recurrent neural networks, offering new insights into brain function and artificial intelligence&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yxw2dctqdi/cover.png"/></item><item><title>Controlling Continuous Relaxation for Combinatorial Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/ykacv1ihjd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ykacv1ihjd/</guid><description>Continuous Relaxation Annealing (CRA) significantly boosts unsupervised learning-based solvers for combinatorial optimization by dynamically shifting from continuous to discrete solutions, eliminating&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ykacv1ihjd/cover.png"/></item><item><title>ControlSynth Neural ODEs: Modeling Dynamical Systems with Guaranteed Convergence</title><link>https://deep-diver.github.io/neurips2024/posters/dbe8khdmfs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dbe8khdmfs/</guid><description>ControlSynth Neural ODEs (CSODEs) guarantee convergence in complex dynamical systems via tractable linear inequalities, improving neural ODE modeling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dbe8khdmfs/cover.png"/></item><item><title>Convolutions and More as Einsum: A Tensor Network Perspective with Advances for Second-Order Methods</title><link>https://deep-diver.github.io/neurips2024/posters/cds8wxnmvp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cds8wxnmvp/</guid><description>This paper accelerates second-order optimization in CNNs by 4.5x, using a novel tensor network representation that simplifies convolutions and reduces memory overhead.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cds8wxnmvp/cover.png"/></item><item><title>DiffusionPDE: Generative PDE-Solving under Partial Observation</title><link>https://deep-diver.github.io/neurips2024/posters/z0i2sbjn0r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z0i2sbjn0r/</guid><description>DiffusionPDE uses generative diffusion models to solve PDEs accurately, even with highly incomplete observations, outperforming state-of-the-art methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/z0i2sbjn0r/cover.png"/></item><item><title>Disentangled Unsupervised Skill Discovery for Efficient Hierarchical Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/epobcwfnfc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/epobcwfnfc/</guid><description>DUSDi: A novel method for learning disentangled skills in unsupervised reinforcement learning, enabling efficient reuse for diverse downstream tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/epobcwfnfc/cover.png"/></item><item><title>Distributed Least Squares in Small Space via Sketching and Bias Reduction</title><link>https://deep-diver.github.io/neurips2024/posters/rkuvyost2c/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rkuvyost2c/</guid><description>Researchers developed a novel sparse sketching method for distributed least squares regression, achieving near-unbiased estimates with optimal space and time complexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rkuvyost2c/cover.png"/></item><item><title>Dynamic Conditional Optimal Transport through Simulation-Free Flows</title><link>https://deep-diver.github.io/neurips2024/posters/tk0uarynhh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tk0uarynhh/</guid><description>Simulation-free flow generates conditional distributions via dynamic conditional optimal transport.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tk0uarynhh/cover.png"/></item><item><title>Dynamic Model Predictive Shielding for Provably Safe Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/x2zy4hzcmg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x2zy4hzcmg/</guid><description>Dynamic Model Predictive Shielding (DMPS) ensures provably safe reinforcement learning by dynamically optimizing reinforcement learning objectives while maintaining provable safety, achieving higher r&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x2zy4hzcmg/cover.png"/></item><item><title>Dynamics of Supervised and Reinforcement Learning in the Non-Linear Perceptron</title><link>https://deep-diver.github.io/neurips2024/posters/doajtihgiz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/doajtihgiz/</guid><description>Researchers developed a novel stochastic-process approach to precisely analyze learning dynamics in nonlinear perceptrons, revealing how input noise and learning rules significantly impact learning sp&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/doajtihgiz/cover.png"/></item><item><title>Efficient Discrepancy Testing for Learning with Distribution Shift</title><link>https://deep-diver.github.io/neurips2024/posters/ojihvhqbaq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ojihvhqbaq/</guid><description>Provably efficient algorithms for learning with distribution shift are introduced, generalizing and improving prior work by achieving near-optimal error rates and offering universal learners for large&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ojihvhqbaq/cover.png"/></item><item><title>Efficient Recurrent Off-Policy RL Requires a Context-Encoder-Specific Learning Rate</title><link>https://deep-diver.github.io/neurips2024/posters/tswot8ttko/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tswot8ttko/</guid><description>Recurrent off-policy RL, while robust, suffers from training instability. RESEL, a novel algorithm, solves this by using a context-encoder-specific learning rate, significantly improving stability an&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tswot8ttko/cover.png"/></item><item><title>Efficient Sign-Based Optimization: Accelerating Convergence via Variance Reduction</title><link>https://deep-diver.github.io/neurips2024/posters/uanzvf1vfe/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uanzvf1vfe/</guid><description>Sign-based optimization gets a speed boost! This paper introduces new algorithms that significantly accelerate convergence in distributed optimization by cleverly using variance reduction and enhanced&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uanzvf1vfe/cover.png"/></item><item><title>Enhancing Diversity in Bayesian Deep Learning via Hyperspherical Energy Minimization of CKA</title><link>https://deep-diver.github.io/neurips2024/posters/s2ha6bz3le/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s2ha6bz3le/</guid><description>Boosting Bayesian deep learning&amp;rsquo;s diversity and uncertainty quantification, this study proposes hyperspherical energy minimization of CKA to generate diverse and reliable neural network ensembles and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/s2ha6bz3le/cover.png"/></item><item><title>Enhancing Efficiency of Safe Reinforcement Learning via Sample Manipulation</title><link>https://deep-diver.github.io/neurips2024/posters/opfjhl6dpr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/opfjhl6dpr/</guid><description>ESPO enhances safe RL efficiency by dynamically manipulating sample size based on reward-safety gradient conflicts, ensuring faster training and superior performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/opfjhl6dpr/cover.png"/></item><item><title>Enhancing Semi-Supervised Learning via Representative and Diverse Sample Selection</title><link>https://deep-diver.github.io/neurips2024/posters/xrdpcodghl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xrdpcodghl/</guid><description>RDSS: a novel sample selection method for semi-supervised learning, boosts model accuracy by minimizing a-MMD, striking a balance between sample representativeness and diversity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xrdpcodghl/cover.png"/></item><item><title>Episodic Future Thinking Mechanism for Multi-agent Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/rl7otnsd9a/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rl7otnsd9a/</guid><description>RL agents make better decisions by simulating future scenarios, considering diverse agent behaviors, and using character inference for improved decision-making.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rl7otnsd9a/cover.png"/></item><item><title>Equivariant Machine Learning on Graphs with Nonlinear Spectral Filters</title><link>https://deep-diver.github.io/neurips2024/posters/y8p633e5hq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y8p633e5hq/</guid><description>Nonlinear spectral filters (NLSFs) enable fully equivariant graph neural networks, improving accuracy and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y8p633e5hq/cover.png"/></item><item><title>Excluding the Irrelevant: Focusing Reinforcement Learning through Continuous Action Masking</title><link>https://deep-diver.github.io/neurips2024/posters/yvzwlfhprw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yvzwlfhprw/</guid><description>Boost RL efficiency in continuous action spaces by masking irrelevant actions using three novel continuous action masking methods!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yvzwlfhprw/cover.png"/></item><item><title>Exploring the Precise Dynamics of Single-Layer GAN Models: Leveraging Multi-Feature Discriminators for High-Dimensional Subspace Learning</title><link>https://deep-diver.github.io/neurips2024/posters/sohfyffnxt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sohfyffnxt/</guid><description>Single-layer GANs learn data subspaces more effectively using multi-feature discriminators, enabling faster training and better feature representation than conventional methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sohfyffnxt/cover.png"/></item><item><title>Fast yet Safe: Early-Exiting with Risk Control</title><link>https://deep-diver.github.io/neurips2024/posters/bbfjpasrgs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bbfjpasrgs/</guid><description>Risk control boosts early-exit neural networks&amp;rsquo; speed and safety by ensuring accurate predictions before exiting early, achieving substantial computational savings across diverse tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bbfjpasrgs/cover.png"/></item><item><title>Federated Ensemble-Directed Offline Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/ypaqe8uwsc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ypaqe8uwsc/</guid><description>FEDORA, a novel algorithm, enables high-quality policy learning in federated offline reinforcement learning by leveraging the collective wisdom of diverse client datasets without data sharing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ypaqe8uwsc/cover.png"/></item><item><title>FedNE: Surrogate-Assisted Federated Neighbor Embedding for Dimensionality Reduction</title><link>https://deep-diver.github.io/neurips2024/posters/zbmkodngkx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zbmkodngkx/</guid><description>FEDNE: a novel approach enabling collaborative dimensionality reduction of distributed data in federated learning without data sharing, achieved via surrogate loss functions and data augmentation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zbmkodngkx/cover.png"/></item><item><title>Forgetting, Ignorance or Myopia: Revisiting Key Challenges in Online Continual Learning</title><link>https://deep-diver.github.io/neurips2024/posters/oparhdvqrd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oparhdvqrd/</guid><description>NsCE framework tackles key OCL challenges: model ignorance (learning effective features in limited time) and myopia (overly simplified features). NsCE integrates non-sparse maximum separation regulari&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oparhdvqrd/cover.png"/></item><item><title>Gated Inference Network: Inference and Learning State-Space Models</title><link>https://deep-diver.github.io/neurips2024/posters/z4duw3kzld/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z4duw3kzld/</guid><description>GIN, a novel approximate Bayesian inference algorithm, efficiently handles nonlinear state-space models with high-dimensional, noisy observations by disentangling observation and dynamics. Achieving l&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/z4duw3kzld/cover.png"/></item><item><title>Generative Forests</title><link>https://deep-diver.github.io/neurips2024/posters/crlqhncjwt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/crlqhncjwt/</guid><description>Generative Forests (GFs) revolutionize tabular data generation with a novel forest-based model and a simple boosting algorithm offering strong convergence guarantees, significantly outperforming curre&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/crlqhncjwt/cover.png"/></item><item><title>Generative Modeling of Molecular Dynamics Trajectories</title><link>https://deep-diver.github.io/neurips2024/posters/yrrch1osgw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yrrch1osgw/</guid><description>MDGEN: Generative modeling unlocks MD data for diverse tasks, achieving significant speedups via flexible multi-task surrogate models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yrrch1osgw/cover.png"/></item><item><title>Generative Semi-supervised Graph Anomaly Detection</title><link>https://deep-diver.github.io/neurips2024/posters/zqlamwvlkt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zqlamwvlkt/</guid><description>GGAD: Generative Semi-supervised Graph Anomaly Detection significantly outperforms existing methods by using a novel approach to generate pseudo-anomaly nodes for training, leveraging asymmetric local&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zqlamwvlkt/cover.png"/></item><item><title>Graph Coarsening with Message-Passing Guarantees</title><link>https://deep-diver.github.io/neurips2024/posters/riotceonc8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/riotceonc8/</guid><description>This paper introduces a new message-passing operation for coarsened graphs with theoretical guarantees, improving GNN efficiency and accuracy on large datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/riotceonc8/cover.png"/></item><item><title>Graph Edit Distance with General Costs Using Neural Set Divergence</title><link>https://deep-diver.github.io/neurips2024/posters/u7jrmrgutt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/u7jrmrgutt/</guid><description>GRAPHEDX, a novel neural network, accurately estimates graph edit distance with varying operation costs, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/u7jrmrgutt/cover.png"/></item><item><title>Graph Neural Flows for Unveiling Systemic Interactions Among Irregularly Sampled Time Series</title><link>https://deep-diver.github.io/neurips2024/posters/tfb5ssabvb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tfb5ssabvb/</guid><description>GNeuralFlow unveils systemic interactions in irregularly sampled time series by learning a directed acyclic graph representing conditional dependencies, achieving superior performance in classificatio&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tfb5ssabvb/cover.png"/></item><item><title>Graph Neural Networks Do Not Always Oversmooth</title><link>https://deep-diver.github.io/neurips2024/posters/ny7fgtsspu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ny7fgtsspu/</guid><description>Deep graph neural networks often suffer from oversmoothing; this paper reveals a non-oversmoothing phase controllable by weight variance, enabling deep, expressive models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ny7fgtsspu/cover.png"/></item><item><title>GraphCroc: Cross-Correlation Autoencoder for Graph Structural Reconstruction</title><link>https://deep-diver.github.io/neurips2024/posters/zn6s6vqyb0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zn6s6vqyb0/</guid><description>GraphCroc, a novel graph autoencoder, leverages cross-correlation to accurately reconstruct complex graph structures, outperforming self-correlation-based methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zn6s6vqyb0/cover.png"/></item><item><title>Guiding Neural Collapse: Optimising Towards the Nearest Simplex Equiangular Tight Frame</title><link>https://deep-diver.github.io/neurips2024/posters/z4fapuslma/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z4fapuslma/</guid><description>Researchers devised a novel method to accelerate neural network training by guiding the optimization process toward a Simplex Equiangular Tight Frame, exploiting the Neural Collapse phenomenon to enha&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/z4fapuslma/cover.png"/></item><item><title>Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects Models</title><link>https://deep-diver.github.io/neurips2024/posters/uxuobobjho/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uxuobobjho/</guid><description>Accelerate Bayesian inference in linear mixed-effects models by efficiently marginalizing random effects using fast linear algebra, enabling faster and more accurate posterior estimations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uxuobobjho/cover.png"/></item><item><title>How Does Message Passing Improve Collaborative Filtering?</title><link>https://deep-diver.github.io/neurips2024/posters/c78u5zi4ea/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/c78u5zi4ea/</guid><description>TAG-CF boosts collaborative filtering accuracy by up to 39.2% on cold users, using only a single message-passing step at test time, avoiding costly training-time computations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/c78u5zi4ea/cover.png"/></item><item><title>Identifiable Object-Centric Representation Learning via Probabilistic Slot Attention</title><link>https://deep-diver.github.io/neurips2024/posters/qmovqbwmcy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qmovqbwmcy/</guid><description>Probabilistic Slot Attention achieves identifiable object-centric representations without supervision, advancing systematic generalization in machine learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qmovqbwmcy/cover.png"/></item><item><title>Identify Then Recommend: Towards Unsupervised Group Recommendation</title><link>https://deep-diver.github.io/neurips2024/posters/otzyhoamhx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/otzyhoamhx/</guid><description>Unsupervised group recommendation model, ITR, achieves superior user and group recommendation accuracy by dynamically identifying user groups and employing self-supervised learning, eliminating the ne&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/otzyhoamhx/cover.png"/></item><item><title>Implicitly Guided Design with PropEn: Match your Data to Follow the Gradient</title><link>https://deep-diver.github.io/neurips2024/posters/dhfho90ink/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dhfho90ink/</guid><description>PropEn: a novel framework for implicitly guided design optimization that leverages &amp;lsquo;matching&amp;rsquo; to boost efficiency by matching samples and approximating the gradient without a discriminator.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dhfho90ink/cover.png"/></item><item><title>Improving Deep Learning Optimization through Constrained Parameter Regularization</title><link>https://deep-diver.github.io/neurips2024/posters/rcxtkihkbf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rcxtkihkbf/</guid><description>Constrained Parameter Regularization (CPR) outperforms traditional weight decay by dynamically adapting regularization strengths for individual parameters, leading to better deep learning model perfor&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rcxtkihkbf/cover.png"/></item><item><title>Improving Deep Reinforcement Learning by Reducing the Chain Effect of Value and Policy Churn</title><link>https://deep-diver.github.io/neurips2024/posters/cqoagpbarc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cqoagpbarc/</guid><description>Deep RL agents often suffer from instability due to the &amp;lsquo;chain effect&amp;rsquo; of value and policy churn; this paper introduces CHAIN, a novel method to reduce this churn, thereby improving DRL performance an&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cqoagpbarc/cover.png"/></item><item><title>Improving Equivariant Model Training via Constraint Relaxation</title><link>https://deep-diver.github.io/neurips2024/posters/twkl7k1u5v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/twkl7k1u5v/</guid><description>Boost equivariant model training by strategically relaxing constraints during training, enhancing optimization and generalization!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/twkl7k1u5v/cover.png"/></item><item><title>Improving Generalization and Convergence by Enhancing Implicit Regularization</title><link>https://deep-diver.github.io/neurips2024/posters/cjm2bhloic/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cjm2bhloic/</guid><description>IRE framework expedites the discovery of flat minima in deep learning, enhancing generalization and convergence. By decoupling the dynamics of flat and sharp directions, IRE boosts sharpness reduction&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cjm2bhloic/cover.png"/></item><item><title>Inference of Neural Dynamics Using Switching Recurrent Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/zb8jlah2vn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zb8jlah2vn/</guid><description>SRNNs reveal behaviorally-relevant neural dynamics switches!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zb8jlah2vn/cover.png"/></item><item><title>Integrating GNN and Neural ODEs for Estimating Non-Reciprocal Two-Body Interactions in Mixed-Species Collective Motion</title><link>https://deep-diver.github.io/neurips2024/posters/qwl3eidi9r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qwl3eidi9r/</guid><description>Deep learning framework integrating GNNs and neural ODEs precisely estimates non-reciprocal two-body interactions in mixed-species collective motion, accurately replicating both individual and collect&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qwl3eidi9r/cover.png"/></item><item><title>Introducing Spectral Attention for Long-Range Dependency in Time Series Forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/dxynvebqmp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dxynvebqmp/</guid><description>Spectral Attention boosts long-range dependency capture in time series forecasting, achieving state-of-the-art results across various models and datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dxynvebqmp/cover.png"/></item><item><title>Inverse Factorized Soft Q-Learning for Cooperative Multi-agent Imitation Learning</title><link>https://deep-diver.github.io/neurips2024/posters/xrbgxjomjp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xrbgxjomjp/</guid><description>New multi-agent imitation learning algorithm (MIFQ) leverages inverse soft Q-learning and factorization for stable, efficient training, achieving state-of-the-art results on challenging benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xrbgxjomjp/cover.png"/></item><item><title>Is Mamba Compatible with Trajectory Optimization in Offline Reinforcement Learning?</title><link>https://deep-diver.github.io/neurips2024/posters/ywsxjlfsmx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ywsxjlfsmx/</guid><description>Decision Mamba (DeMa) outperforms Decision Transformer (DT) in offline RL trajectory optimization with 30% fewer parameters in Atari and a quarter in MuJoCo, demonstrating the efficacy of Mamba&amp;rsquo;s line&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ywsxjlfsmx/cover.png"/></item><item><title>Is Value Learning Really the Main Bottleneck in Offline RL?</title><link>https://deep-diver.github.io/neurips2024/posters/nyp59a31ju/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nyp59a31ju/</guid><description>Offline RL&amp;rsquo;s performance often lags behind imitation learning, but this paper reveals that policy learning and generalization, not value function learning, are often the main bottlenecks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nyp59a31ju/cover.png"/></item><item><title>Large Pre-trained time series models for cross-domain Time series analysis tasks</title><link>https://deep-diver.github.io/neurips2024/posters/vmmzjcr5zj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vmmzjcr5zj/</guid><description>Large Pre-trained Time-series Models (LPTM) achieves superior forecasting and time-series classification results using a novel adaptive segmentation method, requiring up to 40% less data and 50% less &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vmmzjcr5zj/cover.png"/></item><item><title>Learning Infinitesimal Generators of Continuous Symmetries from Data</title><link>https://deep-diver.github.io/neurips2024/posters/wl44w8xpc7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wl44w8xpc7/</guid><description>Learn continuous symmetries from data without pre-defined groups using Neural ODEs and a novel validity score to improve model generalization and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wl44w8xpc7/cover.png"/></item><item><title>Learning Macroscopic Dynamics from Partial Microscopic Observations</title><link>https://deep-diver.github.io/neurips2024/posters/cjh0qsgd0d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cjh0qsgd0d/</guid><description>Learn macroscopic dynamics efficiently using only partial microscopic force computations! This novel method leverages sparsity assumptions and stochastic estimation for accurate, cost-effective modeli&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cjh0qsgd0d/cover.png"/></item><item><title>Learning Multimodal Behaviors from Scratch with Diffusion Policy Gradient</title><link>https://deep-diver.github.io/neurips2024/posters/vu1sibb57j/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vu1sibb57j/</guid><description>DDiffPG: A novel actor-critic algorithm learns multimodal policies from scratch using diffusion models, enabling agents to master versatile behaviors in complex tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vu1sibb57j/cover.png"/></item><item><title>Learning on Large Graphs using Intersecting Communities</title><link>https://deep-diver.github.io/neurips2024/posters/pgr5x4e1gy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pgr5x4e1gy/</guid><description>Learn on massive graphs efficiently using Intersecting Community Graphs (ICGs)! This method approximates large graphs with ICGs, enabling linear time/memory complexity for node classification.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pgr5x4e1gy/cover.png"/></item><item><title>Learning Successor Features the Simple Way</title><link>https://deep-diver.github.io/neurips2024/posters/ri7ozj1wmc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ri7ozj1wmc/</guid><description>Learn deep Successor Features (SFs) directly from pixels, efficiently and without representation collapse, using a novel, simple method combining TD and reward prediction loss!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ri7ozj1wmc/cover.png"/></item><item><title>Learning the Optimal Policy for Balancing Short-Term and Long-Term Rewards</title><link>https://deep-diver.github.io/neurips2024/posters/zgh0chwoco/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zgh0chwoco/</guid><description>A novel Decomposition-based Policy Learning (DPPL) method optimally balances short-term and long-term rewards, even with interrelated objectives, by transforming the problem into intuitive subproblems&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zgh0chwoco/cover.png"/></item><item><title>Learning to Balance Altruism and Self-interest Based on Empathy in Mixed-Motive Games</title><link>https://deep-diver.github.io/neurips2024/posters/ry0rxtjwjy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ry0rxtjwjy/</guid><description>AI agents learn to balance helpfulness and self-preservation using empathy to gauge social relationships and guide reward sharing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ry0rxtjwjy/cover.png"/></item><item><title>Leveraging Contrastive Learning for Enhanced Node Representations in Tokenized Graph Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/u6fuikzt1k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/u6fuikzt1k/</guid><description>GCFormer, a novel graph Transformer, enhances node representation learning by employing a hybrid token generator and contrastive learning, outperforming existing methods on various datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/u6fuikzt1k/cover.png"/></item><item><title>Light Unbalanced Optimal Transport</title><link>https://deep-diver.github.io/neurips2024/posters/co8kzws1yk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/co8kzws1yk/</guid><description>LightUnbalancedOptimalTransport: A fast, theoretically-justified solver for continuous unbalanced optimal transport problems, enabling efficient analysis of large datasets with imbalanced classes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/co8kzws1yk/cover.png"/></item><item><title>Linear Transformers are Versatile In-Context Learners</title><link>https://deep-diver.github.io/neurips2024/posters/p1ft33mu3j/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/p1ft33mu3j/</guid><description>Linear transformers surprisingly learn intricate optimization algorithms, even surpassing baselines on noisy regression problems, showcasing their unexpected learning capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/p1ft33mu3j/cover.png"/></item><item><title>Local Anti-Concentration Class: Logarithmic Regret for Greedy Linear Contextual Bandit</title><link>https://deep-diver.github.io/neurips2024/posters/rblaf2euxq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rblaf2euxq/</guid><description>Greedy algorithms for linear contextual bandits achieve poly-logarithmic regret under the novel Local Anti-Concentration condition, expanding applicable distributions beyond Gaussians and uniforms.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rblaf2euxq/cover.png"/></item><item><title>Making Offline RL Online: Collaborative World Models for Offline Visual Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/ucxqrked0d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ucxqrked0d/</guid><description>CoWorld: a novel model-based RL approach tackles offline visual RL challenges by using online simulators as testbeds, enabling flexible value estimation &amp;amp; mitigating overestimation bias for effective &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ucxqrked0d/cover.png"/></item><item><title>Meta-Reinforcement Learning with Universal Policy Adaptation: Provable Near-Optimality under All-task Optimum Comparator</title><link>https://deep-diver.github.io/neurips2024/posters/rpjh69dux2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rpjh69dux2/</guid><description>Provable near-optimality in meta-RL is achieved using a novel bilevel optimization framework and universal policy adaptation algorithm.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rpjh69dux2/cover.png"/></item><item><title>Mitigating Backdoor Attack by Injecting Proactive Defensive Backdoor</title><link>https://deep-diver.github.io/neurips2024/posters/cbkjbyikid/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cbkjbyikid/</guid><description>Proactive Defensive Backdoor (PDB) thwarts malicious backdoors by injecting a hidden defensive backdoor during training, suppressing attacks while maintaining model utility.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cbkjbyikid/cover.png"/></item><item><title>Model LEGO: Creating Models Like Disassembling and Assembling Building Blocks</title><link>https://deep-diver.github.io/neurips2024/posters/nxl7eazkbi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nxl7eazkbi/</guid><description>Model LEGO (MDA) revolutionizes deep learning by enabling the creation of new models by assembling and disassembling task-aware components from pre-trained models, eliminating the need for retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nxl7eazkbi/cover.png"/></item><item><title>Monomial Matrix Group Equivariant Neural Functional Networks</title><link>https://deep-diver.github.io/neurips2024/posters/rqyywgyuzk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rqyywgyuzk/</guid><description>Monomial-NFNs boost neural network efficiency by leveraging scaling/sign-flipping symmetries, resulting in fewer trainable parameters and competitive performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rqyywgyuzk/cover.png"/></item><item><title>Multi-Label Learning with Stronger Consistency Guarantees</title><link>https://deep-diver.github.io/neurips2024/posters/zauerb1kgx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zauerb1kgx/</guid><description>Novel surrogate losses with label-independent H-consistency bounds enable stronger guarantees for multi-label learning.</description></item><item><title>Navigating the Effect of Parametrization for Dimensionality Reduction</title><link>https://deep-diver.github.io/neurips2024/posters/eynynyle41/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/eynynyle41/</guid><description>ParamRepulsor, a novel parametric dimensionality reduction method, achieves state-of-the-art local structure preservation by mining hard negatives and using a tailored loss function.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/eynynyle41/cover.png"/></item><item><title>Neural Conditional Probability for Uncertainty Quantification</title><link>https://deep-diver.github.io/neurips2024/posters/zxfhhjnmb2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zxfhhjnmb2/</guid><description>Neural Conditional Probability (NCP) offers a new operator-theoretic approach for efficiently learning conditional distributions, enabling streamlined inference and providing theoretical guarantees fo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zxfhhjnmb2/cover.png"/></item><item><title>Neural P$^3$M: A Long-Range Interaction Modeling Enhancer for Geometric GNNs</title><link>https://deep-diver.github.io/neurips2024/posters/ncqauwsyl5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ncqauwsyl5/</guid><description>Neural P³M enhances geometric GNNs by incorporating mesh points to model long-range interactions in molecules, achieving state-of-the-art accuracy in predicting energy and forces.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ncqauwsyl5/cover.png"/></item><item><title>Newton Losses: Using Curvature Information for Learning with Differentiable Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/vjaorqq71s/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vjaorqq71s/</guid><description>Newton Losses enhance training of neural networks with complex objectives by using second-order information from loss functions, achieving significant performance gains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vjaorqq71s/cover.png"/></item><item><title>Nonconvex Federated Learning on Compact Smooth Submanifolds With Heterogeneous Data</title><link>https://deep-diver.github.io/neurips2024/posters/uo53206olj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uo53206olj/</guid><description>This paper proposes a novel federated learning algorithm for nonconvex problems on compact smooth manifolds, achieving both computational and communication efficiency while mitigating client drift.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uo53206olj/cover.png"/></item><item><title>Nuclear Norm Regularization for Deep Learning</title><link>https://deep-diver.github.io/neurips2024/posters/eddhtvb5em/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/eddhtvb5em/</guid><description>This paper presents a novel, efficient method for Jacobian nuclear norm regularization in deep learning, replacing computationally expensive SVDs with equivalent Frobenius norm computations, thereby e&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/eddhtvb5em/cover.png"/></item><item><title>Oja's Algorithm for Streaming Sparse PCA</title><link>https://deep-diver.github.io/neurips2024/posters/clqdptoord/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/clqdptoord/</guid><description>Oja&amp;rsquo;s algorithm achieves minimax optimal error rates for streaming sparse PCA using a simple single-pass thresholding method, requiring only O(d) space and O(nd) time.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/clqdptoord/cover.png"/></item><item><title>On $f$-Divergence Principled Domain Adaptation: An Improved Framework</title><link>https://deep-diver.github.io/neurips2024/posters/xsu27dgwer/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xsu27dgwer/</guid><description>Improved unsupervised domain adaptation framework achieves superior performance via refined f-divergence and novel f-domain discrepancy, enabling faster algorithms and tighter generalization bounds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xsu27dgwer/cover.png"/></item><item><title>On conditional diffusion models for PDE simulations</title><link>https://deep-diver.github.io/neurips2024/posters/nql8ejymzh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nql8ejymzh/</guid><description>This paper introduces novel autoregressive sampling and hybrid training strategies for score-based diffusion models, significantly boosting PDE forecasting and assimilation accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nql8ejymzh/cover.png"/></item><item><title>On the Curses of Future and History in Future-dependent Value Functions for Off-policy Evaluation</title><link>https://deep-diver.github.io/neurips2024/posters/s5917zor6v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s5917zor6v/</guid><description>This paper tackles the &amp;lsquo;curse of horizon&amp;rsquo; in off-policy evaluation for partially observable Markov decision processes (POMDPs) by proposing novel coverage assumptions, enabling polynomial estimation e&amp;hellip;</description></item><item><title>On the Necessity of Collaboration for Online Model Selection with Decentralized Data</title><link>https://deep-diver.github.io/neurips2024/posters/uqwflgzpv1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uqwflgzpv1/</guid><description>Federated online model selection needs collaboration only when clients have limited computing power; otherwise, independent learning suffices.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uqwflgzpv1/cover.png"/></item><item><title>Optimal Design for Human Preference Elicitation</title><link>https://deep-diver.github.io/neurips2024/posters/ccgwj61ael/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ccgwj61ael/</guid><description>Dope: Efficient algorithms optimize human preference elicitation for learning to rank, minimizing ranking loss and prediction error with absolute and ranking feedback models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ccgwj61ael/cover.png"/></item><item><title>Optimistic Verifiable Training by Controlling Hardware Nondeterminism</title><link>https://deep-diver.github.io/neurips2024/posters/bf0mdflz1i/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bf0mdflz1i/</guid><description>Researchers developed a verifiable training method that uses high-precision training with adaptive rounding and logging to achieve exact training replication across different GPUs, enabling efficient &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bf0mdflz1i/cover.png"/></item><item><title>Out-of-Distribution Detection with a Single Unconditional Diffusion Model</title><link>https://deep-diver.github.io/neurips2024/posters/ttnfh7d1h4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ttnfh7d1h4/</guid><description>Single diffusion model achieves competitive out-of-distribution detection across diverse tasks by analyzing diffusion path characteristics.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ttnfh7d1h4/cover.png"/></item><item><title>Personalized Federated Learning with Mixture of Models for Adaptive Prediction and Model Fine-Tuning</title><link>https://deep-diver.github.io/neurips2024/posters/yvuhnbkczd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yvuhnbkczd/</guid><description>Fed-POE: A personalized federated learning algorithm for superior real-time predictions!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yvuhnbkczd/cover.png"/></item><item><title>PGN: The RNN's New Successor is Effective for Long-Range Time Series Forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/ypeamfku2o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ypeamfku2o/</guid><description>TPGN, a novel framework for long-range time series forecasting, uses Parallel Gated Networks (PGN) to efficiently capture long-term dependencies, achieving state-of-the-art results on multiple dataset&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ypeamfku2o/cover.png"/></item><item><title>Policy Mirror Descent with Lookahead</title><link>https://deep-diver.github.io/neurips2024/posters/om2aa0guha/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/om2aa0guha/</guid><description>Boosting reinforcement learning, this paper introduces h-PMD, a novel algorithm enhancing policy mirror descent with lookahead for faster convergence and improved sample complexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/om2aa0guha/cover.png"/></item><item><title>Precise asymptotics of reweighted least-squares algorithms for linear diagonal networks</title><link>https://deep-diver.github.io/neurips2024/posters/nv7ox1vd3q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nv7ox1vd3q/</guid><description>New analysis reveals how reweighted least-squares algorithms for linear diagonal networks achieve favorable performance in high-dimensional settings, improving upon existing theoretical guarantees and&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nv7ox1vd3q/cover.png"/></item><item><title>Predicting Ground State Properties: Constant Sample Complexity and Deep Learning Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/yblxvqjyqa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yblxvqjyqa/</guid><description>Deep learning algorithms now predict quantum ground state properties with constant sample complexity, regardless of system size, improving upon previous methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yblxvqjyqa/cover.png"/></item><item><title>Preferential Normalizing Flows</title><link>https://deep-diver.github.io/neurips2024/posters/srsjr9sdkr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/srsjr9sdkr/</guid><description>Eliciting high-dimensional probability distributions from experts using only preference comparisons is achieved via normalizing flows and a novel functional prior, resolving the problem of collapsing &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/srsjr9sdkr/cover.png"/></item><item><title>Pretraining Codomain Attention Neural Operators for Solving Multiphysics PDEs</title><link>https://deep-diver.github.io/neurips2024/posters/wspiduxzyx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wspiduxzyx/</guid><description>CoDA-NO, a novel neural operator, revolutionizes multiphysics PDE solving via codomain tokenization, enabling efficient self-supervised pretraining and few-shot learning for superior generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wspiduxzyx/cover.png"/></item><item><title>Protected Test-Time Adaptation via Online Entropy Matching: A Betting Approach</title><link>https://deep-diver.github.io/neurips2024/posters/qamfjyhpeg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qamfjyhpeg/</guid><description>POEM: a novel test-time adaptation approach using online self-training improves accuracy under distribution shifts by dynamically updating the classifier, ensuring invariance to shifts while maintaini&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qamfjyhpeg/cover.png"/></item><item><title>Provably Efficient Reinforcement Learning with Multinomial Logit Function Approximation</title><link>https://deep-diver.github.io/neurips2024/posters/z2739hyur3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z2739hyur3/</guid><description>This paper presents novel RL algorithms using multinomial logit function approximation, achieving O(1) computation and storage while nearly closing the regret gap with linear methods.</description></item><item><title>PURE: Prompt Evolution with Graph ODE for Out-of-distribution Fluid Dynamics Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/z86knmjouq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z86knmjouq/</guid><description>PURE: A novel method uses Graph ODE to adapt spatio-temporal forecasting models to various fluid dynamics scenarios, improving model adaptation to unseen parameters and long-term predictions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/z86knmjouq/cover.png"/></item><item><title>REBEL: Reinforcement Learning via Regressing Relative Rewards</title><link>https://deep-diver.github.io/neurips2024/posters/yxjwajzuyv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yxjwajzuyv/</guid><description>REBEL, a novel reinforcement learning algorithm, simplifies policy optimization by regressing relative rewards, achieving strong performance in language and image generation tasks with increased effic&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yxjwajzuyv/cover.png"/></item><item><title>Recurrent Reinforcement Learning with Memoroids</title><link>https://deep-diver.github.io/neurips2024/posters/na4q983a1v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/na4q983a1v/</guid><description>Memoroids and Tape-Based Batching revolutionize recurrent RL, enabling efficient processing of long sequences and improving sample efficiency by eliminating segmentation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/na4q983a1v/cover.png"/></item><item><title>Regularized Adaptive Momentum Dual Averaging with an Efficient Inexact Subproblem Solver for Training Structured Neural Network</title><link>https://deep-diver.github.io/neurips2024/posters/xl7ve14aha/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xl7ve14aha/</guid><description>RAMDA: a new algorithm ensures efficient training of structured neural networks by achieving optimal structure and outstanding predictive performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xl7ve14aha/cover.png"/></item><item><title>Reinforcement Learning with Lookahead Information</title><link>https://deep-diver.github.io/neurips2024/posters/wlqfovltqz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wlqfovltqz/</guid><description>Provably efficient RL algorithms are designed to utilize immediate reward or transition information, significantly improving reward collection in unknown environments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wlqfovltqz/cover.png"/></item><item><title>Rethinking Deep Thinking: Stable Learning of Algorithms using Lipschitz Constraints</title><link>https://deep-diver.github.io/neurips2024/posters/zlgfrk2cqa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zlgfrk2cqa/</guid><description>Stable algorithm learning achieved by Deep Thinking networks with Lipschitz Constraints, ensuring convergence and better extrapolation to complex problems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zlgfrk2cqa/cover.png"/></item><item><title>Rethinking Reconstruction-based Graph-Level Anomaly Detection: Limitations and a Simple Remedy</title><link>https://deep-diver.github.io/neurips2024/posters/e2inndpinb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/e2inndpinb/</guid><description>MUSE, a novel graph anomaly detection method, leverages multifaceted summaries of reconstruction errors, achieving state-of-the-art performance by addressing limitations of existing Graph-AE-based met&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/e2inndpinb/cover.png"/></item><item><title>Sample-Efficient Agnostic Boosting</title><link>https://deep-diver.github.io/neurips2024/posters/ufkbrvyxtp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ufkbrvyxtp/</guid><description>Agnostic boosting gets a major efficiency upgrade! A new algorithm leverages sample reuse to drastically reduce the data needed for accurate learning, closing the gap with computationally expensive al&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ufkbrvyxtp/cover.png"/></item><item><title>Segment, Shuffle, and Stitch: A Simple Layer for Improving Time-Series Representations</title><link>https://deep-diver.github.io/neurips2024/posters/zm1lcgrphm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zm1lcgrphm/</guid><description>Boost time-series model accuracy with Segment, Shuffle, and Stitch (S3)! This simple layer shuffles data segments to enhance representation learning, improving classification, forecasting, and anomaly&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zm1lcgrphm/cover.png"/></item><item><title>Sequential Decision Making with Expert Demonstrations under Unobserved Heterogeneity</title><link>https://deep-diver.github.io/neurips2024/posters/c8cpmlpubi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/c8cpmlpubi/</guid><description>ExPerior leverages expert demonstrations to enhance online decision-making, even when experts use hidden contextual information unseen by the learner.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/c8cpmlpubi/cover.png"/></item><item><title>Sharpness-diversity tradeoff: improving flat ensembles with SharpBalance</title><link>https://deep-diver.github.io/neurips2024/posters/wjacsnt9ue/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wjacsnt9ue/</guid><description>SharpBalance, a novel training approach, effectively improves deep ensemble performance by addressing the sharpness-diversity trade-off, leading to significant improvements in both in-distribution and&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wjacsnt9ue/cover.png"/></item><item><title>Stepping Forward on the Last Mile</title><link>https://deep-diver.github.io/neurips2024/posters/ych1z6dcto/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ych1z6dcto/</guid><description>On-device training with fixed-point forward gradients enables efficient model personalization on resource-constrained edge devices, overcoming backpropagation&amp;rsquo;s memory limitations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ych1z6dcto/cover.png"/></item><item><title>Stochastic contextual bandits with graph feedback: from independence number to MAS number</title><link>https://deep-diver.github.io/neurips2024/posters/t8iosewoyd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/t8iosewoyd/</guid><description>Contextual bandits with graph feedback achieve near-optimal regret by leveraging a novel graph-theoretic quantity that interpolates between independence and maximum acyclic subgraph numbers, depending&amp;hellip;</description></item><item><title>Stochastic Kernel Regularisation Improves Generalisation in Deep Kernel Machines</title><link>https://deep-diver.github.io/neurips2024/posters/prgxz9fybf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/prgxz9fybf/</guid><description>Deep kernel machines now achieve 94.5% accuracy on CIFAR-10, matching neural networks, by using stochastic kernel regularization to improve generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/prgxz9fybf/cover.png"/></item><item><title>Stopping Bayesian Optimization with Probabilistic Regret Bounds</title><link>https://deep-diver.github.io/neurips2024/posters/cm2gu9xgti/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cm2gu9xgti/</guid><description>This paper presents a novel probabilistic regret bound (PRB) framework for Bayesian optimization, replacing the traditional fixed-budget stopping rule with a criterion based on the probability of find&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cm2gu9xgti/cover.png"/></item><item><title>Super Consistency of Neural Network Landscapes and Learning Rate Transfer</title><link>https://deep-diver.github.io/neurips2024/posters/rgwhj7intz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rgwhj7intz/</guid><description>Neural network hyperparameter transferability across vastly different model sizes is achieved via a newly discovered property called &amp;lsquo;Super Consistency&amp;rsquo; of loss landscapes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rgwhj7intz/cover.png"/></item><item><title>Supra-Laplacian Encoding for Transformer on Dynamic Graphs</title><link>https://deep-diver.github.io/neurips2024/posters/vp9qazr2gw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vp9qazr2gw/</guid><description>SLATE: Supra-Laplacian encoding for spatio-temporal Transformers achieves state-of-the-art dynamic link prediction by innovatively using a multi-layer graph representation and a unique cross-attention&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vp9qazr2gw/cover.png"/></item><item><title>Take A Shortcut Back: Mitigating the Gradient Vanishing for Training Spiking Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/xjyu6zmzd7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xjyu6zmzd7/</guid><description>Shortcut back-propagation and an evolutionary training framework conquer gradient vanishing in spiking neural networks, drastically improving training and achieving state-of-the-art accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xjyu6zmzd7/cover.png"/></item><item><title>The Benefits of Balance: From Information Projections to Variance Reduction</title><link>https://deep-diver.github.io/neurips2024/posters/vjmmdffl0a/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vjmmdffl0a/</guid><description>Data balancing in foundation models surprisingly reduces variance, improving model training and performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vjmmdffl0a/cover.png"/></item><item><title>The Iterative Optimal Brain Surgeon: Faster Sparse Recovery by Leveraging Second-Order Information</title><link>https://deep-diver.github.io/neurips2024/posters/snxwd0q4ei/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/snxwd0q4ei/</guid><description>I-OBS, a novel family of sparse recovery algorithms leveraging second-order information, achieves faster convergence rates for sparse DNNs, validated by large-scale experiments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/snxwd0q4ei/cover.png"/></item><item><title>The Limits of Transfer Reinforcement Learning with Latent Low-rank Structure</title><link>https://deep-diver.github.io/neurips2024/posters/pk2qgry2hv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pk2qgry2hv/</guid><description>This paper presents computationally efficient transfer reinforcement learning algorithms that remove the dependence on state/action space sizes while achieving minimax optimality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pk2qgry2hv/cover.png"/></item><item><title>The Many Faces of Optimal Weak-to-Strong Learning</title><link>https://deep-diver.github.io/neurips2024/posters/z7h7zmgypj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z7h7zmgypj/</guid><description>A new, surprisingly simple boosting algorithm achieves provably optimal sample complexity and outperforms existing algorithms on large datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/z7h7zmgypj/cover.png"/></item><item><title>Toward Global Convergence of Gradient EM for Over-Paramterized Gaussian Mixture Models</title><link>https://deep-diver.github.io/neurips2024/posters/zv9gyc3xgf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zv9gyc3xgf/</guid><description>Gradient EM for over-parameterized Gaussian Mixture Models globally converges with a sublinear rate, solving a longstanding open problem in machine learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zv9gyc3xgf/cover.png"/></item><item><title>Towards Diverse Device Heterogeneous Federated Learning via Task Arithmetic Knowledge Integration</title><link>https://deep-diver.github.io/neurips2024/posters/y6jotynerr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y6jotynerr/</guid><description>TAKFL, a novel federated learning framework, tackles device heterogeneity by independently distilling knowledge from diverse devices and integrating it adaptively, achieving state-of-the-art performan&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y6jotynerr/cover.png"/></item><item><title>Transition Constrained Bayesian Optimization via Markov Decision Processes</title><link>https://deep-diver.github.io/neurips2024/posters/efrdruyhr9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/efrdruyhr9/</guid><description>This paper presents a novel BayesOpt framework that incorporates Markov Decision Processes to optimize black-box functions with transition constraints, overcoming limitations of traditional methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/efrdruyhr9/cover.png"/></item><item><title>Uncertainty-based Offline Variational Bayesian Reinforcement Learning for Robustness under Diverse Data Corruptions</title><link>https://deep-diver.github.io/neurips2024/posters/rtxciwsfsd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rtxciwsfsd/</guid><description>TRACER, a novel robust offline RL algorithm, uses Bayesian inference to handle uncertainty from diverse data corruptions, significantly outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rtxciwsfsd/cover.png"/></item><item><title>Understanding Representation of Deep Equilibrium Models from Neural Collapse Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/obuxeummq1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/obuxeummq1/</guid><description>Deep Equilibrium Models excel on imbalanced data due to feature convergence and self-duality properties, unlike explicit models, as shown through Neural Collapse analysis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/obuxeummq1/cover.png"/></item><item><title>Universal Online Convex Optimization with $1$ Projection per Round</title><link>https://deep-diver.github.io/neurips2024/posters/xnncvkbwws/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xnncvkbwws/</guid><description>This paper introduces a novel universal online convex optimization algorithm needing only one projection per round, achieving optimal regret bounds for various function types, including general convex&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xnncvkbwws/cover.png"/></item><item><title>Universal Physics Transformers: A Framework For Efficiently Scaling Neural Operators</title><link>https://deep-diver.github.io/neurips2024/posters/ouxinx5krm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ouxinx5krm/</guid><description>Universal Physics Transformers (UPTs) offer a unified, scalable framework for efficiently training neural operators across diverse spatio-temporal physics problems, overcoming limitations of existing &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ouxinx5krm/cover.png"/></item><item><title>Weight decay induces low-rank attention layers</title><link>https://deep-diver.github.io/neurips2024/posters/odeqjim9sk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/odeqjim9sk/</guid><description>Weight decay in deep learning surprisingly induces low-rank attention layers, potentially harming performance but offering optimization strategies for large language models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/odeqjim9sk/cover.png"/></item></channel></rss>