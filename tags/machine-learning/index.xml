<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine Learning on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/machine-learning/</link><description>Recent content in Machine Learning on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Achieving Optimal Clustering in Gaussian Mixture Models with Anisotropic Covariance Structures</title><link>https://deep-diver.github.io/neurips2024/oral/ge8gzn8gtu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral/ge8gzn8gtu/</guid><description>This research develops rate-optimal clustering algorithms for Gaussian Mixture Models with anisotropic covariance structures, bridging the gap between theoretical guarantees and practical efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral/ge8gzn8gtu/cover.png"/></item><item><title>Convolutional Differentiable Logic Gate Networks</title><link>https://deep-diver.github.io/neurips2024/oral/4bkefyuht4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral/4bkefyuht4/</guid><description>Convolutional Differentiable Logic Gate Networks achieve state-of-the-art accuracy on CIFAR-10 with 29x fewer gates than existing models, demonstrating highly efficient deep learning inference.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral/4bkefyuht4/cover.png"/></item><item><title>DapperFL: Domain Adaptive Federated Learning with Model Fusion Pruning for Edge Devices</title><link>https://deep-diver.github.io/neurips2024/oral/pezt0xttae/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral/pezt0xttae/</guid><description>DapperFL enhances federated learning by introducing a model fusion pruning module and domain adaptive regularization to improve performance and reduce model size for heterogeneous edge devices.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral/pezt0xttae/cover.png"/></item><item><title>Exploitation of a Latent Mechanism in Graph Contrastive Learning: Representation Scattering</title><link>https://deep-diver.github.io/neurips2024/oral/r8solcx62k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral/r8solcx62k/</guid><description>SGRL, a novel graph contrastive learning framework, significantly boosts performance by leveraging the inherent &amp;lsquo;representation scattering&amp;rsquo; mechanism and integrating graph topology, outperforming exis&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral/r8solcx62k/cover.png"/></item><item><title>Improving Environment Novelty Quantification for Effective Unsupervised Environment Design</title><link>https://deep-diver.github.io/neurips2024/oral/udxpjko2f9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral/udxpjko2f9/</guid><description>Boosting AI generalization: CENIE framework quantifies environment novelty via state-action coverage, enhancing unsupervised environment design for robust generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral/udxpjko2f9/cover.png"/></item><item><title>Learning Formal Mathematics From Intrinsic Motivation</title><link>https://deep-diver.github.io/neurips2024/oral/unkltq8mbd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral/unkltq8mbd/</guid><description>AI agent MINIMO learns to generate challenging mathematical conjectures and prove them, bootstrapping from axioms alone and self-improving in both conjecture generation and theorem proving.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral/unkltq8mbd/cover.png"/></item><item><title>Maximum Entropy Inverse Reinforcement Learning of Diffusion Models with Energy-Based Models</title><link>https://deep-diver.github.io/neurips2024/oral/v0ojalqy4e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral/v0ojalqy4e/</guid><description>Boosting diffusion model sample quality, especially with few steps, is achieved via a novel maximum entropy inverse reinforcement learning approach, jointly training the model and an energy-based mode&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral/v0ojalqy4e/cover.png"/></item><item><title>Reinforcement Learning Under Latent Dynamics: Toward Statistical and Algorithmic Modularity</title><link>https://deep-diver.github.io/neurips2024/oral/qf2uzady1n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral/qf2uzady1n/</guid><description>This paper pioneers a modular framework for reinforcement learning, addressing the challenge of learning under complex observations and simpler latent dynamics, offering both statistical and algorithm&amp;hellip;</description></item><item><title>Scale Equivariant Graph Metanetworks</title><link>https://deep-diver.github.io/neurips2024/oral/8fxqn1tzm1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral/8fxqn1tzm1/</guid><description>ScaleGMNs, a new framework, enhances neural network processing by incorporating scaling symmetries, boosting performance across various tasks and datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral/8fxqn1tzm1/cover.png"/></item><item><title>Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs</title><link>https://deep-diver.github.io/neurips2024/oral/pgey8jq3qx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral/pgey8jq3qx/</guid><description>This paper achieves minimax-optimal bounds for learning near-optimal policies in average-reward MDPs, addressing a long-standing open problem in reinforcement learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral/pgey8jq3qx/cover.png"/></item><item><title>Statistical Efficiency of Distributional Temporal Difference Learning</title><link>https://deep-diver.github.io/neurips2024/oral/ewum5hrygh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral/ewum5hrygh/</guid><description>Researchers achieve minimax optimal sample complexity bounds for distributional temporal difference learning, enhancing reinforcement learning algorithm efficiency.</description></item><item><title>Stochastic Taylor Derivative Estimator: Efficient amortization for arbitrary differential operators</title><link>https://deep-diver.github.io/neurips2024/oral/j2wi2rcg2u/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral/j2wi2rcg2u/</guid><description>Stochastic Taylor Derivative Estimator (STDE) drastically accelerates the optimization of neural networks involving high-dimensional, high-order differential operators by efficiently amortizing comput&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral/j2wi2rcg2u/cover.png"/></item><item><title>The Road Less Scheduled</title><link>https://deep-diver.github.io/neurips2024/oral/0xenkkenui/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral/0xenkkenui/</guid><description>Revolutionizing machine learning, Schedule-Free optimization achieves state-of-the-art results without needing learning rate schedules, simplifying training and improving efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral/0xenkkenui/cover.png"/></item><item><title>The Sample-Communication Complexity Trade-off in Federated Q-Learning</title><link>https://deep-diver.github.io/neurips2024/oral/6yipvnkjuk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral/6yipvnkjuk/</guid><description>Federated Q-learning achieves optimal sample &amp;amp; communication complexities simultaneously via Fed-DVR-Q, a novel algorithm.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral/6yipvnkjuk/cover.png"/></item></channel></rss>