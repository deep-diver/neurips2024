<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ Nanjing University of Aeronautics and Astronautics on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-nanjing-university-of-aeronautics-and-astronautics/</link><description>Recent content in üè¢ Nanjing University of Aeronautics and Astronautics on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2025 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-nanjing-university-of-aeronautics-and-astronautics/index.xml" rel="self" type="application/rss+xml"/><item><title>Forgetting, Ignorance or Myopia: Revisiting Key Challenges in Online Continual Learning</title><link>https://deep-diver.github.io/neurips2024/posters/oparhdvqrd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oparhdvqrd/</guid><description>NsCE framework tackles key OCL challenges: model ignorance (learning effective features in limited time) and myopia (overly simplified features). NsCE integrates non-sparse maximum separation regulari&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oparhdvqrd/cover.png"/></item><item><title>Learning Distinguishable Trajectory Representation with Contrastive Loss</title><link>https://deep-diver.github.io/neurips2024/posters/d6nlm2ayhi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/d6nlm2ayhi/</guid><description>Contrastive Trajectory Representation (CTR) boosts multi-agent reinforcement learning by learning distinguishable agent trajectories using contrastive loss, thus improving performance significantly.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/d6nlm2ayhi/cover.png"/></item><item><title>Magnet: We Never Know How Text-to-Image Diffusion Models Work, Until We Learn How Vision-Language Models Function</title><link>https://deep-diver.github.io/neurips2024/posters/4mzgimooxm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4mzgimooxm/</guid><description>Magnet: Enhancing Text-to-Image Synthesis by Disentangling Attributes in CLIP.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4mzgimooxm/cover.png"/></item><item><title>Optimistic Critic Reconstruction and Constrained Fine-Tuning for General Offline-to-Online RL</title><link>https://deep-diver.github.io/neurips2024/posters/xvfevb9xfx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xvfevb9xfx/</guid><description>This paper introduces OCR-CFT, a novel method for general offline-to-online RL, achieving stable and efficient performance improvements by addressing evaluation and improvement mismatches through opti&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xvfevb9xfx/cover.png"/></item></channel></rss>