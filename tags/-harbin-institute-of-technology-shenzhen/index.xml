<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ Harbin Institute of Technology, Shenzhen on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-harbin-institute-of-technology-shenzhen/</link><description>Recent content in üè¢ Harbin Institute of Technology, Shenzhen on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-harbin-institute-of-technology-shenzhen/index.xml" rel="self" type="application/rss+xml"/><item><title>CNCA: Toward Customizable and Natural Generation of Adversarial Camouflage for Vehicle Detectors</title><link>https://deep-diver.github.io/neurips2024/posters/axnzg82izv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/axnzg82izv/</guid><description>Researchers developed CNCA, a novel method that generates realistic and customizable adversarial camouflage for vehicle detectors by leveraging a pre-trained diffusion model, surpassing existing metho&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/axnzg82izv/cover.png"/></item><item><title>Exploiting Descriptive Completeness Prior for Cross Modal Hashing with Incomplete Labels</title><link>https://deep-diver.github.io/neurips2024/posters/ferj6wqshv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ferj6wqshv/</guid><description>PCRIL, a novel prompt contrastive recovery approach, significantly boosts cross-modal hashing accuracy, especially when dealing with incomplete labels by progressively identifying promising positive c&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ferj6wqshv/cover.png"/></item><item><title>MoME: Mixture of Multimodal Experts for Generalist Multimodal Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/xskl7da34u/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xskl7da34u/</guid><description>MoME, a novel Mixture of Multimodal Experts, significantly improves generalist Multimodal Large Language Models (MLLMs) by mitigating task interference through specialized vision and language experts,&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xskl7da34u/cover.png"/></item><item><title>Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks</title><link>https://deep-diver.github.io/neurips2024/posters/xxomcwz6by/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xxomcwz6by/</guid><description>Optimus-1: Hybrid Multimodal Memory empowers AI agents to excel in complex, long-horizon tasks by integrating hierarchical knowledge graphs and multimodal experience for superior planning and reflecti&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xxomcwz6by/cover.png"/></item></channel></rss>