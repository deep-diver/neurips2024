<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Vision-Language Models on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/vision-language-models/</link><description>Recent content in Vision-Language Models on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/vision-language-models/index.xml" rel="self" type="application/rss+xml"/><item><title>A Sober Look at the Robustness of CLIPs to Spurious Features</title><link>https://deep-diver.github.io/neurips2024/posters/wwyumweyv8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wwyumweyv8/</guid><description>CounterAnimal: a new dataset exposes CLIP&amp;rsquo;s reliance on spurious correlations, challenging its perceived robustness and highlighting the need for more comprehensive evaluation benchmarks in vision-lan&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wwyumweyv8/cover.png"/></item><item><title>A Unified Debiasing Approach for Vision-Language Models across Modalities and Tasks</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/181llen2gw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/181llen2gw/</guid><description>SFID, a novel debiasing method, effectively mitigates bias in vision-language models across various tasks without retraining, improving fairness and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/181llen2gw/cover.png"/></item><item><title>AdaNeg: Adaptive Negative Proxy Guided OOD Detection with Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/vs5nc7jtci/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vs5nc7jtci/</guid><description>AdaNeg dynamically generates negative proxies during testing to improve vision-language model OOD detection, significantly outperforming existing methods on ImageNet.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vs5nc7jtci/cover.png"/></item><item><title>Adaptive Image Quality Assessment via Teaching Large Multimodal Model to Compare</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/mhtoyh5taj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/mhtoyh5taj/</guid><description>Compare2Score: A novel IQA model teaches large multimodal models to translate comparative image quality judgments into continuous quality scores, significantly outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/mhtoyh5taj/cover.png"/></item><item><title>Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks and Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/wt5agmvkaj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wt5agmvkaj/</guid><description>This paper presents a novel method to align vision models with human aesthetics in image retrieval, using large language models (LLMs) for query rephrasing and preference-based reinforcement learning &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wt5agmvkaj/cover.png"/></item><item><title>Automated Multi-level Preference for MLLMs</title><link>https://deep-diver.github.io/neurips2024/posters/woenr7fjai/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/woenr7fjai/</guid><description>Automated Multi-level Preference (AMP) framework significantly improves multimodal large language model (MLLM) performance by using multi-level preferences during training, reducing hallucinations and&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/woenr7fjai/cover.png"/></item><item><title>Boosting Vision-Language Models with Transduction</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/go4zzxbwvs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/go4zzxbwvs/</guid><description>TransCLIP significantly boosts vision-language model accuracy by efficiently integrating transduction, a powerful learning paradigm that leverages the structure of unlabeled data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/go4zzxbwvs/cover.png"/></item><item><title>Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs</title><link>https://deep-diver.github.io/neurips2024/oral-others/vi8aepaxgy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-others/vi8aepaxgy/</guid><description>Cambrian-1: Open, vision-centric multimodal LLMs achieve state-of-the-art performance using a novel spatial vision aggregator and high-quality data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-others/vi8aepaxgy/cover.png"/></item><item><title>CIFD: Controlled Information Flow to Enhance Knowledge Distillation</title><link>https://deep-diver.github.io/neurips2024/posters/xutrkezbpf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xutrkezbpf/</guid><description>CIFD, a novel knowledge distillation method, drastically cuts training costs while boosting performance, particularly for large datasets, by using Rate-Distortion Modules instead of Teacher Assistants&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xutrkezbpf/cover.png"/></item><item><title>CLIPLoss and Norm-Based Data Selection Methods for Multimodal Contrastive Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/qvdc0ocx2n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/qvdc0ocx2n/</guid><description>Boosting multimodal contrastive learning, this research introduces negCLIPLoss and NormSim, novel data selection methods surpassing existing techniques by improving data quality and task relevance. Th&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/qvdc0ocx2n/cover.png"/></item><item><title>Conjugated Semantic Pool Improves OOD Detection with Pre-trained Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/qqqfocueqm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qqqfocueqm/</guid><description>Boosting zero-shot OOD detection accuracy, this paper introduces a conjugated semantic pool (CSP) improving FPR95 by 7.89%. CSP leverages modified superclass names for superior OOD label identificatio&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qqqfocueqm/cover.png"/></item><item><title>Cracking the Code of Juxtaposition: Can AI Models Understand the Humorous Contradictions</title><link>https://deep-diver.github.io/neurips2024/oral-others/bcmpdaqcnw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-others/bcmpdaqcnw/</guid><description>Can AI understand humor? A new benchmark, YESBUT, reveals that even state-of-the-art models struggle with the nuanced humor of juxtaposed comics, highlighting the need for improved AI in understandin&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-others/bcmpdaqcnw/cover.png"/></item><item><title>Customized Multiple Clustering via Multi-Modal Subspace Proxy Learning</title><link>https://deep-diver.github.io/neurips2024/posters/xbuastqaez/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xbuastqaez/</guid><description>Multi-Sub leverages multi-modal learning to achieve customized multiple clustering, aligning user-defined textual preferences with visual representations via a subspace proxy learning framework.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xbuastqaez/cover.png"/></item><item><title>Deep Correlated Prompting for Visual Recognition with Missing Modalities</title><link>https://deep-diver.github.io/neurips2024/posters/zo55ovdljw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zo55ovdljw/</guid><description>Deep Correlated Prompting enhances large multimodal models&amp;rsquo; robustness against missing data by leveraging inter-layer and cross-modality correlations in prompts, achieving superior performance with mi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zo55ovdljw/cover.png"/></item><item><title>Enhancing Motion in Text-to-Video Generation with Decomposed Encoding and Conditioning</title><link>https://deep-diver.github.io/neurips2024/posters/nkzse5kkca/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nkzse5kkca/</guid><description>DEMO framework enhances text-to-video generation by decomposing text encoding and conditioning into content and motion components, resulting in videos with significantly improved motion dynamics.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nkzse5kkca/cover.png"/></item><item><title>Enhancing Zero-Shot Vision Models by Label-Free Prompt Distribution Learning and Bias Correcting</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/ojximyclit/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/ojximyclit/</guid><description>Frolic: A label-free framework boosts zero-shot vision model accuracy by learning prompt distributions and correcting label bias, achieving state-of-the-art performance across multiple datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/ojximyclit/cover.png"/></item><item><title>Flex-MoE: Modeling Arbitrary Modality Combination via the Flexible Mixture-of-Experts</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/ihehcbqzex/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/ihehcbqzex/</guid><description>Flex-MoE: A novel framework flexibly handles arbitrary modality combinations in multimodal learning, even with missing data, achieving robust performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/ihehcbqzex/cover.png"/></item><item><title>G2D: From Global to Dense Radiography Representation Learning via Vision-Language Pre-training</title><link>https://deep-diver.github.io/neurips2024/posters/zsxbgjj7oo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zsxbgjj7oo/</guid><description>G2D: a novel medical VLP framework achieves superior performance in medical image analysis by simultaneously learning global and dense visual features using image-text pairs without extra annotations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zsxbgjj7oo/cover.png"/></item><item><title>GenArtist: Multimodal LLM as an Agent for Unified Image Generation and Editing</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/ur00bnk1v2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/ur00bnk1v2/</guid><description>GenArtist uses a multimodal large language model as an AI agent to unify image generation and editing, achieving state-of-the-art performance by decomposing complex tasks and leveraging a comprehensiv&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/ur00bnk1v2/cover.png"/></item><item><title>HAWK: Learning to Understand Open-World Video Anomalies</title><link>https://deep-diver.github.io/neurips2024/posters/vbkoez1pg3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vbkoez1pg3/</guid><description>HAWK: a novel framework leveraging interactive VLMs and motion modality achieves state-of-the-art performance in open-world video anomaly understanding, generating descriptions and answering questions&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vbkoez1pg3/cover.png"/></item><item><title>Homology Consistency Constrained Efficient Tuning for Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/vemngkxvtx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vemngkxvtx/</guid><description>Constraining vision-language model tuning via persistent homology ensures consistent image-text alignment, improving few-shot learning and domain generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vemngkxvtx/cover.png"/></item><item><title>How Control Information Influences Multilingual Text Image Generation and Editing?</title><link>https://deep-diver.github.io/neurips2024/posters/r3c0wgcxgt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/r3c0wgcxgt/</guid><description>TextGen enhances multilingual visual text generation and editing by optimizing control information using Fourier analysis and a two-stage framework, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/r3c0wgcxgt/cover.png"/></item><item><title>InstructG2I: Synthesizing Images from Multimodal Attributed Graphs</title><link>https://deep-diver.github.io/neurips2024/posters/zwnw4zqkum/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zwnw4zqkum/</guid><description>INSTRUCTG2I: a novel graph context-conditioned diffusion model, generates images from multimodal attributed graphs, addressing challenges in graph size, dependencies, and controllability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zwnw4zqkum/cover.png"/></item><item><title>Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/cvasru8leo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cvasru8leo/</guid><description>SpatialEval benchmark reveals that current vision-language models struggle with spatial reasoning, highlighting the need for improved multimodal models that effectively integrate visual and textual in&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cvasru8leo/cover.png"/></item><item><title>Kaleido Diffusion: Improving Conditional Diffusion Models with Autoregressive Latent Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/qzswlclmcs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qzswlclmcs/</guid><description>Kaleido Diffusion boosts the diversity of images generated by diffusion models without sacrificing quality, using autoregressive latent modeling to add more control and interpretability to the image g&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qzswlclmcs/cover.png"/></item><item><title>Learning Cortico-Muscular Dependence through Orthonormal Decomposition of Density Ratios</title><link>https://deep-diver.github.io/neurips2024/posters/wdgvrud1ls/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wdgvrud1ls/</guid><description>Unveiling cortico-muscular dependence using orthonormal decomposition of density ratios, FMCA-T, enhances movement classification and reveals channel-temporal dependencies.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wdgvrud1ls/cover.png"/></item><item><title>Learning to Reason Iteratively and Parallelly for Complex Visual Reasoning Scenarios</title><link>https://deep-diver.github.io/neurips2024/posters/uojq9qadjy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uojq9qadjy/</guid><description>Boosting complex visual reasoning, a new Iterative and Parallel Reasoning Mechanism (IPRM) outperforms existing methods by combining step-by-step and simultaneous computations, improving accuracy and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uojq9qadjy/cover.png"/></item><item><title>LG-VQ: Language-Guided Codebook Learning</title><link>https://deep-diver.github.io/neurips2024/posters/va4s3kn4qe/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/va4s3kn4qe/</guid><description>LG-VQ: A novel language-guided codebook learning framework boosts multi-modal performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/va4s3kn4qe/cover.png"/></item><item><title>Lips Are Lying: Spotting the Temporal Inconsistency between Audio and Visual in Lip-Syncing DeepFakes</title><link>https://deep-diver.github.io/neurips2024/posters/yms7ansbr6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yms7ansbr6/</guid><description>LipFD: a novel method leverages audio-visual inconsistencies to accurately spot lip-syncing deepfakes, outperforming existing methods and introducing a high-quality dataset for future research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yms7ansbr6/cover.png"/></item><item><title>LOVA3: Learning to Visual Question Answering, Asking and Assessment</title><link>https://deep-diver.github.io/neurips2024/posters/vioklml6wu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vioklml6wu/</guid><description>LOVA³ enhances MLLMs by teaching them to ask and assess image-based questions, improving their multimodal understanding and performance on various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vioklml6wu/cover.png"/></item><item><title>Lumen: Unleashing Versatile Vision-Centric Capabilities of Large Multimodal Models</title><link>https://deep-diver.github.io/neurips2024/posters/v5un2qqnrf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v5un2qqnrf/</guid><description>Lumen: A novel LMM architecture decouples perception learning into task-agnostic and task-specific stages, enabling versatile vision-centric capabilities and surpassing existing LMM-based approaches.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v5un2qqnrf/cover.png"/></item><item><title>Multi-modal Transfer Learning between Biological Foundation Models</title><link>https://deep-diver.github.io/neurips2024/posters/ximejtduiw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ximejtduiw/</guid><description>IsoFormer, a novel multi-modal model, accurately predicts RNA transcript isoform expression by integrating DNA, RNA, and protein sequence information, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ximejtduiw/cover.png"/></item><item><title>Multilingual Diversity Improves Vision-Language Representations</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/1wteqrecys/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/1wteqrecys/</guid><description>Boosting vision-language models: Multilingual data improves performance on English-centric benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/1wteqrecys/cover.png"/></item><item><title>Neuro-Vision to Language: Enhancing Brain Recording-based Visual Reconstruction and Language Interaction</title><link>https://deep-diver.github.io/neurips2024/posters/ohi00yht3t/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ohi00yht3t/</guid><description>Researchers enhanced brain recording-based visual reconstruction using a novel Vision Transformer 3D framework integrated with LLMs, achieving superior performance in visual reconstruction, captioning&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ohi00yht3t/cover.png"/></item><item><title>PERIA: Perceive, Reason, Imagine, Act via Holistic Language and Vision Planning for Manipulation</title><link>https://deep-diver.github.io/neurips2024/posters/zw2k6lffi9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zw2k6lffi9/</guid><description>PERIA: Holistic language &amp;amp; vision planning for complex robotic manipulation!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zw2k6lffi9/cover.png"/></item><item><title>Pre-trained Text-to-Image Diffusion Models Are Versatile Representation Learners for Control</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/ky07a73f3y/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/ky07a73f3y/</guid><description>Pre-trained text-to-image diffusion models create highly effective, versatile representations for embodied AI control, surpassing previous methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/ky07a73f3y/cover.png"/></item><item><title>Procedure-Aware Surgical Video-language Pretraining with Hierarchical Knowledge Augmentation</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/zuwperkjnh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/zuwperkjnh/</guid><description>PeskaVLP: Hierarchical knowledge augmentation boosts surgical video-language pretraining!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/zuwperkjnh/cover.png"/></item><item><title>Rethinking Misalignment in Vision-Language Model Adaptation from a Causal Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/vwgwbcxeaq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vwgwbcxeaq/</guid><description>Vision-language model adaptation struggles with misalignment; this paper introduces Causality-Guided Semantic Decoupling and Classification (CDC) to mitigate this, boosting performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vwgwbcxeaq/cover.png"/></item><item><title>RSA: Resolving Scale Ambiguities in Monocular Depth Estimators through Language Descriptions</title><link>https://deep-diver.github.io/neurips2024/posters/vh7gcadhao/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vh7gcadhao/</guid><description>RSA: Language unlocks metric depth from single images!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vh7gcadhao/cover.png"/></item><item><title>Scene Graph Generation with Role-Playing Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/xprui8amtc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xprui8amtc/</guid><description>SDSGG outperforms leading scene graph generation methods by using LLMs to create scene-specific descriptions, adapting to diverse visual relations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xprui8amtc/cover.png"/></item><item><title>Self-Calibrated Tuning of Vision-Language Models for Out-of-Distribution Detection</title><link>https://deep-diver.github.io/neurips2024/posters/w6vbfsc1y0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w6vbfsc1y0/</guid><description>Self-Calibrated Tuning (SCT) enhances vision-language model OOD detection by adaptively weighting OOD regularization based on prediction uncertainty, mitigating issues caused by inaccurate feature ext&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w6vbfsc1y0/cover.png"/></item><item><title>SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy Segment Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/xcf2vbyzts/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xcf2vbyzts/</guid><description>SocialGPT cleverly leverages Vision Foundation Models and Large Language Models for zero-shot social relation reasoning, achieving competitive results and offering interpretable outputs via prompt opt&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xcf2vbyzts/cover.png"/></item><item><title>Text-DiFuse: An Interactive Multi-Modal Image Fusion Framework based on Text-modulated Diffusion Model</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/ybrxzibyeg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/ybrxzibyeg/</guid><description>Text-DiFuse: A novel interactive multi-modal image fusion framework leverages text-modulated diffusion models for superior performance in complex scenarios.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/ybrxzibyeg/cover.png"/></item><item><title>Text-Guided Attention is All You Need for Zero-Shot Robustness in Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/cow65a9fgf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cow65a9fgf/</guid><description>Text-Guided Attention for Zero-Shot Robustness (TGA-ZSR) significantly improves vision-language model robustness against adversarial attacks by aligning and constraining text-guided attention, achievi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cow65a9fgf/cover.png"/></item><item><title>TFG: Unified Training-Free Guidance for Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/n8ybgx98vc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/n8ybgx98vc/</guid><description>TFG: A unified, training-free framework for boosting diffusion model performance by efficiently searching its algorithm-agnostic design space.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/n8ybgx98vc/cover.png"/></item><item><title>Toward a Stable, Fair, and Comprehensive Evaluation of Object Hallucination in Large Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/yql5tutdah/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yql5tutdah/</guid><description>LeHaCE: a novel framework for evaluating object hallucination in LVLMs, improving evaluation stability and fairness by accounting for instruction-induced image description length variations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yql5tutdah/cover.png"/></item><item><title>Towards Understanding the Working Mechanism of Text-to-Image Diffusion Model</title><link>https://deep-diver.github.io/neurips2024/posters/ztu0qepvtz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ztu0qepvtz/</guid><description>Stable Diffusion&amp;rsquo;s text-to-image generation is sped up by 25% by removing text guidance after the initial shape generation, revealing that the [EOS] token is key to early-stage image construction.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ztu0qepvtz/cover.png"/></item><item><title>Towards Unified Multimodal Editing with Enhanced Knowledge Collaboration</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/kf80zs3fvy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/kf80zs3fvy/</guid><description>UniKE: A unified multimodal editing method achieves superior reliability, generality, and locality by disentangling knowledge into semantic and truthfulness spaces, enabling enhanced collaboration bet&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/kf80zs3fvy/cover.png"/></item><item><title>TPR: Topology-Preserving Reservoirs for Generalized Zero-Shot Learning</title><link>https://deep-diver.github.io/neurips2024/posters/zkfca4oesf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zkfca4oesf/</guid><description>Topology-Preserving Reservoirs (TPR) enhances CLIP&amp;rsquo;s zero-shot learning by using a dual-space alignment and a topology-preserving objective to improve generalization to unseen classes, achieving state&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zkfca4oesf/cover.png"/></item><item><title>Unified Generative and Discriminative Training for Multi-modal Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/w67vrhzf13/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w67vrhzf13/</guid><description>Unified generative-discriminative training boosts multimodal large language models (MLLMs)! Sugar, a novel approach, leverages dynamic sequence alignment and a triple kernel to enhance global and fin&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w67vrhzf13/cover.png"/></item><item><title>Unified Lexical Representation for Interpretable Visual-Language Alignment</title><link>https://deep-diver.github.io/neurips2024/posters/xocfd1wkpf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xocfd1wkpf/</guid><description>LexVLA: A novel visual-language alignment framework learns unified lexical representations for improved interpretability and efficient cross-modal retrieval.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xocfd1wkpf/cover.png"/></item><item><title>Unveiling Encoder-Free Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/sppab1tmlc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/sppab1tmlc/</guid><description>EVE, a groundbreaking encoder-free vision-language model, rivals encoder-based counterparts using a fraction of the data and resources, demonstrating efficient, transparent training for pure decoder-o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/sppab1tmlc/cover.png"/></item><item><title>Unveiling the Tapestry of Consistency in Large Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/tu1oc7zhgw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tu1oc7zhgw/</guid><description>ConBench: Unveiling Inconsistency in Large Vision-Language Models</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tu1oc7zhgw/cover.png"/></item><item><title>Vision Model Pre-training on Interleaved Image-Text Data via Latent Compression Learning</title><link>https://deep-diver.github.io/neurips2024/posters/z6knvoe9zq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z6knvoe9zq/</guid><description>Latent Compression Learning (LCL) revolutionizes vision model pre-training by effectively leveraging readily available interleaved image-text data, achieving performance comparable to models trained o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/z6knvoe9zq/cover.png"/></item><item><title>Vision-Language Navigation with Energy-Based Policy</title><link>https://deep-diver.github.io/neurips2024/posters/v3jhuoxmw8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v3jhuoxmw8/</guid><description>Energy-based Navigation Policy (ENP) revolutionizes Vision-Language Navigation by modeling joint state-action distributions, achieving superior performance across diverse benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v3jhuoxmw8/cover.png"/></item><item><title>VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks</title><link>https://deep-diver.github.io/neurips2024/posters/nvydpf4ljk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nvydpf4ljk/</guid><description>VisionLLM v2 unifies visual perception, understanding, and generation, excelling in various vision tasks and achieving performance comparable to task-specific models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nvydpf4ljk/cover.png"/></item><item><title>VLM Agents Generate Their Own Memories: Distilling Experience into Embodied Programs of Thought</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/5g7mrfpngt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/5g7mrfpngt/</guid><description>VLMs learn to generate their own memories by abstracting experiences from noisy demonstrations and human feedback, significantly boosting in-context learning performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/5g7mrfpngt/cover.png"/></item><item><title>Voila-A: Aligning Vision-Language Models with User's Gaze Attention</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/zyrz5v84zi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/zyrz5v84zi/</guid><description>Voila-A enhances vision-language models by aligning their attention with user gaze, improving real-world application effectiveness and interpretability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/zyrz5v84zi/cover.png"/></item><item><title>What matters when building vision-language models?</title><link>https://deep-diver.github.io/neurips2024/posters/dtvjf1vy2i/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dtvjf1vy2i/</guid><description>Idefics2, a new 8B-parameter VLM, achieves state-of-the-art performance, closing the gap with much larger models by meticulously analyzing design choices and training methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dtvjf1vy2i/cover.png"/></item><item><title>Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2)</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/s4yrclbuk1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/s4yrclbuk1/</guid><description>T2IScoreScore objectively evaluates text-to-image prompt faithfulness metrics using semantic error graphs, revealing that simpler metrics surprisingly outperform complex, computationally expensive one&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/s4yrclbuk1/cover.png"/></item><item><title>XMask3D: Cross-modal Mask Reasoning for Open Vocabulary 3D Semantic Segmentation</title><link>https://deep-diver.github.io/neurips2024/posters/z1gwanognr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z1gwanognr/</guid><description>XMask3D uses cross-modal mask reasoning to achieve state-of-the-art open vocabulary 3D semantic segmentation by aligning 2D and 3D features at the mask level, resulting in precise segmentation boundar&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/z1gwanognr/cover.png"/></item></channel></rss>