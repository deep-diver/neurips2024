<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Vision-Language Models on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/vision-language-models/</link><description>Recent content in Vision-Language Models on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2025 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/vision-language-models/index.xml" rel="self" type="application/rss+xml"/><item><title>$ extit{Bifr"ost}$: 3D-Aware Image Compositing with Language Instructions</title><link>https://deep-diver.github.io/neurips2024/posters/vcptu8e6yk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vcptu8e6yk/</guid><description>Bifröst: A novel 3D-aware framework for instruction-based image compositing, leveraging depth maps and an MLLM for high-fidelity results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vcptu8e6yk/cover.png"/></item><item><title>4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities</title><link>https://deep-diver.github.io/neurips2024/posters/qrnmljqhgx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qrnmljqhgx/</guid><description>4M-21 achieves any-to-any predictions across 21 diverse vision modalities using a single model, exceeding prior state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qrnmljqhgx/cover.png"/></item><item><title>A Cat Is A Cat (Not A Dog!): Unraveling Information Mix-ups in Text-to-Image Encoders through Causal Analysis and Embedding Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/ynrywzhmky/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ynrywzhmky/</guid><description>Researchers unveil how causal text encoding in text-to-image models leads to information loss and bias, proposing a novel training-free optimization method that significantly improves information bala&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ynrywzhmky/cover.png"/></item><item><title>A Concept-Based Explainability Framework for Large Multimodal Models</title><link>https://deep-diver.github.io/neurips2024/posters/mvjlrfntw6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mvjlrfntw6/</guid><description>CoX-LMM unveils a novel concept-based explainability framework for large multimodal models, extracting semantically grounded multimodal concepts to enhance interpretability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mvjlrfntw6/cover.png"/></item><item><title>A Sober Look at the Robustness of CLIPs to Spurious Features</title><link>https://deep-diver.github.io/neurips2024/posters/wwyumweyv8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wwyumweyv8/</guid><description>CounterAnimal: a new dataset exposes CLIP&amp;rsquo;s reliance on spurious correlations, challenging its perceived robustness and highlighting the need for more comprehensive evaluation benchmarks in vision-lan&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wwyumweyv8/cover.png"/></item><item><title>A Unified Debiasing Approach for Vision-Language Models across Modalities and Tasks</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/181llen2gw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/181llen2gw/</guid><description>SFID, a novel debiasing method, effectively mitigates bias in vision-language models across various tasks without retraining, improving fairness and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/181llen2gw/cover.png"/></item><item><title>Accelerating Pre-training of Multimodal LLMs via Chain-of-Sight</title><link>https://deep-diver.github.io/neurips2024/posters/khcb1drmrx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/khcb1drmrx/</guid><description>Chain-of-Sight accelerates multimodal LLM pre-training by ~73% using a multi-scale visual resampling technique and a novel post-pretrain token scaling strategy, achieving comparable or superior perfor&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/khcb1drmrx/cover.png"/></item><item><title>Accelerating Transformers with Spectrum-Preserving Token Merging</title><link>https://deep-diver.github.io/neurips2024/posters/ppdjpio3mv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ppdjpio3mv/</guid><description>PITOME: a novel token merging method accelerates Transformers by 40-60% while preserving accuracy, prioritizing informative tokens via an energy score.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ppdjpio3mv/cover.png"/></item><item><title>AdaNeg: Adaptive Negative Proxy Guided OOD Detection with Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/vs5nc7jtci/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vs5nc7jtci/</guid><description>AdaNeg dynamically generates negative proxies during testing to improve vision-language model OOD detection, significantly outperforming existing methods on ImageNet.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vs5nc7jtci/cover.png"/></item><item><title>Adaptive Image Quality Assessment via Teaching Large Multimodal Model to Compare</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/mhtoyh5taj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/mhtoyh5taj/</guid><description>Compare2Score: A novel IQA model teaches large multimodal models to translate comparative image quality judgments into continuous quality scores, significantly outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/mhtoyh5taj/cover.png"/></item><item><title>Advancing Cross-domain Discriminability in Continual Learning of Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/bogxvywzeq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bogxvywzeq/</guid><description>RAIL, a novel continual learning method for vision-language models, tackles catastrophic forgetting and maintains zero-shot abilities without domain-identity hints or reference data. Using a recursiv&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bogxvywzeq/cover.png"/></item><item><title>Aggregate-and-Adapt Natural Language Prompts for Downstream Generalization of CLIP</title><link>https://deep-diver.github.io/neurips2024/posters/yz3wbkok0k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yz3wbkok0k/</guid><description>Aggregate-and-Adapt Prompt Embedding (AAPE) boosts CLIP&amp;rsquo;s downstream generalization by distilling textual knowledge from natural language prompts, achieving competitive performance across various visi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yz3wbkok0k/cover.png"/></item><item><title>Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks and Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/wt5agmvkaj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wt5agmvkaj/</guid><description>This paper presents a novel method to align vision models with human aesthetics in image retrieval, using large language models (LLMs) for query rephrasing and preference-based reinforcement learning &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wt5agmvkaj/cover.png"/></item><item><title>Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/sf2glfhvss/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sf2glfhvss/</guid><description>New Hallucination-Induced Optimization (HIO) significantly reduces hallucinations in Large Vision-Language Models (LVLMs) by amplifying contrast between correct and incorrect tokens, outperforming exi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sf2glfhvss/cover.png"/></item><item><title>An eye for an ear: zero-shot audio description leveraging an image captioner with audio-visual token distribution matching</title><link>https://deep-diver.github.io/neurips2024/posters/u6oqezsp8z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/u6oqezsp8z/</guid><description>Leveraging vision-language models, this research introduces a novel unsupervised zero-shot audio captioning method that achieves state-of-the-art performance by aligning audio and image token distribu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/u6oqezsp8z/cover.png"/></item><item><title>Are We on the Right Way for Evaluating Large Vision-Language Models?</title><link>https://deep-diver.github.io/neurips2024/posters/evp9mxnnxj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/evp9mxnnxj/</guid><description>MMStar benchmark tackles flawed LVLMs evaluation by focusing on vision-critical samples, minimizing data leakage, and introducing new metrics for fair multi-modal gain assessment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/evp9mxnnxj/cover.png"/></item><item><title>Ask, Attend, Attack: An Effective Decision-Based Black-Box Targeted Attack for Image-to-Text Models</title><link>https://deep-diver.github.io/neurips2024/posters/9umjecuekk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/9umjecuekk/</guid><description>This paper introduces AAA, a novel three-stage decision-based black-box targeted attack against image-to-text models. AAA efficiently generates semantically consistent adversarial examples by asking &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/9umjecuekk/cover.png"/></item><item><title>Atlas3D: Physically Constrained Self-Supporting Text-to-3D for Simulation and Fabrication</title><link>https://deep-diver.github.io/neurips2024/posters/5x69cl2w3f/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/5x69cl2w3f/</guid><description>Atlas3D enhances text-to-3D generation by integrating physics-based simulations, producing self-supporting 3D models for seamless real-world applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/5x69cl2w3f/cover.png"/></item><item><title>AttnDreamBooth: Towards Text-Aligned Personalized Text-to-Image Generation</title><link>https://deep-diver.github.io/neurips2024/posters/4binoegdcm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4binoegdcm/</guid><description>AttnDreamBooth: A novel approach to text-to-image generation that overcomes limitations of prior methods by separating learning processes, resulting in significantly improved identity preservation and&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4binoegdcm/cover.png"/></item><item><title>Automated Multi-level Preference for MLLMs</title><link>https://deep-diver.github.io/neurips2024/posters/woenr7fjai/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/woenr7fjai/</guid><description>Automated Multi-level Preference (AMP) framework significantly improves multimodal large language model (MLLM) performance by using multi-level preferences during training, reducing hallucinations and&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/woenr7fjai/cover.png"/></item><item><title>AWT: Transferring Vision-Language Models via Augmentation, Weighting, and Transportation</title><link>https://deep-diver.github.io/neurips2024/posters/yiyww1d3le/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yiyww1d3le/</guid><description>AWT: a novel framework boosts vision-language model&amp;rsquo;s zero-shot capabilities by augmenting inputs, weighting them dynamically, and leveraging optimal transport to enhance semantic correlations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yiyww1d3le/cover.png"/></item><item><title>BendVLM: Test-Time Debiasing of Vision-Language Embeddings</title><link>https://deep-diver.github.io/neurips2024/posters/utmohsgxzb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/utmohsgxzb/</guid><description>BEND-VLM: A novel, efficient test-time debiasing method for vision-language models, resolving bias without retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/utmohsgxzb/cover.png"/></item><item><title>Beyond Accuracy: Ensuring Correct Predictions With Correct Rationales</title><link>https://deep-diver.github.io/neurips2024/posters/adv0pzi3ol/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/adv0pzi3ol/</guid><description>This research introduces a novel two-phase approach to improve AI model trustworthiness by ensuring both correct predictions and correct rationales. A new dataset with structured rationales and a rat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/adv0pzi3ol/cover.png"/></item><item><title>Black-Box Forgetting</title><link>https://deep-diver.github.io/neurips2024/posters/lpfdhc91oj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lpfdhc91oj/</guid><description>Black-Box Forgetting achieves selective forgetting in large pre-trained models by optimizing input prompts, not model parameters, thus enabling targeted class removal without requiring internal model &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lpfdhc91oj/cover.png"/></item><item><title>BoostAdapter: Improving Vision-Language Test-Time Adaptation via Regional Bootstrapping</title><link>https://deep-diver.github.io/neurips2024/posters/8toyl6wsgy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8toyl6wsgy/</guid><description>BoostAdapter enhances vision-language model test-time adaptation by combining instance-agnostic historical samples with instance-aware boosting samples for superior out-of-distribution and cross-domai&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8toyl6wsgy/cover.png"/></item><item><title>Boosting Alignment for Post-Unlearning Text-to-Image Generative Models</title><link>https://deep-diver.github.io/neurips2024/posters/93ktalfvnj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/93ktalfvnj/</guid><description>This research introduces a novel framework for post-unlearning in text-to-image generative models, optimizing model updates to ensure both effective forgetting and maintained text-image alignment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/93ktalfvnj/cover.png"/></item><item><title>Boosting Text-to-Video Generative Model with MLLMs Feedback</title><link>https://deep-diver.github.io/neurips2024/posters/3ivnixhy16/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3ivnixhy16/</guid><description>MLLMs enhance text-to-video generation by providing 135k fine-grained video preferences, creating VIDEOPREFER, and a novel reward model, VIDEORM, boosting video quality and alignment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3ivnixhy16/cover.png"/></item><item><title>Boosting Vision-Language Models with Transduction</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/go4zzxbwvs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/go4zzxbwvs/</guid><description>TransCLIP significantly boosts vision-language model accuracy by efficiently integrating transduction, a powerful learning paradigm that leverages the structure of unlabeled data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/go4zzxbwvs/cover.png"/></item><item><title>Boosting Weakly Supervised Referring Image Segmentation via Progressive Comprehension</title><link>https://deep-diver.github.io/neurips2024/posters/mxdygxok9h/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mxdygxok9h/</guid><description>PCNet boosts weakly-supervised referring image segmentation by progressively processing textual cues, mimicking human comprehension, and significantly improving target localization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mxdygxok9h/cover.png"/></item><item><title>Bridge the Modality and Capability Gaps in Vision-Language Model Selection</title><link>https://deep-diver.github.io/neurips2024/posters/01qa1zjs65/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/01qa1zjs65/</guid><description>SWAB bridges modality and capability gaps in Vision-Language Model selection using optimal transport, enabling accurate prediction of VLM performance without images.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/01qa1zjs65/cover.png"/></item><item><title>Calibrated Self-Rewarding Vision Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/nxyedmtf1t/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nxyedmtf1t/</guid><description>Calibrated Self-Rewarding (CSR) significantly improves vision-language models by using a novel iterative approach that incorporates visual constraints into the self-rewarding process, reducing halluci&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nxyedmtf1t/cover.png"/></item><item><title>CALVIN: Improved Contextual Video Captioning via Instruction Tuning</title><link>https://deep-diver.github.io/neurips2024/posters/7kz7iccz6h/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7kz7iccz6h/</guid><description>CALVIN: Instruction tuning boosts contextual video captioning, achieving state-of-the-art results!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7kz7iccz6h/cover.png"/></item><item><title>Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs</title><link>https://deep-diver.github.io/neurips2024/oral-others/vi8aepaxgy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-others/vi8aepaxgy/</guid><description>Cambrian-1: Open, vision-centric multimodal LLMs achieve state-of-the-art performance using a novel spatial vision aggregator and high-quality data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-others/vi8aepaxgy/cover.png"/></item><item><title>ChatCam: Empowering Camera Control through Conversational AI</title><link>https://deep-diver.github.io/neurips2024/posters/ixazpggf8h/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ixazpggf8h/</guid><description>ChatCam empowers users to control cameras via natural language, using CineGPT for text-conditioned trajectory generation and an Anchor Determinator for precise placement, enabling high-quality video r&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ixazpggf8h/cover.png"/></item><item><title>ChatTracker: Enhancing Visual Tracking Performance via Chatting with Multimodal Large Language Model</title><link>https://deep-diver.github.io/neurips2024/posters/hzanl2uncb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hzanl2uncb/</guid><description>ChatTracker boosts visual tracking by intelligently using a large language model to refine object descriptions, achieving performance on par with state-of-the-art methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hzanl2uncb/cover.png"/></item><item><title>CIFD: Controlled Information Flow to Enhance Knowledge Distillation</title><link>https://deep-diver.github.io/neurips2024/posters/xutrkezbpf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xutrkezbpf/</guid><description>CIFD, a novel knowledge distillation method, drastically cuts training costs while boosting performance, particularly for large datasets, by using Rate-Distortion Modules instead of Teacher Assistants&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xutrkezbpf/cover.png"/></item><item><title>CigTime: Corrective Instruction Generation Through Inverse Motion Editing</title><link>https://deep-diver.github.io/neurips2024/posters/gkta1qycj9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gkta1qycj9/</guid><description>CigTime generates corrective motion instructions from motion pairs using motion editing and large language models. This innovative approach improves upon baselines by leveraging motion triplets for f&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gkta1qycj9/cover.png"/></item><item><title>CLAP4CLIP: Continual Learning with Probabilistic Finetuning for Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/rf1yrtzfoj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rf1yrtzfoj/</guid><description>CLAP4CLIP enhances vision-language model continual learning by using probabilistic finetuning, improving performance and uncertainty estimation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rf1yrtzfoj/cover.png"/></item><item><title>Classification Done Right for Vision-Language Pre-Training</title><link>https://deep-diver.github.io/neurips2024/posters/hd2eowkitm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hd2eowkitm/</guid><description>SuperClass, a novel vision-language pre-training method, achieves superior performance on various downstream tasks by directly using tokenized raw text as supervised classification labels, eliminating&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hd2eowkitm/cover.png"/></item><item><title>CLIP in Mirror: Disentangling text from visual images through reflection</title><link>https://deep-diver.github.io/neurips2024/posters/fym8coxdir/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fym8coxdir/</guid><description>MirrorCLIP disentangles text from images in CLIP using mirror reflection differences, enhancing robustness against text-visual image confusion.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fym8coxdir/cover.png"/></item><item><title>CLIPCEIL: Domain Generalization through CLIP via Channel rEfinement and Image-text aLignment</title><link>https://deep-diver.github.io/neurips2024/posters/mqecu0txay/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mqecu0txay/</guid><description>CLIPCEIL enhances CLIP&amp;rsquo;s domain generalization by refining feature channels for domain invariance and aligning image-text embeddings, achieving state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mqecu0txay/cover.png"/></item><item><title>CLIPLoss and Norm-Based Data Selection Methods for Multimodal Contrastive Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/qvdc0ocx2n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/qvdc0ocx2n/</guid><description>Boosting multimodal contrastive learning, this research introduces negCLIPLoss and NormSim, novel data selection methods surpassing existing techniques by improving data quality and task relevance. Th&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/qvdc0ocx2n/cover.png"/></item><item><title>Cluster-Learngene: Inheriting Adaptive Clusters for Vision Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/92vvujvlvw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/92vvujvlvw/</guid><description>Cluster-Learngene efficiently initializes elastic-scale Vision Transformers by adaptively clustering and inheriting key modules from a large ancestry model, saving resources and boosting downstream ta&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/92vvujvlvw/cover.png"/></item><item><title>CODE: Contrasting Self-generated Description to Combat Hallucination in Large Multi-modal Models</title><link>https://deep-diver.github.io/neurips2024/posters/g6nn2aijdp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/g6nn2aijdp/</guid><description>CODE combats LMM hallucinations by contrasting self-generated descriptions with visual content during decoding, enhancing response accuracy without retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/g6nn2aijdp/cover.png"/></item><item><title>CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching</title><link>https://deep-diver.github.io/neurips2024/posters/ow1ldvmnj6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ow1ldvmnj6/</guid><description>CoMat: Aligning text-to-image diffusion models using image-to-text concept matching for superior text-image alignment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ow1ldvmnj6/cover.png"/></item><item><title>Combining Observational Data and Language for Species Range Estimation</title><link>https://deep-diver.github.io/neurips2024/posters/iokluxb05h/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/iokluxb05h/</guid><description>LE-SINR combines Wikipedia species descriptions with citizen science observations to create accurate species range maps, even with limited data, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/iokluxb05h/cover.png"/></item><item><title>Conjugated Semantic Pool Improves OOD Detection with Pre-trained Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/qqqfocueqm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qqqfocueqm/</guid><description>Boosting zero-shot OOD detection accuracy, this paper introduces a conjugated semantic pool (CSP) improving FPR95 by 7.89%. CSP leverages modified superclass names for superior OOD label identificatio&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qqqfocueqm/cover.png"/></item><item><title>Contrasting with Symile: Simple Model-Agnostic Representation Learning for Unlimited Modalities</title><link>https://deep-diver.github.io/neurips2024/posters/8pwvdarqau/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8pwvdarqau/</guid><description>Symile: A simple model-agnostic approach for learning representations from unlimited modalities, outperforming pairwise CLIP by capturing higher-order information.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8pwvdarqau/cover.png"/></item><item><title>ControlMLLM: Training-Free Visual Prompt Learning for Multimodal Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/ljndqvcre9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ljndqvcre9/</guid><description>ControlMLLM: Inject visual prompts into MLLMs via learnable latent variable optimization for training-free referring abilities, supporting box, mask, scribble, and point prompts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ljndqvcre9/cover.png"/></item><item><title>Coupled Mamba: Enhanced Multimodal Fusion with Coupled State Space Model</title><link>https://deep-diver.github.io/neurips2024/posters/uxeo3unnix/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uxeo3unnix/</guid><description>Coupled Mamba: Enhanced multi-modal fusion via coupled state space model boosts accuracy and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uxeo3unnix/cover.png"/></item><item><title>Cracking the Code of Juxtaposition: Can AI Models Understand the Humorous Contradictions</title><link>https://deep-diver.github.io/neurips2024/oral-others/bcmpdaqcnw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-others/bcmpdaqcnw/</guid><description>Can AI understand humor? A new benchmark, YESBUT, reveals that even state-of-the-art models struggle with the nuanced humor of juxtaposed comics, highlighting the need for improved AI in understandin&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-others/bcmpdaqcnw/cover.png"/></item><item><title>Cross-modal Representation Flattening for Multi-modal Domain Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/uixtytsvol/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uixtytsvol/</guid><description>Cross-Modal Representation Flattening (CMRF) improves multi-modal domain generalization by creating consistent flat loss regions and enhancing knowledge transfer between modalities, outperforming exis&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uixtytsvol/cover.png"/></item><item><title>CuMo: Scaling Multimodal LLM with Co-Upcycled Mixture-of-Experts</title><link>https://deep-diver.github.io/neurips2024/posters/hwuubsmlbf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hwuubsmlbf/</guid><description>CuMo boosts multimodal LLMs by efficiently integrating co-upcycled Mixture-of-Experts, achieving state-of-the-art performance with minimal extra parameters during inference.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hwuubsmlbf/cover.png"/></item><item><title>Customized Multiple Clustering via Multi-Modal Subspace Proxy Learning</title><link>https://deep-diver.github.io/neurips2024/posters/xbuastqaez/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xbuastqaez/</guid><description>Multi-Sub leverages multi-modal learning to achieve customized multiple clustering, aligning user-defined textual preferences with visual representations via a subspace proxy learning framework.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xbuastqaez/cover.png"/></item><item><title>Decomposing and Interpreting Image Representations via Text in ViTs Beyond CLIP</title><link>https://deep-diver.github.io/neurips2024/posters/vhh7ontfvv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vhh7ontfvv/</guid><description>This paper presents a general framework for interpreting Vision Transformer (ViT) components, mapping their contributions to CLIP space for textual interpretation, and introduces a scoring function fo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vhh7ontfvv/cover.png"/></item><item><title>Deep Correlated Prompting for Visual Recognition with Missing Modalities</title><link>https://deep-diver.github.io/neurips2024/posters/zo55ovdljw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zo55ovdljw/</guid><description>Deep Correlated Prompting enhances large multimodal models&amp;rsquo; robustness against missing data by leveraging inter-layer and cross-modality correlations in prompts, achieving superior performance with mi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zo55ovdljw/cover.png"/></item><item><title>DeepStack: Deeply Stacking Visual Tokens is Surprisingly Simple and Effective for LMMs</title><link>https://deep-diver.github.io/neurips2024/posters/fxdpdzhtdv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fxdpdzhtdv/</guid><description>DeepStack: Stacking visual tokens boosts LMMs efficiency and performance!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fxdpdzhtdv/cover.png"/></item><item><title>DeiSAM: Segment Anything with Deictic Prompting</title><link>https://deep-diver.github.io/neurips2024/posters/cmsnx47aeh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cmsnx47aeh/</guid><description>DeiSAM uses large language models and differentiable logic to achieve highly accurate image segmentation using complex, context-dependent descriptions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cmsnx47aeh/cover.png"/></item><item><title>Déjà Vu Memorization in Vision–Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/sfczdxdyns/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sfczdxdyns/</guid><description>Vision-language models (VLMs) memorize training data, impacting generalization. This paper introduces &amp;lsquo;déjà vu memorization,&amp;rsquo; a novel method measuring this, revealing significant memorization even in&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sfczdxdyns/cover.png"/></item><item><title>Dense Connector for MLLMs</title><link>https://deep-diver.github.io/neurips2024/posters/ioabr42b44/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ioabr42b44/</guid><description>Boosting multimodal LLMs, the Dense Connector efficiently integrates multi-layer visual features for significantly enhanced performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ioabr42b44/cover.png"/></item><item><title>DiffPano: Scalable and Consistent Text to Panorama Generation with Spherical Epipolar-Aware Diffusion</title><link>https://deep-diver.github.io/neurips2024/posters/yiovr40hso/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yiovr40hso/</guid><description>DiffPano generates scalable, consistent, and diverse panoramic images from text descriptions and camera poses using a novel spherical epipolar-aware diffusion model.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yiovr40hso/cover.png"/></item><item><title>Diffusion PID: Interpreting Diffusion via Partial Information Decomposition</title><link>https://deep-diver.github.io/neurips2024/posters/abpxukzs37/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/abpxukzs37/</guid><description>DiffusionPID unveils the secrets of text-to-image diffusion models by decomposing text prompts into unique, redundant, and synergistic components, providing insights into how individual words and thei&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/abpxukzs37/cover.png"/></item><item><title>DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/4xtvxmszpo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4xtvxmszpo/</guid><description>DigiRL: Autonomous RL trains robust in-the-wild device-control agents by offline-to-online RL, surpassing prior methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4xtvxmszpo/cover.png"/></item><item><title>Dual Prototype Evolving for Test-Time Generalization of Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/jsgyyxasis/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jsgyyxasis/</guid><description>Dual Prototype Evolving (DPE) significantly boosts vision-language model generalization by cumulatively learning multi-modal prototypes from unlabeled test data, outperforming current state-of-the-art&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jsgyyxasis/cover.png"/></item><item><title>Dual Risk Minimization: Towards Next-Level Robustness in Fine-tuning Zero-Shot Models</title><link>https://deep-diver.github.io/neurips2024/posters/p50dyqk0gx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/p50dyqk0gx/</guid><description>Dual Risk Minimization (DRM) improves fine-tuned zero-shot models&amp;rsquo; robustness by combining empirical and worst-case risk minimization, using LLMs to identify core features, achieving state-of-the-art &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/p50dyqk0gx/cover.png"/></item><item><title>Easy Regional Contrastive Learning of Expressive Fashion Representations</title><link>https://deep-diver.github.io/neurips2024/posters/bcl9u2x9jg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bcl9u2x9jg/</guid><description>E2, a novel regional contrastive learning method, enhances vision-language models for expressive fashion representations by explicitly attending to fashion details with minimal additional parameters, &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bcl9u2x9jg/cover.png"/></item><item><title>Enhancing Large Vision Language Models with Self-Training on Image Comprehension</title><link>https://deep-diver.github.io/neurips2024/posters/fzw7ctyjm3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fzw7ctyjm3/</guid><description>Self-Training on Image Comprehension (STIC) significantly boosts Large Vision Language Model (LVLM) performance using unlabeled image data. STIC generates a preference dataset for image descriptions &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fzw7ctyjm3/cover.png"/></item><item><title>Enhancing Motion in Text-to-Video Generation with Decomposed Encoding and Conditioning</title><link>https://deep-diver.github.io/neurips2024/posters/nkzse5kkca/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nkzse5kkca/</guid><description>DEMO framework enhances text-to-video generation by decomposing text encoding and conditioning into content and motion components, resulting in videos with significantly improved motion dynamics.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nkzse5kkca/cover.png"/></item><item><title>Enhancing Zero-Shot Vision Models by Label-Free Prompt Distribution Learning and Bias Correcting</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/ojximyclit/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/ojximyclit/</guid><description>Frolic: A label-free framework boosts zero-shot vision model accuracy by learning prompt distributions and correcting label bias, achieving state-of-the-art performance across multiple datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/ojximyclit/cover.png"/></item><item><title>Evaluation of Text-to-Video Generation Models: A Dynamics Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/tmx1aumkl6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tmx1aumkl6/</guid><description>DEVIL: a novel text-to-video evaluation protocol focusing on video dynamics, resulting in more realistic video generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tmx1aumkl6/cover.png"/></item><item><title>Everyday Object Meets Vision-and-Language Navigation Agent via Backdoor</title><link>https://deep-diver.github.io/neurips2024/posters/rxgxbdjadh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rxgxbdjadh/</guid><description>Researchers introduce object-aware backdoors in Vision-and-Language Navigation, enabling malicious behavior upon encountering specific objects, demonstrating the vulnerability of real-world AI agents.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rxgxbdjadh/cover.png"/></item><item><title>EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/28bfut6ruy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/28bfut6ruy/</guid><description>EvolveDirector trains competitive text-to-image models using publicly available data by cleverly leveraging large vision-language models to curate and refine training datasets, dramatically reducing d&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/28bfut6ruy/cover.png"/></item><item><title>Extending Multi-modal Contrastive Representations</title><link>https://deep-diver.github.io/neurips2024/posters/pqurxu9pq6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pqurxu9pq6/</guid><description>Ex-MCR: Efficiently build unified multi-modal representations by extending, not connecting, pre-trained spaces, achieving superior performance with less paired data and training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pqurxu9pq6/cover.png"/></item><item><title>Eye-gaze Guided Multi-modal Alignment for Medical Representation Learning</title><link>https://deep-diver.github.io/neurips2024/posters/0binew40u4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0binew40u4/</guid><description>Eye-gaze data boosts medical image-text alignment!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0binew40u4/cover.png"/></item><item><title>EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI Detection</title><link>https://deep-diver.github.io/neurips2024/posters/r1rrb2d5bh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/r1rrb2d5bh/</guid><description>EZ-HOI: Efficient Zero-Shot HOI detection adapts Vision-Language Models (VLMs) for Human-Object Interaction (HOI) tasks using a novel prompt learning framework, achieving state-of-the-art performance &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/r1rrb2d5bh/cover.png"/></item><item><title>Facilitating Multimodal Classification via Dynamically Learning Modality Gap</title><link>https://deep-diver.github.io/neurips2024/posters/qbspz0snyv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qbspz0snyv/</guid><description>Researchers dynamically integrate contrastive and supervised learning to overcome the modality imbalance problem in multimodal classification, significantly improving model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qbspz0snyv/cover.png"/></item><item><title>Federated Learning from Vision-Language Foundation Models: Theoretical Analysis and Method</title><link>https://deep-diver.github.io/neurips2024/posters/y4l8gqxzzo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y4l8gqxzzo/</guid><description>PromptFolio optimizes federated learning of vision-language models by combining global and local prompts, improving generalization and personalization, as proven theoretically and empirically.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y4l8gqxzzo/cover.png"/></item><item><title>Few-Shot Adversarial Prompt Learning on Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/n9xvaqmjnk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/n9xvaqmjnk/</guid><description>Few-shot adversarial prompt learning significantly improves vision-language model robustness by learning adversarially correlated text supervision and a novel training objective that enhances multi-mo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/n9xvaqmjnk/cover.png"/></item><item><title>Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/nbjmmf2izu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nbjmmf2izu/</guid><description>This paper presents a novel RL framework that fine-tunes large vision-language models (VLMs) to become effective decision-making agents. By incorporating chain-of-thought reasoning, the framework enab&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nbjmmf2izu/cover.png"/></item><item><title>FineCLIP: Self-distilled Region-based CLIP for Better Fine-grained Understanding</title><link>https://deep-diver.github.io/neurips2024/posters/nexi4fukwd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nexi4fukwd/</guid><description>FineCLIP boosts fine-grained image understanding by combining real-time self-distillation with semantically rich regional contrastive learning, significantly outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nexi4fukwd/cover.png"/></item><item><title>FineStyle: Fine-grained Controllable Style Personalization for Text-to-image Models</title><link>https://deep-diver.github.io/neurips2024/posters/1smxugzrh8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1smxugzrh8/</guid><description>FineStyle enables fine-grained controllable style personalization for text-to-image models using a novel concept-oriented data scaling and parameter-efficient adapter tuning, mitigating content leakag&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1smxugzrh8/cover.png"/></item><item><title>Flex-MoE: Modeling Arbitrary Modality Combination via the Flexible Mixture-of-Experts</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/ihehcbqzex/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/ihehcbqzex/</guid><description>Flex-MoE: A novel framework flexibly handles arbitrary modality combinations in multimodal learning, even with missing data, achieving robust performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/ihehcbqzex/cover.png"/></item><item><title>FlexCap: Describe Anything in Images in Controllable Detail</title><link>https://deep-diver.github.io/neurips2024/posters/p5dezeecgu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/p5dezeecgu/</guid><description>FlexCap generates controllable, region-specific image descriptions of varying lengths, achieving state-of-the-art zero-shot visual question answering.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/p5dezeecgu/cover.png"/></item><item><title>Flexible Context-Driven Sensory Processing in Dynamical Vision Models</title><link>https://deep-diver.github.io/neurips2024/posters/3pqhu96vvv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3pqhu96vvv/</guid><description>Biologically-inspired DCnet neural network flexibly modulates visual processing based on context, outperforming existing models on visual search and attention tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3pqhu96vvv/cover.png"/></item><item><title>Frustratingly Easy Test-Time Adaptation of Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/eq6vjbhevn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/eq6vjbhevn/</guid><description>Boost VLM performance with ZERO: a simple, fast Test-Time Adaptation method requiring only a single forward pass and exceeding state-of-the-art accuracy!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/eq6vjbhevn/cover.png"/></item><item><title>G2D: From Global to Dense Radiography Representation Learning via Vision-Language Pre-training</title><link>https://deep-diver.github.io/neurips2024/posters/zsxbgjj7oo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zsxbgjj7oo/</guid><description>G2D: a novel medical VLP framework achieves superior performance in medical image analysis by simultaneously learning global and dense visual features using image-text pairs without extra annotations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zsxbgjj7oo/cover.png"/></item><item><title>G3: An Effective and Adaptive Framework for Worldwide Geolocalization Using Large Multi-Modality Models</title><link>https://deep-diver.github.io/neurips2024/posters/21tn63ee15/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/21tn63ee15/</guid><description>G3: A novel framework leverages Retrieval-Augmented Generation to achieve highly accurate worldwide image geolocalization, overcoming limitations of existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/21tn63ee15/cover.png"/></item><item><title>GAMap: Zero-Shot Object Goal Navigation with Multi-Scale Geometric-Affordance Guidance</title><link>https://deep-diver.github.io/neurips2024/posters/ijhraldqnp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ijhraldqnp/</guid><description>GAMap: Zero-shot object goal navigation excels by using multi-scale geometric-affordance guidance, significantly boosting robot success rates in unseen environments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ijhraldqnp/cover.png"/></item><item><title>GenArtist: Multimodal LLM as an Agent for Unified Image Generation and Editing</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/ur00bnk1v2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/ur00bnk1v2/</guid><description>GenArtist uses a multimodal large language model as an AI agent to unify image generation and editing, achieving state-of-the-art performance by decomposing complex tasks and leveraging a comprehensiv&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/ur00bnk1v2/cover.png"/></item><item><title>GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/saodq13jga/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/saodq13jga/</guid><description>GITA, a novel framework, integrates visual graphs into language models for superior vision-language graph reasoning, outperforming existing LLMs and introducing the first vision-language dataset, GVLQ&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/saodq13jga/cover.png"/></item><item><title>GOMAA-Geo: GOal Modality Agnostic Active Geo-localization</title><link>https://deep-diver.github.io/neurips2024/posters/gpcesxd4b4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gpcesxd4b4/</guid><description>GOMAA-Geo, a novel framework, enables efficient and accurate goal localization using aerial imagery, regardless of goal description modality (text or images), demonstrating impressive zero-shot genera&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gpcesxd4b4/cover.png"/></item><item><title>GrounDiT: Grounding Diffusion Transformers via Noisy Patch Transplantation</title><link>https://deep-diver.github.io/neurips2024/posters/sxbyy0a3ry/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sxbyy0a3ry/</guid><description>GrounDiT: Training-free spatial grounding for text-to-image generation using Diffusion Transformers and a novel noisy patch transplantation technique for precise object placement.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sxbyy0a3ry/cover.png"/></item><item><title>GuardT2I: Defending Text-to-Image Models from Adversarial Prompts</title><link>https://deep-diver.github.io/neurips2024/posters/fmrnus3d0n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fmrnus3d0n/</guid><description>GuardT2I: A novel framework defends text-to-image models against adversarial prompts by translating latent guidance embeddings into natural language, enabling effective adversarial prompt detection wi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fmrnus3d0n/cover.png"/></item><item><title>Harmonizing Visual Text Comprehension and Generation</title><link>https://deep-diver.github.io/neurips2024/posters/fqjekshovr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fqjekshovr/</guid><description>TextHarmony: a unified multimodal model harmonizes visual text comprehension &amp;amp; generation, achieving improved performance across benchmarks with minimal parameter increase.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fqjekshovr/cover.png"/></item><item><title>HAWK: Learning to Understand Open-World Video Anomalies</title><link>https://deep-diver.github.io/neurips2024/posters/vbkoez1pg3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vbkoez1pg3/</guid><description>HAWK: a novel framework leveraging interactive VLMs and motion modality achieves state-of-the-art performance in open-world video anomaly understanding, generating descriptions and answering questions&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vbkoez1pg3/cover.png"/></item><item><title>HENASY: Learning to Assemble Scene-Entities for Interpretable Egocentric Video-Language Model</title><link>https://deep-diver.github.io/neurips2024/posters/7uwzogn4kv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7uwzogn4kv/</guid><description>HENASY, a novel egocentric video-language model, uses a compositional approach to assemble scene entities for improved interpretability and performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7uwzogn4kv/cover.png"/></item><item><title>Hierarchical Visual Feature Aggregation for OCR-Free Document Understanding</title><link>https://deep-diver.github.io/neurips2024/posters/pwkjxjgglp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pwkjxjgglp/</guid><description>This paper introduces HVFA, a novel OCR-free document understanding framework using MLLMs and multi-scale visual features, achieving superior performance across various document understanding tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pwkjxjgglp/cover.png"/></item><item><title>Homology Consistency Constrained Efficient Tuning for Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/vemngkxvtx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vemngkxvtx/</guid><description>Constraining vision-language model tuning via persistent homology ensures consistent image-text alignment, improving few-shot learning and domain generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vemngkxvtx/cover.png"/></item><item><title>How Control Information Influences Multilingual Text Image Generation and Editing?</title><link>https://deep-diver.github.io/neurips2024/posters/r3c0wgcxgt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/r3c0wgcxgt/</guid><description>TextGen enhances multilingual visual text generation and editing by optimizing control information using Fourier analysis and a two-stage framework, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/r3c0wgcxgt/cover.png"/></item><item><title>HumanVLA: Towards Vision-Language Directed Object Rearrangement by Physical Humanoid</title><link>https://deep-diver.github.io/neurips2024/posters/pjd08dtah0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pjd08dtah0/</guid><description>Humanoid robot learns to rearrange objects using vision and language instructions, achieving remarkable success on diverse tasks in a novel dataset.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pjd08dtah0/cover.png"/></item><item><title>I2EBench: A Comprehensive Benchmark for Instruction-based Image Editing</title><link>https://deep-diver.github.io/neurips2024/posters/1dpmeh6iha/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1dpmeh6iha/</guid><description>I2EBench: a new benchmark for Instruction-based Image Editing provides a comprehensive evaluation framework using 16 dimensions, aligned with human perception, to evaluate IIE models objectively.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1dpmeh6iha/cover.png"/></item><item><title>Implicit Multimodal Alignment: On the Generalization of Frozen LLMs to Multimodal Inputs</title><link>https://deep-diver.github.io/neurips2024/posters/9622qfvsab/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/9622qfvsab/</guid><description>Frozen LLMs surprisingly excel at multimodal tasks; this paper reveals that their success stems from an implicit multimodal alignment effect, paving the way for efficient LMMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/9622qfvsab/cover.png"/></item><item><title>InstructG2I: Synthesizing Images from Multimodal Attributed Graphs</title><link>https://deep-diver.github.io/neurips2024/posters/zwnw4zqkum/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zwnw4zqkum/</guid><description>INSTRUCTG2I: a novel graph context-conditioned diffusion model, generates images from multimodal attributed graphs, addressing challenges in graph size, dependencies, and controllability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zwnw4zqkum/cover.png"/></item><item><title>Instruction-Guided Visual Masking</title><link>https://deep-diver.github.io/neurips2024/posters/ca9glxfaro/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ca9glxfaro/</guid><description>Instruction-Guided Visual Masking (IVM) boosts multimodal instruction following by precisely focusing models on relevant image regions via visual masking, achieving state-of-the-art results on multipl&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ca9glxfaro/cover.png"/></item><item><title>InterControl: Zero-shot Human Interaction Generation by Controlling Every Joint</title><link>https://deep-diver.github.io/neurips2024/posters/ah1mfs3c7o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ah1mfs3c7o/</guid><description>InterControl: Zero-shot multi-person interaction generation by precisely controlling every joint using only single-person data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ah1mfs3c7o/cover.png"/></item><item><title>InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction</title><link>https://deep-diver.github.io/neurips2024/posters/bupxpo80qp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bupxpo80qp/</guid><description>InterDreamer: Zero-shot text-guided 3D human-object interaction generation without paired data, achieved via decoupled semantic and dynamic modeling, using LLMs and a physics-based world model.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bupxpo80qp/cover.png"/></item><item><title>Interfacing Foundation Models' Embeddings</title><link>https://deep-diver.github.io/neurips2024/posters/u3hqoqgqdj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/u3hqoqgqdj/</guid><description>FIND, a lightweight transformer interface, seamlessly aligns foundation models&amp;rsquo; embeddings for unified image and dataset-level understanding, enabling generalizable, interleaved performance on segment&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/u3hqoqgqdj/cover.png"/></item><item><title>InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD</title><link>https://deep-diver.github.io/neurips2024/posters/nrp0xhtf61/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nrp0xhtf61/</guid><description>InternLM-XComposer2-4KHD pioneers high-resolution image understanding in LVLMs, scaling processing from 336 pixels to 4K HD and beyond, achieving state-of-the-art results on multiple benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nrp0xhtf61/cover.png"/></item><item><title>Interpreting and Analysing CLIP's Zero-Shot Image Classification via Mutual Knowledge</title><link>https://deep-diver.github.io/neurips2024/posters/n01yluy7mj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/n01yluy7mj/</guid><description>CLIP&amp;rsquo;s zero-shot image classification decisions are made interpretable using a novel mutual-knowledge approach based on textual concepts, demonstrating effective and human-friendly analysis across div&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/n01yluy7mj/cover.png"/></item><item><title>Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)</title><link>https://deep-diver.github.io/neurips2024/posters/7uybktfrtd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7uybktfrtd/</guid><description>SpLiCE unlocks CLIP&amp;rsquo;s potential by transforming its dense, opaque representations into sparse, human-interpretable concept embeddings.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7uybktfrtd/cover.png"/></item><item><title>IPO: Interpretable Prompt Optimization for Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/wppc7fhtam/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wppc7fhtam/</guid><description>This paper introduces IPO, a novel interpretable prompt optimizer for vision-language models. IPO uses large language models (LLMs) to dynamically generate human-understandable prompts, improving acc&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wppc7fhtam/cover.png"/></item><item><title>Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/cvasru8leo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cvasru8leo/</guid><description>SpatialEval benchmark reveals that current vision-language models struggle with spatial reasoning, highlighting the need for improved multimodal models that effectively integrate visual and textual in&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cvasru8leo/cover.png"/></item><item><title>Jointly Modeling Inter- &amp; Intra-Modality Dependencies for Multi-modal Learning</title><link>https://deep-diver.github.io/neurips2024/posters/xakalzi3gw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xakalzi3gw/</guid><description>I2M2: A novel framework revolutionizes multi-modal learning by jointly modeling inter- and intra-modality dependencies, achieving superior performance across diverse real-world datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xakalzi3gw/cover.png"/></item><item><title>Kaleido Diffusion: Improving Conditional Diffusion Models with Autoregressive Latent Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/qzswlclmcs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qzswlclmcs/</guid><description>Kaleido Diffusion boosts the diversity of images generated by diffusion models without sacrificing quality, using autoregressive latent modeling to add more control and interpretability to the image g&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qzswlclmcs/cover.png"/></item><item><title>LaSe-E2V: Towards Language-guided Semantic-aware Event-to-Video Reconstruction</title><link>https://deep-diver.github.io/neurips2024/posters/3ilqqhbwtf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3ilqqhbwtf/</guid><description>LaSe-E2V: Language-guided semantic-aware event-to-video reconstruction uses text descriptions to improve video quality and consistency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3ilqqhbwtf/cover.png"/></item><item><title>Learning 1D Causal Visual Representation with De-focus Attention Networks</title><link>https://deep-diver.github.io/neurips2024/posters/lxrmdxf72k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lxrmdxf72k/</guid><description>De-focus Attention Networks achieve comparable performance to 2D non-causal models using 1D causal visual representation, solving the &amp;lsquo;over-focus&amp;rsquo; issue in existing 1D causal vision models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lxrmdxf72k/cover.png"/></item><item><title>Learning Cortico-Muscular Dependence through Orthonormal Decomposition of Density Ratios</title><link>https://deep-diver.github.io/neurips2024/posters/wdgvrud1ls/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wdgvrud1ls/</guid><description>Unveiling cortico-muscular dependence using orthonormal decomposition of density ratios, FMCA-T, enhances movement classification and reveals channel-temporal dependencies.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wdgvrud1ls/cover.png"/></item><item><title>Learning to Reason Iteratively and Parallelly for Complex Visual Reasoning Scenarios</title><link>https://deep-diver.github.io/neurips2024/posters/uojq9qadjy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uojq9qadjy/</guid><description>Boosting complex visual reasoning, a new Iterative and Parallel Reasoning Mechanism (IPRM) outperforms existing methods by combining step-by-step and simultaneous computations, improving accuracy and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uojq9qadjy/cover.png"/></item><item><title>LESS: Label-Efficient and Single-Stage Referring 3D Segmentation</title><link>https://deep-diver.github.io/neurips2024/posters/hrqaot0nzf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hrqaot0nzf/</guid><description>LESS achieves state-of-the-art Referring 3D Segmentation using only binary masks, significantly reducing labeling effort and improving efficiency with a novel single-stage pipeline.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hrqaot0nzf/cover.png"/></item><item><title>Lever LM: Configuring In-Context Sequence to Lever Large Vision Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/tgc7hnf6nk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tgc7hnf6nk/</guid><description>Lever-LM configures effective in-context demonstrations for large vision-language models using a small language model, significantly improving their performance on visual question answering and image &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tgc7hnf6nk/cover.png"/></item><item><title>Leveraging Visual Tokens for Extended Text Contexts in Multi-Modal Learning</title><link>https://deep-diver.github.io/neurips2024/posters/wy3xgxizur/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wy3xgxizur/</guid><description>Visual tokens boost long-text multi-modal models!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wy3xgxizur/cover.png"/></item><item><title>LG-CAV: Train Any Concept Activation Vector with Language Guidance</title><link>https://deep-diver.github.io/neurips2024/posters/mjd9y05q6i/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mjd9y05q6i/</guid><description>LG-CAV: Train any Concept Activation Vector with Language Guidance, leverages vision-language models to train CAVs without labeled data, achieving superior accuracy and enabling state-of-the-art model&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mjd9y05q6i/cover.png"/></item><item><title>LG-VQ: Language-Guided Codebook Learning</title><link>https://deep-diver.github.io/neurips2024/posters/va4s3kn4qe/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/va4s3kn4qe/</guid><description>LG-VQ: A novel language-guided codebook learning framework boosts multi-modal performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/va4s3kn4qe/cover.png"/></item><item><title>Lips Are Lying: Spotting the Temporal Inconsistency between Audio and Visual in Lip-Syncing DeepFakes</title><link>https://deep-diver.github.io/neurips2024/posters/yms7ansbr6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yms7ansbr6/</guid><description>LipFD: a novel method leverages audio-visual inconsistencies to accurately spot lip-syncing deepfakes, outperforming existing methods and introducing a high-quality dataset for future research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yms7ansbr6/cover.png"/></item><item><title>LiveScene: Language Embedding Interactive Radiance Fields for Physical Scene Control and Rendering</title><link>https://deep-diver.github.io/neurips2024/posters/jkt42qyyeh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jkt42qyyeh/</guid><description>LiveScene: Language-embedded interactive radiance fields efficiently reconstruct and control complex scenes with multiple interactive objects, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jkt42qyyeh/cover.png"/></item><item><title>LocCa: Visual Pretraining with Location-aware Captioners</title><link>https://deep-diver.github.io/neurips2024/posters/jfhkaegkwh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jfhkaegkwh/</guid><description>LocCa, a novel visual pretraining paradigm, uses location-aware captioning tasks to boost downstream localization performance while maintaining holistic task capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jfhkaegkwh/cover.png"/></item><item><title>LoTLIP: Improving Language-Image Pre-training for Long Text Understanding</title><link>https://deep-diver.github.io/neurips2024/posters/pc4gsbi1hx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pc4gsbi1hx/</guid><description>LoTLIP boosts language-image pre-training for superior long text understanding by cleverly integrating corner tokens and utilizing a massive dataset of 100M long-caption images.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pc4gsbi1hx/cover.png"/></item><item><title>LOVA3: Learning to Visual Question Answering, Asking and Assessment</title><link>https://deep-diver.github.io/neurips2024/posters/vioklml6wu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vioklml6wu/</guid><description>LOVA³ enhances MLLMs by teaching them to ask and assess image-based questions, improving their multimodal understanding and performance on various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vioklml6wu/cover.png"/></item><item><title>Lumen: Unleashing Versatile Vision-Centric Capabilities of Large Multimodal Models</title><link>https://deep-diver.github.io/neurips2024/posters/v5un2qqnrf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v5un2qqnrf/</guid><description>Lumen: A novel LMM architecture decouples perception learning into task-agnostic and task-specific stages, enabling versatile vision-centric capabilities and surpassing existing LMM-based approaches.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v5un2qqnrf/cover.png"/></item><item><title>M$^3$GPT: An Advanced Multimodal, Multitask Framework for Motion Comprehension and Generation</title><link>https://deep-diver.github.io/neurips2024/posters/odbtlas0oj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/odbtlas0oj/</guid><description>M³GPT, a novel multimodal framework, achieves superior motion comprehension and generation by integrating text, music, and motion data into a unified LLM representation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/odbtlas0oj/cover.png"/></item><item><title>Magnet: We Never Know How Text-to-Image Diffusion Models Work, Until We Learn How Vision-Language Models Function</title><link>https://deep-diver.github.io/neurips2024/posters/4mzgimooxm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4mzgimooxm/</guid><description>Magnet: Enhancing Text-to-Image Synthesis by Disentangling Attributes in CLIP.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4mzgimooxm/cover.png"/></item><item><title>Make-it-Real: Unleashing Large Multimodal Model for Painting 3D Objects with Realistic Materials</title><link>https://deep-diver.github.io/neurips2024/posters/88rbnotaez/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/88rbnotaez/</guid><description>Make-it-Real uses a large multimodal language model to automatically paint realistic materials onto 3D objects, drastically improving realism and saving developers time.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/88rbnotaez/cover.png"/></item><item><title>Matryoshka Query Transformer for Large Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/b1vgisgelw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/b1vgisgelw/</guid><description>Matryoshka Query Transformer (MQT) empowers large vision-language models with flexible visual token encoding, drastically reducing inference costs while maintaining high accuracy across multiple bench&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/b1vgisgelw/cover.png"/></item><item><title>MaVEn: An Effective Multi-granularity Hybrid Visual Encoding Framework for Multimodal Large Language Model</title><link>https://deep-diver.github.io/neurips2024/posters/fhxmoekqbh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fhxmoekqbh/</guid><description>MaVEn: A novel multi-granularity hybrid visual encoding framework significantly boosts MLLM&amp;rsquo;s multi-image reasoning capabilities by combining discrete and continuous visual representations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fhxmoekqbh/cover.png"/></item><item><title>Membership Inference Attacks against Large Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/nv2qt5cj1a/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nv2qt5cj1a/</guid><description>First benchmark for detecting training data in large vision-language models (VLLMs) improves data security.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nv2qt5cj1a/cover.png"/></item><item><title>MemVLT: Vision-Language Tracking with Adaptive Memory-based Prompts</title><link>https://deep-diver.github.io/neurips2024/posters/zk1czxkgg5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zk1czxkgg5/</guid><description>MemVLT: Adaptive Vision-Language Tracking leverages memory to generate dynamic prompts, surpassing existing methods by adapting to changing target appearances.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zk1czxkgg5/cover.png"/></item><item><title>Meteor: Mamba-based Traversal of Rationale for Large Language and Vision Models</title><link>https://deep-diver.github.io/neurips2024/posters/jvkabhr6mp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jvkabhr6mp/</guid><description>Meteor: Mamba-based Traversal of Rationale achieves significant vision-language improvements by efficiently embedding multifaceted rationales in a large language model, without scaling the model or us&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jvkabhr6mp/cover.png"/></item><item><title>Mitigating Object Hallucination via Concentric Causal Attention</title><link>https://deep-diver.github.io/neurips2024/posters/cirpe1bsmv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cirpe1bsmv/</guid><description>Concentric Causal Attention (CCA) significantly reduces object hallucination in LVLMs by cleverly reorganizing visual tokens to mitigate the impact of long-term decay in Rotary Position Encoding.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cirpe1bsmv/cover.png"/></item><item><title>MMSite: A Multi-modal Framework for the Identification of Active Sites in Proteins</title><link>https://deep-diver.github.io/neurips2024/posters/xhdwlbnsvb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xhdwlbnsvb/</guid><description>MMSite: a novel multi-modal framework accurately identifies protein active sites using protein sequences and textual descriptions, achieving state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xhdwlbnsvb/cover.png"/></item><item><title>Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration</title><link>https://deep-diver.github.io/neurips2024/posters/o0nbmrlkc8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/o0nbmrlkc8/</guid><description>Mobile-Agent-v2 uses a three-agent collaborative framework (planning, decision, reflection) to improve mobile device operation accuracy by over 30%, overcoming the limitations of single-agent architec&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/o0nbmrlkc8/cover.png"/></item><item><title>MoGenTS: Motion Generation based on Spatial-Temporal Joint Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/fisyqfojcm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fisyqfojcm/</guid><description>MoGenTS revolutionizes human motion generation by quantizing individual joints into 2D tokens, enabling efficient spatial-temporal modeling and significantly outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fisyqfojcm/cover.png"/></item><item><title>MoLE: Enhancing Human-centric Text-to-image Diffusion via Mixture of Low-rank Experts</title><link>https://deep-diver.github.io/neurips2024/posters/xwzw2dsjwd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xwzw2dsjwd/</guid><description>MoLE: Mixture of Low-rank Experts enhances human-centric text-to-image diffusion models by using low-rank modules trained on high-quality face and hand datasets to improve the realism of faces and han&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xwzw2dsjwd/cover.png"/></item><item><title>MoME: Mixture of Multimodal Experts for Generalist Multimodal Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/xskl7da34u/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xskl7da34u/</guid><description>MoME, a novel Mixture of Multimodal Experts, significantly improves generalist Multimodal Large Language Models (MLLMs) by mitigating task interference through specialized vision and language experts,&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xskl7da34u/cover.png"/></item><item><title>MoVA: Adapting Mixture of Vision Experts to Multimodal Context</title><link>https://deep-diver.github.io/neurips2024/posters/uhs6rjfdsg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uhs6rjfdsg/</guid><description>MoVA, a novel MLLM, enhances multimodal understanding by adaptively routing and fusing task-specific vision experts for improved generalization across diverse image content.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uhs6rjfdsg/cover.png"/></item><item><title>Multi-modal Transfer Learning between Biological Foundation Models</title><link>https://deep-diver.github.io/neurips2024/posters/ximejtduiw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ximejtduiw/</guid><description>IsoFormer, a novel multi-modal model, accurately predicts RNA transcript isoform expression by integrating DNA, RNA, and protein sequence information, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ximejtduiw/cover.png"/></item><item><title>Multi-Object 3D Grounding with Dynamic Modules and Language-Informed Spatial Attention</title><link>https://deep-diver.github.io/neurips2024/posters/jfwl9ewz7z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jfwl9ewz7z/</guid><description>D-LISA: Dynamic modules &amp;amp; language-informed spatial attention revolutionizes multi-object 3D grounding, surpassing state-of-the-art accuracy by 12.8%.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jfwl9ewz7z/cover.png"/></item><item><title>Multi-Object Hallucination in Vision Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/knrwafei1u/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/knrwafei1u/</guid><description>LVLMs often hallucinate objects, a problem worsened when multiple objects are present. This paper introduces ROPE, a novel automated evaluation protocol that reveals how object class distribution and&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/knrwafei1u/cover.png"/></item><item><title>Multilingual Diversity Improves Vision-Language Representations</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/1wteqrecys/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/1wteqrecys/</guid><description>Boosting vision-language models: Multilingual data improves performance on English-centric benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/1wteqrecys/cover.png"/></item><item><title>Multimodal Large Language Models Make Text-to-Image Generative Models Align Better</title><link>https://deep-diver.github.io/neurips2024/posters/irxypm9ipw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/irxypm9ipw/</guid><description>AI-generated preference data improves text-to-image alignment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/irxypm9ipw/cover.png"/></item><item><title>Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning</title><link>https://deep-diver.github.io/neurips2024/posters/w0oktgspvm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w0oktgspvm/</guid><description>Large Multimodal Models (LMMs) are limited by their context length during many-shot in-context learning. This paper introduces Multimodal Task Vectors (MTV), a method to compress numerous in-context &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w0oktgspvm/cover.png"/></item><item><title>Neuro-Vision to Language: Enhancing Brain Recording-based Visual Reconstruction and Language Interaction</title><link>https://deep-diver.github.io/neurips2024/posters/ohi00yht3t/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ohi00yht3t/</guid><description>Researchers enhanced brain recording-based visual reconstruction using a novel Vision Transformer 3D framework integrated with LLMs, achieving superior performance in visual reconstruction, captioning&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ohi00yht3t/cover.png"/></item><item><title>No 'Zero-Shot' Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance</title><link>https://deep-diver.github.io/neurips2024/posters/9vbgjxlzig/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/9vbgjxlzig/</guid><description>Multimodal models&amp;rsquo; impressive &amp;lsquo;zero-shot&amp;rsquo; performance hinges on the frequency of concepts in their training data, not inherent generalization ability; exponentially more data is needed for linear impr&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/9vbgjxlzig/cover.png"/></item><item><title>No Filter: Cultural and Socioeconomic Diversity in Contrastive Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/umw9byj761/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/umw9byj761/</guid><description>Contrastive vision-language models (VLMs) trained only on English data significantly underperform on culturally diverse benchmarks. This paper reveals this bias, proposes novel evaluation metrics, and&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/umw9byj761/cover.png"/></item><item><title>Novel Object Synthesis via Adaptive Text-Image Harmony</title><link>https://deep-diver.github.io/neurips2024/posters/enlsndfys0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/enlsndfys0/</guid><description>Researchers created a novel object synthesis method, Adaptive Text-Image Harmony (ATIH), that harmoniously blends image and text inputs to generate creative, composite objects.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/enlsndfys0/cover.png"/></item><item><title>Octopus: A Multi-modal LLM with Parallel Recognition and Sequential Understanding</title><link>https://deep-diver.github.io/neurips2024/posters/uje83r50tr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uje83r50tr/</guid><description>Octopus, a novel multi-modal LLM, uses parallel visual recognition and sequential understanding to achieve 5x speedup on visual grounding and improved accuracy on various MLLM tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uje83r50tr/cover.png"/></item><item><title>OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding</title><link>https://deep-diver.github.io/neurips2024/posters/weond6prqs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/weond6prqs/</guid><description>OMG-LLaVA: A single model elegantly bridges image, object, and pixel-level reasoning for superior visual understanding.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/weond6prqs/cover.png"/></item><item><title>OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents</title><link>https://deep-diver.github.io/neurips2024/posters/ceio1w0pmt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ceio1w0pmt/</guid><description>OmniJARVIS: Unified vision-language-action tokenization enables open-world instruction-following agents via unified multimodal interaction data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ceio1w0pmt/cover.png"/></item><item><title>OmniTokenizer: A Joint Image-Video Tokenizer for Visual Generation</title><link>https://deep-diver.github.io/neurips2024/posters/h6c4p8dir7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/h6c4p8dir7/</guid><description>OmniTokenizer: A transformer-based tokenizer achieving state-of-the-art image and video reconstruction by leveraging a novel spatial-temporal decoupled architecture and progressive training strategy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/h6c4p8dir7/cover.png"/></item><item><title>On the Comparison between Multi-modal and Single-modal Contrastive Learning</title><link>https://deep-diver.github.io/neurips2024/posters/o2uwxfhy1p/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/o2uwxfhy1p/</guid><description>Multi-modal contrastive learning surpasses single-modal by leveraging inter-modal correlations to improve feature learning and downstream task performance, as demonstrated through a novel theoretical &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/o2uwxfhy1p/cover.png"/></item><item><title>OneRef: Unified One-tower Expression Grounding and Segmentation with Mask Referring Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/sipdcro6ud/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sipdcro6ud/</guid><description>OneRef: Unified one-tower model surpasses existing methods in visual grounding and segmentation by leveraging a novel Mask Referring Modeling paradigm.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sipdcro6ud/cover.png"/></item><item><title>Pandora's Box: Towards Building Universal Attackers against Real-World Large Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/gdpwypoce1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gdpwypoce1/</guid><description>Researchers developed a universal adversarial patch to fool real-world large vision-language models (LVLMs) across multiple tasks, without needing access to internal model details.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gdpwypoce1/cover.png"/></item><item><title>ParallelEdits: Efficient Multi-Aspect Text-Driven Image Editing with Attention Grouping</title><link>https://deep-diver.github.io/neurips2024/posters/ccl92opldz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ccl92opldz/</guid><description>ParallelEdits efficiently edits multiple image aspects simultaneously, guided by text prompts, surpassing sequential methods in speed and accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ccl92opldz/cover.png"/></item><item><title>PERIA: Perceive, Reason, Imagine, Act via Holistic Language and Vision Planning for Manipulation</title><link>https://deep-diver.github.io/neurips2024/posters/zw2k6lffi9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zw2k6lffi9/</guid><description>PERIA: Holistic language &amp;amp; vision planning for complex robotic manipulation!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zw2k6lffi9/cover.png"/></item><item><title>PIVOT-R: Primitive-Driven Waypoint-Aware World Model for Robotic Manipulation</title><link>https://deep-diver.github.io/neurips2024/posters/gnxtdqyxlu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gnxtdqyxlu/</guid><description>PIVOT-R, a novel primitive-driven waypoint-aware world model, significantly boosts robotic manipulation performance and efficiency via an asynchronous hierarchical executor.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gnxtdqyxlu/cover.png"/></item><item><title>Pre-trained Text-to-Image Diffusion Models Are Versatile Representation Learners for Control</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/ky07a73f3y/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/ky07a73f3y/</guid><description>Pre-trained text-to-image diffusion models create highly effective, versatile representations for embodied AI control, surpassing previous methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/ky07a73f3y/cover.png"/></item><item><title>Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs</title><link>https://deep-diver.github.io/neurips2024/posters/qlnxpvvwlx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qlnxpvvwlx/</guid><description>Prism: a novel framework disentangles perception and reasoning in Vision-Language Models (VLMs) for improved model assessment and efficient VLM development.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qlnxpvvwlx/cover.png"/></item><item><title>Procedure-Aware Surgical Video-language Pretraining with Hierarchical Knowledge Augmentation</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/zuwperkjnh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/zuwperkjnh/</guid><description>PeskaVLP: Hierarchical knowledge augmentation boosts surgical video-language pretraining!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/zuwperkjnh/cover.png"/></item><item><title>Propensity Score Alignment of Unpaired Multimodal Data</title><link>https://deep-diver.github.io/neurips2024/posters/ht4y7d2o2t/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ht4y7d2o2t/</guid><description>Unlocking multimodal learning&amp;rsquo;s potential with propensity scores: This novel approach aligns unpaired data across modalities, significantly improving representation learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ht4y7d2o2t/cover.png"/></item><item><title>Q-VLM: Post-training Quantization for Large Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/gxmfnarldp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gxmfnarldp/</guid><description>Q-VLM: A novel post-training quantization framework significantly compresses large vision-language models, boosting inference speed without sacrificing accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gxmfnarldp/cover.png"/></item><item><title>QUEST: Quadruple Multimodal Contrastive Learning with Constraints and Self-Penalization</title><link>https://deep-diver.github.io/neurips2024/posters/la48h7pw3q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/la48h7pw3q/</guid><description>QUEST: Quadruple Multimodal Contrastive Learning tackles feature suppression by using quaternion embedding to extract unique information while penalizing excessive shared information influence, achiev&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/la48h7pw3q/cover.png"/></item><item><title>RaVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/ufrzhfyw8e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ufrzhfyw8e/</guid><description>RAVL: a novel approach that accurately discovers and effectively mitigates spurious correlations in fine-tuned vision-language models, improving zero-shot classification accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ufrzhfyw8e/cover.png"/></item><item><title>Recognize Any Regions</title><link>https://deep-diver.github.io/neurips2024/posters/qkfiwnhp6k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qkfiwnhp6k/</guid><description>RegionSpot efficiently integrates pretrained localization and vision-language models for superior open-world object recognition, achieving significant performance gains with minimal training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qkfiwnhp6k/cover.png"/></item><item><title>Referencing Where to Focus: Improving Visual Grounding with Referential Query</title><link>https://deep-diver.github.io/neurips2024/posters/opvbnptbqv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/opvbnptbqv/</guid><description>RefFormer boosts visual grounding accuracy by intelligently adapting queries using multi-level image features, effectively guiding the decoder towards the target object.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/opvbnptbqv/cover.png"/></item><item><title>Referring Human Pose and Mask Estimation In the Wild</title><link>https://deep-diver.github.io/neurips2024/posters/fxei3lvflp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fxei3lvflp/</guid><description>RefHuman: a new dataset and UniPHD model achieve state-of-the-art referring human pose and mask estimation in the wild, using text or positional prompts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fxei3lvflp/cover.png"/></item><item><title>Renovating Names in Open-Vocabulary Segmentation Benchmarks</title><link>https://deep-diver.github.io/neurips2024/posters/uw2ejoi822/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uw2ejoi822/</guid><description>RENOVATE renovates open-vocabulary segmentation benchmarks by automatically improving class names, leading to stronger models and more accurate evaluations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uw2ejoi822/cover.png"/></item><item><title>ReplaceAnything3D: Text-Guided Object Replacement in 3D Scenes with Compositional Scene Representations</title><link>https://deep-diver.github.io/neurips2024/posters/8hwi6uavyc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8hwi6uavyc/</guid><description>ReplaceAnything3D (RAM3D) revolutionizes 3D scene editing with a text-guided, multi-view consistent approach for seamlessly replacing or adding 3D objects in complex scenes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8hwi6uavyc/cover.png"/></item><item><title>Rethinking Human Evaluation Protocol for Text-to-Video Models: Enhancing Reliability, Reproducibility, and Practicality</title><link>https://deep-diver.github.io/neurips2024/posters/0awmcinshl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0awmcinshl/</guid><description>This paper presents Text-to-Video Human Evaluation (T2VHE), a new protocol for evaluating text-to-video models, improving reliability, reproducibility, and practicality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0awmcinshl/cover.png"/></item><item><title>Rethinking Misalignment in Vision-Language Model Adaptation from a Causal Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/vwgwbcxeaq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vwgwbcxeaq/</guid><description>Vision-language model adaptation struggles with misalignment; this paper introduces Causality-Guided Semantic Decoupling and Classification (CDC) to mitigate this, boosting performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vwgwbcxeaq/cover.png"/></item><item><title>Right this way: Can VLMs Guide Us to See More to Answer Questions?</title><link>https://deep-diver.github.io/neurips2024/posters/7anmkbfp88/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7anmkbfp88/</guid><description>VLMs struggle with insufficient visual info for Q&amp;amp;A; this work introduces a novel Directional Guidance task and a data augmentation framework, significantly improving VLM performance by teaching them &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7anmkbfp88/cover.png"/></item><item><title>RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation</title><link>https://deep-diver.github.io/neurips2024/posters/jxoqeg1nkh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jxoqeg1nkh/</guid><description>RoboMamba: a novel robotic VLA model efficiently combines reasoning and action, achieving high speeds and accuracy while requiring minimal fine-tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jxoqeg1nkh/cover.png"/></item><item><title>Robust Fine-tuning of Zero-shot Models via Variance Reduction</title><link>https://deep-diver.github.io/neurips2024/posters/vitulzvpdu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vitulzvpdu/</guid><description>Variance Reduction Fine-tuning (VRF) simultaneously boosts in-distribution and out-of-distribution accuracy in fine-tuned zero-shot models, overcoming the ID-OOD trade-off.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vitulzvpdu/cover.png"/></item><item><title>RSA: Resolving Scale Ambiguities in Monocular Depth Estimators through Language Descriptions</title><link>https://deep-diver.github.io/neurips2024/posters/vh7gcadhao/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vh7gcadhao/</guid><description>RSA: Language unlocks metric depth from single images!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vh7gcadhao/cover.png"/></item><item><title>Scene Graph Generation with Role-Playing Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/xprui8amtc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xprui8amtc/</guid><description>SDSGG outperforms leading scene graph generation methods by using LLMs to create scene-specific descriptions, adapting to diverse visual relations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xprui8amtc/cover.png"/></item><item><title>SceneCraft: Layout-Guided 3D Scene Generation</title><link>https://deep-diver.github.io/neurips2024/posters/ctvxvacsjn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ctvxvacsjn/</guid><description>SceneCraft generates highly detailed indoor scenes from user-provided textual descriptions and spatial layouts, overcoming limitations of previous text-to-3D methods in scale and control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ctvxvacsjn/cover.png"/></item><item><title>SearchLVLMs: A Plug-and-Play Framework for Augmenting Large Vision-Language Models by Searching Up-to-Date Internet Knowledge</title><link>https://deep-diver.github.io/neurips2024/posters/leeosk2ram/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/leeosk2ram/</guid><description>SearchLVLMs: A plug-and-play framework efficiently augments large vision-language models with up-to-date internet knowledge via hierarchical filtering, significantly improving accuracy on visual quest&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/leeosk2ram/cover.png"/></item><item><title>Seeing Beyond the Crop: Using Language Priors for Out-of-Bounding Box Keypoint Prediction</title><link>https://deep-diver.github.io/neurips2024/posters/lgus3wxpxc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lgus3wxpxc/</guid><description>TokenCLIPose leverages language priors to predict human keypoints beyond bounding boxes, improving pose estimation accuracy significantly on ice hockey, lacrosse and CrowdPose datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lgus3wxpxc/cover.png"/></item><item><title>Seeing the Image: Prioritizing Visual Correlation by Contrastive Alignment</title><link>https://deep-diver.github.io/neurips2024/posters/nsxthtvpqa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nsxthtvpqa/</guid><description>Boosting vision-language model performance, Contrastive ALignment (CAL) prioritizes visually correlated text tokens during training via a simple, computationally efficient re-weighting strategy, signi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nsxthtvpqa/cover.png"/></item><item><title>Self-Calibrated Tuning of Vision-Language Models for Out-of-Distribution Detection</title><link>https://deep-diver.github.io/neurips2024/posters/w6vbfsc1y0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w6vbfsc1y0/</guid><description>Self-Calibrated Tuning (SCT) enhances vision-language model OOD detection by adaptively weighting OOD regularization based on prediction uncertainty, mitigating issues caused by inaccurate feature ext&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w6vbfsc1y0/cover.png"/></item><item><title>SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data</title><link>https://deep-diver.github.io/neurips2024/posters/t9gnehreht/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/t9gnehreht/</guid><description>SELMA boosts text-to-image fidelity by merging skill-specific models trained on automatically generated image-text datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/t9gnehreht/cover.png"/></item><item><title>SeTAR: Out-of-Distribution Detection with Selective Low-Rank Approximation</title><link>https://deep-diver.github.io/neurips2024/posters/65uoj0z7kp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/65uoj0z7kp/</guid><description>SeTAR: Training-free OOD detection via selective low-rank approximation, improving robustness and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/65uoj0z7kp/cover.png"/></item><item><title>Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/jhqyeppmid/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jhqyeppmid/</guid><description>Shadowcast: A new data poisoning attack manipulates vision-language models by injecting visually similar, yet deceptively misleading, image-text pairs, causing them to generate false information.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jhqyeppmid/cover.png"/></item><item><title>SimVG: A Simple Framework for Visual Grounding with Decoupled Multi-modal Fusion</title><link>https://deep-diver.github.io/neurips2024/posters/folnl52q5u/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/folnl52q5u/</guid><description>SimVG: A simpler, faster visual grounding framework with decoupled multi-modal fusion, achieving state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/folnl52q5u/cover.png"/></item><item><title>Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/ynx7ai4zts/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ynx7ai4zts/</guid><description>Single Image Unlearning (SIU) efficiently removes visual data from Multimodal Large Language Models (MLLMs) using only one image per concept, outperforming existing methods and defending against attac&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ynx7ai4zts/cover.png"/></item><item><title>Slot-VLM: Object-Event Slots for Video-Language Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/7hb03vgcjk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7hb03vgcjk/</guid><description>Slot-VLM generates semantically decomposed video tokens using an Object-Event Slots module, improving video-language model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7hb03vgcjk/cover.png"/></item><item><title>SlowFocus: Enhancing Fine-grained Temporal Understanding in Video LLM</title><link>https://deep-diver.github.io/neurips2024/posters/fokkndty5b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fokkndty5b/</guid><description>SlowFocus significantly improves fine-grained temporal understanding in video LLMs by using mixed-frequency sampling and a novel multi-frequency attention mechanism.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fokkndty5b/cover.png"/></item><item><title>SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy Segment Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/xcf2vbyzts/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xcf2vbyzts/</guid><description>SocialGPT cleverly leverages Vision Foundation Models and Large Language Models for zero-shot social relation reasoning, achieving competitive results and offering interpretable outputs via prompt opt&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xcf2vbyzts/cover.png"/></item><item><title>SpatialPIN: Enhancing Spatial Reasoning Capabilities of Vision-Language Models through Prompting and Interacting 3D Priors</title><link>https://deep-diver.github.io/neurips2024/posters/ythj8o6scb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ythj8o6scb/</guid><description>SpatialPIN boosts vision-language models&amp;rsquo; spatial reasoning by cleverly combining prompting techniques with 3D foundation models, achieving zero-shot performance on various spatial tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ythj8o6scb/cover.png"/></item><item><title>SpatialRGPT: Grounded Spatial Reasoning in Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/jkeiyqusuc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jkeiyqusuc/</guid><description>SpatialRGPT enhances Vision-Language Models&amp;rsquo; spatial reasoning by integrating 3D scene graphs and depth information, achieving significant performance gains on spatial reasoning tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jkeiyqusuc/cover.png"/></item><item><title>Stabilizing Zero-Shot Prediction: A Novel Antidote to Forgetting in Continual Vision-Language Tasks</title><link>https://deep-diver.github.io/neurips2024/posters/c4zmr2kyp8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/c4zmr2kyp8/</guid><description>ZAF: a novel replay-free continual learning method for vision-language models, significantly reduces forgetting by stabilizing zero-shot predictions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/c4zmr2kyp8/cover.png"/></item><item><title>Subject-driven Text-to-Image Generation via Preference-based Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/twevq5memw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/twevq5memw/</guid><description>A-Harmonic reward function and Reward Preference Optimization (RPO) improve subject-driven text-to-image generation by enabling faster training and state-of-the-art results with a simpler setup.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/twevq5memw/cover.png"/></item><item><title>Synergistic Dual Spatial-aware Generation of Image-to-text and Text-to-image</title><link>https://deep-diver.github.io/neurips2024/posters/youh3lgryi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/youh3lgryi/</guid><description>Synergistic Dual Spatial-aware Generation boosts image-to-text and text-to-image accuracy using a novel 3D scene graph and dual learning framework.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/youh3lgryi/cover.png"/></item><item><title>T2V-Turbo: Breaking the Quality Bottleneck of Video Consistency Model with Mixed Reward Feedback</title><link>https://deep-diver.github.io/neurips2024/posters/53dai9kbvf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/53dai9kbvf/</guid><description>T2V-Turbo breaks the quality bottleneck of video consistency models by integrating mixed reward feedback during consistency distillation, enabling high-quality video generation with significantly fast&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/53dai9kbvf/cover.png"/></item><item><title>TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy</title><link>https://deep-diver.github.io/neurips2024/posters/aou5yrbqky/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/aou5yrbqky/</guid><description>TabPedia: a novel large vision-language model, achieves superior visual table understanding by seamlessly integrating diverse tasks via a concept synergy mechanism and a new benchmark.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/aou5yrbqky/cover.png"/></item><item><title>Tackling Uncertain Correspondences for Multi-Modal Entity Alignment</title><link>https://deep-diver.github.io/neurips2024/posters/iase6cag26/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/iase6cag26/</guid><description>TMEA: A novel approach significantly boosts multi-modal entity alignment accuracy by effectively handling uncertain correspondences between modalities, improving data integration for diverse knowledge&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/iase6cag26/cover.png"/></item><item><title>Tactile DreamFusion: Exploiting Tactile Sensing for 3D Generation</title><link>https://deep-diver.github.io/neurips2024/posters/fa3rmml8ii/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fa3rmml8ii/</guid><description>Tactile DreamFusion: High-resolution tactile sensing enhances 3D generation, creating realistic geometric details previously unattainable.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fa3rmml8ii/cover.png"/></item><item><title>Temporal Sentence Grounding with Relevance Feedback in Videos</title><link>https://deep-diver.github.io/neurips2024/posters/eoonmxzzno/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/eoonmxzzno/</guid><description>RaTSG network tackles Temporal Sentence Grounding with Relevance Feedback (TSG-RF) by discerning query relevance at multiple granularities before selectively grounding segments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/eoonmxzzno/cover.png"/></item><item><title>Testing Semantic Importance via Betting</title><link>https://deep-diver.github.io/neurips2024/posters/a0hsmrwtlh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/a0hsmrwtlh/</guid><description>This work presents statistically grounded methods to rank semantic concept importance in black-box models, using conditional independence testing for both global and local interpretations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/a0hsmrwtlh/cover.png"/></item><item><title>Text-Aware Diffusion for Policy Learning</title><link>https://deep-diver.github.io/neurips2024/posters/nk6oncpd3n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nk6oncpd3n/</guid><description>Text-Aware Diffusion for Policy Learning (TADPoLe) uses pretrained diffusion models for zero-shot reward generation, enabling natural language-driven policy learning without manual reward design.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nk6oncpd3n/cover.png"/></item><item><title>Text-DiFuse: An Interactive Multi-Modal Image Fusion Framework based on Text-modulated Diffusion Model</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/ybrxzibyeg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/ybrxzibyeg/</guid><description>Text-DiFuse: A novel interactive multi-modal image fusion framework leverages text-modulated diffusion models for superior performance in complex scenarios.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/ybrxzibyeg/cover.png"/></item><item><title>Text-Guided Attention is All You Need for Zero-Shot Robustness in Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/cow65a9fgf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cow65a9fgf/</guid><description>Text-Guided Attention for Zero-Shot Robustness (TGA-ZSR) significantly improves vision-language model robustness against adversarial attacks by aligning and constraining text-guided attention, achievi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cow65a9fgf/cover.png"/></item><item><title>Text-Infused Attention and Foreground-Aware Modeling for Zero-Shot Temporal Action Detection</title><link>https://deep-diver.github.io/neurips2024/posters/ks9dciadty/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ks9dciadty/</guid><description>Ti-FAD: a novel zero-shot temporal action detection model outperforms state-of-the-art methods by enhancing text-related visual focus and foreground awareness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ks9dciadty/cover.png"/></item><item><title>Textual Training for the Hassle-Free Removal of Unwanted Visual Data: Case Studies on OOD and Hateful Image Detection</title><link>https://deep-diver.github.io/neurips2024/posters/xerwgdxafu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xerwgdxafu/</guid><description>Hassle-Free Textual Training (HFTT) uses only textual data to effectively remove unwanted visual data from AI training datasets, significantly reducing human annotation needs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xerwgdxafu/cover.png"/></item><item><title>TFG: Unified Training-Free Guidance for Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/n8ybgx98vc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/n8ybgx98vc/</guid><description>TFG: A unified, training-free framework for boosting diffusion model performance by efficiently searching its algorithm-agnostic design space.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/n8ybgx98vc/cover.png"/></item><item><title>Toward a Stable, Fair, and Comprehensive Evaluation of Object Hallucination in Large Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/yql5tutdah/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yql5tutdah/</guid><description>LeHaCE: a novel framework for evaluating object hallucination in LVLMs, improving evaluation stability and fairness by accounting for instruction-induced image description length variations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yql5tutdah/cover.png"/></item><item><title>Toward Semantic Gaze Target Detection</title><link>https://deep-diver.github.io/neurips2024/posters/bamafraxvf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bamafraxvf/</guid><description>Researchers developed a novel architecture for semantic gaze target detection, achieving state-of-the-art results by simultaneously predicting gaze target localization and semantic label, surpassing e&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bamafraxvf/cover.png"/></item><item><title>Towards Calibrated Robust Fine-Tuning of Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/gnafyr8ahc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gnafyr8ahc/</guid><description>Calibrated robust fine-tuning boosts vision-language model accuracy and confidence in out-of-distribution scenarios by using a constrained multimodal contrastive loss and self-distillation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gnafyr8ahc/cover.png"/></item><item><title>Towards Safe Concept Transfer of Multi-Modal Diffusion via Causal Representation Editing</title><link>https://deep-diver.github.io/neurips2024/posters/qac4ssztlf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qac4ssztlf/</guid><description>Causal Representation Editing (CRE) improves safe image generation by precisely removing unsafe concepts from diffusion models, enhancing efficiency and flexibility.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qac4ssztlf/cover.png"/></item><item><title>Towards Understanding the Working Mechanism of Text-to-Image Diffusion Model</title><link>https://deep-diver.github.io/neurips2024/posters/ztu0qepvtz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ztu0qepvtz/</guid><description>Stable Diffusion&amp;rsquo;s text-to-image generation is sped up by 25% by removing text guidance after the initial shape generation, revealing that the [EOS] token is key to early-stage image construction.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ztu0qepvtz/cover.png"/></item><item><title>Towards Unified Multimodal Editing with Enhanced Knowledge Collaboration</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/kf80zs3fvy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/kf80zs3fvy/</guid><description>UniKE: A unified multimodal editing method achieves superior reliability, generality, and locality by disentangling knowledge into semantic and truthfulness spaces, enabling enhanced collaboration bet&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/kf80zs3fvy/cover.png"/></item><item><title>TPR: Topology-Preserving Reservoirs for Generalized Zero-Shot Learning</title><link>https://deep-diver.github.io/neurips2024/posters/zkfca4oesf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zkfca4oesf/</guid><description>Topology-Preserving Reservoirs (TPR) enhances CLIP&amp;rsquo;s zero-shot learning by using a dual-space alignment and a topology-preserving objective to improve generalization to unseen classes, achieving state&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zkfca4oesf/cover.png"/></item><item><title>Training-Free Open-Ended Object Detection and Segmentation via Attention as Prompts</title><link>https://deep-diver.github.io/neurips2024/posters/xxvfj4p8nr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xxvfj4p8nr/</guid><description>VL-SAM: Training-free open-ended object detection &amp;amp; segmentation using attention maps as prompts, surpassing previous methods on LVIS and CODA datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xxvfj4p8nr/cover.png"/></item><item><title>TransAgent: Transfer Vision-Language Foundation Models with Heterogeneous Agent Collaboration</title><link>https://deep-diver.github.io/neurips2024/posters/tnqbcidjvf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tnqbcidjvf/</guid><description>TransAgent empowers vision-language models by collaborating with diverse expert agents, achieving state-of-the-art performance in low-shot visual recognition.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tnqbcidjvf/cover.png"/></item><item><title>TripletCLIP: Improving Compositional Reasoning of CLIP via Synthetic Vision-Language Negatives</title><link>https://deep-diver.github.io/neurips2024/posters/zfrgrk5kxl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zfrgrk5kxl/</guid><description>TripletCLIP boosts CLIP&amp;rsquo;s compositional reasoning by cleverly generating synthetic hard negative image-text pairs, achieving over 9% absolute improvement on SugarCrepe.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zfrgrk5kxl/cover.png"/></item><item><title>UMFC: Unsupervised Multi-Domain Feature Calibration for Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/dhikahbv6g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dhikahbv6g/</guid><description>UMFC: Unsupervised Multi-domain Feature Calibration improves vision-language model transferability by mitigating inherent model biases via a novel, training-free feature calibration method.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dhikahbv6g/cover.png"/></item><item><title>Understanding the Limits of Vision Language Models Through the Lens of the Binding Problem</title><link>https://deep-diver.github.io/neurips2024/posters/q5ryn6jagc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/q5ryn6jagc/</guid><description>Vision-language models struggle with multi-object reasoning due to the binding problem; this paper reveals human-like capacity limits in VLMs and proposes solutions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/q5ryn6jagc/cover.png"/></item><item><title>Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE</title><link>https://deep-diver.github.io/neurips2024/posters/oyl2fnzune/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oyl2fnzune/</guid><description>Uni-Med, a novel unified medical foundation model, tackles multi-task learning challenges by using Connector-MoE to efficiently bridge modalities, achieving competitive performance across six medical &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oyl2fnzune/cover.png"/></item><item><title>UniAR: A Unified model for predicting human Attention and Responses on visual content</title><link>https://deep-diver.github.io/neurips2024/posters/fjssnguhih/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fjssnguhih/</guid><description>UniAR: A unified model predicts human attention and preferences across diverse visual content (images, webpages, designs), achieving state-of-the-art performance and enabling human-centric improvement&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fjssnguhih/cover.png"/></item><item><title>Unified Generative and Discriminative Training for Multi-modal Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/w67vrhzf13/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w67vrhzf13/</guid><description>Unified generative-discriminative training boosts multimodal large language models (MLLMs)! Sugar, a novel approach, leverages dynamic sequence alignment and a triple kernel to enhance global and fin&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w67vrhzf13/cover.png"/></item><item><title>Unified Lexical Representation for Interpretable Visual-Language Alignment</title><link>https://deep-diver.github.io/neurips2024/posters/xocfd1wkpf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xocfd1wkpf/</guid><description>LexVLA: A novel visual-language alignment framework learns unified lexical representations for improved interpretability and efficient cross-modal retrieval.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xocfd1wkpf/cover.png"/></item><item><title>UNIT: Unifying Image and Text Recognition in One Vision Encoder</title><link>https://deep-diver.github.io/neurips2024/posters/yixkehqzpi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yixkehqzpi/</guid><description>UNIT: One Vision Encoder Unifies Image &amp;amp; Text Recognition!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yixkehqzpi/cover.png"/></item><item><title>Unveiling Encoder-Free Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/sppab1tmlc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/sppab1tmlc/</guid><description>EVE, a groundbreaking encoder-free vision-language model, rivals encoder-based counterparts using a fraction of the data and resources, demonstrating efficient, transparent training for pure decoder-o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/sppab1tmlc/cover.png"/></item><item><title>Unveiling the Tapestry of Consistency in Large Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/tu1oc7zhgw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tu1oc7zhgw/</guid><description>ConBench: Unveiling Inconsistency in Large Vision-Language Models</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tu1oc7zhgw/cover.png"/></item><item><title>VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths Vision Computation</title><link>https://deep-diver.github.io/neurips2024/posters/nkpxhzyusg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nkpxhzyusg/</guid><description>VIDEOLLM-MOD boosts online video-language model efficiency by selectively skipping redundant vision token computations, achieving ~42% faster training and ~30% memory savings without sacrificing perfo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nkpxhzyusg/cover.png"/></item><item><title>VideoTetris: Towards Compositional Text-to-Video Generation</title><link>https://deep-diver.github.io/neurips2024/posters/rpm7strnvz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rpm7strnvz/</guid><description>VideoTetris: a novel framework enabling compositional text-to-video generation by precisely following complex textual semantics through spatio-temporal compositional diffusion, achieving impressive qu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rpm7strnvz/cover.png"/></item><item><title>Vision Model Pre-training on Interleaved Image-Text Data via Latent Compression Learning</title><link>https://deep-diver.github.io/neurips2024/posters/z6knvoe9zq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z6knvoe9zq/</guid><description>Latent Compression Learning (LCL) revolutionizes vision model pre-training by effectively leveraging readily available interleaved image-text data, achieving performance comparable to models trained o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/z6knvoe9zq/cover.png"/></item><item><title>Vision-Language Models are Strong Noisy Label Detectors</title><link>https://deep-diver.github.io/neurips2024/posters/hauneixgq7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hauneixgq7/</guid><description>Vision-language models effectively detect noisy labels, improving image classification accuracy with DEFT.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hauneixgq7/cover.png"/></item><item><title>Vision-Language Navigation with Energy-Based Policy</title><link>https://deep-diver.github.io/neurips2024/posters/v3jhuoxmw8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v3jhuoxmw8/</guid><description>Energy-based Navigation Policy (ENP) revolutionizes Vision-Language Navigation by modeling joint state-action distributions, achieving superior performance across diverse benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v3jhuoxmw8/cover.png"/></item><item><title>VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks</title><link>https://deep-diver.github.io/neurips2024/posters/nvydpf4ljk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nvydpf4ljk/</guid><description>VisionLLM v2 unifies visual perception, understanding, and generation, excelling in various vision tasks and achieving performance comparable to task-specific models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nvydpf4ljk/cover.png"/></item><item><title>VisMin: Visual Minimal-Change Understanding</title><link>https://deep-diver.github.io/neurips2024/posters/bjddxcyosa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bjddxcyosa/</guid><description>VisMin benchmark evaluates visual-language models&amp;rsquo; fine-grained understanding by identifying minimal image-text differences (object, attribute, count, spatial relation). Current VLMs struggle with sp&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bjddxcyosa/cover.png"/></item><item><title>Visual Anchors Are Strong Information Aggregators For Multimodal Large Language Model</title><link>https://deep-diver.github.io/neurips2024/posters/2ypdpwzesf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2ypdpwzesf/</guid><description>AcFormer, a novel vision-language connector for MLLMs, leverages &amp;lsquo;visual anchors&amp;rsquo; to reduce computation cost by ~66% while improving accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2ypdpwzesf/cover.png"/></item><item><title>Visual Perception by Large Language Model’s Weights</title><link>https://deep-diver.github.io/neurips2024/posters/jptobptxkt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jptobptxkt/</guid><description>VLORA: Boosting Multimodal LLMs efficiency by merging visual features into model weights instead of extending input sequences.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jptobptxkt/cover.png"/></item><item><title>Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/gnsml1p5vr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gnsml1p5vr/</guid><description>Visual SKETCHPAD empowers multimodal language models (LLMs) with visual reasoning abilities by allowing them to generate intermediate sketches. This innovative framework substantially enhances LLM per&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gnsml1p5vr/cover.png"/></item><item><title>Vitron: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing</title><link>https://deep-diver.github.io/neurips2024/posters/kpmsfhcm5s/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kpmsfhcm5s/</guid><description>VITRON: a unified pixel-level Vision LLM excels in understanding, generating, segmenting, and editing images and videos.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kpmsfhcm5s/cover.png"/></item><item><title>VLM Agents Generate Their Own Memories: Distilling Experience into Embodied Programs of Thought</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/5g7mrfpngt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/5g7mrfpngt/</guid><description>VLMs learn to generate their own memories by abstracting experiences from noisy demonstrations and human feedback, significantly boosting in-context learning performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/5g7mrfpngt/cover.png"/></item><item><title>Voila-A: Aligning Vision-Language Models with User's Gaze Attention</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/zyrz5v84zi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/zyrz5v84zi/</guid><description>Voila-A enhances vision-language models by aligning their attention with user gaze, improving real-world application effectiveness and interpretability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/zyrz5v84zi/cover.png"/></item><item><title>WATT: Weight Average Test Time Adaptation of CLIP</title><link>https://deep-diver.github.io/neurips2024/posters/4d7hnj9om6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4d7hnj9om6/</guid><description>WATT: a novel test-time adaptation method boosts CLIP&amp;rsquo;s performance on domain shifted images by cleverly averaging weights from multiple text prompts, achieving state-of-the-art results without extra &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4d7hnj9om6/cover.png"/></item><item><title>What Factors Affect Multi-Modal In-Context Learning? An In-Depth Exploration</title><link>https://deep-diver.github.io/neurips2024/posters/revdykgcfb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/revdykgcfb/</guid><description>Unlocking the full potential of multi-modal in-context learning requires understanding its core factors. This research systematically explores these factors, highlighting the importance of a multi-mod&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/revdykgcfb/cover.png"/></item><item><title>What Makes CLIP More Robust to Long-Tailed Pre-Training Data? A Controlled Study for Transferable Insights</title><link>https://deep-diver.github.io/neurips2024/posters/pcyiohomjq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pcyiohomjq/</guid><description>CLIP&amp;rsquo;s robustness to long-tailed pre-training data stems from its dynamic classification task and descriptive language supervision, offering transferable insights for improving model generalizability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pcyiohomjq/cover.png"/></item><item><title>What matters when building vision-language models?</title><link>https://deep-diver.github.io/neurips2024/posters/dtvjf1vy2i/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dtvjf1vy2i/</guid><description>Idefics2, a new 8B-parameter VLM, achieves state-of-the-art performance, closing the gap with much larger models by meticulously analyzing design choices and training methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dtvjf1vy2i/cover.png"/></item><item><title>Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2)</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/s4yrclbuk1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/s4yrclbuk1/</guid><description>T2IScoreScore objectively evaluates text-to-image prompt faithfulness metrics using semantic error graphs, revealing that simpler metrics surprisingly outperform complex, computationally expensive one&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/s4yrclbuk1/cover.png"/></item><item><title>Why are Visually-Grounded Language Models Bad at Image Classification?</title><link>https://deep-diver.github.io/neurips2024/posters/mwmmbg1vyg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mwmmbg1vyg/</guid><description>Visually-grounded Language Models (VLMs) surprisingly underperform in image classification. This study reveals that this is primarily due to a lack of sufficient classification data during VLM trainin&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mwmmbg1vyg/cover.png"/></item><item><title>Wings: Learning Multimodal LLMs without Text-only Forgetting</title><link>https://deep-diver.github.io/neurips2024/posters/nqwaya7hix/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nqwaya7hix/</guid><description>WINGS: A novel multimodal LLM combats &amp;rsquo;text-only forgetting&amp;rsquo; by using complementary visual and textual learners, achieving superior performance on text-only and visual tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nqwaya7hix/cover.png"/></item><item><title>XMask3D: Cross-modal Mask Reasoning for Open Vocabulary 3D Semantic Segmentation</title><link>https://deep-diver.github.io/neurips2024/posters/z1gwanognr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z1gwanognr/</guid><description>XMask3D uses cross-modal mask reasoning to achieve state-of-the-art open vocabulary 3D semantic segmentation by aligning 2D and 3D features at the mask level, resulting in precise segmentation boundar&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/z1gwanognr/cover.png"/></item><item><title>Yo'LLaVA: Your Personalized Language and Vision Assistant</title><link>https://deep-diver.github.io/neurips2024/posters/mjgy8g3pgi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mjgy8g3pgi/</guid><description>Yo&amp;rsquo;LLaVA personalizes Large Multimodal Models (LMMs) to converse about specific subjects using just a few images, embedding concepts into latent tokens for efficient and effective personalized convers&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mjgy8g3pgi/cover.png"/></item></channel></rss>