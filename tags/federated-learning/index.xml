<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Federated Learning on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/federated-learning/</link><description>Recent content in Federated Learning on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/federated-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>$C^2M^3$: Cycle-Consistent Multi-Model Merging</title><link>https://deep-diver.github.io/neurips2024/posters/id18l6pra7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/id18l6pra7/</guid><description>C2M³: A novel data-free method ensures cycle-consistent merging of neural networks, significantly improving model aggregation across various architectures and datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/id18l6pra7/cover.png"/></item><item><title>A Bayesian Approach for Personalized Federated Learning in Heterogeneous Settings</title><link>https://deep-diver.github.io/neurips2024/posters/hilgwnabqb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hilgwnabqb/</guid><description>FedBNN: a novel Bayesian framework for personalized federated learning, achieves superior performance in heterogeneous settings while ensuring strict privacy via differential privacy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hilgwnabqb/cover.png"/></item><item><title>A Kernel Perspective on Distillation-based Collaborative Learning</title><link>https://deep-diver.github.io/neurips2024/posters/ldz0u1fuxb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ldz0u1fuxb/</guid><description>This paper introduces DCL-KR and DCL-NN, novel distillation-based collaborative learning algorithms achieving nearly minimax optimal convergence rates in heterogeneous environments without direct data&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ldz0u1fuxb/cover.png"/></item><item><title>A Swiss Army Knife for Heterogeneous Federated Learning: Flexible Coupling via Trace Norm</title><link>https://deep-diver.github.io/neurips2024/posters/3ykehut1o6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3ykehut1o6/</guid><description>FedSAK, a novel federated multi-task learning framework, flexibly handles data, model, and task heterogeneity using tensor trace norm to learn correlations among client models, achieving superior perf&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3ykehut1o6/cover.png"/></item><item><title>A-FedPD: Aligning Dual-Drift is All Federated Primal-Dual Learning Needs</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/h1imvi2iem/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/h1imvi2iem/</guid><description>A-FedPD tackles federated learning&amp;rsquo;s &amp;lsquo;dual drift&amp;rsquo; problem by aligning global and local dual variables, resulting in faster convergence and enhanced stability for primal-dual methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/h1imvi2iem/cover.png"/></item><item><title>CoBo: Collaborative Learning via Bilevel Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/sjq1iiqpfu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sjq1iiqpfu/</guid><description>CoBo: A novel bilevel optimization algorithm for collaborative learning surpasses existing methods by efficiently selecting helpful clients, resulting in superior performance and scalability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sjq1iiqpfu/cover.png"/></item><item><title>Communication-Efficient Federated Group Distributionally Robust Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/xnzejfe0mh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xnzejfe0mh/</guid><description>Communication-efficient algorithms for federated group distributionally robust optimization (FGDRO) are introduced, achieving lower communication complexity and superior performance on real-world task&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xnzejfe0mh/cover.png"/></item><item><title>Convergence Analysis of Split Federated Learning on Heterogeneous Data</title><link>https://deep-diver.github.io/neurips2024/posters/ud0rbkdbfe/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ud0rbkdbfe/</guid><description>Split Federated Learning (SFL) convergence is analyzed for heterogeneous data, achieving O(1/T) and O(1/√T) rates for strongly convex and general convex objectives respectively. The study also extend&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ud0rbkdbfe/cover.png"/></item><item><title>DapperFL: Domain Adaptive Federated Learning with Model Fusion Pruning for Edge Devices</title><link>https://deep-diver.github.io/neurips2024/oral-others/pezt0xttae/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-others/pezt0xttae/</guid><description>DapperFL enhances federated learning by introducing a model fusion pruning module and domain adaptive regularization to improve performance and reduce model size for heterogeneous edge devices.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-others/pezt0xttae/cover.png"/></item><item><title>Data Acquisition via Experimental Design for Data Markets</title><link>https://deep-diver.github.io/neurips2024/posters/vxjvndmxo4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vxjvndmxo4/</guid><description>Federated data acquisition via experimental design (DAVED) achieves lower prediction error without labeled validation data, optimizing cost-effectively for test-set predictions in decentralized market&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vxjvndmxo4/cover.png"/></item><item><title>Does Egalitarian Fairness Lead to Instability? The Fairness Bounds in Stable Federated Learning Under Altruistic Behaviors</title><link>https://deep-diver.github.io/neurips2024/posters/1kyc4tsofz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1kyc4tsofz/</guid><description>Achieving egalitarian fairness in federated learning without sacrificing stability is possible; this paper derives optimal fairness bounds considering clients&amp;rsquo; altruism and network topology.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1kyc4tsofz/cover.png"/></item><item><title>Don't Compress Gradients in Random Reshuffling: Compress Gradient Differences</title><link>https://deep-diver.github.io/neurips2024/posters/czptbzgfae/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/czptbzgfae/</guid><description>Boost federated learning efficiency! This paper introduces novel algorithms that cleverly combine gradient compression with random reshuffling, significantly reducing communication complexity and impr&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/czptbzgfae/cover.png"/></item><item><title>DU-Shapley: A Shapley Value Proxy for Efficient Dataset Valuation</title><link>https://deep-diver.github.io/neurips2024/posters/ucgfk8np0z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ucgfk8np0z/</guid><description>DU-Shapley efficiently estimates the Shapley value for dataset valuation, enabling fair compensation in collaborative machine learning by leveraging the problem&amp;rsquo;s structure for faster computation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ucgfk8np0z/cover.png"/></item><item><title>Dual-Personalizing Adapter for Federated Foundation Models</title><link>https://deep-diver.github.io/neurips2024/posters/nkwpibsw1f/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nkwpibsw1f/</guid><description>Federated Dual-Personalizing Adapter (FedDPA) tackles test-time distribution shifts and personalization in federated foundation models using a global and local adapter co-working mechanism, achieving &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nkwpibsw1f/cover.png"/></item><item><title>Efficient Federated Learning against Heterogeneous and Non-stationary Client Unavailability</title><link>https://deep-diver.github.io/neurips2024/posters/dlnobja7tm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dlnobja7tm/</guid><description>FedAWE, a novel federated learning algorithm, efficiently handles intermittent and unpredictable client availability, ensuring fast and unbiased model training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dlnobja7tm/cover.png"/></item><item><title>Exploring and Exploiting the Asymmetric Valley of Deep Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/cw0ovwekku/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cw0ovwekku/</guid><description>Deep neural network training reveals asymmetric loss valleys, impacting model fusion and federated learning; sign consistency between noise and convergence is key.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cw0ovwekku/cover.png"/></item><item><title>FACT or Fiction: Can Truthful Mechanisms Eliminate Federated Free Riding?</title><link>https://deep-diver.github.io/neurips2024/posters/jirgxrqhh0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jirgxrqhh0/</guid><description>FACT, a novel federated learning mechanism, eliminates free-riding and incentivizes truthful agent behavior by introducing a penalty system and a competitive environment, boosting model performance si&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jirgxrqhh0/cover.png"/></item><item><title>FedAvP: Augment Local Data via Shared Policy in Federated Learning</title><link>https://deep-diver.github.io/neurips2024/posters/m1pru0x1iz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/m1pru0x1iz/</guid><description>FedAvP enhances federated learning&amp;rsquo;s privacy by sharing only augmentation policies, improving performance in diverse settings.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/m1pru0x1iz/cover.png"/></item><item><title>Federated Behavioural Planes: Explaining the Evolution of Client Behaviour in Federated Learning</title><link>https://deep-diver.github.io/neurips2024/posters/5fhzrrgokr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/5fhzrrgokr/</guid><description>Federated Behavioural Planes visualize client behavior in federated learning, enabling robust aggregation and enhanced security against malicious clients.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/5fhzrrgokr/cover.png"/></item><item><title>Federated Learning over Connected Modes</title><link>https://deep-diver.github.io/neurips2024/posters/jl2emcfdw8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jl2emcfdw8/</guid><description>Federated Learning over Connected Modes (FLOCO) accelerates global training and improves local accuracy in heterogeneous data settings by leveraging mode connectivity for collaborative model personali&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jl2emcfdw8/cover.png"/></item><item><title>Federated Learning under Periodic Client Participation and Heterogeneous Data: A New Communication-Efficient Algorithm and Analysis</title><link>https://deep-diver.github.io/neurips2024/posters/wftavkl6g2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wftavkl6g2/</guid><description>Amplified SCAFFOLD: A new algorithm for federated learning significantly reduces communication rounds under periodic client participation and heterogeneous data, achieving linear speedup and resilienc&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wftavkl6g2/cover.png"/></item><item><title>Federated Online Prediction from Experts with Differential Privacy: Separations and Regret Speed-ups</title><link>https://deep-diver.github.io/neurips2024/posters/t826pwzlci/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/t826pwzlci/</guid><description>This paper presents novel algorithms achieving speed-ups in differentially private federated online prediction from experts, addressing both stochastic and oblivious adversaries, with rigorous theoret&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/t826pwzlci/cover.png"/></item><item><title>FedGMark: Certifiably Robust Watermarking for Federated Graph Learning</title><link>https://deep-diver.github.io/neurips2024/posters/xeviqpxtmu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xeviqpxtmu/</guid><description>FedGMark: the first certified robust watermarking method for protecting Federated Graph Learning models against theft and unauthorized copying.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xeviqpxtmu/cover.png"/></item><item><title>FedGTST: Boosting Global Transferability of Federated Models via Statistics Tuning</title><link>https://deep-diver.github.io/neurips2024/posters/qxkfc7d6p4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qxkfc7d6p4/</guid><description>FedGTST significantly improves federated transfer learning by tuning cross-client statistics, achieving superior global transferability with minimal communication overhead.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qxkfc7d6p4/cover.png"/></item><item><title>FedLPA: One-shot Federated Learning with Layer-Wise Posterior Aggregation</title><link>https://deep-diver.github.io/neurips2024/posters/i3iuclvlfz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/i3iuclvlfz/</guid><description>FedLPA: One-shot federated learning with layer-wise posterior aggregation improves model accuracy in non-IID data by efficiently aggregating layer-wise posteriors of local models using a novel approac&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/i3iuclvlfz/cover.png"/></item><item><title>FedNE: Surrogate-Assisted Federated Neighbor Embedding for Dimensionality Reduction</title><link>https://deep-diver.github.io/neurips2024/posters/zbmkodngkx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zbmkodngkx/</guid><description>FEDNE: a novel approach enabling collaborative dimensionality reduction of distributed data in federated learning without data sharing, achieved via surrogate loss functions and data augmentation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zbmkodngkx/cover.png"/></item><item><title>FedSSP: Federated Graph Learning with Spectral Knowledge and Personalized Preference</title><link>https://deep-diver.github.io/neurips2024/posters/i96gfyalfo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/i96gfyalfo/</guid><description>FedSSP tackles personalized federated graph learning challenges by sharing generic spectral knowledge and incorporating personalized preferences, achieving superior performance in cross-domain scenari&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/i96gfyalfo/cover.png"/></item><item><title>Ferrari: Federated Feature Unlearning via Optimizing Feature Sensitivity</title><link>https://deep-diver.github.io/neurips2024/posters/yxyytcv3hp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yxyytcv3hp/</guid><description>Ferrari, a novel federated feature unlearning framework, minimizes feature sensitivity via Lipschitz continuity, enabling effective and privacy-preserving data removal without full client participatio&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yxyytcv3hp/cover.png"/></item><item><title>FIARSE: Model-Heterogeneous Federated Learning via Importance-Aware Submodel Extraction</title><link>https://deep-diver.github.io/neurips2024/posters/bmbteqrhdi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bmbteqrhdi/</guid><description>FIARSE dynamically optimizes submodels in federated learning based on parameter importance, improving efficiency and global model accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bmbteqrhdi/cover.png"/></item><item><title>Fine-Tuning Personalization in Federated Learning to Mitigate Adversarial Clients</title><link>https://deep-diver.github.io/neurips2024/posters/wblplszji5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wblplszji5/</guid><description>Fine-tune personalization in federated learning to beat adversarial clients; collaboration level depends on data heterogeneity and adversary fraction.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wblplszji5/cover.png"/></item><item><title>FOOGD: Federated Collaboration for Both Out-of-distribution Generalization and Detection</title><link>https://deep-diver.github.io/neurips2024/posters/d6mqrw9hfu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/d6mqrw9hfu/</guid><description>FOOGD: A novel federated learning framework that simultaneously tackles out-of-distribution generalization and detection by estimating probability density for reliable global distribution guidance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/d6mqrw9hfu/cover.png"/></item><item><title>FuseFL: One-Shot Federated Learning through the Lens of Causality with Progressive Model Fusion</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/e7fzooiekl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/e7fzooiekl/</guid><description>FuseFL achieves superior one-shot federated learning performance by leveraging a causal view of data heterogeneity and progressively fusing model blocks, significantly outperforming existing methods w&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/e7fzooiekl/cover.png"/></item><item><title>Heterogeneity-Guided Client Sampling: Towards Fast and Efficient Non-IID Federated Learning</title><link>https://deep-diver.github.io/neurips2024/posters/hhnppisauh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hhnppisauh/</guid><description>HiCS-FL: A novel federated learning client sampling method that leverages data heterogeneity for faster, more efficient global model training in non-IID settings.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hhnppisauh/cover.png"/></item><item><title>Hierarchical Federated Learning with Multi-Timescale Gradient Correction</title><link>https://deep-diver.github.io/neurips2024/posters/acab1qnxi0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/acab1qnxi0/</guid><description>MTGC tackles multi-timescale model drift in hierarchical federated learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/acab1qnxi0/cover.png"/></item><item><title>HyperPrism: An Adaptive Non-linear Aggregation Framework for Distributed Machine Learning over Non-IID Data and Time-varying Communication Links</title><link>https://deep-diver.github.io/neurips2024/posters/3ie8nwa1el/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3ie8nwa1el/</guid><description>HyperPrism, a novel framework, tackles challenges in distributed machine learning by using adaptive non-linear aggregation to handle non-IID data and dynamic communication links, significantly improvi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3ie8nwa1el/cover.png"/></item><item><title>Improving the Worst-Case Bidirectional Communication Complexity for Nonconvex Distributed Optimization under Function Similarity</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/gkj5nbiou4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/gkj5nbiou4/</guid><description>MARINA-P and M3 algorithms drastically cut downlink and overall communication costs in nonconvex distributed optimization, scaling efficiently with the number of worker nodes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/gkj5nbiou4/cover.png"/></item><item><title>Initializing Services in Interactive ML Systems for Diverse Users</title><link>https://deep-diver.github.io/neurips2024/posters/hsjot2hydf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hsjot2hydf/</guid><description>Adaptively initializing multi-service ML systems for diverse users using minimal data, this paper introduces a randomized algorithm achieving near-optimal loss with provable guarantees.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hsjot2hydf/cover.png"/></item><item><title>Leveraging partial stragglers within gradient coding</title><link>https://deep-diver.github.io/neurips2024/posters/qc4e0voanp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qc4e0voanp/</guid><description>New gradient coding protocols efficiently leverage partial results from slow worker nodes, accelerating distributed training by approximately 2x and significantly improving accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qc4e0voanp/cover.png"/></item><item><title>Low Precision Local Training is Enough for Federated Learning</title><link>https://deep-diver.github.io/neurips2024/posters/vvpewjtnvm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vvpewjtnvm/</guid><description>Low-precision local training, surprisingly, is sufficient for accurate federated learning, significantly reducing communication and computation costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vvpewjtnvm/cover.png"/></item><item><title>Nonconvex Federated Learning on Compact Smooth Submanifolds With Heterogeneous Data</title><link>https://deep-diver.github.io/neurips2024/posters/uo53206olj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uo53206olj/</guid><description>This paper proposes a novel federated learning algorithm for nonconvex problems on compact smooth manifolds, achieving both computational and communication efficiency while mitigating client drift.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uo53206olj/cover.png"/></item><item><title>On Sampling Strategies for Spectral Model Sharding</title><link>https://deep-diver.github.io/neurips2024/posters/pgthglufi3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pgthglufi3/</guid><description>Two novel sampling strategies for spectral model sharding in federated learning minimize approximation error and create unbiased estimators, improving performance on various datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pgthglufi3/cover.png"/></item><item><title>On the Necessity of Collaboration for Online Model Selection with Decentralized Data</title><link>https://deep-diver.github.io/neurips2024/posters/uqwflgzpv1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uqwflgzpv1/</guid><description>Federated online model selection needs collaboration only when clients have limited computing power; otherwise, independent learning suffices.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uqwflgzpv1/cover.png"/></item><item><title>Optimal Private and Communication Constraint Distributed Goodness-of-Fit Testing for Discrete Distributions in the Large Sample Regime</title><link>https://deep-diver.github.io/neurips2024/posters/cmc0jmy0wr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cmc0jmy0wr/</guid><description>This paper derives matching minimax bounds for distributed goodness-of-fit testing of discrete data under bandwidth or privacy constraints, bridging theory and practice in federated learning.</description></item><item><title>Optimistic Verifiable Training by Controlling Hardware Nondeterminism</title><link>https://deep-diver.github.io/neurips2024/posters/bf0mdflz1i/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bf0mdflz1i/</guid><description>Researchers developed a verifiable training method that uses high-precision training with adaptive rounding and logging to achieve exact training replication across different GPUs, enabling efficient &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bf0mdflz1i/cover.png"/></item><item><title>Parameter Disparities Dissection for Backdoor Defense in Heterogeneous Federated Learning</title><link>https://deep-diver.github.io/neurips2024/posters/g8wnc1e1os/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/g8wnc1e1os/</guid><description>FDCR defends against backdoor attacks in heterogeneous federated learning by identifying malicious clients via Fisher Information-based parameter importance discrepancies and rescaling crucial paramet&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/g8wnc1e1os/cover.png"/></item><item><title>Personalized Federated Learning via Feature Distribution Adaptation</title><link>https://deep-diver.github.io/neurips2024/posters/wl2optqcng/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wl2optqcng/</guid><description>Personalized federated learning (PFL) often struggles with data scarcity and distribution shifts. pFedFDA, a novel algorithm, tackles this by framing representation learning as a generative modeling &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wl2optqcng/cover.png"/></item><item><title>Personalized Federated Learning with Mixture of Models for Adaptive Prediction and Model Fine-Tuning</title><link>https://deep-diver.github.io/neurips2024/posters/yvuhnbkczd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yvuhnbkczd/</guid><description>Fed-POE: A personalized federated learning algorithm for superior real-time predictions!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yvuhnbkczd/cover.png"/></item><item><title>pFedClub: Controllable Heterogeneous Model Aggregation for Personalized Federated Learning</title><link>https://deep-diver.github.io/neurips2024/posters/xw6ga9i4ea/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xw6ga9i4ea/</guid><description>pFedClub: Controllable heterogeneous model aggregation boosts personalized federated learning by generating reasonable-sized, personalized models, significantly cutting computational costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xw6ga9i4ea/cover.png"/></item><item><title>Probabilistic Federated Prompt-Tuning with Non-IID and Imbalanced Data</title><link>https://deep-diver.github.io/neurips2024/posters/nw6ansc66g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nw6ansc66g/</guid><description>Probabilistic Federated Prompt Tuning (PFPT) significantly improves federated learning accuracy on heterogeneous and imbalanced data by using a probabilistic model for prompt aggregation, outperformin&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nw6ansc66g/cover.png"/></item><item><title>RFLPA: A Robust Federated Learning Framework against Poisoning Attacks with Secure Aggregation</title><link>https://deep-diver.github.io/neurips2024/posters/js74zcddxg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/js74zcddxg/</guid><description>RFLPA: Secure Federated Learning resists poisoning attacks via efficient secure aggregation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/js74zcddxg/cover.png"/></item><item><title>SCAFFLSA: Taming Heterogeneity in Federated Linear Stochastic Approximation and TD Learning</title><link>https://deep-diver.github.io/neurips2024/posters/hej1cbagiv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hej1cbagiv/</guid><description>SCAFFLSA tames heterogeneity in federated learning, achieving logarithmic communication complexity and linear sample complexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hej1cbagiv/cover.png"/></item><item><title>SLowcalSGD : Slow Query Points Improve Local-SGD for Stochastic Convex Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/b29blre26z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/b29blre26z/</guid><description>SLowcal-SGD, a new local update method for distributed learning, provably outperforms Minibatch-SGD and Local-SGD in heterogeneous settings by using a slow querying technique, mitigating bias from loc&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/b29blre26z/cover.png"/></item><item><title>SPEAR: Exact Gradient Inversion of Batches in Federated Learning</title><link>https://deep-diver.github.io/neurips2024/posters/lpdxpvs6ix/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lpdxpvs6ix/</guid><description>SPEAR, a novel algorithm, precisely reconstructs entire data batches from gradients in federated learning, defying previous limitations and enhancing privacy risk assessment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lpdxpvs6ix/cover.png"/></item><item><title>Stabilized Proximal-Point Methods for Federated Optimization</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/wuksyfszdt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/wuksyfszdt/</guid><description>S-DANE &amp;amp; ACC-S-DANE achieve best-known communication complexity for federated learning, improving local computation efficiency via stabilized proximal-point methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/wuksyfszdt/cover.png"/></item><item><title>Time-FFM: Towards LM-Empowered Federated Foundation Model for Time Series Forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/hs0fahrhwd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hs0fahrhwd/</guid><description>TIME-FFM: a Federated Foundation Model empowers time series forecasting using pre-trained Language Models, tackling data scarcity and privacy concerns for superior few-shot and zero-shot predictions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hs0fahrhwd/cover.png"/></item><item><title>Towards Diverse Device Heterogeneous Federated Learning via Task Arithmetic Knowledge Integration</title><link>https://deep-diver.github.io/neurips2024/posters/y6jotynerr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y6jotynerr/</guid><description>TAKFL, a novel federated learning framework, tackles device heterogeneity by independently distilling knowledge from diverse devices and integrating it adaptively, achieving state-of-the-art performan&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y6jotynerr/cover.png"/></item><item><title>Unravelling in Collaborative Learning</title><link>https://deep-diver.github.io/neurips2024/posters/jfxqomos60/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jfxqomos60/</guid><description>Strategic data contributors with varying data quality can cause collaborative learning systems to &amp;lsquo;unravel&amp;rsquo;, but a novel probabilistic verification method effectively mitigates this, ensuring a stable&amp;hellip;</description></item></channel></rss>