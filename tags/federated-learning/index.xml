<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Federated Learning on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/federated-learning/</link><description>Recent content in Federated Learning on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/federated-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>A-FedPD: Aligning Dual-Drift is All Federated Primal-Dual Learning Needs</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/h1imvi2iem/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/h1imvi2iem/</guid><description>A-FedPD tackles federated learning&amp;rsquo;s &amp;lsquo;dual drift&amp;rsquo; problem by aligning global and local dual variables, resulting in faster convergence and enhanced stability for primal-dual methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/h1imvi2iem/cover.png"/></item><item><title>Communication-Efficient Federated Group Distributionally Robust Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/xnzejfe0mh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xnzejfe0mh/</guid><description>Communication-efficient algorithms for federated group distributionally robust optimization (FGDRO) are introduced, achieving lower communication complexity and superior performance on real-world task&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xnzejfe0mh/cover.png"/></item><item><title>DapperFL: Domain Adaptive Federated Learning with Model Fusion Pruning for Edge Devices</title><link>https://deep-diver.github.io/neurips2024/oral-others/pezt0xttae/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-others/pezt0xttae/</guid><description>DapperFL enhances federated learning by introducing a model fusion pruning module and domain adaptive regularization to improve performance and reduce model size for heterogeneous edge devices.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-others/pezt0xttae/cover.png"/></item><item><title>FedGMark: Certifiably Robust Watermarking for Federated Graph Learning</title><link>https://deep-diver.github.io/neurips2024/posters/xeviqpxtmu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xeviqpxtmu/</guid><description>FedGMark: the first certified robust watermarking method for protecting Federated Graph Learning models against theft and unauthorized copying.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xeviqpxtmu/cover.png"/></item><item><title>FedNE: Surrogate-Assisted Federated Neighbor Embedding for Dimensionality Reduction</title><link>https://deep-diver.github.io/neurips2024/posters/zbmkodngkx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zbmkodngkx/</guid><description>FEDNE: a novel approach enabling collaborative dimensionality reduction of distributed data in federated learning without data sharing, achieved via surrogate loss functions and data augmentation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zbmkodngkx/cover.png"/></item><item><title>FuseFL: One-Shot Federated Learning through the Lens of Causality with Progressive Model Fusion</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/e7fzooiekl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/e7fzooiekl/</guid><description>FuseFL achieves superior one-shot federated learning performance by leveraging a causal view of data heterogeneity and progressively fusing model blocks, significantly outperforming existing methods w&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/e7fzooiekl/cover.png"/></item><item><title>Improving the Worst-Case Bidirectional Communication Complexity for Nonconvex Distributed Optimization under Function Similarity</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/gkj5nbiou4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/gkj5nbiou4/</guid><description>MARINA-P and M3 algorithms drastically cut downlink and overall communication costs in nonconvex distributed optimization, scaling efficiently with the number of worker nodes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/gkj5nbiou4/cover.png"/></item><item><title>Nonconvex Federated Learning on Compact Smooth Submanifolds With Heterogeneous Data</title><link>https://deep-diver.github.io/neurips2024/posters/uo53206olj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uo53206olj/</guid><description>This paper proposes a novel federated learning algorithm for nonconvex problems on compact smooth manifolds, achieving both computational and communication efficiency while mitigating client drift.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uo53206olj/cover.png"/></item><item><title>On the Necessity of Collaboration for Online Model Selection with Decentralized Data</title><link>https://deep-diver.github.io/neurips2024/posters/uqwflgzpv1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uqwflgzpv1/</guid><description>Federated online model selection needs collaboration only when clients have limited computing power; otherwise, independent learning suffices.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uqwflgzpv1/cover.png"/></item><item><title>Optimistic Verifiable Training by Controlling Hardware Nondeterminism</title><link>https://deep-diver.github.io/neurips2024/posters/bf0mdflz1i/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bf0mdflz1i/</guid><description>Researchers developed a verifiable training method that uses high-precision training with adaptive rounding and logging to achieve exact training replication across different GPUs, enabling efficient &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bf0mdflz1i/cover.png"/></item><item><title>Personalized Federated Learning with Mixture of Models for Adaptive Prediction and Model Fine-Tuning</title><link>https://deep-diver.github.io/neurips2024/posters/yvuhnbkczd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yvuhnbkczd/</guid><description>Fed-POE: A personalized federated learning algorithm for superior real-time predictions!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yvuhnbkczd/cover.png"/></item><item><title>pFedClub: Controllable Heterogeneous Model Aggregation for Personalized Federated Learning</title><link>https://deep-diver.github.io/neurips2024/posters/xw6ga9i4ea/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xw6ga9i4ea/</guid><description>pFedClub: Controllable heterogeneous model aggregation boosts personalized federated learning by generating reasonable-sized, personalized models, significantly cutting computational costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xw6ga9i4ea/cover.png"/></item><item><title>Stabilized Proximal-Point Methods for Federated Optimization</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/wuksyfszdt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/wuksyfszdt/</guid><description>S-DANE &amp;amp; ACC-S-DANE achieve best-known communication complexity for federated learning, improving local computation efficiency via stabilized proximal-point methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/wuksyfszdt/cover.png"/></item><item><title>Towards Diverse Device Heterogeneous Federated Learning via Task Arithmetic Knowledge Integration</title><link>https://deep-diver.github.io/neurips2024/posters/y6jotynerr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y6jotynerr/</guid><description>TAKFL, a novel federated learning framework, tackles device heterogeneity by independently distilling knowledge from diverse devices and integrating it adaptively, achieving state-of-the-art performan&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y6jotynerr/cover.png"/></item></channel></rss>