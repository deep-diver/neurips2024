<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Multimodal Learning on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/multimodal-learning/</link><description>Recent content in Multimodal Learning on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/multimodal-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>A Unified Debiasing Approach for Vision-Language Models across Modalities and Tasks</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/181llen2gw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/181llen2gw/</guid><description>SFID, a novel debiasing method, effectively mitigates bias in vision-language models across various tasks without retraining, improving fairness and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/181llen2gw/cover.png"/></item><item><title>Adaptive Image Quality Assessment via Teaching Large Multimodal Model to Compare</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/mhtoyh5taj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/mhtoyh5taj/</guid><description>Compare2Score: A novel IQA model teaches large multimodal models to translate comparative image quality judgments into continuous quality scores, significantly outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/mhtoyh5taj/cover.png"/></item><item><title>Boosting Vision-Language Models with Transduction</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/go4zzxbwvs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/go4zzxbwvs/</guid><description>TransCLIP significantly boosts vision-language model accuracy by efficiently integrating transduction, a powerful learning paradigm that leverages the structure of unlabeled data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/go4zzxbwvs/cover.png"/></item><item><title>Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs</title><link>https://deep-diver.github.io/neurips2024/oral-others/vi8aepaxgy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-others/vi8aepaxgy/</guid><description>Cambrian-1: Open, vision-centric multimodal LLMs achieve state-of-the-art performance using a novel spatial vision aggregator and high-quality data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-others/vi8aepaxgy/cover.png"/></item><item><title>CLIPLoss and Norm-Based Data Selection Methods for Multimodal Contrastive Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/qvdc0ocx2n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/qvdc0ocx2n/</guid><description>Boosting multimodal contrastive learning, this research introduces negCLIPLoss and NormSim, novel data selection methods surpassing existing techniques by improving data quality and task relevance. Th&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/qvdc0ocx2n/cover.png"/></item><item><title>Enhancing LLM Reasoning via Vision-Augmented Prompting</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/ngugvt7ar2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/ngugvt7ar2/</guid><description>Vision-Augmented Prompting (VAP) boosts LLM reasoning by automatically generating images from textual problem descriptions, incorporating visual-spatial clues to significantly improve accuracy across &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/ngugvt7ar2/cover.png"/></item><item><title>Enhancing Zero-Shot Vision Models by Label-Free Prompt Distribution Learning and Bias Correcting</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/ojximyclit/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/ojximyclit/</guid><description>Frolic: A label-free framework boosts zero-shot vision model accuracy by learning prompt distributions and correcting label bias, achieving state-of-the-art performance across multiple datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/ojximyclit/cover.png"/></item><item><title>Flex-MoE: Modeling Arbitrary Modality Combination via the Flexible Mixture-of-Experts</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/ihehcbqzex/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/ihehcbqzex/</guid><description>Flex-MoE: A novel framework flexibly handles arbitrary modality combinations in multimodal learning, even with missing data, achieving robust performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/ihehcbqzex/cover.png"/></item><item><title>GenArtist: Multimodal LLM as an Agent for Unified Image Generation and Editing</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/ur00bnk1v2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/ur00bnk1v2/</guid><description>GenArtist uses a multimodal large language model as an AI agent to unify image generation and editing, achieving state-of-the-art performance by decomposing complex tasks and leveraging a comprehensiv&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/ur00bnk1v2/cover.png"/></item><item><title>Multilingual Diversity Improves Vision-Language Representations</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/1wteqrecys/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/1wteqrecys/</guid><description>Boosting vision-language models: Multilingual data improves performance on English-centric benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/1wteqrecys/cover.png"/></item><item><title>MultiOOD: Scaling Out-of-Distribution Detection for Multiple Modalities</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/a5pabdzp2f/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/a5pabdzp2f/</guid><description>MultiOOD benchmark and novel A2D &amp;amp; NP-Mix algorithms drastically improve multimodal out-of-distribution detection.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/a5pabdzp2f/cover.png"/></item><item><title>Pre-trained Text-to-Image Diffusion Models Are Versatile Representation Learners for Control</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/ky07a73f3y/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/ky07a73f3y/</guid><description>Pre-trained text-to-image diffusion models create highly effective, versatile representations for embodied AI control, surpassing previous methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/ky07a73f3y/cover.png"/></item><item><title>Procedure-Aware Surgical Video-language Pretraining with Hierarchical Knowledge Augmentation</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/zuwperkjnh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/zuwperkjnh/</guid><description>PeskaVLP: Hierarchical knowledge augmentation boosts surgical video-language pretraining!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/zuwperkjnh/cover.png"/></item><item><title>Text-DiFuse: An Interactive Multi-Modal Image Fusion Framework based on Text-modulated Diffusion Model</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/ybrxzibyeg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/ybrxzibyeg/</guid><description>Text-DiFuse: A novel interactive multi-modal image fusion framework leverages text-modulated diffusion models for superior performance in complex scenarios.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/ybrxzibyeg/cover.png"/></item><item><title>TFG: Unified Training-Free Guidance for Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/n8ybgx98vc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/n8ybgx98vc/</guid><description>TFG: A unified, training-free framework for boosting diffusion model performance by efficiently searching its algorithm-agnostic design space.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/n8ybgx98vc/cover.png"/></item><item><title>Towards Unified Multimodal Editing with Enhanced Knowledge Collaboration</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/kf80zs3fvy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/kf80zs3fvy/</guid><description>UniKE: A unified multimodal editing method achieves superior reliability, generality, and locality by disentangling knowledge into semantic and truthfulness spaces, enabling enhanced collaboration bet&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/kf80zs3fvy/cover.png"/></item><item><title>Unveiling Encoder-Free Vision-Language Models</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/sppab1tmlc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/sppab1tmlc/</guid><description>EVE, a groundbreaking encoder-free vision-language model, rivals encoder-based counterparts using a fraction of the data and resources, demonstrating efficient, transparent training for pure decoder-o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/sppab1tmlc/cover.png"/></item><item><title>VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time</title><link>https://deep-diver.github.io/neurips2024/oral-others/5zscse0k41/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-others/5zscse0k41/</guid><description>VASA-1: Real-time, lifelike talking faces generated from a single image and audio!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-others/5zscse0k41/cover.png"/></item><item><title>VLM Agents Generate Their Own Memories: Distilling Experience into Embodied Programs of Thought</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/5g7mrfpngt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/5g7mrfpngt/</guid><description>VLMs learn to generate their own memories by abstracting experiences from noisy demonstrations and human feedback, significantly boosting in-context learning performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/5g7mrfpngt/cover.png"/></item><item><title>Voila-A: Aligning Vision-Language Models with User's Gaze Attention</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/zyrz5v84zi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/zyrz5v84zi/</guid><description>Voila-A enhances vision-language models by aligning their attention with user gaze, improving real-world application effectiveness and interpretability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/zyrz5v84zi/cover.png"/></item><item><title>Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2)</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/s4yrclbuk1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/s4yrclbuk1/</guid><description>T2IScoreScore objectively evaluates text-to-image prompt faithfulness metrics using semantic error graphs, revealing that simpler metrics surprisingly outperform complex, computationally expensive one&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/s4yrclbuk1/cover.png"/></item></channel></rss>