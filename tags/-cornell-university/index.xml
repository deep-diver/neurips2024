<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ Cornell University on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-cornell-university/</link><description>Recent content in üè¢ Cornell University on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-cornell-university/index.xml" rel="self" type="application/rss+xml"/><item><title>Code Repair with LLMs gives an Exploration-Exploitation Tradeoff</title><link>https://deep-diver.github.io/neurips2024/posters/o863gx6dxa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/o863gx6dxa/</guid><description>New program synthesis method, REX, leverages Thompson Sampling to balance exploration and exploitation in iterative LLM code refinement, solving more problems with fewer model calls.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/o863gx6dxa/cover.png"/></item><item><title>Diffusion Models With Learned Adaptive Noise</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/loma99a4p8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/loma99a4p8/</guid><description>MuLAN, a novel learned diffusion process, achieves state-of-the-art density estimation by adaptively adding multivariate Gaussian noise at varying rates across an image, significantly reducing trainin&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/loma99a4p8/cover.png"/></item><item><title>Doob's Lagrangian: A Sample-Efficient Variational Approach to Transition Path Sampling</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/shjwt0n7kx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/shjwt0n7kx/</guid><description>Researchers developed a sample-efficient variational approach for transition path sampling using Doob&amp;rsquo;s h-transform, significantly reducing computational costs while accurately capturing transition pa&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/shjwt0n7kx/cover.png"/></item><item><title>Estimating Heterogeneous Treatment Effects by Combining Weak Instruments and Observational Data</title><link>https://deep-diver.github.io/neurips2024/posters/c37x7cxz2y/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/c37x7cxz2y/</guid><description>This study develops a novel two-stage framework for accurately predicting conditional average treatment effects using both observational data and weak instrumental variables, overcoming limitations of&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/c37x7cxz2y/cover.png"/></item><item><title>Hybrid Top-Down Global Causal Discovery with Local Search for Linear and Nonlinear Additive Noise Models</title><link>https://deep-diver.github.io/neurips2024/posters/xnmm1jthkv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xnmm1jthkv/</guid><description>Hybrid causal discovery algorithm efficiently learns unique causal graphs from observational data by leveraging local substructures and topological sorting, outperforming existing methods in accuracy &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xnmm1jthkv/cover.png"/></item><item><title>Is Programming by Example solved by LLMs?</title><link>https://deep-diver.github.io/neurips2024/posters/xqc8yyhscl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xqc8yyhscl/</guid><description>Large Language Models (LLMs) surprisingly improve the challenging task of Programming by Example (PBE) when fine-tuned on problem-specific data, outperforming classic symbolic methods and even surpass&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xqc8yyhscl/cover.png"/></item><item><title>Language Generation in the Limit</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/fgtde6ea0b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/fgtde6ea0b/</guid><description>This paper proves that language generation in the limit is always possible, even with an adversarial setting, contrasting with the impossibility of language identification in the limit.</description></item><item><title>Microstructures and Accuracy of Graph Recall by Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/tnhwg9u767/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tnhwg9u767/</guid><description>LLMs struggle with graph recall, exhibiting biases like favoring triangles and underperforming compared to humans; advanced models show striking domain dependence.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tnhwg9u767/cover.png"/></item><item><title>Neural Gaffer: Relighting Any Object via Diffusion</title><link>https://deep-diver.github.io/neurips2024/posters/zv2gdszb5a/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zv2gdszb5a/</guid><description>Neural Gaffer: Relighting any object via diffusion using a single image and an environment map to produce high-quality, realistic relit images.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zv2gdszb5a/cover.png"/></item><item><title>QTIP: Quantization with Trellises and Incoherence Processing</title><link>https://deep-diver.github.io/neurips2024/spotlight-large-language-models/7sdklvuycu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-large-language-models/7sdklvuycu/</guid><description>QTIP: Ultra-high dimensional LLM quantization using trellis codes for faster, higher-quality inference.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-large-language-models/7sdklvuycu/cover.png"/></item><item><title>REBEL: Reinforcement Learning via Regressing Relative Rewards</title><link>https://deep-diver.github.io/neurips2024/posters/yxjwajzuyv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yxjwajzuyv/</guid><description>REBEL, a novel reinforcement learning algorithm, simplifies policy optimization by regressing relative rewards, achieving strong performance in language and image generation tasks with increased effic&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yxjwajzuyv/cover.png"/></item><item><title>Sample Complexity of Posted Pricing for a Single Item</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ek1tyhcb3w/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ek1tyhcb3w/</guid><description>This paper reveals how many buyer samples are needed to set near-optimal posted prices for a single item, resolving a fundamental problem in online markets and offering both theoretical and practical &amp;hellip;</description></item><item><title>The Collusion of Memory and Nonlinearity in Stochastic Approximation With Constant Stepsize</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/xul75cvhl5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/xul75cvhl5/</guid><description>Unlocking the mysteries of stochastic approximation with constant stepsize, this paper reveals how memory and nonlinearity interact to create bias, providing novel analysis and solutions for more accu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/xul75cvhl5/cover.png"/></item><item><title>The Limits of Transfer Reinforcement Learning with Latent Low-rank Structure</title><link>https://deep-diver.github.io/neurips2024/posters/pk2qgry2hv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pk2qgry2hv/</guid><description>This paper presents computationally efficient transfer reinforcement learning algorithms that remove the dependence on state/action space sizes while achieving minimax optimality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pk2qgry2hv/cover.png"/></item></channel></rss>