<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/deep-learning/</link><description>Recent content in Deep Learning on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Advancing Training Efficiency of Deep Spiking Neural Networks through Rate-based Backpropagation</title><link>https://deep-diver.github.io/neurips2024/posters/wlcm21c4nk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wlcm21c4nk/</guid><description>Rate-based backpropagation boosts deep spiking neural network training efficiency by leveraging rate coding, achieving comparable performance to BPTT with reduced complexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wlcm21c4nk/cover.png"/></item><item><title>An exactly solvable model for emergence and scaling laws in the multitask sparse parity problem</title><link>https://deep-diver.github.io/neurips2024/posters/cuwsr25bbi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cuwsr25bbi/</guid><description>A novel multilinear model analytically explains the emergence and scaling laws of skills in the multitask sparse parity problem, accurately predicting skill emergence in neural networks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cuwsr25bbi/cover.png"/></item><item><title>Approximately Equivariant Neural Processes</title><link>https://deep-diver.github.io/neurips2024/posters/dqt9mc5nql/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dqt9mc5nql/</guid><description>Boosting meta-learning, this paper introduces a novel, flexible approach to create approximately equivariant neural processes that outperform both non-equivariant and strictly equivariant counterparts&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dqt9mc5nql/cover.png"/></item><item><title>Breaking the curse of dimensionality in structured density estimation</title><link>https://deep-diver.github.io/neurips2024/posters/dwwin2ugye/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dwwin2ugye/</guid><description>Researchers break the curse of dimensionality in structured density estimation using graph resilience, a novel graphical parameter that effectively reduces the sample complexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dwwin2ugye/cover.png"/></item><item><title>Bridging Geometric States via Geometric Diffusion Bridge</title><link>https://deep-diver.github.io/neurips2024/posters/zcepob9rcr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zcepob9rcr/</guid><description>Geometric Diffusion Bridge (GDB) accurately predicts geometric state evolution in complex systems by leveraging a probabilistic approach and equivariant diffusion processes, surpassing existing deep l&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zcepob9rcr/cover.png"/></item><item><title>Bridging OOD Detection and Generalization: A Graph-Theoretic View</title><link>https://deep-diver.github.io/neurips2024/posters/qzwag8qxi1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qzwag8qxi1/</guid><description>A novel graph-theoretic framework bridges OOD detection &amp;amp; generalization, offering theoretical error bounds and competitive empirical performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qzwag8qxi1/cover.png"/></item><item><title>Changing the Training Data Distribution to Reduce Simplicity Bias Improves In-distribution Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/yyspldusu2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yyspldusu2/</guid><description>Boosting in-distribution generalization is achieved by strategically altering the training data distribution to reduce simplicity bias and promote uniform feature learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yyspldusu2/cover.png"/></item><item><title>Combining Statistical Depth and Fermat Distance for Uncertainty Quantification</title><link>https://deep-diver.github.io/neurips2024/posters/xexrhtumcf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xexrhtumcf/</guid><description>Boosting neural network prediction reliability, this research ingeniously combines statistical depth and Fermat distance for superior uncertainty quantification, eliminating the need for distributiona&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xexrhtumcf/cover.png"/></item><item><title>Confidence Calibration of Classifiers with Many Classes</title><link>https://deep-diver.github.io/neurips2024/posters/ebbnkvxmcz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ebbnkvxmcz/</guid><description>Boost multi-class classifier calibration by cleverly transforming the problem into a single binary calibration task!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ebbnkvxmcz/cover.png"/></item><item><title>Conformalized Credal Set Predictors</title><link>https://deep-diver.github.io/neurips2024/posters/vbah12uvbd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vbah12uvbd/</guid><description>Conformal prediction empowers robust credal set predictions, handling aleatoric and epistemic uncertainties in classification, guaranteed to be valid with high probability!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vbah12uvbd/cover.png"/></item><item><title>ControlSynth Neural ODEs: Modeling Dynamical Systems with Guaranteed Convergence</title><link>https://deep-diver.github.io/neurips2024/posters/dbe8khdmfs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dbe8khdmfs/</guid><description>ControlSynth Neural ODEs (CSODEs) guarantee convergence in complex dynamical systems via tractable linear inequalities, improving neural ODE modeling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dbe8khdmfs/cover.png"/></item><item><title>Convolutions and More as Einsum: A Tensor Network Perspective with Advances for Second-Order Methods</title><link>https://deep-diver.github.io/neurips2024/posters/cds8wxnmvp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cds8wxnmvp/</guid><description>This paper accelerates second-order optimization in CNNs by 4.5x, using a novel tensor network representation that simplifies convolutions and reduces memory overhead.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cds8wxnmvp/cover.png"/></item><item><title>DiffusionPDE: Generative PDE-Solving under Partial Observation</title><link>https://deep-diver.github.io/neurips2024/posters/z0i2sbjn0r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z0i2sbjn0r/</guid><description>DiffusionPDE uses generative diffusion models to solve PDEs accurately, even with highly incomplete observations, outperforming state-of-the-art methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/z0i2sbjn0r/cover.png"/></item><item><title>Dynamic Conditional Optimal Transport through Simulation-Free Flows</title><link>https://deep-diver.github.io/neurips2024/posters/tk0uarynhh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tk0uarynhh/</guid><description>Simulation-free flow generates conditional distributions via dynamic conditional optimal transport.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tk0uarynhh/cover.png"/></item><item><title>Enhancing Diversity in Bayesian Deep Learning via Hyperspherical Energy Minimization of CKA</title><link>https://deep-diver.github.io/neurips2024/posters/s2ha6bz3le/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s2ha6bz3le/</guid><description>Boosting Bayesian deep learning&amp;rsquo;s diversity and uncertainty quantification, this study proposes hyperspherical energy minimization of CKA to generate diverse and reliable neural network ensembles and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/s2ha6bz3le/cover.png"/></item><item><title>Equivariant Machine Learning on Graphs with Nonlinear Spectral Filters</title><link>https://deep-diver.github.io/neurips2024/posters/y8p633e5hq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y8p633e5hq/</guid><description>Nonlinear spectral filters (NLSFs) enable fully equivariant graph neural networks, improving accuracy and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y8p633e5hq/cover.png"/></item><item><title>Fast yet Safe: Early-Exiting with Risk Control</title><link>https://deep-diver.github.io/neurips2024/posters/bbfjpasrgs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bbfjpasrgs/</guid><description>Risk control boosts early-exit neural networks&amp;rsquo; speed and safety by ensuring accurate predictions before exiting early, achieving substantial computational savings across diverse tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bbfjpasrgs/cover.png"/></item><item><title>Generative Modeling of Molecular Dynamics Trajectories</title><link>https://deep-diver.github.io/neurips2024/posters/yrrch1osgw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yrrch1osgw/</guid><description>MDGEN: Generative modeling unlocks MD data for diverse tasks, achieving significant speedups via flexible multi-task surrogate models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yrrch1osgw/cover.png"/></item><item><title>Graph Edit Distance with General Costs Using Neural Set Divergence</title><link>https://deep-diver.github.io/neurips2024/posters/u7jrmrgutt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/u7jrmrgutt/</guid><description>GRAPHEDX, a novel neural network, accurately estimates graph edit distance with varying operation costs, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/u7jrmrgutt/cover.png"/></item><item><title>Graph Neural Flows for Unveiling Systemic Interactions Among Irregularly Sampled Time Series</title><link>https://deep-diver.github.io/neurips2024/posters/tfb5ssabvb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tfb5ssabvb/</guid><description>GNeuralFlow unveils systemic interactions in irregularly sampled time series by learning a directed acyclic graph representing conditional dependencies, achieving superior performance in classificatio&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tfb5ssabvb/cover.png"/></item><item><title>Guiding Neural Collapse: Optimising Towards the Nearest Simplex Equiangular Tight Frame</title><link>https://deep-diver.github.io/neurips2024/posters/z4fapuslma/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z4fapuslma/</guid><description>Researchers devised a novel method to accelerate neural network training by guiding the optimization process toward a Simplex Equiangular Tight Frame, exploiting the Neural Collapse phenomenon to enha&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/z4fapuslma/cover.png"/></item><item><title>Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects Models</title><link>https://deep-diver.github.io/neurips2024/posters/uxuobobjho/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uxuobobjho/</guid><description>Accelerate Bayesian inference in linear mixed-effects models by efficiently marginalizing random effects using fast linear algebra, enabling faster and more accurate posterior estimations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uxuobobjho/cover.png"/></item><item><title>HEPrune: Fast Private Training of Deep Neural Networks With Encrypted Data Pruning</title><link>https://deep-diver.github.io/neurips2024/posters/y2famldtif/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y2famldtif/</guid><description>HEPrune accelerates private deep learning training 16x by integrating encrypted data pruning, achieving this speedup with minimal accuracy loss.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y2famldtif/cover.png"/></item><item><title>Implicitly Guided Design with PropEn: Match your Data to Follow the Gradient</title><link>https://deep-diver.github.io/neurips2024/posters/dhfho90ink/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dhfho90ink/</guid><description>PropEn: a novel framework for implicitly guided design optimization that leverages &amp;lsquo;matching&amp;rsquo; to boost efficiency by matching samples and approximating the gradient without a discriminator.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dhfho90ink/cover.png"/></item><item><title>Improving Deep Learning Optimization through Constrained Parameter Regularization</title><link>https://deep-diver.github.io/neurips2024/posters/rcxtkihkbf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rcxtkihkbf/</guid><description>Constrained Parameter Regularization (CPR) outperforms traditional weight decay by dynamically adapting regularization strengths for individual parameters, leading to better deep learning model perfor&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rcxtkihkbf/cover.png"/></item><item><title>Improving Equivariant Model Training via Constraint Relaxation</title><link>https://deep-diver.github.io/neurips2024/posters/twkl7k1u5v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/twkl7k1u5v/</guid><description>Boost equivariant model training by strategically relaxing constraints during training, enhancing optimization and generalization!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/twkl7k1u5v/cover.png"/></item><item><title>Improving Generalization and Convergence by Enhancing Implicit Regularization</title><link>https://deep-diver.github.io/neurips2024/posters/cjm2bhloic/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cjm2bhloic/</guid><description>IRE framework expedites the discovery of flat minima in deep learning, enhancing generalization and convergence. By decoupling the dynamics of flat and sharp directions, IRE boosts sharpness reduction&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cjm2bhloic/cover.png"/></item><item><title>Inference of Neural Dynamics Using Switching Recurrent Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/zb8jlah2vn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zb8jlah2vn/</guid><description>SRNNs reveal behaviorally-relevant neural dynamics switches!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zb8jlah2vn/cover.png"/></item><item><title>Integrating GNN and Neural ODEs for Estimating Non-Reciprocal Two-Body Interactions in Mixed-Species Collective Motion</title><link>https://deep-diver.github.io/neurips2024/posters/qwl3eidi9r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qwl3eidi9r/</guid><description>Deep learning framework integrating GNNs and neural ODEs precisely estimates non-reciprocal two-body interactions in mixed-species collective motion, accurately replicating both individual and collect&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qwl3eidi9r/cover.png"/></item><item><title>Introducing Spectral Attention for Long-Range Dependency in Time Series Forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/dxynvebqmp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dxynvebqmp/</guid><description>Spectral Attention boosts long-range dependency capture in time series forecasting, achieving state-of-the-art results across various models and datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dxynvebqmp/cover.png"/></item><item><title>Learning Infinitesimal Generators of Continuous Symmetries from Data</title><link>https://deep-diver.github.io/neurips2024/posters/wl44w8xpc7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wl44w8xpc7/</guid><description>Learn continuous symmetries from data without pre-defined groups using Neural ODEs and a novel validity score to improve model generalization and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wl44w8xpc7/cover.png"/></item><item><title>Learning Macroscopic Dynamics from Partial Microscopic Observations</title><link>https://deep-diver.github.io/neurips2024/posters/cjh0qsgd0d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cjh0qsgd0d/</guid><description>Learn macroscopic dynamics efficiently using only partial microscopic force computations! This novel method leverages sparsity assumptions and stochastic estimation for accurate, cost-effective modeli&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cjh0qsgd0d/cover.png"/></item><item><title>Local Curvature Smoothing with Stein's Identity for Efficient Score Matching</title><link>https://deep-diver.github.io/neurips2024/posters/yppni7vc7n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yppni7vc7n/</guid><description>LCSS, a novel score-matching method, enables efficient and high-quality image generation in score-based diffusion models by using Stein&amp;rsquo;s identity to bypass the computationally expensive Jacobian trac&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yppni7vc7n/cover.png"/></item><item><title>Mitigating Backdoor Attack by Injecting Proactive Defensive Backdoor</title><link>https://deep-diver.github.io/neurips2024/posters/cbkjbyikid/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cbkjbyikid/</guid><description>Proactive Defensive Backdoor (PDB) thwarts malicious backdoors by injecting a hidden defensive backdoor during training, suppressing attacks while maintaining model utility.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cbkjbyikid/cover.png"/></item><item><title>Model LEGO: Creating Models Like Disassembling and Assembling Building Blocks</title><link>https://deep-diver.github.io/neurips2024/posters/nxl7eazkbi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nxl7eazkbi/</guid><description>Model LEGO (MDA) revolutionizes deep learning by enabling the creation of new models by assembling and disassembling task-aware components from pre-trained models, eliminating the need for retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nxl7eazkbi/cover.png"/></item><item><title>Monomial Matrix Group Equivariant Neural Functional Networks</title><link>https://deep-diver.github.io/neurips2024/posters/rqyywgyuzk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rqyywgyuzk/</guid><description>Monomial-NFNs boost neural network efficiency by leveraging scaling/sign-flipping symmetries, resulting in fewer trainable parameters and competitive performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rqyywgyuzk/cover.png"/></item><item><title>Neural Collapse Inspired Feature Alignment for Out-of-Distribution Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/wqpng9jnpk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wqpng9jnpk/</guid><description>Neural Collapse-inspired Feature Alignment (NCFAL) significantly boosts out-of-distribution generalization by aligning semantic features to a simplex ETF, even without environment labels.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wqpng9jnpk/cover.png"/></item><item><title>Neural Conditional Probability for Uncertainty Quantification</title><link>https://deep-diver.github.io/neurips2024/posters/zxfhhjnmb2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zxfhhjnmb2/</guid><description>Neural Conditional Probability (NCP) offers a new operator-theoretic approach for efficiently learning conditional distributions, enabling streamlined inference and providing theoretical guarantees fo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zxfhhjnmb2/cover.png"/></item><item><title>Neural P$^3$M: A Long-Range Interaction Modeling Enhancer for Geometric GNNs</title><link>https://deep-diver.github.io/neurips2024/posters/ncqauwsyl5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ncqauwsyl5/</guid><description>Neural P³M enhances geometric GNNs by incorporating mesh points to model long-range interactions in molecules, achieving state-of-the-art accuracy in predicting energy and forces.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ncqauwsyl5/cover.png"/></item><item><title>Nuclear Norm Regularization for Deep Learning</title><link>https://deep-diver.github.io/neurips2024/posters/eddhtvb5em/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/eddhtvb5em/</guid><description>This paper presents a novel, efficient method for Jacobian nuclear norm regularization in deep learning, replacing computationally expensive SVDs with equivalent Frobenius norm computations, thereby e&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/eddhtvb5em/cover.png"/></item><item><title>On conditional diffusion models for PDE simulations</title><link>https://deep-diver.github.io/neurips2024/posters/nql8ejymzh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nql8ejymzh/</guid><description>This paper introduces novel autoregressive sampling and hybrid training strategies for score-based diffusion models, significantly boosting PDE forecasting and assimilation accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nql8ejymzh/cover.png"/></item><item><title>PGN: The RNN's New Successor is Effective for Long-Range Time Series Forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/ypeamfku2o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ypeamfku2o/</guid><description>TPGN, a novel framework for long-range time series forecasting, uses Parallel Gated Networks (PGN) to efficiently capture long-term dependencies, achieving state-of-the-art results on multiple dataset&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ypeamfku2o/cover.png"/></item><item><title>Predicting Ground State Properties: Constant Sample Complexity and Deep Learning Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/yblxvqjyqa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yblxvqjyqa/</guid><description>Deep learning algorithms now predict quantum ground state properties with constant sample complexity, regardless of system size, improving upon previous methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yblxvqjyqa/cover.png"/></item><item><title>Preferential Normalizing Flows</title><link>https://deep-diver.github.io/neurips2024/posters/srsjr9sdkr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/srsjr9sdkr/</guid><description>Eliciting high-dimensional probability distributions from experts using only preference comparisons is achieved via normalizing flows and a novel functional prior, resolving the problem of collapsing &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/srsjr9sdkr/cover.png"/></item><item><title>PURE: Prompt Evolution with Graph ODE for Out-of-distribution Fluid Dynamics Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/z86knmjouq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z86knmjouq/</guid><description>PURE: A novel method uses Graph ODE to adapt spatio-temporal forecasting models to various fluid dynamics scenarios, improving model adaptation to unseen parameters and long-term predictions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/z86knmjouq/cover.png"/></item><item><title>Regularized Adaptive Momentum Dual Averaging with an Efficient Inexact Subproblem Solver for Training Structured Neural Network</title><link>https://deep-diver.github.io/neurips2024/posters/xl7ve14aha/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xl7ve14aha/</guid><description>RAMDA: a new algorithm ensures efficient training of structured neural networks by achieving optimal structure and outstanding predictive performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xl7ve14aha/cover.png"/></item><item><title>Rethinking Deep Thinking: Stable Learning of Algorithms using Lipschitz Constraints</title><link>https://deep-diver.github.io/neurips2024/posters/zlgfrk2cqa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zlgfrk2cqa/</guid><description>Stable algorithm learning achieved by Deep Thinking networks with Lipschitz Constraints, ensuring convergence and better extrapolation to complex problems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zlgfrk2cqa/cover.png"/></item><item><title>Sharpness-diversity tradeoff: improving flat ensembles with SharpBalance</title><link>https://deep-diver.github.io/neurips2024/posters/wjacsnt9ue/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wjacsnt9ue/</guid><description>SharpBalance, a novel training approach, effectively improves deep ensemble performance by addressing the sharpness-diversity trade-off, leading to significant improvements in both in-distribution and&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wjacsnt9ue/cover.png"/></item><item><title>Stochastic Kernel Regularisation Improves Generalisation in Deep Kernel Machines</title><link>https://deep-diver.github.io/neurips2024/posters/prgxz9fybf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/prgxz9fybf/</guid><description>Deep kernel machines now achieve 94.5% accuracy on CIFAR-10, matching neural networks, by using stochastic kernel regularization to improve generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/prgxz9fybf/cover.png"/></item><item><title>Structural Inference of Dynamical Systems with Conjoined State Space Models</title><link>https://deep-diver.github.io/neurips2024/posters/xqwjbek5rh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xqwjbek5rh/</guid><description>SICSM, a novel framework, integrates selective SSMs and GFNs to accurately infer complex dynamical system structures from irregularly sampled, partially observed trajectories.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xqwjbek5rh/cover.png"/></item><item><title>Super Consistency of Neural Network Landscapes and Learning Rate Transfer</title><link>https://deep-diver.github.io/neurips2024/posters/rgwhj7intz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rgwhj7intz/</guid><description>Neural network hyperparameter transferability across vastly different model sizes is achieved via a newly discovered property called &amp;lsquo;Super Consistency&amp;rsquo; of loss landscapes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rgwhj7intz/cover.png"/></item><item><title>Supra-Laplacian Encoding for Transformer on Dynamic Graphs</title><link>https://deep-diver.github.io/neurips2024/posters/vp9qazr2gw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vp9qazr2gw/</guid><description>SLATE: Supra-Laplacian encoding for spatio-temporal Transformers achieves state-of-the-art dynamic link prediction by innovatively using a multi-layer graph representation and a unique cross-attention&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vp9qazr2gw/cover.png"/></item><item><title>Take A Shortcut Back: Mitigating the Gradient Vanishing for Training Spiking Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/xjyu6zmzd7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xjyu6zmzd7/</guid><description>Shortcut back-propagation and an evolutionary training framework conquer gradient vanishing in spiking neural networks, drastically improving training and achieving state-of-the-art accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xjyu6zmzd7/cover.png"/></item><item><title>The Feature Speed Formula: a flexible approach to scale hyper-parameters of deep neural networks</title><link>https://deep-diver.github.io/neurips2024/posters/wshmb4j2o9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wshmb4j2o9/</guid><description>New &amp;lsquo;Feature Speed Formula&amp;rsquo; predicts &amp;amp; controls deep learning&amp;rsquo;s hierarchical feature learning by linking hyperparameter tuning to the angle between feature updates and backward pass.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wshmb4j2o9/cover.png"/></item><item><title>The Iterative Optimal Brain Surgeon: Faster Sparse Recovery by Leveraging Second-Order Information</title><link>https://deep-diver.github.io/neurips2024/posters/snxwd0q4ei/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/snxwd0q4ei/</guid><description>I-OBS, a novel family of sparse recovery algorithms leveraging second-order information, achieves faster convergence rates for sparse DNNs, validated by large-scale experiments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/snxwd0q4ei/cover.png"/></item><item><title>Understanding Representation of Deep Equilibrium Models from Neural Collapse Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/obuxeummq1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/obuxeummq1/</guid><description>Deep Equilibrium Models excel on imbalanced data due to feature convergence and self-duality properties, unlike explicit models, as shown through Neural Collapse analysis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/obuxeummq1/cover.png"/></item><item><title>Universal Physics Transformers: A Framework For Efficiently Scaling Neural Operators</title><link>https://deep-diver.github.io/neurips2024/posters/ouxinx5krm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ouxinx5krm/</guid><description>Universal Physics Transformers (UPTs) offer a unified, scalable framework for efficiently training neural operators across diverse spatio-temporal physics problems, overcoming limitations of existing &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ouxinx5krm/cover.png"/></item><item><title>Weight decay induces low-rank attention layers</title><link>https://deep-diver.github.io/neurips2024/posters/odeqjim9sk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/odeqjim9sk/</guid><description>Weight decay in deep learning surprisingly induces low-rank attention layers, potentially harming performance but offering optimization strategies for large language models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/odeqjim9sk/cover.png"/></item></channel></rss>