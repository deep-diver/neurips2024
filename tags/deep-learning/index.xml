<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/deep-learning/</link><description>Recent content in Deep Learning on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>$psilon$-Softmax: Approximating One-Hot Vectors for Mitigating Label Noise</title><link>https://deep-diver.github.io/neurips2024/posters/vjsd8bcipv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vjsd8bcipv/</guid><description>e-Softmax: A simple plug-and-play module enhances deep learning model robustness against noisy labels by approximating one-hot vectors, achieving noise-tolerant learning with controllable excess risk.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vjsd8bcipv/cover.png"/></item><item><title>4-bit Shampoo for Memory-Efficient Network Training</title><link>https://deep-diver.github.io/neurips2024/posters/asqdveifn7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/asqdveifn7/</guid><description>4-bit Shampoo achieves comparable performance to its 32-bit counterpart while drastically reducing memory usage, enabling efficient training of significantly larger neural networks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/asqdveifn7/cover.png"/></item><item><title>A Bayesian Approach to Data Point Selection</title><link>https://deep-diver.github.io/neurips2024/posters/9f5toxkomc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/9f5toxkomc/</guid><description>BADS: a novel Bayesian approach to data point selection efficiently optimizes deep learning models by jointly inferring instance weights and model parameters using stochastic gradient Langevin dynamic&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/9f5toxkomc/cover.png"/></item><item><title>A Canonicalization Perspective on Invariant and Equivariant Learning</title><link>https://deep-diver.github.io/neurips2024/posters/jjcy92fx4r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jjcy92fx4r/</guid><description>Canonicalization simplifies invariant and equivariant learning by connecting frames to canonical forms, leading to novel, superior frame designs for eigenvector symmetries.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jjcy92fx4r/cover.png"/></item><item><title>A Flexible, Equivariant Framework for Subgraph GNNs via Graph Products and Graph Coarsening</title><link>https://deep-diver.github.io/neurips2024/posters/9cfyqhjehc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/9cfyqhjehc/</guid><description>Flexible Subgraph GNNs, achieving scalability via graph products and coarsening, consistently outperform baselines and adapt to varying subgraph numbers.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/9cfyqhjehc/cover.png"/></item><item><title>A Foundation Model for Zero-shot Logical Query Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/jrsymbbji6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jrsymbbji6/</guid><description>ULTRAQUERY: a groundbreaking foundation model for zero-shot logical query reasoning on any knowledge graph, surpassing existing methods&amp;rsquo; limitations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jrsymbbji6/cover.png"/></item><item><title>A Functional Extension of Semi-Structured Networks</title><link>https://deep-diver.github.io/neurips2024/posters/wjaiaslhin/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wjaiaslhin/</guid><description>This paper introduces semi-structured functional networks (SSFNNs), a novel approach that combines interpretable functional regression models with deep neural networks, achieving both high accuracy an&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wjaiaslhin/cover.png"/></item><item><title>A Layer-Wise Natural Gradient Optimizer for Training Deep Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/nig3yyb6oa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nig3yyb6oa/</guid><description>LNGD: A Layer-Wise Natural Gradient optimizer drastically cuts deep neural network training time without sacrificing accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nig3yyb6oa/cover.png"/></item><item><title>A New Neural Kernel Regime: The Inductive Bias of Multi-Task Learning</title><link>https://deep-diver.github.io/neurips2024/posters/apbq3kamfa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/apbq3kamfa/</guid><description>Multi-task learning with shallow ReLU networks yields almost always unique solutions equivalent to kernel methods, unlike single-task settings.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/apbq3kamfa/cover.png"/></item><item><title>A PID Controller Approach for Adaptive Probability-dependent Gradient Decay in Model Calibration</title><link>https://deep-diver.github.io/neurips2024/posters/fanubdsfpn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fanubdsfpn/</guid><description>Deep learning models often suffer from overconfidence; this paper introduces a PID controller to adaptively adjust a probability-dependent gradient decay rate, ensuring consistent optimization of both&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fanubdsfpn/cover.png"/></item><item><title>A Recipe for Charge Density Prediction</title><link>https://deep-diver.github.io/neurips2024/posters/b7rekanutv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/b7rekanutv/</guid><description>A novel machine learning recipe drastically accelerates charge density prediction in density functional theory, achieving state-of-the-art accuracy while being significantly faster than existing metho&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/b7rekanutv/cover.png"/></item><item><title>A scalable generative model for dynamical system reconstruction from neuroimaging data</title><link>https://deep-diver.github.io/neurips2024/posters/exatqd4hsv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/exatqd4hsv/</guid><description>New scalable algorithm reconstructs brain dynamics from short neuroimaging data, overcoming limitations of existing methods and enabling more accurate, efficient analysis of large-scale brain activity&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/exatqd4hsv/cover.png"/></item><item><title>A Single-Step, Sharpness-Aware Minimization is All You Need to Achieve Efficient and Accurate Sparse Training</title><link>https://deep-diver.github.io/neurips2024/posters/mjgmmqmdu4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mjgmmqmdu4/</guid><description>Single-step Sharpness-Aware Minimization (S2-SAM) achieves efficient and accurate sparse training by approximating sharpness perturbation via prior gradient information, incurring zero extra cost and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mjgmmqmdu4/cover.png"/></item><item><title>A Topology-aware Graph Coarsening Framework for Continual Graph Learning</title><link>https://deep-diver.github.io/neurips2024/posters/vpineevlx0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vpineevlx0/</guid><description>TACO, a novel topology-aware graph coarsening framework, tackles catastrophic forgetting in continual graph learning by efficiently preserving topological information during experience replay, signifi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vpineevlx0/cover.png"/></item><item><title>A two-scale Complexity Measure for Deep Learning Models</title><link>https://deep-diver.github.io/neurips2024/posters/ty9voszzia/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ty9voszzia/</guid><description>New 2sED measure effectively bounds deep learning model complexity, correlating well with training error and offering efficient computation, particularly for deep models via a layerwise approach.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ty9voszzia/cover.png"/></item><item><title>A versatile informative diffusion model for single-cell ATAC-seq data generation and analysis</title><link>https://deep-diver.github.io/neurips2024/posters/s6ylebmowf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s6ylebmowf/</guid><description>ATAC-Diff: A versatile diffusion model for high-quality single-cell ATAC-seq data generation and analysis, surpassing state-of-the-art.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/s6ylebmowf/cover.png"/></item><item><title>Absorb &amp; Escape: Overcoming Single Model Limitations in Generating Heterogeneous Genomic Sequences</title><link>https://deep-diver.github.io/neurips2024/posters/xhtl2k1lyk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xhtl2k1lyk/</guid><description>Absorb &amp;amp; Escape: a novel post-training sampling method that overcomes single model limitations by combining Autoregressive (AR) and Diffusion Models (DMs), generating high-quality heterogeneous genomi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xhtl2k1lyk/cover.png"/></item><item><title>Accelerating Relative Entropy Coding with Space Partitioning</title><link>https://deep-diver.github.io/neurips2024/posters/ouqywnunxm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ouqywnunxm/</guid><description>Space partitioning dramatically speeds up relative entropy coding (REC) for neural compression, achieving 5-15% better bitrates than previous methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ouqywnunxm/cover.png"/></item><item><title>Activation Map Compression through Tensor Decomposition for Deep Learning</title><link>https://deep-diver.github.io/neurips2024/posters/s93hrwt8u9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s93hrwt8u9/</guid><description>Slash deep learning&amp;rsquo;s memory footprint! This paper introduces a novel activation map compression technique via tensor decomposition, significantly boosting on-device training efficiency for edge AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/s93hrwt8u9/cover.png"/></item><item><title>Ada-MSHyper: Adaptive Multi-Scale Hypergraph Transformer for Time Series Forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/rnbriq0se8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rnbriq0se8/</guid><description>Ada-MSHyper: A novel adaptive multi-scale hypergraph transformer significantly boosts time series forecasting accuracy by modeling group-wise interactions and handling complex temporal variations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rnbriq0se8/cover.png"/></item><item><title>Adapting to Unknown Low-Dimensional Structures in Score-Based Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/posters/sntxbqsrw7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sntxbqsrw7/</guid><description>Score-based diffusion models are improved by a novel coefficient design, enabling efficient adaptation to unknown low-dimensional data structures and achieving a convergence rate of O(kÂ²/âT).</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sntxbqsrw7/cover.png"/></item><item><title>Adaptive Depth Networks with Skippable Sub-Paths</title><link>https://deep-diver.github.io/neurips2024/posters/npu7cdk2f9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/npu7cdk2f9/</guid><description>Adaptive Depth Networks with Skippable Sub-Paths: Train once, deploy efficiently! This paper proposes a novel training method to create adaptive-depth networks, enabling on-demand model depth selectio&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/npu7cdk2f9/cover.png"/></item><item><title>Adaptive Passive-Aggressive Framework for Online Regression with Side Information</title><link>https://deep-diver.github.io/neurips2024/posters/kv80nc1afe/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kv80nc1afe/</guid><description>Adaptive Passive-Aggressive framework with Side information (APAS) significantly boosts online regression accuracy by dynamically adjusting thresholds and integrating side information, leading to supe&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kv80nc1afe/cover.png"/></item><item><title>Addressing Spectral Bias of Deep Neural Networks by Multi-Grade Deep Learning</title><link>https://deep-diver.github.io/neurips2024/posters/iort7ehfap/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/iort7ehfap/</guid><description>Multi-Grade Deep Learning (MGDL) conquers spectral bias in deep neural networks by incrementally learning low-frequency components, ultimately capturing high-frequency features through composition.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/iort7ehfap/cover.png"/></item><item><title>ADOPT: Modified Adam Can Converge with Any $eta_2$ with the Optimal Rate</title><link>https://deep-diver.github.io/neurips2024/posters/rzvvm0lsyk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rzvvm0lsyk/</guid><description>ADOPT, a novel adaptive gradient method, achieves optimal convergence rates without restrictive assumptions, unlike Adam, significantly improving deep learning optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rzvvm0lsyk/cover.png"/></item><item><title>Advancing Training Efficiency of Deep Spiking Neural Networks through Rate-based Backpropagation</title><link>https://deep-diver.github.io/neurips2024/posters/wlcm21c4nk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wlcm21c4nk/</guid><description>Rate-based backpropagation boosts deep spiking neural network training efficiency by leveraging rate coding, achieving comparable performance to BPTT with reduced complexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wlcm21c4nk/cover.png"/></item><item><title>Alias-Free Mamba Neural Operator</title><link>https://deep-diver.github.io/neurips2024/posters/guebxgv8jm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/guebxgv8jm/</guid><description>MambaNO: a novel neural operator achieving linear complexity and state-of-the-art accuracy in solving PDEs by cleverly balancing global and local information using an alias-free architecture.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/guebxgv8jm/cover.png"/></item><item><title>Aligning Target-Aware Molecule Diffusion Models with Exact Energy Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/ewcvxxtznu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ewcvxxtznu/</guid><description>ALIDIFF aligns target-aware molecule diffusion models with exact energy optimization, generating molecules with state-of-the-art binding energies and improved properties.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ewcvxxtznu/cover.png"/></item><item><title>Amortized Fourier Neural Operators</title><link>https://deep-diver.github.io/neurips2024/posters/a6em980m9x/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/a6em980m9x/</guid><description>Amortized Fourier Neural Operators (AM-FNOs) dramatically improve efficiency in solving PDEs by using neural networks for kernel parameterization, achieving up to 31% better accuracy compared to exist&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/a6em980m9x/cover.png"/></item><item><title>An exactly solvable model for emergence and scaling laws in the multitask sparse parity problem</title><link>https://deep-diver.github.io/neurips2024/posters/cuwsr25bbi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cuwsr25bbi/</guid><description>A novel multilinear model analytically explains the emergence and scaling laws of skills in the multitask sparse parity problem, accurately predicting skill emergence in neural networks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cuwsr25bbi/cover.png"/></item><item><title>ANT: Adaptive Noise Schedule for Time Series Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/posters/1ojaktylz4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1ojaktylz4/</guid><description>ANT: An adaptive noise schedule automatically determines optimal noise schedules for time series diffusion models, significantly boosting performance across diverse tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1ojaktylz4/cover.png"/></item><item><title>Approximately Equivariant Neural Processes</title><link>https://deep-diver.github.io/neurips2024/posters/dqt9mc5nql/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dqt9mc5nql/</guid><description>Boosting meta-learning, this paper introduces a novel, flexible approach to create approximately equivariant neural processes that outperform both non-equivariant and strictly equivariant counterparts&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dqt9mc5nql/cover.png"/></item><item><title>Approximation Rate of the Transformer Architecture for Sequence Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/zws2y21mzv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zws2y21mzv/</guid><description>This paper unveils the Transformer&amp;rsquo;s approximation power, deriving explicit Jackson-type rates to reveal its strengths and limitations in handling various sequential relationships.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zws2y21mzv/cover.png"/></item><item><title>Are Multiple Instance Learning Algorithms Learnable for Instances?</title><link>https://deep-diver.github.io/neurips2024/posters/cucvlgkqxp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cucvlgkqxp/</guid><description>Deep MIL algorithms&amp;rsquo; instance-level learnability is theoretically proven, revealing crucial conditions for success and highlighting gaps in existing models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cucvlgkqxp/cover.png"/></item><item><title>Are Self-Attentions Effective for Time Series Forecasting?</title><link>https://deep-diver.github.io/neurips2024/posters/in43sjoib7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/in43sjoib7/</guid><description>Cross-Attention-only Time Series Transformer (CATS) outperforms existing models by removing self-attention, improving long-term forecasting accuracy, and reducing computational cost.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/in43sjoib7/cover.png"/></item><item><title>Are Uncertainty Quantification Capabilities of Evidential Deep Learning a Mirage?</title><link>https://deep-diver.github.io/neurips2024/posters/p6nvdzrzrb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/p6nvdzrzrb/</guid><description>Evidential deep learning&amp;rsquo;s uncertainty quantification is unreliable; this paper reveals its limitations, proposes model uncertainty incorporation for improved performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/p6nvdzrzrb/cover.png"/></item><item><title>AROMA: Preserving Spatial Structure for Latent PDE Modeling with Local Neural Fields</title><link>https://deep-diver.github.io/neurips2024/posters/aj8rkcgwje/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/aj8rkcgwje/</guid><description>AROMA: Attentive Reduced Order Model with Attention enhances PDE modeling with local neural fields, offering efficient processing of diverse geometries and superior performance in simulating 1D and 2D&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/aj8rkcgwje/cover.png"/></item><item><title>Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/feyhzzn7kx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/feyhzzn7kx/</guid><description>Attraos: a novel long-term time series forecasting model leveraging chaos theory, significantly outperforms existing methods by utilizing attractor dynamics for efficient and accurate prediction.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/feyhzzn7kx/cover.png"/></item><item><title>B-ary Tree Push-Pull Method is Provably Efficient for Distributed Learning on Heterogeneous Data</title><link>https://deep-diver.github.io/neurips2024/posters/3mnxactbd3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3mnxactbd3/</guid><description>B-ary Tree Push-Pull (BTPP) achieves linear speedup for distributed learning on heterogeneous data, significantly outperforming state-of-the-art methods with minimal communication.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3mnxactbd3/cover.png"/></item><item><title>BAN: Detecting Backdoors Activated by Neuron Noise</title><link>https://deep-diver.github.io/neurips2024/posters/asyyszl4n5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/asyyszl4n5/</guid><description>BAN: a novel backdoor defense using adversarial neuron noise for efficient detection and mitigation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/asyyszl4n5/cover.png"/></item><item><title>Better by default: Strong pre-tuned MLPs and boosted trees on tabular data</title><link>https://deep-diver.github.io/neurips2024/posters/3bnpudvqmt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3bnpudvqmt/</guid><description>Strong pre-tuned MLPs and meta-tuned default parameters for GBDTs and MLPs improve tabular data classification and regression.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3bnpudvqmt/cover.png"/></item><item><title>Beyond Slow Signs in High-fidelity Model Extraction</title><link>https://deep-diver.github.io/neurips2024/posters/mrs9a1xqap/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mrs9a1xqap/</guid><description>Researchers drastically sped up high-fidelity deep learning model extraction, improving efficiency by up to 14.8x and challenging previous assumptions on the extraction bottleneck.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mrs9a1xqap/cover.png"/></item><item><title>Block Sparse Bayesian Learning: A Diversified Scheme</title><link>https://deep-diver.github.io/neurips2024/posters/a4cppx1xyg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/a4cppx1xyg/</guid><description>Diversified Block Sparse Bayesian Learning (DivSBL) improves block sparse signal recovery by adapting to unknown block structures, enhancing accuracy and robustness over existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/a4cppx1xyg/cover.png"/></item><item><title>Boosted Conformal Prediction Intervals</title><link>https://deep-diver.github.io/neurips2024/posters/tw032h2ons/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tw032h2ons/</guid><description>Boosting conformal prediction intervals improves accuracy and precision by tailoring them to specific desired properties via machine learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tw032h2ons/cover.png"/></item><item><title>Boosting Graph Pooling with Persistent Homology</title><link>https://deep-diver.github.io/neurips2024/posters/wcmqdy2aku/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wcmqdy2aku/</guid><description>Boosting graph neural networks: Topology-Invariant Pooling (TIP) leverages persistent homology to enhance graph pooling, achieving consistent performance gains across diverse datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wcmqdy2aku/cover.png"/></item><item><title>Breaking the curse of dimensionality in structured density estimation</title><link>https://deep-diver.github.io/neurips2024/posters/dwwin2ugye/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dwwin2ugye/</guid><description>Researchers break the curse of dimensionality in structured density estimation using graph resilience, a novel graphical parameter that effectively reduces the sample complexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dwwin2ugye/cover.png"/></item><item><title>Bridging Geometric States via Geometric Diffusion Bridge</title><link>https://deep-diver.github.io/neurips2024/posters/zcepob9rcr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zcepob9rcr/</guid><description>Geometric Diffusion Bridge (GDB) accurately predicts geometric state evolution in complex systems by leveraging a probabilistic approach and equivariant diffusion processes, surpassing existing deep l&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zcepob9rcr/cover.png"/></item><item><title>Bridging OOD Detection and Generalization: A Graph-Theoretic View</title><link>https://deep-diver.github.io/neurips2024/posters/qzwag8qxi1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qzwag8qxi1/</guid><description>A novel graph-theoretic framework bridges OOD detection &amp;amp; generalization, offering theoretical error bounds and competitive empirical performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qzwag8qxi1/cover.png"/></item><item><title>Cardinality-Aware Set Prediction and Top-$k$ Classification</title><link>https://deep-diver.github.io/neurips2024/posters/wat3qu737x/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wat3qu737x/</guid><description>This paper proposes cardinality-aware top-k classification, improving accuracy and efficiency by dynamically adjusting prediction set sizes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wat3qu737x/cover.png"/></item><item><title>Changing the Training Data Distribution to Reduce Simplicity Bias Improves In-distribution Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/yyspldusu2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yyspldusu2/</guid><description>Boosting in-distribution generalization is achieved by strategically altering the training data distribution to reduce simplicity bias and promote uniform feature learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yyspldusu2/cover.png"/></item><item><title>CODA: A Correlation-Oriented Disentanglement and Augmentation Modeling Scheme for Better Resisting Subpopulation Shifts</title><link>https://deep-diver.github.io/neurips2024/posters/lrsrjzzcle/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lrsrjzzcle/</guid><description>CODA: A novel modeling scheme tackles subpopulation shifts in machine learning by disentangling spurious correlations, augmenting data strategically, and using reweighted consistency loss for improved&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lrsrjzzcle/cover.png"/></item><item><title>Collaborative Refining for Learning from Inaccurate Labels</title><link>https://deep-diver.github.io/neurips2024/posters/rqqgbbqvbl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rqqgbbqvbl/</guid><description>Collaborative Refining for Learning from Inaccurate Labels (CRL) refines data using annotator agreement, improving model accuracy with noisy labels.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rqqgbbqvbl/cover.png"/></item><item><title>Combining Statistical Depth and Fermat Distance for Uncertainty Quantification</title><link>https://deep-diver.github.io/neurips2024/posters/xexrhtumcf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xexrhtumcf/</guid><description>Boosting neural network prediction reliability, this research ingeniously combines statistical depth and Fermat distance for superior uncertainty quantification, eliminating the need for distributiona&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xexrhtumcf/cover.png"/></item><item><title>Con4m: Context-aware Consistency Learning Framework for Segmented Time Series Classification</title><link>https://deep-diver.github.io/neurips2024/posters/jcpufqahvb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jcpufqahvb/</guid><description>Con4m, a novel consistency learning framework, leverages contextual information to effectively classify segmented time series with inconsistent boundary labels and varying durations of classes, signif&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jcpufqahvb/cover.png"/></item><item><title>CondTSF: One-line Plugin of Dataset Condensation for Time Series Forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/l1jajnwon5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/l1jajnwon5/</guid><description>CondTSF: One-line plugin for time series forecasting dataset condensation, boosting performance at low condensation ratios.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/l1jajnwon5/cover.png"/></item><item><title>Confidence Calibration of Classifiers with Many Classes</title><link>https://deep-diver.github.io/neurips2024/posters/ebbnkvxmcz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ebbnkvxmcz/</guid><description>Boost multi-class classifier calibration by cleverly transforming the problem into a single binary calibration task!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ebbnkvxmcz/cover.png"/></item><item><title>Conformal Prediction for Class-wise Coverage via Augmented Label Rank Calibration</title><link>https://deep-diver.github.io/neurips2024/posters/t7ds1ghwwu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/t7ds1ghwwu/</guid><description>RC3P, a novel algorithm, significantly reduces prediction set sizes in class-conditional conformal prediction while guaranteeing class-wise coverage, even on imbalanced datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/t7ds1ghwwu/cover.png"/></item><item><title>Conformalized Credal Set Predictors</title><link>https://deep-diver.github.io/neurips2024/posters/vbah12uvbd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vbah12uvbd/</guid><description>Conformal prediction empowers robust credal set predictions, handling aleatoric and epistemic uncertainties in classification, guaranteed to be valid with high probability!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vbah12uvbd/cover.png"/></item><item><title>Conformalized Multiple Testing after Data-dependent Selection</title><link>https://deep-diver.github.io/neurips2024/posters/8wvh0rzpsg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8wvh0rzpsg/</guid><description>This paper introduces Selective Conformal P-Value (SCPV), a novel method for controlling FDR in conformalized multiple testing after data-dependent selection, offering a unified theoretical framework &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8wvh0rzpsg/cover.png"/></item><item><title>Conformalized Time Series with Semantic Features</title><link>https://deep-diver.github.io/neurips2024/posters/kcdcavow1s/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kcdcavow1s/</guid><description>Conformalized Time Series with Semantic Features (CT-SSF) significantly improves time-series forecasting by dynamically weighting latent semantic features, achieving greater prediction efficiency whil&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kcdcavow1s/cover.png"/></item><item><title>Connectivity Shapes Implicit Regularization in Matrix Factorization Models for Matrix Completion</title><link>https://deep-diver.github.io/neurips2024/posters/9jgodkdh0f/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/9jgodkdh0f/</guid><description>Data connectivity profoundly shapes implicit regularization in matrix factorization for matrix completion, transitioning from low nuclear norm to low rank solutions as data shifts from disconnected to&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/9jgodkdh0f/cover.png"/></item><item><title>Consistency Models for Scalable and Fast Simulation-Based Inference</title><link>https://deep-diver.github.io/neurips2024/posters/ihjkpkljyh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ihjkpkljyh/</guid><description>CMPE: a new conditional sampler for SBI, achieves fast few-shot inference with an unconstrained architecture, outperforming current state-of-the-art algorithms on various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ihjkpkljyh/cover.png"/></item><item><title>Constant Acceleration Flow</title><link>https://deep-diver.github.io/neurips2024/posters/hsgnvc5ym9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hsgnvc5ym9/</guid><description>Constant Acceleration Flow (CAF) drastically accelerates image generation in diffusion models by leveraging a constant acceleration equation, outperforming state-of-the-art methods in both speed and q&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hsgnvc5ym9/cover.png"/></item><item><title>Constrained Diffusion Models via Dual Training</title><link>https://deep-diver.github.io/neurips2024/posters/es2ey2tgmm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/es2ey2tgmm/</guid><description>Constrained diffusion models, trained via a novel dual approach, achieve optimal trade-offs between data fidelity and user-defined distribution constraints, enabling fairer and more controlled data ge&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/es2ey2tgmm/cover.png"/></item><item><title>ControlSynth Neural ODEs: Modeling Dynamical Systems with Guaranteed Convergence</title><link>https://deep-diver.github.io/neurips2024/posters/dbe8khdmfs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dbe8khdmfs/</guid><description>ControlSynth Neural ODEs (CSODEs) guarantee convergence in complex dynamical systems via tractable linear inequalities, improving neural ODE modeling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dbe8khdmfs/cover.png"/></item><item><title>Convolutions and More as Einsum: A Tensor Network Perspective with Advances for Second-Order Methods</title><link>https://deep-diver.github.io/neurips2024/posters/cds8wxnmvp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cds8wxnmvp/</guid><description>This paper accelerates second-order optimization in CNNs by 4.5x, using a novel tensor network representation that simplifies convolutions and reduces memory overhead.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cds8wxnmvp/cover.png"/></item><item><title>Counter-Current Learning: A Biologically Plausible Dual Network Approach for Deep Learning</title><link>https://deep-diver.github.io/neurips2024/posters/l3rybqzrmf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/l3rybqzrmf/</guid><description>Biologically inspired Counter-Current Learning (CCL) uses dual networks for deep learning, offering comparable performance to other biologically plausible algorithms while enhancing biological realism&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/l3rybqzrmf/cover.png"/></item><item><title>Credal Deep Ensembles for Uncertainty Quantification</title><link>https://deep-diver.github.io/neurips2024/posters/pcgntigc9k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pcgntigc9k/</guid><description>Credal Deep Ensembles (CreDEs) improve uncertainty quantification in deep learning by predicting probability intervals, enhancing accuracy and calibration, particularly for out-of-distribution data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pcgntigc9k/cover.png"/></item><item><title>CRONOS: Enhancing Deep Learning with Scalable GPU Accelerated Convex Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/yflzyczao3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yflzyczao3/</guid><description>CRONOS: Scaling convex neural network training to ImageNet!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yflzyczao3/cover.png"/></item><item><title>Cross-Device Collaborative Test-Time Adaptation</title><link>https://deep-diver.github.io/neurips2024/posters/yymio0dwmi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yymio0dwmi/</guid><description>CoLA: Collaborative Lifelong Adaptation boosts test-time adaptation efficiency by sharing domain knowledge across multiple devices, achieving significant accuracy gains with minimal computational over&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yymio0dwmi/cover.png"/></item><item><title>DASH: Warm-Starting Neural Network Training in Stationary Settings without Loss of Plasticity</title><link>https://deep-diver.github.io/neurips2024/posters/idquuyma1t/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/idquuyma1t/</guid><description>DASH combats neural network training&amp;rsquo;s plasticity loss during warm-starting by selectively forgetting memorized noise while preserving features, improving accuracy and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/idquuyma1t/cover.png"/></item><item><title>Data Free Backdoor Attacks</title><link>https://deep-diver.github.io/neurips2024/posters/px71tm2mlh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/px71tm2mlh/</guid><description>Data-Free Backdoor Attacks (DFBA) injects undetectable backdoors into pre-trained classifiers without retraining or architectural changes, bypassing existing defenses.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/px71tm2mlh/cover.png"/></item><item><title>DDN: Dual-domain Dynamic Normalization for Non-stationary Time Series Forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/rvzfra6szo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rvzfra6szo/</guid><description>DDN: Dual-domain Dynamic Normalization dynamically improves time series forecasting accuracy by addressing data distribution changes in both time and frequency domains via a plug-in module.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rvzfra6szo/cover.png"/></item><item><title>Decomposable Transformer Point Processes</title><link>https://deep-diver.github.io/neurips2024/posters/oestejf0ls/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oestejf0ls/</guid><description>Decomposable Transformer Point Processes (DTPP) dramatically accelerates marked point process inference by using a mixture of log-normals for inter-event times and Transformers for marks, outperformin&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oestejf0ls/cover.png"/></item><item><title>Deep Equilibrium Algorithmic Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/sulxkxcena/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sulxkxcena/</guid><description>Deep Equilibrium Algorithmic Reasoners (DEARs) achieve superior performance on algorithmic tasks by directly solving for the equilibrium point of a neural network, eliminating the need for iterative r&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sulxkxcena/cover.png"/></item><item><title>Deep Graph Neural Networks via Posteriori-Sampling-based Node-Adaptative Residual Module</title><link>https://deep-diver.github.io/neurips2024/posters/vywzsaghp0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vywzsaghp0/</guid><description>PSNR, a novel node-adaptive residual module, significantly improves deep GNN performance by mitigating over-smoothing and handling missing data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vywzsaghp0/cover.png"/></item><item><title>Deep Learning Through A Telescoping Lens: A Simple Model Provides Empirical Insights On Grokking, Gradient Boosting &amp; Beyond</title><link>https://deep-diver.github.io/neurips2024/posters/nhucgztike/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nhucgztike/</guid><description>A simple, yet accurate model unveils deep learning&amp;rsquo;s mysteries, providing empirical insights into grokking, double descent, and gradient boosting, offering a new lens for analyzing neural network beha&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nhucgztike/cover.png"/></item><item><title>DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection</title><link>https://deep-diver.github.io/neurips2024/posters/ibkppabhvn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ibkppabhvn/</guid><description>DeepDRK, a novel deep learning approach, significantly improves feature selection by effectively balancing false discovery rate and power, surpassing existing methods, especially with limited data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ibkppabhvn/cover.png"/></item><item><title>DeepITE: Designing Variational Graph Autoencoders for Intervention Target Estimation</title><link>https://deep-diver.github.io/neurips2024/posters/gmsi9966dr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gmsi9966dr/</guid><description>DeepITE: a novel variational graph autoencoder, efficiently estimates intervention targets from both labeled and unlabeled data, surpassing existing methods in recall and inference speed.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gmsi9966dr/cover.png"/></item><item><title>DeepLag: Discovering Deep Lagrangian Dynamics for Intuitive Fluid Prediction</title><link>https://deep-diver.github.io/neurips2024/posters/scw6et4per/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/scw6et4per/</guid><description>DeepLag improves fluid prediction by uniquely combining Lagrangian and Eulerian perspectives, tracking key particles to reveal hidden dynamics and improve prediction accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/scw6et4per/cover.png"/></item><item><title>DEFT: Efficient Fine-tuning of Diffusion Models by Learning the Generalised $h$-transform</title><link>https://deep-diver.github.io/neurips2024/posters/akbtfqhcjm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/akbtfqhcjm/</guid><description>DEFT: A novel method efficiently fine-tunes diffusion models for conditional generation via a generalized h-transform, achieving state-of-the-art performance with significant speed improvements.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/akbtfqhcjm/cover.png"/></item><item><title>Dendritic Integration Inspired Artificial Neural Networks Capture Data Correlation</title><link>https://deep-diver.github.io/neurips2024/posters/2wqjnxzbhr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2wqjnxzbhr/</guid><description>Biologically-inspired Dit-CNNs leverage quadratic neuron integration to capture data correlation, achieving state-of-the-art performance on image classification benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2wqjnxzbhr/cover.png"/></item><item><title>Dense Associative Memory Through the Lens of Random Features</title><link>https://deep-diver.github.io/neurips2024/posters/164qnjsyjf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/164qnjsyjf/</guid><description>Boost associative memory capacity without extra parameters! DrDAM uses random features to approximate Dense Associative Memories, enabling efficient memory addition and retrieval.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/164qnjsyjf/cover.png"/></item><item><title>DePLM: Denoising Protein Language Models for Property Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/mu27zjhbcw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mu27zjhbcw/</guid><description>DePLM enhances protein optimization by denoising evolutionary information in protein language models via a rank-based diffusion process, improving mutation effect prediction and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mu27zjhbcw/cover.png"/></item><item><title>Derivative-enhanced Deep Operator Network</title><link>https://deep-diver.github.io/neurips2024/posters/waiqlgfqx6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/waiqlgfqx6/</guid><description>Derivative-enhanced DeepONets boost PDE solution accuracy and derivative approximation, particularly valuable with limited training data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/waiqlgfqx6/cover.png"/></item><item><title>DiffAug: A Diffuse-and-Denoise Augmentation for Training Robust Classifiers</title><link>https://deep-diver.github.io/neurips2024/posters/tpx9gczvbf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tpx9gczvbf/</guid><description>Boost classifier robustness with DiffAug, a novel diffusion-based augmentation method! One forward and reverse diffusion step enhances robustness against covariate shifts, adversarial examples, and o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tpx9gczvbf/cover.png"/></item><item><title>DiffPO: A causal diffusion model for learning distributions of potential outcomes</title><link>https://deep-diver.github.io/neurips2024/posters/merj77jipt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/merj77jipt/</guid><description>DiffPO: A causal diffusion model learns outcome distributions, offering reliable medical interventions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/merj77jipt/cover.png"/></item><item><title>Diffusion Twigs with Loop Guidance for Conditional Graph Generation</title><link>https://deep-diver.github.io/neurips2024/posters/fvocjaaylx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fvocjaaylx/</guid><description>Twigs: a novel score-based diffusion framework using multiple co-evolving flows and loop guidance for superior conditional graph generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fvocjaaylx/cover.png"/></item><item><title>DiffusionPDE: Generative PDE-Solving under Partial Observation</title><link>https://deep-diver.github.io/neurips2024/posters/z0i2sbjn0r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z0i2sbjn0r/</guid><description>DiffusionPDE uses generative diffusion models to solve PDEs accurately, even with highly incomplete observations, outperforming state-of-the-art methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/z0i2sbjn0r/cover.png"/></item><item><title>DiGRAF: Diffeomorphic Graph-Adaptive Activation Function</title><link>https://deep-diver.github.io/neurips2024/posters/zzow4z3le4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zzow4z3le4/</guid><description>DIGRAF, a novel graph-adaptive activation function, significantly boosts Graph Neural Network performance by dynamically adapting to graph structure, offering consistent superior results across divers&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zzow4z3le4/cover.png"/></item><item><title>DisCEdit: Model Editing by Identifying Discriminative Components</title><link>https://deep-diver.github.io/neurips2024/posters/tuiqq1g8i5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tuiqq1g8i5/</guid><description>DISCEDIT efficiently identifies and edits discriminative neural network components for structured pruning and class unlearning, achieving high sparsity and forgetting rates without needing training da&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tuiqq1g8i5/cover.png"/></item><item><title>Discrete-state Continuous-time Diffusion for Graph Generation</title><link>https://deep-diver.github.io/neurips2024/posters/ykskzehiyt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ykskzehiyt/</guid><description>DISCO: a novel discrete-state continuous-time diffusion model for flexible and efficient graph generation, outperforming state-of-the-art methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ykskzehiyt/cover.png"/></item><item><title>Divide-and-Conquer Posterior Sampling for Denoising Diffusion priors</title><link>https://deep-diver.github.io/neurips2024/posters/borut7m2x7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/borut7m2x7/</guid><description>Divide-and-Conquer Posterior Sampling (DCPS) efficiently samples complex posterior distributions from denoising diffusion models (DDMs) for Bayesian inverse problems, significantly improving accuracy &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/borut7m2x7/cover.png"/></item><item><title>DOFEN: Deep Oblivious Forest ENsemble</title><link>https://deep-diver.github.io/neurips2024/posters/umukvcdgi6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/umukvcdgi6/</guid><description>DOFEN: Deep Oblivious Forest Ensemble achieves state-of-the-art performance on tabular data by using a novel DNN architecture inspired by oblivious decision trees, surpassing other DNNs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/umukvcdgi6/cover.png"/></item><item><title>Dual Cone Gradient Descent for Training Physics-Informed Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/gvtcr7dhj3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gvtcr7dhj3/</guid><description>Dual Cone Gradient Descent (DCGD) enhances Physics-Informed Neural Network (PINN) training by resolving gradient imbalance issues, leading to more accurate and stable solutions for complex partial dif&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gvtcr7dhj3/cover.png"/></item><item><title>Dynamic Conditional Optimal Transport through Simulation-Free Flows</title><link>https://deep-diver.github.io/neurips2024/posters/tk0uarynhh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tk0uarynhh/</guid><description>Simulation-free flow generates conditional distributions via dynamic conditional optimal transport.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tk0uarynhh/cover.png"/></item><item><title>Dynamic Neural Regeneration: Enhancing Deep Learning Generalization on Small Datasets</title><link>https://deep-diver.github.io/neurips2024/posters/qcpcy0eqaj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qcpcy0eqaj/</guid><description>Dynamic Neural Regeneration (DNR) enhances deep learning generalization on small datasets using a data-aware dynamic masking scheme inspired by neurogenesis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qcpcy0eqaj/cover.png"/></item><item><title>Dynamic Rescaling for Training GNNs</title><link>https://deep-diver.github.io/neurips2024/posters/ifzwsrpqhl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ifzwsrpqhl/</guid><description>Dynamic rescaling boosts GNN training by controlling layer learning speeds and balancing networks, leading to faster training and improved generalization, especially on heterophilic graphs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ifzwsrpqhl/cover.png"/></item><item><title>EGonc : Energy-based Open-Set Node Classification with substitute Unknowns</title><link>https://deep-diver.github.io/neurips2024/posters/3cl2xdyaeb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3cl2xdyaeb/</guid><description>EGonc, a novel energy-based open-set node classification method, leverages substitute unknowns and energy scores to achieve superior accuracy and robustness in classifying nodes from known classes whi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3cl2xdyaeb/cover.png"/></item><item><title>einspace: Searching for Neural Architectures from Fundamental Operations</title><link>https://deep-diver.github.io/neurips2024/posters/qf1ncvibr5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qf1ncvibr5/</guid><description>Einspace: A novel neural architecture search space built from fundamental operations, enabling discovery of diverse high-performing network architectures and surpassing existing NAS methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qf1ncvibr5/cover.png"/></item><item><title>ElasTST: Towards Robust Varied-Horizon Forecasting with Elastic Time-Series Transformer</title><link>https://deep-diver.github.io/neurips2024/posters/ucxutmpwhv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ucxutmpwhv/</guid><description>ElasTST: A novel time-series transformer enables robust forecasting across various horizons without per-horizon training, enhancing adaptability and accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ucxutmpwhv/cover.png"/></item><item><title>Energy-based Hopfield Boosting for Out-of-Distribution Detection</title><link>https://deep-diver.github.io/neurips2024/posters/vlqytvmtyz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vlqytvmtyz/</guid><description>Hopfield Boosting, a novel energy-based boosting approach, achieves state-of-the-art OOD detection by leveraging Hopfield energy to sharpen the decision boundary between in-distribution and out-of-dis&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vlqytvmtyz/cover.png"/></item><item><title>Energy-Based Modelling for Discrete and Mixed Data via Heat Equations on Structured Spaces</title><link>https://deep-diver.github.io/neurips2024/posters/waqdvck1fv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/waqdvck1fv/</guid><description>Train discrete EBMs efficiently with Energy Discrepancy, a novel loss function that eliminates the need for Markov Chain Monte Carlo, using diffusion processes on structured spaces.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/waqdvck1fv/cover.png"/></item><item><title>Enhancing Diversity in Bayesian Deep Learning via Hyperspherical Energy Minimization of CKA</title><link>https://deep-diver.github.io/neurips2024/posters/s2ha6bz3le/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s2ha6bz3le/</guid><description>Boosting Bayesian deep learning&amp;rsquo;s diversity and uncertainty quantification, this study proposes hyperspherical energy minimization of CKA to generate diverse and reliable neural network ensembles and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/s2ha6bz3le/cover.png"/></item><item><title>Enhancing Protein Mutation Effect Prediction through a Retrieval-Augmented Framework</title><link>https://deep-diver.github.io/neurips2024/posters/lgehswiwef/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lgehswiwef/</guid><description>Revolutionizing protein mutation effect prediction, this work introduces a retrieval-augmented framework achieving state-of-the-art accuracy by efficiently incorporating similar local structure inform&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lgehswiwef/cover.png"/></item><item><title>EnOF-SNN: Training Accurate Spiking Neural Networks via Enhancing the Output Feature</title><link>https://deep-diver.github.io/neurips2024/posters/spcewp6eyt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/spcewp6eyt/</guid><description>EnOF-SNN boosts spiking neural network (SNN) accuracy by enhancing output feature representation using a novel knowledge distillation method and ReLU activation, outperforming current state-of-the-art&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/spcewp6eyt/cover.png"/></item><item><title>Equivariant Blurring Diffusion for Hierarchical Molecular Conformer Generation</title><link>https://deep-diver.github.io/neurips2024/posters/aj0zf28l6o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/aj0zf28l6o/</guid><description>Equivariant Blurring Diffusion (EBD) generates 3D molecular conformers hierarchically, first creating coarse-grained fragments then refining atomic details, significantly outperforming existing method&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/aj0zf28l6o/cover.png"/></item><item><title>Equivariant Machine Learning on Graphs with Nonlinear Spectral Filters</title><link>https://deep-diver.github.io/neurips2024/posters/y8p633e5hq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y8p633e5hq/</guid><description>Nonlinear spectral filters (NLSFs) enable fully equivariant graph neural networks, improving accuracy and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y8p633e5hq/cover.png"/></item><item><title>Equivariant Neural Diffusion for Molecule Generation</title><link>https://deep-diver.github.io/neurips2024/posters/40pe5pfhwl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/40pe5pfhwl/</guid><description>Equivariant Neural Diffusion (END) revolutionizes 3D molecule generation with a learnable forward process, achieving state-of-the-art results and enhanced controllability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/40pe5pfhwl/cover.png"/></item><item><title>Error Correction Output Codes for Robust Neural Networks against Weight-errors: A Neural Tangent Kernel Point of View</title><link>https://deep-diver.github.io/neurips2024/posters/7lim53jiic/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7lim53jiic/</guid><description>Boosting neural network robustness against weight errors, this research leverages neural tangent kernels to theoretically explain and optimize error-correcting output codes (ECOCs), achieving superior&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7lim53jiic/cover.png"/></item><item><title>Estimating Epistemic and Aleatoric Uncertainty with a Single Model</title><link>https://deep-diver.github.io/neurips2024/posters/wpxa6ocidg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wpxa6ocidg/</guid><description>HyperDM accurately estimates both epistemic and aleatoric uncertainty using a single model, overcoming the computational limitations of existing ensemble methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wpxa6ocidg/cover.png"/></item><item><title>Euclidean distance compression via deep random features</title><link>https://deep-diver.github.io/neurips2024/posters/fanbig8dr9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fanbig8dr9/</guid><description>Deep random features enable efficient Euclidean distance compression, offering improved bit storage compared to linear methods for specific parameter ranges, thus significantly advancing high-dimensio&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fanbig8dr9/cover.png"/></item><item><title>Evaluating the design space of diffusion-based generative models</title><link>https://deep-diver.github.io/neurips2024/posters/9cmorofb75/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/9cmorofb75/</guid><description>This paper provides the first complete error analysis for diffusion models, theoretically justifying optimal training and sampling strategies and design choices for enhanced generative capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/9cmorofb75/cover.png"/></item><item><title>Even Sparser Graph Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/k3k4bwunnk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/k3k4bwunnk/</guid><description>Spexphormer achieves significant memory reduction in graph Transformers by leveraging a two-stage training process that leverages attention score consistency across network widths to effectively spars&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/k3k4bwunnk/cover.png"/></item><item><title>Ex Uno Pluria: Insights on Ensembling in Low Precision Number Systems</title><link>https://deep-diver.github.io/neurips2024/posters/cbtkdwzzdq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cbtkdwzzdq/</guid><description>Low Precision Ensembling (LPE) boosts large model accuracy using training-free ensemble creation via stochastic rounding in low-precision number systems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cbtkdwzzdq/cover.png"/></item><item><title>Exploring Behavior-Relevant and Disentangled Neural Dynamics with Generative Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/posters/jl0esbfbav/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jl0esbfbav/</guid><description>BeNeDiff uses generative diffusion models to disentangle and interpret neural dynamics linked to specific behaviors, providing interpretable quantifications of behavior in multi-brain region datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jl0esbfbav/cover.png"/></item><item><title>eXponential FAmily Dynamical Systems (XFADS): Large-scale nonlinear Gaussian state-space modeling</title><link>https://deep-diver.github.io/neurips2024/posters/ln8ogihz2s/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ln8ogihz2s/</guid><description>XFADS: a novel low-rank structured VAE framework for large-scale nonlinear Gaussian state-space modeling, achieving high predictive accuracy and scalability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ln8ogihz2s/cover.png"/></item><item><title>Exponential Quantum Communication Advantage in Distributed Inference and Learning</title><link>https://deep-diver.github.io/neurips2024/posters/ggr9djbe3r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ggr9djbe3r/</guid><description>Quantum computing drastically reduces communication needs for distributed machine learning, enabling faster and more private AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ggr9djbe3r/cover.png"/></item><item><title>Fast yet Safe: Early-Exiting with Risk Control</title><link>https://deep-diver.github.io/neurips2024/posters/bbfjpasrgs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bbfjpasrgs/</guid><description>Risk control boosts early-exit neural networks&amp;rsquo; speed and safety by ensuring accurate predictions before exiting early, achieving substantial computational savings across diverse tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bbfjpasrgs/cover.png"/></item><item><title>Faster Local Solvers for Graph Diffusion Equations</title><link>https://deep-diver.github.io/neurips2024/posters/3z0ltdjim0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3z0ltdjim0/</guid><description>Revolutionizing graph analysis, this paper introduces a novel framework for efficiently solving graph diffusion equations, achieving up to a hundred-fold speed improvement and enabling faster graph ne&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3z0ltdjim0/cover.png"/></item><item><title>FilterNet: Harnessing Frequency Filters for Time Series Forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/ugl2d9idad/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ugl2d9idad/</guid><description>FilterNet: A novel deep learning architecture using learnable frequency filters for superior time series forecasting accuracy and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ugl2d9idad/cover.png"/></item><item><title>Fine-Grained Dynamic Framework for Bias-Variance Joint Optimization on Data Missing Not at Random</title><link>https://deep-diver.github.io/neurips2024/posters/gloe70tn8v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gloe70tn8v/</guid><description>A new fine-grained dynamic framework jointly optimizes bias and variance for accurate predictions from missing-not-at-random data, surpassing existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gloe70tn8v/cover.png"/></item><item><title>FlexSBDD: Structure-Based Drug Design with Flexible Protein Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/4ab54h21qg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4ab54h21qg/</guid><description>FlexSBDD, a novel deep generative model, accurately predicts flexible protein-ligand complex structures, generating high-affinity drug molecules while overcoming the limitations of rigid protein model&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4ab54h21qg/cover.png"/></item><item><title>Foundation Inference Models for Markov Jump Processes</title><link>https://deep-diver.github.io/neurips2024/posters/f4v7cmm5sc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/f4v7cmm5sc/</guid><description>Zero-shot learning achieves accurate Markov jump process inference across diverse datasets, eliminating the need for extensive model retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/f4v7cmm5sc/cover.png"/></item><item><title>Frequency Adaptive Normalization For Non-stationary Time Series Forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/t0axiflvdd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/t0axiflvdd/</guid><description>Frequency Adaptive Normalization (FAN) significantly boosts non-stationary time series forecasting accuracy by using Fourier transforms to identify and model dynamic trends and seasonal patterns, achi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/t0axiflvdd/cover.png"/></item><item><title>Frequency-aware Generative Models for Multivariate Time Series Imputation</title><link>https://deep-diver.github.io/neurips2024/posters/ue6cermnq3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ue6cermnq3/</guid><description>FGTI: a novel frequency-aware model significantly improves multivariate time series imputation by focusing on the often-overlooked residual term, leveraging high-frequency information to enhance accur&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ue6cermnq3/cover.png"/></item><item><title>From Biased to Unbiased Dynamics: An Infinitesimal Generator Approach</title><link>https://deep-diver.github.io/neurips2024/posters/tgmwp9jjxl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tgmwp9jjxl/</guid><description>Learn unbiased molecular dynamics from limited biased data using a novel infinitesimal generator approach; accurately estimating eigenfunctions and eigenvalues even with suboptimal biasing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tgmwp9jjxl/cover.png"/></item><item><title>From Similarity to Superiority: Channel Clustering for Time Series Forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/mdgn9aazo0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mdgn9aazo0/</guid><description>Channel Clustering Module (CCM) boosts time series forecasting accuracy by intelligently grouping similar channels, improving model performance and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mdgn9aazo0/cover.png"/></item><item><title>FSP-Laplace: Function-Space Priors for the Laplace Approximation in Bayesian Deep Learning</title><link>https://deep-diver.github.io/neurips2024/posters/83vxe8alv4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/83vxe8alv4/</guid><description>FSP-LAPLACE efficiently integrates interpretable function-space priors into Bayesian deep learning via a novel Laplace approximation, significantly improving uncertainty estimates and model performanc&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/83vxe8alv4/cover.png"/></item><item><title>Full-Atom Peptide Design with Geometric Latent Diffusion</title><link>https://deep-diver.github.io/neurips2024/posters/iaqnjuje8q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/iaqnjuje8q/</guid><description>PepGLAD, a novel generative model, revolutionizes full-atom peptide design by leveraging geometric latent diffusion to significantly enhance peptide diversity and binding affinity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/iaqnjuje8q/cover.png"/></item><item><title>Functional Gradient Flows for Constrained Sampling</title><link>https://deep-diver.github.io/neurips2024/posters/kpo6zcgvzh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kpo6zcgvzh/</guid><description>Constrained sampling solved! New functional gradient flow method (CFG) efficiently samples from constrained probability distributions via a novel boundary condition for gradient flows, achieving prov&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kpo6zcgvzh/cover.png"/></item><item><title>GDeR: Safeguarding Efficiency, Balancing, and Robustness via Prototypical Graph Pruning</title><link>https://deep-diver.github.io/neurips2024/posters/o97bzln9wh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/o97bzln9wh/</guid><description>GDeR: A novel dynamic graph pruning method boosts GNN training efficiency and robustness by intelligently selecting a representative subset of training data, mitigating issues caused by imbalanced or &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/o97bzln9wh/cover.png"/></item><item><title>Gene-Gene Relationship Modeling Based on Genetic Evidence for Single-Cell RNA-Seq Data Imputation</title><link>https://deep-diver.github.io/neurips2024/posters/gw0zng5jcg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gw0zng5jcg/</guid><description>Novel imputation method, scCR, leverages complete gene-gene relationships (associating &amp;amp; dissociating) for superior single-cell RNA sequencing data recovery, significantly outperforming current state-&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gw0zng5jcg/cover.png"/></item><item><title>Generalized Fast Exact Conformalization</title><link>https://deep-diver.github.io/neurips2024/posters/knzyj5zqsg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/knzyj5zqsg/</guid><description>This paper presents a novel method for fast and exact conformalization, leveraging inherent piecewise smoothness to dramatically accelerate uncertainty quantification in machine learning models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/knzyj5zqsg/cover.png"/></item><item><title>Generalizing CNNs to graphs with learnable neighborhood quantization</title><link>https://deep-diver.github.io/neurips2024/posters/dyiqazxqnv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dyiqazxqnv/</guid><description>QGCNs generalize CNNs to graph data via learnable neighborhood quantization, achieving state-of-the-art performance on graph datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dyiqazxqnv/cover.png"/></item><item><title>Generative Modeling of Molecular Dynamics Trajectories</title><link>https://deep-diver.github.io/neurips2024/posters/yrrch1osgw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yrrch1osgw/</guid><description>MDGEN: Generative modeling unlocks MD data for diverse tasks, achieving significant speedups via flexible multi-task surrogate models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yrrch1osgw/cover.png"/></item><item><title>Genetic-guided GFlowNets for Sample Efficient Molecular Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/b4q98aazwt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/b4q98aazwt/</guid><description>Genetic-guided GFlowNets revolutionize sample-efficient molecular optimization by smartly integrating genetic algorithms into GFlowNets training, achieving state-of-the-art performance with substantia&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/b4q98aazwt/cover.png"/></item><item><title>GENOT: Entropic (Gromov) Wasserstein Flow Matching with Applications to Single-Cell Genomics</title><link>https://deep-diver.github.io/neurips2024/posters/hjspwd7jvg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hjspwd7jvg/</guid><description>GENOT: a flexible neural optimal transport framework for single-cell genomics, enabling stochastic map learning with any cost function, handling unbalanced data, and tackling complex (Fused) Gromov-Wa&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hjspwd7jvg/cover.png"/></item><item><title>Geometric Trajectory Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/posters/oymms5mv9h/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oymms5mv9h/</guid><description>GeoTDM: First diffusion model generating realistic 3D geometric trajectories, capturing complex spatial interactions and temporal correspondence, significantly improving generation quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oymms5mv9h/cover.png"/></item><item><title>Geometry Awakening: Cross-Geometry Learning Exhibits Superiority over Individual Structures</title><link>https://deep-diver.github.io/neurips2024/posters/347adobxea/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/347adobxea/</guid><description>Cross-geometry learning using knowledge distillation significantly improves GNN performance by leveraging both Euclidean and hyperbolic geometric properties of graph data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/347adobxea/cover.png"/></item><item><title>Geometry of naturalistic object representations in recurrent neural network models of working memory</title><link>https://deep-diver.github.io/neurips2024/posters/n2rac7lo6k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/n2rac7lo6k/</guid><description>RNNs represent naturalistic objects in WM using chronological subspaces, defying traditional slot models; object features are less orthogonalized in RNNs vs. perceptual space.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/n2rac7lo6k/cover.png"/></item><item><title>Geometry-aware training of factorized layers in tensor Tucker format</title><link>https://deep-diver.github.io/neurips2024/posters/abtcfcrjm3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/abtcfcrjm3/</guid><description>Train factorized neural network layers efficiently with Geometry-aware training in Tucker format (TDLRT)!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/abtcfcrjm3/cover.png"/></item><item><title>Gradient Rewiring for Editable Graph Neural Network Training</title><link>https://deep-diver.github.io/neurips2024/posters/xy2qrq7cxm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xy2qrq7cxm/</guid><description>Gradient Rewiring (GRE) improves editable GNN training by addressing gradient inconsistencies, preserving training node performance while correcting target node errors.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xy2qrq7cxm/cover.png"/></item><item><title>Gradient-based Discrete Sampling with Automatic Cyclical Scheduling</title><link>https://deep-diver.github.io/neurips2024/posters/4syq5cgwa2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4syq5cgwa2/</guid><description>ACS: Automatic Cyclical Scheduling revolutionizes gradient-based discrete sampling by intelligently switching between exploration and exploitation phases to efficiently navigate complex multimodal dis&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4syq5cgwa2/cover.png"/></item><item><title>GRANOLA: Adaptive Normalization for Graph Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/qd8blc0o0f/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qd8blc0o0f/</guid><description>GRANOLA: A novel graph-adaptive normalization layer significantly boosts GNN performance by dynamically adjusting node features based on the input graph&amp;rsquo;s unique structure.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qd8blc0o0f/cover.png"/></item><item><title>Graph Classification via Reference Distribution Learning: Theory and Practice</title><link>https://deep-diver.github.io/neurips2024/posters/1zvinhehks/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1zvinhehks/</guid><description>GRDL: a novel graph classification method boasting 10x speed improvement over competitors, achieved by treating node embeddings as distributions and avoiding global pooling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1zvinhehks/cover.png"/></item><item><title>Graph Edit Distance with General Costs Using Neural Set Divergence</title><link>https://deep-diver.github.io/neurips2024/posters/u7jrmrgutt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/u7jrmrgutt/</guid><description>GRAPHEDX, a novel neural network, accurately estimates graph edit distance with varying operation costs, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/u7jrmrgutt/cover.png"/></item><item><title>Graph Neural Flows for Unveiling Systemic Interactions Among Irregularly Sampled Time Series</title><link>https://deep-diver.github.io/neurips2024/posters/tfb5ssabvb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tfb5ssabvb/</guid><description>GNeuralFlow unveils systemic interactions in irregularly sampled time series by learning a directed acyclic graph representing conditional dependencies, achieving superior performance in classificatio&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tfb5ssabvb/cover.png"/></item><item><title>Graph Neural Networks Need Cluster-Normalize-Activate Modules</title><link>https://deep-diver.github.io/neurips2024/posters/faj2ebhdhc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/faj2ebhdhc/</guid><description>Boost GNN performance and overcome oversmoothing with Cluster-Normalize-Activate (CNA) modules: a simple yet highly effective plug-and-play solution!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/faj2ebhdhc/cover.png"/></item><item><title>GraphMETRO: Mitigating Complex Graph Distribution Shifts via Mixture of Aligned Experts</title><link>https://deep-diver.github.io/neurips2024/posters/qtyg4g3deu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qtyg4g3deu/</guid><description>GraphMETRO tackles complex graph distribution shifts by using a Mixture-of-Experts model to decompose shifts into interpretable components, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qtyg4g3deu/cover.png"/></item><item><title>Great Minds Think Alike: The Universal Convergence Trend of Input Salience</title><link>https://deep-diver.github.io/neurips2024/posters/7poryhql4v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7poryhql4v/</guid><description>Deep neural networks surprisingly exhibit universal convergence in input salience, aligning more closely as model capacity increases, revealing valuable insights into model behavior and improving deep&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7poryhql4v/cover.png"/></item><item><title>Group and Shuffle: Efficient Structured Orthogonal Parametrization</title><link>https://deep-diver.github.io/neurips2024/posters/7eqx56ysb2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7eqx56ysb2/</guid><description>Group-and-Shuffle (GS) matrices enable efficient structured orthogonal parametrization, improving parameter and computational efficiency in orthogonal fine-tuning for deep learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7eqx56ysb2/cover.png"/></item><item><title>Guiding Neural Collapse: Optimising Towards the Nearest Simplex Equiangular Tight Frame</title><link>https://deep-diver.github.io/neurips2024/posters/z4fapuslma/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z4fapuslma/</guid><description>Researchers devised a novel method to accelerate neural network training by guiding the optimization process toward a Simplex Equiangular Tight Frame, exploiting the Neural Collapse phenomenon to enha&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/z4fapuslma/cover.png"/></item><item><title>Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects Models</title><link>https://deep-diver.github.io/neurips2024/posters/uxuobobjho/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uxuobobjho/</guid><description>Accelerate Bayesian inference in linear mixed-effects models by efficiently marginalizing random effects using fast linear algebra, enabling faster and more accurate posterior estimations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uxuobobjho/cover.png"/></item><item><title>Hamiltonian Monte Carlo on ReLU Neural Networks is Inefficient</title><link>https://deep-diver.github.io/neurips2024/posters/abmiyi7ia7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/abmiyi7ia7/</guid><description>Hamiltonian Monte Carlo struggles with ReLU neural networks: high rejection rates hinder Bayesian deep learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/abmiyi7ia7/cover.png"/></item><item><title>HEPrune: Fast Private Training of Deep Neural Networks With Encrypted Data Pruning</title><link>https://deep-diver.github.io/neurips2024/posters/y2famldtif/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y2famldtif/</guid><description>HEPrune accelerates private deep learning training 16x by integrating encrypted data pruning, achieving this speedup with minimal accuracy loss.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y2famldtif/cover.png"/></item><item><title>HHD-GP: Incorporating Helmholtz-Hodge Decomposition into Gaussian Processes for Learning Dynamical Systems</title><link>https://deep-diver.github.io/neurips2024/posters/t9pfjvimij/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/t9pfjvimij/</guid><description>HHD-GP leverages Helmholtz-Hodge decomposition within Gaussian Processes to learn physically meaningful components of dynamical systems, enhancing prediction accuracy and interpretability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/t9pfjvimij/cover.png"/></item><item><title>Hierarchical Hybrid Sliced Wasserstein: A Scalable Metric for Heterogeneous Joint Distributions</title><link>https://deep-diver.github.io/neurips2024/posters/xwrmd1njqq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xwrmd1njqq/</guid><description>Hierarchical Hybrid Sliced Wasserstein (H2SW) solves the challenge of comparing complex, heterogeneous joint distributions by introducing novel slicing operators, leading to a scalable and statistical&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xwrmd1njqq/cover.png"/></item><item><title>Higher-Rank Irreducible Cartesian Tensors for Equivariant Message Passing</title><link>https://deep-diver.github.io/neurips2024/posters/fabxevbnqz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fabxevbnqz/</guid><description>Higher-rank irreducible Cartesian tensors boost accuracy and efficiency in equivariant message-passing neural networks for atomistic simulations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fabxevbnqz/cover.png"/></item><item><title>HORSE: Hierarchical Representation for Large-Scale Neural Subset Selection</title><link>https://deep-diver.github.io/neurips2024/posters/donsoc7ry1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/donsoc7ry1/</guid><description>HORSE: A novel attention-based neural network significantly improves large-scale neural subset selection by up to 20%, addressing limitations in existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/donsoc7ry1/cover.png"/></item><item><title>How many classifiers do we need?</title><link>https://deep-diver.github.io/neurips2024/posters/m5dykarvn8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/m5dykarvn8/</guid><description>Boost ensemble accuracy by predicting performance with fewer classifiers using a novel polarization law and refined error bounds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/m5dykarvn8/cover.png"/></item><item><title>How Sparse Can We Prune A Deep Network: A Fundamental Limit Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/iaapholhcx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/iaapholhcx/</guid><description>Deep network pruning&amp;rsquo;s fundamental limits are characterized, revealing how weight magnitude and network sharpness determine the maximum achievable sparsity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/iaapholhcx/cover.png"/></item><item><title>Hyper-opinion Evidential Deep Learning for Out-of-Distribution Detection</title><link>https://deep-diver.github.io/neurips2024/posters/te8vi2wgth/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/te8vi2wgth/</guid><description>Hyper-opinion Evidential Deep Learning (HEDL) enhances out-of-distribution detection by integrating sharp and vague evidence for superior uncertainty estimation and classification accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/te8vi2wgth/cover.png"/></item><item><title>HyperLogic: Enhancing Diversity and Accuracy in Rule Learning with HyperNets</title><link>https://deep-diver.github.io/neurips2024/posters/gjbzykgfd6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gjbzykgfd6/</guid><description>HyperLogic uses hypernetworks to generate diverse, accurate, and concise rule sets from neural networks, enhancing both interpretability and accuracy in rule learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gjbzykgfd6/cover.png"/></item><item><title>Implicitly Guided Design with PropEn: Match your Data to Follow the Gradient</title><link>https://deep-diver.github.io/neurips2024/posters/dhfho90ink/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dhfho90ink/</guid><description>PropEn: a novel framework for implicitly guided design optimization that leverages &amp;lsquo;matching&amp;rsquo; to boost efficiency by matching samples and approximating the gradient without a discriminator.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dhfho90ink/cover.png"/></item><item><title>Improved off-policy training of diffusion samplers</title><link>https://deep-diver.github.io/neurips2024/posters/vieiamy2gi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vieiamy2gi/</guid><description>Researchers enhanced diffusion samplers by developing a novel exploration strategy and a unified library, improving sample quality and addressing reproducibility challenges.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vieiamy2gi/cover.png"/></item><item><title>Improved Sample Complexity Bounds for Diffusion Model Training</title><link>https://deep-diver.github.io/neurips2024/posters/oxcqkyoy8q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oxcqkyoy8q/</guid><description>Training high-quality diffusion models efficiently is now possible, thanks to novel sample complexity bounds improving exponentially on previous work.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oxcqkyoy8q/cover.png"/></item><item><title>Improving Deep Learning Optimization through Constrained Parameter Regularization</title><link>https://deep-diver.github.io/neurips2024/posters/rcxtkihkbf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rcxtkihkbf/</guid><description>Constrained Parameter Regularization (CPR) outperforms traditional weight decay by dynamically adapting regularization strengths for individual parameters, leading to better deep learning model perfor&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rcxtkihkbf/cover.png"/></item><item><title>Improving Equivariant Model Training via Constraint Relaxation</title><link>https://deep-diver.github.io/neurips2024/posters/twkl7k1u5v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/twkl7k1u5v/</guid><description>Boost equivariant model training by strategically relaxing constraints during training, enhancing optimization and generalization!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/twkl7k1u5v/cover.png"/></item><item><title>Improving Generalization and Convergence by Enhancing Implicit Regularization</title><link>https://deep-diver.github.io/neurips2024/posters/cjm2bhloic/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cjm2bhloic/</guid><description>IRE framework expedites the discovery of flat minima in deep learning, enhancing generalization and convergence. By decoupling the dynamics of flat and sharp directions, IRE boosts sharpness reduction&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cjm2bhloic/cover.png"/></item><item><title>Improving Neural Network Surface Processing with Principal Curvatures</title><link>https://deep-diver.github.io/neurips2024/posters/8koaqrdryh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8koaqrdryh/</guid><description>Boosting neural network surface processing: Using principal curvatures as input significantly improves segmentation and classification accuracy while reducing computational overhead.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8koaqrdryh/cover.png"/></item><item><title>Improving Neural ODE Training with Temporal Adaptive Batch Normalization</title><link>https://deep-diver.github.io/neurips2024/posters/arleuvvftl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/arleuvvftl/</guid><description>Boosting Neural ODE training, Temporal Adaptive Batch Normalization (TA-BN) resolves traditional Batch Normalization&amp;rsquo;s limitations by providing a continuous-time counterpart, enabling deeper networks &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/arleuvvftl/cover.png"/></item><item><title>Improving Temporal Link Prediction via Temporal Walk Matrix Projection</title><link>https://deep-diver.github.io/neurips2024/posters/ti3ciyqls3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ti3ciyqls3/</guid><description>TPNet boosts temporal link prediction accuracy and efficiency by unifying relative encodings via temporal walk matrices and using random feature propagation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ti3ciyqls3/cover.png"/></item><item><title>Inference of Neural Dynamics Using Switching Recurrent Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/zb8jlah2vn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zb8jlah2vn/</guid><description>SRNNs reveal behaviorally-relevant neural dynamics switches!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zb8jlah2vn/cover.png"/></item><item><title>Inferring stochastic low-rank recurrent neural networks from neural data</title><link>https://deep-diver.github.io/neurips2024/posters/c0ehyopptn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/c0ehyopptn/</guid><description>Researchers developed a method using variational sequential Monte Carlo to fit stochastic low-rank recurrent neural networks to neural data, enabling efficient analysis and generation of realistic neu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/c0ehyopptn/cover.png"/></item><item><title>Infinite Limits of Multi-head Transformer Dynamics</title><link>https://deep-diver.github.io/neurips2024/posters/p0bbkhd5ai/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/p0bbkhd5ai/</guid><description>Researchers reveal how the training dynamics of transformer models behave at infinite width, depth, and head count, providing key insights for scaling up these models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/p0bbkhd5ai/cover.png"/></item><item><title>Inflationary Flows: Calibrated Bayesian Inference with Diffusion-Based Models</title><link>https://deep-diver.github.io/neurips2024/posters/im4ltyrwde/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/im4ltyrwde/</guid><description>Calibrated Bayesian inference achieved via novel diffusion models uniquely mapping high-dimensional data to lower-dimensional Gaussian distributions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/im4ltyrwde/cover.png"/></item><item><title>Infusing Self-Consistency into Density Functional Theory Hamiltonian Prediction via Deep Equilibrium Models</title><link>https://deep-diver.github.io/neurips2024/posters/psvkinbs4u/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/psvkinbs4u/</guid><description>Deep Equilibrium Models (DEQs) infused into DFT Hamiltonian prediction achieves self-consistency, accelerating large-scale materials simulations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/psvkinbs4u/cover.png"/></item><item><title>Integrating GNN and Neural ODEs for Estimating Non-Reciprocal Two-Body Interactions in Mixed-Species Collective Motion</title><link>https://deep-diver.github.io/neurips2024/posters/qwl3eidi9r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qwl3eidi9r/</guid><description>Deep learning framework integrating GNNs and neural ODEs precisely estimates non-reciprocal two-body interactions in mixed-species collective motion, accurately replicating both individual and collect&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qwl3eidi9r/cover.png"/></item><item><title>Introducing Spectral Attention for Long-Range Dependency in Time Series Forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/dxynvebqmp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dxynvebqmp/</guid><description>Spectral Attention boosts long-range dependency capture in time series forecasting, achieving state-of-the-art results across various models and datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dxynvebqmp/cover.png"/></item><item><title>Inverse M-Kernels for Linear Universal Approximators of Non-Negative Functions</title><link>https://deep-diver.github.io/neurips2024/posters/hgss4ono4s/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hgss4ono4s/</guid><description>Unlocking efficient non-negative function approximation: This paper introduces inverse M-kernels, enabling flexible, linear universal approximators for one-dimensional inputs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hgss4ono4s/cover.png"/></item><item><title>Iteratively Refined Early Interaction Alignment for Subgraph Matching based Graph Retrieval</title><link>https://deep-diver.github.io/neurips2024/posters/udtwwf7tks/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/udtwwf7tks/</guid><description>IsoNet++ iteratively refines subgraph matching via early interaction GNNs and node-pair partner interactions, significantly boosting graph retrieval accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/udtwwf7tks/cover.png"/></item><item><title>Kernel PCA for Out-of-Distribution Detection</title><link>https://deep-diver.github.io/neurips2024/posters/ezpkbc1ohs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ezpkbc1ohs/</guid><description>Boosting Out-of-Distribution Detection with Kernel PCA!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ezpkbc1ohs/cover.png"/></item><item><title>Knowledge Graph Completion by Intermediate Variables Regularization</title><link>https://deep-diver.github.io/neurips2024/posters/d226uywyuo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/d226uywyuo/</guid><description>Novel intermediate variables regularization boosts knowledge graph completion!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/d226uywyuo/cover.png"/></item><item><title>Large Stepsize Gradient Descent for Non-Homogeneous Two-Layer Networks: Margin Improvement and Fast Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/chloluhnai/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/chloluhnai/</guid><description>Large stepsize GD on non-homogeneous neural networks shows monotonic risk reduction after an initial oscillating phase, demonstrating implicit bias and optimization gains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/chloluhnai/cover.png"/></item><item><title>Layer-Adaptive State Pruning for Deep State Space Models</title><link>https://deep-diver.github.io/neurips2024/posters/t9gbbwbnqg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/t9gbbwbnqg/</guid><description>Layer-Adaptive STate pruning (LAST) optimizes deep state space models by efficiently reducing state dimensions, improving performance and scalability without retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/t9gbbwbnqg/cover.png"/></item><item><title>Learning from higher-order correlations, efficiently: hypothesis tests, random features, and neural networks</title><link>https://deep-diver.github.io/neurips2024/posters/uhml6eyovf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uhml6eyovf/</guid><description>Neural networks learn efficiently from higher-order correlations, exceeding the capabilities of random features, as demonstrated through hypothesis tests and novel theoretical analysis in high-dimensi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uhml6eyovf/cover.png"/></item><item><title>Learning from Highly Sparse Spatio-temporal Data</title><link>https://deep-diver.github.io/neurips2024/posters/rtonicccjm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rtonicccjm/</guid><description>OPCR, a novel one-step spatio-temporal imputation method, surpasses existing iterative approaches by directly propagating limited observations to the global context, achieving superior accuracy and ef&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rtonicccjm/cover.png"/></item><item><title>Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate</title><link>https://deep-diver.github.io/neurips2024/posters/b1foes6cyq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/b1foes6cyq/</guid><description>Boost deep learning generalization with Learning from Teaching (LOT)! LOT trains auxiliary &amp;lsquo;student&amp;rsquo; models to imitate a primary &amp;rsquo;teacher&amp;rsquo; model, improving the teacher&amp;rsquo;s ability to capture generalizab&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/b1foes6cyq/cover.png"/></item><item><title>Learning Infinitesimal Generators of Continuous Symmetries from Data</title><link>https://deep-diver.github.io/neurips2024/posters/wl44w8xpc7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wl44w8xpc7/</guid><description>Learn continuous symmetries from data without pre-defined groups using Neural ODEs and a novel validity score to improve model generalization and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wl44w8xpc7/cover.png"/></item><item><title>Learning Macroscopic Dynamics from Partial Microscopic Observations</title><link>https://deep-diver.github.io/neurips2024/posters/cjh0qsgd0d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cjh0qsgd0d/</guid><description>Learn macroscopic dynamics efficiently using only partial microscopic force computations! This novel method leverages sparsity assumptions and stochastic estimation for accurate, cost-effective modeli&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cjh0qsgd0d/cover.png"/></item><item><title>Learning symmetries via weight-sharing with doubly stochastic tensors</title><link>https://deep-diver.github.io/neurips2024/posters/44wwow4gpf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/44wwow4gpf/</guid><description>Learn data symmetries directly from data with flexible weight-sharing using learnable doubly stochastic tensors!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/44wwow4gpf/cover.png"/></item><item><title>Learning the Infinitesimal Generator of Stochastic Diffusion Processes</title><link>https://deep-diver.github.io/neurips2024/posters/h7saaqfcui/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/h7saaqfcui/</guid><description>Learn infinitesimal generators of stochastic diffusion processes efficiently via a novel energy-based risk functional, overcoming the unbounded nature of the generator and providing learning bounds in&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/h7saaqfcui/cover.png"/></item><item><title>Learning to Predict Structural Vibrations</title><link>https://deep-diver.github.io/neurips2024/posters/i4jz6fcddy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/i4jz6fcddy/</guid><description>Deep learning predicts structural vibrations faster than traditional methods, reducing noise in airplanes, cars, and buildings, as shown by a new benchmark and frequency-query operator network.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/i4jz6fcddy/cover.png"/></item><item><title>Leveraging Drift to Improve Sample Complexity of Variance Exploding Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/posters/euq0c4is7o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/euq0c4is7o/</guid><description>Drifted VESDE: Faster convergence, efficient sampling for variance-exploding diffusion models!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/euq0c4is7o/cover.png"/></item><item><title>LLM-AutoDA: Large Language Model-Driven Automatic Data Augmentation for Long-tailed Problems</title><link>https://deep-diver.github.io/neurips2024/posters/vpuouzovhp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vpuouzovhp/</guid><description>LLM-AutoDA: Automating data augmentation for long-tailed learning using large language models, significantly boosting model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vpuouzovhp/cover.png"/></item><item><title>LM-HT SNN: Enhancing the Performance of SNN to ANN Counterpart through Learnable Multi-hierarchical Threshold Model</title><link>https://deep-diver.github.io/neurips2024/posters/ilidnmvwmx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ilidnmvwmx/</guid><description>LM-HT SNN: A learnable multi-hierarchical threshold model dramatically improves SNN performance, achieving near-ANN accuracy through dynamic current regulation and seamless ANN-SNN conversion.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ilidnmvwmx/cover.png"/></item><item><title>Local Curvature Smoothing with Stein's Identity for Efficient Score Matching</title><link>https://deep-diver.github.io/neurips2024/posters/yppni7vc7n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yppni7vc7n/</guid><description>LCSS, a novel score-matching method, enables efficient and high-quality image generation in score-based diffusion models by using Stein&amp;rsquo;s identity to bypass the computationally expensive Jacobian trac&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yppni7vc7n/cover.png"/></item><item><title>Lorentz-Equivariant Geometric Algebra Transformers for High-Energy Physics</title><link>https://deep-diver.github.io/neurips2024/posters/x34gkv8syt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x34gkv8syt/</guid><description>Lorentz Geometric Algebra Transformer (L-GATr): A novel, scalable architecture for high-energy physics, achieving high-precision, data-efficient learning and outperforming existing methods on regressi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x34gkv8syt/cover.png"/></item><item><title>Markovian Flow Matching: Accelerating MCMC with Continuous Normalizing Flows</title><link>https://deep-diver.github.io/neurips2024/posters/amjyuvqsaf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/amjyuvqsaf/</guid><description>Adaptive MCMC with CNFs accelerates probabilistic inference by combining local and flow-informed transition kernels, achieving state-of-the-art results efficiently.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/amjyuvqsaf/cover.png"/></item><item><title>Metric Flow Matching for Smooth Interpolations on the Data Manifold</title><link>https://deep-diver.github.io/neurips2024/posters/fe3rqif4nx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fe3rqif4nx/</guid><description>METRIC FLOW MATCHING (MFM) generates smooth interpolations on data manifolds by minimizing kinetic energy, outperforming Euclidean methods and achieving state-of-the-art results in single-cell traject&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fe3rqif4nx/cover.png"/></item><item><title>MiSO: Optimizing brain stimulation to create neural activity states</title><link>https://deep-diver.github.io/neurips2024/posters/gb0mxhn5h3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gb0mxhn5h3/</guid><description>MiSO: a novel closed-loop brain stimulation framework optimizes stimulation parameters to achieve desired neural population activity states, overcoming limitations of current methods by merging data a&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gb0mxhn5h3/cover.png"/></item><item><title>Mitigating Backdoor Attack by Injecting Proactive Defensive Backdoor</title><link>https://deep-diver.github.io/neurips2024/posters/cbkjbyikid/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cbkjbyikid/</guid><description>Proactive Defensive Backdoor (PDB) thwarts malicious backdoors by injecting a hidden defensive backdoor during training, suppressing attacks while maintaining model utility.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cbkjbyikid/cover.png"/></item><item><title>Mixture of Experts Meets Prompt-Based Continual Learning</title><link>https://deep-diver.github.io/neurips2024/posters/erwatqq4p8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/erwatqq4p8/</guid><description>Non-linear Residual Gates (NoRGa) boosts prompt-based continual learning by theoretically framing prefix tuning as adding new experts to a pre-trained Mixture-of-Experts model, achieving state-of-the-&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/erwatqq4p8/cover.png"/></item><item><title>Mixture of Link Predictors on Graphs</title><link>https://deep-diver.github.io/neurips2024/posters/x3oeoyjlmw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x3oeoyjlmw/</guid><description>Link-MoE boosts link prediction accuracy by strategically selecting the best model for each node pair, surpassing single-model approaches.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x3oeoyjlmw/cover.png"/></item><item><title>Model LEGO: Creating Models Like Disassembling and Assembling Building Blocks</title><link>https://deep-diver.github.io/neurips2024/posters/nxl7eazkbi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nxl7eazkbi/</guid><description>Model LEGO (MDA) revolutionizes deep learning by enabling the creation of new models by assembling and disassembling task-aware components from pre-trained models, eliminating the need for retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nxl7eazkbi/cover.png"/></item><item><title>Modeling Latent Neural Dynamics with Gaussian Process Switching Linear Dynamical Systems</title><link>https://deep-diver.github.io/neurips2024/posters/lx1lwp90kt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lx1lwp90kt/</guid><description>gpSLDS, a novel model, balances expressiveness and interpretability in modeling complex neural dynamics by combining Gaussian processes with switching linear dynamical systems, improving accuracy and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lx1lwp90kt/cover.png"/></item><item><title>Molecule Generation with Fragment Retrieval Augmentation</title><link>https://deep-diver.github.io/neurips2024/posters/56q0qggdlp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/56q0qggdlp/</guid><description>f-RAG: A novel fragment-based molecular generation framework boosts drug discovery by combining retrieval augmentation with a generative model, enabling exploration beyond existing fragments and signi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/56q0qggdlp/cover.png"/></item><item><title>Monomial Matrix Group Equivariant Neural Functional Networks</title><link>https://deep-diver.github.io/neurips2024/posters/rqyywgyuzk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rqyywgyuzk/</guid><description>Monomial-NFNs boost neural network efficiency by leveraging scaling/sign-flipping symmetries, resulting in fewer trainable parameters and competitive performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rqyywgyuzk/cover.png"/></item><item><title>MOTE-NAS: Multi-Objective Training-based Estimate for Efficient Neural Architecture Search</title><link>https://deep-diver.github.io/neurips2024/posters/jklykezfzv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jklykezfzv/</guid><description>MOTE-NAS: A new multi-objective training-based estimate drastically improves neural architecture search efficiency, achieving state-of-the-art accuracy with significantly reduced costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jklykezfzv/cover.png"/></item><item><title>MSAGPT: Neural Prompting Protein Structure Prediction via MSA Generative Pre-Training</title><link>https://deep-diver.github.io/neurips2024/posters/ppexybyhnd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ppexybyhnd/</guid><description>MSAGPT: Revolutionizing protein structure prediction by generating accurate virtual MSAs from limited data, boosting prediction accuracy by up to +8.5% TM-Score!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ppexybyhnd/cover.png"/></item><item><title>Multi-Label Open Set Recognition</title><link>https://deep-diver.github.io/neurips2024/posters/k1vrxrs6wz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/k1vrxrs6wz/</guid><description>SLAN: A novel approach for multi-label open-set recognition, enriching sub-labeling info using structural data to identify unknown labels.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/k1vrxrs6wz/cover.png"/></item><item><title>Multi-model Ensemble Conformal Prediction in Dynamic Environments</title><link>https://deep-diver.github.io/neurips2024/posters/j1y70keorq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/j1y70keorq/</guid><description>Adaptive multi-model ensemble conformal prediction achieves strongly adaptive regret, yielding more efficient prediction sets in dynamic environments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/j1y70keorq/cover.png"/></item><item><title>Multivariate Probabilistic Time Series Forecasting with Correlated Errors</title><link>https://deep-diver.github.io/neurips2024/posters/cafvxvfaii/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cafvxvfaii/</guid><description>Boost multivariate time series forecasting accuracy by efficiently learning the complex correlation structure of prediction errors, enhancing reliability without expanding model size.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cafvxvfaii/cover.png"/></item><item><title>Mutual Information Estimation via $f$-Divergence and Data Derangements</title><link>https://deep-diver.github.io/neurips2024/posters/pthi9hf9ut/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pthi9hf9ut/</guid><description>f-DIME: a novel class of discriminative mutual information estimators using f-divergence outperforms state-of-the-art methods by achieving an excellent bias-variance trade-off. This is achieved throug&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pthi9hf9ut/cover.png"/></item><item><title>Navigating Chemical Space with Latent Flows</title><link>https://deep-diver.github.io/neurips2024/posters/aaav4zbq9j/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/aaav4zbq9j/</guid><description>ChemFlow: a new framework efficiently explores chemical space using latent flows, unifying existing methods &amp;amp; incorporating physical priors for molecule manipulation and optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/aaav4zbq9j/cover.png"/></item><item><title>Neural Characteristic Activation Analysis and Geometric Parameterization for ReLU Networks</title><link>https://deep-diver.github.io/neurips2024/posters/7hfqfrjdcn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7hfqfrjdcn/</guid><description>Researchers introduce Geometric Parameterization (GmP), a novel neural network parameterization resolving instability in ReLU network training, leading to faster convergence and better generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7hfqfrjdcn/cover.png"/></item><item><title>Neural Collapse Inspired Feature Alignment for Out-of-Distribution Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/wqpng9jnpk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wqpng9jnpk/</guid><description>Neural Collapse-inspired Feature Alignment (NCFAL) significantly boosts out-of-distribution generalization by aligning semantic features to a simplex ETF, even without environment labels.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wqpng9jnpk/cover.png"/></item><item><title>Neural Collapse To Multiple Centers For Imbalanced Data</title><link>https://deep-diver.github.io/neurips2024/posters/rjec9fz9ma/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rjec9fz9ma/</guid><description>Researchers enhance imbalanced data classification by inducing Neural Collapse to Multiple Centers (NCMC) using a novel cosine loss function, achieving performance comparable to state-of-the-art metho&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rjec9fz9ma/cover.png"/></item><item><title>Neural Conditional Probability for Uncertainty Quantification</title><link>https://deep-diver.github.io/neurips2024/posters/zxfhhjnmb2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zxfhhjnmb2/</guid><description>Neural Conditional Probability (NCP) offers a new operator-theoretic approach for efficiently learning conditional distributions, enabling streamlined inference and providing theoretical guarantees fo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zxfhhjnmb2/cover.png"/></item><item><title>Neural Embeddings Rank: Aligning 3D latent dynamics with movements</title><link>https://deep-diver.github.io/neurips2024/posters/hlcek7aygp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hlcek7aygp/</guid><description>Neural Embeddings Rank (NER) aligns 3D latent neural dynamics with movements, enabling cross-session decoding and revealing consistent neural dynamics across brain areas.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hlcek7aygp/cover.png"/></item><item><title>Neural Flow Diffusion Models: Learnable Forward Process for Improved Diffusion Modelling</title><link>https://deep-diver.github.io/neurips2024/posters/z0wibvtbxc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z0wibvtbxc/</guid><description>Neural Flow Diffusion Models (NFDM) revolutionize generative modeling by introducing a learnable forward process, resulting in state-of-the-art likelihoods and versatile generative dynamics.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/z0wibvtbxc/cover.png"/></item><item><title>Neural P$^3$M: A Long-Range Interaction Modeling Enhancer for Geometric GNNs</title><link>https://deep-diver.github.io/neurips2024/posters/ncqauwsyl5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ncqauwsyl5/</guid><description>Neural PÂ³M enhances geometric GNNs by incorporating mesh points to model long-range interactions in molecules, achieving state-of-the-art accuracy in predicting energy and forces.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ncqauwsyl5/cover.png"/></item><item><title>NeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes</title><link>https://deep-diver.github.io/neurips2024/posters/npoht6wv1f/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/npoht6wv1f/</guid><description>NeuralFuse: A novel add-on module learns input transformations to maintain accuracy in low-voltage DNN inference, achieving up to 57% accuracy recovery and 24% energy savings without retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/npoht6wv1f/cover.png"/></item><item><title>Neuronal Competition Groups with Supervised STDP for Spike-Based Classification</title><link>https://deep-diver.github.io/neurips2024/posters/gee5qf6icg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gee5qf6icg/</guid><description>Neuronal Competition Groups (NCGs) enhance supervised STDP training in spiking neural networks by promoting balanced competition and improved class separation, resulting in significantly higher classi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gee5qf6icg/cover.png"/></item><item><title>Noether's Razor: Learning Conserved Quantities</title><link>https://deep-diver.github.io/neurips2024/posters/dpvqbkep1f/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dpvqbkep1f/</guid><description>Noether&amp;rsquo;s Razor learns conserved quantities and symmetries directly from data via Bayesian model selection, improving dynamical systems modeling accuracy and generalizability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dpvqbkep1f/cover.png"/></item><item><title>Non-parametric classification via expand-and-sparsify representation</title><link>https://deep-diver.github.io/neurips2024/posters/0d50il6eng/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0d50il6eng/</guid><description>New non-parametric classifiers using expand-and-sparsify representation achieve minimax-optimal convergence, adapting to low-dimensional manifold structure.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0d50il6eng/cover.png"/></item><item><title>Nonparametric Classification on Low Dimensional Manifolds using Overparameterized Convolutional Residual Networks</title><link>https://deep-diver.github.io/neurips2024/posters/guzwig7ody/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/guzwig7ody/</guid><description>Overparameterized ConvResNets surprisingly excel at prediction; this study proves they efficiently learn smooth functions on low-dimensional manifolds, avoiding the curse of dimensionality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/guzwig7ody/cover.png"/></item><item><title>Nonparametric Evaluation of Noisy ICA Solutions</title><link>https://deep-diver.github.io/neurips2024/posters/gvgrbz8mvg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gvgrbz8mvg/</guid><description>Adaptive algorithm selection for noisy ICA is achieved via a novel nonparametric independence score, improving accuracy and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gvgrbz8mvg/cover.png"/></item><item><title>Nonstationary Sparse Spectral Permanental Process</title><link>https://deep-diver.github.io/neurips2024/posters/js34qpqdws/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/js34qpqdws/</guid><description>Nonstationary Sparse Spectral Permanental Process (NSSPP) enhances point process modeling by using sparse spectral representations, enabling flexible, efficient, nonstationary kernel learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/js34qpqdws/cover.png"/></item><item><title>Nuclear Norm Regularization for Deep Learning</title><link>https://deep-diver.github.io/neurips2024/posters/eddhtvb5em/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/eddhtvb5em/</guid><description>This paper presents a novel, efficient method for Jacobian nuclear norm regularization in deep learning, replacing computationally expensive SVDs with equivalent Frobenius norm computations, thereby e&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/eddhtvb5em/cover.png"/></item><item><title>On conditional diffusion models for PDE simulations</title><link>https://deep-diver.github.io/neurips2024/posters/nql8ejymzh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nql8ejymzh/</guid><description>This paper introduces novel autoregressive sampling and hybrid training strategies for score-based diffusion models, significantly boosting PDE forecasting and assimilation accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nql8ejymzh/cover.png"/></item><item><title>On the Complexity of Learning Sparse Functions with Statistical and Gradient Queries</title><link>https://deep-diver.github.io/neurips2024/posters/q0kwoyzlso/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/q0kwoyzlso/</guid><description>Learning sparse functions efficiently with gradient methods is challenging; this paper introduces Differentiable Learning Queries (DLQ) to precisely characterize gradient query complexity, revealing s&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/q0kwoyzlso/cover.png"/></item><item><title>On the Efficiency of ERM in Feature Learning</title><link>https://deep-diver.github.io/neurips2024/posters/5kthqxbk7r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/5kthqxbk7r/</guid><description>ERM&amp;rsquo;s efficiency in feature learning surprisingly remains high even with massive feature maps; its excess risk asymptotically matches an oracle procedure&amp;rsquo;s, implying potential for streamlined feature-&amp;hellip;</description></item><item><title>On the Limitations of Fractal Dimension as a Measure of Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/yo6gvpurkn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yo6gvpurkn/</guid><description>Fractal dimension, while showing promise, fails to consistently predict neural network generalization due to hyperparameter influence and adversarial initializations; prompting further research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yo6gvpurkn/cover.png"/></item><item><title>On the Scalability of Certified Adversarial Robustness with Generated Data</title><link>https://deep-diver.github.io/neurips2024/posters/tfag9uznpv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tfag9uznpv/</guid><description>Boosting certified robustness of machine learning models by 3-4% using generated data from diffusion models!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tfag9uznpv/cover.png"/></item><item><title>On the Scalability of GNNs for Molecular Graphs</title><link>https://deep-diver.github.io/neurips2024/posters/klqhrq7fvb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/klqhrq7fvb/</guid><description>Giant leap in molecular GNNs! MolGPS, a new foundation model, achieves state-of-the-art performance on molecular property prediction by leveraging massive datasets and demonstrating the scalability o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/klqhrq7fvb/cover.png"/></item><item><title>On the Target-kernel Alignment: a Unified Analysis with Kernel Complexity</title><link>https://deep-diver.github.io/neurips2024/posters/hkcx2wa3p0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hkcx2wa3p0/</guid><description>Truncated kernel methods consistently outperform standard methods by eliminating the saturation effect, offering faster learning rates and enhanced theoretical guarantees.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hkcx2wa3p0/cover.png"/></item><item><title>Online Relational Inference for Evolving Multi-agent Interacting Systems</title><link>https://deep-diver.github.io/neurips2024/posters/mio8odrzto/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mio8odrzto/</guid><description>ORI: a novel online relational inference framework efficiently identifies hidden interaction graphs in evolving multi-agent systems using streaming data and real-time adaptation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mio8odrzto/cover.png"/></item><item><title>Open-Book Neural Algorithmic Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/6ho33urpai/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/6ho33urpai/</guid><description>This paper introduces open-book neural algorithmic reasoning, a novel framework that significantly enhances neural reasoning capabilities by allowing networks to access and utilize all training instan&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/6ho33urpai/cover.png"/></item><item><title>Optimal Rates for Vector-Valued Spectral Regularization Learning Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/u9e1d2xoc8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/u9e1d2xoc8/</guid><description>Vector-valued spectral learning algorithms finally get rigorous theoretical backing, showing optimal learning rates and resolving the saturation effect puzzle.</description></item><item><title>Ordered Momentum for Asynchronous SGD</title><link>https://deep-diver.github.io/neurips2024/posters/u2mx0hsrwa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/u2mx0hsrwa/</guid><description>Ordered Momentum (OrMo) significantly boosts asynchronous stochastic gradient descent (ASGD) convergence by cleverly incorporating momentum, resolving prior convergence issues. This novel approach is&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/u2mx0hsrwa/cover.png"/></item><item><title>Out-Of-Distribution Detection with Diversification (Provably)</title><link>https://deep-diver.github.io/neurips2024/posters/c1hirbzeh9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/c1hirbzeh9/</guid><description>Boost OOD detection accuracy with diverseMix: a novel method enhancing auxiliary outlier diversity, provably improving generalization and achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/c1hirbzeh9/cover.png"/></item><item><title>P$^2$C$^2$Net: PDE-Preserved Coarse Correction Network for efficient prediction of spatiotemporal dynamics</title><link>https://deep-diver.github.io/neurips2024/posters/motimxq3b1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/motimxq3b1/</guid><description>P2C2Net: A physics-encoded neural network efficiently predicts complex spatiotemporal dynamics using coarse grids and limited training data, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/motimxq3b1/cover.png"/></item><item><title>PACE: Pacing Operator Learning to Accurate Optical Field Simulation for Complicated Photonic Devices</title><link>https://deep-diver.github.io/neurips2024/posters/uxjlgkwdci/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uxjlgkwdci/</guid><description>PACE, a novel neural operator, achieves unprecedented accuracy and speed in optical field simulation for complex photonic devices, surpassing existing methods by significantly reducing errors and boos&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uxjlgkwdci/cover.png"/></item><item><title>PageRank Bandits for Link Prediction</title><link>https://deep-diver.github.io/neurips2024/posters/vsz9na5jtl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vsz9na5jtl/</guid><description>PageRank Bandits (PRB) revolutionizes link prediction by framing it as a sequential decision-making problem, thus enabling the system to adapt to evolving data. Combining contextual bandits with PageR&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vsz9na5jtl/cover.png"/></item><item><title>Parametric model reduction of mean-field and stochastic systems via higher-order action matching</title><link>https://deep-diver.github.io/neurips2024/posters/qyaz3xp0fn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qyaz3xp0fn/</guid><description>HOAM learns reduced models of population dynamics for complex systems, enabling fast predictions across various physics parameters, outperforming state-of-the-art techniques.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qyaz3xp0fn/cover.png"/></item><item><title>pcaGAN: Improving Posterior-Sampling cGANs via Principal Component Regularization</title><link>https://deep-diver.github.io/neurips2024/posters/z0nq3hheeg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z0nq3hheeg/</guid><description>pcaGAN boosts posterior-sampling cGANs by using principal component regularization, achieving faster, more accurate results in various imaging tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/z0nq3hheeg/cover.png"/></item><item><title>Persistent Test-time Adaptation in Recurring Testing Scenarios</title><link>https://deep-diver.github.io/neurips2024/posters/ffeubotcds/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ffeubotcds/</guid><description>Persistent Test-Time Adaptation (PeTTA) prevents AI model collapse in recurring scenarios by dynamically adjusting the adaptation strategy based on divergence from the initial model, ensuring long-ter&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ffeubotcds/cover.png"/></item><item><title>PGN: The RNN's New Successor is Effective for Long-Range Time Series Forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/ypeamfku2o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ypeamfku2o/</guid><description>TPGN, a novel framework for long-range time series forecasting, uses Parallel Gated Networks (PGN) to efficiently capture long-term dependencies, achieving state-of-the-art results on multiple dataset&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ypeamfku2o/cover.png"/></item><item><title>Physical Consistency Bridges Heterogeneous Data in Molecular Multi-Task Learning</title><link>https://deep-diver.github.io/neurips2024/posters/gnf9tavqgc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gnf9tavqgc/</guid><description>Physically consistent multi-task learning bridges heterogeneous molecular data by directly leveraging physical laws to improve predictions, enhancing accuracy beyond the limitations of individual data&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gnf9tavqgc/cover.png"/></item><item><title>Physics-Informed Regularization for Domain-Agnostic Dynamical System Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/iwlqbne8p7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/iwlqbne8p7/</guid><description>TREAT: a novel framework boosting dynamical systems modeling accuracy by enforcing Time-Reversal Symmetry (TRS) via a regularization term. High-precision modeling is achieved across diverse systems, &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/iwlqbne8p7/cover.png"/></item><item><title>Physics-Informed Variational State-Space Gaussian Processes</title><link>https://deep-diver.github.io/neurips2024/posters/tcf7s75xfa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tcf7s75xfa/</guid><description>PHYSS-GP: a novel physics-informed state-space Gaussian process model for efficient spatio-temporal data modeling, outperforming existing methods in predictive accuracy and computational speed.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tcf7s75xfa/cover.png"/></item><item><title>Post-Hoc Reversal: Are We Selecting Models Prematurely?</title><link>https://deep-diver.github.io/neurips2024/posters/3r7go6wkdm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3r7go6wkdm/</guid><description>Post-hoc model transformations can reverse performance trends, prompting a reevaluation of model selection strategies and suggesting a new &amp;lsquo;post-hoc selection&amp;rsquo; method for improved model development.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3r7go6wkdm/cover.png"/></item><item><title>Practical Shuffle Coding</title><link>https://deep-diver.github.io/neurips2024/posters/m2daxpcoii/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/m2daxpcoii/</guid><description>Revolutionizing unordered data compression, this paper introduces autoregressive shuffle coding, achieving state-of-the-art speeds and compression rates on massive datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/m2daxpcoii/cover.png"/></item><item><title>Pre-training Differentially Private Models with Limited Public Data</title><link>https://deep-diver.github.io/neurips2024/posters/gqrk0wgnic/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gqrk0wgnic/</guid><description>Researchers achieved high-accuracy differentially private (DP) models by using a novel DP continual pre-training strategy with only 10% public data, mitigating the performance degradation common in DP&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gqrk0wgnic/cover.png"/></item><item><title>Predicting Ground State Properties: Constant Sample Complexity and Deep Learning Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/yblxvqjyqa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yblxvqjyqa/</guid><description>Deep learning algorithms now predict quantum ground state properties with constant sample complexity, regardless of system size, improving upon previous methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yblxvqjyqa/cover.png"/></item><item><title>Predictive Attractor Models</title><link>https://deep-diver.github.io/neurips2024/posters/lxhovdf1sw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lxhovdf1sw/</guid><description>Predictive Attractor Models (PAM) offer a biologically-plausible, streaming sequence memory architecture that avoids catastrophic forgetting and generates multiple future possibilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lxhovdf1sw/cover.png"/></item><item><title>Preferential Normalizing Flows</title><link>https://deep-diver.github.io/neurips2024/posters/srsjr9sdkr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/srsjr9sdkr/</guid><description>Eliciting high-dimensional probability distributions from experts using only preference comparisons is achieved via normalizing flows and a novel functional prior, resolving the problem of collapsing &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/srsjr9sdkr/cover.png"/></item><item><title>Pretraining with Random Noise for Fast and Robust Learning without Weight Transport</title><link>https://deep-diver.github.io/neurips2024/posters/dngfcvbonu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dngfcvbonu/</guid><description>Random noise pretraining dramatically speeds up and enhances neural network learning without weight transport, mimicking the brain&amp;rsquo;s developmental process and achieving performance comparable to backp&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dngfcvbonu/cover.png"/></item><item><title>Probabilistic Decomposed Linear Dynamical Systems for Robust Discovery of Latent Neural Dynamics</title><link>https://deep-diver.github.io/neurips2024/posters/xphsbybd73/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xphsbybd73/</guid><description>Probabilistic Decomposed Linear Dynamical Systems (p-dLDS) improve latent variable inference in nonlinear neural systems by using a probabilistic approach that&amp;rsquo;s robust to noise and includes a time-va&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xphsbybd73/cover.png"/></item><item><title>Probabilistic Graph Rewiring via Virtual Nodes</title><link>https://deep-diver.github.io/neurips2024/posters/lpvshl9lck/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lpvshl9lck/</guid><description>IPR-MPNNs revolutionize graph neural networks by implicitly rewiring graphs using virtual nodes, achieving state-of-the-art performance with significantly faster computation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lpvshl9lck/cover.png"/></item><item><title>Probabilistic size-and-shape functional mixed models</title><link>https://deep-diver.github.io/neurips2024/posters/dbynjebat0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dbynjebat0/</guid><description>This study introduces a novel Bayesian functional mixed model that reliably recovers the size and shape of fixed effects from noisy functional data with phase variability, outperforming current state-&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dbynjebat0/cover.png"/></item><item><title>ProSST: Protein Language Modeling with Quantized Structure and Disentangled Attention</title><link>https://deep-diver.github.io/neurips2024/posters/4z7rzixpjq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4z7rzixpjq/</guid><description>ProSST, a novel protein language model, integrates protein sequences and structures using quantized structure representation and disentangled attention, achieving state-of-the-art performance in zero-&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4z7rzixpjq/cover.png"/></item><item><title>Provable and Efficient Dataset Distillation for Kernel Ridge Regression</title><link>https://deep-diver.github.io/neurips2024/posters/wi2vpcbdnd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wi2vpcbdnd/</guid><description>One data point per class suffices for efficient and provable dataset distillation in kernel ridge regression, significantly reducing computational costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wi2vpcbdnd/cover.png"/></item><item><title>Provable Posterior Sampling with Denoising Oracles via Tilted Transport</title><link>https://deep-diver.github.io/neurips2024/posters/phlle8uoev/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/phlle8uoev/</guid><description>Boosting posterior sampling in challenging high-dimensional inverse problems, this paper introduces &amp;rsquo;tilted transport&amp;rsquo;, a novel technique leveraging denoising oracles for provably easier sampling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/phlle8uoev/cover.png"/></item><item><title>Provably Robust Score-Based Diffusion Posterior Sampling for Plug-and-Play Image Reconstruction</title><link>https://deep-diver.github.io/neurips2024/posters/slnsoay4u1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/slnsoay4u1/</guid><description>Provably robust diffusion posterior sampling for plug-and-play image reconstruction is achieved via a novel algorithmic framework, DPnP, offering both asymptotic and non-asymptotic performance guarant&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/slnsoay4u1/cover.png"/></item><item><title>Pruning neural network models for gene regulatory dynamics using data and domain knowledge</title><link>https://deep-diver.github.io/neurips2024/posters/fntszlwkgr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fntszlwkgr/</guid><description>DASH: a novel pruning framework leverages domain knowledge to improve the interpretability and sparsity of neural network models for gene regulatory dynamics, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fntszlwkgr/cover.png"/></item><item><title>PURE: Prompt Evolution with Graph ODE for Out-of-distribution Fluid Dynamics Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/z86knmjouq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z86knmjouq/</guid><description>PURE: A novel method uses Graph ODE to adapt spatio-temporal forecasting models to various fluid dynamics scenarios, improving model adaptation to unseen parameters and long-term predictions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/z86knmjouq/cover.png"/></item><item><title>PureGen: Universal Data Purification for Train-Time Poison Defense via Generative Model Dynamics</title><link>https://deep-diver.github.io/neurips2024/posters/zeihwoddvh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zeihwoddvh/</guid><description>PUREGEN uses generative model dynamics to purify poisoned training data, providing a universal, effective, and efficient train-time defense against various data poisoning attacks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zeihwoddvh/cover.png"/></item><item><title>Quantum Deep Equilibrium Models</title><link>https://deep-diver.github.io/neurips2024/posters/cwhwkb0q4k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cwhwkb0q4k/</guid><description>Quantum Deep Equilibrium Models (QDEQs) achieve higher QML performance with shallower circuits by using a DEQ training paradigm, improving near-term quantum computation efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cwhwkb0q4k/cover.png"/></item><item><title>Quasi-Bayes meets Vines</title><link>https://deep-diver.github.io/neurips2024/posters/gcpeeg88r3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gcpeeg88r3/</guid><description>Quasi-Bayesian Vine (QB-Vine) efficiently models high-dimensional densities by recursively updating 1D marginal predictives and a vine copula, significantly outperforming state-of-the-art methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gcpeeg88r3/cover.png"/></item><item><title>QVAE-Mole: The Quantum VAE with Spherical Latent Variable Learning for 3-D Molecule Generation</title><link>https://deep-diver.github.io/neurips2024/posters/rqvesbxqdo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rqvesbxqdo/</guid><description>Quantum VAE with spherical latent variable learning enables efficient, one-shot 3D molecule generation, outperforming classic and other quantum methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rqvesbxqdo/cover.png"/></item><item><title>RandNet-Parareal: a time-parallel PDE solver using Random Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/974ojun0ju/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/974ojun0ju/</guid><description>RandNet-Parareal: A novel time-parallel PDE solver using Random Neural Networks achieves speed gains up to x125, dramatically improving scalability for large-scale simulations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/974ojun0ju/cover.png"/></item><item><title>REDUCR: Robust Data Downsampling using Class Priority Reweighting</title><link>https://deep-diver.github.io/neurips2024/posters/jz7z7kkr94/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jz7z7kkr94/</guid><description>REDUCR, a novel data downsampling method, significantly improves worst-class test accuracy in imbalanced datasets by using class priority reweighting, surpassing state-of-the-art methods by ~15%.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jz7z7kkr94/cover.png"/></item><item><title>Regularized Adaptive Momentum Dual Averaging with an Efficient Inexact Subproblem Solver for Training Structured Neural Network</title><link>https://deep-diver.github.io/neurips2024/posters/xl7ve14aha/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xl7ve14aha/</guid><description>RAMDA: a new algorithm ensures efficient training of structured neural networks by achieving optimal structure and outstanding predictive performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xl7ve14aha/cover.png"/></item><item><title>Rejection via Learning Density Ratios</title><link>https://deep-diver.github.io/neurips2024/posters/jzciknnopj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jzciknnopj/</guid><description>This paper introduces a novel framework for classification with rejection by learning density ratios between data and idealized distributions, improving model robustness and accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jzciknnopj/cover.png"/></item><item><title>Relational Concept Bottleneck Models</title><link>https://deep-diver.github.io/neurips2024/posters/g99bsv9pt5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/g99bsv9pt5/</guid><description>Relational Concept Bottleneck Models (R-CBMs) merge interpretable CBMs with powerful GNNs for high-performing, explainable relational deep learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/g99bsv9pt5/cover.png"/></item><item><title>Reparameterized Multi-Resolution Convolutions for Long Sequence Modelling</title><link>https://deep-diver.github.io/neurips2024/posters/rwgnbipcpk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rwgnbipcpk/</guid><description>MRConv: Reparameterized multi-resolution convolutions efficiently model long sequences, improving performance across various data modalities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rwgnbipcpk/cover.png"/></item><item><title>Rethinking Deep Thinking: Stable Learning of Algorithms using Lipschitz Constraints</title><link>https://deep-diver.github.io/neurips2024/posters/zlgfrk2cqa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zlgfrk2cqa/</guid><description>Stable algorithm learning achieved by Deep Thinking networks with Lipschitz Constraints, ensuring convergence and better extrapolation to complex problems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zlgfrk2cqa/cover.png"/></item><item><title>Rethinking Fourier Transform from A Basis Functions Perspective for Long-term Time Series Forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/bafkbkr8ip/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bafkbkr8ip/</guid><description>Revolutionizing long-term time series forecasting, a new Fourier Basis Mapping method enhances accuracy by precisely interpreting frequency coefficients and considering time-frequency relationships, a&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bafkbkr8ip/cover.png"/></item><item><title>Rethinking the Membrane Dynamics and Optimization Objectives of Spiking Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/6aepmnrz7a/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/6aepmnrz7a/</guid><description>Boosting spiking neural network accuracy by 4.05% on ImageNet and achieving state-of-the-art results on CIFAR10-DVS and N-Caltech101 through learnable initial membrane potential and refined training s&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/6aepmnrz7a/cover.png"/></item><item><title>Retrieval &amp; Fine-Tuning for In-Context Tabular Models</title><link>https://deep-diver.github.io/neurips2024/posters/337dhoexcm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/337dhoexcm/</guid><description>LoCalPFN: boosting in-context tabular learning via retrieval &amp;amp; fine-tuning!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/337dhoexcm/cover.png"/></item><item><title>Retrieval-Augmented Diffusion Models for Time Series Forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/drjjt0ji48/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/drjjt0ji48/</guid><description>Boosting time series forecasting accuracy, Retrieval-Augmented Diffusion Models (RATD) leverage relevant historical data to guide the diffusion process, overcoming limitations of existing models and d&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/drjjt0ji48/cover.png"/></item><item><title>Revive Re-weighting in Imbalanced Learning by Density Ratio Estimation</title><link>https://deep-diver.github.io/neurips2024/posters/vx4ngdyyvg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vx4ngdyyvg/</guid><description>Revive Re-weighting in Imbalanced Learning by Density Ratio Estimation dynamically adjusts class weights during training using density ratio estimation, significantly improving model generalization, e&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vx4ngdyyvg/cover.png"/></item><item><title>RMLR: Extending Multinomial Logistic Regression into General Geometries</title><link>https://deep-diver.github.io/neurips2024/posters/lbp2cda7sp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lbp2cda7sp/</guid><description>RMLR: A novel framework extends multinomial logistic regression to diverse geometries, overcoming limitations of existing methods by requiring minimal geometric properties for broad applicability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lbp2cda7sp/cover.png"/></item><item><title>Robust group and simultaneous inferences for high-dimensional single index model</title><link>https://deep-diver.github.io/neurips2024/posters/melygfpy4x/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/melygfpy4x/</guid><description>This paper introduces robust group inference procedures for high-dimensional single index models, offering substantial efficiency gains for heavy-tailed errors and handling group testing effectively w&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/melygfpy4x/cover.png"/></item><item><title>Rough Transformers: Lightweight Continuous-Time Sequence Modelling with Path Signatures</title><link>https://deep-diver.github.io/neurips2024/posters/gxwmhzevmh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gxwmhzevmh/</guid><description>Rough Transformers: A lightweight continuous-time sequence modeling approach using path signatures to significantly reduce computational costs, improving efficiency and accuracy, particularly for long&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gxwmhzevmh/cover.png"/></item><item><title>S2HPruner: Soft-to-Hard Distillation Bridges the Discretization Gap in Pruning</title><link>https://deep-diver.github.io/neurips2024/posters/mtyy3myyhz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mtyy3myyhz/</guid><description>S2HPruner bridges the discretization gap in neural network pruning via a novel soft-to-hard distillation framework, achieving superior performance across various benchmarks without fine-tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mtyy3myyhz/cover.png"/></item><item><title>SampDetox: Black-box Backdoor Defense via Perturbation-based Sample Detoxification</title><link>https://deep-diver.github.io/neurips2024/posters/y6rv6z98pk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y6rv6z98pk/</guid><description>SampDetox uses diffusion models to purify poisoned machine learning samples by strategically adding noise to eliminate backdoors without compromising data integrity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y6rv6z98pk/cover.png"/></item><item><title>Sample Selection via Contrastive Fragmentation for Noisy Label Regression</title><link>https://deep-diver.github.io/neurips2024/posters/gyd5afzaor/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gyd5afzaor/</guid><description>ConFrag, a novel approach to noisy label regression, leverages contrastive fragmentation and neighborhood agreement to select clean samples, significantly outperforming state-of-the-art baselines on s&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gyd5afzaor/cover.png"/></item><item><title>SAND: Smooth imputation of sparse and noisy functional data with Transformer networks</title><link>https://deep-diver.github.io/neurips2024/posters/mxro5kukst/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mxro5kukst/</guid><description>SAND, a novel transformer network variant, smoothly imputes sparse and noisy functional data by leveraging self-attention on derivatives, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mxro5kukst/cover.png"/></item><item><title>SARAD: Spatial Association-Aware Anomaly Detection and Diagnosis for Multivariate Time Series</title><link>https://deep-diver.github.io/neurips2024/posters/gmf5aj01hz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gmf5aj01hz/</guid><description>SARAD: A novel anomaly detection approach for multivariate time series leverages spatial information and association reduction patterns to achieve state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gmf5aj01hz/cover.png"/></item><item><title>Scalable Optimization in the Modular Norm</title><link>https://deep-diver.github.io/neurips2024/posters/sfxajb7uxx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sfxajb7uxx/</guid><description>Deep learning optimization gets a major upgrade with Modula, a new method that uses the modular norm to normalize weight updates, enabling learning rate transfer across network widths and depths, thus&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sfxajb7uxx/cover.png"/></item><item><title>Scale-invariant Optimal Sampling for Rare-events Data and Sparse Models</title><link>https://deep-diver.github.io/neurips2024/posters/6sanp0vr9x/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/6sanp0vr9x/</guid><description>Scale-invariant optimal subsampling tackles computational challenges in analyzing massive rare-events data with sparse models, enhancing parameter estimation and variable selection without being affec&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/6sanp0vr9x/cover.png"/></item><item><title>Scaling Law for Time Series Forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/cr2jehjb9q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cr2jehjb9q/</guid><description>Unlocking the potential of deep learning for time series forecasting: this study reveals a scaling law influenced by dataset size, model complexity, and the crucial look-back horizon, leading to impro&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cr2jehjb9q/cover.png"/></item><item><title>Scaling laws for learning with real and surrogate data</title><link>https://deep-diver.github.io/neurips2024/posters/nachv7vtl2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nachv7vtl2/</guid><description>Boost machine learning with surrogate data! A novel weighted ERM method effectively integrates surrogate data, significantly reducing test errors even with unrelated data, guided by a predictable sca&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nachv7vtl2/cover.png"/></item><item><title>Scaling transformer neural networks for skillful and reliable medium-range weather forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/abp01akha9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/abp01akha9/</guid><description>Stormer, a simple transformer model, achieves state-of-the-art medium-range weather forecasting accuracy by using weather-specific embedding, randomized dynamics forecasting, and a pressure-weighted l&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/abp01akha9/cover.png"/></item><item><title>Scanning Trojaned Models Using Out-of-Distribution Samples</title><link>https://deep-diver.github.io/neurips2024/posters/m296wjxyzq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/m296wjxyzq/</guid><description>TRODO: a novel trojan detection method using out-of-distribution samples, effectively identifies trojaned classifiers even against adversarial attacks and with limited data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/m296wjxyzq/cover.png"/></item><item><title>Score-based 3D molecule generation with neural fields</title><link>https://deep-diver.github.io/neurips2024/posters/9lgjrkqjuw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/9lgjrkqjuw/</guid><description>FuncMol: A new neural field model generates 3D molecules efficiently, outperforming existing methods by achieving an order of magnitude faster sampling speed.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/9lgjrkqjuw/cover.png"/></item><item><title>Score-Optimal Diffusion Schedules</title><link>https://deep-diver.github.io/neurips2024/posters/0rl5vwozru/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0rl5vwozru/</guid><description>Researchers developed a novel algorithm to automatically find optimal schedules for denoising diffusion models (DDMs), significantly improving sample quality and efficiency without manual parameter tu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0rl5vwozru/cover.png"/></item><item><title>Searching for Efficient Linear Layers over a Continuous Space of Structured Matrices</title><link>https://deep-diver.github.io/neurips2024/posters/fc88anwvdf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fc88anwvdf/</guid><description>Revolutionizing large neural networks, this paper introduces a continuous parameterization of structured matrices, discovering that full-rank structures without parameter sharing achieve optimal scali&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fc88anwvdf/cover.png"/></item><item><title>Self-Refining Diffusion Samplers: Enabling Parallelization via Parareal Iterations</title><link>https://deep-diver.github.io/neurips2024/posters/xhwkhfwi3k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xhwkhfwi3k/</guid><description>Self-Refining Diffusion Samplers (SRDS) dramatically speeds up diffusion model sampling by leveraging Parareal iterations for parallel-in-time computation, maintaining high-quality outputs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xhwkhfwi3k/cover.png"/></item><item><title>Sequential Harmful Shift Detection Without Labels</title><link>https://deep-diver.github.io/neurips2024/posters/jps9kkusd3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jps9kkusd3/</guid><description>This paper introduces a novel, label-free method for detecting harmful distribution shifts in machine learning models deployed in production environments, leveraging a proxy error derived from an erro&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jps9kkusd3/cover.png"/></item><item><title>SequentialAttention++ for Block Sparsification: Differentiable Pruning Meets Combinatorial Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/brpzmoqisn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/brpzmoqisn/</guid><description>SequentialAttention++ unites differentiable pruning with combinatorial optimization for efficient and accurate neural network block sparsification, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/brpzmoqisn/cover.png"/></item><item><title>Set-based Neural Network Encoding Without Weight Tying</title><link>https://deep-diver.github.io/neurips2024/posters/i3me9bcscy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/i3me9bcscy/</guid><description>Set-based Neural Network Encoder (SNE) efficiently encodes neural network weights for property prediction, eliminating the need for architecture-specific models and improving generalization across dat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/i3me9bcscy/cover.png"/></item><item><title>Sharpness-diversity tradeoff: improving flat ensembles with SharpBalance</title><link>https://deep-diver.github.io/neurips2024/posters/wjacsnt9ue/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wjacsnt9ue/</guid><description>SharpBalance, a novel training approach, effectively improves deep ensemble performance by addressing the sharpness-diversity trade-off, leading to significant improvements in both in-distribution and&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wjacsnt9ue/cover.png"/></item><item><title>Sigmoid Gating is More Sample Efficient than Softmax Gating in Mixture of Experts</title><link>https://deep-diver.github.io/neurips2024/posters/ig6kd5v4kd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ig6kd5v4kd/</guid><description>Sigmoid gating significantly boosts sample efficiency in Mixture of Experts models compared to softmax gating, offering faster convergence rates for various expert functions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ig6kd5v4kd/cover.png"/></item><item><title>Simulation-Free Training of Neural ODEs on Paired Data</title><link>https://deep-diver.github.io/neurips2024/posters/gogkhunkfw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gogkhunkfw/</guid><description>Train Neural ODEs without simulations, achieving high performance on regression and classification by using flow matching in the embedding space of data pairs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gogkhunkfw/cover.png"/></item><item><title>Sketched Lanczos uncertainty score: a low-memory summary of the Fisher information</title><link>https://deep-diver.github.io/neurips2024/posters/1vpqomqsfo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1vpqomqsfo/</guid><description>SLU: a novel, low-memory uncertainty score for neural networks, achieves logarithmic memory scaling with model parameters, providing well-calibrated uncertainties and outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1vpqomqsfo/cover.png"/></item><item><title>Soft ascent-descent as a stable and flexible alternative to flooding</title><link>https://deep-diver.github.io/neurips2024/posters/y1zslondi2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y1zslondi2/</guid><description>Soft ascent-descent (SoftAD) improves test accuracy and generalization by softening the flooding method, offering competitive accuracy with reduced loss and model complexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y1zslondi2/cover.png"/></item><item><title>Sourcerer: Sample-based Maximum Entropy Source Distribution Estimation</title><link>https://deep-diver.github.io/neurips2024/posters/0cgdda4ofr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0cgdda4ofr/</guid><description>Sourcerer: A novel sample-based method for maximum entropy source distribution estimation, resolving ill-posedness while maintaining simulation accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0cgdda4ofr/cover.png"/></item><item><title>Sparse Bayesian Generative Modeling for Compressive Sensing</title><link>https://deep-diver.github.io/neurips2024/posters/gqefkjw1or/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gqefkjw1or/</guid><description>A new learnable prior for compressive sensing solves the inverse problem using only a few corrupted data samples, enabling sparse signal recovery without ground-truth information and uncertainty quant&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gqefkjw1or/cover.png"/></item><item><title>Sparse maximal update parameterization: A holistic approach to sparse training dynamics</title><link>https://deep-diver.github.io/neurips2024/posters/owmu3qoa0o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/owmu3qoa0o/</guid><description>SÂµPar stabilizes sparse neural network training, slashing tuning costs and boosting performance, especially at high sparsity levels, via a novel parameterization technique.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/owmu3qoa0o/cover.png"/></item><item><title>Spatio-Spectral Graph Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/cb3kcwybgw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cb3kcwybgw/</guid><description>Spatio-Spectral GNNs synergistically combine spatial and spectral graph filters for efficient, global information propagation, overcoming limitations of existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cb3kcwybgw/cover.png"/></item><item><title>Spectral Learning of Shared Dynamics Between Generalized-Linear Processes</title><link>https://deep-diver.github.io/neurips2024/posters/dupvyqqlag/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dupvyqqlag/</guid><description>PGLDM, a novel algorithm, accurately identifies shared and private dynamics in two generalized-linear time series, improving model accuracy and enabling lower-dimensional latent state representations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dupvyqqlag/cover.png"/></item><item><title>Spiking Graph Neural Network on Riemannian Manifolds</title><link>https://deep-diver.github.io/neurips2024/posters/vkt0k3iomo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vkt0k3iomo/</guid><description>Spiking Graph Neural Networks (SGNNs) on Riemannian Manifolds achieve superior performance and energy efficiency via a novel Manifold Spiking GNN (MSG).</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vkt0k3iomo/cover.png"/></item><item><title>Spiking Token Mixer: A event-driven friendly Former structure for spiking neural networks</title><link>https://deep-diver.github.io/neurips2024/posters/iycy7kaksy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/iycy7kaksy/</guid><description>STMixer: a novel SNN architecture enabling high performance on both synchronous and asynchronous neuromorphic hardware, achieving comparable results to spiking transformers with drastically lower powe&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/iycy7kaksy/cover.png"/></item><item><title>ST$_k$: A Scalable Module for Solving Top-k Problems</title><link>https://deep-diver.github.io/neurips2024/posters/odjkb9jsa5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/odjkb9jsa5/</guid><description>STk: a novel, differentiable module solves Top-k problems in neural networks without extra time/GPU memory, boosting performance in long-tailed learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/odjkb9jsa5/cover.png"/></item><item><title>State Space Models on Temporal Graphs: A First-Principles Study</title><link>https://deep-diver.github.io/neurips2024/posters/uajeraossn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uajeraossn/</guid><description>GRAPHSSM: a novel graph state space model efficiently captures temporal graph dynamics, overcoming limitations of existing sequence models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uajeraossn/cover.png"/></item><item><title>Stepping on the Edge: Curvature Aware Learning Rate Tuners</title><link>https://deep-diver.github.io/neurips2024/posters/sefllhihhj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sefllhihhj/</guid><description>Adaptive learning rate tuners often underperform; Curvature Dynamics Aware Tuning (CDAT) prioritizes long-term curvature stabilization, outperforming tuned constant learning rates.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sefllhihhj/cover.png"/></item><item><title>Stochastic Kernel Regularisation Improves Generalisation in Deep Kernel Machines</title><link>https://deep-diver.github.io/neurips2024/posters/prgxz9fybf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/prgxz9fybf/</guid><description>Deep kernel machines now achieve 94.5% accuracy on CIFAR-10, matching neural networks, by using stochastic kernel regularization to improve generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/prgxz9fybf/cover.png"/></item><item><title>Stochastic Optimal Control for Diffusion Bridges in Function Spaces</title><link>https://deep-diver.github.io/neurips2024/posters/wyqw4g57zd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wyqw4g57zd/</guid><description>Researchers extended stochastic optimal control theory to infinite-dimensional spaces, enabling the creation of diffusion bridges for generative modeling in function spaces, demonstrating applications&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wyqw4g57zd/cover.png"/></item><item><title>Structural Inference of Dynamical Systems with Conjoined State Space Models</title><link>https://deep-diver.github.io/neurips2024/posters/xqwjbek5rh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xqwjbek5rh/</guid><description>SICSM, a novel framework, integrates selective SSMs and GFNs to accurately infer complex dynamical system structures from irregularly sampled, partially observed trajectories.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xqwjbek5rh/cover.png"/></item><item><title>Structured Matrix Basis for Multivariate Time Series Forecasting with Interpretable Dynamics</title><link>https://deep-diver.github.io/neurips2024/posters/co7dsowcop/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/co7dsowcop/</guid><description>Sumba: a novel forecasting model achieves up to 8.5% improvement by using a structured matrix basis to generate dynamic spatial structures with lower variance and better interpretability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/co7dsowcop/cover.png"/></item><item><title>Super Consistency of Neural Network Landscapes and Learning Rate Transfer</title><link>https://deep-diver.github.io/neurips2024/posters/rgwhj7intz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rgwhj7intz/</guid><description>Neural network hyperparameter transferability across vastly different model sizes is achieved via a newly discovered property called &amp;lsquo;Super Consistency&amp;rsquo; of loss landscapes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rgwhj7intz/cover.png"/></item><item><title>Supra-Laplacian Encoding for Transformer on Dynamic Graphs</title><link>https://deep-diver.github.io/neurips2024/posters/vp9qazr2gw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vp9qazr2gw/</guid><description>SLATE: Supra-Laplacian encoding for spatio-temporal Transformers achieves state-of-the-art dynamic link prediction by innovatively using a multi-layer graph representation and a unique cross-attention&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vp9qazr2gw/cover.png"/></item><item><title>Surge Phenomenon in Optimal Learning Rate and Batch Size Scaling</title><link>https://deep-diver.github.io/neurips2024/posters/hd9tuv4xdz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hd9tuv4xdz/</guid><description>Deep learning&amp;rsquo;s Adam-style optimizers exhibit a surprising surge phenomenon: optimal learning rates initially increase, then decrease, before converging to a non-zero value as batch size grows.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hd9tuv4xdz/cover.png"/></item><item><title>Swift Sampler: Efficient Learning of Sampler by 10 Parameters</title><link>https://deep-diver.github.io/neurips2024/posters/mlhz8znoek/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mlhz8znoek/</guid><description>Swift Sampler (SS) automates the learning of efficient data samplers for deep learning, achieving significant performance gains (e.g., 1.5% on ImageNet) with minimal computational cost using only 10 p&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mlhz8znoek/cover.png"/></item><item><title>Symmetry-Informed Governing Equation Discovery</title><link>https://deep-diver.github.io/neurips2024/posters/aegsa8uoxf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/aegsa8uoxf/</guid><description>Leveraging symmetry in automated equation discovery improves accuracy and simplicity of learned governing equations, enhancing robustness against noise and achieving higher success rates across divers&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/aegsa8uoxf/cover.png"/></item><item><title>Take A Shortcut Back: Mitigating the Gradient Vanishing for Training Spiking Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/xjyu6zmzd7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xjyu6zmzd7/</guid><description>Shortcut back-propagation and an evolutionary training framework conquer gradient vanishing in spiking neural networks, drastically improving training and achieving state-of-the-art accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xjyu6zmzd7/cover.png"/></item><item><title>Task Confusion and Catastrophic Forgetting in Class-Incremental Learning: A Mathematical Framework for Discriminative and Generative Modelings</title><link>https://deep-diver.github.io/neurips2024/posters/tj5wjslj0r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tj5wjslj0r/</guid><description>Researchers unveil the Infeasibility Theorem, proving optimal class-incremental learning is impossible with discriminative models due to task confusion, and the Feasibility Theorem, showing generative&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tj5wjslj0r/cover.png"/></item><item><title>The Best of Both Worlds: On the Dilemma of Out-of-distribution Detection</title><link>https://deep-diver.github.io/neurips2024/posters/b9fppdnmyk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/b9fppdnmyk/</guid><description>Researchers found that superior OOD detection performance comes at the cost of reduced generalization. Their novel Decoupled Uncertainty Learning (DUL) algorithm harmonizes OOD detection and generali&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/b9fppdnmyk/cover.png"/></item><item><title>The Feature Speed Formula: a flexible approach to scale hyper-parameters of deep neural networks</title><link>https://deep-diver.github.io/neurips2024/posters/wshmb4j2o9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wshmb4j2o9/</guid><description>New &amp;lsquo;Feature Speed Formula&amp;rsquo; predicts &amp;amp; controls deep learning&amp;rsquo;s hierarchical feature learning by linking hyperparameter tuning to the angle between feature updates and backward pass.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wshmb4j2o9/cover.png"/></item><item><title>The Implicit Bias of Gradient Descent on Separable Multiclass Data</title><link>https://deep-diver.github.io/neurips2024/posters/jlwn80mtji/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jlwn80mtji/</guid><description>Researchers extended implicit bias theory to multiclass classification using a novel framework, proving that gradient descent prefers simple solutions even with complex alternatives.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jlwn80mtji/cover.png"/></item><item><title>The Importance of Being Scalable: Improving the Speed and Accuracy of Neural Network Interatomic Potentials Across Chemical Domains</title><link>https://deep-diver.github.io/neurips2024/posters/y4mbazu4vy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y4mbazu4vy/</guid><description>ESCAIP, a novel neural network architecture, dramatically boosts the speed and accuracy of atomic simulations by leveraging attention mechanisms, enabling efficient large-scale modeling across diverse&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y4mbazu4vy/cover.png"/></item><item><title>The Iterative Optimal Brain Surgeon: Faster Sparse Recovery by Leveraging Second-Order Information</title><link>https://deep-diver.github.io/neurips2024/posters/snxwd0q4ei/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/snxwd0q4ei/</guid><description>I-OBS, a novel family of sparse recovery algorithms leveraging second-order information, achieves faster convergence rates for sparse DNNs, validated by large-scale experiments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/snxwd0q4ei/cover.png"/></item><item><title>The Poisson Midpoint Method for Langevin Dynamics: Provably Efficient Discretization for Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/posters/ylvviju6md/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ylvviju6md/</guid><description>Poisson Midpoint Method quadratically accelerates Langevin Monte Carlo for diffusion models, achieving high-quality image generation with significantly fewer computations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ylvviju6md/cover.png"/></item><item><title>The Prevalence of Neural Collapse in Neural Multivariate Regression</title><link>https://deep-diver.github.io/neurips2024/posters/wq6ay6fc2h/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wq6ay6fc2h/</guid><description>Neural networks exhibit &amp;lsquo;Neural Regression Collapse&amp;rsquo; (NRC) during training, where feature vectors collapse to subspaces spanned by principal components of features and weights, and the weight vector G&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wq6ay6fc2h/cover.png"/></item><item><title>The Selective $G$-Bispectrum and its Inversion: Applications to $G$-Invariant Networks</title><link>https://deep-diver.github.io/neurips2024/posters/lptwdyiy4o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lptwdyiy4o/</guid><description>This paper introduces a selective G-Bispectrum algorithm, slashing the computational complexity from O(|G|^2) to O(|G|), making G-invariant deep learning faster and more scalable.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lptwdyiy4o/cover.png"/></item><item><title>Time Makes Space: Emergence of Place Fields in Networks Encoding Temporally Continuous Sensory Experiences</title><link>https://deep-diver.github.io/neurips2024/posters/ioe66jecmf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ioe66jecmf/</guid><description>Networks trained on continuous sensory data spontaneously develop place cell-like responses, demonstrating that time-encoded experience can create spatial maps in the brain.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ioe66jecmf/cover.png"/></item><item><title>TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables</title><link>https://deep-diver.github.io/neurips2024/posters/inaeuq04lt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/inaeuq04lt/</guid><description>TimeXer empowers transformers for superior time series forecasting by cleverly integrating exogenous variables, achieving state-of-the-art results on diverse benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/inaeuq04lt/cover.png"/></item><item><title>TinyTTA: Efficient Test-time Adaptation via Early-exit Ensembles on Edge Devices</title><link>https://deep-diver.github.io/neurips2024/posters/xicbcbe6c3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xicbcbe6c3/</guid><description>TinyTTA enables efficient test-time adaptation on memory-constrained edge devices using a novel self-ensemble and early-exit strategy, improving accuracy and reducing memory usage.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xicbcbe6c3/cover.png"/></item><item><title>Towards a Scalable Reference-Free Evaluation of Generative Models</title><link>https://deep-diver.github.io/neurips2024/posters/ex3rpvect8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ex3rpvect8/</guid><description>FKEA: a novel, scalable method for reference-free evaluation of generative models&amp;rsquo; diversity using random Fourier features, overcoming computational limitations of existing entropy-based scores.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ex3rpvect8/cover.png"/></item><item><title>Towards Dynamic Message Passing on Graphs</title><link>https://deep-diver.github.io/neurips2024/posters/4bwlujf0e9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4bwlujf0e9/</guid><description>N2: A novel dynamic message-passing GNN tackles message-passing bottlenecks and high computational costs by introducing learnable pseudo-nodes and dynamic pathways in a common state space, achieving s&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4bwlujf0e9/cover.png"/></item><item><title>Towards Exact Gradient-based Training on Analog In-memory Computing</title><link>https://deep-diver.github.io/neurips2024/posters/5gwbklbiif/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/5gwbklbiif/</guid><description>Analog in-memory computing (AIMC) training suffers from asymptotic errors due to asymmetric updates. This paper rigorously proves this limitation, proposes a novel discrete-time model to characterize &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/5gwbklbiif/cover.png"/></item><item><title>Training Binary Neural Networks via Gaussian Variational Inference and Low-Rank Semidefinite Programming</title><link>https://deep-diver.github.io/neurips2024/posters/cixetwtkhk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cixetwtkhk/</guid><description>VISPA, a novel BNN training framework using Gaussian variational inference and low-rank SDP, achieves state-of-the-art accuracy on various benchmarks.</description></item><item><title>Transferable Boltzmann Generators</title><link>https://deep-diver.github.io/neurips2024/posters/ayq6gxxrry/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ayq6gxxrry/</guid><description>Transferable Boltzmann Generators enable efficient, zero-shot sampling of unseen molecular systems&amp;rsquo; equilibrium distributions, boosting molecular simulations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ayq6gxxrry/cover.png"/></item><item><title>Treeffuser: probabilistic prediction via conditional diffusions with gradient-boosted trees</title><link>https://deep-diver.github.io/neurips2024/posters/4kesvavnmr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4kesvavnmr/</guid><description>Treeffuser: Accurate probabilistic predictions from tabular data using conditional diffusion models and gradient-boosted trees!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4kesvavnmr/cover.png"/></item><item><title>TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks</title><link>https://deep-diver.github.io/neurips2024/posters/fofu3qhcig/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fofu3qhcig/</guid><description>TuneTables optimizes PFNs for scalability via context optimization, achieving state-of-the-art performance on large tabular datasets while using fewer parameters and reducing inference time.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fofu3qhcig/cover.png"/></item><item><title>UGC: Universal Graph Coarsening</title><link>https://deep-diver.github.io/neurips2024/posters/nn6nsd1qds/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nn6nsd1qds/</guid><description>UGC: Blazing-fast graph coarsening for big data, preserving key insights across diverse graph types.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nn6nsd1qds/cover.png"/></item><item><title>Unconditional stability of a recurrent neural circuit implementing divisive normalization</title><link>https://deep-diver.github.io/neurips2024/posters/5llb7axrn9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/5llb7axrn9/</guid><description>Biologically-inspired ORGANICs neural circuit achieves dynamic divisive normalization, ensuring unconditional stability and seamless backpropagation training for high-dimensional recurrent networks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/5llb7axrn9/cover.png"/></item><item><title>Understanding Generalizability of Diffusion Models Requires Rethinking the Hidden Gaussian Structure</title><link>https://deep-diver.github.io/neurips2024/posters/sk2dubgvrk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sk2dubgvrk/</guid><description>Diffusion models&amp;rsquo; surprising generalizability stems from an inductive bias towards learning Gaussian data structures, a finding that reshapes our understanding of their training and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sk2dubgvrk/cover.png"/></item><item><title>Understanding Representation of Deep Equilibrium Models from Neural Collapse Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/obuxeummq1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/obuxeummq1/</guid><description>Deep Equilibrium Models excel on imbalanced data due to feature convergence and self-duality properties, unlike explicit models, as shown through Neural Collapse analysis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/obuxeummq1/cover.png"/></item><item><title>Understanding the Expressivity and Trainability of Fourier Neural Operator: A Mean-Field Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/qjr02btm7j/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qjr02btm7j/</guid><description>A mean-field theory explains Fourier Neural Operator (FNO) behavior, linking expressivity to trainability by identifying ordered and chaotic phases that correspond to vanishing or exploding gradients,&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qjr02btm7j/cover.png"/></item><item><title>Unifying Generation and Prediction on Graphs with Latent Graph Diffusion</title><link>https://deep-diver.github.io/neurips2024/posters/lvibangnas/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lvibangnas/</guid><description>Latent Graph Diffusion (LGD) unifies graph learning, solving all task levels and types with a single framework and state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lvibangnas/cover.png"/></item><item><title>Unifying Homophily and Heterophily for Spectral Graph Neural Networks via Triple Filter Ensembles</title><link>https://deep-diver.github.io/neurips2024/posters/uatpopwzzu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uatpopwzzu/</guid><description>TFE-GNN: A novel spectral GNN using triple filter ensembles for superior homophily/heterophily handling and improved generalization on real-world graphs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uatpopwzzu/cover.png"/></item><item><title>UniIF: Unified Molecule Inverse Folding</title><link>https://deep-diver.github.io/neurips2024/posters/clqx9cvdkv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/clqx9cvdkv/</guid><description>UniIF: A unified model revolutionizes molecule inverse folding, achieving state-of-the-art results across protein, RNA, and material design by employing a novel geometric block attention network.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/clqx9cvdkv/cover.png"/></item><item><title>United We Stand, Divided We Fall: Fingerprinting Deep Neural Networks via Adversarial Trajectories</title><link>https://deep-diver.github.io/neurips2024/posters/ywpl0bvxts/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ywpl0bvxts/</guid><description>ADV-TRA uses adversarial trajectories to robustly fingerprint deep neural networks, outperforming state-of-the-art methods against various removal attacks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ywpl0bvxts/cover.png"/></item><item><title>UniTS: A Unified Multi-Task Time Series Model</title><link>https://deep-diver.github.io/neurips2024/posters/nbodybptww/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nbodybptww/</guid><description>UniTS: one model to rule them all! This unified multi-task time series model excels in forecasting, classification, anomaly detection, and imputation, outperforming specialized models across 38 divers&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nbodybptww/cover.png"/></item><item><title>Universal Neural Functionals</title><link>https://deep-diver.github.io/neurips2024/posters/w89fkkp2ao/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w89fkkp2ao/</guid><description>Universal Neural Functionals (UNFs) automatically construct permutation-equivariant models for any weight space, improving learned optimizer performance and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w89fkkp2ao/cover.png"/></item><item><title>Universal Physics Transformers: A Framework For Efficiently Scaling Neural Operators</title><link>https://deep-diver.github.io/neurips2024/posters/ouxinx5krm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ouxinx5krm/</guid><description>Universal Physics Transformers (UPTs) offer a unified, scalable framework for efficiently training neural operators across diverse spatio-temporal physics problems, overcoming limitations of existing &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ouxinx5krm/cover.png"/></item><item><title>Unveiling The Matthew Effect Across Channels: Assessing Layer Width Sufficiency via Weight Norm Variance</title><link>https://deep-diver.github.io/neurips2024/posters/tcft2v63vd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tcft2v63vd/</guid><description>Neural network efficiency is improved by analyzing weight norm variance across channels to identify optimal layer widths, resulting in reduced parameters and boosted performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tcft2v63vd/cover.png"/></item><item><title>Utilizing Image Transforms and Diffusion Models for Generative Modeling of Short and Long Time Series</title><link>https://deep-diver.github.io/neurips2024/posters/2nfbbpbn9x/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2nfbbpbn9x/</guid><description>ImagenTime transforms time series into images, leveraging advanced diffusion models for superior generative modeling of both short and long sequences.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2nfbbpbn9x/cover.png"/></item><item><title>Variational Flow Matching for Graph Generation</title><link>https://deep-diver.github.io/neurips2024/posters/uahrhr5hqh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uahrhr5hqh/</guid><description>CatFlow: a novel flow matching method for graph generation, offering superior computational efficiency and performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uahrhr5hqh/cover.png"/></item><item><title>Wasserstein Gradient Boosting: A Framework for Distribution-Valued Supervised Learning</title><link>https://deep-diver.github.io/neurips2024/posters/cuo0denqml/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cuo0denqml/</guid><description>Wasserstein Gradient Boosting (WGBoost) extends gradient boosting to handle probability distributions as outputs, enabling more robust and informative predictions in various applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cuo0denqml/cover.png"/></item><item><title>WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks Against Deep Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/j6nbyzllnj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/j6nbyzllnj/</guid><description>WaveAttack: A new backdoor attack method leveraging asymmetric frequency obfuscation for high stealthiness and effectiveness in Deep Neural Networks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/j6nbyzllnj/cover.png"/></item><item><title>Weight decay induces low-rank attention layers</title><link>https://deep-diver.github.io/neurips2024/posters/odeqjim9sk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/odeqjim9sk/</guid><description>Weight decay in deep learning surprisingly induces low-rank attention layers, potentially harming performance but offering optimization strategies for large language models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/odeqjim9sk/cover.png"/></item><item><title>Weight Diffusion for Future: Learn to Generalize in Non-Stationary Environments</title><link>https://deep-diver.github.io/neurips2024/posters/2cfuynnl1m/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2cfuynnl1m/</guid><description>Weight Diffusion (W-Diff) masters evolving domain generalization by using conditional diffusion models to learn classifier weight evolution patterns, enabling superior generalization to unseen future &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2cfuynnl1m/cover.png"/></item><item><title>Weight for Robustness: A Comprehensive Approach towards Optimal Fault-Tolerant Asynchronous ML</title><link>https://deep-diver.github.io/neurips2024/posters/v1kpc060ac/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v1kpc060ac/</guid><description>Optimal fault-tolerant asynchronous machine learning is achieved via a novel weighted robust aggregation framework, ensuring efficient training despite Byzantine failures and heterogeneous resources.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v1kpc060ac/cover.png"/></item><item><title>WeiPer: OOD Detection using Weight Perturbations of Class Projections</title><link>https://deep-diver.github.io/neurips2024/posters/8heuvbimkt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8heuvbimkt/</guid><description>WeiPer enhances OOD detection by cleverly perturbing class projections, creating a richer representation that improves various existing methods and achieves state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8heuvbimkt/cover.png"/></item><item><title>What If the Input is Expanded in OOD Detection?</title><link>https://deep-diver.github.io/neurips2024/posters/xfpifrnuas/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xfpifrnuas/</guid><description>Boost OOD detection accuracy by averaging model confidence scores from original and corrupted inputs!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xfpifrnuas/cover.png"/></item><item><title>What is my quantum computer good for? Quantum capability learning with physics-aware neural networks</title><link>https://deep-diver.github.io/neurips2024/posters/4cu9zvokbz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4cu9zvokbz/</guid><description>Quantum-physics-aware neural networks achieve up to 50% improved accuracy in predicting quantum computer capabilities, scaling to 100+ qubits.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4cu9zvokbz/cover.png"/></item><item><title>What Matters in Graph Class Incremental Learning? An Information Preservation Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/tjgx7tpgo8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tjgx7tpgo8/</guid><description>GSIP framework mitigates catastrophic forgetting in graph class incremental learning by preserving crucial graph information, achieving a 10% improvement in forgetting metrics.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tjgx7tpgo8/cover.png"/></item></channel></rss>