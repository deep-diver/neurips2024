<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/deep-learning/</link><description>Recent content in Deep Learning on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>$psilon$-Softmax: Approximating One-Hot Vectors for Mitigating Label Noise</title><link>https://deep-diver.github.io/neurips2024/posters/vjsd8bcipv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vjsd8bcipv/</guid><description>e-Softmax: A simple plug-and-play module enhances deep learning model robustness against noisy labels by approximating one-hot vectors, achieving noise-tolerant learning with controllable excess risk.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vjsd8bcipv/cover.png"/></item><item><title>A Foundation Model for Zero-shot Logical Query Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/jrsymbbji6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jrsymbbji6/</guid><description>ULTRAQUERY: a groundbreaking foundation model for zero-shot logical query reasoning on any knowledge graph, surpassing existing methods&amp;rsquo; limitations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jrsymbbji6/cover.png"/></item><item><title>A Functional Extension of Semi-Structured Networks</title><link>https://deep-diver.github.io/neurips2024/posters/wjaiaslhin/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wjaiaslhin/</guid><description>This paper introduces semi-structured functional networks (SSFNNs), a novel approach that combines interpretable functional regression models with deep neural networks, achieving both high accuracy an&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wjaiaslhin/cover.png"/></item><item><title>A Layer-Wise Natural Gradient Optimizer for Training Deep Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/nig3yyb6oa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nig3yyb6oa/</guid><description>LNGD: A Layer-Wise Natural Gradient optimizer drastically cuts deep neural network training time without sacrificing accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nig3yyb6oa/cover.png"/></item><item><title>A PID Controller Approach for Adaptive Probability-dependent Gradient Decay in Model Calibration</title><link>https://deep-diver.github.io/neurips2024/posters/fanubdsfpn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fanubdsfpn/</guid><description>Deep learning models often suffer from overconfidence; this paper introduces a PID controller to adaptively adjust a probability-dependent gradient decay rate, ensuring consistent optimization of both&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fanubdsfpn/cover.png"/></item><item><title>A Recipe for Charge Density Prediction</title><link>https://deep-diver.github.io/neurips2024/posters/b7rekanutv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/b7rekanutv/</guid><description>A novel machine learning recipe drastically accelerates charge density prediction in density functional theory, achieving state-of-the-art accuracy while being significantly faster than existing metho&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/b7rekanutv/cover.png"/></item><item><title>A Topology-aware Graph Coarsening Framework for Continual Graph Learning</title><link>https://deep-diver.github.io/neurips2024/posters/vpineevlx0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vpineevlx0/</guid><description>TACO, a novel topology-aware graph coarsening framework, tackles catastrophic forgetting in continual graph learning by efficiently preserving topological information during experience replay, signifi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vpineevlx0/cover.png"/></item><item><title>A two-scale Complexity Measure for Deep Learning Models</title><link>https://deep-diver.github.io/neurips2024/posters/ty9voszzia/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ty9voszzia/</guid><description>New 2sED measure effectively bounds deep learning model complexity, correlating well with training error and offering efficient computation, particularly for deep models via a layerwise approach.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ty9voszzia/cover.png"/></item><item><title>A versatile informative diffusion model for single-cell ATAC-seq data generation and analysis</title><link>https://deep-diver.github.io/neurips2024/posters/s6ylebmowf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s6ylebmowf/</guid><description>ATAC-Diff: A versatile diffusion model for high-quality single-cell ATAC-seq data generation and analysis, surpassing state-of-the-art.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/s6ylebmowf/cover.png"/></item><item><title>Absorb &amp; Escape: Overcoming Single Model Limitations in Generating Heterogeneous Genomic Sequences</title><link>https://deep-diver.github.io/neurips2024/posters/xhtl2k1lyk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xhtl2k1lyk/</guid><description>Absorb &amp;amp; Escape: a novel post-training sampling method that overcomes single model limitations by combining Autoregressive (AR) and Diffusion Models (DMs), generating high-quality heterogeneous genomi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xhtl2k1lyk/cover.png"/></item><item><title>Accelerating Relative Entropy Coding with Space Partitioning</title><link>https://deep-diver.github.io/neurips2024/posters/ouqywnunxm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ouqywnunxm/</guid><description>Space partitioning dramatically speeds up relative entropy coding (REC) for neural compression, achieving 5-15% better bitrates than previous methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ouqywnunxm/cover.png"/></item><item><title>Activation Map Compression through Tensor Decomposition for Deep Learning</title><link>https://deep-diver.github.io/neurips2024/posters/s93hrwt8u9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s93hrwt8u9/</guid><description>Slash deep learning&amp;rsquo;s memory footprint! This paper introduces a novel activation map compression technique via tensor decomposition, significantly boosting on-device training efficiency for edge AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/s93hrwt8u9/cover.png"/></item><item><title>Ada-MSHyper: Adaptive Multi-Scale Hypergraph Transformer for Time Series Forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/rnbriq0se8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rnbriq0se8/</guid><description>Ada-MSHyper: A novel adaptive multi-scale hypergraph transformer significantly boosts time series forecasting accuracy by modeling group-wise interactions and handling complex temporal variations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rnbriq0se8/cover.png"/></item><item><title>Adapting to Unknown Low-Dimensional Structures in Score-Based Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/posters/sntxbqsrw7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sntxbqsrw7/</guid><description>Score-based diffusion models are improved by a novel coefficient design, enabling efficient adaptation to unknown low-dimensional data structures and achieving a convergence rate of O(kÂ²/âˆšT).</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sntxbqsrw7/cover.png"/></item><item><title>Adaptive Passive-Aggressive Framework for Online Regression with Side Information</title><link>https://deep-diver.github.io/neurips2024/posters/kv80nc1afe/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kv80nc1afe/</guid><description>Adaptive Passive-Aggressive framework with Side information (APAS) significantly boosts online regression accuracy by dynamically adjusting thresholds and integrating side information, leading to supe&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kv80nc1afe/cover.png"/></item><item><title>Addressing Spectral Bias of Deep Neural Networks by Multi-Grade Deep Learning</title><link>https://deep-diver.github.io/neurips2024/posters/iort7ehfap/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/iort7ehfap/</guid><description>Multi-Grade Deep Learning (MGDL) conquers spectral bias in deep neural networks by incrementally learning low-frequency components, ultimately capturing high-frequency features through composition.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/iort7ehfap/cover.png"/></item><item><title>ADOPT: Modified Adam Can Converge with Any $eta_2$ with the Optimal Rate</title><link>https://deep-diver.github.io/neurips2024/posters/rzvvm0lsyk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rzvvm0lsyk/</guid><description>ADOPT, a novel adaptive gradient method, achieves optimal convergence rates without restrictive assumptions, unlike Adam, significantly improving deep learning optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rzvvm0lsyk/cover.png"/></item><item><title>Advancing Training Efficiency of Deep Spiking Neural Networks through Rate-based Backpropagation</title><link>https://deep-diver.github.io/neurips2024/posters/wlcm21c4nk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wlcm21c4nk/</guid><description>Rate-based backpropagation boosts deep spiking neural network training efficiency by leveraging rate coding, achieving comparable performance to BPTT with reduced complexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wlcm21c4nk/cover.png"/></item><item><title>Alias-Free Mamba Neural Operator</title><link>https://deep-diver.github.io/neurips2024/posters/guebxgv8jm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/guebxgv8jm/</guid><description>MambaNO: a novel neural operator achieving linear complexity and state-of-the-art accuracy in solving PDEs by cleverly balancing global and local information using an alias-free architecture.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/guebxgv8jm/cover.png"/></item><item><title>Amortized Fourier Neural Operators</title><link>https://deep-diver.github.io/neurips2024/posters/a6em980m9x/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/a6em980m9x/</guid><description>Amortized Fourier Neural Operators (AM-FNOs) dramatically improve efficiency in solving PDEs by using neural networks for kernel parameterization, achieving up to 31% better accuracy compared to exist&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/a6em980m9x/cover.png"/></item><item><title>An exactly solvable model for emergence and scaling laws in the multitask sparse parity problem</title><link>https://deep-diver.github.io/neurips2024/posters/cuwsr25bbi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cuwsr25bbi/</guid><description>A novel multilinear model analytically explains the emergence and scaling laws of skills in the multitask sparse parity problem, accurately predicting skill emergence in neural networks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cuwsr25bbi/cover.png"/></item><item><title>ANT: Adaptive Noise Schedule for Time Series Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/posters/1ojaktylz4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1ojaktylz4/</guid><description>ANT: An adaptive noise schedule automatically determines optimal noise schedules for time series diffusion models, significantly boosting performance across diverse tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1ojaktylz4/cover.png"/></item><item><title>Approximately Equivariant Neural Processes</title><link>https://deep-diver.github.io/neurips2024/posters/dqt9mc5nql/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dqt9mc5nql/</guid><description>Boosting meta-learning, this paper introduces a novel, flexible approach to create approximately equivariant neural processes that outperform both non-equivariant and strictly equivariant counterparts&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dqt9mc5nql/cover.png"/></item><item><title>Approximation Rate of the Transformer Architecture for Sequence Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/zws2y21mzv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zws2y21mzv/</guid><description>This paper unveils the Transformer&amp;rsquo;s approximation power, deriving explicit Jackson-type rates to reveal its strengths and limitations in handling various sequential relationships.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zws2y21mzv/cover.png"/></item><item><title>Are Self-Attentions Effective for Time Series Forecasting?</title><link>https://deep-diver.github.io/neurips2024/posters/in43sjoib7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/in43sjoib7/</guid><description>Cross-Attention-only Time Series Transformer (CATS) outperforms existing models by removing self-attention, improving long-term forecasting accuracy, and reducing computational cost.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/in43sjoib7/cover.png"/></item><item><title>Are Uncertainty Quantification Capabilities of Evidential Deep Learning a Mirage?</title><link>https://deep-diver.github.io/neurips2024/posters/p6nvdzrzrb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/p6nvdzrzrb/</guid><description>Evidential deep learning&amp;rsquo;s uncertainty quantification is unreliable; this paper reveals its limitations, proposes model uncertainty incorporation for improved performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/p6nvdzrzrb/cover.png"/></item><item><title>AROMA: Preserving Spatial Structure for Latent PDE Modeling with Local Neural Fields</title><link>https://deep-diver.github.io/neurips2024/posters/aj8rkcgwje/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/aj8rkcgwje/</guid><description>AROMA: Attentive Reduced Order Model with Attention enhances PDE modeling with local neural fields, offering efficient processing of diverse geometries and superior performance in simulating 1D and 2D&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/aj8rkcgwje/cover.png"/></item><item><title>B-ary Tree Push-Pull Method is Provably Efficient for Distributed Learning on Heterogeneous Data</title><link>https://deep-diver.github.io/neurips2024/posters/3mnxactbd3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3mnxactbd3/</guid><description>B-ary Tree Push-Pull (BTPP) achieves linear speedup for distributed learning on heterogeneous data, significantly outperforming state-of-the-art methods with minimal communication.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3mnxactbd3/cover.png"/></item><item><title>BAN: Detecting Backdoors Activated by Neuron Noise</title><link>https://deep-diver.github.io/neurips2024/posters/asyyszl4n5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/asyyszl4n5/</guid><description>BAN: a novel backdoor defense using adversarial neuron noise for efficient detection and mitigation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/asyyszl4n5/cover.png"/></item><item><title>Better by default: Strong pre-tuned MLPs and boosted trees on tabular data</title><link>https://deep-diver.github.io/neurips2024/posters/3bnpudvqmt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3bnpudvqmt/</guid><description>Strong pre-tuned MLPs and meta-tuned default parameters for GBDTs and MLPs improve tabular data classification and regression.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3bnpudvqmt/cover.png"/></item><item><title>Block Sparse Bayesian Learning: A Diversified Scheme</title><link>https://deep-diver.github.io/neurips2024/posters/a4cppx1xyg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/a4cppx1xyg/</guid><description>Diversified Block Sparse Bayesian Learning (DivSBL) improves block sparse signal recovery by adapting to unknown block structures, enhancing accuracy and robustness over existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/a4cppx1xyg/cover.png"/></item><item><title>Boosting Graph Pooling with Persistent Homology</title><link>https://deep-diver.github.io/neurips2024/posters/wcmqdy2aku/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wcmqdy2aku/</guid><description>Boosting graph neural networks: Topology-Invariant Pooling (TIP) leverages persistent homology to enhance graph pooling, achieving consistent performance gains across diverse datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wcmqdy2aku/cover.png"/></item><item><title>Breaking the curse of dimensionality in structured density estimation</title><link>https://deep-diver.github.io/neurips2024/posters/dwwin2ugye/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dwwin2ugye/</guid><description>Researchers break the curse of dimensionality in structured density estimation using graph resilience, a novel graphical parameter that effectively reduces the sample complexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dwwin2ugye/cover.png"/></item><item><title>Bridging Geometric States via Geometric Diffusion Bridge</title><link>https://deep-diver.github.io/neurips2024/posters/zcepob9rcr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zcepob9rcr/</guid><description>Geometric Diffusion Bridge (GDB) accurately predicts geometric state evolution in complex systems by leveraging a probabilistic approach and equivariant diffusion processes, surpassing existing deep l&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zcepob9rcr/cover.png"/></item><item><title>Bridging OOD Detection and Generalization: A Graph-Theoretic View</title><link>https://deep-diver.github.io/neurips2024/posters/qzwag8qxi1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qzwag8qxi1/</guid><description>A novel graph-theoretic framework bridges OOD detection &amp;amp; generalization, offering theoretical error bounds and competitive empirical performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qzwag8qxi1/cover.png"/></item><item><title>Cardinality-Aware Set Prediction and Top-$k$ Classification</title><link>https://deep-diver.github.io/neurips2024/posters/wat3qu737x/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wat3qu737x/</guid><description>This paper proposes cardinality-aware top-k classification, improving accuracy and efficiency by dynamically adjusting prediction set sizes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wat3qu737x/cover.png"/></item><item><title>Changing the Training Data Distribution to Reduce Simplicity Bias Improves In-distribution Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/yyspldusu2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yyspldusu2/</guid><description>Boosting in-distribution generalization is achieved by strategically altering the training data distribution to reduce simplicity bias and promote uniform feature learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yyspldusu2/cover.png"/></item><item><title>CODA: A Correlation-Oriented Disentanglement and Augmentation Modeling Scheme for Better Resisting Subpopulation Shifts</title><link>https://deep-diver.github.io/neurips2024/posters/lrsrjzzcle/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lrsrjzzcle/</guid><description>CODA: A novel modeling scheme tackles subpopulation shifts in machine learning by disentangling spurious correlations, augmenting data strategically, and using reweighted consistency loss for improved&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lrsrjzzcle/cover.png"/></item><item><title>Collaborative Refining for Learning from Inaccurate Labels</title><link>https://deep-diver.github.io/neurips2024/posters/rqqgbbqvbl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rqqgbbqvbl/</guid><description>Collaborative Refining for Learning from Inaccurate Labels (CRL) refines data using annotator agreement, improving model accuracy with noisy labels.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rqqgbbqvbl/cover.png"/></item><item><title>Combining Statistical Depth and Fermat Distance for Uncertainty Quantification</title><link>https://deep-diver.github.io/neurips2024/posters/xexrhtumcf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xexrhtumcf/</guid><description>Boosting neural network prediction reliability, this research ingeniously combines statistical depth and Fermat distance for superior uncertainty quantification, eliminating the need for distributiona&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xexrhtumcf/cover.png"/></item><item><title>Con4m: Context-aware Consistency Learning Framework for Segmented Time Series Classification</title><link>https://deep-diver.github.io/neurips2024/posters/jcpufqahvb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jcpufqahvb/</guid><description>Con4m, a novel consistency learning framework, leverages contextual information to effectively classify segmented time series with inconsistent boundary labels and varying durations of classes, signif&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jcpufqahvb/cover.png"/></item><item><title>CondTSF: One-line Plugin of Dataset Condensation for Time Series Forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/l1jajnwon5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/l1jajnwon5/</guid><description>CondTSF: One-line plugin for time series forecasting dataset condensation, boosting performance at low condensation ratios.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/l1jajnwon5/cover.png"/></item><item><title>Confidence Calibration of Classifiers with Many Classes</title><link>https://deep-diver.github.io/neurips2024/posters/ebbnkvxmcz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ebbnkvxmcz/</guid><description>Boost multi-class classifier calibration by cleverly transforming the problem into a single binary calibration task!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ebbnkvxmcz/cover.png"/></item><item><title>Conformal Prediction for Class-wise Coverage via Augmented Label Rank Calibration</title><link>https://deep-diver.github.io/neurips2024/posters/t7ds1ghwwu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/t7ds1ghwwu/</guid><description>RC3P, a novel algorithm, significantly reduces prediction set sizes in class-conditional conformal prediction while guaranteeing class-wise coverage, even on imbalanced datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/t7ds1ghwwu/cover.png"/></item><item><title>Conformalized Credal Set Predictors</title><link>https://deep-diver.github.io/neurips2024/posters/vbah12uvbd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vbah12uvbd/</guid><description>Conformal prediction empowers robust credal set predictions, handling aleatoric and epistemic uncertainties in classification, guaranteed to be valid with high probability!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vbah12uvbd/cover.png"/></item><item><title>Conformalized Time Series with Semantic Features</title><link>https://deep-diver.github.io/neurips2024/posters/kcdcavow1s/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kcdcavow1s/</guid><description>Conformalized Time Series with Semantic Features (CT-SSF) significantly improves time-series forecasting by dynamically weighting latent semantic features, achieving greater prediction efficiency whil&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kcdcavow1s/cover.png"/></item><item><title>Consistency Models for Scalable and Fast Simulation-Based Inference</title><link>https://deep-diver.github.io/neurips2024/posters/ihjkpkljyh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ihjkpkljyh/</guid><description>CMPE: a new conditional sampler for SBI, achieves fast few-shot inference with an unconstrained architecture, outperforming current state-of-the-art algorithms on various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ihjkpkljyh/cover.png"/></item><item><title>ControlSynth Neural ODEs: Modeling Dynamical Systems with Guaranteed Convergence</title><link>https://deep-diver.github.io/neurips2024/posters/dbe8khdmfs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dbe8khdmfs/</guid><description>ControlSynth Neural ODEs (CSODEs) guarantee convergence in complex dynamical systems via tractable linear inequalities, improving neural ODE modeling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dbe8khdmfs/cover.png"/></item><item><title>Convolutions and More as Einsum: A Tensor Network Perspective with Advances for Second-Order Methods</title><link>https://deep-diver.github.io/neurips2024/posters/cds8wxnmvp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cds8wxnmvp/</guid><description>This paper accelerates second-order optimization in CNNs by 4.5x, using a novel tensor network representation that simplifies convolutions and reduces memory overhead.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cds8wxnmvp/cover.png"/></item><item><title>Counter-Current Learning: A Biologically Plausible Dual Network Approach for Deep Learning</title><link>https://deep-diver.github.io/neurips2024/posters/l3rybqzrmf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/l3rybqzrmf/</guid><description>Biologically inspired Counter-Current Learning (CCL) uses dual networks for deep learning, offering comparable performance to other biologically plausible algorithms while enhancing biological realism&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/l3rybqzrmf/cover.png"/></item><item><title>Credal Deep Ensembles for Uncertainty Quantification</title><link>https://deep-diver.github.io/neurips2024/posters/pcgntigc9k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pcgntigc9k/</guid><description>Credal Deep Ensembles (CreDEs) improve uncertainty quantification in deep learning by predicting probability intervals, enhancing accuracy and calibration, particularly for out-of-distribution data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pcgntigc9k/cover.png"/></item><item><title>CRONOS: Enhancing Deep Learning with Scalable GPU Accelerated Convex Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/yflzyczao3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yflzyczao3/</guid><description>CRONOS: Scaling convex neural network training to ImageNet!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yflzyczao3/cover.png"/></item><item><title>Cross-Device Collaborative Test-Time Adaptation</title><link>https://deep-diver.github.io/neurips2024/posters/yymio0dwmi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yymio0dwmi/</guid><description>CoLA: Collaborative Lifelong Adaptation boosts test-time adaptation efficiency by sharing domain knowledge across multiple devices, achieving significant accuracy gains with minimal computational over&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yymio0dwmi/cover.png"/></item><item><title>DASH: Warm-Starting Neural Network Training in Stationary Settings without Loss of Plasticity</title><link>https://deep-diver.github.io/neurips2024/posters/idquuyma1t/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/idquuyma1t/</guid><description>DASH combats neural network training&amp;rsquo;s plasticity loss during warm-starting by selectively forgetting memorized noise while preserving features, improving accuracy and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/idquuyma1t/cover.png"/></item><item><title>DDN: Dual-domain Dynamic Normalization for Non-stationary Time Series Forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/rvzfra6szo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rvzfra6szo/</guid><description>DDN: Dual-domain Dynamic Normalization dynamically improves time series forecasting accuracy by addressing data distribution changes in both time and frequency domains via a plug-in module.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rvzfra6szo/cover.png"/></item><item><title>Deep Equilibrium Algorithmic Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/sulxkxcena/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sulxkxcena/</guid><description>Deep Equilibrium Algorithmic Reasoners (DEARs) achieve superior performance on algorithmic tasks by directly solving for the equilibrium point of a neural network, eliminating the need for iterative r&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sulxkxcena/cover.png"/></item><item><title>Deep Graph Neural Networks via Posteriori-Sampling-based Node-Adaptative Residual Module</title><link>https://deep-diver.github.io/neurips2024/posters/vywzsaghp0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vywzsaghp0/</guid><description>PSNR, a novel node-adaptive residual module, significantly improves deep GNN performance by mitigating over-smoothing and handling missing data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vywzsaghp0/cover.png"/></item><item><title>DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection</title><link>https://deep-diver.github.io/neurips2024/posters/ibkppabhvn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ibkppabhvn/</guid><description>DeepDRK, a novel deep learning approach, significantly improves feature selection by effectively balancing false discovery rate and power, surpassing existing methods, especially with limited data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ibkppabhvn/cover.png"/></item><item><title>DeepLag: Discovering Deep Lagrangian Dynamics for Intuitive Fluid Prediction</title><link>https://deep-diver.github.io/neurips2024/posters/scw6et4per/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/scw6et4per/</guid><description>DeepLag improves fluid prediction by uniquely combining Lagrangian and Eulerian perspectives, tracking key particles to reveal hidden dynamics and improve prediction accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/scw6et4per/cover.png"/></item><item><title>Dendritic Integration Inspired Artificial Neural Networks Capture Data Correlation</title><link>https://deep-diver.github.io/neurips2024/posters/2wqjnxzbhr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2wqjnxzbhr/</guid><description>Biologically-inspired Dit-CNNs leverage quadratic neuron integration to capture data correlation, achieving state-of-the-art performance on image classification benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2wqjnxzbhr/cover.png"/></item><item><title>Dense Associative Memory Through the Lens of Random Features</title><link>https://deep-diver.github.io/neurips2024/posters/164qnjsyjf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/164qnjsyjf/</guid><description>Boost associative memory capacity without extra parameters! DrDAM uses random features to approximate Dense Associative Memories, enabling efficient memory addition and retrieval.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/164qnjsyjf/cover.png"/></item><item><title>Derivative-enhanced Deep Operator Network</title><link>https://deep-diver.github.io/neurips2024/posters/waiqlgfqx6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/waiqlgfqx6/</guid><description>Derivative-enhanced DeepONets boost PDE solution accuracy and derivative approximation, particularly valuable with limited training data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/waiqlgfqx6/cover.png"/></item><item><title>DiffPO: A causal diffusion model for learning distributions of potential outcomes</title><link>https://deep-diver.github.io/neurips2024/posters/merj77jipt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/merj77jipt/</guid><description>DiffPO: A causal diffusion model learns outcome distributions, offering reliable medical interventions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/merj77jipt/cover.png"/></item><item><title>DiffusionPDE: Generative PDE-Solving under Partial Observation</title><link>https://deep-diver.github.io/neurips2024/posters/z0i2sbjn0r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z0i2sbjn0r/</guid><description>DiffusionPDE uses generative diffusion models to solve PDEs accurately, even with highly incomplete observations, outperforming state-of-the-art methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/z0i2sbjn0r/cover.png"/></item><item><title>DiGRAF: Diffeomorphic Graph-Adaptive Activation Function</title><link>https://deep-diver.github.io/neurips2024/posters/zzow4z3le4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zzow4z3le4/</guid><description>DIGRAF, a novel graph-adaptive activation function, significantly boosts Graph Neural Network performance by dynamically adapting to graph structure, offering consistent superior results across divers&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zzow4z3le4/cover.png"/></item><item><title>DisCEdit: Model Editing by Identifying Discriminative Components</title><link>https://deep-diver.github.io/neurips2024/posters/tuiqq1g8i5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tuiqq1g8i5/</guid><description>DISCEDIT efficiently identifies and edits discriminative neural network components for structured pruning and class unlearning, achieving high sparsity and forgetting rates without needing training da&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tuiqq1g8i5/cover.png"/></item><item><title>Discrete-state Continuous-time Diffusion for Graph Generation</title><link>https://deep-diver.github.io/neurips2024/posters/ykskzehiyt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ykskzehiyt/</guid><description>DISCO: a novel discrete-state continuous-time diffusion model for flexible and efficient graph generation, outperforming state-of-the-art methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ykskzehiyt/cover.png"/></item><item><title>Divide-and-Conquer Posterior Sampling for Denoising Diffusion priors</title><link>https://deep-diver.github.io/neurips2024/posters/borut7m2x7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/borut7m2x7/</guid><description>Divide-and-Conquer Posterior Sampling (DCPS) efficiently samples complex posterior distributions from denoising diffusion models (DDMs) for Bayesian inverse problems, significantly improving accuracy &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/borut7m2x7/cover.png"/></item><item><title>DOFEN: Deep Oblivious Forest ENsemble</title><link>https://deep-diver.github.io/neurips2024/posters/umukvcdgi6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/umukvcdgi6/</guid><description>DOFEN: Deep Oblivious Forest Ensemble achieves state-of-the-art performance on tabular data by using a novel DNN architecture inspired by oblivious decision trees, surpassing other DNNs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/umukvcdgi6/cover.png"/></item><item><title>Dynamic Conditional Optimal Transport through Simulation-Free Flows</title><link>https://deep-diver.github.io/neurips2024/posters/tk0uarynhh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tk0uarynhh/</guid><description>Simulation-free flow generates conditional distributions via dynamic conditional optimal transport.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tk0uarynhh/cover.png"/></item><item><title>Dynamic Rescaling for Training GNNs</title><link>https://deep-diver.github.io/neurips2024/posters/ifzwsrpqhl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ifzwsrpqhl/</guid><description>Dynamic rescaling boosts GNN training by controlling layer learning speeds and balancing networks, leading to faster training and improved generalization, especially on heterophilic graphs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ifzwsrpqhl/cover.png"/></item><item><title>EGonc : Energy-based Open-Set Node Classification with substitute Unknowns</title><link>https://deep-diver.github.io/neurips2024/posters/3cl2xdyaeb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3cl2xdyaeb/</guid><description>EGonc, a novel energy-based open-set node classification method, leverages substitute unknowns and energy scores to achieve superior accuracy and robustness in classifying nodes from known classes whi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3cl2xdyaeb/cover.png"/></item><item><title>ElasTST: Towards Robust Varied-Horizon Forecasting with Elastic Time-Series Transformer</title><link>https://deep-diver.github.io/neurips2024/posters/ucxutmpwhv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ucxutmpwhv/</guid><description>ElasTST: A novel time-series transformer enables robust forecasting across various horizons without per-horizon training, enhancing adaptability and accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ucxutmpwhv/cover.png"/></item><item><title>Energy-Based Modelling for Discrete and Mixed Data via Heat Equations on Structured Spaces</title><link>https://deep-diver.github.io/neurips2024/posters/waqdvck1fv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/waqdvck1fv/</guid><description>Train discrete EBMs efficiently with Energy Discrepancy, a novel loss function that eliminates the need for Markov Chain Monte Carlo, using diffusion processes on structured spaces.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/waqdvck1fv/cover.png"/></item><item><title>Enhancing Diversity in Bayesian Deep Learning via Hyperspherical Energy Minimization of CKA</title><link>https://deep-diver.github.io/neurips2024/posters/s2ha6bz3le/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s2ha6bz3le/</guid><description>Boosting Bayesian deep learning&amp;rsquo;s diversity and uncertainty quantification, this study proposes hyperspherical energy minimization of CKA to generate diverse and reliable neural network ensembles and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/s2ha6bz3le/cover.png"/></item><item><title>Enhancing Protein Mutation Effect Prediction through a Retrieval-Augmented Framework</title><link>https://deep-diver.github.io/neurips2024/posters/lgehswiwef/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lgehswiwef/</guid><description>Revolutionizing protein mutation effect prediction, this work introduces a retrieval-augmented framework achieving state-of-the-art accuracy by efficiently incorporating similar local structure inform&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lgehswiwef/cover.png"/></item><item><title>EnOF-SNN: Training Accurate Spiking Neural Networks via Enhancing the Output Feature</title><link>https://deep-diver.github.io/neurips2024/posters/spcewp6eyt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/spcewp6eyt/</guid><description>EnOF-SNN boosts spiking neural network (SNN) accuracy by enhancing output feature representation using a novel knowledge distillation method and ReLU activation, outperforming current state-of-the-art&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/spcewp6eyt/cover.png"/></item><item><title>Equivariant Blurring Diffusion for Hierarchical Molecular Conformer Generation</title><link>https://deep-diver.github.io/neurips2024/posters/aj0zf28l6o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/aj0zf28l6o/</guid><description>Equivariant Blurring Diffusion (EBD) generates 3D molecular conformers hierarchically, first creating coarse-grained fragments then refining atomic details, significantly outperforming existing method&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/aj0zf28l6o/cover.png"/></item><item><title>Equivariant Machine Learning on Graphs with Nonlinear Spectral Filters</title><link>https://deep-diver.github.io/neurips2024/posters/y8p633e5hq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y8p633e5hq/</guid><description>Nonlinear spectral filters (NLSFs) enable fully equivariant graph neural networks, improving accuracy and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y8p633e5hq/cover.png"/></item><item><title>Equivariant Neural Diffusion for Molecule Generation</title><link>https://deep-diver.github.io/neurips2024/posters/40pe5pfhwl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/40pe5pfhwl/</guid><description>Equivariant Neural Diffusion (END) revolutionizes 3D molecule generation with a learnable forward process, achieving state-of-the-art results and enhanced controllability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/40pe5pfhwl/cover.png"/></item><item><title>Estimating Epistemic and Aleatoric Uncertainty with a Single Model</title><link>https://deep-diver.github.io/neurips2024/posters/wpxa6ocidg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wpxa6ocidg/</guid><description>HyperDM accurately estimates both epistemic and aleatoric uncertainty using a single model, overcoming the computational limitations of existing ensemble methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wpxa6ocidg/cover.png"/></item><item><title>Even Sparser Graph Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/k3k4bwunnk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/k3k4bwunnk/</guid><description>Spexphormer achieves significant memory reduction in graph Transformers by leveraging a two-stage training process that leverages attention score consistency across network widths to effectively spars&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/k3k4bwunnk/cover.png"/></item><item><title>Ex Uno Pluria: Insights on Ensembling in Low Precision Number Systems</title><link>https://deep-diver.github.io/neurips2024/posters/cbtkdwzzdq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cbtkdwzzdq/</guid><description>Low Precision Ensembling (LPE) boosts large model accuracy using training-free ensemble creation via stochastic rounding in low-precision number systems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cbtkdwzzdq/cover.png"/></item><item><title>eXponential FAmily Dynamical Systems (XFADS): Large-scale nonlinear Gaussian state-space modeling</title><link>https://deep-diver.github.io/neurips2024/posters/ln8ogihz2s/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ln8ogihz2s/</guid><description>XFADS: a novel low-rank structured VAE framework for large-scale nonlinear Gaussian state-space modeling, achieving high predictive accuracy and scalability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ln8ogihz2s/cover.png"/></item><item><title>Fast yet Safe: Early-Exiting with Risk Control</title><link>https://deep-diver.github.io/neurips2024/posters/bbfjpasrgs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bbfjpasrgs/</guid><description>Risk control boosts early-exit neural networks&amp;rsquo; speed and safety by ensuring accurate predictions before exiting early, achieving substantial computational savings across diverse tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bbfjpasrgs/cover.png"/></item><item><title>Faster Local Solvers for Graph Diffusion Equations</title><link>https://deep-diver.github.io/neurips2024/posters/3z0ltdjim0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3z0ltdjim0/</guid><description>Revolutionizing graph analysis, this paper introduces a novel framework for efficiently solving graph diffusion equations, achieving up to a hundred-fold speed improvement and enabling faster graph ne&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3z0ltdjim0/cover.png"/></item><item><title>FilterNet: Harnessing Frequency Filters for Time Series Forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/ugl2d9idad/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ugl2d9idad/</guid><description>FilterNet: A novel deep learning architecture using learnable frequency filters for superior time series forecasting accuracy and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ugl2d9idad/cover.png"/></item><item><title>Fine-Grained Dynamic Framework for Bias-Variance Joint Optimization on Data Missing Not at Random</title><link>https://deep-diver.github.io/neurips2024/posters/gloe70tn8v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gloe70tn8v/</guid><description>A new fine-grained dynamic framework jointly optimizes bias and variance for accurate predictions from missing-not-at-random data, surpassing existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gloe70tn8v/cover.png"/></item><item><title>FlexSBDD: Structure-Based Drug Design with Flexible Protein Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/4ab54h21qg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4ab54h21qg/</guid><description>FlexSBDD, a novel deep generative model, accurately predicts flexible protein-ligand complex structures, generating high-affinity drug molecules while overcoming the limitations of rigid protein model&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4ab54h21qg/cover.png"/></item><item><title>Frequency Adaptive Normalization For Non-stationary Time Series Forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/t0axiflvdd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/t0axiflvdd/</guid><description>Frequency Adaptive Normalization (FAN) significantly boosts non-stationary time series forecasting accuracy by using Fourier transforms to identify and model dynamic trends and seasonal patterns, achi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/t0axiflvdd/cover.png"/></item><item><title>From Biased to Unbiased Dynamics: An Infinitesimal Generator Approach</title><link>https://deep-diver.github.io/neurips2024/posters/tgmwp9jjxl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tgmwp9jjxl/</guid><description>Learn unbiased molecular dynamics from limited biased data using a novel infinitesimal generator approach; accurately estimating eigenfunctions and eigenvalues even with suboptimal biasing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tgmwp9jjxl/cover.png"/></item><item><title>Full-Atom Peptide Design with Geometric Latent Diffusion</title><link>https://deep-diver.github.io/neurips2024/posters/iaqnjuje8q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/iaqnjuje8q/</guid><description>PepGLAD, a novel generative model, revolutionizes full-atom peptide design by leveraging geometric latent diffusion to significantly enhance peptide diversity and binding affinity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/iaqnjuje8q/cover.png"/></item><item><title>Gene-Gene Relationship Modeling Based on Genetic Evidence for Single-Cell RNA-Seq Data Imputation</title><link>https://deep-diver.github.io/neurips2024/posters/gw0zng5jcg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gw0zng5jcg/</guid><description>Novel imputation method, scCR, leverages complete gene-gene relationships (associating &amp;amp; dissociating) for superior single-cell RNA sequencing data recovery, significantly outperforming current state-&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gw0zng5jcg/cover.png"/></item><item><title>Generalized Fast Exact Conformalization</title><link>https://deep-diver.github.io/neurips2024/posters/knzyj5zqsg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/knzyj5zqsg/</guid><description>This paper presents a novel method for fast and exact conformalization, leveraging inherent piecewise smoothness to dramatically accelerate uncertainty quantification in machine learning models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/knzyj5zqsg/cover.png"/></item><item><title>Generative Modeling of Molecular Dynamics Trajectories</title><link>https://deep-diver.github.io/neurips2024/posters/yrrch1osgw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yrrch1osgw/</guid><description>MDGEN: Generative modeling unlocks MD data for diverse tasks, achieving significant speedups via flexible multi-task surrogate models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yrrch1osgw/cover.png"/></item><item><title>Genetic-guided GFlowNets for Sample Efficient Molecular Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/b4q98aazwt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/b4q98aazwt/</guid><description>Genetic-guided GFlowNets revolutionize sample-efficient molecular optimization by smartly integrating genetic algorithms into GFlowNets training, achieving state-of-the-art performance with substantia&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/b4q98aazwt/cover.png"/></item><item><title>GENOT: Entropic (Gromov) Wasserstein Flow Matching with Applications to Single-Cell Genomics</title><link>https://deep-diver.github.io/neurips2024/posters/hjspwd7jvg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hjspwd7jvg/</guid><description>GENOT: a flexible neural optimal transport framework for single-cell genomics, enabling stochastic map learning with any cost function, handling unbalanced data, and tackling complex (Fused) Gromov-Wa&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hjspwd7jvg/cover.png"/></item><item><title>Geometry Awakening: Cross-Geometry Learning Exhibits Superiority over Individual Structures</title><link>https://deep-diver.github.io/neurips2024/posters/347adobxea/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/347adobxea/</guid><description>Cross-geometry learning using knowledge distillation significantly improves GNN performance by leveraging both Euclidean and hyperbolic geometric properties of graph data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/347adobxea/cover.png"/></item><item><title>Geometry-aware training of factorized layers in tensor Tucker format</title><link>https://deep-diver.github.io/neurips2024/posters/abtcfcrjm3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/abtcfcrjm3/</guid><description>Train factorized neural network layers efficiently with Geometry-aware training in Tucker format (TDLRT)!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/abtcfcrjm3/cover.png"/></item><item><title>Gradient Rewiring for Editable Graph Neural Network Training</title><link>https://deep-diver.github.io/neurips2024/posters/xy2qrq7cxm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xy2qrq7cxm/</guid><description>Gradient Rewiring (GRE) improves editable GNN training by addressing gradient inconsistencies, preserving training node performance while correcting target node errors.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xy2qrq7cxm/cover.png"/></item><item><title>Gradient-based Discrete Sampling with Automatic Cyclical Scheduling</title><link>https://deep-diver.github.io/neurips2024/posters/4syq5cgwa2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4syq5cgwa2/</guid><description>ACS: Automatic Cyclical Scheduling revolutionizes gradient-based discrete sampling by intelligently switching between exploration and exploitation phases to efficiently navigate complex multimodal dis&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4syq5cgwa2/cover.png"/></item><item><title>Graph Classification via Reference Distribution Learning: Theory and Practice</title><link>https://deep-diver.github.io/neurips2024/posters/1zvinhehks/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1zvinhehks/</guid><description>GRDL: a novel graph classification method boasting 10x speed improvement over competitors, achieved by treating node embeddings as distributions and avoiding global pooling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1zvinhehks/cover.png"/></item><item><title>Graph Edit Distance with General Costs Using Neural Set Divergence</title><link>https://deep-diver.github.io/neurips2024/posters/u7jrmrgutt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/u7jrmrgutt/</guid><description>GRAPHEDX, a novel neural network, accurately estimates graph edit distance with varying operation costs, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/u7jrmrgutt/cover.png"/></item><item><title>Graph Neural Flows for Unveiling Systemic Interactions Among Irregularly Sampled Time Series</title><link>https://deep-diver.github.io/neurips2024/posters/tfb5ssabvb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tfb5ssabvb/</guid><description>GNeuralFlow unveils systemic interactions in irregularly sampled time series by learning a directed acyclic graph representing conditional dependencies, achieving superior performance in classificatio&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tfb5ssabvb/cover.png"/></item><item><title>Graph Neural Networks Need Cluster-Normalize-Activate Modules</title><link>https://deep-diver.github.io/neurips2024/posters/faj2ebhdhc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/faj2ebhdhc/</guid><description>Boost GNN performance and overcome oversmoothing with Cluster-Normalize-Activate (CNA) modules: a simple yet highly effective plug-and-play solution!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/faj2ebhdhc/cover.png"/></item><item><title>GraphMETRO: Mitigating Complex Graph Distribution Shifts via Mixture of Aligned Experts</title><link>https://deep-diver.github.io/neurips2024/posters/qtyg4g3deu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qtyg4g3deu/</guid><description>GraphMETRO tackles complex graph distribution shifts by using a Mixture-of-Experts model to decompose shifts into interpretable components, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qtyg4g3deu/cover.png"/></item><item><title>Guiding Neural Collapse: Optimising Towards the Nearest Simplex Equiangular Tight Frame</title><link>https://deep-diver.github.io/neurips2024/posters/z4fapuslma/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z4fapuslma/</guid><description>Researchers devised a novel method to accelerate neural network training by guiding the optimization process toward a Simplex Equiangular Tight Frame, exploiting the Neural Collapse phenomenon to enha&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/z4fapuslma/cover.png"/></item><item><title>Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects Models</title><link>https://deep-diver.github.io/neurips2024/posters/uxuobobjho/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uxuobobjho/</guid><description>Accelerate Bayesian inference in linear mixed-effects models by efficiently marginalizing random effects using fast linear algebra, enabling faster and more accurate posterior estimations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uxuobobjho/cover.png"/></item><item><title>Hamiltonian Monte Carlo on ReLU Neural Networks is Inefficient</title><link>https://deep-diver.github.io/neurips2024/posters/abmiyi7ia7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/abmiyi7ia7/</guid><description>Hamiltonian Monte Carlo struggles with ReLU neural networks: high rejection rates hinder Bayesian deep learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/abmiyi7ia7/cover.png"/></item><item><title>HEPrune: Fast Private Training of Deep Neural Networks With Encrypted Data Pruning</title><link>https://deep-diver.github.io/neurips2024/posters/y2famldtif/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y2famldtif/</guid><description>HEPrune accelerates private deep learning training 16x by integrating encrypted data pruning, achieving this speedup with minimal accuracy loss.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y2famldtif/cover.png"/></item><item><title>HHD-GP: Incorporating Helmholtz-Hodge Decomposition into Gaussian Processes for Learning Dynamical Systems</title><link>https://deep-diver.github.io/neurips2024/posters/t9pfjvimij/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/t9pfjvimij/</guid><description>HHD-GP leverages Helmholtz-Hodge decomposition within Gaussian Processes to learn physically meaningful components of dynamical systems, enhancing prediction accuracy and interpretability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/t9pfjvimij/cover.png"/></item><item><title>Hierarchical Hybrid Sliced Wasserstein: A Scalable Metric for Heterogeneous Joint Distributions</title><link>https://deep-diver.github.io/neurips2024/posters/xwrmd1njqq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xwrmd1njqq/</guid><description>Hierarchical Hybrid Sliced Wasserstein (H2SW) solves the challenge of comparing complex, heterogeneous joint distributions by introducing novel slicing operators, leading to a scalable and statistical&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xwrmd1njqq/cover.png"/></item><item><title>HORSE: Hierarchical Representation for Large-Scale Neural Subset Selection</title><link>https://deep-diver.github.io/neurips2024/posters/donsoc7ry1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/donsoc7ry1/</guid><description>HORSE: A novel attention-based neural network significantly improves large-scale neural subset selection by up to 20%, addressing limitations in existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/donsoc7ry1/cover.png"/></item><item><title>How Sparse Can We Prune A Deep Network: A Fundamental Limit Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/iaapholhcx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/iaapholhcx/</guid><description>Deep network pruning&amp;rsquo;s fundamental limits are characterized, revealing how weight magnitude and network sharpness determine the maximum achievable sparsity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/iaapholhcx/cover.png"/></item><item><title>Hyper-opinion Evidential Deep Learning for Out-of-Distribution Detection</title><link>https://deep-diver.github.io/neurips2024/posters/te8vi2wgth/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/te8vi2wgth/</guid><description>Hyper-opinion Evidential Deep Learning (HEDL) enhances out-of-distribution detection by integrating sharp and vague evidence for superior uncertainty estimation and classification accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/te8vi2wgth/cover.png"/></item><item><title>HyperLogic: Enhancing Diversity and Accuracy in Rule Learning with HyperNets</title><link>https://deep-diver.github.io/neurips2024/posters/gjbzykgfd6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gjbzykgfd6/</guid><description>HyperLogic uses hypernetworks to generate diverse, accurate, and concise rule sets from neural networks, enhancing both interpretability and accuracy in rule learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gjbzykgfd6/cover.png"/></item><item><title>Implicitly Guided Design with PropEn: Match your Data to Follow the Gradient</title><link>https://deep-diver.github.io/neurips2024/posters/dhfho90ink/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dhfho90ink/</guid><description>PropEn: a novel framework for implicitly guided design optimization that leverages &amp;lsquo;matching&amp;rsquo; to boost efficiency by matching samples and approximating the gradient without a discriminator.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dhfho90ink/cover.png"/></item><item><title>Improved off-policy training of diffusion samplers</title><link>https://deep-diver.github.io/neurips2024/posters/vieiamy2gi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vieiamy2gi/</guid><description>Researchers enhanced diffusion samplers by developing a novel exploration strategy and a unified library, improving sample quality and addressing reproducibility challenges.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vieiamy2gi/cover.png"/></item><item><title>Improved Sample Complexity Bounds for Diffusion Model Training</title><link>https://deep-diver.github.io/neurips2024/posters/oxcqkyoy8q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oxcqkyoy8q/</guid><description>Training high-quality diffusion models efficiently is now possible, thanks to novel sample complexity bounds improving exponentially on previous work.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oxcqkyoy8q/cover.png"/></item><item><title>Improving Deep Learning Optimization through Constrained Parameter Regularization</title><link>https://deep-diver.github.io/neurips2024/posters/rcxtkihkbf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rcxtkihkbf/</guid><description>Constrained Parameter Regularization (CPR) outperforms traditional weight decay by dynamically adapting regularization strengths for individual parameters, leading to better deep learning model perfor&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rcxtkihkbf/cover.png"/></item><item><title>Improving Equivariant Model Training via Constraint Relaxation</title><link>https://deep-diver.github.io/neurips2024/posters/twkl7k1u5v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/twkl7k1u5v/</guid><description>Boost equivariant model training by strategically relaxing constraints during training, enhancing optimization and generalization!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/twkl7k1u5v/cover.png"/></item><item><title>Improving Generalization and Convergence by Enhancing Implicit Regularization</title><link>https://deep-diver.github.io/neurips2024/posters/cjm2bhloic/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cjm2bhloic/</guid><description>IRE framework expedites the discovery of flat minima in deep learning, enhancing generalization and convergence. By decoupling the dynamics of flat and sharp directions, IRE boosts sharpness reduction&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cjm2bhloic/cover.png"/></item><item><title>Inference of Neural Dynamics Using Switching Recurrent Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/zb8jlah2vn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zb8jlah2vn/</guid><description>SRNNs reveal behaviorally-relevant neural dynamics switches!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zb8jlah2vn/cover.png"/></item><item><title>Inferring stochastic low-rank recurrent neural networks from neural data</title><link>https://deep-diver.github.io/neurips2024/posters/c0ehyopptn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/c0ehyopptn/</guid><description>Researchers developed a method using variational sequential Monte Carlo to fit stochastic low-rank recurrent neural networks to neural data, enabling efficient analysis and generation of realistic neu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/c0ehyopptn/cover.png"/></item><item><title>Inflationary Flows: Calibrated Bayesian Inference with Diffusion-Based Models</title><link>https://deep-diver.github.io/neurips2024/posters/im4ltyrwde/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/im4ltyrwde/</guid><description>Calibrated Bayesian inference achieved via novel diffusion models uniquely mapping high-dimensional data to lower-dimensional Gaussian distributions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/im4ltyrwde/cover.png"/></item><item><title>Infusing Self-Consistency into Density Functional Theory Hamiltonian Prediction via Deep Equilibrium Models</title><link>https://deep-diver.github.io/neurips2024/posters/psvkinbs4u/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/psvkinbs4u/</guid><description>Deep Equilibrium Models (DEQs) infused into DFT Hamiltonian prediction achieves self-consistency, accelerating large-scale materials simulations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/psvkinbs4u/cover.png"/></item><item><title>Integrating GNN and Neural ODEs for Estimating Non-Reciprocal Two-Body Interactions in Mixed-Species Collective Motion</title><link>https://deep-diver.github.io/neurips2024/posters/qwl3eidi9r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qwl3eidi9r/</guid><description>Deep learning framework integrating GNNs and neural ODEs precisely estimates non-reciprocal two-body interactions in mixed-species collective motion, accurately replicating both individual and collect&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qwl3eidi9r/cover.png"/></item><item><title>Introducing Spectral Attention for Long-Range Dependency in Time Series Forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/dxynvebqmp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dxynvebqmp/</guid><description>Spectral Attention boosts long-range dependency capture in time series forecasting, achieving state-of-the-art results across various models and datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dxynvebqmp/cover.png"/></item><item><title>Inverse M-Kernels for Linear Universal Approximators of Non-Negative Functions</title><link>https://deep-diver.github.io/neurips2024/posters/hgss4ono4s/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hgss4ono4s/</guid><description>Unlocking efficient non-negative function approximation: This paper introduces inverse M-kernels, enabling flexible, linear universal approximators for one-dimensional inputs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hgss4ono4s/cover.png"/></item><item><title>Iteratively Refined Early Interaction Alignment for Subgraph Matching based Graph Retrieval</title><link>https://deep-diver.github.io/neurips2024/posters/udtwwf7tks/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/udtwwf7tks/</guid><description>IsoNet++ iteratively refines subgraph matching via early interaction GNNs and node-pair partner interactions, significantly boosting graph retrieval accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/udtwwf7tks/cover.png"/></item><item><title>Layer-Adaptive State Pruning for Deep State Space Models</title><link>https://deep-diver.github.io/neurips2024/posters/t9gbbwbnqg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/t9gbbwbnqg/</guid><description>Layer-Adaptive STate pruning (LAST) optimizes deep state space models by efficiently reducing state dimensions, improving performance and scalability without retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/t9gbbwbnqg/cover.png"/></item><item><title>Learning from higher-order correlations, efficiently: hypothesis tests, random features, and neural networks</title><link>https://deep-diver.github.io/neurips2024/posters/uhml6eyovf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uhml6eyovf/</guid><description>Neural networks learn efficiently from higher-order correlations, exceeding the capabilities of random features, as demonstrated through hypothesis tests and novel theoretical analysis in high-dimensi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uhml6eyovf/cover.png"/></item><item><title>Learning from Highly Sparse Spatio-temporal Data</title><link>https://deep-diver.github.io/neurips2024/posters/rtonicccjm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rtonicccjm/</guid><description>OPCR, a novel one-step spatio-temporal imputation method, surpasses existing iterative approaches by directly propagating limited observations to the global context, achieving superior accuracy and ef&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rtonicccjm/cover.png"/></item><item><title>Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate</title><link>https://deep-diver.github.io/neurips2024/posters/b1foes6cyq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/b1foes6cyq/</guid><description>Boost deep learning generalization with Learning from Teaching (LOT)! LOT trains auxiliary &amp;lsquo;student&amp;rsquo; models to imitate a primary &amp;rsquo;teacher&amp;rsquo; model, improving the teacher&amp;rsquo;s ability to capture generalizab&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/b1foes6cyq/cover.png"/></item><item><title>Learning Infinitesimal Generators of Continuous Symmetries from Data</title><link>https://deep-diver.github.io/neurips2024/posters/wl44w8xpc7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wl44w8xpc7/</guid><description>Learn continuous symmetries from data without pre-defined groups using Neural ODEs and a novel validity score to improve model generalization and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wl44w8xpc7/cover.png"/></item><item><title>Learning Macroscopic Dynamics from Partial Microscopic Observations</title><link>https://deep-diver.github.io/neurips2024/posters/cjh0qsgd0d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cjh0qsgd0d/</guid><description>Learn macroscopic dynamics efficiently using only partial microscopic force computations! This novel method leverages sparsity assumptions and stochastic estimation for accurate, cost-effective modeli&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cjh0qsgd0d/cover.png"/></item><item><title>Learning symmetries via weight-sharing with doubly stochastic tensors</title><link>https://deep-diver.github.io/neurips2024/posters/44wwow4gpf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/44wwow4gpf/</guid><description>Learn data symmetries directly from data with flexible weight-sharing using learnable doubly stochastic tensors!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/44wwow4gpf/cover.png"/></item><item><title>Leveraging Drift to Improve Sample Complexity of Variance Exploding Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/posters/euq0c4is7o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/euq0c4is7o/</guid><description>Drifted VESDE: Faster convergence, efficient sampling for variance-exploding diffusion models!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/euq0c4is7o/cover.png"/></item><item><title>LLM-AutoDA: Large Language Model-Driven Automatic Data Augmentation for Long-tailed Problems</title><link>https://deep-diver.github.io/neurips2024/posters/vpuouzovhp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vpuouzovhp/</guid><description>LLM-AutoDA: Automating data augmentation for long-tailed learning using large language models, significantly boosting model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vpuouzovhp/cover.png"/></item><item><title>LM-HT SNN: Enhancing the Performance of SNN to ANN Counterpart through Learnable Multi-hierarchical Threshold Model</title><link>https://deep-diver.github.io/neurips2024/posters/ilidnmvwmx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ilidnmvwmx/</guid><description>LM-HT SNN: A learnable multi-hierarchical threshold model dramatically improves SNN performance, achieving near-ANN accuracy through dynamic current regulation and seamless ANN-SNN conversion.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ilidnmvwmx/cover.png"/></item><item><title>Local Curvature Smoothing with Stein's Identity for Efficient Score Matching</title><link>https://deep-diver.github.io/neurips2024/posters/yppni7vc7n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yppni7vc7n/</guid><description>LCSS, a novel score-matching method, enables efficient and high-quality image generation in score-based diffusion models by using Stein&amp;rsquo;s identity to bypass the computationally expensive Jacobian trac&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yppni7vc7n/cover.png"/></item><item><title>Lorentz-Equivariant Geometric Algebra Transformers for High-Energy Physics</title><link>https://deep-diver.github.io/neurips2024/posters/x34gkv8syt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x34gkv8syt/</guid><description>Lorentz Geometric Algebra Transformer (L-GATr): A novel, scalable architecture for high-energy physics, achieving high-precision, data-efficient learning and outperforming existing methods on regressi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x34gkv8syt/cover.png"/></item><item><title>Markovian Flow Matching: Accelerating MCMC with Continuous Normalizing Flows</title><link>https://deep-diver.github.io/neurips2024/posters/amjyuvqsaf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/amjyuvqsaf/</guid><description>Adaptive MCMC with CNFs accelerates probabilistic inference by combining local and flow-informed transition kernels, achieving state-of-the-art results efficiently.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/amjyuvqsaf/cover.png"/></item><item><title>Mitigating Backdoor Attack by Injecting Proactive Defensive Backdoor</title><link>https://deep-diver.github.io/neurips2024/posters/cbkjbyikid/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cbkjbyikid/</guid><description>Proactive Defensive Backdoor (PDB) thwarts malicious backdoors by injecting a hidden defensive backdoor during training, suppressing attacks while maintaining model utility.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cbkjbyikid/cover.png"/></item><item><title>Mixture of Experts Meets Prompt-Based Continual Learning</title><link>https://deep-diver.github.io/neurips2024/posters/erwatqq4p8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/erwatqq4p8/</guid><description>Non-linear Residual Gates (NoRGa) boosts prompt-based continual learning by theoretically framing prefix tuning as adding new experts to a pre-trained Mixture-of-Experts model, achieving state-of-the-&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/erwatqq4p8/cover.png"/></item><item><title>Mixture of Link Predictors on Graphs</title><link>https://deep-diver.github.io/neurips2024/posters/x3oeoyjlmw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x3oeoyjlmw/</guid><description>Link-MoE boosts link prediction accuracy by strategically selecting the best model for each node pair, surpassing single-model approaches.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x3oeoyjlmw/cover.png"/></item><item><title>Model LEGO: Creating Models Like Disassembling and Assembling Building Blocks</title><link>https://deep-diver.github.io/neurips2024/posters/nxl7eazkbi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nxl7eazkbi/</guid><description>Model LEGO (MDA) revolutionizes deep learning by enabling the creation of new models by assembling and disassembling task-aware components from pre-trained models, eliminating the need for retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nxl7eazkbi/cover.png"/></item><item><title>Modeling Latent Neural Dynamics with Gaussian Process Switching Linear Dynamical Systems</title><link>https://deep-diver.github.io/neurips2024/posters/lx1lwp90kt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lx1lwp90kt/</guid><description>gpSLDS, a novel model, balances expressiveness and interpretability in modeling complex neural dynamics by combining Gaussian processes with switching linear dynamical systems, improving accuracy and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lx1lwp90kt/cover.png"/></item><item><title>Molecule Generation with Fragment Retrieval Augmentation</title><link>https://deep-diver.github.io/neurips2024/posters/56q0qggdlp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/56q0qggdlp/</guid><description>f-RAG: A novel fragment-based molecular generation framework boosts drug discovery by combining retrieval augmentation with a generative model, enabling exploration beyond existing fragments and signi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/56q0qggdlp/cover.png"/></item><item><title>Monomial Matrix Group Equivariant Neural Functional Networks</title><link>https://deep-diver.github.io/neurips2024/posters/rqyywgyuzk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rqyywgyuzk/</guid><description>Monomial-NFNs boost neural network efficiency by leveraging scaling/sign-flipping symmetries, resulting in fewer trainable parameters and competitive performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rqyywgyuzk/cover.png"/></item><item><title>MOTE-NAS: Multi-Objective Training-based Estimate for Efficient Neural Architecture Search</title><link>https://deep-diver.github.io/neurips2024/posters/jklykezfzv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jklykezfzv/</guid><description>MOTE-NAS: A new multi-objective training-based estimate drastically improves neural architecture search efficiency, achieving state-of-the-art accuracy with significantly reduced costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jklykezfzv/cover.png"/></item><item><title>Multi-Label Open Set Recognition</title><link>https://deep-diver.github.io/neurips2024/posters/k1vrxrs6wz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/k1vrxrs6wz/</guid><description>SLAN: A novel approach for multi-label open-set recognition, enriching sub-labeling info using structural data to identify unknown labels.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/k1vrxrs6wz/cover.png"/></item><item><title>Multi-model Ensemble Conformal Prediction in Dynamic Environments</title><link>https://deep-diver.github.io/neurips2024/posters/j1y70keorq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/j1y70keorq/</guid><description>Adaptive multi-model ensemble conformal prediction achieves strongly adaptive regret, yielding more efficient prediction sets in dynamic environments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/j1y70keorq/cover.png"/></item><item><title>Mutual Information Estimation via $f$-Divergence and Data Derangements</title><link>https://deep-diver.github.io/neurips2024/posters/pthi9hf9ut/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pthi9hf9ut/</guid><description>f-DIME: a novel class of discriminative mutual information estimators using f-divergence outperforms state-of-the-art methods by achieving an excellent bias-variance trade-off. This is achieved throug&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pthi9hf9ut/cover.png"/></item><item><title>Navigating Chemical Space with Latent Flows</title><link>https://deep-diver.github.io/neurips2024/posters/aaav4zbq9j/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/aaav4zbq9j/</guid><description>ChemFlow: a new framework efficiently explores chemical space using latent flows, unifying existing methods &amp;amp; incorporating physical priors for molecule manipulation and optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/aaav4zbq9j/cover.png"/></item><item><title>Neural Collapse Inspired Feature Alignment for Out-of-Distribution Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/wqpng9jnpk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wqpng9jnpk/</guid><description>Neural Collapse-inspired Feature Alignment (NCFAL) significantly boosts out-of-distribution generalization by aligning semantic features to a simplex ETF, even without environment labels.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wqpng9jnpk/cover.png"/></item><item><title>Neural Collapse To Multiple Centers For Imbalanced Data</title><link>https://deep-diver.github.io/neurips2024/posters/rjec9fz9ma/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rjec9fz9ma/</guid><description>Researchers enhance imbalanced data classification by inducing Neural Collapse to Multiple Centers (NCMC) using a novel cosine loss function, achieving performance comparable to state-of-the-art metho&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rjec9fz9ma/cover.png"/></item><item><title>Neural Conditional Probability for Uncertainty Quantification</title><link>https://deep-diver.github.io/neurips2024/posters/zxfhhjnmb2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zxfhhjnmb2/</guid><description>Neural Conditional Probability (NCP) offers a new operator-theoretic approach for efficiently learning conditional distributions, enabling streamlined inference and providing theoretical guarantees fo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zxfhhjnmb2/cover.png"/></item><item><title>Neural Embeddings Rank: Aligning 3D latent dynamics with movements</title><link>https://deep-diver.github.io/neurips2024/posters/hlcek7aygp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hlcek7aygp/</guid><description>Neural Embeddings Rank (NER) aligns 3D latent neural dynamics with movements, enabling cross-session decoding and revealing consistent neural dynamics across brain areas.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hlcek7aygp/cover.png"/></item><item><title>Neural Flow Diffusion Models: Learnable Forward Process for Improved Diffusion Modelling</title><link>https://deep-diver.github.io/neurips2024/posters/z0wibvtbxc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z0wibvtbxc/</guid><description>Neural Flow Diffusion Models (NFDM) revolutionize generative modeling by introducing a learnable forward process, resulting in state-of-the-art likelihoods and versatile generative dynamics.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/z0wibvtbxc/cover.png"/></item><item><title>Neural P$^3$M: A Long-Range Interaction Modeling Enhancer for Geometric GNNs</title><link>https://deep-diver.github.io/neurips2024/posters/ncqauwsyl5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ncqauwsyl5/</guid><description>Neural PÂ³M enhances geometric GNNs by incorporating mesh points to model long-range interactions in molecules, achieving state-of-the-art accuracy in predicting energy and forces.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ncqauwsyl5/cover.png"/></item><item><title>NeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes</title><link>https://deep-diver.github.io/neurips2024/posters/npoht6wv1f/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/npoht6wv1f/</guid><description>NeuralFuse: A novel add-on module learns input transformations to maintain accuracy in low-voltage DNN inference, achieving up to 57% accuracy recovery and 24% energy savings without retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/npoht6wv1f/cover.png"/></item><item><title>Nuclear Norm Regularization for Deep Learning</title><link>https://deep-diver.github.io/neurips2024/posters/eddhtvb5em/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/eddhtvb5em/</guid><description>This paper presents a novel, efficient method for Jacobian nuclear norm regularization in deep learning, replacing computationally expensive SVDs with equivalent Frobenius norm computations, thereby e&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/eddhtvb5em/cover.png"/></item><item><title>On conditional diffusion models for PDE simulations</title><link>https://deep-diver.github.io/neurips2024/posters/nql8ejymzh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nql8ejymzh/</guid><description>This paper introduces novel autoregressive sampling and hybrid training strategies for score-based diffusion models, significantly boosting PDE forecasting and assimilation accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nql8ejymzh/cover.png"/></item><item><title>On the Complexity of Learning Sparse Functions with Statistical and Gradient Queries</title><link>https://deep-diver.github.io/neurips2024/posters/q0kwoyzlso/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/q0kwoyzlso/</guid><description>Learning sparse functions efficiently with gradient methods is challenging; this paper introduces Differentiable Learning Queries (DLQ) to precisely characterize gradient query complexity, revealing s&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/q0kwoyzlso/cover.png"/></item><item><title>On the Limitations of Fractal Dimension as a Measure of Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/yo6gvpurkn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yo6gvpurkn/</guid><description>Fractal dimension, while showing promise, fails to consistently predict neural network generalization due to hyperparameter influence and adversarial initializations; prompting further research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yo6gvpurkn/cover.png"/></item><item><title>On the Scalability of Certified Adversarial Robustness with Generated Data</title><link>https://deep-diver.github.io/neurips2024/posters/tfag9uznpv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tfag9uznpv/</guid><description>Boosting certified robustness of machine learning models by 3-4% using generated data from diffusion models!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tfag9uznpv/cover.png"/></item><item><title>On the Scalability of GNNs for Molecular Graphs</title><link>https://deep-diver.github.io/neurips2024/posters/klqhrq7fvb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/klqhrq7fvb/</guid><description>Giant leap in molecular GNNs! MolGPS, a new foundation model, achieves state-of-the-art performance on molecular property prediction by leveraging massive datasets and demonstrating the scalability o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/klqhrq7fvb/cover.png"/></item><item><title>On the Target-kernel Alignment: a Unified Analysis with Kernel Complexity</title><link>https://deep-diver.github.io/neurips2024/posters/hkcx2wa3p0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hkcx2wa3p0/</guid><description>Truncated kernel methods consistently outperform standard methods by eliminating the saturation effect, offering faster learning rates and enhanced theoretical guarantees.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hkcx2wa3p0/cover.png"/></item><item><title>Out-Of-Distribution Detection with Diversification (Provably)</title><link>https://deep-diver.github.io/neurips2024/posters/c1hirbzeh9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/c1hirbzeh9/</guid><description>Boost OOD detection accuracy with diverseMix: a novel method enhancing auxiliary outlier diversity, provably improving generalization and achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/c1hirbzeh9/cover.png"/></item><item><title>PACE: Pacing Operator Learning to Accurate Optical Field Simulation for Complicated Photonic Devices</title><link>https://deep-diver.github.io/neurips2024/posters/uxjlgkwdci/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uxjlgkwdci/</guid><description>PACE, a novel neural operator, achieves unprecedented accuracy and speed in optical field simulation for complex photonic devices, surpassing existing methods by significantly reducing errors and boos&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uxjlgkwdci/cover.png"/></item><item><title>PageRank Bandits for Link Prediction</title><link>https://deep-diver.github.io/neurips2024/posters/vsz9na5jtl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vsz9na5jtl/</guid><description>PageRank Bandits (PRB) revolutionizes link prediction by framing it as a sequential decision-making problem, thus enabling the system to adapt to evolving data. Combining contextual bandits with PageR&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vsz9na5jtl/cover.png"/></item><item><title>pcaGAN: Improving Posterior-Sampling cGANs via Principal Component Regularization</title><link>https://deep-diver.github.io/neurips2024/posters/z0nq3hheeg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z0nq3hheeg/</guid><description>pcaGAN boosts posterior-sampling cGANs by using principal component regularization, achieving faster, more accurate results in various imaging tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/z0nq3hheeg/cover.png"/></item><item><title>PGN: The RNN's New Successor is Effective for Long-Range Time Series Forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/ypeamfku2o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ypeamfku2o/</guid><description>TPGN, a novel framework for long-range time series forecasting, uses Parallel Gated Networks (PGN) to efficiently capture long-term dependencies, achieving state-of-the-art results on multiple dataset&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ypeamfku2o/cover.png"/></item><item><title>Physics-Informed Regularization for Domain-Agnostic Dynamical System Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/iwlqbne8p7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/iwlqbne8p7/</guid><description>TREAT: a novel framework boosting dynamical systems modeling accuracy by enforcing Time-Reversal Symmetry (TRS) via a regularization term. High-precision modeling is achieved across diverse systems, &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/iwlqbne8p7/cover.png"/></item><item><title>Physics-Informed Variational State-Space Gaussian Processes</title><link>https://deep-diver.github.io/neurips2024/posters/tcf7s75xfa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tcf7s75xfa/</guid><description>PHYSS-GP: a novel physics-informed state-space Gaussian process model for efficient spatio-temporal data modeling, outperforming existing methods in predictive accuracy and computational speed.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tcf7s75xfa/cover.png"/></item><item><title>Post-Hoc Reversal: Are We Selecting Models Prematurely?</title><link>https://deep-diver.github.io/neurips2024/posters/3r7go6wkdm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3r7go6wkdm/</guid><description>Post-hoc model transformations can reverse performance trends, prompting a reevaluation of model selection strategies and suggesting a new &amp;lsquo;post-hoc selection&amp;rsquo; method for improved model development.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3r7go6wkdm/cover.png"/></item><item><title>Practical Shuffle Coding</title><link>https://deep-diver.github.io/neurips2024/posters/m2daxpcoii/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/m2daxpcoii/</guid><description>Revolutionizing unordered data compression, this paper introduces autoregressive shuffle coding, achieving state-of-the-art speeds and compression rates on massive datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/m2daxpcoii/cover.png"/></item><item><title>Predicting Ground State Properties: Constant Sample Complexity and Deep Learning Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/yblxvqjyqa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yblxvqjyqa/</guid><description>Deep learning algorithms now predict quantum ground state properties with constant sample complexity, regardless of system size, improving upon previous methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yblxvqjyqa/cover.png"/></item><item><title>Preferential Normalizing Flows</title><link>https://deep-diver.github.io/neurips2024/posters/srsjr9sdkr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/srsjr9sdkr/</guid><description>Eliciting high-dimensional probability distributions from experts using only preference comparisons is achieved via normalizing flows and a novel functional prior, resolving the problem of collapsing &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/srsjr9sdkr/cover.png"/></item><item><title>Pretraining with Random Noise for Fast and Robust Learning without Weight Transport</title><link>https://deep-diver.github.io/neurips2024/posters/dngfcvbonu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dngfcvbonu/</guid><description>Random noise pretraining dramatically speeds up and enhances neural network learning without weight transport, mimicking the brain&amp;rsquo;s developmental process and achieving performance comparable to backp&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dngfcvbonu/cover.png"/></item><item><title>Probabilistic Decomposed Linear Dynamical Systems for Robust Discovery of Latent Neural Dynamics</title><link>https://deep-diver.github.io/neurips2024/posters/xphsbybd73/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xphsbybd73/</guid><description>Probabilistic Decomposed Linear Dynamical Systems (p-dLDS) improve latent variable inference in nonlinear neural systems by using a probabilistic approach that&amp;rsquo;s robust to noise and includes a time-va&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xphsbybd73/cover.png"/></item><item><title>Probabilistic Graph Rewiring via Virtual Nodes</title><link>https://deep-diver.github.io/neurips2024/posters/lpvshl9lck/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lpvshl9lck/</guid><description>IPR-MPNNs revolutionize graph neural networks by implicitly rewiring graphs using virtual nodes, achieving state-of-the-art performance with significantly faster computation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lpvshl9lck/cover.png"/></item><item><title>ProSST: Protein Language Modeling with Quantized Structure and Disentangled Attention</title><link>https://deep-diver.github.io/neurips2024/posters/4z7rzixpjq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4z7rzixpjq/</guid><description>ProSST, a novel protein language model, integrates protein sequences and structures using quantized structure representation and disentangled attention, achieving state-of-the-art performance in zero-&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4z7rzixpjq/cover.png"/></item><item><title>Provable and Efficient Dataset Distillation for Kernel Ridge Regression</title><link>https://deep-diver.github.io/neurips2024/posters/wi2vpcbdnd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wi2vpcbdnd/</guid><description>One data point per class suffices for efficient and provable dataset distillation in kernel ridge regression, significantly reducing computational costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wi2vpcbdnd/cover.png"/></item><item><title>Provable Posterior Sampling with Denoising Oracles via Tilted Transport</title><link>https://deep-diver.github.io/neurips2024/posters/phlle8uoev/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/phlle8uoev/</guid><description>Boosting posterior sampling in challenging high-dimensional inverse problems, this paper introduces &amp;rsquo;tilted transport&amp;rsquo;, a novel technique leveraging denoising oracles for provably easier sampling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/phlle8uoev/cover.png"/></item><item><title>Provably Robust Score-Based Diffusion Posterior Sampling for Plug-and-Play Image Reconstruction</title><link>https://deep-diver.github.io/neurips2024/posters/slnsoay4u1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/slnsoay4u1/</guid><description>Provably robust diffusion posterior sampling for plug-and-play image reconstruction is achieved via a novel algorithmic framework, DPnP, offering both asymptotic and non-asymptotic performance guarant&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/slnsoay4u1/cover.png"/></item><item><title>PURE: Prompt Evolution with Graph ODE for Out-of-distribution Fluid Dynamics Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/z86knmjouq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z86knmjouq/</guid><description>PURE: A novel method uses Graph ODE to adapt spatio-temporal forecasting models to various fluid dynamics scenarios, improving model adaptation to unseen parameters and long-term predictions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/z86knmjouq/cover.png"/></item><item><title>PureGen: Universal Data Purification for Train-Time Poison Defense via Generative Model Dynamics</title><link>https://deep-diver.github.io/neurips2024/posters/zeihwoddvh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zeihwoddvh/</guid><description>PUREGEN uses generative model dynamics to purify poisoned training data, providing a universal, effective, and efficient train-time defense against various data poisoning attacks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zeihwoddvh/cover.png"/></item><item><title>Quantum Deep Equilibrium Models</title><link>https://deep-diver.github.io/neurips2024/posters/cwhwkb0q4k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cwhwkb0q4k/</guid><description>Quantum Deep Equilibrium Models (QDEQs) achieve higher QML performance with shallower circuits by using a DEQ training paradigm, improving near-term quantum computation efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cwhwkb0q4k/cover.png"/></item><item><title>QVAE-Mole: The Quantum VAE with Spherical Latent Variable Learning for 3-D Molecule Generation</title><link>https://deep-diver.github.io/neurips2024/posters/rqvesbxqdo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rqvesbxqdo/</guid><description>Quantum VAE with spherical latent variable learning enables efficient, one-shot 3D molecule generation, outperforming classic and other quantum methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rqvesbxqdo/cover.png"/></item><item><title>REDUCR: Robust Data Downsampling using Class Priority Reweighting</title><link>https://deep-diver.github.io/neurips2024/posters/jz7z7kkr94/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jz7z7kkr94/</guid><description>REDUCR, a novel data downsampling method, significantly improves worst-class test accuracy in imbalanced datasets by using class priority reweighting, surpassing state-of-the-art methods by ~15%.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jz7z7kkr94/cover.png"/></item><item><title>Regularized Adaptive Momentum Dual Averaging with an Efficient Inexact Subproblem Solver for Training Structured Neural Network</title><link>https://deep-diver.github.io/neurips2024/posters/xl7ve14aha/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xl7ve14aha/</guid><description>RAMDA: a new algorithm ensures efficient training of structured neural networks by achieving optimal structure and outstanding predictive performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xl7ve14aha/cover.png"/></item><item><title>Rejection via Learning Density Ratios</title><link>https://deep-diver.github.io/neurips2024/posters/jzciknnopj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jzciknnopj/</guid><description>This paper introduces a novel framework for classification with rejection by learning density ratios between data and idealized distributions, improving model robustness and accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jzciknnopj/cover.png"/></item><item><title>Reparameterized Multi-Resolution Convolutions for Long Sequence Modelling</title><link>https://deep-diver.github.io/neurips2024/posters/rwgnbipcpk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rwgnbipcpk/</guid><description>MRConv: Reparameterized multi-resolution convolutions efficiently model long sequences, improving performance across various data modalities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rwgnbipcpk/cover.png"/></item><item><title>Rethinking Deep Thinking: Stable Learning of Algorithms using Lipschitz Constraints</title><link>https://deep-diver.github.io/neurips2024/posters/zlgfrk2cqa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zlgfrk2cqa/</guid><description>Stable algorithm learning achieved by Deep Thinking networks with Lipschitz Constraints, ensuring convergence and better extrapolation to complex problems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zlgfrk2cqa/cover.png"/></item><item><title>Rethinking Fourier Transform from A Basis Functions Perspective for Long-term Time Series Forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/bafkbkr8ip/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bafkbkr8ip/</guid><description>Revolutionizing long-term time series forecasting, a new Fourier Basis Mapping method enhances accuracy by precisely interpreting frequency coefficients and considering time-frequency relationships, a&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bafkbkr8ip/cover.png"/></item><item><title>Retrieval &amp; Fine-Tuning for In-Context Tabular Models</title><link>https://deep-diver.github.io/neurips2024/posters/337dhoexcm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/337dhoexcm/</guid><description>LoCalPFN: boosting in-context tabular learning via retrieval &amp;amp; fine-tuning!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/337dhoexcm/cover.png"/></item><item><title>Revive Re-weighting in Imbalanced Learning by Density Ratio Estimation</title><link>https://deep-diver.github.io/neurips2024/posters/vx4ngdyyvg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vx4ngdyyvg/</guid><description>Revive Re-weighting in Imbalanced Learning by Density Ratio Estimation dynamically adjusts class weights during training using density ratio estimation, significantly improving model generalization, e&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vx4ngdyyvg/cover.png"/></item><item><title>RMLR: Extending Multinomial Logistic Regression into General Geometries</title><link>https://deep-diver.github.io/neurips2024/posters/lbp2cda7sp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lbp2cda7sp/</guid><description>RMLR: A novel framework extends multinomial logistic regression to diverse geometries, overcoming limitations of existing methods by requiring minimal geometric properties for broad applicability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lbp2cda7sp/cover.png"/></item><item><title>S2HPruner: Soft-to-Hard Distillation Bridges the Discretization Gap in Pruning</title><link>https://deep-diver.github.io/neurips2024/posters/mtyy3myyhz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mtyy3myyhz/</guid><description>S2HPruner bridges the discretization gap in neural network pruning via a novel soft-to-hard distillation framework, achieving superior performance across various benchmarks without fine-tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mtyy3myyhz/cover.png"/></item><item><title>SampDetox: Black-box Backdoor Defense via Perturbation-based Sample Detoxification</title><link>https://deep-diver.github.io/neurips2024/posters/y6rv6z98pk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y6rv6z98pk/</guid><description>SampDetox uses diffusion models to purify poisoned machine learning samples by strategically adding noise to eliminate backdoors without compromising data integrity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y6rv6z98pk/cover.png"/></item><item><title>SARAD: Spatial Association-Aware Anomaly Detection and Diagnosis for Multivariate Time Series</title><link>https://deep-diver.github.io/neurips2024/posters/gmf5aj01hz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gmf5aj01hz/</guid><description>SARAD: A novel anomaly detection approach for multivariate time series leverages spatial information and association reduction patterns to achieve state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gmf5aj01hz/cover.png"/></item><item><title>Scalable Optimization in the Modular Norm</title><link>https://deep-diver.github.io/neurips2024/posters/sfxajb7uxx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sfxajb7uxx/</guid><description>Deep learning optimization gets a major upgrade with Modula, a new method that uses the modular norm to normalize weight updates, enabling learning rate transfer across network widths and depths, thus&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sfxajb7uxx/cover.png"/></item><item><title>Scaling Law for Time Series Forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/cr2jehjb9q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cr2jehjb9q/</guid><description>Unlocking the potential of deep learning for time series forecasting: this study reveals a scaling law influenced by dataset size, model complexity, and the crucial look-back horizon, leading to impro&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cr2jehjb9q/cover.png"/></item><item><title>Scaling transformer neural networks for skillful and reliable medium-range weather forecasting</title><link>https://deep-diver.github.io/neurips2024/posters/abp01akha9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/abp01akha9/</guid><description>Stormer, a simple transformer model, achieves state-of-the-art medium-range weather forecasting accuracy by using weather-specific embedding, randomized dynamics forecasting, and a pressure-weighted l&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/abp01akha9/cover.png"/></item><item><title>Scanning Trojaned Models Using Out-of-Distribution Samples</title><link>https://deep-diver.github.io/neurips2024/posters/m296wjxyzq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/m296wjxyzq/</guid><description>TRODO: a novel trojan detection method using out-of-distribution samples, effectively identifies trojaned classifiers even against adversarial attacks and with limited data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/m296wjxyzq/cover.png"/></item><item><title>Score-Optimal Diffusion Schedules</title><link>https://deep-diver.github.io/neurips2024/posters/0rl5vwozru/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0rl5vwozru/</guid><description>Researchers developed a novel algorithm to automatically find optimal schedules for denoising diffusion models (DDMs), significantly improving sample quality and efficiency without manual parameter tu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0rl5vwozru/cover.png"/></item><item><title>Self-Refining Diffusion Samplers: Enabling Parallelization via Parareal Iterations</title><link>https://deep-diver.github.io/neurips2024/posters/xhwkhfwi3k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xhwkhfwi3k/</guid><description>Self-Refining Diffusion Samplers (SRDS) dramatically speeds up diffusion model sampling by leveraging Parareal iterations for parallel-in-time computation, maintaining high-quality outputs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xhwkhfwi3k/cover.png"/></item><item><title>Sequential Harmful Shift Detection Without Labels</title><link>https://deep-diver.github.io/neurips2024/posters/jps9kkusd3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jps9kkusd3/</guid><description>This paper introduces a novel, label-free method for detecting harmful distribution shifts in machine learning models deployed in production environments, leveraging a proxy error derived from an erro&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jps9kkusd3/cover.png"/></item><item><title>SequentialAttention++ for Block Sparsification: Differentiable Pruning Meets Combinatorial Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/brpzmoqisn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/brpzmoqisn/</guid><description>SequentialAttention++ unites differentiable pruning with combinatorial optimization for efficient and accurate neural network block sparsification, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/brpzmoqisn/cover.png"/></item><item><title>Sharpness-diversity tradeoff: improving flat ensembles with SharpBalance</title><link>https://deep-diver.github.io/neurips2024/posters/wjacsnt9ue/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wjacsnt9ue/</guid><description>SharpBalance, a novel training approach, effectively improves deep ensemble performance by addressing the sharpness-diversity trade-off, leading to significant improvements in both in-distribution and&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wjacsnt9ue/cover.png"/></item><item><title>Sigmoid Gating is More Sample Efficient than Softmax Gating in Mixture of Experts</title><link>https://deep-diver.github.io/neurips2024/posters/ig6kd5v4kd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ig6kd5v4kd/</guid><description>Sigmoid gating significantly boosts sample efficiency in Mixture of Experts models compared to softmax gating, offering faster convergence rates for various expert functions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ig6kd5v4kd/cover.png"/></item><item><title>Sketched Lanczos uncertainty score: a low-memory summary of the Fisher information</title><link>https://deep-diver.github.io/neurips2024/posters/1vpqomqsfo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1vpqomqsfo/</guid><description>SLU: a novel, low-memory uncertainty score for neural networks, achieves logarithmic memory scaling with model parameters, providing well-calibrated uncertainties and outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1vpqomqsfo/cover.png"/></item><item><title>Soft ascent-descent as a stable and flexible alternative to flooding</title><link>https://deep-diver.github.io/neurips2024/posters/y1zslondi2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y1zslondi2/</guid><description>Soft ascent-descent (SoftAD) improves test accuracy and generalization by softening the flooding method, offering competitive accuracy with reduced loss and model complexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y1zslondi2/cover.png"/></item><item><title>Spatio-Spectral Graph Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/cb3kcwybgw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cb3kcwybgw/</guid><description>Spatio-Spectral GNNs synergistically combine spatial and spectral graph filters for efficient, global information propagation, overcoming limitations of existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cb3kcwybgw/cover.png"/></item><item><title>Spiking Token Mixer: A event-driven friendly Former structure for spiking neural networks</title><link>https://deep-diver.github.io/neurips2024/posters/iycy7kaksy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/iycy7kaksy/</guid><description>STMixer: a novel SNN architecture enabling high performance on both synchronous and asynchronous neuromorphic hardware, achieving comparable results to spiking transformers with drastically lower powe&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/iycy7kaksy/cover.png"/></item><item><title>Stepping on the Edge: Curvature Aware Learning Rate Tuners</title><link>https://deep-diver.github.io/neurips2024/posters/sefllhihhj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sefllhihhj/</guid><description>Adaptive learning rate tuners often underperform; Curvature Dynamics Aware Tuning (CDAT) prioritizes long-term curvature stabilization, outperforming tuned constant learning rates.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sefllhihhj/cover.png"/></item><item><title>Stochastic Kernel Regularisation Improves Generalisation in Deep Kernel Machines</title><link>https://deep-diver.github.io/neurips2024/posters/prgxz9fybf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/prgxz9fybf/</guid><description>Deep kernel machines now achieve 94.5% accuracy on CIFAR-10, matching neural networks, by using stochastic kernel regularization to improve generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/prgxz9fybf/cover.png"/></item><item><title>Stochastic Optimal Control for Diffusion Bridges in Function Spaces</title><link>https://deep-diver.github.io/neurips2024/posters/wyqw4g57zd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wyqw4g57zd/</guid><description>Researchers extended stochastic optimal control theory to infinite-dimensional spaces, enabling the creation of diffusion bridges for generative modeling in function spaces, demonstrating applications&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wyqw4g57zd/cover.png"/></item><item><title>Structural Inference of Dynamical Systems with Conjoined State Space Models</title><link>https://deep-diver.github.io/neurips2024/posters/xqwjbek5rh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xqwjbek5rh/</guid><description>SICSM, a novel framework, integrates selective SSMs and GFNs to accurately infer complex dynamical system structures from irregularly sampled, partially observed trajectories.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xqwjbek5rh/cover.png"/></item><item><title>Super Consistency of Neural Network Landscapes and Learning Rate Transfer</title><link>https://deep-diver.github.io/neurips2024/posters/rgwhj7intz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rgwhj7intz/</guid><description>Neural network hyperparameter transferability across vastly different model sizes is achieved via a newly discovered property called &amp;lsquo;Super Consistency&amp;rsquo; of loss landscapes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rgwhj7intz/cover.png"/></item><item><title>Supra-Laplacian Encoding for Transformer on Dynamic Graphs</title><link>https://deep-diver.github.io/neurips2024/posters/vp9qazr2gw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vp9qazr2gw/</guid><description>SLATE: Supra-Laplacian encoding for spatio-temporal Transformers achieves state-of-the-art dynamic link prediction by innovatively using a multi-layer graph representation and a unique cross-attention&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vp9qazr2gw/cover.png"/></item><item><title>Symmetry-Informed Governing Equation Discovery</title><link>https://deep-diver.github.io/neurips2024/posters/aegsa8uoxf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/aegsa8uoxf/</guid><description>Leveraging symmetry in automated equation discovery improves accuracy and simplicity of learned governing equations, enhancing robustness against noise and achieving higher success rates across divers&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/aegsa8uoxf/cover.png"/></item><item><title>Take A Shortcut Back: Mitigating the Gradient Vanishing for Training Spiking Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/xjyu6zmzd7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xjyu6zmzd7/</guid><description>Shortcut back-propagation and an evolutionary training framework conquer gradient vanishing in spiking neural networks, drastically improving training and achieving state-of-the-art accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xjyu6zmzd7/cover.png"/></item><item><title>The Best of Both Worlds: On the Dilemma of Out-of-distribution Detection</title><link>https://deep-diver.github.io/neurips2024/posters/b9fppdnmyk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/b9fppdnmyk/</guid><description>Researchers found that superior OOD detection performance comes at the cost of reduced generalization. Their novel Decoupled Uncertainty Learning (DUL) algorithm harmonizes OOD detection and generali&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/b9fppdnmyk/cover.png"/></item><item><title>The Feature Speed Formula: a flexible approach to scale hyper-parameters of deep neural networks</title><link>https://deep-diver.github.io/neurips2024/posters/wshmb4j2o9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wshmb4j2o9/</guid><description>New &amp;lsquo;Feature Speed Formula&amp;rsquo; predicts &amp;amp; controls deep learning&amp;rsquo;s hierarchical feature learning by linking hyperparameter tuning to the angle between feature updates and backward pass.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wshmb4j2o9/cover.png"/></item><item><title>The Implicit Bias of Gradient Descent on Separable Multiclass Data</title><link>https://deep-diver.github.io/neurips2024/posters/jlwn80mtji/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jlwn80mtji/</guid><description>Researchers extended implicit bias theory to multiclass classification using a novel framework, proving that gradient descent prefers simple solutions even with complex alternatives.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jlwn80mtji/cover.png"/></item><item><title>The Importance of Being Scalable: Improving the Speed and Accuracy of Neural Network Interatomic Potentials Across Chemical Domains</title><link>https://deep-diver.github.io/neurips2024/posters/y4mbazu4vy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y4mbazu4vy/</guid><description>ESCAIP, a novel neural network architecture, dramatically boosts the speed and accuracy of atomic simulations by leveraging attention mechanisms, enabling efficient large-scale modeling across diverse&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y4mbazu4vy/cover.png"/></item><item><title>The Iterative Optimal Brain Surgeon: Faster Sparse Recovery by Leveraging Second-Order Information</title><link>https://deep-diver.github.io/neurips2024/posters/snxwd0q4ei/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/snxwd0q4ei/</guid><description>I-OBS, a novel family of sparse recovery algorithms leveraging second-order information, achieves faster convergence rates for sparse DNNs, validated by large-scale experiments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/snxwd0q4ei/cover.png"/></item><item><title>The Poisson Midpoint Method for Langevin Dynamics: Provably Efficient Discretization for Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/posters/ylvviju6md/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ylvviju6md/</guid><description>Poisson Midpoint Method quadratically accelerates Langevin Monte Carlo for diffusion models, achieving high-quality image generation with significantly fewer computations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ylvviju6md/cover.png"/></item><item><title>The Prevalence of Neural Collapse in Neural Multivariate Regression</title><link>https://deep-diver.github.io/neurips2024/posters/wq6ay6fc2h/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wq6ay6fc2h/</guid><description>Neural networks exhibit &amp;lsquo;Neural Regression Collapse&amp;rsquo; (NRC) during training, where feature vectors collapse to subspaces spanned by principal components of features and weights, and the weight vector G&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wq6ay6fc2h/cover.png"/></item><item><title>The Selective $G$-Bispectrum and its Inversion: Applications to $G$-Invariant Networks</title><link>https://deep-diver.github.io/neurips2024/posters/lptwdyiy4o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lptwdyiy4o/</guid><description>This paper introduces a selective G-Bispectrum algorithm, slashing the computational complexity from O(|G|^2) to O(|G|), making G-invariant deep learning faster and more scalable.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lptwdyiy4o/cover.png"/></item><item><title>TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables</title><link>https://deep-diver.github.io/neurips2024/posters/inaeuq04lt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/inaeuq04lt/</guid><description>TimeXer empowers transformers for superior time series forecasting by cleverly integrating exogenous variables, achieving state-of-the-art results on diverse benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/inaeuq04lt/cover.png"/></item><item><title>TinyTTA: Efficient Test-time Adaptation via Early-exit Ensembles on Edge Devices</title><link>https://deep-diver.github.io/neurips2024/posters/xicbcbe6c3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xicbcbe6c3/</guid><description>TinyTTA enables efficient test-time adaptation on memory-constrained edge devices using a novel self-ensemble and early-exit strategy, improving accuracy and reducing memory usage.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xicbcbe6c3/cover.png"/></item><item><title>Towards Dynamic Message Passing on Graphs</title><link>https://deep-diver.github.io/neurips2024/posters/4bwlujf0e9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4bwlujf0e9/</guid><description>N2: A novel dynamic message-passing GNN tackles message-passing bottlenecks and high computational costs by introducing learnable pseudo-nodes and dynamic pathways in a common state space, achieving s&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4bwlujf0e9/cover.png"/></item><item><title>Towards Exact Gradient-based Training on Analog In-memory Computing</title><link>https://deep-diver.github.io/neurips2024/posters/5gwbklbiif/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/5gwbklbiif/</guid><description>Analog in-memory computing (AIMC) training suffers from asymptotic errors due to asymmetric updates. This paper rigorously proves this limitation, proposes a novel discrete-time model to characterize &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/5gwbklbiif/cover.png"/></item><item><title>Treeffuser: probabilistic prediction via conditional diffusions with gradient-boosted trees</title><link>https://deep-diver.github.io/neurips2024/posters/4kesvavnmr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4kesvavnmr/</guid><description>Treeffuser: Accurate probabilistic predictions from tabular data using conditional diffusion models and gradient-boosted trees!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4kesvavnmr/cover.png"/></item><item><title>UGC: Universal Graph Coarsening</title><link>https://deep-diver.github.io/neurips2024/posters/nn6nsd1qds/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nn6nsd1qds/</guid><description>UGC: Blazing-fast graph coarsening for big data, preserving key insights across diverse graph types.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nn6nsd1qds/cover.png"/></item><item><title>Understanding Generalizability of Diffusion Models Requires Rethinking the Hidden Gaussian Structure</title><link>https://deep-diver.github.io/neurips2024/posters/sk2dubgvrk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sk2dubgvrk/</guid><description>Diffusion models&amp;rsquo; surprising generalizability stems from an inductive bias towards learning Gaussian data structures, a finding that reshapes our understanding of their training and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sk2dubgvrk/cover.png"/></item><item><title>Understanding Representation of Deep Equilibrium Models from Neural Collapse Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/obuxeummq1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/obuxeummq1/</guid><description>Deep Equilibrium Models excel on imbalanced data due to feature convergence and self-duality properties, unlike explicit models, as shown through Neural Collapse analysis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/obuxeummq1/cover.png"/></item><item><title>Understanding the Expressivity and Trainability of Fourier Neural Operator: A Mean-Field Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/qjr02btm7j/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qjr02btm7j/</guid><description>A mean-field theory explains Fourier Neural Operator (FNO) behavior, linking expressivity to trainability by identifying ordered and chaotic phases that correspond to vanishing or exploding gradients,&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qjr02btm7j/cover.png"/></item><item><title>Unifying Homophily and Heterophily for Spectral Graph Neural Networks via Triple Filter Ensembles</title><link>https://deep-diver.github.io/neurips2024/posters/uatpopwzzu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uatpopwzzu/</guid><description>TFE-GNN: A novel spectral GNN using triple filter ensembles for superior homophily/heterophily handling and improved generalization on real-world graphs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uatpopwzzu/cover.png"/></item><item><title>United We Stand, Divided We Fall: Fingerprinting Deep Neural Networks via Adversarial Trajectories</title><link>https://deep-diver.github.io/neurips2024/posters/ywpl0bvxts/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ywpl0bvxts/</guid><description>ADV-TRA uses adversarial trajectories to robustly fingerprint deep neural networks, outperforming state-of-the-art methods against various removal attacks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ywpl0bvxts/cover.png"/></item><item><title>UniTS: A Unified Multi-Task Time Series Model</title><link>https://deep-diver.github.io/neurips2024/posters/nbodybptww/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nbodybptww/</guid><description>UniTS: one model to rule them all! This unified multi-task time series model excels in forecasting, classification, anomaly detection, and imputation, outperforming specialized models across 38 divers&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nbodybptww/cover.png"/></item><item><title>Universal Neural Functionals</title><link>https://deep-diver.github.io/neurips2024/posters/w89fkkp2ao/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w89fkkp2ao/</guid><description>Universal Neural Functionals (UNFs) automatically construct permutation-equivariant models for any weight space, improving learned optimizer performance and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w89fkkp2ao/cover.png"/></item><item><title>Universal Physics Transformers: A Framework For Efficiently Scaling Neural Operators</title><link>https://deep-diver.github.io/neurips2024/posters/ouxinx5krm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ouxinx5krm/</guid><description>Universal Physics Transformers (UPTs) offer a unified, scalable framework for efficiently training neural operators across diverse spatio-temporal physics problems, overcoming limitations of existing &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ouxinx5krm/cover.png"/></item><item><title>Unveiling The Matthew Effect Across Channels: Assessing Layer Width Sufficiency via Weight Norm Variance</title><link>https://deep-diver.github.io/neurips2024/posters/tcft2v63vd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tcft2v63vd/</guid><description>Neural network efficiency is improved by analyzing weight norm variance across channels to identify optimal layer widths, resulting in reduced parameters and boosted performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tcft2v63vd/cover.png"/></item><item><title>Utilizing Image Transforms and Diffusion Models for Generative Modeling of Short and Long Time Series</title><link>https://deep-diver.github.io/neurips2024/posters/2nfbbpbn9x/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2nfbbpbn9x/</guid><description>ImagenTime transforms time series into images, leveraging advanced diffusion models for superior generative modeling of both short and long sequences.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2nfbbpbn9x/cover.png"/></item><item><title>WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks Against Deep Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/j6nbyzllnj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/j6nbyzllnj/</guid><description>WaveAttack: A new backdoor attack method leveraging asymmetric frequency obfuscation for high stealthiness and effectiveness in Deep Neural Networks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/j6nbyzllnj/cover.png"/></item><item><title>Weight decay induces low-rank attention layers</title><link>https://deep-diver.github.io/neurips2024/posters/odeqjim9sk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/odeqjim9sk/</guid><description>Weight decay in deep learning surprisingly induces low-rank attention layers, potentially harming performance but offering optimization strategies for large language models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/odeqjim9sk/cover.png"/></item><item><title>Weight Diffusion for Future: Learn to Generalize in Non-Stationary Environments</title><link>https://deep-diver.github.io/neurips2024/posters/2cfuynnl1m/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2cfuynnl1m/</guid><description>Weight Diffusion (W-Diff) masters evolving domain generalization by using conditional diffusion models to learn classifier weight evolution patterns, enabling superior generalization to unseen future &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2cfuynnl1m/cover.png"/></item><item><title>Weight for Robustness: A Comprehensive Approach towards Optimal Fault-Tolerant Asynchronous ML</title><link>https://deep-diver.github.io/neurips2024/posters/v1kpc060ac/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v1kpc060ac/</guid><description>Optimal fault-tolerant asynchronous machine learning is achieved via a novel weighted robust aggregation framework, ensuring efficient training despite Byzantine failures and heterogeneous resources.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v1kpc060ac/cover.png"/></item><item><title>What If the Input is Expanded in OOD Detection?</title><link>https://deep-diver.github.io/neurips2024/posters/xfpifrnuas/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xfpifrnuas/</guid><description>Boost OOD detection accuracy by averaging model confidence scores from original and corrupted inputs!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xfpifrnuas/cover.png"/></item><item><title>What is my quantum computer good for? Quantum capability learning with physics-aware neural networks</title><link>https://deep-diver.github.io/neurips2024/posters/4cu9zvokbz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4cu9zvokbz/</guid><description>Quantum-physics-aware neural networks achieve up to 50% improved accuracy in predicting quantum computer capabilities, scaling to 100+ qubits.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4cu9zvokbz/cover.png"/></item><item><title>What Matters in Graph Class Incremental Learning? An Information Preservation Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/tjgx7tpgo8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tjgx7tpgo8/</guid><description>GSIP framework mitigates catastrophic forgetting in graph class incremental learning by preserving crucial graph information, achieving a 10% improvement in forgetting metrics.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tjgx7tpgo8/cover.png"/></item></channel></rss>