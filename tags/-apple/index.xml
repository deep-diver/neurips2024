<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ Apple on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-apple/</link><description>Recent content in üè¢ Apple on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-apple/index.xml" rel="self" type="application/rss+xml"/><item><title>Aggregate-and-Adapt Natural Language Prompts for Downstream Generalization of CLIP</title><link>https://deep-diver.github.io/neurips2024/posters/yz3wbkok0k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yz3wbkok0k/</guid><description>Aggregate-and-Adapt Prompt Embedding (AAPE) boosts CLIP&amp;rsquo;s downstream generalization by distilling textual knowledge from natural language prompts, achieving competitive performance across various visi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yz3wbkok0k/cover.png"/></item><item><title>Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum</title><link>https://deep-diver.github.io/neurips2024/posters/r8m9sfymdi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/r8m9sfymdi/</guid><description>This paper introduces dataset decomposition (DD), a novel approach to accelerate LLM training while enhancing performance. DD significantly reduces training time by decomposing datasets into buckets &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/r8m9sfymdi/cover.png"/></item><item><title>Instance-Optimal Private Density Estimation in the Wasserstein Distance</title><link>https://deep-diver.github.io/neurips2024/posters/apq6corvfz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/apq6corvfz/</guid><description>Instance-optimal private density estimation algorithms, adapting to data characteristics for improved accuracy in the Wasserstein distance, are introduced.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/apq6corvfz/cover.png"/></item><item><title>Kaleido Diffusion: Improving Conditional Diffusion Models with Autoregressive Latent Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/qzswlclmcs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qzswlclmcs/</guid><description>Kaleido Diffusion boosts the diversity of images generated by diffusion models without sacrificing quality, using autoregressive latent modeling to add more control and interpretability to the image g&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qzswlclmcs/cover.png"/></item><item><title>Learning Elastic Costs to Shape Monge Displacements</title><link>https://deep-diver.github.io/neurips2024/posters/aauvnpqvbz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/aauvnpqvbz/</guid><description>Learn optimal transport maps with structured displacements using elastic costs and a novel bilevel loss function!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/aauvnpqvbz/cover.png"/></item><item><title>Private Stochastic Convex Optimization with Heavy Tails: Near-Optimality from Simple Reductions</title><link>https://deep-diver.github.io/neurips2024/posters/ox6ail9f0y/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ox6ail9f0y/</guid><description>Achieving near-optimal rates for differentially private stochastic convex optimization with heavy-tailed gradients is possible using simple reduction-based techniques.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ox6ail9f0y/cover.png"/></item></channel></rss>