<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ UC Santa Cruz on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-uc-santa-cruz/</link><description>Recent content in üè¢ UC Santa Cruz on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-uc-santa-cruz/index.xml" rel="self" type="application/rss+xml"/><item><title>Fairness without Harm: An Influence-Guided Active Sampling Approach</title><link>https://deep-diver.github.io/neurips2024/posters/yyjojvbccd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yyjojvbccd/</guid><description>FairnessWithoutHarm achieves fairer ML models without sacrificing accuracy by using an influence-guided active sampling method that doesn&amp;rsquo;t require sensitive training data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yyjojvbccd/cover.png"/></item><item><title>Large Language Model Unlearning via Embedding-Corrupted Prompts</title><link>https://deep-diver.github.io/neurips2024/posters/e5icsxbd8q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/e5icsxbd8q/</guid><description>ECO prompts enable efficient LLM unlearning by corrupting prompts flagged for forgetting, achieving promising results across various LLMs and tasks with minimal side effects.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/e5icsxbd8q/cover.png"/></item><item><title>Scaling White-Box Transformers for Vision</title><link>https://deep-diver.github.io/neurips2024/posters/wkwgedn19x/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wkwgedn19x/</guid><description>CRATE-a: A new white-box vision transformer architecture achieves 85.1% ImageNet accuracy by strategically scaling model size and datasets, outperforming prior white-box models and preserving interpre&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wkwgedn19x/cover.png"/></item></channel></rss>