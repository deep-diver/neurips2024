<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ Zhejiang University on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-zhejiang-university/</link><description>Recent content in üè¢ Zhejiang University on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-zhejiang-university/index.xml" rel="self" type="application/rss+xml"/><item><title>Context and Geometry Aware Voxel Transformer for Semantic Scene Completion</title><link>https://deep-diver.github.io/neurips2024/spotlight/9bu627mtfs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight/9bu627mtfs/</guid><description>CGFormer: a novel voxel transformer boosting semantic scene completion accuracy by using context-aware queries and 3D deformable attention, outperforming existing methods on SemanticKITTI and SSCBench&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight/9bu627mtfs/cover.png"/></item><item><title>Enhancing LLM Reasoning via Vision-Augmented Prompting</title><link>https://deep-diver.github.io/neurips2024/spotlight/ngugvt7ar2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight/ngugvt7ar2/</guid><description>Vision-Augmented Prompting (VAP) boosts LLM reasoning by automatically generating images from textual problem descriptions, incorporating visual-spatial clues to significantly improve accuracy across &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight/ngugvt7ar2/cover.png"/></item><item><title>MKGL: Mastery of a Three-Word Language</title><link>https://deep-diver.github.io/neurips2024/spotlight/eqmnwxvoqn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight/eqmnwxvoqn/</guid><description>Researchers taught a large language model (LLM) a three-word &amp;lsquo;Knowledge Graph Language&amp;rsquo; (KGL) to improve knowledge graph (KG) completion, drastically reducing errors compared to other methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight/eqmnwxvoqn/cover.png"/></item><item><title>TOPA: Extending Large Language Models for Video Understanding via Text-Only Pre-Alignment</title><link>https://deep-diver.github.io/neurips2024/spotlight/5nmbqpy7bn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight/5nmbqpy7bn/</guid><description>TOPA: Extending LLMs for video understanding using only text data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight/5nmbqpy7bn/cover.png"/></item><item><title>Towards Unified Multimodal Editing with Enhanced Knowledge Collaboration</title><link>https://deep-diver.github.io/neurips2024/spotlight/kf80zs3fvy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight/kf80zs3fvy/</guid><description>UniKE: A unified multimodal editing method achieves superior reliability, generality, and locality by disentangling knowledge into semantic and truthfulness spaces, enabling enhanced collaboration bet&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight/kf80zs3fvy/cover.png"/></item></channel></rss>