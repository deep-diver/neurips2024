<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ EPFL on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-epfl/</link><description>Recent content in üè¢ EPFL on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-epfl/index.xml" rel="self" type="application/rss+xml"/><item><title>CoBo: Collaborative Learning via Bilevel Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/sjq1iiqpfu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sjq1iiqpfu/</guid><description>CoBo: A novel bilevel optimization algorithm for collaborative learning surpasses existing methods by efficiently selecting helpful clients, resulting in superior performance and scalability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sjq1iiqpfu/cover.png"/></item><item><title>DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging</title><link>https://deep-diver.github.io/neurips2024/posters/kmnoh7cxrq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kmnoh7cxrq/</guid><description>DenseFormer enhances transformers by adding a depth-weighted averaging step, improving data efficiency and outperforming baselines in memory and inference time without increasing model size.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kmnoh7cxrq/cover.png"/></item><item><title>Fine-Tuning Personalization in Federated Learning to Mitigate Adversarial Clients</title><link>https://deep-diver.github.io/neurips2024/posters/wblplszji5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wblplszji5/</guid><description>Fine-tune personalization in federated learning to beat adversarial clients; collaboration level depends on data heterogeneity and adversary fraction.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wblplszji5/cover.png"/></item><item><title>Generative Modelling of Structurally Constrained Graphs</title><link>https://deep-diver.github.io/neurips2024/posters/a3hxp0eenw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/a3hxp0eenw/</guid><description>ConStruct: Generating realistic graphs with guaranteed structural properties via constrained diffusion.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/a3hxp0eenw/cover.png"/></item><item><title>Graph Edit Distance with General Costs Using Neural Set Divergence</title><link>https://deep-diver.github.io/neurips2024/posters/u7jrmrgutt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/u7jrmrgutt/</guid><description>GRAPHEDX, a novel neural network, accurately estimates graph edit distance with varying operation costs, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/u7jrmrgutt/cover.png"/></item><item><title>Implicit Bias of Mirror Flow on Separable Data</title><link>https://deep-diver.github.io/neurips2024/posters/wimaws0fwb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wimaws0fwb/</guid><description>Mirror descent&amp;rsquo;s implicit bias on separable data is formally characterized, revealing convergence towards a maximum margin classifier determined by the potential&amp;rsquo;s &amp;lsquo;horizon function&amp;rsquo;.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wimaws0fwb/cover.png"/></item><item><title>Local to Global: Learning Dynamics and Effect of Initialization for Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/ox4yll3x53/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ox4yll3x53/</guid><description>Transformers&amp;rsquo; learning dynamics depend heavily on initialization and Markovian data properties, leading to either global or local minima; this paper proves this, offers initialization guidelines, and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ox4yll3x53/cover.png"/></item><item><title>Revisiting Ensembling in One-Shot Federated Learning</title><link>https://deep-diver.github.io/neurips2024/posters/7rwts2wuyx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7rwts2wuyx/</guid><description>FENS: a novel federated ensembling scheme that boosts one-shot federated learning accuracy to near iterative FL levels, while maintaining low communication costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7rwts2wuyx/cover.png"/></item><item><title>SAMPa: Sharpness-aware Minimization Parallelized</title><link>https://deep-diver.github.io/neurips2024/posters/ign0ktydwv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ign0ktydwv/</guid><description>SAMPa: Parallelizing gradient computations in Sharpness-Aware Minimization (SAM) achieves a 2x speedup and superior generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ign0ktydwv/cover.png"/></item><item><title>Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations</title><link>https://deep-diver.github.io/neurips2024/spotlight-large-language-models/y13gsftjgr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-large-language-models/y13gsftjgr/</guid><description>Revolutionizing LLM training: Constant learning rate with cooldown replaces cosine schedule, enabling cost-effective scaling experiments!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-large-language-models/y13gsftjgr/cover.png"/></item><item><title>SGD vs GD: Rank Deficiency in Linear Networks</title><link>https://deep-diver.github.io/neurips2024/posters/tsaieshx3j/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tsaieshx3j/</guid><description>SGD surprisingly diminishes network rank, unlike GD, due to a repulsive force between eigenvalues, offering insights into deep learning generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tsaieshx3j/cover.png"/></item><item><title>SuperDeepFool: a new fast and accurate minimal adversarial attack</title><link>https://deep-diver.github.io/neurips2024/posters/pqd7ckr8af/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pqd7ckr8af/</guid><description>SuperDeepFool: a fast, accurate algorithm generating minimal adversarial perturbations, significantly improving deep learning model robustness evaluation and adversarial training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pqd7ckr8af/cover.png"/></item><item><title>Why Do We Need Weight Decay in Modern Deep Learning?</title><link>https://deep-diver.github.io/neurips2024/posters/yraxxsckm2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yraxxsckm2/</guid><description>Weight decay&amp;rsquo;s role in modern deep learning is surprisingly multifaceted, impacting optimization dynamics rather than solely regularization, improving generalization and training stability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yraxxsckm2/cover.png"/></item><item><title>Why the Metric Backbone Preserves Community Structure</title><link>https://deep-diver.github.io/neurips2024/posters/kx8i0rp7w2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kx8i0rp7w2/</guid><description>Metric backbone graph sparsification surprisingly preserves community structure, offering an efficient and robust method for analyzing large networks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kx8i0rp7w2/cover.png"/></item></channel></rss>