<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI Theory on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/ai-theory/</link><description>Recent content in AI Theory on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/ai-theory/index.xml" rel="self" type="application/rss+xml"/><item><title>4+3 Phases of Compute-Optimal Neural Scaling Laws</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/avsxwicpak/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/avsxwicpak/</guid><description>Researchers discovered four distinct compute-optimal phases for training neural networks, offering new predictions for resource-efficient large model training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/avsxwicpak/cover.png"/></item><item><title>A generalized neural tangent kernel for surrogate gradient learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/kfdexqu6mc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/kfdexqu6mc/</guid><description>Researchers introduce a generalized neural tangent kernel for analyzing surrogate gradient learning in neural networks with non-differentiable activation functions, providing a strong theoretical foun&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/kfdexqu6mc/cover.png"/></item><item><title>A Neural Network Approach for Efficiently Answering Most Probable Explanation Queries in Probabilistic Models</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ufppf9ghzp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ufppf9ghzp/</guid><description>A novel neural network efficiently answers arbitrary Most Probable Explanation (MPE) queries in large probabilistic models, eliminating the need for slow inference algorithms.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ufppf9ghzp/cover.png"/></item><item><title>A Non-parametric Direct Learning Approach to Heterogeneous Treatment Effect Estimation under Unmeasured Confounding</title><link>https://deep-diver.github.io/neurips2024/posters/bwluqsqumh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bwluqsqumh/</guid><description>Estimating heterogeneous treatment effects (CATE) under unmeasured confounding is revolutionized by a novel non-parametric direct learning approach using instrumental variables, offering efficient and&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bwluqsqumh/cover.png"/></item><item><title>A Primal-Dual-Assisted Penalty Approach to Bilevel Optimization with Coupled Constraints</title><link>https://deep-diver.github.io/neurips2024/posters/uzi7h5ac0x/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uzi7h5ac0x/</guid><description>BLOCC, a novel first-order algorithm, efficiently solves bilevel optimization problems with coupled constraints, offering improved scalability and convergence for machine learning applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uzi7h5ac0x/cover.png"/></item><item><title>A provable control of sensitivity of neural networks through a direct parameterization of the overall bi-Lipschitzness</title><link>https://deep-diver.github.io/neurips2024/posters/ww62xltefb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ww62xltefb/</guid><description>New framework directly controls neural network sensitivity by precisely parameterizing overall bi-Lipschitzness, offering improved robustness and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ww62xltefb/cover.png"/></item><item><title>A Separation in Heavy-Tailed Sampling: Gaussian vs. Stable Oracles for Proximal Samplers</title><link>https://deep-diver.github.io/neurips2024/posters/zuwlghgxtq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zuwlghgxtq/</guid><description>Stable oracles outperform Gaussian oracles in high-accuracy heavy-tailed sampling, overcoming limitations of Gaussian-based proximal samplers.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zuwlghgxtq/cover.png"/></item><item><title>A Simple and Optimal Approach for Universal Online Learning with Gradient Variations</title><link>https://deep-diver.github.io/neurips2024/posters/yo5dvychzr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yo5dvychzr/</guid><description>A novel universal online learning algorithm achieves optimal gradient-variation regret across diverse function curvatures, boasting efficiency with only one gradient query per round.</description></item><item><title>A Walsh Hadamard Derived Linear Vector Symbolic Architecture</title><link>https://deep-diver.github.io/neurips2024/posters/p3hnrpewme/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/p3hnrpewme/</guid><description>Hadamard-derived Linear Binding (HLB): A novel, efficient vector symbolic architecture surpassing existing methods in classical AI tasks and deep learning applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/p3hnrpewme/cover.png"/></item><item><title>Accelerating ERM for data-driven algorithm design using output-sensitive techniques</title><link>https://deep-diver.github.io/neurips2024/posters/yw3tlswusb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yw3tlswusb/</guid><description>Accelerating ERM for data-driven algorithm design using output-sensitive techniques achieves computationally efficient learning by scaling with the actual number of pieces in the dual loss function, n&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yw3tlswusb/cover.png"/></item><item><title>Accelerating Nash Equilibrium Convergence in Monte Carlo Settings Through Counterfactual Value Based Fictitious Play</title><link>https://deep-diver.github.io/neurips2024/posters/efd9n5zdfc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/efd9n5zdfc/</guid><description>MCCFVFP, a novel Monte Carlo-based algorithm, accelerates Nash equilibrium convergence in large-scale games by combining CFR&amp;rsquo;s counterfactual value calculations with fictitious play&amp;rsquo;s best response st&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/efd9n5zdfc/cover.png"/></item><item><title>Achieving Domain-Independent Certified Robustness via Knowledge Continuity</title><link>https://deep-diver.github.io/neurips2024/posters/v07krlyxdx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v07krlyxdx/</guid><description>Certifying neural network robustness across diverse domains, this paper introduces knowledge continuity—a novel framework ensuring model stability independent of input type, norms, and distribution.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v07krlyxdx/cover.png"/></item><item><title>Adaptive Proximal Gradient Method for Convex Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/qlh21ig1ic/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qlh21ig1ic/</guid><description>Adaptive gradient descent methods are improved by leveraging local curvature information for entirely adaptive algorithms without added computational cost, proving convergence with only local Lipschit&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qlh21ig1ic/cover.png"/></item><item><title>Almost-Linear RNNs Yield Highly Interpretable Symbolic Codes in Dynamical Systems Reconstruction</title><link>https://deep-diver.github.io/neurips2024/posters/sepsxteekj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sepsxteekj/</guid><description>Almost-linear RNNs (AL-RNNs) offer highly interpretable symbolic codes for dynamical systems reconstruction, simplifying the analysis of complex systems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sepsxteekj/cover.png"/></item><item><title>Approximating mutual information of high-dimensional variables using learned representations</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/hn05dqxyll/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/hn05dqxyll/</guid><description>Latent Mutual Information (LMI) approximation accurately estimates mutual information in high-dimensional data using low-dimensional learned representations, solving a critical problem in various scie&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/hn05dqxyll/cover.png"/></item><item><title>Approximating the Top Eigenvector in Random Order Streams</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/gitgmieinf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/gitgmieinf/</guid><description>Random-order stream data necessitates efficient top eigenvector approximation; this paper presents novel algorithms with improved space complexity, achieving near-optimal bounds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/gitgmieinf/cover.png"/></item><item><title>Are Graph Neural Networks Optimal Approximation Algorithms?</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/sxrblm9ams/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/sxrblm9ams/</guid><description>Graph Neural Networks (GNNs) learn optimal approximation algorithms for combinatorial optimization problems, achieving high-quality solutions for Max-Cut, Min-Vertex-Cover, and Max-3-SAT, while also p&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/sxrblm9ams/cover.png"/></item><item><title>Auditing Local Explanations is Hard</title><link>https://deep-diver.github.io/neurips2024/posters/ybmrn4tdn0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ybmrn4tdn0/</guid><description>Auditing local explanations is surprisingly hard: proving explanation trustworthiness requires far more data than previously thought, especially in high dimensions, challenging current AI explainabil&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ybmrn4tdn0/cover.png"/></item><item><title>Auditing Privacy Mechanisms via Label Inference Attacks</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ai76atrb2y/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ai76atrb2y/</guid><description>New metrics audit label privatization, revealing differentially private schemes often outperform heuristic methods in the privacy-utility tradeoff.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ai76atrb2y/cover.png"/></item><item><title>Automated Efficient Estimation using Monte Carlo Efficient Influence Functions</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/2wfd3pti8v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/2wfd3pti8v/</guid><description>MC-EIF automates efficient statistical estimation for high-dimensional models, integrating seamlessly with existing differentiable probabilistic programming systems and achieving optimal convergence r&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/2wfd3pti8v/cover.png"/></item><item><title>Automatic Outlier Rectification via Optimal Transport</title><link>https://deep-diver.github.io/neurips2024/posters/udxhmgjvjb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/udxhmgjvjb/</guid><description>This study presents a novel single-step outlier rectification method using optimal transport with a concave cost function, surpassing the limitations of conventional two-stage approaches by jointly op&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/udxhmgjvjb/cover.png"/></item><item><title>Average gradient outer product as a mechanism for deep neural collapse</title><link>https://deep-diver.github.io/neurips2024/posters/vtrotud539/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vtrotud539/</guid><description>Deep Neural Collapse (DNC) explained via Average Gradient Outer Product (AGOP).</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vtrotud539/cover.png"/></item><item><title>Axioms for AI Alignment from Human Feedback</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/cmbjkpruvw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/cmbjkpruvw/</guid><description>This paper revolutionizes AI alignment by applying social choice theory axioms to RLHF, exposing flaws in existing methods and proposing novel, axiomatically guaranteed reward learning rules.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/cmbjkpruvw/cover.png"/></item><item><title>Barely Random Algorithms and Collective Metrical Task Systems</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/oajhfvrtbq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/oajhfvrtbq/</guid><description>Randomness-efficient algorithms are developed for online decision making, requiring only 2log n random bits and achieving near-optimal competitiveness for metrical task systems.</description></item><item><title>Benign overfitting in leaky ReLU networks with moderate input dimension</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/88tzdgypt6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/88tzdgypt6/</guid><description>Leaky ReLU networks exhibit benign overfitting under surprisingly relaxed conditions: input dimension only needs to linearly scale with sample size, challenging prior assumptions in the field.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/88tzdgypt6/cover.png"/></item><item><title>Bridging Multicalibration and Out-of-distribution Generalization Beyond Covariate Shift</title><link>https://deep-diver.github.io/neurips2024/posters/bos6wpv0jf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bos6wpv0jf/</guid><description>New model-agnostic framework for out-of-distribution generalization uses multicalibration across overlapping groups, showing improved robustness and prediction under various distribution shifts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bos6wpv0jf/cover.png"/></item><item><title>Can an AI Agent Safely Run a Government? Existence of Probably Approximately Aligned Policies</title><link>https://deep-diver.github.io/neurips2024/posters/xm5m7j6lbl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xm5m7j6lbl/</guid><description>This paper introduces a novel quantitative definition of AI alignment for social decision-making, proposing probably approximately aligned policies and a method to safeguard any autonomous agent&amp;rsquo;s act&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xm5m7j6lbl/cover.png"/></item><item><title>Can Transformers Smell Like Humans?</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3f8i9glbzu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3f8i9glbzu/</guid><description>Pre-trained transformer models can predict human smell perception by encoding odorant chemical structures, aligning with expert labels, continuous ratings, and similarity assessments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3f8i9glbzu/cover.png"/></item><item><title>Causal Dependence Plots</title><link>https://deep-diver.github.io/neurips2024/posters/pu0z2snm1m/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pu0z2snm1m/</guid><description>Causal Dependence Plots (CDPs) visualize how machine learning model predictions causally depend on input features, overcoming limitations of existing methods that ignore causal relationships.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pu0z2snm1m/cover.png"/></item><item><title>Causal Discovery from Event Sequences by Local Cause-Effect Attribution</title><link>https://deep-diver.github.io/neurips2024/posters/y9zirxshzj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y9zirxshzj/</guid><description>CASCADE algorithm unveils hidden causal structures in event sequences by minimizing description length, surpassing existing Granger causality-based methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y9zirxshzj/cover.png"/></item><item><title>Causal vs. Anticausal merging of predictors</title><link>https://deep-diver.github.io/neurips2024/posters/xzkxgvlb0c/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xzkxgvlb0c/</guid><description>Causal assumptions drastically alter predictor merging, with CMAXENT revealing logistic regression for causal and LDA for anticausal directions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xzkxgvlb0c/cover.png"/></item><item><title>Challenges of Generating Structurally Diverse Graphs</title><link>https://deep-diver.github.io/neurips2024/posters/bbgpol1nlo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bbgpol1nlo/</guid><description>Researchers developed novel algorithms to generate structurally diverse graphs, improving graph algorithm testing and neural network evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bbgpol1nlo/cover.png"/></item><item><title>ChronoEpilogi: Scalable Time Series Selection with Multiple Solutions</title><link>https://deep-diver.github.io/neurips2024/posters/y8huxkwaog/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y8huxkwaog/</guid><description>ChronoEpilogi efficiently finds all minimal sets of time-series variables optimally predicting a target, improving forecasting while providing crucial insights for knowledge discovery and causal model&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y8huxkwaog/cover.png"/></item><item><title>Class Distribution Shifts in Zero-Shot Learning: Learning Robust Representations</title><link>https://deep-diver.github.io/neurips2024/posters/yuqubgiobg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yuqubgiobg/</guid><description>Zero-shot learning models often fail in real-world scenarios due to unseen class distribution shifts. This work introduces a novel algorithm that learns robust representations by creating synthetic d&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yuqubgiobg/cover.png"/></item><item><title>Conditional Generative Models are Sufficient to Sample from Any Causal Effect Estimand</title><link>https://deep-diver.github.io/neurips2024/posters/vymkubmllh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vymkubmllh/</guid><description>ID-GEN: Sample high-dimensional interventional distributions using any conditional generative model!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vymkubmllh/cover.png"/></item><item><title>Conditional Outcome Equivalence: A Quantile Alternative to CATE</title><link>https://deep-diver.github.io/neurips2024/posters/typcietpwm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/typcietpwm/</guid><description>Researchers introduce the Conditional Quantile Comparator (CQC) for analyzing heterogeneous treatment effects, offering an improved approach by combining the strengths of CATE and CQTE while overcomin&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/typcietpwm/cover.png"/></item><item><title>Constrained Adaptive Attack: Effective Adversarial Attack Against Deep Neural Networks for Tabular Data</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/zttwkr51yh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/zttwkr51yh/</guid><description>Constrained Adaptive Attack (CAA) significantly improves adversarial attacks on deep learning models for tabular data by combining gradient and search-based methods, achieving up to 96.1% accuracy dro&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/zttwkr51yh/cover.png"/></item><item><title>Continual learning with the neural tangent ensemble</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/qosfijdvkz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/qosfijdvkz/</guid><description>Neural networks, viewed as Bayesian ensembles of fixed classifiers, enable continual learning without forgetting; posterior updates mirror stochastic gradient descent, offering insights into optimizat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/qosfijdvkz/cover.png"/></item><item><title>Corruption-Robust Linear Bandits: Minimax Optimality and Gap-Dependent Misspecification</title><link>https://deep-diver.github.io/neurips2024/posters/wqs2rmq4cw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wqs2rmq4cw/</guid><description>This paper presents novel algorithms for linear bandits that are robust to corrupted rewards, achieving minimax optimality and optimal scaling for gap-dependent misspecification, extending to reinforc&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wqs2rmq4cw/cover.png"/></item><item><title>Credit Attribution and Stable Compression</title><link>https://deep-diver.github.io/neurips2024/posters/crlfvsorzt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/crlfvsorzt/</guid><description>New definitions of differential privacy enable machine learning algorithms to credit sources appropriately, balancing data utility and copyright compliance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/crlfvsorzt/cover.png"/></item><item><title>Derandomizing Multi-Distribution Learning</title><link>https://deep-diver.github.io/neurips2024/posters/twye75mnkt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/twye75mnkt/</guid><description>Derandomizing multi-distribution learning is computationally hard, but a structural condition allows efficient black-box conversion of randomized predictors to deterministic ones.</description></item><item><title>Diffusion Models are Certifiably Robust Classifiers</title><link>https://deep-diver.github.io/neurips2024/posters/wgp1tbcp1e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wgp1tbcp1e/</guid><description>Diffusion models are certifiably robust classifiers due to their inherent O(1) Lipschitzness, a property further enhanced by generalizing to noisy data, achieving over 80% certified robustness on CIFA&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wgp1tbcp1e/cover.png"/></item><item><title>Dimension-free deterministic equivalents and scaling laws for random feature regression</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/fbljifw64d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/fbljifw64d/</guid><description>This work delivers dimension-free deterministic equivalents for random feature regression, revealing sharp excess error rates and scaling laws.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/fbljifw64d/cover.png"/></item><item><title>Distributional regression: CRPS-error bounds for model fitting, model selection and convex aggregation</title><link>https://deep-diver.github.io/neurips2024/posters/csfxzcozpu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/csfxzcozpu/</guid><description>This paper provides the first statistical learning guarantees for distributional regression using CRPS, offering concentration bounds for model fitting, selection, and convex aggregation, applicable t&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/csfxzcozpu/cover.png"/></item><item><title>Divide-and-Conquer Predictive Coding: a structured Bayesian inference algorithm</title><link>https://deep-diver.github.io/neurips2024/posters/dxwiacvkwu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dxwiacvkwu/</guid><description>Divide-and-conquer predictive coding (DCPC) revolutionizes structured Bayesian inference by achieving superior performance in high-dimensional problems while remaining biologically plausible.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dxwiacvkwu/cover.png"/></item><item><title>Do Finetti: On Causal Effects for Exchangeable Data</title><link>https://deep-diver.github.io/neurips2024/oral-ai-theory/4rczeczaon/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-ai-theory/4rczeczaon/</guid><description>Causal inference revolutionized: New framework estimates causal effects from exchangeable data, enabling simultaneous causal discovery and effect estimation via the Do-Finetti algorithm.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-ai-theory/4rczeczaon/cover.png"/></item><item><title>ECLipsE: Efficient Compositional Lipschitz Constant Estimation for Deep Neural Networks</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/61yysy078z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/61yysy078z/</guid><description>ECLipsE: A novel compositional approach drastically accelerates Lipschitz constant estimation for deep neural networks, achieving speedups of thousands of times compared to the state-of-the-art while &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/61yysy078z/cover.png"/></item><item><title>Efficient Combinatorial Optimization via Heat Diffusion</title><link>https://deep-diver.github.io/neurips2024/posters/psdrko9v1d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/psdrko9v1d/</guid><description>Heat Diffusion Optimization (HeO) framework efficiently solves combinatorial optimization problems by enabling information propagation through heat diffusion, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/psdrko9v1d/cover.png"/></item><item><title>Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept Space</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/owuect6btl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/owuect6btl/</guid><description>Generative models learn hidden capabilities suddenly during training, which can be explained and predicted using a novel &amp;lsquo;concept space&amp;rsquo; framework that analyzes learning dynamics and concept signal.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/owuect6btl/cover.png"/></item><item><title>Energy-based Epistemic Uncertainty for Graph Neural Networks</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/6vnpptwh1q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/6vnpptwh1q/</guid><description>GEBM: a novel graph-based energy model for robust GNN uncertainty estimation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/6vnpptwh1q/cover.png"/></item><item><title>Enhancing Robustness of Graph Neural Networks on Social Media with Explainable Inverse Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/zieha15y8k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/zieha15y8k/</guid><description>MoE-BiEntIRL: A novel explainable inverse reinforcement learning method enhances GNN robustness against diverse social media attacks by reconstructing attacker policies and generating more robust trai&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/zieha15y8k/cover.png"/></item><item><title>Enriching Disentanglement: From Logical Definitions to Quantitative Metrics</title><link>https://deep-diver.github.io/neurips2024/posters/tvq3xckwbb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tvq3xckwbb/</guid><description>This paper presents a novel approach to deriving theoretically grounded disentanglement metrics by linking logical definitions to quantitative measures, offering strong theoretical guarantees and easi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tvq3xckwbb/cover.png"/></item><item><title>Entrywise error bounds for low-rank approximations of kernel matrices</title><link>https://deep-diver.github.io/neurips2024/posters/ziyc4fhrnr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ziyc4fhrnr/</guid><description>This paper provides novel entrywise error bounds for low-rank kernel matrix approximations, showing how many data points are needed to get statistically consistent results for low-rank approximations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ziyc4fhrnr/cover.png"/></item><item><title>Estimating Generalization Performance Along the Trajectory of Proximal SGD in Robust Regression</title><link>https://deep-diver.github.io/neurips2024/posters/ntf7d8talq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ntf7d8talq/</guid><description>New consistent estimators precisely track generalization error during robust regression&amp;rsquo;s iterative model training, enabling optimal stopping iteration for minimized error.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ntf7d8talq/cover.png"/></item><item><title>Estimating Heterogeneous Treatment Effects by Combining Weak Instruments and Observational Data</title><link>https://deep-diver.github.io/neurips2024/posters/c37x7cxz2y/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/c37x7cxz2y/</guid><description>This study develops a novel two-stage framework for accurately predicting conditional average treatment effects using both observational data and weak instrumental variables, overcoming limitations of&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/c37x7cxz2y/cover.png"/></item><item><title>Exact, Tractable Gauss-Newton Optimization in Deep Reversible Architectures Reveal Poor Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/p37nlki9vl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/p37nlki9vl/</guid><description>Exact Gauss-Newton optimization in deep reversible networks surprisingly reveals poor generalization, despite faster training, challenging existing deep learning optimization theories.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/p37nlki9vl/cover.png"/></item><item><title>Exogenous Matching: Learning Good Proposals for Tractable Counterfactual Estimation</title><link>https://deep-diver.github.io/neurips2024/posters/ys9xu6ania/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ys9xu6ania/</guid><description>Exogenous Matching learns optimal proposals for efficient counterfactual estimation by transforming variance minimization into conditional distribution learning, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ys9xu6ania/cover.png"/></item><item><title>Exploring Jacobian Inexactness in Second-Order Methods for Variational Inequalities: Lower Bounds, Optimal Algorithms and Quasi-Newton Approximations</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/uvfdaefr9x/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/uvfdaefr9x/</guid><description>VIJI, a novel second-order algorithm, achieves optimal convergence rates for variational inequalities even with inexact Jacobian information, bridging the gap between theory and practice in machine le&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/uvfdaefr9x/cover.png"/></item><item><title>Fair Allocation in Dynamic Mechanism Design</title><link>https://deep-diver.github.io/neurips2024/posters/beungps83o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/beungps83o/</guid><description>This paper presents optimal fair mechanisms for dynamic auction design, maximizing seller revenue while guaranteeing minimum allocations to multiple buyer groups.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/beungps83o/cover.png"/></item><item><title>Fair Secretaries with Unfair Predictions</title><link>https://deep-diver.github.io/neurips2024/posters/dxxj4s06yl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dxxj4s06yl/</guid><description>Fair algorithms can leverage biased predictions to improve performance while guaranteeing fairness for all candidates.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dxxj4s06yl/cover.png"/></item><item><title>Fair Wasserstein Coresets</title><link>https://deep-diver.github.io/neurips2024/posters/ylcej2xiw5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ylcej2xiw5/</guid><description>Fair Wasserstein Coresets (FWC) efficiently generates fair, representative subsets of large datasets for downstream machine learning tasks, improving fairness and utility.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ylcej2xiw5/cover.png"/></item><item><title>Fast T2T: Optimization Consistency Speeds Up Diffusion-Based Training-to-Testing Solving for Combinatorial Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/xdrkzozeoc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xdrkzozeoc/</guid><description>Fast T2T: Optimization Consistency Boosts Diffusion-Based Combinatorial Optimization!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xdrkzozeoc/cover.png"/></item><item><title>Faster Accelerated First-order Methods for Convex Optimization with Strongly Convex Function Constraints</title><link>https://deep-diver.github.io/neurips2024/posters/pg380vlyru/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pg380vlyru/</guid><description>Faster primal-dual algorithms achieve order-optimal complexity for convex optimization with strongly convex constraints, improving convergence rates and solving large-scale problems efficiently.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pg380vlyru/cover.png"/></item><item><title>Feedback control guides credit assignment in recurrent neural networks</title><link>https://deep-diver.github.io/neurips2024/posters/xavwvnjtst/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xavwvnjtst/</guid><description>Brain-inspired recurrent neural networks learn efficiently by using feedback control to approximate optimal gradients, enabling rapid movement corrections and efficient adaptation to persistent errors&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xavwvnjtst/cover.png"/></item><item><title>From Linear to Linearizable Optimization: A Novel Framework with Applications to Stationary and Non-stationary DR-submodular Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/dgamsmeef8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dgamsmeef8/</guid><description>A novel framework extends optimization algorithms from linear/quadratic functions to a broader class of &amp;lsquo;upper-linearizable&amp;rsquo; functions, providing a unified approach for concave and DR-submodular optim&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dgamsmeef8/cover.png"/></item><item><title>Generalization Analysis for Label-Specific Representation Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/dtpiuxdjhy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/dtpiuxdjhy/</guid><description>Researchers derived tighter generalization bounds for label-specific representation learning (LSRL) methods, improving understanding of LSRL&amp;rsquo;s success and offering guidance for future algorithm develo&amp;hellip;</description></item><item><title>Generalization Bounds via Conditional $f$-Information</title><link>https://deep-diver.github.io/neurips2024/posters/ocxvxe5xn1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ocxvxe5xn1/</guid><description>New information-theoretic generalization bounds, based on conditional f-information, improve existing methods by addressing unboundedness and offering a generic approach applicable to various loss fun&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ocxvxe5xn1/cover.png"/></item><item><title>Generalization Error Bounds for Two-stage Recommender Systems with Tree Structure</title><link>https://deep-diver.github.io/neurips2024/oral-ai-theory/m1a4crrjr7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-ai-theory/m1a4crrjr7/</guid><description>Two-stage recommender systems using tree structures achieve better generalization with more branches and harmonized training data distributions across stages.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-ai-theory/m1a4crrjr7/cover.png"/></item><item><title>Get rich quick: exact solutions reveal how unbalanced initializations promote rapid feature learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/enm94i7r3a/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/enm94i7r3a/</guid><description>Unbalanced initializations dramatically accelerate neural network feature learning by modifying the geometry of learning trajectories, enabling faster feature extraction and improved generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/enm94i7r3a/cover.png"/></item><item><title>HardCore Generation: Generating Hard UNSAT Problems for Data Augmentation</title><link>https://deep-diver.github.io/neurips2024/posters/njvpjg0bfk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/njvpjg0bfk/</guid><description>HardCore: Fast generation of hard, realistic UNSAT problems for improved SAT solver runtime prediction.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/njvpjg0bfk/cover.png"/></item><item><title>Honor Among Bandits: No-Regret Learning for Online Fair Division</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ocqbc0edjj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ocqbc0edjj/</guid><description>Online fair division algorithm achieves Õ(T²/³) regret while guaranteeing envy-freeness or proportionality in expectation, a result proven tight.</description></item><item><title>Hybrid Top-Down Global Causal Discovery with Local Search for Linear and Nonlinear Additive Noise Models</title><link>https://deep-diver.github.io/neurips2024/posters/xnmm1jthkv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xnmm1jthkv/</guid><description>Hybrid causal discovery algorithm efficiently learns unique causal graphs from observational data by leveraging local substructures and topological sorting, outperforming existing methods in accuracy &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xnmm1jthkv/cover.png"/></item><item><title>Identification and Estimation of the Bi-Directional MR with Some Invalid Instruments</title><link>https://deep-diver.github.io/neurips2024/oral-ai-theory/s2p6kpltm8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-ai-theory/s2p6kpltm8/</guid><description>PReBiM algorithm accurately estimates bi-directional causal effects from observational data, even with invalid instruments, using a novel cluster fusion approach.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-ai-theory/s2p6kpltm8/cover.png"/></item><item><title>Identification of Analytic Nonlinear Dynamical Systems with Non-asymptotic Guarantees</title><link>https://deep-diver.github.io/neurips2024/posters/nf34qxcy0b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nf34qxcy0b/</guid><description>This paper proves that non-active exploration suffices for identifying linearly parameterized nonlinear systems with real-analytic features, providing non-asymptotic guarantees for least-squares and s&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nf34qxcy0b/cover.png"/></item><item><title>Identifying Causal Effects Under Functional Dependencies</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/oisuwqsvkd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/oisuwqsvkd/</guid><description>Unlocking identifiability of causal effects: This paper leverages functional dependencies in causal graphs to improve identifiability, leading to fewer needed variables in observational data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/oisuwqsvkd/cover.png"/></item><item><title>Identifying Equivalent Training Dynamics</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/boyvesx7pk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/boyvesx7pk/</guid><description>New framework uses Koopman operator theory to identify equivalent training dynamics in deep neural networks, enabling quantitative comparison of different architectures and optimization methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/boyvesx7pk/cover.png"/></item><item><title>Implicit Bias of Mirror Flow on Separable Data</title><link>https://deep-diver.github.io/neurips2024/posters/wimaws0fwb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wimaws0fwb/</guid><description>Mirror descent&amp;rsquo;s implicit bias on separable data is formally characterized, revealing convergence towards a maximum margin classifier determined by the potential&amp;rsquo;s &amp;lsquo;horizon function&amp;rsquo;.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wimaws0fwb/cover.png"/></item><item><title>Implicit Regularization Paths of Weighted Neural Representations</title><link>https://deep-diver.github.io/neurips2024/posters/oxcmwwkqtz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oxcmwwkqtz/</guid><description>Weighted pretrained features implicitly regularize models, and this paper reveals equivalent paths between weighting schemes and ridge regularization, enabling efficient hyperparameter tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oxcmwwkqtz/cover.png"/></item><item><title>Information-theoretic Generalization Analysis for Expected Calibration Error</title><link>https://deep-diver.github.io/neurips2024/posters/yltjalwtw9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yltjalwtw9/</guid><description>New theoretical analysis reveals optimal binning strategies for minimizing bias in expected calibration error (ECE), improving machine learning model calibration evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yltjalwtw9/cover.png"/></item><item><title>Interpolating Item and User Fairness in Multi-Sided Recommendations</title><link>https://deep-diver.github.io/neurips2024/posters/taog1hdvgy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/taog1hdvgy/</guid><description>Problem (FAIR) framework and FORM algorithm achieve flexible multi-stakeholder fairness in online recommendation systems, balancing platform revenue with user and item fairness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/taog1hdvgy/cover.png"/></item><item><title>Intruding with Words: Towards Understanding Graph Injection Attacks at the Text Level</title><link>https://deep-diver.github.io/neurips2024/posters/otzydukwpq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/otzydukwpq/</guid><description>Researchers unveil text-level graph injection attacks, revealing a new vulnerability in GNNs and highlighting the importance of text interpretability in attack success.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/otzydukwpq/cover.png"/></item><item><title>Iterative Methods via Locally Evolving Set Process</title><link>https://deep-diver.github.io/neurips2024/posters/wt2kheb97a/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wt2kheb97a/</guid><description>This paper proposes a novel framework, the locally evolving set process, to develop faster localized iterative methods for solving large-scale graph problems, achieving significant speedup over existi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wt2kheb97a/cover.png"/></item><item><title>Langevin Unlearning: A New Perspective of Noisy Gradient Descent for Machine Unlearning</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3lkuc8rbyv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3lkuc8rbyv/</guid><description>Langevin unlearning offers a novel, privacy-preserving machine unlearning framework based on noisy gradient descent, handling both convex and non-convex problems efficiently.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3lkuc8rbyv/cover.png"/></item><item><title>Learning Better Representations From Less Data For Propositional Satisfiability</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/vmshnv8cvs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/vmshnv8cvs/</guid><description>NeuRes, a novel neuro-symbolic approach, achieves superior SAT solving accuracy using significantly less training data than existing methods by combining certificate-driven learning with expert iterat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/vmshnv8cvs/cover.png"/></item><item><title>Learning diffusion at lightspeed</title><link>https://deep-diver.github.io/neurips2024/oral-ai-theory/y10avdrfnk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-ai-theory/y10avdrfnk/</guid><description>JKOnet* learns diffusion processes at unprecedented speed and accuracy by directly minimizing a simple quadratic loss function, bypassing complex bilevel optimization problems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-ai-theory/y10avdrfnk/cover.png"/></item><item><title>Learning Discrete Concepts in Latent Hierarchical Models</title><link>https://deep-diver.github.io/neurips2024/posters/bo5buxvh6m/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bo5buxvh6m/</guid><description>This paper introduces a novel framework for learning discrete concepts from high-dimensional data, establishing theoretical conditions for identifying underlying hierarchical causal structures and pro&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bo5buxvh6m/cover.png"/></item><item><title>Learning diverse causally emergent representations from time series data</title><link>https://deep-diver.github.io/neurips2024/posters/z6relfqv6w/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z6relfqv6w/</guid><description>AI learns emergent system features from time-series data using a novel differentiable architecture maximizing causal emergence, outperforming pure mutual information maximization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/z6relfqv6w/cover.png"/></item><item><title>Learning from Uncertain Data: From Possible Worlds to Possible Models</title><link>https://deep-diver.github.io/neurips2024/posters/v9rqrfslq2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v9rqrfslq2/</guid><description>ZORRO: A new method for learning linear models from uncertain data, providing sound over-approximations of all possible models and prediction ranges.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v9rqrfslq2/cover.png"/></item><item><title>Learning Generalized Linear Programming Value Functions</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/vxijl0ioid/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/vxijl0ioid/</guid><description>Learn optimal LP values faster with a novel neural network method!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/vxijl0ioid/cover.png"/></item><item><title>Learning Human-like Representations to Enable Learning Human Values</title><link>https://deep-diver.github.io/neurips2024/posters/sqapqmbqip/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sqapqmbqip/</guid><description>Aligning AI&amp;rsquo;s world representation with humans enables faster, safer learning of human values, improving both exploration and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sqapqmbqip/cover.png"/></item><item><title>Learning Linear Causal Representations from General Environments: Identifiability and Intrinsic Ambiguity</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/db99jjwx3h/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/db99jjwx3h/</guid><description>LiNGCREL, a novel algorithm, provably recovers linear causal representations from diverse environments, achieving identifiability despite intrinsic ambiguities, thus advancing causal AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/db99jjwx3h/cover.png"/></item><item><title>Learning Social Welfare Functions</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/7o6ktaar8n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/7o6ktaar8n/</guid><description>Learning social welfare functions from past decisions is possible! This paper shows how to efficiently learn power mean functions, a widely used family, using both cardinal and pairwise welfare compar&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/7o6ktaar8n/cover.png"/></item><item><title>Learning to Mitigate Externalities: the Coase Theorem with Hindsight Rationality</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/omyzrkacme/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/omyzrkacme/</guid><description>Economists learn to resolve externalities efficiently even when players lack perfect information, maximizing social welfare by leveraging bargaining and online learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/omyzrkacme/cover.png"/></item><item><title>Learning to Solve Quadratic Unconstrained Binary Optimization in a Classification Way</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/p43obiwjfw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/p43obiwjfw/</guid><description>Researchers developed Value Classification Model (VCM), a neural solver that swiftly solves quadratic unconstrained binary optimization (QUBO) problems by directly generating solutions using a classif&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/p43obiwjfw/cover.png"/></item><item><title>Linear Causal Representation Learning from Unknown Multi-node Interventions</title><link>https://deep-diver.github.io/neurips2024/posters/weemasptzg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/weemasptzg/</guid><description>Unlocking Causal Structures: New algorithms identify latent causal relationships from interventions, even when multiple variables are affected simultaneously.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/weemasptzg/cover.png"/></item><item><title>Lookback Prophet Inequalities</title><link>https://deep-diver.github.io/neurips2024/posters/cg1vwt5xou/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cg1vwt5xou/</guid><description>This paper enhances prophet inequalities by allowing lookback, improving competitive ratios and providing algorithms for diverse observation orders, thereby bridging theory and real-world online selec&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cg1vwt5xou/cover.png"/></item><item><title>MALT Powers Up Adversarial Attacks</title><link>https://deep-diver.github.io/neurips2024/posters/bcqix5q8qx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bcqix5q8qx/</guid><description>MALT: a novel adversarial attack, is 5x faster than AutoAttack, achieving higher success rates on CIFAR-100 and ImageNet by exploiting mesoscopic almost linearity in neural networks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bcqix5q8qx/cover.png"/></item><item><title>Marginal Causal Flows for Validation and Inference</title><link>https://deep-diver.github.io/neurips2024/posters/zjremskvyh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zjremskvyh/</guid><description>Frugal Flows: Generate realistic causal benchmarks with exact marginal causal effects, enabling robust causal method validation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zjremskvyh/cover.png"/></item><item><title>Mean-Field Langevin Dynamics for Signed Measures via a Bilevel Approach</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/oo7hy9kmk6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/oo7hy9kmk6/</guid><description>This paper presents a novel bilevel approach to extend mean-field Langevin dynamics to solve convex optimization problems over signed measures, achieving stronger guarantees and faster convergence rat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/oo7hy9kmk6/cover.png"/></item><item><title>Measuring Goal-Directedness</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/o4codiby7e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/o4codiby7e/</guid><description>New metric, Maximum Entropy Goal-Directedness (MEG), quantifies AI goal-directedness, crucial for assessing AI safety and agency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/o4codiby7e/cover.png"/></item><item><title>Mechanism design augmented with output advice</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ajgks7qozm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ajgks7qozm/</guid><description>Mechanism design enhanced with output advice improves approximation guarantees by using imperfect predictions of the output, not agent types, offering robust, practical solutions.</description></item><item><title>Metric Transforms and Low Rank Representations of Kernels for Fast Attention</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/k9pxsryuwg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/k9pxsryuwg/</guid><description>Researchers unveil novel linear-algebraic tools revealing the limits of fast attention, classifying positive definite kernels for Manhattan distance, and fully characterizing metric transforms for Man&amp;hellip;</description></item><item><title>Minimum Entropy Coupling with Bottleneck</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ylmym7shde/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ylmym7shde/</guid><description>A novel lossy compression framework, Minimum Entropy Coupling with Bottleneck (MEC-B), extends existing methods by integrating a bottleneck for controlled stochasticity, enhancing performance in scen&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ylmym7shde/cover.png"/></item><item><title>Mirror and Preconditioned Gradient Descent in Wasserstein Space</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/n12b6wva55/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/n12b6wva55/</guid><description>This paper presents novel mirror and preconditioned gradient descent algorithms for optimizing functionals over Wasserstein space, offering improved convergence and efficiency for various machine lear&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/n12b6wva55/cover.png"/></item><item><title>Model Collapse Demystified: The Case of Regression</title><link>https://deep-diver.github.io/neurips2024/posters/biohntrnqk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/biohntrnqk/</guid><description>Training AI models on AI-generated data leads to performance degradation, known as model collapse. This paper offers analytical formulas that precisely quantify this effect in high-dimensional regress&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/biohntrnqk/cover.png"/></item><item><title>Most Influential Subset Selection: Challenges, Promises, and Beyond</title><link>https://deep-diver.github.io/neurips2024/posters/qwi33ppecc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qwi33ppecc/</guid><description>Adaptive greedy algorithms significantly improve the accuracy of identifying the most influential subset of training data, overcoming limitations of existing methods that fail to capture complex inter&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qwi33ppecc/cover.png"/></item><item><title>Multiclass Transductive Online Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3erevfwalz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3erevfwalz/</guid><description>Unbounded label spaces conquered! New algorithm achieves optimal mistake bounds in multiclass transductive online learning.</description></item><item><title>Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization</title><link>https://deep-diver.github.io/neurips2024/posters/bia03matxq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bia03matxq/</guid><description>Multilinear Mixture of Experts (μMoE) achieves scalable expert specialization in deep neural networks through tensor factorization, enabling efficient fine-tuning and interpretable model editing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bia03matxq/cover.png"/></item><item><title>Nearly Optimal Approximation of Matrix Functions by the Lanczos Method</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3s8v8qp9xv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3s8v8qp9xv/</guid><description>Lanczos-FA, a simple algorithm for approximating matrix functions, surprisingly outperforms newer methods; this paper proves its near-optimality for rational functions, explaining its practical succes&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3s8v8qp9xv/cover.png"/></item><item><title>Nearly Tight Black-Box Auditing of Differentially Private Machine Learning</title><link>https://deep-diver.github.io/neurips2024/posters/ccdmxxiamp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ccdmxxiamp/</guid><description>This paper presents a new auditing method for DP-SGD that provides substantially tighter black-box privacy analyses than previous methods, yielding significantly closer empirical estimates to theoreti&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ccdmxxiamp/cover.png"/></item><item><title>Neural Model Checking</title><link>https://deep-diver.github.io/neurips2024/posters/dj9kzkq0oh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dj9kzkq0oh/</guid><description>Neural networks revolutionize hardware model checking by generating formal proof certificates, outperforming state-of-the-art techniques in speed and scalability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dj9kzkq0oh/cover.png"/></item><item><title>Neural Persistence Dynamics</title><link>https://deep-diver.github.io/neurips2024/posters/rcnzrfikx6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rcnzrfikx6/</guid><description>Neural Persistence Dynamics learns collective behavior from topological features, accurately predicting parameters of governing equations without tracking individual entities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rcnzrfikx6/cover.png"/></item><item><title>Neural Pfaffians: Solving Many Many-Electron Schrödinger Equations</title><link>https://deep-diver.github.io/neurips2024/oral-ai-theory/hrknicwm3e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-ai-theory/hrknicwm3e/</guid><description>Neural Pfaffians revolutionize many-electron Schrödinger equation solutions by using fully learnable neural wave functions based on Pfaffians, achieving unprecedented accuracy and generalizability acr&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-ai-theory/hrknicwm3e/cover.png"/></item><item><title>No-regret Learning in Harmonic Games: Extrapolation in the Face of Conflicting Interests</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/hw9s9vy5gz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/hw9s9vy5gz/</guid><description>Extrapolated FTRL ensures Nash equilibrium convergence in harmonic games, defying standard no-regret learning limitations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/hw9s9vy5gz/cover.png"/></item><item><title>On Convergence of Adam for Stochastic Optimization under Relaxed Assumptions</title><link>https://deep-diver.github.io/neurips2024/posters/x7usmidzxj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x7usmidzxj/</guid><description>Adam optimizer achieves near-optimal convergence in non-convex scenarios with unbounded gradients and relaxed noise assumptions, improving its theoretical understanding and practical application.</description></item><item><title>On Differentially Private U Statistics</title><link>https://deep-diver.github.io/neurips2024/posters/zapfyclg6k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zapfyclg6k/</guid><description>New algorithms achieve near-optimal differentially private U-statistic estimation, significantly improving accuracy over existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zapfyclg6k/cover.png"/></item><item><title>On Statistical Rates and Provably Efficient Criteria of Latent Diffusion Transformers (DiTs)</title><link>https://deep-diver.github.io/neurips2024/posters/cv2lkbdlz4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cv2lkbdlz4/</guid><description>Latent Diffusion Transformers (DiTs) achieve almost-linear time training and inference through low-rank gradient approximations and efficient criteria, overcoming high dimensionality challenges.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cv2lkbdlz4/cover.png"/></item><item><title>On the Ability of Developers' Training Data Preservation of Learnware</title><link>https://deep-diver.github.io/neurips2024/posters/wsqdjhpuhn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wsqdjhpuhn/</guid><description>Learnware systems enable model reuse; this paper proves RKME specifications protect developers&amp;rsquo; training data while enabling effective model identification.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wsqdjhpuhn/cover.png"/></item><item><title>On the Complexity of Identification in Linear Structural Causal Models</title><link>https://deep-diver.github.io/neurips2024/posters/bndwooxj6w/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bndwooxj6w/</guid><description>New polynomial-space algorithm for causal parameter identification in linear models vastly improves upon existing methods, showing that this crucial task is computationally hard.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bndwooxj6w/cover.png"/></item><item><title>On the Expressive Power of Tree-Structured Probabilistic Circuits</title><link>https://deep-diver.github.io/neurips2024/posters/suyaaoi5bd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/suyaaoi5bd/</guid><description>Tree-structured probabilistic circuits are surprisingly efficient: this paper proves a quasi-polynomial upper bound on their size, showing they&amp;rsquo;re almost as expressive as more complex DAG structures.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/suyaaoi5bd/cover.png"/></item><item><title>On the Identifiability of Poisson Branching Structural Causal Model Using Probability Generating Function</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/tuwwbljfk9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/tuwwbljfk9/</guid><description>Researchers developed a novel, efficient causal discovery method using Probability Generating Functions to identify causal structures within Poisson Branching Structural Causal Models, overcoming limi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/tuwwbljfk9/cover.png"/></item><item><title>On the Impacts of the Random Initialization in the Neural Tangent Kernel Theory</title><link>https://deep-diver.github.io/neurips2024/posters/ni3ud2bv3g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ni3ud2bv3g/</guid><description>Standard initialization in neural networks negatively impacts generalization ability under Neural Tangent Kernel theory, contradicting real-world performance, urging the development of improved theore&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ni3ud2bv3g/cover.png"/></item><item><title>Online Bayesian Persuasion Without a Clue</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/xnpvz8e1ty/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/xnpvz8e1ty/</guid><description>Researchers developed a novel online Bayesian persuasion algorithm that achieves sublinear regret without prior knowledge of the receiver or the state distribution, providing tight theoretical guarant&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/xnpvz8e1ty/cover.png"/></item><item><title>Online Consistency of the Nearest Neighbor Rule</title><link>https://deep-diver.github.io/neurips2024/posters/eox0smruv7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/eox0smruv7/</guid><description>The 1-nearest neighbor rule achieves online consistency under surprisingly broad conditions: measurable label functions and mild assumptions on instance generation in doubling metric spaces.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/eox0smruv7/cover.png"/></item><item><title>Online Convex Optimisation: The Optimal Switching Regret for all Segmentations Simultaneously</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/u6xxyud3ro/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/u6xxyud3ro/</guid><description>Algorithm RESET achieves optimal switching regret simultaneously across all segmentations, offering efficiency and parameter-free operation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/u6xxyud3ro/cover.png"/></item><item><title>Online Estimation via Offline Estimation: An Information-Theoretic Framework</title><link>https://deep-diver.github.io/neurips2024/posters/sks7x4i8bh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sks7x4i8bh/</guid><description>This paper introduces a novel information-theoretic framework, showing how to convert offline into online estimation algorithms efficiently, impacting interactive decision-making.</description></item><item><title>Online Weighted Paging with Unknown Weights</title><link>https://deep-diver.github.io/neurips2024/posters/ctxty3vggq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ctxty3vggq/</guid><description>First algorithm for online weighted paging that learns page weights from samples, achieving optimal O(log k) competitiveness and sublinear regret.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ctxty3vggq/cover.png"/></item><item><title>Only Strict Saddles in the Energy Landscape of Predictive Coding Networks?</title><link>https://deep-diver.github.io/neurips2024/posters/etu6kvrksq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/etu6kvrksq/</guid><description>Predictive coding networks learn faster than backpropagation by changing the loss landscape&amp;rsquo;s geometry, making saddles easier to escape and improving robustness to vanishing gradients.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/etu6kvrksq/cover.png"/></item><item><title>Optimal ablation for interpretability</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/opt72tyzwz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/opt72tyzwz/</guid><description>Optimal ablation (OA) improves model interpretability by precisely measuring component importance, outperforming existing methods. OA-based importance shines in circuit discovery, factual recall, and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/opt72tyzwz/cover.png"/></item><item><title>Optimal Algorithms for Learning Partitions with Faulty Oracles</title><link>https://deep-diver.github.io/neurips2024/posters/ygdl8q02ga/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ygdl8q02ga/</guid><description>Optimal algorithms for learning partitions are designed, achieving minimum query complexity even with up to l faulty oracle responses.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ygdl8q02ga/cover.png"/></item><item><title>Optimal Algorithms for Online Convex Optimization with Adversarial Constraints</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/txffvjmnby/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/txffvjmnby/</guid><description>Optimal algorithms for online convex optimization with adversarial constraints are developed, achieving O(√T) regret and Õ(√T) constraint violation—a breakthrough in the field.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/txffvjmnby/cover.png"/></item><item><title>Optimal Parallelization of Boosting</title><link>https://deep-diver.github.io/neurips2024/oral-ai-theory/rtz4df9if1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-ai-theory/rtz4df9if1/</guid><description>This paper closes the performance gap in parallel boosting algorithms by presenting improved lower bounds and a novel algorithm matching these bounds, settling the parallel complexity of sample-optima&amp;hellip;</description></item><item><title>Optimization Algorithm Design via Electric Circuits</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/9jmt1eer9p/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/9jmt1eer9p/</guid><description>Design provably convergent optimization algorithms swiftly using electric circuit analogies; a novel methodology automating discretization for diverse algorithms.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/9jmt1eer9p/cover.png"/></item><item><title>OxonFair: A Flexible Toolkit for Algorithmic Fairness</title><link>https://deep-diver.github.io/neurips2024/posters/ztwl4ubnxv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ztwl4ubnxv/</guid><description>OxonFair: a new open-source toolkit for enforcing fairness in binary classification, supporting NLP, Computer Vision, and tabular data, optimizing any fairness metric, and minimizing performance degra&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ztwl4ubnxv/cover.png"/></item><item><title>Parameter Symmetry and Noise Equilibrium of Stochastic Gradient Descent</title><link>https://deep-diver.github.io/neurips2024/posters/uhki1re2nz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uhki1re2nz/</guid><description>SGD&amp;rsquo;s dynamics are precisely characterized by the interplay of noise and symmetry in loss functions, leading to unique, initialization-independent fixed points.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uhki1re2nz/cover.png"/></item><item><title>Partial Structure Discovery is Sufficient for No-regret Learning in Causal Bandits</title><link>https://deep-diver.github.io/neurips2024/posters/um3rq14iex/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/um3rq14iex/</guid><description>Learning optimal interventions in causal bandits with unknown causal graphs is now efficient; this paper identifies the minimal causal knowledge needed and offers a two-stage algorithm with sublinear &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/um3rq14iex/cover.png"/></item><item><title>Paths to Equilibrium in Games</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/lxxiiinmuf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/lxxiiinmuf/</guid><description>In n-player games, a satisficing path always exists leading from any initial strategy profile to a Nash equilibrium by allowing unsatisfied players to explore suboptimal strategies.</description></item><item><title>Policy Aggregation</title><link>https://deep-diver.github.io/neurips2024/posters/ybiuvixjth/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ybiuvixjth/</guid><description>This paper introduces efficient algorithms that leverage social choice theory to aggregate multiple individual preferences, resulting in a desirable collective AI policy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ybiuvixjth/cover.png"/></item><item><title>Principled Bayesian Optimization in Collaboration with Human Experts</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/idn9sikgly/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/idn9sikgly/</guid><description>COBOL: a novel Bayesian Optimization algorithm leverages human expert advice via binary labels, achieving both fast convergence and robustness to noisy input, while guaranteeing minimal expert effort.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/idn9sikgly/cover.png"/></item><item><title>Private Edge Density Estimation for Random Graphs: Optimal, Efficient and Robust</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/4nq24chnoi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/4nq24chnoi/</guid><description>This paper delivers a groundbreaking polynomial-time algorithm for optimally estimating edge density in random graphs while ensuring node privacy and robustness against data corruption.</description></item><item><title>Private Geometric Median</title><link>https://deep-diver.github.io/neurips2024/posters/cpzjn7kabv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cpzjn7kabv/</guid><description>This paper introduces new differentially private algorithms to compute the geometric median, achieving improved accuracy by scaling with the effective data diameter instead of a known radius.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cpzjn7kabv/cover.png"/></item><item><title>Proving Theorems Recursively</title><link>https://deep-diver.github.io/neurips2024/posters/yaa5l92ttq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yaa5l92ttq/</guid><description>POETRY: a recursive neural theorem prover achieving 5.1% higher success rate and solving substantially longer proofs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yaa5l92ttq/cover.png"/></item><item><title>Quantum Algorithms for Non-smooth Non-convex Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/wsgzvhnoax/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wsgzvhnoax/</guid><description>Quantum algorithms achieve speedups in non-smooth, non-convex optimization, outperforming classical methods by a factor of ε⁻²/³ in query complexity for finding (δ,ε)-Goldstein stationary points.</description></item><item><title>Random Function Descent</title><link>https://deep-diver.github.io/neurips2024/posters/xzcubjhqbs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xzcubjhqbs/</guid><description>Random Function Descent (RFD) replaces the classical convex function framework with a random function approach, providing a scalable gradient descent method with inherent scale invariance and a theore&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xzcubjhqbs/cover.png"/></item><item><title>RashomonGB: Analyzing the Rashomon Effect and Mitigating Predictive Multiplicity in Gradient Boosting</title><link>https://deep-diver.github.io/neurips2024/posters/zpw6nmhvku/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zpw6nmhvku/</guid><description>RashomonGB tackles predictive multiplicity in gradient boosting by introducing a novel inference technique to efficiently identify and mitigate conflicting model predictions, improving model selection&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zpw6nmhvku/cover.png"/></item><item><title>Refusal in Language Models Is Mediated by a Single Direction</title><link>https://deep-diver.github.io/neurips2024/posters/ph3xaqme6c/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ph3xaqme6c/</guid><description>LLM refusal is surprisingly mediated by a single, easily manipulated direction in the model&amp;rsquo;s activation space.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ph3xaqme6c/cover.png"/></item><item><title>Reliable Learning of Halfspaces under Gaussian Marginals</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/0lb8vzt1db/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/0lb8vzt1db/</guid><description>New algorithm reliably learns Gaussian halfspaces with significantly improved sample and computational complexity compared to existing methods, offering strong computational separation from standard a&amp;hellip;</description></item><item><title>ReLIZO: Sample Reusable Linear Interpolation-based Zeroth-order Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/yzvianpvu6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yzvianpvu6/</guid><description>ReLIZO boosts zeroth-order optimization by cleverly reusing past queries, drastically cutting computation costs while maintaining gradient estimation accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yzvianpvu6/cover.png"/></item><item><title>Revisiting K-mer Profile for Effective and Scalable Genome Representation Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ehsd856ltb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ehsd856ltb/</guid><description>This paper proposes a lightweight and scalable k-mer based model for metagenomic binning, achieving comparable performance to computationally expensive genome foundation models while significantly imp&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ehsd856ltb/cover.png"/></item><item><title>Robust Graph Neural Networks via Unbiased Aggregation</title><link>https://deep-diver.github.io/neurips2024/posters/dz6ex9ee0q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dz6ex9ee0q/</guid><description>RUNG: a novel GNN architecture boasting superior robustness against adaptive attacks by employing an unbiased aggregation technique.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dz6ex9ee0q/cover.png"/></item><item><title>RoPINN: Region Optimized Physics-Informed Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/wzigmvfurk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wzigmvfurk/</guid><description>ROPINN: Revolutionizing Physics-Informed Neural Networks with Region Optimization</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wzigmvfurk/cover.png"/></item><item><title>Sample Complexity of Posted Pricing for a Single Item</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ek1tyhcb3w/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ek1tyhcb3w/</guid><description>This paper reveals how many buyer samples are needed to set near-optimal posted prices for a single item, resolving a fundamental problem in online markets and offering both theoretical and practical &amp;hellip;</description></item><item><title>Sample-Efficient Private Learning of Mixtures of Gaussians</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/74b6qx62vw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/74b6qx62vw/</guid><description>Researchers achieve a breakthrough in privacy-preserving machine learning by developing sample-efficient algorithms for learning Gaussian Mixture Models, significantly reducing the data needed while m&amp;hellip;</description></item><item><title>Semidefinite Relaxations of the Gromov-Wasserstein Distance</title><link>https://deep-diver.github.io/neurips2024/posters/rm3ffh1mqk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rm3ffh1mqk/</guid><description>This paper introduces a novel, tractable semidefinite program (SDP) relaxation for the Gromov-Wasserstein distance, enabling the computation of globally optimal transportation plans.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rm3ffh1mqk/cover.png"/></item><item><title>Sequential Probability Assignment with Contexts: Minimax Regret, Contextual Shtarkov Sums, and Contextual Normalized Maximum Likelihood</title><link>https://deep-diver.github.io/neurips2024/posters/urntypkf3v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/urntypkf3v/</guid><description>This paper introduces contextual Shtarkov sums, a new complexity measure characterizing minimax regret in sequential probability assignment with contexts, and derives the minimax optimal algorithm, co&amp;hellip;</description></item><item><title>Shaping the distribution of neural responses with interneurons in a recurrent circuit model</title><link>https://deep-diver.github.io/neurips2024/posters/ojlieq0j9t/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ojlieq0j9t/</guid><description>Researchers developed a recurrent neural circuit model that efficiently transforms sensory signals into neural representations by dynamically adjusting interneuron connectivity and activation function&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ojlieq0j9t/cover.png"/></item><item><title>Sketchy Moment Matching: Toward Fast and Provable Data Selection for Finetuning</title><link>https://deep-diver.github.io/neurips2024/posters/yaaqwbmgit/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yaaqwbmgit/</guid><description>Sketchy Moment Matching (SkMM) is a fast and theoretically sound data selection method for deep learning finetuning. By controlling variance-bias tradeoffs in high dimensions, SkMM drastically reduces&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yaaqwbmgit/cover.png"/></item><item><title>Solving Inverse Problems via Diffusion Optimal Control</title><link>https://deep-diver.github.io/neurips2024/posters/wqlc4g1gn3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wqlc4g1gn3/</guid><description>Revolutionizing inverse problem solving, this paper introduces diffusion optimal control, a novel framework converting signal recovery into a discrete optimal control problem, surpassing limitations o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wqlc4g1gn3/cover.png"/></item><item><title>Stable Minima Cannot Overfit in Univariate ReLU Networks: Generalization by Large Step Sizes</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/7swrtm9qsp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/7swrtm9qsp/</guid><description>Deep ReLU networks trained with large, constant learning rates avoid overfitting in univariate regression due to minima stability, generalizing well even with noisy labels.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/7swrtm9qsp/cover.png"/></item><item><title>Statistical Estimation in the Spiked Tensor Model via the Quantum Approximate Optimization Algorithm</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/wtlvxdzhmp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/wtlvxdzhmp/</guid><description>Quantum Approximate Optimization Algorithm (QAOA) achieves weak recovery in spiked tensor models matching classical methods, but with potential constant factor advantages for certain parameters.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/wtlvxdzhmp/cover.png"/></item><item><title>Statistical Multicriteria Benchmarking via the GSD-Front</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/jxxvskb9hd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/jxxvskb9hd/</guid><description>Researchers can now reliably benchmark classifiers using multiple quality metrics via the GSD-front, a new information-efficient technique that accounts for statistical uncertainty and deviations from&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/jxxvskb9hd/cover.png"/></item><item><title>Stochastic Optimal Control Matching</title><link>https://deep-diver.github.io/neurips2024/posters/wfu2cdgmwt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wfu2cdgmwt/</guid><description>Stochastic Optimal Control Matching (SOCM) significantly reduces errors in stochastic optimal control by learning a matching vector field using a novel iterative diffusion optimization technique.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wfu2cdgmwt/cover.png"/></item><item><title>Symmetries in Overparametrized Neural Networks: A Mean Field View</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/l86glqncuj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/l86glqncuj/</guid><description>Overparametrized neural networks&amp;rsquo; learning dynamics are analyzed under data symmetries using mean-field theory, revealing that data augmentation, feature averaging, and equivariant architectures asymp&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/l86glqncuj/cover.png"/></item><item><title>The Empirical Impact of Neural Parameter Symmetries, or Lack Thereof</title><link>https://deep-diver.github.io/neurips2024/posters/pcvxyw6fkg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pcvxyw6fkg/</guid><description>Breaking neural network parameter symmetries leads to faster training, better generalization, and improved loss landscape behavior, as demonstrated by novel asymmetric network architectures.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pcvxyw6fkg/cover.png"/></item><item><title>The Group Robustness is in the Details: Revisiting Finetuning under Spurious Correlations</title><link>https://deep-diver.github.io/neurips2024/posters/ehziwahj06/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ehziwahj06/</guid><description>Finetuning&amp;rsquo;s impact on worst-group accuracy is surprisingly nuanced, with common class-balancing methods sometimes hurting performance; a novel mixture method consistently outperforms others.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ehziwahj06/cover.png"/></item><item><title>The Implicit Bias of Adam on Separable Data</title><link>https://deep-diver.github.io/neurips2024/posters/xrqxan3wkm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xrqxan3wkm/</guid><description>Adam&amp;rsquo;s implicit bias revealed: On separable data, Adam converges towards the maximum l∞-margin solution, a finding contrasting with gradient descent&amp;rsquo;s l2-margin preference. This polynomial-time conver&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xrqxan3wkm/cover.png"/></item><item><title>The Implicit Bias of Heterogeneity towards Invariance: A Study of Multi-Environment Matrix Sensing</title><link>https://deep-diver.github.io/neurips2024/posters/pmpbxmf8t3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pmpbxmf8t3/</guid><description>Leveraging data heterogeneity, this study reveals that standard SGD implicitly learns invariant features across multiple environments, achieving robust generalization without explicit regularization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pmpbxmf8t3/cover.png"/></item><item><title>Towards the Dynamics of a DNN Learning Symbolic Interactions</title><link>https://deep-diver.github.io/neurips2024/posters/dihxwkjxre/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dihxwkjxre/</guid><description>DNNs learn interactions in two phases: initially removing complex interactions, then gradually learning higher-order ones, leading to overfitting.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dihxwkjxre/cover.png"/></item><item><title>Trace is the Next AutoDiff: Generative Optimization with Rich Feedback, Execution Traces, and LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/rys2dmn9td/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rys2dmn9td/</guid><description>Trace: Automating AI workflow design with LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rys2dmn9td/cover.png"/></item><item><title>Trading off Consistency and Dimensionality of Convex Surrogates for Multiclass Classification</title><link>https://deep-diver.github.io/neurips2024/posters/xcibvuxwpm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xcibvuxwpm/</guid><description>Researchers achieve a balance between accuracy and efficiency in multiclass classification by introducing partially consistent surrogate losses and novel methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xcibvuxwpm/cover.png"/></item><item><title>Trading Place for Space: Increasing Location Resolution Reduces Contextual Capacity in Hippocampal Codes</title><link>https://deep-diver.github.io/neurips2024/oral-ai-theory/reik4szmjt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-ai-theory/reik4szmjt/</guid><description>Boosting hippocampal spatial resolution surprisingly shrinks its contextual memory capacity, revealing a crucial trade-off between precision and context storage.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-ai-theory/reik4szmjt/cover.png"/></item><item><title>Treatment of Statistical Estimation Problems in Randomized Smoothing for Adversarial Robustness</title><link>https://deep-diver.github.io/neurips2024/posters/s4wx2qxhv9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s4wx2qxhv9/</guid><description>This paper optimizes randomized smoothing, a crucial certified defense against adversarial attacks, by introducing novel statistical methods that drastically reduce the computational cost, leading to &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/s4wx2qxhv9/cover.png"/></item><item><title>Truthfulness of Calibration Measures</title><link>https://deep-diver.github.io/neurips2024/posters/cda8hftygc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cda8hftygc/</guid><description>Researchers developed Subsampled Smooth Calibration Error (SSCE), a new truthful calibration measure for sequential prediction, solving the problem of existing measures being easily gamed.</description></item><item><title>Uncovering, Explaining, and Mitigating the Superficial Safety of Backdoor Defense</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/qzfshkbwdo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/qzfshkbwdo/</guid><description>Current backdoor defenses, while effective at reducing attack success rates, are vulnerable to rapid re-learning. This work unveils this superficial safety, proposes a novel attack, and introduces a p&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/qzfshkbwdo/cover.png"/></item><item><title>Universality of AdaGrad Stepsizes for Stochastic Optimization: Inexact Oracle, Acceleration and Variance Reduction</title><link>https://deep-diver.github.io/neurips2024/posters/rniiavjhi5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rniiavjhi5/</guid><description>Adaptive gradient methods using AdaGrad stepsizes achieve optimal convergence rates for convex composite optimization problems, handling inexact oracles, acceleration, and variance reduction without n&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rniiavjhi5/cover.png"/></item><item><title>User-Creator Feature Polarization in Recommender Systems with Dual Influence</title><link>https://deep-diver.github.io/neurips2024/posters/ywq89o19wf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ywq89o19wf/</guid><description>Recommender systems, when influenced by both users and creators, inevitably polarize; however, prioritizing efficiency through methods like top-k truncation can surprisingly enhance diversity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ywq89o19wf/cover.png"/></item><item><title>Validating Climate Models with Spherical Convolutional Wasserstein Distance</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/mmsffib6pi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/mmsffib6pi/</guid><description>Researchers developed Spherical Convolutional Wasserstein Distance (SCWD) to more accurately validate climate models by considering spatial variability and local distributional differences.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/mmsffib6pi/cover.png"/></item><item><title>Weisfeiler and Leman Go Loopy: A New Hierarchy for Graph Representational Learning</title><link>https://deep-diver.github.io/neurips2024/oral-ai-theory/9o2svnehor/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-ai-theory/9o2svnehor/</guid><description>This paper introduces r-lWL, a new graph isomorphism test hierarchy that surpasses the limitations of the Weisfeiler-Leman test by counting cycles up to length r+2, and its GNN counterpart, r-lMPNN, w&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-ai-theory/9o2svnehor/cover.png"/></item><item><title>What type of inference is planning?</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/txsrgrzicz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/txsrgrzicz/</guid><description>Planning is redefined as a distinct inference type within a variational framework, enabling efficient approximate planning in complex environments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/txsrgrzicz/cover.png"/></item><item><title>When Is Inductive Inference Possible?</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/2agcshccuv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/2agcshccuv/</guid><description>This paper provides a tight characterization of inductive inference, proving it&amp;rsquo;s possible if and only if the hypothesis class is a countable union of online learnable classes, resolving a long-standi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/2agcshccuv/cover.png"/></item><item><title>Zipper: Addressing Degeneracy in Algorithm-Agnostic Inference</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ahvohpkkmx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ahvohpkkmx/</guid><description>Zipper: A novel statistical device resolves the degeneracy issue in algorithm-agnostic inference, enabling reliable goodness-of-fit tests with enhanced power.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ahvohpkkmx/cover.png"/></item></channel></rss>