<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI Theory on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/ai-theory/</link><description>Recent content in AI Theory on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/ai-theory/index.xml" rel="self" type="application/rss+xml"/><item><title>2D-OOB: Attributing Data Contribution Through Joint Valuation Framework</title><link>https://deep-diver.github.io/neurips2024/posters/vbxeeh1x4y/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vbxeeh1x4y/</guid><description>2D-OOB: a novel framework for jointly attributing data values to individual features, enabling fine-grained outlier detection and improved model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vbxeeh1x4y/cover.png"/></item><item><title>4+3 Phases of Compute-Optimal Neural Scaling Laws</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/avsxwicpak/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/avsxwicpak/</guid><description>Researchers discovered four distinct compute-optimal phases for training neural networks, offering new predictions for resource-efficient large model training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/avsxwicpak/cover.png"/></item><item><title>A Boosting-Type Convergence Result for AdaBoost.MH with Factorized Multi-Class Classifiers</title><link>https://deep-diver.github.io/neurips2024/posters/7lv8zhqwws/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7lv8zhqwws/</guid><description>Solved a long-standing open problem: Factorized ADABOOST.MH now has a proven convergence rate!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7lv8zhqwws/cover.png"/></item><item><title>A Closer Look at AUROC and AUPRC under Class Imbalance</title><link>https://deep-diver.github.io/neurips2024/posters/s3hva808gk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s3hva808gk/</guid><description>Debunking a common myth, this paper proves that AUPRC is not superior to AUROC for imbalanced datasets, and in fact, can worsen algorithmic bias.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/s3hva808gk/cover.png"/></item><item><title>A Combinatorial Algorithm for the Semi-Discrete Optimal Transport Problem</title><link>https://deep-diver.github.io/neurips2024/posters/xq0jwbczkn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xq0jwbczkn/</guid><description>A new combinatorial algorithm dramatically speeds up semi-discrete optimal transport calculations, offering an efficient solution for large datasets and higher dimensions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xq0jwbczkn/cover.png"/></item><item><title>A Compositional Atlas for Algebraic Circuits</title><link>https://deep-diver.github.io/neurips2024/posters/mxlr1flfdc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mxlr1flfdc/</guid><description>This paper introduces a compositional framework for algebraic circuits, deriving novel tractability conditions for compositional inference queries and unifying existing results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mxlr1flfdc/cover.png"/></item><item><title>A Comprehensive Analysis on the Learning Curve in Kernel Ridge Regression</title><link>https://deep-diver.github.io/neurips2024/posters/imldpzmlnl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/imldpzmlnl/</guid><description>This study provides a unified theory for kernel ridge regression&amp;rsquo;s learning curve, improving existing bounds and validating the Gaussian Equivalence Property under minimal assumptions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/imldpzmlnl/cover.png"/></item><item><title>A Fast Convoluted Story: Scaling Probabilistic Inference for Integer Arithmetics</title><link>https://deep-diver.github.io/neurips2024/posters/ooirs6fim7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ooirs6fim7/</guid><description>Revolutionizing probabilistic inference, PLIA₁ uses tensor operations and FFT to scale integer arithmetic, achieving orders-of-magnitude speedup in inference and learning times.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ooirs6fim7/cover.png"/></item><item><title>A generalized neural tangent kernel for surrogate gradient learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/kfdexqu6mc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/kfdexqu6mc/</guid><description>Researchers introduce a generalized neural tangent kernel for analyzing surrogate gradient learning in neural networks with non-differentiable activation functions, providing a strong theoretical foun&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/kfdexqu6mc/cover.png"/></item><item><title>A Huber Loss Minimization Approach to Mean Estimation under User-level Differential Privacy</title><link>https://deep-diver.github.io/neurips2024/posters/tutginejzz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tutginejzz/</guid><description>Huber loss minimization ensures accurate and robust mean estimation under user-level differential privacy, especially for imbalanced datasets and heavy-tailed distributions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tutginejzz/cover.png"/></item><item><title>A Neural Network Approach for Efficiently Answering Most Probable Explanation Queries in Probabilistic Models</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ufppf9ghzp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ufppf9ghzp/</guid><description>A novel neural network efficiently answers arbitrary Most Probable Explanation (MPE) queries in large probabilistic models, eliminating the need for slow inference algorithms.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ufppf9ghzp/cover.png"/></item><item><title>A Non-parametric Direct Learning Approach to Heterogeneous Treatment Effect Estimation under Unmeasured Confounding</title><link>https://deep-diver.github.io/neurips2024/posters/bwluqsqumh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bwluqsqumh/</guid><description>Estimating heterogeneous treatment effects (CATE) under unmeasured confounding is revolutionized by a novel non-parametric direct learning approach using instrumental variables, offering efficient and&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bwluqsqumh/cover.png"/></item><item><title>A Primal-Dual-Assisted Penalty Approach to Bilevel Optimization with Coupled Constraints</title><link>https://deep-diver.github.io/neurips2024/posters/uzi7h5ac0x/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uzi7h5ac0x/</guid><description>BLOCC, a novel first-order algorithm, efficiently solves bilevel optimization problems with coupled constraints, offering improved scalability and convergence for machine learning applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uzi7h5ac0x/cover.png"/></item><item><title>A provable control of sensitivity of neural networks through a direct parameterization of the overall bi-Lipschitzness</title><link>https://deep-diver.github.io/neurips2024/posters/ww62xltefb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ww62xltefb/</guid><description>New framework directly controls neural network sensitivity by precisely parameterizing overall bi-Lipschitzness, offering improved robustness and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ww62xltefb/cover.png"/></item><item><title>A Separation in Heavy-Tailed Sampling: Gaussian vs. Stable Oracles for Proximal Samplers</title><link>https://deep-diver.github.io/neurips2024/posters/zuwlghgxtq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zuwlghgxtq/</guid><description>Stable oracles outperform Gaussian oracles in high-accuracy heavy-tailed sampling, overcoming limitations of Gaussian-based proximal samplers.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zuwlghgxtq/cover.png"/></item><item><title>A Simple and Adaptive Learning Rate for FTRL in Online Learning with Minimax Regret of Θ(T^{2/3}) and its Application to Best-of-Both-Worlds</title><link>https://deep-diver.github.io/neurips2024/posters/xlvuz9f50g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xlvuz9f50g/</guid><description>A new adaptive learning rate for FTRL achieves minimax regret of O(T²/³) in online learning, improving existing best-of-both-worlds algorithms for various hard problems.</description></item><item><title>A Simple and Optimal Approach for Universal Online Learning with Gradient Variations</title><link>https://deep-diver.github.io/neurips2024/posters/yo5dvychzr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yo5dvychzr/</guid><description>A novel universal online learning algorithm achieves optimal gradient-variation regret across diverse function curvatures, boasting efficiency with only one gradient query per round.</description></item><item><title>A Simple Remedy for Dataset Bias via Self-Influence: A Mislabeled Sample Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/zvrrpnqhfw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zvrrpnqhfw/</guid><description>This paper introduces Bias-Conditioned Self-Influence (BCSI) for precise bias-conflicting sample detection and model rectification, enhancing fairness in machine learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zvrrpnqhfw/cover.png"/></item><item><title>A Simple yet Scalable Granger Causal Structural Learning Approach for Topological Event Sequences</title><link>https://deep-diver.github.io/neurips2024/posters/mp084amfsd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mp084amfsd/</guid><description>S²GCSL: A novel scalable Granger causal structural learning approach efficiently identifies root causes of telecommunication network alarms by leveraging a linear kernel and incorporating expert knowl&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mp084amfsd/cover.png"/></item><item><title>A theoretical design of concept sets: improving the predictability of concept bottleneck models</title><link>https://deep-diver.github.io/neurips2024/posters/otv6qa12g0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/otv6qa12g0/</guid><description>Boosting concept bottleneck model predictability, this paper introduces a theoretical framework linking concept set properties to model performance, proposing a method for effective concept identifica&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/otv6qa12g0/cover.png"/></item><item><title>A Theory of Optimistically Universal Online Learnability for General Concept Classes</title><link>https://deep-diver.github.io/neurips2024/posters/eabnopo3os/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/eabnopo3os/</guid><description>This paper fully characterizes concept classes optimistically universally learnable online, introducing novel algorithms and revealing equivalences between agnostic and realizable settings.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/eabnopo3os/cover.png"/></item><item><title>A Unifying Post-Processing Framework for Multi-Objective Learn-to-Defer Problems</title><link>https://deep-diver.github.io/neurips2024/posters/mtsi1eddbh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mtsi1eddbh/</guid><description>A novel post-processing framework, based on a d-dimensional generalization of the Neyman-Pearson lemma, optimally solves multi-objective learn-to-defer problems under various constraints, improving co&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mtsi1eddbh/cover.png"/></item><item><title>A Universal Growth Rate for Learning with Smooth Surrogate Losses</title><link>https://deep-diver.github.io/neurips2024/posters/itztwtacn6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/itztwtacn6/</guid><description>This paper reveals a universal square-root growth rate for H-consistency bounds of smooth surrogate losses in classification, significantly advancing our understanding of loss function selection.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/itztwtacn6/cover.png"/></item><item><title>A Walsh Hadamard Derived Linear Vector Symbolic Architecture</title><link>https://deep-diver.github.io/neurips2024/posters/p3hnrpewme/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/p3hnrpewme/</guid><description>Hadamard-derived Linear Binding (HLB): A novel, efficient vector symbolic architecture surpassing existing methods in classical AI tasks and deep learning applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/p3hnrpewme/cover.png"/></item><item><title>Abductive Reasoning in Logical Credal Networks</title><link>https://deep-diver.github.io/neurips2024/posters/glxuxni6tn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/glxuxni6tn/</guid><description>This paper presents efficient algorithms for abductive reasoning in Logical Credal Networks (LCNs), addressing the MAP and Marginal MAP inference tasks to enable scalable solutions for complex real-wo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/glxuxni6tn/cover.png"/></item><item><title>Accelerated Regularized Learning in Finite N-Person Games</title><link>https://deep-diver.github.io/neurips2024/posters/lw2zyqm0ox/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lw2zyqm0ox/</guid><description>Accelerated learning in games achieved! FTXL algorithm exponentially speeds up convergence to Nash equilibria in finite N-person games, even under limited feedback.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lw2zyqm0ox/cover.png"/></item><item><title>Accelerating ERM for data-driven algorithm design using output-sensitive techniques</title><link>https://deep-diver.github.io/neurips2024/posters/yw3tlswusb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yw3tlswusb/</guid><description>Accelerating ERM for data-driven algorithm design using output-sensitive techniques achieves computationally efficient learning by scaling with the actual number of pieces in the dual loss function, n&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yw3tlswusb/cover.png"/></item><item><title>Accelerating Matroid Optimization through Fast Imprecise Oracles</title><link>https://deep-diver.github.io/neurips2024/posters/0qb8kopsej/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0qb8kopsej/</guid><description>Fast imprecise oracles drastically reduce query times in matroid optimization, achieving near-optimal performance with few accurate queries.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0qb8kopsej/cover.png"/></item><item><title>Accelerating Nash Equilibrium Convergence in Monte Carlo Settings Through Counterfactual Value Based Fictitious Play</title><link>https://deep-diver.github.io/neurips2024/posters/efd9n5zdfc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/efd9n5zdfc/</guid><description>MCCFVFP, a novel Monte Carlo-based algorithm, accelerates Nash equilibrium convergence in large-scale games by combining CFR&amp;rsquo;s counterfactual value calculations with fictitious play&amp;rsquo;s best response st&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/efd9n5zdfc/cover.png"/></item><item><title>Acceleration Exists! Optimization Problems When Oracle Can Only Compare Objective Function Values</title><link>https://deep-diver.github.io/neurips2024/posters/kxbsnewb42/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kxbsnewb42/</guid><description>Accelerated gradient-free optimization is achieved using only function value comparisons, significantly improving black-box optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kxbsnewb42/cover.png"/></item><item><title>Achievable distributional robustness when the robust risk is only partially identified</title><link>https://deep-diver.github.io/neurips2024/posters/g2dyzjo4be/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/g2dyzjo4be/</guid><description>This paper introduces a novel framework for evaluating the robustness of machine learning models when the true data distribution is only partially known. It defines a new risk measure (&amp;lsquo;identifiable r&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/g2dyzjo4be/cover.png"/></item><item><title>Achievable Fairness on Your Data With Utility Guarantees</title><link>https://deep-diver.github.io/neurips2024/posters/gtemizlzmr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gtemizlzmr/</guid><description>This paper introduces a computationally efficient method to approximate the optimal accuracy-fairness trade-off curve for various datasets, providing rigorous statistical guarantees and quantifying un&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gtemizlzmr/cover.png"/></item><item><title>Achieving Domain-Independent Certified Robustness via Knowledge Continuity</title><link>https://deep-diver.github.io/neurips2024/posters/v07krlyxdx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v07krlyxdx/</guid><description>Certifying neural network robustness across diverse domains, this paper introduces knowledge continuity—a novel framework ensuring model stability independent of input type, norms, and distribution.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v07krlyxdx/cover.png"/></item><item><title>Adam with model exponential moving average is effective for nonconvex optimization</title><link>https://deep-diver.github.io/neurips2024/posters/v416yloquu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v416yloquu/</guid><description>Clipped Adam with EMA achieves optimal convergence rates for smooth and non-smooth nonconvex optimization, particularly when scales vary across different coordinates.</description></item><item><title>Adaptive and Optimal Second-order Optimistic Methods for Minimax Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/nvdygefxcy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nvdygefxcy/</guid><description>New adaptive second-order optimistic methods for minimax optimization achieve optimal convergence without line search, simplifying updates and improving efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nvdygefxcy/cover.png"/></item><item><title>Adaptive Experimentation When You Can't Experiment</title><link>https://deep-diver.github.io/neurips2024/posters/2mqitijkrx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2mqitijkrx/</guid><description>Adaptive experimentation tackles confounding in online A/B tests using encouragement designs and a novel linear bandit approach, achieving near-optimal sample complexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2mqitijkrx/cover.png"/></item><item><title>Adaptive Proximal Gradient Method for Convex Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/qlh21ig1ic/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qlh21ig1ic/</guid><description>Adaptive gradient descent methods are improved by leveraging local curvature information for entirely adaptive algorithms without added computational cost, proving convergence with only local Lipschit&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qlh21ig1ic/cover.png"/></item><item><title>Addressing Bias in Online Selection with Limited Budget of Comparisons</title><link>https://deep-diver.github.io/neurips2024/posters/bdgfgkrlhl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bdgfgkrlhl/</guid><description>This paper introduces efficient algorithms for online selection with a budget constraint when comparing candidates from different groups has a cost, improving fairness and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bdgfgkrlhl/cover.png"/></item><item><title>Adjust Pearson's $r$ to Measure Arbitrary Monotone Dependence</title><link>https://deep-diver.github.io/neurips2024/posters/8dkz60ygfj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8dkz60ygfj/</guid><description>Researchers refine Pearson&amp;rsquo;s correlation coefficient to precisely measure arbitrary monotone dependence, expanding its applicability beyond linear relationships.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8dkz60ygfj/cover.png"/></item><item><title>Adversarially Robust Decision Transformer</title><link>https://deep-diver.github.io/neurips2024/posters/wef2lt8nty/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wef2lt8nty/</guid><description>Adversarially Robust Decision Transformer (ARDT) enhances offline RL robustness against powerful adversaries by conditioning policies on minimax returns, achieving superior worst-case performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wef2lt8nty/cover.png"/></item><item><title>Adversarially Robust Dense-Sparse Tradeoffs via Heavy-Hitters</title><link>https://deep-diver.github.io/neurips2024/posters/mpidscd9e7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mpidscd9e7/</guid><description>Improved adversarially robust streaming algorithms for L_p estimation are presented, surpassing previous state-of-the-art space bounds and disproving the existence of inherent barriers.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mpidscd9e7/cover.png"/></item><item><title>Aggregating Quantitative Relative Judgments: From Social Choice to Ranking Prediction</title><link>https://deep-diver.github.io/neurips2024/posters/37cya1k0vv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/37cya1k0vv/</guid><description>This paper introduces Quantitative Relative Judgment Aggregation (QRJA), a novel social choice model, and applies it to ranking prediction, yielding effective and interpretable results on various real&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/37cya1k0vv/cover.png"/></item><item><title>Aligning Model Properties via Conformal Risk Control</title><link>https://deep-diver.github.io/neurips2024/posters/9ohxqybmzb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/9ohxqybmzb/</guid><description>Post-processing pre-trained models for alignment using conformal risk control and property testing guarantees better alignment, even when training data is biased.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/9ohxqybmzb/cover.png"/></item><item><title>Almost Free: Self-concordance in Natural Exponential Families and an Application to Bandits</title><link>https://deep-diver.github.io/neurips2024/posters/lkwvyvx66i/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lkwvyvx66i/</guid><description>Generalized linear bandits with subexponential reward distributions are self-concordant, enabling second-order regret bounds free of exponential dependence on problem parameters.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lkwvyvx66i/cover.png"/></item><item><title>Almost Surely Asymptotically Constant Graph Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/dn68qdftry/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dn68qdftry/</guid><description>Many graph neural networks (GNNs) surprisingly converge to constant outputs with increasing graph size, limiting their expressiveness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dn68qdftry/cover.png"/></item><item><title>Almost-Linear RNNs Yield Highly Interpretable Symbolic Codes in Dynamical Systems Reconstruction</title><link>https://deep-diver.github.io/neurips2024/posters/sepsxteekj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sepsxteekj/</guid><description>Almost-linear RNNs (AL-RNNs) offer highly interpretable symbolic codes for dynamical systems reconstruction, simplifying the analysis of complex systems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sepsxteekj/cover.png"/></item><item><title>Amortized Eigendecomposition for Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/oyokkqrlvj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oyokkqrlvj/</guid><description>Accelerate neural network training using &amp;lsquo;amortized eigendecomposition&amp;rsquo; – a novel method replacing expensive eigendecomposition with faster QR decomposition while preserving accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oyokkqrlvj/cover.png"/></item><item><title>An Analysis of Elo Rating Systems via Markov Chains</title><link>https://deep-diver.github.io/neurips2024/posters/kliwxudcew/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kliwxudcew/</guid><description>Elo rating system&amp;rsquo;s convergence rigorously analyzed via Markov chains under the Bradley-Terry-Luce model, demonstrating competitive learning rates and informing efficient tournament design.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kliwxudcew/cover.png"/></item><item><title>An effective framework for estimating individualized treatment rules</title><link>https://deep-diver.github.io/neurips2024/posters/g7l65b2p0y/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/g7l65b2p0y/</guid><description>This paper introduces a unified ITR estimation framework using covariate balancing weights, achieving significant gains in robustness and effectiveness compared to existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/g7l65b2p0y/cover.png"/></item><item><title>An Efficient High-dimensional Gradient Estimator for Stochastic Differential Equations</title><link>https://deep-diver.github.io/neurips2024/posters/780uxna4wn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/780uxna4wn/</guid><description>New unbiased gradient estimator for high-dimensional SDEs drastically reduces computation time without sacrificing estimation accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/780uxna4wn/cover.png"/></item><item><title>An engine not a camera: Measuring performative power of online search</title><link>https://deep-diver.github.io/neurips2024/posters/hqfcrtbhed/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hqfcrtbhed/</guid><description>New research quantifies how search engines steer web traffic by subtly changing results, offering a powerful method for antitrust investigations and digital market analysis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hqfcrtbhed/cover.png"/></item><item><title>An Equivalence Between Static and Dynamic Regret Minimization</title><link>https://deep-diver.github.io/neurips2024/posters/hd8et4uz1o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hd8et4uz1o/</guid><description>Dynamic regret minimization equals static regret in an extended space; this equivalence reveals a trade-off between loss variance and comparator variability, leading to a new algorithm achieving impro&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hd8et4uz1o/cover.png"/></item><item><title>An In-depth Investigation of Sparse Rate Reduction in Transformer-like Models</title><link>https://deep-diver.github.io/neurips2024/posters/cac74vumwx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cac74vumwx/</guid><description>Deep learning model interpretability improved via Sparse Rate Reduction (SRR), showing improved generalization and offering principled model design.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cac74vumwx/cover.png"/></item><item><title>Analytically deriving Partial Information Decomposition for affine systems of stable and convolution-closed distributions</title><link>https://deep-diver.github.io/neurips2024/posters/7cuutpdeqn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7cuutpdeqn/</guid><description>This paper presents novel theoretical results enabling the analytical calculation of Partial Information Decomposition for various probability distributions, including those relevant to neuroscience, &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7cuutpdeqn/cover.png"/></item><item><title>Approximating mutual information of high-dimensional variables using learned representations</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/hn05dqxyll/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/hn05dqxyll/</guid><description>Latent Mutual Information (LMI) approximation accurately estimates mutual information in high-dimensional data using low-dimensional learned representations, solving a critical problem in various scie&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/hn05dqxyll/cover.png"/></item><item><title>Approximating the Top Eigenvector in Random Order Streams</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/gitgmieinf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/gitgmieinf/</guid><description>Random-order stream data necessitates efficient top eigenvector approximation; this paper presents novel algorithms with improved space complexity, achieving near-optimal bounds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/gitgmieinf/cover.png"/></item><item><title>Are Graph Neural Networks Optimal Approximation Algorithms?</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/sxrblm9ams/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/sxrblm9ams/</guid><description>Graph Neural Networks (GNNs) learn optimal approximation algorithms for combinatorial optimization problems, achieving high-quality solutions for Max-Cut, Min-Vertex-Cover, and Max-3-SAT, while also p&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/sxrblm9ams/cover.png"/></item><item><title>Are High-Degree Representations Really Unnecessary in Equivariant Graph Neural Networks?</title><link>https://deep-diver.github.io/neurips2024/posters/m0ncnvugyn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/m0ncnvugyn/</guid><description>High-degree representations significantly boost the expressiveness of E(3)-equivariant GNNs, overcoming limitations of lower-degree models on symmetric structures, as demonstrated theoretically and em&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/m0ncnvugyn/cover.png"/></item><item><title>Are Your Models Still Fair? Fairness Attacks on Graph Neural Networks via Node Injections</title><link>https://deep-diver.github.io/neurips2024/posters/luqrikguru/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/luqrikguru/</guid><description>Node Injection-based Fairness Attack (NIFA) reveals GNNs&amp;rsquo; vulnerability to realistic fairness attacks by injecting a small percentage of nodes, significantly undermining fairness even in fairness-awar&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/luqrikguru/cover.png"/></item><item><title>Attack-Aware Noise Calibration for Differential Privacy</title><link>https://deep-diver.github.io/neurips2024/posters/hocsuroy0d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hocsuroy0d/</guid><description>Boosting machine learning model accuracy in privacy-preserving applications, this research introduces novel noise calibration methods directly targeting desired attack risk levels, bypassing conventio&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hocsuroy0d/cover.png"/></item><item><title>Auditing Local Explanations is Hard</title><link>https://deep-diver.github.io/neurips2024/posters/ybmrn4tdn0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ybmrn4tdn0/</guid><description>Auditing local explanations is surprisingly hard: proving explanation trustworthiness requires far more data than previously thought, especially in high dimensions, challenging current AI explainabil&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ybmrn4tdn0/cover.png"/></item><item><title>Auditing Privacy Mechanisms via Label Inference Attacks</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ai76atrb2y/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ai76atrb2y/</guid><description>New metrics audit label privatization, revealing differentially private schemes often outperform heuristic methods in the privacy-utility tradeoff.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ai76atrb2y/cover.png"/></item><item><title>Autobidder's Dilemma: Why More Sophisticated Autobidders Lead to Worse Auction Efficiency</title><link>https://deep-diver.github.io/neurips2024/posters/hqjksiskaa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hqjksiskaa/</guid><description>More sophisticated autobidders surprisingly worsen online auction efficiency; a fine-grained analysis reveals that less powerful, uniform bidders lead to better market outcomes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hqjksiskaa/cover.png"/></item><item><title>Automated Efficient Estimation using Monte Carlo Efficient Influence Functions</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/2wfd3pti8v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/2wfd3pti8v/</guid><description>MC-EIF automates efficient statistical estimation for high-dimensional models, integrating seamlessly with existing differentiable probabilistic programming systems and achieving optimal convergence r&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/2wfd3pti8v/cover.png"/></item><item><title>Automatic Outlier Rectification via Optimal Transport</title><link>https://deep-diver.github.io/neurips2024/posters/udxhmgjvjb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/udxhmgjvjb/</guid><description>This study presents a novel single-step outlier rectification method using optimal transport with a concave cost function, surpassing the limitations of conventional two-stage approaches by jointly op&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/udxhmgjvjb/cover.png"/></item><item><title>Automating Data Annotation under Strategic Human Agents: Risks and Potential Solutions</title><link>https://deep-diver.github.io/neurips2024/posters/2ujlv3kpgo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2ujlv3kpgo/</guid><description>AI models retraining with model-annotated data incorporating human strategic responses can lead to unexpected outcomes, potentially reducing the proportion of agents with positive labels over time, wh&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2ujlv3kpgo/cover.png"/></item><item><title>Average gradient outer product as a mechanism for deep neural collapse</title><link>https://deep-diver.github.io/neurips2024/posters/vtrotud539/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vtrotud539/</guid><description>Deep Neural Collapse (DNC) explained via Average Gradient Outer Product (AGOP).</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vtrotud539/cover.png"/></item><item><title>Axioms for AI Alignment from Human Feedback</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/cmbjkpruvw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/cmbjkpruvw/</guid><description>This paper revolutionizes AI alignment by applying social choice theory axioms to RLHF, exposing flaws in existing methods and proposing novel, axiomatically guaranteed reward learning rules.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/cmbjkpruvw/cover.png"/></item><item><title>B-cosification: Transforming Deep Neural Networks to be Inherently Interpretable</title><link>https://deep-diver.github.io/neurips2024/posters/ta5zpfh8ii/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ta5zpfh8ii/</guid><description>B-cosification: cheaply transform any pre-trained deep neural network into an inherently interpretable model.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ta5zpfh8ii/cover.png"/></item><item><title>Back to the Continuous Attractor</title><link>https://deep-diver.github.io/neurips2024/posters/fvg6zhrh0b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fvg6zhrh0b/</guid><description>Despite their brittleness, continuous attractors remain functionally robust analog memory models due to persistent slow manifolds surviving bifurcations, enabling accurate approximation and generaliza&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fvg6zhrh0b/cover.png"/></item><item><title>Banded Square Root Matrix Factorization for Differentially Private Model Training</title><link>https://deep-diver.github.io/neurips2024/posters/ksytvgosrx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ksytvgosrx/</guid><description>This paper introduces BSR, a novel banded square root matrix factorization for differentially private model training. Unlike existing methods, BSR avoids computationally expensive optimization, enabli&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ksytvgosrx/cover.png"/></item><item><title>Bandit-Feedback Online Multiclass Classification: Variants and Tradeoffs</title><link>https://deep-diver.github.io/neurips2024/posters/90ipkvvdxd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/90ipkvvdxd/</guid><description>This paper reveals the optimal mistake bounds for online multiclass classification under bandit feedback, showing the cost of limited feedback is at most O(k) times higher than full information, where&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/90ipkvvdxd/cover.png"/></item><item><title>Barely Random Algorithms and Collective Metrical Task Systems</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/oajhfvrtbq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/oajhfvrtbq/</guid><description>Randomness-efficient algorithms are developed for online decision making, requiring only 2log n random bits and achieving near-optimal competitiveness for metrical task systems.</description></item><item><title>Bayes-optimal learning of an extensive-width neural network from quadratically many samples</title><link>https://deep-diver.github.io/neurips2024/posters/r8znyrjxj3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/r8znyrjxj3/</guid><description>This study solves a key challenge in neural network learning, deriving a closed-form expression for the Bayes-optimal test error of extensive-width networks with quadratic activation functions from qu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/r8znyrjxj3/cover.png"/></item><item><title>Bayesian Nonparametrics Meets Data-Driven Distributionally Robust Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/8cgupoe3tp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8cgupoe3tp/</guid><description>Boost machine learning model robustness by minimizing a novel data-driven risk criterion that blends Bayesian nonparametrics and smooth ambiguity aversion, ensuring superior out-of-sample performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8cgupoe3tp/cover.png"/></item><item><title>Bayesian Optimization of Functions over Node Subsets in Graphs</title><link>https://deep-diver.github.io/neurips2024/posters/kxjgi1krbi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kxjgi1krbi/</guid><description>GraphComBO efficiently optimizes functions defined on node subsets within graphs using Bayesian Optimization. It tackles challenges posed by combinatorial complexity and computationally expensive fun&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kxjgi1krbi/cover.png"/></item><item><title>Bayesian Strategic Classification</title><link>https://deep-diver.github.io/neurips2024/posters/sadbrpog2k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sadbrpog2k/</guid><description>Learners can improve accuracy in strategic classification by selectively revealing partial classifier information to agents, strategically guiding agent behavior and maximizing accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sadbrpog2k/cover.png"/></item><item><title>Benign overfitting in leaky ReLU networks with moderate input dimension</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/88tzdgypt6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/88tzdgypt6/</guid><description>Leaky ReLU networks exhibit benign overfitting under surprisingly relaxed conditions: input dimension only needs to linearly scale with sample size, challenging prior assumptions in the field.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/88tzdgypt6/cover.png"/></item><item><title>Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?</title><link>https://deep-diver.github.io/neurips2024/posters/kyhma7hzjr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kyhma7hzjr/</guid><description>This paper presents a novel method to make black box neural networks intervenable using only a small validation set with concept labels, improving the effectiveness of concept-based interventions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kyhma7hzjr/cover.png"/></item><item><title>Beyond Primal-Dual Methods in Bandits with Stochastic and Adversarial Constraints</title><link>https://deep-diver.github.io/neurips2024/posters/ijgwd5mwyg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ijgwd5mwyg/</guid><description>This paper presents a novel, UCB-like algorithm for bandits with stochastic and adversarial constraints, achieving optimal performance without the stringent assumptions of prior primal-dual methods.</description></item><item><title>Bias Detection via Signaling</title><link>https://deep-diver.github.io/neurips2024/posters/4d7hah4pdr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4d7hah4pdr/</guid><description>This paper presents efficient algorithms to detect whether an agent updates beliefs optimally (Bayesian) or exhibits bias towards their prior beliefs, using information design and signaling schemes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4d7hah4pdr/cover.png"/></item><item><title>Bias in Motion: Theoretical Insights into the Dynamics of Bias in SGD Training</title><link>https://deep-diver.github.io/neurips2024/posters/quylbzwttv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/quylbzwttv/</guid><description>AI systems acquire bias during training, impacting accuracy across sub-populations. This research unveils bias&amp;rsquo;s dynamic nature, revealing how classifier preferences shift over time, influenced by dat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/quylbzwttv/cover.png"/></item><item><title>Binary Search with Distributional Predictions</title><link>https://deep-diver.github.io/neurips2024/posters/jekxtljeiq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jekxtljeiq/</guid><description>This paper presents a novel algorithm for binary search using distributional predictions, achieving optimal query complexity O(H(p) + log n) and demonstrating enhanced robustness against prediction er&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jekxtljeiq/cover.png"/></item><item><title>Binding in hippocampal-entorhinal circuits enables compositionality in cognitive maps</title><link>https://deep-diver.github.io/neurips2024/posters/jo6t4rej32/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jo6t4rej32/</guid><description>A novel model reveals how hippocampal-entorhinal circuits use compositional coding and modular attractor networks to enable robust and flexible spatial representation, advancing our understanding of c&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jo6t4rej32/cover.png"/></item><item><title>Bisimulation Metrics are Optimal Transport Distances, and Can be Computed Efficiently</title><link>https://deep-diver.github.io/neurips2024/posters/csjvsnvtbg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/csjvsnvtbg/</guid><description>Bisimulation metrics and optimal transport distances are equivalent and can be computed efficiently using a novel Sinkhorn Value Iteration algorithm.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/csjvsnvtbg/cover.png"/></item><item><title>Boundary Decomposition for Nadir Objective Vector Estimation</title><link>https://deep-diver.github.io/neurips2024/posters/f829mkqmug/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/f829mkqmug/</guid><description>BDNE: a novel boundary decomposition method accurately estimates the nadir objective vector in complex multi-objective optimization problems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/f829mkqmug/cover.png"/></item><item><title>Bounds for the smallest eigenvalue of the NTK for arbitrary spherical data of arbitrary dimension</title><link>https://deep-diver.github.io/neurips2024/posters/mhvmsy9len/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mhvmsy9len/</guid><description>This paper delivers novel, universally applicable bounds for the smallest NTK eigenvalue, regardless of data distribution or dimension, leveraging the hemisphere transform.</description></item><item><title>Bridging Multicalibration and Out-of-distribution Generalization Beyond Covariate Shift</title><link>https://deep-diver.github.io/neurips2024/posters/bos6wpv0jf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bos6wpv0jf/</guid><description>New model-agnostic framework for out-of-distribution generalization uses multicalibration across overlapping groups, showing improved robustness and prediction under various distribution shifts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bos6wpv0jf/cover.png"/></item><item><title>Building a stable classifier with the inflated argmax</title><link>https://deep-diver.github.io/neurips2024/posters/m7znxntzsp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/m7znxntzsp/</guid><description>Boost classifier stability with the novel inflated argmax, guaranteeing reliable multiclass classification without distributional assumptions!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/m7znxntzsp/cover.png"/></item><item><title>Can an AI Agent Safely Run a Government? Existence of Probably Approximately Aligned Policies</title><link>https://deep-diver.github.io/neurips2024/posters/xm5m7j6lbl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xm5m7j6lbl/</guid><description>This paper introduces a novel quantitative definition of AI alignment for social decision-making, proposing probably approximately aligned policies and a method to safeguard any autonomous agent&amp;rsquo;s act&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xm5m7j6lbl/cover.png"/></item><item><title>Can Graph Neural Networks Expose Training Data Properties? An Efficient Risk Assessment Approach</title><link>https://deep-diver.github.io/neurips2024/posters/luxk3z1tsg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/luxk3z1tsg/</guid><description>New efficient attack reveals GNN model training data properties.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/luxk3z1tsg/cover.png"/></item><item><title>Can neural operators always be continuously discretized?</title><link>https://deep-diver.github.io/neurips2024/posters/cyjxphdw3b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cyjxphdw3b/</guid><description>Neural operators&amp;rsquo; continuous discretization is proven impossible in general Hilbert spaces, but achievable using strongly monotone operators, opening new avenues for numerical methods in scientific ma&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cyjxphdw3b/cover.png"/></item><item><title>Can Transformers Smell Like Humans?</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3f8i9glbzu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3f8i9glbzu/</guid><description>Pre-trained transformer models can predict human smell perception by encoding odorant chemical structures, aligning with expert labels, continuous ratings, and similarity assessments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3f8i9glbzu/cover.png"/></item><item><title>Causal Dependence Plots</title><link>https://deep-diver.github.io/neurips2024/posters/pu0z2snm1m/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pu0z2snm1m/</guid><description>Causal Dependence Plots (CDPs) visualize how machine learning model predictions causally depend on input features, overcoming limitations of existing methods that ignore causal relationships.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pu0z2snm1m/cover.png"/></item><item><title>Causal Discovery from Event Sequences by Local Cause-Effect Attribution</title><link>https://deep-diver.github.io/neurips2024/posters/y9zirxshzj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y9zirxshzj/</guid><description>CASCADE algorithm unveils hidden causal structures in event sequences by minimizing description length, surpassing existing Granger causality-based methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y9zirxshzj/cover.png"/></item><item><title>Causal Effect Identification in a Sub-Population with Latent Variables</title><link>https://deep-diver.github.io/neurips2024/posters/iesyrsg6t1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/iesyrsg6t1/</guid><description>This paper introduces a novel algorithm to accurately compute causal effects within specific sub-populations, even when hidden factors influence the data, advancing causal inference significantly.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/iesyrsg6t1/cover.png"/></item><item><title>Causal Inference in the Closed-Loop: Marginal Structural Models for Sequential Excursion Effects</title><link>https://deep-diver.github.io/neurips2024/posters/bgzcuesyu8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bgzcuesyu8/</guid><description>Researchers introduce a non-parametric causal inference framework to analyze closed-loop optogenetics designs, revealing previously hidden causal effects of neural circuit manipulations on behavior.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bgzcuesyu8/cover.png"/></item><item><title>Causal Temporal Representation Learning with Nonstationary Sparse Transition</title><link>https://deep-diver.github.io/neurips2024/posters/j709rtaud1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/j709rtaud1/</guid><description>CtrlNS: A novel framework for causal temporal representation learning tackles the challenge of nonstationary time series by leveraging sparse transition assumptions, achieving improved accuracy in ide&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/j709rtaud1/cover.png"/></item><item><title>Causal vs. Anticausal merging of predictors</title><link>https://deep-diver.github.io/neurips2024/posters/xzkxgvlb0c/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xzkxgvlb0c/</guid><description>Causal assumptions drastically alter predictor merging, with CMAXENT revealing logistic regression for causal and LDA for anticausal directions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xzkxgvlb0c/cover.png"/></item><item><title>CausalDiff: Causality-Inspired Disentanglement via Diffusion Model for Adversarial Defense</title><link>https://deep-diver.github.io/neurips2024/posters/bzldxbjb8o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bzldxbjb8o/</guid><description>CausalDiff leverages causal inference and diffusion models to create a robust AI defense against unseen adversarial attacks, significantly outperforming state-of-the-art methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bzldxbjb8o/cover.png"/></item><item><title>Certified Machine Unlearning via Noisy Stochastic Gradient Descent</title><link>https://deep-diver.github.io/neurips2024/posters/h3k2nxu5bj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/h3k2nxu5bj/</guid><description>This paper introduces a novel machine unlearning method using projected noisy stochastic gradient descent, providing the first approximate unlearning guarantee under convexity, significantly improving&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/h3k2nxu5bj/cover.png"/></item><item><title>Certified Robustness for Deep Equilibrium Models via Serialized Random Smoothing</title><link>https://deep-diver.github.io/neurips2024/posters/x64ijvdftr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x64ijvdftr/</guid><description>Accelerate DEQ certification up to 7x with Serialized Random Smoothing (SRS), achieving certified robustness on large-scale datasets without sacrificing accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x64ijvdftr/cover.png"/></item><item><title>Challenges of Generating Structurally Diverse Graphs</title><link>https://deep-diver.github.io/neurips2024/posters/bbgpol1nlo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bbgpol1nlo/</guid><description>Researchers developed novel algorithms to generate structurally diverse graphs, improving graph algorithm testing and neural network evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bbgpol1nlo/cover.png"/></item><item><title>ChronoEpilogi: Scalable Time Series Selection with Multiple Solutions</title><link>https://deep-diver.github.io/neurips2024/posters/y8huxkwaog/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y8huxkwaog/</guid><description>ChronoEpilogi efficiently finds all minimal sets of time-series variables optimally predicting a target, improving forecasting while providing crucial insights for knowledge discovery and causal model&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y8huxkwaog/cover.png"/></item><item><title>Class Distribution Shifts in Zero-Shot Learning: Learning Robust Representations</title><link>https://deep-diver.github.io/neurips2024/posters/yuqubgiobg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yuqubgiobg/</guid><description>Zero-shot learning models often fail in real-world scenarios due to unseen class distribution shifts. This work introduces a novel algorithm that learns robust representations by creating synthetic d&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yuqubgiobg/cover.png"/></item><item><title>Clustering in Causal Attention Masking</title><link>https://deep-diver.github.io/neurips2024/posters/oivxyf9trg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oivxyf9trg/</guid><description>Researchers strengthen understanding of transformer self-attention by proving asymptotic convergence to single clusters under causal masking, linking it to the Rényi parking problem.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oivxyf9trg/cover.png"/></item><item><title>Coded Computing for Resilient Distributed Computing: A Learning-Theoretic Framework</title><link>https://deep-diver.github.io/neurips2024/posters/9xdyeebrv6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/9xdyeebrv6/</guid><description>LeTCC: A novel learning-theoretic framework for resilient distributed computing, achieving faster convergence and higher accuracy than existing methods by integrating learning theory principles with c&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/9xdyeebrv6/cover.png"/></item><item><title>Coherence-free Entrywise Estimation of Eigenvectors in Low-rank Signal-plus-noise Matrix Models</title><link>https://deep-diver.github.io/neurips2024/posters/ciuh7zobcq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ciuh7zobcq/</guid><description>New method for eigenvector estimation achieves optimal rates without coherence dependence, improving low-rank matrix denoising and related tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ciuh7zobcq/cover.png"/></item><item><title>Collaboration! Towards Robust Neural Methods for Routing Problems</title><link>https://deep-diver.github.io/neurips2024/posters/yfqa78gefa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yfqa78gefa/</guid><description>A novel Collaborative Neural Framework (CNF) enhances the robustness of neural vehicle routing methods against adversarial attacks by collaboratively training multiple models and intelligently distrib&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yfqa78gefa/cover.png"/></item><item><title>Communication Bounds for the Distributed Experts Problem</title><link>https://deep-diver.github.io/neurips2024/posters/hyxjsi3szf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hyxjsi3szf/</guid><description>This paper presents communication-efficient protocols for the distributed experts problem, achieving near-optimal regret with theoretical and empirical validation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hyxjsi3szf/cover.png"/></item><item><title>Community Detection Guarantees using Embeddings Learned by Node2Vec</title><link>https://deep-diver.github.io/neurips2024/posters/cnpr4e2hcq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cnpr4e2hcq/</guid><description>Node2Vec, a popular network embedding method, is proven to consistently recover community structure in stochastic block models, paving the way for more reliable unsupervised community detection.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cnpr4e2hcq/cover.png"/></item><item><title>Compact Proofs of Model Performance via Mechanistic Interpretability</title><link>https://deep-diver.github.io/neurips2024/posters/2zwbzx50mh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2zwbzx50mh/</guid><description>Researchers developed a novel method using mechanistic interpretability to create compact formal proofs for AI model performance, improving AI safety and reliability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2zwbzx50mh/cover.png"/></item><item><title>Compositional PAC-Bayes: Generalization of GNNs with persistence and beyond</title><link>https://deep-diver.github.io/neurips2024/posters/zncjtnn3e8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zncjtnn3e8/</guid><description>Novel compositional PAC-Bayes framework delivers data-dependent generalization bounds for persistence-enhanced Graph Neural Networks, improving model design and performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zncjtnn3e8/cover.png"/></item><item><title>Computational Aspects of Bayesian Persuasion under Approximate Best Response</title><link>https://deep-diver.github.io/neurips2024/posters/9b0iokn3up/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/9b0iokn3up/</guid><description>This paper presents efficient algorithms for Bayesian persuasion under approximate best response, offering polynomial-time solutions for specific cases and a quasi-polynomial-time approximation scheme&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/9b0iokn3up/cover.png"/></item><item><title>Conditional Generative Models are Sufficient to Sample from Any Causal Effect Estimand</title><link>https://deep-diver.github.io/neurips2024/posters/vymkubmllh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vymkubmllh/</guid><description>ID-GEN: Sample high-dimensional interventional distributions using any conditional generative model!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vymkubmllh/cover.png"/></item><item><title>Conditional Outcome Equivalence: A Quantile Alternative to CATE</title><link>https://deep-diver.github.io/neurips2024/posters/typcietpwm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/typcietpwm/</guid><description>Researchers introduce the Conditional Quantile Comparator (CQC) for analyzing heterogeneous treatment effects, offering an improved approach by combining the strengths of CATE and CQTE while overcomin&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/typcietpwm/cover.png"/></item><item><title>Conformal Classification with Equalized Coverage for Adaptively Selected Groups</title><link>https://deep-diver.github.io/neurips2024/posters/3pwhkxk1sc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3pwhkxk1sc/</guid><description>This paper introduces AFCP, a novel conformal inference method that generates prediction sets with valid coverage conditional on adaptively selected features, achieving a practical balance between eff&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3pwhkxk1sc/cover.png"/></item><item><title>Conformal Inverse Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/y2nwklrdrx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y2nwklrdrx/</guid><description>Conformal inverse optimization learns uncertainty sets for parameters in optimization models, then solves a robust optimization model for high-quality, human-aligned decisions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y2nwklrdrx/cover.png"/></item><item><title>Consistency of Neural Causal Partial Identification</title><link>https://deep-diver.github.io/neurips2024/posters/gebnpxd9ef/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gebnpxd9ef/</guid><description>Neural causal models consistently estimate partial causal effects, even with continuous/categorical variables, thanks to Lipschitz regularization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gebnpxd9ef/cover.png"/></item><item><title>Constrained Adaptive Attack: Effective Adversarial Attack Against Deep Neural Networks for Tabular Data</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/zttwkr51yh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/zttwkr51yh/</guid><description>Constrained Adaptive Attack (CAA) significantly improves adversarial attacks on deep learning models for tabular data by combining gradient and search-based methods, achieving up to 96.1% accuracy dro&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/zttwkr51yh/cover.png"/></item><item><title>Constrained Binary Decision Making</title><link>https://deep-diver.github.io/neurips2024/posters/ntv5xzfzek/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ntv5xzfzek/</guid><description>This paper presents a unified framework for solving binary statistical decision-making problems, enabling efficient derivation of optimal strategies for diverse applications like OOD detection and sel&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ntv5xzfzek/cover.png"/></item><item><title>Constrained Sampling with Primal-Dual Langevin Monte Carlo</title><link>https://deep-diver.github.io/neurips2024/posters/o6hk6vld20/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/o6hk6vld20/</guid><description>Constrained sampling made easy! Primal-Dual Langevin Monte Carlo efficiently samples from complex probability distributions while satisfying statistical constraints.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/o6hk6vld20/cover.png"/></item><item><title>Contextual Decision-Making with Knapsacks Beyond the Worst Case</title><link>https://deep-diver.github.io/neurips2024/posters/dgt6sh2ruq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dgt6sh2ruq/</guid><description>This work unveils a novel algorithm for contextual decision-making with knapsacks, achieving significantly improved regret bounds beyond worst-case scenarios, thereby offering a more practical and eff&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dgt6sh2ruq/cover.png"/></item><item><title>Contextual Linear Optimization with Bandit Feedback</title><link>https://deep-diver.github.io/neurips2024/posters/lodbhkqzrh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lodbhkqzrh/</guid><description>This paper introduces induced empirical risk minimization for contextual linear optimization with bandit feedback, providing theoretical guarantees and computationally tractable solutions for improved&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lodbhkqzrh/cover.png"/></item><item><title>Continual Counting with Gradual Privacy Expiration</title><link>https://deep-diver.github.io/neurips2024/posters/v6qdb1agsm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v6qdb1agsm/</guid><description>Continual counting with gradual privacy expiration: A new algorithm achieves optimal accuracy with exponentially decaying privacy!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v6qdb1agsm/cover.png"/></item><item><title>Continual learning with the neural tangent ensemble</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/qosfijdvkz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/qosfijdvkz/</guid><description>Neural networks, viewed as Bayesian ensembles of fixed classifiers, enable continual learning without forgetting; posterior updates mirror stochastic gradient descent, offering insights into optimizat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/qosfijdvkz/cover.png"/></item><item><title>Contracting with a Learning Agent</title><link>https://deep-diver.github.io/neurips2024/posters/bh0llup8oa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bh0llup8oa/</guid><description>Repeated contracts with learning agents are optimized by a simple dynamic contract: initially linear, then switching to zero-cost, causing the agent&amp;rsquo;s actions to &amp;lsquo;free-fall&amp;rsquo; and yield non-zero rewards&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bh0llup8oa/cover.png"/></item><item><title>Contrastive losses as generalized models of global epistasis</title><link>https://deep-diver.github.io/neurips2024/posters/hloixozoly/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hloixozoly/</guid><description>Contrastive losses unlock efficient fitness function modeling by leveraging the ranking information inherent in global epistasis, significantly improving accuracy and data efficiency in protein engine&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hloixozoly/cover.png"/></item><item><title>Controlling Counterfactual Harm in Decision Support Systems Based on Prediction Sets</title><link>https://deep-diver.github.io/neurips2024/posters/pytka6hkzx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pytka6hkzx/</guid><description>AI decision support systems can unintentionally harm users; this paper introduces a novel framework to design systems that minimize this counterfactual harm, balancing accuracy and user well-being.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pytka6hkzx/cover.png"/></item><item><title>Controlling Multiple Errors Simultaneously with a PAC-Bayes Bound</title><link>https://deep-diver.github.io/neurips2024/posters/lwpfh9wvko/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lwpfh9wvko/</guid><description>New PAC-Bayes bound controls multiple error types simultaneously, providing richer generalization guarantees.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lwpfh9wvko/cover.png"/></item><item><title>Convergence of $ ext{log}(1/psilon)$ for Gradient-Based Algorithms in Zero-Sum Games without the Condition Number: A Smoothed Analysis</title><link>https://deep-diver.github.io/neurips2024/posters/hovxlc8vqu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hovxlc8vqu/</guid><description>Gradient-based methods for solving large zero-sum games achieve polynomial smoothed complexity, demonstrating efficiency even in high-precision scenarios without condition number dependence.</description></item><item><title>Convergence of No-Swap-Regret Dynamics in Self-Play</title><link>https://deep-diver.github.io/neurips2024/posters/cenoujeqnx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cenoujeqnx/</guid><description>In symmetric zero-sum games, no-swap-regret dynamics guarantee strong convergence to Nash Equilibrium under symmetric initial conditions, but this advantage disappears when constraints are relaxed.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cenoujeqnx/cover.png"/></item><item><title>Corruption-Robust Linear Bandits: Minimax Optimality and Gap-Dependent Misspecification</title><link>https://deep-diver.github.io/neurips2024/posters/wqs2rmq4cw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wqs2rmq4cw/</guid><description>This paper presents novel algorithms for linear bandits that are robust to corrupted rewards, achieving minimax optimality and optimal scaling for gap-dependent misspecification, extending to reinforc&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wqs2rmq4cw/cover.png"/></item><item><title>Cost-aware Bayesian Optimization via the Pandora's Box Gittins Index</title><link>https://deep-diver.github.io/neurips2024/posters/ouc1f0sfb7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ouc1f0sfb7/</guid><description>Cost-aware Bayesian optimization gets a boost with the Pandora&amp;rsquo;s Box Gittins Index, a novel acquisition function that efficiently balances exploration and exploitation while considering evaluation cos&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ouc1f0sfb7/cover.png"/></item><item><title>Counterfactual Fairness by Combining Factual and Counterfactual Predictions</title><link>https://deep-diver.github.io/neurips2024/posters/j0itri0uin/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/j0itri0uin/</guid><description>This paper proposes a novel method to achieve optimal counterfactual fairness in machine learning models while minimizing predictive performance degradation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/j0itri0uin/cover.png"/></item><item><title>Covariate Shift Corrected Conditional Randomization Test</title><link>https://deep-diver.github.io/neurips2024/posters/me5esztrqw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/me5esztrqw/</guid><description>A new Covariate Shift Corrected Pearson Chi-squared Conditional Randomization (csPCR) test accurately assesses conditional independence even when data distributions vary between source and target popu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/me5esztrqw/cover.png"/></item><item><title>Credal Learning Theory</title><link>https://deep-diver.github.io/neurips2024/posters/ah5kwussln/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ah5kwussln/</guid><description>Credal Learning Theory uses convex sets of probabilities to model data distribution variability, providing theoretical risk bounds for machine learning models in dynamic environments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ah5kwussln/cover.png"/></item><item><title>Credit Attribution and Stable Compression</title><link>https://deep-diver.github.io/neurips2024/posters/crlfvsorzt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/crlfvsorzt/</guid><description>New definitions of differential privacy enable machine learning algorithms to credit sources appropriately, balancing data utility and copyright compliance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/crlfvsorzt/cover.png"/></item><item><title>Cryptographic Hardness of Score Estimation</title><link>https://deep-diver.github.io/neurips2024/posters/urqxbwm0md/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/urqxbwm0md/</guid><description>Score estimation, crucial for diffusion models, is computationally hard even with polynomial sample complexity unless strong distributional assumptions are made.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/urqxbwm0md/cover.png"/></item><item><title>CSPG: Crossing Sparse Proximity Graphs for Approximate Nearest Neighbor Search</title><link>https://deep-diver.github.io/neurips2024/posters/ohvxbipv7e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ohvxbipv7e/</guid><description>CSPG: a novel framework boosting Approximate Nearest Neighbor Search speed by 1.5-2x, using sparse proximity graphs and efficient two-staged search.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ohvxbipv7e/cover.png"/></item><item><title>DAT: Improving Adversarial Robustness via Generative Amplitude Mix-up in Frequency Domain</title><link>https://deep-diver.github.io/neurips2024/posters/teqvz5ali8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/teqvz5ali8/</guid><description>Boost AI model robustness against adversarial attacks by creatively mixing training sample&amp;rsquo;s frequency amplitude with distractor images, focusing model learning on phase patterns, thus enhancing accur&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/teqvz5ali8/cover.png"/></item><item><title>Data Distribution Valuation</title><link>https://deep-diver.github.io/neurips2024/posters/1067784f6e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1067784f6e/</guid><description>This paper proposes a novel MMD-based method for data distribution valuation, enabling theoretically-principled comparison of data distributions from limited samples, outperforming existing methods in&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1067784f6e/cover.png"/></item><item><title>Data subsampling for Poisson regression with pth-root-link</title><link>https://deep-diver.github.io/neurips2024/posters/es0gj1kvuk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/es0gj1kvuk/</guid><description>Sublinear coresets for Poisson regression are developed, offering 1±ε approximation guarantees, with complexity analyzed using a novel parameter and domain shifting.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/es0gj1kvuk/cover.png"/></item><item><title>Data-faithful Feature Attribution: Mitigating Unobservable Confounders via Instrumental Variables</title><link>https://deep-diver.github.io/neurips2024/posters/jzv9a8tg9p/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jzv9a8tg9p/</guid><description>Data-faithful feature attribution tackles misinterpretations from unobservable confounders by using instrumental variables to train confounder-free models, leading to more robust and accurate feature &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jzv9a8tg9p/cover.png"/></item><item><title>Debiasing Synthetic Data Generated by Deep Generative Models</title><link>https://deep-diver.github.io/neurips2024/posters/aetbfmccwg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/aetbfmccwg/</guid><description>Debiasing synthetic data generated by deep generative models enhances statistical convergence rates, yielding reliable results for specific analyses.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/aetbfmccwg/cover.png"/></item><item><title>Decision-Focused Learning with Directional Gradients</title><link>https://deep-diver.github.io/neurips2024/posters/g8kflzdcax/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/g8kflzdcax/</guid><description>New Perturbation Gradient losses connect expected decisions with directional derivatives, enabling Lipschitz continuous surrogates for predict-then-optimize, asymptotically yielding best-in-class poli&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/g8kflzdcax/cover.png"/></item><item><title>Deep Homomorphism Networks</title><link>https://deep-diver.github.io/neurips2024/posters/kxuijdmfdg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kxuijdmfdg/</guid><description>Deep Homomorphism Networks (DHNs) boost graph neural network (GNN) expressiveness by efficiently detecting subgraph patterns using a novel graph homomorphism layer.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kxuijdmfdg/cover.png"/></item><item><title>Deep linear networks for regression are implicitly regularized towards flat minima</title><link>https://deep-diver.github.io/neurips2024/posters/f738wy1xm4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/f738wy1xm4/</guid><description>Deep linear networks implicitly regularize towards flat minima, with sharpness (Hessian&amp;rsquo;s largest eigenvalue) of minimizers linearly increasing with depth but bounded by a constant times the lower bou&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/f738wy1xm4/cover.png"/></item><item><title>DeNetDM: Debiasing by Network Depth Modulation</title><link>https://deep-diver.github.io/neurips2024/posters/0dta21q83c/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0dta21q83c/</guid><description>DeNetDM uses network depth modulation to automatically debiase image classifiers without bias annotations or data augmentation, improving accuracy by 5%.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0dta21q83c/cover.png"/></item><item><title>Denoising Diffusion Path: Attribution Noise Reduction with An Auxiliary Diffusion Model</title><link>https://deep-diver.github.io/neurips2024/posters/bsv0mbdbf2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bsv0mbdbf2/</guid><description>Denoising Diffusion Path (DDPath) uses diffusion models to dramatically reduce noise in attribution methods for deep neural networks, leading to clearer explanations and improved quantitative results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bsv0mbdbf2/cover.png"/></item><item><title>Derandomizing Multi-Distribution Learning</title><link>https://deep-diver.github.io/neurips2024/posters/twye75mnkt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/twye75mnkt/</guid><description>Derandomizing multi-distribution learning is computationally hard, but a structural condition allows efficient black-box conversion of randomized predictors to deterministic ones.</description></item><item><title>Derivatives of Stochastic Gradient Descent in parametric optimization</title><link>https://deep-diver.github.io/neurips2024/posters/7woophiz8u/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7woophiz8u/</guid><description>Stochastic gradient descent&amp;rsquo;s derivatives, crucial for hyperparameter optimization, converge to the solution mapping derivative; rates depend on step size, exhibiting O(log(k)²/k) convergence with van&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7woophiz8u/cover.png"/></item><item><title>Detecting and Measuring Confounding Using Causal Mechanism Shifts</title><link>https://deep-diver.github.io/neurips2024/posters/svmjjjs0q1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/svmjjjs0q1/</guid><description>This paper proposes novel measures to detect and quantify confounding biases from observational data using causal mechanism shifts, even with unobserved confounders.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/svmjjjs0q1/cover.png"/></item><item><title>Detecting Brittle Decisions for Free: Leveraging Margin Consistency in Deep Robust Classifiers</title><link>https://deep-diver.github.io/neurips2024/posters/xhcyznmqnv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xhcyznmqnv/</guid><description>Deep learning models&amp;rsquo; robustness can be efficiently evaluated using a novel method, margin consistency, which leverages the correlation between input and logit margins for faster, accurate vulnerabili&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xhcyznmqnv/cover.png"/></item><item><title>Diffeomorphic interpolation for efficient persistence-based topological optimization</title><link>https://deep-diver.github.io/neurips2024/posters/gyjm1bzzdx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gyjm1bzzdx/</guid><description>Diffeomorphic interpolation boosts topological optimization by transforming sparse gradients into smooth vector fields, enabling efficient large-scale point cloud optimization and black-box autoencode&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gyjm1bzzdx/cover.png"/></item><item><title>Differentiable Structure Learning with Partial Orders</title><link>https://deep-diver.github.io/neurips2024/posters/b2ctlakrhv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/b2ctlakrhv/</guid><description>This research introduces a novel plug-and-play module that efficiently integrates prior partial order constraints into differentiable structure learning, significantly improving structure recovery qua&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/b2ctlakrhv/cover.png"/></item><item><title>Differential Privacy in Scalable General Kernel Learning via $K$-means Nystr{"o}m Random Features</title><link>https://deep-diver.github.io/neurips2024/posters/mdmzaeznhq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mdmzaeznhq/</guid><description>Differentially private scalable kernel learning is achieved via a novel DP K-means Nyström method, enabling efficient and accurate model training for general kernels while safeguarding privacy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mdmzaeznhq/cover.png"/></item><item><title>Differentially Private Equivalence Testing for Continuous Distributions and Applications</title><link>https://deep-diver.github.io/neurips2024/posters/qduqp1nzz6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qduqp1nzz6/</guid><description>First differentially private algorithm for testing equivalence between continuous distributions, enabling privacy-preserving comparisons of sensitive data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qduqp1nzz6/cover.png"/></item><item><title>Differentially Private Graph Diffusion with Applications in Personalized PageRanks</title><link>https://deep-diver.github.io/neurips2024/posters/aon7bwybiq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/aon7bwybiq/</guid><description>This paper introduces a novel differentially private graph diffusion framework ensuring edge-level privacy, significantly improving utility-privacy trade-offs for personalized PageRank computation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/aon7bwybiq/cover.png"/></item><item><title>Differentially Private Optimization with Sparse Gradients</title><link>https://deep-diver.github.io/neurips2024/posters/4ktifp48wd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4ktifp48wd/</guid><description>This paper presents new, nearly optimal differentially private algorithms for handling sparse gradients, significantly improving efficiency and scalability in large embedding models.</description></item><item><title>Differentially Private Reinforcement Learning with Self-Play</title><link>https://deep-diver.github.io/neurips2024/posters/t07ohxceyp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/t07ohxceyp/</guid><description>This paper presents DP-Nash-VI, a novel algorithm ensuring trajectory-wise privacy in multi-agent reinforcement learning, achieving near-optimal regret bounds under both joint and local differential p&amp;hellip;</description></item><item><title>Differentially Private Set Representations</title><link>https://deep-diver.github.io/neurips2024/posters/gqnvvqquo0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gqnvvqquo0/</guid><description>Differentially private set representations achieve optimal privacy-utility tradeoffs with exponentially smaller error than prior histogram methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gqnvvqquo0/cover.png"/></item><item><title>Differentially Private Stochastic Gradient Descent with Fixed-Size Minibatches: Tighter RDP Guarantees with or without Replacement</title><link>https://deep-diver.github.io/neurips2024/posters/tjskngasmy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tjskngasmy/</guid><description>Tighter differential privacy (RDP) guarantees for DP-SGD with fixed-size minibatches are achieved, improving private deep learning model training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tjskngasmy/cover.png"/></item><item><title>DiffHammer: Rethinking the Robustness of Diffusion-Based Adversarial Purification</title><link>https://deep-diver.github.io/neurips2024/posters/zj2onmsgcs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zj2onmsgcs/</guid><description>DiffHammer unveils weaknesses in diffusion-based adversarial defenses by introducing a novel attack bypassing existing evaluation limitations, leading to more robust security solutions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zj2onmsgcs/cover.png"/></item><item><title>Diffusion Models are Certifiably Robust Classifiers</title><link>https://deep-diver.github.io/neurips2024/posters/wgp1tbcp1e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wgp1tbcp1e/</guid><description>Diffusion models are certifiably robust classifiers due to their inherent O(1) Lipschitzness, a property further enhanced by generalizing to noisy data, achieving over 80% certified robustness on CIFA&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wgp1tbcp1e/cover.png"/></item><item><title>Dimension-free deterministic equivalents and scaling laws for random feature regression</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/fbljifw64d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/fbljifw64d/</guid><description>This work delivers dimension-free deterministic equivalents for random feature regression, revealing sharp excess error rates and scaling laws.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/fbljifw64d/cover.png"/></item><item><title>Dimension-free Private Mean Estimation for Anisotropic Distributions</title><link>https://deep-diver.github.io/neurips2024/posters/krwqcaia7z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/krwqcaia7z/</guid><description>Dimension-free private mean estimation is achieved for anisotropic data, breaking the curse of dimensionality in privacy-preserving high-dimensional analysis.</description></item><item><title>Direct Preference-Based Evolutionary Multi-Objective Optimization with Dueling Bandits</title><link>https://deep-diver.github.io/neurips2024/posters/owhj0g15cd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/owhj0g15cd/</guid><description>D-PBEMO: A novel framework for preference-based multi-objective optimization using clustering-based stochastic dueling bandits to directly leverage human feedback, improving efficiency and managing co&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/owhj0g15cd/cover.png"/></item><item><title>Directional Smoothness and Gradient Methods: Convergence and Adaptivity</title><link>https://deep-diver.github.io/neurips2024/posters/m9wzrexwl5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/m9wzrexwl5/</guid><description>New sub-optimality bounds for gradient descent leverage directional smoothness, a localized gradient variation measure, achieving tighter convergence guarantees and adapting to optimization paths.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/m9wzrexwl5/cover.png"/></item><item><title>Discretely beyond $1/e$: Guided Combinatorial Algortihms for Submodular Maximization</title><link>https://deep-diver.github.io/neurips2024/posters/cgiox8lfwg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cgiox8lfwg/</guid><description>Researchers surpass the 1/e barrier in submodular maximization with novel combinatorial algorithms!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cgiox8lfwg/cover.png"/></item><item><title>Disentangled Representation Learning in Non-Markovian Causal Systems</title><link>https://deep-diver.github.io/neurips2024/posters/ulgyobn7hm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ulgyobn7hm/</guid><description>This paper introduces graphical criteria and an algorithm for disentangling causal factors from heterogeneous data in non-Markovian settings, advancing causal representation learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ulgyobn7hm/cover.png"/></item><item><title>Dissecting the Failure of Invariant Learning on Graphs</title><link>https://deep-diver.github.io/neurips2024/posters/7efs8azham/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7efs8azham/</guid><description>Cross-environment Intra-class Alignment (CIA) and its label-free variant, CIA-LRA, significantly improve node-level OOD generalization on graphs by aligning representations and eliminating spurious fe&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7efs8azham/cover.png"/></item><item><title>Dissecting the Interplay of Attention Paths in a Statistical Mechanics Theory of Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/uz804qljt2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uz804qljt2/</guid><description>Researchers dissected attention paths in Transformers using statistical mechanics, revealing a task-relevant kernel combination mechanism boosting generalization performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uz804qljt2/cover.png"/></item><item><title>Distribution Learning with Valid Outputs Beyond the Worst-Case</title><link>https://deep-diver.github.io/neurips2024/posters/l7i5fjgkjc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/l7i5fjgkjc/</guid><description>Generative models often produce invalid outputs; this work shows that ensuring validity is easier than expected when using log-loss and carefully selecting model classes and data distributions.</description></item><item><title>Distributional regression: CRPS-error bounds for model fitting, model selection and convex aggregation</title><link>https://deep-diver.github.io/neurips2024/posters/csfxzcozpu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/csfxzcozpu/</guid><description>This paper provides the first statistical learning guarantees for distributional regression using CRPS, offering concentration bounds for model fitting, selection, and convex aggregation, applicable t&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/csfxzcozpu/cover.png"/></item><item><title>Distributionally Robust Performative Prediction</title><link>https://deep-diver.github.io/neurips2024/posters/e8wdxddiqu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/e8wdxddiqu/</guid><description>This research introduces distributionally robust performative prediction, offering a new solution concept (DRPO) that minimizes performative risk even with misspecified distribution maps, ensuring rob&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/e8wdxddiqu/cover.png"/></item><item><title>DistrictNet: Decision-aware learning for geographical districting</title><link>https://deep-diver.github.io/neurips2024/posters/njwybfau8e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/njwybfau8e/</guid><description>DISTRICTNET: A novel decision-aware learning approach drastically cuts geographical districting costs by integrating combinatorial optimization and graph neural networks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/njwybfau8e/cover.png"/></item><item><title>Diversity Is Not All You Need: Training A Robust Cooperative Agent Needs Specialist Partners</title><link>https://deep-diver.github.io/neurips2024/posters/15460jjoco/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/15460jjoco/</guid><description>Training robust cooperative AI agents requires diverse and specialized training partners, but existing methods often produce overfit partners. This paper proposes novel methods using reinforcement and&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/15460jjoco/cover.png"/></item><item><title>Divide-and-Conquer Predictive Coding: a structured Bayesian inference algorithm</title><link>https://deep-diver.github.io/neurips2024/posters/dxwiacvkwu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dxwiacvkwu/</guid><description>Divide-and-conquer predictive coding (DCPC) revolutionizes structured Bayesian inference by achieving superior performance in high-dimensional problems while remaining biologically plausible.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dxwiacvkwu/cover.png"/></item><item><title>Do Finetti: On Causal Effects for Exchangeable Data</title><link>https://deep-diver.github.io/neurips2024/oral-ai-theory/4rczeczaon/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-ai-theory/4rczeczaon/</guid><description>Causal inference revolutionized: New framework estimates causal effects from exchangeable data, enabling simultaneous causal discovery and effect estimation via the Do-Finetti algorithm.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-ai-theory/4rczeczaon/cover.png"/></item><item><title>DOPPLER: Differentially Private Optimizers with Low-pass Filter for Privacy Noise Reduction</title><link>https://deep-diver.github.io/neurips2024/posters/r8yntmad0g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/r8yntmad0g/</guid><description>DOPPLER, a novel low-pass filter, significantly enhances differentially private (DP) optimizer performance by reducing the impact of privacy noise, bridging the gap between DP and non-DP training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/r8yntmad0g/cover.png"/></item><item><title>Drago: Primal-Dual Coupled Variance Reduction for Faster Distributionally Robust Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/ujk0xrntqz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ujk0xrntqz/</guid><description>DRAGO: A novel primal-dual algorithm delivers faster, state-of-the-art convergence for distributionally robust optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ujk0xrntqz/cover.png"/></item><item><title>DropEdge not Foolproof: Effective Augmentation Method for Signed Graph Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/cde2zbpioj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cde2zbpioj/</guid><description>SGA: A novel framework boosts Signed Graph Neural Network performance by addressing graph sparsity and unbalanced triangles, achieving up to 26.2% F1-micro improvement.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cde2zbpioj/cover.png"/></item><item><title>Dual Lagrangian Learning for Conic Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/gn1ikwxll5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gn1ikwxll5/</guid><description>Dual Lagrangian Learning (DLL) revolutionizes conic optimization by leveraging machine learning to efficiently learn high-quality dual-feasible solutions, achieving 1000x speedups over traditional sol&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gn1ikwxll5/cover.png"/></item><item><title>Dual-Perspective Activation: Efficient Channel Denoising via Joint Forward-Backward Criterion for Artificial Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/ku35qkpveg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ku35qkpveg/</guid><description>Dual-Perspective Activation (DPA) efficiently denoises ANN channels by jointly using forward and backward propagation criteria, improving sparsity and accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ku35qkpveg/cover.png"/></item><item><title>Dueling over Dessert, Mastering the Art of Repeated Cake Cutting</title><link>https://deep-diver.github.io/neurips2024/posters/mftvnzhsht/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mftvnzhsht/</guid><description>Repeated cake-cutting game reveals that strategic players can exploit myopic opponents, but equitable outcomes are achievable through specific strategies.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mftvnzhsht/cover.png"/></item><item><title>Dynamic Service Fee Pricing under Strategic Behavior: Actions as Instruments and Phase Transition</title><link>https://deep-diver.github.io/neurips2024/posters/tnl2k6iz9j/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tnl2k6iz9j/</guid><description>This research introduces novel algorithms to dynamically price third-party platform service fees under strategic buyer behavior, achieving optimal revenue with a theoretically proven regret bound.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tnl2k6iz9j/cover.png"/></item><item><title>ECLipsE: Efficient Compositional Lipschitz Constant Estimation for Deep Neural Networks</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/61yysy078z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/61yysy078z/</guid><description>ECLipsE: A novel compositional approach drastically accelerates Lipschitz constant estimation for deep neural networks, achieving speedups of thousands of times compared to the state-of-the-art while &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/61yysy078z/cover.png"/></item><item><title>Efficiency of the First-Price Auction in the Autobidding World</title><link>https://deep-diver.github.io/neurips2024/posters/4dhosjet4r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4dhosjet4r/</guid><description>First-price auction efficiency in autobidding plummets to 45.7% with mixed bidders, but machine-learned advice restores optimality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4dhosjet4r/cover.png"/></item><item><title>Efficient $ hi$-Regret Minimization with Low-Degree Swap Deviations in Extensive-Form Games</title><link>https://deep-diver.github.io/neurips2024/posters/c4elkpa0kh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/c4elkpa0kh/</guid><description>New efficient algorithms minimize regret in extensive-form games by cleverly using low-degree swap deviations and a relaxed fixed-point concept, improving correlated equilibrium computation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/c4elkpa0kh/cover.png"/></item><item><title>Efficient and Private Marginal Reconstruction with Local Non-Negativity</title><link>https://deep-diver.github.io/neurips2024/posters/lknl4clhhs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lknl4clhhs/</guid><description>Efficiently and privately reconstructing marginal queries from noisy data using residuals improves accuracy of existing differential privacy mechanisms.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lknl4clhhs/cover.png"/></item><item><title>Efficient Combinatorial Optimization via Heat Diffusion</title><link>https://deep-diver.github.io/neurips2024/posters/psdrko9v1d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/psdrko9v1d/</guid><description>Heat Diffusion Optimization (HeO) framework efficiently solves combinatorial optimization problems by enabling information propagation through heat diffusion, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/psdrko9v1d/cover.png"/></item><item><title>Efficient Graph Matching for Correlated Stochastic Block Models</title><link>https://deep-diver.github.io/neurips2024/posters/nbhficdnrp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nbhficdnrp/</guid><description>Efficient algorithm achieves near-perfect graph matching in correlated stochastic block models, resolving a key open problem and enabling improved community detection.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nbhficdnrp/cover.png"/></item><item><title>Efficient Policy Evaluation Across Multiple Different Experimental Datasets</title><link>https://deep-diver.github.io/neurips2024/posters/psubtzaitm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/psubtzaitm/</guid><description>This paper presents novel graphical criteria and estimators for accurately evaluating policy effectiveness across multiple experimental datasets, even when data distributions differ.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/psubtzaitm/cover.png"/></item><item><title>Efficient Streaming Algorithms for Graphlet Sampling</title><link>https://deep-diver.github.io/neurips2024/posters/ec9hfi9v3k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ec9hfi9v3k/</guid><description>STREAM-UGS: a novel semi-streaming algorithm for efficient graphlet sampling, enabling fast analysis of massive graphs with limited memory.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ec9hfi9v3k/cover.png"/></item><item><title>Efficiently Learning Significant Fourier Feature Pairs for Statistical Independence Testing</title><link>https://deep-diver.github.io/neurips2024/posters/beiqnqziky/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/beiqnqziky/</guid><description>This research introduces LFHSIC, a novel, linear-time independence test that significantly outperforms existing methods, especially for high-dimensional data, by learning optimal Fourier feature pairs&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/beiqnqziky/cover.png"/></item><item><title>Elliptical Attention</title><link>https://deep-diver.github.io/neurips2024/posters/ejg4d4fvrs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ejg4d4fvrs/</guid><description>Elliptical Attention enhances transformers by using a Mahalanobis distance metric, stretching the feature space to focus on contextually relevant information, thus improving robustness and reducing re&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ejg4d4fvrs/cover.png"/></item><item><title>Emergence of heavy tails in homogenized stochastic gradient descent</title><link>https://deep-diver.github.io/neurips2024/posters/efrgbp9au6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/efrgbp9au6/</guid><description>Homogenized SGD reveals heavy-tailed neural network parameters, offering quantifiable bounds on tail-index and showcasing the interplay between optimization hyperparameters and model generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/efrgbp9au6/cover.png"/></item><item><title>Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept Space</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/owuect6btl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/owuect6btl/</guid><description>Generative models learn hidden capabilities suddenly during training, which can be explained and predicted using a novel &amp;lsquo;concept space&amp;rsquo; framework that analyzes learning dynamics and concept signal.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/owuect6btl/cover.png"/></item><item><title>Energy-based Epistemic Uncertainty for Graph Neural Networks</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/6vnpptwh1q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/6vnpptwh1q/</guid><description>GEBM: a novel graph-based energy model for robust GNN uncertainty estimation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/6vnpptwh1q/cover.png"/></item><item><title>Enhancing Robustness of Graph Neural Networks on Social Media with Explainable Inverse Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/zieha15y8k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/zieha15y8k/</guid><description>MoE-BiEntIRL: A novel explainable inverse reinforcement learning method enhances GNN robustness against diverse social media attacks by reconstructing attacker policies and generating more robust trai&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/zieha15y8k/cover.png"/></item><item><title>Enhancing Robustness of Last Layer Two-Stage Fair Model Corrections</title><link>https://deep-diver.github.io/neurips2024/posters/chnj3w4hfg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/chnj3w4hfg/</guid><description>Boosting fair machine learning&amp;rsquo;s robustness against noisy labels, this work introduces a novel label-spreading method, achieving state-of-the-art worst-group accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/chnj3w4hfg/cover.png"/></item><item><title>Enriching Disentanglement: From Logical Definitions to Quantitative Metrics</title><link>https://deep-diver.github.io/neurips2024/posters/tvq3xckwbb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tvq3xckwbb/</guid><description>This paper presents a novel approach to deriving theoretically grounded disentanglement metrics by linking logical definitions to quantitative measures, offering strong theoretical guarantees and easi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tvq3xckwbb/cover.png"/></item><item><title>Entropy testing and its application to testing Bayesian networks</title><link>https://deep-diver.github.io/neurips2024/posters/bmsxealci4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bmsxealci4/</guid><description>This paper presents near-optimal algorithms for entropy identity testing, significantly improving Bayesian network testing efficiency.</description></item><item><title>Entrywise error bounds for low-rank approximations of kernel matrices</title><link>https://deep-diver.github.io/neurips2024/posters/ziyc4fhrnr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ziyc4fhrnr/</guid><description>This paper provides novel entrywise error bounds for low-rank kernel matrix approximations, showing how many data points are needed to get statistically consistent results for low-rank approximations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ziyc4fhrnr/cover.png"/></item><item><title>Estimating Generalization Performance Along the Trajectory of Proximal SGD in Robust Regression</title><link>https://deep-diver.github.io/neurips2024/posters/ntf7d8talq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ntf7d8talq/</guid><description>New consistent estimators precisely track generalization error during robust regression&amp;rsquo;s iterative model training, enabling optimal stopping iteration for minimized error.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ntf7d8talq/cover.png"/></item><item><title>Estimating Heterogeneous Treatment Effects by Combining Weak Instruments and Observational Data</title><link>https://deep-diver.github.io/neurips2024/posters/c37x7cxz2y/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/c37x7cxz2y/</guid><description>This study develops a novel two-stage framework for accurately predicting conditional average treatment effects using both observational data and weak instrumental variables, overcoming limitations of&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/c37x7cxz2y/cover.png"/></item><item><title>Evaluating alignment between humans and neural network representations in image-based learning tasks</title><link>https://deep-diver.github.io/neurips2024/posters/8i6px5w1rf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8i6px5w1rf/</guid><description>Pretrained neural networks surprisingly capture fundamental aspects of human cognition, enabling generalization in image-based learning tasks, as demonstrated by aligning neural network representation&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8i6px5w1rf/cover.png"/></item><item><title>Evidence of Learned Look-Ahead in a Chess-Playing Neural Network</title><link>https://deep-diver.github.io/neurips2024/posters/8zg9so4ttv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8zg9so4ttv/</guid><description>Chess AI Leela Zero surprisingly uses learned look-ahead, internally representing future optimal moves, significantly improving its strategic decision-making.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8zg9so4ttv/cover.png"/></item><item><title>Exact Gradients for Stochastic Spiking Neural Networks Driven by Rough Signals</title><link>https://deep-diver.github.io/neurips2024/posters/mcwzj7pa0m/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mcwzj7pa0m/</guid><description>New framework uses rough path theory to enable gradient-based training of SSNNs driven by rough signals, allowing for noise in spike timing and network dynamics.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mcwzj7pa0m/cover.png"/></item><item><title>Exact, Tractable Gauss-Newton Optimization in Deep Reversible Architectures Reveal Poor Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/p37nlki9vl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/p37nlki9vl/</guid><description>Exact Gauss-Newton optimization in deep reversible networks surprisingly reveals poor generalization, despite faster training, challenging existing deep learning optimization theories.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/p37nlki9vl/cover.png"/></item><item><title>Exactly Minimax-Optimal Locally Differentially Private Sampling</title><link>https://deep-diver.github.io/neurips2024/posters/dr7uarlhve/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dr7uarlhve/</guid><description>This paper provides the first exact minimax-optimal mechanisms for locally differentially private sampling, applicable across all f-divergences.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dr7uarlhve/cover.png"/></item><item><title>Exogenous Matching: Learning Good Proposals for Tractable Counterfactual Estimation</title><link>https://deep-diver.github.io/neurips2024/posters/ys9xu6ania/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ys9xu6ania/</guid><description>Exogenous Matching learns optimal proposals for efficient counterfactual estimation by transforming variance minimization into conditional distribution learning, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ys9xu6ania/cover.png"/></item><item><title>Expectation Alignment: Handling Reward Misspecification in the Presence of Expectation Mismatch</title><link>https://deep-diver.github.io/neurips2024/posters/io7viyaat7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/io7viyaat7/</guid><description>This paper introduces Expectation Alignment (EAL), a novel framework and interactive algorithm to address reward misspecification in AI, aligning AI behavior with user expectations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/io7viyaat7/cover.png"/></item><item><title>Explanations that reveal all through the deﬁnition of encoding</title><link>https://deep-diver.github.io/neurips2024/posters/mkw6x0oexg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mkw6x0oexg/</guid><description>New method, STRIPE-X, powerfully detects &amp;rsquo;encoding&amp;rsquo; in AI explanations—a sneaky phenomenon where explanations predict outcomes better than their constituent parts alone would suggest.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mkw6x0oexg/cover.png"/></item><item><title>Explicit Eigenvalue Regularization Improves Sharpness-Aware Minimization</title><link>https://deep-diver.github.io/neurips2024/posters/jfuhby34sc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jfuhby34sc/</guid><description>Eigen-SAM significantly boosts generalization in deep learning by directly addressing SAM&amp;rsquo;s limitations through explicit top Hessian eigenvalue regularization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jfuhby34sc/cover.png"/></item><item><title>Exploring Adversarial Robustness of Deep State Space Models</title><link>https://deep-diver.github.io/neurips2024/posters/rsb32ebmbj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rsb32ebmbj/</guid><description>Deep state space models (SSMs) gain adversarial robustness through an adaptive scaling mechanism, improving performance without overfitting issues.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rsb32ebmbj/cover.png"/></item><item><title>Exploring Jacobian Inexactness in Second-Order Methods for Variational Inequalities: Lower Bounds, Optimal Algorithms and Quasi-Newton Approximations</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/uvfdaefr9x/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/uvfdaefr9x/</guid><description>VIJI, a novel second-order algorithm, achieves optimal convergence rates for variational inequalities even with inexact Jacobian information, bridging the gap between theory and practice in machine le&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/uvfdaefr9x/cover.png"/></item><item><title>Externally Valid Policy Evaluation from Randomized Trials Using Additional Observational Data</title><link>https://deep-diver.github.io/neurips2024/posters/2pgc5xdj1b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2pgc5xdj1b/</guid><description>This paper introduces a novel nonparametric method to make policy evaluations from randomized trials externally valid, even when trial and target populations differ. It leverages additional covariate&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2pgc5xdj1b/cover.png"/></item><item><title>Extracting Training Data from Molecular Pre-trained Models</title><link>https://deep-diver.github.io/neurips2024/posters/cv4fcjcwmz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cv4fcjcwmz/</guid><description>Researchers reveal a high risk of training data extraction from molecular pre-trained models, challenging the assumption that model sharing alone adequately protects against data theft.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cv4fcjcwmz/cover.png"/></item><item><title>Fair Allocation in Dynamic Mechanism Design</title><link>https://deep-diver.github.io/neurips2024/posters/beungps83o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/beungps83o/</guid><description>This paper presents optimal fair mechanisms for dynamic auction design, maximizing seller revenue while guaranteeing minimum allocations to multiple buyer groups.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/beungps83o/cover.png"/></item><item><title>Fair and Welfare-Efficient Constrained Multi-Matchings under Uncertainty</title><link>https://deep-diver.github.io/neurips2024/posters/6kthdqfgma/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/6kthdqfgma/</guid><description>This paper presents novel, scalable algorithms for fair and efficient constrained resource allocation under uncertainty using robust and CVaR optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/6kthdqfgma/cover.png"/></item><item><title>Fair Bilevel Neural Network (FairBiNN): On Balancing fairness and accuracy via Stackelberg Equilibrium</title><link>https://deep-diver.github.io/neurips2024/posters/e2r4wnhhgq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/e2r4wnhhgq/</guid><description>FairBiNN, a novel bilevel neural network, achieves Pareto optimal solutions by simultaneously optimizing for accuracy and fairness, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/e2r4wnhhgq/cover.png"/></item><item><title>Fair GLASSO: Estimating Fair Graphical Models with Unbiased Statistical Behavior</title><link>https://deep-diver.github.io/neurips2024/posters/a3cauwmxnv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/a3cauwmxnv/</guid><description>Fair GLASSO ensures fair Gaussian graphical models by introducing novel bias metrics and a penalized maximum likelihood estimator to mitigate group biases in data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/a3cauwmxnv/cover.png"/></item><item><title>Fair Online Bilateral Trade</title><link>https://deep-diver.github.io/neurips2024/posters/i90ypqplgl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/i90ypqplgl/</guid><description>This paper proposes a novel online bilateral trading algorithm maximizing the &lt;em>fair&lt;/em> gain from trade and provides tight regret bounds under various settings.</description></item><item><title>Fair Secretaries with Unfair Predictions</title><link>https://deep-diver.github.io/neurips2024/posters/dxxj4s06yl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dxxj4s06yl/</guid><description>Fair algorithms can leverage biased predictions to improve performance while guaranteeing fairness for all candidates.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dxxj4s06yl/cover.png"/></item><item><title>Fair Wasserstein Coresets</title><link>https://deep-diver.github.io/neurips2024/posters/ylcej2xiw5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ylcej2xiw5/</guid><description>Fair Wasserstein Coresets (FWC) efficiently generates fair, representative subsets of large datasets for downstream machine learning tasks, improving fairness and utility.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ylcej2xiw5/cover.png"/></item><item><title>Fairness and Efficiency in Online Class Matching</title><link>https://deep-diver.github.io/neurips2024/posters/kmaxn7hf6d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kmaxn7hf6d/</guid><description>First non-wasteful algorithm achieving 1/2-approximation for class envy-freeness, class proportionality, and utilitarian social welfare in online class matching.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kmaxn7hf6d/cover.png"/></item><item><title>Fairness in Social Influence Maximization via Optimal Transport</title><link>https://deep-diver.github.io/neurips2024/posters/axw8xvqpkf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/axw8xvqpkf/</guid><description>Fairness in social influence maximization is achieved via optimal transport, optimizing both outreach and a new &amp;lsquo;mutual fairness&amp;rsquo; metric that considers variability in outreach scenarios.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/axw8xvqpkf/cover.png"/></item><item><title>Fairness without Harm: An Influence-Guided Active Sampling Approach</title><link>https://deep-diver.github.io/neurips2024/posters/yyjojvbccd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yyjojvbccd/</guid><description>FairnessWithoutHarm achieves fairer ML models without sacrificing accuracy by using an influence-guided active sampling method that doesn&amp;rsquo;t require sensitive training data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yyjojvbccd/cover.png"/></item><item><title>Fairness-Aware Estimation of Graphical Models</title><link>https://deep-diver.github.io/neurips2024/posters/wvws8gowyr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wvws8gowyr/</guid><description>Fairness-aware estimation of graphical models (GMs) tackles bias in GM estimations by integrating graph disparity error and a tailored loss function into multi-objective optimization, effectively miti&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wvws8gowyr/cover.png"/></item><item><title>FairWire: Fair Graph Generation</title><link>https://deep-diver.github.io/neurips2024/posters/v0jvwcqlje/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v0jvwcqlje/</guid><description>FairWire tackles structural bias in graph machine learning, proposing a novel fairness regularizer and a fair graph generation framework for unbiased link prediction and graph generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v0jvwcqlje/cover.png"/></item><item><title>Fast Channel Simulation via Error-Correcting Codes</title><link>https://deep-diver.github.io/neurips2024/posters/8jpsenkvos/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8jpsenkvos/</guid><description>Polar codes revolutionize channel simulation, offering scalable, high-performance schemes that significantly outperform existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8jpsenkvos/cover.png"/></item><item><title>Fast Last-Iterate Convergence of Learning in Games Requires Forgetful Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/hk7xtpctbi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hk7xtpctbi/</guid><description>Forgetful algorithms are essential for fast last-iterate convergence in learning games; otherwise, even popular methods like OMWU fail.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hk7xtpctbi/cover.png"/></item><item><title>Fast Proxy Experiment Design for Causal Effect Identification</title><link>https://deep-diver.github.io/neurips2024/posters/ci7ii4cpwm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ci7ii4cpwm/</guid><description>This paper presents efficient algorithms for designing cost-optimal proxy experiments to identify causal effects, significantly improving upon prior methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ci7ii4cpwm/cover.png"/></item><item><title>Fast Rates in Stochastic Online Convex Optimization by Exploiting the Curvature of Feasible Sets</title><link>https://deep-diver.github.io/neurips2024/posters/y58t1mqhh6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y58t1mqhh6/</guid><description>This paper introduces a novel approach for fast rates in online convex optimization by exploiting the curvature of feasible sets, achieving logarithmic regret bounds under specific conditions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y58t1mqhh6/cover.png"/></item><item><title>Fast T2T: Optimization Consistency Speeds Up Diffusion-Based Training-to-Testing Solving for Combinatorial Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/xdrkzozeoc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xdrkzozeoc/</guid><description>Fast T2T: Optimization Consistency Boosts Diffusion-Based Combinatorial Optimization!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xdrkzozeoc/cover.png"/></item><item><title>Fast Tree-Field Integrators: From Low Displacement Rank to Topological Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/eok6hbcsri/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/eok6hbcsri/</guid><description>Fast Tree-Field Integrators (FTFIs) revolutionize graph processing by enabling polylog-linear time computation for integrating tensor fields on trees, providing significant speedups for various machin&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/eok6hbcsri/cover.png"/></item><item><title>Faster Accelerated First-order Methods for Convex Optimization with Strongly Convex Function Constraints</title><link>https://deep-diver.github.io/neurips2024/posters/pg380vlyru/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pg380vlyru/</guid><description>Faster primal-dual algorithms achieve order-optimal complexity for convex optimization with strongly convex constraints, improving convergence rates and solving large-scale problems efficiently.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pg380vlyru/cover.png"/></item><item><title>Faster Algorithms for User-Level Private Stochastic Convex Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/hnlk9cigo9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hnlk9cigo9/</guid><description>Faster algorithms achieve optimal excess risk in user-level private stochastic convex optimization, overcoming limitations of prior methods without restrictive assumptions.</description></item><item><title>Faster Differentially Private Top-$k$ Selection: A Joint Exponential Mechanism with Pruning</title><link>https://deep-diver.github.io/neurips2024/posters/qyxe3w9yni/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qyxe3w9yni/</guid><description>Faster differentially private top-k selection achieved via a novel joint exponential mechanism with pruning, reducing time complexity from O(dk) to O(d+k²/ɛlnd).</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qyxe3w9yni/cover.png"/></item><item><title>Faster Repeated Evasion Attacks in Tree Ensembles</title><link>https://deep-diver.github.io/neurips2024/posters/ugr0ypzy71/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ugr0ypzy71/</guid><description>Speed up repeated evasion attacks on tree ensembles by 36x using feature perturbation insights!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ugr0ypzy71/cover.png"/></item><item><title>Feedback control guides credit assignment in recurrent neural networks</title><link>https://deep-diver.github.io/neurips2024/posters/xavwvnjtst/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xavwvnjtst/</guid><description>Brain-inspired recurrent neural networks learn efficiently by using feedback control to approximate optimal gradients, enabling rapid movement corrections and efficient adaptation to persistent errors&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xavwvnjtst/cover.png"/></item><item><title>FEEL-SNN: Robust Spiking Neural Networks with Frequency Encoding and Evolutionary Leak Factor</title><link>https://deep-diver.github.io/neurips2024/posters/tucqdbo4nc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tucqdbo4nc/</guid><description>FEEL-SNN enhances spiking neural network robustness by mimicking biological visual attention and adaptive leak factors, resulting in improved resilience against noise and attacks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tucqdbo4nc/cover.png"/></item><item><title>FERERO: A Flexible Framework for Preference-Guided Multi-Objective Learning</title><link>https://deep-diver.github.io/neurips2024/posters/bmg3ngh5xu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bmg3ngh5xu/</guid><description>FERERO, a novel framework, tackles multi-objective learning by efficiently finding preference-guided Pareto solutions using flexible preference modeling and convergent algorithms.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bmg3ngh5xu/cover.png"/></item><item><title>First-Order Methods for Linearly Constrained Bilevel Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/encyptcghr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/encyptcghr/</guid><description>First-order methods conquer linearly constrained bilevel optimization, achieving near-optimal convergence rates and enhancing high-dimensional applicability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/encyptcghr/cover.png"/></item><item><title>From Causal to Concept-Based Representation Learning</title><link>https://deep-diver.github.io/neurips2024/posters/r5nev2shtj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/r5nev2shtj/</guid><description>This paper introduces a novel geometric approach to concept-based representation learning, provably recovering interpretable concepts from diverse data without strict causal assumptions or many interv&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/r5nev2shtj/cover.png"/></item><item><title>From Linear to Linearizable Optimization: A Novel Framework with Applications to Stationary and Non-stationary DR-submodular Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/dgamsmeef8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dgamsmeef8/</guid><description>A novel framework extends optimization algorithms from linear/quadratic functions to a broader class of &amp;lsquo;upper-linearizable&amp;rsquo; functions, providing a unified approach for concave and DR-submodular optim&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dgamsmeef8/cover.png"/></item><item><title>FUGAL: Feature-fortified Unrestricted Graph Alignment</title><link>https://deep-diver.github.io/neurips2024/posters/sdlos1fr4h/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sdlos1fr4h/</guid><description>FUGAL: a groundbreaking graph alignment method surpassing state-of-the-art accuracy without compromising efficiency by directly aligning adjacency matrices.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sdlos1fr4h/cover.png"/></item><item><title>Functionally Constrained Algorithm Solves Convex Simple Bilevel Problem</title><link>https://deep-diver.github.io/neurips2024/posters/paighjppam/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/paighjppam/</guid><description>Near-optimal algorithms solve convex simple bilevel problems by reformulating them into functionally constrained problems, achieving near-optimal convergence rates.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/paighjppam/cover.png"/></item><item><title>Fundamental Convergence Analysis of Sharpness-Aware Minimization</title><link>https://deep-diver.github.io/neurips2024/posters/puxyi4hoqu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/puxyi4hoqu/</guid><description>This research establishes fundamental convergence properties for the widely-used SAM optimization algorithm, significantly advancing our theoretical understanding and practical applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/puxyi4hoqu/cover.png"/></item><item><title>General bounds on the quality of Bayesian coresets</title><link>https://deep-diver.github.io/neurips2024/posters/sazeqv2ptt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sazeqv2ptt/</guid><description>New theoretical bounds on Bayesian coreset approximation errors enable efficient large-scale Bayesian inference, overcoming prior limitations and improving coreset construction methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sazeqv2ptt/cover.png"/></item><item><title>Generalizablity of Memorization Neural Network</title><link>https://deep-diver.github.io/neurips2024/posters/sabwo1ztfi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sabwo1ztfi/</guid><description>Unlocking deep learning&amp;rsquo;s generalization mystery, this research pioneers a theoretical understanding of memorization neural network generalizability, revealing critical network structural requirements&amp;hellip;</description></item><item><title>Generalization Analysis for Label-Specific Representation Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/dtpiuxdjhy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/dtpiuxdjhy/</guid><description>Researchers derived tighter generalization bounds for label-specific representation learning (LSRL) methods, improving understanding of LSRL&amp;rsquo;s success and offering guidance for future algorithm develo&amp;hellip;</description></item><item><title>Generalization Bound and Learning Methods for Data-Driven Projections in Linear Programming</title><link>https://deep-diver.github.io/neurips2024/posters/jhh804fz5l/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jhh804fz5l/</guid><description>Learn to project, solve faster! This paper introduces data-driven projections for solving high-dimensional linear programs, proving theoretical guarantees and demonstrating significant improvements in&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jhh804fz5l/cover.png"/></item><item><title>Generalization Bounds via Conditional $f$-Information</title><link>https://deep-diver.github.io/neurips2024/posters/ocxvxe5xn1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ocxvxe5xn1/</guid><description>New information-theoretic generalization bounds, based on conditional f-information, improve existing methods by addressing unboundedness and offering a generic approach applicable to various loss fun&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ocxvxe5xn1/cover.png"/></item><item><title>Generalization Error Bounds for Two-stage Recommender Systems with Tree Structure</title><link>https://deep-diver.github.io/neurips2024/oral-ai-theory/m1a4crrjr7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-ai-theory/m1a4crrjr7/</guid><description>Two-stage recommender systems using tree structures achieve better generalization with more branches and harmonized training data distributions across stages.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-ai-theory/m1a4crrjr7/cover.png"/></item><item><title>Generalization of Hamiltonian algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/0csq1sg7db/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0csq1sg7db/</guid><description>New, tighter generalization bounds are derived for a class of stochastic learning algorithms that generate absolutely continuous probability distributions; enhancing our understanding of their perform&amp;hellip;</description></item><item><title>Get rich quick: exact solutions reveal how unbalanced initializations promote rapid feature learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/enm94i7r3a/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/enm94i7r3a/</guid><description>Unbalanced initializations dramatically accelerate neural network feature learning by modifying the geometry of learning trajectories, enabling faster feature extraction and improved generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/enm94i7r3a/cover.png"/></item><item><title>Gliding over the Pareto Front with Uniform Designs</title><link>https://deep-diver.github.io/neurips2024/posters/woexvqchfw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/woexvqchfw/</guid><description>UMOD: a novel multi-objective optimization algorithm efficiently generates uniformly distributed Pareto-optimal solutions by maximizing minimal pairwise distances, providing high-quality representatio&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/woexvqchfw/cover.png"/></item><item><title>GLinSAT: The General Linear Satisfiability Neural Network Layer By Accelerated Gradient Descent</title><link>https://deep-diver.github.io/neurips2024/posters/m1pvjnhvtp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/m1pvjnhvtp/</guid><description>GLinSAT: A novel neural network layer efficiently solves general linear constraint satisfaction problems via accelerated gradient descent, enabling differentiable backpropagation and improved GPU perf&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/m1pvjnhvtp/cover.png"/></item><item><title>Global Convergence in Training Large-Scale Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/9wtlfrkwzs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/9wtlfrkwzs/</guid><description>Large-scale Transformer training&amp;rsquo;s global convergence is proven using weight decay regularization and a refined mean-field analysis, bridging theory and practice.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/9wtlfrkwzs/cover.png"/></item><item><title>Global Distortions from Local Rewards: Neural Coding Strategies in Path-Integrating Neural Systems</title><link>https://deep-diver.github.io/neurips2024/posters/938eyyewtq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/938eyyewtq/</guid><description>Reward-driven distortions in grid cell patterns are global, not local, preserving path integration while encoding environmental landmarks in spatial navigation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/938eyyewtq/cover.png"/></item><item><title>Globally Convergent Variational Inference</title><link>https://deep-diver.github.io/neurips2024/posters/8x48xflvyd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8x48xflvyd/</guid><description>Researchers achieve globally convergent variational inference by minimizing the expected forward KL divergence, overcoming the limitations of traditional methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8x48xflvyd/cover.png"/></item><item><title>Gradient Guidance for Diffusion Models: An Optimization Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/x1qeuybxke/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x1qeuybxke/</guid><description>This paper provides a novel optimization framework for guided diffusion models, proving Õ(1/K) convergence for concave objective functions and demonstrating structure-preserving guidance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x1qeuybxke/cover.png"/></item><item><title>Gradient Methods for Online DR-Submodular Maximization with Stochastic Long-Term Constraints</title><link>https://deep-diver.github.io/neurips2024/posters/ptxrruephq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ptxrruephq/</guid><description>Novel gradient-based algorithms achieve O(√T) regret and O(T3/4) constraint violation for online DR-submodular maximization with stochastic long-term constraints.</description></item><item><title>Gradient-Variation Online Learning under Generalized Smoothness</title><link>https://deep-diver.github.io/neurips2024/posters/v75gaxpw40/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v75gaxpw40/</guid><description>This paper presents a novel optimistic mirror descent algorithm achieving optimal gradient-variation regret under generalized smoothness, applicable across convex, strongly convex functions, and fast-&amp;hellip;</description></item><item><title>Graph Neural Networks and Arithmetic Circuits</title><link>https://deep-diver.github.io/neurips2024/posters/0zeonp33f0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0zeonp33f0/</guid><description>Graph Neural Networks&amp;rsquo; (GNNs) computational power precisely mirrors that of arithmetic circuits, as proven via a novel C-GNN model; this reveals fundamental limits to GNN scalability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0zeonp33f0/cover.png"/></item><item><title>Graphcode: Learning from multiparameter persistent homology using graph neural networks</title><link>https://deep-diver.github.io/neurips2024/posters/o23xftnhwr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/o23xftnhwr/</guid><description>Graphcodes efficiently summarize complex datasets&amp;rsquo; topological properties using graph neural networks, enhancing machine learning accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/o23xftnhwr/cover.png"/></item><item><title>GraphTrail: Translating GNN Predictions into Human-Interpretable Logical Rules</title><link>https://deep-diver.github.io/neurips2024/posters/fzlmza6drz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fzlmza6drz/</guid><description>GRAPHTRAIL unveils the first end-to-end global GNN explainer, translating black-box GNN predictions into easily interpretable boolean formulas over subgraph concepts, achieving significant improvement&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fzlmza6drz/cover.png"/></item><item><title>GREAT Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models</title><link>https://deep-diver.github.io/neurips2024/posters/vunjcq9pwu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vunjcq9pwu/</guid><description>GREAT Score: A novel framework using generative models for efficiently and accurately evaluating the global robustness of machine learning models against adversarial attacks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vunjcq9pwu/cover.png"/></item><item><title>Group-wise oracle-efficient algorithms for online multi-group learning</title><link>https://deep-diver.github.io/neurips2024/posters/klsyhjllx5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/klsyhjllx5/</guid><description>Oracle-efficient algorithms conquer online multi-group learning, achieving sublinear regret even with massive, overlapping groups, paving the way for fair and efficient large-scale online systems.</description></item><item><title>HardCore Generation: Generating Hard UNSAT Problems for Data Augmentation</title><link>https://deep-diver.github.io/neurips2024/posters/njvpjg0bfk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/njvpjg0bfk/</guid><description>HardCore: Fast generation of hard, realistic UNSAT problems for improved SAT solver runtime prediction.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/njvpjg0bfk/cover.png"/></item><item><title>Harnessing Multiple Correlated Networks for Exact Community Recovery</title><link>https://deep-diver.github.io/neurips2024/posters/7fzx3akdt5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7fzx3akdt5/</guid><description>Unlocking latent community structures from multiple correlated networks is now possible with greater precision, as this research pinpoints the information-theoretic threshold for exact recovery, even &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7fzx3akdt5/cover.png"/></item><item><title>High-probability complexity bounds for stochastic non-convex minimax optimization</title><link>https://deep-diver.github.io/neurips2024/posters/xmqtnzlgtj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xmqtnzlgtj/</guid><description>First high-probability complexity guarantees for solving stochastic nonconvex minimax problems using a single-loop method are established.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xmqtnzlgtj/cover.png"/></item><item><title>Higher-Order Causal Message Passing for Experimentation with Complex Interference</title><link>https://deep-diver.github.io/neurips2024/posters/3vjbgcjgvd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3vjbgcjgvd/</guid><description>Higher-Order Causal Message Passing (HO-CMP) accurately estimates treatment effects in complex systems with unknown interference by using observed data to learn the system&amp;rsquo;s dynamics over time.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3vjbgcjgvd/cover.png"/></item><item><title>Honor Among Bandits: No-Regret Learning for Online Fair Division</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ocqbc0edjj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ocqbc0edjj/</guid><description>Online fair division algorithm achieves Õ(T²/³) regret while guaranteeing envy-freeness or proportionality in expectation, a result proven tight.</description></item><item><title>How Does Black-Box Impact the Learning Guarantee of Stochastic Compositional Optimization?</title><link>https://deep-diver.github.io/neurips2024/posters/4aueq1ffuf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4aueq1ffuf/</guid><description>This study reveals how black-box settings affect the learning guarantee of stochastic compositional optimization, offering sharper generalization bounds and novel learning guarantees for derivative-fr&amp;hellip;</description></item><item><title>How does Gradient Descent Learn Features --- A Local Analysis for Regularized Two-Layer Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/xyw051zmun/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xyw051zmun/</guid><description>Neural networks learn features effectively through gradient descent, not just at the beginning, but also at the end of training, even with carefully regularized objectives.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xyw051zmun/cover.png"/></item><item><title>How does PDE order affect the convergence of PINNs?</title><link>https://deep-diver.github.io/neurips2024/posters/8k6ul0hgtc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8k6ul0hgtc/</guid><description>Higher-order PDEs hinder Physics-Informed Neural Network (PINN) convergence; this paper provides theoretical explanation and proposes variable splitting for improved accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8k6ul0hgtc/cover.png"/></item><item><title>How to Boost Any Loss Function</title><link>https://deep-diver.github.io/neurips2024/posters/mlgfu6dqyc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mlgfu6dqyc/</guid><description>Boosting, traditionally limited by assumptions about loss functions, is proven in this paper to efficiently optimize any loss function regardless of differentiability or convexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mlgfu6dqyc/cover.png"/></item><item><title>Hybrid Top-Down Global Causal Discovery with Local Search for Linear and Nonlinear Additive Noise Models</title><link>https://deep-diver.github.io/neurips2024/posters/xnmm1jthkv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xnmm1jthkv/</guid><description>Hybrid causal discovery algorithm efficiently learns unique causal graphs from observational data by leveraging local substructures and topological sorting, outperforming existing methods in accuracy &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xnmm1jthkv/cover.png"/></item><item><title>Identifiability Guarantees for Causal Disentanglement from Purely Observational Data</title><link>https://deep-diver.github.io/neurips2024/posters/m20p6tq9hq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/m20p6tq9hq/</guid><description>This paper provides identifiability guarantees for causal disentanglement from purely observational data using nonlinear additive Gaussian noise models, addressing a major challenge in causal represen&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/m20p6tq9hq/cover.png"/></item><item><title>Identification and Estimation of the Bi-Directional MR with Some Invalid Instruments</title><link>https://deep-diver.github.io/neurips2024/oral-ai-theory/s2p6kpltm8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-ai-theory/s2p6kpltm8/</guid><description>PReBiM algorithm accurately estimates bi-directional causal effects from observational data, even with invalid instruments, using a novel cluster fusion approach.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-ai-theory/s2p6kpltm8/cover.png"/></item><item><title>Identification of Analytic Nonlinear Dynamical Systems with Non-asymptotic Guarantees</title><link>https://deep-diver.github.io/neurips2024/posters/nf34qxcy0b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nf34qxcy0b/</guid><description>This paper proves that non-active exploration suffices for identifying linearly parameterized nonlinear systems with real-analytic features, providing non-asymptotic guarantees for least-squares and s&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nf34qxcy0b/cover.png"/></item><item><title>Identifying Causal Effects Under Functional Dependencies</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/oisuwqsvkd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/oisuwqsvkd/</guid><description>Unlocking identifiability of causal effects: This paper leverages functional dependencies in causal graphs to improve identifiability, leading to fewer needed variables in observational data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/oisuwqsvkd/cover.png"/></item><item><title>Identifying Equivalent Training Dynamics</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/boyvesx7pk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/boyvesx7pk/</guid><description>New framework uses Koopman operator theory to identify equivalent training dynamics in deep neural networks, enabling quantitative comparison of different architectures and optimization methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/boyvesx7pk/cover.png"/></item><item><title>Identifying Functionally Important Features with End-to-End Sparse Dictionary Learning</title><link>https://deep-diver.github.io/neurips2024/posters/7txpaupunc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7txpaupunc/</guid><description>End-to-end sparse autoencoders revolutionize neural network interpretability by learning functionally important features, outperforming traditional methods in efficiency and accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7txpaupunc/cover.png"/></item><item><title>Identifying General Mechanism Shifts in Linear Causal Representations</title><link>https://deep-diver.github.io/neurips2024/posters/jwaxhcytv1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jwaxhcytv1/</guid><description>Researchers can now pinpoint the sources of data shifts in complex linear causal systems using a new algorithm, even with limited perfect interventions, opening exciting possibilities for causal disco&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jwaxhcytv1/cover.png"/></item><item><title>If You Want to Be Robust, Be Wary of Initialization</title><link>https://deep-diver.github.io/neurips2024/posters/nxumywxjpb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nxumywxjpb/</guid><description>Proper weight initialization significantly boosts Graph Neural Network (GNN) and Deep Neural Network (DNN) robustness against adversarial attacks, highlighting a critical, often-overlooked factor.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nxumywxjpb/cover.png"/></item><item><title>Implicit Bias of Mirror Flow on Separable Data</title><link>https://deep-diver.github.io/neurips2024/posters/wimaws0fwb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wimaws0fwb/</guid><description>Mirror descent&amp;rsquo;s implicit bias on separable data is formally characterized, revealing convergence towards a maximum margin classifier determined by the potential&amp;rsquo;s &amp;lsquo;horizon function&amp;rsquo;.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wimaws0fwb/cover.png"/></item><item><title>Implicit Regularization of Decentralized Gradient Descent for Sparse Regression</title><link>https://deep-diver.github.io/neurips2024/posters/mladrqi0wf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mladrqi0wf/</guid><description>Decentralized Gradient Descent achieves statistically optimal sparse model learning via implicit regularization, even with communication-efficient truncation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mladrqi0wf/cover.png"/></item><item><title>Implicit Regularization Paths of Weighted Neural Representations</title><link>https://deep-diver.github.io/neurips2024/posters/oxcmwwkqtz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oxcmwwkqtz/</guid><description>Weighted pretrained features implicitly regularize models, and this paper reveals equivalent paths between weighting schemes and ridge regularization, enabling efficient hyperparameter tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oxcmwwkqtz/cover.png"/></item><item><title>Improved Algorithms for Contextual Dynamic Pricing</title><link>https://deep-diver.github.io/neurips2024/posters/imeahxdinp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/imeahxdinp/</guid><description>New algorithms achieve optimal regret bounds for contextual dynamic pricing under minimal assumptions, improving revenue management with better price adjustments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/imeahxdinp/cover.png"/></item><item><title>Improved Analysis for Bandit Learning in Matching Markets</title><link>https://deep-diver.github.io/neurips2024/posters/07n0qoaz2l/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/07n0qoaz2l/</guid><description>A new algorithm, AOGS, achieves significantly lower regret in two-sided matching markets by cleverly integrating exploration and exploitation, thus removing the dependence on the number of arms (K) in&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/07n0qoaz2l/cover.png"/></item><item><title>Improved Guarantees for Fully Dynamic $k$-Center Clustering with Outliers in General Metric Spaces</title><link>https://deep-diver.github.io/neurips2024/posters/otycp1yfbx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/otycp1yfbx/</guid><description>A novel fully dynamic algorithm achieves a (4+ε)-approximate solution for the k-center clustering problem with outliers in general metric spaces, boasting an efficient update time.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/otycp1yfbx/cover.png"/></item><item><title>Improved Regret for Bandit Convex Optimization with Delayed Feedback</title><link>https://deep-diver.github.io/neurips2024/posters/ar9jvkogjm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ar9jvkogjm/</guid><description>A novel algorithm, D-FTBL, achieves improved regret bounds for bandit convex optimization with delayed feedback, tightly matching existing lower bounds in worst-case scenarios.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ar9jvkogjm/cover.png"/></item><item><title>Improving Adaptivity via Over-Parameterization in Sequence Models</title><link>https://deep-diver.github.io/neurips2024/posters/uflh4t676k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uflh4t676k/</guid><description>Over-parameterized gradient descent dynamically adapts to signal structure, improving sequence model generalization and outperforming fixed-kernel methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uflh4t676k/cover.png"/></item><item><title>Improving Adversarial Robust Fairness via Anti-Bias Soft Label Distillation</title><link>https://deep-diver.github.io/neurips2024/posters/kw30lbnwdv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kw30lbnwdv/</guid><description>Boosting adversarial robustness fairness in deep neural networks, Anti-Bias Soft Label Distillation (ABSLD) adaptively adjusts soft label smoothness to reduce error gap between classes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kw30lbnwdv/cover.png"/></item><item><title>Improving Alignment and Robustness with Circuit Breakers</title><link>https://deep-diver.github.io/neurips2024/posters/ibib8sbkfv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ibib8sbkfv/</guid><description>AI systems are made safer by &amp;lsquo;circuit breakers&amp;rsquo; that directly control harmful internal representations, significantly improving alignment and robustness against adversarial attacks with minimal impact&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ibib8sbkfv/cover.png"/></item><item><title>Improving Decision Sparsity</title><link>https://deep-diver.github.io/neurips2024/posters/ghqdnlzmaz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ghqdnlzmaz/</guid><description>Boosting machine learning model interpretability, this paper introduces cluster-based and tree-based Sparse Explanation Values (SEV) for generating more meaningful and credible explanations by optimiz&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ghqdnlzmaz/cover.png"/></item><item><title>Improving Subgroup Robustness via Data Selection</title><link>https://deep-diver.github.io/neurips2024/posters/vjltccbzvt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vjltccbzvt/</guid><description>Data Debiasing with Datamodels (D3M) efficiently improves machine learning model robustness by identifying and removing specific training examples that disproportionately harm minority groups&amp;rsquo; accurac&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vjltccbzvt/cover.png"/></item><item><title>Incorporating Surrogate Gradient Norm to Improve Offline Optimization Techniques</title><link>https://deep-diver.github.io/neurips2024/posters/ag7piyoyut/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ag7piyoyut/</guid><description>IGNITE improves offline optimization by incorporating surrogate gradient norm to reduce model sharpness, boosting performance up to 9.6%</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ag7piyoyut/cover.png"/></item><item><title>Inexact Augmented Lagrangian Methods for Conic Optimization: Quadratic Growth and Linear Convergence</title><link>https://deep-diver.github.io/neurips2024/posters/sj8g020adl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sj8g020adl/</guid><description>This paper proves that inexact ALMs applied to SDPs achieve &lt;strong>linear convergence for both primal and dual iterates&lt;/strong>, contingent solely on strict complementarity and a bounded solution set, thus resol&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sj8g020adl/cover.png"/></item><item><title>Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference</title><link>https://deep-diver.github.io/neurips2024/posters/pocs4jq7cv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pocs4jq7cv/</guid><description>Contrastive learning enables efficient probabilistic inference in high-dimensional time series by creating Gaussian representations that form a Gauss-Markov chain, allowing for closed-form solutions t&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pocs4jq7cv/cover.png"/></item><item><title>Information-theoretic Generalization Analysis for Expected Calibration Error</title><link>https://deep-diver.github.io/neurips2024/posters/yltjalwtw9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yltjalwtw9/</guid><description>New theoretical analysis reveals optimal binning strategies for minimizing bias in expected calibration error (ECE), improving machine learning model calibration evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yltjalwtw9/cover.png"/></item><item><title>Information-theoretic Limits of Online Classification with Noisy Labels</title><link>https://deep-diver.github.io/neurips2024/posters/ke3msp8nr6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ke3msp8nr6/</guid><description>This paper unveils the information-theoretic limits of online classification with noisy labels, showing that the minimax risk is tightly characterized by the Hellinger gap of noisy label distributions&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ke3msp8nr6/cover.png"/></item><item><title>Injecting Undetectable Backdoors in Obfuscated Neural Networks and Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/kyvbzkcono/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kyvbzkcono/</guid><description>Researchers developed a novel method to inject undetectable backdoors into obfuscated neural networks and language models, even with white-box access, posing significant security risks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kyvbzkcono/cover.png"/></item><item><title>Instance-Optimal Private Density Estimation in the Wasserstein Distance</title><link>https://deep-diver.github.io/neurips2024/posters/apq6corvfz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/apq6corvfz/</guid><description>Instance-optimal private density estimation algorithms, adapting to data characteristics for improved accuracy in the Wasserstein distance, are introduced.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/apq6corvfz/cover.png"/></item><item><title>Instance-Specific Asymmetric Sensitivity in Differential Privacy</title><link>https://deep-diver.github.io/neurips2024/posters/4i2aeav51n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4i2aeav51n/</guid><description>New algorithm improves differentially private estimations by adapting to dataset hardness, enhancing accuracy for variance, classification, and regression tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4i2aeav51n/cover.png"/></item><item><title>Interpolating Item and User Fairness in Multi-Sided Recommendations</title><link>https://deep-diver.github.io/neurips2024/posters/taog1hdvgy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/taog1hdvgy/</guid><description>Problem (FAIR) framework and FORM algorithm achieve flexible multi-stakeholder fairness in online recommendation systems, balancing platform revenue with user and item fairness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/taog1hdvgy/cover.png"/></item><item><title>Interpretable Concept-Based Memory Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/willwyvmp8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/willwyvmp8/</guid><description>CMR: A novel Concept-Based Memory Reasoner delivers human-understandable, verifiable AI task predictions by using a neural selection mechanism over a set of human-understandable logic rules, achievin&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/willwyvmp8/cover.png"/></item><item><title>Intervention and Conditioning in Causal Bayesian Networks</title><link>https://deep-diver.github.io/neurips2024/posters/dc28fpk76s/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dc28fpk76s/</guid><description>Researchers uniquely estimate probabilities in Causal Bayesian Networks using simple independence assumptions, enabling analysis from observational data and simplifying counterfactual probability calc&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dc28fpk76s/cover.png"/></item><item><title>Interventional Causal Discovery in a Mixture of DAGs</title><link>https://deep-diver.github.io/neurips2024/posters/mfrlci8sov/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mfrlci8sov/</guid><description>This study presents CADIM, an adaptive algorithm using interventions to learn true causal relationships from mixtures of DAGs, achieving near-optimal intervention sizes and providing quantifiable opti&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mfrlci8sov/cover.png"/></item><item><title>Interventionally Consistent Surrogates for Complex Simulation Models</title><link>https://deep-diver.github.io/neurips2024/posters/uttjgmdtfo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uttjgmdtfo/</guid><description>This paper introduces a novel framework for creating interventionally consistent surrogate models for complex simulations, addressing computational limitations and ensuring accurate policy evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uttjgmdtfo/cover.png"/></item><item><title>Intrinsic Robustness of Prophet Inequality to Strategic Reward Signaling</title><link>https://deep-diver.github.io/neurips2024/posters/mmcy1p15hc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mmcy1p15hc/</guid><description>Strategic players can manipulate reward signals, but simple threshold policies still achieve a surprisingly good approximation to the optimal prophet value, even in this more realistic setting.</description></item><item><title>Intruding with Words: Towards Understanding Graph Injection Attacks at the Text Level</title><link>https://deep-diver.github.io/neurips2024/posters/otzydukwpq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/otzydukwpq/</guid><description>Researchers unveil text-level graph injection attacks, revealing a new vulnerability in GNNs and highlighting the importance of text interpretability in attack success.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/otzydukwpq/cover.png"/></item><item><title>Invariant subspaces and PCA in nearly matrix multiplication time</title><link>https://deep-diver.github.io/neurips2024/posters/wyp8vsl9de/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wyp8vsl9de/</guid><description>Generalized eigenvalue problems get solved in nearly matrix multiplication time, providing new, faster PCA algorithms!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wyp8vsl9de/cover.png"/></item><item><title>IPM-LSTM: A Learning-Based Interior Point Method for Solving Nonlinear Programs</title><link>https://deep-diver.github.io/neurips2024/posters/9c3iiawein/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/9c3iiawein/</guid><description>IPM-LSTM accelerates nonlinear program solving by up to 70% using LSTM networks to approximate linear system solutions within the interior point method.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/9c3iiawein/cover.png"/></item><item><title>Is Cross-validation the Gold Standard to Estimate Out-of-sample Model Performance?</title><link>https://deep-diver.github.io/neurips2024/posters/4lgpsbge11/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4lgpsbge11/</guid><description>Cross-validation isn&amp;rsquo;t always superior; simple plug-in methods often perform equally well for estimating out-of-sample model performance, especially when considering computational costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4lgpsbge11/cover.png"/></item><item><title>Is Knowledge Power? On the (Im)possibility of Learning from Strategic Interactions</title><link>https://deep-diver.github.io/neurips2024/posters/dlm6z1rrjv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dlm6z1rrjv/</guid><description>In strategic settings, repeated interactions alone may not enable uninformed players to achieve optimal outcomes, highlighting the persistent impact of information asymmetry.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dlm6z1rrjv/cover.png"/></item><item><title>Is O(log N) practical? Near-Equivalence Between Delay Robustness and Bounded Regret in Bandits and RL</title><link>https://deep-diver.github.io/neurips2024/posters/hyjofwfw1p/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hyjofwfw1p/</guid><description>Zero Graves-Lai constant ensures both bounded regret and delay robustness in online decision-making, particularly for linear models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hyjofwfw1p/cover.png"/></item><item><title>Is Score Matching Suitable for Estimating Point Processes?</title><link>https://deep-diver.github.io/neurips2024/posters/hqghcvzihw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hqghcvzihw/</guid><description>Weighted score matching offers a consistent, efficient solution for estimating parameters in point processes, overcoming the limitations of previous methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hqghcvzihw/cover.png"/></item><item><title>Iterative Methods via Locally Evolving Set Process</title><link>https://deep-diver.github.io/neurips2024/posters/wt2kheb97a/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wt2kheb97a/</guid><description>This paper proposes a novel framework, the locally evolving set process, to develop faster localized iterative methods for solving large-scale graph problems, achieving significant speedup over existi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wt2kheb97a/cover.png"/></item><item><title>John Ellipsoids via Lazy Updates</title><link>https://deep-diver.github.io/neurips2024/posters/lcj0rvr4d6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lcj0rvr4d6/</guid><description>Faster John ellipsoid computation achieved via lazy updates and fast matrix multiplication, improving efficiency and enabling low-space streaming algorithms.</description></item><item><title>Langevin Unlearning: A New Perspective of Noisy Gradient Descent for Machine Unlearning</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3lkuc8rbyv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3lkuc8rbyv/</guid><description>Langevin unlearning offers a novel, privacy-preserving machine unlearning framework based on noisy gradient descent, handling both convex and non-convex problems efficiently.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3lkuc8rbyv/cover.png"/></item><item><title>Last-Iterate Convergence for Generalized Frank-Wolfe in Monotone Variational Inequalities</title><link>https://deep-diver.github.io/neurips2024/posters/ejknsersmj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ejknsersmj/</guid><description>Generalized Frank-Wolfe algorithm achieves fast last-iterate convergence for constrained monotone variational inequalities, even with noisy data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ejknsersmj/cover.png"/></item><item><title>Latent Neural Operator for Solving Forward and Inverse PDE Problems</title><link>https://deep-diver.github.io/neurips2024/posters/vlw8zykfcm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vlw8zykfcm/</guid><description>Latent Neural Operator (LNO) dramatically improves solving PDEs by using a latent space, boosting accuracy and reducing computation costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vlw8zykfcm/cover.png"/></item><item><title>Learnability of high-dimensional targets by two-parameter models and gradient flow</title><link>https://deep-diver.github.io/neurips2024/posters/8xowofmzki/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8xowofmzki/</guid><description>Two-parameter models can surprisingly learn high-dimensional targets with near-perfect accuracy using gradient flow, challenging the need for high-dimensional models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8xowofmzki/cover.png"/></item><item><title>Learning a Single Neuron Robustly to Distributional Shifts and Adversarial Label Noise</title><link>https://deep-diver.github.io/neurips2024/posters/rv5dug4jcz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rv5dug4jcz/</guid><description>This work presents a computationally efficient algorithm that robustly learns a single neuron despite adversarial label noise and distributional shifts, providing provable approximation guarantees.</description></item><item><title>Learning Better Representations From Less Data For Propositional Satisfiability</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/vmshnv8cvs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/vmshnv8cvs/</guid><description>NeuRes, a novel neuro-symbolic approach, achieves superior SAT solving accuracy using significantly less training data than existing methods by combining certificate-driven learning with expert iterat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/vmshnv8cvs/cover.png"/></item><item><title>Learning Cut Generating Functions for Integer Programming</title><link>https://deep-diver.github.io/neurips2024/posters/8mzc259r8x/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8mzc259r8x/</guid><description>This research develops data-driven methods for selecting optimal cut generating functions in integer programming, providing theoretical guarantees and empirical improvements over existing techniques.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8mzc259r8x/cover.png"/></item><item><title>Learning diffusion at lightspeed</title><link>https://deep-diver.github.io/neurips2024/oral-ai-theory/y10avdrfnk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-ai-theory/y10avdrfnk/</guid><description>JKOnet* learns diffusion processes at unprecedented speed and accuracy by directly minimizing a simple quadratic loss function, bypassing complex bilevel optimization problems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-ai-theory/y10avdrfnk/cover.png"/></item><item><title>Learning Discrete Concepts in Latent Hierarchical Models</title><link>https://deep-diver.github.io/neurips2024/posters/bo5buxvh6m/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bo5buxvh6m/</guid><description>This paper introduces a novel framework for learning discrete concepts from high-dimensional data, establishing theoretical conditions for identifying underlying hierarchical causal structures and pro&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bo5buxvh6m/cover.png"/></item><item><title>Learning Discrete Latent Variable Structures with Tensor Rank Conditions</title><link>https://deep-diver.github.io/neurips2024/posters/6eqfoqklsw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/6eqfoqklsw/</guid><description>This paper introduces a novel tensor rank condition for identifying causal structures among discrete latent variables, advancing causal discovery in complex scenarios.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/6eqfoqklsw/cover.png"/></item><item><title>Learning diverse causally emergent representations from time series data</title><link>https://deep-diver.github.io/neurips2024/posters/z6relfqv6w/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z6relfqv6w/</guid><description>AI learns emergent system features from time-series data using a novel differentiable architecture maximizing causal emergence, outperforming pure mutual information maximization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/z6relfqv6w/cover.png"/></item><item><title>Learning Elastic Costs to Shape Monge Displacements</title><link>https://deep-diver.github.io/neurips2024/posters/aauvnpqvbz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/aauvnpqvbz/</guid><description>Learn optimal transport maps with structured displacements using elastic costs and a novel bilevel loss function!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/aauvnpqvbz/cover.png"/></item><item><title>Learning from Snapshots of Discrete and Continuous Data Streams</title><link>https://deep-diver.github.io/neurips2024/posters/gxwnq8sxkl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gxwnq8sxkl/</guid><description>This paper presents novel theoretical frameworks and algorithms for learning from snapshots of discrete and continuous data streams, resolving key learnability challenges in online learning under cont&amp;hellip;</description></item><item><title>Learning from Uncertain Data: From Possible Worlds to Possible Models</title><link>https://deep-diver.github.io/neurips2024/posters/v9rqrfslq2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v9rqrfslq2/</guid><description>ZORRO: A new method for learning linear models from uncertain data, providing sound over-approximations of all possible models and prediction ranges.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v9rqrfslq2/cover.png"/></item><item><title>Learning Generalized Linear Programming Value Functions</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/vxijl0ioid/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/vxijl0ioid/</guid><description>Learn optimal LP values faster with a novel neural network method!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/vxijl0ioid/cover.png"/></item><item><title>Learning Human-like Representations to Enable Learning Human Values</title><link>https://deep-diver.github.io/neurips2024/posters/sqapqmbqip/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sqapqmbqip/</guid><description>Aligning AI&amp;rsquo;s world representation with humans enables faster, safer learning of human values, improving both exploration and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sqapqmbqip/cover.png"/></item><item><title>Learning Identifiable Factorized Causal Representations of Cellular Responses</title><link>https://deep-diver.github.io/neurips2024/posters/ahlabdhmqh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ahlabdhmqh/</guid><description>FCR, a novel method, reveals causal structure in single-cell perturbation data by learning disentangled cellular representations specific to covariates, treatments, and their interactions, outperformi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ahlabdhmqh/cover.png"/></item><item><title>Learning Linear Causal Representations from General Environments: Identifiability and Intrinsic Ambiguity</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/db99jjwx3h/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/db99jjwx3h/</guid><description>LiNGCREL, a novel algorithm, provably recovers linear causal representations from diverse environments, achieving identifiability despite intrinsic ambiguities, thus advancing causal AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/db99jjwx3h/cover.png"/></item><item><title>Learning Mixtures of Unknown Causal Interventions</title><link>https://deep-diver.github.io/neurips2024/posters/ac9mb1pqyj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ac9mb1pqyj/</guid><description>Researchers developed an efficient algorithm to uniquely identify causal relationships from mixed interventional and observational data with noisy interventions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ac9mb1pqyj/cover.png"/></item><item><title>Learning Neural Contracting Dynamics: Extended Linearization and Global Guarantees</title><link>https://deep-diver.github.io/neurips2024/posters/yynp3xpv3y/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yynp3xpv3y/</guid><description>ELCD: The first neural network guaranteeing globally contracting dynamics!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yynp3xpv3y/cover.png"/></item><item><title>Learning Optimal Tax Design in Nonatomic Congestion Games</title><link>https://deep-diver.github.io/neurips2024/posters/qdprhde3jb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qdprhde3jb/</guid><description>AI learns optimal taxes for congestion games, maximizing social welfare with limited feedback, via a novel algorithm.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qdprhde3jb/cover.png"/></item><item><title>Learning Partitions from Context</title><link>https://deep-diver.github.io/neurips2024/posters/prsgf5vdd0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/prsgf5vdd0/</guid><description>Learning hidden structures from sparse interactions in data is computationally hard but can be achieved with sufficient samples using gradient-based methods; This is shown by analyzing the gradient dy&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/prsgf5vdd0/cover.png"/></item><item><title>Learning Place Cell Representations and Context-Dependent Remapping</title><link>https://deep-diver.github.io/neurips2024/posters/7eshfpqjno/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7eshfpqjno/</guid><description>Neural networks learn place cell-like representations and context-dependent remapping using a novel similarity-based objective function, providing insights into hippocampal encoding.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7eshfpqjno/cover.png"/></item><item><title>Learning Plaintext-Ciphertext Cryptographic Problems via ANF-based SAT Instance Representation</title><link>https://deep-diver.github.io/neurips2024/posters/fzwaqjk4cg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fzwaqjk4cg/</guid><description>CryptoANFNet accelerates solving cryptographic problems by 50x using a novel graph neural network and ANF representation, outperforming existing methods in accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fzwaqjk4cg/cover.png"/></item><item><title>Learning Representations for Hierarchies with Minimal Support</title><link>https://deep-diver.github.io/neurips2024/posters/hfs800rezk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hfs800rezk/</guid><description>Learn graph representations efficiently by identifying the minimal data needed to uniquely define a graph&amp;rsquo;s structure, achieving robust performance with fewer resources.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hfs800rezk/cover.png"/></item><item><title>Learning Social Welfare Functions</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/7o6ktaar8n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/7o6ktaar8n/</guid><description>Learning social welfare functions from past decisions is possible! This paper shows how to efficiently learn power mean functions, a widely used family, using both cardinal and pairwise welfare compar&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/7o6ktaar8n/cover.png"/></item><item><title>Learning Structure-Aware Representations of Dependent Types</title><link>https://deep-diver.github.io/neurips2024/posters/e397soezh8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/e397soezh8/</guid><description>This research pioneers the integration of machine learning with the dependently-typed programming language Agda, introducing a novel dataset and neural architecture for faithful program representation&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/e397soezh8/cover.png"/></item><item><title>Learning the Expected Core of Strictly Convex Stochastic Cooperative Games</title><link>https://deep-diver.github.io/neurips2024/posters/zryfftr4xn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zryfftr4xn/</guid><description>A novel Common-Points-Picking algorithm efficiently learns stable reward allocations (expected core) in strictly convex stochastic cooperative games with unknown reward distributions, achieving high p&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zryfftr4xn/cover.png"/></item><item><title>Learning to compute Gröbner bases</title><link>https://deep-diver.github.io/neurips2024/posters/zrz7xlxbzq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zrz7xlxbzq/</guid><description>AI learns to compute Gröbner bases, solving a notorious computational algebra problem efficiently via Transformers and novel algebraic techniques.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zrz7xlxbzq/cover.png"/></item><item><title>Learning to Handle Complex Constraints for Vehicle Routing Problems</title><link>https://deep-diver.github.io/neurips2024/posters/ktx95zurjp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ktx95zurjp/</guid><description>Proactive Infeasibility Prevention (PIP) framework significantly improves neural methods for solving complex Vehicle Routing Problems by proactively preventing infeasible solutions and enhancing const&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ktx95zurjp/cover.png"/></item><item><title>Learning to Mitigate Externalities: the Coase Theorem with Hindsight Rationality</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/omyzrkacme/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/omyzrkacme/</guid><description>Economists learn to resolve externalities efficiently even when players lack perfect information, maximizing social welfare by leveraging bargaining and online learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/omyzrkacme/cover.png"/></item><item><title>Learning to Solve Quadratic Unconstrained Binary Optimization in a Classification Way</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/p43obiwjfw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/p43obiwjfw/</guid><description>Researchers developed Value Classification Model (VCM), a neural solver that swiftly solves quadratic unconstrained binary optimization (QUBO) problems by directly generating solutions using a classif&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/p43obiwjfw/cover.png"/></item><item><title>Learning to Understand: Identifying Interactions via the Möbius Transform</title><link>https://deep-diver.github.io/neurips2024/posters/glgexu1zg4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/glgexu1zg4/</guid><description>Unlocking complex models&amp;rsquo; secrets: New algorithm identifies input interactions using the Möbius Transform, boosting interpretability with surprising speed and accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/glgexu1zg4/cover.png"/></item><item><title>Learning with Fitzpatrick Losses</title><link>https://deep-diver.github.io/neurips2024/posters/7dep87tmjs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7dep87tmjs/</guid><description>Tighter losses than Fenchel-Young losses are presented, refining Fenchel-Young inequalities using the Fitzpatrick function to improve model accuracy while preserving prediction link functions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7dep87tmjs/cover.png"/></item><item><title>Learning-Augmented Algorithms for the Bahncard Problem</title><link>https://deep-diver.github.io/neurips2024/posters/3cb6pf3tvf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3cb6pf3tvf/</guid><description>PFSUM, a novel learning-augmented algorithm, leverages short-term predictions to achieve superior performance in solving the Bahncard problem, outperforming existing methods with improved consistency &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3cb6pf3tvf/cover.png"/></item><item><title>Learning-Augmented Algorithms with Explicit Predictors</title><link>https://deep-diver.github.io/neurips2024/posters/0xkvw4ijxp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0xkvw4ijxp/</guid><description>This paper introduces a novel framework for learning-augmented algorithms that improves performance by integrating the learning process into the algorithm itself, rather than treating the predictor as&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0xkvw4ijxp/cover.png"/></item><item><title>Learning-Augmented Approximation Algorithms for Maximum Cut and Related Problems</title><link>https://deep-diver.github.io/neurips2024/posters/mirkqqx6po/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mirkqqx6po/</guid><description>This paper shows how noisy predictions about optimal solutions can improve approximation algorithms for NP-hard problems like MAX-CUT, exceeding classical hardness bounds.</description></item><item><title>Learning-Augmented Dynamic Submodular Maximization</title><link>https://deep-diver.github.io/neurips2024/posters/sty80vvbs8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sty80vvbs8/</guid><description>Leveraging predictions, this paper presents a novel algorithm for dynamic submodular maximization achieving significantly faster update times (O(poly(log n, log w, log k)) amortized) compared to exist&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sty80vvbs8/cover.png"/></item><item><title>Learning-Augmented Priority Queues</title><link>https://deep-diver.github.io/neurips2024/posters/1atllgvuru/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1atllgvuru/</guid><description>This paper introduces learning-augmented priority queues, using predictions to boost efficiency and optimality, achieving significant performance gains over traditional methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1atllgvuru/cover.png"/></item><item><title>Least Squares Regression Can Exhibit Under-Parameterized Double Descent</title><link>https://deep-diver.github.io/neurips2024/posters/gzh9ntutsy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gzh9ntutsy/</guid><description>Under-parameterized linear regression models can surprisingly exhibit double descent, contradicting traditional bias-variance assumptions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gzh9ntutsy/cover.png"/></item><item><title>Linear Causal Bandits: Unknown Graph and Soft Interventions</title><link>https://deep-diver.github.io/neurips2024/posters/pau0w5yakc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pau0w5yakc/</guid><description>Causal bandits with unknown graphs and soft interventions are solved by establishing novel upper and lower regret bounds, plus a computationally efficient algorithm.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pau0w5yakc/cover.png"/></item><item><title>Linear Causal Representation Learning from Unknown Multi-node Interventions</title><link>https://deep-diver.github.io/neurips2024/posters/weemasptzg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/weemasptzg/</guid><description>Unlocking Causal Structures: New algorithms identify latent causal relationships from interventions, even when multiple variables are affected simultaneously.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/weemasptzg/cover.png"/></item><item><title>Localized Adaptive Risk Control</title><link>https://deep-diver.github.io/neurips2024/posters/fogjgrozu1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fogjgrozu1/</guid><description>Localized Adaptive Risk Control (L-ARC) improves fairness and reliability of online prediction by providing localized statistical risk guarantees, surpassing existing methods in high-stakes applicatio&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fogjgrozu1/cover.png"/></item><item><title>Locally Private and Robust Multi-Armed Bandits</title><link>https://deep-diver.github.io/neurips2024/posters/bohnxyipww/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bohnxyipww/</guid><description>This research unveils a fundamental interplay between local differential privacy (LDP) and robustness against data corruption and heavy-tailed rewards in multi-armed bandits, offering a tight characte&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bohnxyipww/cover.png"/></item><item><title>Log-concave Sampling from a Convex Body with a Barrier: a Robust and Unified Dikin Walk</title><link>https://deep-diver.github.io/neurips2024/posters/xkrsb5a79f/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xkrsb5a79f/</guid><description>This paper introduces robust Dikin walks for log-concave sampling, achieving faster mixing times and lower iteration costs than existing methods, particularly for high-dimensional settings.</description></item><item><title>Logical characterizations of recurrent graph neural networks with reals and floats</title><link>https://deep-diver.github.io/neurips2024/posters/atdcnwqg5n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/atdcnwqg5n/</guid><description>Recurrent Graph Neural Networks (GNNs) with real and floating-point numbers are precisely characterized by rule-based and infinitary modal logics, respectively, enabling a deeper understanding of thei&amp;hellip;</description></item><item><title>Lookback Prophet Inequalities</title><link>https://deep-diver.github.io/neurips2024/posters/cg1vwt5xou/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cg1vwt5xou/</guid><description>This paper enhances prophet inequalities by allowing lookback, improving competitive ratios and providing algorithms for diverse observation orders, thereby bridging theory and real-world online selec&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cg1vwt5xou/cover.png"/></item><item><title>Looks Too Good To Be True: An Information-Theoretic Analysis of Hallucinations in Generative Restoration Models</title><link>https://deep-diver.github.io/neurips2024/posters/85tu7k06i3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/85tu7k06i3/</guid><description>Generative image restoration models face a critical trade-off: higher perceptual quality often leads to increased hallucinations (unreliable predictions).</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/85tu7k06i3/cover.png"/></item><item><title>Loss Landscape Characterization of Neural Networks without Over-Parametrization</title><link>https://deep-diver.github.io/neurips2024/posters/h0a3p5wtxu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/h0a3p5wtxu/</guid><description>Deep learning optimization is revolutionized by a new function class, enabling convergence guarantees without over-parameterization and accommodating saddle points.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/h0a3p5wtxu/cover.png"/></item><item><title>Low Degree Hardness for Broadcasting on Trees</title><link>https://deep-diver.github.io/neurips2024/posters/3ioefhez5e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3ioefhez5e/</guid><description>Low-degree polynomials fail to efficiently infer roots in broadcasting tree problems below the Kesten-Stigum bound.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3ioefhez5e/cover.png"/></item><item><title>MAC Advice for facility location mechanism design</title><link>https://deep-diver.github.io/neurips2024/posters/lpbqzszt8y/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lpbqzszt8y/</guid><description>Improved facility location mechanisms are designed using &amp;lsquo;Mostly Approximately Correct&amp;rsquo; predictions, exceeding prior bounds despite large prediction errors.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lpbqzszt8y/cover.png"/></item><item><title>MALT Powers Up Adversarial Attacks</title><link>https://deep-diver.github.io/neurips2024/posters/bcqix5q8qx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bcqix5q8qx/</guid><description>MALT: a novel adversarial attack, is 5x faster than AutoAttack, achieving higher success rates on CIFAR-100 and ImageNet by exploiting mesoscopic almost linearity in neural networks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bcqix5q8qx/cover.png"/></item><item><title>MambaLRP: Explaining Selective State Space Sequence Models</title><link>https://deep-diver.github.io/neurips2024/posters/2n1ysn1edl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2n1ysn1edl/</guid><description>MambaLRP enhances explainability of Mamba sequence models by ensuring faithful relevance propagation, achieving state-of-the-art explanation performance, and uncovering model biases.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2n1ysn1edl/cover.png"/></item><item><title>Marginal Causal Flows for Validation and Inference</title><link>https://deep-diver.github.io/neurips2024/posters/zjremskvyh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zjremskvyh/</guid><description>Frugal Flows: Generate realistic causal benchmarks with exact marginal causal effects, enabling robust causal method validation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zjremskvyh/cover.png"/></item><item><title>Markov Equivalence and Consistency in Differentiable Structure Learning</title><link>https://deep-diver.github.io/neurips2024/posters/tmlgqw7ebc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tmlgqw7ebc/</guid><description>Researchers developed a new, differentiable score function for learning causal relationships from data that reliably recovers the simplest causal model, even with complex data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tmlgqw7ebc/cover.png"/></item><item><title>Marrying Causal Representation Learning with Dynamical Systems for Science</title><link>https://deep-diver.github.io/neurips2024/posters/mwhrxkz4mq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mwhrxkz4mq/</guid><description>This study marries causal representation learning with dynamical systems to enable parameter identification in real-world scientific data, unlocking downstream causal analysis for various applications&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mwhrxkz4mq/cover.png"/></item><item><title>Masked Hard-Attention Transformers Recognize Exactly the Star-Free Languages</title><link>https://deep-diver.github.io/neurips2024/posters/fbmsbdh0yz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fbmsbdh0yz/</guid><description>Masked hard-attention transformers, with strict masking, precisely capture star-free languages, matching the expressive power of linear temporal logic.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fbmsbdh0yz/cover.png"/></item><item><title>Matching the Statistical Query Lower Bound for $k$-Sparse Parity Problems with Sign Stochastic Gradient Descent</title><link>https://deep-diver.github.io/neurips2024/posters/ebssbvwuww/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ebssbvwuww/</guid><description>Sign Stochastic Gradient Descent (SGD) achieves optimal sample complexity for solving k-sparse parity problems, matching Statistical Query lower bounds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ebssbvwuww/cover.png"/></item><item><title>Matrix Denoising with Doubly Heteroscedastic Noise: Fundamental Limits and Optimal Spectral Methods</title><link>https://deep-diver.github.io/neurips2024/posters/ngyt80ipuk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ngyt80ipuk/</guid><description>Optimal matrix denoising with doubly heteroscedastic noise achieved!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ngyt80ipuk/cover.png"/></item><item><title>MatrixNet: Learning over symmetry groups using learned group representations</title><link>https://deep-diver.github.io/neurips2024/posters/b8jwgzraxg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/b8jwgzraxg/</guid><description>MatrixNet learns efficient group representations for improved deep learning on symmetry groups, achieving higher sample efficiency and generalization than existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/b8jwgzraxg/cover.png"/></item><item><title>Maximizing utility in multi-agent environments by anticipating the behavior of other learners</title><link>https://deep-diver.github.io/neurips2024/posters/0uglkys7a2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0uglkys7a2/</guid><description>Optimizing against learning agents: New algorithms and computational limits revealed!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0uglkys7a2/cover.png"/></item><item><title>Mean-Field Analysis for Learning Subspace-Sparse Polynomials with Gaussian Input</title><link>https://deep-diver.github.io/neurips2024/posters/una5hxin6v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/una5hxin6v/</guid><description>Researchers establish basis-free conditions for SGD learnability in two-layer neural networks learning subspace-sparse polynomials with Gaussian input, offering insights into training dynamics.</description></item><item><title>Mean-Field Langevin Dynamics for Signed Measures via a Bilevel Approach</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/oo7hy9kmk6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/oo7hy9kmk6/</guid><description>This paper presents a novel bilevel approach to extend mean-field Langevin dynamics to solve convex optimization problems over signed measures, achieving stronger guarantees and faster convergence rat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/oo7hy9kmk6/cover.png"/></item><item><title>Measuring Goal-Directedness</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/o4codiby7e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/o4codiby7e/</guid><description>New metric, Maximum Entropy Goal-Directedness (MEG), quantifies AI goal-directedness, crucial for assessing AI safety and agency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/o4codiby7e/cover.png"/></item><item><title>Mechanism design augmented with output advice</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ajgks7qozm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ajgks7qozm/</guid><description>Mechanism design enhanced with output advice improves approximation guarantees by using imperfect predictions of the output, not agent types, offering robust, practical solutions.</description></item><item><title>Metric Space Magnitude for Evaluating the Diversity of Latent Representations</title><link>https://deep-diver.github.io/neurips2024/posters/glgzzafssh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/glgzzafssh/</guid><description>Novel metric space magnitude measures rigorously quantify the diversity of latent representations across multiple scales, showing superior performance in detecting mode collapse and characterizing emb&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/glgzzafssh/cover.png"/></item><item><title>Metric Transforms and Low Rank Representations of Kernels for Fast Attention</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/k9pxsryuwg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/k9pxsryuwg/</guid><description>Researchers unveil novel linear-algebraic tools revealing the limits of fast attention, classifying positive definite kernels for Manhattan distance, and fully characterizing metric transforms for Man&amp;hellip;</description></item><item><title>MG-Net: Learn to Customize QAOA with Circuit Depth Awareness</title><link>https://deep-diver.github.io/neurips2024/posters/5hdg5ik18b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/5hdg5ik18b/</guid><description>MG-Net dynamically designs optimal mixer Hamiltonians for QAOA, overcoming the limitation of fixed-depth quantum circuits and significantly improving approximation ratios.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/5hdg5ik18b/cover.png"/></item><item><title>MILP-StuDio: MILP Instance Generation via Block Structure Decomposition</title><link>https://deep-diver.github.io/neurips2024/posters/w433ri0vu4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w433ri0vu4/</guid><description>MILP-StuDio generates high-quality mixed-integer linear programming instances by preserving crucial block structures, significantly improving learning-based solver performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w433ri0vu4/cover.png"/></item><item><title>Mind the Gap: A Causal Perspective on Bias Amplification in Prediction &amp; Decision-Making</title><link>https://deep-diver.github.io/neurips2024/posters/axyl24yhjn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/axyl24yhjn/</guid><description>AI bias amplification in decision-making is uncovered, showing how fair prediction scores can become discriminatory after thresholding, urging stronger regulatory oversight.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/axyl24yhjn/cover.png"/></item><item><title>Mind the Graph When Balancing Data for Fairness or Robustness</title><link>https://deep-diver.github.io/neurips2024/posters/lqr22jm5l3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lqr22jm5l3/</guid><description>Data balancing in machine learning can hurt fairness and robustness; this paper reveals when and why, offering solutions for safer AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lqr22jm5l3/cover.png"/></item><item><title>Minimum Entropy Coupling with Bottleneck</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ylmym7shde/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ylmym7shde/</guid><description>A novel lossy compression framework, Minimum Entropy Coupling with Bottleneck (MEC-B), extends existing methods by integrating a bottleneck for controlled stochasticity, enhancing performance in scen&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ylmym7shde/cover.png"/></item><item><title>Mirror and Preconditioned Gradient Descent in Wasserstein Space</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/n12b6wva55/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/n12b6wva55/</guid><description>This paper presents novel mirror and preconditioned gradient descent algorithms for optimizing functionals over Wasserstein space, offering improved convergence and efficiency for various machine lear&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/n12b6wva55/cover.png"/></item><item><title>Mitigating Spurious Correlations via Disagreement Probability</title><link>https://deep-diver.github.io/neurips2024/posters/obujbhbx8o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/obujbhbx8o/</guid><description>DPR, a novel bias mitigation method, robustly improves model performance by leveraging disagreement probability without needing bias labels, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/obujbhbx8o/cover.png"/></item><item><title>Mixed Dynamics In Linear Networks: Unifying the Lazy and Active Regimes</title><link>https://deep-diver.github.io/neurips2024/posters/9zql27mqwe/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/9zql27mqwe/</guid><description>A new formula unifies lazy and active neural network training regimes, revealing a mixed regime that combines their strengths for faster convergence and low-rank bias.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/9zql27mqwe/cover.png"/></item><item><title>Model Collapse Demystified: The Case of Regression</title><link>https://deep-diver.github.io/neurips2024/posters/biohntrnqk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/biohntrnqk/</guid><description>Training AI models on AI-generated data leads to performance degradation, known as model collapse. This paper offers analytical formulas that precisely quantify this effect in high-dimensional regress&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/biohntrnqk/cover.png"/></item><item><title>Model Reconstruction Using Counterfactual Explanations: A Perspective From Polytope Theory</title><link>https://deep-diver.github.io/neurips2024/posters/9uoldxbylm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/9uoldxbylm/</guid><description>Counterfactual Clamping Attack (CCA) improves model reconstruction using counterfactual explanations by leveraging decision boundary proximity, offering theoretical guarantees and enhanced fidelity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/9uoldxbylm/cover.png"/></item><item><title>Monoculture in Matching Markets</title><link>https://deep-diver.github.io/neurips2024/posters/p5yezhumss/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/p5yezhumss/</guid><description>Algorithmic monoculture harms applicant selection and market efficiency; this paper introduces a model to analyze its effects in two-sided matching markets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/p5yezhumss/cover.png"/></item><item><title>Most Influential Subset Selection: Challenges, Promises, and Beyond</title><link>https://deep-diver.github.io/neurips2024/posters/qwi33ppecc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qwi33ppecc/</guid><description>Adaptive greedy algorithms significantly improve the accuracy of identifying the most influential subset of training data, overcoming limitations of existing methods that fail to capture complex inter&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qwi33ppecc/cover.png"/></item><item><title>Motif-oriented influence maximization for viral marketing in large-scale social networks</title><link>https://deep-diver.github.io/neurips2024/posters/uyztzchaqb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uyztzchaqb/</guid><description>Motif-oriented influence maximization tackles viral marketing&amp;rsquo;s challenge of reaching groups by proving a greedy algorithm with guaranteed approximation ratio and near-linear time complexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uyztzchaqb/cover.png"/></item><item><title>Multi-Agent Imitation Learning: Value is Easy, Regret is Hard</title><link>https://deep-diver.github.io/neurips2024/posters/qk3ibhyv6z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qk3ibhyv6z/</guid><description>In multi-agent imitation learning, achieving regret equivalence is harder than value equivalence; this paper introduces novel algorithms that efficiently minimize the regret gap under various assumpti&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qk3ibhyv6z/cover.png"/></item><item><title>Multi-Group Proportional Representation in Retrieval</title><link>https://deep-diver.github.io/neurips2024/posters/brzyhvhvsg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/brzyhvhvsg/</guid><description>Multi-group Proportional Representation (MPR) tackles skewed search results by measuring representation across intersectional groups, improving fairness in image retrieval.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/brzyhvhvsg/cover.png"/></item><item><title>Multi-Winner Reconfiguration</title><link>https://deep-diver.github.io/neurips2024/posters/kzfxicbxd1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kzfxicbxd1/</guid><description>This paper introduces a novel model for multi-winner reconfiguration, analyzing the computational complexity of transitioning between committees using four approval-based voting rules, demonstrating b&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kzfxicbxd1/cover.png"/></item><item><title>Multiclass Transductive Online Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3erevfwalz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3erevfwalz/</guid><description>Unbounded label spaces conquered! New algorithm achieves optimal mistake bounds in multiclass transductive online learning.</description></item><item><title>Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization</title><link>https://deep-diver.github.io/neurips2024/posters/bia03matxq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bia03matxq/</guid><description>Multilinear Mixture of Experts (μMoE) achieves scalable expert specialization in deep neural networks through tensor factorization, enabling efficient fine-tuning and interpretable model editing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bia03matxq/cover.png"/></item><item><title>Mutli-Armed Bandits with Network Interference</title><link>https://deep-diver.github.io/neurips2024/posters/zxzovvoiil/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zxzovvoiil/</guid><description>New algorithms conquer regret in multi-armed bandits challenged by network interference, achieving provably low regret with both known and unknown network structures.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zxzovvoiil/cover.png"/></item><item><title>Mutual Information Estimation via Normalizing Flows</title><link>https://deep-diver.github.io/neurips2024/posters/jiqxslvdls/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jiqxslvdls/</guid><description>Researchers introduce a novel approach to mutual information (MI) estimation using normalizing flows, providing accurate estimates even in high dimensions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jiqxslvdls/cover.png"/></item><item><title>Natural Counterfactuals With Necessary Backtracking</title><link>https://deep-diver.github.io/neurips2024/posters/n6zj8dclc2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/n6zj8dclc2/</guid><description>This paper proposes &amp;rsquo;natural counterfactuals&amp;rsquo; for more realistic counterfactual reasoning in AI, using backtracking to minimize deviations from observed data while ensuring feasibility.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/n6zj8dclc2/cover.png"/></item><item><title>Nature-Inspired Local Propagation</title><link>https://deep-diver.github.io/neurips2024/posters/ds6xmv3yvv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ds6xmv3yvv/</guid><description>Inspired by nature, researchers introduce a novel spatiotemporal local algorithm for machine learning that outperforms backpropagation in online learning scenarios with limited data or long video stre&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ds6xmv3yvv/cover.png"/></item><item><title>Navigable Graphs for High-Dimensional Nearest Neighbor Search: Constructions and Limits</title><link>https://deep-diver.github.io/neurips2024/posters/7flsqgz4rt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7flsqgz4rt/</guid><description>Sparse navigable graphs enable efficient nearest neighbor search, but their construction and limits in high dimensions remain unclear. This paper presents an efficient method to construct navigable gr&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7flsqgz4rt/cover.png"/></item><item><title>Near-Optimal Streaming Heavy-Tailed Statistical Estimation with Clipped SGD</title><link>https://deep-diver.github.io/neurips2024/posters/8jauriwdeh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8jauriwdeh/</guid><description>Clipped SGD achieves near-optimal sub-Gaussian rates for high-dimensional heavy-tailed statistical estimation in streaming settings, improving upon existing state-of-the-art results.</description></item><item><title>Nearly Minimax Optimal Regret for Multinomial Logistic Bandit</title><link>https://deep-diver.github.io/neurips2024/posters/q4nwfstqvf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/q4nwfstqvf/</guid><description>This paper presents OFU-MNL+, a constant-time algorithm achieving nearly minimax optimal regret for contextual multinomial logistic bandits, closing the gap between existing upper and lower bounds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/q4nwfstqvf/cover.png"/></item><item><title>Nearly Minimax Optimal Submodular Maximization with Bandit Feedback</title><link>https://deep-diver.github.io/neurips2024/posters/vn0fwrimra/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vn0fwrimra/</guid><description>This research establishes the first minimax optimal algorithm for submodular maximization with bandit feedback, achieving a regret bound matching the lower bound.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vn0fwrimra/cover.png"/></item><item><title>Nearly Optimal Approximation of Matrix Functions by the Lanczos Method</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3s8v8qp9xv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3s8v8qp9xv/</guid><description>Lanczos-FA, a simple algorithm for approximating matrix functions, surprisingly outperforms newer methods; this paper proves its near-optimality for rational functions, explaining its practical succes&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3s8v8qp9xv/cover.png"/></item><item><title>Nearly Tight Black-Box Auditing of Differentially Private Machine Learning</title><link>https://deep-diver.github.io/neurips2024/posters/ccdmxxiamp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ccdmxxiamp/</guid><description>This paper presents a new auditing method for DP-SGD that provides substantially tighter black-box privacy analyses than previous methods, yielding significantly closer empirical estimates to theoreti&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ccdmxxiamp/cover.png"/></item><item><title>Nesterov acceleration despite very noisy gradients</title><link>https://deep-diver.github.io/neurips2024/posters/khxub494sy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/khxub494sy/</guid><description>AGNES, a novel accelerated gradient descent algorithm, achieves accelerated convergence even with very noisy gradients, significantly improving training efficiency for machine learning models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/khxub494sy/cover.png"/></item><item><title>Neur2BiLO: Neural Bilevel Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/esvleaqkrc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/esvleaqkrc/</guid><description>NEUR2BILO: a neural network-based heuristic solves mixed-integer bilevel optimization problems extremely fast, achieving high-quality solutions for diverse applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/esvleaqkrc/cover.png"/></item><item><title>Neural collapse vs. low-rank bias: Is deep neural collapse really optimal?</title><link>https://deep-diver.github.io/neurips2024/posters/0jld45xggj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0jld45xggj/</guid><description>Deep neural collapse, previously believed optimal, is shown suboptimal in multi-class, multi-layer networks due to a low-rank bias, yielding even lower-rank solutions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0jld45xggj/cover.png"/></item><item><title>Neural Combinatorial Optimization for Robust Routing Problem with Uncertain Travel Times</title><link>https://deep-diver.github.io/neurips2024/posters/doewnm2ut3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/doewnm2ut3/</guid><description>Neural networks efficiently solve robust routing problems with uncertain travel times, minimizing worst-case deviations from optimal routes under the min-max regret criterion.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/doewnm2ut3/cover.png"/></item><item><title>Neural Model Checking</title><link>https://deep-diver.github.io/neurips2024/posters/dj9kzkq0oh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dj9kzkq0oh/</guid><description>Neural networks revolutionize hardware model checking by generating formal proof certificates, outperforming state-of-the-art techniques in speed and scalability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dj9kzkq0oh/cover.png"/></item><item><title>Neural network learns low-dimensional polynomials with SGD near the information-theoretic limit</title><link>https://deep-diver.github.io/neurips2024/posters/qk4is49kdm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qk4is49kdm/</guid><description>SGD can train neural networks to learn low-dimensional polynomials near the information-theoretic limit, surpassing previous correlational statistical query lower bounds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qk4is49kdm/cover.png"/></item><item><title>Neural Network Reparametrization for Accelerated Optimization in Molecular Simulations</title><link>https://deep-diver.github.io/neurips2024/posters/fwxohl0bel/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fwxohl0bel/</guid><description>Accelerate molecular simulations using neural network reparametrization! This flexible method adjusts system complexity, enhances optimization, and maintains continuous access to fine-grained modes, o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fwxohl0bel/cover.png"/></item><item><title>Neural Persistence Dynamics</title><link>https://deep-diver.github.io/neurips2024/posters/rcnzrfikx6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rcnzrfikx6/</guid><description>Neural Persistence Dynamics learns collective behavior from topological features, accurately predicting parameters of governing equations without tracking individual entities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rcnzrfikx6/cover.png"/></item><item><title>Neural Pfaffians: Solving Many Many-Electron Schrödinger Equations</title><link>https://deep-diver.github.io/neurips2024/oral-ai-theory/hrknicwm3e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-ai-theory/hrknicwm3e/</guid><description>Neural Pfaffians revolutionize many-electron Schrödinger equation solutions by using fully learnable neural wave functions based on Pfaffians, achieving unprecedented accuracy and generalizability acr&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-ai-theory/hrknicwm3e/cover.png"/></item><item><title>Nimbus: Secure and Efficient Two-Party Inference for Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/g7qs68icpj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/g7qs68icpj/</guid><description>Nimbus achieves 2.7-4.7x speedup in BERT base inference using novel two-party computation techniques for efficient matrix multiplication and non-linear layer approximation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/g7qs68icpj/cover.png"/></item><item><title>No Free Delivery Service: Epistemic limits of passive data collection in complex social systems</title><link>https://deep-diver.github.io/neurips2024/posters/xz0fpoakeb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xz0fpoakeb/</guid><description>Passive data collection in complex social systems invalidates standard AI model validation; new methods are needed.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xz0fpoakeb/cover.png"/></item><item><title>No Free Lunch Theorem and Black-Box Complexity Analysis for Adversarial Optimisation</title><link>https://deep-diver.github.io/neurips2024/posters/nkuysm8qvs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nkuysm8qvs/</guid><description>No free lunch for adversarial optimization: This paper proves that no single algorithm universally outperforms others when finding Nash Equilibrium, introducing black-box complexity analysis to estab&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nkuysm8qvs/cover.png"/></item><item><title>No-Regret Learning for Fair Multi-Agent Social Welfare Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/15jm9v7wco/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/15jm9v7wco/</guid><description>This paper solves the open problem of achieving no-regret learning in online multi-agent Nash social welfare maximization.</description></item><item><title>No-regret Learning in Harmonic Games: Extrapolation in the Face of Conflicting Interests</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/hw9s9vy5gz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/hw9s9vy5gz/</guid><description>Extrapolated FTRL ensures Nash equilibrium convergence in harmonic games, defying standard no-regret learning limitations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/hw9s9vy5gz/cover.png"/></item><item><title>No-Regret M${}^{
atural}$-Concave Function Maximization: Stochastic Bandit Algorithms and NP-Hardness of Adversarial Full-Information Setting</title><link>https://deep-diver.github.io/neurips2024/posters/nnoaj91hzx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nnoaj91hzx/</guid><description>This paper reveals efficient stochastic bandit algorithms for maximizing M-concave functions and proves NP-hardness for adversarial full-information settings.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nnoaj91hzx/cover.png"/></item><item><title>Noise-Aware Differentially Private Regression via Meta-Learning</title><link>https://deep-diver.github.io/neurips2024/posters/99roam7jfm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/99roam7jfm/</guid><description>Meta-learning and differential privacy combine to enable accurate, well-calibrated private regression, even with limited data, via the novel DPConvCNP model.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/99roam7jfm/cover.png"/></item><item><title>Noisy Dual Mirror Descent: A Near Optimal Algorithm for Jointly-DP Convex Resource Allocation</title><link>https://deep-diver.github.io/neurips2024/posters/6arnmbmpkf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/6arnmbmpkf/</guid><description>Near-optimal algorithm for private resource allocation is introduced, achieving improved accuracy and privacy guarantees.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/6arnmbmpkf/cover.png"/></item><item><title>Non-asymptotic Convergence of Training Transformers for Next-token Prediction</title><link>https://deep-diver.github.io/neurips2024/posters/nfofbppyii/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nfofbppyii/</guid><description>This paper reveals how a one-layer transformer&amp;rsquo;s training converges for next-token prediction, showing sub-linear convergence for both layers and shedding light on its surprising generalization abilit&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nfofbppyii/cover.png"/></item><item><title>Non-geodesically-convex optimization in the Wasserstein space</title><link>https://deep-diver.github.io/neurips2024/posters/lgg1iqhbor/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lgg1iqhbor/</guid><description>A novel semi Forward-Backward Euler scheme provides convergence guarantees for non-geodesically-convex optimization in Wasserstein space, advancing both sampling and optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lgg1iqhbor/cover.png"/></item><item><title>Nonparametric Instrumental Variable Regression through Stochastic Approximate Gradients</title><link>https://deep-diver.github.io/neurips2024/posters/vqxodxhu4k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vqxodxhu4k/</guid><description>SAGD-IV: a novel functional stochastic gradient descent algorithm for stable nonparametric instrumental variable regression, excelling in handling binary outcomes and various loss functions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vqxodxhu4k/cover.png"/></item><item><title>Not so griddy: Internal representations of RNNs path integrating more than one agent</title><link>https://deep-diver.github.io/neurips2024/posters/dsmswubn8f/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dsmswubn8f/</guid><description>RNNs trained on dual-agent path integration develop distinct internal representations compared to single-agent models, exhibiting weaker grid cell responses and enhanced border/band cell activity, wit&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dsmswubn8f/cover.png"/></item><item><title>Off-policy estimation with adaptively collected data: the power of online learning</title><link>https://deep-diver.github.io/neurips2024/posters/8qekjsedls/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8qekjsedls/</guid><description>This paper develops novel finite-sample bounds for off-policy linear treatment effect estimation with adaptively collected data, proposing online learning algorithms to improve estimation accuracy and&amp;hellip;</description></item><item><title>On Causal Discovery in the Presence of Deterministic Relations</title><link>https://deep-diver.github.io/neurips2024/posters/pfvcsgfrj6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pfvcsgfrj6/</guid><description>DGES, a novel framework, efficiently detects &amp;amp; handles deterministic relations in causal discovery, enhancing accuracy and scalability for real-world applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pfvcsgfrj6/cover.png"/></item><item><title>On Convergence of Adam for Stochastic Optimization under Relaxed Assumptions</title><link>https://deep-diver.github.io/neurips2024/posters/x7usmidzxj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x7usmidzxj/</guid><description>Adam optimizer achieves near-optimal convergence in non-convex scenarios with unbounded gradients and relaxed noise assumptions, improving its theoretical understanding and practical application.</description></item><item><title>On Differentially Private Subspace Estimation in a Distribution-Free Setting</title><link>https://deep-diver.github.io/neurips2024/posters/acchvnwnlf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/acchvnwnlf/</guid><description>This paper presents novel measures quantifying data easiness for DP subspace estimation, supporting them with improved upper and lower bounds and a practical algorithm.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/acchvnwnlf/cover.png"/></item><item><title>On Differentially Private U Statistics</title><link>https://deep-diver.github.io/neurips2024/posters/zapfyclg6k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zapfyclg6k/</guid><description>New algorithms achieve near-optimal differentially private U-statistic estimation, significantly improving accuracy over existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zapfyclg6k/cover.png"/></item><item><title>On Feature Learning in Structured State Space Models</title><link>https://deep-diver.github.io/neurips2024/posters/aqv5abn1wf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/aqv5abn1wf/</guid><description>Unlocking the scaling secrets of structured state-space models, this research identifies novel scaling rules for improved stability, generalization, and hyperparameter transferability, revolutionizing&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/aqv5abn1wf/cover.png"/></item><item><title>On Mesa-Optimization in Autoregressively Trained Transformers: Emergence and Capability</title><link>https://deep-diver.github.io/neurips2024/posters/e2bypreuu8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/e2bypreuu8/</guid><description>Autoregressively trained transformers surprisingly learn algorithms during pretraining, enabling in-context learning; this paper reveals when and why this &amp;lsquo;mesa-optimization&amp;rsquo; happens.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/e2bypreuu8/cover.png"/></item><item><title>On Neural Networks as Infinite Tree-Structured Probabilistic Graphical Models</title><link>https://deep-diver.github.io/neurips2024/posters/kcmhsrhzjb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kcmhsrhzjb/</guid><description>DNNs are powerful but lack the clear semantics of PGMs. This paper innovatively constructs infinite tree-structured PGMs that exactly correspond to DNNs, revealing that DNN forward propagation approxi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kcmhsrhzjb/cover.png"/></item><item><title>On provable privacy vulnerabilities of graph representations</title><link>https://deep-diver.github.io/neurips2024/posters/lsqdcfx3xu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lsqdcfx3xu/</guid><description>Graph representation learning&amp;rsquo;s structural vulnerabilities are proven and mitigated via noisy aggregation, revealing crucial privacy-utility trade-offs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lsqdcfx3xu/cover.png"/></item><item><title>On Socially Fair Low-Rank Approximation and Column Subset Selection</title><link>https://deep-diver.github.io/neurips2024/posters/eo1qev952p/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/eo1qev952p/</guid><description>This paper reveals the surprising computational hardness of achieving fairness in low-rank approximation while offering efficient approximation algorithms.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/eo1qev952p/cover.png"/></item><item><title>On Sparse Canonical Correlation Analysis</title><link>https://deep-diver.github.io/neurips2024/posters/mz47wpr6c3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mz47wpr6c3/</guid><description>This paper presents novel, efficient algorithms and formulations for Sparse Canonical Correlation Analysis (SCCA), a method that improves the interpretability of traditional CCA. SCCA is especially us&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mz47wpr6c3/cover.png"/></item><item><title>On Statistical Rates and Provably Efficient Criteria of Latent Diffusion Transformers (DiTs)</title><link>https://deep-diver.github.io/neurips2024/posters/cv2lkbdlz4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cv2lkbdlz4/</guid><description>Latent Diffusion Transformers (DiTs) achieve almost-linear time training and inference through low-rank gradient approximations and efficient criteria, overcoming high dimensionality challenges.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cv2lkbdlz4/cover.png"/></item><item><title>On the Ability of Developers' Training Data Preservation of Learnware</title><link>https://deep-diver.github.io/neurips2024/posters/wsqdjhpuhn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wsqdjhpuhn/</guid><description>Learnware systems enable model reuse; this paper proves RKME specifications protect developers&amp;rsquo; training data while enabling effective model identification.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wsqdjhpuhn/cover.png"/></item><item><title>On the Adversarial Robustness of Benjamini Hochberg</title><link>https://deep-diver.github.io/neurips2024/posters/5jyfoldunm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/5jyfoldunm/</guid><description>Even a few data changes can break the Benjamini-Hochberg (BH) procedure, a widely used multiple testing method, highlighting a critical vulnerability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/5jyfoldunm/cover.png"/></item><item><title>On the Benefits of Public Representations for Private Transfer Learning under Distribution Shift</title><link>https://deep-diver.github.io/neurips2024/posters/e1nblreajo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/e1nblreajo/</guid><description>Public data boosts private AI accuracy even with extreme distribution shifts, improving private model training by up to 67% in three tasks. This is due to shared low-dimensional representations betwe&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/e1nblreajo/cover.png"/></item><item><title>On the cohesion and separability of average-link for hierarchical agglomerative clustering</title><link>https://deep-diver.github.io/neurips2024/posters/2lushtfwzk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2lushtfwzk/</guid><description>Average-link hierarchical clustering gets a comprehensive evaluation using new criteria, showing it outperforms other methods when both cohesion and separability matter.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2lushtfwzk/cover.png"/></item><item><title>On the Complexity of Identification in Linear Structural Causal Models</title><link>https://deep-diver.github.io/neurips2024/posters/bndwooxj6w/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bndwooxj6w/</guid><description>New polynomial-space algorithm for causal parameter identification in linear models vastly improves upon existing methods, showing that this crucial task is computationally hard.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bndwooxj6w/cover.png"/></item><item><title>On the Computational Complexity of Private High-dimensional Model Selection</title><link>https://deep-diver.github.io/neurips2024/posters/pzg7xvlyqm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pzg7xvlyqm/</guid><description>This paper proposes a computationally efficient, differentially private best subset selection method for high-dimensional sparse linear regression, achieving both strong statistical utility and provab&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pzg7xvlyqm/cover.png"/></item><item><title>On the Computational Landscape of Replicable Learning</title><link>https://deep-diver.github.io/neurips2024/posters/1pcsdng6jg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1pcsdng6jg/</guid><description>This paper reveals surprising computational connections between algorithmic replicability and other learning paradigms, offering novel algorithms and demonstrating separations between replicability an&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1pcsdng6jg/cover.png"/></item><item><title>On the Expressive Power of Tree-Structured Probabilistic Circuits</title><link>https://deep-diver.github.io/neurips2024/posters/suyaaoi5bd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/suyaaoi5bd/</guid><description>Tree-structured probabilistic circuits are surprisingly efficient: this paper proves a quasi-polynomial upper bound on their size, showing they&amp;rsquo;re almost as expressive as more complex DAG structures.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/suyaaoi5bd/cover.png"/></item><item><title>On the Expressivity and Sample Complexity of Node-Individualized Graph Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/8appyps0yn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8appyps0yn/</guid><description>Boosting GNN expressivity and generalization: Novel node individualization schemes lower sample complexity, improving substructure identification.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8appyps0yn/cover.png"/></item><item><title>On the Identifiability of Poisson Branching Structural Causal Model Using Probability Generating Function</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/tuwwbljfk9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/tuwwbljfk9/</guid><description>Researchers developed a novel, efficient causal discovery method using Probability Generating Functions to identify causal structures within Poisson Branching Structural Causal Models, overcoming limi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/tuwwbljfk9/cover.png"/></item><item><title>On the Impact of Feature Heterophily on Link Prediction with Graph Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/3lzhatxua9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3lzhatxua9/</guid><description>Graph Neural Networks (GNNs) struggle with heterophilic link prediction; this paper introduces formal definitions, theoretical analysis, improved designs, and real-world benchmarks to address this cha&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3lzhatxua9/cover.png"/></item><item><title>On the Impacts of the Random Initialization in the Neural Tangent Kernel Theory</title><link>https://deep-diver.github.io/neurips2024/posters/ni3ud2bv3g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ni3ud2bv3g/</guid><description>Standard initialization in neural networks negatively impacts generalization ability under Neural Tangent Kernel theory, contradicting real-world performance, urging the development of improved theore&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ni3ud2bv3g/cover.png"/></item><item><title>On the Optimality of Dilated Entropy and Lower Bounds for Online Learning in Extensive-Form Games</title><link>https://deep-diver.github.io/neurips2024/posters/6pmfjt2o7g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/6pmfjt2o7g/</guid><description>Researchers discover Dilated Entropy is the optimal distance-generating function for solving extensive-form games using first-order methods, achieving near-optimal regret bounds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/6pmfjt2o7g/cover.png"/></item><item><title>On the Parameter Identifiability of Partially Observed Linear Causal Models</title><link>https://deep-diver.github.io/neurips2024/posters/eqzlefjrkv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/eqzlefjrkv/</guid><description>Researchers achieve full parameter identifiability in partially observed linear causal models using novel graphical conditions and a likelihood-based estimation method, addressing previous limitations&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/eqzlefjrkv/cover.png"/></item><item><title>On the Power of Small-size Graph Neural Networks for Linear Programming</title><link>https://deep-diver.github.io/neurips2024/posters/orqiboarqy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/orqiboarqy/</guid><description>Small-size Graph Neural Networks effectively solve Linear Programs!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/orqiboarqy/cover.png"/></item><item><title>On the Robustness of Spectral Algorithms for Semirandom Stochastic Block Models</title><link>https://deep-diver.github.io/neurips2024/posters/klen1xyw6p/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/klen1xyw6p/</guid><description>Spectral algorithms for graph bisection show surprising robustness to helpful adversaries in semirandom models, with unnormalized Laplacian consistently outperforming the normalized one.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/klen1xyw6p/cover.png"/></item><item><title>On the Role of Attention Masks and LayerNorm in Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/lih6ocdppg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lih6ocdppg/</guid><description>Transformers&amp;rsquo; self-attention mechanism, while powerful, suffers from rank collapse with increasing depth. This paper reveals that while masked attention still leads to exponential collapse, sparse att&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lih6ocdppg/cover.png"/></item><item><title>On the Saturation Effects of Spectral Algorithms in Large Dimensions</title><link>https://deep-diver.github.io/neurips2024/posters/kjzeclysri/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kjzeclysri/</guid><description>High-dimensional spectral algorithms show saturation effects: Kernel Ridge Regression underperforms optimal algorithms like gradient flow when regression functions are very smooth.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kjzeclysri/cover.png"/></item><item><title>On the Sparsity of the Strong Lottery Ticket Hypothesis</title><link>https://deep-diver.github.io/neurips2024/posters/abmesb1ajx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/abmesb1ajx/</guid><description>Researchers rigorously prove the Strong Lottery Ticket Hypothesis, offering the first theoretical guarantees on the sparsity of winning neural network subnetworks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/abmesb1ajx/cover.png"/></item><item><title>On Tractable $ hi$-Equilibria in Non-Concave Games</title><link>https://deep-diver.github.io/neurips2024/posters/3cttmf5zzm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3cttmf5zzm/</guid><description>This paper presents efficient algorithms for approximating equilibria in non-concave games, focusing on tractable ɸ-equilibria and addressing computational challenges posed by infinite strategy sets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3cttmf5zzm/cover.png"/></item><item><title>On Weak Regret Analysis for Dueling Bandits</title><link>https://deep-diver.github.io/neurips2024/posters/dy4ygqvfgw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dy4ygqvfgw/</guid><description>New algorithms achieve optimal weak regret in K-armed dueling bandits by leveraging the full problem structure, improving upon state-of-the-art methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dy4ygqvfgw/cover.png"/></item><item><title>One Sample Fits All: Approximating All Probabilistic Values Simultaneously and Efficiently</title><link>https://deep-diver.github.io/neurips2024/posters/aug9d2vjcf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/aug9d2vjcf/</guid><description>One-Sample-Fits-All (OFA) framework efficiently approximates all probabilistic values simultaneously, achieving faster convergence rates than existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/aug9d2vjcf/cover.png"/></item><item><title>One-Layer Transformer Provably Learns One-Nearest Neighbor In Context</title><link>https://deep-diver.github.io/neurips2024/posters/wdx45lnzxe/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wdx45lnzxe/</guid><description>One-layer transformers provably learn the one-nearest neighbor prediction rule, offering theoretical insights into their in-context learning capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wdx45lnzxe/cover.png"/></item><item><title>Online Bayesian Persuasion Without a Clue</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/xnpvz8e1ty/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/xnpvz8e1ty/</guid><description>Researchers developed a novel online Bayesian persuasion algorithm that achieves sublinear regret without prior knowledge of the receiver or the state distribution, providing tight theoretical guarant&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/xnpvz8e1ty/cover.png"/></item><item><title>Online Budgeted Matching with General Bids</title><link>https://deep-diver.github.io/neurips2024/posters/vtxy8wfptj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vtxy8wfptj/</guid><description>MetaAd, a novel meta-algorithm, achieves provable competitive ratios for online budgeted matching with general bids, removing prior restrictive assumptions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vtxy8wfptj/cover.png"/></item><item><title>Online Composite Optimization Between Stochastic and Adversarial Environments</title><link>https://deep-diver.github.io/neurips2024/posters/mbeb5akmmk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mbeb5akmmk/</guid><description>Researchers achieve optimal regret bounds in online composite optimization under stochastic and adversarial settings using a novel optimistic composite mirror descent algorithm and a universal strateg&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mbeb5akmmk/cover.png"/></item><item><title>Online Consistency of the Nearest Neighbor Rule</title><link>https://deep-diver.github.io/neurips2024/posters/eox0smruv7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/eox0smruv7/</guid><description>The 1-nearest neighbor rule achieves online consistency under surprisingly broad conditions: measurable label functions and mild assumptions on instance generation in doubling metric spaces.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/eox0smruv7/cover.png"/></item><item><title>Online Convex Optimisation: The Optimal Switching Regret for all Segmentations Simultaneously</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/u6xxyud3ro/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/u6xxyud3ro/</guid><description>Algorithm RESET achieves optimal switching regret simultaneously across all segmentations, offering efficiency and parameter-free operation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/u6xxyud3ro/cover.png"/></item><item><title>Online Estimation via Offline Estimation: An Information-Theoretic Framework</title><link>https://deep-diver.github.io/neurips2024/posters/sks7x4i8bh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sks7x4i8bh/</guid><description>This paper introduces a novel information-theoretic framework, showing how to convert offline into online estimation algorithms efficiently, impacting interactive decision-making.</description></item><item><title>Online Learning of Delayed Choices</title><link>https://deep-diver.github.io/neurips2024/posters/gc3bznwqqp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gc3bznwqqp/</guid><description>New algorithms conquer delayed feedback in online choice modeling, achieving optimal decision-making even with unknown customer preferences and delayed responses.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gc3bznwqqp/cover.png"/></item><item><title>Online Weighted Paging with Unknown Weights</title><link>https://deep-diver.github.io/neurips2024/posters/ctxty3vggq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ctxty3vggq/</guid><description>First algorithm for online weighted paging that learns page weights from samples, achieving optimal O(log k) competitiveness and sublinear regret.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ctxty3vggq/cover.png"/></item><item><title>Only Strict Saddles in the Energy Landscape of Predictive Coding Networks?</title><link>https://deep-diver.github.io/neurips2024/posters/etu6kvrksq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/etu6kvrksq/</guid><description>Predictive coding networks learn faster than backpropagation by changing the loss landscape&amp;rsquo;s geometry, making saddles easier to escape and improving robustness to vanishing gradients.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/etu6kvrksq/cover.png"/></item><item><title>Optimal ablation for interpretability</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/opt72tyzwz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/opt72tyzwz/</guid><description>Optimal ablation (OA) improves model interpretability by precisely measuring component importance, outperforming existing methods. OA-based importance shines in circuit discovery, factual recall, and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/opt72tyzwz/cover.png"/></item><item><title>Optimal Algorithms for Augmented Testing of Discrete Distributions</title><link>https://deep-diver.github.io/neurips2024/posters/talmacqk9s/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/talmacqk9s/</guid><description>Leveraging predictions, this research presents novel algorithms for uniformity, identity, and closeness testing of discrete distributions, achieving information-theoretically optimal sample complexity&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/talmacqk9s/cover.png"/></item><item><title>Optimal Algorithms for Learning Partitions with Faulty Oracles</title><link>https://deep-diver.github.io/neurips2024/posters/ygdl8q02ga/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ygdl8q02ga/</guid><description>Optimal algorithms for learning partitions are designed, achieving minimum query complexity even with up to l faulty oracle responses.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ygdl8q02ga/cover.png"/></item><item><title>Optimal Algorithms for Online Convex Optimization with Adversarial Constraints</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/txffvjmnby/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/txffvjmnby/</guid><description>Optimal algorithms for online convex optimization with adversarial constraints are developed, achieving O(√T) regret and Õ(√T) constraint violation—a breakthrough in the field.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/txffvjmnby/cover.png"/></item><item><title>Optimal Classification under Performative Distribution Shift</title><link>https://deep-diver.github.io/neurips2024/posters/3j5hvo5uaw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3j5hvo5uaw/</guid><description>This paper introduces a novel push-forward model for performative learning, proving the convexity of performative risk under new assumptions and linking performative learning to adversarial robustness&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3j5hvo5uaw/cover.png"/></item><item><title>Optimal Hypothesis Selection in (Almost) Linear Time</title><link>https://deep-diver.github.io/neurips2024/posters/skv26jtefz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/skv26jtefz/</guid><description>This paper presents the first almost linear-time algorithm achieving the optimal accuracy parameter for hypothesis selection, solving a decades-long open problem.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/skv26jtefz/cover.png"/></item><item><title>Optimal Multiclass U-Calibration Error and Beyond</title><link>https://deep-diver.github.io/neurips2024/posters/7afrgcc8q7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7afrgcc8q7/</guid><description>This paper proves the minimax optimal U-calibration error is Θ(√KT) for online multiclass prediction, resolving an open problem and showing logarithmic error is achievable for specific loss functions.</description></item><item><title>Optimal Parallelization of Boosting</title><link>https://deep-diver.github.io/neurips2024/oral-ai-theory/rtz4df9if1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-ai-theory/rtz4df9if1/</guid><description>This paper closes the performance gap in parallel boosting algorithms by presenting improved lower bounds and a novel algorithm matching these bounds, settling the parallel complexity of sample-optima&amp;hellip;</description></item><item><title>Optimal Scalarizations for Sublinear Hypervolume Regret</title><link>https://deep-diver.github.io/neurips2024/posters/30ns22tgcw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/30ns22tgcw/</guid><description>Optimal multi-objective optimization achieved via hypervolume scalarization, offering sublinear regret bounds and outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/30ns22tgcw/cover.png"/></item><item><title>Optimization Algorithm Design via Electric Circuits</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/9jmt1eer9p/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/9jmt1eer9p/</guid><description>Design provably convergent optimization algorithms swiftly using electric circuit analogies; a novel methodology automating discretization for diverse algorithms.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/9jmt1eer9p/cover.png"/></item><item><title>Optimization Can Learn Johnson Lindenstrauss Embeddings</title><link>https://deep-diver.github.io/neurips2024/posters/w3jctbrduf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w3jctbrduf/</guid><description>Optimization can learn optimal Johnson-Lindenstrauss embeddings, avoiding the limitations of randomized methods and achieving comparable theoretical guarantees.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w3jctbrduf/cover.png"/></item><item><title>Optimizing the coalition gain in Online Auctions with Greedy Structured Bandits</title><link>https://deep-diver.github.io/neurips2024/posters/b74mb0tey6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/b74mb0tey6/</guid><description>Two novel algorithms, Local-Greedy and Greedy-Grid, optimize coalition gain in online auctions with limited observations, achieving constant regret and problem-independent guarantees while respecting &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/b74mb0tey6/cover.png"/></item><item><title>Oracle-Efficient Differentially Private Learning with Public Data</title><link>https://deep-diver.github.io/neurips2024/posters/bajjinf0oh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bajjinf0oh/</guid><description>This paper introduces computationally efficient algorithms for differentially private learning by leveraging public data, overcoming previous computational limitations and enabling broader practical a&amp;hellip;</description></item><item><title>Ordering-Based Causal Discovery for Linear and Nonlinear Relations</title><link>https://deep-diver.github.io/neurips2024/posters/oqug2t4qjb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oqug2t4qjb/</guid><description>Causal discovery algorithm CaPS efficiently handles mixed linear and nonlinear relationships in observational data, outperforming existing methods on synthetic and real-world datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oqug2t4qjb/cover.png"/></item><item><title>OSLO: One-Shot Label-Only Membership Inference Attacks</title><link>https://deep-diver.github.io/neurips2024/posters/zjbbeyeayx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zjbbeyeayx/</guid><description>One-shot label-only attack (OSLO) achieves high membership inference accuracy with only one query, surpassing existing methods by a large margin.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zjbbeyeayx/cover.png"/></item><item><title>OT4P: Unlocking Effective Orthogonal Group Path for Permutation Relaxation</title><link>https://deep-diver.github.io/neurips2024/posters/pmjfabzog3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pmjfabzog3/</guid><description>OT4P: a novel temperature-controlled differentiable transformation efficiently relaxes permutation matrices onto the orthogonal group for gradient-based optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pmjfabzog3/cover.png"/></item><item><title>Outlier-Robust Distributionally Robust Optimization via Unbalanced Optimal Transport</title><link>https://deep-diver.github.io/neurips2024/posters/v8hvsytsu6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v8hvsytsu6/</guid><description>Outlier-robust distributionally robust optimization achieved via a novel Unbalanced Optimal Transport (UOT) distance, improving efficiency and accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v8hvsytsu6/cover.png"/></item><item><title>Overcoming Brittleness in Pareto-Optimal Learning Augmented Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/estpcujzhe/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/estpcujzhe/</guid><description>This research introduces a novel framework that overcomes the brittleness of Pareto-optimal learning-augmented algorithms by enforcing smoothness in performance using user-specified profiles and devel&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/estpcujzhe/cover.png"/></item><item><title>Overfitting Behaviour of Gaussian Kernel Ridgeless Regression: Varying Bandwidth or Dimensionality</title><link>https://deep-diver.github.io/neurips2024/posters/7sh0xkn1ks/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7sh0xkn1ks/</guid><description>Ridgeless regression, surprisingly, generalizes well even with noisy data if dimension scales sub-polynomially with sample size.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7sh0xkn1ks/cover.png"/></item><item><title>OxonFair: A Flexible Toolkit for Algorithmic Fairness</title><link>https://deep-diver.github.io/neurips2024/posters/ztwl4ubnxv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ztwl4ubnxv/</guid><description>OxonFair: a new open-source toolkit for enforcing fairness in binary classification, supporting NLP, Computer Vision, and tabular data, optimizing any fairness metric, and minimizing performance degra&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ztwl4ubnxv/cover.png"/></item><item><title>PAC-Bayes-Chernoff bounds for unbounded losses</title><link>https://deep-diver.github.io/neurips2024/posters/cyzzend3lb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cyzzend3lb/</guid><description>New PAC-Bayes oracle bound extends Cramér-Chernoff to unbounded losses, enabling exact parameter optimization and richer assumptions for tighter generalization bounds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cyzzend3lb/cover.png"/></item><item><title>Parameter Symmetry and Noise Equilibrium of Stochastic Gradient Descent</title><link>https://deep-diver.github.io/neurips2024/posters/uhki1re2nz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uhki1re2nz/</guid><description>SGD&amp;rsquo;s dynamics are precisely characterized by the interplay of noise and symmetry in loss functions, leading to unique, initialization-independent fixed points.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uhki1re2nz/cover.png"/></item><item><title>Parameterized Approximation Schemes for Fair-Range Clustering</title><link>https://deep-diver.github.io/neurips2024/posters/zzgbudspzj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zzgbudspzj/</guid><description>First parameterized approximation schemes for fair-range k-median &amp;amp; k-means in Euclidean spaces are presented, offering faster (1+ε)-approximation algorithms.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zzgbudspzj/cover.png"/></item><item><title>Partial observation can induce mechanistic mismatches in data-constrained models of neural dynamics</title><link>https://deep-diver.github.io/neurips2024/posters/lcegp7ir6k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lcegp7ir6k/</guid><description>Partially observing neural circuits during experiments can create misleading models, even if single neuron activity matches; researchers need better validation methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lcegp7ir6k/cover.png"/></item><item><title>Partial Structure Discovery is Sufficient for No-regret Learning in Causal Bandits</title><link>https://deep-diver.github.io/neurips2024/posters/um3rq14iex/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/um3rq14iex/</guid><description>Learning optimal interventions in causal bandits with unknown causal graphs is now efficient; this paper identifies the minimal causal knowledge needed and offers a two-stage algorithm with sublinear &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/um3rq14iex/cover.png"/></item><item><title>Partial Transportability for Domain Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/2v5ltfhcfd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2v5ltfhcfd/</guid><description>This paper introduces a novel technique to bound prediction risks in new domains using causal diagrams, enabling reliable AI performance guarantees.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2v5ltfhcfd/cover.png"/></item><item><title>Paths to Equilibrium in Games</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/lxxiiinmuf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/lxxiiinmuf/</guid><description>In n-player games, a satisficing path always exists leading from any initial strategy profile to a Nash equilibrium by allowing unsatisfied players to explore suboptimal strategies.</description></item><item><title>Persistent Homology for High-dimensional Data Based on Spectral Methods</title><link>https://deep-diver.github.io/neurips2024/posters/arv1gjsozv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/arv1gjsozv/</guid><description>Spectral distances on k-nearest neighbor graphs enable robust topological analysis of high-dimensional noisy data using persistent homology, overcoming limitations of Euclidean distance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/arv1gjsozv/cover.png"/></item><item><title>Piecewise-Stationary Bandits with Knapsacks</title><link>https://deep-diver.github.io/neurips2024/posters/haa457jwjw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/haa457jwjw/</guid><description>A novel inventory reserving algorithm achieves near-optimal performance for bandit problems with knapsacks in piecewise-stationary settings, offering a competitive ratio of O(log(nmax/min)).</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/haa457jwjw/cover.png"/></item><item><title>Plant-and-Steal: Truthful Fair Allocations via Predictions</title><link>https://deep-diver.github.io/neurips2024/posters/afb97f8qsf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/afb97f8qsf/</guid><description>Learning-augmented mechanisms for fair allocation achieve constant-factor approximation with accurate predictions and near-optimal approximation even with inaccurate ones.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/afb97f8qsf/cover.png"/></item><item><title>Policy Aggregation</title><link>https://deep-diver.github.io/neurips2024/posters/ybiuvixjth/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ybiuvixjth/</guid><description>This paper introduces efficient algorithms that leverage social choice theory to aggregate multiple individual preferences, resulting in a desirable collective AI policy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ybiuvixjth/cover.png"/></item><item><title>Poseidon: Efficient Foundation Models for PDEs</title><link>https://deep-diver.github.io/neurips2024/posters/jc1vkk3uxk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jc1vkk3uxk/</guid><description>POSEIDON: a novel foundation model for PDEs achieves significant gains in accuracy and sample efficiency, generalizing well to unseen physics.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jc1vkk3uxk/cover.png"/></item><item><title>Position Coupling: Improving Length Generalization of Arithmetic Transformers Using Task Structure</title><link>https://deep-diver.github.io/neurips2024/posters/5cirdgm1ug/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/5cirdgm1ug/</guid><description>Position coupling, a novel method, enhances the length generalization ability of arithmetic Transformers by directly embedding task structures into positional encodings. This simple technique enables&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/5cirdgm1ug/cover.png"/></item><item><title>Principled Bayesian Optimization in Collaboration with Human Experts</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/idn9sikgly/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/idn9sikgly/</guid><description>COBOL: a novel Bayesian Optimization algorithm leverages human expert advice via binary labels, achieving both fast convergence and robustness to noisy input, while guaranteeing minimal expert effort.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/idn9sikgly/cover.png"/></item><item><title>Prior-itizing Privacy: A Bayesian Approach to Setting the Privacy Budget in Differential Privacy</title><link>https://deep-diver.github.io/neurips2024/posters/kamaxsjxgv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kamaxsjxgv/</guid><description>This paper introduces a Bayesian approach to setting the privacy budget in differential privacy, enabling agencies to balance data utility and confidentiality by customizing risk profiles.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kamaxsjxgv/cover.png"/></item><item><title>Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models</title><link>https://deep-diver.github.io/neurips2024/posters/kppbawjbry/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kppbawjbry/</guid><description>Researchers reveal &amp;lsquo;privacy backdoors,&amp;rsquo; a new attack that exploits pre-trained models to leak user training data, highlighting critical vulnerabilities and prompting stricter model security measures.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kppbawjbry/cover.png"/></item><item><title>Privacy without Noisy Gradients: Slicing Mechanism for Generative Model Training</title><link>https://deep-diver.github.io/neurips2024/posters/3mcr7zndsw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3mcr7zndsw/</guid><description>Train high-quality generative models with strong differential privacy using a novel slicing mechanism that injects noise into random low-dimensional data projections, avoiding noisy gradients.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3mcr7zndsw/cover.png"/></item><item><title>Private Algorithms for Stochastic Saddle Points and Variational Inequalities: Beyond Euclidean Geometry</title><link>https://deep-diver.github.io/neurips2024/posters/8ugolbjjpp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8ugolbjjpp/</guid><description>This paper presents novel, privacy-preserving algorithms achieving near-optimal rates for solving stochastic saddle point problems and variational inequalities in non-Euclidean geometries.</description></item><item><title>Private Edge Density Estimation for Random Graphs: Optimal, Efficient and Robust</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/4nq24chnoi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/4nq24chnoi/</guid><description>This paper delivers a groundbreaking polynomial-time algorithm for optimally estimating edge density in random graphs while ensuring node privacy and robustness against data corruption.</description></item><item><title>Private Geometric Median</title><link>https://deep-diver.github.io/neurips2024/posters/cpzjn7kabv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cpzjn7kabv/</guid><description>This paper introduces new differentially private algorithms to compute the geometric median, achieving improved accuracy by scaling with the effective data diameter instead of a known radius.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cpzjn7kabv/cover.png"/></item><item><title>Private Online Learning via Lazy Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/fkf0oqud3q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fkf0oqud3q/</guid><description>New transformation boosts privacy in online learning!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fkf0oqud3q/cover.png"/></item><item><title>Private Stochastic Convex Optimization with Heavy Tails: Near-Optimality from Simple Reductions</title><link>https://deep-diver.github.io/neurips2024/posters/ox6ail9f0y/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ox6ail9f0y/</guid><description>Achieving near-optimal rates for differentially private stochastic convex optimization with heavy-tailed gradients is possible using simple reduction-based techniques.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ox6ail9f0y/cover.png"/></item><item><title>PrivCirNet: Efficient Private Inference via Block Circulant Transformation</title><link>https://deep-diver.github.io/neurips2024/posters/vpsx3n6ice/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vpsx3n6ice/</guid><description>PrivCirNet accelerates private deep learning inference by cleverly transforming DNN weights into circulant matrices, converting matrix-vector multiplications into efficient 1D convolutions suitable fo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vpsx3n6ice/cover.png"/></item><item><title>PRODuctive bandits: Importance Weighting No More</title><link>https://deep-diver.github.io/neurips2024/posters/vdpze0nbpe/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vdpze0nbpe/</guid><description>Prod-family algorithms achieve optimal regret in adversarial multi-armed bandits, disproving prior suboptimality conjectures.</description></item><item><title>Promoting Fairness Among Dynamic Agents in Online-Matching Markets under Known Stationary Arrival Distributions</title><link>https://deep-diver.github.io/neurips2024/posters/0c3blhwjsy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0c3blhwjsy/</guid><description>This paper presents novel algorithms for online matching markets that prioritize fairness among dynamic agents, achieving asymptotic optimality in various scenarios and offering extensions to group-le&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0c3blhwjsy/cover.png"/></item><item><title>Proportional Fairness in Clustering: A Social Choice Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/kslx5pfpos/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kslx5pfpos/</guid><description>This paper reveals the surprising connection between individual and proportional fairness in clustering, showing that any approximation to one directly implies an approximation to the other, enabling &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kslx5pfpos/cover.png"/></item><item><title>Proportional Fairness in Non-Centroid Clustering</title><link>https://deep-diver.github.io/neurips2024/posters/actjv6wect/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/actjv6wect/</guid><description>This paper introduces proportionally fair non-centroid clustering, achieving fairness guarantees via novel algorithms and auditing methods, demonstrating significant improvements over traditional meth&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/actjv6wect/cover.png"/></item><item><title>ProTransformer: Robustify Transformers via Plug-and-Play Paradigm</title><link>https://deep-diver.github.io/neurips2024/posters/ukauurtbxx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ukauurtbxx/</guid><description>ProTransformer robustifies transformers with a novel plug-and-play attention mechanism, significantly improving robustness across various tasks and domains without retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ukauurtbxx/cover.png"/></item><item><title>Provable Acceleration of Nesterov's Accelerated Gradient for Asymmetric Matrix Factorization and Linear Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/x44oawaq7b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x44oawaq7b/</guid><description>This paper proves Nesterov&amp;rsquo;s Accelerated Gradient achieves faster convergence for rectangular matrix factorization and linear neural networks, using a novel unbalanced initialization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x44oawaq7b/cover.png"/></item><item><title>Provable Benefits of Complex Parameterizations for Structured State Space Models</title><link>https://deep-diver.github.io/neurips2024/posters/h15ryej151/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/h15ryej151/</guid><description>Complex numbers boost neural network performance! This study proves that complex parameterizations in structured state space models (SSMs) enable more efficient and practical learning of complex mappi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/h15ryej151/cover.png"/></item><item><title>Provable Editing of Deep Neural Networks using Parametric Linear Relaxation</title><link>https://deep-diver.github.io/neurips2024/posters/ighpud496d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ighpud496d/</guid><description>PREPARED efficiently edits DNNs to provably satisfy properties by relaxing the problem to a linear program, minimizing parameter changes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ighpud496d/cover.png"/></item><item><title>Provable Tempered Overfitting of Minimal Nets and Typical Nets</title><link>https://deep-diver.github.io/neurips2024/posters/qyr1dndxrp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qyr1dndxrp/</guid><description>Deep learning&amp;rsquo;s generalization ability defies conventional wisdom; this paper proves that overfitting in deep neural networks is &amp;rsquo;tempered&amp;rsquo;, neither catastrophic nor perfectly benign, for both minimal&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qyr1dndxrp/cover.png"/></item><item><title>Provably Optimal Memory Capacity for Modern Hopfield Models: Transformer-Compatible Dense Associative Memories as Spherical Codes</title><link>https://deep-diver.github.io/neurips2024/posters/4urew4ez6s/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4urew4ez6s/</guid><description>Researchers achieve provably optimal memory capacity in transformer-compatible Hopfield models by framing the problem as an optimal spherical code arrangement, resulting in a novel sublinear time algo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4urew4ez6s/cover.png"/></item><item><title>Provably Safe Neural Network Controllers via Differential Dynamic Logic</title><link>https://deep-diver.github.io/neurips2024/posters/sialfxa0nn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sialfxa0nn/</guid><description>Verifiably safe AI controllers are created via a novel framework, VerSAILLE, which uses differential dynamic logic and open-loop NN verification to prove safety for unbounded time horizons.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sialfxa0nn/cover.png"/></item><item><title>Proving Theorems Recursively</title><link>https://deep-diver.github.io/neurips2024/posters/yaa5l92ttq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yaa5l92ttq/</guid><description>POETRY: a recursive neural theorem prover achieving 5.1% higher success rate and solving substantially longer proofs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yaa5l92ttq/cover.png"/></item><item><title>Proximal Causal Inference With Text Data</title><link>https://deep-diver.github.io/neurips2024/posters/l4rwa0qyud/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/l4rwa0qyud/</guid><description>Unmeasured confounders hinder causal inference; this paper introduces a novel method using two pre-treatment text instances and zero-shot models to infer proxies for unobserved confounders, enabling p&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/l4rwa0qyud/cover.png"/></item><item><title>Pseudo-Private Data Guided Model Inversion Attacks</title><link>https://deep-diver.github.io/neurips2024/posters/pyqpuf36d2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pyqpuf36d2/</guid><description>Pseudo-Private Data Guided Model Inversion (PPDG-MI) significantly improves model inversion attacks by dynamically tuning the generative model to increase the sampling probability of actual private da&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pyqpuf36d2/cover.png"/></item><item><title>Public-data Assisted Private Stochastic Optimization: Power and Limitations</title><link>https://deep-diver.github.io/neurips2024/posters/j14wstqzni/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/j14wstqzni/</guid><description>Leveraging public data enhances differentially private (DP) learning, but its limits are unclear. This paper establishes tight theoretical bounds for DP stochastic convex optimization, revealing when &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/j14wstqzni/cover.png"/></item><item><title>Putting Gale &amp; Shapley to Work: Guaranteeing Stability Through Learning</title><link>https://deep-diver.github.io/neurips2024/posters/ivjs67xa44/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ivjs67xa44/</guid><description>Researchers improve two-sided matching market algorithms by prioritizing stability through novel bandit-learning algorithms, providing theoretical bounds on sample complexity and demonstrating intrigu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ivjs67xa44/cover.png"/></item><item><title>Quadratic Quantum Variational Monte Carlo</title><link>https://deep-diver.github.io/neurips2024/posters/ldtabi541u/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ldtabi541u/</guid><description>Q2VMC, a novel quantum chemistry algorithm, drastically boosts the efficiency and accuracy of solving the Schrödinger equation using a quadratic update mechanism and neural network ansatzes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ldtabi541u/cover.png"/></item><item><title>Qualitative Mechanism Independence</title><link>https://deep-diver.github.io/neurips2024/posters/re5lsv8qyh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/re5lsv8qyh/</guid><description>Researchers introduce QIM-compatibility, a novel framework for modeling qualitative relationships in probability distributions using directed hypergraphs, significantly expanding beyond standard condi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/re5lsv8qyh/cover.png"/></item><item><title>Quantifying Aleatoric Uncertainty of the Treatment Effect: A Novel Orthogonal Learner</title><link>https://deep-diver.github.io/neurips2024/posters/rdsdvshgka/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rdsdvshgka/</guid><description>New orthogonal learner quantifies treatment effect&amp;rsquo;s randomness, providing sharper insights beyond average effects.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rdsdvshgka/cover.png"/></item><item><title>Quantum algorithm for large-scale market equilibrium computation</title><link>https://deep-diver.github.io/neurips2024/posters/0de1dlmw2b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0de1dlmw2b/</guid><description>Quantum speedup achieved for large-scale market equilibrium computation!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0de1dlmw2b/cover.png"/></item><item><title>Quantum Algorithms for Non-smooth Non-convex Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/wsgzvhnoax/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wsgzvhnoax/</guid><description>Quantum algorithms achieve speedups in non-smooth, non-convex optimization, outperforming classical methods by a factor of ε⁻²/³ in query complexity for finding (δ,ε)-Goldstein stationary points.</description></item><item><title>Query-Efficient Correlation Clustering with Noisy Oracle</title><link>https://deep-diver.github.io/neurips2024/posters/wrcfuoiz1h/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wrcfuoiz1h/</guid><description>Novel algorithms for query-efficient correlation clustering with noisy oracles achieve a balance between query complexity and solution quality, offering theoretical guarantees and outperforming baseli&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wrcfuoiz1h/cover.png"/></item><item><title>Queueing Matching Bandits with Preference Feedback</title><link>https://deep-diver.github.io/neurips2024/posters/0tumaab3of/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0tumaab3of/</guid><description>Novel algorithms stabilize multi-server queueing systems with unknown service rates, achieving sublinear regret by learning server preferences via preference feedback.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0tumaab3of/cover.png"/></item><item><title>QWO: Speeding Up Permutation-Based Causal Discovery in LiGAMs</title><link>https://deep-diver.github.io/neurips2024/posters/bptjgapn9c/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bptjgapn9c/</guid><description>QWO: a novel method dramatically speeds up permutation-based causal discovery in linear Gaussian models, enabling the analysis of larger datasets and advancing causal inference.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bptjgapn9c/cover.png"/></item><item><title>Random Cycle Coding: Lossless Compression of Cluster Assignments via Bits-Back Coding</title><link>https://deep-diver.github.io/neurips2024/posters/xkvnqpdfqv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xkvnqpdfqv/</guid><description>Random Cycle Coding (RCC) optimally compresses cluster assignments in large datasets, saving up to 70% storage in vector databases by eliminating the need for integer IDs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xkvnqpdfqv/cover.png"/></item><item><title>Random Function Descent</title><link>https://deep-diver.github.io/neurips2024/posters/xzcubjhqbs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xzcubjhqbs/</guid><description>Random Function Descent (RFD) replaces the classical convex function framework with a random function approach, providing a scalable gradient descent method with inherent scale invariance and a theore&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xzcubjhqbs/cover.png"/></item><item><title>Randomized Strategic Facility Location with Predictions</title><link>https://deep-diver.github.io/neurips2024/posters/yvoen0kuzt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yvoen0kuzt/</guid><description>Randomized strategies improve truthful learning-augmented mechanisms for strategic facility location, achieving better approximations than deterministic methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yvoen0kuzt/cover.png"/></item><item><title>Randomized Truthful Auctions with Learning Agents</title><link>https://deep-diver.github.io/neurips2024/posters/tt2xjaxdc4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tt2xjaxdc4/</guid><description>Randomized truthful auctions outperform deterministic ones when bidders employ learning algorithms, maximizing revenue in repeated interactions.</description></item><item><title>RashomonGB: Analyzing the Rashomon Effect and Mitigating Predictive Multiplicity in Gradient Boosting</title><link>https://deep-diver.github.io/neurips2024/posters/zpw6nmhvku/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zpw6nmhvku/</guid><description>RashomonGB tackles predictive multiplicity in gradient boosting by introducing a novel inference technique to efficiently identify and mitigate conflicting model predictions, improving model selection&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zpw6nmhvku/cover.png"/></item><item><title>Reconstruction Attacks on Machine Unlearning: Simple Models are Vulnerable</title><link>https://deep-diver.github.io/neurips2024/posters/i4gqcm1r3z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/i4gqcm1r3z/</guid><description>Deleting data from machine learning models exposes individuals to highly accurate reconstruction attacks, even when models are simple; this research demonstrates the vulnerability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/i4gqcm1r3z/cover.png"/></item><item><title>Recurrent neural networks: vanishing and exploding gradients are not the end of the story</title><link>https://deep-diver.github.io/neurips2024/posters/46jr4sgtwa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/46jr4sgtwa/</guid><description>Recurrent neural networks struggle with long-term memory due to a newly identified &amp;lsquo;curse of memory&amp;rsquo;: increasing parameter sensitivity with longer memory. This work provides insights into RNN optimiza&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/46jr4sgtwa/cover.png"/></item><item><title>ReEvo: Large Language Models as Hyper-Heuristics with Reflective Evolution</title><link>https://deep-diver.github.io/neurips2024/posters/483ipg0hwl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/483ipg0hwl/</guid><description>ReEvo, a novel integration of evolutionary search and LLM reflections, generates state-of-the-art heuristics for combinatorial optimization problems, demonstrating superior sample efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/483ipg0hwl/cover.png"/></item><item><title>Refusal in Language Models Is Mediated by a Single Direction</title><link>https://deep-diver.github.io/neurips2024/posters/ph3xaqme6c/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ph3xaqme6c/</guid><description>LLM refusal is surprisingly mediated by a single, easily manipulated direction in the model&amp;rsquo;s activation space.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ph3xaqme6c/cover.png"/></item><item><title>RegExplainer: Generating Explanations for Graph Neural Networks in Regression Tasks</title><link>https://deep-diver.github.io/neurips2024/posters/ejwvcpluwu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ejwvcpluwu/</guid><description>RegExplainer unveils a novel method for interpreting graph neural networks in regression tasks, bridging the explanation gap by addressing distribution shifts and tackling continuously ordered decisio&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ejwvcpluwu/cover.png"/></item><item><title>Regression under demographic parity constraints via unlabeled post-processing</title><link>https://deep-diver.github.io/neurips2024/posters/utbjd5lgnc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/utbjd5lgnc/</guid><description>Ensuring fair regression predictions without using sensitive attributes? This paper presents a novel post-processing algorithm, achieving demographic parity with strong theoretical guarantees and comp&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/utbjd5lgnc/cover.png"/></item><item><title>Reimagining Mutual Information for Enhanced Defense against Data Leakage in Collaborative Inference</title><link>https://deep-diver.github.io/neurips2024/posters/tdzlky9usl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tdzlky9usl/</guid><description>InfoScissors defends collaborative inference from data leakage by cleverly reducing the mutual information between model outputs and sensitive device data, thus ensuring robust privacy without comprom&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tdzlky9usl/cover.png"/></item><item><title>Relational Verification Leaps Forward with RABBit</title><link>https://deep-diver.github.io/neurips2024/posters/w5u3xb1c11/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w5u3xb1c11/</guid><description>RABBit: A novel Branch-and-Bound verifier for precise relational verification of Deep Neural Networks, achieving substantial precision gains over current state-of-the-art baselines.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w5u3xb1c11/cover.png"/></item><item><title>Reliable Learning of Halfspaces under Gaussian Marginals</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/0lb8vzt1db/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/0lb8vzt1db/</guid><description>New algorithm reliably learns Gaussian halfspaces with significantly improved sample and computational complexity compared to existing methods, offering strong computational separation from standard a&amp;hellip;</description></item><item><title>ReLIZO: Sample Reusable Linear Interpolation-based Zeroth-order Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/yzvianpvu6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yzvianpvu6/</guid><description>ReLIZO boosts zeroth-order optimization by cleverly reusing past queries, drastically cutting computation costs while maintaining gradient estimation accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yzvianpvu6/cover.png"/></item><item><title>Replicability in Learning: Geometric Partitions and KKM-Sperner Lemma</title><link>https://deep-diver.github.io/neurips2024/posters/2ll7s5estj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2ll7s5estj/</guid><description>This paper reveals near-optimal relationships between geometric partitions and replicability in machine learning, establishing the optimality of existing algorithms and introducing a new neighborhood &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2ll7s5estj/cover.png"/></item><item><title>Replicable Uniformity Testing</title><link>https://deep-diver.github.io/neurips2024/posters/lciqpxcyc0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lciqpxcyc0/</guid><description>This paper presents the first replicable uniformity tester with nearly linear dependence on the replicability parameter, enhancing the reliability of scientific studies using distribution testing algo&amp;hellip;</description></item><item><title>Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/c4sinflvub/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/c4sinflvub/</guid><description>Reshuffling data splits during hyperparameter optimization surprisingly improves model generalization, offering a computationally cheaper alternative to standard methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/c4sinflvub/cover.png"/></item><item><title>Rethinking Parity Check Enhanced Symmetry-Preserving Ansatz</title><link>https://deep-diver.github.io/neurips2024/posters/aiubyryhhv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/aiubyryhhv/</guid><description>Enhanced VQAs via Hamming Weight Preserving ansatz and parity checks achieve superior performance on quantum chemistry and combinatorial problems, showcasing quantum advantage potential in NISQ era.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/aiubyryhhv/cover.png"/></item><item><title>Rethinking the Capacity of Graph Neural Networks for Branching Strategy</title><link>https://deep-diver.github.io/neurips2024/posters/femag0szwo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/femag0szwo/</guid><description>This paper proves that higher-order GNNs can universally approximate strong branching in MILP solvers, whereas simpler GNNs can only accurately approximate for a specific class of problems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/femag0szwo/cover.png"/></item><item><title>Rethinking Weight Decay for Robust Fine-Tuning of Foundation Models</title><link>https://deep-diver.github.io/neurips2024/posters/4neqdbz8eg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4neqdbz8eg/</guid><description>Selective Projection Decay (SPD) enhances robust fine-tuning of foundation models by selectively applying weight decay, improving generalization and out-of-distribution robustness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4neqdbz8eg/cover.png"/></item><item><title>Revisiting Differentially Private ReLU Regression</title><link>https://deep-diver.github.io/neurips2024/posters/3uuiwmxybr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3uuiwmxybr/</guid><description>Differentially private ReLU regression algorithms, DP-GLMtron and DP-TAGLMtron, achieve comparable performance with only an additional factor of O(log N) in the utility upper bound compared to the con&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3uuiwmxybr/cover.png"/></item><item><title>Revisiting K-mer Profile for Effective and Scalable Genome Representation Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ehsd856ltb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ehsd856ltb/</guid><description>This paper proposes a lightweight and scalable k-mer based model for metagenomic binning, achieving comparable performance to computationally expensive genome foundation models while significantly imp&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ehsd856ltb/cover.png"/></item><item><title>Robust and Faster Zeroth-Order Minimax Optimization: Complexity and Applications</title><link>https://deep-diver.github.io/neurips2024/posters/f8wkosfsaa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/f8wkosfsaa/</guid><description>ZO-GDEGA: A unified algorithm achieves faster, more robust zeroth-order minimax optimization with lower complexity and weaker conditions, solving stochastic nonconvex-concave problems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/f8wkosfsaa/cover.png"/></item><item><title>Robust Graph Neural Networks via Unbiased Aggregation</title><link>https://deep-diver.github.io/neurips2024/posters/dz6ex9ee0q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dz6ex9ee0q/</guid><description>RUNG: a novel GNN architecture boasting superior robustness against adaptive attacks by employing an unbiased aggregation technique.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dz6ex9ee0q/cover.png"/></item><item><title>Robust Mixture Learning when Outliers Overwhelm Small Groups</title><link>https://deep-diver.github.io/neurips2024/posters/trxv4dmdcg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/trxv4dmdcg/</guid><description>Outlier-robust mixture learning gets order-optimal error guarantees, even when outliers massively outnumber small groups, via a novel meta-algorithm leveraging mixture structure.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/trxv4dmdcg/cover.png"/></item><item><title>Robust Neural Contextual Bandit against Adversarial Corruptions</title><link>https://deep-diver.github.io/neurips2024/posters/6u8iv9hvps/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/6u8iv9hvps/</guid><description>R-NeuralUCB: A robust neural contextual bandit algorithm uses a context-aware gradient descent training to defend against adversarial reward corruptions, achieving better performance with theoretical &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/6u8iv9hvps/cover.png"/></item><item><title>Robust Sparse Regression with Non-Isotropic Designs</title><link>https://deep-diver.github.io/neurips2024/posters/ybsvnfd21c/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ybsvnfd21c/</guid><description>New algorithms achieve near-optimal error rates for sparse linear regression, even under adversarial data corruption and heavy-tailed noise distributions.</description></item><item><title>RoPINN: Region Optimized Physics-Informed Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/wzigmvfurk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wzigmvfurk/</guid><description>ROPINN: Revolutionizing Physics-Informed Neural Networks with Region Optimization</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wzigmvfurk/cover.png"/></item><item><title>Rule Based Rewards for Language Model Safety</title><link>https://deep-diver.github.io/neurips2024/posters/qvtwpt5dmg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qvtwpt5dmg/</guid><description>Rule-Based Rewards (RBRs) enhance LLM safety by using AI feedback and a few-shot prompt-based approach, achieving higher safety-behavior accuracy with less human annotation than existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qvtwpt5dmg/cover.png"/></item><item><title>S-SOS: Stochastic Sum-Of-Squares for Parametric Polynomial Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/ichqijtjhb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ichqijtjhb/</guid><description>S-SOS: A new algorithm solves complex, parameterized polynomial problems with provable convergence, enabling efficient solutions for high-dimensional applications like sensor network localization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ichqijtjhb/cover.png"/></item><item><title>Safe and Sparse Newton Method for Entropic-Regularized Optimal Transport</title><link>https://deep-diver.github.io/neurips2024/posters/nmmiyjw7xg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nmmiyjw7xg/</guid><description>A novel safe &amp;amp; sparse Newton method (SSNS) for entropic-regularized optimal transport boasts strict error control, avoids singularity, needs no hyperparameter tuning, and offers rigorous convergence a&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nmmiyjw7xg/cover.png"/></item><item><title>Safe Exploitative Play with Untrusted Type Beliefs</title><link>https://deep-diver.github.io/neurips2024/posters/qztj22aov4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qztj22aov4/</guid><description>This paper characterizes the fundamental tradeoff between trusting and distrusting learned type beliefs in games, establishing upper and lower bounds for optimal strategies in both normal-form and sto&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qztj22aov4/cover.png"/></item><item><title>Sample and Computationally Efficient Robust Learning of Gaussian Single-Index Models</title><link>https://deep-diver.github.io/neurips2024/posters/mn7d0s2i1d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mn7d0s2i1d/</guid><description>This paper presents a computationally efficient algorithm for robustly learning Gaussian single-index models under adversarial label noise, achieving near-optimal sample complexity.</description></item><item><title>Sample Complexity of Algorithm Selection Using Neural Networks and Its Applications to Branch-and-Cut</title><link>https://deep-diver.github.io/neurips2024/posters/uovrwvw1ya/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uovrwvw1ya/</guid><description>Neural networks enhance algorithm selection in branch-and-cut, significantly reducing tree sizes and improving efficiency for mixed-integer optimization, as proven by rigorous theoretical bounds and e&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uovrwvw1ya/cover.png"/></item><item><title>Sample Complexity of Interventional Causal Representation Learning</title><link>https://deep-diver.github.io/neurips2024/posters/xl9aaxl0u6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xl9aaxl0u6/</guid><description>First finite-sample analysis of interventional causal representation learning shows that surprisingly few samples suffice for accurate graph and latent variable recovery.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xl9aaxl0u6/cover.png"/></item><item><title>Sample Complexity of Posted Pricing for a Single Item</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ek1tyhcb3w/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ek1tyhcb3w/</guid><description>This paper reveals how many buyer samples are needed to set near-optimal posted prices for a single item, resolving a fundamental problem in online markets and offering both theoretical and practical &amp;hellip;</description></item><item><title>Sample Efficient Bayesian Learning of Causal Graphs from Interventions</title><link>https://deep-diver.github.io/neurips2024/posters/rfsvaom7ss/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rfsvaom7ss/</guid><description>Efficiently learn causal graphs from limited interventions using a novel Bayesian algorithm that outperforms existing methods and requires fewer experiments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rfsvaom7ss/cover.png"/></item><item><title>Sample-efficient Bayesian Optimisation Using Known Invariances</title><link>https://deep-diver.github.io/neurips2024/posters/rerls4opnm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rerls4opnm/</guid><description>Boost Bayesian Optimization&amp;rsquo;s efficiency by leveraging known invariances in objective functions for faster, more effective solutions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rerls4opnm/cover.png"/></item><item><title>Sample-Efficient Geometry Reconstruction from Euclidean Distances using Non-Convex Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/yu7h8zoui2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yu7h8zoui2/</guid><description>Reconstructing geometry from minimal Euclidean distance samples: A novel algorithm achieves state-of-the-art data efficiency with theoretical guarantees.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yu7h8zoui2/cover.png"/></item><item><title>Sample-Efficient Private Learning of Mixtures of Gaussians</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/74b6qx62vw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/74b6qx62vw/</guid><description>Researchers achieve a breakthrough in privacy-preserving machine learning by developing sample-efficient algorithms for learning Gaussian Mixture Models, significantly reducing the data needed while m&amp;hellip;</description></item><item><title>Scalable DP-SGD: Shuffling vs. Poisson Subsampling</title><link>https://deep-diver.github.io/neurips2024/posters/6gmnj9oc6d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/6gmnj9oc6d/</guid><description>This paper reveals significant privacy gaps in shuffling-based DP-SGD, proposes a scalable Poisson subsampling method, and demonstrates its superior utility for private model training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/6gmnj9oc6d/cover.png"/></item><item><title>Scalable Neural Network Verification with Branch-and-bound Inferred Cutting Planes</title><link>https://deep-diver.github.io/neurips2024/posters/fwhm1zpyft/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fwhm1zpyft/</guid><description>BICCOS: Scalable neural network verification via branch-and-bound inferred cutting planes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fwhm1zpyft/cover.png"/></item><item><title>Scaling Laws in Linear Regression: Compute, Parameters, and Data</title><link>https://deep-diver.github.io/neurips2024/posters/ph7sdeanxp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ph7sdeanxp/</guid><description>Deep learning&amp;rsquo;s neural scaling laws defy conventional wisdom; this paper uses infinite-dimensional linear regression to theoretically explain this phenomenon, showing that implicit regularization of S&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ph7sdeanxp/cover.png"/></item><item><title>Schur Nets: exploiting local structure for equivariance in higher order graph neural networks</title><link>https://deep-diver.github.io/neurips2024/posters/hrnsvflpgt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hrnsvflpgt/</guid><description>Schur Nets boost higher-order GNNs by efficiently exploiting local graph structure for automorphism equivariance, achieving improved performance without the computational burden of traditional methods&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hrnsvflpgt/cover.png"/></item><item><title>Score-based generative models are provably robust: an uncertainty quantification perspective</title><link>https://deep-diver.github.io/neurips2024/posters/ki5tane02e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ki5tane02e/</guid><description>Score-based generative models are provably robust to multiple error sources, as shown via a novel Wasserstein uncertainty propagation theorem.</description></item><item><title>Secret Collusion among AI Agents: Multi-Agent Deception via Steganography</title><link>https://deep-diver.github.io/neurips2024/posters/bnnsqhzj88/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bnnsqhzj88/</guid><description>AI agents can secretly collude using steganography, hiding their interactions from oversight. This research formalizes this threat, analyzes LLMs&amp;rsquo; capabilities, and proposes mitigation strategies.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bnnsqhzj88/cover.png"/></item><item><title>SEEV: Synthesis with Efficient Exact Verification for ReLU Neural Barrier Functions</title><link>https://deep-diver.github.io/neurips2024/posters/nwmqqhzi3w/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nwmqqhzi3w/</guid><description>SEEV framework efficiently verifies ReLU neural barrier functions by reducing activation regions and using tight over-approximations, significantly improving verification efficiency without sacrificin&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nwmqqhzi3w/cover.png"/></item><item><title>Semi-Random Matrix Completion via Flow-Based Adaptive Reweighting</title><link>https://deep-diver.github.io/neurips2024/posters/xzp1up0hh2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xzp1up0hh2/</guid><description>New nearly-linear time algorithm achieves high-accuracy semi-random matrix completion, overcoming previous limitations on accuracy and noise tolerance.</description></item><item><title>Semidefinite Relaxations of the Gromov-Wasserstein Distance</title><link>https://deep-diver.github.io/neurips2024/posters/rm3ffh1mqk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rm3ffh1mqk/</guid><description>This paper introduces a novel, tractable semidefinite program (SDP) relaxation for the Gromov-Wasserstein distance, enabling the computation of globally optimal transportation plans.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rm3ffh1mqk/cover.png"/></item><item><title>Separation and Bias of Deep Equilibrium Models on Expressivity and Learning Dynamics</title><link>https://deep-diver.github.io/neurips2024/posters/uj9k3j93md/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uj9k3j93md/</guid><description>Deep Equilibrium Models (DEQs) outperform standard neural networks, but lack theoretical understanding. This paper provides general separation results showing DEQ&amp;rsquo;s superior expressivity and character&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uj9k3j93md/cover.png"/></item><item><title>Sequential Probability Assignment with Contexts: Minimax Regret, Contextual Shtarkov Sums, and Contextual Normalized Maximum Likelihood</title><link>https://deep-diver.github.io/neurips2024/posters/urntypkf3v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/urntypkf3v/</guid><description>This paper introduces contextual Shtarkov sums, a new complexity measure characterizing minimax regret in sequential probability assignment with contexts, and derives the minimax optimal algorithm, co&amp;hellip;</description></item><item><title>Sequential Signal Mixing Aggregation for Message Passing Graph Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/arokfufiqs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/arokfufiqs/</guid><description>Sequential Signal Mixing Aggregation (SSMA) boosts message-passing graph neural network performance by effectively mixing neighbor features, achieving state-of-the-art results across various benchmark&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/arokfufiqs/cover.png"/></item><item><title>SGD vs GD: Rank Deficiency in Linear Networks</title><link>https://deep-diver.github.io/neurips2024/posters/tsaieshx3j/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tsaieshx3j/</guid><description>SGD surprisingly diminishes network rank, unlike GD, due to a repulsive force between eigenvalues, offering insights into deep learning generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tsaieshx3j/cover.png"/></item><item><title>Shaping the distribution of neural responses with interneurons in a recurrent circuit model</title><link>https://deep-diver.github.io/neurips2024/posters/ojlieq0j9t/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ojlieq0j9t/</guid><description>Researchers developed a recurrent neural circuit model that efficiently transforms sensory signals into neural representations by dynamically adjusting interneuron connectivity and activation function&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ojlieq0j9t/cover.png"/></item><item><title>Sharpness-Aware Minimization Activates the Interactive Teaching's Understanding and Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/prw98p1nv0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/prw98p1nv0/</guid><description>Sharpness Reduction Interactive Teaching (SRIT) boosts interactive teaching&amp;rsquo;s performance by integrating SAM&amp;rsquo;s generalization capabilities, leading to improved model accuracy and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/prw98p1nv0/cover.png"/></item><item><title>Shuffling Gradient-Based Methods for Nonconvex-Concave Minimax Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/lfy0sut3m9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lfy0sut3m9/</guid><description>New shuffling gradient methods achieve state-of-the-art oracle complexity for nonconvex-concave minimax optimization problems, offering improved performance and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lfy0sut3m9/cover.png"/></item><item><title>Sketchy Moment Matching: Toward Fast and Provable Data Selection for Finetuning</title><link>https://deep-diver.github.io/neurips2024/posters/yaaqwbmgit/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yaaqwbmgit/</guid><description>Sketchy Moment Matching (SkMM) is a fast and theoretically sound data selection method for deep learning finetuning. By controlling variance-bias tradeoffs in high dimensions, SkMM drastically reduces&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yaaqwbmgit/cover.png"/></item><item><title>SkipPredict: When to Invest in Predictions for Scheduling</title><link>https://deep-diver.github.io/neurips2024/posters/kvuw8vzsqz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kvuw8vzsqz/</guid><description>SkipPredict optimizes scheduling by prioritizing cheap predictions and using expensive ones only when necessary, achieving cost-effective performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kvuw8vzsqz/cover.png"/></item><item><title>Slack-Free Spiking Neural Network Formulation for Hypergraph Minimum Vertex Cover</title><link>https://deep-diver.github.io/neurips2024/posters/4a5iqejg8c/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4a5iqejg8c/</guid><description>A novel slack-free spiking neural network efficiently solves the Hypergraph Minimum Vertex Cover problem on neuromorphic hardware, outperforming CPU-based methods in both speed and energy consumption.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4a5iqejg8c/cover.png"/></item><item><title>Smoke and Mirrors in Causal Downstream Tasks</title><link>https://deep-diver.github.io/neurips2024/posters/iq2iawoznr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/iq2iawoznr/</guid><description>AI for science faces hidden biases in causal inference; this paper reveals these flaws using ant behavior data, introducing ISTAnt benchmark, and provides guidelines for more accurate causal AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/iq2iawoznr/cover.png"/></item><item><title>Smoothed Online Classification can be Harder than Batch Classification</title><link>https://deep-diver.github.io/neurips2024/posters/no9msezs6g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/no9msezs6g/</guid><description>Smoothed online classification can be harder than batch classification when label spaces are unbounded, challenging existing assumptions in machine learning.</description></item><item><title>Solving Inverse Problems via Diffusion Optimal Control</title><link>https://deep-diver.github.io/neurips2024/posters/wqlc4g1gn3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wqlc4g1gn3/</guid><description>Revolutionizing inverse problem solving, this paper introduces diffusion optimal control, a novel framework converting signal recovery into a discrete optimal control problem, surpassing limitations o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wqlc4g1gn3/cover.png"/></item><item><title>Spectral Graph Pruning Against Over-Squashing and Over-Smoothing</title><link>https://deep-diver.github.io/neurips2024/posters/emkrwjy2de/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/emkrwjy2de/</guid><description>Spectral graph pruning simultaneously mitigates over-squashing and over-smoothing in GNNs via edge deletion, improving generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/emkrwjy2de/cover.png"/></item><item><title>Stability and Generalization of Adversarial Training for Shallow Neural Networks with Smooth Activation</title><link>https://deep-diver.github.io/neurips2024/posters/9nsa4lvzed/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/9nsa4lvzed/</guid><description>This paper provides novel theoretical guarantees for adversarial training of shallow neural networks, improving generalization bounds via early stopping and Moreau&amp;rsquo;s envelope smoothing.</description></item><item><title>Stability and Generalization of Asynchronous SGD: Sharper Bounds Beyond Lipschitz and Smoothness</title><link>https://deep-diver.github.io/neurips2024/posters/bhp9hx4svi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bhp9hx4svi/</guid><description>Sharper ASGD generalization bounds achieved by leveraging on-average model stability, even without Lipschitz and smoothness assumptions; validated with diverse machine learning models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bhp9hx4svi/cover.png"/></item><item><title>Stable Minima Cannot Overfit in Univariate ReLU Networks: Generalization by Large Step Sizes</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/7swrtm9qsp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/7swrtm9qsp/</guid><description>Deep ReLU networks trained with large, constant learning rates avoid overfitting in univariate regression due to minima stability, generalizing well even with noisy labels.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/7swrtm9qsp/cover.png"/></item><item><title>Statistical and Geometrical properties of the Kernel Kullback-Leibler divergence</title><link>https://deep-diver.github.io/neurips2024/posters/rxqoiekea2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rxqoiekea2/</guid><description>Regularized Kernel Kullback-Leibler divergence solves the original KKL&amp;rsquo;s disjoint support limitation, enabling comparison of any probability distributions with a closed-form solution and efficient gra&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rxqoiekea2/cover.png"/></item><item><title>Statistical Estimation in the Spiked Tensor Model via the Quantum Approximate Optimization Algorithm</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/wtlvxdzhmp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/wtlvxdzhmp/</guid><description>Quantum Approximate Optimization Algorithm (QAOA) achieves weak recovery in spiked tensor models matching classical methods, but with potential constant factor advantages for certain parameters.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/wtlvxdzhmp/cover.png"/></item><item><title>Statistical Multicriteria Benchmarking via the GSD-Front</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/jxxvskb9hd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/jxxvskb9hd/</guid><description>Researchers can now reliably benchmark classifiers using multiple quality metrics via the GSD-front, a new information-efficient technique that accounts for statistical uncertainty and deviations from&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/jxxvskb9hd/cover.png"/></item><item><title>Statistical-Computational Trade-offs for Density Estimation</title><link>https://deep-diver.github.io/neurips2024/posters/ptd4azpzcr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ptd4azpzcr/</guid><description>Density estimation algorithms face inherent trade-offs: reducing sample needs often increases query time. This paper proves these trade-offs are fundamental, showing limits to how much improvement is&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ptd4azpzcr/cover.png"/></item><item><title>Stepwise Alignment for Constrained Language Model Policy Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/vrvx83bkqx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vrvx83bkqx/</guid><description>Stepwise Alignment for Constrained Policy Optimization (SACPO) efficiently aligns LLMs with human values, prioritizing both helpfulness and harmlessness via a novel stepwise approach.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vrvx83bkqx/cover.png"/></item><item><title>Stochastic Amortization: A Unified Approach to Accelerate Feature and Data Attribution</title><link>https://deep-diver.github.io/neurips2024/posters/zdwtn2hoie/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zdwtn2hoie/</guid><description>Stochastic Amortization accelerates feature and data attribution by training amortized models using noisy, yet unbiased, labels, achieving order-of-magnitude speedups over existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zdwtn2hoie/cover.png"/></item><item><title>Stochastic Concept Bottleneck Models</title><link>https://deep-diver.github.io/neurips2024/posters/isjqtq5s1f/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/isjqtq5s1f/</guid><description>Stochastic Concept Bottleneck Models (SCBMs) revolutionize interpretable ML by efficiently modeling concept dependencies, drastically improving intervention effectiveness and enabling CLIP-based conce&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/isjqtq5s1f/cover.png"/></item><item><title>Stochastic Extragradient with Flip-Flop Shuffling &amp; Anchoring: Provable Improvements</title><link>https://deep-diver.github.io/neurips2024/posters/ojxua0paio/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ojxua0paio/</guid><description>Stochastic extragradient with flip-flop shuffling &amp;amp; anchoring achieves provably faster convergence in minimax optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ojxua0paio/cover.png"/></item><item><title>Stochastic Newton Proximal Extragradient Method</title><link>https://deep-diver.github.io/neurips2024/posters/v4tzn87dtn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v4tzn87dtn/</guid><description>Stochastic Newton Proximal Extragradient (SNPE) achieves faster global and local convergence rates for strongly convex functions, improving upon existing stochastic Newton methods by requiring signifi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v4tzn87dtn/cover.png"/></item><item><title>Stochastic Optimal Control and Estimation with Multiplicative and Internal Noise</title><link>https://deep-diver.github.io/neurips2024/posters/mzhbkbywtp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mzhbkbywtp/</guid><description>A novel algorithm significantly improves stochastic optimal control by accurately modeling sensorimotor noise, achieving substantially lower costs than current state-of-the-art solutions, particularly&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mzhbkbywtp/cover.png"/></item><item><title>Stochastic Optimal Control Matching</title><link>https://deep-diver.github.io/neurips2024/posters/wfu2cdgmwt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wfu2cdgmwt/</guid><description>Stochastic Optimal Control Matching (SOCM) significantly reduces errors in stochastic optimal control by learning a matching vector field using a novel iterative diffusion optimization technique.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wfu2cdgmwt/cover.png"/></item><item><title>Stochastic Optimization Algorithms for Instrumental Variable Regression with Streaming Data</title><link>https://deep-diver.github.io/neurips2024/posters/2rs0fl7eet/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2rs0fl7eet/</guid><description>New streaming algorithms for instrumental variable regression achieve fast convergence rates, solving the problem efficiently without matrix inversions or mini-batches, enabling real-time causal analy&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2rs0fl7eet/cover.png"/></item><item><title>Stochastic Zeroth-Order Optimization under Strongly Convexity and Lipschitz Hessian: Minimax Sample Complexity</title><link>https://deep-diver.github.io/neurips2024/posters/jtyjwrplz5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jtyjwrplz5/</guid><description>Stochastic zeroth-order optimization of strongly convex functions with Lipschitz Hessian achieves optimal sample complexity, as proven by matching upper and lower bounds with a novel two-stage algorit&amp;hellip;</description></item><item><title>Strategic Linear Contextual Bandits</title><link>https://deep-diver.github.io/neurips2024/posters/apphmfe63y/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/apphmfe63y/</guid><description>Strategic agents gaming recommender systems is solved by a novel mechanism that incentivizes truthful behavior while minimizing regret, offering a solution to a key challenge in online learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/apphmfe63y/cover.png"/></item><item><title>Strategic Littlestone Dimension: Improved Bounds on Online Strategic Classification</title><link>https://deep-diver.github.io/neurips2024/posters/4lkzghiep1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4lkzghiep1/</guid><description>This paper introduces the Strategic Littlestone Dimension, a novel complexity measure for online strategic classification, proving instance-optimal mistake bounds in the realizable setting and improve&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4lkzghiep1/cover.png"/></item><item><title>Structured flexibility in recurrent neural networks via neuromodulation</title><link>https://deep-diver.github.io/neurips2024/posters/hbibqn3grd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hbibqn3grd/</guid><description>Neuromodulated RNNs (NM-RNNs) enhance RNN flexibility by dynamically scaling recurrent weights using a neuromodulatory subnetwork, achieving higher accuracy and generalizability on various tasks compa&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hbibqn3grd/cover.png"/></item><item><title>Structured Learning of Compositional Sequential Interventions</title><link>https://deep-diver.github.io/neurips2024/posters/fsa0ossdzj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fsa0ossdzj/</guid><description>Predicting outcomes of combined sequential interventions is challenging, especially in sparse data. This paper introduces CSI-VAE, a novel compositional model that provides reliable predictions for u&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fsa0ossdzj/cover.png"/></item><item><title>SuperDeepFool: a new fast and accurate minimal adversarial attack</title><link>https://deep-diver.github.io/neurips2024/posters/pqd7ckr8af/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pqd7ckr8af/</guid><description>SuperDeepFool: a fast, accurate algorithm generating minimal adversarial perturbations, significantly improving deep learning model robustness evaluation and adversarial training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pqd7ckr8af/cover.png"/></item><item><title>SureMap: Simultaneous mean estimation for single-task and multi-task disaggregated evaluation</title><link>https://deep-diver.github.io/neurips2024/posters/atnt3fuvbg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/atnt3fuvbg/</guid><description>SureMap, a new method, significantly boosts accuracy in single and multi-task disaggregated evaluations of AI models using limited data by transforming the problem into Gaussian mean estimation and cl&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/atnt3fuvbg/cover.png"/></item><item><title>SymILO: A Symmetry-Aware Learning Framework for Integer Linear Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/rtmytziw6l/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rtmytziw6l/</guid><description>SymILO: A novel symmetry-aware learning framework dramatically improves integer linear program (ILP) solutions by addressing data variability caused by ILP symmetry.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rtmytziw6l/cover.png"/></item><item><title>Symmetries in Overparametrized Neural Networks: A Mean Field View</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/l86glqncuj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/l86glqncuj/</guid><description>Overparametrized neural networks&amp;rsquo; learning dynamics are analyzed under data symmetries using mean-field theory, revealing that data augmentation, feature averaging, and equivariant architectures asymp&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/l86glqncuj/cover.png"/></item><item><title>Tangent Space Causal Inference: Leveraging Vector Fields for Causal Discovery in Dynamical Systems</title><link>https://deep-diver.github.io/neurips2024/posters/bj2cpb9dey/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bj2cpb9dey/</guid><description>Tangent Space Causal Inference (TSCI) enhances causal discovery in dynamical systems by leveraging vector fields, outperforming existing methods in accuracy and interpretability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bj2cpb9dey/cover.png"/></item><item><title>Targeted Sequential Indirect Experiment Design</title><link>https://deep-diver.github.io/neurips2024/posters/u3rgdb4li9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/u3rgdb4li9/</guid><description>Adaptive experiment design optimizes indirect experiments in complex systems by sequentially narrowing the gap between upper and lower bounds on a targeted query, providing more efficient and informat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/u3rgdb4li9/cover.png"/></item><item><title>Temporal Graph Neural Tangent Kernel with Graphon-Guaranteed</title><link>https://deep-diver.github.io/neurips2024/posters/266nh7klsv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/266nh7klsv/</guid><description>Temp-G³NTK: a novel temporal graph neural tangent kernel guarantees convergence to graphon NTK, offering superior performance in temporal graph classification and node-level tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/266nh7klsv/cover.png"/></item><item><title>Testably Learning Polynomial Threshold Functions</title><link>https://deep-diver.github.io/neurips2024/posters/5g0z6pdogj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/5g0z6pdogj/</guid><description>Testably learning polynomial threshold functions efficiently, matching agnostic learning&amp;rsquo;s best guarantees, is achieved, solving a key problem in robust machine learning.</description></item><item><title>Testing Calibration in Nearly-Linear Time</title><link>https://deep-diver.github.io/neurips2024/posters/01xv5za56k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/01xv5za56k/</guid><description>This paper presents nearly-linear time algorithms for testing model calibration, improving upon existing methods and providing theoretical lower bounds for various calibration measures.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/01xv5za56k/cover.png"/></item><item><title>The Bayesian sampling in a canonical recurrent circuit with a diversity of inhibitory interneurons</title><link>https://deep-diver.github.io/neurips2024/posters/vnmi0fhn6z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vnmi0fhn6z/</guid><description>Diverse inhibitory neurons in brain circuits enable faster Bayesian computation via Hamiltonian sampling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vnmi0fhn6z/cover.png"/></item><item><title>The Challenges of the Nonlinear Regime for Physics-Informed Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/fy6vptitte/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fy6vptitte/</guid><description>Physics-Informed Neural Networks (PINNs) training dynamics for nonlinear PDEs are fundamentally different than linear ones; this paper reveals why using second-order methods is crucial for solving non&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fy6vptitte/cover.png"/></item><item><title>The Empirical Impact of Neural Parameter Symmetries, or Lack Thereof</title><link>https://deep-diver.github.io/neurips2024/posters/pcvxyw6fkg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pcvxyw6fkg/</guid><description>Breaking neural network parameter symmetries leads to faster training, better generalization, and improved loss landscape behavior, as demonstrated by novel asymmetric network architectures.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pcvxyw6fkg/cover.png"/></item><item><title>The Fairness-Quality Tradeoff in Clustering</title><link>https://deep-diver.github.io/neurips2024/posters/bui2xeca7w/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bui2xeca7w/</guid><description>Novel algorithms trace the optimal balance between clustering quality and fairness, revealing all non-dominated solutions for various objectives.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bui2xeca7w/cover.png"/></item><item><title>The Group Robustness is in the Details: Revisiting Finetuning under Spurious Correlations</title><link>https://deep-diver.github.io/neurips2024/posters/ehziwahj06/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ehziwahj06/</guid><description>Finetuning&amp;rsquo;s impact on worst-group accuracy is surprisingly nuanced, with common class-balancing methods sometimes hurting performance; a novel mixture method consistently outperforms others.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ehziwahj06/cover.png"/></item><item><title>The Implicit Bias of Adam on Separable Data</title><link>https://deep-diver.github.io/neurips2024/posters/xrqxan3wkm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xrqxan3wkm/</guid><description>Adam&amp;rsquo;s implicit bias revealed: On separable data, Adam converges towards the maximum l∞-margin solution, a finding contrasting with gradient descent&amp;rsquo;s l2-margin preference. This polynomial-time conver&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xrqxan3wkm/cover.png"/></item><item><title>The Implicit Bias of Gradient Descent toward Collaboration between Layers: A Dynamic Analysis of Multilayer Perceptions</title><link>https://deep-diver.github.io/neurips2024/posters/jv6z08u7y0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jv6z08u7y0/</guid><description>Deep learning models&amp;rsquo; success hinges on understanding gradient descent&amp;rsquo;s implicit bias. This study reveals how this bias influences layer collaboration, revealing a decreasing trend in adversarial rob&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jv6z08u7y0/cover.png"/></item><item><title>The Implicit Bias of Heterogeneity towards Invariance: A Study of Multi-Environment Matrix Sensing</title><link>https://deep-diver.github.io/neurips2024/posters/pmpbxmf8t3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pmpbxmf8t3/</guid><description>Leveraging data heterogeneity, this study reveals that standard SGD implicitly learns invariant features across multiple environments, achieving robust generalization without explicit regularization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pmpbxmf8t3/cover.png"/></item><item><title>The Intelligible and Effective Graph Neural Additive Network</title><link>https://deep-diver.github.io/neurips2024/posters/sky1scutwa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sky1scutwa/</guid><description>GNAN: a novel interpretable graph neural network achieving accuracy comparable to black-box models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sky1scutwa/cover.png"/></item><item><title>The Limits of Differential Privacy in Online Learning</title><link>https://deep-diver.github.io/neurips2024/posters/cqr6e81ib7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cqr6e81ib7/</guid><description>This paper reveals fundamental limits of differential privacy in online learning, demonstrating a clear separation between pure, approximate, and non-private settings.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cqr6e81ib7/cover.png"/></item><item><title>The Minimax Rate of HSIC Estimation for Translation-Invariant Kernels</title><link>https://deep-diver.github.io/neurips2024/posters/kyno0n1bj9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kyno0n1bj9/</guid><description>Researchers found the minimax optimal rate of HSIC estimation for translation-invariant kernels is O(n⁻¹/²), settling a two-decade-old open question and validating many existing HSIC estimators.</description></item><item><title>The motion planning neural circuit in goal-directed navigation as Lie group operator search</title><link>https://deep-diver.github.io/neurips2024/posters/qz7bfmwizk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qz7bfmwizk/</guid><description>Neural circuits for goal-directed navigation are modeled as Lie group operator searches, implemented by a two-layer feedforward circuit mimicking Drosophila&amp;rsquo;s navigation system.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qz7bfmwizk/cover.png"/></item><item><title>The Power of Hard Attention Transformers on Data Sequences: A formal language theoretic perspective</title><link>https://deep-diver.github.io/neurips2024/posters/nbq1vmfp4x/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nbq1vmfp4x/</guid><description>Hard attention transformers show surprisingly greater power when processing numerical data sequences, exceeding capabilities on string data; this advancement is theoretically analyzed via circuit comp&amp;hellip;</description></item><item><title>The Price of Implicit Bias in Adversarially Robust Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/h1grus6cjn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/h1grus6cjn/</guid><description>Optimization&amp;rsquo;s implicit bias in robust machine learning hurts generalization; this work reveals how algorithm/architecture choices impact robustness, suggesting better optimization strategies are need&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/h1grus6cjn/cover.png"/></item><item><title>The Reliability of OKRidge Method in Solving Sparse Ridge Regression Problems</title><link>https://deep-diver.github.io/neurips2024/posters/r3ruv1gf8r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/r3ruv1gf8r/</guid><description>OKRidge&amp;rsquo;s reliability for solving sparse ridge regression problems is rigorously proven through theoretical error analysis, enhancing its applicability in machine learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/r3ruv1gf8r/cover.png"/></item><item><title>The Sample Complexity of Gradient Descent in Stochastic Convex Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/2inctkpby4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2inctkpby4/</guid><description>Gradient descent&amp;rsquo;s sample complexity in non-smooth stochastic convex optimization is Õ(d/m+1/√m), matching worst-case ERMs and showing no advantage over naive methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2inctkpby4/cover.png"/></item><item><title>The Secretary Problem with Predicted Additive Gap</title><link>https://deep-diver.github.io/neurips2024/posters/lbuxdzg1pd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lbuxdzg1pd/</guid><description>Beat the 1/e barrier in the secretary problem using only an additive gap prediction!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lbuxdzg1pd/cover.png"/></item><item><title>The Space Complexity of Approximating Logistic Loss</title><link>https://deep-diver.github.io/neurips2024/posters/vdlj3vee9a/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vdlj3vee9a/</guid><description>This paper proves fundamental space complexity lower bounds for approximating logistic loss, revealing that existing coreset constructions are surprisingly optimal.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vdlj3vee9a/cover.png"/></item><item><title>The Surprising Effectiveness of SP Voting with Partial Preferences</title><link>https://deep-diver.github.io/neurips2024/posters/cl9k2pauqb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cl9k2pauqb/</guid><description>Partial preferences and noisy votes hinder accurate ranking recovery; this paper introduces scalable SP voting variants, empirically demonstrating superior performance in recovering ground truth ranki&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cl9k2pauqb/cover.png"/></item><item><title>Theoretical Analysis of Weak-to-Strong Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/hosh0skkle/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hosh0skkle/</guid><description>Strong student models can learn from weaker teachers, even correcting errors and generalizing beyond the teacher&amp;rsquo;s expertise. This paper provides new theoretical bounds explaining this &amp;lsquo;weak-to-strong&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hosh0skkle/cover.png"/></item><item><title>Theoretical and Empirical Insights into the Origins of Degree Bias in Graph Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/1maaewthcz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1maaewthcz/</guid><description>Researchers unveil the origins of degree bias in Graph Neural Networks (GNNs), proving high-degree nodes&amp;rsquo; lower misclassification probability and proposing methods to alleviate this bias for fairer GN&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1maaewthcz/cover.png"/></item><item><title>Theoretical Characterisation of the Gauss Newton Conditioning in Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/fponumjlio/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fponumjlio/</guid><description>New theoretical bounds reveal how neural network architecture impacts the Gauss-Newton matrix&amp;rsquo;s conditioning, paving the way for improved optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fponumjlio/cover.png"/></item><item><title>Theoretical Foundations of Deep Selective State-Space Models</title><link>https://deep-diver.github.io/neurips2024/posters/3szrqwupux/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3szrqwupux/</guid><description>Deep learning&amp;rsquo;s sequence modeling is revolutionized by selective state-space models (SSMs)! This paper provides theoretical grounding for their superior performance, revealing the crucial role of gati&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3szrqwupux/cover.png"/></item><item><title>Theoretical guarantees in KL for Diffusion Flow Matching</title><link>https://deep-diver.github.io/neurips2024/posters/ia4wucwha9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ia4wucwha9/</guid><description>Novel theoretical guarantees for Diffusion Flow Matching (DFM) models are established, bounding the KL divergence under mild assumptions on data and base distributions.</description></item><item><title>Tight Bounds for Learning RUMs from Small Slates</title><link>https://deep-diver.github.io/neurips2024/posters/0nsy8niilp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0nsy8niilp/</guid><description>Learning user preferences accurately from limited data is key; this paper shows that surprisingly small datasets suffice for precise prediction, and provides efficient algorithms to achieve this.</description></item><item><title>Tight Rates for Bandit Control Beyond Quadratics</title><link>https://deep-diver.github.io/neurips2024/posters/mlm3nuwoeq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mlm3nuwoeq/</guid><description>This paper presents an algorithm achieving Õ(√T) optimal regret for bandit non-stochastic control with strongly-convex and smooth cost functions, overcoming prior limitations of suboptimal bounds.</description></item><item><title>Tighter Convergence Bounds for Shuffled SGD via Primal-Dual Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/qcplgtzww9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qcplgtzww9/</guid><description>Shuffled SGD&amp;rsquo;s convergence is now better understood through a primal-dual analysis, yielding tighter bounds that align with its superior empirical performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qcplgtzww9/cover.png"/></item><item><title>Topological Generalization Bounds for Discrete-Time Stochastic Optimization Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/6u5fchiwoc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/6u5fchiwoc/</guid><description>New topology-based complexity measures reliably predict deep learning model generalization, outperforming existing methods and offering practical computational efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/6u5fchiwoc/cover.png"/></item><item><title>Topological obstruction to the training of shallow ReLU neural networks</title><link>https://deep-diver.github.io/neurips2024/posters/3hcn0uxp72/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3hcn0uxp72/</guid><description>Shallow ReLU neural networks face topological training obstructions due to gradient flow confinement on disconnected quadric hypersurfaces.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3hcn0uxp72/cover.png"/></item><item><title>Towards Estimating Bounds on the Effect of Policies under Unobserved Confounding</title><link>https://deep-diver.github.io/neurips2024/posters/u5enpcwalt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/u5enpcwalt/</guid><description>This paper presents a novel framework for estimating bounds on policy effects under unobserved confounding, offering tighter bounds and robust estimators for higher-dimensional data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/u5enpcwalt/cover.png"/></item><item><title>Towards Harmless Rawlsian Fairness Regardless of Demographic Prior</title><link>https://deep-diver.github.io/neurips2024/posters/7u5mwus3rw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7u5mwus3rw/</guid><description>VFair achieves harmless Rawlsian fairness in regression tasks without relying on sensitive demographic data by minimizing the variance of training losses.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7u5mwus3rw/cover.png"/></item><item><title>Towards the Dynamics of a DNN Learning Symbolic Interactions</title><link>https://deep-diver.github.io/neurips2024/posters/dihxwkjxre/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dihxwkjxre/</guid><description>DNNs learn interactions in two phases: initially removing complex interactions, then gradually learning higher-order ones, leading to overfitting.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dihxwkjxre/cover.png"/></item><item><title>Trace is the Next AutoDiff: Generative Optimization with Rich Feedback, Execution Traces, and LLMs</title><link>https://deep-diver.github.io/neurips2024/posters/rys2dmn9td/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rys2dmn9td/</guid><description>Trace: Automating AI workflow design with LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rys2dmn9td/cover.png"/></item><item><title>Trade-Offs of Diagonal Fisher Information Matrix Estimators</title><link>https://deep-diver.github.io/neurips2024/posters/tvbckaqod8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tvbckaqod8/</guid><description>This paper examines the trade-offs between two popular diagonal Fisher Information Matrix (FIM) estimators in neural networks, deriving variance bounds and highlighting the importance of considering e&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tvbckaqod8/cover.png"/></item><item><title>Trading off Consistency and Dimensionality of Convex Surrogates for Multiclass Classification</title><link>https://deep-diver.github.io/neurips2024/posters/xcibvuxwpm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xcibvuxwpm/</guid><description>Researchers achieve a balance between accuracy and efficiency in multiclass classification by introducing partially consistent surrogate losses and novel methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xcibvuxwpm/cover.png"/></item><item><title>Trading Place for Space: Increasing Location Resolution Reduces Contextual Capacity in Hippocampal Codes</title><link>https://deep-diver.github.io/neurips2024/oral-ai-theory/reik4szmjt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-ai-theory/reik4szmjt/</guid><description>Boosting hippocampal spatial resolution surprisingly shrinks its contextual memory capacity, revealing a crucial trade-off between precision and context storage.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-ai-theory/reik4szmjt/cover.png"/></item><item><title>Training for Stable Explanation for Free</title><link>https://deep-diver.github.io/neurips2024/posters/hya3eu8scg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hya3eu8scg/</guid><description>R2ET: training for robust ranking explanations by an effective regularizer.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hya3eu8scg/cover.png"/></item><item><title>Transcendence: Generative Models Can Outperform The Experts That Train Them</title><link>https://deep-diver.github.io/neurips2024/posters/ejg9udqcy9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ejg9udqcy9/</guid><description>Generative models can outperform their human trainers: A groundbreaking study shows how autoregressive transformers, trained on chess game data, can achieve higher game ratings than any of the human &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ejg9udqcy9/cover.png"/></item><item><title>Transductive Learning is Compact</title><link>https://deep-diver.github.io/neurips2024/posters/ywtpmlktmj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ywtpmlktmj/</guid><description>Supervised learning&amp;rsquo;s sample complexity is compact: a hypothesis class is learnable if and only if all its finite projections are learnable, simplifying complexity analysis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ywtpmlktmj/cover.png"/></item><item><title>Transferability Bound Theory: Exploring Relationship between Adversarial Transferability and Flatness</title><link>https://deep-diver.github.io/neurips2024/posters/g522upazh3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/g522upazh3/</guid><description>Challenging common assumptions, researchers prove that flatter adversarial examples don&amp;rsquo;t guarantee better transferability and introduce TPA, a theoretically-grounded attack creating more transferable&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/g522upazh3/cover.png"/></item><item><title>Transformation-Invariant Learning and Theoretical Guarantees for OOD Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/u2gzfxrlan/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/u2gzfxrlan/</guid><description>This paper introduces a novel theoretical framework for robust machine learning under distribution shifts, offering learning rules and guarantees, highlighting the game-theoretic viewpoint of distribu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/u2gzfxrlan/cover.png"/></item><item><title>Trap-MID: Trapdoor-based Defense against Model Inversion Attacks</title><link>https://deep-diver.github.io/neurips2024/posters/gnhrgrcerd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gnhrgrcerd/</guid><description>Trap-MID: Outsmarting model inversion attacks with cleverly placed &amp;rsquo;trapdoors'!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gnhrgrcerd/cover.png"/></item><item><title>Treatment of Statistical Estimation Problems in Randomized Smoothing for Adversarial Robustness</title><link>https://deep-diver.github.io/neurips2024/posters/s4wx2qxhv9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s4wx2qxhv9/</guid><description>This paper optimizes randomized smoothing, a crucial certified defense against adversarial attacks, by introducing novel statistical methods that drastically reduce the computational cost, leading to &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/s4wx2qxhv9/cover.png"/></item><item><title>Truthful High Dimensional Sparse Linear Regression</title><link>https://deep-diver.github.io/neurips2024/posters/zmiad3jazn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zmiad3jazn/</guid><description>This paper presents a novel, truthful, and privacy-preserving mechanism for high-dimensional sparse linear regression, incentivizing data contribution while safeguarding individual privacy.</description></item><item><title>Truthfulness of Calibration Measures</title><link>https://deep-diver.github.io/neurips2024/posters/cda8hftygc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cda8hftygc/</guid><description>Researchers developed Subsampled Smooth Calibration Error (SSCE), a new truthful calibration measure for sequential prediction, solving the problem of existing measures being easily gamed.</description></item><item><title>UDC: A Unified Neural Divide-and-Conquer Framework for Large-Scale Combinatorial Optimization Problems</title><link>https://deep-diver.github.io/neurips2024/posters/dcgbyvmlwl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dcgbyvmlwl/</guid><description>A unified neural divide-and-conquer framework (UDC) achieves superior performance on large-scale combinatorial optimization problems by employing a novel Divide-Conquer-Reunion training method and a h&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dcgbyvmlwl/cover.png"/></item><item><title>Ultrafast classical phylogenetic method beats large protein language models on variant effect prediction</title><link>https://deep-diver.github.io/neurips2024/posters/h7menkyb2j/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/h7menkyb2j/</guid><description>A revolutionary ultrafast phylogenetic method outperforms protein language models in variant effect prediction by efficiently estimating amino acid substitution rates from massive datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/h7menkyb2j/cover.png"/></item><item><title>Uncovering, Explaining, and Mitigating the Superficial Safety of Backdoor Defense</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/qzfshkbwdo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/qzfshkbwdo/</guid><description>Current backdoor defenses, while effective at reducing attack success rates, are vulnerable to rapid re-learning. This work unveils this superficial safety, proposes a novel attack, and introduces a p&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/qzfshkbwdo/cover.png"/></item><item><title>Understanding and Improving Adversarial Collaborative Filtering for Robust Recommendation</title><link>https://deep-diver.github.io/neurips2024/posters/k8ayft5ed1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/k8ayft5ed1/</guid><description>PamaCF, a novel personalized adversarial collaborative filtering technique, significantly improves recommendation robustness and accuracy against poisoning attacks by dynamically adjusting perturbatio&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/k8ayft5ed1/cover.png"/></item><item><title>Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/0o7rd5jngv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0o7rd5jngv/</guid><description>This work systematically investigates the approximation properties of Transformer networks for sequence modeling, revealing the distinct roles of key components (self-attention, positional encoding, f&amp;hellip;</description></item><item><title>Unelicitable Backdoors via Cryptographic Transformer Circuits</title><link>https://deep-diver.github.io/neurips2024/posters/a560klf3v5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/a560klf3v5/</guid><description>Researchers unveil unelicitable backdoors in language models, using cryptographic transformer circuits, defying conventional detection methods and raising crucial AI safety concerns.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/a560klf3v5/cover.png"/></item><item><title>Unified Covariate Adjustment for Causal Inference</title><link>https://deep-diver.github.io/neurips2024/posters/ax9z2et6ul/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ax9z2et6ul/</guid><description>Unified Covariate Adjustment (UCA) offers a scalable, doubly robust estimator for a wide array of causal estimands beyond standard methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ax9z2et6ul/cover.png"/></item><item><title>Unified Mechanism-Specific Amplification by Subsampling and Group Privacy Amplification</title><link>https://deep-diver.github.io/neurips2024/posters/mvfrrmfgdy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mvfrrmfgdy/</guid><description>This paper presents a novel framework for achieving tighter differential privacy guarantees via mechanism-specific amplification using subsampling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mvfrrmfgdy/cover.png"/></item><item><title>Universal Exact Compression of Differentially Private Mechanisms</title><link>https://deep-diver.github.io/neurips2024/posters/cggjt8eg8a/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cggjt8eg8a/</guid><description>Poisson Private Representation (PPR) enables exact compression of any local differential privacy mechanism, achieving order-wise optimal trade-offs between communication, accuracy, and privacy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cggjt8eg8a/cover.png"/></item><item><title>Universality of AdaGrad Stepsizes for Stochastic Optimization: Inexact Oracle, Acceleration and Variance Reduction</title><link>https://deep-diver.github.io/neurips2024/posters/rniiavjhi5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rniiavjhi5/</guid><description>Adaptive gradient methods using AdaGrad stepsizes achieve optimal convergence rates for convex composite optimization problems, handling inexact oracles, acceleration, and variance reduction without n&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rniiavjhi5/cover.png"/></item><item><title>Unraveling the Gradient Descent Dynamics of Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/xswqeljjo5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xswqeljjo5/</guid><description>This paper reveals how large embedding dimensions and appropriate initialization guarantee convergence in Transformer training, highlighting Gaussian attention&amp;rsquo;s superior landscape over Softmax.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xswqeljjo5/cover.png"/></item><item><title>Unrolled denoising networks provably learn to perform optimal Bayesian inference</title><link>https://deep-diver.github.io/neurips2024/posters/cpklmjqzde/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cpklmjqzde/</guid><description>Unrolled neural networks, trained via gradient descent, provably achieve optimal Bayesian inference for compressed sensing, surpassing prior-aware counterparts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cpklmjqzde/cover.png"/></item><item><title>Unveiling the Hidden Structure of Self-Attention via Kernel Principal Component Analysis</title><link>https://deep-diver.github.io/neurips2024/posters/vuwvvvni6r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vuwvvvni6r/</guid><description>Self-attention, a key component of transformers, is revealed to be a projection of query vectors onto the principal components of the key matrix, derived from kernel PCA. This novel perspective leads&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vuwvvvni6r/cover.png"/></item><item><title>Unveiling the Potential of Robustness in Selecting Conditional Average Treatment Effect Estimators</title><link>https://deep-diver.github.io/neurips2024/posters/k4ep46q9x2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/k4ep46q9x2/</guid><description>A new, nuisance-free Distributionally Robust Metric (DRM) is proposed for selecting robust Conditional Average Treatment Effect (CATE) estimators, improving the reliability of personalized decision-ma&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/k4ep46q9x2/cover.png"/></item><item><title>Unveiling User Satisfaction and Creator Productivity Trade-Offs in Recommendation Platforms</title><link>https://deep-diver.github.io/neurips2024/posters/igan7rldcf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/igan7rldcf/</guid><description>Recommendation algorithms on UGC platforms face a critical trade-off: prioritizing user satisfaction reduces creator engagement, jeopardizing long-term content diversity. This research introduces a ga&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/igan7rldcf/cover.png"/></item><item><title>User-Creator Feature Polarization in Recommender Systems with Dual Influence</title><link>https://deep-diver.github.io/neurips2024/posters/ywq89o19wf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ywq89o19wf/</guid><description>Recommender systems, when influenced by both users and creators, inevitably polarize; however, prioritizing efficiency through methods like top-k truncation can surprisingly enhance diversity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ywq89o19wf/cover.png"/></item><item><title>User-item fairness tradeoffs in recommendations</title><link>https://deep-diver.github.io/neurips2024/posters/zozjms3jts/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zozjms3jts/</guid><description>Recommendation systems must balance user satisfaction with fair item exposure. This research provides a theoretical model and empirical validation showing that user preference diversity can significan&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zozjms3jts/cover.png"/></item><item><title>Using Noise to Infer Aspects of Simplicity Without Learning</title><link>https://deep-diver.github.io/neurips2024/posters/b172ac0r4l/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/b172ac0r4l/</guid><description>Noise in data surprisingly simplifies machine learning models, improving their interpretability without sacrificing accuracy; this paper quantifies this effect across various hypothesis spaces.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/b172ac0r4l/cover.png"/></item><item><title>Utilizing Human Behavior Modeling to Manipulate Explanations in AI-Assisted Decision Making: The Good, the Bad, and the Scary</title><link>https://deep-diver.github.io/neurips2024/posters/7xkwzapmvx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7xkwzapmvx/</guid><description>AI explanations can be subtly manipulated to influence human decisions, highlighting the urgent need for more robust and ethical AI explanation design.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7xkwzapmvx/cover.png"/></item><item><title>Validating Climate Models with Spherical Convolutional Wasserstein Distance</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/mmsffib6pi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/mmsffib6pi/</guid><description>Researchers developed Spherical Convolutional Wasserstein Distance (SCWD) to more accurately validate climate models by considering spatial variability and local distributional differences.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/mmsffib6pi/cover.png"/></item><item><title>Variance estimation in compound decision theory under boundedness</title><link>https://deep-diver.github.io/neurips2024/posters/hvcppndykt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hvcppndykt/</guid><description>Unlocking the optimal variance estimation rate in compound decision theory under bounded means, this paper reveals a surprising (log log n/log n)² rate and introduces a rate-optimal cumulant-based est&amp;hellip;</description></item><item><title>Warm-starting Push-Relabel</title><link>https://deep-diver.github.io/neurips2024/posters/yyy5lze547/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yyy5lze547/</guid><description>This research introduces the first theoretical guarantees for warm-starting the celebrated Push-Relabel network flow algorithm, improving its speed using a predicted flow, while maintaining worst-case&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yyy5lze547/cover.png"/></item><item><title>Wasserstein convergence of Cech persistence diagrams for samplings of submanifolds</title><link>https://deep-diver.github.io/neurips2024/posters/zehccykknh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zehccykknh/</guid><description>This paper proves that Čech persistence diagrams converge to the true underlying shape precisely when using Wasserstein distances with p &amp;gt; m, where m is the submanifold dimension, significantly advanc&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zehccykknh/cover.png"/></item><item><title>Wasserstein Distributionally Robust Optimization through the Lens of Structural Causal Models and Individual Fairness</title><link>https://deep-diver.github.io/neurips2024/posters/piozfx9whu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/piozfx9whu/</guid><description>This paper introduces Causally Fair DRO, a novel framework for robust optimization that addresses individual fairness concerns by incorporating causal structures and sensitive attributes, providing th&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/piozfx9whu/cover.png"/></item><item><title>Weisfeiler and Leman Go Loopy: A New Hierarchy for Graph Representational Learning</title><link>https://deep-diver.github.io/neurips2024/oral-ai-theory/9o2svnehor/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-ai-theory/9o2svnehor/</guid><description>This paper introduces r-lWL, a new graph isomorphism test hierarchy that surpasses the limitations of the Weisfeiler-Leman test by counting cycles up to length r+2, and its GNN counterpart, r-lMPNN, w&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-ai-theory/9o2svnehor/cover.png"/></item><item><title>What do Graph Neural Networks learn? Insights from Tropical Geometry</title><link>https://deep-diver.github.io/neurips2024/posters/oy2x0xfx0u/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oy2x0xfx0u/</guid><description>Using tropical geometry, researchers reveal that ReLU-activated message-passing GNNs learn continuous piecewise linear functions, highlighting their expressivity limits and paving the way for enhanced&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oy2x0xfx0u/cover.png"/></item><item><title>What does guidance do? A fine-grained analysis in a simple setting</title><link>https://deep-diver.github.io/neurips2024/posters/ads3h8sapi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ads3h8sapi/</guid><description>Diffusion guidance, a common generative modeling technique, is shown to not sample from its intended distribution; instead, it heavily biases samples towards the boundary of the conditional distributi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ads3h8sapi/cover.png"/></item><item><title>What Is Missing For Graph Homophily? Disentangling Graph Homophily For Graph Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/gmdgef8xxu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gmdgef8xxu/</guid><description>Tri-Hom disentangles graph homophily into label, structural, and feature aspects, providing a more comprehensive and accurate metric for predicting GNN performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gmdgef8xxu/cover.png"/></item><item><title>What makes unlearning hard and what to do about it</title><link>https://deep-diver.github.io/neurips2024/posters/qabhlbf72k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qabhlbf72k/</guid><description>Researchers developed RUM, a refined unlearning meta-algorithm, that significantly improves existing unlearning methods by strategically refining forget sets and employing appropriate unlearning algor&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qabhlbf72k/cover.png"/></item><item><title>What type of inference is planning?</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/txsrgrzicz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/txsrgrzicz/</guid><description>Planning is redefined as a distinct inference type within a variational framework, enabling efficient approximate planning in complex environments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/txsrgrzicz/cover.png"/></item><item><title>When are dynamical systems learned from time series data statistically accurate?</title><link>https://deep-diver.github.io/neurips2024/posters/4t3ox9hj3z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4t3ox9hj3z/</guid><description>Learned dynamical systems often fail to capture true physical behavior; this work introduces an ergodic theoretic approach to improve statistical accuracy by incorporating Jacobian information during &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4t3ox9hj3z/cover.png"/></item><item><title>When is an Embedding Model More Promising than Another?</title><link>https://deep-diver.github.io/neurips2024/posters/vqfz7itgcl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vqfz7itgcl/</guid><description>This paper introduces a novel, task-agnostic method for ranking embedding models using information sufficiency, a concept derived from communication theory and statistical experiments comparison, demo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vqfz7itgcl/cover.png"/></item><item><title>When Is Inductive Inference Possible?</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/2agcshccuv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/2agcshccuv/</guid><description>This paper provides a tight characterization of inductive inference, proving it&amp;rsquo;s possible if and only if the hypothesis class is a countable union of online learnable classes, resolving a long-standi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/2agcshccuv/cover.png"/></item><item><title>When is Multicalibration Post-Processing Necessary?</title><link>https://deep-diver.github.io/neurips2024/posters/oonojmx3wh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oonojmx3wh/</guid><description>Multicalibration post-processing isn&amp;rsquo;t always necessary; models often implicitly achieve it, especially calibrated ones. For uncalibrated models, though, it significantly improves fairness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oonojmx3wh/cover.png"/></item><item><title>When to Act and When to Ask: Policy Learning With Deferral Under Hidden Confounding</title><link>https://deep-diver.github.io/neurips2024/posters/tai8m5dixj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tai8m5dixj/</guid><description>CARED: a novel causal action recommendation model improves policy learning by collaborating with human experts and mitigating hidden confounding in observational data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tai8m5dixj/cover.png"/></item><item><title>Where Do Large Learning Rates Lead Us?</title><link>https://deep-diver.github.io/neurips2024/posters/g5lmfotfha/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/g5lmfotfha/</guid><description>Unlocking optimal neural network training: A narrow range of initially high learning rates, slightly above the convergence threshold, consistently yields superior generalization after fine-tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/g5lmfotfha/cover.png"/></item><item><title>Why Do We Need Weight Decay in Modern Deep Learning?</title><link>https://deep-diver.github.io/neurips2024/posters/yraxxsckm2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yraxxsckm2/</guid><description>Weight decay&amp;rsquo;s role in modern deep learning is surprisingly multifaceted, impacting optimization dynamics rather than solely regularization, improving generalization and training stability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yraxxsckm2/cover.png"/></item><item><title>Why the Metric Backbone Preserves Community Structure</title><link>https://deep-diver.github.io/neurips2024/posters/kx8i0rp7w2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kx8i0rp7w2/</guid><description>Metric backbone graph sparsification surprisingly preserves community structure, offering an efficient and robust method for analyzing large networks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kx8i0rp7w2/cover.png"/></item><item><title>Why Transformers Need Adam: A Hessian Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/x6rqepbnj3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x6rqepbnj3/</guid><description>Adam&amp;rsquo;s superiority over SGD in Transformer training is explained by the &amp;lsquo;block heterogeneity&amp;rsquo; of the Hessian matrix, highlighting the need for adaptive learning rates.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x6rqepbnj3/cover.png"/></item><item><title>Why Warmup the Learning Rate? Underlying Mechanisms and Improvements</title><link>https://deep-diver.github.io/neurips2024/posters/nvl4samz5c/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nvl4samz5c/</guid><description>Deep learning&amp;rsquo;s learning rate warmup improves performance by allowing larger learning rates, pushing networks to better-conditioned loss landscape areas.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nvl4samz5c/cover.png"/></item><item><title>Wide Two-Layer Networks can Learn from Adversarial Perturbations</title><link>https://deep-diver.github.io/neurips2024/posters/1yggaouvgz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1yggaouvgz/</guid><description>Wide two-layer neural networks can generalize well from mislabeled adversarial examples because adversarial perturbations surprisingly contain sufficient class-specific features.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1yggaouvgz/cover.png"/></item><item><title>Zeroth-Order Sampling Methods for Non-Log-Concave Distributions: Alleviating Metastability by Denoising Diffusion</title><link>https://deep-diver.github.io/neurips2024/posters/x3aljulsw5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x3aljulsw5/</guid><description>Zeroth-Order Diffusion Monte Carlo (ZOD-MC) efficiently samples from non-log-concave distributions using only zeroth-order queries, overcoming metastability issues and outperforming state-of-the-art s&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x3aljulsw5/cover.png"/></item><item><title>Zipper: Addressing Degeneracy in Algorithm-Agnostic Inference</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ahvohpkkmx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ahvohpkkmx/</guid><description>Zipper: A novel statistical device resolves the degeneracy issue in algorithm-agnostic inference, enabling reliable goodness-of-fit tests with enhanced power.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ahvohpkkmx/cover.png"/></item><item><title>πP^2: Effective Sharpness Aware Minimization Requires Layerwise Perturbation Scaling</title><link>https://deep-diver.github.io/neurips2024/posters/pr5g1bbqov/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pr5g1bbqov/</guid><description>µP²: Layerwise perturbation scaling in SAM enables hyperparameter transfer and improved generalization in large models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pr5g1bbqov/cover.png"/></item></channel></rss>