<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Reinforcement Learning on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/reinforcement-learning/</link><description>Recent content in Reinforcement Learning on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/reinforcement-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>A Nearly Optimal and Low-Switching Algorithm for Reinforcement Learning with General Function Approximation</title><link>https://deep-diver.github.io/neurips2024/posters/s3iczc2nlq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s3iczc2nlq/</guid><description>MQL-UCB: Near-optimal reinforcement learning with low policy switching cost, solving the exploration-exploitation dilemma for complex models.</description></item><item><title>A Study of Plasticity Loss in On-Policy Deep Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/msuf8kpktf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/msuf8kpktf/</guid><description>On-policy deep RL agents suffer from plasticity loss, but this paper introduces &amp;lsquo;regenerative&amp;rsquo; methods that consistently mitigate this, improving performance in challenging environments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/msuf8kpktf/cover.png"/></item><item><title>Adversarial Environment Design via Regret-Guided Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/eezclkwx6t/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/eezclkwx6t/</guid><description>Regret-Guided Diffusion Models enhance unsupervised environment design by generating challenging, diverse training environments that improve agent robustness and zero-shot generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/eezclkwx6t/cover.png"/></item><item><title>Artificial Generational Intelligence: Cultural Accumulation in Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/pmacrgu8gv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pmacrgu8gv/</guid><description>Reinforcement learning agents achieve emergent cultural accumulation by balancing social and independent learning, outperforming single-lifetime agents.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pmacrgu8gv/cover.png"/></item><item><title>Assouad, Fano, and Le Cam with Interaction: A Unifying Lower Bound Framework and Characterization for Bandit Learnability</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/hugd1anmrp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/hugd1anmrp/</guid><description>This paper presents a novel unified framework for deriving information-theoretic lower bounds for bandit learnability, unifying classical methods with interactive learning techniques and introducing a&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/hugd1anmrp/cover.png"/></item><item><title>Avoiding Undesired Future with Minimal Cost in Non-Stationary Environments</title><link>https://deep-diver.github.io/neurips2024/posters/yhd2khhntb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yhd2khhntb/</guid><description>AUF-MICNS: A novel sequential method efficiently solves the avoiding undesired future problem by dynamically updating influence relations in non-stationary environments while minimizing action costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yhd2khhntb/cover.png"/></item><item><title>Bigger, Regularized, Optimistic: scaling for compute and sample efficient continuous control</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/fu0xdh4aej/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/fu0xdh4aej/</guid><description>BRO (Bigger, Regularized, Optimistic) achieves state-of-the-art sample efficiency in continuous control by scaling critic networks and using strong regularization with optimistic exploration.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/fu0xdh4aej/cover.png"/></item><item><title>Bridging Model-Based Optimization and Generative Modeling via Conservative Fine-Tuning of Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/posters/zir2qju4hl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zir2qju4hl/</guid><description>BRAID: A novel, conservative fine-tuning method surpasses offline design optimization by cleverly combining generative diffusion models with reward models, preventing over-optimization and generating &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zir2qju4hl/cover.png"/></item><item><title>Can Learned Optimization Make Reinforcement Learning Less Difficult?</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ybxfwasa9z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ybxfwasa9z/</guid><description>Learned optimizer OPEN tackles RL&amp;rsquo;s non-stationarity, plasticity loss, and exploration using meta-learning, significantly outperforming traditional and other learned optimizers.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ybxfwasa9z/cover.png"/></item><item><title>Carrot and Stick: Eliciting Comparison Data and Beyond</title><link>https://deep-diver.github.io/neurips2024/posters/ofjtu2ktxo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ofjtu2ktxo/</guid><description>Truthful comparison data is hard to obtain without ground truth. This paper presents novel peer prediction mechanisms using bonus-penalty payments that incentivize truthful comparisons, even in networ&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ofjtu2ktxo/cover.png"/></item><item><title>Constrained Latent Action Policies for Model-Based Offline Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/pehvscmsgg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pehvscmsgg/</guid><description>Constrained Latent Action Policies (C-LAP) revolutionizes offline reinforcement learning by jointly modeling state-action distributions, implicitly constraining policies to improve efficiency and redu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pehvscmsgg/cover.png"/></item><item><title>Controlled maximal variability along with reliable performance in recurrent neural networks</title><link>https://deep-diver.github.io/neurips2024/posters/yxw2dctqdi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yxw2dctqdi/</guid><description>NeuroMOP, a novel neural principle, maximizes neural variability while ensuring reliable performance in recurrent neural networks, offering new insights into brain function and artificial intelligence&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yxw2dctqdi/cover.png"/></item><item><title>DiffTORI: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/mwj57tchwx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/mwj57tchwx/</guid><description>DiffTORI leverages differentiable trajectory optimization for superior deep reinforcement and imitation learning, outperforming prior state-of-the-art methods on high-dimensional robotic tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/mwj57tchwx/cover.png"/></item><item><title>Diffusion for World Modeling: Visual Details Matter in Atari</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/nadtwtodgc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/nadtwtodgc/</guid><description>DIAMOND, a novel reinforcement learning agent using a diffusion world model, achieves state-of-the-art performance on the Atari 100k benchmark by leveraging visual details often ignored by discrete la&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/nadtwtodgc/cover.png"/></item><item><title>Disentangled Unsupervised Skill Discovery for Efficient Hierarchical Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/epobcwfnfc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/epobcwfnfc/</guid><description>DUSDi: A novel method for learning disentangled skills in unsupervised reinforcement learning, enabling efficient reuse for diverse downstream tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/epobcwfnfc/cover.png"/></item><item><title>Dynamic Model Predictive Shielding for Provably Safe Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/x2zy4hzcmg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x2zy4hzcmg/</guid><description>Dynamic Model Predictive Shielding (DMPS) ensures provably safe reinforcement learning by dynamically optimizing reinforcement learning objectives while maintaining provable safety, achieving higher r&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x2zy4hzcmg/cover.png"/></item><item><title>Dynamics of Supervised and Reinforcement Learning in the Non-Linear Perceptron</title><link>https://deep-diver.github.io/neurips2024/posters/doajtihgiz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/doajtihgiz/</guid><description>Researchers developed a novel stochastic-process approach to precisely analyze learning dynamics in nonlinear perceptrons, revealing how input noise and learning rules significantly impact learning sp&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/doajtihgiz/cover.png"/></item><item><title>Efficient Recurrent Off-Policy RL Requires a Context-Encoder-Specific Learning Rate</title><link>https://deep-diver.github.io/neurips2024/posters/tswot8ttko/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tswot8ttko/</guid><description>Recurrent off-policy RL, while robust, suffers from training instability. RESEL, a novel algorithm, solves this by using a context-encoder-specific learning rate, significantly improving stability an&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tswot8ttko/cover.png"/></item><item><title>Enhancing Efficiency of Safe Reinforcement Learning via Sample Manipulation</title><link>https://deep-diver.github.io/neurips2024/posters/opfjhl6dpr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/opfjhl6dpr/</guid><description>ESPO enhances safe RL efficiency by dynamically manipulating sample size based on reward-safety gradient conflicts, ensuring faster training and superior performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/opfjhl6dpr/cover.png"/></item><item><title>Episodic Future Thinking Mechanism for Multi-agent Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/rl7otnsd9a/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rl7otnsd9a/</guid><description>RL agents make better decisions by simulating future scenarios, considering diverse agent behaviors, and using character inference for improved decision-making.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rl7otnsd9a/cover.png"/></item><item><title>Excluding the Irrelevant: Focusing Reinforcement Learning through Continuous Action Masking</title><link>https://deep-diver.github.io/neurips2024/posters/yvzwlfhprw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yvzwlfhprw/</guid><description>Boost RL efficiency in continuous action spaces by masking irrelevant actions using three novel continuous action masking methods!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yvzwlfhprw/cover.png"/></item><item><title>Exclusively Penalized Q-learning for Offline Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/2bdsnxeqcw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/2bdsnxeqcw/</guid><description>EPQ, a novel offline RL algorithm, significantly reduces underestimation bias by selectively penalizing states prone to errors, improving performance over existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/2bdsnxeqcw/cover.png"/></item><item><title>Extensive-Form Game Solving via Blackwell Approachability on Treeplexes</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/8aa3dhlk5h/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/8aa3dhlk5h/</guid><description>First algorithmic framework for Blackwell approachability on treeplexes, enabling stepsize-invariant EFG solvers with state-of-the-art convergence rates.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/8aa3dhlk5h/cover.png"/></item><item><title>Federated Ensemble-Directed Offline Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/ypaqe8uwsc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ypaqe8uwsc/</guid><description>FEDORA, a novel algorithm, enables high-quality policy learning in federated offline reinforcement learning by leveraging the collective wisdom of diverse client datasets without data sharing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ypaqe8uwsc/cover.png"/></item><item><title>Functional Bilevel Optimization for Machine Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/enlxhlwwff/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/enlxhlwwff/</guid><description>Functional Bilevel Optimization tackles the ambiguity of using neural networks in bilevel optimization by minimizing the inner objective over a function space, leading to scalable &amp;amp; efficient algorith&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/enlxhlwwff/cover.png"/></item><item><title>Generalized Linear Bandits with Limited Adaptivity</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ftpdbqut4g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ftpdbqut4g/</guid><description>This paper introduces two novel algorithms, achieving optimal regret in generalized linear contextual bandits despite limited policy updates, a significant advancement for real-world applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ftpdbqut4g/cover.png"/></item><item><title>Goal Reduction with Loop-Removal Accelerates RL and Models Human Brain Activity in Goal-Directed Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/y0efjjeb4v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/y0efjjeb4v/</guid><description>Goal Reduction with Loop-Removal accelerates Reinforcement Learning (RL) and accurately models human brain activity during goal-directed learning by efficiently deriving subgoals from distant original&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/y0efjjeb4v/cover.png"/></item><item><title>Implicit Curriculum in Procgen Made Explicit</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/nzb1fpxuu6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/nzb1fpxuu6/</guid><description>C-Procgen reveals implicit curriculum in Procgen&amp;rsquo;s multi-level training, showing learning shifts gradually from easy to hard contexts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/nzb1fpxuu6/cover.png"/></item><item><title>Improving Deep Reinforcement Learning by Reducing the Chain Effect of Value and Policy Churn</title><link>https://deep-diver.github.io/neurips2024/posters/cqoagpbarc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cqoagpbarc/</guid><description>Deep RL agents often suffer from instability due to the &amp;lsquo;chain effect&amp;rsquo; of value and policy churn; this paper introduces CHAIN, a novel method to reduce this churn, thereby improving DRL performance an&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cqoagpbarc/cover.png"/></item><item><title>Improving Environment Novelty Quantification for Effective Unsupervised Environment Design</title><link>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/udxpjko2f9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/udxpjko2f9/</guid><description>Boosting AI generalization: CENIE framework quantifies environment novelty via state-action coverage, enhancing unsupervised environment design for robust generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/udxpjko2f9/cover.png"/></item><item><title>Inverse Factorized Soft Q-Learning for Cooperative Multi-agent Imitation Learning</title><link>https://deep-diver.github.io/neurips2024/posters/xrbgxjomjp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xrbgxjomjp/</guid><description>New multi-agent imitation learning algorithm (MIFQ) leverages inverse soft Q-learning and factorization for stable, efficient training, achieving state-of-the-art results on challenging benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xrbgxjomjp/cover.png"/></item><item><title>Is Behavior Cloning All You Need? Understanding Horizon in Imitation Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/8kpyjm4gt5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/8kpyjm4gt5/</guid><description>Offline imitation learning achieves surprisingly strong performance, matching online methods&amp;rsquo; efficiency under certain conditions, contradicting prior assumptions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/8kpyjm4gt5/cover.png"/></item><item><title>Is Mamba Compatible with Trajectory Optimization in Offline Reinforcement Learning?</title><link>https://deep-diver.github.io/neurips2024/posters/ywsxjlfsmx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ywsxjlfsmx/</guid><description>Decision Mamba (DeMa) outperforms Decision Transformer (DT) in offline RL trajectory optimization with 30% fewer parameters in Atari and a quarter in MuJoCo, demonstrating the efficacy of Mamba&amp;rsquo;s line&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ywsxjlfsmx/cover.png"/></item><item><title>Is Value Learning Really the Main Bottleneck in Offline RL?</title><link>https://deep-diver.github.io/neurips2024/posters/nyp59a31ju/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nyp59a31ju/</guid><description>Offline RL&amp;rsquo;s performance often lags behind imitation learning, but this paper reveals that policy learning and generalization, not value function learning, are often the main bottlenecks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nyp59a31ju/cover.png"/></item><item><title>Learning Formal Mathematics From Intrinsic Motivation</title><link>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/unkltq8mbd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/unkltq8mbd/</guid><description>AI agent MINIMO learns to generate challenging mathematical conjectures and prove them, bootstrapping from axioms alone and self-improving in both conjecture generation and theorem proving.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/unkltq8mbd/cover.png"/></item><item><title>Learning Multimodal Behaviors from Scratch with Diffusion Policy Gradient</title><link>https://deep-diver.github.io/neurips2024/posters/vu1sibb57j/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vu1sibb57j/</guid><description>DDiffPG: A novel actor-critic algorithm learns multimodal policies from scratch using diffusion models, enabling agents to master versatile behaviors in complex tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vu1sibb57j/cover.png"/></item><item><title>Learning Successor Features the Simple Way</title><link>https://deep-diver.github.io/neurips2024/posters/ri7ozj1wmc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ri7ozj1wmc/</guid><description>Learn deep Successor Features (SFs) directly from pixels, efficiently and without representation collapse, using a novel, simple method combining TD and reward prediction loss!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ri7ozj1wmc/cover.png"/></item><item><title>Learning the Optimal Policy for Balancing Short-Term and Long-Term Rewards</title><link>https://deep-diver.github.io/neurips2024/posters/zgh0chwoco/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zgh0chwoco/</guid><description>A novel Decomposition-based Policy Learning (DPPL) method optimally balances short-term and long-term rewards, even with interrelated objectives, by transforming the problem into intuitive subproblems&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zgh0chwoco/cover.png"/></item><item><title>Learning to Balance Altruism and Self-interest Based on Empathy in Mixed-Motive Games</title><link>https://deep-diver.github.io/neurips2024/posters/ry0rxtjwjy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ry0rxtjwjy/</guid><description>AI agents learn to balance helpfulness and self-preservation using empathy to gauge social relationships and guide reward sharing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ry0rxtjwjy/cover.png"/></item><item><title>Local Anti-Concentration Class: Logarithmic Regret for Greedy Linear Contextual Bandit</title><link>https://deep-diver.github.io/neurips2024/posters/rblaf2euxq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rblaf2euxq/</guid><description>Greedy algorithms for linear contextual bandits achieve poly-logarithmic regret under the novel Local Anti-Concentration condition, expanding applicable distributions beyond Gaussians and uniforms.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rblaf2euxq/cover.png"/></item><item><title>Logarithmic Smoothing for Pessimistic Off-Policy Evaluation, Selection and Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/zlclygerk8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/zlclygerk8/</guid><description>Logarithmic Smoothing enhances pessimistic offline contextual bandit algorithms by providing tighter concentration bounds for improved policy evaluation, selection and learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/zlclygerk8/cover.png"/></item><item><title>Making Offline RL Online: Collaborative World Models for Offline Visual Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/ucxqrked0d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ucxqrked0d/</guid><description>CoWorld: a novel model-based RL approach tackles offline visual RL challenges by using online simulators as testbeds, enabling flexible value estimation &amp;amp; mitigating overestimation bias for effective &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ucxqrked0d/cover.png"/></item><item><title>Maximum Entropy Inverse Reinforcement Learning of Diffusion Models with Energy-Based Models</title><link>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/v0ojalqy4e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/v0ojalqy4e/</guid><description>Boosting diffusion model sample quality, especially with few steps, is achieved via a novel maximum entropy inverse reinforcement learning approach, jointly training the model and an energy-based mode&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/v0ojalqy4e/cover.png"/></item><item><title>Meta-Reinforcement Learning with Universal Policy Adaptation: Provable Near-Optimality under All-task Optimum Comparator</title><link>https://deep-diver.github.io/neurips2024/posters/rpjh69dux2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rpjh69dux2/</guid><description>Provable near-optimality in meta-RL is achieved using a novel bilevel optimization framework and universal policy adaptation algorithm.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rpjh69dux2/cover.png"/></item><item><title>NeoRL: Efficient Exploration for Nonepisodic RL</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/zwndgc13aw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/zwndgc13aw/</guid><description>NEORL: Novel nonepisodic RL algorithm guarantees optimal average cost with sublinear regret for nonlinear systems!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/zwndgc13aw/cover.png"/></item><item><title>On the Curses of Future and History in Future-dependent Value Functions for Off-policy Evaluation</title><link>https://deep-diver.github.io/neurips2024/posters/s5917zor6v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s5917zor6v/</guid><description>This paper tackles the &amp;lsquo;curse of horizon&amp;rsquo; in off-policy evaluation for partially observable Markov decision processes (POMDPs) by proposing novel coverage assumptions, enabling polynomial estimation e&amp;hellip;</description></item><item><title>Optimal Design for Human Preference Elicitation</title><link>https://deep-diver.github.io/neurips2024/posters/ccgwj61ael/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ccgwj61ael/</guid><description>Dope: Efficient algorithms optimize human preference elicitation for learning to rank, minimizing ranking loss and prediction error with absolute and ranking feedback models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ccgwj61ael/cover.png"/></item><item><title>Optimizing Automatic Differentiation with Deep Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/hvmi98a0ki/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/hvmi98a0ki/</guid><description>Deep reinforcement learning optimizes automatic differentiation, achieving up to 33% improvement in Jacobian computation by finding efficient elimination orders.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/hvmi98a0ki/cover.png"/></item><item><title>Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/grg6szbw9p/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/grg6szbw9p/</guid><description>VPL: a novel multimodal RLHF personalizes AI by inferring user-specific latent preferences, enabling accurate reward modeling and improved policy alignment for diverse populations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/grg6szbw9p/cover.png"/></item><item><title>Policy Mirror Descent with Lookahead</title><link>https://deep-diver.github.io/neurips2024/posters/om2aa0guha/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/om2aa0guha/</guid><description>Boosting reinforcement learning, this paper introduces h-PMD, a novel algorithm enhancing policy mirror descent with lookahead for faster convergence and improved sample complexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/om2aa0guha/cover.png"/></item><item><title>Provably Efficient Reinforcement Learning with Multinomial Logit Function Approximation</title><link>https://deep-diver.github.io/neurips2024/posters/z2739hyur3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z2739hyur3/</guid><description>This paper presents novel RL algorithms using multinomial logit function approximation, achieving O(1) computation and storage while nearly closing the regret gap with linear methods.</description></item><item><title>REBEL: Reinforcement Learning via Regressing Relative Rewards</title><link>https://deep-diver.github.io/neurips2024/posters/yxjwajzuyv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yxjwajzuyv/</guid><description>REBEL, a novel reinforcement learning algorithm, simplifies policy optimization by regressing relative rewards, achieving strong performance in language and image generation tasks with increased effic&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yxjwajzuyv/cover.png"/></item><item><title>Recurrent Reinforcement Learning with Memoroids</title><link>https://deep-diver.github.io/neurips2024/posters/na4q983a1v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/na4q983a1v/</guid><description>Memoroids and Tape-Based Batching revolutionize recurrent RL, enabling efficient processing of long sequences and improving sample efficiency by eliminating segmentation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/na4q983a1v/cover.png"/></item><item><title>Reinforcement Learning Gradients as Vitamin for Online Finetuning Decision Transformers</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/5l5bhyexyo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/5l5bhyexyo/</guid><description>Boost online finetuning of Decision Transformers by adding TD3 gradients, especially when pretrained with low-reward data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/5l5bhyexyo/cover.png"/></item><item><title>Reinforcement Learning Under Latent Dynamics: Toward Statistical and Algorithmic Modularity</title><link>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/qf2uzady1n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/qf2uzady1n/</guid><description>This paper pioneers a modular framework for reinforcement learning, addressing the challenge of learning under complex observations and simpler latent dynamics, offering both statistical and algorithm&amp;hellip;</description></item><item><title>Reinforcement Learning with Lookahead Information</title><link>https://deep-diver.github.io/neurips2024/posters/wlqfovltqz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wlqfovltqz/</guid><description>Provably efficient RL algorithms are designed to utilize immediate reward or transition information, significantly improving reward collection in unknown environments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wlqfovltqz/cover.png"/></item><item><title>Rethinking Exploration in Reinforcement Learning with Effective Metric-Based Exploration Bonus</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/qpkwfltzki/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/qpkwfltzki/</guid><description>Effective Metric-based Exploration Bonus (EME) enhances reinforcement learning exploration by using a robust metric for state discrepancy and a dynamically adjusted scaling factor based on reward mode&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/qpkwfltzki/cover.png"/></item><item><title>Sample Complexity Reduction via Policy Difference Estimation in Tabular Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ryq0kuzvkl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ryq0kuzvkl/</guid><description>This paper reveals that estimating only policy differences, while effective in bandits, is insufficient for tabular reinforcement learning. However, it introduces a novel algorithm achieving near-opti&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ryq0kuzvkl/cover.png"/></item><item><title>Sample-Efficient Agnostic Boosting</title><link>https://deep-diver.github.io/neurips2024/posters/ufkbrvyxtp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ufkbrvyxtp/</guid><description>Agnostic boosting gets a major efficiency upgrade! A new algorithm leverages sample reuse to drastically reduce the data needed for accurate learning, closing the gap with computationally expensive al&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ufkbrvyxtp/cover.png"/></item><item><title>Scalable and Effective Arithmetic Tree Generation for Adder and Multiplier Designs</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/5pnhgedg98/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/5pnhgedg98/</guid><description>ArithTreeRL, a novel reinforcement learning approach, generates optimized arithmetic tree structures for adders and multipliers, significantly improving computational efficiency and reducing hardware &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/5pnhgedg98/cover.png"/></item><item><title>Sequential Decision Making with Expert Demonstrations under Unobserved Heterogeneity</title><link>https://deep-diver.github.io/neurips2024/posters/c8cpmlpubi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/c8cpmlpubi/</guid><description>ExPerior leverages expert demonstrations to enhance online decision-making, even when experts use hidden contextual information unseen by the learner.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/c8cpmlpubi/cover.png"/></item><item><title>Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs</title><link>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/pgey8jq3qx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/pgey8jq3qx/</guid><description>This paper achieves minimax-optimal bounds for learning near-optimal policies in average-reward MDPs, addressing a long-standing open problem in reinforcement learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/pgey8jq3qx/cover.png"/></item><item><title>Statistical Efficiency of Distributional Temporal Difference Learning</title><link>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/ewum5hrygh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/ewum5hrygh/</guid><description>Researchers achieve minimax optimal sample complexity bounds for distributional temporal difference learning, enhancing reinforcement learning algorithm efficiency.</description></item><item><title>Stochastic contextual bandits with graph feedback: from independence number to MAS number</title><link>https://deep-diver.github.io/neurips2024/posters/t8iosewoyd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/t8iosewoyd/</guid><description>Contextual bandits with graph feedback achieve near-optimal regret by leveraging a novel graph-theoretic quantity that interpolates between independence and maximum acyclic subgraph numbers, depending&amp;hellip;</description></item><item><title>The Collusion of Memory and Nonlinearity in Stochastic Approximation With Constant Stepsize</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/xul75cvhl5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/xul75cvhl5/</guid><description>Unlocking the mysteries of stochastic approximation with constant stepsize, this paper reveals how memory and nonlinearity interact to create bias, providing novel analysis and solutions for more accu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/xul75cvhl5/cover.png"/></item><item><title>The Limits of Transfer Reinforcement Learning with Latent Low-rank Structure</title><link>https://deep-diver.github.io/neurips2024/posters/pk2qgry2hv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pk2qgry2hv/</guid><description>This paper presents computationally efficient transfer reinforcement learning algorithms that remove the dependence on state/action space sizes while achieving minimax optimality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pk2qgry2hv/cover.png"/></item><item><title>The Power of Resets in Online Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/7saccaomgi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/7saccaomgi/</guid><description>Leveraging local simulator resets in online reinforcement learning dramatically improves sample efficiency, especially for high-dimensional problems with general function approximation.</description></item><item><title>The Sample-Communication Complexity Trade-off in Federated Q-Learning</title><link>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/6yipvnkjuk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/6yipvnkjuk/</guid><description>Federated Q-learning achieves optimal sample &amp;amp; communication complexities simultaneously via Fed-DVR-Q, a novel algorithm.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/6yipvnkjuk/cover.png"/></item><item><title>The Value of Reward Lookahead in Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/uryeu8mwz1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/uryeu8mwz1/</guid><description>Reinforcement learning agents can achieve significantly higher rewards by using advance knowledge of future rewards; this paper mathematically analyzes this advantage by computing the worst-case perfo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/uryeu8mwz1/cover.png"/></item><item><title>Thompson Sampling For Combinatorial Bandits: Polynomial Regret and Mismatched Sampling Paradox</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/pgoubhydbr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/pgoubhydbr/</guid><description>A novel Thompson Sampling variant achieves polynomial regret for combinatorial bandits, solving a key limitation of existing methods and offering significantly improved performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/pgoubhydbr/cover.png"/></item><item><title>Towards an Information Theoretic Framework of Context-Based Offline Meta-Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/qfuszvw9mx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/qfuszvw9mx/</guid><description>UNICORN: a unified framework reveals that existing offline meta-reinforcement learning algorithms optimize variations of mutual information, leading to improved generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/qfuszvw9mx/cover.png"/></item><item><title>Transition Constrained Bayesian Optimization via Markov Decision Processes</title><link>https://deep-diver.github.io/neurips2024/posters/efrdruyhr9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/efrdruyhr9/</guid><description>This paper presents a novel BayesOpt framework that incorporates Markov Decision Processes to optimize black-box functions with transition constraints, overcoming limitations of traditional methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/efrdruyhr9/cover.png"/></item><item><title>Uncertainty-based Offline Variational Bayesian Reinforcement Learning for Robustness under Diverse Data Corruptions</title><link>https://deep-diver.github.io/neurips2024/posters/rtxciwsfsd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rtxciwsfsd/</guid><description>TRACER, a novel robust offline RL algorithm, uses Bayesian inference to handle uncertainty from diverse data corruptions, significantly outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rtxciwsfsd/cover.png"/></item><item><title>Variational Delayed Policy Optimization</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/datndzhbqj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/datndzhbqj/</guid><description>VDPO: A novel framework for delayed reinforcement learning achieving 50% sample efficiency improvement without compromising performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/datndzhbqj/cover.png"/></item></channel></rss>