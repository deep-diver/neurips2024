<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Reinforcement Learning on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/reinforcement-learning/</link><description>Recent content in Reinforcement Learning on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/reinforcement-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>A Study of Plasticity Loss in On-Policy Deep Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/msuf8kpktf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/msuf8kpktf/</guid><description>On-policy deep RL agents suffer from plasticity loss, but this paper introduces &amp;lsquo;regenerative&amp;rsquo; methods that consistently mitigate this, improving performance in challenging environments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/msuf8kpktf/cover.png"/></item><item><title>Adversarial Environment Design via Regret-Guided Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/eezclkwx6t/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/eezclkwx6t/</guid><description>Regret-Guided Diffusion Models enhance unsupervised environment design by generating challenging, diverse training environments that improve agent robustness and zero-shot generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/eezclkwx6t/cover.png"/></item><item><title>Assouad, Fano, and Le Cam with Interaction: A Unifying Lower Bound Framework and Characterization for Bandit Learnability</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/hugd1anmrp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/hugd1anmrp/</guid><description>This paper presents a novel unified framework for deriving information-theoretic lower bounds for bandit learnability, unifying classical methods with interactive learning techniques and introducing a&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/hugd1anmrp/cover.png"/></item><item><title>Bigger, Regularized, Optimistic: scaling for compute and sample efficient continuous control</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/fu0xdh4aej/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/fu0xdh4aej/</guid><description>BRO (Bigger, Regularized, Optimistic) achieves state-of-the-art sample efficiency in continuous control by scaling critic networks and using strong regularization with optimistic exploration.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/fu0xdh4aej/cover.png"/></item><item><title>Can Learned Optimization Make Reinforcement Learning Less Difficult?</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ybxfwasa9z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ybxfwasa9z/</guid><description>Learned optimizer OPEN tackles RL&amp;rsquo;s non-stationarity, plasticity loss, and exploration using meta-learning, significantly outperforming traditional and other learned optimizers.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ybxfwasa9z/cover.png"/></item><item><title>DiffTORI: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/mwj57tchwx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/mwj57tchwx/</guid><description>DiffTORI leverages differentiable trajectory optimization for superior deep reinforcement and imitation learning, outperforming prior state-of-the-art methods on high-dimensional robotic tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/mwj57tchwx/cover.png"/></item><item><title>Diffusion for World Modeling: Visual Details Matter in Atari</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/nadtwtodgc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/nadtwtodgc/</guid><description>DIAMOND, a novel reinforcement learning agent using a diffusion world model, achieves state-of-the-art performance on the Atari 100k benchmark by leveraging visual details often ignored by discrete la&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/nadtwtodgc/cover.png"/></item><item><title>Exclusively Penalized Q-learning for Offline Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/2bdsnxeqcw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/2bdsnxeqcw/</guid><description>EPQ, a novel offline RL algorithm, significantly reduces underestimation bias by selectively penalizing states prone to errors, improving performance over existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/2bdsnxeqcw/cover.png"/></item><item><title>Extensive-Form Game Solving via Blackwell Approachability on Treeplexes</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/8aa3dhlk5h/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/8aa3dhlk5h/</guid><description>First algorithmic framework for Blackwell approachability on treeplexes, enabling stepsize-invariant EFG solvers with state-of-the-art convergence rates.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/8aa3dhlk5h/cover.png"/></item><item><title>Functional Bilevel Optimization for Machine Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/enlxhlwwff/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/enlxhlwwff/</guid><description>Functional Bilevel Optimization tackles the ambiguity of using neural networks in bilevel optimization by minimizing the inner objective over a function space, leading to scalable &amp;amp; efficient algorith&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/enlxhlwwff/cover.png"/></item><item><title>Generalized Linear Bandits with Limited Adaptivity</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ftpdbqut4g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ftpdbqut4g/</guid><description>This paper introduces two novel algorithms, achieving optimal regret in generalized linear contextual bandits despite limited policy updates, a significant advancement for real-world applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ftpdbqut4g/cover.png"/></item><item><title>Goal Reduction with Loop-Removal Accelerates RL and Models Human Brain Activity in Goal-Directed Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/y0efjjeb4v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/y0efjjeb4v/</guid><description>Goal Reduction with Loop-Removal accelerates Reinforcement Learning (RL) and accurately models human brain activity during goal-directed learning by efficiently deriving subgoals from distant original&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/y0efjjeb4v/cover.png"/></item><item><title>Implicit Curriculum in Procgen Made Explicit</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/nzb1fpxuu6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/nzb1fpxuu6/</guid><description>C-Procgen reveals implicit curriculum in Procgen&amp;rsquo;s multi-level training, showing learning shifts gradually from easy to hard contexts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/nzb1fpxuu6/cover.png"/></item><item><title>Improving Environment Novelty Quantification for Effective Unsupervised Environment Design</title><link>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/udxpjko2f9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/udxpjko2f9/</guid><description>Boosting AI generalization: CENIE framework quantifies environment novelty via state-action coverage, enhancing unsupervised environment design for robust generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/udxpjko2f9/cover.png"/></item><item><title>Is Behavior Cloning All You Need? Understanding Horizon in Imitation Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/8kpyjm4gt5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/8kpyjm4gt5/</guid><description>Offline imitation learning achieves surprisingly strong performance, matching online methods&amp;rsquo; efficiency under certain conditions, contradicting prior assumptions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/8kpyjm4gt5/cover.png"/></item><item><title>Learning Formal Mathematics From Intrinsic Motivation</title><link>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/unkltq8mbd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/unkltq8mbd/</guid><description>AI agent MINIMO learns to generate challenging mathematical conjectures and prove them, bootstrapping from axioms alone and self-improving in both conjecture generation and theorem proving.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/unkltq8mbd/cover.png"/></item><item><title>Logarithmic Smoothing for Pessimistic Off-Policy Evaluation, Selection and Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/zlclygerk8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/zlclygerk8/</guid><description>Logarithmic Smoothing enhances pessimistic offline contextual bandit algorithms by providing tighter concentration bounds for improved policy evaluation, selection and learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/zlclygerk8/cover.png"/></item><item><title>Maximum Entropy Inverse Reinforcement Learning of Diffusion Models with Energy-Based Models</title><link>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/v0ojalqy4e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/v0ojalqy4e/</guid><description>Boosting diffusion model sample quality, especially with few steps, is achieved via a novel maximum entropy inverse reinforcement learning approach, jointly training the model and an energy-based mode&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/v0ojalqy4e/cover.png"/></item><item><title>NeoRL: Efficient Exploration for Nonepisodic RL</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/zwndgc13aw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/zwndgc13aw/</guid><description>NEORL: Novel nonepisodic RL algorithm guarantees optimal average cost with sublinear regret for nonlinear systems!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/zwndgc13aw/cover.png"/></item><item><title>Optimizing Automatic Differentiation with Deep Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/hvmi98a0ki/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/hvmi98a0ki/</guid><description>Deep reinforcement learning optimizes automatic differentiation, achieving up to 33% improvement in Jacobian computation by finding efficient elimination orders.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/hvmi98a0ki/cover.png"/></item><item><title>Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/grg6szbw9p/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/grg6szbw9p/</guid><description>VPL: a novel multimodal RLHF personalizes AI by inferring user-specific latent preferences, enabling accurate reward modeling and improved policy alignment for diverse populations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/grg6szbw9p/cover.png"/></item><item><title>Reinforcement Learning Gradients as Vitamin for Online Finetuning Decision Transformers</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/5l5bhyexyo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/5l5bhyexyo/</guid><description>Boost online finetuning of Decision Transformers by adding TD3 gradients, especially when pretrained with low-reward data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/5l5bhyexyo/cover.png"/></item><item><title>Reinforcement Learning Under Latent Dynamics: Toward Statistical and Algorithmic Modularity</title><link>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/qf2uzady1n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/qf2uzady1n/</guid><description>This paper pioneers a modular framework for reinforcement learning, addressing the challenge of learning under complex observations and simpler latent dynamics, offering both statistical and algorithm&amp;hellip;</description></item><item><title>Rethinking Exploration in Reinforcement Learning with Effective Metric-Based Exploration Bonus</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/qpkwfltzki/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/qpkwfltzki/</guid><description>Effective Metric-based Exploration Bonus (EME) enhances reinforcement learning exploration by using a robust metric for state discrepancy and a dynamically adjusted scaling factor based on reward mode&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/qpkwfltzki/cover.png"/></item><item><title>Sample Complexity Reduction via Policy Difference Estimation in Tabular Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ryq0kuzvkl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ryq0kuzvkl/</guid><description>This paper reveals that estimating only policy differences, while effective in bandits, is insufficient for tabular reinforcement learning. However, it introduces a novel algorithm achieving near-opti&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ryq0kuzvkl/cover.png"/></item><item><title>Scalable and Effective Arithmetic Tree Generation for Adder and Multiplier Designs</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/5pnhgedg98/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/5pnhgedg98/</guid><description>ArithTreeRL, a novel reinforcement learning approach, generates optimized arithmetic tree structures for adders and multipliers, significantly improving computational efficiency and reducing hardware &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/5pnhgedg98/cover.png"/></item><item><title>Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs</title><link>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/pgey8jq3qx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/pgey8jq3qx/</guid><description>This paper achieves minimax-optimal bounds for learning near-optimal policies in average-reward MDPs, addressing a long-standing open problem in reinforcement learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/pgey8jq3qx/cover.png"/></item><item><title>Statistical Efficiency of Distributional Temporal Difference Learning</title><link>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/ewum5hrygh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/ewum5hrygh/</guid><description>Researchers achieve minimax optimal sample complexity bounds for distributional temporal difference learning, enhancing reinforcement learning algorithm efficiency.</description></item><item><title>The Collusion of Memory and Nonlinearity in Stochastic Approximation With Constant Stepsize</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/xul75cvhl5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/xul75cvhl5/</guid><description>Unlocking the mysteries of stochastic approximation with constant stepsize, this paper reveals how memory and nonlinearity interact to create bias, providing novel analysis and solutions for more accu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/xul75cvhl5/cover.png"/></item><item><title>The Power of Resets in Online Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/7saccaomgi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/7saccaomgi/</guid><description>Leveraging local simulator resets in online reinforcement learning dramatically improves sample efficiency, especially for high-dimensional problems with general function approximation.</description></item><item><title>The Sample-Communication Complexity Trade-off in Federated Q-Learning</title><link>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/6yipvnkjuk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/6yipvnkjuk/</guid><description>Federated Q-learning achieves optimal sample &amp;amp; communication complexities simultaneously via Fed-DVR-Q, a novel algorithm.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/6yipvnkjuk/cover.png"/></item><item><title>The Value of Reward Lookahead in Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/uryeu8mwz1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/uryeu8mwz1/</guid><description>Reinforcement learning agents can achieve significantly higher rewards by using advance knowledge of future rewards; this paper mathematically analyzes this advantage by computing the worst-case perfo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/uryeu8mwz1/cover.png"/></item><item><title>Thompson Sampling For Combinatorial Bandits: Polynomial Regret and Mismatched Sampling Paradox</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/pgoubhydbr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/pgoubhydbr/</guid><description>A novel Thompson Sampling variant achieves polynomial regret for combinatorial bandits, solving a key limitation of existing methods and offering significantly improved performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/pgoubhydbr/cover.png"/></item><item><title>Towards an Information Theoretic Framework of Context-Based Offline Meta-Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/qfuszvw9mx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/qfuszvw9mx/</guid><description>UNICORN: a unified framework reveals that existing offline meta-reinforcement learning algorithms optimize variations of mutual information, leading to improved generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/qfuszvw9mx/cover.png"/></item><item><title>Variational Delayed Policy Optimization</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/datndzhbqj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/datndzhbqj/</guid><description>VDPO: A novel framework for delayed reinforcement learning achieving 50% sample efficiency improvement without compromising performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/datndzhbqj/cover.png"/></item></channel></rss>