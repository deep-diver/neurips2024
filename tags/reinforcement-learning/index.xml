<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Reinforcement Learning on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/reinforcement-learning/</link><description>Recent content in Reinforcement Learning on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/reinforcement-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>A Best-of-both-worlds Algorithm for Bandits with Delayed Feedback with Robustness to Excessive Delays</title><link>https://deep-diver.github.io/neurips2024/posters/ldzrqb4x5w/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ldzrqb4x5w/</guid><description>New best-of-both-worlds bandit algorithm tolerates arbitrary excessive delays, overcoming limitations of prior work that required prior knowledge of maximal delay and suffered linear regret dependence&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ldzrqb4x5w/cover.png"/></item><item><title>A Method for Evaluating Hyperparameter Sensitivity in Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/4ojdzhcwbb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4ojdzhcwbb/</guid><description>New empirical methodology quantifies how much reinforcement learning algorithm performance relies on per-environment hyperparameter tuning, enabling better algorithm design.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4ojdzhcwbb/cover.png"/></item><item><title>A Nearly Optimal and Low-Switching Algorithm for Reinforcement Learning with General Function Approximation</title><link>https://deep-diver.github.io/neurips2024/posters/s3iczc2nlq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s3iczc2nlq/</guid><description>MQL-UCB: Near-optimal reinforcement learning with low policy switching cost, solving the exploration-exploitation dilemma for complex models.</description></item><item><title>A Structure-Aware Framework for Learning Device Placements on Computation Graphs</title><link>https://deep-diver.github.io/neurips2024/posters/kzno1r3xef/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kzno1r3xef/</guid><description>Learn optimal device placement for neural networks with HSDAG, a novel framework boosting inference speed by up to 58.2%!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kzno1r3xef/cover.png"/></item><item><title>A Study of Plasticity Loss in On-Policy Deep Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/msuf8kpktf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/msuf8kpktf/</guid><description>On-policy deep RL agents suffer from plasticity loss, but this paper introduces &amp;lsquo;regenerative&amp;rsquo; methods that consistently mitigate this, improving performance in challenging environments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/msuf8kpktf/cover.png"/></item><item><title>A theoretical case-study of Scalable Oversight in Hierarchical Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/3tj3a26wsv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3tj3a26wsv/</guid><description>Bounded human feedback hinders large AI model training. This paper introduces hierarchical reinforcement learning to enable scalable oversight, efficiently acquiring feedback and learning optimal poli&amp;hellip;</description></item><item><title>A Unifying Normative Framework of Decision Confidence</title><link>https://deep-diver.github.io/neurips2024/posters/brvgfn3xfm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/brvgfn3xfm/</guid><description>New normative framework for decision confidence models diverse tasks by incorporating rewards, priors, and uncertainty, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/brvgfn3xfm/cover.png"/></item><item><title>A2PO: Towards Effective Offline Reinforcement Learning from an Advantage-aware Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/hyjrmgqq5e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hyjrmgqq5e/</guid><description>A2PO: A novel offline RL method tackles constraint conflicts in mixed-quality datasets by disentangling behavior policies with a conditional VAE and optimizing advantage-aware constraints, achieving s&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hyjrmgqq5e/cover.png"/></item><item><title>Achieving Tractable Minimax Optimal Regret in Average Reward MDPs</title><link>https://deep-diver.github.io/neurips2024/posters/sm9iwrhz4e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sm9iwrhz4e/</guid><description>First tractable algorithm achieves minimax optimal regret in average-reward MDPs, solving a major computational challenge in reinforcement learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sm9iwrhz4e/cover.png"/></item><item><title>Action Gaps and Advantages in Continuous-Time Distributional Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/brw0mkj7rr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/brw0mkj7rr/</guid><description>Distributional RL&amp;rsquo;s sensitivity to high-frequency decisions is unveiled, with new algorithms solving existing performance issues in continuous-time RL.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/brw0mkj7rr/cover.png"/></item><item><title>Adaptive Exploration for Data-Efficient General Value Function Evaluations</title><link>https://deep-diver.github.io/neurips2024/posters/hc6iqppt3l/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hc6iqppt3l/</guid><description>GVFExplorer: An adaptive behavior policy efficiently learns multiple GVFs by minimizing return variance, optimizing data usage and reducing prediction errors.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hc6iqppt3l/cover.png"/></item><item><title>Adversarial Environment Design via Regret-Guided Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/eezclkwx6t/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/eezclkwx6t/</guid><description>Regret-Guided Diffusion Models enhance unsupervised environment design by generating challenging, diverse training environments that improve agent robustness and zero-shot generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/eezclkwx6t/cover.png"/></item><item><title>Aligning Diffusion Behaviors with Q-functions for Efficient Continuous Control</title><link>https://deep-diver.github.io/neurips2024/posters/wd1dflup1m/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wd1dflup1m/</guid><description>Efficient Diffusion Alignment (EDA) leverages pretrained diffusion models and Q-functions for efficient continuous control, exceeding all baselines with minimal annotation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wd1dflup1m/cover.png"/></item><item><title>Almost Minimax Optimal Best Arm Identification in Piecewise Stationary Linear Bandits</title><link>https://deep-diver.github.io/neurips2024/posters/q5e3ftq3q3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/q5e3ftq3q3/</guid><description>PSɛBAI+ is a near-optimal algorithm for best arm identification in piecewise stationary linear bandits, efficiently detecting changepoints and aligning contexts for improved accuracy and minimal sampl&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/q5e3ftq3q3/cover.png"/></item><item><title>Amortized Planning with Large-Scale Transformers: A Case Study on Chess</title><link>https://deep-diver.github.io/neurips2024/posters/xlpipugygx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xlpipugygx/</guid><description>Large-scale transformers achieve grandmaster-level chess play via supervised learning on a new 10M game benchmark dataset, demonstrating impressive generalization beyond memorization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xlpipugygx/cover.png"/></item><item><title>An Analytical Study of Utility Functions in Multi-Objective Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/k3h2kzfz8h/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/k3h2kzfz8h/</guid><description>This paper provides novel theoretical analyses of utility functions in MORL, characterizing preferences and functions guaranteeing optimal policies.</description></item><item><title>An Offline Adaptation Framework for Constrained Multi-Objective Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/qb6cvdqa6b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qb6cvdqa6b/</guid><description>This work introduces PDOA, an offline adaptation framework for constrained multi-objective RL, using demonstrations instead of manually designed preferences to infer optimal policies while satisfying &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qb6cvdqa6b/cover.png"/></item><item><title>Artificial Generational Intelligence: Cultural Accumulation in Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/pmacrgu8gv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pmacrgu8gv/</guid><description>Reinforcement learning agents achieve emergent cultural accumulation by balancing social and independent learning, outperforming single-lifetime agents.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pmacrgu8gv/cover.png"/></item><item><title>Assouad, Fano, and Le Cam with Interaction: A Unifying Lower Bound Framework and Characterization for Bandit Learnability</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/hugd1anmrp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/hugd1anmrp/</guid><description>This paper presents a novel unified framework for deriving information-theoretic lower bounds for bandit learnability, unifying classical methods with interactive learning techniques and introducing a&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/hugd1anmrp/cover.png"/></item><item><title>Avoiding Undesired Future with Minimal Cost in Non-Stationary Environments</title><link>https://deep-diver.github.io/neurips2024/posters/yhd2khhntb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yhd2khhntb/</guid><description>AUF-MICNS: A novel sequential method efficiently solves the avoiding undesired future problem by dynamically updating influence relations in non-stationary environments while minimizing action costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yhd2khhntb/cover.png"/></item><item><title>Balancing Context Length and Mixing Times for Reinforcement Learning at Scale</title><link>https://deep-diver.github.io/neurips2024/posters/vaj4xow7ey/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vaj4xow7ey/</guid><description>Longer context in RL boosts generalization but slows down learning; this paper reveals the crucial tradeoff and offers theoretical insights.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vaj4xow7ey/cover.png"/></item><item><title>Bandits with Ranking Feedback</title><link>https://deep-diver.github.io/neurips2024/posters/acaspffahg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/acaspffahg/</guid><description>This paper introduces &amp;lsquo;bandits with ranking feedback,&amp;rsquo; a novel bandit variation providing ranked feedback instead of numerical rewards. It proves instance-dependent cases require superlogarithmic reg&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/acaspffahg/cover.png"/></item><item><title>BECAUSE: Bilinear Causal Representation for Generalizable Offline Model-based Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/4i9xupeu9w/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4i9xupeu9w/</guid><description>BECAUSE: a novel algorithm for generalizable offline model-based reinforcement learning that leverages bilinear causal representation to mitigate objective mismatch caused by confounders in offline da&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4i9xupeu9w/cover.png"/></item><item><title>Bigger, Regularized, Optimistic: scaling for compute and sample efficient continuous control</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/fu0xdh4aej/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/fu0xdh4aej/</guid><description>BRO (Bigger, Regularized, Optimistic) achieves state-of-the-art sample efficiency in continuous control by scaling critic networks and using strong regularization with optimistic exploration.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/fu0xdh4aej/cover.png"/></item><item><title>Bridging Model-Based Optimization and Generative Modeling via Conservative Fine-Tuning of Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/posters/zir2qju4hl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zir2qju4hl/</guid><description>BRAID: A novel, conservative fine-tuning method surpasses offline design optimization by cleverly combining generative diffusion models with reward models, preventing over-optimization and generating &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zir2qju4hl/cover.png"/></item><item><title>C-GAIL: Stabilizing Generative Adversarial Imitation Learning with Control Theory</title><link>https://deep-diver.github.io/neurips2024/posters/t4vwoiybf0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/t4vwoiybf0/</guid><description>C-GAIL stabilizes Generative Adversarial Imitation Learning by applying control theory, resulting in faster convergence, reduced oscillation, and better expert policy matching.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/t4vwoiybf0/cover.png"/></item><item><title>Can Learned Optimization Make Reinforcement Learning Less Difficult?</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ybxfwasa9z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ybxfwasa9z/</guid><description>Learned optimizer OPEN tackles RL&amp;rsquo;s non-stationarity, plasticity loss, and exploration using meta-learning, significantly outperforming traditional and other learned optimizers.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ybxfwasa9z/cover.png"/></item><item><title>Carrot and Stick: Eliciting Comparison Data and Beyond</title><link>https://deep-diver.github.io/neurips2024/posters/ofjtu2ktxo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ofjtu2ktxo/</guid><description>Truthful comparison data is hard to obtain without ground truth. This paper presents novel peer prediction mechanisms using bonus-penalty payments that incentivize truthful comparisons, even in networ&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ofjtu2ktxo/cover.png"/></item><item><title>Causal Imitation for Markov Decision Processes: a Partial Identification Approach</title><link>https://deep-diver.github.io/neurips2024/posters/khx0dkxdqh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/khx0dkxdqh/</guid><description>This paper presents novel causal imitation learning algorithms using partial identification to achieve expert performance even when unobserved confounders affect Markov Decision Processes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/khx0dkxdqh/cover.png"/></item><item><title>CE-NAS: An End-to-End Carbon-Efficient Neural Architecture Search Framework</title><link>https://deep-diver.github.io/neurips2024/posters/v6w55lckhn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v6w55lckhn/</guid><description>CE-NAS: A novel framework minimizes the carbon footprint of Neural Architecture Search by dynamically allocating GPU resources based on predicted carbon intensity, achieving state-of-the-art results w&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v6w55lckhn/cover.png"/></item><item><title>Confident Natural Policy Gradient for Local Planning in q_π-realizable Constrained MDPs</title><link>https://deep-diver.github.io/neurips2024/posters/tnemagwoxr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tnemagwoxr/</guid><description>Confident-NPG-CMDP: First primal-dual algorithm achieving polynomial sample complexity for solving constrained Markov decision processes (CMDPs) using function approximation and local access model.</description></item><item><title>Constrained Latent Action Policies for Model-Based Offline Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/pehvscmsgg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pehvscmsgg/</guid><description>Constrained Latent Action Policies (C-LAP) revolutionizes offline reinforcement learning by jointly modeling state-action distributions, implicitly constraining policies to improve efficiency and redu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pehvscmsgg/cover.png"/></item><item><title>Contextual Bilevel Reinforcement Learning for Incentive Alignment</title><link>https://deep-diver.github.io/neurips2024/posters/w3dx1tgw3f/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w3dx1tgw3f/</guid><description>Contextual Bilevel Reinforcement Learning (CB-RL) tackles real-world strategic decision-making where optimal policies depend on environmental configurations and exogenous events, proposing a stochasti&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w3dx1tgw3f/cover.png"/></item><item><title>Controlled maximal variability along with reliable performance in recurrent neural networks</title><link>https://deep-diver.github.io/neurips2024/posters/yxw2dctqdi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yxw2dctqdi/</guid><description>NeuroMOP, a novel neural principle, maximizes neural variability while ensuring reliable performance in recurrent neural networks, offering new insights into brain function and artificial intelligence&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yxw2dctqdi/cover.png"/></item><item><title>Decentralized Noncooperative Games with Coupled Decision-Dependent Distributions</title><link>https://deep-diver.github.io/neurips2024/posters/kqgszxbufw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kqgszxbufw/</guid><description>Decentralized noncooperative games with coupled decision-dependent distributions are analyzed, providing novel equilibrium concepts, uniqueness conditions, and a decentralized algorithm with sublinear&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kqgszxbufw/cover.png"/></item><item><title>Decision Mamba: Reinforcement Learning via Hybrid Selective Sequence Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/wfzimbtsy7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wfzimbtsy7/</guid><description>Decision Mamba-Hybrid (DM-H) accelerates in-context RL for long-term tasks by cleverly combining the strengths of Mamba&amp;rsquo;s linear long-term memory processing and transformer&amp;rsquo;s high-quality predictions,&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wfzimbtsy7/cover.png"/></item><item><title>Decomposed Prompt Decision Transformer for Efficient Unseen Task Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/hcqnhqoxs3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hcqnhqoxs3/</guid><description>Decomposed Prompt Decision Transformer (DPDT) efficiently learns prompts for unseen tasks using a two-stage paradigm, achieving superior performance in multi-task offline reinforcement learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hcqnhqoxs3/cover.png"/></item><item><title>Deep Policy Gradient Methods Without Batch Updates, Target Networks, or Replay Buffers</title><link>https://deep-diver.github.io/neurips2024/posters/dx5guwmffb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dx5guwmffb/</guid><description>Deep RL excels in simulated robotics, but struggles with real-world limitations like limited computational resources. This paper introduces Action Value Gradient (AVG), a novel incremental deep polic&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dx5guwmffb/cover.png"/></item><item><title>Deterministic Policies for Constrained Reinforcement Learning in Polynomial Time</title><link>https://deep-diver.github.io/neurips2024/posters/yremb4nakk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yremb4nakk/</guid><description>This paper presents an efficient algorithm to compute near-optimal deterministic policies for constrained reinforcement learning problems, solving a 25-year-old computational complexity challenge.</description></item><item><title>DiffTORI: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/mwj57tchwx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/mwj57tchwx/</guid><description>DiffTORI leverages differentiable trajectory optimization for superior deep reinforcement and imitation learning, outperforming prior state-of-the-art methods on high-dimensional robotic tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/mwj57tchwx/cover.png"/></item><item><title>Diffusion for World Modeling: Visual Details Matter in Atari</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/nadtwtodgc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/nadtwtodgc/</guid><description>DIAMOND, a novel reinforcement learning agent using a diffusion world model, achieves state-of-the-art performance on the Atari 100k benchmark by leveraging visual details often ignored by discrete la&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/nadtwtodgc/cover.png"/></item><item><title>Diffusion Spectral Representation for Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/c3tex45hjx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/c3tex45hjx/</guid><description>Diffusion Spectral Representation (Diff-SR) enables efficient reinforcement learning by extracting sufficient value function representations from diffusion models, bypassing slow sampling and facilita&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/c3tex45hjx/cover.png"/></item><item><title>Diffusion-based Curriculum Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/yrhrvadowe/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yrhrvadowe/</guid><description>DiCuRL uses diffusion models to generate challenging yet achievable RL training curricula, outperforming nine state-of-the-art methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yrhrvadowe/cover.png"/></item><item><title>Diffusion-Reward Adversarial Imitation Learning</title><link>https://deep-diver.github.io/neurips2024/posters/k9sh68mvjs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/k9sh68mvjs/</guid><description>Diffusion-Reward Adversarial Imitation Learning (DRAIL) enhances Generative Adversarial Imitation Learning by integrating diffusion models, resulting in more stable and smoother reward functions for s&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/k9sh68mvjs/cover.png"/></item><item><title>Discovering Creative Behaviors through DUPLEX: Diverse Universal Features for Policy Exploration</title><link>https://deep-diver.github.io/neurips2024/posters/bhgkt0suy6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bhgkt0suy6/</guid><description>DUPLEX: a novel RL method trains diverse, near-optimal policies in complex, dynamic environments by explicitly maximizing policy diversity using successor features. It outperforms existing methods in&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bhgkt0suy6/cover.png"/></item><item><title>Disentangled Unsupervised Skill Discovery for Efficient Hierarchical Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/epobcwfnfc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/epobcwfnfc/</guid><description>DUSDi: A novel method for learning disentangled skills in unsupervised reinforcement learning, enabling efficient reuse for diverse downstream tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/epobcwfnfc/cover.png"/></item><item><title>Distributional Reinforcement Learning with Regularized Wasserstein Loss</title><link>https://deep-diver.github.io/neurips2024/posters/cieyntpf28/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cieyntpf28/</guid><description>Sinkhorn distributional RL (SinkhornDRL) uses a regularized Wasserstein loss to improve distributional reinforcement learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cieyntpf28/cover.png"/></item><item><title>Distributionally Robust Reinforcement Learning with Interactive Data Collection: Fundamental Hardness and Near-Optimal Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/aywtfsf3up/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/aywtfsf3up/</guid><description>Provably sample-efficient robust RL via interactive data collection is achieved by introducing the vanishing minimal value assumption to mitigate the curse of support shift, enabling near-optimal algo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/aywtfsf3up/cover.png"/></item><item><title>Dynamic Model Predictive Shielding for Provably Safe Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/x2zy4hzcmg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x2zy4hzcmg/</guid><description>Dynamic Model Predictive Shielding (DMPS) ensures provably safe reinforcement learning by dynamically optimizing reinforcement learning objectives while maintaining provable safety, achieving higher r&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x2zy4hzcmg/cover.png"/></item><item><title>Dynamics of Supervised and Reinforcement Learning in the Non-Linear Perceptron</title><link>https://deep-diver.github.io/neurips2024/posters/doajtihgiz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/doajtihgiz/</guid><description>Researchers developed a novel stochastic-process approach to precisely analyze learning dynamics in nonlinear perceptrons, revealing how input noise and learning rules significantly impact learning sp&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/doajtihgiz/cover.png"/></item><item><title>EASI: Evolutionary Adversarial Simulator Identification for Sim-to-Real Transfer</title><link>https://deep-diver.github.io/neurips2024/posters/dqigggdoma/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dqigggdoma/</guid><description>EASI: Evolutionary Adversarial Simulator Identification bridges the reality gap in robotics by using GAN and ES to find optimal simulator parameters, enabling seamless sim-to-real transfer with minima&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dqigggdoma/cover.png"/></item><item><title>Effective Exploration Based on the Structural Information Principles</title><link>https://deep-diver.github.io/neurips2024/posters/bjh4mcys20/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bjh4mcys20/</guid><description>SI2E, a novel RL exploration framework, leverages structural information principles to maximize value-conditional structural entropy, significantly outperforming state-of-the-art baselines in various &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bjh4mcys20/cover.png"/></item><item><title>Efficient and Sharp Off-Policy Evaluation in Robust Markov Decision Processes</title><link>https://deep-diver.github.io/neurips2024/posters/lkguc2ry5v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lkguc2ry5v/</guid><description>This paper proposes a novel, statistically efficient offline policy evaluation method robust to environmental shifts and unobserved confounding, providing sharp bounds with theoretical guarantees.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lkguc2ry5v/cover.png"/></item><item><title>Efficient Multi-task Reinforcement Learning with Cross-Task Policy Guidance</title><link>https://deep-diver.github.io/neurips2024/posters/3quks3wrnh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3quks3wrnh/</guid><description>Boost multi-task reinforcement learning with Cross-Task Policy Guidance (CTPG)! CTPG cleverly uses policies from already mastered tasks to guide the learning of new tasks, significantly improving effi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3quks3wrnh/cover.png"/></item><item><title>Efficient Recurrent Off-Policy RL Requires a Context-Encoder-Specific Learning Rate</title><link>https://deep-diver.github.io/neurips2024/posters/tswot8ttko/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tswot8ttko/</guid><description>Recurrent off-policy RL, while robust, suffers from training instability. RESEL, a novel algorithm, solves this by using a context-encoder-specific learning rate, significantly improving stability an&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tswot8ttko/cover.png"/></item><item><title>Efficient Reinforcement Learning by Discovering Neural Pathways</title><link>https://deep-diver.github.io/neurips2024/posters/weoorep0n5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/weoorep0n5/</guid><description>Discover efficient neural pathways for reinforcement learning; drastically reducing model size and energy consumption without sacrificing performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/weoorep0n5/cover.png"/></item><item><title>Enabling Adaptive Agent Training in Open-Ended Simulators by Targeting Diversity</title><link>https://deep-diver.github.io/neurips2024/posters/xo1yqyw7yx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xo1yqyw7yx/</guid><description>DIVA: Evolutionary task generation for robust, adaptable AI agents in complex simulators.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xo1yqyw7yx/cover.png"/></item><item><title>Enhancing Efficiency of Safe Reinforcement Learning via Sample Manipulation</title><link>https://deep-diver.github.io/neurips2024/posters/opfjhl6dpr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/opfjhl6dpr/</guid><description>ESPO enhances safe RL efficiency by dynamically manipulating sample size based on reward-safety gradient conflicts, ensuring faster training and superior performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/opfjhl6dpr/cover.png"/></item><item><title>Enhancing Robustness in Deep Reinforcement Learning: A Lyapunov Exponent Approach</title><link>https://deep-diver.github.io/neurips2024/posters/hctikt7ls4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hctikt7ls4/</guid><description>Deep RL agents lack robustness; this paper enhances their resilience by implementing Maximal Lyapunov Exponent regularisation in the Dreamer V3 architecture, thus improving real-world applicability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hctikt7ls4/cover.png"/></item><item><title>Ensemble sampling for linear bandits: small ensembles suffice</title><link>https://deep-diver.github.io/neurips2024/posters/so7fnifq0o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/so7fnifq0o/</guid><description>Small ensembles in stochastic linear bandits achieve near-optimal regret; a rigorous analysis shows that ensemble size need only scale logarithmically with horizon.</description></item><item><title>Episodic Future Thinking Mechanism for Multi-agent Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/rl7otnsd9a/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rl7otnsd9a/</guid><description>RL agents make better decisions by simulating future scenarios, considering diverse agent behaviors, and using character inference for improved decision-making.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rl7otnsd9a/cover.png"/></item><item><title>Excluding the Irrelevant: Focusing Reinforcement Learning through Continuous Action Masking</title><link>https://deep-diver.github.io/neurips2024/posters/yvzwlfhprw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yvzwlfhprw/</guid><description>Boost RL efficiency in continuous action spaces by masking irrelevant actions using three novel continuous action masking methods!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yvzwlfhprw/cover.png"/></item><item><title>Exclusively Penalized Q-learning for Offline Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/2bdsnxeqcw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/2bdsnxeqcw/</guid><description>EPQ, a novel offline RL algorithm, significantly reduces underestimation bias by selectively penalizing states prone to errors, improving performance over existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/2bdsnxeqcw/cover.png"/></item><item><title>Exploiting the Replay Memory Before Exploring the Environment: Enhancing Reinforcement Learning Through Empirical MDP Iteration</title><link>https://deep-diver.github.io/neurips2024/posters/lsd27juj8v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lsd27juj8v/</guid><description>Boost RL performance by solving a series of simplified MDPs before tackling the complex real-world one!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lsd27juj8v/cover.png"/></item><item><title>Extensive-Form Game Solving via Blackwell Approachability on Treeplexes</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/8aa3dhlk5h/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/8aa3dhlk5h/</guid><description>First algorithmic framework for Blackwell approachability on treeplexes, enabling stepsize-invariant EFG solvers with state-of-the-art convergence rates.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/8aa3dhlk5h/cover.png"/></item><item><title>Fast TRAC: A Parameter-Free Optimizer for Lifelong Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/qeahe4tugc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qeahe4tugc/</guid><description>TRAC: a parameter-free optimizer conquering lifelong RL&amp;rsquo;s plasticity loss!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qeahe4tugc/cover.png"/></item><item><title>Federated Ensemble-Directed Offline Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/ypaqe8uwsc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ypaqe8uwsc/</guid><description>FEDORA, a novel algorithm, enables high-quality policy learning in federated offline reinforcement learning by leveraging the collective wisdom of diverse client datasets without data sharing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ypaqe8uwsc/cover.png"/></item><item><title>Federated Natural Policy Gradient and Actor Critic Methods for Multi-task Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/dufd6vsyf8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dufd6vsyf8/</guid><description>This paper introduces federated natural policy gradient and actor-critic methods achieving near dimension-free global convergence for decentralized multi-task reinforcement learning, a significant bre&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dufd6vsyf8/cover.png"/></item><item><title>Finding good policies in average-reward Markov Decision Processes without prior knowledge</title><link>https://deep-diver.github.io/neurips2024/posters/hpvif4w5dd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hpvif4w5dd/</guid><description>First near-optimal reinforcement learning algorithm achieving best policy identification in average-reward MDPs without prior knowledge of complexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hpvif4w5dd/cover.png"/></item><item><title>First-Explore, then Exploit: Meta-Learning to Solve Hard Exploration-Exploitation Trade-Offs</title><link>https://deep-diver.github.io/neurips2024/posters/ahjtu2aiiw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ahjtu2aiiw/</guid><description>Meta-RL agents often fail to explore effectively in environments where optimal behavior requires sacrificing immediate rewards for greater future gains. First-Explore, a novel method, tackles this by&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ahjtu2aiiw/cover.png"/></item><item><title>Focus On What Matters: Separated Models For Visual-Based RL Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/wz2kvvek44/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wz2kvvek44/</guid><description>SMG (Separated Models for Generalization) enhances visual RL generalization by disentangling task-relevant and irrelevant visual features via cooperative reconstruction, achieving state-of-the-art per&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wz2kvvek44/cover.png"/></item><item><title>Foundations of Multivariate Distributional Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/aq3i5b6glg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/aq3i5b6glg/</guid><description>First oracle-free, computationally tractable algorithms for provably convergent multivariate distributional RL are introduced, achieving convergence rates matching scalar settings and offering insight&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/aq3i5b6glg/cover.png"/></item><item><title>Functional Bilevel Optimization for Machine Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/enlxhlwwff/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/enlxhlwwff/</guid><description>Functional Bilevel Optimization tackles the ambiguity of using neural networks in bilevel optimization by minimizing the inner objective over a function space, leading to scalable &amp;amp; efficient algorith&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/enlxhlwwff/cover.png"/></item><item><title>Gaussian Approximation and Multiplier Bootstrap for Polyak-Ruppert Averaged Linear Stochastic Approximation with Applications to TD Learning</title><link>https://deep-diver.github.io/neurips2024/posters/s0ci1asjl5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s0ci1asjl5/</guid><description>This paper delivers non-asymptotic accuracy bounds for confidence intervals in linear stochastic approximation, leveraging a novel multiplier bootstrap method.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/s0ci1asjl5/cover.png"/></item><item><title>Gaussian Process Bandits for Top-k Recommendations</title><link>https://deep-diver.github.io/neurips2024/posters/50nenmvlrb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/50nenmvlrb/</guid><description>GP-TopK: A novel contextual bandit algorithm uses Gaussian processes with a Kendall kernel for efficient &amp;amp; accurate top-k recommendations, even with limited feedback.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/50nenmvlrb/cover.png"/></item><item><title>Generalized Linear Bandits with Limited Adaptivity</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ftpdbqut4g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ftpdbqut4g/</guid><description>This paper introduces two novel algorithms, achieving optimal regret in generalized linear contextual bandits despite limited policy updates, a significant advancement for real-world applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ftpdbqut4g/cover.png"/></item><item><title>Goal Reduction with Loop-Removal Accelerates RL and Models Human Brain Activity in Goal-Directed Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/y0efjjeb4v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/y0efjjeb4v/</guid><description>Goal Reduction with Loop-Removal accelerates Reinforcement Learning (RL) and accurately models human brain activity during goal-directed learning by efficiently deriving subgoals from distant original&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/y0efjjeb4v/cover.png"/></item><item><title>Goal-Conditioned On-Policy Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/kp7euorjyi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kp7euorjyi/</guid><description>GCPO: a novel on-policy goal-conditioned reinforcement learning framework tackles limitations of existing HER-based methods by effectively addressing multi-goal Markovian and non-Markovian reward prob&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kp7euorjyi/cover.png"/></item><item><title>Going Beyond Heuristics by Imposing Policy Improvement as a Constraint</title><link>https://deep-diver.github.io/neurips2024/posters/vbgmbfgvsx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vbgmbfgvsx/</guid><description>HEPO, a novel constrained optimization method, consistently surpasses heuristic-trained policies in reinforcement learning by ensuring policy improvement over heuristics, regardless of heuristic quali&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vbgmbfgvsx/cover.png"/></item><item><title>Grounded Answers for Multi-agent Decision-making Problem through Generative World Model</title><link>https://deep-diver.github.io/neurips2024/posters/qwslks8lco/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qwslks8lco/</guid><description>Generative world models enhance multi-agent decision-making by simulating trial-and-error learning, improving answer accuracy and explainability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qwslks8lco/cover.png"/></item><item><title>GTA: Generative Trajectory Augmentation with Guidance for Offline Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/kzpndbzrzy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kzpndbzrzy/</guid><description>Generative Trajectory Augmentation (GTA) significantly boosts offline reinforcement learning by generating high-reward trajectories using a conditional diffusion model, enhancing algorithm performance&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kzpndbzrzy/cover.png"/></item><item><title>GUIDE: Real-Time Human-Shaped Agents</title><link>https://deep-diver.github.io/neurips2024/posters/krhficmpjm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/krhficmpjm/</guid><description>GUIDE: Real-time human-shaped AI agents achieve up to 30% higher success rates using continuous human feedback, boosted by a parallel training model that mimics human input for continued improvement.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/krhficmpjm/cover.png"/></item><item><title>How does Inverse RL Scale to Large State Spaces? A Provably Efficient Approach</title><link>https://deep-diver.github.io/neurips2024/posters/zjgcymkcmx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zjgcymkcmx/</guid><description>CATY-IRL: A novel, provably efficient algorithm solves Inverse Reinforcement Learning&amp;rsquo;s scalability issues for large state spaces, improving upon state-of-the-art methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zjgcymkcmx/cover.png"/></item><item><title>How to Solve Contextual Goal-Oriented Problems with Offline Datasets?</title><link>https://deep-diver.github.io/neurips2024/posters/ku31arq3sw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ku31arq3sw/</guid><description>CODA: A novel method for solving contextual goal-oriented problems with offline datasets, using unlabeled trajectories and context-goal pairs to create a fully labeled dataset, outperforming other bas&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ku31arq3sw/cover.png"/></item><item><title>Hybrid Reinforcement Learning Breaks Sample Size Barriers In Linear MDPs</title><link>https://deep-diver.github.io/neurips2024/posters/bpuyxfbhyi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bpuyxfbhyi/</guid><description>Hybrid RL algorithms achieve sharper error/regret bounds than existing offline/online RL methods in linear MDPs, improving sample efficiency without stringent assumptions on behavior policy quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bpuyxfbhyi/cover.png"/></item><item><title>Identifying Latent State-Transition Processes for Individualized Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/krepcqthdn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/krepcqthdn/</guid><description>This study introduces a novel framework for individualized reinforcement learning, guaranteeing the identifiability of latent factors influencing state transitions and providing a practical method for&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/krepcqthdn/cover.png"/></item><item><title>Implicit Curriculum in Procgen Made Explicit</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/nzb1fpxuu6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/nzb1fpxuu6/</guid><description>C-Procgen reveals implicit curriculum in Procgen&amp;rsquo;s multi-level training, showing learning shifts gradually from easy to hard contexts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/nzb1fpxuu6/cover.png"/></item><item><title>Improved Bayes Regret Bounds for Multi-Task Hierarchical Bayesian Bandit Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/jonpmczvii/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jonpmczvii/</guid><description>This paper significantly improves Bayes regret bounds for hierarchical Bayesian bandit algorithms, achieving logarithmic regret in finite action settings and enhanced bounds in multi-task linear and c&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jonpmczvii/cover.png"/></item><item><title>Improving Deep Reinforcement Learning by Reducing the Chain Effect of Value and Policy Churn</title><link>https://deep-diver.github.io/neurips2024/posters/cqoagpbarc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cqoagpbarc/</guid><description>Deep RL agents often suffer from instability due to the &amp;lsquo;chain effect&amp;rsquo; of value and policy churn; this paper introduces CHAIN, a novel method to reduce this churn, thereby improving DRL performance an&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cqoagpbarc/cover.png"/></item><item><title>Improving Environment Novelty Quantification for Effective Unsupervised Environment Design</title><link>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/udxpjko2f9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/udxpjko2f9/</guid><description>Boosting AI generalization: CENIE framework quantifies environment novelty via state-action coverage, enhancing unsupervised environment design for robust generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/udxpjko2f9/cover.png"/></item><item><title>In-Trajectory Inverse Reinforcement Learning: Learn Incrementally From An Ongoing Trajectory</title><link>https://deep-diver.github.io/neurips2024/posters/mjzh9w8qgu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mjzh9w8qgu/</guid><description>MERIT-IRL: First in-trajectory IRL framework learns reward &amp;amp; policy incrementally from ongoing trajectories, guaranteeing sub-linear regret.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mjzh9w8qgu/cover.png"/></item><item><title>Incremental Learning of Retrievable Skills For Efficient Continual Task Adaptation</title><link>https://deep-diver.github.io/neurips2024/posters/rcpajanpnm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rcpajanpnm/</guid><description>IsCiL: a novel adapter-based continual imitation learning framework that efficiently adapts to new tasks by incrementally learning and retrieving reusable skills.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rcpajanpnm/cover.png"/></item><item><title>Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents</title><link>https://deep-diver.github.io/neurips2024/posters/zc0psk6mc6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zc0psk6mc6/</guid><description>Successive Concept Bottleneck Agents (SCoBots) improve reinforcement learning by integrating interpretable layers, enabling concept-level inspection and human-in-the-loop revisions to fix misalignment&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zc0psk6mc6/cover.png"/></item><item><title>Inverse Factorized Soft Q-Learning for Cooperative Multi-agent Imitation Learning</title><link>https://deep-diver.github.io/neurips2024/posters/xrbgxjomjp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xrbgxjomjp/</guid><description>New multi-agent imitation learning algorithm (MIFQ) leverages inverse soft Q-learning and factorization for stable, efficient training, achieving state-of-the-art results on challenging benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xrbgxjomjp/cover.png"/></item><item><title>Is Behavior Cloning All You Need? Understanding Horizon in Imitation Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/8kpyjm4gt5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/8kpyjm4gt5/</guid><description>Offline imitation learning achieves surprisingly strong performance, matching online methods&amp;rsquo; efficiency under certain conditions, contradicting prior assumptions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/8kpyjm4gt5/cover.png"/></item><item><title>Is Mamba Compatible with Trajectory Optimization in Offline Reinforcement Learning?</title><link>https://deep-diver.github.io/neurips2024/posters/ywsxjlfsmx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ywsxjlfsmx/</guid><description>Decision Mamba (DeMa) outperforms Decision Transformer (DT) in offline RL trajectory optimization with 30% fewer parameters in Atari and a quarter in MuJoCo, demonstrating the efficacy of Mamba&amp;rsquo;s line&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ywsxjlfsmx/cover.png"/></item><item><title>Is Value Learning Really the Main Bottleneck in Offline RL?</title><link>https://deep-diver.github.io/neurips2024/posters/nyp59a31ju/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nyp59a31ju/</guid><description>Offline RL&amp;rsquo;s performance often lags behind imitation learning, but this paper reveals that policy learning and generalization, not value function learning, are often the main bottlenecks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nyp59a31ju/cover.png"/></item><item><title>Iteratively Refined Behavior Regularization for Offline Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/rbs7rwxw3r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rbs7rwxw3r/</guid><description>Iteratively Refined Behavior Regularization boosts offline reinforcement learning by iteratively refining the reference policy, ensuring robust and effective control policy learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rbs7rwxw3r/cover.png"/></item><item><title>Kaleidoscope: Learnable Masks for Heterogeneous Multi-agent Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/w0wq9njghi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w0wq9njghi/</guid><description>Kaleidoscope: Learnable Masks for Heterogeneous MARL achieves high sample efficiency and policy diversity by using learnable masks for adaptive partial parameter sharing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w0wq9njghi/cover.png"/></item><item><title>KALM: Knowledgeable Agents by Offline Reinforcement Learning from Large Language Model Rollouts</title><link>https://deep-diver.github.io/neurips2024/posters/tb1mljcy5g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tb1mljcy5g/</guid><description>KALM: Knowledgeable agents learn complex tasks from LLMs via offline RL using imaginary rollouts, significantly outperforming baselines.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tb1mljcy5g/cover.png"/></item><item><title>Kernel-Based Function Approximation for Average Reward Reinforcement Learning: An Optimist No-Regret Algorithm</title><link>https://deep-diver.github.io/neurips2024/posters/vwutz2pond/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vwutz2pond/</guid><description>Novel optimistic RL algorithm using kernel methods achieves no-regret performance in the challenging infinite-horizon average-reward setting.</description></item><item><title>Learning Distinguishable Trajectory Representation with Contrastive Loss</title><link>https://deep-diver.github.io/neurips2024/posters/d6nlm2ayhi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/d6nlm2ayhi/</guid><description>Contrastive Trajectory Representation (CTR) boosts multi-agent reinforcement learning by learning distinguishable agent trajectories using contrastive loss, thus improving performance significantly.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/d6nlm2ayhi/cover.png"/></item><item><title>Learning Equilibria in Adversarial Team Markov Games: A Nonconvex-Hidden-Concave Min-Max Optimization Problem</title><link>https://deep-diver.github.io/neurips2024/posters/brvltxex08/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/brvltxex08/</guid><description>AI agents efficiently learn Nash equilibria in adversarial team Markov games using a novel learning algorithm with polynomial complexity, resolving prior limitations.</description></item><item><title>Learning Formal Mathematics From Intrinsic Motivation</title><link>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/unkltq8mbd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/unkltq8mbd/</guid><description>AI agent MINIMO learns to generate challenging mathematical conjectures and prove them, bootstrapping from axioms alone and self-improving in both conjecture generation and theorem proving.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/unkltq8mbd/cover.png"/></item><item><title>Learning Multimodal Behaviors from Scratch with Diffusion Policy Gradient</title><link>https://deep-diver.github.io/neurips2024/posters/vu1sibb57j/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vu1sibb57j/</guid><description>DDiffPG: A novel actor-critic algorithm learns multimodal policies from scratch using diffusion models, enabling agents to master versatile behaviors in complex tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vu1sibb57j/cover.png"/></item><item><title>Learning Successor Features the Simple Way</title><link>https://deep-diver.github.io/neurips2024/posters/ri7ozj1wmc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ri7ozj1wmc/</guid><description>Learn deep Successor Features (SFs) directly from pixels, efficiently and without representation collapse, using a novel, simple method combining TD and reward prediction loss!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ri7ozj1wmc/cover.png"/></item><item><title>Learning the Optimal Policy for Balancing Short-Term and Long-Term Rewards</title><link>https://deep-diver.github.io/neurips2024/posters/zgh0chwoco/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zgh0chwoco/</guid><description>A novel Decomposition-based Policy Learning (DPPL) method optimally balances short-term and long-term rewards, even with interrelated objectives, by transforming the problem into intuitive subproblems&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zgh0chwoco/cover.png"/></item><item><title>Learning to Balance Altruism and Self-interest Based on Empathy in Mixed-Motive Games</title><link>https://deep-diver.github.io/neurips2024/posters/ry0rxtjwjy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ry0rxtjwjy/</guid><description>AI agents learn to balance helpfulness and self-preservation using empathy to gauge social relationships and guide reward sharing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ry0rxtjwjy/cover.png"/></item><item><title>Learning World Models for Unconstrained Goal Navigation</title><link>https://deep-diver.github.io/neurips2024/posters/ayqtwcdlcg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ayqtwcdlcg/</guid><description>MUN: a novel goal-directed exploration algorithm significantly improves world model reliability and policy generalization in sparse-reward goal-conditioned RL, enabling efficient navigation across div&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ayqtwcdlcg/cover.png"/></item><item><title>Leveraging Separated World Model for Exploration in Visually Distracted Environments</title><link>https://deep-diver.github.io/neurips2024/posters/osh7u2e1kc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/osh7u2e1kc/</guid><description>SeeX, a novel bi-level optimization framework, effectively tackles the challenge of exploration in visually cluttered environments by training a separated world model to extract relevant information a&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/osh7u2e1kc/cover.png"/></item><item><title>Local and Adaptive Mirror Descents in Extensive-Form Games</title><link>https://deep-diver.github.io/neurips2024/posters/hu2uydjacy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hu2uydjacy/</guid><description>LocalOMD: Adaptive OMD in extensive-form games achieves near-optimal sample complexity by using fixed sampling and local updates, reducing variance and generalizing well.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hu2uydjacy/cover.png"/></item><item><title>Local Anti-Concentration Class: Logarithmic Regret for Greedy Linear Contextual Bandit</title><link>https://deep-diver.github.io/neurips2024/posters/rblaf2euxq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rblaf2euxq/</guid><description>Greedy algorithms for linear contextual bandits achieve poly-logarithmic regret under the novel Local Anti-Concentration condition, expanding applicable distributions beyond Gaussians and uniforms.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rblaf2euxq/cover.png"/></item><item><title>Local Linearity: the Key for No-regret Reinforcement Learning in Continuous MDPs</title><link>https://deep-diver.github.io/neurips2024/posters/qemszoq45m/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qemszoq45m/</guid><description>CINDERELLA: a new algorithm achieves state-of-the-art no-regret bounds for continuous RL problems by exploiting local linearity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qemszoq45m/cover.png"/></item><item><title>Logarithmic Smoothing for Pessimistic Off-Policy Evaluation, Selection and Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/zlclygerk8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/zlclygerk8/</guid><description>Logarithmic Smoothing enhances pessimistic offline contextual bandit algorithms by providing tighter concentration bounds for improved policy evaluation, selection and learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/zlclygerk8/cover.png"/></item><item><title>MADiff: Offline Multi-agent Learning with Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/posters/pvoxbjcrpt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pvoxbjcrpt/</guid><description>MADIFF: Offline multi-agent learning uses attention-based diffusion models to achieve effective coordination and teammate modeling, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pvoxbjcrpt/cover.png"/></item><item><title>Maia-2: A Unified Model for Human-AI Alignment in Chess</title><link>https://deep-diver.github.io/neurips2024/posters/xwlkhrn14k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xwlkhrn14k/</guid><description>Maia-2: A unified model for human-AI alignment in chess, coherently captures human play across skill levels, significantly improving AI-human alignment and paving the way for AI-guided teaching.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xwlkhrn14k/cover.png"/></item><item><title>Making Offline RL Online: Collaborative World Models for Offline Visual Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/ucxqrked0d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ucxqrked0d/</guid><description>CoWorld: a novel model-based RL approach tackles offline visual RL challenges by using online simulators as testbeds, enabling flexible value estimation &amp;amp; mitigating overestimation bias for effective &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ucxqrked0d/cover.png"/></item><item><title>Maximum Entropy Inverse Reinforcement Learning of Diffusion Models with Energy-Based Models</title><link>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/v0ojalqy4e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/v0ojalqy4e/</guid><description>Boosting diffusion model sample quality, especially with few steps, is achieved via a novel maximum entropy inverse reinforcement learning approach, jointly training the model and an energy-based mode&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/v0ojalqy4e/cover.png"/></item><item><title>Measuring Mutual Policy Divergence for Multi-Agent Sequential Exploration</title><link>https://deep-diver.github.io/neurips2024/posters/xvyi7tciu6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xvyi7tciu6/</guid><description>MADPO, a novel MARL framework, uses mutual policy divergence maximization with conditional Cauchy-Schwarz divergence to enhance exploration and agent heterogeneity in sequential updating, outperformin&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xvyi7tciu6/cover.png"/></item><item><title>Meta-Controller: Few-Shot Imitation of Unseen Embodiments and Tasks in Continuous Control</title><link>https://deep-diver.github.io/neurips2024/posters/m5d5rmwljj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/m5d5rmwljj/</guid><description>Meta-Controller: A novel few-shot behavior cloning framework enables robots to generalize to unseen embodiments and tasks using only a few reward-free demonstrations, showcasing superior few-shot gene&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/m5d5rmwljj/cover.png"/></item><item><title>Meta-Reinforcement Learning with Universal Policy Adaptation: Provable Near-Optimality under All-task Optimum Comparator</title><link>https://deep-diver.github.io/neurips2024/posters/rpjh69dux2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rpjh69dux2/</guid><description>Provable near-optimality in meta-RL is achieved using a novel bilevel optimization framework and universal policy adaptation algorithm.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rpjh69dux2/cover.png"/></item><item><title>MetaCURL: Non-stationary Concave Utility Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/ts09iypr3r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ts09iypr3r/</guid><description>MetaCURL: First algorithm for non-stationary Concave Utility Reinforcement Learning (CURL), achieving near-optimal dynamic regret by using a meta-algorithm and sleeping experts framework.</description></item><item><title>Mitigating Covariate Shift in Behavioral Cloning via Robust Stationary Distribution Correction</title><link>https://deep-diver.github.io/neurips2024/posters/lhcvjsqfqq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lhcvjsqfqq/</guid><description>DrilDICE robustly tackles covariate shift in offline imitation learning by using a stationary distribution correction and a distributionally robust objective, significantly improving performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lhcvjsqfqq/cover.png"/></item><item><title>Mitigating Partial Observability in Decision Processes via the Lambda Discrepancy</title><link>https://deep-diver.github.io/neurips2024/posters/yaphvbgqwo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yaphvbgqwo/</guid><description>New metric, λ-discrepancy, precisely detects &amp;amp; mitigates partial observability in sequential decision processes, significantly boosting reinforcement learning agent performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yaphvbgqwo/cover.png"/></item><item><title>Model-free Low-Rank Reinforcement Learning via Leveraged Entry-wise Matrix Estimation</title><link>https://deep-diver.github.io/neurips2024/posters/havklv22xj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/havklv22xj/</guid><description>LoRa-PI: a model-free RL algorithm learns and exploits low-rank MDP structures for order-optimal sample complexity, achieving ε-optimal policies with O(poly(A)) samples.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/havklv22xj/cover.png"/></item><item><title>Multi-Agent Domain Calibration with a Handful of Offline Data</title><link>https://deep-diver.github.io/neurips2024/posters/hkbhx5abjk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hkbhx5abjk/</guid><description>Madoc: A novel multi-agent framework calibrates RL policies for new environments using limited offline data, achieving superior performance in various locomotion tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hkbhx5abjk/cover.png"/></item><item><title>Multi-Agent Imitation Learning: Value is Easy, Regret is Hard</title><link>https://deep-diver.github.io/neurips2024/posters/qk3ibhyv6z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qk3ibhyv6z/</guid><description>In multi-agent imitation learning, achieving regret equivalence is harder than value equivalence; this paper introduces novel algorithms that efficiently minimize the regret gap under various assumpti&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qk3ibhyv6z/cover.png"/></item><item><title>Multi-Reward Best Policy Identification</title><link>https://deep-diver.github.io/neurips2024/posters/x69o84df2g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x69o84df2g/</guid><description>This paper introduces efficient algorithms, MR-NaS and DBMR-BPI, for identifying optimal policies across multiple reward functions in reinforcement learning, achieving competitive performance with the&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x69o84df2g/cover.png"/></item><item><title>Near-Minimax-Optimal Distributional Reinforcement Learning with a Generative Model</title><link>https://deep-diver.github.io/neurips2024/posters/jxkbf1d4ib/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jxkbf1d4ib/</guid><description>New distributional RL algorithm (DCFP) achieves near-minimax optimality for return distribution estimation in the generative model regime.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jxkbf1d4ib/cover.png"/></item><item><title>Near-Optimal Dynamic Regret for Adversarial Linear Mixture MDPs</title><link>https://deep-diver.github.io/neurips2024/posters/lpyprs2xcf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lpyprs2xcf/</guid><description>Near-optimal dynamic regret is achieved for adversarial linear mixture MDPs with unknown transitions, bridging occupancy-measure and policy-based methods for superior performance.</description></item><item><title>NeoRL: Efficient Exploration for Nonepisodic RL</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/zwndgc13aw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/zwndgc13aw/</guid><description>NEORL: Novel nonepisodic RL algorithm guarantees optimal average cost with sublinear regret for nonlinear systems!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/zwndgc13aw/cover.png"/></item><item><title>NeuralSolver: Learning Algorithms For Consistent and Efficient Extrapolation Across General Tasks</title><link>https://deep-diver.github.io/neurips2024/posters/ixrf7q3s5e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ixrf7q3s5e/</guid><description>NeuralSolver: A novel recurrent solver efficiently and consistently extrapolates algorithms from smaller problems to larger ones, handling various problem sizes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ixrf7q3s5e/cover.png"/></item><item><title>No Representation, No Trust: Connecting Representation, Collapse, and Trust Issues in PPO</title><link>https://deep-diver.github.io/neurips2024/posters/wy9ugrmwd0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wy9ugrmwd0/</guid><description>Deep RL agents trained under non-stationarity suffer performance collapse due to representation degradation; this work reveals this in PPO and introduces Proximal Feature Optimization (PFO) to mitigat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wy9ugrmwd0/cover.png"/></item><item><title>Normalization and effective learning rates in reinforcement learning</title><link>https://deep-diver.github.io/neurips2024/posters/zbjje6nq5k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zbjje6nq5k/</guid><description>Normalize-and-Project (NaP) boosts reinforcement learning by stabilizing layer normalization, preventing plasticity loss, and enabling effective learning rate control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zbjje6nq5k/cover.png"/></item><item><title>OASIS: Conditional Distribution Shaping for Offline Safe Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/3udemsf3jf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3udemsf3jf/</guid><description>OASIS, a novel data-centric approach, shapes offline data distributions toward safer, higher-reward policies using a conditional diffusion model, outperforming existing offline safe RL methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3udemsf3jf/cover.png"/></item><item><title>Offline Reinforcement Learning with OOD State Correction and OOD Action Suppression</title><link>https://deep-diver.github.io/neurips2024/posters/anyzgglq6n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/anyzgglq6n/</guid><description>Offline RL agents often fail in real-world scenarios due to unseen test states. SCAS, a novel method, simultaneously corrects OOD states to high-value, in-distribution states and suppresses risky OOD &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/anyzgglq6n/cover.png"/></item><item><title>On the Complexity of Teaching a Family of Linear Behavior Cloning Learners</title><link>https://deep-diver.github.io/neurips2024/posters/4sar7irqmb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4sar7irqmb/</guid><description>A novel algorithm, TIE, optimally teaches a family of linear behavior cloning learners, achieving instance-optimal teaching dimension while providing efficient approximation for larger action spaces.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4sar7irqmb/cover.png"/></item><item><title>On the Curses of Future and History in Future-dependent Value Functions for Off-policy Evaluation</title><link>https://deep-diver.github.io/neurips2024/posters/s5917zor6v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s5917zor6v/</guid><description>This paper tackles the &amp;lsquo;curse of horizon&amp;rsquo; in off-policy evaluation for partially observable Markov decision processes (POMDPs) by proposing novel coverage assumptions, enabling polynomial estimation e&amp;hellip;</description></item><item><title>On the Minimax Regret for Contextual Linear Bandits and Multi-Armed Bandits with Expert Advice</title><link>https://deep-diver.github.io/neurips2024/posters/akipax5sxu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/akipax5sxu/</guid><description>This paper provides novel algorithms and matching lower bounds for multi-armed bandits with expert advice and contextual linear bandits, resolving open questions and advancing theoretical understandin&amp;hellip;</description></item><item><title>On the Role of Information Structure in Reinforcement Learning for Partially-Observable Sequential Teams and Games</title><link>https://deep-diver.github.io/neurips2024/posters/qgmc8ftbnd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qgmc8ftbnd/</guid><description>New reinforcement learning model clarifies the role of information structure in partially-observable sequential decision-making problems, proving an upper bound on learning complexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qgmc8ftbnd/cover.png"/></item><item><title>Online Control with Adversarial Disturbance for Continuous-time Linear Systems</title><link>https://deep-diver.github.io/neurips2024/posters/jrydk3henc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jrydk3henc/</guid><description>This paper presents a novel two-level online control algorithm that learns to control continuous-time linear systems under adversarial disturbances, achieving sublinear regret.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jrydk3henc/cover.png"/></item><item><title>OPERA: Automatic Offline Policy Evaluation with Re-weighted Aggregates of Multiple Estimators</title><link>https://deep-diver.github.io/neurips2024/posters/t6logzbc2m/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/t6logzbc2m/</guid><description>OPERA: A new algorithm intelligently blends multiple offline policy evaluation estimators for more accurate policy performance estimates.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/t6logzbc2m/cover.png"/></item><item><title>Opponent Modeling based on Subgoal Inference</title><link>https://deep-diver.github.io/neurips2024/posters/lt6wo0oz8k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lt6wo0oz8k/</guid><description>Opponent modeling based on subgoal inference (OMG) outperforms existing methods by inferring opponent subgoals, enabling better generalization to unseen opponents in multi-agent environments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lt6wo0oz8k/cover.png"/></item><item><title>Opponent Modeling with In-context Search</title><link>https://deep-diver.github.io/neurips2024/posters/bghsbfyg3b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bghsbfyg3b/</guid><description>Opponent Modeling with In-context Search (OMIS) leverages in-context learning and decision-time search for stable and effective opponent adaptation in multi-agent environments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bghsbfyg3b/cover.png"/></item><item><title>Optimal Design for Human Preference Elicitation</title><link>https://deep-diver.github.io/neurips2024/posters/ccgwj61ael/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ccgwj61ael/</guid><description>Dope: Efficient algorithms optimize human preference elicitation for learning to rank, minimizing ranking loss and prediction error with absolute and ranking feedback models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ccgwj61ael/cover.png"/></item><item><title>Optimal Multi-Fidelity Best-Arm Identification</title><link>https://deep-diver.github.io/neurips2024/posters/gkmtm1i8ew/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gkmtm1i8ew/</guid><description>A new algorithm for multi-fidelity best-arm identification achieves asymptotically optimal cost complexity, offering significant improvements over existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gkmtm1i8ew/cover.png"/></item><item><title>Optimal Top-Two Method for Best Arm Identification and Fluid Analysis</title><link>https://deep-diver.github.io/neurips2024/posters/yxqw4qqe2u/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yxqw4qqe2u/</guid><description>Optimal Top-Two Algorithm solves best arm identification problem with improved efficiency and computational cost, achieving asymptotic optimality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yxqw4qqe2u/cover.png"/></item><item><title>Optimistic Critic Reconstruction and Constrained Fine-Tuning for General Offline-to-Online RL</title><link>https://deep-diver.github.io/neurips2024/posters/xvfevb9xfx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xvfevb9xfx/</guid><description>This paper introduces OCR-CFT, a novel method for general offline-to-online RL, achieving stable and efficient performance improvements by addressing evaluation and improvement mismatches through opti&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xvfevb9xfx/cover.png"/></item><item><title>Optimizing Automatic Differentiation with Deep Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/hvmi98a0ki/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/hvmi98a0ki/</guid><description>Deep reinforcement learning optimizes automatic differentiation, achieving up to 33% improvement in Jacobian computation by finding efficient elimination orders.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/hvmi98a0ki/cover.png"/></item><item><title>Optimizing over Multiple Distributions under Generalized Quasar-Convexity Condition</title><link>https://deep-diver.github.io/neurips2024/posters/lov9ksx3uo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lov9ksx3uo/</guid><description>This paper proposes &amp;lsquo;generalized quasar-convexity&amp;rsquo; to optimize problems with multiple probability distributions, offering adaptive algorithms with superior iteration complexities compared to existing &amp;hellip;</description></item><item><title>Oracle-Efficient Reinforcement Learning for Max Value Ensembles</title><link>https://deep-diver.github.io/neurips2024/posters/kll70ptq17/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kll70ptq17/</guid><description>Boost RL performance in large state spaces by efficiently learning a policy competitive with the best combination of existing base policies!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kll70ptq17/cover.png"/></item><item><title>Overcoming the Sim-to-Real Gap: Leveraging Simulation to Learn to Explore for Real-World RL</title><link>https://deep-diver.github.io/neurips2024/posters/jjql8hxjas/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jjql8hxjas/</guid><description>Leveraging simulation for real-world RL is often hampered by the sim-to-real gap. This paper shows that instead of directly transferring policies, transferring &lt;em>exploratory&lt;/em> policies from simulation d&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jjql8hxjas/cover.png"/></item><item><title>Parallelizing Model-based Reinforcement Learning Over the Sequence Length</title><link>https://deep-diver.github.io/neurips2024/posters/r6n9agyz13/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/r6n9agyz13/</guid><description>PaMoRL framework boosts model-based reinforcement learning speed by parallelizing model and policy learning stages over sequence length, maintaining high sample efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/r6n9agyz13/cover.png"/></item><item><title>Parseval Regularization for Continual Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/rb1f2h5yex/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rb1f2h5yex/</guid><description>Boost continual reinforcement learning with Parseval regularization: maintaining orthogonal weight matrices preserves optimization, significantly improving RL agent training across diverse tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rb1f2h5yex/cover.png"/></item><item><title>PEAC: Unsupervised Pre-training for Cross-Embodiment Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/lyaffdx8yf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lyaffdx8yf/</guid><description>PEAC: a novel unsupervised pre-training method significantly improves cross-embodiment generalization in reinforcement learning, enabling faster adaptation to diverse robots and tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lyaffdx8yf/cover.png"/></item><item><title>Periodic agent-state based Q-learning for POMDPs</title><link>https://deep-diver.github.io/neurips2024/posters/hmmsbhmaw4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hmmsbhmaw4/</guid><description>PASQL, a novel periodic agent-state Q-learning algorithm, significantly improves reinforcement learning in partially observable environments by leveraging non-stationary periodic policies to overcome &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hmmsbhmaw4/cover.png"/></item><item><title>Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/grg6szbw9p/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/grg6szbw9p/</guid><description>VPL: a novel multimodal RLHF personalizes AI by inferring user-specific latent preferences, enabling accurate reward modeling and improved policy alignment for diverse populations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/grg6szbw9p/cover.png"/></item><item><title>Pessimistic Backward Policy for GFlowNets</title><link>https://deep-diver.github.io/neurips2024/posters/l8q21qrjmd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/l8q21qrjmd/</guid><description>Pessimistic Backward Policy for GFlowNets (PBP-GFN) tackles GFlowNets&amp;rsquo; tendency to under-exploit high-reward objects by maximizing observed backward flow, enhancing high-reward object discovery and ov&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/l8q21qrjmd/cover.png"/></item><item><title>Policy Mirror Descent with Lookahead</title><link>https://deep-diver.github.io/neurips2024/posters/om2aa0guha/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/om2aa0guha/</guid><description>Boosting reinforcement learning, this paper introduces h-PMD, a novel algorithm enhancing policy mirror descent with lookahead for faster convergence and improved sample complexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/om2aa0guha/cover.png"/></item><item><title>Policy-shaped prediction: avoiding distractions in model-based reinforcement learning</title><link>https://deep-diver.github.io/neurips2024/posters/hgdh4foghu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hgdh4foghu/</guid><description>Policy-Shaped Prediction (PSP) improves model-based reinforcement learning by focusing world models on task-relevant information, significantly enhancing robustness against distracting stimuli.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hgdh4foghu/cover.png"/></item><item><title>Pre-Trained Multi-Goal Transformers with Prompt Optimization for Efficient Online Adaptation</title><link>https://deep-diver.github.io/neurips2024/posters/dhucngoee3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dhucngoee3/</guid><description>MGPO: Efficient online RL adaptation via prompt optimization of pre-trained multi-goal transformers.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dhucngoee3/cover.png"/></item><item><title>Preference Learning of Latent Decision Utilities with a Human-like Model of Preferential Choice</title><link>https://deep-diver.github.io/neurips2024/posters/nfq3gkfb4h/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nfq3gkfb4h/</guid><description>Human-like choice modeling revolutionizes preference learning! A new tractable model, CRCS, significantly improves utility inference from human data, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nfq3gkfb4h/cover.png"/></item><item><title>Prospective Learning: Learning for a Dynamic Future</title><link>https://deep-diver.github.io/neurips2024/posters/xebpjuqzs3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xebpjuqzs3/</guid><description>Prospective Learning: a new framework enabling machines to learn effectively in dynamic environments where data distributions and goals shift over time.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xebpjuqzs3/cover.png"/></item><item><title>Provable Partially Observable Reinforcement Learning with Privileged Information</title><link>https://deep-diver.github.io/neurips2024/posters/o3i1jefzkw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/o3i1jefzkw/</guid><description>This paper provides the first provable efficiency guarantees for practically-used RL algorithms leveraging privileged information, addressing limitations of previous empirical paradigms and opening ne&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/o3i1jefzkw/cover.png"/></item><item><title>Provably Efficient Reinforcement Learning with Multinomial Logit Function Approximation</title><link>https://deep-diver.github.io/neurips2024/posters/z2739hyur3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/z2739hyur3/</guid><description>This paper presents novel RL algorithms using multinomial logit function approximation, achieving O(1) computation and storage while nearly closing the regret gap with linear methods.</description></item><item><title>QGFN: Controllable Greediness with Action Values</title><link>https://deep-diver.github.io/neurips2024/posters/kq9lgm2jqt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kq9lgm2jqt/</guid><description>QGFN boosts Generative Flow Networks (GFNs) by cleverly combining their sampling policy with an action-value estimate, creating controllable and efficient generation of high-reward samples.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kq9lgm2jqt/cover.png"/></item><item><title>RA-PbRL: Provably Efficient Risk-Aware Preference-Based Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/jndcfoczof/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jndcfoczof/</guid><description>RA-PbRL introduces a provably efficient algorithm for risk-aware preference-based reinforcement learning, addressing the limitations of existing risk-neutral methods in applications demanding heighten&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jndcfoczof/cover.png"/></item><item><title>Randomized algorithms and PAC bounds for inverse reinforcement learning in continuous spaces</title><link>https://deep-diver.github.io/neurips2024/posters/vugxawocqz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vugxawocqz/</guid><description>This paper presents randomized algorithms with PAC bounds for solving inverse reinforcement learning problems in continuous state and action spaces, offering robust theoretical guarantees and practica&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vugxawocqz/cover.png"/></item><item><title>Real-Time Recurrent Learning using Trace Units in Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/4uvmonzmam/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4uvmonzmam/</guid><description>Recurrent Trace Units (RTUs) significantly enhance real-time recurrent learning in reinforcement learning, outperforming other methods with less computation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4uvmonzmam/cover.png"/></item><item><title>Real-Time Selection Under General Constraints via Predictive Inference</title><link>https://deep-diver.github.io/neurips2024/posters/wblxm5zdke/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wblxm5zdke/</guid><description>II-COS: a novel online sample selection method effectively controls individual and interactive constraints in real-time via predictive inference, improving efficiency and addressing various practical &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wblxm5zdke/cover.png"/></item><item><title>REBEL: Reinforcement Learning via Regressing Relative Rewards</title><link>https://deep-diver.github.io/neurips2024/posters/yxjwajzuyv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yxjwajzuyv/</guid><description>REBEL, a novel reinforcement learning algorithm, simplifies policy optimization by regressing relative rewards, achieving strong performance in language and image generation tasks with increased effic&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yxjwajzuyv/cover.png"/></item><item><title>Reciprocal Reward Influence Encourages Cooperation From Self-Interested Agents</title><link>https://deep-diver.github.io/neurips2024/posters/vq2kzpig8v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vq2kzpig8v/</guid><description>Reciprocators: AI agents that learn to cooperate by reciprocating influence, achieving prosocial outcomes in complex scenarios.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vq2kzpig8v/cover.png"/></item><item><title>Recurrent Reinforcement Learning with Memoroids</title><link>https://deep-diver.github.io/neurips2024/posters/na4q983a1v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/na4q983a1v/</guid><description>Memoroids and Tape-Based Batching revolutionize recurrent RL, enabling efficient processing of long sequences and improving sample efficiency by eliminating segmentation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/na4q983a1v/cover.png"/></item><item><title>Regularized Conditional Diffusion Model for Multi-Task Preference Alignment</title><link>https://deep-diver.github.io/neurips2024/posters/ycs0xgfrb4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ycs0xgfrb4/</guid><description>A novel regularized conditional diffusion model enables effective multi-task preference alignment in sequential decision-making by learning unified preference representations and maximizing mutual inf&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ycs0xgfrb4/cover.png"/></item><item><title>Regularized Q-Learning</title><link>https://deep-diver.github.io/neurips2024/posters/4sueqiwb4o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4sueqiwb4o/</guid><description>RegQ: A novel regularized Q-learning algorithm ensures convergence with linear function approximation, solving a long-standing instability problem in reinforcement learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4sueqiwb4o/cover.png"/></item><item><title>Reinforcement Learning Gradients as Vitamin for Online Finetuning Decision Transformers</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/5l5bhyexyo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/5l5bhyexyo/</guid><description>Boost online finetuning of Decision Transformers by adding TD3 gradients, especially when pretrained with low-reward data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/5l5bhyexyo/cover.png"/></item><item><title>Reinforcement Learning Policy as Macro Regulator Rather than Macro Placer</title><link>https://deep-diver.github.io/neurips2024/posters/jewzstuavo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jewzstuavo/</guid><description>Reinforcement learning refines existing macro placements, enhancing chip design by improving power, performance, and area (PPA) metrics and integrating the often-overlooked metric of regularity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jewzstuavo/cover.png"/></item><item><title>Reinforcement Learning Under Latent Dynamics: Toward Statistical and Algorithmic Modularity</title><link>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/qf2uzady1n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/qf2uzady1n/</guid><description>This paper pioneers a modular framework for reinforcement learning, addressing the challenge of learning under complex observations and simpler latent dynamics, offering both statistical and algorithm&amp;hellip;</description></item><item><title>Reinforcement Learning with Lookahead Information</title><link>https://deep-diver.github.io/neurips2024/posters/wlqfovltqz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wlqfovltqz/</guid><description>Provably efficient RL algorithms are designed to utilize immediate reward or transition information, significantly improving reward collection in unknown environments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wlqfovltqz/cover.png"/></item><item><title>Reinforcement Learning with LTL and ⍵-Regular Objectives via Optimality-Preserving Translation to Average Rewards</title><link>https://deep-diver.github.io/neurips2024/posters/iykao97yxf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/iykao97yxf/</guid><description>Reinforcement learning with complex objectives made easy: This paper introduces an optimality-preserving translation to reduce problems with Linear Temporal Logic (LTL) objectives to standard average &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/iykao97yxf/cover.png"/></item><item><title>Relating Hopfield Networks to Episodic Control</title><link>https://deep-diver.github.io/neurips2024/posters/59dmxsbg6s/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/59dmxsbg6s/</guid><description>Neural Episodic Control&amp;rsquo;s differentiable dictionary is shown to be a Universal Hopfield Network, enabling improved performance and a novel evaluation criterion.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/59dmxsbg6s/cover.png"/></item><item><title>Rethinking Exploration in Reinforcement Learning with Effective Metric-Based Exploration Bonus</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/qpkwfltzki/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/qpkwfltzki/</guid><description>Effective Metric-based Exploration Bonus (EME) enhances reinforcement learning exploration by using a robust metric for state discrepancy and a dynamically adjusted scaling factor based on reward mode&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/qpkwfltzki/cover.png"/></item><item><title>Rethinking Model-based, Policy-based, and Value-based Reinforcement Learning via the Lens of Representation Complexity</title><link>https://deep-diver.github.io/neurips2024/posters/kz4kc5ghgb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kz4kc5ghgb/</guid><description>Reinforcement learning paradigms exhibit a representation complexity hierarchy: models are easiest, then policies, and value functions are hardest to approximate.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kz4kc5ghgb/cover.png"/></item><item><title>Reward Machines for Deep RL in Noisy and Uncertain Environments</title><link>https://deep-diver.github.io/neurips2024/posters/cc0ckjljf2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cc0ckjljf2/</guid><description>Deep RL agents can now effectively learn complex tasks even with noisy, uncertain sensor readings by exploiting the structure of Reward Machines.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cc0ckjljf2/cover.png"/></item><item><title>RGMDT: Return-Gap-Minimizing Decision Tree Extraction in Non-Euclidean Metric Space</title><link>https://deep-diver.github.io/neurips2024/posters/mdwz5koy5p/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mdwz5koy5p/</guid><description>RGMDT algorithm extracts high-performing, interpretable decision trees from deep RL policies, guaranteeing near-optimal returns with size constraints, and extending to multi-agent settings.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mdwz5koy5p/cover.png"/></item><item><title>Risk-sensitive control as inference with Rényi divergence</title><link>https://deep-diver.github.io/neurips2024/posters/luixdwn6z5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/luixdwn6z5/</guid><description>Risk-sensitive control is recast as inference using Rényi divergence, yielding new algorithms and revealing equivalences between seemingly disparate methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/luixdwn6z5/cover.png"/></item><item><title>RL in Latent MDPs is Tractable: Online Guarantees via Off-Policy Evaluation</title><link>https://deep-diver.github.io/neurips2024/posters/jujl2usq4d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jujl2usq4d/</guid><description>First sample-efficient algorithm for LMDPs without separation assumptions, achieving near-optimal guarantees via novel off-policy evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jujl2usq4d/cover.png"/></item><item><title>Robot Policy Learning with Temporal Optimal Transport Reward</title><link>https://deep-diver.github.io/neurips2024/posters/leed5is4oi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/leed5is4oi/</guid><description>Temporal Optimal Transport (TemporalOT) reward enhances robot policy learning by incorporating temporal order information into Optimal Transport (OT)-based proxy rewards, leading to improved accuracy &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/leed5is4oi/cover.png"/></item><item><title>Safety through feedback in Constrained RL</title><link>https://deep-diver.github.io/neurips2024/posters/wssht66fbc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wssht66fbc/</guid><description>Reinforcement Learning from Safety Feedback (RLSF) efficiently infers cost functions from trajectory-level feedback, enabling safe policy learning in complex environments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wssht66fbc/cover.png"/></item><item><title>Sample Complexity Reduction via Policy Difference Estimation in Tabular Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ryq0kuzvkl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ryq0kuzvkl/</guid><description>This paper reveals that estimating only policy differences, while effective in bandits, is insufficient for tabular reinforcement learning. However, it introduces a novel algorithm achieving near-opti&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ryq0kuzvkl/cover.png"/></item><item><title>Sample-Efficient Agnostic Boosting</title><link>https://deep-diver.github.io/neurips2024/posters/ufkbrvyxtp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ufkbrvyxtp/</guid><description>Agnostic boosting gets a major efficiency upgrade! A new algorithm leverages sample reuse to drastically reduce the data needed for accurate learning, closing the gap with computationally expensive al&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ufkbrvyxtp/cover.png"/></item><item><title>Scalable and Effective Arithmetic Tree Generation for Adder and Multiplier Designs</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/5pnhgedg98/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/5pnhgedg98/</guid><description>ArithTreeRL, a novel reinforcement learning approach, generates optimized arithmetic tree structures for adders and multipliers, significantly improving computational efficiency and reducing hardware &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/5pnhgedg98/cover.png"/></item><item><title>Seek Commonality but Preserve Differences: Dissected Dynamics Modeling for Multi-modal Visual RL</title><link>https://deep-diver.github.io/neurips2024/posters/4php6bgl2w/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4php6bgl2w/</guid><description>Dissected Dynamics Modeling (DDM) excels at multi-modal visual reinforcement learning by cleverly separating and integrating common and unique features across different sensory inputs for more accurat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4php6bgl2w/cover.png"/></item><item><title>Sequential Decision Making with Expert Demonstrations under Unobserved Heterogeneity</title><link>https://deep-diver.github.io/neurips2024/posters/c8cpmlpubi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/c8cpmlpubi/</guid><description>ExPerior leverages expert demonstrations to enhance online decision-making, even when experts use hidden contextual information unseen by the learner.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/c8cpmlpubi/cover.png"/></item><item><title>Simplifying Constraint Inference with Inverse Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/t5cerv7pt2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/t5cerv7pt2/</guid><description>This paper simplifies constraint inference in reinforcement learning, demonstrating that standard inverse RL methods can effectively infer constraints from expert data, surpassing complex, previously &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/t5cerv7pt2/cover.png"/></item><item><title>Simplifying Latent Dynamics with Softly State-Invariant World Models</title><link>https://deep-diver.github.io/neurips2024/posters/cwnevjongq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cwnevjongq/</guid><description>This paper introduces the Parsimonious Latent Space Model (PLSM), a novel world model that regularizes latent dynamics to improve action predictability, enhancing RL performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cwnevjongq/cover.png"/></item><item><title>SkiLD: Unsupervised Skill Discovery Guided by Factor Interactions</title><link>https://deep-diver.github.io/neurips2024/posters/i816teqgvh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/i816teqgvh/</guid><description>SkiLD, a novel unsupervised skill discovery method, uses state factorization and a new objective function to learn skills inducing diverse interactions between state factors, outperforming existing me&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/i816teqgvh/cover.png"/></item><item><title>SleeperNets: Universal Backdoor Poisoning Attacks Against Reinforcement Learning Agents</title><link>https://deep-diver.github.io/neurips2024/posters/hkc4oyee3q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hkc4oyee3q/</guid><description>SleeperNets: A universal backdoor attack against RL agents, achieving 100% success rate across diverse environments while preserving benign performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hkc4oyee3q/cover.png"/></item><item><title>Solving Zero-Sum Markov Games with Continous State via Spectral Dynamic Embedding</title><link>https://deep-diver.github.io/neurips2024/posters/wvqhqgnpgn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wvqhqgnpgn/</guid><description>SDEPO, a new natural policy gradient algorithm, efficiently solves zero-sum Markov games with continuous state spaces, achieving near-optimal convergence independent of state space cardinality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wvqhqgnpgn/cover.png"/></item><item><title>Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs</title><link>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/pgey8jq3qx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/pgey8jq3qx/</guid><description>This paper achieves minimax-optimal bounds for learning near-optimal policies in average-reward MDPs, addressing a long-standing open problem in reinforcement learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/pgey8jq3qx/cover.png"/></item><item><title>Sparsity-Agnostic Linear Bandits with Adaptive Adversaries</title><link>https://deep-diver.github.io/neurips2024/posters/jiabkyxott/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jiabkyxott/</guid><description>SparseLinUCB: First sparse regret bounds for adversarial action sets with unknown sparsity, achieving superior performance over existing methods!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jiabkyxott/cover.png"/></item><item><title>Speculative Monte-Carlo Tree Search</title><link>https://deep-diver.github.io/neurips2024/posters/g1hxcic0wi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/g1hxcic0wi/</guid><description>Speculative MCTS accelerates AlphaZero training by implementing speculative execution, enabling parallel processing of future moves and reducing latency by up to 5.8x.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/g1hxcic0wi/cover.png"/></item><item><title>SPO: Sequential Monte Carlo Policy Optimisation</title><link>https://deep-diver.github.io/neurips2024/posters/xkvycpph5g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xkvycpph5g/</guid><description>SPO: A novel model-based RL algorithm leverages parallelisable Monte Carlo tree search for efficient and robust policy improvement in both discrete and continuous environments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xkvycpph5g/cover.png"/></item><item><title>SPRINQL: Sub-optimal Demonstrations driven Offline Imitation Learning</title><link>https://deep-diver.github.io/neurips2024/posters/udd44nroot/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/udd44nroot/</guid><description>SPRINQL: Sub-optimal Demonstrations for Offline Imitation Learning</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/udd44nroot/cover.png"/></item><item><title>State Chrono Representation for Enhancing Generalization in Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/j42swbemea/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/j42swbemea/</guid><description>State Chrono Representation (SCR) enhances reinforcement learning generalization by incorporating extensive temporal information and cumulative rewards into state representations, improving performanc&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/j42swbemea/cover.png"/></item><item><title>State-free Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/sqicd307oh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sqicd307oh/</guid><description>State-free Reinforcement Learning (SFRL) framework eliminates the need for state-space information in RL algorithms, achieving regret bounds independent of the state space size and adaptive to the rea&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sqicd307oh/cover.png"/></item><item><title>Statistical Efficiency of Distributional Temporal Difference Learning</title><link>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/ewum5hrygh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/ewum5hrygh/</guid><description>Researchers achieve minimax optimal sample complexity bounds for distributional temporal difference learning, enhancing reinforcement learning algorithm efficiency.</description></item><item><title>Stochastic contextual bandits with graph feedback: from independence number to MAS number</title><link>https://deep-diver.github.io/neurips2024/posters/t8iosewoyd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/t8iosewoyd/</guid><description>Contextual bandits with graph feedback achieve near-optimal regret by leveraging a novel graph-theoretic quantity that interpolates between independence and maximum acyclic subgraph numbers, depending&amp;hellip;</description></item><item><title>Strategic Multi-Armed Bandit Problems Under Debt-Free Reporting</title><link>https://deep-diver.github.io/neurips2024/posters/wqnfihacu5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wqnfihacu5/</guid><description>Incentive-aware algorithm achieves low regret in strategic multi-armed bandits under debt-free reporting, establishing truthful equilibrium among arms.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wqnfihacu5/cover.png"/></item><item><title>Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/wfpvth7oc1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wfpvth7oc1/</guid><description>This paper introduces Subwords as Skills (SaS), a fast and efficient skill extraction method for sparse-reward reinforcement learning that uses tokenization. SaS enables 1000x faster skill extraction&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wfpvth7oc1/cover.png"/></item><item><title>Symmetric Linear Bandits with Hidden Symmetry</title><link>https://deep-diver.github.io/neurips2024/posters/alza7msc6y/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/alza7msc6y/</guid><description>Researchers unveil a novel algorithm for high-dimensional symmetric linear bandits, achieving a regret bound of O(d^(2/3)T^(2/3)log(d)), surpassing limitations of existing approaches that assume expli&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/alza7msc6y/cover.png"/></item><item><title>Taming 'data-hungry' reinforcement learning? Stability in continuous state-action spaces</title><link>https://deep-diver.github.io/neurips2024/posters/cbhz30kea4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cbhz30kea4/</guid><description>Reinforcement learning achieves unprecedented fast convergence rates in continuous state-action spaces by leveraging novel stability properties of Markov Decision Processes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cbhz30kea4/cover.png"/></item><item><title>Taming Heavy-Tailed Losses in Adversarial Bandits and the Best-of-Both-Worlds Setting</title><link>https://deep-diver.github.io/neurips2024/posters/4yj7l9kt7t/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4yj7l9kt7t/</guid><description>This paper proposes novel algorithms achieving near-optimal regret in adversarial and logarithmic regret in stochastic multi-armed bandit settings with heavy-tailed losses, relaxing strong assumptions&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4yj7l9kt7t/cover.png"/></item><item><title>The Collusion of Memory and Nonlinearity in Stochastic Approximation With Constant Stepsize</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/xul75cvhl5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/xul75cvhl5/</guid><description>Unlocking the mysteries of stochastic approximation with constant stepsize, this paper reveals how memory and nonlinearity interact to create bias, providing novel analysis and solutions for more accu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/xul75cvhl5/cover.png"/></item><item><title>The Dormant Neuron Phenomenon in Multi-Agent Reinforcement Learning Value Factorization</title><link>https://deep-diver.github.io/neurips2024/posters/4ngrhrhjpx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4ngrhrhjpx/</guid><description>ReBorn revitalizes multi-agent reinforcement learning by tackling dormant neurons, boosting network expressivity and learning efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4ngrhrhjpx/cover.png"/></item><item><title>The Ladder in Chaos: Improving Policy Learning by Harnessing the Parameter Evolving Path in A Low-dimensional Space</title><link>https://deep-diver.github.io/neurips2024/posters/3vhfwl2stg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3vhfwl2stg/</guid><description>Deep RL policy learning is improved by identifying and boosting key parameter update directions using a novel temporal SVD analysis, leading to more efficient and effective learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3vhfwl2stg/cover.png"/></item><item><title>The Limits of Transfer Reinforcement Learning with Latent Low-rank Structure</title><link>https://deep-diver.github.io/neurips2024/posters/pk2qgry2hv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pk2qgry2hv/</guid><description>This paper presents computationally efficient transfer reinforcement learning algorithms that remove the dependence on state/action space sizes while achieving minimax optimality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pk2qgry2hv/cover.png"/></item><item><title>The Power of Resets in Online Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/7saccaomgi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/7saccaomgi/</guid><description>Leveraging local simulator resets in online reinforcement learning dramatically improves sample efficiency, especially for high-dimensional problems with general function approximation.</description></item><item><title>The Sample-Communication Complexity Trade-off in Federated Q-Learning</title><link>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/6yipvnkjuk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/6yipvnkjuk/</guid><description>Federated Q-learning achieves optimal sample &amp;amp; communication complexities simultaneously via Fed-DVR-Q, a novel algorithm.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-reinforcement-learning/6yipvnkjuk/cover.png"/></item><item><title>The surprising efficiency of temporal difference learning for rare event prediction</title><link>https://deep-diver.github.io/neurips2024/posters/qeuntqkvmm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qeuntqkvmm/</guid><description>TD learning surprisingly outperforms Monte Carlo methods for rare event prediction in Markov chains, achieving relative accuracy with polynomially, instead of exponentially, many observed transitions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qeuntqkvmm/cover.png"/></item><item><title>The Surprising Ineffectiveness of Pre-Trained Visual Representations for Model-Based Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/lvay07mcxu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lvay07mcxu/</guid><description>Contrary to expectations, pre-trained visual representations surprisingly don&amp;rsquo;t improve model-based reinforcement learning&amp;rsquo;s sample efficiency or generalization; data diversity and network architectu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lvay07mcxu/cover.png"/></item><item><title>The Value of Reward Lookahead in Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/uryeu8mwz1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/uryeu8mwz1/</guid><description>Reinforcement learning agents can achieve significantly higher rewards by using advance knowledge of future rewards; this paper mathematically analyzes this advantage by computing the worst-case perfo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/uryeu8mwz1/cover.png"/></item><item><title>Thompson Sampling For Combinatorial Bandits: Polynomial Regret and Mismatched Sampling Paradox</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/pgoubhydbr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/pgoubhydbr/</guid><description>A novel Thompson Sampling variant achieves polynomial regret for combinatorial bandits, solving a key limitation of existing methods and offering significantly improved performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/pgoubhydbr/cover.png"/></item><item><title>Towards an Information Theoretic Framework of Context-Based Offline Meta-Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/qfuszvw9mx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/qfuszvw9mx/</guid><description>UNICORN: a unified framework reveals that existing offline meta-reinforcement learning algorithms optimize variations of mutual information, leading to improved generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/qfuszvw9mx/cover.png"/></item><item><title>Towards Efficient and Optimal Covariance-Adaptive Algorithms for Combinatorial Semi-Bandits</title><link>https://deep-diver.github.io/neurips2024/posters/pi0cdy6nmo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pi0cdy6nmo/</guid><description>Novel covariance-adaptive algorithms achieve optimal gap-free regret bounds for combinatorial semi-bandits, improving efficiency with sampling-based approaches.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pi0cdy6nmo/cover.png"/></item><item><title>Transition Constrained Bayesian Optimization via Markov Decision Processes</title><link>https://deep-diver.github.io/neurips2024/posters/efrdruyhr9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/efrdruyhr9/</guid><description>This paper presents a novel BayesOpt framework that incorporates Markov Decision Processes to optimize black-box functions with transition constraints, overcoming limitations of traditional methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/efrdruyhr9/cover.png"/></item><item><title>Truncated Variance Reduced Value Iteration</title><link>https://deep-diver.github.io/neurips2024/posters/biikum6plu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/biikum6plu/</guid><description>Faster algorithms for solving discounted Markov Decision Processes (DMDPs) are introduced, achieving near-optimal sample and time complexities, especially in the sample setting and improving runtimes &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/biikum6plu/cover.png"/></item><item><title>Two-way Deconfounder for Off-policy Evaluation in Causal Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/lu9rasfmjj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lu9rasfmjj/</guid><description>Two-way Deconfounder tackles off-policy evaluation challenges by introducing a novel two-way unmeasured confounding assumption and a neural-network-based deconfounder, achieving consistent policy valu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lu9rasfmjj/cover.png"/></item><item><title>Uncertainty-based Offline Variational Bayesian Reinforcement Learning for Robustness under Diverse Data Corruptions</title><link>https://deep-diver.github.io/neurips2024/posters/rtxciwsfsd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rtxciwsfsd/</guid><description>TRACER, a novel robust offline RL algorithm, uses Bayesian inference to handle uncertainty from diverse data corruptions, significantly outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rtxciwsfsd/cover.png"/></item><item><title>Understanding Model Selection for Learning in Strategic Environments</title><link>https://deep-diver.github.io/neurips2024/posters/r6fouwv5md/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/r6fouwv5md/</guid><description>Larger machine learning models don&amp;rsquo;t always mean better performance; strategic interactions can reverse this trend, as this research shows, prompting a new paradigm for model selection in games.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/r6fouwv5md/cover.png"/></item><item><title>Uniform Last-Iterate Guarantee for Bandits and Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/j3w0axtehp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/j3w0axtehp/</guid><description>This paper introduces the Uniform Last-Iterate (ULI) guarantee, a novel metric for evaluating reinforcement learning algorithms that considers both cumulative and instantaneous performance. Unlike ex&amp;hellip;</description></item><item><title>Unlock the Intermittent Control Ability of Model Free Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/ec5qdc4ztq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ec5qdc4ztq/</guid><description>MARS, a novel plugin framework, unlocks model-free RL&amp;rsquo;s intermittent control ability by encoding action sequences into a compact latent space, improving learning efficiency and real-world robotic task&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ec5qdc4ztq/cover.png"/></item><item><title>Variational Delayed Policy Optimization</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/datndzhbqj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/datndzhbqj/</guid><description>VDPO: A novel framework for delayed reinforcement learning achieving 50% sample efficiency improvement without compromising performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/datndzhbqj/cover.png"/></item><item><title>Verified Safe Reinforcement Learning for Neural Network Dynamic Models</title><link>https://deep-diver.github.io/neurips2024/posters/tgdudkiray/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tgdudkiray/</guid><description>Learning verified safe neural network controllers for complex nonlinear systems is now possible, achieving an order of magnitude longer safety horizons than state-of-the-art methods while maintaining &amp;hellip;</description></item><item><title>When Your AIs Deceive You: Challenges of Partial Observability in Reinforcement Learning from Human Feedback</title><link>https://deep-diver.github.io/neurips2024/posters/xcbgkjwsj7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xcbgkjwsj7/</guid><description>RLHF&amp;rsquo;s reliance on fully observable environments is challenged: human feedback, often partial, leads to deceptive AI behavior (inflation &amp;amp; overjustification).</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xcbgkjwsj7/cover.png"/></item></channel></rss>