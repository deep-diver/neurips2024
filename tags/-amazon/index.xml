<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ Amazon on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-amazon/</link><description>Recent content in üè¢ Amazon on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2025 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-amazon/index.xml" rel="self" type="application/rss+xml"/><item><title>Causal vs. Anticausal merging of predictors</title><link>https://deep-diver.github.io/neurips2024/posters/xzkxgvlb0c/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xzkxgvlb0c/</guid><description>Causal assumptions drastically alter predictor merging, with CMAXENT revealing logistic regression for causal and LDA for anticausal directions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xzkxgvlb0c/cover.png"/></item><item><title>Pre-training Differentially Private Models with Limited Public Data</title><link>https://deep-diver.github.io/neurips2024/posters/gqrk0wgnic/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gqrk0wgnic/</guid><description>Researchers achieved high-accuracy differentially private (DP) models by using a novel DP continual pre-training strategy with only 10% public data, mitigating the performance degradation common in DP&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gqrk0wgnic/cover.png"/></item><item><title>Reconstruction Attacks on Machine Unlearning: Simple Models are Vulnerable</title><link>https://deep-diver.github.io/neurips2024/posters/i4gqcm1r3z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/i4gqcm1r3z/</guid><description>Deleting data from machine learning models exposes individuals to highly accurate reconstruction attacks, even when models are simple; this research demonstrates the vulnerability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/i4gqcm1r3z/cover.png"/></item><item><title>Risk-Averse Fine-tuning of Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/1bzkqzphsw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1bzkqzphsw/</guid><description>Risk-Averse RLHF fine-tunes LLMs to minimize toxic outputs while maintaining performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1bzkqzphsw/cover.png"/></item><item><title>Sample-Efficient Agnostic Boosting</title><link>https://deep-diver.github.io/neurips2024/posters/ufkbrvyxtp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ufkbrvyxtp/</guid><description>Agnostic boosting gets a major efficiency upgrade! A new algorithm leverages sample reuse to drastically reduce the data needed for accurate learning, closing the gap with computationally expensive al&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ufkbrvyxtp/cover.png"/></item></channel></rss>