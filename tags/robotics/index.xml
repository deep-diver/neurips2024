<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Robotics on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/robotics/</link><description>Recent content in Robotics on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/robotics/index.xml" rel="self" type="application/rss+xml"/><item><title>Accurate and Steady Inertial Pose Estimation through Sequence Structure Learning and Modulation</title><link>https://deep-diver.github.io/neurips2024/posters/j2hzctqbf0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/j2hzctqbf0/</guid><description>Researchers enhanced transformer models for inertial pose estimation by introducing a Sequence Structure Module, leveraging inherent fixed-length sequence structures for improved accuracy and steadine&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/j2hzctqbf0/cover.png"/></item><item><title>Active Perception for Grasp Detection via Neural Graspness Field</title><link>https://deep-diver.github.io/neurips2024/posters/6fyh6gxzpf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/6fyh6gxzpf/</guid><description>ActiveNGF achieves superior robotic grasping by using a Neural Graspness Field to model scene grasp distribution online, enabling efficient camera movement and improved grasp pose detection.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/6fyh6gxzpf/cover.png"/></item><item><title>AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies</title><link>https://deep-diver.github.io/neurips2024/posters/ugxkinqdcc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ugxkinqdcc/</guid><description>AdaFlow: a novel imitation learning framework boasts fast inference and diverse action generation via variance-adaptive flow-based policies, significantly outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ugxkinqdcc/cover.png"/></item><item><title>BAKU: An Efficient Transformer for Multi-Task Policy Learning</title><link>https://deep-diver.github.io/neurips2024/posters/ufxgsiykkx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ufxgsiykkx/</guid><description>BAKU: A simple transformer enables efficient multi-task robot policy learning, achieving 91% success on real-world tasks with limited data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ufxgsiykkx/cover.png"/></item><item><title>Belief-State Query Policies for User-Aligned Planning under Partial Observability</title><link>https://deep-diver.github.io/neurips2024/posters/i2oacrdf5l/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/i2oacrdf5l/</guid><description>This paper introduces Belief-State Query (BSQ) constraints for user-aligned planning in partially observable settings, providing algorithms with guaranteed user alignment and computational feasibility&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/i2oacrdf5l/cover.png"/></item><item><title>BricksRL: A Platform for Democratizing Robotics and Reinforcement Learning Research and Education with LEGO</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/8iytzcnxiu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/8iytzcnxiu/</guid><description>BricksRL: A low-cost, open-source platform democratizes robotics and reinforcement learning research using LEGO, enabling accessible real-world experiments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/8iytzcnxiu/cover.png"/></item><item><title>Closed-Loop Visuomotor Control with Generative Expectation for Robotic Manipulation</title><link>https://deep-diver.github.io/neurips2024/posters/1ptdkwzbmg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1ptdkwzbmg/</guid><description>CLOVER: A closed-loop visuomotor framework using generative visual plans &amp;amp; feedback mechanisms achieves state-of-the-art results in long-horizon robotic manipulation tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1ptdkwzbmg/cover.png"/></item><item><title>Constrained Synthesis with Projected Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/posters/fsdb3i9y24/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fsdb3i9y24/</guid><description>Projected Diffusion Models (PDM) revolutionizes generative modeling by directly incorporating constraints into the sampling process, ensuring high-fidelity outputs that strictly adhere to predefined c&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fsdb3i9y24/cover.png"/></item><item><title>CooHOI: Learning Cooperative Human-Object Interaction with Manipulated Object Dynamics</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/iyzytmd3jd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/iyzytmd3jd/</guid><description>CooHOI: A two-phase learning framework enables physically simulated characters to perform cooperative object transportation tasks naturally and efficiently, overcoming the limitations of existing meth&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/iyzytmd3jd/cover.png"/></item><item><title>Designs for Enabling Collaboration in Human-Machine Teaming via Interactive and Explainable Systems</title><link>https://deep-diver.github.io/neurips2024/posters/xrk4jk2jbr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xrk4jk2jbr/</guid><description>Boosting Human-AI teamwork via interactive, explainable AI!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xrk4jk2jbr/cover.png"/></item><item><title>DG-SLAM: Robust Dynamic Gaussian Splatting SLAM with Hybrid Pose Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/tgozvltdy3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tgozvltdy3/</guid><description>DG-SLAM achieves robust real-time visual SLAM in dynamic scenes using 3D Gaussian splatting and a novel hybrid pose optimization, significantly outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tgozvltdy3/cover.png"/></item><item><title>Differentiable Quantum Computing for Large-scale Linear Control</title><link>https://deep-diver.github.io/neurips2024/posters/ghqw3xlavd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ghqw3xlavd/</guid><description>Quantum algorithm achieves super-quadratic speedup for large-scale linear control, offering a novel approach to address the computational challenges of optimizing complex dynamical systems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ghqw3xlavd/cover.png"/></item><item><title>DiffPhyCon: A Generative Approach to Control Complex Physical Systems</title><link>https://deep-diver.github.io/neurips2024/posters/mbzuh8l0xg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mbzuh8l0xg/</guid><description>DiffPhyCon uses diffusion models to generate near-optimal control sequences for complex physical systems, outperforming existing methods by simultaneously optimizing a generative energy function and c&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mbzuh8l0xg/cover.png"/></item><item><title>DiffuserLite: Towards Real-time Diffusion Planning</title><link>https://deep-diver.github.io/neurips2024/posters/2txdhuqyrq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2txdhuqyrq/</guid><description>DiffuserLite: a super-fast diffusion planning framework achieving real-time performance (122Hz).</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2txdhuqyrq/cover.png"/></item><item><title>Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion</title><link>https://deep-diver.github.io/neurips2024/posters/ydo1ynarjj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ydo1ynarjj/</guid><description>Diffusion Forcing merges next-token prediction and full-sequence diffusion for superior sequence generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ydo1ynarjj/cover.png"/></item><item><title>Diffusion Policy Attacker: Crafting Adversarial Attacks for Diffusion-based Policies</title><link>https://deep-diver.github.io/neurips2024/posters/1l5vaniok5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1l5vaniok5/</guid><description>DP-Attacker unveils diffusion-based policy vulnerabilities by crafting effective adversarial attacks, significantly impacting robot safety and paving the way for more robust AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1l5vaniok5/cover.png"/></item><item><title>Disentangling Linear Quadratic Control with Untrusted ML Predictions</title><link>https://deep-diver.github.io/neurips2024/posters/wxqukapoa7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wxqukapoa7/</guid><description>DISC, a novel control policy, disentangles untrusted ML predictions to achieve near-optimal performance when accurate, while guaranteeing competitive ratio bounds even with significant prediction erro&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wxqukapoa7/cover.png"/></item><item><title>DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control</title><link>https://deep-diver.github.io/neurips2024/posters/vurouc6nr3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vurouc6nr3/</guid><description>DynaMo: a novel self-supervised method significantly boosts visuo-motor control by learning in-domain dynamics from limited expert demonstrations, improving policy performance across various environme&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vurouc6nr3/cover.png"/></item><item><title>EGODE: An Event-attended Graph ODE Framework for Modeling Rigid Dynamics</title><link>https://deep-diver.github.io/neurips2024/posters/js5vztyoiq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/js5vztyoiq/</guid><description>EGODE, a novel framework, leverages coupled graph ODEs and an event module to accurately model continuous and instantaneous changes in rigid body dynamics, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/js5vztyoiq/cover.png"/></item><item><title>FactorSim: Generative Simulation via Factorized Representation</title><link>https://deep-diver.github.io/neurips2024/posters/wbzvyh3pra/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wbzvyh3pra/</guid><description>FACTORSim generates full, coded simulations from natural language descriptions, outperforming existing methods in accuracy and zero-shot transfer learning by using a factored POMDP representation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wbzvyh3pra/cover.png"/></item><item><title>GarmentLab: A Unified Simulation and Benchmark for Garment Manipulation</title><link>https://deep-diver.github.io/neurips2024/posters/bircf8i1kp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bircf8i1kp/</guid><description>GarmentLab: A new benchmark and simulation platform tackles garment manipulation challenges by offering realistic simulations, diverse assets, and tasks bridging the sim-to-real gap for more robust AI&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bircf8i1kp/cover.png"/></item><item><title>Graph Learning for Numeric Planning</title><link>https://deep-diver.github.io/neurips2024/posters/wxc6kvqglq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wxc6kvqglq/</guid><description>GOOSE: a novel framework using graph learning for efficient and interpretable numeric planning, outperforming existing methods in many benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wxc6kvqglq/cover.png"/></item><item><title>Grasp as You Say: Language-guided Dexterous Grasp Generation</title><link>https://deep-diver.github.io/neurips2024/posters/qewibatmnn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qewibatmnn/</guid><description>Robots can now dexterously grasp objects based on natural language commands thanks to DexGYS, a new language-guided dexterous grasp generation framework and dataset.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qewibatmnn/cover.png"/></item><item><title>Humanoid Locomotion as Next Token Prediction</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/grmczqgtla/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/grmczqgtla/</guid><description>Humanoid robots now walk in San Francisco zero-shot, thanks to a novel &amp;rsquo;next token prediction&amp;rsquo; approach trained on diverse sensorimotor data, enabling real-world generalization and data efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/grmczqgtla/cover.png"/></item><item><title>Input-to-State Stable Coupled Oscillator Networks for Closed-form Model-based Control in Latent Space</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/kxkrlsr4aj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/kxkrlsr4aj/</guid><description>Stable closed-loop control in latent space is achieved using a novel Coupled Oscillator Network, offering efficient model-based control for complex nonlinear systems directly from image data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/kxkrlsr4aj/cover.png"/></item><item><title>Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity</title><link>https://deep-diver.github.io/neurips2024/posters/4tlue0ufiz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4tlue0ufiz/</guid><description>Robots using LLMs for task planning often make unsafe or wrong decisions due to LLM hallucination and ambiguity in instructions. This paper introduces &amp;lsquo;introspective planning,&amp;rsquo; a novel method that us&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4tlue0ufiz/cover.png"/></item><item><title>iVideoGPT: Interactive VideoGPTs are Scalable World Models</title><link>https://deep-diver.github.io/neurips2024/posters/4tenzbftzr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4tenzbftzr/</guid><description>iVideoGPT: A scalable, interactive world model trained on millions of human &amp;amp; robot manipulation videos, enabling efficient video prediction and model-based reinforcement learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4tenzbftzr/cover.png"/></item><item><title>Learning an Actionable Discrete Diffusion Policy via Large-Scale Actionless Video Pre-Training</title><link>https://deep-diver.github.io/neurips2024/posters/q7s8mfwqsx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/q7s8mfwqsx/</guid><description>Actionable AI agents are trained efficiently via a novel framework, VPDD, which uses discrete diffusion to pre-train on massive human videos, and fine-tunes on limited robot data for superior multi-ta&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/q7s8mfwqsx/cover.png"/></item><item><title>Learning rigid-body simulators over implicit shapes for large-scale scenes and vision</title><link>https://deep-diver.github.io/neurips2024/oral-ai-applications/qdyts5dygq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-ai-applications/qdyts5dygq/</guid><description>SDF-Sim: A novel learned rigid-body simulator that leverages SDFs to achieve unprecedented scalability, enabling simulations with hundreds of objects and millions of nodes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-ai-applications/qdyts5dygq/cover.png"/></item><item><title>LLM-based Skill Diffusion for Zero-shot Policy Adaptation</title><link>https://deep-diver.github.io/neurips2024/posters/ugldvc0gtu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ugldvc0gtu/</guid><description>LLM-based Skill Diffusion (LDuS) enables zero-shot robotic policy adaptation to various contexts specified in natural language by generating controllable skill trajectories via loss-guided diffusion a&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ugldvc0gtu/cover.png"/></item><item><title>Long-Horizon Planning for Multi-Agent Robots in Partially Observable Environments</title><link>https://deep-diver.github.io/neurips2024/posters/y1rows2z4i/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y1rows2z4i/</guid><description>LLaMAR: LM-based planner for multi-agent robots excels in long-horizon, partially observable tasks, achieving 30% higher success than existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y1rows2z4i/cover.png"/></item><item><title>Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion</title><link>https://deep-diver.github.io/neurips2024/posters/phitmesafz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/phitmesafz/</guid><description>Make-An-Agent generates high-performing robotic control policies from single behavioral demonstrations using behavior-prompted diffusion, showcasing impressive generalization and real-world applicabil&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/phitmesafz/cover.png"/></item><item><title>MeMo: Meaningful, Modular Controllers via Noise Injection</title><link>https://deep-diver.github.io/neurips2024/posters/5djbbacqim/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/5djbbacqim/</guid><description>MeMo: a novel framework for pretraining meaningful, modular robot controllers via noise injection, enabling efficient transfer learning across different robot morphologies and tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/5djbbacqim/cover.png"/></item><item><title>MGF: Mixed Gaussian Flow for Diverse Trajectory Prediction</title><link>https://deep-diver.github.io/neurips2024/posters/muyhndlxwc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/muyhndlxwc/</guid><description>MGF: Mixed Gaussian Flow enhances trajectory prediction by using a mixed Gaussian prior, achieving state-of-the-art diversity and alignment accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/muyhndlxwc/cover.png"/></item><item><title>Model-based Diffusion for Trajectory Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/bjndysco6o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bjndysco6o/</guid><description>Model-Based Diffusion (MBD) uses diffusion processes and model information for data-free trajectory optimization, outperforming existing methods on complex tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bjndysco6o/cover.png"/></item><item><title>NeuralFluid: Nueral Fluidic System Design and Control with Differentiable Simulation</title><link>https://deep-diver.github.io/neurips2024/posters/llsomvjbbm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/llsomvjbbm/</guid><description>NeuralFluid: Design &amp;amp; control complex fluidic systems with dynamic boundaries using differentiable simulation, achieving superior results in benchmark tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/llsomvjbbm/cover.png"/></item><item><title>NeuralSteiner: Learning Steiner Tree for Overflow-avoiding Global Routing in Chip Design</title><link>https://deep-diver.github.io/neurips2024/posters/oekfpsowpp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oekfpsowpp/</guid><description>NeuralSteiner uses deep learning to predict Steiner points for efficient, overflow-avoiding global routing in chip design, achieving up to a 99.8% overflow reduction on large benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oekfpsowpp/cover.png"/></item><item><title>Omnigrasp: Simulated Humanoid Grasping on Diverse Objects</title><link>https://deep-diver.github.io/neurips2024/posters/glt37xou7e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/glt37xou7e/</guid><description>Omnigrasp: A novel RL-based method enables simulated humanoids to grasp diverse objects and precisely follow complex trajectories, advancing realistic human-object interaction in virtual environments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/glt37xou7e/cover.png"/></item><item><title>Predicting Future Actions of Reinforcement Learning Agents</title><link>https://deep-diver.github.io/neurips2024/posters/qgags7peye/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qgags7peye/</guid><description>Predicting RL agent behavior is key for safety and interaction; this study reveals that explicitly planned agents are significantly easier to predict due to their internal plans.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qgags7peye/cover.png"/></item><item><title>Prediction with Action: Visual Policy Learning via Joint Denoising Process</title><link>https://deep-diver.github.io/neurips2024/posters/tevxvdy8r2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tevxvdy8r2/</guid><description>PAD, a novel visual policy learning framework, unifies image prediction and robot action in a joint denoising process, achieving significant performance improvements in robotic manipulation tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tevxvdy8r2/cover.png"/></item><item><title>QueST: Self-Supervised Skill Abstractions for Learning Continuous Control</title><link>https://deep-diver.github.io/neurips2024/posters/p3v3x7hnv0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/p3v3x7hnv0/</guid><description>QueST: A novel self-supervised skill abstraction architecture for continuous robot control, achieves state-of-the-art performance on multitask and few-shot learning benchmarks by learning flexible, tr&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/p3v3x7hnv0/cover.png"/></item><item><title>RL-GPT: Integrating Reinforcement Learning and Code-as-policy</title><link>https://deep-diver.github.io/neurips2024/oral-ai-applications/lezx6qrkrh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-ai-applications/lezx6qrkrh/</guid><description>RL-GPT seamlessly integrates Large Language Models (LLMs) and Reinforcement Learning (RL) to create highly efficient agents mastering complex tasks in open-world environments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-ai-applications/lezx6qrkrh/cover.png"/></item><item><title>Safe Time-Varying Optimization based on Gaussian Processes with Spatio-Temporal Kernel</title><link>https://deep-diver.github.io/neurips2024/posters/ykvhjje9le/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ykvhjje9le/</guid><description>TVSAFEOPT: Safe time-varying optimization using spatio-temporal kernels ensures safety while tracking time-varying reward and safety functions, providing optimality guarantees in stationary settings.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ykvhjje9le/cover.png"/></item><item><title>Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/pf7kdijhrf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/pf7kdijhrf/</guid><description>Heterogeneous Pre-trained Transformers (HPT) enables robots to learn generalizable policies from diverse data, drastically improving performance on unseen tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/pf7kdijhrf/cover.png"/></item><item><title>SCaR: Refining Skill Chaining for Long-Horizon Robotic Manipulation via Dual Regularization</title><link>https://deep-diver.github.io/neurips2024/posters/rnxjc4vtvi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rnxjc4vtvi/</guid><description>SCaR refines skill chaining for long-horizon robotic manipulation via dual regularization, achieving higher success rates and robustness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rnxjc4vtvi/cover.png"/></item><item><title>Shared Autonomy with IDA: Interventional Diffusion Assistance</title><link>https://deep-diver.github.io/neurips2024/posters/njvkqsu9z5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/njvkqsu9z5/</guid><description>IDA, a novel intervention assistance, dynamically shares control between human and AI copilots by intervening only when the AI&amp;rsquo;s action is superior across all goals, maximizing performance and preserv&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/njvkqsu9z5/cover.png"/></item><item><title>STL: Still Tricky Logic (for System Validation, Even When Showing Your Work)</title><link>https://deep-diver.github.io/neurips2024/posters/lxz1xiebkf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lxz1xiebkf/</guid><description>Human understanding of formal specifications for robot validation is surprisingly poor; active learning, while improving engagement, doesn&amp;rsquo;t significantly boost accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lxz1xiebkf/cover.png"/></item><item><title>Team-Fictitious Play for Reaching Team-Nash Equilibrium in Multi-team Games</title><link>https://deep-diver.github.io/neurips2024/posters/6vvgagvfxw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/6vvgagvfxw/</guid><description>Team-Fictitious Play (Team-FP) enables self-interested agents to learn near-optimal team coordination in multi-team games, reaching a Team-Nash equilibrium with quantifiable error bounds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/6vvgagvfxw/cover.png"/></item><item><title>TrajCLIP: Pedestrian trajectory prediction method using contrastive learning and idempotent networks</title><link>https://deep-diver.github.io/neurips2024/posters/fubfy8tb3z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fubfy8tb3z/</guid><description>TrajCLIP: a novel pedestrian trajectory prediction method using contrastive learning and idempotent networks to achieve state-of-the-art performance and enhance generalization across diverse scenarios&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fubfy8tb3z/cover.png"/></item><item><title>Variational Distillation of Diffusion Policies into Mixture of Experts</title><link>https://deep-diver.github.io/neurips2024/posters/iiyadgkhwo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/iiyadgkhwo/</guid><description>VDD distills complex diffusion policies into efficient Mixture of Experts (MoE) models via variational inference, enabling faster inference and improved performance in behavior learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/iiyadgkhwo/cover.png"/></item><item><title>VidMan: Exploiting Implicit Dynamics from Video Diffusion Model for Effective Robot Manipulation</title><link>https://deep-diver.github.io/neurips2024/posters/ybhhz0x2j5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ybhhz0x2j5/</guid><description>VidMan: a novel framework leverages video diffusion models and a two-stage training mechanism to significantly improve robot manipulation precision by effectively using robot trajectory data and impli&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ybhhz0x2j5/cover.png"/></item><item><title>VLMimic: Vision Language Models are Visual Imitation Learner for Fine-grained Actions</title><link>https://deep-diver.github.io/neurips2024/posters/c3zhiij9qe/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/c3zhiij9qe/</guid><description>VLMimic: Vision-Language Models enable robots to master intricate actions using only a few human video demonstrations, surpassing existing methods by a significant margin.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/c3zhiij9qe/cover.png"/></item><item><title>Zero-Shot Transfer of Neural ODEs</title><link>https://deep-diver.github.io/neurips2024/posters/ognyoixtin/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ognyoixtin/</guid><description>Zero-shot Neural ODEs enable autonomous systems to rapidly adapt to unseen scenarios by learning a space of dynamical systems spanned by neural ODE basis functions, achieving efficient online adaptati&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ognyoixtin/cover.png"/></item></channel></rss>