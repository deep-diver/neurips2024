<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ Dept. of CSE &amp; School of AI &amp; MoE Key Lab of AI, Shanghai Jiao Tong University on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-dept.-of-cse--school-of-ai--moe-key-lab-of-ai-shanghai-jiao-tong-university/</link><description>Recent content in üè¢ Dept. of CSE &amp; School of AI &amp; MoE Key Lab of AI, Shanghai Jiao Tong University on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-dept.-of-cse--school-of-ai--moe-key-lab-of-ai-shanghai-jiao-tong-university/index.xml" rel="self" type="application/rss+xml"/><item><title>Boundary Matters: A Bi-Level Active Finetuning Method</title><link>https://deep-diver.github.io/neurips2024/posters/444lah3mhg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/444lah3mhg/</guid><description>Bi-Level Active Finetuning Framework (BiLAF) revolutionizes sample selection for efficient model finetuning. Unlike existing methods, BiLAF incorporates both global diversity and local decision bounda&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/444lah3mhg/cover.png"/></item><item><title>FlexPlanner: Flexible 3D Floorplanning via Deep Reinforcement Learning in Hybrid Action Space with Multi-Modality Representation</title><link>https://deep-diver.github.io/neurips2024/posters/q9rlsvyob3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/q9rlsvyob3/</guid><description>FlexPlanner: Deep reinforcement learning solves flexible 3D floorplanning, improving wirelength and alignment significantly.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/q9rlsvyob3/cover.png"/></item><item><title>Unveiling The Matthew Effect Across Channels: Assessing Layer Width Sufficiency via Weight Norm Variance</title><link>https://deep-diver.github.io/neurips2024/posters/tcft2v63vd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tcft2v63vd/</guid><description>Neural network efficiency is improved by analyzing weight norm variance across channels to identify optimal layer widths, resulting in reduced parameters and boosted performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tcft2v63vd/cover.png"/></item><item><title>What Rotary Position Embedding Can Tell Us: Identifying Query and Key Weights Corresponding to Basic Syntactic or High-level Semantic Information</title><link>https://deep-diver.github.io/neurips2024/posters/e5mv7iwfvw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/e5mv7iwfvw/</guid><description>LLM fine-tuning made easy! This paper reveals how analyzing weight vector angles in RoPE positional embeddings helps optimize LLMs, reducing parameter count and improving efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/e5mv7iwfvw/cover.png"/></item></channel></rss>