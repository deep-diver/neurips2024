<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ Harvard University on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-harvard-university/</link><description>Recent content in üè¢ Harvard University on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2025 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-harvard-university/index.xml" rel="self" type="application/rss+xml"/><item><title>A Closer Look at AUROC and AUPRC under Class Imbalance</title><link>https://deep-diver.github.io/neurips2024/posters/s3hva808gk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s3hva808gk/</guid><description>Debunking a common myth, this paper proves that AUPRC is not superior to AUROC for imbalanced datasets, and in fact, can worsen algorithmic bias.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/s3hva808gk/cover.png"/></item><item><title>A Label is Worth A Thousand Images in Dataset Distillation</title><link>https://deep-diver.github.io/neurips2024/posters/onmnr0nj2e/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/onmnr0nj2e/</guid><description>Soft labels, not sophisticated data synthesis, are the key to successful dataset distillation, significantly improving data-efficient learning and challenging existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/onmnr0nj2e/cover.png"/></item><item><title>A teacher-teacher framework for clinical language representation learning</title><link>https://deep-diver.github.io/neurips2024/posters/zdad8zv8tg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zdad8zv8tg/</guid><description>A lightweight knowledge alignment module enables two pre-trained LLMs to mutually learn and improve clinical language representation, exceeding individual model performance on various downstream tasks&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zdad8zv8tg/cover.png"/></item><item><title>Approximating mutual information of high-dimensional variables using learned representations</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/hn05dqxyll/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/hn05dqxyll/</guid><description>Latent Mutual Information (LMI) approximation accurately estimates mutual information in high-dimensional data using low-dimensional learned representations, solving a critical problem in various scie&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/hn05dqxyll/cover.png"/></item><item><title>Axioms for AI Alignment from Human Feedback</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/cmbjkpruvw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/cmbjkpruvw/</guid><description>This paper revolutionizes AI alignment by applying social choice theory axioms to RLHF, exposing flaws in existing methods and proposing novel, axiomatically guaranteed reward learning rules.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/cmbjkpruvw/cover.png"/></item><item><title>Bias Detection via Signaling</title><link>https://deep-diver.github.io/neurips2024/posters/4d7hah4pdr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4d7hah4pdr/</guid><description>This paper presents efficient algorithms to detect whether an agent updates beliefs optimally (Bayesian) or exhibits bias towards their prior beliefs, using information design and signaling schemes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4d7hah4pdr/cover.png"/></item><item><title>Carrot and Stick: Eliciting Comparison Data and Beyond</title><link>https://deep-diver.github.io/neurips2024/posters/ofjtu2ktxo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ofjtu2ktxo/</guid><description>Truthful comparison data is hard to obtain without ground truth. This paper presents novel peer prediction mechanisms using bonus-penalty payments that incentivize truthful comparisons, even in networ&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ofjtu2ktxo/cover.png"/></item><item><title>Covariate Shift Corrected Conditional Randomization Test</title><link>https://deep-diver.github.io/neurips2024/posters/me5esztrqw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/me5esztrqw/</guid><description>A new Covariate Shift Corrected Pearson Chi-squared Conditional Randomization (csPCR) test accurately assesses conditional independence even when data distributions vary between source and target popu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/me5esztrqw/cover.png"/></item><item><title>Dissecting the Interplay of Attention Paths in a Statistical Mechanics Theory of Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/uz804qljt2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uz804qljt2/</guid><description>Researchers dissected attention paths in Transformers using statistical mechanics, revealing a task-relevant kernel combination mechanism boosting generalization performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uz804qljt2/cover.png"/></item><item><title>Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept Space</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/owuect6btl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/owuect6btl/</guid><description>Generative models learn hidden capabilities suddenly during training, which can be explained and predicted using a novel &amp;lsquo;concept space&amp;rsquo; framework that analyzes learning dynamics and concept signal.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/owuect6btl/cover.png"/></item><item><title>Evaluating the World Model Implicit in a Generative Model</title><link>https://deep-diver.github.io/neurips2024/spotlight-large-language-models/avk4jfpegy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-large-language-models/avk4jfpegy/</guid><description>New metrics reveal that generative models often possess surprisingly incoherent world models, despite seemingly accurate next-token predictions. This incoherence leads to fragility in solving related &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-large-language-models/avk4jfpegy/cover.png"/></item><item><title>Eye-gaze Guided Multi-modal Alignment for Medical Representation Learning</title><link>https://deep-diver.github.io/neurips2024/posters/0binew40u4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0binew40u4/</guid><description>Eye-gaze data boosts medical image-text alignment!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0binew40u4/cover.png"/></item><item><title>Fast TRAC: A Parameter-Free Optimizer for Lifelong Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/qeahe4tugc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qeahe4tugc/</guid><description>TRAC: a parameter-free optimizer conquering lifelong RL&amp;rsquo;s plasticity loss!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qeahe4tugc/cover.png"/></item><item><title>FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal Reinforcement for Enhanced Financial Decision Making</title><link>https://deep-diver.github.io/neurips2024/posters/dg1hwkmybc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dg1hwkmybc/</guid><description>FINCON: an LLM-based multi-agent system uses conceptual verbal reinforcement for superior financial decision-making, generalizing well across various tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dg1hwkmybc/cover.png"/></item><item><title>Generalized Protein Pocket Generation with Prior-Informed Flow Matching</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/wyvtj77kev/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/wyvtj77kev/</guid><description>PocketFlow: a novel generative model designs high-affinity protein pockets using prior-informed flow matching, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/wyvtj77kev/cover.png"/></item><item><title>Hardness of Learning Neural Networks under the Manifold Hypothesis</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/dkkgkzmni7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/dkkgkzmni7/</guid><description>Neural network learnability under the manifold hypothesis is hard except for efficiently sampleable manifolds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/dkkgkzmni7/cover.png"/></item><item><title>Honor Among Bandits: No-Regret Learning for Online Fair Division</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ocqbc0edjj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ocqbc0edjj/</guid><description>Online fair division algorithm achieves √ï(T¬≤/¬≥) regret while guaranteeing envy-freeness or proportionality in expectation, a result proven tight.</description></item><item><title>Infinite Limits of Multi-head Transformer Dynamics</title><link>https://deep-diver.github.io/neurips2024/posters/p0bbkhd5ai/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/p0bbkhd5ai/</guid><description>Researchers reveal how the training dynamics of transformer models behave at infinite width, depth, and head count, providing key insights for scaling up these models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/p0bbkhd5ai/cover.png"/></item><item><title>Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)</title><link>https://deep-diver.github.io/neurips2024/posters/7uybktfrtd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7uybktfrtd/</guid><description>SpLiCE unlocks CLIP&amp;rsquo;s potential by transforming its dense, opaque representations into sparse, human-interpretable concept embeddings.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7uybktfrtd/cover.png"/></item><item><title>Multi-Group Proportional Representation in Retrieval</title><link>https://deep-diver.github.io/neurips2024/posters/brzyhvhvsg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/brzyhvhvsg/</guid><description>Multi-group Proportional Representation (MPR) tackles skewed search results by measuring representation across intersectional groups, improving fairness in image retrieval.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/brzyhvhvsg/cover.png"/></item><item><title>Multistable Shape from Shading Emerges from Patch Diffusion</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/bhsfbjs6j9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/bhsfbjs6j9/</guid><description>A novel diffusion model reconstructs multimodal shape distributions from shading, mirroring human multistable perception.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/bhsfbjs6j9/cover.png"/></item><item><title>Optimal ablation for interpretability</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/opt72tyzwz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/opt72tyzwz/</guid><description>Optimal ablation (OA) improves model interpretability by precisely measuring component importance, outperforming existing methods. OA-based importance shines in circuit discovery, factual recall, and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/opt72tyzwz/cover.png"/></item><item><title>Order-Independence Without Fine Tuning</title><link>https://deep-diver.github.io/neurips2024/posters/lq45ar8l7d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lq45ar8l7d/</guid><description>Set-Based Prompting guarantees order-independent LLM outputs by modifying input representations, eliminating unwanted inconsistencies without fine-tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lq45ar8l7d/cover.png"/></item><item><title>Partial observation can induce mechanistic mismatches in data-constrained models of neural dynamics</title><link>https://deep-diver.github.io/neurips2024/posters/lcegp7ir6k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lcegp7ir6k/</guid><description>Partially observing neural circuits during experiments can create misleading models, even if single neuron activity matches; researchers need better validation methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lcegp7ir6k/cover.png"/></item><item><title>Pruning neural network models for gene regulatory dynamics using data and domain knowledge</title><link>https://deep-diver.github.io/neurips2024/posters/fntszlwkgr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fntszlwkgr/</guid><description>DASH: a novel pruning framework leverages domain knowledge to improve the interpretability and sparsity of neural network models for gene regulatory dynamics, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fntszlwkgr/cover.png"/></item><item><title>SkipPredict: When to Invest in Predictions for Scheduling</title><link>https://deep-diver.github.io/neurips2024/posters/kvuw8vzsqz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kvuw8vzsqz/</guid><description>SkipPredict optimizes scheduling by prioritizing cheap predictions and using expensive ones only when necessary, achieving cost-effective performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kvuw8vzsqz/cover.png"/></item><item><title>SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy Segment Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/xcf2vbyzts/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xcf2vbyzts/</guid><description>SocialGPT cleverly leverages Vision Foundation Models and Large Language Models for zero-shot social relation reasoning, achieving competitive results and offering interpretable outputs via prompt opt&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xcf2vbyzts/cover.png"/></item><item><title>Stabilizing Linear Passive-Aggressive Online Learning with Weighted Reservoir Sampling</title><link>https://deep-diver.github.io/neurips2024/posters/fnobf6jm7r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fnobf6jm7r/</guid><description>Weighted reservoir sampling stabilizes online learning algorithms by creating a robust ensemble of intermediate solutions, significantly improving accuracy and mitigating sensitivity to outliers.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fnobf6jm7r/cover.png"/></item><item><title>Testing Calibration in Nearly-Linear Time</title><link>https://deep-diver.github.io/neurips2024/posters/01xv5za56k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/01xv5za56k/</guid><description>This paper presents nearly-linear time algorithms for testing model calibration, improving upon existing methods and providing theoretical lower bounds for various calibration measures.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/01xv5za56k/cover.png"/></item><item><title>The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains</title><link>https://deep-diver.github.io/neurips2024/posters/qart6qtiqj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qart6qtiqj/</guid><description>Transformers learn to perform in-context learning of Markov chains hierarchically, progressing from simpler unigram strategies to more complex bigram solutions, with the presence of simpler solutions &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qart6qtiqj/cover.png"/></item><item><title>Trading off Consistency and Dimensionality of Convex Surrogates for Multiclass Classification</title><link>https://deep-diver.github.io/neurips2024/posters/xcibvuxwpm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xcibvuxwpm/</guid><description>Researchers achieve a balance between accuracy and efficiency in multiclass classification by introducing partially consistent surrogate losses and novel methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xcibvuxwpm/cover.png"/></item><item><title>Unitary Convolutions for Learning on Graphs and Groups</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/lg1veqjvuh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/lg1veqjvuh/</guid><description>Stable deep learning on graphs achieved using novel unitary group convolutions, preventing over-smoothing and enhancing model robustness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/lg1veqjvuh/cover.png"/></item><item><title>UniTS: A Unified Multi-Task Time Series Model</title><link>https://deep-diver.github.io/neurips2024/posters/nbodybptww/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nbodybptww/</guid><description>UniTS: one model to rule them all! This unified multi-task time series model excels in forecasting, classification, anomaly detection, and imputation, outperforming specialized models across 38 divers&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nbodybptww/cover.png"/></item><item><title>Unrolled denoising networks provably learn to perform optimal Bayesian inference</title><link>https://deep-diver.github.io/neurips2024/posters/cpklmjqzde/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cpklmjqzde/</guid><description>Unrolled neural networks, trained via gradient descent, provably achieve optimal Bayesian inference for compressed sensing, surpassing prior-aware counterparts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cpklmjqzde/cover.png"/></item><item><title>User-Creator Feature Polarization in Recommender Systems with Dual Influence</title><link>https://deep-diver.github.io/neurips2024/posters/ywq89o19wf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ywq89o19wf/</guid><description>Recommender systems, when influenced by both users and creators, inevitably polarize; however, prioritizing efficiency through methods like top-k truncation can surprisingly enhance diversity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ywq89o19wf/cover.png"/></item></channel></rss>