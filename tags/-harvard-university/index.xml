<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ Harvard University on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-harvard-university/</link><description>Recent content in üè¢ Harvard University on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-harvard-university/index.xml" rel="self" type="application/rss+xml"/><item><title>A teacher-teacher framework for clinical language representation learning</title><link>https://deep-diver.github.io/neurips2024/posters/zdad8zv8tg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zdad8zv8tg/</guid><description>A lightweight knowledge alignment module enables two pre-trained LLMs to mutually learn and improve clinical language representation, exceeding individual model performance on various downstream tasks&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zdad8zv8tg/cover.png"/></item><item><title>Approximating mutual information of high-dimensional variables using learned representations</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/hn05dqxyll/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/hn05dqxyll/</guid><description>Latent Mutual Information (LMI) approximation accurately estimates mutual information in high-dimensional data using low-dimensional learned representations, solving a critical problem in various scie&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/hn05dqxyll/cover.png"/></item><item><title>Axioms for AI Alignment from Human Feedback</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/cmbjkpruvw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/cmbjkpruvw/</guid><description>This paper revolutionizes AI alignment by applying social choice theory axioms to RLHF, exposing flaws in existing methods and proposing novel, axiomatically guaranteed reward learning rules.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/cmbjkpruvw/cover.png"/></item><item><title>Carrot and Stick: Eliciting Comparison Data and Beyond</title><link>https://deep-diver.github.io/neurips2024/posters/ofjtu2ktxo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ofjtu2ktxo/</guid><description>Truthful comparison data is hard to obtain without ground truth. This paper presents novel peer prediction mechanisms using bonus-penalty payments that incentivize truthful comparisons, even in networ&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ofjtu2ktxo/cover.png"/></item><item><title>Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept Space</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/owuect6btl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/owuect6btl/</guid><description>Generative models learn hidden capabilities suddenly during training, which can be explained and predicted using a novel &amp;lsquo;concept space&amp;rsquo; framework that analyzes learning dynamics and concept signal.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/owuect6btl/cover.png"/></item><item><title>Evaluating the World Model Implicit in a Generative Model</title><link>https://deep-diver.github.io/neurips2024/spotlight-large-language-models/avk4jfpegy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-large-language-models/avk4jfpegy/</guid><description>New metrics reveal that generative models often possess surprisingly incoherent world models, despite seemingly accurate next-token predictions. This incoherence leads to fragility in solving related &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-large-language-models/avk4jfpegy/cover.png"/></item><item><title>Generalized Protein Pocket Generation with Prior-Informed Flow Matching</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/wyvtj77kev/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/wyvtj77kev/</guid><description>PocketFlow: a novel generative model designs high-affinity protein pockets using prior-informed flow matching, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/wyvtj77kev/cover.png"/></item><item><title>Hardness of Learning Neural Networks under the Manifold Hypothesis</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/dkkgkzmni7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/dkkgkzmni7/</guid><description>Neural network learnability under the manifold hypothesis is hard except for efficiently sampleable manifolds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/dkkgkzmni7/cover.png"/></item><item><title>Honor Among Bandits: No-Regret Learning for Online Fair Division</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ocqbc0edjj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/ocqbc0edjj/</guid><description>Online fair division algorithm achieves √ï(T¬≤/¬≥) regret while guaranteeing envy-freeness or proportionality in expectation, a result proven tight.</description></item><item><title>Multistable Shape from Shading Emerges from Patch Diffusion</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/bhsfbjs6j9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/bhsfbjs6j9/</guid><description>A novel diffusion model reconstructs multimodal shape distributions from shading, mirroring human multistable perception.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/bhsfbjs6j9/cover.png"/></item><item><title>Optimal ablation for interpretability</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/opt72tyzwz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/opt72tyzwz/</guid><description>Optimal ablation (OA) improves model interpretability by precisely measuring component importance, outperforming existing methods. OA-based importance shines in circuit discovery, factual recall, and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/opt72tyzwz/cover.png"/></item><item><title>The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains</title><link>https://deep-diver.github.io/neurips2024/posters/qart6qtiqj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qart6qtiqj/</guid><description>Transformers learn to perform in-context learning of Markov chains hierarchically, progressing from simpler unigram strategies to more complex bigram solutions, with the presence of simpler solutions &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qart6qtiqj/cover.png"/></item><item><title>Unitary Convolutions for Learning on Graphs and Groups</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/lg1veqjvuh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/lg1veqjvuh/</guid><description>Stable deep learning on graphs achieved using novel unitary group convolutions, preventing over-smoothing and enhancing model robustness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/lg1veqjvuh/cover.png"/></item><item><title>User-Creator Feature Polarization in Recommender Systems with Dual Influence</title><link>https://deep-diver.github.io/neurips2024/posters/ywq89o19wf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ywq89o19wf/</guid><description>Recommender systems, when influenced by both users and creators, inevitably polarize; however, prioritizing efficiency through methods like top-k truncation can surprisingly enhance diversity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ywq89o19wf/cover.png"/></item></channel></rss>