<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ Tongji University on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-tongji-university/</link><description>Recent content in üè¢ Tongji University on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-tongji-university/index.xml" rel="self" type="application/rss+xml"/><item><title>AlchemistCoder: Harmonizing and Eliciting Code Capability by Hindsight Tuning on Multi-source Data</title><link>https://deep-diver.github.io/neurips2024/posters/saqxbnvv4t/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/saqxbnvv4t/</guid><description>AlchemistCoder enhances code LLMs by pioneering hindsight tuning on multi-source data, harmonizing conflicting styles via AlchemistPrompts, and achieving state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/saqxbnvv4t/cover.png"/></item><item><title>Does Video-Text Pretraining Help Open-Vocabulary Online Action Detection?</title><link>https://deep-diver.github.io/neurips2024/posters/pwzb2v2b6r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pwzb2v2b6r/</guid><description>Zero-shot online action detection gets a boost! OV-OAD leverages vision-language models and text supervision to achieve impressive performance on various benchmarks without relying on manual annotati&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pwzb2v2b6r/cover.png"/></item><item><title>Generalization Bounds via Conditional $f$-Information</title><link>https://deep-diver.github.io/neurips2024/posters/ocxvxe5xn1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ocxvxe5xn1/</guid><description>New information-theoretic generalization bounds, based on conditional f-information, improve existing methods by addressing unboundedness and offering a generic approach applicable to various loss fun&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ocxvxe5xn1/cover.png"/></item><item><title>GeoNLF: Geometry guided Pose-Free Neural LiDAR Fields</title><link>https://deep-diver.github.io/neurips2024/posters/v3y785tn7b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v3y785tn7b/</guid><description>GeoNLF: Geometry-guided Pose-free Neural LiDAR Fields revolutionizes LiDAR point cloud processing by cleverly combining neural and geometric optimization for superior novel view synthesis and multi-vi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v3y785tn7b/cover.png"/></item><item><title>Hyper-opinion Evidential Deep Learning for Out-of-Distribution Detection</title><link>https://deep-diver.github.io/neurips2024/posters/te8vi2wgth/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/te8vi2wgth/</guid><description>Hyper-opinion Evidential Deep Learning (HEDL) enhances out-of-distribution detection by integrating sharp and vague evidence for superior uncertainty estimation and classification accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/te8vi2wgth/cover.png"/></item><item><title>MoTE: Reconciling Generalization with Specialization for Visual-Language to Video Knowledge Transfer</title><link>https://deep-diver.github.io/neurips2024/posters/vpeq2bzss0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vpeq2bzss0/</guid><description>MoTE: A novel framework harmonizes generalization and specialization for visual-language video knowledge transfer, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vpeq2bzss0/cover.png"/></item><item><title>NeuroClips: Towards High-fidelity and Smooth fMRI-to-Video Reconstruction</title><link>https://deep-diver.github.io/neurips2024/oral-others/8qu52fl1dt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-others/8qu52fl1dt/</guid><description>NeuroClips: groundbreaking fMRI-to-video reconstruction, achieving high-fidelity smooth video up to 6s at 8FPS by decoding both high-level semantics and low-level perception flows.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-others/8qu52fl1dt/cover.png"/></item><item><title>On $f$-Divergence Principled Domain Adaptation: An Improved Framework</title><link>https://deep-diver.github.io/neurips2024/posters/xsu27dgwer/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xsu27dgwer/</guid><description>Improved unsupervised domain adaptation framework achieves superior performance via refined f-divergence and novel f-domain discrepancy, enabling faster algorithms and tighter generalization bounds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xsu27dgwer/cover.png"/></item><item><title>RCDN: Towards Robust Camera-Insensitivity Collaborative Perception via Dynamic Feature-based 3D Neural Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/xvveszovjo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xvveszovjo/</guid><description>RCDN: Robust, camera-insensitive collaborative perception via dynamic 3D neural modeling, overcoming camera failures for high-performance autonomous systems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xvveszovjo/cover.png"/></item><item><title>Tri-Level Navigator: LLM-Empowered Tri-Level Learning for Time Series OOD Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/a6hzeu4kpo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/a6hzeu4kpo/</guid><description>LLM-powered Tri-level learning framework enhances time series OOD generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/a6hzeu4kpo/cover.png"/></item></channel></rss>