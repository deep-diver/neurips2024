<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ Chinese University of Hong Kong, Shenzhen on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-chinese-university-of-hong-kong-shenzhen/</link><description>Recent content in üè¢ Chinese University of Hong Kong, Shenzhen on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2025 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-chinese-university-of-hong-kong-shenzhen/index.xml" rel="self" type="application/rss+xml"/><item><title>B-ary Tree Push-Pull Method is Provably Efficient for Distributed Learning on Heterogeneous Data</title><link>https://deep-diver.github.io/neurips2024/posters/3mnxactbd3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3mnxactbd3/</guid><description>B-ary Tree Push-Pull (BTPP) achieves linear speedup for distributed learning on heterogeneous data, significantly outperforming state-of-the-art methods with minimal communication.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3mnxactbd3/cover.png"/></item><item><title>BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/0uxtfk5knj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0uxtfk5knj/</guid><description>BAdam: A memory-efficient optimization method enabling full parameter fine-tuning of large language models using a block coordinate descent framework with Adam&amp;rsquo;s update rule, achieving comparable or s&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0uxtfk5knj/cover.png"/></item><item><title>Boosting Graph Pooling with Persistent Homology</title><link>https://deep-diver.github.io/neurips2024/posters/wcmqdy2aku/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wcmqdy2aku/</guid><description>Boosting graph neural networks: Topology-Invariant Pooling (TIP) leverages persistent homology to enhance graph pooling, achieving consistent performance gains across diverse datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wcmqdy2aku/cover.png"/></item><item><title>Disentangling Linear Quadratic Control with Untrusted ML Predictions</title><link>https://deep-diver.github.io/neurips2024/posters/wxqukapoa7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wxqukapoa7/</guid><description>DISC, a novel control policy, disentangles untrusted ML predictions to achieve near-optimal performance when accurate, while guaranteeing competitive ratio bounds even with significant prediction erro&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wxqukapoa7/cover.png"/></item><item><title>Graph Classification via Reference Distribution Learning: Theory and Practice</title><link>https://deep-diver.github.io/neurips2024/posters/1zvinhehks/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1zvinhehks/</guid><description>GRDL: a novel graph classification method boasting 10x speed improvement over competitors, achieved by treating node embeddings as distributions and avoiding global pooling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1zvinhehks/cover.png"/></item></channel></rss>