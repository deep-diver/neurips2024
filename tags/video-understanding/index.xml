<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Video Understanding on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/video-understanding/</link><description>Recent content in Video Understanding on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/video-understanding/index.xml" rel="self" type="application/rss+xml"/><item><title>4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/posters/so1arpwvlk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/so1arpwvlk/</guid><description>4Real: Photorealistic 4D scene generation from text prompts using video diffusion models, exceeding object-centric approaches for higher realism and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/so1arpwvlk/cover.png"/></item><item><title>A Motion-aware Spatio-temporal Graph for Video Salient Object Ranking</title><link>https://deep-diver.github.io/neurips2024/posters/vubtacqn44/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vubtacqn44/</guid><description>A novel motion-aware spatio-temporal graph model surpasses existing methods in video salient object ranking by jointly optimizing multi-scale spatial and temporal features, thus accurately prioritizin&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vubtacqn44/cover.png"/></item><item><title>ActAnywhere: Subject-Aware Video Background Generation</title><link>https://deep-diver.github.io/neurips2024/posters/ntlfrew59a/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ntlfrew59a/</guid><description>ActAnywhere, a novel video diffusion model, seamlessly integrates foreground subjects into new backgrounds by generating realistic video backgrounds tailored to subject motion, significantly reducing &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ntlfrew59a/cover.png"/></item><item><title>AverNet: All-in-one Video Restoration for Time-varying Unknown Degradations</title><link>https://deep-diver.github.io/neurips2024/posters/ucdanf2pkl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ucdanf2pkl/</guid><description>AverNet: All-in-one video restoration defying time-varying unknown degradations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ucdanf2pkl/cover.png"/></item><item><title>Beyond Accuracy: Tracking more like Human via Visual Search</title><link>https://deep-diver.github.io/neurips2024/posters/lezaeimfoc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lezaeimfoc/</guid><description>CPDTrack: Human-like Visual Search Boosts Object Tracking!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lezaeimfoc/cover.png"/></item><item><title>Beyond Euclidean: Dual-Space Representation Learning for Weakly Supervised Video Violence Detection</title><link>https://deep-diver.github.io/neurips2024/posters/tbpv0qfnho/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tbpv0qfnho/</guid><description>Beyond Euclidean spaces, Dual-Space Representation Learning (DSRL) enhances weakly supervised video violence detection by cleverly integrating Euclidean and hyperbolic geometries for superior discrimi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tbpv0qfnho/cover.png"/></item><item><title>bit2bit: 1-bit quanta video reconstruction via self-supervised photon prediction</title><link>https://deep-diver.github.io/neurips2024/posters/htlfnbyfon/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/htlfnbyfon/</guid><description>bit2bit reconstructs high-quality videos from sparse, binary quanta image sensor data using self-supervised photon location prediction, significantly improving resolution and usability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/htlfnbyfon/cover.png"/></item><item><title>Collaborative Video Diffusion: Consistent Multi-video Generation with Camera Control</title><link>https://deep-diver.github.io/neurips2024/posters/arhjlyiy2j/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/arhjlyiy2j/</guid><description>Collaborative Video Diffusion (CVD) generates multiple consistent videos from various camera angles using a novel cross-video synchronization module, significantly improving consistency compared to ex&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/arhjlyiy2j/cover.png"/></item><item><title>COVE: Unleashing the Diffusion Feature Correspondence for Consistent Video Editing</title><link>https://deep-diver.github.io/neurips2024/posters/474m9aei4u/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/474m9aei4u/</guid><description>COVE: Consistent high-quality video editing achieved by leveraging diffusion feature correspondence for temporal consistency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/474m9aei4u/cover.png"/></item><item><title>CV-VAE: A Compatible Video VAE for Latent Generative Video Models</title><link>https://deep-diver.github.io/neurips2024/posters/8z4isrqbcf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8z4isrqbcf/</guid><description>CV-VAE: A compatible video VAE enabling efficient, high-quality latent video generation by bridging the gap between image and video latent spaces.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8z4isrqbcf/cover.png"/></item><item><title>DeltaDEQ: Exploiting Heterogeneous Convergence for Accelerating Deep Equilibrium Iterations</title><link>https://deep-diver.github.io/neurips2024/posters/7qbkadv4zd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7qbkadv4zd/</guid><description>DeltaDEQ accelerates deep equilibrium model inference by 73-84% via a novel &amp;lsquo;heterogeneous convergence&amp;rsquo; exploitation technique, maintaining accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7qbkadv4zd/cover.png"/></item><item><title>Differentiable Task Graph Learning: Procedural Activity Representation and Online Mistake Detection from Egocentric Videos</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/2hvgvb4awq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/2hvgvb4awq/</guid><description>This paper introduces a novel differentiable framework for learning task graphs from video demonstrations of procedural activities. By directly optimizing the weights of a task graph&amp;rsquo;s edges, the mod&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/2hvgvb4awq/cover.png"/></item><item><title>Don't Look Twice: Faster Video Transformers with Run-Length Tokenization</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/b1ggjw00ni/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/b1ggjw00ni/</guid><description>Run-Length Tokenization (RLT) dramatically speeds up video transformer training and inference by efficiently removing redundant video tokens, matching baseline model performance with significant time &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/b1ggjw00ni/cover.png"/></item><item><title>DreamScene4D: Dynamic Multi-Object Scene Generation from Monocular Videos</title><link>https://deep-diver.github.io/neurips2024/posters/ylivhhfwq2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ylivhhfwq2/</guid><description>DreamScene4D generates realistic 3D dynamic multi-object scenes from monocular videos via novel view synthesis, addressing limitations of existing methods with a novel decompose-recompose approach.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ylivhhfwq2/cover.png"/></item><item><title>E-Motion: Future Motion Simulation via Event Sequence Diffusion</title><link>https://deep-diver.github.io/neurips2024/posters/pwowk7jqok/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pwowk7jqok/</guid><description>E-Motion: Predicting future motion with unprecedented accuracy using event cameras and video diffusion models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pwowk7jqok/cover.png"/></item><item><title>EEG2Video: Towards Decoding Dynamic Visual Perception from EEG Signals</title><link>https://deep-diver.github.io/neurips2024/posters/rfsfrn9ofd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rfsfrn9ofd/</guid><description>EEG2Video reconstructs dynamic videos from EEG signals, achieving 79.8% accuracy in semantic classification and 0.256 SSIM in video reconstruction.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rfsfrn9ofd/cover.png"/></item><item><title>Efficient Temporal Action Segmentation via Boundary-aware Query Voting</title><link>https://deep-diver.github.io/neurips2024/posters/jij4vovu7i/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jij4vovu7i/</guid><description>BaFormer: a novel boundary-aware Transformer network achieves efficient and accurate temporal action segmentation by using instance and global queries for segment classification and boundary predictio&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jij4vovu7i/cover.png"/></item><item><title>End-to-End Video Semantic Segmentation in Adverse Weather using Fusion Blocks and Temporal-Spatial Teacher-Student Learning</title><link>https://deep-diver.github.io/neurips2024/posters/paobkszgia/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/paobkszgia/</guid><description>Optical-flow-free video semantic segmentation excels in adverse weather by merging adjacent frame information via a fusion block and a novel temporal-spatial teacher-student learning strategy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/paobkszgia/cover.png"/></item><item><title>Exocentric-to-Egocentric Video Generation</title><link>https://deep-diver.github.io/neurips2024/posters/uhdcbircfl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uhdcbircfl/</guid><description>Exo2Ego-V generates realistic egocentric videos from sparse exocentric views, significantly outperforming state-of-the-art methods on a challenging benchmark.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uhdcbircfl/cover.png"/></item><item><title>Extending Video Masked Autoencoders to 128 frames</title><link>https://deep-diver.github.io/neurips2024/posters/bfrnplwchg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bfrnplwchg/</guid><description>Long-video masked autoencoders (LVMAE) achieve state-of-the-art performance by using an adaptive masking strategy that prioritizes important video tokens, enabling efficient training on 128 frames.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bfrnplwchg/cover.png"/></item><item><title>FactorizePhys: Matrix Factorization for Multidimensional Attention in Remote Physiological Sensing</title><link>https://deep-diver.github.io/neurips2024/posters/qrfp4eez47/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qrfp4eez47/</guid><description>FactorizePhys leverages Non-negative Matrix Factorization for a novel multidimensional attention mechanism (FSAM) to improve remote PPG signal extraction from videos.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qrfp4eez47/cover.png"/></item><item><title>Fast and Memory-Efficient Video Diffusion Using Streamlined Inference</title><link>https://deep-diver.github.io/neurips2024/posters/invxyqrkpi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/invxyqrkpi/</guid><description>Streamlined Inference, a novel training-free framework, dramatically reduces the computation and memory costs of video diffusion models without sacrificing quality, enabling high-resolution video gene&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/invxyqrkpi/cover.png"/></item><item><title>FIFO-Diffusion: Generating Infinite Videos from Text without Training</title><link>https://deep-diver.github.io/neurips2024/posters/uikhna4wam/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uikhna4wam/</guid><description>FIFO-Diffusion generates infinitely long, high-quality videos from text prompts using a pretrained model, solving the challenge of long video generation without retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uikhna4wam/cover.png"/></item><item><title>FreeLong: Training-Free Long Video Generation with SpectralBlend Temporal Attention</title><link>https://deep-diver.github.io/neurips2024/posters/x9fga52oov/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x9fga52oov/</guid><description>FreeLong: Generate high-fidelity long videos without retraining using spectral blending of global and local video features!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x9fga52oov/cover.png"/></item><item><title>Generalizable Implicit Motion Modeling for Video Frame Interpolation</title><link>https://deep-diver.github.io/neurips2024/posters/zlpjlqsr2v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zlpjlqsr2v/</guid><description>Generalizable Implicit Motion Modeling (GIMM) revolutionizes video frame interpolation by accurately predicting optical flows at any timestep, surpassing existing methods and achieving state-of-the-ar&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zlpjlqsr2v/cover.png"/></item><item><title>GenRec: Unifying Video Generation and Recognition with Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/posters/ydfzp7qmzp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ydfzp7qmzp/</guid><description>GenRec: One diffusion model to rule both video generation and recognition!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ydfzp7qmzp/cover.png"/></item><item><title>GoMatching: A Simple Baseline for Video Text Spotting via Long and Short Term Matching</title><link>https://deep-diver.github.io/neurips2024/posters/asv9lqchcc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/asv9lqchcc/</guid><description>GoMatching, a novel video text spotting baseline, enhances tracking efficiency while maintaining strong recognition by integrating long- and short-term matching via a Transformer-based module and a re&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/asv9lqchcc/cover.png"/></item><item><title>HOI-Swap: Swapping Objects in Videos with Hand-Object Interaction Awareness</title><link>https://deep-diver.github.io/neurips2024/posters/gkhxbasqwm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gkhxbasqwm/</guid><description>HOI-Swap: a novel diffusion model flawlessly swaps objects in videos while intelligently preserving natural hand interactions, producing high-quality edits.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gkhxbasqwm/cover.png"/></item><item><title>Learning Truncated Causal History Model for Video Restoration</title><link>https://deep-diver.github.io/neurips2024/posters/cugf2hancs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cugf2hancs/</guid><description>TURTLE: a novel video restoration framework that learns a truncated causal history model for efficient and high-performing video restoration, achieving state-of-the-art results on various benchmark ta&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cugf2hancs/cover.png"/></item><item><title>Long-Range Feedback Spiking Network Captures Dynamic and Static Representations of the Visual Cortex under Movie Stimuli</title><link>https://deep-diver.github.io/neurips2024/posters/bxdok3uak6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bxdok3uak6/</guid><description>Long-range feedback spiking network (LoRaFB-SNet) surpasses other models in capturing dynamic and static visual cortical representations under movie stimuli, advancing our understanding of visual syst&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bxdok3uak6/cover.png"/></item><item><title>MambaSCI: Efficient Mamba-UNet for Quad-Bayer Patterned Video Snapshot Compressive Imaging</title><link>https://deep-diver.github.io/neurips2024/posters/u4weoyrhpd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/u4weoyrhpd/</guid><description>MambaSCI: Efficient, novel deep learning model reconstructs high-quality quad-Bayer video from compressed snapshots, surpassing existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/u4weoyrhpd/cover.png"/></item><item><title>MECD: Unlocking Multi-Event Causal Discovery in Video Reasoning</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/f8asoovlep/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/f8asoovlep/</guid><description>MECD: A new task and dataset unlocks multi-event causal discovery in videos, enabling a novel framework that outperforms existing models by efficiently identifying causal relationships between chronol&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/f8asoovlep/cover.png"/></item><item><title>MoTE: Reconciling Generalization with Specialization for Visual-Language to Video Knowledge Transfer</title><link>https://deep-diver.github.io/neurips2024/posters/vpeq2bzss0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vpeq2bzss0/</guid><description>MoTE: A novel framework harmonizes generalization and specialization for visual-language video knowledge transfer, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vpeq2bzss0/cover.png"/></item><item><title>Motion Consistency Model: Accelerating Video Diffusion with Disentangled Motion-Appearance Distillation</title><link>https://deep-diver.github.io/neurips2024/posters/nsqxn9ioj7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nsqxn9ioj7/</guid><description>Boosting video diffusion: Motion Consistency Model (MCM) disentangles motion and appearance learning for high-fidelity, fast video generation using few sampling steps.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nsqxn9ioj7/cover.png"/></item><item><title>Motion Graph Unleashed: A Novel Approach to Video Prediction</title><link>https://deep-diver.github.io/neurips2024/posters/4ztp4pujog/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4ztp4pujog/</guid><description>Motion Graph unleashes efficient and accurate video prediction by transforming video frames into interconnected graph nodes, capturing complex motion patterns with minimal computational cost.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4ztp4pujog/cover.png"/></item><item><title>MotionCraft: Physics-Based Zero-Shot Video Generation</title><link>https://deep-diver.github.io/neurips2024/posters/lvcwa24dxb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lvcwa24dxb/</guid><description>MotionCraft: Physics-based zero-shot video generation creates realistic videos with complex motion dynamics by cleverly warping the noise latent space of an image diffusion model using optical flow fr&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lvcwa24dxb/cover.png"/></item><item><title>Moving Off-the-Grid: Scene-Grounded Video Representations</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/rjspdvduaw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/rjspdvduaw/</guid><description>MooG: Self-supervised video model learns off-the-grid representations, enabling consistent scene element tracking even with motion; outperforming grid-based baselines on various vision tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/rjspdvduaw/cover.png"/></item><item><title>MTGS: A Novel Framework for Multi-Person Temporal Gaze Following and Social Gaze Prediction</title><link>https://deep-diver.github.io/neurips2024/posters/alu676zgfe/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/alu676zgfe/</guid><description>MTGS: a unified framework jointly predicts gaze and social gaze (shared attention, mutual gaze) for multiple people in videos, achieving state-of-the-art results using a temporal transformer model and&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/alu676zgfe/cover.png"/></item><item><title>Multi-view Masked Contrastive Representation Learning for Endoscopic Video Analysis</title><link>https://deep-diver.github.io/neurips2024/posters/1m67admbbg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1m67admbbg/</guid><description>Multi-view Masked Contrastive Representation Learning (MÂ²CRL) significantly boosts endoscopic video analysis by using a novel multi-view masking strategy and contrastive learning, achieving state-of-t&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1m67admbbg/cover.png"/></item><item><title>NaRCan: Natural Refined Canonical Image with Integration of Diffusion Prior for Video Editing</title><link>https://deep-diver.github.io/neurips2024/posters/bcr2nlm1qw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bcr2nlm1qw/</guid><description>NaRCan: High-quality video editing via diffusion priors and hybrid deformation fields.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bcr2nlm1qw/cover.png"/></item><item><title>NeuroClips: Towards High-fidelity and Smooth fMRI-to-Video Reconstruction</title><link>https://deep-diver.github.io/neurips2024/oral-others/8qu52fl1dt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-others/8qu52fl1dt/</guid><description>NeuroClips: groundbreaking fMRI-to-video reconstruction, achieving high-fidelity smooth video up to 6s at 8FPS by decoding both high-level semantics and low-level perception flows.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-others/8qu52fl1dt/cover.png"/></item><item><title>NVRC: Neural Video Representation Compression</title><link>https://deep-diver.github.io/neurips2024/posters/i29aimdm4u/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/i29aimdm4u/</guid><description>NVRC: A novel end-to-end neural video codec achieves 23% coding gain over VVC VTM by optimizing representation compression.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/i29aimdm4u/cover.png"/></item><item><title>On Learning Multi-Modal Forgery Representation for Diffusion Generated Video Detection</title><link>https://deep-diver.github.io/neurips2024/posters/4bjufos6no/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4bjufos6no/</guid><description>MM-Det, a novel algorithm, uses multimodal learning and spatiotemporal attention to detect diffusion-generated videos, achieving state-of-the-art performance on the new DVF dataset.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4bjufos6no/cover.png"/></item><item><title>OnlineTAS: An Online Baseline for Temporal Action Segmentation</title><link>https://deep-diver.github.io/neurips2024/posters/bkletzd97m/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bkletzd97m/</guid><description>OnlineTAS, a novel framework, achieves state-of-the-art performance in online temporal action segmentation by using an adaptive memory and a post-processing method to mitigate over-segmentation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bkletzd97m/cover.png"/></item><item><title>OPEL: Optimal Transport Guided ProcedurE Learning</title><link>https://deep-diver.github.io/neurips2024/posters/leqd3bj4ly/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/leqd3bj4ly/</guid><description>OPEL: a novel optimal transport framework for procedure learning, significantly outperforms SOTA methods by aligning similar video frames and relaxing strict temporal assumptions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/leqd3bj4ly/cover.png"/></item><item><title>ReVideo: Remake a Video with Motion and Content Control</title><link>https://deep-diver.github.io/neurips2024/posters/xujbzr6b1t/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xujbzr6b1t/</guid><description>ReVideo enables precise local video editing by independently controlling content and motion, overcoming limitations of existing methods and paving the way for advanced video manipulation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xujbzr6b1t/cover.png"/></item><item><title>SF-V: Single Forward Video Generation Model</title><link>https://deep-diver.github.io/neurips2024/posters/pvgaemm3mw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pvgaemm3mw/</guid><description>Researchers developed SF-V, a single-step image-to-video generation model, achieving a 23x speedup compared to existing models without sacrificing quality, paving the way for real-time video synthesis&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pvgaemm3mw/cover.png"/></item><item><title>Slot State Space Models</title><link>https://deep-diver.github.io/neurips2024/posters/bjv1t4xnjw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bjv1t4xnjw/</guid><description>SlotSSMs: a novel framework for modular sequence modeling, achieving significant performance gains by incorporating independent mechanisms and sparse interactions into State Space Models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bjv1t4xnjw/cover.png"/></item><item><title>Splatter a Video: Video Gaussian Representation for Versatile Processing</title><link>https://deep-diver.github.io/neurips2024/posters/bzuqtvdxv0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bzuqtvdxv0/</guid><description>Researchers introduce Video Gaussian Representation (VGR) for versatile video processing, embedding videos into explicit 3D Gaussians for intuitive motion and appearance modeling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bzuqtvdxv0/cover.png"/></item><item><title>StreamFlow: Streamlined Multi-Frame Optical Flow Estimation for Video Sequences</title><link>https://deep-diver.github.io/neurips2024/posters/fylch4hazr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fylch4hazr/</guid><description>StreamFlow accelerates video optical flow estimation by 44% via a streamlined in-batch multi-frame pipeline and innovative spatiotemporal modeling, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fylch4hazr/cover.png"/></item><item><title>SyncVIS: Synchronized Video Instance Segmentation</title><link>https://deep-diver.github.io/neurips2024/posters/ttpvhsqtkf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ttpvhsqtkf/</guid><description>SyncVIS: A new framework for video instance segmentation achieves state-of-the-art results by synchronously modeling video and frame-level information, overcoming limitations of asynchronous approache&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ttpvhsqtkf/cover.png"/></item><item><title>TAPTRv2: Attention-based Position Update Improves Tracking Any Point</title><link>https://deep-diver.github.io/neurips2024/posters/cx2o6xz03h/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cx2o6xz03h/</guid><description>TAPTRv2 enhances point tracking by introducing an attention-based position update, eliminating cost-volume reliance for improved accuracy and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cx2o6xz03h/cover.png"/></item><item><title>Temporally Consistent Atmospheric Turbulence Mitigation with Neural Representations</title><link>https://deep-diver.github.io/neurips2024/posters/yurca4wi2l/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yurca4wi2l/</guid><description>ConVRT: A novel framework restores turbulence-distorted videos by decoupling spatial and temporal information in a neural representation, achieving temporally consistent mitigation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yurca4wi2l/cover.png"/></item><item><title>Towards Multi-Domain Learning for Generalizable Video Anomaly Detection</title><link>https://deep-diver.github.io/neurips2024/posters/yweqkcmimh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yweqkcmimh/</guid><description>Researchers propose Multi-Domain learning for Video Anomaly Detection (MDVAD) to create generalizable models handling conflicting abnormality criteria across diverse datasets, improving accuracy and a&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yweqkcmimh/cover.png"/></item><item><title>TrackIME: Enhanced Video Point Tracking via Instance Motion Estimation</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/ekhqbgvl3g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/ekhqbgvl3g/</guid><description>TrackIME enhances video point tracking by cleverly pruning the search space, resulting in improved accuracy and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/ekhqbgvl3g/cover.png"/></item><item><title>VFIMamba: Video Frame Interpolation with State Space Models</title><link>https://deep-diver.github.io/neurips2024/posters/4s5usbusus/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4s5usbusus/</guid><description>VFIMamba uses state-space models for efficient and dynamic video frame interpolation, achieving state-of-the-art results by introducing a novel Mixed-SSM Block and curriculum learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4s5usbusus/cover.png"/></item><item><title>Video Diffusion Models are Training-free Motion Interpreter and Controller</title><link>https://deep-diver.github.io/neurips2024/posters/zvq4bn75kn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zvq4bn75kn/</guid><description>Training-free video motion control achieved via novel Motion Feature (MOFT) extraction from existing video diffusion models, offering architecture-agnostic insights and high performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zvq4bn75kn/cover.png"/></item><item><title>Video Token Merging for Long Video Understanding</title><link>https://deep-diver.github.io/neurips2024/posters/wdurabdrbs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wdurabdrbs/</guid><description>Researchers boost long-form video understanding efficiency by 6.89x and reduce memory usage by 84% using a novel learnable video token merging algorithm.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wdurabdrbs/cover.png"/></item></channel></rss>