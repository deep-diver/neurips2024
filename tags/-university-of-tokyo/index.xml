<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ University of Tokyo on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-university-of-tokyo/</link><description>Recent content in üè¢ University of Tokyo on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2025 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-university-of-tokyo/index.xml" rel="self" type="application/rss+xml"/><item><title>A provable control of sensitivity of neural networks through a direct parameterization of the overall bi-Lipschitzness</title><link>https://deep-diver.github.io/neurips2024/posters/ww62xltefb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ww62xltefb/</guid><description>New framework directly controls neural network sensitivity by precisely parameterizing overall bi-Lipschitzness, offering improved robustness and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ww62xltefb/cover.png"/></item><item><title>A Simple and Adaptive Learning Rate for FTRL in Online Learning with Minimax Regret of Œò(T^{2/3}) and its Application to Best-of-Both-Worlds</title><link>https://deep-diver.github.io/neurips2024/posters/xlvuz9f50g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xlvuz9f50g/</guid><description>A new adaptive learning rate for FTRL achieves minimax regret of O(T¬≤/¬≥) in online learning, improving existing best-of-both-worlds algorithms for various hard problems.</description></item><item><title>ADOPT: Modified Adam Can Converge with Any $eta_2$ with the Optimal Rate</title><link>https://deep-diver.github.io/neurips2024/posters/rzvvm0lsyk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rzvvm0lsyk/</guid><description>ADOPT, a novel adaptive gradient method, achieves optimal convergence rates without restrictive assumptions, unlike Adam, significantly improving deep learning optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rzvvm0lsyk/cover.png"/></item><item><title>Continuous Temporal Domain Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/g24fopc3je/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/g24fopc3je/</guid><description>Koodos: a novel Koopman operator-driven framework that tackles Continuous Temporal Domain Generalization (CTDG) by modeling continuous data dynamics and learning model evolution across irregular time &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/g24fopc3je/cover.png"/></item><item><title>Dealing with Synthetic Data Contamination in Online Continual Learning</title><link>https://deep-diver.github.io/neurips2024/posters/lc8gemv97y/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lc8gemv97y/</guid><description>AI-generated images contaminate online continual learning datasets, hindering performance. A new method, ESRM, leverages entropy and real/synthetic similarity maximization to select high-quality data&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lc8gemv97y/cover.png"/></item><item><title>Enriching Disentanglement: From Logical Definitions to Quantitative Metrics</title><link>https://deep-diver.github.io/neurips2024/posters/tvq3xckwbb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tvq3xckwbb/</guid><description>This paper presents a novel approach to deriving theoretically grounded disentanglement metrics by linking logical definitions to quantitative measures, offering strong theoretical guarantees and easi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tvq3xckwbb/cover.png"/></item><item><title>Fast Rates in Stochastic Online Convex Optimization by Exploiting the Curvature of Feasible Sets</title><link>https://deep-diver.github.io/neurips2024/posters/y58t1mqhh6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y58t1mqhh6/</guid><description>This paper introduces a novel approach for fast rates in online convex optimization by exploiting the curvature of feasible sets, achieving logarithmic regret bounds under specific conditions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y58t1mqhh6/cover.png"/></item><item><title>Generalizable and Animatable Gaussian Head Avatar</title><link>https://deep-diver.github.io/neurips2024/posters/gvm2az5xa6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gvm2az5xa6/</guid><description>One-shot animatable head avatar reconstruction is achieved using a novel dual-lifting method that generates 3D Gaussians from a single image, enabling real-time expression control and rendering with s&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gvm2az5xa6/cover.png"/></item><item><title>Generalization Bound and Learning Methods for Data-Driven Projections in Linear Programming</title><link>https://deep-diver.github.io/neurips2024/posters/jhh804fz5l/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jhh804fz5l/</guid><description>Learn to project, solve faster! This paper introduces data-driven projections for solving high-dimensional linear programs, proving theoretical guarantees and demonstrating significant improvements in&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jhh804fz5l/cover.png"/></item><item><title>Geometric-Averaged Preference Optimization for Soft Preference Labels</title><link>https://deep-diver.github.io/neurips2024/posters/3hpcvzv9it/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3hpcvzv9it/</guid><description>Improving LLM alignment, this paper introduces soft preference labels &amp;amp; geometric averaging in Direct Preference Optimization, consistently improving performance on standard benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3hpcvzv9it/cover.png"/></item><item><title>Integrating GNN and Neural ODEs for Estimating Non-Reciprocal Two-Body Interactions in Mixed-Species Collective Motion</title><link>https://deep-diver.github.io/neurips2024/posters/qwl3eidi9r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qwl3eidi9r/</guid><description>Deep learning framework integrating GNNs and neural ODEs precisely estimates non-reciprocal two-body interactions in mixed-species collective motion, accurately replicating both individual and collect&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qwl3eidi9r/cover.png"/></item><item><title>Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation</title><link>https://deep-diver.github.io/neurips2024/posters/1ihmhmhnya/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1ihmhmhnya/</guid><description>LLM agents effectively generate realistic personal mobility patterns using semantically rich data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1ihmhmhnya/cover.png"/></item><item><title>On the Minimax Regret for Contextual Linear Bandits and Multi-Armed Bandits with Expert Advice</title><link>https://deep-diver.github.io/neurips2024/posters/akipax5sxu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/akipax5sxu/</guid><description>This paper provides novel algorithms and matching lower bounds for multi-armed bandits with expert advice and contextual linear bandits, resolving open questions and advancing theoretical understandin&amp;hellip;</description></item><item><title>Risk-sensitive control as inference with R√©nyi divergence</title><link>https://deep-diver.github.io/neurips2024/posters/luixdwn6z5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/luixdwn6z5/</guid><description>Risk-sensitive control is recast as inference using R√©nyi divergence, yielding new algorithms and revealing equivalences between seemingly disparate methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/luixdwn6z5/cover.png"/></item><item><title>Taming the Long Tail in Human Mobility Prediction</title><link>https://deep-diver.github.io/neurips2024/posters/wt2tifhkp8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wt2tifhkp8/</guid><description>LoTNext framework tackles human mobility prediction&amp;rsquo;s long-tail problem by using graph and loss adjustments to improve the accuracy of predicting less-visited locations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wt2tifhkp8/cover.png"/></item><item><title>Transformers are Minimax Optimal Nonparametric In-Context Learners</title><link>https://deep-diver.github.io/neurips2024/posters/hf6vatntqc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hf6vatntqc/</guid><description>Transformers excel at in-context learning by leveraging minimax-optimal nonparametric learning, achieving near-optimal risk with sufficient pretraining data diversity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hf6vatntqc/cover.png"/></item><item><title>Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/1v4gksygfe/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1v4gksygfe/</guid><description>Linear probing then fine-tuning (LP-FT) significantly improves language model fine-tuning; this paper uses Neural Tangent Kernel (NTK) theory to explain why.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1v4gksygfe/cover.png"/></item><item><title>Understanding the Expressivity and Trainability of Fourier Neural Operator: A Mean-Field Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/qjr02btm7j/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qjr02btm7j/</guid><description>A mean-field theory explains Fourier Neural Operator (FNO) behavior, linking expressivity to trainability by identifying ordered and chaotic phases that correspond to vanishing or exploding gradients,&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qjr02btm7j/cover.png"/></item><item><title>Unveil Benign Overfitting for Transformer in Vision: Training Dynamics, Convergence, and Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/fgjb0pey4r/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fgjb0pey4r/</guid><description>Vision Transformers (ViTs) generalize surprisingly well, even when overfitting training data; this work provides the first theoretical explanation by characterizing the optimization dynamics of ViTs a&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fgjb0pey4r/cover.png"/></item><item><title>Wide Two-Layer Networks can Learn from Adversarial Perturbations</title><link>https://deep-diver.github.io/neurips2024/posters/1yggaouvgz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1yggaouvgz/</guid><description>Wide two-layer neural networks can generalize well from mislabeled adversarial examples because adversarial perturbations surprisingly contain sufficient class-specific features.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1yggaouvgz/cover.png"/></item></channel></rss>