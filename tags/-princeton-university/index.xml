<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ Princeton University on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-princeton-university/</link><description>Recent content in üè¢ Princeton University on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-princeton-university/index.xml" rel="self" type="application/rss+xml"/><item><title>A Theoretical Perspective for Speculative Decoding Algorithm</title><link>https://deep-diver.github.io/neurips2024/posters/wsqpnemvlu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wsqpnemvlu/</guid><description>This paper theoretically analyzes speculative decoding, revealing its optimality and providing formulas for expected rejections, paving the way for more efficient large language model inference.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wsqpnemvlu/cover.png"/></item><item><title>Achieving Optimal Clustering in Gaussian Mixture Models with Anisotropic Covariance Structures</title><link>https://deep-diver.github.io/neurips2024/oral-others/ge8gzn8gtu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-others/ge8gzn8gtu/</guid><description>This research develops rate-optimal clustering algorithms for Gaussian Mixture Models with anisotropic covariance structures, bridging the gap between theoretical guarantees and practical efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-others/ge8gzn8gtu/cover.png"/></item><item><title>Disentangling the Roles of Distinct Cell Classes with Cell-Type Dynamical Systems</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/9sp4oejtjb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/9sp4oejtjb/</guid><description>New Cell-Type Dynamical Systems (CTDS) model disentangles neural population dynamics by incorporating distinct cell types, improving prediction accuracy and biological interpretability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/9sp4oejtjb/cover.png"/></item><item><title>Finding Transformer Circuits With Edge Pruning</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/8osy3ra9jy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/8osy3ra9jy/</guid><description>Edge Pruning efficiently discovers sparse, yet accurate, computational subgraphs (circuits) in large language models via gradient-based edge pruning, advancing mechanistic interpretability research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/8osy3ra9jy/cover.png"/></item><item><title>Gradient Guidance for Diffusion Models: An Optimization Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/x1qeuybxke/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x1qeuybxke/</guid><description>This paper provides a novel optimization framework for guided diffusion models, proving √ï(1/K) convergence for concave objective functions and demonstrating structure-preserving guidance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x1qeuybxke/cover.png"/></item><item><title>GREATS: Online Selection of High-Quality Data for LLM Training in Every Iteration</title><link>https://deep-diver.github.io/neurips2024/spotlight-large-language-models/232vcn8tsx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-large-language-models/232vcn8tsx/</guid><description>GREATS: a novel online batch selection method significantly speeds up LLM training by greedily selecting high-quality data batches in every iteration, improving both convergence and generalization per&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-large-language-models/232vcn8tsx/cover.png"/></item><item><title>Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference</title><link>https://deep-diver.github.io/neurips2024/posters/pocs4jq7cv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pocs4jq7cv/</guid><description>Contrastive learning enables efficient probabilistic inference in high-dimensional time series by creating Gaussian representations that form a Gauss-Markov chain, allowing for closed-form solutions t&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pocs4jq7cv/cover.png"/></item><item><title>Kraken: Inherently Parallel Transformers For Efficient Multi-Device Inference</title><link>https://deep-diver.github.io/neurips2024/posters/jrtxzzk0a6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jrtxzzk0a6/</guid><description>Kraken: A new Transformer architecture boosts multi-device inference speed by 35.6% by cleverly overlapping communication with computation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jrtxzzk0a6/cover.png"/></item><item><title>Learning and Transferring Sparse Contextual Bigrams with Linear Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/pukavawybo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pukavawybo/</guid><description>Linear transformers efficiently learn sparse contextual bigrams by leveraging both in-context and global information, achieving polynomial sample complexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pukavawybo/cover.png"/></item><item><title>Learning Human-like Representations to Enable Learning Human Values</title><link>https://deep-diver.github.io/neurips2024/posters/sqapqmbqip/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sqapqmbqip/</guid><description>Aligning AI&amp;rsquo;s world representation with humans enables faster, safer learning of human values, improving both exploration and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sqapqmbqip/cover.png"/></item><item><title>Low-Rank Optimal Transport through Factor Relaxation with Latent Coupling</title><link>https://deep-diver.github.io/neurips2024/posters/hggkdff2hr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hggkdff2hr/</guid><description>FRLC: a novel algorithm for low-rank optimal transport using latent coupling, enabling faster computation and better interpretability for diverse applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hggkdff2hr/cover.png"/></item><item><title>One-Layer Transformer Provably Learns One-Nearest Neighbor In Context</title><link>https://deep-diver.github.io/neurips2024/posters/wdx45lnzxe/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wdx45lnzxe/</guid><description>One-layer transformers provably learn the one-nearest neighbor prediction rule, offering theoretical insights into their in-context learning capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wdx45lnzxe/cover.png"/></item><item><title>Probabilistic Federated Prompt-Tuning with Non-IID and Imbalanced Data</title><link>https://deep-diver.github.io/neurips2024/posters/nw6ansc66g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nw6ansc66g/</guid><description>Probabilistic Federated Prompt Tuning (PFPT) significantly improves federated learning accuracy on heterogeneous and imbalanced data by using a probabilistic model for prompt aggregation, outperformin&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nw6ansc66g/cover.png"/></item><item><title>SureMap: Simultaneous mean estimation for single-task and multi-task disaggregated evaluation</title><link>https://deep-diver.github.io/neurips2024/posters/atnt3fuvbg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/atnt3fuvbg/</guid><description>SureMap, a new method, significantly boosts accuracy in single and multi-task disaggregated evaluations of AI models using limited data by transforming the problem into Gaussian mean estimation and cl&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/atnt3fuvbg/cover.png"/></item><item><title>SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering</title><link>https://deep-diver.github.io/neurips2024/posters/mxpq6ut8j3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mxpq6ut8j3/</guid><description>SWE-agent achieves state-of-the-art performance on software engineering benchmarks by creating a custom agent-computer interface that enhances LM agents&amp;rsquo; ability to use computers.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mxpq6ut8j3/cover.png"/></item><item><title>The Road Less Scheduled</title><link>https://deep-diver.github.io/neurips2024/oral-others/0xenkkenui/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-others/0xenkkenui/</guid><description>Revolutionizing machine learning, Schedule-Free optimization achieves state-of-the-art results without needing learning rate schedules, simplifying training and improving efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-others/0xenkkenui/cover.png"/></item><item><title>Training Dynamics of Transformers to Recognize Word Co-occurrence via Gradient Flow Analysis</title><link>https://deep-diver.github.io/neurips2024/posters/w6q46islsr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w6q46islsr/</guid><description>Researchers reveal how transformers learn word co-occurrence using a novel gradient flow analysis, uncovering a two-phase training process that leads to near-minimum loss and improved model performanc&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w6q46islsr/cover.png"/></item><item><title>Understanding the Limits of Vision Language Models Through the Lens of the Binding Problem</title><link>https://deep-diver.github.io/neurips2024/posters/q5ryn6jagc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/q5ryn6jagc/</guid><description>Vision-language models struggle with multi-object reasoning due to the binding problem; this paper reveals human-like capacity limits in VLMs and proposes solutions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/q5ryn6jagc/cover.png"/></item><item><title>When Is Inductive Inference Possible?</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/2agcshccuv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/2agcshccuv/</guid><description>This paper provides a tight characterization of inductive inference, proving it&amp;rsquo;s possible if and only if the hypothesis class is a countable union of online learnable classes, resolving a long-standi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/2agcshccuv/cover.png"/></item></channel></rss>