<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ Princeton University on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-princeton-university/</link><description>Recent content in üè¢ Princeton University on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2025 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-princeton-university/index.xml" rel="self" type="application/rss+xml"/><item><title>A Metalearned Neural Circuit for Nonparametric Bayesian Inference</title><link>https://deep-diver.github.io/neurips2024/posters/cp7hd618bd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cp7hd618bd/</guid><description>Metalearning a neural circuit mimics nonparametric Bayesian inference, enabling fast, accurate, open-set classification.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cp7hd618bd/cover.png"/></item><item><title>A Theoretical Perspective for Speculative Decoding Algorithm</title><link>https://deep-diver.github.io/neurips2024/posters/wsqpnemvlu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wsqpnemvlu/</guid><description>This paper theoretically analyzes speculative decoding, revealing its optimality and providing formulas for expected rejections, paving the way for more efficient large language model inference.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wsqpnemvlu/cover.png"/></item><item><title>Achieving Optimal Clustering in Gaussian Mixture Models with Anisotropic Covariance Structures</title><link>https://deep-diver.github.io/neurips2024/oral-others/ge8gzn8gtu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-others/ge8gzn8gtu/</guid><description>This research develops rate-optimal clustering algorithms for Gaussian Mixture Models with anisotropic covariance structures, bridging the gap between theoretical guarantees and practical efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-others/ge8gzn8gtu/cover.png"/></item><item><title>Can Models Learn Skill Composition from Examples?</title><link>https://deep-diver.github.io/neurips2024/posters/1sldprsbmk/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1sldprsbmk/</guid><description>Smaller language models can learn skill composition from limited examples, substantially improving their ability to combine skills in novel ways through fine-tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1sldprsbmk/cover.png"/></item><item><title>Disentangling the Roles of Distinct Cell Classes with Cell-Type Dynamical Systems</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/9sp4oejtjb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/9sp4oejtjb/</guid><description>New Cell-Type Dynamical Systems (CTDS) model disentangles neural population dynamics by incorporating distinct cell types, improving prediction accuracy and biological interpretability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/9sp4oejtjb/cover.png"/></item><item><title>Finding Transformer Circuits With Edge Pruning</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/8osy3ra9jy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/8osy3ra9jy/</guid><description>Edge Pruning efficiently discovers sparse, yet accurate, computational subgraphs (circuits) in large language models via gradient-based edge pruning, advancing mechanistic interpretability research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/8osy3ra9jy/cover.png"/></item><item><title>FlexSBDD: Structure-Based Drug Design with Flexible Protein Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/4ab54h21qg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4ab54h21qg/</guid><description>FlexSBDD, a novel deep generative model, accurately predicts flexible protein-ligand complex structures, generating high-affinity drug molecules while overcoming the limitations of rigid protein model&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4ab54h21qg/cover.png"/></item><item><title>Global Convergence in Training Large-Scale Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/9wtlfrkwzs/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/9wtlfrkwzs/</guid><description>Large-scale Transformer training&amp;rsquo;s global convergence is proven using weight decay regularization and a refined mean-field analysis, bridging theory and practice.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/9wtlfrkwzs/cover.png"/></item><item><title>Gradient Guidance for Diffusion Models: An Optimization Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/x1qeuybxke/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x1qeuybxke/</guid><description>This paper provides a novel optimization framework for guided diffusion models, proving √ï(1/K) convergence for concave objective functions and demonstrating structure-preserving guidance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x1qeuybxke/cover.png"/></item><item><title>GREATS: Online Selection of High-Quality Data for LLM Training in Every Iteration</title><link>https://deep-diver.github.io/neurips2024/spotlight-large-language-models/232vcn8tsx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-large-language-models/232vcn8tsx/</guid><description>GREATS: a novel online batch selection method significantly speeds up LLM training by greedily selecting high-quality data batches in every iteration, improving both convergence and generalization per&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-large-language-models/232vcn8tsx/cover.png"/></item><item><title>Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference</title><link>https://deep-diver.github.io/neurips2024/posters/pocs4jq7cv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pocs4jq7cv/</guid><description>Contrastive learning enables efficient probabilistic inference in high-dimensional time series by creating Gaussian representations that form a Gauss-Markov chain, allowing for closed-form solutions t&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pocs4jq7cv/cover.png"/></item><item><title>Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity</title><link>https://deep-diver.github.io/neurips2024/posters/4tlue0ufiz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/4tlue0ufiz/</guid><description>Robots using LLMs for task planning often make unsafe or wrong decisions due to LLM hallucination and ambiguity in instructions. This paper introduces &amp;lsquo;introspective planning,&amp;rsquo; a novel method that us&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/4tlue0ufiz/cover.png"/></item><item><title>Kraken: Inherently Parallel Transformers For Efficient Multi-Device Inference</title><link>https://deep-diver.github.io/neurips2024/posters/jrtxzzk0a6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jrtxzzk0a6/</guid><description>Kraken: A new Transformer architecture boosts multi-device inference speed by 35.6% by cleverly overlapping communication with computation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jrtxzzk0a6/cover.png"/></item><item><title>Learning and Transferring Sparse Contextual Bigrams with Linear Transformers</title><link>https://deep-diver.github.io/neurips2024/posters/pukavawybo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pukavawybo/</guid><description>Linear transformers efficiently learn sparse contextual bigrams by leveraging both in-context and global information, achieving polynomial sample complexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pukavawybo/cover.png"/></item><item><title>Learning Human-like Representations to Enable Learning Human Values</title><link>https://deep-diver.github.io/neurips2024/posters/sqapqmbqip/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sqapqmbqip/</guid><description>Aligning AI&amp;rsquo;s world representation with humans enables faster, safer learning of human values, improving both exploration and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sqapqmbqip/cover.png"/></item><item><title>Low-Rank Optimal Transport through Factor Relaxation with Latent Coupling</title><link>https://deep-diver.github.io/neurips2024/posters/hggkdff2hr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hggkdff2hr/</guid><description>FRLC: a novel algorithm for low-rank optimal transport using latent coupling, enabling faster computation and better interpretability for diverse applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hggkdff2hr/cover.png"/></item><item><title>Neural network learns low-dimensional polynomials with SGD near the information-theoretic limit</title><link>https://deep-diver.github.io/neurips2024/posters/qk4is49kdm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qk4is49kdm/</guid><description>SGD can train neural networks to learn low-dimensional polynomials near the information-theoretic limit, surpassing previous correlational statistical query lower bounds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qk4is49kdm/cover.png"/></item><item><title>One-Layer Transformer Provably Learns One-Nearest Neighbor In Context</title><link>https://deep-diver.github.io/neurips2024/posters/wdx45lnzxe/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wdx45lnzxe/</guid><description>One-layer transformers provably learn the one-nearest neighbor prediction rule, offering theoretical insights into their in-context learning capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wdx45lnzxe/cover.png"/></item><item><title>Optimal Aggregation of Prediction Intervals under Unsupervised Domain Shift</title><link>https://deep-diver.github.io/neurips2024/posters/ldxynsvxer/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ldxynsvxer/</guid><description>This paper introduces a novel method for creating highly accurate and narrow prediction intervals even when data distribution shifts unexpectedly, significantly improving machine learning model reliab&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ldxynsvxer/cover.png"/></item><item><title>Probabilistic Federated Prompt-Tuning with Non-IID and Imbalanced Data</title><link>https://deep-diver.github.io/neurips2024/posters/nw6ansc66g/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nw6ansc66g/</guid><description>Probabilistic Federated Prompt Tuning (PFPT) significantly improves federated learning accuracy on heterogeneous and imbalanced data by using a probabilistic model for prompt aggregation, outperformin&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nw6ansc66g/cover.png"/></item><item><title>SimPO: Simple Preference Optimization with a Reference-Free Reward</title><link>https://deep-diver.github.io/neurips2024/posters/3tzcot1lkb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3tzcot1lkb/</guid><description>SimPO: a simpler, reference-free reward algorithm significantly outperforming existing offline preference optimization methods, achieving higher accuracy and efficiency in aligning LLMs with human pre&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3tzcot1lkb/cover.png"/></item><item><title>SureMap: Simultaneous mean estimation for single-task and multi-task disaggregated evaluation</title><link>https://deep-diver.github.io/neurips2024/posters/atnt3fuvbg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/atnt3fuvbg/</guid><description>SureMap, a new method, significantly boosts accuracy in single and multi-task disaggregated evaluations of AI models using limited data by transforming the problem into Gaussian mean estimation and cl&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/atnt3fuvbg/cover.png"/></item><item><title>SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering</title><link>https://deep-diver.github.io/neurips2024/posters/mxpq6ut8j3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mxpq6ut8j3/</guid><description>SWE-agent achieves state-of-the-art performance on software engineering benchmarks by creating a custom agent-computer interface that enhances LM agents&amp;rsquo; ability to use computers.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mxpq6ut8j3/cover.png"/></item><item><title>The Road Less Scheduled</title><link>https://deep-diver.github.io/neurips2024/oral-others/0xenkkenui/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-others/0xenkkenui/</guid><description>Revolutionizing machine learning, Schedule-Free optimization achieves state-of-the-art results without needing learning rate schedules, simplifying training and improving efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-others/0xenkkenui/cover.png"/></item><item><title>Tight Rates for Bandit Control Beyond Quadratics</title><link>https://deep-diver.github.io/neurips2024/posters/mlm3nuwoeq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mlm3nuwoeq/</guid><description>This paper presents an algorithm achieving √ï(‚àöT) optimal regret for bandit non-stochastic control with strongly-convex and smooth cost functions, overcoming prior limitations of suboptimal bounds.</description></item><item><title>Training Dynamics of Transformers to Recognize Word Co-occurrence via Gradient Flow Analysis</title><link>https://deep-diver.github.io/neurips2024/posters/w6q46islsr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w6q46islsr/</guid><description>Researchers reveal how transformers learn word co-occurrence using a novel gradient flow analysis, uncovering a two-phase training process that leads to near-minimum loss and improved model performanc&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w6q46islsr/cover.png"/></item><item><title>Understanding the Limits of Vision Language Models Through the Lens of the Binding Problem</title><link>https://deep-diver.github.io/neurips2024/posters/q5ryn6jagc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/q5ryn6jagc/</guid><description>Vision-language models struggle with multi-object reasoning due to the binding problem; this paper reveals human-like capacity limits in VLMs and proposes solutions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/q5ryn6jagc/cover.png"/></item><item><title>When Is Inductive Inference Possible?</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/2agcshccuv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/2agcshccuv/</guid><description>This paper provides a tight characterization of inductive inference, proving it&amp;rsquo;s possible if and only if the hypothesis class is a countable union of online learnable classes, resolving a long-standi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/2agcshccuv/cover.png"/></item></channel></rss>