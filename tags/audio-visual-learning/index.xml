<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Audio-Visual Learning on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/audio-visual-learning/</link><description>Recent content in Audio-Visual Learning on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/audio-visual-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>A Versatile Diffusion Transformer with Mixture of Noise Levels for Audiovisual Generation</title><link>https://deep-diver.github.io/neurips2024/posters/cs1hisjklu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cs1hisjklu/</guid><description>A single model tackles diverse audiovisual generation tasks using a novel Mixture of Noise Levels approach, resulting in temporally consistent and high-quality outputs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cs1hisjklu/cover.png"/></item><item><title>Aligning Audio-Visual Joint Representations with an Agentic Workflow</title><link>https://deep-diver.github.io/neurips2024/posters/qmals4vey3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qmals4vey3/</guid><description>AVAgent uses an LLM-driven workflow to intelligently align audio and visual data, resulting in improved AV joint representations and state-of-the-art performance on various downstream tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qmals4vey3/cover.png"/></item><item><title>AV-Cloud: Spatial Audio Rendering Through Audio-Visual Cloud Splatting</title><link>https://deep-diver.github.io/neurips2024/posters/yxorsms5wr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yxorsms5wr/</guid><description>AV-Cloud: Real-time, high-quality 3D spatial audio rendering synced with visuals, bypassing pre-rendered images for immersive virtual experiences.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yxorsms5wr/cover.png"/></item><item><title>Continual Audio-Visual Sound Separation</title><link>https://deep-diver.github.io/neurips2024/posters/pzciwtqjaw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pzciwtqjaw/</guid><description>ContAV-Sep: a novel approach to continual audio-visual sound separation, effectively mitigating catastrophic forgetting and improving model adaptability by preserving cross-modal semantic similarity a&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pzciwtqjaw/cover.png"/></item><item><title>DARNet: Dual Attention Refinement Network with Spatiotemporal Construction for Auditory Attention Detection</title><link>https://deep-diver.github.io/neurips2024/posters/jwggedyors/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jwggedyors/</guid><description>DARNet: a dual attention network for auditory attention detection surpasses current state-of-the-art models, especially in short decision windows, achieving this with a 91% reduction in parameters.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jwggedyors/cover.png"/></item><item><title>Learning Spatially-Aware Language and Audio Embeddings</title><link>https://deep-diver.github.io/neurips2024/posters/wddvjzvvbr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wddvjzvvbr/</guid><description>ELSA: a new model that learns spatially aware language and audio embeddings, achieving state-of-the-art performance in semantic retrieval and 3D sound source localization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wddvjzvvbr/cover.png"/></item><item><title>Listenable Maps for Zero-Shot Audio Classifiers</title><link>https://deep-diver.github.io/neurips2024/posters/lv1wghkd5x/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lv1wghkd5x/</guid><description>LMAC-ZS: First decoder-based method for explaining zero-shot audio classifiers, ensuring transparency and trustworthiness in AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lv1wghkd5x/cover.png"/></item><item><title>Mixtures of Experts for Audio-Visual Learning</title><link>https://deep-diver.github.io/neurips2024/posters/snmukbu0am/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/snmukbu0am/</guid><description>AVMoE: a novel parameter-efficient transfer learning approach for audio-visual learning, dynamically allocates expert models (unimodal and cross-modal adapters) based on task demands, achieving superi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/snmukbu0am/cover.png"/></item><item><title>Tell What You Hear From What You See - Video to Audio Generation Through Text</title><link>https://deep-diver.github.io/neurips2024/posters/kr7en85mit/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kr7en85mit/</guid><description>VATT: Text-guided video-to-audio generation, enabling refined audio control via text prompts and improved compatibility.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kr7en85mit/cover.png"/></item><item><title>Unified Speech Recognition: A Single Model for Auditory, Visual, and Audiovisual Inputs</title><link>https://deep-diver.github.io/neurips2024/posters/vwsll6m9pj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vwsll6m9pj/</guid><description>One model to rule them all! This paper introduces Unified Speech Recognition (USR), a single model trained for auditory, visual, and audiovisual speech recognition, achieving state-of-the-art results &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vwsll6m9pj/cover.png"/></item></channel></rss>