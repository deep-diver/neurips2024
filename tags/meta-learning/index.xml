<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Meta Learning on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/meta-learning/</link><description>Recent content in Meta Learning on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/meta-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>A Framework for Bilevel Optimization on Riemannian Manifolds</title><link>https://deep-diver.github.io/neurips2024/posters/lvndqnjkld/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lvndqnjkld/</guid><description>This paper introduces a novel framework for bilevel optimization on Riemannian manifolds, providing efficient hypergradient estimation strategies and convergence analysis, with successful applications&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lvndqnjkld/cover.png"/></item><item><title>An Accelerated Algorithm for Stochastic Bilevel Optimization under Unbounded Smoothness</title><link>https://deep-diver.github.io/neurips2024/posters/v7vyvvmfru/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v7vyvvmfru/</guid><description>AccBO: A new accelerated algorithm achieves O(ε⁻³) oracle complexity for stochastic bilevel optimization with unbounded smoothness, significantly improving upon existing O(ε⁻⁴) methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v7vyvvmfru/cover.png"/></item><item><title>Discovering plasticity rules that organize and maintain neural circuits</title><link>https://deep-diver.github.io/neurips2024/posters/nw4twuepgx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nw4twuepgx/</guid><description>AI discovers robust, biologically-plausible plasticity rules that self-organize and maintain neural circuits&amp;rsquo; sequential activity, even with synaptic turnover.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nw4twuepgx/cover.png"/></item><item><title>Fairness-Aware Meta-Learning via Nash Bargaining</title><link>https://deep-diver.github.io/neurips2024/posters/egjnb3tugv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/egjnb3tugv/</guid><description>Nash bargaining resolves hypergradient conflicts in fairness-aware meta-learning, boosting model performance and fairness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/egjnb3tugv/cover.png"/></item><item><title>FasMe: Fast and Sample-efficient Meta Estimator for Precision Matrix Learning in Small Sample Settings</title><link>https://deep-diver.github.io/neurips2024/posters/whfaah3e8z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/whfaah3e8z/</guid><description>FasMe: a novel meta-learning approach delivers fast and sample-efficient precision matrix estimation, surpassing existing methods in accuracy and speed for small sample datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/whfaah3e8z/cover.png"/></item><item><title>In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/lfxiasylxb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/lfxiasylxb/</guid><description>Softmax attention in transformers adapts its attention window to function Lipschitzness and noise, enabling efficient in-context learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/lfxiasylxb/cover.png"/></item><item><title>Linear Regression using Heterogeneous Data Batches</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/4g2dn4kjk1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/4g2dn4kjk1/</guid><description>New algorithm efficiently solves linear regression with heterogeneous data batches, handling diverse input distributions and achieving high accuracy with fewer samples.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/4g2dn4kjk1/cover.png"/></item><item><title>Model Based Inference of Synaptic Plasticity Rules</title><link>https://deep-diver.github.io/neurips2024/posters/ri80phlnfm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ri80phlnfm/</guid><description>New computational method infers complex brain learning rules from experimental data, revealing active forgetting in reward learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ri80phlnfm/cover.png"/></item><item><title>On the Identifiability of Hybrid Deep Generative Models: Meta-Learning as a Solution</title><link>https://deep-diver.github.io/neurips2024/posters/sxy1nvgyo7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sxy1nvgyo7/</guid><description>Meta-learning solves hybrid deep generative model unidentifiability!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sxy1nvgyo7/cover.png"/></item><item><title>SPARKLE: A Unified Single-Loop Primal-Dual Framework for Decentralized Bilevel Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/g5dyqerupx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/g5dyqerupx/</guid><description>SPARKLE: A single-loop primal-dual framework unifies decentralized bilevel optimization, enabling flexible heterogeneity-correction and mixed update strategies for improved convergence.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/g5dyqerupx/cover.png"/></item></channel></rss>