<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Meta Learning on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/meta-learning/</link><description>Recent content in Meta Learning on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/meta-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>A Framework for Bilevel Optimization on Riemannian Manifolds</title><link>https://deep-diver.github.io/neurips2024/posters/lvndqnjkld/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lvndqnjkld/</guid><description>This paper introduces a novel framework for bilevel optimization on Riemannian manifolds, providing efficient hypergradient estimation strategies and convergence analysis, with successful applications&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lvndqnjkld/cover.png"/></item><item><title>A Metalearned Neural Circuit for Nonparametric Bayesian Inference</title><link>https://deep-diver.github.io/neurips2024/posters/cp7hd618bd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cp7hd618bd/</guid><description>Metalearning a neural circuit mimics nonparametric Bayesian inference, enabling fast, accurate, open-set classification.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cp7hd618bd/cover.png"/></item><item><title>Addressing Hidden Confounding with Heterogeneous Observational Datasets for Recommendation</title><link>https://deep-diver.github.io/neurips2024/posters/6cfhg7exjy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/6cfhg7exjy/</guid><description>MetaDebias tackles hidden confounding in recommender systems using heterogeneous observational data, achieving state-of-the-art performance without expensive RCT data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/6cfhg7exjy/cover.png"/></item><item><title>Advancing Open-Set Domain Generalization Using Evidential Bi-Level Hardest Domain Scheduler</title><link>https://deep-diver.github.io/neurips2024/posters/c3jcwbmxbu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/c3jcwbmxbu/</guid><description>EBiL-HaDS, a novel adaptive domain scheduler, significantly boosts open-set domain generalization by strategically sequencing training domains based on model reliability assessments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/c3jcwbmxbu/cover.png"/></item><item><title>An Accelerated Algorithm for Stochastic Bilevel Optimization under Unbounded Smoothness</title><link>https://deep-diver.github.io/neurips2024/posters/v7vyvvmfru/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v7vyvvmfru/</guid><description>AccBO: A new accelerated algorithm achieves O(ε⁻³) oracle complexity for stochastic bilevel optimization with unbounded smoothness, significantly improving upon existing O(ε⁻⁴) methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v7vyvvmfru/cover.png"/></item><item><title>Boosting Generalization in Parametric PDE Neural Solvers through Adaptive Conditioning</title><link>https://deep-diver.github.io/neurips2024/posters/guy0zb2xvu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/guy0zb2xvu/</guid><description>GEPS enhances parametric PDE solver generalization by using adaptive conditioning, achieving superior performance with limited data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/guy0zb2xvu/cover.png"/></item><item><title>Discovering plasticity rules that organize and maintain neural circuits</title><link>https://deep-diver.github.io/neurips2024/posters/nw4twuepgx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nw4twuepgx/</guid><description>AI discovers robust, biologically-plausible plasticity rules that self-organize and maintain neural circuits&amp;rsquo; sequential activity, even with synaptic turnover.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nw4twuepgx/cover.png"/></item><item><title>Fairness-Aware Meta-Learning via Nash Bargaining</title><link>https://deep-diver.github.io/neurips2024/posters/egjnb3tugv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/egjnb3tugv/</guid><description>Nash bargaining resolves hypergradient conflicts in fairness-aware meta-learning, boosting model performance and fairness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/egjnb3tugv/cover.png"/></item><item><title>FasMe: Fast and Sample-efficient Meta Estimator for Precision Matrix Learning in Small Sample Settings</title><link>https://deep-diver.github.io/neurips2024/posters/whfaah3e8z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/whfaah3e8z/</guid><description>FasMe: a novel meta-learning approach delivers fast and sample-efficient precision matrix estimation, surpassing existing methods in accuracy and speed for small sample datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/whfaah3e8z/cover.png"/></item><item><title>First-Order Minimax Bilevel Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/gzoauvskaw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gzoauvskaw/</guid><description>Two novel first-order algorithms, FOSL and MemCS, efficiently solve multi-block minimax bilevel optimization problems, significantly improving performance in deep AUC maximization and robust meta-lear&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gzoauvskaw/cover.png"/></item><item><title>In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/lfxiasylxb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/lfxiasylxb/</guid><description>Softmax attention in transformers adapts its attention window to function Lipschitzness and noise, enabling efficient in-context learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/lfxiasylxb/cover.png"/></item><item><title>Learning via Surrogate PAC-Bayes</title><link>https://deep-diver.github.io/neurips2024/posters/ieyxwuxaqt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ieyxwuxaqt/</guid><description>Surrogate PAC-Bayes Learning (SuPAC) efficiently optimizes generalization bounds by iteratively optimizing surrogate training objectives, enabling faster and more scalable learning for complex models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ieyxwuxaqt/cover.png"/></item><item><title>Linear Regression using Heterogeneous Data Batches</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/4g2dn4kjk1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/4g2dn4kjk1/</guid><description>New algorithm efficiently solves linear regression with heterogeneous data batches, handling diverse input distributions and achieving high accuracy with fewer samples.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/4g2dn4kjk1/cover.png"/></item><item><title>Memory-Efficient Gradient Unrolling for Large-Scale Bi-level Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/mi8z9gutin/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mi8z9gutin/</guid><description>FG²U: a novel memory-efficient algorithm for unbiased stochastic approximation of meta-gradients in large-scale bi-level optimization, showing superior performance across diverse tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mi8z9gutin/cover.png"/></item><item><title>Meta-Learning Universal Priors Using Non-Injective Change of Variables</title><link>https://deep-diver.github.io/neurips2024/posters/e8b4yolgz5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/e8b4yolgz5/</guid><description>MetaNCoV: Learn data-driven priors via non-injective change of variables for enhanced few-shot learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/e8b4yolgz5/cover.png"/></item><item><title>Model Based Inference of Synaptic Plasticity Rules</title><link>https://deep-diver.github.io/neurips2024/posters/ri80phlnfm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ri80phlnfm/</guid><description>New computational method infers complex brain learning rules from experimental data, revealing active forgetting in reward learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ri80phlnfm/cover.png"/></item><item><title>On the Identifiability of Hybrid Deep Generative Models: Meta-Learning as a Solution</title><link>https://deep-diver.github.io/neurips2024/posters/sxy1nvgyo7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sxy1nvgyo7/</guid><description>Meta-learning solves hybrid deep generative model unidentifiability!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sxy1nvgyo7/cover.png"/></item><item><title>On the Stability and Generalization of Meta-Learning</title><link>https://deep-diver.github.io/neurips2024/posters/j8row29df2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/j8row29df2/</guid><description>This paper introduces uniform meta-stability for meta-learning, providing tighter generalization bounds for convex and weakly-convex problems, addressing computational limitations of existing algorith&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/j8row29df2/cover.png"/></item><item><title>SPARKLE: A Unified Single-Loop Primal-Dual Framework for Decentralized Bilevel Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/g5dyqerupx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/g5dyqerupx/</guid><description>SPARKLE: A single-loop primal-dual framework unifies decentralized bilevel optimization, enabling flexible heterogeneity-correction and mixed update strategies for improved convergence.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/g5dyqerupx/cover.png"/></item><item><title>Theoretical Investigations and Practical Enhancements on Tail Task Risk Minimization in Meta Learning</title><link>https://deep-diver.github.io/neurips2024/posters/mcrzoo0hwr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mcrzoo0hwr/</guid><description>This research enhances meta-learning robustness by theoretically grounding and practically improving tail-risk minimization, achieving improved fast adaptation in the task space.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mcrzoo0hwr/cover.png"/></item><item><title>Transformers are Minimax Optimal Nonparametric In-Context Learners</title><link>https://deep-diver.github.io/neurips2024/posters/hf6vatntqc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hf6vatntqc/</guid><description>Transformers excel at in-context learning by leveraging minimax-optimal nonparametric learning, achieving near-optimal risk with sufficient pretraining data diversity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hf6vatntqc/cover.png"/></item></channel></rss>