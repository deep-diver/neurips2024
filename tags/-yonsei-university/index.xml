<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ Yonsei University on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-yonsei-university/</link><description>Recent content in üè¢ Yonsei University on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2025 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-yonsei-university/index.xml" rel="self" type="application/rss+xml"/><item><title>ANT: Adaptive Noise Schedule for Time Series Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/posters/1ojaktylz4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/1ojaktylz4/</guid><description>ANT: An adaptive noise schedule automatically determines optimal noise schedules for time series diffusion models, significantly boosting performance across diverse tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/1ojaktylz4/cover.png"/></item><item><title>Graph Convolutions Enrich the Self-Attention in Transformers!</title><link>https://deep-diver.github.io/neurips2024/posters/ffnrpcbpi6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ffnrpcbpi6/</guid><description>Graph Filter-based Self-Attention (GFSA) enhances Transformers by addressing oversmoothing, boosting performance across various tasks with minimal added parameters.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ffnrpcbpi6/cover.png"/></item><item><title>Train-Attention: Meta-Learning Where to Focus in Continual Knowledge Learning</title><link>https://deep-diver.github.io/neurips2024/posters/pwldvyimrf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pwldvyimrf/</guid><description>Train-Attention (TAALM) tackles catastrophic forgetting in LLMs by dynamically weighting tokens during training, boosting learning efficiency and knowledge retention, outperforming existing methods on&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pwldvyimrf/cover.png"/></item></channel></rss>