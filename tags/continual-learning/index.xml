<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Continual Learning on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/continual-learning/</link><description>Recent content in Continual Learning on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2025 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/continual-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Continual Learning in the Frequency Domain</title><link>https://deep-diver.github.io/neurips2024/posters/xgazclsjaq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xgazclsjaq/</guid><description>Boost continual learning efficiency with CLFD: a novel frequency domain approach that improves accuracy by up to 6.83% and slashes training time by 2.6x on edge devices!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xgazclsjaq/cover.png"/></item><item><title>Forgetting, Ignorance or Myopia: Revisiting Key Challenges in Online Continual Learning</title><link>https://deep-diver.github.io/neurips2024/posters/oparhdvqrd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oparhdvqrd/</guid><description>NsCE framework tackles key OCL challenges: model ignorance (learning effective features in limited time) and myopia (overly simplified features). NsCE integrates non-sparse maximum separation regulari&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oparhdvqrd/cover.png"/></item><item><title>GACL: Exemplar-Free Generalized Analytic Continual Learning</title><link>https://deep-diver.github.io/neurips2024/posters/p6aj7bqylc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/p6aj7bqylc/</guid><description>GACL: a novel exemplar-free technique for generalized analytic continual learning, achieves superior performance by analytically solving the weight-invariant property for handling real-world data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/p6aj7bqylc/cover.png"/></item><item><title>Label Delay in Online Continual Learning</title><link>https://deep-diver.github.io/neurips2024/posters/m5canuui0z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/m5canuui0z/</guid><description>Bridging the accuracy gap in online continual learning caused by label delays, a new framework with Importance Weighted Memory Sampling prioritizes relevant memory samples, significantly outperforming&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/m5canuui0z/cover.png"/></item><item><title>Make Continual Learning Stronger via C-Flat</title><link>https://deep-diver.github.io/neurips2024/posters/dokew2u49m/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dokew2u49m/</guid><description>Boost continual learning with C-Flat: a novel, one-line-code optimizer creating flatter loss landscapes for enhanced stability and generalization across various continual learning scenarios.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dokew2u49m/cover.png"/></item><item><title>Model Sensitivity Aware Continual Learning</title><link>https://deep-diver.github.io/neurips2024/posters/b5vq7iqw7d/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/b5vq7iqw7d/</guid><description>Model Sensitivity Aware Continual Learning (MACL) tackles the CL challenge by optimizing model performance based on parameter distribution, achieving superior old knowledge retention and new task perf&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/b5vq7iqw7d/cover.png"/></item><item><title>Replay-and-Forget-Free Graph Class-Incremental Learning: A Task Profiling and Prompting Approach</title><link>https://deep-diver.github.io/neurips2024/posters/fxdmgfcder/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fxdmgfcder/</guid><description>Forget-free graph class-incremental learning achieved via a novel task profiling and prompting approach, significantly outperforming state-of-the-art methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fxdmgfcder/cover.png"/></item><item><title>SAFE: Slow and Fast Parameter-Efﬁcient Tuning for Continual Learning with Pre-Trained Models</title><link>https://deep-diver.github.io/neurips2024/posters/cjnirz5pan/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cjnirz5pan/</guid><description>SAFE, a novel parameter-efficient tuning framework, boosts pre-trained model performance in continual learning by balancing model stability and plasticity through slow and fast learning stages, signif&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cjnirz5pan/cover.png"/></item><item><title>Task-recency bias strikes back: Adapting covariances in Exemplar-Free Class Incremental Learning</title><link>https://deep-diver.github.io/neurips2024/posters/5h4l37isz8/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/5h4l37isz8/</guid><description>AdaGauss tackles task-recency bias in exemplar-free class incremental learning by adapting class covariances and introducing an anti-collapse loss, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/5h4l37isz8/cover.png"/></item><item><title>Vector Quantization Prompting for Continual Learning</title><link>https://deep-diver.github.io/neurips2024/posters/accqglviig/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/accqglviig/</guid><description>VQ-Prompt uses vector quantization to optimize discrete prompts for continual learning, achieving state-of-the-art performance by effectively abstracting task knowledge and optimizing prompt selection&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/accqglviig/cover.png"/></item></channel></rss>