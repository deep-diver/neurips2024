<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ State Key Laboratory for Novel Software Technology, Nanjing University on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-state-key-laboratory-for-novel-software-technology-nanjing-university/</link><description>Recent content in üè¢ State Key Laboratory for Novel Software Technology, Nanjing University on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-state-key-laboratory-for-novel-software-technology-nanjing-university/index.xml" rel="self" type="application/rss+xml"/><item><title>A Prompt-Based Knowledge Graph Foundation Model for Universal In-Context Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/vqyb9lkmuh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vqyb9lkmuh/</guid><description>KG-ICL, a novel prompt-based knowledge graph foundation model, achieves universal in-context reasoning by leveraging in-context learning and a unified tokenizer, outperforming various baselines on 43 &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vqyb9lkmuh/cover.png"/></item><item><title>AP-Adapter: Improving Generalization of Automatic Prompts on Unseen Text-to-Image Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/posters/46v9axmouu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/46v9axmouu/</guid><description>AP-Adapter boosts text-to-image diffusion model generalization by using a two-stage prompt optimization method that leverages large language models and inter-model differences.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/46v9axmouu/cover.png"/></item><item><title>AWT: Transferring Vision-Language Models via Augmentation, Weighting, and Transportation</title><link>https://deep-diver.github.io/neurips2024/posters/yiyww1d3le/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yiyww1d3le/</guid><description>AWT: a novel framework boosts vision-language model&amp;rsquo;s zero-shot capabilities by augmenting inputs, weighting them dynamically, and leveraging optimal transport to enhance semantic correlations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yiyww1d3le/cover.png"/></item><item><title>Bridge the Modality and Capability Gaps in Vision-Language Model Selection</title><link>https://deep-diver.github.io/neurips2024/posters/01qa1zjs65/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/01qa1zjs65/</guid><description>SWAB bridges modality and capability gaps in Vision-Language Model selection using optimal transport, enabling accurate prediction of VLM performance without images.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/01qa1zjs65/cover.png"/></item></channel></rss>