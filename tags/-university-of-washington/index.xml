<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ University of Washington on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-university-of-washington/</link><description>Recent content in üè¢ University of Washington on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-university-of-washington/index.xml" rel="self" type="application/rss+xml"/><item><title>A Near-optimal Algorithm for Learning Margin Halfspaces with Massart Noise</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/4aewzkwb5z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/4aewzkwb5z/</guid><description>Near-optimal algorithm achieves computationally efficient learning of margin halfspaces with Massart noise, nearly matching theoretical lower bounds.</description></item><item><title>AV-Cloud: Spatial Audio Rendering Through Audio-Visual Cloud Splatting</title><link>https://deep-diver.github.io/neurips2024/posters/yxorsms5wr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/yxorsms5wr/</guid><description>AV-Cloud: Real-time, high-quality 3D spatial audio rendering synced with visuals, bypassing pre-rendered images for immersive virtual experiences.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/yxorsms5wr/cover.png"/></item><item><title>CLIPLoss and Norm-Based Data Selection Methods for Multimodal Contrastive Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/qvdc0ocx2n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/qvdc0ocx2n/</guid><description>Boosting multimodal contrastive learning, this research introduces negCLIPLoss and NormSim, novel data selection methods surpassing existing techniques by improving data quality and task relevance. Th&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/qvdc0ocx2n/cover.png"/></item><item><title>Customized Multiple Clustering via Multi-Modal Subspace Proxy Learning</title><link>https://deep-diver.github.io/neurips2024/posters/xbuastqaez/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xbuastqaez/</guid><description>Multi-Sub leverages multi-modal learning to achieve customized multiple clustering, aligning user-defined textual preferences with visual representations via a subspace proxy learning framework.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xbuastqaez/cover.png"/></item><item><title>Deep Submodular Peripteral Networks</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/tupcrqnvvm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/tupcrqnvvm/</guid><description>Deep Submodular Peripteral Networks (DSPNs) learn submodular functions efficiently using graded pairwise comparisons, surpassing traditional methods and demonstrating superiority in experimental desig&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/tupcrqnvvm/cover.png"/></item><item><title>Discovering plasticity rules that organize and maintain neural circuits</title><link>https://deep-diver.github.io/neurips2024/posters/nw4twuepgx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nw4twuepgx/</guid><description>AI discovers robust, biologically-plausible plasticity rules that self-organize and maintain neural circuits&amp;rsquo; sequential activity, even with synaptic turnover.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nw4twuepgx/cover.png"/></item><item><title>Drago: Primal-Dual Coupled Variance Reduction for Faster Distributionally Robust Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/ujk0xrntqz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ujk0xrntqz/</guid><description>DRAGO: A novel primal-dual algorithm delivers faster, state-of-the-art convergence for distributionally robust optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ujk0xrntqz/cover.png"/></item><item><title>How does Gradient Descent Learn Features --- A Local Analysis for Regularized Two-Layer Neural Networks</title><link>https://deep-diver.github.io/neurips2024/posters/xyw051zmun/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xyw051zmun/</guid><description>Neural networks learn features effectively through gradient descent, not just at the beginning, but also at the end of training, even with carefully regularized objectives.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xyw051zmun/cover.png"/></item><item><title>Is O(log N) practical? Near-Equivalence Between Delay Robustness and Bounded Regret in Bandits and RL</title><link>https://deep-diver.github.io/neurips2024/posters/hyjofwfw1p/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hyjofwfw1p/</guid><description>Zero Graves-Lai constant ensures both bounded regret and delay robustness in online decision-making, particularly for linear models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hyjofwfw1p/cover.png"/></item><item><title>Large Scale Transfer Learning for Tabular Data via Language Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/wh5blx5tz1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wh5blx5tz1/</guid><description>TABULA-8B, a novel language model for tabular prediction, achieves state-of-the-art zero-shot and few-shot performance across various benchmarks, exceeding existing methods by 5-15 percentage points.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wh5blx5tz1/cover.png"/></item><item><title>Learning Optimal Tax Design in Nonatomic Congestion Games</title><link>https://deep-diver.github.io/neurips2024/posters/qdprhde3jb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qdprhde3jb/</guid><description>AI learns optimal taxes for congestion games, maximizing social welfare with limited feedback, via a novel algorithm.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qdprhde3jb/cover.png"/></item><item><title>Learning to Cooperate with Humans using Generative Agents</title><link>https://deep-diver.github.io/neurips2024/posters/v4dxl3lsgx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v4dxl3lsgx/</guid><description>Generative Agent Modeling for Multi-agent Adaptation (GAMMA) improves human-AI cooperation by training AI agents against diverse partners generated from a latent model, enhancing zero-shot coordinatio&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v4dxl3lsgx/cover.png"/></item><item><title>Learning to Price Homogeneous Data</title><link>https://deep-diver.github.io/neurips2024/posters/koytqns6sz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/koytqns6sz/</guid><description>This paper develops efficient algorithms for pricing homogeneous data in online settings, achieving low regret using novel discretization schemes that scale well with data size and number of buyer typ&amp;hellip;</description></item><item><title>MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive Clinical Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/w4pibq7bai/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w4pibq7bai/</guid><description>MEDIQ benchmark revolutionizes LLM evaluation by shifting from static to interactive clinical reasoning, revealing LLMs&amp;rsquo; struggles with proactive information-seeking and highlighting the importance of&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w4pibq7bai/cover.png"/></item><item><title>Multilingual Diversity Improves Vision-Language Representations</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/1wteqrecys/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/1wteqrecys/</guid><description>Boosting vision-language models: Multilingual data improves performance on English-centric benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/1wteqrecys/cover.png"/></item><item><title>Nearly Minimax Optimal Submodular Maximization with Bandit Feedback</title><link>https://deep-diver.github.io/neurips2024/posters/vn0fwrimra/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vn0fwrimra/</guid><description>This research establishes the first minimax optimal algorithm for submodular maximization with bandit feedback, achieving a regret bound matching the lower bound.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vn0fwrimra/cover.png"/></item><item><title>Nearly Optimal Approximation of Matrix Functions by the Lanczos Method</title><link>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3s8v8qp9xv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3s8v8qp9xv/</guid><description>Lanczos-FA, a simple algorithm for approximating matrix functions, surprisingly outperforms newer methods; this paper proves its near-optimality for rational functions, explaining its practical succes&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-ai-theory/3s8v8qp9xv/cover.png"/></item><item><title>Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/grg6szbw9p/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/grg6szbw9p/</guid><description>VPL: a novel multimodal RLHF personalizes AI by inferring user-specific latent preferences, enabling accurate reward modeling and improved policy alignment for diverse populations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/grg6szbw9p/cover.png"/></item><item><title>Query-Based Adversarial Prompt Generation</title><link>https://deep-diver.github.io/neurips2024/posters/jbf3eiyd2x/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jbf3eiyd2x/</guid><description>Researchers developed a query-based attack that generates adversarial prompts, fooling language models into producing harmful outputs with significantly higher success rates than previous methods, eff&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jbf3eiyd2x/cover.png"/></item><item><title>Sample Complexity Reduction via Policy Difference Estimation in Tabular Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ryq0kuzvkl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ryq0kuzvkl/</guid><description>This paper reveals that estimating only policy differences, while effective in bandits, is insufficient for tabular reinforcement learning. However, it introduces a novel algorithm achieving near-opti&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-reinforcement-learning/ryq0kuzvkl/cover.png"/></item><item><title>Smoothed Energy Guidance: Guiding Diffusion Models with Reduced Energy Curvature of Attention</title><link>https://deep-diver.github.io/neurips2024/posters/jk728xy8g7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jk728xy8g7/</guid><description>Smoothed Energy Guidance (SEG) improves unconditional image generation by reducing self-attention&amp;rsquo;s energy curvature, leading to higher-quality outputs with fewer artifacts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jk728xy8g7/cover.png"/></item><item><title>Superposed Decoding: Multiple Generations from a Single Autoregressive Inference Pass</title><link>https://deep-diver.github.io/neurips2024/posters/ksokkhm9i7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ksokkhm9i7/</guid><description>Generate multiple text drafts from a single language model pass with Superposed Decoding, significantly boosting efficiency!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ksokkhm9i7/cover.png"/></item><item><title>Tell What You Hear From What You See - Video to Audio Generation Through Text</title><link>https://deep-diver.github.io/neurips2024/posters/kr7en85mit/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kr7en85mit/</guid><description>VATT: Text-guided video-to-audio generation, enabling refined audio control via text prompts and improved compatibility.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kr7en85mit/cover.png"/></item><item><title>The Benefits of Balance: From Information Projections to Variance Reduction</title><link>https://deep-diver.github.io/neurips2024/posters/vjmmdffl0a/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vjmmdffl0a/</guid><description>Data balancing in foundation models surprisingly reduces variance, improving model training and performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vjmmdffl0a/cover.png"/></item><item><title>Toward Global Convergence of Gradient EM for Over-Paramterized Gaussian Mixture Models</title><link>https://deep-diver.github.io/neurips2024/posters/zv9gyc3xgf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zv9gyc3xgf/</guid><description>Gradient EM for over-parameterized Gaussian Mixture Models globally converges with a sublinear rate, solving a longstanding open problem in machine learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zv9gyc3xgf/cover.png"/></item><item><title>Understanding the Gains from Repeated Self-Distillation</title><link>https://deep-diver.github.io/neurips2024/posters/gmqakjcocb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gmqakjcocb/</guid><description>Repeated self-distillation significantly reduces excess risk in linear regression, achieving up to a &amp;rsquo;d&amp;rsquo; factor improvement over single-step methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gmqakjcocb/cover.png"/></item><item><title>Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback</title><link>https://deep-diver.github.io/neurips2024/posters/jmbwtlazjw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jmbwtlazjw/</guid><description>This study disentangles best practices for learning from preference feedback in LLMs, revealing that data quality, algorithm choice, and reward model significantly impact performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jmbwtlazjw/cover.png"/></item></channel></rss>