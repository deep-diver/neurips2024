<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Self-Supervised Learning on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/self-supervised-learning/</link><description>Recent content in Self-Supervised Learning on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/self-supervised-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>A probability contrastive learning framework for 3D molecular representation learning</title><link>https://deep-diver.github.io/neurips2024/posters/hyir6tgqpv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hyir6tgqpv/</guid><description>A novel probability-based contrastive learning framework significantly improves 3D molecular representation learning by mitigating false pairs, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hyir6tgqpv/cover.png"/></item><item><title>Abstracted Shapes as Tokens - A Generalizable and Interpretable Model for Time-series Classification</title><link>https://deep-diver.github.io/neurips2024/posters/pwkknsuues/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pwkknsuues/</guid><description>VQShape: a pre-trained model uses abstracted shapes as interpretable tokens for generalizable time-series classification, achieving comparable performance to black-box models and excelling in zero-sho&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pwkknsuues/cover.png"/></item><item><title>Accelerating Augmentation Invariance Pretraining</title><link>https://deep-diver.github.io/neurips2024/posters/wh9ssqlcng/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wh9ssqlcng/</guid><description>Boost Vision Transformer pretraining speed by 4x with novel sequence compression techniques!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wh9ssqlcng/cover.png"/></item><item><title>Addressing Spatial-Temporal Heterogeneity: General Mixed Time Series Analysis via Latent Continuity Recovery and Alignment</title><link>https://deep-diver.github.io/neurips2024/posters/emv8nidzjn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/emv8nidzjn/</guid><description>MiTSformer, a novel framework, recovers latent continuous variables from discrete data to enable complete spatial-temporal modeling of mixed time series, achieving state-of-the-art performance on mult&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/emv8nidzjn/cover.png"/></item><item><title>Brain-JEPA: Brain Dynamics Foundation Model with Gradient Positioning and Spatiotemporal Masking</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/gtu2elsamo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/gtu2elsamo/</guid><description>Brain-JEPA: a novel brain dynamics foundation model leverages fMRI data via innovative gradient positioning and spatiotemporal masking to achieve state-of-the-art performance in diverse brain activity&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/gtu2elsamo/cover.png"/></item><item><title>Causal Contrastive Learning for Counterfactual Regression Over Time</title><link>https://deep-diver.github.io/neurips2024/posters/bkozybje4z/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bkozybje4z/</guid><description>Causal CPC: a novel method for accurate and efficient counterfactual regression over time using RNNs, CPC, and InfoMax, achieving state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bkozybje4z/cover.png"/></item><item><title>Cell ontology guided transcriptome foundation model</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/aeynvtto7o/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/aeynvtto7o/</guid><description>scCello: A Cell Ontology-Guided Transcriptome Foundation Model improves single-cell RNA sequencing analysis by incorporating cell lineage information, significantly boosting accuracy and generalizabil&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/aeynvtto7o/cover.png"/></item><item><title>Connecting Joint-Embedding Predictive Architecture with Contrastive Self-supervised Learning</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/jvqnjwij6m/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/jvqnjwij6m/</guid><description>C-JEPA boosts self-supervised visual learning by integrating contrastive learning with a joint-embedding predictive architecture, enhancing stability and representation quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/jvqnjwij6m/cover.png"/></item><item><title>Contrastive-Equivariant Self-Supervised Learning Improves Alignment with Primate Visual Area IT</title><link>https://deep-diver.github.io/neurips2024/posters/aims8gpp5q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/aims8gpp5q/</guid><description>Self-supervised learning models can now better predict primate IT neural responses by preserving structured variability to input transformations, improving alignment with biological visual perception.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/aims8gpp5q/cover.png"/></item><item><title>Data-Efficient Operator Learning via Unsupervised Pretraining and In-Context Learning</title><link>https://deep-diver.github.io/neurips2024/posters/muplj9ft4b/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/muplj9ft4b/</guid><description>Data-efficient neural operator learning is achieved via unsupervised pretraining and in-context learning, significantly reducing simulation costs and improving generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/muplj9ft4b/cover.png"/></item><item><title>EEGPT: Pretrained Transformer for Universal and Reliable Representation of EEG Signals</title><link>https://deep-diver.github.io/neurips2024/posters/lvs2b8cjg5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lvs2b8cjg5/</guid><description>EEGPT: A pretrained transformer model revolutionizes EEG signal representation by using a dual self-supervised learning method, achieving state-of-the-art results across various tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lvs2b8cjg5/cover.png"/></item><item><title>Efficiency for Free: Ideal Data Are Transportable Representations</title><link>https://deep-diver.github.io/neurips2024/posters/upxmisfnco/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/upxmisfnco/</guid><description>RELA accelerates representation learning by leveraging freely available pre-trained models to generate efficient data, reducing computational costs by up to 50% while maintaining accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/upxmisfnco/cover.png"/></item><item><title>Efficient Availability Attacks against Supervised and Contrastive Learning Simultaneously</title><link>https://deep-diver.github.io/neurips2024/posters/fbuscraxeb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fbuscraxeb/</guid><description>New attacks foil both supervised and contrastive learning, achieving state-of-the-art unlearnability with less computation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fbuscraxeb/cover.png"/></item><item><title>Exploitation of a Latent Mechanism in Graph Contrastive Learning: Representation Scattering</title><link>https://deep-diver.github.io/neurips2024/oral-others/r8solcx62k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-others/r8solcx62k/</guid><description>SGRL, a novel graph contrastive learning framework, significantly boosts performance by leveraging the inherent &amp;lsquo;representation scattering&amp;rsquo; mechanism and integrating graph topology, outperforming exis&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-others/r8solcx62k/cover.png"/></item><item><title>Exploiting Representation Curvature for Boundary Detection in Time Series</title><link>https://deep-diver.github.io/neurips2024/posters/wk2kxpamqv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wk2kxpamqv/</guid><description>RECURVE: A novel boundary detection method leveraging representation trajectory curvature, surpassing state-of-the-art techniques by accommodating both gradual and abrupt changes in time series.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wk2kxpamqv/cover.png"/></item><item><title>Exploring Molecular Pretraining Model at Scale</title><link>https://deep-diver.github.io/neurips2024/posters/64v40k2fdv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/64v40k2fdv/</guid><description>Uni-Mol2, a groundbreaking 1.1B parameter molecular pretraining model, reveals power-law scaling in molecular representation learning, achieving significant performance improvements on downstream task&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/64v40k2fdv/cover.png"/></item><item><title>Flexible mapping of abstract domains by grid cells via self-supervised extraction and projection of generalized velocity signals</title><link>https://deep-diver.github.io/neurips2024/posters/hocac3qit7/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hocac3qit7/</guid><description>Brain&amp;rsquo;s flexible mapping of abstract domains is achieved via self-supervised extraction and projection of generalized velocity signals by grid cells, enabling efficient map generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hocac3qit7/cover.png"/></item><item><title>FUG: Feature-Universal Graph Contrastive Pre-training for Graphs with Diverse Node Features</title><link>https://deep-diver.github.io/neurips2024/posters/vuuosbrqaw/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vuuosbrqaw/</guid><description>FUG: A new graph contrastive pre-training strategy solves GNN transferability issues across datasets with diverse node features, achieving comparable performance to retraining while significantly impr&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vuuosbrqaw/cover.png"/></item><item><title>Harnessing small projectors and multiple views for efficient vision pretraining</title><link>https://deep-diver.github.io/neurips2024/posters/y5dpsjzpra/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y5dpsjzpra/</guid><description>Boost self-supervised visual learning: This paper introduces theoretical insights and practical recommendations to significantly improve SSL&amp;rsquo;s efficiency and reduce data needs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y5dpsjzpra/cover.png"/></item><item><title>Identify Then Recommend: Towards Unsupervised Group Recommendation</title><link>https://deep-diver.github.io/neurips2024/posters/otzyhoamhx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/otzyhoamhx/</guid><description>Unsupervised group recommendation model, ITR, achieves superior user and group recommendation accuracy by dynamically identifying user groups and employing self-supervised learning, eliminating the ne&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/otzyhoamhx/cover.png"/></item><item><title>In-Context Symmetries: Self-Supervised Learning through Contextual World Models</title><link>https://deep-diver.github.io/neurips2024/posters/etpah4xsun/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/etpah4xsun/</guid><description>CONTEXTSSL: A novel self-supervised learning algorithm that adapts to task-specific symmetries by using context, achieving significant performance gains over existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/etpah4xsun/cover.png"/></item><item><title>Large Pre-trained time series models for cross-domain Time series analysis tasks</title><link>https://deep-diver.github.io/neurips2024/posters/vmmzjcr5zj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vmmzjcr5zj/</guid><description>Large Pre-trained Time-series Models (LPTM) achieves superior forecasting and time-series classification results using a novel adaptive segmentation method, requiring up to 40% less data and 50% less &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vmmzjcr5zj/cover.png"/></item><item><title>Learning predictable and robust neural representations by straightening image sequences</title><link>https://deep-diver.github.io/neurips2024/posters/fyfliutfhx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fyfliutfhx/</guid><description>Self-supervised learning gets a boost: New objective function trains robust &amp;amp; predictive neural networks by straightening video trajectories, surpassing invariance methods for better spatiotemporal re&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fyfliutfhx/cover.png"/></item><item><title>Localizing Memorization in SSL Vision Encoders</title><link>https://deep-diver.github.io/neurips2024/posters/r46hglijcg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/r46hglijcg/</guid><description>SSL vision encoders, while trained on massive datasets, surprisingly memorize individual data points. This paper introduces novel methods to precisely pinpoint this memorization within encoders at bot&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/r46hglijcg/cover.png"/></item><item><title>MSA Generation with Seqs2Seqs Pretraining: Advancing Protein Structure Predictions</title><link>https://deep-diver.github.io/neurips2024/posters/d0dllmoufv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/d0dllmoufv/</guid><description>Self-supervised generative model MSA-Generator boosts protein structure prediction accuracy by producing high-quality MSAs, especially for challenging sequences lacking homologs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/d0dllmoufv/cover.png"/></item><item><title>Multiple Physics Pretraining for Spatiotemporal Surrogate Models</title><link>https://deep-diver.github.io/neurips2024/posters/dksi3buliz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dksi3buliz/</guid><description>Multiple Physics Pretraining (MPP) revolutionizes spatiotemporal physical surrogate modeling by pretraining transformers on diverse physics simultaneously, enabling accurate predictions on unseen syst&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dksi3buliz/cover.png"/></item><item><title>Online Feature Updates Improve Online (Generalized) Label Shift Adaptation</title><link>https://deep-diver.github.io/neurips2024/posters/hnh1ykrjxf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hnh1ykrjxf/</guid><description>Online Label Shift adaptation with Online Feature Updates (OLS-OFU) significantly boosts online label shift adaptation by dynamically refining feature extractors using self-supervised learning, achiev&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hnh1ykrjxf/cover.png"/></item><item><title>Preventing Dimensional Collapse in Self-Supervised Learning via Orthogonality Regularization</title><link>https://deep-diver.github.io/neurips2024/posters/y3fjkssfmy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/y3fjkssfmy/</guid><description>Orthogonal regularization prevents dimensional collapse in self-supervised learning, significantly boosting model performance across diverse benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/y3fjkssfmy/cover.png"/></item><item><title>Protected Test-Time Adaptation via Online Entropy Matching: A Betting Approach</title><link>https://deep-diver.github.io/neurips2024/posters/qamfjyhpeg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/qamfjyhpeg/</guid><description>POEM: a novel test-time adaptation approach using online self-training improves accuracy under distribution shifts by dynamically updating the classifier, ensuring invariance to shifts while maintaini&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/qamfjyhpeg/cover.png"/></item><item><title>Resource-Aware Federated Self-Supervised Learning with Global Class Representations</title><link>https://deep-diver.github.io/neurips2024/posters/of4inaiuse/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/of4inaiuse/</guid><description>FedMKD: A multi-teacher framework for federated self-supervised learning, enabling global class representations even with diverse client models and skewed data distributions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/of4inaiuse/cover.png"/></item><item><title>Revisiting Self-Supervised Heterogeneous Graph Learning from Spectral Clustering Perspective</title><link>https://deep-diver.github.io/neurips2024/posters/i6trenm5ya/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/i6trenm5ya/</guid><description>SCHOOL: A novel SHGL framework enhancing spectral clustering with rank and dual consistency constraints, effectively mitigating noise and leveraging cluster-level information for improved downstream t&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/i6trenm5ya/cover.png"/></item><item><title>Self-Guided Masked Autoencoder</title><link>https://deep-diver.github.io/neurips2024/posters/7vxufiezsy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7vxufiezsy/</guid><description>Self-guided MAE boosts self-supervised learning by intelligently masking image patches based on internal clustering patterns, dramatically accelerating training without external data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7vxufiezsy/cover.png"/></item><item><title>Self-Healing Machine Learning: A Framework for Autonomous Adaptation in Real-World Environments</title><link>https://deep-diver.github.io/neurips2024/posters/f63dkipx0i/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/f63dkipx0i/</guid><description>Self-healing machine learning (SHML) autonomously diagnoses and fixes model performance degradation caused by data shifts, outperforming reason-agnostic methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/f63dkipx0i/cover.png"/></item><item><title>Self-Labeling the Job Shop Scheduling Problem</title><link>https://deep-diver.github.io/neurips2024/posters/buqvmt3b4k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/buqvmt3b4k/</guid><description>Self-Labeling Improves Generative Model Training for Combinatorial Problems</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/buqvmt3b4k/cover.png"/></item><item><title>Self-Supervised Adversarial Training via Diverse Augmented Queries and Self-Supervised Double Perturbation</title><link>https://deep-diver.github.io/neurips2024/posters/cihdlhfroo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cihdlhfroo/</guid><description>DAQ-SDP enhances self-supervised adversarial training by using diverse augmented queries, a self-supervised double perturbation scheme, and a novel Aug-Adv Pairwise-BatchNorm method, bridging the gap &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cihdlhfroo/cover.png"/></item><item><title>Self-supervised Transformation Learning for Equivariant Representations</title><link>https://deep-diver.github.io/neurips2024/posters/87axdbkryd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/87axdbkryd/</guid><description>Self-Supervised Transformation Learning (STL) enhances equivariant representations by replacing transformation labels with image-pair-derived representations, improving performance on diverse classifi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/87axdbkryd/cover.png"/></item><item><title>The Benefits of Balance: From Information Projections to Variance Reduction</title><link>https://deep-diver.github.io/neurips2024/posters/vjmmdffl0a/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vjmmdffl0a/</guid><description>Data balancing in foundation models surprisingly reduces variance, improving model training and performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vjmmdffl0a/cover.png"/></item><item><title>Towards a 'Universal Translator' for Neural Dynamics at Single-Cell, Single-Spike Resolution</title><link>https://deep-diver.github.io/neurips2024/posters/nrrjsdaheg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nrrjsdaheg/</guid><description>A new self-supervised learning approach, Multi-task Masking (MtM), significantly improves the prediction accuracy of neural population activity by capturing neural dynamics at multiple spatial scales,&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nrrjsdaheg/cover.png"/></item><item><title>Uncovering the Redundancy in Graph Self-supervised Learning Models</title><link>https://deep-diver.github.io/neurips2024/posters/7ntft3u7jj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7ntft3u7jj/</guid><description>Graph self-supervised learning models surprisingly exhibit high redundancy, allowing for significant parameter reduction without performance loss. A novel framework, SLIDE, leverages this discovery f&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7ntft3u7jj/cover.png"/></item><item><title>Understanding the Role of Equivariance in Self-supervised Learning</title><link>https://deep-diver.github.io/neurips2024/posters/nlqdudgbfy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nlqdudgbfy/</guid><description>E-SSL&amp;rsquo;s generalization ability is rigorously analyzed via an information-theoretic lens, revealing key design principles for improved performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nlqdudgbfy/cover.png"/></item><item><title>Unified Graph Augmentations for Generalized Contrastive Learning on Graphs</title><link>https://deep-diver.github.io/neurips2024/posters/jgkkrolxec/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jgkkrolxec/</guid><description>Unified Graph Augmentations (UGA) module boosts graph contrastive learning by unifying diverse augmentation strategies, improving model generalizability and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jgkkrolxec/cover.png"/></item><item><title>You Donât Need Domain-Specific Data Augmentations When Scaling Self-Supervised Learning</title><link>https://deep-diver.github.io/neurips2024/posters/7rwkmrmnrc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/7rwkmrmnrc/</guid><description>Self-supervised learning&amp;rsquo;s reliance on complex data augmentations is challenged; a large-scale study shows comparable performance using only cropping, suggesting dataset size is more important than au&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/7rwkmrmnrc/cover.png"/></item><item><title>Your contrastive learning problem is secretly a distribution alignment problem</title><link>https://deep-diver.github.io/neurips2024/posters/inukolu8xb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/inukolu8xb/</guid><description>Contrastive learning is reframed as a distribution alignment problem, leading to a flexible framework (GCA) that improves representation learning with unbalanced optimal transport.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/inukolu8xb/cover.png"/></item><item><title>Your Diffusion Model is Secretly a Noise Classifier and Benefits from Contrastive Training</title><link>https://deep-diver.github.io/neurips2024/posters/re7wpi4vft/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/re7wpi4vft/</guid><description>Diffusion models benefit from contrastive training, improving sample quality and speed by addressing poor denoiser estimation in out-of-distribution regions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/re7wpi4vft/cover.png"/></item></channel></rss>