<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Transfer Learning on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/transfer-learning/</link><description>Recent content in Transfer Learning on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/transfer-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Adversarially Robust Multi-task Representation Learning</title><link>https://deep-diver.github.io/neurips2024/posters/w2l3ll1jbv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w2l3ll1jbv/</guid><description>Multi-task learning boosts adversarial robustness in transfer learning by leveraging diverse source data to build a shared representation, enabling effective learning in data-scarce target tasks, as p&amp;hellip;</description></item><item><title>Bayesian Domain Adaptation with Gaussian Mixture Domain-Indexing</title><link>https://deep-diver.github.io/neurips2024/posters/grd7yzfm5v/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/grd7yzfm5v/</guid><description>GMDI: a novel Bayesian domain adaptation algorithm significantly improves adaptation by dynamically modeling domain indices using Gaussian Mixture Models, outperforming state-of-the-art methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/grd7yzfm5v/cover.png"/></item><item><title>Bayesian-guided Label Mapping for Visual Reprogramming</title><link>https://deep-diver.github.io/neurips2024/oral-others/135ekqdorr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-others/135ekqdorr/</guid><description>Bayesian-guided Label Mapping (BLM) enhances visual reprogramming!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-others/135ekqdorr/cover.png"/></item><item><title>Beyond Efficiency: Molecular Data Pruning for Enhanced Generalization</title><link>https://deep-diver.github.io/neurips2024/posters/gj0qievgjd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gj0qievgjd/</guid><description>MolPeg, a novel molecular data pruning framework, enhances model generalization in transfer learning by using a source-free approach and consistently outperforming other methods, even surpassing full-&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gj0qievgjd/cover.png"/></item><item><title>Boosting Transferability and Discriminability for Time Series Domain Adaptation</title><link>https://deep-diver.github.io/neurips2024/posters/cibssxowmr/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/cibssxowmr/</guid><description>ACON: Adversarial CO-learning Networks enhances time series domain adaptation by cleverly combining temporal and frequency features. Frequency features boost within-domain discriminability, while temp&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/cibssxowmr/cover.png"/></item><item><title>Deep Graph Mating</title><link>https://deep-diver.github.io/neurips2024/posters/m4ni2yiwja/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/m4ni2yiwja/</guid><description>Deep Graph Mating (GRAMA) enables training-free knowledge transfer in GNNs, achieving results comparable to pre-trained models without retraining or labeled data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/m4ni2yiwja/cover.png"/></item><item><title>Diffusion Tuning: Transferring Diffusion Models via Chain of Forgetting</title><link>https://deep-diver.github.io/neurips2024/posters/s98ozjd3jn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s98ozjd3jn/</guid><description>Diff-Tuning: a simple yet effective approach transfers pre-trained diffusion models to various downstream tasks by leveraging the &amp;lsquo;chain of forgetting&amp;rsquo; phenomenon, improving transferability and conver&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/s98ozjd3jn/cover.png"/></item><item><title>Disentangling and mitigating the impact of task similarity for continual learning</title><link>https://deep-diver.github.io/neurips2024/posters/be7gwlqzkm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/be7gwlqzkm/</guid><description>This study reveals that high input similarity paired with low output similarity is detrimental to continual learning, whereas the opposite scenario is relatively benign; offering insights into mitigat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/be7gwlqzkm/cover.png"/></item><item><title>Efficient Discrepancy Testing for Learning with Distribution Shift</title><link>https://deep-diver.github.io/neurips2024/posters/ojihvhqbaq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ojihvhqbaq/</guid><description>Provably efficient algorithms for learning with distribution shift are introduced, generalizing and improving prior work by achieving near-optimal error rates and offering universal learners for large&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ojihvhqbaq/cover.png"/></item><item><title>Enhancing Domain Adaptation through Prompt Gradient Alignment</title><link>https://deep-diver.github.io/neurips2024/posters/14hljr6kz3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/14hljr6kz3/</guid><description>Prompt Gradient Alignment (PGA) enhances unsupervised domain adaptation by aligning per-objective gradients in a multi-objective optimization framework, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/14hljr6kz3/cover.png"/></item><item><title>Expanding Sparse Tuning for Low Memory Usage</title><link>https://deep-diver.github.io/neurips2024/posters/abzyngwfpn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/abzyngwfpn/</guid><description>SNELL: Sparse tuning with kerNElized LoRA achieves state-of-the-art parameter-efficient fine-tuning performance with drastically reduced memory usage.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/abzyngwfpn/cover.png"/></item><item><title>Fine-Tuning is Fine, if Calibrated</title><link>https://deep-diver.github.io/neurips2024/posters/xrjxkbeetd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xrjxkbeetd/</guid><description>Fine-tuning pre-trained models often degrades performance on unseen classes. This work reveals that the problem stems from logit scale discrepancies, not feature loss, and shows that post-processing c&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xrjxkbeetd/cover.png"/></item><item><title>Geodesic Optimization for Predictive Shift Adaptation on EEG data</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/qtypwxvnja/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/qtypwxvnja/</guid><description>GOPSA: a novel geodesic optimization method significantly improves cross-site age prediction from EEG data by jointly handling shifts in data and predictive variables.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/qtypwxvnja/cover.png"/></item><item><title>GFT: Graph Foundation Model with Transferable Tree Vocabulary</title><link>https://deep-diver.github.io/neurips2024/posters/0mxzbav8xy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0mxzbav8xy/</guid><description>GFT: a novel graph foundation model using transferable computation trees as tokens, improving generalization and reducing negative transfer in graph learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0mxzbav8xy/cover.png"/></item><item><title>Handling Learnwares from Heterogeneous Feature Spaces with Explicit Label Exploitation</title><link>https://deep-diver.github.io/neurips2024/posters/3yiyb82rjx/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3yiyb82rjx/</guid><description>This paper enhances learnware dock systems by using model outputs to improve heterogeneous learnware management, enabling effective task handling even without perfectly matched models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3yiyb82rjx/cover.png"/></item><item><title>Inductive biases of multi-task learning and finetuning: multiple regimes of feature reuse</title><link>https://deep-diver.github.io/neurips2024/posters/uwvjjzwjpt/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uwvjjzwjpt/</guid><description>Multi-task learning and finetuning show surprising feature reuse biases, including a novel &amp;rsquo;nested feature selection&amp;rsquo; regime where finetuning prioritizes a sparse subset of pretrained features, signif&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uwvjjzwjpt/cover.png"/></item><item><title>Large Scale Transfer Learning for Tabular Data via Language Modeling</title><link>https://deep-diver.github.io/neurips2024/posters/wh5blx5tz1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wh5blx5tz1/</guid><description>TABULA-8B, a novel language model for tabular prediction, achieves state-of-the-art zero-shot and few-shot performance across various benchmarks, exceeding existing methods by 5-15 percentage points.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wh5blx5tz1/cover.png"/></item><item><title>Monte Carlo Tree Search based Space Transfer for Black Box Optimization</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/t5ufifmdbq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/t5ufifmdbq/</guid><description>MCTS-transfer: Iteratively refining Bayesian optimization via Monte Carlo tree search for efficient black-box optimization using transfer learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/t5ufifmdbq/cover.png"/></item><item><title>Neural decoding from stereotactic EEG: accounting for electrode variability across subjects</title><link>https://deep-diver.github.io/neurips2024/posters/lr1nnsd7h0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lr1nnsd7h0/</guid><description>Scalable SEEG decoding model, seegnificant, leverages transformers to decode behavior across subjects despite electrode variability, achieving high accuracy and transfer learning capability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lr1nnsd7h0/cover.png"/></item><item><title>On $f$-Divergence Principled Domain Adaptation: An Improved Framework</title><link>https://deep-diver.github.io/neurips2024/posters/xsu27dgwer/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xsu27dgwer/</guid><description>Improved unsupervised domain adaptation framework achieves superior performance via refined f-divergence and novel f-domain discrepancy, enabling faster algorithms and tighter generalization bounds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xsu27dgwer/cover.png"/></item><item><title>Optimal Aggregation of Prediction Intervals under Unsupervised Domain Shift</title><link>https://deep-diver.github.io/neurips2024/posters/ldxynsvxer/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ldxynsvxer/</guid><description>This paper introduces a novel method for creating highly accurate and narrow prediction intervals even when data distribution shifts unexpectedly, significantly improving machine learning model reliab&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ldxynsvxer/cover.png"/></item><item><title>Reinforced Cross-Domain Knowledge Distillation on Time Series Data</title><link>https://deep-diver.github.io/neurips2024/posters/tuhabdzp0q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tuhabdzp0q/</guid><description>Reinforced Cross-Domain Knowledge Distillation (RCD-KD) dynamically selects target samples for efficient knowledge transfer from a complex teacher model to a compact student model, achieving superior &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tuhabdzp0q/cover.png"/></item><item><title>Revealing Distribution Discrepancy by Sampling Transfer in Unlabeled Data</title><link>https://deep-diver.github.io/neurips2024/posters/bnzeog0yey/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bnzeog0yey/</guid><description>I-Div accurately quantifies distribution discrepancy between training and test datasets without test labels, enabling reliable hypothesis applicability evaluation in complex scenarios.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bnzeog0yey/cover.png"/></item><item><title>Schrodinger Bridge Flow for Unpaired Data Translation</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/1f32icjffa/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/1f32icjffa/</guid><description>Accelerate unpaired data translation with Schrödinger Bridge Flow, a novel algorithm solving optimal transport problems efficiently without repeatedly training models!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/1f32icjffa/cover.png"/></item><item><title>Style Adaptation and Uncertainty Estimation for Multi-Source Blended-Target Domain Adaptation</title><link>https://deep-diver.github.io/neurips2024/posters/kvaaijhqhi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/kvaaijhqhi/</guid><description>SAUE: A novel multi-source blended-target domain adaptation approach using style adaptation and uncertainty estimation to improve model robustness and accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/kvaaijhqhi/cover.png"/></item><item><title>TFGDA: Exploring Topology and Feature Alignment in Semi-supervised Graph Domain Adaptation through Robust Clustering</title><link>https://deep-diver.github.io/neurips2024/posters/26bdxiy3ik/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/26bdxiy3ik/</guid><description>TFGDA: Leveraging graph topology and feature alignment for superior semi-supervised domain adaptation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/26bdxiy3ik/cover.png"/></item><item><title>The Impact of Geometric Complexity on Neural Collapse in Transfer Learning</title><link>https://deep-diver.github.io/neurips2024/posters/plbfid00au/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/plbfid00au/</guid><description>Lowering a neural network&amp;rsquo;s geometric complexity during pre-training enhances neural collapse and improves transfer learning, especially in few-shot scenarios.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/plbfid00au/cover.png"/></item><item><title>To Learn or Not to Learn, That is the Question — A Feature-Task Dual Learning Model of Perceptual Learning</title><link>https://deep-diver.github.io/neurips2024/posters/g3mbzow0qo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/g3mbzow0qo/</guid><description>A new dual-learning model resolves the paradox of perceptual learning, showing how task-based and feature-based learning interact to produce both specific and transferable improvements in sensory perc&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/g3mbzow0qo/cover.png"/></item><item><title>Towards Understanding Extrapolation: a Causal Lens</title><link>https://deep-diver.github.io/neurips2024/posters/2squ766iq4/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2squ766iq4/</guid><description>This work unveils a causal lens on extrapolation, offering theoretical guarantees for accurate predictions on out-of-support data, even with limited target samples.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2squ766iq4/cover.png"/></item><item><title>Transfer Learning for Latent Variable Network Models</title><link>https://deep-diver.github.io/neurips2024/posters/pk8xocbqro/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pk8xocbqro/</guid><description>This paper presents efficient algorithms for transfer learning in latent variable network models, achieving vanishing error under specific conditions, and attaining minimax optimal rates for stochasti&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pk8xocbqro/cover.png"/></item><item><title>Universality in Transfer Learning for Linear Models</title><link>https://deep-diver.github.io/neurips2024/posters/mhwamokon3/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mhwamokon3/</guid><description>Linear model transfer learning achieves universal generalization error improvements, depending only on first and second-order target statistics, and defying Gaussian assumptions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mhwamokon3/cover.png"/></item></channel></rss>