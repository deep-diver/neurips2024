<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ University of Texas at Austin on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-university-of-texas-at-austin/</link><description>Recent content in üè¢ University of Texas at Austin on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-university-of-texas-at-austin/index.xml" rel="self" type="application/rss+xml"/><item><title>$ extit{Read-ME}$: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design</title><link>https://deep-diver.github.io/neurips2024/posters/i8jaxy7tdi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/i8jaxy7tdi/</guid><description>Read-ME refactors pre-trained dense LLMs into efficient, router-decoupled Mixture-of-Experts (MoEs) via activation sparsity, achieving up to 10.1% improvement on MMLU and 6.1% reduction in latency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/i8jaxy7tdi/cover.png"/></item><item><title>A Bayesian Approach for Personalized Federated Learning in Heterogeneous Settings</title><link>https://deep-diver.github.io/neurips2024/posters/hilgwnabqb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hilgwnabqb/</guid><description>FedBNN: a novel Bayesian framework for personalized federated learning, achieves superior performance in heterogeneous settings while ensuring strict privacy via differential privacy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hilgwnabqb/cover.png"/></item><item><title>Active Classification with Few Queries under Misspecification</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/ma0993kzlq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/ma0993kzlq/</guid><description>Learning halfspaces efficiently under noise is cracked! A novel query language enables a polylog query algorithm for Massart noise, overcoming previous limitations.</description></item><item><title>AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies</title><link>https://deep-diver.github.io/neurips2024/posters/ugxkinqdcc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ugxkinqdcc/</guid><description>AdaFlow: a novel imitation learning framework boasts fast inference and diverse action generation via variance-adaptive flow-based policies, significantly outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ugxkinqdcc/cover.png"/></item><item><title>Adaptive and Optimal Second-order Optimistic Methods for Minimax Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/nvdygefxcy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/nvdygefxcy/</guid><description>New adaptive second-order optimistic methods for minimax optimization achieve optimal convergence without line search, simplifying updates and improving efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/nvdygefxcy/cover.png"/></item><item><title>An Accelerated Gradient Method for Convex Smooth Simple Bilevel Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/afodln7jbv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/afodln7jbv/</guid><description>Accelerated Gradient Method for Bilevel Optimization (AGM-BiO) achieves state-of-the-art convergence rates for simple bilevel optimization problems, requiring fewer iterations than existing methods to&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/afodln7jbv/cover.png"/></item><item><title>Bayesian Nonparametrics Meets Data-Driven Distributionally Robust Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/8cgupoe3tp/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/8cgupoe3tp/</guid><description>Boost machine learning model robustness by minimizing a novel data-driven risk criterion that blends Bayesian nonparametrics and smooth ambiguity aversion, ensuring superior out-of-sample performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/8cgupoe3tp/cover.png"/></item><item><title>Causal language modeling can elicit search and reasoning capabilities on logic puzzles</title><link>https://deep-diver.github.io/neurips2024/posters/i5poejmwoc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/i5poejmwoc/</guid><description>LLMs surprisingly master complex logic puzzles like Sudoku and Zebra puzzles after training on strategically ordered solution steps, revealing hidden reasoning abilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/i5poejmwoc/cover.png"/></item><item><title>CoFie: Learning Compact Neural Surface Representations with Coordinate Fields</title><link>https://deep-diver.github.io/neurips2024/posters/0ksesacluj/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/0ksesacluj/</guid><description>CoFie: A novel local geometry-aware neural surface representation dramatically improves accuracy and efficiency in 3D shape modeling by using coordinate fields to compress local shape information.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/0ksesacluj/cover.png"/></item><item><title>Communication Efficient Distributed Training with Distributed Lion</title><link>https://deep-diver.github.io/neurips2024/posters/wdircetioz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wdircetioz/</guid><description>Distributed Lion: Training large AI models efficiently by communicating only binary or low-precision vectors between workers and a server, significantly reducing communication costs and maintaining co&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wdircetioz/cover.png"/></item><item><title>Detecting Bugs with Substantial Monetary Consequences by LLM and Rule-based Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/hb5nkiet32/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hb5nkiet32/</guid><description>Hybrid LLM &amp;amp; rule-based system accurately detects costly smart contract bugs!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hb5nkiet32/cover.png"/></item><item><title>Discovering Creative Behaviors through DUPLEX: Diverse Universal Features for Policy Exploration</title><link>https://deep-diver.github.io/neurips2024/posters/bhgkt0suy6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bhgkt0suy6/</guid><description>DUPLEX: a novel RL method trains diverse, near-optimal policies in complex, dynamic environments by explicitly maximizing policy diversity using successor features. It outperforms existing methods in&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bhgkt0suy6/cover.png"/></item><item><title>Disentangled Unsupervised Skill Discovery for Efficient Hierarchical Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/epobcwfnfc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/epobcwfnfc/</guid><description>DUSDi: A novel method for learning disentangled skills in unsupervised reinforcement learning, enabling efficient reuse for diverse downstream tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/epobcwfnfc/cover.png"/></item><item><title>Dynamic Model Predictive Shielding for Provably Safe Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/x2zy4hzcmg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x2zy4hzcmg/</guid><description>Dynamic Model Predictive Shielding (DMPS) ensures provably safe reinforcement learning by dynamically optimizing reinforcement learning objectives while maintaining provable safety, achieving higher r&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x2zy4hzcmg/cover.png"/></item><item><title>Efficient Discrepancy Testing for Learning with Distribution Shift</title><link>https://deep-diver.github.io/neurips2024/posters/ojihvhqbaq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ojihvhqbaq/</guid><description>Provably efficient algorithms for learning with distribution shift are introduced, generalizing and improving prior work by achieving near-optimal error rates and offering universal learners for large&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ojihvhqbaq/cover.png"/></item><item><title>Expressive Gaussian Human Avatars from Monocular RGB Video</title><link>https://deep-diver.github.io/neurips2024/posters/3cwelzfnyl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3cwelzfnyl/</guid><description>EVA: a novel method generates expressive 3D Gaussian human avatars from monocular RGB videos, excelling in detailed hand and facial expressions via context-aware density control and improved SMPL-X al&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3cwelzfnyl/cover.png"/></item><item><title>Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding</title><link>https://deep-diver.github.io/neurips2024/posters/fpmscvb1td/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fpmscvb1td/</guid><description>Ms-PoE, a simple plug-and-play positional encoding, significantly improves LLMs&amp;rsquo; ability to utilize long contexts by mitigating the &amp;rsquo;lost-in-the-middle&amp;rsquo; problem and enhancing the capacity to capture i&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fpmscvb1td/cover.png"/></item><item><title>Fundamental Limits of Prompt Compression: A Rate-Distortion Framework for Black-Box Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/tebkvfhp2m/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tebkvfhp2m/</guid><description>This paper introduces a rate-distortion framework for prompt compression in LLMs, bridging the gap between existing methods and optimal performance. By formulating prompt compression as a linear progr&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tebkvfhp2m/cover.png"/></item><item><title>Heterogeneity-Guided Client Sampling: Towards Fast and Efficient Non-IID Federated Learning</title><link>https://deep-diver.github.io/neurips2024/posters/hhnppisauh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hhnppisauh/</guid><description>HiCS-FL: A novel federated learning client sampling method that leverages data heterogeneity for faster, more efficient global model training in non-IID settings.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hhnppisauh/cover.png"/></item><item><title>Hierarchical Hybrid Sliced Wasserstein: A Scalable Metric for Heterogeneous Joint Distributions</title><link>https://deep-diver.github.io/neurips2024/posters/xwrmd1njqq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xwrmd1njqq/</guid><description>Hierarchical Hybrid Sliced Wasserstein (H2SW) solves the challenge of comparing complex, heterogeneous joint distributions by introducing novel slicing operators, leading to a scalable and statistical&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xwrmd1njqq/cover.png"/></item><item><title>HOI-Swap: Swapping Objects in Videos with Hand-Object Interaction Awareness</title><link>https://deep-diver.github.io/neurips2024/posters/gkhxbasqwm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/gkhxbasqwm/</guid><description>HOI-Swap: a novel diffusion model flawlessly swaps objects in videos while intelligently preserving natural hand interactions, producing high-quality edits.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/gkhxbasqwm/cover.png"/></item><item><title>HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning</title><link>https://deep-diver.github.io/neurips2024/oral-large-language-models/qepi8uwx3n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-large-language-models/qepi8uwx3n/</guid><description>HydraLoRA: Asymmetric LoRA boosts LLM fine-tuning efficiency by sharing parameters across tasks while specializing others, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-large-language-models/qepi8uwx3n/cover.png"/></item><item><title>Identifying General Mechanism Shifts in Linear Causal Representations</title><link>https://deep-diver.github.io/neurips2024/posters/jwaxhcytv1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/jwaxhcytv1/</guid><description>Researchers can now pinpoint the sources of data shifts in complex linear causal systems using a new algorithm, even with limited perfect interventions, opening exciting possibilities for causal disco&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/jwaxhcytv1/cover.png"/></item><item><title>Improved Sample Complexity Bounds for Diffusion Model Training</title><link>https://deep-diver.github.io/neurips2024/posters/oxcqkyoy8q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oxcqkyoy8q/</guid><description>Training high-quality diffusion models efficiently is now possible, thanks to novel sample complexity bounds improving exponentially on previous work.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oxcqkyoy8q/cover.png"/></item><item><title>In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/lfxiasylxb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/lfxiasylxb/</guid><description>Softmax attention in transformers adapts its attention window to function Lipschitzness and noise, enabling efficient in-context learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/lfxiasylxb/cover.png"/></item><item><title>Learning Noisy Halfspaces with a Margin: Massart is No Harder than Random</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/enlubvb262/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/enlubvb262/</guid><description>Proper learning of noisy halfspaces with margins is achievable with sample complexity matching random classification noise, defying prior expectations.</description></item><item><title>LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/6aeidnrtn2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/6aeidnrtn2/</guid><description>LightGaussian achieves 15x compression of 3D Gaussian scene representations, boosting rendering speed to 200+ FPS while maintaining visual quality, solving storage and efficiency issues in real-time n&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/6aeidnrtn2/cover.png"/></item><item><title>LoFiT: Localized Fine-tuning on LLM Representations</title><link>https://deep-diver.github.io/neurips2024/posters/dfixfbecsz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/dfixfbecsz/</guid><description>LOFIT: Localized fine-tuning boosts LLMs&amp;rsquo; performance by selectively training only a small subset of attention heads, achieving comparable accuracy to other methods while using significantly fewer par&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/dfixfbecsz/cover.png"/></item><item><title>MatFormer: Nested Transformer for Elastic Inference</title><link>https://deep-diver.github.io/neurips2024/posters/fya6ezmxd5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fya6ezmxd5/</guid><description>MatFormer: Train one universal model, extract hundreds of accurate submodels for elastic inference!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fya6ezmxd5/cover.png"/></item><item><title>Memory-Efficient LLM Training with Online Subspace Descent</title><link>https://deep-diver.github.io/neurips2024/posters/p8rtct6g45/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/p8rtct6g45/</guid><description>Online Subspace Descent: a novel memory-efficient LLM training algorithm guaranteed to converge, closing the performance gap with full-rank methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/p8rtct6g45/cover.png"/></item><item><title>N-agent Ad Hoc Teamwork</title><link>https://deep-diver.github.io/neurips2024/posters/q7txguwlhd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/q7txguwlhd/</guid><description>New algorithm, POAM, excels at multi-agent cooperation by adapting to diverse and changing teammates in dynamic scenarios.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/q7txguwlhd/cover.png"/></item><item><title>Neural Cover Selection for Image Steganography</title><link>https://deep-diver.github.io/neurips2024/posters/tzzz5kaee2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/tzzz5kaee2/</guid><description>This study introduces a neural cover selection framework for image steganography, optimizing latent spaces in generative models to improve message recovery and image quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/tzzz5kaee2/cover.png"/></item><item><title>Non-asymptotic Global Convergence Analysis of BFGS with the Armijo-Wolfe Line Search</title><link>https://deep-diver.github.io/neurips2024/spotlight-optimization/mkzpn2t87c/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-optimization/mkzpn2t87c/</guid><description>BFGS algorithm achieves global linear and superlinear convergence rates with inexact Armijo-Wolfe line search, even without precise Hessian knowledge.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-optimization/mkzpn2t87c/cover.png"/></item><item><title>Oja's Algorithm for Streaming Sparse PCA</title><link>https://deep-diver.github.io/neurips2024/posters/clqdptoord/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/clqdptoord/</guid><description>Oja&amp;rsquo;s algorithm achieves minimax optimal error rates for streaming sparse PCA using a simple single-pass thresholding method, requiring only O(d) space and O(nd) time.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/clqdptoord/cover.png"/></item><item><title>Optimization Can Learn Johnson Lindenstrauss Embeddings</title><link>https://deep-diver.github.io/neurips2024/posters/w3jctbrduf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w3jctbrduf/</guid><description>Optimization can learn optimal Johnson-Lindenstrauss embeddings, avoiding the limitations of randomized methods and achieving comparable theoretical guarantees.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w3jctbrduf/cover.png"/></item><item><title>PACE: Pacing Operator Learning to Accurate Optical Field Simulation for Complicated Photonic Devices</title><link>https://deep-diver.github.io/neurips2024/posters/uxjlgkwdci/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uxjlgkwdci/</guid><description>PACE, a novel neural operator, achieves unprecedented accuracy and speed in optical field simulation for complex photonic devices, surpassing existing methods by significantly reducing errors and boos&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uxjlgkwdci/cover.png"/></item><item><title>PPLNs: Parametric Piecewise Linear Networks for Event-Based Temporal Modeling and Beyond</title><link>https://deep-diver.github.io/neurips2024/posters/s8wfxyt4dy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s8wfxyt4dy/</guid><description>Parametric Piecewise Linear Networks (PPLNs) achieve state-of-the-art results in event-based and frame-based computer vision tasks by mimicking biological neural principles.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/s8wfxyt4dy/cover.png"/></item><item><title>Pseudo-Private Data Guided Model Inversion Attacks</title><link>https://deep-diver.github.io/neurips2024/posters/pyqpuf36d2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pyqpuf36d2/</guid><description>Pseudo-Private Data Guided Model Inversion (PPDG-MI) significantly improves model inversion attacks by dynamically tuning the generative model to increase the sampling probability of actual private da&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pyqpuf36d2/cover.png"/></item><item><title>Quadratic Quantum Variational Monte Carlo</title><link>https://deep-diver.github.io/neurips2024/posters/ldtabi541u/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ldtabi541u/</guid><description>Q2VMC, a novel quantum chemistry algorithm, drastically boosts the efficiency and accuracy of solving the Schr√∂dinger equation using a quadratic update mechanism and neural network ansatzes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ldtabi541u/cover.png"/></item><item><title>Sigmoid Gating is More Sample Efficient than Softmax Gating in Mixture of Experts</title><link>https://deep-diver.github.io/neurips2024/posters/ig6kd5v4kd/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ig6kd5v4kd/</guid><description>Sigmoid gating significantly boosts sample efficiency in Mixture of Experts models compared to softmax gating, offering faster convergence rates for various expert functions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ig6kd5v4kd/cover.png"/></item><item><title>SkiLD: Unsupervised Skill Discovery Guided by Factor Interactions</title><link>https://deep-diver.github.io/neurips2024/posters/i816teqgvh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/i816teqgvh/</guid><description>SkiLD, a novel unsupervised skill discovery method, uses state factorization and a new objective function to learn skills inducing diverse interactions between state factors, outperforming existing me&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/i816teqgvh/cover.png"/></item><item><title>Stochastic Newton Proximal Extragradient Method</title><link>https://deep-diver.github.io/neurips2024/posters/v4tzn87dtn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/v4tzn87dtn/</guid><description>Stochastic Newton Proximal Extragradient (SNPE) achieves faster global and local convergence rates for strongly convex functions, improving upon existing stochastic Newton methods by requiring signifi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/v4tzn87dtn/cover.png"/></item><item><title>SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors</title><link>https://deep-diver.github.io/neurips2024/posters/us0pwibzc0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/us0pwibzc0/</guid><description>SVFT: a novel parameter-efficient fine-tuning method achieves near full fine-tuning accuracy using only 0.006% to 0.25% of parameters, significantly outperforming existing techniques.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/us0pwibzc0/cover.png"/></item><item><title>Symbolic Regression with a Learned Concept Library</title><link>https://deep-diver.github.io/neurips2024/posters/b7s4jjglvl/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/b7s4jjglvl/</guid><description>LASR, a novel symbolic regression method, uses zero-shot LLM queries to discover and evolve abstract concepts, substantially outperforming state-of-the-art approaches and discovering a new LLM scaling&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/b7s4jjglvl/cover.png"/></item><item><title>Synthesize, Partition, then Adapt: Eliciting Diverse Samples from Foundation Models</title><link>https://deep-diver.github.io/neurips2024/posters/sp8whisnu9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sp8whisnu9/</guid><description>The Synthesize-Partition-Adapt (SPA) framework leverages synthetic data to generate diverse, high-quality responses from foundation models, enriching user experience.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sp8whisnu9/cover.png"/></item><item><title>Transfer Learning for Latent Variable Network Models</title><link>https://deep-diver.github.io/neurips2024/posters/pk8xocbqro/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pk8xocbqro/</guid><description>This paper presents efficient algorithms for transfer learning in latent variable network models, achieving vanishing error under specific conditions, and attaining minimax optimal rates for stochasti&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pk8xocbqro/cover.png"/></item><item><title>Warped Diffusion: Solving Video Inverse Problems with Image Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/posters/lh94zpv8cu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lh94zpv8cu/</guid><description>Warped Diffusion cleverly adapts image diffusion models for video inverse problems, solving flickering and temporal inconsistency issues by viewing video frames as continuous warping transformations a&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lh94zpv8cu/cover.png"/></item><item><title>YouDream: Generating Anatomically Controllable Consistent Text-to-3D Animals</title><link>https://deep-diver.github.io/neurips2024/posters/rh7tfqhizy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rh7tfqhizy/</guid><description>YOUDREAM generates anatomically consistent, high-quality 3D animal models from text and 2D pose priors, pushing creative boundaries in text-to-3D generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rh7tfqhizy/cover.png"/></item><item><title>Zero-Shot Transfer of Neural ODEs</title><link>https://deep-diver.github.io/neurips2024/posters/ognyoixtin/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ognyoixtin/</guid><description>Zero-shot Neural ODEs enable autonomous systems to rapidly adapt to unseen scenarios by learning a space of dynamical systems spanned by neural ODE basis functions, achieving efficient online adaptati&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ognyoixtin/cover.png"/></item></channel></rss>