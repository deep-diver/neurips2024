<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ University of Texas at Austin on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-university-of-texas-at-austin/</link><description>Recent content in üè¢ University of Texas at Austin on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-university-of-texas-at-austin/index.xml" rel="self" type="application/rss+xml"/><item><title>$ extit{Read-ME}$: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design</title><link>https://deep-diver.github.io/neurips2024/posters/i8jaxy7tdi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/i8jaxy7tdi/</guid><description>Read-ME refactors pre-trained dense LLMs into efficient, router-decoupled Mixture-of-Experts (MoEs) via activation sparsity, achieving up to 10.1% improvement on MMLU and 6.1% reduction in latency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/i8jaxy7tdi/cover.png"/></item><item><title>A Bayesian Approach for Personalized Federated Learning in Heterogeneous Settings</title><link>https://deep-diver.github.io/neurips2024/posters/hilgwnabqb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hilgwnabqb/</guid><description>FedBNN: a novel Bayesian framework for personalized federated learning, achieves superior performance in heterogeneous settings while ensuring strict privacy via differential privacy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hilgwnabqb/cover.png"/></item><item><title>Active Classification with Few Queries under Misspecification</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/ma0993kzlq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/ma0993kzlq/</guid><description>Learning halfspaces efficiently under noise is cracked! A novel query language enables a polylog query algorithm for Massart noise, overcoming previous limitations.</description></item><item><title>AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies</title><link>https://deep-diver.github.io/neurips2024/posters/ugxkinqdcc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ugxkinqdcc/</guid><description>AdaFlow: a novel imitation learning framework boasts fast inference and diverse action generation via variance-adaptive flow-based policies, significantly outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ugxkinqdcc/cover.png"/></item><item><title>An Accelerated Gradient Method for Convex Smooth Simple Bilevel Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/afodln7jbv/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/afodln7jbv/</guid><description>Accelerated Gradient Method for Bilevel Optimization (AGM-BiO) achieves state-of-the-art convergence rates for simple bilevel optimization problems, requiring fewer iterations than existing methods to&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/afodln7jbv/cover.png"/></item><item><title>Causal language modeling can elicit search and reasoning capabilities on logic puzzles</title><link>https://deep-diver.github.io/neurips2024/posters/i5poejmwoc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/i5poejmwoc/</guid><description>LLMs surprisingly master complex logic puzzles like Sudoku and Zebra puzzles after training on strategically ordered solution steps, revealing hidden reasoning abilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/i5poejmwoc/cover.png"/></item><item><title>Communication Efficient Distributed Training with Distributed Lion</title><link>https://deep-diver.github.io/neurips2024/posters/wdircetioz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wdircetioz/</guid><description>Distributed Lion: Training large AI models efficiently by communicating only binary or low-precision vectors between workers and a server, significantly reducing communication costs and maintaining co&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wdircetioz/cover.png"/></item><item><title>Detecting Bugs with Substantial Monetary Consequences by LLM and Rule-based Reasoning</title><link>https://deep-diver.github.io/neurips2024/posters/hb5nkiet32/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/hb5nkiet32/</guid><description>Hybrid LLM &amp;amp; rule-based system accurately detects costly smart contract bugs!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/hb5nkiet32/cover.png"/></item><item><title>Discovering Creative Behaviors through DUPLEX: Diverse Universal Features for Policy Exploration</title><link>https://deep-diver.github.io/neurips2024/posters/bhgkt0suy6/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/bhgkt0suy6/</guid><description>DUPLEX: a novel RL method trains diverse, near-optimal policies in complex, dynamic environments by explicitly maximizing policy diversity using successor features. It outperforms existing methods in&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/bhgkt0suy6/cover.png"/></item><item><title>Disentangled Unsupervised Skill Discovery for Efficient Hierarchical Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/epobcwfnfc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/epobcwfnfc/</guid><description>DUSDi: A novel method for learning disentangled skills in unsupervised reinforcement learning, enabling efficient reuse for diverse downstream tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/epobcwfnfc/cover.png"/></item><item><title>Dynamic Model Predictive Shielding for Provably Safe Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/x2zy4hzcmg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x2zy4hzcmg/</guid><description>Dynamic Model Predictive Shielding (DMPS) ensures provably safe reinforcement learning by dynamically optimizing reinforcement learning objectives while maintaining provable safety, achieving higher r&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x2zy4hzcmg/cover.png"/></item><item><title>Efficient Discrepancy Testing for Learning with Distribution Shift</title><link>https://deep-diver.github.io/neurips2024/posters/ojihvhqbaq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ojihvhqbaq/</guid><description>Provably efficient algorithms for learning with distribution shift are introduced, generalizing and improving prior work by achieving near-optimal error rates and offering universal learners for large&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ojihvhqbaq/cover.png"/></item><item><title>Hierarchical Hybrid Sliced Wasserstein: A Scalable Metric for Heterogeneous Joint Distributions</title><link>https://deep-diver.github.io/neurips2024/posters/xwrmd1njqq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xwrmd1njqq/</guid><description>Hierarchical Hybrid Sliced Wasserstein (H2SW) solves the challenge of comparing complex, heterogeneous joint distributions by introducing novel slicing operators, leading to a scalable and statistical&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xwrmd1njqq/cover.png"/></item><item><title>HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning</title><link>https://deep-diver.github.io/neurips2024/oral-large-language-models/qepi8uwx3n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-large-language-models/qepi8uwx3n/</guid><description>HydraLoRA: Asymmetric LoRA boosts LLM fine-tuning efficiency by sharing parameters across tasks while specializing others, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-large-language-models/qepi8uwx3n/cover.png"/></item><item><title>Improved Sample Complexity Bounds for Diffusion Model Training</title><link>https://deep-diver.github.io/neurips2024/posters/oxcqkyoy8q/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/oxcqkyoy8q/</guid><description>Training high-quality diffusion models efficiently is now possible, thanks to novel sample complexity bounds improving exponentially on previous work.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/oxcqkyoy8q/cover.png"/></item><item><title>In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/lfxiasylxb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/lfxiasylxb/</guid><description>Softmax attention in transformers adapts its attention window to function Lipschitzness and noise, enabling efficient in-context learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/lfxiasylxb/cover.png"/></item><item><title>Learning Noisy Halfspaces with a Margin: Massart is No Harder than Random</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/enlubvb262/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/enlubvb262/</guid><description>Proper learning of noisy halfspaces with margins is achievable with sample complexity matching random classification noise, defying prior expectations.</description></item><item><title>LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/6aeidnrtn2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/6aeidnrtn2/</guid><description>LightGaussian achieves 15x compression of 3D Gaussian scene representations, boosting rendering speed to 200+ FPS while maintaining visual quality, solving storage and efficiency issues in real-time n&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/6aeidnrtn2/cover.png"/></item><item><title>MatFormer: Nested Transformer for Elastic Inference</title><link>https://deep-diver.github.io/neurips2024/posters/fya6ezmxd5/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fya6ezmxd5/</guid><description>MatFormer: Train one universal model, extract hundreds of accurate submodels for elastic inference!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fya6ezmxd5/cover.png"/></item><item><title>Memory-Efficient LLM Training with Online Subspace Descent</title><link>https://deep-diver.github.io/neurips2024/posters/p8rtct6g45/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/p8rtct6g45/</guid><description>Online Subspace Descent: a novel memory-efficient LLM training algorithm guaranteed to converge, closing the performance gap with full-rank methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/p8rtct6g45/cover.png"/></item><item><title>Non-asymptotic Global Convergence Analysis of BFGS with the Armijo-Wolfe Line Search</title><link>https://deep-diver.github.io/neurips2024/spotlight-optimization/mkzpn2t87c/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-optimization/mkzpn2t87c/</guid><description>BFGS algorithm achieves global linear and superlinear convergence rates with inexact Armijo-Wolfe line search, even without precise Hessian knowledge.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-optimization/mkzpn2t87c/cover.png"/></item><item><title>Oja's Algorithm for Streaming Sparse PCA</title><link>https://deep-diver.github.io/neurips2024/posters/clqdptoord/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/clqdptoord/</guid><description>Oja&amp;rsquo;s algorithm achieves minimax optimal error rates for streaming sparse PCA using a simple single-pass thresholding method, requiring only O(d) space and O(nd) time.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/clqdptoord/cover.png"/></item><item><title>Optimization Can Learn Johnson Lindenstrauss Embeddings</title><link>https://deep-diver.github.io/neurips2024/posters/w3jctbrduf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w3jctbrduf/</guid><description>Optimization can learn optimal Johnson-Lindenstrauss embeddings, avoiding the limitations of randomized methods and achieving comparable theoretical guarantees.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w3jctbrduf/cover.png"/></item><item><title>PACE: Pacing Operator Learning to Accurate Optical Field Simulation for Complicated Photonic Devices</title><link>https://deep-diver.github.io/neurips2024/posters/uxjlgkwdci/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/uxjlgkwdci/</guid><description>PACE, a novel neural operator, achieves unprecedented accuracy and speed in optical field simulation for complex photonic devices, surpassing existing methods by significantly reducing errors and boos&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/uxjlgkwdci/cover.png"/></item><item><title>PPLNs: Parametric Piecewise Linear Networks for Event-Based Temporal Modeling and Beyond</title><link>https://deep-diver.github.io/neurips2024/posters/s8wfxyt4dy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/s8wfxyt4dy/</guid><description>Parametric Piecewise Linear Networks (PPLNs) achieve state-of-the-art results in event-based and frame-based computer vision tasks by mimicking biological neural principles.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/s8wfxyt4dy/cover.png"/></item><item><title>Quadratic Quantum Variational Monte Carlo</title><link>https://deep-diver.github.io/neurips2024/posters/ldtabi541u/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ldtabi541u/</guid><description>Q2VMC, a novel quantum chemistry algorithm, drastically boosts the efficiency and accuracy of solving the Schr√∂dinger equation using a quadratic update mechanism and neural network ansatzes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ldtabi541u/cover.png"/></item><item><title>SkiLD: Unsupervised Skill Discovery Guided by Factor Interactions</title><link>https://deep-diver.github.io/neurips2024/posters/i816teqgvh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/i816teqgvh/</guid><description>SkiLD, a novel unsupervised skill discovery method, uses state factorization and a new objective function to learn skills inducing diverse interactions between state factors, outperforming existing me&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/i816teqgvh/cover.png"/></item><item><title>SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors</title><link>https://deep-diver.github.io/neurips2024/posters/us0pwibzc0/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/us0pwibzc0/</guid><description>SVFT: a novel parameter-efficient fine-tuning method achieves near full fine-tuning accuracy using only 0.006% to 0.25% of parameters, significantly outperforming existing techniques.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/us0pwibzc0/cover.png"/></item><item><title>Synthesize, Partition, then Adapt: Eliciting Diverse Samples from Foundation Models</title><link>https://deep-diver.github.io/neurips2024/posters/sp8whisnu9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sp8whisnu9/</guid><description>The Synthesize-Partition-Adapt (SPA) framework leverages synthetic data to generate diverse, high-quality responses from foundation models, enriching user experience.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sp8whisnu9/cover.png"/></item><item><title>Transfer Learning for Latent Variable Network Models</title><link>https://deep-diver.github.io/neurips2024/posters/pk8xocbqro/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/pk8xocbqro/</guid><description>This paper presents efficient algorithms for transfer learning in latent variable network models, achieving vanishing error under specific conditions, and attaining minimax optimal rates for stochasti&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/pk8xocbqro/cover.png"/></item><item><title>Warped Diffusion: Solving Video Inverse Problems with Image Diffusion Models</title><link>https://deep-diver.github.io/neurips2024/posters/lh94zpv8cu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lh94zpv8cu/</guid><description>Warped Diffusion cleverly adapts image diffusion models for video inverse problems, solving flickering and temporal inconsistency issues by viewing video frames as continuous warping transformations a&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lh94zpv8cu/cover.png"/></item><item><title>YouDream: Generating Anatomically Controllable Consistent Text-to-3D Animals</title><link>https://deep-diver.github.io/neurips2024/posters/rh7tfqhizy/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rh7tfqhizy/</guid><description>YOUDREAM generates anatomically consistent, high-quality 3D animal models from text and 2D pose priors, pushing creative boundaries in text-to-3D generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rh7tfqhizy/cover.png"/></item><item><title>Zero-Shot Transfer of Neural ODEs</title><link>https://deep-diver.github.io/neurips2024/posters/ognyoixtin/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ognyoixtin/</guid><description>Zero-shot Neural ODEs enable autonomous systems to rapidly adapt to unseen scenarios by learning a space of dynamical systems spanned by neural ODE basis functions, achieving efficient online adaptati&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ognyoixtin/cover.png"/></item></channel></rss>