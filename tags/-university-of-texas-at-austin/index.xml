<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ University of Texas at Austin on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-university-of-texas-at-austin/</link><description>Recent content in üè¢ University of Texas at Austin on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-university-of-texas-at-austin/index.xml" rel="self" type="application/rss+xml"/><item><title>Active Classification with Few Queries under Misspecification</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/ma0993kzlq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/ma0993kzlq/</guid><description>Learning halfspaces efficiently under noise is cracked! A novel query language enables a polylog query algorithm for Massart noise, overcoming previous limitations.</description></item><item><title>Communication Efficient Distributed Training with Distributed Lion</title><link>https://deep-diver.github.io/neurips2024/posters/wdircetioz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wdircetioz/</guid><description>Distributed Lion: Training large AI models efficiently by communicating only binary or low-precision vectors between workers and a server, significantly reducing communication costs and maintaining co&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wdircetioz/cover.png"/></item><item><title>Disentangled Unsupervised Skill Discovery for Efficient Hierarchical Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/epobcwfnfc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/epobcwfnfc/</guid><description>DUSDi: A novel method for learning disentangled skills in unsupervised reinforcement learning, enabling efficient reuse for diverse downstream tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/epobcwfnfc/cover.png"/></item><item><title>Dynamic Model Predictive Shielding for Provably Safe Reinforcement Learning</title><link>https://deep-diver.github.io/neurips2024/posters/x2zy4hzcmg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x2zy4hzcmg/</guid><description>Dynamic Model Predictive Shielding (DMPS) ensures provably safe reinforcement learning by dynamically optimizing reinforcement learning objectives while maintaining provable safety, achieving higher r&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x2zy4hzcmg/cover.png"/></item><item><title>Efficient Discrepancy Testing for Learning with Distribution Shift</title><link>https://deep-diver.github.io/neurips2024/posters/ojihvhqbaq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ojihvhqbaq/</guid><description>Provably efficient algorithms for learning with distribution shift are introduced, generalizing and improving prior work by achieving near-optimal error rates and offering universal learners for large&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ojihvhqbaq/cover.png"/></item><item><title>HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning</title><link>https://deep-diver.github.io/neurips2024/oral-large-language-models/qepi8uwx3n/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/oral-large-language-models/qepi8uwx3n/</guid><description>HydraLoRA: Asymmetric LoRA boosts LLM fine-tuning efficiency by sharing parameters across tasks while specializing others, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/oral-large-language-models/qepi8uwx3n/cover.png"/></item><item><title>In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/lfxiasylxb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/lfxiasylxb/</guid><description>Softmax attention in transformers adapts its attention window to function Lipschitzness and noise, enabling efficient in-context learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/lfxiasylxb/cover.png"/></item><item><title>Learning Noisy Halfspaces with a Margin: Massart is No Harder than Random</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/enlubvb262/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/enlubvb262/</guid><description>Proper learning of noisy halfspaces with margins is achievable with sample complexity matching random classification noise, defying prior expectations.</description></item><item><title>LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/6aeidnrtn2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/6aeidnrtn2/</guid><description>LightGaussian achieves 15x compression of 3D Gaussian scene representations, boosting rendering speed to 200+ FPS while maintaining visual quality, solving storage and efficiency issues in real-time n&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/6aeidnrtn2/cover.png"/></item><item><title>Non-asymptotic Global Convergence Analysis of BFGS with the Armijo-Wolfe Line Search</title><link>https://deep-diver.github.io/neurips2024/spotlight-optimization/mkzpn2t87c/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-optimization/mkzpn2t87c/</guid><description>BFGS algorithm achieves global linear and superlinear convergence rates with inexact Armijo-Wolfe line search, even without precise Hessian knowledge.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-optimization/mkzpn2t87c/cover.png"/></item><item><title>Oja's Algorithm for Streaming Sparse PCA</title><link>https://deep-diver.github.io/neurips2024/posters/clqdptoord/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/clqdptoord/</guid><description>Oja&amp;rsquo;s algorithm achieves minimax optimal error rates for streaming sparse PCA using a simple single-pass thresholding method, requiring only O(d) space and O(nd) time.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/clqdptoord/cover.png"/></item><item><title>Optimization Can Learn Johnson Lindenstrauss Embeddings</title><link>https://deep-diver.github.io/neurips2024/posters/w3jctbrduf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w3jctbrduf/</guid><description>Optimization can learn optimal Johnson-Lindenstrauss embeddings, avoiding the limitations of randomized methods and achieving comparable theoretical guarantees.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w3jctbrduf/cover.png"/></item><item><title>Synthesize, Partition, then Adapt: Eliciting Diverse Samples from Foundation Models</title><link>https://deep-diver.github.io/neurips2024/posters/sp8whisnu9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/sp8whisnu9/</guid><description>The Synthesize-Partition-Adapt (SPA) framework leverages synthetic data to generate diverse, high-quality responses from foundation models, enriching user experience.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/sp8whisnu9/cover.png"/></item></channel></rss>