<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ IBM Research on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-ibm-research/</link><description>Recent content in üè¢ IBM Research on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-ibm-research/index.xml" rel="self" type="application/rss+xml"/><item><title>A two-scale Complexity Measure for Deep Learning Models</title><link>https://deep-diver.github.io/neurips2024/posters/ty9voszzia/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ty9voszzia/</guid><description>New 2sED measure effectively bounds deep learning model complexity, correlating well with training error and offering efficient computation, particularly for deep models via a layerwise approach.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ty9voszzia/cover.png"/></item><item><title>Balancing Context Length and Mixing Times for Reinforcement Learning at Scale</title><link>https://deep-diver.github.io/neurips2024/posters/vaj4xow7ey/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vaj4xow7ey/</guid><description>Longer context in RL boosts generalization but slows down learning; this paper reveals the crucial tradeoff and offers theoretical insights.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vaj4xow7ey/cover.png"/></item><item><title>Invariant subspaces and PCA in nearly matrix multiplication time</title><link>https://deep-diver.github.io/neurips2024/posters/wyp8vsl9de/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wyp8vsl9de/</guid><description>Generalized eigenvalue problems get solved in nearly matrix multiplication time, providing new, faster PCA algorithms!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wyp8vsl9de/cover.png"/></item><item><title>Limits of Transformer Language Models on Learning to Compose Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/x7ad0343jz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x7ad0343jz/</guid><description>Large Language Models struggle with compositional tasks, requiring exponentially more data than expected for learning compared to learning sub-tasks individually. This paper reveals surprising sample &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x7ad0343jz/cover.png"/></item><item><title>Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning</title><link>https://deep-diver.github.io/neurips2024/posters/w0oktgspvm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w0oktgspvm/</guid><description>Large Multimodal Models (LMMs) are limited by their context length during many-shot in-context learning. This paper introduces Multimodal Task Vectors (MTV), a method to compress numerous in-context &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w0oktgspvm/cover.png"/></item><item><title>NeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes</title><link>https://deep-diver.github.io/neurips2024/posters/npoht6wv1f/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/npoht6wv1f/</guid><description>NeuralFuse: A novel add-on module learns input transformations to maintain accuracy in low-voltage DNN inference, achieving up to 57% accuracy recovery and 24% energy savings without retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/npoht6wv1f/cover.png"/></item><item><title>Thought of Search: Planning with Language Models Through The Lens of Efficiency</title><link>https://deep-diver.github.io/neurips2024/posters/lncsya5us1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lncsya5us1/</guid><description>This paper introduces &amp;lsquo;Thought of Search,&amp;rsquo; a novel, efficient planning approach using LLMs that prioritizes soundness and completeness. It leverages LLMs to generate Python code for search components,&amp;hellip;</description></item><item><title>WAGLE: Strategic Weight Attribution for Effective and Modular Unlearning in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/vzogndjmgh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vzogndjmgh/</guid><description>WAGLE: A novel weight attribution-guided LLM unlearning framework boosts unlearning performance by strategically identifying and manipulating influential model weights, achieving a better balance betw&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vzogndjmgh/cover.png"/></item></channel></rss>