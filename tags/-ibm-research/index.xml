<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ IBM Research on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-ibm-research/</link><description>Recent content in üè¢ IBM Research on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2025 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-ibm-research/index.xml" rel="self" type="application/rss+xml"/><item><title>A Surprisingly Simple Approach to Generalized Few-Shot Semantic Segmentation</title><link>https://deep-diver.github.io/neurips2024/posters/p3nphmpx04/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/p3nphmpx04/</guid><description>Simple rule-based base-class mining (BCM) significantly boosts generalized few-shot semantic segmentation (GFSS) performance, surpassing complex existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/p3nphmpx04/cover.png"/></item><item><title>A two-scale Complexity Measure for Deep Learning Models</title><link>https://deep-diver.github.io/neurips2024/posters/ty9voszzia/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/ty9voszzia/</guid><description>New 2sED measure effectively bounds deep learning model complexity, correlating well with training error and offering efficient computation, particularly for deep models via a layerwise approach.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/ty9voszzia/cover.png"/></item><item><title>Abductive Reasoning in Logical Credal Networks</title><link>https://deep-diver.github.io/neurips2024/posters/glxuxni6tn/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/glxuxni6tn/</guid><description>This paper presents efficient algorithms for abductive reasoning in Logical Credal Networks (LCNs), addressing the MAP and Marginal MAP inference tasks to enable scalable solutions for complex real-wo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/glxuxni6tn/cover.png"/></item><item><title>Balancing Context Length and Mixing Times for Reinforcement Learning at Scale</title><link>https://deep-diver.github.io/neurips2024/posters/vaj4xow7ey/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vaj4xow7ey/</guid><description>Longer context in RL boosts generalization but slows down learning; this paper reveals the crucial tradeoff and offers theoretical insights.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vaj4xow7ey/cover.png"/></item><item><title>Dense Associative Memory Through the Lens of Random Features</title><link>https://deep-diver.github.io/neurips2024/posters/164qnjsyjf/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/164qnjsyjf/</guid><description>Boost associative memory capacity without extra parameters! DrDAM uses random features to approximate Dense Associative Memories, enabling efficient memory addition and retrieval.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/164qnjsyjf/cover.png"/></item><item><title>Distributional Preference Alignment of LLMs via Optimal Transport</title><link>https://deep-diver.github.io/neurips2024/posters/2lctgfn6ty/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/2lctgfn6ty/</guid><description>LLMs are aligned to human preferences distributionally using Optimal Transport, achieving state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/2lctgfn6ty/cover.png"/></item><item><title>Geometry of naturalistic object representations in recurrent neural network models of working memory</title><link>https://deep-diver.github.io/neurips2024/posters/n2rac7lo6k/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/n2rac7lo6k/</guid><description>RNNs represent naturalistic objects in WM using chronological subspaces, defying traditional slot models; object features are less orthogonalized in RNNs vs. perceptual space.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/n2rac7lo6k/cover.png"/></item><item><title>Invariant subspaces and PCA in nearly matrix multiplication time</title><link>https://deep-diver.github.io/neurips2024/posters/wyp8vsl9de/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/wyp8vsl9de/</guid><description>Generalized eigenvalue problems get solved in nearly matrix multiplication time, providing new, faster PCA algorithms!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/wyp8vsl9de/cover.png"/></item><item><title>Limits of Transformer Language Models on Learning to Compose Algorithms</title><link>https://deep-diver.github.io/neurips2024/posters/x7ad0343jz/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/x7ad0343jz/</guid><description>Large Language Models struggle with compositional tasks, requiring exponentially more data than expected for learning compared to learning sub-tasks individually. This paper reveals surprising sample &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/x7ad0343jz/cover.png"/></item><item><title>Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning</title><link>https://deep-diver.github.io/neurips2024/posters/w0oktgspvm/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/w0oktgspvm/</guid><description>Large Multimodal Models (LMMs) are limited by their context length during many-shot in-context learning. This paper introduces Multimodal Task Vectors (MTV), a method to compress numerous in-context &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/w0oktgspvm/cover.png"/></item><item><title>Neural Network Reparametrization for Accelerated Optimization in Molecular Simulations</title><link>https://deep-diver.github.io/neurips2024/posters/fwxohl0bel/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/fwxohl0bel/</guid><description>Accelerate molecular simulations using neural network reparametrization! This flexible method adjusts system complexity, enhances optimization, and maintains continuous access to fine-grained modes, o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/fwxohl0bel/cover.png"/></item><item><title>NeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes</title><link>https://deep-diver.github.io/neurips2024/posters/npoht6wv1f/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/npoht6wv1f/</guid><description>NeuralFuse: A novel add-on module learns input transformations to maintain accuracy in low-voltage DNN inference, achieving up to 57% accuracy recovery and 24% energy savings without retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/npoht6wv1f/cover.png"/></item><item><title>Shuffling Gradient-Based Methods for Nonconvex-Concave Minimax Optimization</title><link>https://deep-diver.github.io/neurips2024/posters/lfy0sut3m9/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lfy0sut3m9/</guid><description>New shuffling gradient methods achieve state-of-the-art oracle complexity for nonconvex-concave minimax optimization problems, offering improved performance and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/lfy0sut3m9/cover.png"/></item><item><title>Thought of Search: Planning with Language Models Through The Lens of Efficiency</title><link>https://deep-diver.github.io/neurips2024/posters/lncsya5us1/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/lncsya5us1/</guid><description>This paper introduces &amp;lsquo;Thought of Search,&amp;rsquo; a novel, efficient planning approach using LLMs that prioritizes soundness and completeness. It leverages LLMs to generate Python code for search components,&amp;hellip;</description></item><item><title>Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series</title><link>https://deep-diver.github.io/neurips2024/posters/3o5ycewetq/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/3o5ycewetq/</guid><description>Tiny Time Mixers (TTMs) achieve state-of-the-art zero/few-shot multivariate time series forecasting, outperforming existing benchmarks while drastically reducing computational requirements.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/3o5ycewetq/cover.png"/></item><item><title>WAGLE: Strategic Weight Attribution for Effective and Modular Unlearning in Large Language Models</title><link>https://deep-diver.github.io/neurips2024/posters/vzogndjmgh/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/vzogndjmgh/</guid><description>WAGLE: A novel weight attribution-guided LLM unlearning framework boosts unlearning performance by strategically identifying and manipulating influential model weights, achieving a better balance betw&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/vzogndjmgh/cover.png"/></item><item><title>Worst-Case Offline Reinforcement Learning with Arbitrary Data Support</title><link>https://deep-diver.github.io/neurips2024/posters/63vajkideu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/63vajkideu/</guid><description>Worst-case offline RL guarantees near-optimal policy performance without data support assumptions, achieving a sample complexity bound of O(Œµ‚Åª¬≤).</description></item></channel></rss>