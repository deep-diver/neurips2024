<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ University of British Columbia on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-university-of-british-columbia/</link><description>Recent content in üè¢ University of British Columbia on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-university-of-british-columbia/index.xml" rel="self" type="application/rss+xml"/><item><title>3D Gaussian Splatting as Markov Chain Monte Carlo</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/ucst4gk6ix/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/ucst4gk6ix/</guid><description>Researchers rethink 3D Gaussian Splatting as MCMC sampling, improving rendering quality and Gaussian control via a novel relocation strategy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/ucst4gk6ix/cover.png"/></item><item><title>Adaptive Randomized Smoothing: Certified Adversarial Robustness for Multi-Step Defences</title><link>https://deep-diver.github.io/neurips2024/spotlight-others/mn4nt01teo/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-others/mn4nt01teo/</guid><description>Adaptive Randomized Smoothing certifies deep learning model predictions against adversarial attacks by cleverly combining randomized smoothing with adaptive, multi-step input masking for improved accu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-others/mn4nt01teo/cover.png"/></item><item><title>Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models</title><link>https://deep-diver.github.io/neurips2024/spotlight-large-language-models/t56j6av8oc/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/spotlight-large-language-models/t56j6av8oc/</guid><description>Adam&amp;rsquo;s superior performance on language models stems from its resilience to heavy-tailed class imbalance, unlike SGD, which struggles with infrequent word losses.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/spotlight-large-language-models/t56j6av8oc/cover.png"/></item><item><title>Implicit Optimization Bias of Next-token Prediction in Linear Models</title><link>https://deep-diver.github.io/neurips2024/posters/xszio6gqgg/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xszio6gqgg/</guid><description>Researchers reveal implicit optimization biases in next-token prediction for language models, showing how gradient descent selects solutions based on data sparsity and a novel margin concept, impactin&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xszio6gqgg/cover.png"/></item></channel></rss>