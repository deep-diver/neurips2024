<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ Pennsylvania State University on NeurIPS 2024</title><link>https://deep-diver.github.io/neurips2024/tags/-pennsylvania-state-university/</link><description>Recent content in üè¢ Pennsylvania State University on NeurIPS 2024</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2024 AI Paper Reviewer</copyright><lastBuildDate>Thu, 26 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/neurips2024/tags/-pennsylvania-state-university/index.xml" rel="self" type="application/rss+xml"/><item><title>Federated Online Prediction from Experts with Differential Privacy: Separations and Regret Speed-ups</title><link>https://deep-diver.github.io/neurips2024/posters/t826pwzlci/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/t826pwzlci/</guid><description>This paper presents novel algorithms achieving speed-ups in differentially private federated online prediction from experts, addressing both stochastic and oblivious adversaries, with rigorous theoret&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/t826pwzlci/cover.png"/></item><item><title>In-Trajectory Inverse Reinforcement Learning: Learn Incrementally From An Ongoing Trajectory</title><link>https://deep-diver.github.io/neurips2024/posters/mjzh9w8qgu/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/mjzh9w8qgu/</guid><description>MERIT-IRL: First in-trajectory IRL framework learns reward &amp;amp; policy incrementally from ongoing trajectories, guaranteeing sub-linear regret.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/mjzh9w8qgu/cover.png"/></item><item><title>Learn more, but bother less: parameter efficient continual learning</title><link>https://deep-diver.github.io/neurips2024/posters/zxtanh5uyb/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/zxtanh5uyb/</guid><description>LB-CL: A novel parameter-efficient continual learning method for LLMs that boosts performance and reduces forgetting by leveraging parametric knowledge transfer and maintaining orthogonal low-rank sub&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/zxtanh5uyb/cover.png"/></item><item><title>Meta-Reinforcement Learning with Universal Policy Adaptation: Provable Near-Optimality under All-task Optimum Comparator</title><link>https://deep-diver.github.io/neurips2024/posters/rpjh69dux2/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/rpjh69dux2/</guid><description>Provable near-optimality in meta-RL is achieved using a novel bilevel optimization framework and universal policy adaptation algorithm.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/rpjh69dux2/cover.png"/></item><item><title>pFedClub: Controllable Heterogeneous Model Aggregation for Personalized Federated Learning</title><link>https://deep-diver.github.io/neurips2024/posters/xw6ga9i4ea/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/xw6ga9i4ea/</guid><description>pFedClub: Controllable heterogeneous model aggregation boosts personalized federated learning by generating reasonable-sized, personalized models, significantly cutting computational costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/xw6ga9i4ea/cover.png"/></item><item><title>Speculative Monte-Carlo Tree Search</title><link>https://deep-diver.github.io/neurips2024/posters/g1hxcic0wi/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/neurips2024/posters/g1hxcic0wi/</guid><description>Speculative MCTS accelerates AlphaZero training by implementing speculative execution, enabling parallel processing of future moves and reducing latency by up to 5.8x.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/neurips2024/posters/g1hxcic0wi/cover.png"/></item></channel></rss>