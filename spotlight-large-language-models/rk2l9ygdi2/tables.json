[{"figure_path": "rk2L9YGDi2/tables/tables_4_1.jpg", "caption": "Table 1: On-device results (A100): The optimal tree configuration and speedup for different pairs of draft and target models, and different temperatures, for SEQUOIA vs. SpecInfer. We specify the average number of generated tokens per decoding step in parentheses, next to the speedup factor. SEQUOIA attains up to 4.04\u00d7 speedup on an A100. The speed of incremental decoding is 24.2ms/token with Huggingface. The draft model speed is 0.5ms/token. TBT refers to time between tokens.", "description": "This table presents the experimental results of SEQUOIA and SpecInfer on an A100 GPU.  It shows the optimal tree configuration (size and depth), speedup achieved by SEQUOIA compared to SpecInfer, time between tokens (TBT), and the average number of generated tokens per decoding step for various combinations of draft and target language models and different temperatures. The baseline speed of incremental decoding using HuggingFace is also provided, along with the draft model's speed.", "section": "4.1 End-to-end Results"}, {"figure_path": "rk2L9YGDi2/tables/tables_7_1.jpg", "caption": "Table 1: On-device results (A100): The optimal tree configuration and speedup for different pairs of draft and target models, and different temperatures, for SEQUOIA vs. SpecInfer. We specify the average number of generated tokens per decoding step in parentheses, next to the speedup factor. SEQUOIA attains up to 4.04\u00d7 speedup on an A100. The speed of incremental decoding is 24.2ms/token with Huggingface. The draft model speed is 0.5ms/token. TBT refers to time between tokens.", "description": "This table presents the results of on-device experiments conducted on an A100 GPU, comparing the performance of SEQUOIA and SpecInfer.  The table shows the optimal tree configuration (size and depth) for each model and temperature combination, the speedup achieved by SEQUOIA compared to SpecInfer, the time between tokens (TBT), and the average number of tokens generated per decoding step.  The results highlight SEQUOIA's improved speed, showing speedups of up to 4.04x. Note that incremental decoding speed and draft model speed are provided for context.", "section": "4.1 End-to-end Results"}, {"figure_path": "rk2L9YGDi2/tables/tables_7_2.jpg", "caption": "Table 2: Offloading results (L40): The optimal tree configuration and speedup for different pairs of draft and target models, and different temperatures, for SEQUOIA vs. SpecInfer. We specify the average number of generated tokens per decoding step in parentheses, next to the speedup factor. SEQUOIA attains up to 9.5\u00d7 speedup in the offloading setting on an L40. The speed of incremental decoding is 5.7s/token with DeepSpeed Zero Inference. TBT refers to time between tokens.", "description": "This table shows the performance of SEQUOIA and SpecInfer on an L40 GPU in an offloading setting.  It presents speedup factors for different LLMs (Llama2-70B-chat, Llama3-70B-Instruct) using various draft models at different temperatures.  The speedup is relative to incremental decoding using DeepSpeed Zero Inference.  The table also provides the optimal tree configuration (size and depth) used by SEQUOIA for each experiment, and the average number of tokens generated per decoding step.", "section": "4.1 End-to-end Results"}, {"figure_path": "rk2L9YGDi2/tables/tables_24_1.jpg", "caption": "Table 1: On-device results (A100): The optimal tree configuration and speedup for different pairs of draft and target models, and different temperatures, for SEQUOIA vs. SpecInfer. We specify the average number of generated tokens per decoding step in parentheses, next to the speedup factor. SEQUOIA attains up to 4.04\u00d7 speedup on an A100. The speed of incremental decoding is 24.2ms/token with Huggingface. The draft model speed is 0.5ms/token. TBT refers to time between tokens.", "description": "This table presents the results of on-device experiments conducted on an A100 GPU.  It compares the performance of SEQUOIA and SpecInfer for various large language models (LLMs) at different temperatures. The table shows the optimal tree configuration (size and depth) used for SEQUOIA, the speedup achieved by SEQUOIA compared to SpecInfer, and the time between tokens (TBT) for both methods.  The average number of generated tokens per decoding step is also provided for both SEQUOIA and SpecInfer.", "section": "4.1 End-to-end Results"}, {"figure_path": "rk2L9YGDi2/tables/tables_24_2.jpg", "caption": "Table 3: On-device results (A100): The optimal tree configuration and speedup for different pairs of draft and target models, and different temperatures, for SEQUOIA vs. SpecInfer. We specify the average number of generated tokens per decoding step in parentheses, next to the speedup factor. SEQUOIA attains up to 4.04\u00d7 speedup on an A100.", "description": "This table presents the results of on-device experiments conducted on an A100 GPU.  It compares the performance of the SEQUOIA algorithm against SpecInfer for various Large Language Models (LLMs).  The table shows the optimal tree configuration (size and depth) determined by SEQUOIA, the achieved speedup compared to SpecInfer, and the average number of generated tokens per decoding step for different models, temperatures, and datasets. The speedup values represent the improvement in decoding speed achieved by SEQUOIA compared to SpecInfer. The numbers in parentheses represent the average number of tokens generated per decoding step by SEQUOIA. ", "section": "4.1 End-to-end Results"}, {"figure_path": "rk2L9YGDi2/tables/tables_25_1.jpg", "caption": "Table 1: On-device results (A100): The optimal tree configuration and speedup for different pairs of draft and target models, and different temperatures, for SEQUOIA vs. SpecInfer. We specify the average number of generated tokens per decoding step in parentheses, next to the speedup factor. SEQUOIA attains up to 4.04\u00d7 speedup on an A100. The speed of incremental decoding is 24.2ms/token with Huggingface. The draft model speed is 0.5ms/token. TBT refers to time between tokens.", "description": "This table presents the on-device (A100 GPU) experimental results comparing SEQUOIA and SpecInfer.  It shows the optimal tree configuration (size and depth), speedup achieved by SEQUOIA relative to SpecInfer, time between tokens (TBT), and the average number of generated tokens per step for various model pairs (Llama2-7B, Llama2-13B) and different temperatures. The baseline incremental decoding speed and draft model speed are also provided.", "section": "4.1 End-to-end Results"}, {"figure_path": "rk2L9YGDi2/tables/tables_25_2.jpg", "caption": "Table 6: A sweep of tree configurations and their corresponding speedups of SpecInfer [28] on A100. The draft model is JF68M, and the target model is Llama2-7B in stochastic decoding. The evaluated dataset is C4. The default tree configuration in SpecInfer is 5 \u00d7 8, which brings 2.47\u00d7 speedup while SEQUOIA achieves 3.18\u00d7 speedup, surpassing all tree configurations below.", "description": "This table shows the speedups achieved by SpecInfer for various tree configurations in stochastic decoding using Llama2-7B as the target model and JF68M as the draft model on the C4 dataset.  It highlights that SEQUOIA's speedup of 3.18x surpasses all of SpecInfer's configurations.", "section": "4.2.1 The Scalability of SEQUOIA"}, {"figure_path": "rk2L9YGDi2/tables/tables_25_3.jpg", "caption": "Table 7: A sweep of tree configurations and their corresponding speedups of SpecInfer [28] on L40 offloading setting. The draft model is Llama2-7B-chat, and the target model is Llama2-70B-chat in stochastic decoding. The evaluated dataset is MT-Bench. SEQUOIA achieves 8.4\u00d7 speedup, surpassing all tree configurations below.", "description": "This table shows the speedup achieved by SpecInfer for various tree configurations in the L40 offloading setting using Llama2-7B-chat as the draft model and Llama2-70B-chat as the target model, evaluated on the MT-Bench dataset.  It highlights the performance difference between SEQUOIA and SpecInfer across various tree structures (width and depth).", "section": "4.1 End-to-end Results"}, {"figure_path": "rk2L9YGDi2/tables/tables_26_1.jpg", "caption": "Table 8: We compare the robustness of the SEQUOIA sampling and verification algorithm to the top-p hyperparameter, relative to SpecInfer and top-k sampling. We present total speedups on an A100 GPU for the different methods (number of generated tokens in parentheses). We hold the tree structure fixed across methods, use JF68M as the draft model, and Llama2-7B as the target model.", "description": "This table compares the robustness of three different sampling and verification algorithms (SEQUOIA, SpecInfer, and top-k sampling) to variations in the top-p hyperparameter.  It reports the total speedup achieved by each algorithm on an A100 GPU, showing the average number of generated tokens in parentheses. The experiment kept the tree structure consistent across all three algorithms and used JF68M as the draft model and Llama2-7B as the target model.", "section": "4.2.2 Robustness of SEQUOIA Sampling Algorithm"}]