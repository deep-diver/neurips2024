[{"figure_path": "Y13gSfTjGr/figures/figures_1_1.jpg", "caption": "Figure 1: Revisiting cosine optimality for language models. We revisit the observation from Chinchilla (Hoffmann et al., 2022) that in order to achieve the best model after a certain training length (tokens), the cosine schedule must match the total duration of training. This comes at the cost of neither being able to stop before or going beyond the cycle\u2014an issue we show how to alleviate in Section 3.", "description": "This figure shows that to achieve optimal performance with a cosine learning rate schedule, the total training duration must match the cycle length.  The left panel shows that for different training lengths, the optimal perplexity is only achieved when the cycle length of the cosine schedule matches the training duration. The right panel shows the corresponding learning rate schedules used for each of these training durations.  This limitation of the cosine schedule motivates the exploration of alternative scheduling techniques presented in Section 3 of the paper.", "section": "Revisiting cosine optimality for language models"}, {"figure_path": "Y13gSfTjGr/figures/figures_2_1.jpg", "caption": "Figure 2: Illustration of schedules. Cosine (red) follows a slow decrease in learning rate, typically to 10% of the maximum for LLMs. The alternative is characterized by an aggressive decrease in learning rate, e.g., via a linear (blue) or square root (yellow) cooldown.", "description": "The figure compares different learning rate schedules used in training large language models.  It shows the cosine schedule, which gradually reduces the learning rate over a long period, and two alternative schedules that use a constant learning rate followed by a sharp cooldown (linear and square root). The plot illustrates the different shapes of these schedules and how they vary over a certain number of training steps.", "section": "3 A Different Route: Constant Learning Rate with Cooldown"}, {"figure_path": "Y13gSfTjGr/figures/figures_3_1.jpg", "caption": "Figure 3: The difference in loss curves of cosine vs. constant learning rate with cooldown. The cooldown phase initiates a sharp decrease in loss (left) to match cosine; the training perplexity follows the same behavior (Fig. 15). We find the LR sensitivity (right) to be similar for both schedules, albeit less for cooldown, where the optimum lies slightly below at half of the optimal cosine maximum LR.", "description": "This figure compares the performance of the cosine learning rate schedule with a constant learning rate schedule that incorporates a cooldown phase. The left panel shows the loss curves for both schedules, demonstrating that the cooldown schedule achieves a similar sharp decrease in loss as the cosine schedule. The right panel shows the learning rate sensitivity for both schedules, indicating that the cooldown schedule is less sensitive to variations in the learning rate.  The optimal learning rate for the cooldown schedule is slightly lower than the optimal learning rate for the cosine schedule.", "section": "A Different Route: Constant Learning Rate with Cooldown"}, {"figure_path": "Y13gSfTjGr/figures/figures_3_2.jpg", "caption": "Figure 4: A different cooldown schedule can improve performance. Perhaps surprisingly, we find that a different decay phase in the functional form of (1-sqrt) can consistently outperform the standard linear decay, where both are better than the chosen (untuned) cosine for long lengths.", "description": "This figure compares the performance of different cooldown schedules (linear and 1 - sqrt) against the cosine schedule for long training durations.  The results show that a square root decay function (1-sqrt) consistently outperforms the linear decay function, and both outperform the cosine schedule, especially for longer training runs.", "section": "3 A Different Route: Constant Learning Rate with Cooldown"}, {"figure_path": "Y13gSfTjGr/figures/figures_4_1.jpg", "caption": "Figure 5: Longer cooldown helps to achieve lower loss. We investigate the effect of the cooldown length as a fraction of the total steps for a 124M model. We find that the cooldown surpasses cosine between 10\u201320% of steps (left), but largely stops improving when done over a majority of training. This also holds when sweeping the LR (right). Additional ablations are provided in Appendix B.1.", "description": "This figure shows the relationship between the cooldown length (as a fraction of total training steps) and the final loss (perplexity) achieved for a 124M parameter model.  The left panel shows that increasing the cooldown length to 10-20% of the total training steps significantly improves the model's performance, surpassing the cosine schedule's performance.  However, increasing the cooldown beyond this point does not result in further improvement. The right panel demonstrates the robustness of this finding by showing similar results across different learning rates.", "section": "3.2 Experimental Comparison"}, {"figure_path": "Y13gSfTjGr/figures/figures_4_2.jpg", "caption": "Figure 6: A long training run suggests that a small number of cooldown steps can match cosine for long training. From Fig. 5 and Fig. 20, we find that the required duration of cooldown to match the cosine loss decreases with longer training; we validate with a long training run (200k steps, same LRs), and find that just 10k cooldown steps almost perfectly match cosine in performance.", "description": "This figure shows the results of a long training run (200k steps) comparing the performance of a constant learning rate with a short cooldown (10k steps, which is 5% of the total steps) against the cosine learning rate schedule.  The results show that a short cooldown is sufficient to match the performance of the cosine schedule for long training runs, confirming the findings from previous figures (5 and 20) that the needed cooldown duration shrinks with longer training durations.", "section": "3 A Different Route: Constant Learning Rate with Cooldown"}, {"figure_path": "Y13gSfTjGr/figures/figures_5_1.jpg", "caption": "Figure 3: The difference in loss curves of cosine vs. constant learning rate with cooldown. The cooldown phase initiates a sharp decrease in loss (left) to match cosine; the training perplexity follows the same behavior (Fig. 15). We find the LR sensitivity (right) to be similar for both schedules, albeit less for cooldown, where the optimum lies slightly below at half of the optimal cosine maximum LR.", "description": "This figure compares the loss curves and learning rate sensitivity of two learning rate schedules: cosine and constant learning rate with cooldown. The left panel shows that the cooldown schedule achieves a sharp decrease in loss similar to the cosine schedule, while maintaining similar perplexity. The right panel demonstrates that the optimal learning rate for both schedules is comparable, with the cooldown schedule exhibiting slightly lower sensitivity to changes in the learning rate.", "section": "3 A Different Route: Constant Learning Rate with Cooldown"}, {"figure_path": "Y13gSfTjGr/figures/figures_5_2.jpg", "caption": "Figure 8: The performances of both schedules also match for downstream tasks. We run a realistic setting of a 1B model trained for 100B tokens of FineWeb and establish matching performance of both schedules for common LLM benchmarks. Interestingly, there is a similar boost in performance with the cooldown. Detailed numbers over the course of training and 460B token runs are in Appx. B.5.", "description": "This figure compares the performance of cosine and linear cooldown schedules on a 1B parameter model trained with 100B tokens from the FineWeb dataset.  The left subplot shows the aggregated score across various downstream tasks throughout the training process. The right subplot shows the final aggregated scores for both schedules after 100B tokens.  The results indicate that both schedules achieve comparable performance on downstream tasks, with a potential performance boost observed at the beginning of the cooldown phase.", "section": "3.3 Scaling Up: 1B and 8B Models"}, {"figure_path": "Y13gSfTjGr/figures/figures_5_3.jpg", "caption": "Figure 3: The difference in loss curves of cosine vs. constant learning rate with cooldown. The cooldown phase initiates a sharp decrease in loss (left) to match cosine; the training perplexity follows the same behavior (Fig. 15). We find the LR sensitivity (right) to be similar for both schedules, albeit less for cooldown, where the optimum lies slightly below at half of the optimal cosine maximum LR.", "description": "The figure compares two learning rate schedules: cosine and constant learning rate with cooldown. The left panel shows the loss curves, demonstrating that the cooldown schedule achieves a similar sharp decrease in loss as the cosine schedule, resulting in comparable training perplexity. The right panel illustrates that both schedules have similar sensitivities to changes in learning rate, however, the cooldown schedule's optimal learning rate is slightly lower than that of the cosine schedule.", "section": "3 A Different Route: Constant Learning Rate with Cooldown"}, {"figure_path": "Y13gSfTjGr/figures/figures_5_4.jpg", "caption": "Figure 9: Results at 8B scale. We validate the behavior with a much larger model (architecture of Llama 3) for a single short run (12B tokens of FineWeb-Edu), where the cooldown matches the cosine schedule again.", "description": "This figure shows the training perplexity curves for an 8B parameter model trained on 12B tokens of the FineWeb-Edu dataset.  Two learning rate schedules are compared: a cosine schedule and a 1-sqrt cooldown schedule (where the cooldown constitutes 20% of the total training steps). The results demonstrate that the 1-sqrt cooldown schedule achieves a comparable final training perplexity to the cosine schedule, even for this much larger model size. This finding supports the authors' claim that the 1-sqrt cooldown is a reliable alternative to the cosine schedule for training large language models, regardless of model size.", "section": "3.3 Scaling Up: 1B and 8B Models"}, {"figure_path": "Y13gSfTjGr/figures/figures_6_1.jpg", "caption": "Figure 10: SWA improves generalization and simulates decayed learning rates. Using SWA for the constant LR phase (left) strongly boosts the loss, but a gap to the cooldown remains. SWA also improves the generalization of a cosine schedule (right), where the intermediate checkpoints of SWA largely overlap with optimal loss trajectory of shorter cosine runs.", "description": "This figure shows the effect of Stochastic Weight Averaging (SWA) on two different learning rate schedules: a constant learning rate with cooldown and a cosine learning rate schedule.  The left panel demonstrates that applying SWA to a constant learning rate schedule significantly improves the model's performance, though it doesn't fully close the gap to the performance achieved with the explicit cooldown.  The right panel shows that SWA also improves the performance of a cosine learning rate schedule, with the SWA checkpoints closely tracking the optimal loss trajectory of shorter cosine training runs. This suggests that SWA acts as a form of implicit learning rate decay.", "section": "4 Do We Even Need to Cooldown?"}, {"figure_path": "Y13gSfTjGr/figures/figures_6_2.jpg", "caption": "Figure 11: The cooldown schedule outperforms SFO even when tuning momentum parameters. We find that SFO is sensitive to the choice of (\u03b2\u2081, \u03b2\u2082) momentum parameters. It gives strong performance for well-tuned momentum, but falls short of cooldown.", "description": "This figure compares the performance of a schedule-free optimizer (SFO) with a linear cooldown schedule for a 210M parameter language model.  Two different momentum parameter settings ((\u03b2\u2081, \u03b2\u2082) = (0.90, 0.95) and (\u03b2\u2081, \u03b2\u2082) = (0.95, 0.99)) are used for both the SFO and linear cooldown to assess the impact of these parameters on performance. The graph plots perplexity against training steps, showing that, regardless of the momentum setting, the linear cooldown always yields lower perplexity (better performance) than the SFO.", "section": "4 Do We Even Need to Cooldown?"}, {"figure_path": "Y13gSfTjGr/figures/figures_8_1.jpg", "caption": "Figure 12: Cooldown LR schedule + SWA scale reliably. We train a range of models (33M\u2013360M), each for three different cosine lengths. We then run each model with a constant LR just once and take the snapshots at the same token count as cosine (for SWA) or perform post-train cooldowns to the same length. Each final model is represented by a dot. Left: The loss curve envelopes. Right: The perplexity of cosine (y-axis) vs. cooldowns and SWA. Points on the diagonal indicate the same loss for both methods; above outperforming cosine, below fall short. We see alignment between both methods.", "description": "This figure demonstrates the scalability and reliability of the cooldown learning rate schedule and stochastic weight averaging (SWA) in comparison to the cosine schedule.  It shows that models trained using the constant learning rate with cooldown or SWA achieve similar performance to those trained with the cosine schedule, with the cooldown method often outperforming cosine. The left panel displays loss envelopes for various model sizes, showing similar trends across methods. The right panel compares cosine perplexity against cooldown and SWA perplexity, highlighting that models trained with either alternative reach similar performance to cosine-trained models for the same FLOPs.", "section": "5 Scaling Up: 1B and 8B Models"}, {"figure_path": "Y13gSfTjGr/figures/figures_8_2.jpg", "caption": "Figure 13: Scaling laws for a fraction of the cost. The reliable behavior of both the cooldown schedule and SWA allows scaling experiments with a drastic reduce in both compute and GPU hours; in both our experiments (left) and the original Chinchilla (right) a factor of 1 or less. The more training runs are performed per model size (e.g. 4 for Chinchilla), the larger the difference becomes.", "description": "This figure compares the computational cost (FLOPs and GPU hours) of scaling experiments using the cosine schedule (Chinchilla's approach) versus the proposed cooldown schedule and stochastic weight averaging (SWA).  The left panel shows the results from the authors' experiments, demonstrating a significant reduction in both FLOPs and GPU hours when using the cooldown/SWA methods. The right panel illustrates the potential savings for the original Chinchilla experiments, further highlighting the efficiency gains.", "section": "5 Scaling Up: 1B and 8B Models"}, {"figure_path": "Y13gSfTjGr/figures/figures_8_3.jpg", "caption": "Figure 13: Scaling laws for a fraction of the cost. The reliable behavior of both the cooldown schedule and SWA allows scaling experiments with a drastic reduce in both compute and GPU hours; in both our experiments (left) and the original Chinchilla (right) a factor of 1 or less. The more training runs are performed per model size (e.g. 4 for Chinchilla), the larger the difference becomes.", "description": "This figure demonstrates the significant reduction in computational cost achieved by using the proposed cooldown schedule and stochastic weight averaging (SWA) in scaling law experiments.  The left panel shows the computational savings (FLOPS and GPU hours) for the experiments conducted in the paper, while the right panel illustrates the savings compared to the original Chinchilla experiments.  It highlights that the proposed methods substantially reduce the computational requirements for scaling experiments by requiring fewer, reusable training runs.", "section": "5 Scaling Up: 1B and 8B Models"}, {"figure_path": "Y13gSfTjGr/figures/figures_17_1.jpg", "caption": "Figure 3: The difference in loss curves of cosine vs. constant learning rate with cooldown. The cooldown phase initiates a sharp decrease in loss (left) to match cosine; the training perplexity follows the same behavior (Fig. 15). We find the LR sensitivity (right) to be similar for both schedules, albeit less for cooldown, where the optimum lies slightly below at half of the optimal cosine maximum LR.", "description": "This figure compares the loss curves and learning rate sensitivity of two learning rate schedules: cosine and constant learning rate with cooldown.  The left panel shows that the cooldown phase in the constant learning rate schedule causes a sharp decrease in loss, similar to the behavior observed with the cosine schedule. The right panel shows that the optimal learning rate for both schedules is comparable, but the constant learning rate schedule with cooldown exhibits slightly less sensitivity to variations in learning rate and its optimal value is about half the maximum cosine learning rate.", "section": "3 A Different Route: Constant Learning Rate with Cooldown"}, {"figure_path": "Y13gSfTjGr/figures/figures_18_1.jpg", "caption": "Figure 3: The difference in loss curves of cosine vs. constant learning rate with cooldown. The cooldown phase initiates a sharp decrease in loss (left) to match cosine; the training perplexity follows the same behavior (Fig. 15). We find the LR sensitivity (right) to be similar for both schedules, albeit less for cooldown, where the optimum lies slightly below at half of the optimal cosine maximum LR.", "description": "This figure compares the loss curves and learning rate sensitivity of two learning rate schedules: cosine annealing and constant learning rate with cooldown. The left panel shows that the cooldown schedule produces a sharp decrease in loss, comparable to the cosine schedule.  The right panel demonstrates that the optimal learning rate for both schedules is similar, but the constant learning rate with cooldown shows slightly less sensitivity, with the optimum learning rate being around half of the cosine schedule's optimal maximum learning rate.", "section": "A Different Route: Constant Learning Rate with Cooldown"}, {"figure_path": "Y13gSfTjGr/figures/figures_19_1.jpg", "caption": "Figure 3: The difference in loss curves of cosine vs. constant learning rate with cooldown. The cooldown phase initiates a sharp decrease in loss (left) to match cosine; the training perplexity follows the same behavior (Fig. 15). We find the LR sensitivity (right) to be similar for both schedules, albeit less for cooldown, where the optimum lies slightly below at half of the optimal cosine maximum LR.", "description": "This figure compares the loss curves and learning rate sensitivity of two different learning rate schedules: cosine and constant learning rate with cooldown.  The left panel shows that both schedules achieve a similar sharp decrease in loss during the cooldown phase. The right panel demonstrates that both schedules exhibit similar sensitivity to the learning rate, although the optimal learning rate for the cooldown schedule is slightly lower than that of the cosine schedule.", "section": "3 A Different Route: Constant Learning Rate with Cooldown"}, {"figure_path": "Y13gSfTjGr/figures/figures_19_2.jpg", "caption": "Figure 1: Revisiting cosine optimality for language models. We revisit the observation from Chinchilla (Hoffmann et al., 2022) that in order to achieve the best model after a certain training length (tokens), the cosine schedule must match the total duration of training. This comes at the cost of neither being able to stop before or going beyond the cycle\u2014an issue we show how to alleviate in Section 3.", "description": "This figure shows that the cosine learning rate schedule achieves optimal loss only when its cycle length matches the training duration.  It highlights the problem that stopping training before or extending it beyond the cycle leads to suboptimal results. The authors illustrate how a constant learning rate with cooldown addresses this limitation.", "section": "1 Introduction"}, {"figure_path": "Y13gSfTjGr/figures/figures_19_3.jpg", "caption": "Figure 1: Revisiting cosine optimality for language models. We revisit the observation from Chinchilla (Hoffmann et al., 2022) that in order to achieve the best model after a certain training length (tokens), the cosine schedule must match the total duration of training. This comes at the cost of neither being able to stop before or going beyond the cycle\u2014an issue we show how to alleviate in Section 3.", "description": "The figure shows two plots. The left plot shows the perplexity versus steps for different cycle lengths using the cosine scheduler. It demonstrates that the cosine scheduler only reaches optimality when the cycle length matches the total training duration. The right plot shows the learning rate versus steps for different cycle lengths using the cosine scheduler. It highlights that the learning rate decreases gradually with training steps, reaching its minimum value at the end of the cycle. This behavior demonstrates the limitation of cosine schedulers in terms of flexibility and ability to achieve optimality across different training lengths.", "section": "2 Background: Cosine Learning Rate Schedule for LLMs"}, {"figure_path": "Y13gSfTjGr/figures/figures_20_1.jpg", "caption": "Figure 19: Parabola shape of the relationship between cooldown length and final perplexity. We repeat the experiment from Fig. 5 with a 210M parameter model and the zoomed-in view on the left.", "description": "This figure shows the relationship between the length of the cooldown phase and the final perplexity achieved during training for a 210M parameter language model. The experiment was repeated from Figure 5 but with a 210M parameter model. The x-axis represents the fraction of training steps dedicated to the cooldown phase, and the y-axis represents the final perplexity. It shows that there's an optimal cooldown length that minimizes the perplexity, and that extending the cooldown beyond this point does not further improve the results.  A zoomed-in view is presented on the left for better visualization of the optimal range.", "section": "3 A Different Route: Constant Learning Rate with Cooldown"}, {"figure_path": "Y13gSfTjGr/figures/figures_20_2.jpg", "caption": "Figure 19: Parabola shape of the relationship between cooldown length and final perplexity. We repeat the experiment from Fig. 5 with a 210M parameter model and the zoomed-in view on the left.", "description": "This figure shows the relationship between the length of the cooldown period and the final perplexity achieved by a 210M parameter language model. The left panel shows a zoomed-in view of the relationship, while the right panel provides a broader overview.  The results suggest a parabolic relationship, where increasing the cooldown length initially improves performance but then leads to diminishing returns.", "section": "3 A Different Route: Constant Learning Rate with Cooldown"}, {"figure_path": "Y13gSfTjGr/figures/figures_20_3.jpg", "caption": "Figure 3: The difference in loss curves of cosine vs. constant learning rate with cooldown. The cooldown phase initiates a sharp decrease in loss (left) to match cosine; the training perplexity follows the same behavior (Fig. 15). We find the LR sensitivity (right) to be similar for both schedules, albeit less for cooldown, where the optimum lies slightly below at half of the optimal cosine maximum LR.", "description": "This figure compares the loss curves and learning rate sensitivity of two learning rate schedules: cosine annealing and constant learning rate with cooldown.  The left panel shows that a cooldown phase added to a constant learning rate schedule leads to a sharp decrease in loss similar to cosine annealing. The right panel shows the learning rate sensitivity of both schedules; they are similar, but the optimal learning rate for the constant learning rate with cooldown is slightly lower than the optimal learning rate for cosine annealing.", "section": "A Different Route: Constant Learning Rate with Cooldown"}, {"figure_path": "Y13gSfTjGr/figures/figures_21_1.jpg", "caption": "Figure 3: The difference in loss curves of cosine vs. constant learning rate with cooldown. The cooldown phase initiates a sharp decrease in loss (left) to match cosine; the training perplexity follows the same behavior (Fig. 15). We find the LR sensitivity (right) to be similar for both schedules, albeit less for cooldown, where the optimum lies slightly below at half of the optimal cosine maximum LR.", "description": "This figure compares the performance of cosine learning rate scheduling with a constant learning rate with cooldown.  The left panel shows that the cooldown phase produces a sharp drop in loss, mirroring the behavior of cosine scheduling. The right panel shows that both methods have similar sensitivity to learning rate, although the constant + cooldown approach exhibits slightly lower sensitivity.  The optimal learning rate for the constant + cooldown is also slightly lower than that for the cosine schedule. ", "section": "A Different Route: Constant Learning Rate with Cooldown"}, {"figure_path": "Y13gSfTjGr/figures/figures_21_2.jpg", "caption": "Figure 23: Final validation perplexity of all models in scaling experiments across different runs.", "description": "This figure shows the final validation perplexity for different model sizes (33M, 53M, 60M, 93M, 124M, 151M, 210M, 360M parameters) across four different training methods: Cosine LR, Cooldown, SWA Cosine, and SWA Constant.  The bar chart visually compares the performance of each method for each model size, allowing for a direct assessment of how different training schedules affect the final validation perplexity. This data supports the paper's findings on the effectiveness of alternative training schedules compared to the traditional cosine learning rate schedule.", "section": "3.3 Scaling Up: 1B and 8B Models"}, {"figure_path": "Y13gSfTjGr/figures/figures_21_3.jpg", "caption": "Figure 13: Scaling laws for a fraction of the cost. The reliable behavior of both the cooldown schedule and SWA allows scaling experiments with a drastic reduce in both compute and GPU hours; in both our experiments (left) and the original Chinchilla (right) a factor of 4 or less. The more training runs are performed per model size (e.g. 4 for Chinchilla), the larger the difference becomes.", "description": "This figure compares the computational cost (in terms of FLOPs and GPU hours) of scaling experiments using the cosine schedule (Chinchilla's approach) versus the proposed cooldown and SWA methods. The left panel shows the results of the authors' experiments, demonstrating a significant reduction in both compute and GPU hours when using cooldown and SWA compared to the cosine schedule.  The right panel shows a similar cost reduction for the Chinchilla experiments, highlighting the overall efficiency gains from using the alternative methods. The reduction in cost is more pronounced when more training runs are performed per model size, emphasizing the scalability and cost-effectiveness of the proposed approach.", "section": "5 Scaling Up: 1B and 8B Models"}, {"figure_path": "Y13gSfTjGr/figures/figures_22_1.jpg", "caption": "Figure 13: Scaling laws for a fraction of the cost. The reliable behavior of both the cooldown schedule and SWA allows scaling experiments with a drastic reduce in both compute and GPU hours; in both our experiments (left) and the original Chinchilla (right) a factor of 1 or less. The more training runs are performed per model size (e.g. 4 for Chinchilla), the larger the difference becomes.", "description": "This figure compares the computational cost of obtaining scaling laws using the cosine learning rate schedule (as in the Chinchilla paper) and the proposed cooldown schedule and stochastic weight averaging (SWA) techniques. The left panel shows the FLOPS and GPU hours used in the authors' experiments across various model sizes, highlighting the significant reduction in computational resources achieved by the proposed methods. The right panel provides an estimation of the computational savings that would have been achieved in the Chinchilla experiments if a cooldown schedule was used instead of the cosine schedule.", "section": "5 Scaling Up: 1B and 8B Models"}, {"figure_path": "Y13gSfTjGr/figures/figures_22_2.jpg", "caption": "Figure 26: Validation Loss Curves (Perplexity) of all Models in Scaling Experiments. We visualize all training runs for the models used in the scaling experiments in Section 5.", "description": "This figure shows the validation loss curves (perplexity) for all the models used in the scaling experiments described in Section 5 of the paper.  The curves illustrate the training progress for different model sizes and configurations (Cosine LR, Constant LR, SWA Cosine LR, and SWA Constant LR) over a number of training steps.  The visualization helps to compare the performance of different training schedules and the impact of Stochastic Weight Averaging (SWA) on the learning process.", "section": "B.3 Learning Curves for All Models of Scaling Experiments"}, {"figure_path": "Y13gSfTjGr/figures/figures_23_1.jpg", "caption": "Figure 27: Results Transfer to OpenWebText2. We see the same behavior for cooldown schedules and SWA, verifying the reliability of our findings.", "description": "This figure demonstrates the transferability of the findings from the SlimPajama dataset to the OpenWebText2 dataset.  It shows that the consistent performance improvements observed with cooldown schedules and Stochastic Weight Averaging (SWA) on SlimPajama also hold true for OpenWebText2, reinforcing the robustness and generalizability of the proposed methods.", "section": "Additional Experiments on OpenWebText2"}, {"figure_path": "Y13gSfTjGr/figures/figures_23_2.jpg", "caption": "Figure 13: Scaling laws for a fraction of the cost. The reliable behavior of both the cooldown schedule and SWA allows scaling experiments with a drastic reduce in both compute and GPU hours; in both our experiments (left) and the original Chinchilla (right) a factor of 1 or less. The more training runs are performed per model size (e.g., 4 for Chinchilla), the larger the difference becomes.", "description": "This figure compares the computational cost (in FLOPs and GPU hours) of scaling experiments using the proposed cooldown and SWA methods against the traditional Chinchilla method.  It shows that the cooldown and SWA methods significantly reduce the computational resources required to obtain scaling laws, even more so when multiple experiments are run for each model size.", "section": "5 Scaling Up: 1B and 8B Models"}, {"figure_path": "Y13gSfTjGr/figures/figures_24_1.jpg", "caption": "Figure 3: The difference in loss curves of cosine vs. constant learning rate with cooldown. The cooldown phase initiates a sharp decrease in loss (left) to match cosine; the training perplexity follows the same behavior (Fig. 15). We find the LR sensitivity (right) to be similar for both schedules, albeit less for cooldown, where the optimum lies slightly below at half of the optimal cosine maximum LR.", "description": "This figure compares the loss curves and learning rate sensitivity of two learning rate schedules: cosine annealing and constant learning rate with cooldown. The left panel shows that the cooldown phase in the constant learning rate schedule leads to a sharp decrease in loss, similar to the cosine annealing schedule. The right panel demonstrates that the optimal learning rate for both schedules is comparable, with the constant learning rate schedule showing slightly less sensitivity and the optimum learning rate being slightly lower than that for the cosine annealing schedule.", "section": "3 A Different Route: Constant Learning Rate with Cooldown"}, {"figure_path": "Y13gSfTjGr/figures/figures_24_2.jpg", "caption": "Figure 1: Revisiting cosine optimality for language models. We revisit the observation from Chinchilla (Hoffmann et al., 2022) that in order to achieve the best model after a certain training length (tokens), the cosine schedule must match the total duration of training. This comes at the cost of neither being able to stop before or going beyond the cycle \u2013 an issue we show how to alleviate in Section 3.", "description": "This figure shows the perplexity and learning rate curves for language models trained with cosine and constant learning rate schedules.  It highlights that cosine schedules achieve optimal performance only when the training length perfectly matches the schedule cycle length.  Stopping training early or continuing beyond the cycle leads to suboptimal results.  This motivates the exploration of alternative training schedules that can produce good performance without this constraint.", "section": "2 Background: Cosine Learning Rate Schedule for LLMs"}]