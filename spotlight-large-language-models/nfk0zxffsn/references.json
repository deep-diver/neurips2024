{"references": [{"fullname_first_author": "Ayush Agrawal", "paper_title": "Do language models know when they're hallucinating references?", "publication_date": "2024-04-01", "reason": "This paper explores the phenomenon of language models hallucinating references, a critical issue in evaluating the reliability of LLM outputs and a key topic of this paper."}, {"fullname_first_author": "Amos Azaria", "paper_title": "The internal state of an LLM knows when it's lying", "publication_date": "2023-12-01", "reason": "This paper investigates the ability of LLMs to detect their own lying, directly addressing the core problem of hallucination detection."}, {"fullname_first_author": "Chao Chen", "paper_title": "Inside: LLMs' internal states retain the power of hallucination detection", "publication_date": "2024-04-01", "reason": "This paper delves into the internal states of LLMs to identify indicators of hallucination, a method directly compared against in this paper"}, {"fullname_first_author": "Collin Burns", "paper_title": "Discovering latent knowledge in language models without supervision", "publication_date": "2023-04-01", "reason": "This paper proposes a method for discovering latent knowledge in LLMs without labeled data, relevant to the unsupervised nature of this paper's approach."}, {"fullname_first_author": "Stephanie Lin", "paper_title": "TruthfulQA: Measuring how models mimic human falsehoods", "publication_date": "2022-12-01", "reason": "This paper introduces a benchmark dataset, TRUTHFULQA, specifically designed for evaluating the truthfulness of language model generations, which is used as a key dataset in this paper."}]}