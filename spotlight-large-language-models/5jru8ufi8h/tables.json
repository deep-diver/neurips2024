[{"figure_path": "5jRU8ufi8H/tables/tables_7_1.jpg", "caption": "Table 1: Non-vacuous generalization bounds using different compression techniques for GPT2 pretraining. We find that with the larger complexity budget afforded by the token-level bounds, subspace compression is no longer necessary or even beneficial for the bounds. Of the structures we consider, the Monarch parametrization performs best.", "description": "This table presents the results of non-vacuous generalization bounds for GPT2 models trained using different compression techniques: SubLoRA, Enhanced SubLoRA, Enhanced LoRA, Monarch Only, Kronecker Only, Kronecker + Subspace.  The token-level bounds allow for less restrictive compression strategies compared to previous document-level approaches. The table shows that Monarch parametrization achieves the best results in terms of generalization bounds (BPD), with significantly lower error rates compared to random guessing and other techniques.", "section": "5 Compressing LLMs to Minimize Complexity"}, {"figure_path": "5jRU8ufi8H/tables/tables_8_1.jpg", "caption": "Table 2: Pretrained GPT2 models achieve non-vacuous bounds for next token prediction on OpenWebText through post-training quantization only and without altering the pretraining.", "description": "This table presents the results of applying post-training quantization to pretrained GPT2 models of varying sizes (124M, 355M, and 774M parameters).  The table shows the achieved BPD (bits per dimension) bound, along with the Top-1 and Top-100 error rates.  The key finding is that even with relatively large models, non-vacuous generalization bounds are achieved using only post-training quantization without modifying the original pretraining process.", "section": "6.2 Non-vacuous Bounds for Pretrained LLMs: GPT2, LLaMA1 and LLAMA2"}, {"figure_path": "5jRU8ufi8H/tables/tables_8_2.jpg", "caption": "Table 3: Pretrained LLaMA2 models achieve non-vacuous token-level bounds for next token prediction on the Amber dataset via 2-bit post-training QuIP quantization only.", "description": "This table presents the results of applying post-training quantization to pretrained LLAMA2 models of varying sizes (7B, 13B, and 70B parameters).  It shows the achieved non-vacuous generalization bounds (bits per dimension (BPD), Top-1 error, and Top-100 error) for the next-token prediction task on the Amber dataset.  The results demonstrate that even very large language models can achieve non-vacuous generalization bounds with appropriate compression techniques (in this case, 2-bit quantization).  A random guess baseline is also included for comparison.", "section": "6 Non-Vacuous Bounds for LLMs with Billions of Parameters"}, {"figure_path": "5jRU8ufi8H/tables/tables_9_1.jpg", "caption": "Table 4: Our LLM bounds provide a much stronger statement than what would be explained by low order Markov models.", "description": "This table compares the bits-per-dimension (BPD) achieved by a quantized GPT2 small model and k-th order Markov chains on the OpenWebText dataset.  The results show that the GPT2 model achieves significantly better bounds than the Markov chains, even for relatively high values of k (context length).  This demonstrates that the model's ability to predict the next token goes beyond simply memorizing short sequences of tokens (n-grams) and captures much longer-range correlations within the text.", "section": "6.4 Contextualizing GPT2 Bounds Against Markov Chains"}, {"figure_path": "5jRU8ufi8H/tables/tables_15_1.jpg", "caption": "Table 1: Non-vacuous generalization bounds using different compression techniques for GPT2 pretraining. We find that with the larger complexity budget afforded by the token-level bounds, subspace compression is no longer necessary or even beneficial for the bounds. Of the structures we consider, the Monarch parametrization performs best.", "description": "This table shows the results of applying various compression techniques (SubLoRA, enhanced SubLoRA, enhanced LoRA, Monarch Only, Kronecker Only, Kronecker + Subspace) to the GPT2 model for pretraining.  The token-level bounds allow for less restrictive compression compared to document-level approaches. The table presents the BPD bound, Top-1, Top-10, and Top-100 errors for each compression technique, showing that Monarch parametrization gives the best results in terms of generalization bounds.", "section": "5 Compressing LLMs to Minimize Complexity"}, {"figure_path": "5jRU8ufi8H/tables/tables_16_1.jpg", "caption": "Table 6: Zero-shot downstream task performance for GPT2 models with different model sizes as reported in Radford et al. [39].", "description": "This table presents the zero-shot performance of GPT-2 models of varying sizes (117M, 345M, 762M, and 1542M parameters) on several downstream tasks.  The tasks include LAMBADA (perplexity and accuracy), CBT-CN (accuracy), CBT-NE (accuracy), WikiText2 (perplexity), PTB (perplexity), WikiText103 (perplexity), and 1BW (perplexity). The results are adapted from Radford et al. [39] and showcase how model size affects zero-shot performance across diverse language modeling tasks.", "section": "6.2 Non-vacuous Bounds for Pretrained LLMs: GPT2, LLaMA1 and LLAMA2"}, {"figure_path": "5jRU8ufi8H/tables/tables_18_1.jpg", "caption": "Table 7: Models pretrained on antibody sequences achieve non-vacuous token-level generalization bounds. Language models pretrained on antibody sequences achieve non-vacuous bounds for next token prediction on a processed subset of Observed Antibody Sequences (OAS) through post-training quantization only. The vocabulary size of an antibody LLM is 29.", "description": "This table shows the results of applying the proposed token-level generalization bounds to language models pretrained on antibody sequences.  The models used were Mistral 377M, 212M, and 94M, each with post-training quantization applied.  The table shows the bits per dimension (BPD) bound achieved, along with the Top-1, Top-10, and Top-100 error rates.  It highlights that even small models achieve non-vacuous generalization bounds on this specific dataset.", "section": "6.3 Token-Level Generalization Bounds on Antibody Sequences"}, {"figure_path": "5jRU8ufi8H/tables/tables_19_1.jpg", "caption": "Table 8: Non-vacuous token-level generalization bounds for open-source pretrained LLM checkpoints on the Amber dataset. All of these models were quantized post-training using QuIP# to different numbers of bits as shown above. All the bounds are non-vacuous compared to random guess performance.", "description": "This table presents non-vacuous token-level generalization bounds for various open-source pretrained LLMs (LLaMA1 and LLaMA2, including chat versions) after post-training quantization using QuIP#.  The bounds are calculated for different bit precisions (2, 3, and 4 bits) and model sizes, showing the bits per dimension (BPD), Top-1, Top-10, and Top-100 errors.  The results demonstrate that even large language models with billions of parameters can achieve non-vacuous bounds using this method.  The comparison with a random guess baseline highlights the significance of the obtained bounds.", "section": "6 Non-Vacuous Bounds for LLMs with Billions of Parameters"}, {"figure_path": "5jRU8ufi8H/tables/tables_20_1.jpg", "caption": "Table 1: Non-vacuous generalization bounds using different compression techniques for GPT2 pretraining. We find that with the larger complexity budget afforded by the token-level bounds, subspace compression is no longer necessary or even beneficial for the bounds. Of the structures we consider, the Monarch parametrization performs best.", "description": "This table presents the non-vacuous generalization bounds achieved using different compression techniques for GPT2 pretraining.  The results show the impact of different compression methods (SubLoRA, enhanced SubLoRA, enhanced LoRA, Monarch only, Kronecker only, Kronecker + Subspace) on the BPD bound and Top-k errors. The token-level bounds allow for less restrictive compression, demonstrating that the Monarch parametrization yields the best results.", "section": "5 Compressing LLMs to Minimize Complexity"}]