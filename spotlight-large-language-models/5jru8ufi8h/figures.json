[{"figure_path": "5jRU8ufi8H/figures/figures_2_1.jpg", "caption": "Figure 1: Non-vacuous bounds for LLMs that scale up to 70B parameters. Left: Bits per dimension (BPD) bounds on the Amber dataset [29] which contains 1.2 trillion tokens for different LLMs from the LLaMA family ranging in scale from 7 billion to 70 billion parameters [47]. All of these models are quantized to 2-bits, 3-bits and 4-bits per-weight using QuIP# and are publicly available [48]. The different quantization precisions are accounted for in the compressed model size. The trade-off between the empirical performance and the model complexity in our bounds favors models with a smaller compressed size in general, though we observe that across different architectures we can find larger models yielding better bounds. Middle: The BPD training loss for different models from the LLaMA family-the legend is shared with the figure on the left. Overall, we observe that larger models yield a lower BPD while having a higher compressed size. Right: Validation negative log-likelihood loss as a function of the total number of trainable parameters for different nonlinear parametrization; namely low rank adaptation (LoRA), the Kronecker decomposition of dense matrices and Monarch matrices. The x-axis is in the log scale. As we vary the number of trainable parameters, there are different optimal compression techniques.", "description": "This figure displays the results of applying different compression techniques to large language models (LLMs). The left panel shows the bits-per-dimension (BPD) generalization bounds for several LLMs of varying sizes, demonstrating that the proposed method yields non-vacuous bounds for large models. The middle panel compares the training BPD loss against the model size, showing a trade-off between performance and size. The right panel shows the validation loss for different LLMs compressed with different methods (LoRA, Kronecker, Monarch), indicating that the optimal compression method varies with the number of trainable parameters.", "section": "6 Non-Vacuous Bounds for LLMs with Billions of Parameters"}, {"figure_path": "5jRU8ufi8H/figures/figures_5_1.jpg", "caption": "Figure 1: Non-vacuous bounds for LLMs that scale up to 70B parameters. Left: Bits per dimension (BPD) bounds on the Amber dataset [29] which contains 1.2 trillion tokens for different LLMs from the LLaMA family ranging in scale from 7 billion to 70 billion parameters [47]. All of these models are quantized to 2-bits, 3-bits and 4-bits per-weight using QuIP# and are publicly available [48]. The different quantization precisions are accounted for in the compressed model size. The trade-off between the empirical performance and the model complexity in our bounds favors models with a smaller compressed size in general, though we observe that across different architectures we can find larger models yielding better bounds. Middle: The BPD training loss for different models from the LLaMA family-the legend is shared with the figure on the left. Overall, we observe that larger models yield a lower BPD while having a higher compressed size. Right: Validation negative log-likelihood loss as a function of the total number of trainable parameters for different nonlinear parametrization; namely low rank adaptation (LoRA), the Kronecker decomposition of dense matrices and Monarch matrices. The x-axis is in the log scale. As we vary the number of trainable parameters, there are different optimal compression techniques.", "description": "This figure demonstrates the non-vacuous generalization bounds achieved for various LLMs (LLaMA family) with up to 70 billion parameters.  The left panel shows the bits-per-dimension (BPD) bounds against compressed model size, highlighting the trade-off between compression and model performance. The middle panel compares training loss (BPD) across different LLaMA models, showcasing the relationship between model size and training loss. The right panel illustrates the validation loss against different compression techniques (LoRA, Kronecker, Monarch) across varying numbers of trainable parameters, revealing the optimal compression technique based on model size.", "section": "6 Non-Vacuous Bounds for LLMs with Billions of Parameters"}, {"figure_path": "5jRU8ufi8H/figures/figures_17_1.jpg", "caption": "Figure 3: As language models are compressed, they retain their understanding of patterns, but they forget highly random and unstructured data rapidly. Experiments performed on GPT-2 models with datasets created as detailed in Section 6.5. Compression performed via post-training quantization where lower quantization levels reflect more aggressive compression.", "description": "This figure shows the result of an experiment designed to differentiate between a model's ability to memorize facts and its ability to recognize patterns.  Two types of sequences were generated: structured (highly compressible) and random (incompressible).  A GPT-2 model was trained on these sequences and then compressed using post-training quantization with varying levels of aggressiveness (quantization levels). The plot shows the accuracy of integer prediction for both types of sequences at different compression levels. The results indicate that as the model is compressed more aggressively, the ability to recall unstructured sequences deteriorates much faster than the ability to recognize structured patterns, highlighting the difference between memorization and pattern recognition in LLMs.", "section": "6.5 Memorization vs. Reasoning"}, {"figure_path": "5jRU8ufi8H/figures/figures_18_1.jpg", "caption": "Figure 1: Non-vacuous bounds for LLMs that scale up to 70B parameters. Left: Bits per dimension (BPD) bounds on the Amber dataset [29] which contains 1.2 trillion tokens for different LLMs from the LLaMA family ranging in scale from 7 billion to 70 billion parameters [47]. All of these models are quantized to 2-bits, 3-bits and 4-bits per-weight using QuIP# and are publicly available [48]. The different quantization precisions are accounted for in the compressed model size. The trade-off between the empirical performance and the model complexity in our bounds favors models with a smaller compressed size in general, though we observe that across different architectures we can find larger models yielding better bounds. Middle: The BPD training loss for different models from the LLaMA family-the legend is shared with the figure on the left. Overall, we observe that larger models yield a lower BPD while having a higher compressed size. Right: Validation negative log-likelihood loss as a function of the total number of trainable parameters for different nonlinear parametrization; namely low rank adaptation (LoRA), the Kronecker decomposition of dense matrices and Monarch matrices. The x-axis is in the log scale. As we vary the number of trainable parameters, there are different optimal compression techniques.", "description": "This figure shows the results of applying different compression techniques to several LLMs (LLaMA family) and evaluating their generalization bounds using three different metrics.  The left panel shows bits per dimension (BPD) bounds versus compressed model size for various LLMs and quantization levels. The middle panel displays training BPD loss versus compressed size for the LLMs. The right panel compares validation loss for various compression methods (LoRA, Kronecker, Monarch) at different numbers of trainable parameters.  It demonstrates the achieved non-vacuous generalization bounds for large models and the trade-off between compression and performance.", "section": "6 Non-Vacuous Bounds for LLMs with Billions of Parameters"}, {"figure_path": "5jRU8ufi8H/figures/figures_20_1.jpg", "caption": "Figure 1: Non-vacuous bounds for LLMs that scale up to 70B parameters. Left: Bits per dimension (BPD) bounds on the Amber dataset [29] which contains 1.2 trillion tokens for different LLMs from the LLaMA family ranging in scale from 7 billion to 70 billion parameters [47]. All of these models are quantized to 2-bits, 3-bits and 4-bits per-weight using QuIP# and are publicly available [48]. The different quantization precisions are accounted for in the compressed model size. The trade-off between the empirical performance and the model complexity in our bounds favors models with a smaller compressed size in general, though we observe that across different architectures we can find larger models yielding better bounds. Middle: The BPD training loss for different models from the LLaMA family-the legend is shared with the figure on the left. Overall, we observe that larger models yield a lower BPD while having a higher compressed size. Right: Validation negative log-likelihood loss as a function of the total number of trainable parameters for different nonlinear parametrization; namely low rank adaptation (LoRA), the Kronecker decomposition of dense matrices and Monarch matrices. The x-axis is in the log scale. As we vary the number of trainable parameters, there are different optimal compression techniques.", "description": "This figure presents a comprehensive analysis of non-vacuous generalization bounds for large language models (LLMs), specifically focusing on the LLaMA family.  It shows the relationship between model size, compression techniques, and generalization performance using three key metrics: BPD (bits per dimension) bounds, training loss, and validation loss. The left panel demonstrates that even with models up to 70B parameters, non-vacuous bounds can be achieved. The middle panel reveals the relationship between training loss and model size, and the right panel shows how the choice of compression technique (LoRA, Kronecker, Monarch) influences validation loss at different parameter scales.", "section": "6 Non-Vacuous Bounds for LLMs with Billions of Parameters"}]