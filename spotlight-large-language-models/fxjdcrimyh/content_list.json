[{"type": "text", "text": "Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wenyu $\\mathbf{D}\\mathbf{u}^{1*}$ Tongxu Luo2,3\u2217\u2020 Zihan $\\mathbf{Qiu^{4}}$ Zeyu Huang5 Yikang Shen6 Reynold Cheng1 Yike Guo2 Jie $\\mathbf{Fu}^{2\\ddagger}$ ", "page_idx": 0}, {"type": "text", "text": "1School of Computing and Data Science, The University of Hong Kong 2HKUST 3USTB 4Tsinghua University 5University of Edinburgh 6MIT-IBM Watson AI Lab wydu@cs.hku.hk tongxuluo@gmail.com jiefu@ust.hk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "LLMs are computationally expensive to pre-train due to their large scale. Model growth emerges as a promising approach by leveraging smaller models to accelerate the training of larger ones. However, the viability of these model growth methods in efficient LLM pre-training remains underexplored. This work identifies three critical Obstacles: $(O1)$ lack of comprehensive evaluation, $(O2)$ untested viability for scaling, and $(O3)$ lack of empirical guidelines. To tackle $O1$ , we summarize existing approaches into four atomic growth operators and systematically evaluate them in a standardized LLM pre-training setting. Our findings reveal that a depthwise stacking operator, called $G_{\\mathrm{stack}}$ , exhibits remarkable acceleration in training, leading to decreased loss and improved overall performance on eight standard NLP benchmarks compared to strong baselines. Motivated by these promising results, we conduct extensive experiments to delve deeper into $G_{\\mathrm{stack}}$ to address $O2$ and $O3$ . For $O2$ (untested scalability), our study shows that $G_{\\mathrm{stack}}$ is scalable and consistently performs well, with experiments up to 7B LLMs after growth and pre-training LLMs with 750B tokens. For example, compared to a conventionally trained 7B model using 300B tokens, our $G_{\\mathrm{stack}}$ model converges to the same loss with 194B tokens, resulting in a $54.6\\%$ speedup. We further address $O3$ (lack of empirical guidelines) by formalizing guidelines to determine growth timing and growth factor for $G_{\\mathrm{stack}}$ , making it practical in general LLM pre-training. We also provide in-depth discussions and comprehensive ablation studies of $G_{\\mathrm{stack}}$ . Our code and pre-trained model are available at https://llm-stacking.github.io/. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Emergent abilities of Large Language Models (LLMs) rely on scaling-up [1, 2]. Empirical evidence from scaling laws [3\u20135] fuels the development of increasingly larger models, pushing the boundaries of LLMs capabilities. However, pre-training these gigantic models comes at a significant cost in terms of energy consumption and environmental impact [6] (e.g., pre-training Llama-3 [7] consumes a total of 7.7M GPU hours and generates 2290 tons of carbon dioxide equivalent of carbon emissions). The efficient pre-training of LLMs is thus crucial, both from a scientific and a societal perspective, to ensure the continual growth and adoption of AI [8, 9]. ", "page_idx": 0}, {"type": "text", "text": "One promising research direction to accelerate model training involves leveraging trained smaller (base) models to expedite the training of larger (target) models, a technique known as model growth. ", "page_idx": 0}, {"type": "text", "text": "Concretely, model growth studies how to leverage the trained smaller model\u2019s parameters $\\Theta^{(s)}$ to initialize the larger model\u2019s parameters $\\Theta^{(l)}$ . Current popular methods generally focus on expanding the parameters of the base model through techniques like splitting [10\u201312], copying [13, 14], or matrix mapping [15]. There are also some approaches that initialize new parameters from scratch [16, 12, 17]. The primary objective is to accelerate the training of large models, and existing methods demonstrate promising speedup results on models such as BERT [11, 14, 18, 15, 12, 13]. Despite such empirical evidence and its alignment with the goal of efficient LLM pre-training, model growth methods are not widely adopted in the context of LLM pre-training [7, 19]. To our best knowledge, the only LLM that utilizes model growth for accelerating is FLM-101B [20], but it lacks a baseline LLM trained from scratch to compare. We observe three key Obstacles that hinder LLM pre-training from using existing model growth techniques, specifically: ", "page_idx": 1}, {"type": "text", "text": "\u2022 O1: Lack of comprehensive assessment. Some existing model growth methods report results on LLM pre-training, but either lack a baseline comparison [20] or are still in exploratory stages [15, 13]. In contrast, most growth approaches are evaluated in encoder-based BERT models [14, 11, 18, 12, 13, 16, 17], which have different architecture and training configurations compared to prominent decoder-based LLMs such as Llama [21]. ", "page_idx": 1}, {"type": "text", "text": "\u2022 $O2$ : The untested scalability. This scalability has two aspects: the model size and the amount of pretraining data. Regarding the model size, the existing approaches are only evaluated on smaller-scale BERT models or in preliminary experiments with LLMs. It is unclear whether these growth methods will continue accelerating training when applied to large-scale LLMs with more extensive evaluation. As for the amount of pre-training data, there are debates [22] over whether certain efficient training strategies may initially converge faster but ultimately perform similarly or worse than vanilla training methods when given ample computational resources (i.e., more training data). ", "page_idx": 1}, {"type": "text", "text": "\u2022 $O3$ : Lack of empirical guidelines. Scaling laws [3, 4] give clear empirical guidelines on pre-training computational-optimized LLMs, greatly stimulating and advancing the field. Yet, there is a lack of empirical guidelines on growth techniques, discouraging LLM practitioners from adopting these approaches, especially considering the high costs of LLM pre-training. ", "page_idx": 1}, {"type": "text", "text": "These three obstacles are consequential in nature. Hence, in this work, we empirically revisit the concept of model growth as a solution to efficient LLM pre-training by tackling them one by one. ", "page_idx": 1}, {"type": "text", "text": "To tackle $O1$ , we systematically evaluate model growth techniques on practical LLM pre-training. We first categorize existing growth methods and summarize them into four atomic growth operators, each of which can grow along two directions: widthwise (intra-layer) and depthwise (layer-wise). We illustrate them in Figure 2. These operators serve as representative choices for evaluating the performance of model growth techniques. We use these operators to expand 400M base models to 1.1B Llama-like LLMs and continually pre-train them. Next, we evaluate these growth techniques on the training loss and eight standard NLP benchmarks from the Harness toolkit [23]. We found the direct operator that stacks depthwisely $G_{s t a c k}$ consistently outperforms others across overall evaluation metrics, demonstrating its potential in accelerating LLM pre-training. This motivates us to investigate extensively by addressing $O2$ and $O3$ on $G_{s t a c k}$ . ", "page_idx": 1}, {"type": "text", "text": "To address $O2$ , we investigate the $G_{s t a c k}$ operator\u2019s scalability to larger model sizes and to more training data. We conduct extensive experiments ", "page_idx": 1}, {"type": "image", "img_path": "FXJDcriMYH/tmp/4a27df40c191c3bf49acf8c5ab6a3c808198ef219af334b849f6f10c45d626ac.jpg", "img_caption": ["Figure 1: The training loss for two 7B LLMs, trained from scratch and with $G_{d i r e c t}^{\\uparrow}\\,(G_{\\mathrm{stack}}).$ . At 300B tokens, $G_{\\mathrm{stack}}$ accelerates by $54.6\\%$ compared to scratch. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "by scaling model size up to 7B parameters trained with 300B tokens, and pre-training a 410M model with over 750B training tokens. This is in contrast to the previous largest LLM pre-training experiment that uses model growth methods and has baselines for comparison, which is reported in Ligo [15], where a GPT2-1.5B model is trained for $15\\mathbf{k}$ steps (approximately 15B tokens). The results are encouraging, as we consistently observe significant improvements $G_{s t a c k}$ offers in both scenarios. For example, we achieve a remarkable $54.6\\%$ speedup in pre-training for a 7B model with 300B tokens (Figure 1). Interestingly, the loss improvement in our 750B-token experiment aligns with a logarithmic function. We further extend this logarithmic curve and determine that the improvement continues to be substantial even for the LLM trained with over 8T tokens. Moreover, we summarize all our experiments by estimating the LLM scaling law for LLMs pre-trained with $G_{s t a c k}$ . Given the same target loss value, our analysis reveals a significantly reduced computational cost compared to the common scaling law [4]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "For $O3$ , we explore the practical guidelines for using $G_{\\mathrm{stack}}$ in LLM pre-training. Given a computational budget, we determine the optimal strategy for two key factors of $G_{\\mathrm{stack}}$ , growth timing $d$ and growth factor $g$ . Growth timing $d$ relates to the training tokens used for small models before growing, and growth factor $g$ refers to the factor between the non-embedding parameter number of the large models and the small models. We formalize our findings into equations that offer concrete suggestions for utilizing $G_{\\mathrm{stack}}$ . We believe this work could significantly pique the interest and bolster confidence in future LLM pre-training with model growth techniques, both in academia and industry. ", "page_idx": 2}, {"type": "text", "text": "To summarize, our contributions are four-fold: 1) We first systematically investigate model growth techniques and identify four atomic model growth operators, establishing a better understanding of the field in Section 3.1. 2) We then design a standard LLM pre-training testbed and perform comprehensive evaluations on these operators, finding that a simple depthwise stacking $G_{\\mathrm{stack}}$ exhibits significant superiority in Section 3. 3) We further demonstrate the scalability of $G_{\\mathrm{stack}}$ with experiments on LLMs ranging from 410M to 7B parameters and up to 750B training tokens in Section 4.1. 4) We also provide guidelines of equations on determining growth timing and growth factors for optimal use of $G_{\\mathrm{stack}}$ in Section 4.2. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work - Model Growth for Efficient Pre-training ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The idea of growing neural networks dates back to the 1990s [24\u201326]. The pioneering work of Net2Net [10] marks a milestone, for the first attempt to study model growth in deep learning era. Net2Net expands width and depth while keeping original functions (namely function preserving) via randomly splitting old neurons and injecting new identity layers. The widthwise splitting method of Net2Net represents a series of works that aim to \u201cexpand\u201d the existing neurons to the desired larger size. Bert2Bert [11] serves as a BERT-based extension of the widthwise Net2Net. StagedGrow[13] doubles the width by concatenating two identical layers and halves final loss to keep functionpreserving. Lemon [12] suggests integrating a parameter into the splitting of neurons in Bert2Bert, aiming to break weight symmetry. Depthwisely, StackedBert [14] simply stacks duplicated layers to form a deeper model. In contrast to the above direct copy/split approaches, LiGO [15]presents a learning-based method that initializes the larger model\u2019s parameters via learning a linear mapping from the smaller model\u2019s parameters. ", "page_idx": 2}, {"type": "text", "text": "Alongside the approaches that expand existing parameters, there are works that initialize new ones without relying on existing ones. For instance, MSG [17] proposes a multi-staged growing strategy that progressively expands transformer components, where the newly grown neurons are randomly initialized using a masking mechanism to ensure function preservation. Besides, some works have assigned specific values, like zero, to the newly initialized neurons to negate their influence [16, 12]. ", "page_idx": 2}, {"type": "text", "text": "All the above methods are primarily explored in BERT or earlier stages of LLM pre-training. On the other hand, our objective is to present the first systematic review of model growth techniques in the LLMs era. To our knowledge, FLM-101B [20] is the only existing LLM that uses the growth method [17] for accelerating billion-scale LLM pre-training. Nonetheless, this work lacks a baseline model trained from scratch, making it difficult to assess the effectiveness of the model growth technique. In contrast, we aim to provide a comprehensive study by establishing a standardized testbed to compare LLMs trained from scratch and with various growth methods in LLM pre-training. ", "page_idx": 2}, {"type": "text", "text": "3 Systematically Assessing Model Growth for LLM Pre-Training ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Existing model growth methods [14, 11, 18, 15, 12, 13, 16, 17] are mainly evaluated on BERT [27], with limited focus on decoder-only large-scale language models such as Llama [21]. Moreover, these growth methods are often not comparable due to different training settings [14, 11, 17, 12]. ", "page_idx": 2}, {"type": "text", "text": "Even some growth LLMs experiments are evaluated, their results are often incomplete [20, 15]. To overcome these limitations, we first summarize existing works [14, 11, 18, 15, 12, 13, 16, 17] into four atomic growth operators to represent these growth techniques. Then we build a standardized LLMs training testbed to pre-train LLMs with four growth operators on depthwise and widthwise directions and evaluate the results with both training loss and eight evaluation metrics in Harness [23]. ", "page_idx": 3}, {"type": "text", "text": "3.1 Growing LLMs with Growth Operators ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Recent years, researchers have focused on enhancing the efficiency of training large models by making use of smaller pre-existing models [10, 11, 14, 18, 15, 12, 13, 16, 17]. These state-of-the-art methods can be categorized into two distinct groups. The first group focuses on deriving new neurons from the existing ones [10, 11, 14, 12, 15], while the second group focuses on initializing new parameters separately [18, 13, 16, 17]. Drawing from these two lines of research, we summarize four atomic growth operators. These operators include: (A) directly duplicating and stacking old layers in a depthwise manner or splitting neurons in the same layer widthwisely, denoted as $G_{\\mathrm{direct}}$ , (B) generating expanded parameters using a learnable mapping matrix to the existing parameters, denoted as $G_{\\mathrm{learn}}$ , (C) setting the new parameters to zero, denoted as $G_{\\mathrm{zero}}$ , and $\\mathbf{\\eta}(\\mathbf{D})$ randomly initializing the new parameters, denoted as $G_{\\mathrm{random}}$ . The illustration of four operators is shown in Figure 2. The $G_{\\mathrm{direct}}$ and $G_{\\mathrm{learn}}$ growth operators produce new neurons from the current ones, in contrast to $G_{\\mathrm{zero}}$ and $G_{\\mathrm{random}}$ which initialize new parameters independently. For the formal definitions of the operators and the differences to the existing growth methods in design, please refer to Appendix A. Complex growth methods, such as those involving auxiliary loss or exploring training dynamics like learning rates [28, 29, 16] are interesting. But considering the high computational cost of LLM pre-training, we focus on simple, universally applicable growth operators for different LLM pre-training settings. ", "page_idx": 3}, {"type": "image", "img_path": "FXJDcriMYH/tmp/54b3b6acd081393fd598f6b2a366b805d029a1c4756cbecf25778dcb58fac01e.jpg", "img_caption": ["Figure 2: The simplified illustration of four growth operators $G_{\\mathrm{direct}}$ , $G_{\\mathrm{learn}}$ , $G_{\\mathrm{zero}}$ and $G_{\\mathrm{random}}$ , each of which can grow along widthwise (intra-layer) $G^{\\rightarrow}$ or depthwise (layer-wise) $G^{\\uparrow}$ . $\\mathbf{W_{n}}$ is the parameters before growth, while $\\mathbf{D_{n}}$ , $\\mathbf{R_{n}}$ and $\\mathbf{O}$ are the growth parameters derived from the old, randomly initialized, and zero-initialized respectively. Except $G_{\\mathrm{direct}}$ , other three operators only illustrates the widthwise growth. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "To make a fair comparison of the four growth operators for LLM pre-training, we define a standardized \u201cone-hop\u201d growth process that involves two training phases, small model training before growth and large model training after growth. We first train the small LLMs with $d$ tokens before growing. Then, we use operator $G$ to grow them to the target LLMs by a factor of $g$ for non-embedding parameters and then continual pre-training the large LLMs for $D$ tokens. Two key factors in the procedure are worth noting: the growth factor $g$ and the data for base model training $d$ , which can be interpreted as \u201cgrowth timing\u201d. We further evaluate each growth operator by separately examining in depthwise (intra-layer) growth $G^{\\uparrow}$ and widthwise (layer-wise) growth $G^{\\rightarrow}$ . Concretely, we start with base models (400M LLMs) trained on $d=10B$ tokens, apply the four operators in both directions to scale them up to the target size of 1.1B (approximately a growth factor of $g=4,$ ), and then continue training for an additional $D=97.5B$ tokens. 4 Appendix B contains the LLM\u2019s architecture configuration and training details. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.2 Pre-Training 1.1B LLMs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We report results on training loss, eight standard Harness NLP benchmarks along with the average accuracy and the speedup ratio in Figure 3. Our key discovery reveals that depthwise growth $\\bar{G}^{\\uparrow}$ exhibits a significant acceleration over both widthwise growth $G^{\\rightarrow}$ and training models from scratch, while surprisingly, $G^{\\rightarrow}$ does not offer any notable advantages. Among the depthwise growth operators, $G_{\\mathrm{direct}}^{\\uparrow},\\bar{G}_{\\mathrm{learn}}^{\\uparrow}$ , and $G_{z e r o}^{\\uparrow}$ , all outperform the baseline and Gr\u2191andom. The underperformance of Gr\u2191an $G_{\\mathrm{random}}^{\\uparrow}$ in our study may be attributed to its design for gradual \u201cmini-step\u201d growth [17], whereas our unified approach uses a single step. Most notably, depthwise stacking $G_{\\mathbf{direct}}^{\\uparrow}$ emerges as the clear winner among growth operators, surpassing its competitors in speedup, training loss and nearly every $G_{\\mathrm{direct}}^{\\uparrow}$ u ap cphlieeavsees  rae fseirg tnoi fAicpapnte nedfifxic iBe.n2c. yT ghaei nA, pinpcernedaisxi nCg  ptrreasieninntsg  mspoeree de xbpy $49.1\\%$ .s  Tonh et hceaslec uolpaetiroatno rosf, including their loss training and evaluation figures. ", "page_idx": 4}, {"type": "table", "img_path": "FXJDcriMYH/tmp/3a8f5d36ad7d247c055c54a98f6675f4d05b5334065d95b347048c116a9ef0c5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: We evaluate operators using training loss and Lambada [30], ARC-c [31], ARC-e [31], Logiqa [32], PIQA [33], Sciq [34], Winogrande [35] and Wikitext PPL [36] totaling eight standard NLP benchmarks. After $8\\times10^{20}$ FLOPs of training, $G_{\\mathrm{direct}}^{\\uparrow}$ demonstrates a significant speedup. ", "page_idx": 4}, {"type": "text", "text": "4 Delving Deeper Into Depthwise Stacking $\\left(G_{\\mathrm{stack}}\\right)$ ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The empirical evidence suggests that certain growth operators, most notably $G_{\\mathrm{direct}}^{\\uparrow}$ , exhibit an impressive acceleration in LLM pre-training compared to the baseline approach of training models $G_{\\mathrm{direct}}^{\\uparrow}$ of reference, we will henceforth denote this depthwise stacking approach as operator $G_{\\mathrm{stack}}$ : ", "page_idx": 4}, {"type": "text", "text": "$\\mathcal{M}=M\\circ M\\circ\\cdot\\cdot\\cdot\\circ M_{\\cdot}$ where $M$ is a small base model trained with $d$ tokens, $\\mathcal{M}$ is the target $g\\times M$   \nmodel and $g$ is the growth factor. ", "page_idx": 5}, {"type": "text", "text": "This section addresses the two main challenges ( $O2$ and $O3$ ) outlined in the introduction: 1) evaluating the performance of $G_{\\mathrm{stack}}$ in scaling scenarios, i.e. larger model sizes and more training tokens; and 2) determining the hyperparameters when using $G_{\\mathrm{stack}}$ , i.e., the growth timing $d$ and growth factor $g$ . ", "page_idx": 5}, {"type": "text", "text": "4.1 Scaling $G_{\\mathrm{stack}}$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Scaling Model Sizes for $G_{\\mathrm{stack}}$ Our scaled-up experiments involve two larger model sizes: 3B and 7B. We initially train smaller models with a layer count that is one-quarter of our target layers (growth factor $g\\ =\\ 4)$ , utilizing 10B tokens $(d=10B)$ . Then, we train the stacked models using over 300B tokens $(D\\ =\\ 300B)$ ) for both sizes. Figures 4 and 5 show the loss, and the NLP benchmarks average accuracy evaluated using the Harness evaluator for training 3B and 7B LLMs with 300B tokens, respectively. 5 The acceleration of $G_{\\mathrm{stack}}$ is consistent across two ", "page_idx": 5}, {"type": "image", "img_path": "FXJDcriMYH/tmp/10b0855764935c3d8b49e6ef899cc8945642139fa87da4de70f41d7ed04c8963.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "FXJDcriMYH/tmp/64afff4f0d865729f4721a36878d1cbf47c8b00ba385052eebc62ab857d46316.jpg", "img_caption": ["(a) Training Loss "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 4: Training 3B LLMs with 300B tokens. $G_{\\mathrm{stack}}$ significantly outperforms scratch in (a) loss and (b) average accuracy across NLP benchmarks. At 180B and 240B tokens, $G_{\\mathrm{stack}}$ accelerates by $48.6\\%$ and $54.5\\%$ compared to scratch. ", "page_idx": 5}, {"type": "text", "text": "models and all evaluation metrics. For instance, considering the 3B model, Figure 4 demonstrates that $G_{\\mathrm{stack}}$ achieves a $54.5\\%$ speedup in pre-training, improvement of 2.1 in NLP benchmarks average accuracy compared to the baseline 3B model trained with 240B tokens. ", "page_idx": 5}, {"type": "text", "text": "When comparing the 1B, 3B, and 7B models, it is evident that the benefits of $G_{\\mathrm{stack}}$ are not reduced as the model size increases, implying that its acceleration effect can be leveraged even with larger models. Details of the evaluation results, including evaluation with instruction tuning, can be found in Appendix D. Appendix E compares our baselines with the open-source LLMs Pythia and tinyLlama. ", "page_idx": 5}, {"type": "text", "text": "Scaling Training Tokens for $G_{\\mathrm{stack}}$ . We next evaluate the scalability of the stacking operator on another dimension - training with more tokens. This is especially important in light of recent discussions about the validity of efficient training algorithms, which have sparked debate [22] over whether certain strategies may initially learn faster but ultimately perform similarly or worse than vanilla training methods when given more training data. Hence, we aim to pre-train a LLM using $G_{\\mathrm{stack}}$ on a substantial amount of training tokens. ", "page_idx": 5}, {"type": "image", "img_path": "FXJDcriMYH/tmp/f5c20d132e5dc7b92698abaa7ed6b11aefe7104f30a07ad3a47c7a454b90e9ea.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "FXJDcriMYH/tmp/4cc61f1e25c41d8293c8f0bb982df3cb8d2a1a26404df2d016908a5b5e33f507.jpg", "img_caption": ["Figure 5: Training 7B LLMs with 300B tokens. $G_{\\mathrm{stack}}$ significantly outperforms scratch in (a) loss and (b) average accuracy across NLP benchmarks. At 160B, 220B and 280B tokens, $G_{\\mathrm{stack}}$ accelerates by $40.8\\%$ , $55.3\\%$ and $53.8\\%$ compared to scratch. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Concretely, we conduct an experiment on a 410M LLM using 750B tokens. Following the experimental setup in the previous section, we set growth ratio $g\\ =\\ 4$ and growth timing $d\\,=\\,10B$ and conduct continuous pre-training on the target 410M LLMs for 750B tokens. Compared to the chinchillarecommended 8B tokens [4] for the 410M model, our experimental setting also surpasses this value by nearly 100 times, reaching 750B tokens. ", "page_idx": 6}, {"type": "text", "text": "The training dynamics on Figure 6a indicate that $G_{\\mathrm{stack}}$ remains effective in such cases. Details of the evaluation results with the similar findings can be ", "page_idx": 6}, {"type": "image", "img_path": "FXJDcriMYH/tmp/847a3f8a866c445d649d0f0f68ab3fc49e4e3ad72d63da80932e811ddcd0a502.jpg", "img_caption": ["(a) Training Loss "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "FXJDcriMYH/tmp/d27a7ff2bbfed237f94669a794e2ffa279544cbad25303a987b1f80a641d0204.jpg", "img_caption": ["(b) Estimated Loss Difference "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 6: Training 410M LLMs with 750B tokens. $G_{\\mathrm{stack}}$ significantly outperforms scratch in (a) loss. At 400B tokens, we observe a $53.1\\%$ acceleration, and even at 700B tokens, there is still a $31.0\\%$ acceleration. (b) We fit the difference between the losses of the scratch and $G_{\\mathrm{stack}}$ and find that the acceleration with $G_{\\mathrm{stack}}$ remain sustainable for longer training. ", "page_idx": 6}, {"type": "text", "text": "found in Appendix D.3. Building upon the exceptional stability of LLM pre-training [37, 38], we estimate loss improvements and plot them in Figure 6b. The fitting curve indicates $G_{\\mathrm{stack}}$ will continue to exhibit acceleration effects even after 8T tokens, which is over 1000 times longer than the recommended token number [4]. It is also notable that this loss improvement after 8T training is not trivial for LLM pre-training, as previous studies [39] suggest that even minor improvements in the later phase can have a relatively substantial impact on downstream performance. ", "page_idx": 6}, {"type": "text", "text": "From a LLM practitioner\u2019s perspective, this is also crucial considering \u201covertraining\u201d, which involves training LLMs with significantly larger amounts of data than recommended by scaling laws [3\u20135], a common practice that has become prevalent. A notable example is the training of LLama 3-8B with 15T tokens, which is nearly 100 times greater than the token count recommended by the chinchilla scaling laws [4]. Hence, this finding provides confidence in the consistent excellent acceleration of $G_{\\mathrm{stack}}$ throughout the entire practical LLM pre-training process. ", "page_idx": 6}, {"type": "text", "text": "Estimating Scaling Laws. To further explore our findings, we graph our four models (410M, 1.1B, 3B, and 7B) on the same figure and attempt to uncover our \u201cscaling law\u201d using the $G_{\\mathrm{stack}}$ operator. Following [3, 4], we define the scaling power law using the equation $L_{C}=a C^{b}$ , where $a$ and $b$ are constants we need to fti, $C$ represents the FLOPs, and $L_{C}$ denotes the model\u2019s final loss under this FLOP. We use the curve_filt function in SciPy [40] to fit both the scratch model and the $G_{\\mathrm{stack}}$ model and present the estimation scaling law in Figure 7. The figure shows that our $G_{\\mathrm{stack}}$ scaling law exhibits improved efficiency compared to the scaling law estimated from baseline LLMs, achieving the same target loss while requiring much less computational resources. However, in light of the significant computational resources devoted to other scaling law studies [3, 4], we acknowledge that our $G_{\\mathrm{stack}}$ scaling law is an initial estimate subject to computation constraints, and a comprehensive study is left for future research. ", "page_idx": 6}, {"type": "image", "img_path": "FXJDcriMYH/tmp/b11d4d846e7ad579ef357801ea2bd8312b78c3dc4088998e1a3a4740cb6842d6.jpg", "img_caption": ["Figure 7: We plot scaling law lines based on 410M, 1.1B, 3B, 7B LLMs and make two predictions at the same losses of original computationaloptimized 13B and 70B LLMs. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.2 Determining Growth Timing and Growth Factor for Using $G_{\\mathrm{stack}}$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We comprehensively validate the effectiveness of the $G_{\\mathrm{stack}}$ compared to training from scratch in Section 4.1. However, to incorporate $G_{\\mathrm{stack}}$ into a LLM\u2019s pre-training process, we need to determine two crucial hyperparameters: the growing time $(d)$ and the growing factor $(g)$ . In our previous experiments, we rely on ad-hoc choices for these parameters, thereby lacking a systematic approach to determining them when use $G_{\\mathrm{stack}}$ . There exists research on investigating the growth timing [41], but the settings are quite different from the LLM pre-training. Therefore, this section offers a clear guide for practitioners looking to optimize using the $G_{\\mathrm{stack}}$ operator in LLM pre-training processes. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We begin by offering a formal definition. When given a computational budget $C$ , established scaling power laws [3, 4] exist to guide the non-embedding parameters $N$ and the number of training tokens $D$ to achieve the lowest model loss in the case of training from scratch. However, tuning hyperparameters becomes more complex when the fixed budget $C$ is allocated to find the optimal model training strategy using the $G_{\\mathrm{stack}}$ operator, which involves two training phases. Consequently, the overall computational budget $C$ can be expressed as the sum of the two components: $C\\,=\\,C1+C2$ . Here, $C1$ and $C2$ represent the flops required to train the initial small models $C1=F L O P s(n,d)$ , and the large model $\\bar{C}2=F L O\\bar{P}s(N,\\bar{D})$ respectively, where $n$ and $d$ denote the parameters and training tokens of the small model, and $N$ and $D$ represent the parameters and training tokens of the large model. Since the large model is grown by a factor of $g$ such that $N\\,=\\,g n$ , we have $C=\\bar{C}1+C2=F L O P s(g,\\bar{N_{,}}d)+F L O\\bar{P}s(N,\\bar{D_{)}}=F L O P s(g,N,d,D).$ ", "page_idx": 7}, {"type": "image", "img_path": "FXJDcriMYH/tmp/9ce80b58144c426513e42a3e00930ca68f454d60629fabb1794a9b0e37657f26.jpg", "img_caption": ["Figure 8: In 410M, 1.1B, and 3B LLMs, we plot smoothed loss curves for different growth timing $d$ given a set of FLOPs to form IsoFLOP figures. We find a clear valley in loss, indicating that for a given FLOP budget, there exists an optimal growth timing $d$ for the $G_{\\mathrm{stack}}$ operation. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "So when given a budget $C$ , our objective is to identify the optimized values $D,\\;N,\\;d,\\;g$ that minimize the loss $\\mathcal{L}(D,N,d,g)$ . However, simultaneously optimizing the above four variables can be computationally expensive. Therefore, instead of searching for global optimals, we separately determine two factors closely related to the $G_{\\mathrm{stack}}$ : the training tokens for the small model (growth timing) $d$ and the growth factor $g$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n{\\underset{f,h}{\\operatorname{arg\\,min}}}\\ {\\mathcal{L}}(D,N,d,g),\\quad{\\mathrm{where~}}d=f(D,N),g=h(D,N)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Determining Growth Timing: $d$ . We first explore the effect of growth timing, i.e. the training token $d$ for the small model. Particularly, we apply the $G_{\\mathrm{stack}}$ operator to a series of small models trained with $d\\,=\\,0B,1B,5B,10B,20B,50B$ tokens. Subsequently, we stack them to the target layers with growth factor $g=4$ and train for a fixed set of computational FLOPs. We replicate the above experiments using three target model sizes $N=\\bar{4}10M,1.1B,3B$ and plot each set of IsoFLOP points in Figure 8a, 8b and 8c. Surprisingly, even a small model trained with just $1B$ tokens exhibits a significant speedup compared to the directly stacking small random initialized models (represented as \u201c0B\u201d). While 0B\u2019s performance is similar to models trained from scratch, implying stacking itself does not serve as an effective initialization method. Furthermore, by applying smoothing techniques to model IsoFLOP curves as parabolas, we identify $d$ ", "page_idx": 7}, {"type": "text", "text": "the optimized value of that minimizes loss for each FLOP count, leading us to hypothesize the existence of a logarithmic equation involving ", "page_idx": 7}, {"type": "image", "img_path": "FXJDcriMYH/tmp/491de3dac63ddd441c7306be6420e1d253a21edc7969df635437074a274e0928.jpg", "img_caption": ["Figure 9: We fit a contour figure for predicting $d$ given $C$ and $N$ . These optimal growth timing $d$ fti the figure well. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "$N,C$ , and $d$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\nl o g_{10}(d)=a\\log_{10}(N)+\\frac{b}{\\log_{10}(C)}+c\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "After fitting, we obtain $a\\,=\\,0.88$ , $b=163.27$ and $c=-5.74$ and we plot the contour figure in Figure 9. It can be observed that our estimated curves align well with the actual optimal points. ", "page_idx": 8}, {"type": "text", "text": "Determining Growth Factor: ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "$g$ . Another factor we determine is the growth factor $g$ . As models with 3B and 7B parameters have identical depths, we run experiments using two model sizes: 1.1B (24 layers) and 3B (32 layers). Specifically, we vary the stack factors to $g\\;=\\;2,4,8,24$ for the 1.1B model and $g=4,8$ , 16, 32 for the 3B model while keeping the base models trained with $d=10\\mathbf{B}$ tokens. The smoothed IsoFLOP curves are plotted in Figure 10. Interestingly, even with a relatively shallow 2-layer base model and a growth factor ", "page_idx": 8}, {"type": "image", "img_path": "FXJDcriMYH/tmp/ff8c32cc3e3c8d17bf81edbe308eef1ef91902dfe3b0700f5d915e72b3e57f9a.jpg", "img_caption": ["Figure 10: In 1.1B, and 3B LLMs, we plot smoothed loss curves for different growth factor $g$ given a set of FLOPs as IsoFLOP figures. The optimal $\\mathrm{g}$ falls between 2 and 4. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "of $g\\,=\\,16$ , we observe a remarkable improvement compared to the baseline 3B model $(g\\,=\\,1)$ ). However, when using a 1-layer base model, $G_{\\mathrm{stack}}$ underperforms compared to the baseline. Our curves indicate that the optimal growth factor $g$ lies between 2 and 4. ", "page_idx": 8}, {"type": "text", "text": "However, unlike determining training token $d$ , we cannot generate sufficient data to estimate the relationship between $N,\\,C$ , and $g$ , due to computational constraints. Thus, this work suggests a constant growth factor of $g=4$ . We also include our preliminary estimated equation and contour figure for $g$ in the Appendix F. All evaluation results of Section 4.2 are listed in Appendix G. ", "page_idx": 8}, {"type": "text", "text": "5 Ablation and Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To further give insights into adopting model growth techniques in LLM pre-training, we ablate variances for $G_{\\mathrm{stack}}$ and discuss function preserving in general model growth techniques. ", "page_idx": 8}, {"type": "text", "text": "5.1 Ablation: How to Stack? ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "It is worth noting that $G_{\\mathrm{stack}}$ differs from the algorithm proposed in StackedBERT [14], which utilizes a gradually stacking strategy. Hence, we compare our \u201cone-hop\u201d $G_{\\mathrm{stack}}$ and their gradual stacking approach. Following the methodology introduced in StackBERT, we employ a two-step stack strategy. Given our target model size of 1.1B with 24 layers, we start with a 6-layer model. Subsequently, we train it on 10B tokens and double the model\u2019s depth through stacking, repeating this step twice (train-stack-train-stack) to achieve the desired scale. Our experiments demonstrate that $G_{\\mathrm{stack}}$ outperforms gradual stacking approaches on loss and downstream evaluations. For example, the evaluation results show that $G_{\\mathrm{stack}}$ achieves a 2.4 higher average accuracy and 0.6 better Wikitext PPL than gradual stacking when pre-training large models for 100B tokens. The results can be found in Appendix H.1. We further compare other stacking variations, such as stacking via interpolation and partial stacking of certain layers which are also adopted in LlamaPro [42] and Solar [43], and leave our detailed findings in the Appendix H.2 and H.3. ", "page_idx": 8}, {"type": "text", "text": "5.2 Discussion: Why Does Function Preserving Fail? ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Function preservation (FP) is a key concept that underlies most model growth approaches [10\u201312, 17]. The idea is intuitive that a larger model should initialize parameters that can represent the same ", "page_idx": 8}, {"type": "text", "text": "function as the ones in the smaller model, i.e. $\\forall x,f(x;\\Theta^{(s)})=f(x;\\Theta_{i n i t}^{(l)})$ , where $x$ is the input.   \nWe give a mathematical definition of FP in the Appendix I.1. ", "page_idx": 9}, {"type": "text", "text": "We find it intriguing that our $G_{\\mathrm{stack}}$ approach, which violates FP, emerges as the most effective in our study. To further investigate, we conduct a simple ablation study to break FP by introducing noise on the strict-FP operator $G_{\\mathrm{direct}}^{\\rightarrow}$ . We initialize the new neurons by a weighted combination of two sets of parameters: those from $G_{\\mathrm{direct}}^{\\rightarrow}$ and those from random initialization. The weighting factor is controlled by a noise ratio. Our findings are intriguing. After 40B tokens training, adding $20\\%$ noise outperforms original $G_{\\mathrm{direct}}^{\\rightarrow}$ by 0.27 on the Wikitext PPL and 0.41 on the average accuracy score. ", "page_idx": 9}, {"type": "text", "text": "We also add noise for $G_{\\mathrm{stack}}$ . When we add $20\\%$ noise, our LLM performs slightly better than the no-noise model. However, when the noise level exceeds $20\\%$ , the performance significantly deteriorates. These results indicate that function preservation may not be the sole determining factor for model growth. In other words, exploring ways to accelerate the training of larger models and strict preserving function during growth might represent two overlapping yet distinct research directions. The experimental details are provided in the Appendix I.2. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work empirically explores model growth approaches for efficient LLM pre-training. We address three key challenges of current model growth research for efficient LLM pre-training. We first comprehensively evaluate model growth techniques into four atomic operators and explore depthwise growth $G_{\\mathrm{stack}}$ beats all other methods and baselines in various evaluations. We next address concerns about the scalability of $G_{\\mathrm{stack}}$ by extending the model and training data scales. Furthermore, we systematically analyze the usage of the $G_{\\mathrm{stack}}$ operator, focusing on growth timing and growth factor. Based on this analysis, we formalize a set of guidelines for effectively utilizing the $G_{\\mathrm{stack}}$ operator. In addition, we provide in-depth discussions and comprehensive ablation studies of $G_{\\mathrm{stack}}$ , shedding light on the broader implications of our work. ", "page_idx": 9}, {"type": "text", "text": "7 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "While our work has demonstrated remarkable potential, four limitations deserve further attention. One limitation is the constraint of computation resources. For example, we only compare two sets of growth factor $d$ configurations, which limits the capacity to derive a formula for determining the optimal growth factor $d$ . Another limitation of our work is the focus on relatively simple operator choices, where we prioritize simplicity over exploring more sophisticated strategies. For instance, we do not extensively investigate the multi-step growth or dynamic modifications to the training process, such as adjusting the learning rate during continual pre-training. The third limitation involves the incomplete cosine learning rate schedule during training. This also arises from the resource-intensive nature of pre-training LLMs and the constraints on available computational resources. Therefore, we adopt a strategy where we initially set a large number of training tokens and then we pre-train LLMs until the training runs are interrupted by tasks with higher priority. Lastly, although this study\u2019s scope is an empirical exploration and the content is self-contained, there is a lack of theoretical insights into the success of $G_{\\mathrm{stack}}$ in LLM pre-training.6 Nonetheless, we will release all LLM checkpoints to facilitate the community\u2019s investigation into the theoretical principles behind our observations. ", "page_idx": 9}, {"type": "text", "text": "8 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank all constructive comments from anonymous reviewers. Reynold Cheng and Wenyu Du were supported by the Hong Kong Jockey Club Charities Trust (Project 260920140), the University of Hong Kong (Project 109000579), the HKU Outstanding Research Student Supervisor Award 2022-23, and the HKU Faculty Exchange Award 2024 (Faculty of Engineering). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. Cited on page 1.   \n[2] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models, 2022. Cited on page 1.   \n[3] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. Cited on pages 1, 2, 7, and 8.   \n[4] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. Cited on pages 2, 3, 7, and 8.   \n[5] Ibrahim Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisiting neural scaling laws in language and vision, 2022. Cited on pages 1 and 7.   \n[6] Mengwei Xu, Wangsong Yin, Dongqi Cai, Rongjie Yi, Daliang Xu, Qipeng Wang, Bingyang Wu, Yihao Zhao, Chen Yang, Shihe Wang, Qiyang Zhang, Zhenyan Lu, Li Zhang, Shangguang Wang, Yuanchun Li, Yunxin Liu, Xin Jin, and Xuanzhe Liu. A survey of resource-efficient llm and multimodal foundation models, 2024. Cited on page 1.   \n[7] AI@Meta. Llama 3 model card, 2024. Cited on pages 1, 2, and 30.   \n[8] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga Behram, James Huang, Charles Bai, Michael Gschwind, Anurag Gupta, Myle Ott, Anastasia Melnikov, Salvatore Candido, David Brooks, Geeta Chauhan, Benjamin Lee, Hsien-Hsin S. Lee, Bugra Akyildiz, Maximilian Balandat, Joe Spisak, Ravi Jain, Mike Rabbat, and Kim Hazelwood. Sustainable ai: Environmental implications, challenges and opportunities, 2022. Cited on page 1.   \n[9] Alex de Vries. The growing energy footprint of artificial intelligence. Joule, 7(10):2191\u20132194, 2023. Cited on page 1.   \n[10] Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge transfer. arXiv preprint arXiv:1511.05641, 2015. Cited on pages 2, 3, 4, 9, and 16.   \n[11] Cheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang, Yujia Qin, Fengyu Wang, Zhi Wang, Xiao Chen, Zhiyuan Liu, and Qun Liu. bert2bert: Towards reusable pretrained language models. arXiv preprint arXiv:2110.07143, 2021. Cited on pages 2, 3, 4, and 16.   \n[12] Yite Wang, Jiahao Su, Hanlin Lu, Cong Xie, Tianyi Liu, Jianbo Yuan, Haibin Lin, Ruoyu Sun, and Hongxia Yang. Lemon: Lossless model expansion, 2023. Cited on pages 2, 3, 4, 9, and 16.   \n[13] Sheng Shen, Pete Walsh, Kurt Keutzer, Jesse Dodge, Matthew Peters, and Iz Beltagy. Staged training for transformer language models. In International Conference on Machine Learning, pages 19893\u201319908. PMLR, 2022. Cited on pages 2, 3, 4, and 16.   \n[14] Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu. Efficient training of bert by progressively stacking. In International conference on machine learning, pages 2337\u20132346. PMLR, 2019. Cited on pages 2, 3, 4, 9, and 16.   \n[15] Peihao Wang, Rameswar Panda, Lucas Torroba Hennigen, Philip Greengard, Leonid Karlinsky, Rogerio Feris, David Daniel Cox, Zhangyang Wang, and Yoon Kim. Learning to grow pretrained models for efficient transformer training. arXiv preprint arXiv:2303.00980, 2023. Cited on pages 2, 3, 4, and 16.   \n[16] Utku Evci, Bart van Merrienboer, Thomas Unterthiner, Max Vladymyrov, and Fabian Pedregosa. Gradmax: Growing neural networks using gradient information. arXiv preprint arXiv:2201.05125, 2022. Cited on pages 2, 3, 4, and 16.   \n[17] Yiqun Yao, Zheng Zhang, Jing Li, and Yequan Wang. Masked structural growth for $2\\mathbf{x}$ faster language model pre-training, 2024. Cited on pages 2, 3, 4, 5, 9, and 16.   \n[18] Cheng Yang, Shengnan Wang, Chao Yang, Yuechuan Li, Ru He, and Jingqiao Zhang. Progressively stacking 2.0: A multi-stage layerwise training method for bert training speedup. arXiv preprint arXiv:2011.13635, 2020. Cited on pages 2, 3, 4, and 16.   \n[19] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mistral 7b, 2023. Cited on page 2.   \n[20] Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Xuying Meng, Siqi Fan, Peng Han, Jing Li, Li Du, Bowen Qin, Zheng Zhang, Aixin Sun, and Yequan Wang. Flm-101b: An open llm and how to train it with $\\mathbb{S}100\\mathbf{k}$ budget, 2023. Cited on pages 2, 3, and 4.   \n[21] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. Cited on pages 2, 3, and 30.   \n[22] Jean Kaddour, Oscar Key, Piotr Nawrot, Pasquale Minervini, and Matt J. Kusner. No train no gain: Revisiting efficient training algorithms for transformer-based language models, 2023. Cited on pages 2 and 6.   \n[23] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPof,i Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac\u2019h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023. Cited on pages 2 and 4.   \n[24] Scott Fahlman and Christian Lebiere. The cascade-correlation learning architecture. In D. Touretzky, editor, Advances in Neural Information Processing Systems, volume 2. Morgan-Kaufmann, 1989. Cited on page 3.   \n[25] Scott E. Fahlman. The recurrent cascade-correlation architecture. In Neural Information Processing Systems, 1990. No citations.   \n[26] Steven Gutstein, Olac Fuentes, and Eric A. Freudenthal. Knowledge transfer in deep convolutional neural nets. In Int. J. Artif. Intell. Tools, 2007. Cited on page 3.   \n[27] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019. Cited on page 3.   \n[28] Lemeng Wu, Bo Liu, Peter Stone, and Qiang Liu. Firefly neural architecture descent: a general approach for growing neural networks, 2021. Cited on page 4.   \n[29] Xin Yuan, Pedro Savarese, and Michael Maire. Accelerated training via incrementally growing neural networks using variance transfer and learning rate adaptation, 2023. Cited on page 4.   \n[30] Denis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. The lambada dataset: Word prediction requiring a broad discourse context, 2016. Cited on page 5.   \n[31] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. Cited on page 5.   \n[32] Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: A challenge dataset for machine reading comprehension with logical reasoning, 2020. Cited on page 5.   \n[33] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language, 2019. Cited on page 5.   \n[34] Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science questions, 2017. Cited on page 5.   \n[35] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019. Cited on page 5.   \n[36] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. Cited on page 5.   \n[37] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models, 2023. Cited on page 7.   \n[38] Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi Zhang, Yanghua Peng, Xiang Li, Cong Xie, Shibiao Nong, Yulu Jia, Sun He, Hongmin Chen, Zhihao Bai, Qi Hou, Shipeng Yan, Ding Zhou, Yiyao Sheng, Zhuo Jiang, Haohan Xu, Haoran Wei, Zhang Zhang, Pengfei Nie, Leqi Zou, Sida Zhao, Liang Xiang, Zherui Liu, Zhe Li, Xiaoying Jia, Jianxi Ye, Xin Jin, and Xin Liu. Megascale: Scaling large language model training to more than 10,000 gpus, 2024. Cited on page 7.   \n[39] Zhengxiao Du, Aohan Zeng, Yuxiao Dong, and Jie Tang. Understanding emergent abilities of language models from the loss perspective, 2024. Cited on page 7.   \n[40] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St\u00e9fan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, \u02d9Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Ant\u00f4nio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, Aditya Vijaykumar, Alessandro Pietro Bardelli, Alex Rothberg, Andreas Hilboll, Andreas Kloeckner, Anthony Scopatz, Antony Lee, Ariel Rokem, C. Nathan Woods, Chad Fulton, Charles Masson, Christian H\u00e4ggstr\u00f6m, Clark Fitzgerald, David A. Nicholson, David R. Hagen, Dmitrii V. Pasechnik, Emanuele Olivetti, Eric Martin, Eric Wieser, Fabrice Silva, Felix Lenders, Florian Wilhelm, G. Young, Gavin A. Price, Gert-Ludwig Ingold, Gregory E. Allen, Gregory R. Lee, Herv\u00e9 Audren, Irvin Probst, J\u00f6rg P. Dietrich, Jacob Silterra, James T Webber, Janko Slavic\u02c7, Joel Nothman, Johannes Buchner, Johannes Kulick, Johannes L. Sch\u00f6nberger, Jos\u00e9 Vin\u00edcius de Miranda Cardoso, Joscha Reimer, Joseph Harrington, Juan Luis Cano Rodr\u00edguez, Juan Nunez-Iglesias, Justin Kuczynski, Kevin Tritz, Martin Thoma, Matthew Newville, Matthias K\u00fcmmerer, Maximilian Bolingbroke, Michael Tartre, Mikhail Pak, Nathaniel J. Smith, Nikolai Nowaczyk, Nikolay Shebanov, Oleksandr Pavlyk, Per A. Brodtkorb, Perry Lee, Robert T. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "McGibbon, Roman Feldbauer, Sam Lewis, Sam Tygier, Scott Sievert, Sebastiano Vigna, Stefan Peterson, Surhud More, Tadeusz Pudlik, Takuya Oshima, Thomas J. Pingel, Thomas P. Robitaille, Thomas Spura, Thouis R. Jones, Tim Cera, Tim Leslie, Tiziano Zito, Tom Krauss, Utkarsh Upadhyay, Yaroslav O. Halchenko, and Yoshiki V\u00e1zquez-Baeza. Scipy 1.0: fundamental algorithms for scientific computing in python. Nature Methods, 17(3):261\u2013272, February 2020. Cited on page 7. ", "page_idx": 13}, {"type": "text", "text": "[41] Haihang Wu, Wei Wang, Tamasha Malepathirana, Damith Senanayake, Denny Oetomo, and Saman Halgamuge. When to grow? a ftiting risk-aware policy for layer growing in deep neural networks, 2024. Cited on page 8.   \n[42] Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye Feng, Ping Luo, and Ying Shan. Llama pro: Progressive llama with block expansion. arXiv preprint arXiv:2401.02415, 2024. Cited on pages 9 and 37.   \n[43] Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, et al. Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling. arXiv preprint arXiv:2312.15166, 2023. Cited on pages 9 and 37.   \n[44] Nikunj Saunshi, Stefani Karp, Shankar Krishnan, Sobhan Miryoosef,i Sashank J. Reddi, and Sanjiv Kumar. On the inductive bias of stacking towards improving reasoning, 2024. Cited on page 10.   \n[45] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model, 2024. Cited on page 22.   \n[46] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022. Cited on page 22.   \n[47] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama. https://www.cerebras.net/blog/ slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpa 2023. Cited on page 22.   \n[48] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence?, 2019. Cited on page 27.   \n[49] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods, 2022. Cited on page 27.   \n[50] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021. Cited on page 27.   \n[51] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 2397\u20132430. PMLR, 2023. Cited on page 29.   \n[52] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An $800\\mathrm{gb}$ dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. Cited on pages 29 and 39.   \n[53] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. ", "page_idx": 13}, {"type": "text", "text": "Smith, and Hannaneh Hajishirzi. Olmo: Accelerating the science of language models. Preprint, 2024. Cited on page 39. ", "page_idx": 14}, {"type": "text", "text": "[54] Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Tim Baldwin, and Eric P. Xing. Llm360: Towards fully transparent open-source llms, 2023. Cited on page 39.   \n[55] Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research. arXiv preprint, 2024. Cited on page 39. ", "page_idx": 14}, {"type": "text", "text": "A Details of Growth Operators ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Four Growth Operators ", "page_idx": 15}, {"type": "text", "text": "A.1.1 Operator $G_{\\mathrm{direct}}$ : Direct Derivation of Grown Parameters From Old Parameters ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "One intuitive strategy for expanding neural networks involves directly duplicating or splitting existing neurons. [14, 11, 12]. Unlike other growth operators, we distinguish between growth in terms of depth and width. ", "page_idx": 15}, {"type": "text", "text": "For width-wise expansion, the Net2Net technique and its transformer implementations [10, 11] involve splitting old neurons into two or more parts, with each splitting step achieving $\\scriptstyle{\\mathrm{a=b+c}}$ Depending on the specific splitting mechanism, there are two variations: even splitting and uneven splitting. The latter is proposed to address symmetry issues that arise when neurons are evenly split. In this paper, we adopt the approach of uneven splitting. ", "page_idx": 15}, {"type": "text", "text": "In the context of depth-wise expansion, a common practice is to duplicate layers, often referred to as \u201cstacking\u201d [14]. Therefore, we use the term $G_{\\mathrm{stack}}$ to represent this operator. While this approach may appear to deviate from function preservation, it surprisingly yields a strong baseline. ", "page_idx": 15}, {"type": "text", "text": "A.1.2 Operator $G_{\\mathrm{learn}}$ : Generation of New Parameters through Matrix Transformation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "$G_{\\mathrm{learn}}$ is an operator that learns a matrix transformation function to map small models to a larger one [15]. This operator is applicable to both width and depth expansion. Considering the original model $f$ with parameters $\\theta$ , the target model $F$ with parameters $\\Theta$ , and $G_{\\mathrm{learn}}$ as the hypernetwork for meta-learning, the training corpus is denoted as $\\mathcal{D}$ , and the language model loss is denoted as $\\mathcal{L}$ . Then, we optimize the following objective: ", "page_idx": 15}, {"type": "equation", "text": "$$\na r g\\;m i n\\;\\mathbb{E}_{x\\sim\\mathcal{D}}\\;\\mathcal{L}(x;F_{\\Theta}),\\;\\;\\;\\mathrm{where}\\;\\Theta=G_{\\mathrm{learn}}(\\theta)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.1.3 Operator $G_{\\mathbf{zero}}$ : Setting New Parameters to 0 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Setting new parameters to zero is often considered a simple method to achieve function preservation. However, optimizing networks with a significant number of zeros can present challenges. To tackle this issue, we adopt current practices that selectively zero out either the fan-in or fan-out parameters [13, 16, 12]. Specifically, for operator $G_{\\mathrm{zero}}$ , during width growing, we zero out only the set of fan-out parameters for new neurons and randomly initialize the remaining ones. In the case of depthwise expansion, we zero out the final output layer of the newly-duplicated transformer blocks\u2019 MultiHead Attention and MLP. ", "page_idx": 15}, {"type": "text", "text": "A.1.4 Operator $G_{\\mathrm{random}}$ : Random Initialization of New Parameters ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This group follows the common practice of randomly initializing new parameters. In earlier attempts, old neurons were frozen after the growth process [18, 17]. However, to ensure function preservation, a recent study introduces a mask for new neurons after expansion [17]. This mask is gradually removed during ongoing training. We refer to this new approach as the growth operator $G_{\\mathrm{random}}$ . ", "page_idx": 15}, {"type": "text", "text": "A.2 Difference of Our Operators and Base Methods ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The operators $G_{\\mathrm{direct}}^{\\rightarrow}$ shares a similar setting to Lemon with minor variances due to Llama achitectures. $G_{\\mathrm{learn}}$ is consistent with the methods LiGO, but with our own implementation. For $G_{\\mathrm{zero}}$ , our approach aligns with Lemon in terms of depth, but differs from stagedTraining in width. Unlike stagedTraining, we do not double the width and assign zeros to the off-diagonal entries. Instead, our approach is more flexible; by zeroing out the submatrix in the bottom-left corner, we can extend it to any dimension. Our $G_{\\mathrm{random}}$ does not exhibit the \u201cmulti-hop\u201d growth like MSG, instead, it grows \u201cone-hop\u201d directly to the target size. Our implementation of $G_{\\mathrm{direct}}^{\\uparrow}\\;(G_{\\mathrm{stack}})$ differs from the algorithm employed in stackedBert. In stackedBert, a gradual growing technique is utilized, whereas our operator follows a more direct approach. ", "page_idx": 15}, {"type": "text", "text": "A.3 Details of $G_{\\mathrm{direct}}$ ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Embedding Consider $E\\in\\mathbb{R}^{V\\times d}$ , and our goal is to expand it to $E^{\\prime}\\in\\mathbb{R}^{V\\times D}$ , $G_{\\mathrm{direct}}$ just copy some columns: ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\begin{array}{l l l}{E^{\\prime}}&{=}&{G_{\\mathrm{direct}}(E)}\\\\ &{=}&{E R}\\\\ &{=}&{E\\left[\\underbrace{I}_{d},\\quad I\\right]}\\end{array}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $R\\in\\mathbb{R}^{d\\times D}$ is used to copy the embedding matrix $E$ . ", "page_idx": 16}, {"type": "text", "text": "Linear Consider $W\\in\\mathbb{R}^{d_{o u t}\\times d_{i n}}$ , target parameter $W^{\\prime}\\in\\mathbb{R}^{D_{o u t}\\times D_{i n}}$ , where $d_{o u t}\\leq D_{o u t},d_{i n}\\leq$ $D_{i n}$ , $G_{\\mathrm{direct}}$ is defined as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c c l}{{W^{\\prime}}}&{{=}}&{{G_{\\mathrm{direct}}(W)}}\\\\ {{}}&{{=}}&{{L W R}}\\\\ {{}}&{{=}}&{{d_{o u t}\\left\\{\\begin{array}{l l}{{\\displaystyle\\left[I_{}^{T}\\right.}}&{{\\displaystyle I\\right]W\\left[\\underbrace{\\alpha}_{d_{i n}}\\!\\!\\!\\!\\!}}&{{\\displaystyle\\beta\\right]}}\\end{array}\\right.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $R\\in\\mathbb{R}^{d_{i n}\\times D_{i n}}$ is used for expanding the fan-in and $L\\in\\mathbb{R}^{D_{o u t}\\times d_{o u t}}$ is used for expanding the fan-out. To satisfy function preserving, we ensure that $\\alpha+\\beta=I$ . ", "page_idx": 16}, {"type": "text", "text": "RMSNorm For\u221a RMSNorm, a similar approach is adopted, consider parameter $\\mu\\in\\mathbb{R}^{d}$ , expanded parameter $\\begin{array}{r}{\\mu^{\\prime}=\\frac{\\sqrt{d}}{\\sqrt{D}}[\\mu,\\;\\mu_{0,D-d}]\\in\\mathbb{R}^{D}}\\end{array}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{R M S N o r m^{\\prime}(x^{\\prime})=\\frac{x^{\\prime}}{\\sqrt{\\frac{1}{D}\\sum_{i=1}^{D}x^{\\prime}_{i}^{2}}}\\odot\\mu^{\\prime}}\\\\ &{}&\\\\ &{}&{\\,=[\\sqrt{\\frac{\\sum_{i=1}^{d}x_{i}^{2}}{\\sum_{i=1}^{D}x^{\\prime}_{i}^{2}}}\\times R M S N o r m(x),\\,\\zeta]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, using the $G_{\\mathrm{direct}}$ , it is not possible to achieve function preservation for RMSNorm ", "page_idx": 16}, {"type": "text", "text": "Depth $\\mathbf{\\Gamma}(G_{\\mathbf{stack}})$ Consider a transformer with $l$ layers represented as $F=f_{0}\\circ f_{1}\\circ\\cdot\\cdot\\cdot\\circ f_{l}$ . Our objective is to expand it to $L$ layers, where $L$ is a multiple of $l$ . We have various stacking forms for this purpose, such as (a) direct stacking: $F^{\\prime}=F\\circ F\\circ\\cdot\\cdot\\cdot\\circ F$ . ", "page_idx": 16}, {"type": "text", "text": "Algorithm 1 Operator $G_{\\mathrm{stack}}$ ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Input: Base model $M_{k}^{l}$ with $l$ layers trained using dataset $d_{k}$ where $k$ is iteration steps. Growth factor $g$ .   \nOutput: Target Model $\\mathcal{M}_{0}^{g l}$ with $_{g l}$ layers   \n$\\mathcal{M}_{0}^{l}{=}M_{k}^{l}$   \nfor $t\\;=\\;2$ to $g$ do \u25b7Model Stacking Mt0l = M(0t\u22121)l\u25e6 ", "page_idx": 16}, {"type": "text", "text": "end ", "page_idx": 16}, {"type": "text", "text": "A.4 Details of $G_{\\mathbf{zero}}$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Embedding Consider an embedding matrix $E\\in\\mathbb{R}^{V\\times d}$ . The $G_{\\mathrm{zero}}$ operator expands it to $E^{\\prime}\\in$ RV \u00d7D with O, where d \u2264D. Formally: ", "page_idx": 17}, {"type": "equation", "text": "$$\nE^{\\prime}=[E,\\,O]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, give a token $x$ , the expanded embedding can be expressed as: ", "page_idx": 17}, {"type": "equation", "text": "$$\nE m b e d d i n g^{\\prime}(x)=\\mathbb{1}_{x}E^{\\prime}=[E m b e d d i n g(x),\\;0_{D-d}]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Linear Consider parameter $W\\in\\mathbb{R}^{d_{o u t}\\times d_{i n}}$ . $G_{\\mathrm{zero}}$ expand it to $W^{\\prime}\\in\\mathbb{R}^{D_{o u t}\\times D_{i n}}$ , where $d_{o u t}\\leq$ $D_{o u t}$ and $d_{i n}\\leq D_{i n}$ . Formally: ", "page_idx": 17}, {"type": "equation", "text": "$$\nW^{\\prime}=\\left[\\!\\!\\begin{array}{c c}{{W}}&{{{\\mathcal{A}}}}\\\\ {{O}}&{{{\\mathcal{C}}}}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathcal{A},\\mathcal{C}$ are randomly initialized new parameters. Considering the input token $\\boldsymbol{x}\\in\\mathbb{R}^{d_{i n}}$ before expansion, and the input after expansion $\\dot{x}^{\\prime}\\in\\mathbb{R}^{D_{i n}}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\nx^{\\prime}=[x,\\;0_{D_{i n}-d_{i n}}]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l c l}{L i n e a r^{\\prime}(x^{\\prime})}&{=}&{x^{\\prime}W^{\\prime T}}\\\\ &{=}&{[x,\\;0_{D_{i n}-d_{i n}}]\\left[\\stackrel{W^{T}}{A^{T}}\\right.\\;\\left.\\mathcal{C}^{T}\\right]}\\\\ &{=}&{[x W^{T},\\;0_{D_{o u t}-d_{o u t}}]}\\\\ &{=}&{[L i n e a r(x),\\;0_{D_{o u t}-d_{o u t}}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "RMSNorm Considering the parameter $\\mu\\in\\mathbb{R}^{d}$ , $G_{\\mathrm{zero}}$ expand it to $\\mu^{\\prime}=[\\alpha\\mu,\\,\\xi]$ like $G_{\\mathrm{random}}$ in Appendix A.5, because the input must be $x^{\\prime}=[x,\\;0_{D-d}]\\in\\mathbb R^{D}$ . ", "page_idx": 17}, {"type": "text", "text": "Depth In depth, by retaining only the residual part and initializing the MHA and SwiGLU final linear projections to zero, the MHA and SwiGLU layers can achieve function preservation. ", "page_idx": 17}, {"type": "text", "text": "A.5 Details of $G_{\\mathrm{random}}$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Embedding Consider an embedding matrix $E\\in\\mathbb{R}^{V\\times d}$ . The goal of $G_{\\mathrm{random}}$ is to expand it to $E^{\\prime}\\in\\mathbb{R}^{V\\times\\breve{D}}$ , where $d\\leq D$ . Formally: ", "page_idx": 17}, {"type": "equation", "text": "$$\nE^{\\prime}=[E,\\,\\mathcal{E}]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathcal{E}\\in\\mathbb{R}^{V\\times(D-d)}$ represents randomly initialized new parameters. We use a mask $c\\in\\mathbb{R}^{D}$ to mask out the randomly initialized parts: ", "page_idx": 17}, {"type": "equation", "text": "$$\nc=[1_{d},\\;0_{D-d}]\\rightarrow[1_{d},\\;1_{D-d}]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, for a token $x$ , the masked embedding can be expressed as: ", "page_idx": 17}, {"type": "equation", "text": "$$\nE m b e d d i n g^{\\prime}(x)=\\mathbb{1}_{x}E^{\\prime}\\odot c=[E m b e d d i n g(x),\\;0_{D-d}]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Linear Consider parameter $W\\in\\mathbb{R}^{d_{o u t}\\times d_{i n}}$ . Our goal is to expand it to $W^{\\prime}\\in\\mathbb{R}^{D_{o u t}\\times D_{i n}}$ , where $d_{o u t}\\leq D_{o u t}$ and $d_{i n}\\leq D_{i n}$ . Formally: ", "page_idx": 18}, {"type": "equation", "text": "$$\nW^{\\prime}=\\left[{W\\atop B}\\quad{\\mathcal{C}}\\right]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $A,B,C$ are randomly initialized new parameters. Considering the input token $\\boldsymbol{x}\\in\\mathbb{R}^{d_{i n}}$ before expansion, and the input after expansion $\\boldsymbol{x^{\\prime}}\\in\\mathbb{R}^{D_{i n}}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\nx^{\\prime}=[x,\\;0_{D_{i n}-d_{i n}}]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{x^{\\prime}W^{\\prime T}}&{=}&{[x,\\;0_{D_{i n}-d_{i n}}]\\;\\Big[\\b{W}^{T}}&{\\b{B}^{T}\\Big]}\\\\ &{=}&{[x\\b{W}^{T},\\;x\\b{B}^{T}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To ensure that the expanded part of $x^{\\prime}$ starts with zeros, we still utilize a mask: ", "page_idx": 18}, {"type": "equation", "text": "$$\nc=[1_{d_{o u t}},\\;0_{D_{o u t}-d_{o u t}}]\\rightarrow[1_{d_{o u t}},\\;1_{D_{o u t}-d_{o u t}}]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\nL i n e a r^{\\prime}(x^{\\prime})=x^{\\prime}W^{\\prime T}\\odot c=[L i n e a r(x),\\;0_{D_{o u t}-d_{o u t}}]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "RMSNorm Considering the parameter $\\mu\\in\\mathbb{R}^{d}$ , our objective is to expand it to $\\mu^{\\prime}=[\\alpha\\mu,\\,\\xi]\\in\\mathbb{R}^{D}$ , where $\\alpha$ is an undetermined coefficient and $\\xi$ is a randomly initialized new parameter. Let the input be $x^{\\prime}=[x,\\;0_{D-d}]\\in\\mathbb R^{D}$ , then we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{D}x^{\\prime2}=\\sum_{i=0}^{d}x^{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{{R M S N o r m^{\\prime}(x^{\\prime})}}&{{=}}&{{\\displaystyle\\frac{x^{\\prime}}{\\sqrt{\\frac{1}{D}\\sum_{i=0}^{D}x_{i}^{\\prime2}}}\\odot\\mu^{\\prime}}}\\\\ {{}}&{{=}}&{{\\displaystyle\\frac{[x,\\,0_{D-d}]}{\\sqrt{\\frac{1}{D}\\sum_{i=0}^{d}x_{i}^{\\,2}}}\\odot[\\alpha\\mu,\\,\\xi]}}\\\\ {{}}&{{=}}&{{\\displaystyle\\left[\\frac{\\sqrt{D}}{\\sqrt{d}}\\frac{x}{\\sqrt{\\frac{1}{d}\\sum_{i=0}^{d}x_{i}^{\\,2}}}\\odot\\alpha\\mu,\\,0_{D-d}\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By observing equation 32, we can conclude that, to achieve function preservation, $\\begin{array}{r}{\\alpha=\\frac{\\sqrt{d}}{\\sqrt{D}}}\\end{array}$ . Finally, we can conclude: ", "page_idx": 18}, {"type": "equation", "text": "$$\nR M S N o r m^{\\prime}(x^{\\prime})=[R M S N o r m(x),\\;0_{D-d}]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Depth In depth, preserving only the residual part and masking the MHA and SwiGLU layers can achieve function preservation: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Y=X+M H A(R M S N o r m(X))\\odot c}\\\\ {Y=X+S w i G L U(R M S N o r m(X))\\odot c}\\\\ {c=0_{D}\\to1_{D}\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "A.6 Details of $G_{\\mathrm{learn}}$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Using $G_{\\mathrm{learn}}$ for width expansion, for the embedding layer $E\\in\\mathbb{R}^{V\\times d}$ , the parameter $B_{e m b}\\in\\mathbb{R}^{D\\times d}$ is defined as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\nE^{\\prime}=E B_{e m b}^{T}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For Attention layer, where $W_{Q},W_{K},W_{V}$ , and $W_{O}\\in\\mathbb{R}^{d\\times d}$ , and RMSNorm $\\mu_{1}\\in\\mathbb{R}^{d}$ , the parameters $B_{Q},B_{K}$ , and $B_{V}\\in\\mathbb{R}^{D\\times d}$ , we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{r c l}{{W_{Q}^{\\prime}}}&{{=}}&{{B_{Q}W_{Q}B_{e m b}^{T}}}\\\\ {{W_{K}^{\\prime}}}&{{=}}&{{B_{K}W_{K}B_{e m b}^{T}}}\\\\ {{W_{V}^{\\prime}}}&{{=}}&{{B_{V}W_{V}B_{e m b}^{T}}}\\\\ {{W_{O}^{\\prime}}}&{{=}}&{{B_{e m b}W_{O}B_{V}^{T}}}\\\\ {{\\mu_{1}^{\\prime}}}&{{=}}&{{B_{e m b}\\mu_{1}}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For MLP, where $W_{u p},W_{g a t e}\\in\\mathbb{R}^{d_{m l p}\\times d}$ , $W_{d o w n}\\in\\mathbb{R}^{d\\times d_{m l p}}$ , RMSNorm $\\mu_{2}\\in\\mathbb{R}^{d}$ , the parameter $B_{m l p}\\in\\mathbb{R}^{D_{m l p}\\times d_{m l p}}$ , we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{c c c}{W_{u p}^{\\prime}}&{=}&{B_{m l p}W_{u p}B_{e m b}^{T}}\\\\ {W_{d o w n}^{\\prime}}&{=}&{B_{e m b}W_{m l p}B_{m l p}^{T}}\\\\ {W_{g a t e}^{\\prime}}&{=}&{B_{m l p}W_{g a t e}B_{e m b}^{T}}\\\\ {\\mu_{2}^{\\prime}}&{=}&{B_{e m b}\\mu_{2}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For the output head $W_{h e a d}\\in\\mathbb{R}^{V\\times d}$ , we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\nW_{h e a d}^{\\prime}=W_{h e a d}B_{e m b}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using $G_{\\mathrm{learn}}$ for depth expansion, consider a transformer model with $L_{1}$ layers, we use $G_{\\mathrm{learn}}$ to expand it to $L_{2}$ layers. For $l\\in\\{1,2,\\cdots,L_{2}\\}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{c c c}{{W_{l}^{Q^{\\prime}}}}&{{=}}&{{\\sum_{j=1}^{L_{1}}D_{l,j}^{Q}W_{j}^{Q}}}\\\\ {{W_{l}^{K^{\\prime}}}}&{{=}}&{{\\sum_{j=1}^{L_{1}}D_{l,j}^{K}W_{j}^{K}}}\\\\ {{W_{l}^{V^{\\prime}}}}&{{=}}&{{\\sum_{j=1}^{L_{1}}D_{l,j}^{V}W_{j}^{V}}}\\\\ {{W_{l}^{O^{\\prime}}}}&{{=}}&{{\\sum_{j=1}^{L_{1}}D_{l,j}^{O}W_{j}^{O}}}\\\\ {{\\mu_{l}^{(l n1)^{\\prime}}}}&{{=}}&{{\\sum_{j=1}^{L_{1}}D_{l,j}^{(l n1)}\\mu_{j}^{(l n1)}}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $D^{Q,K,V,O,l n1}\\ \\in\\ \\mathbb{R}^{L_{2}\\times L_{1}}$ represents learnable parameters. These parameters are used to expand the MHA vertically in depth. Similarly, for SwiGLU, we also perform expansion using a similar method. Formally, this can be written as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{c c c}{W_{l}^{u p^{\\prime}}}&{=}&{\\sum_{j=1}^{L_{1}}D_{l,j}^{u p}W_{j}^{u p}}\\\\ {W_{l}^{d o w n^{\\prime}}}&{=}&{\\sum_{j=1}^{L_{1}}D_{l,j}^{d o w n}W_{j}^{d o w n}}\\\\ {W_{l}^{g a t e^{\\prime}}}&{=}&{\\sum_{j=1}^{L_{1}}D_{l,j}^{g a t e}W_{j}^{g a t e}}\\\\ {\\mu_{l}^{(l n2)^{\\prime}}}&{=}&{\\sum_{j=1}^{L_{1}}D_{l,j}^{(l n2)}\\mu_{j}^{(l n2)}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $D^{u p,d o w n,g a t e,l n2}\\in\\mathbb{R}^{L_{2}\\times L_{1}}$ represents learnable parameters used for expanding SwiGLU in the depth. ", "page_idx": 19}, {"type": "text", "text": "B LLMs Framework and Training Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Embedding Consider a vocabulary size $V$ and embedding size $d$ . Then, the embedding matrix $E\\in\\mathbb{R}^{V\\times d}$ , and the one-hot vector for input tokens $X$ is denoted as $\\mathbb{1}_{X}\\in\\mathbb{R}^{T\\times V}$ , where $T$ is the sequence length. Formally, it can be written as: ", "page_idx": 20}, {"type": "equation", "text": "$$\nE m b e d d i n g(X)=\\mathbb{1}_{X}E\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for $i,v\\in[V]$ , where $i\\neq j$ , it is guaranteed that $E_{i}\\neq E_{j}$ . ", "page_idx": 20}, {"type": "text", "text": "Multi-Head Attention Multi-Head Attention (MHA) consists of multiple attention heads, each of which computes its own self-attention. The results of these attention heads are then concatenated and projected to obtain the following output: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{i},K_{i},V_{i}=X W_{i}^{Q},X W_{i}^{K},X W_{i}^{V}}\\\\ &{~~~~~~H_{i}=s o f t m a x(\\frac{Q_{i}K_{i}^{T}}{\\sqrt{d_{h}}})V_{i}}\\\\ &{M H A(X)=C o n c a t(H_{1},\\cdot\\cdot\\cdot\\cdot,H_{n})W^{O}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "here, the input $X\\in\\mathbb{R}^{T\\times d}$ , parameters $W_{i}^{Q}\\in\\mathbb{R}^{d\\times d_{h}}$ , $W_{i}^{K}\\in\\mathbb{R}^{d\\times d_{h}}$ , $W_{i}^{V}\\in\\mathbb{R}^{d\\times d_{h}}$ , and $W^{O}\\in$ $\\mathbb{R}^{d\\times d}$ , where $n\\times d_{h}=d$ . ", "page_idx": 20}, {"type": "text", "text": "Feed Forward Network The Feed Forward Network (FFN) consists of two linear layers and the activation function GeLU. Typically, the two linear layers first perform an up-projection to $d_{F F N}$ and then down-project back to the dimension $d$ . Therefore, FFN is defined as: ", "page_idx": 20}, {"type": "equation", "text": "$$\nF F N(X)=G e L U(X W_{u p})W_{d o w n}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the input $X\\in\\mathbb{R}^{T\\times d}$ , parameter $W_{u p}\\in\\mathbb{R}^{d\\times d_{F F N}}$ and $W_{d o w n}\\in\\mathbb{R}^{d_{F F N}\\times d}$ . ", "page_idx": 20}, {"type": "text", "text": "SwiGLU LLaMA replaces the original FFN in the Transformer Decoder with SwiGLU, resulting in improved performance. SwiGLU consists of three linear layers and the swiglu activation function. It can be defined as: ", "page_idx": 20}, {"type": "equation", "text": "$$\nS w i G L U(X)=(X W_{g a t e}\\odot s w i g l u(X W_{u p}))W_{d o w n}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\odot$ means the element-wise multiplication, the input $X\\in\\mathbb{R}^{T\\times d}$ d, parameter Wup \u2208Rd\u00d7dF F N , $W_{g a t e}\\in\\mathbb{R}^{d\\times d_{F F N}}$ and $W_{d o w n}\\in\\mathbb{R}^{d_{F F N}\\times d}$ . ", "page_idx": 20}, {"type": "text", "text": "RMSNorm Before MHA, FFN, or SwiGLU, there is a layer of RMSNorm to enhance the stability of the model. Compared to LayerNorm, RMSNorm is simpler in form. Formally, it can be written as: ", "page_idx": 20}, {"type": "equation", "text": "$$\nR M S N o r m(X)=\\frac{X}{\\sqrt{\\frac{1}{d}\\sum_{i=1}^{d}X_{i}^{2}}}\\odot\\mu\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $X\\in\\mathbb{R}^{T\\times d}$ , parameter $\\mu\\in\\mathbb{R}^{d}$ . ", "page_idx": 20}, {"type": "text", "text": "B.1 LLMs Training with Growth Operator ", "text_level": 1, "page_idx": 21}, {"type": "table", "img_path": "FXJDcriMYH/tmp/866b03bdbad7f6a0bda60d5524fad1eae602b3d8b4ca73a1b6263546e3f26cc3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "B.2 Details of Speedup Calculation ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We calculate speedup $s p$ between operator $G$ and scratch model pre-training by: ", "page_idx": 21}, {"type": "equation", "text": "$$\ns p=\\frac{F L O P s_{s c r a t c h}}{F L O P s_{G}}-1\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $F L O P s_{s c r a t c h}$ and $F L O P s_{G}$ represent the FLOPs required by the scratch model and the $G$ model, respectively, to achieve the same loss. ", "page_idx": 21}, {"type": "text", "text": "B.3 Details of Training Settings ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We use TinyLlama 7 [45] as our pre-training codebase. We employ FSDP (Fully Sharded DataParallel) along with FlashAttention [46] 2.0, and other acceleration techniques. We use the open-source dataset Slimpajama- ${\\bf\\nabla}627{\\bf B}^{\\mathrm{~8~}}$ [47] for pre-training. The hyperparameters used for each model size are listed in Table 1. Our 7B model is trained over around 100B tokens per day on an NVIDIA Hopper cluster. ", "page_idx": 21}, {"type": "table", "img_path": "FXJDcriMYH/tmp/0302e4b0571a740426d298b2db6395075e40c289431b260fbe30efd6430486c7.jpg", "table_caption": ["Table 1: Hyperparameters "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "C Training Loss and Evaluation Results of Four Operators in both Depth and Width growth ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We have two small (base) models, one trained with token count $d=10B$ and another trained with token count $d=50B$ . ", "page_idx": 21}, {"type": "image", "img_path": "FXJDcriMYH/tmp/e4732c9647c4ca70e0d02a583732cbfc27097f4c4fb1d766bd44b468f3d75a25.jpg", "img_caption": ["Tokens (Billions) ", "(a) Growing in depth from small model (10B) Tokens (Billions) "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "FXJDcriMYH/tmp/de934ba3ce52b418d3912e0ddcce972d50986f98555d765d09a6eac4732e96ad.jpg", "img_caption": ["(c) Growing in width from small model (10B) "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "FXJDcriMYH/tmp/d28897ba09fbe9a69888365135c06b64bf75036deab2f2f1b0289fefbf4596a4.jpg", "img_caption": ["Tokens (Billions) ", "(b) Growing in depth from small model (50B) Tokens (Billions) "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "FXJDcriMYH/tmp/896ff20e39d5c96558f271745c8502ffbddc4ed27a0a664374facfe543207c3d.jpg", "img_caption": ["Figure 11: Training Loss on Slimpajama. ", "(d) Growing in width from small model (50B) "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "FXJDcriMYH/tmp/b4d284a9c0c5c83bc8a379d2088df9e267725e0ccb7017d5117f32f76892ed00.jpg", "img_caption": ["(f) Sciq (Acc \u2191) ", "(g) Winogrande (Acc \u2191) "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 12: Evaluation results on growth in depth from small model (10B) by four operators. ", "page_idx": 23}, {"type": "image", "img_path": "FXJDcriMYH/tmp/a82aa2dfe6e247eae6bac81c194c5f4605e353deafd8c3f6dfa904001a32abe1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 13: Evaluation results on growth in depth from small model (50B) by four operators. ", "page_idx": 23}, {"type": "image", "img_path": "FXJDcriMYH/tmp/26625fbc6c3f913e5d2e6b2ec660e91a14e1b45cc6b7da560d56a4c53c8f70d5.jpg", "img_caption": ["(e) PIQA (Acc \u2191) (f) Sciq (Acc \u2191) (g) Winogrande (Acc \u2191) (h) Wikitext (ppl \u2193) "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 14: Evaluation results on growth in width from small model (10B) by four operators. ", "page_idx": 24}, {"type": "image", "img_path": "FXJDcriMYH/tmp/76115f9165b825341f03a81c2132c58c85e98a5838c7044ace99676492755aa6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 15: Evaluation results on growth in width from small model (50B) by four operators. ", "page_idx": 24}, {"type": "image", "img_path": "FXJDcriMYH/tmp/b98fa4458a8089c1d20555054547e7e4eb55419e636f2ed4dd7201ddf702cf44.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "(a) Growing in depth from (b) Growing in depth from (c) Growing in width from (d) Growing in width from small model (10B) small model (50B) small model (10B) small model (50B) ", "page_idx": 25}, {"type": "text", "text": "Figure 16: Average accuracy of seven standard NLP benchmarks. ", "page_idx": 25}, {"type": "text", "text": "D Evaluation Results of Scaling $G_{\\mathrm{stack}}$ ", "text_level": 1, "page_idx": 25}, {"type": "image", "img_path": "FXJDcriMYH/tmp/233320a09b61f75682fdaa119d02d4ecc8484291f82c47b361f658062af9053c.jpg", "img_caption": ["Figure 17: Average accuracy of standard NLP benchmarks at 3B size. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "FXJDcriMYH/tmp/dd46c56b9dbe7316cd01bb392542c9d307672fbea3eb857a1ffe075df886bc69.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 18: Evaluation results on scratch model and $G_{\\mathrm{stack}}$ model at 3B size. ", "page_idx": 26}, {"type": "text", "text": "D.2 7B ", "text_level": 1, "page_idx": 26}, {"type": "image", "img_path": "FXJDcriMYH/tmp/3e3a3baa1f5ac06d014c5a635ec8f20931eab506199443b8713b607b84800386.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 19: Evaluation results on scratch model and $G_{\\mathrm{stack}}$ model at 7B size. ", "page_idx": 26}, {"type": "image", "img_path": "FXJDcriMYH/tmp/69fab56ef02d302782c1434d751b9d8d91f6acb94b8fe6be0636349defa3c1cf.jpg", "img_caption": ["Figure 20: Average accuracy of standard NLP benchmarks at 410M size. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "FXJDcriMYH/tmp/5795c3b5447e3f230ba2d92fed370e9be45450f3e50ec157bce821aca4e02b45.jpg", "img_caption": ["Figure 21: Evaluation results on scratch model and $G_{\\mathrm{stack}}$ model at 410M size. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "D.4 Instruction Tuning Results on 3B ", "text_level": 1, "page_idx": 27}, {"type": "table", "img_path": "FXJDcriMYH/tmp/5f9f382859e9eea16e23a695a7d4c7226404267ab24529d20b3f5cb2aff4d3cc.jpg", "table_caption": ["Table 2: Evaluation Results after Instruction-Tuning (Higher better) "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "E Compare with Other Opensource LLMs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In Table 3, we compare the harness evaluation results after training the $G_{\\mathrm{stack}}$ model and the scratch model (Baseline) for 100B tokens with Pythia-1B [51] and TinyLlama-1.1B, which are trained on the same number of tokens. The comparative results indicate that our baseline performs normally, comparable to pythia-1B. Meanwhile, the $G_{\\mathrm{stack}}$ model significantly outperforms both the baseline and pythia-1B, demonstrating the acceleration effect of $G_{\\mathrm{stack}}$ on the pre-training process. ", "page_idx": 28}, {"type": "table", "img_path": "FXJDcriMYH/tmp/00242b8030a99b6a1faa32c17f73083ac9620ec239a11a1093b0a87aa9092579.jpg", "table_caption": ["Table 3: Compare with opensource LLMs on 1B "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "F Fitting Results for the Growth Factor $g$ ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Although due to computational resource limitations, we only explore predicting $g$ given $N$ and $C$ on the 1.1B and 3B models, we still attempted to fit using equation: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\log_{10}(g)=a\\log_{10}(N)+{\\frac{b}{\\log_{10}(C)}}+c\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In the equation 49, $N$ represents the number of target parameters, $g$ represents the growth factor. The fitting result is as follows: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\log_{10}(g)=1.01\\log_{10}(N)-{\\frac{29.88}{\\log_{10}(C)}}-7.36\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We also visualize the ftited curves in Figure 22, but the results were mediocre due to the lack of data. ", "page_idx": 28}, {"type": "image", "img_path": "FXJDcriMYH/tmp/c9eab64f95989a4222510775ad2f8faa2dd288795918cc3bd330e493be6be4de.jpg", "img_caption": ["Figure 22: Visualization of the Equation 50. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "F.1 Stacking Law Guidelines For Llama Families ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We give an example of empirical usage of $G_{\\mathrm{stack}}$ by using the configurations of Llama2 and Llama3 families [21, 7] to show the estimated optimal base model training tokens $d$ and growth factor $g$ in Table 4. ", "page_idx": 29}, {"type": "table", "img_path": "FXJDcriMYH/tmp/3fd7870abd4d55fd53b8bb18c84e8c668428afd264a5338457efe957a0ece8f5.jpg", "table_caption": ["Table 4: \u201cStacking Law\u201d Guidelines "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "G Training Loss and Evaluation Results of \u201cgrowth timing\u201d and \u201cgrowth factor\u201d ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "G.1 \u201cGrowth Timing\u201d d ", "page_idx": 29}, {"type": "image", "img_path": "FXJDcriMYH/tmp/0d174ab37f348065ee03b71bfd717af0b108368447414f3418929c01091a43b1.jpg", "img_caption": ["Figure 23: Training loss and standard NLP benchmarks average accuracy of 410M. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "FXJDcriMYH/tmp/2832c6a337404fe75649b10cd83137d9c5a0c594edf73fc15d84d9e8e27064b6.jpg", "img_caption": ["Figure 24: Evaluation results on 410M. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "FXJDcriMYH/tmp/6e591a41df9036258a656dc86aeb4de707af735e3a908414ed4365cf7ec1f559.jpg", "img_caption": ["Figure 25: Training loss and standard NLP benchmarks average accuracy of 1.1B. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "FXJDcriMYH/tmp/f88baba75a1d9c2354393854081168cc1f4024feef40048bd02b0762b65a19a2.jpg", "img_caption": ["Figure 26: Evaluation results on 1.1B. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "FXJDcriMYH/tmp/f132d2b2f2581b5b7341ba37b9e99a3bc74ea45936a0d45bd9d9ccfa9f429589.jpg", "img_caption": ["Figure 27: Training loss and standard NLP benchmarks average accuracy of 3B. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "FXJDcriMYH/tmp/a30d66fd9e4fb6d1eb8a37918d4090b6c19740dacd2eb38e14c362f5e946cde5.jpg", "img_caption": ["Figure 28: Evaluation results on 3B. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "G.2 \u201cGrowth Factor\u201d $g$ ", "page_idx": 32}, {"type": "image", "img_path": "FXJDcriMYH/tmp/f1c39ce9f337cc327c437e40fb21111e37d413d055db590f4994752363fa0021.jpg", "img_caption": ["Figure 29: Training loss and standard NLP benchmarks average accuracy of 1.1B. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "FXJDcriMYH/tmp/61d06b98c48d73d9cb9ecec51d4056072f58217f43e58786315f7d8e3f579de6.jpg", "img_caption": ["Figure 30: Evaluation results on 1.1B. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "FXJDcriMYH/tmp/649801c5ab794f04780383f5db9cca66936dfca29607553028bd4c71abd2282c.jpg", "img_caption": ["Figure 31: Training loss and standard NLP benchmarks average accuracy of 3B. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "FXJDcriMYH/tmp/516f9a9d6e7620681ec9f2563c71c4322ff575a540004b8b2f3370fc41e71b31.jpg", "img_caption": ["Figure 32: Evaluation results on 3B. "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "H Discussion on \u201cHow to stack?\u201d and Evaluation Results ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "H.1 Training Loss and Evaluation Results of Gradual Stack ", "page_idx": 34}, {"type": "image", "img_path": "FXJDcriMYH/tmp/55e5fc5d20b119e952d9e80722383eeb819b636e5496b0bab4c44aa09d37500c.jpg", "img_caption": ["Figure 33: Training loss and standard NLP benchmarks average accuracy of scratch, $G_{\\mathrm{stack}}$ and Ggradual. "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "FXJDcriMYH/tmp/406871493801f5df8c8cfff8d7f2cdbe10c30ae4090c1e8ef6ba4d32100f6652.jpg", "img_caption": ["Figure 34: Evaluation results on scratch, $G_{\\mathrm{stack}}$ and gradual stacking in StackBert. "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "H.2 Ablation: $f_{2}\\circ f_{1}\\circ f_{0}\\circ f_{2}\\circ f_{1}\\circ f_{0}$ or $f_{2}\\circ f_{2}\\circ f_{1}\\circ f_{1}\\circ f_{0}\\circ f_{0}$ (interpolation) ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "To investigate whether the connections between layers affect the performance of stacking, we conduct a comparison of two approaches for stacking small models into larger ones. We explore two approaches for stacking small models into larger ones. The first approach involves taking the entire small model as a unit and directly stacking it, which can retain the connections between most layers. The second approach involves replicating and interleaving each layer in the small model, which almost break the connections. To measure the degree of retention of inter-layer connections after stacking, we define the connection rate $R_{c}$ : ", "page_idx": 35}, {"type": "equation", "text": "$$\nR_{c}=\\frac{C o n_{r}}{C o n_{a l l}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the $C o n_{r}$ is number of retained connections, the $C o n_{a l l}$ is number of all connections. ", "page_idx": 35}, {"type": "text", "text": "For example, if we had a small model with three layers, denoted as $f_{2}\\circ f_{1}\\circ f_{0}$ , and desired a model depth of 6, the first approach would result in $f_{2}\\circ f_{1}\\circ f_{0}\\circ f_{2}\\circ f_{1}\\circ f_{0}$ , where its $R_{c}=80\\%$ . The second approach would result in $f_{2}\\circ f_{2}\\circ f_{1}\\circ f_{1}\\circ f_{0}\\circ f_{0}$ , where its $R_{c}=40\\%$ . ", "page_idx": 35}, {"type": "text", "text": "In our experiments, we stack a small model with 8 layers to a 24 layers target model. The growth timing $d$ is $10B$ tokens and growing factor $s$ is 3. The $R_{c}$ of $G_{\\mathrm{stack}}$ is $91.3\\%$ and the $R_{c}$ of Ginterpolate is $30.4\\%$ . We report the training loss and standard NLP benchmarks average accuracy in Figure 35. At the beginning of training, interpolated stacking perform as well as stacking entire small model. However, as the training continues, the performance of interpolated stacking deteriorates. ", "page_idx": 35}, {"type": "text", "text": "Therefore, we can conclude that the higher the connection rate of stacking, the better the effect of stacking. In Appendix H.3, we continue to validate this conclusion. ", "page_idx": 35}, {"type": "image", "img_path": "FXJDcriMYH/tmp/3c33e0e76dcb5e395ad60315d0a67d9ff1ffc8df3a68e680747f86b2dc4dcc56.jpg", "img_caption": ["Figure 35: Training loss and standard NLP benchmarks average accuracy of scratch, $G_{\\mathrm{stack}}$ and interpolation. "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "We also report the details of evaluation results about 8 standard NLP benchmarks. ", "page_idx": 36}, {"type": "image", "img_path": "FXJDcriMYH/tmp/86c7394aa70c5220e89ade3f1d041f68ea67aec9a13e2ca10562cfd06ae24d02.jpg", "img_caption": ["Figure 36: Evaluation results on scratch, $G_{\\mathrm{stack}}$ and interpolation. "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "H.3 Ablation: Partial Stacking ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Partial stacking has been explored in LLMs like LlamaPro [42], Solar [43]. But their goal is to stack an off-the-shelf LLMs such as Llama2, while our aim is to accelerate LLM pre-training process. ", "page_idx": 36}, {"type": "text", "text": "To explore stacking which layers of the small model can achieve the best performance, we conduct experiments on partial stacking. In our experiments, we stack a small model with 6 layers $(\\{L_{1},L_{2},\\cdot\\cdot\\cdot\\,,L_{6}\\})$ to a 24 layers target model. We set growth timing $d\\,=\\,10B$ tokens and growth factor $g=4$ . For simplicity, we use a format such as $1{-}234^{*}7{-}56$ to denote stacking 234 layers 7 times. ", "page_idx": 36}, {"type": "image", "img_path": "FXJDcriMYH/tmp/c846fc4ca6df69d4396821da9ef39b19d9ae93dc4d9d03074929afb81d08cf86.jpg", "img_caption": ["Figure 37: Training loss and standard NLP benchmarks average accuracy of scratch, $G_{\\mathrm{stack}}$ and other partial stacking. "], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "We report the training loss and standard NLP benchmarks average accuracy in Figure 37. By observing the loss curves in Figure 37a, we can find that the eight partial stacking methods are clearly divided into three groups based on their loss. The first group, $\\{123456^{*}4$ , $12{-}3456^{*}5{-}56$ , $12{-}345^{\\ast}7{-}6$ , $123\\!-\\!456^{*}7\\!\\ ]$ }, achieves the best performance. The second group consisting of $\\{1234.56^{*}10\\}$ , $12{-}34^{*}10\\cdot$ - 56, $1{-}234{\\ast}7{-}56\\}$ , performs just so-so. The third group, $\\{123^{\\ast}7\\mathrm{-}456\\}$ , performs poorly, even worse than the baseline. ", "page_idx": 37}, {"type": "text", "text": "In Table 5, we summarize the eight partial stacking and calculate the $R_{c}$ of each partial stacking methods based on Equation 51. ", "page_idx": 37}, {"type": "text", "text": "For partial stacking, we conclude that: all $>$ middle $\\approx$ back $\\gg$ front. Meanwhile, when the stacked parts are the same, the larger the $R_{c}$ , the better the performance. ", "page_idx": 37}, {"type": "table", "img_path": "FXJDcriMYH/tmp/b5e0da2fdf788caad800a7cf7a86aef7f879212a5af0c96438c3217f94ca705e.jpg", "table_caption": ["Table 5: $R_{c}$ and stacked parts of each partial stacking method "], "table_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "FXJDcriMYH/tmp/7830f986383d776a1e73dcf40f57af51e99250b6ae53d4d740d4bfc9c0d7edd8.jpg", "img_caption": ["Figure 38: Evaluation results on scratch, $G_{\\mathrm{stack}}$ and other partial stacking. "], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "H.4 Compare with Pythia, OLMo and Amber on 7B Size ", "text_level": 1, "page_idx": 38}, {"type": "table", "img_path": "FXJDcriMYH/tmp/97094bf0e2a15426b5aa16b98b07ecea1ebd119a5617a97a1516f6d0b836422a.jpg", "table_caption": ["Table 6: Compare with opensource 7B LLMs on 130B tokens. "], "table_footnote": [], "page_idx": 38}, {"type": "text", "text": "I Details of Function Preserving ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "I.1 Function Preserving ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Function preservation is a key concept that underlies diverse model growth approaches. It entails ensuring consistent output from a model, regardless of its expansion. Mathematically, let us define a function as $F$ and a growth operator as $G$ . The ultimate aim is to apply the operator $G$ to the function $F$ , thereby obtaining the target function denoted as $\\mathcal{F}$ . The core objective here is to maintain the model\u2019s function to generate the same output for a given input. Formally, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\forall x,{\\mathcal{F}}(x)=F(x),{\\mathrm{where~}}{\\mathcal{F}}=G(F)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "I.2 Breaking Function Preserving by Adding Noise ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "For the down projection in SwiGLU and the output projection in MultiHeadAttention, we apply noise: ", "page_idx": 39}, {"type": "equation", "text": "$$\nW_{n o i s e}\\leftarrow(1-\\alpha)W+\\alpha\\epsilon\\quad\\mathrm{where}\\;\\epsilon\\sim\\mathcal N(0,\\frac{1}{d\\times l^{2}})\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "For the Embedding Layer and other Linear Layers, we apply noise: ", "page_idx": 39}, {"type": "equation", "text": "$$\nW_{n o i s e}\\leftarrow(1-\\alpha)W+\\alpha\\epsilon\\quad\\mathrm{where}\\;\\epsilon\\sim\\mathcal{N}(0,\\frac{2}{5d})\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Adding Noise on $G_{\\mathrm{direct}}$ to Break FP ", "page_idx": 39}, {"type": "image", "img_path": "FXJDcriMYH/tmp/05d870d9f9a042607982a95abd2203399ddae90ecfb30fcd604b92d0927af72a.jpg", "img_caption": ["Figure 39: Training loss and standard NLP benchmarks average accuracy of scratch, $G_{\\mathrm{direct}}^{\\rightarrow}$ and $G_{\\mathrm{direct}}^{\\bar{\\rightarrow}}$ with $20\\%$ noise. "], "img_footnote": [], "page_idx": 39}, {"type": "image", "img_path": "FXJDcriMYH/tmp/0043373ee94e267c922d090fed99ab83dbdfcb505f8c4e4489852608d26dcaae.jpg", "img_caption": ["Figure 40: Evaluation results on scratch, $G_{\\mathrm{direct}}^{\\rightarrow}$ and $G_{\\mathrm{direct}}^{\\rightarrow}$ with noise. "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "Training Loss And Evaluation Results on Adding Noise Gd\u2192irect ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Adding Noise on $G_{\\mathrm{stack}}$ Since adding noise actually improve the $G_{\\mathrm{direct}}$ performance, we also add noise on Gstack. ", "page_idx": 40}, {"type": "text", "text": "We stack an 8 layers small model to 24 layers, and then add noise with $\\alpha=0.2$ . We report training loss and standard NLP benchmarks average accuracy in Figure 41. Adding noise demonstrates an advantage in Training loss. ", "page_idx": 40}, {"type": "image", "img_path": "FXJDcriMYH/tmp/c046e3516c6c455bafa7eb755b1c54dd10b0e10c06764760c6c8b4ad564ec943.jpg", "img_caption": ["Figure 41: Training loss and standard NLP benchmarks average accuracy of scratch, $G_{\\mathrm{stack}}$ and $G_{\\mathrm{stack}}$ with $20\\%$ noise. "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "Details of the evaluation results are as follows: ", "page_idx": 40}, {"type": "image", "img_path": "FXJDcriMYH/tmp/607fd83bcf18e431759cff80214023aa2ef0b30b3c7da28f9769c5593896b0f1.jpg", "img_caption": ["Figure 42: Evaluation results on scratch, $G_{\\mathrm{stack}}$ and $G_{\\mathrm{stack}}$ with $20\\%$ noise. "], "img_footnote": [], "page_idx": 41}, {"type": "text", "text": "J Results on Samba ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "We utilize the codebase from Samba9, which implements a hybrid State Space Model using the Slimpajama dataset for LM. In this experiment, we follow the guidelines outlined in the main paper to guide our stacking process. With a parameter size of $410\\mathbb{M}$ and training on 100B tokens, we set the growth timing to 8B and the growth factor to 3. We opted for 3 instead of 4 because Samba is an interleaving of Mamba and self-attention layers. Since the target model has 12 layers, we can only stack even layers, leading us to select a 4-layer base model (Mamba-SA-Mamba-SA). ", "page_idx": 41}, {"type": "text", "text": "Our experiments results on loss curves 43 and downstream tasks 7 indicate stacking also works beyond Transformer-based LLMs. Please note that in Table 7, we select stack with 47B rather than 50B to count the additional consumption required to train the base model on 8B tokens. ", "page_idx": 41}, {"type": "image", "img_path": "FXJDcriMYH/tmp/ebf02e870935fde733bad3322443767f1b2c51d74d91cf1201aaacd5a67e21e3.jpg", "img_caption": ["Figure 43: The training loss for two Samba LLMs, trained from scratch and with $G_{s t a c k}$ . At los $\\mathrel{\\leftr{=}2.48\\right.}$ , 2.45, 2.42, $G_{\\mathrm{stack}}$ accelerates by $61.7\\%$ , $61.5\\%$ and $58.2\\%$ compared to scratch. "], "img_footnote": [], "page_idx": 41}, {"type": "table", "img_path": "FXJDcriMYH/tmp/e811ed3c1c311e36568754cdcc3f20cf5e18ccad7f91f72ea822b62c438b6459.jpg", "table_caption": ["Table 7: Evaluation Results on Samba LLMs "], "table_footnote": [], "page_idx": 42}, {"type": "text", "text": "K Loss Spikes ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Figure 44 illustrates the loss spikes that occur right after stacking. ", "page_idx": 42}, {"type": "image", "img_path": "FXJDcriMYH/tmp/e257e9941b1e34a158b54f4a4cd27c0e49ceb0a1b8a98a114b9742319130dfdd.jpg", "img_caption": ["Figure 44: Loss Spikes in $G_{\\mathrm{stack}}$ (Non-FP) and $G_{r a n d o m}^{\\uparrow}$ random (FP) "], "img_footnote": [], "page_idx": 42}, {"type": "text", "text": "L Societal Impacts ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "As a successful exploration for efficient LLM pre-training, our work has great potential to give positive societal impact towards sustainable AI. Nevertheless, as a common drawback for LLMs, there are also chances that our LLMs might be misused intentionally or uniintentionally. ", "page_idx": 42}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: In the Abstract, we clearly elucidate our contributions, and at the end of Section 1 Introduction, we further detail our contributions and scope. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 43}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Justification: In Section 7, we discuss the limitations of our work. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 43}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: Our study is empirical exploration. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 44}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We report our detailed training settings in Appendix B.3. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 44}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We use open-source dataset Slimpajama-627B for pre-training, we report this in Appendix B.3. We have submitted our code on OpenReview and will open-source it on GitHub in the final version. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 45}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We report the detailed settings in Appendix B.3 ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 45}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 45}, {"type": "text", "text": "Answer: [No] ", "page_idx": 45}, {"type": "text", "text": "Justification: LLMs pre-training consumes a significant amount of computational resources, making it impractical to conduct multiple experiments to obtain error bars. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 45}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 46}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We report the needed Compute Resources in Appendix B.3 and required FLOPs of each experiments in each Figures. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 46}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We have read this code. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 46}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We have a section in the Appendix L to discuss societal impacts. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 46}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 47}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [No] ", "page_idx": 47}, {"type": "text", "text": "Justification: Our study is an empirical exploration. The dataset we use is a open-source high-quality corpus, and the models we release are intended solely for further research and are not meant for direct industrial application. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 47}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: Please refer to Appendix B.3. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 47}, {"type": "text", "text": "", "page_idx": 48}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: All codes and models are will be full released under the license of CC-BY 4.0. Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 48}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 48}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 48}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 49}]