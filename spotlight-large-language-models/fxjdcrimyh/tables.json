[{"figure_path": "FXJDcriMYH/tables/tables_4_1.jpg", "caption": "Figure 3: We evaluate operators using training loss and Lambada [30], ARC-c [31], ARC-e [31], Logiqa [32], PIQA [33], Sciq [34], Winogrande [35] and Wikitext PPL [36] totaling eight standard NLP benchmarks. After 8 \u00d7 10<sup>20</sup> FLOPs of training, G<sub>direct</sub> demonstrates a significant speedup.", "description": "This table presents the results of an experiment evaluating four growth operators in both depthwise and widthwise directions on a 1.1B Llama-like LLM pre-training setting.  The table compares the training loss, and performance on eight standard NLP benchmarks (Lambada, ARC-C, ARC-e, Logiqa, PIQA, Sciq, Winogrande, and Wikitext) for each growth operator and a baseline trained from scratch.  The G<sub>direct</sub> operator, in particular, demonstrates significant speed improvements.", "section": "3 Systematically Assessing Model Growth for LLM Pre-Training"}, {"figure_path": "FXJDcriMYH/tables/tables_21_1.jpg", "caption": "Table 2: Evaluation Results after Instruction-Tuning (Higher better)", "description": "This table presents the results of instruction tuning on a 3B parameter model, comparing two methods: training from scratch with 400B tokens and using Gstack with 290B tokens.  It evaluates the performance on several NLP benchmarks, including Lambada, ARC-c, ARC-e, Logiqa, PIQA, Sciq, Winogrande, and an average across these benchmarks.  Higher scores indicate better performance.  The results show that even with significantly fewer training tokens, the Gstack approach achieves comparable or better performance across various benchmarks.", "section": "D.4 Instruction Tuning Results on 3B"}, {"figure_path": "FXJDcriMYH/tables/tables_21_2.jpg", "caption": "Table 1: Hyperparameters", "description": "This table shows the hyperparameters used for training different sized LLMs (410M, 1.1B, 3B, and 7B).  The hyperparameters include the context length, batch size, maximum learning rate (max-LR), minimum learning rate (min-LR), warmup steps, and learning rate scheduler used in the training process.  All models used a context length of 2048 and a batch size of 2M tokens. The learning rate scheduler used was cosine annealing for all models.", "section": "B.3 Details of Training Settings"}, {"figure_path": "FXJDcriMYH/tables/tables_27_1.jpg", "caption": "Table 2: Evaluation Results after Instruction-Tuning (Higher better)", "description": "This table presents the results of instruction tuning experiments on a 3B parameter Language Model (LLM).  It compares the performance of a model trained from scratch using 400B tokens with a model trained using the Gstack method with 290B tokens. The comparison is done with and without instruction tuning, across various NLP benchmarks including lambada, arc-c, arc-e, logiqa, piqa, sciq, winogrande and the average score of all benchmarks.  Higher scores indicate better performance.", "section": "D.4 Instruction Tuning Results on 3B"}, {"figure_path": "FXJDcriMYH/tables/tables_28_1.jpg", "caption": "Table 3: Compare with opensource LLMs on 1B", "description": "This table compares the performance of the Gstack-1.1B model with other open-source LLMs (Pythia-1B and TinyLlama-1.1B) and a baseline model trained from scratch. All models were trained on 100B tokens. The table shows the average accuracy scores achieved on eight standard NLP benchmarks. The results show that the Gstack model significantly outperforms the other models.", "section": "E Compare with Other Opensource LLMS"}, {"figure_path": "FXJDcriMYH/tables/tables_29_1.jpg", "caption": "Table 4: \"Stacking Law\" Guidelines", "description": "This table presents guidelines for using the Gstack model growth technique on different Llama models.  It shows the recommended base model training token amount (d) and growth factor (g) for achieving optimal results. The growth factor remains constant at 4 across all models, while the optimal d increases with model size, reflecting the larger training data requirements for larger models.", "section": "4.2 Determining Growth Timing and Growth Factor for Using Gstack"}, {"figure_path": "FXJDcriMYH/tables/tables_37_1.jpg", "caption": "Table 5: Re and stacked parts of each partial stacking method", "description": "This table presents the results of an ablation study exploring different partial stacking strategies.  It categorizes eight partial stacking methods into three groups based on their performance, indicating how much of the model's inter-layer connections are retained (Rc).  The groups highlight the impact of stacking on different parts of the model (all layers, middle-back layers, middle layers, back layers, front-middle layers, and front layers), showing a correlation between the connection retention rate (Rc) and performance.", "section": "H.2 Ablation: f2 \u00b0 f1 \u00b0 fo \u00b0 f2 \u00b0 f1 \u00b0 fo or f2 \u00b0 f2 \u00b0 f1 \u00b0 f1 \u00b0 fo fo (interpolation)"}, {"figure_path": "FXJDcriMYH/tables/tables_38_1.jpg", "caption": "Table 6: Compare with opensource 7B LLMs on 130B tokens.", "description": "This table compares the performance of four different 7B parameter language models on eight standard NLP benchmarks using 130B tokens.  The models compared are Pythia-6.9B, OLMo-7B, Amber-7B, and the Gstack-7B model introduced in this paper.  The table shows the average accuracy scores for each model on each benchmark, allowing for a direct comparison of their performance.  The Wikitext column presents perplexity scores, where lower values indicate better performance.", "section": "H.4 Compare with Pythia, OLMo and Amber on 7B Size"}, {"figure_path": "FXJDcriMYH/tables/tables_42_1.jpg", "caption": "Table 7: Evaluation Results on Samba LLMs", "description": "This table presents the evaluation results obtained using the Samba LLMs.  It compares the performance of a model trained from scratch with 50B tokens against a model trained using the Gstack method with 47B tokens.  The results are shown for various NLP benchmarks, including Lambada, ARC-c, ARC-e, Logiqa, PIQA, Sciq, and an average across all benchmarks. The table highlights the improved performance achieved using the Gstack method.", "section": "D.4 Instruction Tuning Results on 3B"}]