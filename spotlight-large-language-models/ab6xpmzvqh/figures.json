[{"figure_path": "AB6XpMzvqH/figures/figures_1_1.jpg", "caption": "Figure 1: Many-shot vs Few-Shot In-Context Learning (ICL) across several tasks. Many-shot ICL consistently outperforms few-shot ICL, particularly on difficult non-natural language tasks. Optimal number of shots for many-shot ICL are shown inside the bar for each task. For few-shot ICL, we either use typical number of shots used on a benchmark, for example, 4-shot for MATH, or the longest prompt among the ones we tested with less than the GPT-3 context length of 2048 tokens. Reasoning-oriented tasks, namely MATH, GSM8K, BBH, and GPQA use chain-of-thought rationales. For translation, we report performance on English to Bemba, summarization uses XLSum, MATH corresponds to the MATH500 test set, and sentiment analysis results are reported with semantically-unrelated labels. See \u00a72, \u00a73, and \u00a74 for more details.", "description": "This figure compares the performance of few-shot and many-shot in-context learning (ICL) across various tasks.  It demonstrates that many-shot ICL consistently outperforms few-shot ICL, especially on complex tasks not involving natural language. The optimal number of shots for each task in the many-shot regime is indicated.  The figure also notes the types of prompts (e.g., chain-of-thought) used for different tasks and benchmarks.", "section": "Scaling In-Context Learning"}, {"figure_path": "AB6XpMzvqH/figures/figures_2_1.jpg", "caption": "Figure 1: Many-shot vs Few-Shot In-Context Learning (ICL) across several tasks. Many-shot ICL consistently outperforms few-shot ICL, particularly on difficult non-natural language tasks. Optimal number of shots for many-shot ICL are shown inside the bar for each task. For few-shot ICL, we either use typical number of shots used on a benchmark, for example, 4-shot for MATH, or the longest prompt among the ones we tested with less than the GPT-3 context length of 2048 tokens. Reasoning-oriented tasks, namely MATH, GSM8K, BBH, and GPQA use chain-of-thought rationales. For translation, we report performance on English to Bemba, summarization uses XLSum, MATH corresponds to the MATH500 test set, and sentiment analysis results are reported with semantically-unrelated labels. See \u00a72, \u00a73, and \u00a74 for more details.", "description": "This figure compares the performance of few-shot and many-shot in-context learning (ICL) across various tasks.  The bar chart shows that many-shot ICL significantly outperforms few-shot ICL, especially on complex tasks that are not based on natural language. Optimal numbers of shots for each task in the many-shot setting are indicated on the bars.  The figure also notes the methodology for determining the number of shots used in the few-shot ICL experiments and specifies the types of rationales used for different tasks.", "section": "Scaling In-Context Learning"}, {"figure_path": "AB6XpMzvqH/figures/figures_3_1.jpg", "caption": "Figure 1: Many-shot vs Few-Shot In-Context Learning (ICL) across several tasks. Many-shot ICL consistently outperforms few-shot ICL, particularly on difficult non-natural language tasks. Optimal number of shots for many-shot ICL are shown inside the bar for each task. For few-shot ICL, we either use typical number of shots used on a benchmark, for example, 4-shot for MATH, or the longest prompt among the ones we tested with less than the GPT-3 context length of 2048 tokens. Reasoning-oriented tasks, namely MATH, GSM8K, BBH, and GPQA use chain-of-thought rationales. For translation, we report performance on English to Bemba, summarization uses XLSum, MATH corresponds to the MATH500 test set, and sentiment analysis results are reported with semantically-unrelated labels. See \u00a72, \u00a73, and \u00a74 for more details.", "description": "This figure compares the performance of few-shot and many-shot in-context learning (ICL) across various tasks.  It demonstrates that many-shot ICL consistently outperforms few-shot ICL, especially on complex, non-natural language tasks.  The optimal number of shots for each task in the many-shot setting is indicated.  The figure also highlights the use of chain-of-thought rationales in some tasks and points to additional sections of the paper for more details.", "section": "Scaling In-Context Learning"}, {"figure_path": "AB6XpMzvqH/figures/figures_4_1.jpg", "caption": "Figure 5: Many-shot Reinforced and Unsupervised ICL for problem-solving generally outperform ICL with ground-truth MATH solutions. MATH. (Left) The bar plots depict the average performance across five random seeds on the MATH500 test set. Each random seed (denoted by the dots) corresponds to a different subset of problems along with ground truth or model-generated solutions (if any) in the prompt. Transfer to GSM8K. (Right) We see that the prompt obtained from MATH transfers well to the GSM8K test split containing 500 problems. Our results with many-shot ICL outperform the 4-shot Minerva prompt, which obtains a test accuracy of 55.7% on MATH500 and 90.6% on GSM8K.", "description": "This figure compares the performance of three different in-context learning (ICL) methods on two mathematical problem-solving datasets: MATH500 and GSM8K.  The left panel shows results for MATH500, while the right shows transfer performance to GSM8K using prompts from MATH.  The three methods are: ICL with ground-truth solutions, unsupervised ICL (using only problem statements), and reinforced ICL (using model-generated rationales). The figure demonstrates that reinforced and unsupervised ICL, especially with many shots, significantly outperform standard ICL on both datasets.  The results highlight the effectiveness of model-generated rationales in many-shot ICL and its ability to generalize across datasets.", "section": "3 Many-shot Learning without Human-Written Rationales"}, {"figure_path": "AB6XpMzvqH/figures/figures_5_1.jpg", "caption": "Figure 6: Many-shot Reinforced and Unsupervised ICL for GPQA. The baseline zero-shot prompt, which is used for generating rationales for Reinforced ICL and appended to the prompt for Unsupervised ICL, obtains a performance of 38.8%. The average test accuracy with 125-shot prompt with both ground-truth or model-generated rationales surpass the 40.4% obtained by Claude-3 Sonnet. As we vary the number of shots, while Unsupervised ICL matches or outperforms the zero-shot prompt, Reinforced ICL consistently outperforms it.", "description": "This figure shows the performance comparison of three different methods for Google-Proof QA (GPQA) task across different numbers of shots (in-context examples). The three methods are: 1) ICL (Ground-Truth) using human-written rationales, 2) Unsupervised ICL prompting the model only with questions, and 3) Reinforced ICL using model-generated rationales. The results indicate that Reinforced ICL consistently outperforms both Unsupervised ICL and the baseline zero-shot prompt, especially with 125 shots, almost matching the state-of-the-art performance of Claude-3 Sonnet. Unsupervised ICL shows mixed results, sometimes matching or surpassing the zero-shot baseline but generally underperforming Reinforced ICL.", "section": "3.2 Question Answering: Google-Proof QA (GPQA)"}, {"figure_path": "AB6XpMzvqH/figures/figures_5_2.jpg", "caption": "Figure 7: BIG-Bench Hard. Reinforced and Unsupervised ICL with varying number of shots, averaged across five random seeds. We evaluate test performance on a held-out set of 100 problems. The error bars denote standard deviation. Reinforced ICL outperforms Unsupervised ICL for all tasks, which in turns outperforms the human-written chain-of-thought (CoT) prompt. Averaged across tasks, CoT prompting using human-written rationales gets a success rate of 72.1%, Unsupervised ICL obtains 77.1%, while Reinforced ICL gets 83%.", "description": "This figure shows the performance comparison of different in-context learning methods on the BIG-Bench Hard benchmark.  Reinforced ICL consistently outperforms Unsupervised ICL and the baseline human-written chain-of-thought prompting across eight algorithmic and symbolic reasoning tasks. The results demonstrate the effectiveness of model-generated rationales in improving many-shot in-context learning performance.", "section": "3.3 Algorithmic and Symbolic Reasoning: Big-Bench Hard"}, {"figure_path": "AB6XpMzvqH/figures/figures_6_1.jpg", "caption": "Figure 8: Overcoming Pre-Training Bias with Many-Shot ICL. (Left) Many-shot ICL overcomes label flips: Test accuracy for sentiment analysis typically improves with more training shots. Flipped and abstract labels eventually approaching the performance of default labels. (Right) Confidence shift in overcoming bias. For flipped and abstract labels, model confidence in its predicted sentiment labels initially drops, then sharply increases with more training shots to similar value, suggesting a period of overcoming pre-training bias.", "description": "This figure shows the results of an experiment designed to evaluate the ability of many-shot in-context learning (ICL) to overcome pre-training biases.  Two sets of experiments were performed on the Financial PhraseBank (FP) sentiment analysis dataset using three types of labels: default labels, flipped labels (where the order of sentiment categories is reversed), and abstract labels (where non-descriptive labels 'A', 'B', 'C' are used). The left panel displays the test accuracy for each label type as a function of the number of shots (in-context examples).  The right panel shows the change in confidence (label probability) for the same label types, demonstrating the learning process of overcoming the bias.  The results indicate that many-shot ICL successfully overcomes pre-training biases as the number of shots increases, demonstrating a significant improvement in accuracy compared to a few-shot setting and eventually achieving similar performance to the default label setting.", "section": "4.1 Overcoming Pre-training Biases with Many-Shot ICL"}, {"figure_path": "AB6XpMzvqH/figures/figures_7_1.jpg", "caption": "Figure 1: Many-shot vs Few-Shot In-Context Learning (ICL) across several tasks. Many-shot ICL consistently outperforms few-shot ICL, particularly on difficult non-natural language tasks. Optimal number of shots for many-shot ICL are shown inside the bar for each task. For few-shot ICL, we either use typical number of shots used on a benchmark, for example, 4-shot for MATH, or the longest prompt among the ones we tested with less than the GPT-3 context length of 2048 tokens. Reasoning-oriented tasks, namely MATH, GSM8K, BBH, and GPQA use chain-of-thought rationales. For translation, we report performance on English to Bemba, summarization uses XLSum, MATH corresponds to the MATH500 test set, and sentiment analysis results are reported with semantically-unrelated labels. See \u00a72, \u00a73, and \u00a74 for more details.", "description": "This figure compares the performance of few-shot and many-shot in-context learning (ICL) across various tasks.  It shows that many-shot ICL significantly outperforms few-shot ICL, especially on complex tasks not involving natural language.  The optimal number of shots for each task in the many-shot setting is indicated.  The figure also notes the type of prompts and datasets used for each task.", "section": "Scaling In-Context Learning"}, {"figure_path": "AB6XpMzvqH/figures/figures_7_2.jpg", "caption": "Figure 1: Many-shot vs Few-Shot In-Context Learning (ICL) across several tasks. Many-shot ICL consistently outperforms few-shot ICL, particularly on difficult non-natural language tasks. Optimal number of shots for many-shot ICL are shown inside the bar for each task. For few-shot ICL, we either use typical number of shots used on a benchmark, for example, 4-shot for MATH, or the longest prompt among the ones we tested with less than the GPT-3 context length of 2048 tokens. Reasoning-oriented tasks, namely MATH, GSM8K, BBH, and GPQA use chain-of-thought rationales. For translation, we report performance on English to Bemba, summarization uses XLSum, MATH corresponds to the MATH500 test set, and sentiment analysis results are reported with semantically-unrelated labels. See \u00a72, \u00a73, and \u00a74 for more details.", "description": "The figure compares the performance of few-shot and many-shot in-context learning across various tasks. It shows that many-shot ICL significantly improves performance compared to few-shot ICL, especially on challenging non-language tasks. The optimal number of shots varies across tasks.  Reasoning tasks utilize chain-of-thought prompting.", "section": "Scaling In-Context Learning"}, {"figure_path": "AB6XpMzvqH/figures/figures_7_3.jpg", "caption": "Figure 1: Many-shot vs Few-Shot In-Context Learning (ICL) across several tasks. Many-shot ICL consistently outperforms few-shot ICL, particularly on difficult non-natural language tasks. Optimal number of shots for many-shot ICL are shown inside the bar for each task. For few-shot ICL, we either use typical number of shots used on a benchmark, for example, 4-shot for MATH, or the longest prompt among the ones we tested with less than the GPT-3 context length of 2048 tokens. Reasoning-oriented tasks, namely MATH, GSM8K, BBH, and GPQA use chain-of-thought rationales. For translation, we report performance on English to Bemba, summarization uses XLSum, MATH corresponds to the MATH500 test set, and sentiment analysis results are reported with semantically-unrelated labels. See \u00a72, \u00a73, and \u00a74 for more details.", "description": "This figure compares the performance of few-shot and many-shot in-context learning (ICL) across various tasks.  It shows that many-shot ICL consistently outperforms few-shot ICL, especially on complex tasks not involving natural language.  The optimal number of shots for each task in the many-shot setting is indicated.  Different task types (problem-solving, summarization, translation, etc.) are included, and details about prompting techniques used (like chain-of-thought rationales) are noted.", "section": "Scaling In-Context Learning"}, {"figure_path": "AB6XpMzvqH/figures/figures_8_1.jpg", "caption": "Figure 1: Many-shot vs Few-Shot In-Context Learning (ICL) across several tasks. Many-shot ICL consistently outperforms few-shot ICL, particularly on difficult non-natural language tasks. Optimal number of shots for many-shot ICL are shown inside the bar for each task. For few-shot ICL, we either use typical number of shots used on a benchmark, for example, 4-shot for MATH, or the longest prompt among the ones we tested with less than the GPT-3 context length of 2048 tokens. Reasoning-oriented tasks, namely MATH, GSM8K, BBH, and GPQA use chain-of-thought rationales. For translation, we report performance on English to Bemba, summarization uses XLSum, MATH corresponds to the MATH500 test set, and sentiment analysis results are reported with semantically-unrelated labels. See \u00a72, \u00a73, and \u00a74 for more details.", "description": "This figure compares the performance of few-shot and many-shot in-context learning (ICL) across various tasks, demonstrating the significant performance gains achieved with many-shot ICL, especially on complex, non-natural language tasks.  It highlights the optimal number of shots for each task in the many-shot setting and notes the methodology used for few-shot ICL. Specific details on the types of prompts and datasets used for each task are provided.", "section": "Scaling In-Context Learning"}, {"figure_path": "AB6XpMzvqH/figures/figures_8_2.jpg", "caption": "Figure 1: Many-shot vs Few-Shot In-Context Learning (ICL) across several tasks. Many-shot ICL consistently outperforms few-shot ICL, particularly on difficult non-natural language tasks. Optimal number of shots for many-shot ICL are shown inside the bar for each task. For few-shot ICL, we either use typical number of shots used on a benchmark, for example, 4-shot for MATH, or the longest prompt among the ones we tested with less than the GPT-3 context length of 2048 tokens. Reasoning-oriented tasks, namely MATH, GSM8K, BBH, and GPQA use chain-of-thought rationales. For translation, we report performance on English to Bemba, summarization uses XLSum, MATH corresponds to the MATH500 test set, and sentiment analysis results are reported with semantically-unrelated labels. See \u00a72, \u00a73, and \u00a74 for more details.", "description": "This figure compares the performance of few-shot and many-shot in-context learning (ICL) across various tasks.  The bar chart shows that many-shot ICL consistently outperforms few-shot ICL, especially on complex tasks not involving natural language. The optimal number of shots for each task in the many-shot setting is indicated within each bar.  The few-shot results use either standard benchmarks' shot numbers (e.g., 4 shots for MATH) or the maximum number of shots possible while remaining within GPT-3's context window. Tasks requiring reasoning (MATH, GSM8K, BBH, GPQA) utilize chain-of-thought rationales.  Translation results are specifically for English to Bemba, summarization uses XLSum, MATH results show the MATH500 test set, and sentiment analysis utilizes semantically unrelated labels.", "section": "Scaling In-Context Learning"}, {"figure_path": "AB6XpMzvqH/figures/figures_9_1.jpg", "caption": "Figure 1: Many-shot vs Few-Shot In-Context Learning (ICL) across several tasks. Many-shot ICL consistently outperforms few-shot ICL, particularly on difficult non-natural language tasks. Optimal number of shots for many-shot ICL are shown inside the bar for each task. For few-shot ICL, we either use typical number of shots used on a benchmark, for example, 4-shot for MATH, or the longest prompt among the ones we tested with less than the GPT-3 context length of 2048 tokens. Reasoning-oriented tasks, namely MATH, GSM8K, BBH, and GPQA use chain-of-thought rationales. For translation, we report performance on English to Bemba, summarization uses XLSum, MATH corresponds to the MATH500 test set, and sentiment analysis results are reported with semantically-unrelated labels. See \u00a72, \u00a73, and \u00a74 for more details.", "description": "This figure compares the performance of few-shot and many-shot in-context learning (ICL) across various tasks.  It shows that many-shot ICL significantly outperforms few-shot ICL, especially on complex tasks that are not based on natural language.  The optimal number of shots is indicated for each task in the many-shot setting.  Different types of prompts and datasets are used for different tasks.", "section": "Scaling In-Context Learning"}, {"figure_path": "AB6XpMzvqH/figures/figures_15_1.jpg", "caption": "Figure 1: Many-shot vs Few-Shot In-Context Learning (ICL) across several tasks. Many-shot ICL consistently outperforms few-shot ICL, particularly on difficult non-natural language tasks. Optimal number of shots for many-shot ICL are shown inside the bar for each task. For few-shot ICL, we either use typical number of shots used on a benchmark, for example, 4-shot for MATH, or the longest prompt among the ones we tested with less than the GPT-3 context length of 2048 tokens. Reasoning-oriented tasks, namely MATH, GSM8K, BBH, and GPQA use chain-of-thought rationales. For translation, we report performance on English to Bemba, summarization uses XLSum, MATH corresponds to the MATH500 test set, and sentiment analysis results are reported with semantically-unrelated labels. See \u00a72, \u00a73, and \u00a74 for more details.", "description": "This figure compares the performance of few-shot and many-shot in-context learning (ICL) across various tasks.  It demonstrates that many-shot ICL consistently achieves higher accuracy than few-shot ICL, especially for tasks involving complex reasoning.  The optimal number of shots for each task is also shown, ranging from just a few to several hundreds, suggesting the substantial increase in context window size significantly affects the performance.  The figure highlights the benefits of using many-shot ICL by showcasing substantial performance gains across different tasks such as problem solving, question answering, machine translation and summarization.", "section": "Scaling In-Context Learning"}, {"figure_path": "AB6XpMzvqH/figures/figures_15_2.jpg", "caption": "Figure A.2: Many-shot performance with distinct examples vs repeating the same 25 examples N times on low-resource MT. Bars show avg. perf with std across 3 seeds. Most of the benefit of many-shot ICL stems from adding new information as opposed to increasing context length.", "description": "This figure compares the performance of many-shot in-context learning (ICL) using two different approaches: using distinct examples and repeating a small set of examples multiple times.  The results show that using distinct examples yields significantly better performance in low-resource machine translation than simply increasing the context length by repeating the same examples. This indicates that the primary benefit of many-shot ICL comes from adding new information rather than simply increasing the context window size.", "section": "A.2 Where Do Gains in Many-Shot ICL Stem From?"}, {"figure_path": "AB6XpMzvqH/figures/figures_18_1.jpg", "caption": "Figure 1: Many-shot vs Few-Shot In-Context Learning (ICL) across several tasks. Many-shot ICL consistently outperforms few-shot ICL, particularly on difficult non-natural language tasks. Optimal number of shots for many-shot ICL are shown inside the bar for each task. For few-shot ICL, we either use typical number of shots used on a benchmark, for example, 4-shot for MATH, or the longest prompt among the ones we tested with less than the GPT-3 context length of 2048 tokens. Reasoning-oriented tasks, namely MATH, GSM8K, BBH, and GPQA use chain-of-thought rationales. For translation, we report performance on English to Bemba, summarization uses XLSum, MATH corresponds to the MATH500 test set, and sentiment analysis results are reported with semantically-unrelated labels. See \u00a72, \u00a73, and \u00a74 for more details.", "description": "This figure compares the performance of few-shot and many-shot in-context learning (ICL) across various tasks. It demonstrates that many-shot ICL consistently outperforms few-shot ICL, especially on complex tasks that are not based on natural language.  The optimal number of shots is indicated for each task in the many-shot setting.  Few-shot ICL results are based on either the standard number of shots used in the task's benchmark or the maximum prompt length tested (less than the 2048-token limit of GPT-3). Tasks involving reasoning use chain-of-thought rationales.  Specific details for each task's experimental setup are provided in the caption.", "section": "Scaling In-Context Learning"}, {"figure_path": "AB6XpMzvqH/figures/figures_19_1.jpg", "caption": "Figure 1: Many-shot vs Few-Shot In-Context Learning (ICL) across several tasks. Many-shot ICL consistently outperforms few-shot ICL, particularly on difficult non-natural language tasks. Optimal number of shots for many-shot ICL are shown inside the bar for each task. For few-shot ICL, we either use typical number of shots used on a benchmark, for example, 4-shot for MATH, or the longest prompt among the ones we tested with less than the GPT-3 context length of 2048 tokens. Reasoning-oriented tasks, namely MATH, GSM8K, BBH, and GPQA use chain-of-thought rationales. For translation, we report performance on English to Bemba, summarization uses XLSum, MATH corresponds to the MATH500 test set, and sentiment analysis results are reported with semantically-unrelated labels. See \u00a72, \u00a73, and \u00a74 for more details.", "description": "This figure compares the performance of few-shot and many-shot in-context learning (ICL) across various tasks.  It demonstrates that many-shot ICL significantly improves performance compared to few-shot ICL, especially on complex tasks involving reasoning,  indicating the benefits of scaling ICL to many examples. The chart displays percentage improvements, optimal shot numbers for many-shot ICL, and the type of prompts and datasets used for each specific task. ", "section": "Scaling ICL"}, {"figure_path": "AB6XpMzvqH/figures/figures_19_2.jpg", "caption": "Figure A.5: Many-Shot Sensitivity To Example Ordering. Each colored data point represents a different random ordering of 50 in-context examples provided to Gemini 1.5 Pro.", "description": "This figure shows the sensitivity of many-shot in-context learning (ICL) to the order of examples in the prompt. Ten different random orderings of 50 in-context examples from the MATH training split were used to evaluate performance on the MATH500 test set.  The results demonstrate that performance varies significantly across different problem subcategories within MATH500. An ordering that performs well in one subcategory might perform poorly in another. However, the average performance across all subcategories shows less variation than individual subcategories.", "section": "A.6 Is Many-Shot ICL Sensitive to Example Ordering?"}, {"figure_path": "AB6XpMzvqH/figures/figures_20_1.jpg", "caption": "Figure 1: Many-shot vs Few-Shot In-Context Learning (ICL) across several tasks. Many-shot ICL consistently outperforms few-shot ICL, particularly on difficult non-natural language tasks. Optimal number of shots for many-shot ICL are shown inside the bar for each task. For few-shot ICL, we either use typical number of shots used on a benchmark, for example, 4-shot for MATH, or the longest prompt among the ones we tested with less than the GPT-3 context length of 2048 tokens. Reasoning-oriented tasks, namely MATH, GSM8K, BBH, and GPQA use chain-of-thought rationales. For translation, we report performance on English to Bemba, summarization uses XLSum, MATH corresponds to the MATH500 test set, and sentiment analysis results are reported with semantically-unrelated labels. See \u00a72, \u00a73, and \u00a74 for more details.", "description": "This figure compares the performance of few-shot and many-shot in-context learning (ICL) across various tasks.  It shows that many-shot ICL significantly outperforms few-shot ICL, especially on complex tasks that are not based on natural language.  The optimal number of shots for each task in the many-shot setting is indicated. Different types of tasks are included, and the prompts used are described.", "section": "Scaling ICL"}, {"figure_path": "AB6XpMzvqH/figures/figures_21_1.jpg", "caption": "Figure 1: Many-shot vs Few-Shot In-Context Learning (ICL) across several tasks. Many-shot ICL consistently outperforms few-shot ICL, particularly on difficult non-natural language tasks. Optimal number of shots for many-shot ICL are shown inside the bar for each task. For few-shot ICL, we either use typical number of shots used on a benchmark, for example, 4-shot for MATH, or the longest prompt among the ones we tested with less than the GPT-3 context length of 2048 tokens. Reasoning-oriented tasks, namely MATH, GSM8K, BBH, and GPQA use chain-of-thought rationales. For translation, we report performance on English to Bemba, summarization uses XLSum, MATH corresponds to the MATH500 test set, and sentiment analysis results are reported with semantically-unrelated labels. See \u00a72, \u00a73, and \u00a74 for more details.", "description": "This figure compares the performance of few-shot and many-shot in-context learning (ICL) across various tasks.  It demonstrates that many-shot ICL consistently outperforms few-shot ICL, especially on complex, non-natural language tasks. The optimal number of shots for each task in the many-shot setting is indicated within each bar. The figure also notes the type of prompt used for each task and the specific datasets involved, offering context for interpreting the results.  The significant performance improvements seen with many-shot ICL highlight its potential for enhancing LLM capabilities.", "section": "Scaling In-Context Learning"}, {"figure_path": "AB6XpMzvqH/figures/figures_22_1.jpg", "caption": "Figure 1: Many-shot vs Few-Shot In-Context Learning (ICL) across several tasks. Many-shot ICL consistently outperforms few-shot ICL, particularly on difficult non-natural language tasks. Optimal number of shots for many-shot ICL are shown inside the bar for each task. For few-shot ICL, we either use typical number of shots used on a benchmark, for example, 4-shot for MATH, or the longest prompt among the ones we tested with less than the GPT-3 context length of 2048 tokens. Reasoning-oriented tasks, namely MATH, GSM8K, BBH, and GPQA use chain-of-thought rationales. For translation, we report performance on English to Bemba, summarization uses XLSum, MATH corresponds to the MATH500 test set, and sentiment analysis results are reported with semantically-unrelated labels. See \u00a72, \u00a73, and \u00a74 for more details.", "description": "This figure compares the performance of few-shot and many-shot in-context learning (ICL) across various tasks.  It demonstrates that many-shot ICL consistently outperforms few-shot ICL, especially on complex tasks that don't involve natural language. The figure provides a bar chart showing the percentage improvement achieved by many-shot ICL for each task, along with the optimal number of shots used.  The tasks include summarization, planning, problem solving, question answering, translation, and sentiment analysis.", "section": "Scaling In-Context Learning"}, {"figure_path": "AB6XpMzvqH/figures/figures_23_1.jpg", "caption": "Figure 1: Many-shot vs Few-Shot In-Context Learning (ICL) across several tasks. Many-shot ICL consistently outperforms few-shot ICL, particularly on difficult non-natural language tasks. Optimal number of shots for many-shot ICL are shown inside the bar for each task. For few-shot ICL, we either use typical number of shots used on a benchmark, for example, 4-shot for MATH, or the longest prompt among the ones we tested with less than the GPT-3 context length of 2048 tokens. Reasoning-oriented tasks, namely MATH, GSM8K, BBH, and GPQA use chain-of-thought rationales. For translation, we report performance on English to Bemba, summarization uses XLSum, MATH corresponds to the MATH500 test set, and sentiment analysis results are reported with semantically-unrelated labels. See \u00a72, \u00a73, and \u00a74 for more details.", "description": "This figure compares the performance of few-shot and many-shot in-context learning across various tasks. It shows that many-shot learning consistently outperforms few-shot learning, especially on complex tasks that are not based on natural language.  The optimal number of shots for each task is also displayed. Note that different tasks may use different numbers of shots for few-shot learning.", "section": "Scaling In-Context Learning"}, {"figure_path": "AB6XpMzvqH/figures/figures_24_1.jpg", "caption": "Figure 1: Many-shot vs Few-Shot In-Context Learning (ICL) across several tasks. Many-shot ICL consistently outperforms few-shot ICL, particularly on difficult non-natural language tasks. Optimal number of shots for many-shot ICL are shown inside the bar for each task. For few-shot ICL, we either use typical number of shots used on a benchmark, for example, 4-shot for MATH, or the longest prompt among the ones we tested with less than the GPT-3 context length of 2048 tokens. Reasoning-oriented tasks, namely MATH, GSM8K, BBH, and GPQA use chain-of-thought rationales. For translation, we report performance on English to Bemba, summarization uses XLSum, MATH corresponds to the MATH500 test set, and sentiment analysis results are reported with semantically-unrelated labels. See \u00a72, \u00a73, and \u00a74 for more details.", "description": "This figure compares the performance of few-shot and many-shot in-context learning (ICL) across various tasks.  It shows that many-shot ICL consistently outperforms few-shot ICL, especially on complex tasks that aren't based on natural language. The optimal number of shots for each task in the many-shot setting is also provided. Different tasks (summarization, translation, problem solving, etc.) used different numbers of shots, and reasoning-based tasks used chain-of-thought prompting. The results demonstrate that scaling up the number of shots significantly improves ICL performance.", "section": "Scaling In-Context Learning"}, {"figure_path": "AB6XpMzvqH/figures/figures_25_1.jpg", "caption": "Figure 1: Many-shot vs Few-Shot In-Context Learning (ICL) across several tasks. Many-shot ICL consistently outperforms few-shot ICL, particularly on difficult non-natural language tasks. Optimal number of shots for many-shot ICL are shown inside the bar for each task. For few-shot ICL, we either use typical number of shots used on a benchmark, for example, 4-shot for MATH, or the longest prompt among the ones we tested with less than the GPT-3 context length of 2048 tokens. Reasoning-oriented tasks, namely MATH, GSM8K, BBH, and GPQA use chain-of-thought rationales. For translation, we report performance on English to Bemba, summarization uses XLSum, MATH corresponds to the MATH500 test set, and sentiment analysis results are reported with semantically-unrelated labels. See \u00a72, \u00a73, and \u00a74 for more details.", "description": "This figure compares the performance of few-shot and many-shot in-context learning (ICL) across various tasks.  It demonstrates that many-shot ICL consistently outperforms few-shot ICL, especially on complex, non-natural language tasks.  The optimal number of shots for each task in the many-shot setting is indicated.  The figure also notes the methodology for determining the number of shots in the few-shot setting and the types of prompts used for different task categories.", "section": "Scaling In-Context Learning"}, {"figure_path": "AB6XpMzvqH/figures/figures_26_1.jpg", "caption": "Figure 1: Many-shot vs Few-Shot In-Context Learning (ICL) across several tasks. Many-shot ICL consistently outperforms few-shot ICL, particularly on difficult non-natural language tasks. Optimal number of shots for many-shot ICL are shown inside the bar for each task. For few-shot ICL, we either use typical number of shots used on a benchmark, for example, 4-shot for MATH, or the longest prompt among the ones we tested with less than the GPT-3 context length of 2048 tokens. Reasoning-oriented tasks, namely MATH, GSM8K, BBH, and GPQA use chain-of-thought rationales. For translation, we report performance on English to Bemba, summarization uses XLSum, MATH corresponds to the MATH500 test set, and sentiment analysis results are reported with semantically-unrelated labels. See \u00a72, \u00a73, and \u00a74 for more details.", "description": "This figure compares the performance of few-shot and many-shot in-context learning (ICL) across various tasks.  It demonstrates that many-shot ICL significantly improves performance compared to few-shot ICL, especially on challenging tasks that don't involve natural language processing. The optimal number of shots for each task is shown. The figure highlights the impact of increased context window size on ICL performance.", "section": "Scaling In-Context Learning"}, {"figure_path": "AB6XpMzvqH/figures/figures_28_1.jpg", "caption": "Figure A.17: Unsupervised ICL does not work for low-resource machine translation. This is expected as providing only source sentences for translation task doesn't improve the task specification. See Figure A.16 for the prompt used for unsupervised ICL for this experiment.", "description": "This figure shows the performance of unsupervised ICL on machine translation from English to Kurdish.  The results demonstrate that providing only source sentences (without target translations) in the prompt does not improve performance. This is expected because the model needs both source and target examples to learn the translation task effectively. The figure is referenced in the appendix section.  It complements Figure 2 in the main body of the paper, which shows that supervised many-shot ICL (with both source and target language pairs) does improve the performance of low-resource machine translation.", "section": "A.10 Unsupervised ICL on Machine Translation"}, {"figure_path": "AB6XpMzvqH/figures/figures_28_2.jpg", "caption": "Figure 5: Many-shot Reinforced and Unsupervised ICL for problem-solving generally outperform ICL with ground-truth MATH solutions. MATH. (Left) The bar plots depict the average performance across five random seeds on the MATH500 test set. Each random seed (denoted by the dots) corresponds to a different subset of problems along with ground truth or model-generated solutions (if any) in the prompt. Transfer to GSM8K. (Right) We see that the prompt obtained from MATH transfers well to the GSM8K test split containing 500 problems. Our results with many-shot ICL outperform the 4-shot Minerva prompt, which obtains a test accuracy of 55.7% on MATH500 and 90.6% on GSM8K.", "description": "The figure shows the performance comparison of three different methods: ICL with ground truth, reinforced ICL, and unsupervised ICL on MATH500 and GSM8K datasets. The left panel shows that reinforced and unsupervised ICL outperform ICL with ground truth on MATH500. The right panel demonstrates the transferability of the learned knowledge from MATH to GSM8K, where reinforced ICL again shows superior performance. Overall, the results highlight the effectiveness of reinforced and unsupervised ICL in many-shot learning.", "section": "3 Many-shot Learning without Human-Written Rationales"}, {"figure_path": "AB6XpMzvqH/figures/figures_29_1.jpg", "caption": "Figure 1: Many-shot vs Few-Shot In-Context Learning (ICL) across several tasks. Many-shot ICL consistently outperforms few-shot ICL, particularly on difficult non-natural language tasks. Optimal number of shots for many-shot ICL are shown inside the bar for each task. For few-shot ICL, we either use typical number of shots used on a benchmark, for example, 4-shot for MATH, or the longest prompt among the ones we tested with less than the GPT-3 context length of 2048 tokens. Reasoning-oriented tasks, namely MATH, GSM8K, BBH, and GPQA use chain-of-thought rationales. For translation, we report performance on English to Bemba, summarization uses XLSum, MATH corresponds to the MATH500 test set, and sentiment analysis results are reported with semantically-unrelated labels. See \u00a72, \u00a73, and \u00a74 for more details.", "description": "This figure compares the performance of few-shot and many-shot in-context learning (ICL) across various tasks.  It demonstrates that many-shot ICL consistently outperforms few-shot ICL, especially for complex tasks that do not involve natural language. The optimal number of shots for each task in the many-shot regime is provided.  The figure also highlights the use of chain-of-thought prompting for reasoning-intensive tasks.", "section": "Scaling ICL"}]