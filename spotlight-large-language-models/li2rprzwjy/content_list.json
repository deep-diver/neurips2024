[{"type": "text", "text": "Rule Extrapolation in Language Models: A Study of Compositional Generalization on OOD Prompts ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anna M\u00e9sz\u00e1ros1, Szilvia Ujv\u00e1ry1,5, Wieland Brendel2,3,4, Patrik Reizinger\u22172, and Ferenc Husz\u00e1r\u22171 ", "page_idx": 0}, {"type": "text", "text": "1University of Cambridge, Cambridge, United Kingdom 2Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany 3ELLIS Institute T\u00fcbingen, T\u00fcbingen, Germany 4T\u00fcbingen AI Center, T\u00fcbingen, Germany 5 AI Center, UCL, London, United Kingdom ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "LLMs show remarkable emergent abilities, such as inferring concepts from presumably out-of-distribution prompts, known as in-context learning. Though this success is often attributed to the Transformer architecture, our systematic understanding is limited. In complex real-world data sets, even defining what is out-of-distribution is not obvious. To better understand the OOD behaviour of autoregressive LLMs, we focus on formal languages, which are defined by the intersection of rules. We define a new scenario of OOD compositional generalization, termed rule extrapolation. Rule extrapolation describes OOD scenarios, where the prompt violates at least one rule. We evaluate rule extrapolation in formal languages with varying complexity in linear and recurrent architectures, the Transformer, and state space models to understand the architectures\u2019 influence on rule extrapolation. We also lay the first stones of a normative theory of rule extrapolation, inspired by the Solomonoff prior in algorithmic information theory. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Autoregressive language models (AR LMs) can reach both low training and test loss, but even minimal test loss is not predictive for out-of-distribution (OOD) model performance [Liu et al., 2023, Reizinger et al., 2024], i.e. when the test data has vanishing probability under the training distribution. Despite the success of deploying modern language models in OOD situations, OOD generalization is not well understood theoretically. Recently, studies started to focus on a specific form of OOD generalization: compositional generalization in language models [Ahuja and Mansouri, 2024, Han and Pad\u00f3, 2024, Ramesh et al., 2024, Lake and Baroni, 2023, Reizinger et al., 2024]. To systematically examine compositional generalization of AR LMs, we study a particular notion of OOD generalization, which we call rule extrapolation. ", "page_idx": 0}, {"type": "text", "text": "Rule extrapolation is a form of compositional generalization: it studies OOD behavior of language models trained on formal languages defined by a logical conjunction of rules. ", "page_idx": 0}, {"type": "text", "text": "For example, the $a^{n}b^{n}$ language is the intersection of two rules: (R1) the number of $a$ \u2019s is equal to the number of $b$ \u2019s and (R2) $a$ \u2019s precede $b$ \u2019s. The prompt bbaab cannot be completed to obey the R2, but it is still possible to satisfy (R1) (e.g., bbaaba). When a language model trained on an intersection of rules remains consistent with one of the rules when another is broken, we say it successfully extrapolated the rule beyond its training data. ", "page_idx": 0}, {"type": "text", "text": "A limited experiment by Reizinger et al. [2024] indicated that Transformers exhibit much-better-thanchance rule extrapolation performance on the formal grammar $a^{n}b^{n}$ , despite lacking any explicit inductive biases encouraging this behaviour. However, it remains unclear whether the behaviour observed was specific to the Transformer or whether it holds more generally on a wider range of formal languages. Inspired by this work, we conduct a thorough empirical investigation of the role of architecture in rule extrapolation on a range of formal languages. As a non-rigorous baseline, we also conducted a small pilot human study to understand how people would generalize the rules. ", "page_idx": 1}, {"type": "text", "text": "We chose to study rule extrapolation because it appears to be a rational, or at least desirable, behaviour. However, we lack a normative reason why this behaviour should be considered rational. It is unclear whether any OOD behaviours could be considered rational. This question led us to investigate how a general rational algorithm for OOD prompt completion might be formalized. That is, instead of asking what models do, we ask what they should do if they were to be consistent with some principles of rational inference. We turn to Algorithmic Information Theory (AIT) to formalize a normative model. We propose a non-parametric prior for next-token prediction inspired by the Solomonoff prior [Solomonoff, 2001, Li and Vit\u00e1nyi, 1997]. This prior helps resolve how a rational model should behave in situations that are mathematically underspecified by their training: to extrapolate the simplest theories consistent with training data. Although, like Solomonoff\u2019s induction, our rational algorithm is uncomputable, it helps explain some of our empirical observations about rule extrapolation in practical language models. Our contributions are: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We use formal languages to define scenarios for evaluating sequence models\u2019 OOD compositional generalization, which we call rule extrapolation $(\\S\\,2.2)$ ;   \n\u2022 We empirically evaluate different models\u2019 rule extrapolation in formal languages with varying complexity, we study linear, recurrent, Transformer and State Space models. We show that there is no single architecture that emerges as a clear winner of rule extrapolation. Though Transformers fare very well in most scenarios we investigated, they struggle on regular languages $(\\S\\,4)$ ;   \n\u2022 Inspired by algorithmic information theory, we propose a normative theory for OOD prompt completion, which posits that rule learning and extrapolation should be governed by the relative simplicities of rules $(\\S\\ S)$ ;   \n\u2022 To demonstrate the presence of a similar simplicity bias in Transformers, We visualise the training dynamics enabling rule extrapolation on the $a^{n}b^{n}$ language. We find that the model first learns a set obeying the easier rule, and then identifies the language as its subset $(\\S\\ S.3)$ . ", "page_idx": 1}, {"type": "image", "img_path": "Li2rpRZWjy/tmp/660dd3c606cec5575d7458134dd0b7a7e6e84a2db45e29a9c76c27352f2a5985.jpg", "img_caption": ["Figure 1: Rule extrapolation summary for all models and languages (Tab. 1): The Transformer is the best on context-free and context-sensitive languages, whereas the LSTM and Mamba excel on regular languages. We also plot chance-level performance as gray rectangles. Mean accuracies and standard deviations (averaged over 5 seeds) ", "Table 1: Formal languages used in our paper: The languages are categorized according to the Chomsky hierarchy, and they can be considered as the intersection of two rules: (R1) and (R2) "], "img_footnote": [], "page_idx": 1}, {"type": "table", "img_path": "Li2rpRZWjy/tmp/dd0102fd5f4c728ecccc2ec722160d75cb2be95a284b7dac3513005ea15b831f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "2 Background and related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Formal languages ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Formal languages are linguistic constructions that simplify the study of natural languages. Their advantage is their well-defined set of symbols and rules. Although they fall short of capturing the nuances and irregularities of human languages, they are very powerful with immense practical relevance\u2014e.g., programming languages are formal languages. ", "page_idx": 2}, {"type": "text", "text": "Formal languages consist of words with symbols coming from a possibly infinite alphabet. Chomsky [1956] has categorized formal languages into four types with increasing complexity: regular, context-free, context-sensitive, and recursively enumerable languages. Regular languages have rules that can be expressed via regular expressions, e.g., $L_{1}\\,=\\,\\{b\\alpha\\,:\\,\\alpha$ contains even number of $a^{\\ast}\\mathrm{s}\\}$ and ${\\cal L}_{2}~=~\\{\\dot{b}^{n}a^{2m}~:~n~\\stackrel{\\smile}{>}~0\\}$ . Context-free grammars have rules that do not depend on the context\u2014programming languages such as $\\mathbf{C}$ or Python belong to this category, e.g., an if-else block in the programming language $\\mathbf{C}$ always has the same structure. For demonstration purposes, we will use two simpler languages: $L_{3}\\ =\\ \\{a^{n}b^{n}}\\ :\\ n\\ >\\ 0\\}$ , $\\begin{array}{r l}{L_{4}}&{{}=}\\end{array}$ {sequences of nested parentheses and brackets $\\}$ . Context-sensitive grammars have rules that depend on the position in the sequence\u2014we will use the standard example of $L_{5}=\\{a^{n}b^{n}c^{n}\\colon n>0\\}$ and $L_{6}=\\left\\{\\begin{array}{l l}{\\begin{array}{r l r l}\\end{array}}\\end{array}\\right.$ {sequences of paired, but not necessarily nested parentheses and brackets $\\}$ . We omitted the recursively enumerable grammars similar to Deletang et al. [2022], as they require an infinite tape to simulate, which is impossible. ", "page_idx": 2}, {"type": "text", "text": "2.2 Out-of-distribution (OOD) generalization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In modern deep learning theory, the test loss distinguishes the performance of models with low training loss by evaluating the model on unseen data sampled from the same distribution (i. e., i.i.d.) as the data it was trained on. When the test loss is (near-)minimal, the model has statistical generalization ability. Therefore several studies focused on establishing bounds on the generalization gap [Vapnik and Chervonenkis, 1971, Dziugaite and Roy, 2017, P\u00e9rez-Ortiz et al., 2021]. Along with the question of whether the test loss is sufficiently low, another one arose: does the test loss have a unique minimum? Identifiability is a property of a family of statistical models, concerning the uniqueness of the data generator model recovered from the observed data. In machine learning, identifiability implies the uniqueness of the test loss\u2019 minimum, and the model it corresponds to, which is desirable since it enables us to interpret the model and reason about its properties. ", "page_idx": 2}, {"type": "text", "text": "Theoretical tools such as statistical generalization or identifiability are mostly concerned about the i.i.d. scenario, i.e., when the training and test data come from the same distribution. However, this is an unrealistic assumption for language models (LMs), especially when pretrained models are used for various downstream tasks. Despite the clear OOD nature of these tasks, OOD generalization of these models is not understood theoretically. Recently, several works addressed a special type of OOD generalization called compositional generalization in vision models [Schott et al., 2021, Wiedemer et al., 2023b,a, Brady et al., 2023, Yang et al., 2023, Lachapelle et al., 2023]; however, such studies are only started emerging for natural language [Ahuja and Mansouri, 2024, Han and Pad\u00f3, 2024, Ramesh et al., 2024, Lake and Baroni, 2023, Nogueira et al., 2021, Dziri et al., 2023, Saparov et al., 2023]. Deletang et al. [2022] and Ruoss et al. [2023] conducted a similar experimental investigation to ours, the tasks they evaluate on are also derived from formal language recognition and thus grouped according to the Chomsky hierarchy, but they focus on length generalization. ", "page_idx": 2}, {"type": "text", "text": "Reizinger et al. [2024] show that despite any explicit inductive bias or regularization, Transformers can exhibit much-better-than-chance extrapolation performance on some synthetic grammars. However, it is unclear whether this behavior is specific to the Transformer and/or the formal language. Furthermore, there are some tasks such as addition and parity that are known to be very hard (or even impossible) to solve by Transformers, at least without tricks [Zhou et al., 2023]. Inspired by these works, our paper investigates the role of architecture in different formal languages. ", "page_idx": 2}, {"type": "text", "text": "Rule extrapolation. To understand the OOD behavior in AR LMs, we study a particular notion of OOD generalization, which we term rule extrapolation. Rule extrapolation is a subclass of compositional generalization, for formal languages are defined by composing multiple rules. When assessing rule extrapolation, the model is pre-trained on formal language data, i.e., the support is the intersection of all language rules. Then, OOD data is presented, where a subset of rules is violated, thus having zero probability over the training distribution. If the completed OOD prompts satisfy the not violated rules, we say the model extrapolates the rules. For example, the $a^{n}b^{n}$ language is the intersection of two rules: (R1) the number of $a$ \u2019s equals the number of $b$ \u2019s and (R2) $a$ \u2019s precede $b$ \u2019s. The prompt bbaab cannot be completed to obey the second rule. In this case, rule extrapolation means that the completed prompt satisfies the first rule (e.g., bbaaba). ", "page_idx": 2}, {"type": "text", "text": "2.3 Inductive biases in sequence models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Several deep learning architectures, such as CNNs or GNNs, were designed to capture specific structural data properties. Such inductive biases in sequence models remain to be understood [Reizinger et al., 2024]. McCoy et al. [2020] and Murty et al. [2023] studied whether different architectures on language processing tasks have an inductive bias towards hierarchical structure. [Murty et al., 2023] showed that with sufficient training, the transformer architecture can represent hierarchical sentence structure and use this structure to generalize correctly. Several works establish forms of simplicity bias [Valle-P\u00e9rez et al., 2019, Dingle et al., 2018, Mingard et al., 2020]. Goldblum et al. [2023] demonstrate that (even randomly initialized) language models are biased towards low algorithmic complexity. Weiss et al. [2021] developed a formal programming language called RASP to model the inner workings of the Transformer, whereas Zhou et al. [2023] defined a subset, called RASP-L, and proved length generalization in Transformer, emphasizing a simplicity bias in terms of RASP-L code length. Chen et al. [2024] attribute the development of grammatical capabilities to Syntactic Attention Structure (SAS), wherein specific Transformer heads tend to focus on specific syntactic relations. These approaches leverage the tools of theoretical computer science to reason about the success of Transformers, hinting at the role of a structural inductive bias. For example, in-context learning (ICL) performance depends on the ordering of layers in the Transformer [Press et al., 2020], and also the structure of the training data [Chan et al., 2022]. LM inductive biases have also been studied from a mechanistic interpretability perspective. Most notably, Olsson et al. [2022] propose that ICL is due to induction heads (a type of specialised attention heads). Mechanistic interpretability approaches can also identify and disable the computational circuits responsible for bad behaviors [Li et al., 2024] and locate ones that capture factual knowledge [Meng et al., 2023]. These works constitute important progress; though we take a step back to ask: is the good performance attributable to the Transformer? Are (at least some of) these emergent capabilities present in simpler models such as linear models or RNNs? ", "page_idx": 3}, {"type": "text", "text": "3 Experimental setup ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Architectures ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To study when rule extrapolation emerges, we compare five architectures: linear models, LSTMs [Hochreiter and Schmidhuber, 1997], Transformers [Vaswani et al., 2023], and State Space Models (SSMs) (focusing on Mamba [Gu and Dao, 2023]), and the recently introduced xLSTM [Beck et al., 2024]. The Transformer [Vaswani et al., 2023] caused a breakthrough in Natural Language Processing (NLP) by introducing the (self-)attention mechanism, allowing it to capture global dependencies efficiently in both directions, unlike the standard LSTM. Adapted from dynamical systems, SSMs have recently entered language modeling, and became increasingly popular, such as this work\u2019s focus, Mamba [Gu and Dao, 2023]. In this architecture, the attention mechanism (where every token must \u201cattend\" to every other token) is replaced by a single SSM block, allowing the model to selectively focus on relevant information. The on-par performance of the Transformer and the SSM along with the removal of the attention block raises the question of whether the SSM also show rule extrapolation abilities. Recently, Beck et al. [2024] proposed an extension of the LSTM, which includes matrix-valued memory cells, new gating and memory mixing mechanisms, and several computational improvements. Training details, data set sizes, and model parameters are in Appx. B. ", "page_idx": 3}, {"type": "text", "text": "3.2 Datasets ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our data sets follow the hierarchy of [Chomsky, 1956]. The advantage of the classification is that the categories exhibit fundamental differences. However, this hierarchy is based on computational linguistics concepts. Therefore, there might be no connection between the language\u2019s complexity in the Chomsky hierarchy and what a neural network finds difficult to learn. Each language we study obeys two rules, and the OOD prompts violate the corresponding R2, but the prompt can still be completed to satisfy the other. Following (R1) and/or (R2) provide different information: following (R1) means the LM still adheres to a rule even when the other is violated (in the whole sequence), whereas adhering to (R2) on the completion shows that the LM still tries to satisfy that. ", "page_idx": 3}, {"type": "text", "text": "The used formal languages and their categorization and rules are included in Tab. 1. We define two rules for each language to keep the results comparable; however, we acknowledge that these can lead to rules of different complexity (cf. the chance levels for $L_{3}$ and $L_{5}$ in Tabs. 4 and 6), and also that the rules can potentially be defined in multiple equivalent ways. ", "page_idx": 3}, {"type": "text", "text": "Regular grammars. Regarding the hierarchy, the two simplest data sets are regular languages $L_{1}\\,=\\,\\{b\\alpha\\,:\\,\\alpha$ contains even number of $\\left.\\mathrm{a}^{\\circ}\\mathrm{s}\\right\\}$ and $L_{2}\\,=\\,\\{b^{n}\\dot{a}^{2m}\\,:\\,n,m\\,>\\,0\\}$ . The rules of the language $L_{1}$ are: (R1) there are even number of $a\\mathbf{S}$ in the sequence; and (R2) the sequence starts with a $b$ . For $L_{1}$ , the OOD prompts consist of prompts that violate (R2) , i.e. start with an $a$ , but all these prompts can be completed to satisfy (R1). For language $L_{2}$ , the rules are: (R1) there are even number of $a\\mathbf{S}$ in the sequence; and (R2) bs precede as. The OOD prompts for $L_{2}$ violate (R2). They start with a single $a$ , then a block of bs and possibly a block of as. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Context-free grammars. We implemented two context-free grammars $L_{3}$ and $L_{4}$ : $L_{3}=\\{a^{n}b^{n}:$ $n>0\\}$ , i.e., (R1) the number of as and bs match; and (R2) as precede bs. For $L_{3}$ , OOD prompts violate (R2) , i.e., the prompts include $b$ tokens followed by $a$ tokens. ", "page_idx": 4}, {"type": "text", "text": "Our fourth formal language is a bracketing (Dyck-) language, i.e., $\\begin{array}{r l r l}{L_{4}}&{{}}&{=}&{}\\end{array}$ {sequences of nested and paired parentheses and brackets $\\}$ , e.g. $\\sqrt[6]{()}=\\sqrt[3]{}$ The rules of the language are: (R1) brackets are nested and paired; and (R2) parentheses are nested and paired. Paired means that every opening bracket/parenthesis has a closing pair; nested means that between an opening and closing bracket/parenthesis, all other tokens must be paired\u2014contrast this with $L_{6}$ . For $L_{4}$ , ID prompts begin with \"([\" and OOD prompts start with \")[\"; both are followed by a sequence where the parentheses and the square brackets are matched. ", "page_idx": 4}, {"type": "text", "text": "Context-sensitive grammars. We implemented two context-sensitive grammars $L_{5}$ and $L_{6}$ . $L_{5}~=~\\{a^{n}b^{n}c^{n}~:~\\bar{n}~>~0\\}$ . Though it seems very similar to $L_{3}$ , its grammar rules make it context-sensitive, i.e., the tokens generated depend on multiple tokens. The grammar rules can be summarized as: (R1) the number of as, bs, and cs are the same; (R2) as precede bs and bs precede cs; and The OOD prompts are sequences which violate (R2) . All these prompts can still be completed to obey (R1). $L_{6}$ is a context-sensistve Dyck-language, i.e., $L_{6}\\,=\\,\\left\\{\\begin{array}{l l}{\\begin{array}{r l r l}\\end{array}}\\end{array}\\right.$ {sequences of paired, but not necessarily nested parentheses and brackets}, e.g. $\\bar{\\bullet}(\\ [\\ \\bar{\\ }])^{\\prime}$ \" The rules of the language are: (R1) brackets are paired; and (R2) parentheses are paired. Akin to $L_{4}$ , for $L_{6}$ ID prompts begin with \"([\" and OOD prompts start with \")[\"; both are followed by a sequence where the parentheses and the square brackets are matched. ", "page_idx": 4}, {"type": "text", "text": "3.3 Metrics. ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We monitor training and test loss. We evaluate the accuracy of both rules (R1/R2) separately and simultaneously both for in-distribution samples, and also for OOD prompts. As OOD prompts are designed that (R2) cannot be satisfied, we evaluate its accuracy in the most lenient way. That is, we either calculate it on the completion or, for the Dyck languages, on the part after the closing parenthesis \u201c)\". An example for the $L_{3}$ OOD prompt abbb is as follows: the completion abbbaa is considered correct for (R2), but abbbabaa is not, as it has an $a$ after a $b$ in the completion. Our evaluation is restricted to prompt completions with an EOS token. We also monitor the accuracy of the next token prediction via greedy decoding (i.e., using the token with the largest probability). Our results report the minimum of the test loss to measure whether the models are in the saturation regime [Reizinger et al., 2024]. We select the largest values for the rule accuracies. We choose this evaluation as small variations in the test loss could lead to large deviations (as predicted by Liu et al. [2023]). We also report chance level accuracies as a baseline, quantifying how complex a given rule is. Chance level accuracy in each case refers to the performance of a model that always predicts each token (excluding the start-of-sequence (SOS) token) as the next token with equal probability2. We report means and standard deviations across 5 seeds. Similar to [Rajamanoharan et al., 2024], we provide a non-representative human baseline based on a small pilot study, where participants have seen three examples for $L_{1},L_{3}$ then were asked to complete five OOD sequences for each (Appx. B.6). We corrected for invalid answers and emphasize that we only aim to provide a sense of how humans measure against neural networks, without reaching any statistical conclusions. ", "page_idx": 4}, {"type": "text", "text": "4 Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Regular grammars. Perhaps surprisingly, modern architectures perform the worst on regular languages $L_{1}$ (Tab. 2) and $L_{2}$ (Tab. 3) : both Mamba and the Transformer are worse in- and out-ofdistribution than the LSTM\u2014the xLSTM only matches the LSTM in OOD performance on (R1). Furthermore, the Transformer\u2019s accuracies are below chance level even for in-distribution, despite having approximately the same test loss as the LSTM and Mamba. The Linear model seemingly manages to obey perfectly (R2) in-distribution on $L_{2}$ , which happens because this model only predicts EOS on test prompts, and the ID test prompt already satisfies (R2). In the other categories, Linear is at or below chance-level. In our small pilot study, humans performed akin to Mamba on $L_{1}$ (Tab. 14). Zhou et al. [2023] observed that Transformers struggle with addition or parity calculation, which might explain the Transformer\u2019s low performance on regular languages, as both $L_{1},L_{2}$ require calculating the parity of $a$ tokens. ", "page_idx": 4}, {"type": "table", "img_path": "Li2rpRZWjy/tmp/e965b11a146b7c4abbd6f20f3ffec2d4fb94c6e5921ecb006812ee384e1cdae3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "Li2rpRZWjy/tmp/707360221a7df6c9150ef685f373cd55a0eebde9846f5d048acf36e9206e9650.jpg", "table_caption": ["Table 2: Test loss and rule-following accuracies for the regular language $L_{1}=\\{b\\alpha\\}$ : the LSTM can extrapolate (R1) the best. The column R2 is left out as it is satisfied by design. ", "Table 3: Test loss and rule-following accuracies for the regular language $L_{2}=\\{b^{n}a^{2m}\\}$ : the LSTM and the xLSTM can extrapolate (R1) the best, closely followed by Mamba "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Context-free grammars. On the context-free grammars $L_{3},L_{4}$ , the conclusion is different. On $L_{3}$ (Tab. 4), although all four models achieve perfect accuracy on (R2) both in- and out-of-distribution, and all models except the Linear, (near) perfectly obey (R1) in-distribution, the Transformer extrapolates (R1) to the largest extent $(66\\%)$ , followed by the LSTM $(38\\%)$ and Mamba $(30\\%)$ . The seemingly perfect (R2) ID and OOD extrapolation for the Linear model is, again, due to EOS token generation. On the Dyck language $L_{4}$ (Tab. 5), the Transformer has the best extrapolation performance, and Mamba is better than the LSTM. On $L_{3}$ , the human participants in our small study had performed better on following (R2) on the completion than extrapolating (R1); however, the Transformer was better than humans in extrapolating both (R1) and (R2). ", "page_idx": 5}, {"type": "table", "img_path": "Li2rpRZWjy/tmp/11b05aa4eeaa2765a15c8020ae09478b9e841e5f9f8744f5bed37d387129fd94.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Table 4: Test loss and rule-following accuracies for the context-free language $L_{3}=\\{a^{n}b^{n}\\}$ : the Transformer can extrapolate (R1) the best. ", "page_idx": 5}, {"type": "text", "text": "Context-sensitive grammars. The grammar $L_{5}$ (Tab. 6) is similar to $L_{3}$ , i.e., the Transformer performs best. Intuitively, the sequences in the form of $\\{a^{n}b^{n}\\}$ and $\\{a^{n}b^{n}c^{n}\\}$ are rather similar, despite the latter being context-sensitive in Chomsky\u2019s hierarchy. Rule extraplation accuracies for (R1) in $L_{5}$ are lower than for $L_{3}$ , which can be attributed to the higher complexity of (R1) in the context-sensitive grammar (cf. chance levels in Tabs. 4 and 6). For the context-sensitive Dyck language $L_{6}$ (Tab. 7), the Transformer and LSTM perform similarly on both OOD (R1) and (R2). ", "page_idx": 5}, {"type": "text", "text": "Results summary. We conclude that on different grammars, different architectures perform best (Fig. 1). Although the Transformer has a consistently good performance on the investigated contextfree and -sensitive grammars, LSTM and Mamba are better choices for the studied regular grammars. We hypothesize that it happens because these languages require calculating parity, in which the Transformer struggles [Zhou et al., 2023]. The xLSTM generally lies somewhere between the LSTM and the Transformer. The Linear model has very limited capabilities for modeling formal grammars as it cannot even minimize the test loss. In our small pilot study on $L_{1},L_{3}$ , humans found the tasks difficult: they performed better than chance, though the LSTM performed better on $L_{1}$ , and the Transformer on $L_{3}$ (Tab. 14)\u2014we emphasize that our human-machine comparison only provides intuition, rather than a rigorous evaluation of human performance, which is left for future work. ", "page_idx": 5}, {"type": "table", "img_path": "Li2rpRZWjy/tmp/9f909063145d98ec0efb3cfd71432eca7fe51388acfb7dc51257c286d2794948.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "Li2rpRZWjy/tmp/64c1a766aac822ed184cf1a87acafff75a1e802dec8a048fe80f8149bc291749.jpg", "table_caption": ["Table 5: Test loss and rule-following accuracies for the context-free Dyck language $L_{4}$ : the Transformer can extrapolate (R1) the best. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "Li2rpRZWjy/tmp/07a3cec84ed38cf367791b2f6a2ec2d04c1ddfb1bf9d320fe6aa14f7eaab18e1.jpg", "table_caption": ["Table 6: Test loss and rule-following accuracies for the context-sensitive language ${\\cal L}_{5}=\\{a^{n}b^{n}c^{n}\\}$ : the Transformer can extrapolate (R1) the best "], "table_footnote": ["Table 7: Test loss and rule-following accuracies for the context-sensitive Dyck language $L_{6}$ : the Transformer and the LSTM can extrapolate the best "], "page_idx": 6}, {"type": "text", "text": "5 Normative theory of OOD prompt completion ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The previous sections empirically assessed an example of rational OOD prompt completion: rule extrapolation. In this section, instead of asking what happens, we take a step back to ask what should happen: how an ideal model should learn and extrapolate rules. We propose a non-parametric prior and prediction scheme for OOD prompt completion, that can be seen as a generalization of Solomonoff induction [Solomonoff, 2001, Li and Vit\u00e1nyi, 1997] to settings relevant for AR LMs. Although our algorithm, just like Solomonoff induction, is uncomputable, we argue that it formalises a rational approach capable of OOD extrapolation in AR sequence models. Rather than a practical algorithm itself, it should be interpreted as a guide towards building and assessing future practical models. Our conceptual approach is not without precedent: ideas from AIT have recently been popularized as \u201cNorth Stars\u201d for guiding practical implementations [Theis, 2024, Goldblum et al., 2023], and have been applied in practical algorithms [Grau-Moya et al., 2024]. ", "page_idx": 6}, {"type": "text", "text": "We first introduce our approach on the high-level, via the following story. ", "page_idx": 6}, {"type": "text", "text": "A story of OOD prompt completion. Suppose that Bob has a Language Model $p_{\\mathrm{data}}$ , that autoregressively generates $M$ i.i.d. sequences of length $m$ , $\\{(x_{1,j},x_{2,j},\\ldots{}\\stackrel{\\leftarrow}{x}_{m,j})\\}_{j=1}^{M}:=\\widehat{(x_{1j}^{m})_{i=1}^{M}}$ . Since the sequences are generated autoregressively, we may call $(x_{1j}^{m-1})_{j=1}^{M}$ the $I D$ prompts, and each $m^{t h}$ element (xm,j)j its $I D$ completions. Suppose that Charlie, Bob\u2019s enemy, generates a $n$ \u2212length sequence from the same LM, and intervenes (in the causal sense) on it, so that the resulting sequence $(x_{1},x_{2},\\ldots x_{n-1}):=x_{1}^{n-1}$ has zero probability under the LM. We call this the OOD prompt. Despite $p_{\\mathrm{data}}(x_{1}^{n-1})=0$ , the LM still defines the conditional probability of completing the OOD prompt $x_{1}^{n-1}$ . Charlie then asks an observer, Alice, to predict how Bob\u2019s LM will complete the OOD prompt $x_{1}^{n-1}$ , i.e., what $x_{n}$ will be. Fig. 2 shows the probabilistic assumptions of Alice: the completions are generated independently, according to the same procedure (i.e., using the same LM). We use the conditional independence assumption xn \u22a5(x1m )jM=1 | x1n in eq. (4) below. ", "page_idx": 6}, {"type": "image", "img_path": "Li2rpRZWjy/tmp/2f51ffce005d746b41be04a34c378da1f90d671e7a6ce8f8e30753d4ad819014.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 2: Graphical model representing our approach for OOD prompt completion. Although Bob\u2019s LM $p_{\\mathrm{data}}$ assigns zero probability to the OOD prompt, it defines a conditional probability distribution for its completions. Our probabilistic model assumes that Bob\u2019s LM completes the ID and OOD prompt independently, according to the same procedure (e.g. the same LM architecture and parameters are used for generating the completions). This is the same as assuming that the Markov factors marked in blue are the same, i.e. $p$ (completion|prompt, $p_{\\mathrm{data}})=p$ (completion|OOD prompt, $p_{\\mathrm{data}}$ ), and the conditional independence OOD completion $\\perp\\mathrm{ID}$ prompt | OOD prompt. ", "page_idx": 7}, {"type": "text", "text": "In the rest of this section, we construct an algorithmic prior that formalizes these assumptions, and argue why it is a promising approach to study OOD compositional generalization theoretically. ", "page_idx": 7}, {"type": "text", "text": "5.1 The Solomonoff prior ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The Solomonoff prior assigns a prior probability to individual data points based on some algorithmic notion of how difficult it is to generate that data point. It embodies Occam\u2019s razor and Epicure\u2019s principle, as simple data points have a larger probability, and every possible explanation is included in the prior (see also Appx. C.2). For simplicity, we define the Solomonoff prior for discrete sample spaces, though similar arguments hold for the continuous case. To encourage readability, we define technical terms in Appx. C.2, and highlight them in blue here. Let us fix a monotone universal Turing machine (UTM). Solomonoff\u2019s universal prior [Solomonoff, 2001] is defined over arbitrary-length sequences $x_{1}^{N}:=(x_{1},x_{2},\\ldots,x_{N})$ as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\stackrel{\\triangledown{\\bf w}}{p_{S}}(x_{1}^{N})=\\sum_{i\\mathrm{~\\ldots~},\\mathrm{~\\ldots~}}\\alpha(p_{i})p_{i}(x_{1}^{N}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where we sum over all discrete lower semicomputable semimeasures $p_{i}(x_{1}^{N})$ implementable on the UTM [Li and Vit\u00e1nyi, 1997]. We will refer to the $p_{i}(x_{1}^{N})$ as mixture components or explanations of the data. The prior on weights $\\alpha(p_{i})$ is an arbitrary semimeasure, i.e., $\\forall i\\,:\\,\\,\\,\\alpha(p_{i})\\,>\\,0$ and $\\textstyle\\sum_{i}\\alpha(p_{i})\\leq1$ . Frequently, $\\alpha(p_{i})$ is chosen as $2^{-K(p_{i})}$ , the prefix Kolmogorov complexity of $p_{i}$ in the UTM (see Defn. C.5 in Appx. C.2). ", "page_idx": 7}, {"type": "text", "text": "Predictive form. The above formulation of the Solomonoff prior has the predictive form [Hutter, 2005, Chapter 3.2.3], where $\\alpha(p_{i}\\mid x_{1}^{N-1})$ is updated via Bayesian inference: ", "page_idx": 7}, {"type": "equation", "text": "$$\np_{S}(x_{N}\\mid x_{1}^{N-1})=\\sum_{i}\\alpha(p_{i}\\mid x_{1}^{N-1})p_{i}(x_{N}\\mid x_{1}^{N-1}),\\mathrm{~where~}\\alpha(p_{i}\\mid x_{1}^{N-1})=\\frac{\\alpha(p_{i})p_{i}(x_{1}^{N-1})}{p_{S}(x_{1}^{N-1})}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Convergence of predictions. Suppose that the true distribution of $(x_{1},x_{2},\\ldots,x_{N})$ is $\\mu$ . The Solomonoff prior (with any valid sequence of weights) satisfies [Hutter, 2005]. ", "page_idx": 7}, {"type": "text", "text": "5.2 A predictive model for OOD prompt completion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our goal is to define a similar prior, and predictive scheme that fits our scenario of AR next-token prediction, and where we can express the notion of completing an out-of-distribution prompt $x_{1}^{n-1}$ even when our prior assigns zero probability to the prompt. ", "page_idx": 7}, {"type": "text", "text": "The Solomonoff prior assigns nonzero prior mass to every possible prompt, i.e. there exist no OOD problems for the Solomonoff prior, as each possible test distribution is included in the prior as a mixture component $p_{i}$ . However, by definition, the Solomonoff prior can only take in a single sequence $x_{1}^{n}$ . This means that it can only model pre-training and (OOD) testing together, since the pre-training and testing data need to be concatenated into the same sequence [Hutter, 2011]. Intuitively, it is more natural to separate those processes. To achieve this, we propose an adapted version of the Solomonoff prior, modifying it two ways, and justifying our approach below: ", "page_idx": 7}, {"type": "text", "text": "(i) We condition the prediction on a pre-training dataset $\\mathcal{D}$ of $M$ independent and identically distributed (i.i.d.) sequences of finite length $m$ , i.e. $\\boldsymbol{\\mathcal{D}}=\\{x_{1j}^{m}\\}_{j=1}^{M}$ . $\\mathcal{D}$ is sampled from the distribution $\\begin{array}{r}{p_{\\mathrm{data}}^{M}(\\mathcal{D})=\\prod_{j=1}^{M}\\prod_{k=2}^{m}p_{\\mathrm{data}}(x_{k,j}\\ |\\ x_{1,j}^{k-1})}\\end{array}$ . For simplicity, we assume that each pre-training datapoint has equal length $m$ . ", "page_idx": 8}, {"type": "text", "text": "(ii) Instead of modelling semimeasures as joints over sequences $\\{(x_{1}^{N})\\}_{N\\in\\mathbb{N}}$ , we model semimeasures as lists of conditionals, just as how AR LMs model probability distributions over $\\{(x_{k}\\ \\mid\\ x_{1}^{k-1})\\}_{k=2}^{N}$ , enumerating them with index $i\\;=\\;1,2,\\ldots$ , denoting each semimeasure as $p_{i|}$ to emphasize the lists of conditionals representation. That is, $p_{i|}(x_{k}\\mid x_{1}^{k-1})$ and $p_{i|}(\\mathcal{D})$ mean $p_{i}(x_{k}\\mid x_{1}^{k-1})$ and $\\begin{array}{r}{p_{i}(\\mathcal{D})=\\prod_{j=1}^{M}\\prod_{k=2}^{m}p_{i}(x_{k,j}\\mid x_{1,j}^{k-1})}\\end{array}$ , respectively. Note that the pre-training distribution $p_{\\mathrm{data}}$ also belongs to the set of $p_{i|}$ . We define a mixture over all lists of discrete lower semicomputable semimeasures $p_{i|}$ implementable on the UTM See Appx. C.1 for details. ", "page_idx": 8}, {"type": "text", "text": "The motivation for modelling $x_{1}^{N}$ as a list of conditionals is because the mapping from lists of conditional factorizations to joint semimeasures consistent with them is a many-to-one mapping, because zero-probability sequences have multiple factorizations (see Appx. C.1 for justification and more details on this notation). If the prompt $x_{1}^{n-1}$ comes from a distribution different from $\\mathcal{D}\\sim p_{\\mathrm{data}}^{M}$ that assigns zero probability mass to $x_{1}^{n-1}$ , the probability $p_{\\mathrm{data}}(x_{n}\\mid x_{1}^{n-1})$ is left undefined if only the joint probability $p_{\\mathrm{data}}(x_{1}^{n})$ is specified. This is not a problem in the Solomonoff prior, as it assigns nonzero probability mass to every (computable) sequence. But once we introduce the conditioning on $\\mathcal{D}$ , this step becomes necessary. The above two modifications generalize the predictive form of the Solomonoff prior as follows (we color-code the equation denoting modification (i) in red and modification (ii) in green): ", "page_idx": 8}, {"type": "equation", "text": "$$\np_{R}(x_{n}\\mid x_{1}^{n-1},\\mathcal{D}):=\\sum_{i}\\alpha(p_{i\\mid}\\mid\\mathcal{D})p_{i\\mid}(x_{n}\\mid x_{1}^{n-1}),\\ \\mathrm{with}\\ \\alpha(p_{i\\mid}\\mid\\mathcal{D})=\\frac{\\alpha(p_{i\\mid})p_{i\\mid}(\\mathcal{D})}{p_{\\mathrm{data}}(\\mathcal{D})}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Interpreting $p_{R}$ . Starting from a prior weight $\\alpha$ over all possible explanations $p_{i|}\\,=\\,\\{p_{i}(x_{k}\\,\\,|\\,\\,$ $x_{1}^{k-1})\\}_{k=2}^{n}$ , the posterior probability of $p_{i|}$ given $\\mathcal{D}$ is computed (eq. (4), right). The $n^{t h}$ step prediction by $p_{i}$ , conditioned on a possibly OOD test prompt, is then weighted by this posterior. It is important that the prediction $p_{i|}(x_{n}\\mid x_{1}^{n-1})$ is not conditioned on the pre-training data $\\mathcal{D}$ , and the posterior $\\alpha(p_{i|}\\mid D)$ is not conditioned on the test prompt $x_{1}^{n-1}$ . This, as stated above, separates pre-training from testing, enabling us to define the completion of OOD test prompts. When $\\mathcal{D}$ equals $\\bar{x}_{1}^{n-1}$ , $p_{R}$ reduces to $p_{S}$ , and thus the posterior prediction converges according to eq. (3). ", "page_idx": 8}, {"type": "text", "text": "Choice of the weight prior $\\alpha(p_{i|})$ . For OOD test prompts, there are multiple explanations $p_{i|}$ consistent with $\\mathcal{D}$ . Therefore, the behaviour of $p_{R}$ , even when $|\\mathcal D|$ tends to infinity, depends on the prior weight $\\alpha(p_{i|})$ . This differs from the Solomonoff prior, which converges to the true posterior regardless of the weights (eq. (3)) [Hutter, 2005]. Thus, $\\alpha$ must be chosen to allow the extrapolation of simple explanations consistent with the data. We define $\\alpha(p_{i|}):=2^{-K(p_{i|})}$ , penalising exponentially the length of the shortest program (implemented on the fixed UTM) $K(p_{i|})$ that can approximate $p_{i|}$ (each conditional probability) for every prompt $x_{1}^{n}$ . This encodes Occam\u2019s razor into the prior, and is consistent with the optimal weights of the Solomonoff prior [Hutter, 2005]. ", "page_idx": 8}, {"type": "text", "text": "5.3 Towards explaining training dynamics and rule extrapolation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Here, we argue informally that our normative algorithm provides a notion of a rational pre-training process, and thus helps explain the training dynamics of practical LMs, and is also capable of rule extrapolation. We support our arguments by showing the role of simplicity bias (towards low Kolmogorov complexity) in the dynamics of learning the $a^{n}b^{n}$ language with Transformers. ", "page_idx": 8}, {"type": "text", "text": "Explaining training dynamics. We analize the dynamics of learning rule extrapolation. We report results on the Transformer (training dynamics of Mamba and the LSTM are in Appx. A), trained on the $a^{n}b^{n}$ language\u2014where high rule extrapolation ability is achieved. Fig. 3 shows that first, the model learns the sequences obeying (R2), then it learns the language (R1) $\\cap$ (R2) as its subset. We argue that the order in which rules are learnt is governed by the relative simplicity of the rules, quantified by Kolmogorov complexity. Given a formal language with rules (R1) and (R2), let $p_{1}$ , $p_{2}$ and $p_{1,2}$ be distributions defined by LMs that generate sequences that satisfy (R1), (R2) and (R1) $\\cap$ (R2), respectively. If, e.g., $K(p_{2})\\overset{.}{\\ll}K(p_{1,2})$ , our normative algorithm will first learn (R2), and then learn the (R1) $\\cap$ (R2) as its subset. In the $a^{n}b^{n}$ language, (R2) ( $a$ \u2019s before $b$ \u2019s), is, on average, simpler to generate than (R1) $(\\#a=\\#b)$ and (R1) $\\cap$ (R2). Therefore, we expect our normative algorithm to first learn (R2), and then learn (R1) $\\cap$ (R2) as its subset. Remarkably, our Transformer employs the same strategy ( Fig. 3), verifying the presence of simplicity bias. This result is matches past observations that Transformers are biased towards low Kolmogorov complexity [Goldblum et al., 2023]. ", "page_idx": 8}, {"type": "image", "img_path": "Li2rpRZWjy/tmp/1686a01a020a609ce45a0c0ada8d28df7ebe67212305e65287189d81bfec961a.jpg", "img_caption": ["Figure 3: Training dynamics of rule learning for a Transformer trained on the $a^{n}b^{n}$ language: we color-code the log probability of all sequences of length 8 consisting of $a$ \u2019s and $b$ \u2019s and ending with EOS at initialization (left left), during (left middle) and after training (left right). The sequences are separated according to which rule they obey. While at initialization, the probabilities are distributed roughly evenly, during training the model starts to assign higher probabilities to sequences satisfying (R2). After training the most likely sequences are the ones in (R1) $\\cap$ (R2), the others are negligible. The same trend can be seen on the right, where the normalized sum of the probabilities of the four categories (satisfying (R1) and (R2), only (R1), only (R2) and neither) is plotted during training. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Towards explaining rule extrapolation. Our normative algorithm has been designed to complete OOD prompt based on the simplest explanations consistent with the pre-training data. On the high level, this approach is consistent with rule extrapolation. We conjecture that approximating our normative algorithm similarly to the approach of Grau-Moya et al. [2024], will result in models with superior rule extrapolation properties. We leave this promising direction to future work. ", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Conclusion. We argue that focusing on rule extrapolation and formal languages gives us sound (theoretical) tools to analyze and better understand out-of-distribution behaviour in language models, such as the role of different architectures. Our empirical findings emphasize that no single universal architecture exists for autoregressive sequence modeling. Though Transformers fare very well in most scenarios we investigated, they struggled on regular languages. Therefore, we argue that the architecture\u2019s inductive bias should be considered when selecting models since the architecture that performs the best depends on the nature of the task. Furthermore, we analyse the training process enabling rule extrapolation, we find that the model first identifies the whole set obeying one of the rules, then it learns the language (intersection of all rules) as its subset. Beyond advancing our empirical understanding, we also proposed a normative theory of OOD prompt completion. Our normative algorithm predicts the next token based on simple explanations consistent with the data, and allows us to explain and contextualise some of our empirical observations. ", "page_idx": 9}, {"type": "text", "text": "Impact. Rule extrapolation is a special case of compositional generalization in language models. While other OOD generalisation types were examined previously, this is the first work studying rule extrapolation. This novel concept has the potential to impact LLM research both on conceptual and practical levels. General compositional generalization notions examine whether from learning multiple concepts/rules separately, the model can understand the composition of the concepts/intersection of the rules. However, in rule extrapolation, we measure the reverse direction: from the composition/intersection, can the model identify the concepts/rules separately? Importantly, this direction is less straightforward. Rule extrapolation allows for easy study of compositional generalisation ability on a variety of datasets, such as formal or programming languages. Therefore rule extrapolation has the potential to become an established benchmark task for evaluating current and future LM architectures. ", "page_idx": 9}, {"type": "text", "text": "Limitations. We defined and empirically evaluated rule extrapolation in simple formal languages, where analysis is tractable and demonstrates that models can \u201cgo beyond\" their training data. We acknowledge that our data sets are far from natural language where rule extrapolation may be difficult to demonstrate. Studying formal languages may still have practical relevance, e. g. for programming languages or formal mathematics. Even though we considered different hyperparameter setups presented in the appendix, we have not performed exhaustive ablations over the hyperparameters or analysis of architectures. Furthermore, model variants, like different attention or positional encoding, may impact our findings. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors would like to thank Gergely Flamich for several inspiring discussions on Solomonoff induction, Bence Ny\u00e9ki for insights on practical aspects of natural language processing and Gail Weiss for her insights on PCFGs. This work was supported by a Turing AI World-Leading Researcher Fellowship G111021. Patrik Reizinger acknowledges his membership in the European Laboratory for Learning and Intelligent Systems (ELLIS) PhD program and thanks the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for its support. This work was supported by the German Federal Ministry of Education and Research (BMBF): T\u00fcbingen AI Center, FKZ: 01IS18039A. Wieland Brendel acknowledges financial support via an Emmy Noether Grant funded by the German Research Foundation (DFG) under grant no. BR 6382/1-1 and via the Open Philantropy Foundation funded by the Good Ventures Foundation. Wieland Brendel is a member of the Machine Learning Cluster of Excellence, EXC number 2064/1 \u2013 Project number 390727645. This research utilized compute resources at the T\u00fcbingen Machine Learning Cloud, DFG FKZ INST 37/1057-1 FUGG. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "K. Ahuja and A. Mansouri. On Provable Length and Compositional Generalization, Feb. 2024. URL http://arxiv.org/abs/2402.04875. arXiv:2402.04875 [cs, stat]. 1, 3   \nM. Beck, K. P\u00f6ppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter. xLSTM: Extended Long Short-Term Memory, May 2024. URL http:// arxiv.org/abs/2405.04517. arXiv:2405.04517 [cs, stat]. 4, 16   \nJ. Brady, R. S. Zimmermann, Y. Sharma, B. Sch\u00f6lkopf, J. von K\u00fcgelgen, and W. Brendel. Provably Learning Object-Centric Representations, May 2023. URL http://arxiv.org/abs/2305. 14229. arXiv:2305.14229 [cs]. 3   \nS. Chan, A. Santoro, A. Lampinen, J. Wang, A. Singh, P. Richemond, J. McClelland, and F. Hill. Data distributional properties drive emergent in-context learning in transformers. Advances in Neural Information Processing Systems, 35:18878\u201318891, 2022. 4   \nA. Chen, R. Shwartz-Ziv, K. Cho, M. L. Leavitt, and N. Saphra. Sudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in mlms, 2024. 4   \nN. Chomsky. Three models for the description of language. IRE Transactions on Information Theory, 2(3):113\u2013124, 1956. doi: 10.1109/TIT.1956.1056813. 3, 4   \nG. Deletang, A. Ruoss, J. Grau-Moya, T. Genewein, L. K. Wenliang, E. Catt, C. Cundy, M. Hutter, S. Legg, J. Veness, and P. A. Ortega. Neural Networks and the Chomsky Hierarchy. Sept. 2022. URL https://openreview.net/forum?id=WbxHAzkeQcn. 3   \nK. Dingle, C. Q. Camargo, and A. A. Louis. Input\u2013output maps are strongly biased towards simple outputs. Nature Communications, 9(1):761, Feb. 2018. ISSN 2041-1723. doi: 10.1038/ s41467-018-03101-6. URL https://www.nature.com/articles/s41467-018-03101-6. Number: 1 Publisher: Nature Publishing Group. 4   \nN. Dziri, X. Lu, M. Sclar, X. L. Li, L. Jiang, B. Y. Lin, P. West, C. Bhagavatula, R. L. Bras, J. D. Hwang, S. Sanyal, S. Welleck, X. Ren, A. Ettinger, Z. Harchaoui, and Y. Choi. Faith and fate: Limits of transformers on compositionality, 2023. 3   \nG. K. Dziugaite and D. M. Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data, 2017. 3   \nP. L. Falcon. Pytorch lightning: Accelerated research from prototypes to production. https: //github.com/PyTorchLightning/pytorch-lightning, 2019. 16   \nM. Goldblum, M. Finzi, K. Rowan, and A. G. Wilson. The no free lunch theorem, kolmogorov complexity, and the role of inductive biases in machine learning, 2023. 4, 7, 10   \nJ. Grau-Moya, T. Genewein, M. Hutter, L. Orseau, G. Del\u00e9tang, E. Catt, A. Ruoss, L. K. Wenliang, C. Mattern, M. Aitchison, and J. Veness. Learning Universal Predictors, Jan. 2024. URL http: //arxiv.org/abs/2401.14953. arXiv:2401.14953 [cs]. 7, 10, 22 A. Gu and T. Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces, Dec. 2023. URL http://arxiv.org/abs/2312.00752. arXiv:2312.00752 [cs]. 4 S. Han and S. Pad\u00f3. Towards Understanding the Relationship between In-context Learning and Compositional Generalization, Mar. 2024. URL http://arxiv.org/abs/2403.11834. arXiv:2403.11834 [cs]. 1, 3 S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780,   \n1997. 4 M. Hutter. Universal Artificial Intelligence. Texts in Theoretical Computer Science. An EATCS Series. Springer, Berlin and Heidelberg, 2005. ISBN 978-3-540-22139-5. doi: 10.1007/b138233.   \n8, 9, 21 M. Hutter. Universal learning theory. 02 2011. doi: 10.1007/978-0-387-30164-8_861. 8, 21, 22 S. Lachapelle, D. Mahajan, I. Mitliagkas, and S. Lacoste-Julien. Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation, July 2023. URL http://arxiv. org/abs/2307.02598. arXiv:2307.02598 [cs, stat]. 3 B. M. Lake and M. Baroni. Human-like systematic generalization through a meta-learning neural network. Nature, 623(7985):115\u2013121, Nov. 2023. ISSN 1476-4687. doi: 10.1038/ s41586-023-06668-3. URL https://www.nature.com/articles/s41586-023-06668-3. Publisher: Nature Publishing Group. 1, 3 A. T. LeGuet. Mamba repository. URL https://github.com/alxndrTL/mamba.py. 16 M. Li and P. Vit\u00e1nyi. An Introduction to Kolmogorov Complexity and Its Applications. Springer,   \n1997. URL http://books.google.com/books?vid $=$ ISBN0387948686. 2, 7, 8, 21, 22 M. Li, X. Davies, and M. Nadeau. Circuit breaking: Removing model behaviors with targeted ablation, 2024. 4 H. Liu, S. M. Xie, Z. Li, and T. Ma. Same pre-training loss, better downstream: Implicit bias matters for language models. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 22188\u201322214. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/liu23ao.html. 1, 5 R. T. McCoy, R. Frank, and T. Linzen. Does syntax need to grow on trees? sources of hierarchical inductive bias in sequence-to-sequence networks. Transactions of the Association for Computational Linguistics, 8:125\u2013140, 2020. doi: 10.1162/tacl_a_00304. URL https://aclanthology.org/2020.tacl-1.9. 4 K. Meng, D. Bau, A. Andonian, and Y. Belinkov. Locating and editing factual associations in gpt,   \n2023. 4 C. Mingard, G. Valle-P\u00e9rez, J. Skalse, and A. A. Louis. Is SGD a Bayesian sampler? Well, almost, Oct. 2020. URL http://arxiv.org/abs/2006.15191. arXiv:2006.15191 [cs, stat]. 4 S. Murty, P. Sharma, J. Andreas, and C. Manning. Grokking of hierarchical structure in vanilla transformers. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 439\u2013448, Toronto, Canada, July 2023. Association for Computational Linguistics. doi:   \n10.18653/v1/2023.acl-short.38. URL https://aclanthology.org/2023.acl-short.38. 4 R. Nogueira, Z. Jiang, and J. Lin. Investigating the limitations of transformers with simple arithmetic tasks, 2021. 3 C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. In-context learning and induction heads, 2022. 4   \nA. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. K\u00f6pf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019. 16   \nO. Press, N. A. Smith, and O. Levy. Improving Transformer Models by Reordering their Sublayers. In D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2996\u20133005, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.270. URL https: //aclanthology.org/2020.acl-main.270. 4   \nM. P\u00e9rez-Ortiz, O. Rivasplata, J. Shawe-Taylor, and C. Szepesv\u00e1ri. Tighter risk certificates for neural networks, 2021. 3   \nS. Rajamanoharan, A. Conmy, L. Smith, T. Lieberum, V. Varma, J. Kram\u00e1r, R. Shah, and N. Nanda. Improving dictionary learning with gated sparse autoencoders, 2024. 5   \nR. Ramesh, E. S. Lubana, M. Khona, R. P. Dick, and H. Tanaka. Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks, Feb. 2024. URL http://arxiv.org/abs/2311.12997. arXiv:2311.12997 [cs]. 1, 3   \nP. Reizinger, S. Ujv\u00e1ry, A. M\u00e9sz\u00e1ros, A. Kerekes, W. Brendel, and F. Husz\u00e1r. Llm non-identifiability repository. URL https://github.com/rpatrik96/llm-non-identifiability. 16   \nP. Reizinger, S. Ujv\u00e1ry, A. M\u00e9sz\u00e1ros, A. Kerekes, W. Brendel, and F. Husz\u00e1r. Understanding llms requires more than statistical generalization, 2024. 1, 2, 3, 4, 5   \nA. Ruoss, G. Del\u00e9tang, T. Genewein, J. Grau-Moya, R. Csord\u00e1s, M. Bennani, S. Legg, and J. Veness. Randomized Positional Encodings Boost Length Generalization of Transformers, May 2023. URL http://arxiv.org/abs/2305.16843. arXiv:2305.16843 [cs, stat]. 3   \nA. Saparov, R. Y. Pang, V. Padmakumar, N. Joshi, S. M. Kazemi, N. Kim, and H. He. Testing the general deductive reasoning capacity of large language models using ood examples, 2023. 3   \nL. Schott, J. von K\u00fcgelgen, F. Tr\u00e4uble, P. Gehler, C. Russell, M. Bethge, B. Sch\u00f6lkopf, F. Locatello, and W. Brendel. Visual Representation Learning Does Not Generalize Strongly Within the Same Domain. arXiv:2107.08221 [cs], July 2021. URL http://arxiv.org/abs/2107.08221. arXiv: 2107.08221. 3   \nR. J. Solomonoff. A preliminary report on a general theory of inductive inference. 2001. URL https://api.semanticscholar.org/CorpusID:17118014. 2, 7, 8   \nL. Theis. What makes an image realistic?, 2024. 7   \nA. M. Turing. On computable numbers, with an application to the Entscheidungsproblem. Proceedings of the London Mathematical Society, 2(42):230\u2013265, 1936. URL http://www.cs. helsinki.fi/u/gionis/cc05/OnComputableNumbers.pdf. 22   \nG. Valle-P\u00e9rez, C. Q. Camargo, and A. A. Louis. Deep learning generalizes because the parameterfunction map is biased towards simple functions, Apr. 2019. URL http://arxiv.org/abs/ 1805.08522. arXiv:1805.08522 [cs, stat]. 4   \nV. Vapnik and A. Y. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability & Its Applications, 16(2):264, 1971. 3   \nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need, 2023. 4   \nG. Weiss, Y. Goldberg, and E. Yahav. Thinking Like Transformers, July 2021. URL http://arxiv. org/abs/2106.06981. arXiv:2106.06981 [cs]. 4   \nT. Wiedemer, J. Brady, A. Panfilov, A. Juhos, M. Bethge, and W. Brendel. Provable Compositional Generalization for Object-Centric Learning, Oct. 2023a. URL http://arxiv.org/abs/2310. 05327. arXiv:2310.05327 [cs]. 3   \nT. Wiedemer, P. Mayilvahanan, M. Bethge, and W. Brendel. Compositional Generalization from First Principles, July 2023b. URL http://arxiv.org/abs/2307.05596. arXiv:2307.05596 [cs, stat]. 3   \nT. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush. Huggingface\u2019s transformers: State-of-the-art natural language processing, 2020. 16   \nT. Yang, Y. Wang, C. Lan, Y. Lu, and N. Zheng. Vector-based Representation is the Key: A Study on Disentanglement and Compositional Generalization, May 2023. URL http://arxiv.org/abs/ 2305.18063. arXiv:2305.18063 [cs]. 3   \nH. Zhou, A. Bradley, E. Littwin, N. Razin, O. Saremi, J. Susskind, S. Bengio, and P. Nakkiran. What Algorithms can Transformers Learn? A Study in Length Generalization, Oct. 2023. URL http://arxiv.org/abs/2310.16028. arXiv:2310.16028 [cs, stat]. 3, 4, 5, 6 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "image", "img_path": "Li2rpRZWjy/tmp/af4c7e9b2cf9c02799e01ca149573bed32ba696b00daa56999a869c5fde5acfc.jpg", "img_caption": ["Figure 4: Training dynamics of the LSTM Training an LSTM on the $a^{n}b^{n}$ language, the normalized probability of all sequences, grouped into the four categories (satisfying (R1) and (R2), only (R1), only (R2) and neither) of length 8 consisting of $a$ \u2019s and $b$ \u2019s and ending with EOS is plotted during training. The sequences are separated according to which rule they obey. At initialization, sequences obeying any of the rules have low probability. During training, the model first starts assigning higher probabilities to sequences satisfying (R2), but soon after, sequences in (R1) $\\cap$ (R2) dominate. After training the most likely sequences are the ones in (R1) $\\cap$ (R2), the others are negligible. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "Li2rpRZWjy/tmp/56a555c9cfe184a400062d6b4d14ea735701287aeef7440a165edbc13a4dd75b.jpg", "img_caption": ["Figure 5: Training dynamics of Mamba Training a Mamba architecture on the $a^{n}b^{n}$ language, the normalized probability of all sequences, grouped into the four categories (satisfying (R1) and (R2), only (R1), only (R2) and neither) of length 8 consisting of $a$ \u2019s and $b$ \u2019s and ending with EOS is plotted during training. The sequences are separated according to which rule they obey. Intriguingly, at initialization, sequences obeying (R2) are assigned largest probability. During training, the model learns (R1) $\\cap$ (R2) consistently after 3000 epochs. After training the most likely sequences are the ones in (R1) $\\cap$ (R2), the others are negligible. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Experimental details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Reproducibility and codebase. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We use PyTorch [Paszke et al., 2019], PyTorch Lightning [Falcon, 2019], and HuggingFace Transformers [Wolf et al., 2020]. Our training pipeline builds on [Reizinger et al.] and we use the PyTorch implementation of Mamba from [LeGuet] and the code released by the authors for the xLSTM [Beck et al., 2024]. Our code and experimental logs are publicly available at https://github.com/meszarosanna/rule_extrapolation. ", "page_idx": 15}, {"type": "text", "text": "B.2 Formal grammars ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Training data. We generate data from the formal languages $L_{1}$ $\\phantom{}_{1},\\,L_{2},\\,L_{3}$ , $L_{4}$ and $L_{5}$ described in $\\S\\ 3.2$ up to length 256\u2014excluding the SOS and EOS tokens, i.e., those two tokens add two to the maximal length. The SOS (0), EOS (1), and padding (2) tokens are always represented by these numbers. When the grammar consists of letters, their representations are $a$ (3), $b$ (4) and $c$ (5), and when the language is the nested brackets and parentheses the tokens are the $'('\\left(3\\right),')^{\\prime}$ (4), $^\\prime\\big[^{\\prime}$ (5) and $^{\\prime}]^{\\prime}$ (6). ", "page_idx": 15}, {"type": "text", "text": "We used different data set sizes for the different languages. This is explained by the highly different size of all possible sequences that obey all rules of any language. For the languages, $L_{1},\\,L_{2}$ the training set consists of 15000 samples (as these languages have rules satisfied by many sequences), and for $L_{4}$ , 512 samples. For $L_{3}$ and $L_{5}$ , the corresponding data sets include all unique sequences up to length 256, which is 128 for $L_{3}$ and 85 for $L_{5}$ , respectively. ", "page_idx": 15}, {"type": "text", "text": "Test prompts. We define our test prompts as all possible sequences of length 8 (prepended with SOS) for $L_{1}$ and $L_{3}$ , and all possible sequences of length 5 (prepended with SOS) for ${\\cal L}_{5}=\\{a^{n}b^{n}c^{n}:$ $n>0\\}$ \u2014we chose different lengths to have a comparable number of test samples, i.e., $2^{8}$ and $5^{3}$ , respectively. We split these sets into in-distribution and OOD test prompts, based on whether they can be completed to obey the rules of the specific grammar. ", "page_idx": 15}, {"type": "text", "text": "For $L_{2}$ , first, we generate in-distribution test prompts of length 8\u2014these can be completed according to the grammar rules by definition. From these, we create the OOD prompts by adding a single $a$ to the beginning of the sequences. For $L_{4}$ , we sample length-6 sequences obeying both rules, then the ID prompts are prepended with \u2032( [\u2032 and the OOD prompts with \u2032) [\u2032. Then the prompts are prepended with SOS. ", "page_idx": 15}, {"type": "text", "text": "B.3 Model and training parameters ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We observed that the Linear model constantly predicts PAD tokens, unless we ignore those by setting the ignore_index $\\tt=P A D$ in torch.nn.CrossEntropyLoss(). However, for comparison, when reporting the losses, we report the loss where we do not set the ignore_index parameter. ", "page_idx": 15}, {"type": "table", "img_path": "Li2rpRZWjy/tmp/017f8326011ceb2272a86d54028b244121ee58316422b6d6ce8d22ec700ee7c1.jpg", "table_caption": ["Table 8: General parameters "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "Li2rpRZWjy/tmp/5e2caf0a301f3299f3981cc6d22c98265b6de696e3376a41fc59295ca537ab02.jpg", "table_caption": ["Table 9: Linear model parameters "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 10: LSTM parameters ", "page_idx": 16}, {"type": "table", "img_path": "Li2rpRZWjy/tmp/5a22261036b233b8629c77ebfac220c5c36db68c3837c48a4d2700578ee20458.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "Li2rpRZWjy/tmp/7841dbdfd2dbb4d8b5ca4ba48bec395275d6c707b2b1950bd69a47c21529c879.jpg", "table_caption": ["Table 11: Transformer parameters "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "Li2rpRZWjy/tmp/75c36f8bc8e5bd02ce3fb174f9161af0be56a69a5c7ff998986be70d6dcda0bf.jpg", "table_caption": ["Table 12: Mamba parameters "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "Li2rpRZWjy/tmp/32b1121bfcd84aebd02bd0d72658d22ebdce1f58854fa7b0f2e527578a6809cd.jpg", "table_caption": ["Table 13: xLSTM parameters3 "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.4 Training dynamics plot generation details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Figure 3 was plotted on the $a^{n}b^{n}$ language with the Transformer on seed 63656. The left left was plotted at the Lightning module\u2019s \"self.global_step ${\\it=}0\"$ , left middle at \"self.current_epoch $=600\"$ and left middle at nearly end of the training at \"self.current_epoch $\\mathrm{=}9700^{\\circ}$ . On the right, ", "page_idx": 16}, {"type": "text", "text": "the sum of the probabilities was computed at every epoch divisible by 100. Similarly, Figure 4 was plotted on $a^{n}b^{n}$ with LSTM with seed 8556, and Figure 5 on $a^{n}b^{n}$ with Mamba with seed 91686. ", "page_idx": 17}, {"type": "text", "text": "B.5 Additional experimental results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Greedy decoding vs sampling. Our initial results use greedy decoding, but we conducted experiments to evaluate the sampling method for next token prediction. As shown in Fig. 6, we conclude that while the the Transformer is the best choice with greedy decoding, except for regular languages where the LSTM performs better (Fig. 6a); the LSTM appears to excel when using sampling (Fig. 6b). These results also open up new interesting future directions, e.g., investigating the influence of different temperature values in the softmax. ", "page_idx": 17}, {"type": "image", "img_path": "Li2rpRZWjy/tmp/0cec34dae1cd7b20c5c38a5f1c58ccc3ccce653e528d44b08f422da55ff05c19.jpg", "img_caption": ["Figure 6: Rule extrapolation summary for all models but the xLSTM and languages $L_{1}-L_{5}$ (Tab. 1) with greedy (Fig. 6a) and sampling (Fig. 6b) next-token decoding "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "Li2rpRZWjy/tmp/261b12a60afc8112d4bb8d9df95b515c78e44d4aef1a8fc9b1467998fbdb05cf.jpg", "img_caption": ["Figure 7: Rule extrapolation performance for different optimizers and learning rates for all models except the xLSTM and languages $L_{1}-L_{5}$ (Tab. 1): top row left is the same as Fig. 6a "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Hyperparameter sensitivity. We tested multiple hyperparameters, including three learning rates and two optimization algorithms, and plotted the results in Fig. 7. Though our hyperparameter search is not exhaustive, we can state that when considering the best settings for each architecture, the LSTM consistently performs best on regular languages, while the Transformer excels on everything else. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Model size ablation. We tested varying size settings (different numbers of layers and heads) for the Transformer architecture to determine whether increasing size can improve performance on regular languages. As shown in Fig. 8, increasing the Transformer model size does not meaningfully improve performance on regular languages; the best values remain those originally used $\\scriptstyle\\mathrm{(num\\_1ayers\\;=\\;}$ 7, num_heads $=5$ ). For non-regular languages, the Transformer already outperformed the other architectures. ", "page_idx": 18}, {"type": "image", "img_path": "Li2rpRZWjy/tmp/6efcf91217a196ce06bb5786a5f1b4a003581b5c563fdcb67ecf56060e1a7866.jpg", "img_caption": ["Figure 8: Rule extrapolation performance for different number of attention heads and decoder layers in the Transformer for languages $L_{1}-L_{5}$ (Tab. 1) "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.6 Human pilot study details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We conducted a small pilot study with humans using an online questionnaire (the study size was 14).   \nWe did not collect any personal information, only task-relevant responses. ", "page_idx": 18}, {"type": "text", "text": "Instructions. The participants received the following instruction: ", "page_idx": 18}, {"type": "text", "text": "This questionnaire asks you to perform a task of completing sequences based on examples for 2 cases. Each case follows the same layout: ", "page_idx": 18}, {"type": "text", "text": "\u2022 first, we show some example sequences \u2022 then on, we start sequences and ask you to finish them as you see fit ", "page_idx": 18}, {"type": "text", "text": "Then, on the three following pages of the questionnaire, they were presented the following: We generate sequences according to some patterns. You see below examples, which are considered completed (whitespace is only for visibility reasons):   \n[Examples came here in the questionnaire; detailed below]   \nNow you will see 5 incomplete messages. What you see are the first characters of sequences of unknown length. Your task is to finish them.   \nWhen writing down your answer: ", "page_idx": 18}, {"type": "text", "text": "\u2022 DO NOT include the beginning of the sequence already provided, only your completion of it. \u2022 The length of your answer is up to you, choose what you see fit. \u2022 If you think the sequence is already completed, leave the space for the completion empty ", "page_idx": 18}, {"type": "text", "text": "Case 1: regular grammar $L_{1}$ . The examples for the context-free grammar $L_{1}$ were: ", "page_idx": 18}, {"type": "text", "text": "\u2022 baba \u2022 baa \u2022 babbaaa ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "The prompts the participants needed to complete were: ", "page_idx": 19}, {"type": "text", "text": "\u2022 aba \u2022 abba \u2022 abaaba \u2022 abab \u2022 aaba ", "page_idx": 19}, {"type": "text", "text": "Case 2: context-free grammar $L_{3}$ . The examples for the context-free grammar $L_{3}$ were: ", "page_idx": 19}, {"type": "text", "text": "\u2022 aabb   \n\u2022 aaabbb   \n\u2022 aaaaaabbbbbb ", "page_idx": 19}, {"type": "text", "text": "The prompts the participants needed to complete were: ", "page_idx": 19}, {"type": "text", "text": "\u2022 baa \u2022 abaa \u2022 bba \u2022 baab ", "page_idx": 19}, {"type": "text", "text": "Results. We preprocessed the questionnaire results to remove invalid responses (e.g., those with invalid characters, where we assumed that we did not explain the task well to the study subjects). We report the OOD (R1) and (R2) accuracies, the latter only on the completion in Tab. 14. ", "page_idx": 19}, {"type": "table", "img_path": "Li2rpRZWjy/tmp/a81179d2dab7feac05882205f0e6d8edea0417d6a15020c8febac5e39f392d4d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 14: Human pilot study OOD accuracies: humans in our study performed better than chance, though they could not beat the LSTM on $L_{1}$ and the Transformer on $L_{3}$ ", "page_idx": 19}, {"type": "text", "text": "B.7 Computational requirements ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Our models and data sets are small scale and were designed to fit into an NVIDIA GeForce RTX 2080 Ti with 11GB VRAM, this guided our parameter choices (Appx. B.3). As we used SLURM and Condor managed clusters, our experiments were, due to GPU availability, in some cases, allocated on NVIDIA A100 GPUs. Although in the paper we report statistics over 5 seeds, in some cases, we ran more experiments during the lifetime of the project. For transparency, we report overall numbers, given in GPU hours for each synthetic grammar (Tab. 1). The runtimes differed based on model architecture, data set size, and the stochasticity of the training (i.e., the use of early stopping) ", "page_idx": 19}, {"type": "text", "text": "\u2022 $L_{1}:1{,}455$ GPU hours for 107 runs \u2022 $L_{2}:707$ GPU hours for 59 runs \u2022 $L_{3}:301$ GPU hours for 334 runs \u2022 $L_{4}:269$ GPU hours for 208 runs \u2022 $L_{5}:264.5$ GPU hours for 65 runs, ", "page_idx": 19}, {"type": "text", "text": "which amounts to approximately 3,000 GPU hours and also includes the GPU hours required for creating the figures (Fig. 3). ", "page_idx": 19}, {"type": "text", "text": "To estimate the energy consumption, we take the maximum power consumption of an NVIDIA A100 (PCIe version), which is $250\\bar{\\mathbf{W}}^{4}$ . This amounts to approximately 750kWh, which is equivalent to the emission of 0.313 metric ton $\\mathrm{{CO}_{2}}$ , i.e., approximately 1290 kilometres driven by an average gasoline-powered passenger vehicle5. ", "page_idx": 19}, {"type": "text", "text": "C Details on the normative theory of OOD extrapolation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1 The set of joints and conditional factorizations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we denote sets of probability distributions as $D$ and use subscripts $J$ and $C$ to refer to joint and conditional distributions, respectively. Consider the set of joint probabilities on $N$ \u2212length sequences, where the $p_{i}$ are drawn from some set $S$ . For example, $S\\ =\\ \\{p_{i}\\ :$ is computable w.r.t. a $\\mathrm{UTM}\\}$ . ", "page_idx": 20}, {"type": "equation", "text": "$$\nD_{J,N}=\\{p_{i}(x_{1},x_{2},...,x_{N}),\\ p_{i}\\in S\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and the set of conditional factorizations consistent i.e., such conditionals that the joint equals the product of the conditionals, with them ", "page_idx": 20}, {"type": "equation", "text": "$$\nD_{C,N}=\\{\\{p_{i}|(x_{k}\\mid x_{1}^{k-1})\\}_{k=1}^{N},\\ {\\mathrm{consistent}}\\,{\\mathrm{with~elements~of}}\\,S\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We claim that $D_{J,N}\\,\\subset\\,D_{C,N}$ . To see that $D_{J,N}\\,\\subseteq\\,D_{C,N}$ , note that the list $\\{p_{i|}(x_{k}\\mid x_{1}^{k-1})\\}_{k=1}^{N}$ uniquely determines $p_{i}(x_{1},x_{2},...,x_{N})$ as the product of its elements. To see that the sets are not equal, consider the following example. ", "page_idx": 20}, {"type": "text", "text": "Example. Let $X_{1}$ and $X_{2}$ be two binary random variables. Let us define a probability mass function (pmf) $p$ such that $p(X_{1}=0)=0$ and $p(X_{1}=0,X_{2}=0)=p(X_{1}=\\mathbf{\\bar{0}},X_{2}=\\mathbf{\\bar{1}})=0$ Now consider two sets of conditional pmfs $q_{1}$ and $q_{2}$ , satisfying ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{q_{1}(X_{2}=x\\mid X_{1}=1)=q_{2}(X_{2}=x\\mid X_{1}=1)=p(X_{2}=x\\mid X_{1}=1)=p(X_{2}=x),}}\\\\ {{{}}}\\\\ {{q_{1}(X_{1}=1)=q_{2}(X_{1}=1)=1,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "but ", "page_idx": 20}, {"type": "equation", "text": "$$\nq_{1}(X_{2}=0\\mid X_{1}=0)=1\\;\\mathrm{and}\\;q_{2}(X_{2}=0\\mid X_{1}=0)=0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Due to the first two equations, both $q_{1}$ and $q_{2}$ are consistent with $p$ , but they can differ on the zero-probability prompt $X_{1}=0$ . ", "page_idx": 20}, {"type": "text", "text": "Hence the set corresponding to $D_{C,N}$ is larger, and the extra elements correspond to the zeroprobability sequences under each $p_{i}$ . These are precisely the prompts on which we assess rule extrapolation. ", "page_idx": 20}, {"type": "text", "text": "The lists of conditionals notation. In $\\S\\ S$ , we distinguish between the joint probability representation $p_{k}:=\\{p_{k}(x_{1}^{N})\\}$ and the lists of conditionals representation $p_{i|}$ . Let $\\phi$ denote the mapping from lists of conditionals to the joint probabilities. Consider the set of pre-images of $p_{k}$ under $\\phi$ , i.e., $\\phi^{-1}(p_{k})$ , which has cardinality $\\bar{|\\phi^{-1}(p_{k})|}$ . If this set has multiple elements, we can enumerate them Ians $\\{p_{k|,j}\\}_{j=1}^{|\\phi^{-1}(p_{k})|}$ ,i twh $p_{k|,j}:=\\{p_{k,j}(x_{k}\\mid x_{1}^{k-1})\\}_{k=1}^{N}\\}$ i, s wuhnedree $p_{k,j}$ dis  ttoh le $j^{t h}$ oelveemr ealnlt  porf $\\phi^{-1}(p_{k})$ $p_{R}$ $p_{i|}$ $i$ $\\{p_{i|}\\}_{i}\\equiv\\{\\{p_{k|,j}\\}_{j=1}^{|\\phi^{-1}(p_{k})|}\\}_{k}$ ,j}|j\u03d5=\u221211(pk)|}k, where the enumerations over (k, j) are combined into an enumeration over $i$ in a dovetail fashion. Index $k$ loops over the joint probability distributions, and $j$ loops through each of their pre-images. Note that this is a different enumeration than the one in the Solomonoff prior $p_{S}$ , where only the joint probabilities are enumerated (here with index $k$ ). ", "page_idx": 20}, {"type": "text", "text": "C.2 Solomonoff Induction ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "This section has been adapted from Li and Vit\u00e1nyi [1997], Hutter [2005] and Hutter [2011]. ", "page_idx": 20}, {"type": "text", "text": "Epicure\u2019s principle states that if more than one theory is consistent with the observations, one should keep all the theories. The Solomonoff prior follows this principle in including all (lower semicomputable) semimeasures in the prior. ", "page_idx": 20}, {"type": "text", "text": "Occam\u2019s razor states to keep the simplest theory consistent with the observations. The Solomonoff prior follows this in assigning larger probabilities to algorithmically more complex strings. ", "page_idx": 20}, {"type": "text", "text": "Definition C.1 (Prefix code). A prefix code $P$ is a set of binary strings such that no element is proper prefix of another. It satisfies Kraft\u2019s inequality $\\begin{array}{r}{\\sum_{p\\in P}2^{-l(p)}\\stackrel{\\cdot}{\\leq}1}\\end{array}$ . ", "page_idx": 20}, {"type": "text", "text": "Turing machines. A Turing machine can be thought of as an idealised form of a computer. Informally, it consists of tapes, read/write heads, a table of rules and an internal state. There are multiple technical variants of Turing machines. Here, we define prefix Turing machines.6 ", "page_idx": 21}, {"type": "text", "text": "Definition C.2 (Prefix Turing Machine). A prefix Turing machine $T$ is a Turing machine with one unidirectional (i.e. the head can only move from left to right) input tape, one unidirectional output tape, and some bidirectional work tapes. Input tapes are read only, output tapes are write only. All tapes are are binary (no blank), work tapes are initially filled with zeros. ", "page_idx": 21}, {"type": "text", "text": "We say that $T$ halts on input $p$ with output $x$ , and write $T(p)=x$ if $p$ is to the left of the input head and $x$ is to the left of the output head after $T$ halts. The set of $p$ on which $T$ halts forms a prefix code. We call such codes $p$ self-delimiting programs. The Turing machine may take another input $y$ on its input tape. Since $T$ is a prefix Turing machine, $y$ needs to be prefix encoded, denoted as $y^{\\circ}$ , and then concatenated to the program $p$ . In this case, we say $T(y^{\\leftarrow}p)=x$ . ", "page_idx": 21}, {"type": "text", "text": "The table of rules of a Turing machine $T$ can be encoded as a binary string, which we denote by $\\langle T\\rangle$ . Hence the set of Turing machines $\\{T_{1},T_{2},\\dots\\}$ can be enumerated (computably). We will use this property when we sum over Turing machines. ", "page_idx": 21}, {"type": "text", "text": "Universal Turing Machines. There are so-called universal Turing machines, which can \u201csimulate\u201d all Turing machines. We define a particular one which simulates a prefix Turing machine $T(q)$ if fed with input $\\langle T\\rangle q$ , i.e. $U(\\langle T\\rangle q)=T(q)\\;\\forall T,q$ . If $p$ is not of the form $\\langle T\\rangle q,U(p)$ does not output anything. We call this particular $U$ the reference universal Turing machine. ", "page_idx": 21}, {"type": "text", "text": "Semimeasures. Let $\\varkappa^{*}$ be the set of finite strings and $\\chi\\infty$ be the set of infinite sequences over some alphabet $\\mathcal{X}$ of size $|{\\mathcal{X}}|$ . Recall our sequence notation from $\\S\\ S$ : for a string $(x_{1},x_{2},...,x_{n})\\in\\mathcal{X}^{*}$ of length $n$ we write use the shorthand $x_{1}^{n}$ with $x_{i}\\in\\mathcal{X}\\quad\\forall i\\in\\{1,2,\\dots,n\\}$ . ", "page_idx": 21}, {"type": "text", "text": "Definition C.3 (Semimeasure). Let $\\epsilon$ denote the empty string. A function $\\mu\\,:\\,\\mathcal{X}^{*}\\,\\to\\,\\mathbb{R}$ is a semimeasure if for all $x\\in\\;\\mathcal{X}^{*}$ , $\\mu(\\varepsilon)\\,\\leq\\,1$ , and $\\begin{array}{r}{\\mu(\\dot{x})\\stackrel{}{{}=}\\sum_{b\\in\\mathcal{X}}\\mu(x b)}\\end{array}$ , where $x b$ denotes the concatenation of $x$ and $b$ , also an element of $\\boldsymbol{\\mathcal{X}}^{*}$ . If equalities hold, $\\mu$ is called a probability measure. ", "page_idx": 21}, {"type": "text", "text": "Remark C.1. $p_{S}$ and $p_{R}\\left(\\S\\right.5\\right)$ are semimeasures, because $\\begin{array}{r}{\\sum_{x_{1}^{n}}p_{S}(x_{1},x_{2},...,x_{n})<1}\\end{array}$ . The fact that the integral is less than 1 is due to the halting problem of UTMs [Turing, 1936], which means that there are some programs in the sum that never stop running. ", "page_idx": 21}, {"type": "text", "text": "Definition C.4 (Lower semicomputability). A function $f:\\mathbb{N}\\to\\mathbb{R}$ is lower semicomputable iff there exists a computable function $\\phi(x,k):\\mathbb{Q}\\times\\mathbb{N}\\to\\mathbb{Q}$ , such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ \\operatorname*{lim}_{k\\to\\infty}\\phi(x,k)=f(x)}\\\\ &{\\bullet\\ \\forall k\\in\\mathbb{N}:\\phi(x,k+1)\\geq\\phi(x,k).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "i.e, if it can be approximated from below to arbitrary precision. ", "page_idx": 21}, {"type": "text", "text": "Kolmogorov complexity. Kolmogorov complexity measures the complexity of an object as the length of the shortest program that generates the object. There is also a conditional version, based on the length of programs that input some other objects. ", "page_idx": 21}, {"type": "text", "text": "Definition C.5 ((Conditional) prefix Kolmogorov complexity). The (conditional) prefix Kolmogorov complexity of a string $x$ is the length $l$ of the shortest halting program $p$ for which $U$ outputs $x$ (given $y$ ): ", "page_idx": 21}, {"type": "equation", "text": "$$\nK(x):=\\operatorname*{min}_{p}\\{l(p):U(p)=x{\\mathrm{~halts}}\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\nK(x|y):=\\operatorname*{min}_{p}\\{l(p):U(y^{\\bullet}p)=x\\operatorname{halts}\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The Kolmogorov complexity of a semimeasure, $p_{i}(x_{1}^{n})$ , is understood to be the length of the shortest self-delimiting program on $U$ , computing $p_{i}(x_{1}^{n})$ given $x_{1}^{n}$ , for every $x_{1}^{n}$ . ", "page_idx": 21}, {"type": "text", "text": "D Acronyms ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "AR LM autoregressive language model   \nCS context-sensitive   \nEOS end-of-sequence   \ni.i.d. independent and identically distributed   \nICL in-context learning   \nLM language model   \nNLP Natural Language Processing   \nOOD out-of-distribution   \nRASP Restricted-Access Sequence Processing Language   \nSOS start-of-sequence   \nSSM State Space Model ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We define the formal languages we use and the term rule extrapolation in Section 2; the detailed empirical findings can be found in Section 4; and the proposed normative theory is in Section 5. Furthermore, we clearly state the scope/ main limitation of the paper when we write \"We empirically evaluate different models\u2019 rule extrapolation in formal languages with varying complexity, we study linear, recurrent, Transformer and State Space models\" in the Introduction. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: There can be found a Limitations paragraph in Section 6, which clearly states the limitations of our datasets and the architectures analysed. As we did not propose new algorithms, concerns regarding their computational efficiency are not applicabble ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not contain novel theorems and proofs. However, this section of the checklist is still applicable, as $\\S\\ S$ proposes a novel prior inspired by the Solomonoff prior, and provides high-level intuition on why the prior is a suitable first-step for explaining OOD compositional generalization. $\\S\\ S$ and the corresponding Appx. C.2 provides all necessary background and definitions. For all results that are recalled from other sources, we provide references containing the proof (e.g. eq. (3)). ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: All experimental details can be found in Section 3 and Appendix B, including how the metrics were evaluated, how the datasets were generated, and the parameters of the architectures and algorithm we use. The experimental results can be found in Section 4. Furthermore, we upload our code and experimental logs as supplementary and make them publicly available upon acceptance. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We upload our code and experimental logs as supplementary and make them publicly available upon acceptance. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: All experimental details can be found in Section 3 and Appendix B, e.g. data generation, metrics, hyperparameters, type of architectures, type of the optimizer. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We report standard deviations in our tables and figures. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We report the experimental details, including our estimated energy consumption in Appx. B). ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our paper aims to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which, we feel, must be specifically highlighted here. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper presents work that aims to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The paper poses no such risks since our data sets are artificial and are far from real-world data sets. Moreover, the paper aims for deeper understanding, not for improvement. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We only use open source assets, i.e., code, which we properly cite in Appx. B. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: we release our codebase and experimental logs as supplementary material. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification:We included the details for our small pilot human questionnaire in Appx. B.   \nParticipant were not compensated. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Although our work includes a small human study, as it involved only a questionnaire participants could fill out whenever and wherever they wished, and their participation was fully voluntary, no potential risks were involved. Thus, no IRB approval, or equivalent, was necessart. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]