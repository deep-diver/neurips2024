[{"figure_path": "jXs6Cvpe7k/tables/tables_6_1.jpg", "caption": "Table 1: Attack success rate of RPO and baseline defenses on JailbreakBench. All prompts and responses are classified using Llama Guard. The RPO suffix is optimized on Llama-2-7B. RPO significantly outperforms baseline defenses for both open-source and closed-source models.", "description": "This table presents the attack success rates of different defense methods against various attacks on the JailbreakBench benchmark.  It compares the performance of Robust Prompt Optimization (RPO) against several baseline defense techniques (SmoothLLM, Perplexity Filter, Rephrasing, and Few-Shot) across multiple open-source and closed-source language models (Vicuna, Llama-2-7B, Qwen-7B, Llama2-13B, GPT-3.5, GPT-4). The results show RPO's superior performance in reducing attack success rates, especially against more challenging attacks such as PAIR and GCG.", "section": "4.1 Experimental Setup"}, {"figure_path": "jXs6Cvpe7k/tables/tables_7_1.jpg", "caption": "Table 2: Transfer attack success rate of RPO on the six highest performing attacks from HarmBench. Four of the attacks, AutoDAN, TAP, Few-Shot, and PAP, are not seen during optimization, requiring RPO to generalize to unknown attacks. RPO reduces ASR across all six attacks for all four models, including both open-source and closed-source models.", "description": "This table presents the attack success rate (ASR) of Robust Prompt Optimization (RPO) against six different attacks from the HarmBench dataset. Four of these attacks were unseen during RPO's optimization, evaluating its generalization capabilities. The results demonstrate RPO's effectiveness across various models (including open-source and closed-source), consistently reducing ASR compared to the baseline.", "section": "4.2 Main Results"}, {"figure_path": "jXs6Cvpe7k/tables/tables_8_1.jpg", "caption": "Table 1: Attack success rate of RPO and baseline defenses on JailbreakBench. All prompts and responses are classified using Llama Guard. The RPO suffix is optimized on Llama-2-7B. RPO significantly outperforms baseline defenses for both open-source and closed-source models.", "description": "This table presents the attack success rates of different defense methods against jailbreaking attacks on the JailbreakBench benchmark.  It compares the performance of Robust Prompt Optimization (RPO) against several baseline defenses (SmoothLLM, Perplexity Filter, Rephrasing) across various open-source and closed-source LLMs (Vicuna, Llama-2-7B, Qwen-7B, Llama2-13B, GPT-3.5, GPT-4).  The results show RPO's superior performance in reducing the attack success rate across all models tested, highlighting its effectiveness as a defense mechanism against jailbreaking.", "section": "4.1 Experimental Setup"}, {"figure_path": "jXs6Cvpe7k/tables/tables_8_2.jpg", "caption": "Table 4: General LM evaluations with RPO. We find a small performance reduction with benign use on MT-Bench and negligible reduction on MMLU.", "description": "This table presents the results of evaluating the performance of language models (LMs) with and without the Robust Prompt Optimization (RPO) technique on two benchmark datasets: MT-Bench and MMLU.  MT-Bench assesses multi-turn interaction capabilities, while MMLU evaluates domain knowledge. The table shows that RPO leads to a small decrease in performance on MT-Bench but has almost no impact on MMLU scores. This suggests that RPO, while enhancing robustness to adversarial attacks, does not significantly hinder the general capabilities of the models.", "section": "4 Experiments"}]