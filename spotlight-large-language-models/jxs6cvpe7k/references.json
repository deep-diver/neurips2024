{"references": [{"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper is foundational to the field of aligning LLMs with human preferences, a core topic of the present paper."}, {"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper introduced the concept of few-shot learning in LLMs, a technique relevant to the current paper's defense methods."}, {"fullname_first_author": "Nicholas Carlini", "paper_title": "Extracting training data from large language models", "publication_date": "2021-12-01", "reason": "This paper discusses a significant attack against LLMs that the current paper aims to defend against."}, {"fullname_first_author": "Zhexin Zhang", "paper_title": "Defending large language models against jailbreaking attacks through goal prioritization", "publication_date": "2023-11-01", "reason": "This is a contemporary work that addresses a similar problem with a different approach."}, {"fullname_first_author": "Andy Zhou", "paper_title": "Language agent tree search unifies reasoning, acting, and planning in language models", "publication_date": "2023-12-01", "reason": "This paper by the same authors expands the techniques presented in the current paper to a more general setting."}]}