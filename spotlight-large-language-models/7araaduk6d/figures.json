[{"figure_path": "7arAADUK6D/figures/figures_2_1.jpg", "caption": "Figure 1: Visualizations for relative representations between models with the same vocabulary and between models with different vocabularies. PCA and K-means clustering are applied only for visualization. Different colors indicate different clusters of samples (word embeddings). The red block indicates the representation of tokens that only appear in Mistral's vocabulary. Relative representation consistency is obtained by calculating the cosine similarity between the relative representations of the same token in different models.", "description": "This figure visualizes the relative representations of word embeddings from three different language models: LLaMA2-7B, LLaMA2-13B, and Mistral-7B.  LLaMA2-7B and LLaMA2-13B share the same vocabulary, while Mistral-7B has a different vocabulary. PCA and K-means clustering are used for visualization.  The plots show that relative representations (based on cosine similarity to anchor tokens) are largely consistent across models with the same vocabulary, indicating cross-model invariance, even when model architectures or sizes differ.  The red block highlights tokens unique to Mistral's vocabulary, which still shows some alignment in the relative space, although consistency is lower compared to the models sharing vocabularies.", "section": "2.1 Relative Representation"}, {"figure_path": "7arAADUK6D/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of DEEPEN. The relative representation matrix of each LLM is directly derived by calculating the embedding similarities between each token with the anchor tokens.", "description": "This figure illustrates the DEEPEN framework's process.  First, each LLM transforms its probability distribution from its own absolute space (vocabulary) to a shared relative space using its relative representation matrix.  These relative representations are aggregated.  Finally, a search-based inverse transformation maps the result back to the probability space of the selected main model, which produces the next token.", "section": "3 Methodology"}, {"figure_path": "7arAADUK6D/figures/figures_6_1.jpg", "caption": "Figure 3: Test set results of ensemble learning on various number of models. Individual models are arranged in descending order of their performance on the development set, and sequentially incorporated into the ensemble. A indicates the largest improvement achieved by DEEPEN.", "description": "This figure shows the results of ensemble learning using different numbers of models on three benchmark datasets: MMLU, PIQA, and NQ.  The models are added sequentially to the ensemble, ordered by their individual performance on a development set. The y-axis represents the performance (accuracy or exact match) on the test set, and the x-axis shows the number of models in the ensemble.  The purple line shows the results of the DEEPEN-Adapt method (an adaptive weighting scheme), and the blue bars depict the individual model performances. The 'A' values indicate the maximum improvement gained by DEEPEN-Adapt over the best single model in each ensemble.", "section": "4.3 Results on Different Numbers of Models"}, {"figure_path": "7arAADUK6D/figures/figures_7_1.jpg", "caption": "Figure 4: Effect of the number of anchor words. The x-axis indicates the number of anchor words randomly sampled from the common words for 4 times.", "description": "This figure shows the impact of the number of anchor words used in the relative representation on the model's performance.  The experiment was repeated four times for each number of anchor words.  The results demonstrate an improvement in model performance as the number of anchor words increases, with a peak performance achieved when using the full set of common words.", "section": "5.2 Analysis on Relative Transformation"}, {"figure_path": "7arAADUK6D/figures/figures_7_2.jpg", "caption": "Figure 3: Test set results of ensemble learning on various number of models. Individual models are arranged in descending order of their performance on the development set, and sequentially incorporated into the ensemble. A indicates the largest improvement achieved by DEEPEN.", "description": "This figure shows the results of ensemble learning on different numbers of models for three benchmark tasks (MMLU, PIQA, NQ).  The models are added sequentially to the ensemble, in descending order of their individual performance on a development set.  The graph shows that adding more models initially improves performance, but eventually leads to diminishing returns or even a slight performance decrease, suggesting an optimal ensemble size exists for each task. The 'A' values indicate the largest performance improvement achieved by the DEEPEN method compared to using a single model.", "section": "4.3 Results on Different Numbers of Models"}, {"figure_path": "7arAADUK6D/figures/figures_13_1.jpg", "caption": "Figure 1: Visualizations for relative representations between models with the same vocabulary and between models with different vocabularies. PCA and K-means clustering are applied only for visualization. Different colors indicate different clusters of samples (word embeddings). The red block indicates the representation of tokens that only appear in Mistral's vocabulary. Relative representation consistency is obtained by calculating the cosine similarity between the relative representations of the same token in different models.", "description": "This figure visualizes relative representations of word embeddings from three different large language models (LLMs): LLaMA2-7B, LLaMA2-13B, and Mistral-7B.  LLaMA2-7B and LLaMA2-13B share the same vocabulary, while Mistral-7B has a different one.  The visualizations (using PCA and K-means clustering) show that despite vocabulary differences, the relative representations (measuring cosine similarity between embeddings and anchor tokens) exhibit a high degree of consistency between models with the same vocabulary. The red block highlights tokens unique to Mistral-7B's vocabulary, illustrating the impact of vocabulary differences on absolute representations.", "section": "3 Methodology"}, {"figure_path": "7arAADUK6D/figures/figures_14_1.jpg", "caption": "Figure 1: Visualizations for relative representations between models with the same vocabulary and between models with different vocabularies. PCA and K-means clustering are applied only for visualization. Different colors indicate different clusters of samples (word embeddings). The red block indicates the representation of tokens that only appear in Mistral's vocabulary. Relative representation consistency is obtained by calculating the cosine similarity between the relative representations of the same token in different models.", "description": "This figure visualizes relative representations of word embeddings from three different language models.  Two models (LLama2-7B and LLama2-13B) share the same vocabulary, while Mistral-7B has a different vocabulary.  The visualization uses PCA and K-means clustering to group similar embeddings.  The plots show the consistency of relative representations across models, even with vocabulary differences.  The red block highlights tokens unique to Mistral-7B's vocabulary.", "section": "2.1 Relative Representation"}, {"figure_path": "7arAADUK6D/figures/figures_15_1.jpg", "caption": "Figure 9: Effect of different number of relative ensemble learning steps.", "description": "The figure shows the relationship between the number of relative ensemble learning steps (T) and the performance of DEEPEN. As the number of steps increases, the loss initially decreases sharply and then gradually plateaus, while the accuracy initially increases and then shows a slight decrease. This suggests that an optimal number of steps exists to balance the trade-off between reducing the loss and maintaining high accuracy.", "section": "5.3 Analysis of Reverse Transformation"}]