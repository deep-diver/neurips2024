[{"type": "text", "text": "Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Frederik Kunstner1 Robin Yadav1 Alan Milligan1 kunstner@cs.ubc.ca robiny12@student.ubc.ca alanmil@cs.ubc.ca ", "page_idx": 0}, {"type": "text", "text": "Mark Schmidt1,2 Alberto Bietti3 schmidtm@cs.ubc.ca abietti@flatironinstitute.org 1 University of British Columbia 2 Canada CIFAR AI Chair 3 Flatiron Institute ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Adam has been shown to outperform gradient descent on large language models by a larger margin than on other tasks, but it is unclear why. We show that a key factor in this performance gap is the heavy-tailed class imbalance found in language tasks. When trained with gradient descent, the loss of infrequent words decreases more slowly than the loss of frequent ones. This leads to a slow decrease on the average loss as most samples come from infrequent words. On the other hand, Adam and sign-based methods are less sensitive to this problem. To establish that this behavior is caused by class imbalance, we show empirically that it can be reproduced across architectures and data types, on language transformers, vision CNNs, and linear models. On a linear model with cross-entropy loss, we show that class imbalance leads to imbalanced, correlated gradients and Hessians that have been hypothesized to benefti Adam. We also prove that, in continuous time, gradient descent converges slowly on low-frequency classes while sign descent does not. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The recent success of large language models such as GPT-3 (Brown et al., 2020) and its successors has relied on costly training procedures at unprecedented scale. A key ingredient in their training is the Adam optimizer (Kingma and Ba, 2015), which outperforms stochastic gradient descent (SGD) on language problems by a large margin. Despite this large performance gap, we have a poor understanding of why Adam works better and it has been difficult to find new optimizers that consistently improve over Adam (Schmidt et al., 2021). Not only is it computationally difficult to validate new optimizers on large models, but we also lack theoretical guidance; we do not know what \u201cproblem\u201d Adam solves to outperform SGD. ", "page_idx": 0}, {"type": "text", "text": "The success of Adam on language transformers has been well documented. Multiple works have found metrics or statistics that correlate with the improved performance of Adam, showing that it yields uniform updates across parameters despite imbalanced gradients (Liu et al., 2020), gives a better descent direction than the gradient (Pan and Li, 2023), and takes a path over which a robust variant of the condition number is smaller (Jiang et al., 2022). But these observations do not provide a mechanism explaining what property of the problem leads to the improved performance of Adam. ", "page_idx": 0}, {"type": "text", "text": "Plausible mechanisms have been put forward, but they do not provide a complete explanation. Zhang et al. (2020b) show that Adam-like methods are more resilient to heavy-tailed noise, which seems more prominent in language than in vision tasks. But noise is not the primary cause of the gap, as it already appears in deterministic training (Kunstner et al., 2023). An alternative hypothesis is that the magnitude of the gradient and Hessian are correlated, which justifies clipping (Zhang et al., 2020a). But to justify methods that normalize element-wise, like Adam and sign-like methods, we additionally need the gradient and Hessian to be correlated across parameters (Crawshaw et al., 2022). While there is empirical evidence for this behavior in neural networks, we do not have a good understanding of why this occurs, nor why this would be more pronounced on language rather than vision tasks. ", "page_idx": 0}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/5fca7bc4896fc829e2ef5ec87bd5cd4a6016ea15eb726d9ec3286a0e46b3d826.jpg", "img_caption": ["Figure 1: Gradient descent does not make progress on low-frequency classes, while Adam does. Training GPT2-Small on WikiText-103. (a) Distribution of the classes sorted by class frequency, split into groups corresponding to ${\\approx}10\\%$ of the data. (b) Overall training loss. $(\\mathbf{c},\\mathbf{d})$ Training loss for each group using SGD and Adam. SGD makes little to no progress on low-frequency classes while Adam makes progress on all groups. (b) is the average of $(\\mathbf{c},\\mathbf{d})$ for the respective optimizer. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our goal is to answer the following question: what is the \u201cproblem\u201d that makes SGD slow on language tasks, that Adam \u201cfixes\u201d to perform better? ", "page_idx": 1}, {"type": "text", "text": "We argue the problem is what we call heavy-tailed class imbalance, where rare classes account for a large fraction of the data. Language data is imbalanced as some words are much more frequent than others, typically following a power-law. A common modeling assumption is Zipf\u2019s law, where the $k$ th most frequent word has frequency $\\propto1/k$ (Piantadosi, 2014). For language tasks framed as next-token prediction, this property is reflected in the tokens and leads to heavy-tailed class imbalance. This contrasts with typical vision datasets such as MNIST, CIFAR, and ImageNet, which are curated to have uniform classes, but also with imbalanced problems with a small number of classes. For example, in binary classification, extreme imbalance implies the minority class has a limited impact on the loss; with an imbalance of 99:1, only $1\\%$ of the data comes from the minority class. ", "page_idx": 1}, {"type": "text", "text": "The performance gap arises because SGD makes slow progress on rare classes, see Figure 1. On a binary problem, slow performance on $1\\%$ of the data need not have a large impact on the average loss if we make fast progress on the remaining $99\\%$ of the samples. In contrast, the heavy-tailed class imbalance found in language tasks makes it possible for low-frequency classes to account for most of the data and significantly contribute to the loss, leading to slow performance overall. ", "page_idx": 1}, {"type": "text", "text": "We show that heavy-tailed class imbalance makes SGD slow across tasks in Section 2. We show that modifying vision datasets to exhibit heavy-tailed imbalance leads to slow progress with SGD on architectures where the performance gap with Adam is typically smaller. The impact of heavy-tailed imbalance can even be seen on linear models. Additionally, the performance of SGD improves with techniques that address imbalance such as upweighting rare classes. ", "page_idx": 1}, {"type": "text", "text": "Our findings provide a simple model where Adam outperforms SGD, a softmax linear model under heavy-tailed class imbalance, which we analyze in Section 3. We show empirically that a correlation between the magnitude of the gradient and Hessian across coordinates, used to justify the benefits of Adam, appears naturally even on a linear model with class imbalance. We provide intuition as to how this pattern emerges through an assignment mechanism that leads to a correlation between class frequencies and the magnitude of the gradient and Hessian across parameters. We additionally prove that, on a simple dataset and in continuous time, GD is slow on low-frequency classes while sign descent is insensitive to the class frequencies. ", "page_idx": 1}, {"type": "text", "text": "We do not claim that class imbalance is the only reason Adam outperforms SGD, as other properties of the data or architectures likely also contribute to this gap. Instead, we show that Adam consistently outperforms SGD under heavy-tailed class imbalance. The difficulty of minimizing the loss of minority classes has been explored for binary problems or problems few classes (Anand et al., 1993; Francazi et al., 2023), but the recent scaling of large language models to predictions over more than $100\\,000$ classes puts the problem on a new scale. Our findings indicate that heavy-tailed class imbalance has a significant impact on training performance and should be a consideration for future optimizers to perform well on language and other tasks exhibiting heavy-tailed class imbalance. ", "page_idx": 1}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/b8027a90c6da25d7024fd11c40d96e7e214cb60a7a0e2fefb8667fe3870b5d56.jpg", "img_caption": ["Figure 2: Adam outperforms GD for training a CNN under heavy-tailed class labels. (a) Performance on the MNIST dataset. (b) Performance on a modified MNIST with two groups of classes. The first group consists of the 10 original classes with $\\approx5\\mathbf{k}$ samples each, while the second consists of ${\\approx}10\\mathbf{k}$ added classes with 5 examples each. $(\\mathbf{c},\\mathbf{d})$ Performance of GD and Adam on the two groups. The initial loss is higher for imbalanced MNIST as there are ${\\approx}10^{4}$ classes instead of 10, leading to a loss of $-\\log(1/10^{4})\\approx9.2$ for a uniform prediction instead of $-\\log(1/10)\\approx2.3$ . "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2 Experimental results and ablation studies ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Figure 1 suggests a correlation between class frequencies and optimization performance that impacts SGD more than Adam. The goal of this section is to verify that (i) class imbalance is a root cause for the performance gap between SGD and Adam, and (ii) whether this gap can be reproduced with simpler algorithms, such as deterministic optimizers, or using sign descent as a proxy for Adam. ", "page_idx": 2}, {"type": "text", "text": "To test these hypotheses, we perform experiments focusing on the training loss as our objective is to understand what makes optimization difficult. We use a simple training procedure, with a constant step-size tuned by grid search. For visualization, we split the data into groups of classes with similar frequencies, as in Figure 1. For instance, for 10 groups, the first group corresponds to ${\\approx}10\\%$ of the samples from the most frequent classes. This grouping is only used for visualization and does not affect training. The models, datasets and training procedures are described in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "In Appendix B, we give additional information and additional ablation experiments on language models. We show that the heavy-tailed class distribution appears across datasets and tokenizers, and that the separation across class frequencies observed on the training loss in Figure 1 also affects the validation loss. We show that similar dynamics appear on smaller language models, including when training only the last layer while keeping the embedding and attention modules frozen at initialization. Finally, we show that stochasticity is not necessary to reproduce the impact of heavy-tailed class imbalance, and that it also appears when using deterministic updates (i.e., GD instead of SGD). As a result, we use deterministic updates whenever possible, denoted by GD in the figures. ", "page_idx": 2}, {"type": "text", "text": "2.1 Reproducing the frequency gap with vision models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Language transformers are often contrasted with vision CNNs, where we do not see a large performance gap between SGD and Adam. Our hypothesis is that a key differentiation between the two settings is the heavy-tailed class imbalance present in language data. In this section, we show that making heavy-tailed vision datasets leads to slower performance with SGD and a larger performance gap with Adam. These experiments show that heavy-tailed imbalance has a significant impact on performance and can make an otherwise \u201ceasy\u201d problem into a \u201chard\u201d one for SGD. ", "page_idx": 2}, {"type": "text", "text": "CNN. We first use a CNN on a variant of MNIST with heavy-tailed class imbalance. We augment the dataset to have two equally-sized groups of classes with a relative frequency difference of 1000. The first group consists of the original 10 classes with ${\\approx}5\\mathrm{k}$ samples/class. For the second, we create ${\\approx}10\\mathbf{k}$ new classes with 5 samples/class. We create new classes by copying existing images and adding a \u201cbarcode\u201d in a corner of the image, see Appendix A. The performance of GD and Adam is shown in Figure 2. On the original MNIST dataset, both optimizers drive the loss to 0, and Adam still makes progress on both groups in the imbalanced case. But on the imbalanced variant, GD makes almost no progress on half of the data corresponding to the low-frequency classes and progress stalls. However, it eventually converge if run for much longer (see Appendix D.2), indicating that the problem is one of slow optimization rather than getting stuck in a local minima. ", "page_idx": 2}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/971716d921a6ed253ad181425a9abb853c3e4e1d539e326ffde35e779cab84cb.jpg", "img_caption": ["Figure 3: Adam outperforms SGD for training a ResNet under heavy-tailed class labels. (a) Performance on a subset of ImageNet and (b) an imbalanced subset of ImageNet with class frequencies $\\pi_{k}\\propto1/k$ . (c, d) Performance of GD and Adam on groups corresponding to ${\\approx}10\\%$ of the data. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "ResNet. We replicate this effect with a ResNet18 on an imbalanced variant of ImageNet. We subsample classes with frequencies $\\pi_{k}\\propto1/k$ and compare against a uniform subset with a similar number of samples. In Figure 3, we see that SGD and Adam perform similarly on uniform data but a performance gap appears across class frequencies on the heavy-tailed imbalanced dataset. As in Figures 1 and 2, SGD is slower on imbalanced data, especially on low-frequency classes. ", "page_idx": 3}, {"type": "text", "text": "Vision Transformers. This performance gap also appears with vision transformers (ViTs). In Appendix C, we see that SGD and Adam both perform well on ImageNet, but exhibit a similar performance gap as in Figure 1 on the imbalanced variant. While ViTs may require more raw data, data augmentations, or regularization to generalize as well as ResNets (Steiner et al., 2022), there does not seem to be a large gap between SGD and Adam without class imbalance. ", "page_idx": 3}, {"type": "text", "text": "2.2 Reproducing the frequency gap with a linear model on uniform data ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To highlight that heavy-tailed imbalance alone can lead to the observed difficulties, we reproduce this behavior in a simple setting: a softmax linear model with cross-entropy loss. We create a dataset where the class frequencies approximate $\\pi_{k}\\propto1/k$ and draw $n$ samples uniformly from $[0,1]$ in $d$ dimensions, independently of the label. While there is no relationship to learn, the optimization problem is still well posed and a linear model can separate the data if $n\\ll d$ . As on the transformer of Figure 1, GD makes less progress on low-frequency classes than Adam, as shown in Figure 4. ", "page_idx": 3}, {"type": "text", "text": "This example illustrates that a problem that might look innocuous at first is hard to optimize with GD due to heavy-tailed imbalance, while the performance of Adam is less negatively impacted. Nonetheless, imbalance alone is not sufficient to make GD slow. It is possible to generate pathological datasets with heavy-tailed imbalance where GD ftis all classes fast, by making all the samples (close to) orthogonal. In this case, each sample is learned independently of the others, and there is no difference across classes. However, perfectly orthogonal data is unlikely, especially as we expect samples from similar classes to be assigned a similar (correlated) representation. We discuss this issue and give additional examples on the linear model in Appendix D. ", "page_idx": 3}, {"type": "text", "text": "2.3 Interactions between optimizer and imbalance ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We have shown that heavy-tailed class imbalance can lead to different performance across class frequencies, but it is not clear which component of the training process has the highest impact on this behavior. We next experiment with simple algorithms to answer the following questions. (i) Is the impact of class imbalance due to stochasticity, or does it happen with deterministic training? (ii) Which component of Adam leads to an improved performance? and (iii) If imbalance is the problem, can we improve the performance of SGD by reweighting the losses? ", "page_idx": 3}, {"type": "text", "text": "Class imbalance already impacts deterministic optimization. A natural hypothesis to explain the impact of class imbalance is that it may be due to small batch sizes in SGD; rare classes could be sampled less often, and thus learned more slowly. On the other hand, stochasticity has been found to have little impact on the gap between SGD and Adam (Kunstner et al., 2023). Our experiments in Figures 2, 4 and 5 and further examples in Appendix B.3 reproduce the dynamics of Figure 1 with full batch GD and Adam, indicating the problem already arises in the deterministic setting. ", "page_idx": 3}, {"type": "text", "text": "Adam and sign descent both perform well under imbalance. Following Kunstner et al. (2023), we check whether the benefti of Adam is due to a change in the magnitude of the update or its direction. ", "page_idx": 3}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/51a8414c29b6613398bfd36bee6dfc9eefeeaf6d7ef904edfb710060713250ec.jpg", "img_caption": ["Figure 4: The impact of heavy-tailed class imbalance is reproducible with linear models. Softmax regression on synthetic data. The inputs are drawn from a uniform distribution on $[0,1]^{d}$ . The target classes are heavy-tailed (a) and independent of the inputs, but the model can still fit the data as it is overparameterized. $(\\mathbf{b},\\mathbf{c},\\mathbf{d})$ Overall training loss and performance of GD and Adam on each subset. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Changing the magnitude as in normalized GD is known to perform better on separable problems (Nacson et al., 2019), while the beneftis of Adam have been attributed to the change of direction close to sign descent (Tieleman and Hinton, 2012; Balles and Hennig, 2018). We compare the performance of GD, Adam, normalized GD and sign descent, with and without momentum, for training the last layer of a small transformer in Figure 5 and on additional problems in Appendix E. Normalization and momentum helps across problems, but they have less impact on the performance gap across class frequencies than changing the update direction. Sign descent and Adam have a similar performance. ", "page_idx": 4}, {"type": "text", "text": "Upweighting low-frequency classes can help. Given our hypothesis that the performance gap between (S)GD and Adam is due to class imbalance, we expect interventions directly targeting imbalance to improve performance. In Appendix E.1, we show that upweighting the loss of lowfrequency classes can improve the performance of SGD. While reweighting is not complete solution as it changes the objective function, this experiment supports the hypothesis that the optimization problem is due to heavy-tailed class imbalance. ", "page_idx": 4}, {"type": "text", "text": "3 An investigation on linear models ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Heavy-tailed imbalance already leads to slow performance on the linear softmax model of Figure 4, but we do not have a good understanding of why GD becomes slow while Adam is less affected. In this section, we explore the effect of heavy-tailed class imbalance on the special case of softmax linear models, showing that it leads to correlated, imbalanced gradients and Hessians. In Section 3.1, we give an example on a quadratic where imbalanced Hessians lead to a performance gap between GD and Adam. In Section 3.2, we show that class imbalance leads to imbalanced gradients and Hessians that are correlated with class frequencies through an assignment mechanism, showing that this pattern emerges naturally. Finally, we prove that on a simple imbalanced problem and in continuous time, GD is slow on low-frequency classes while sign descent is fast on all classes in Section 3.3. ", "page_idx": 4}, {"type": "text", "text": "3.1 Intuition on a weighted quadratic problem ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Consider the following toy problem which is purposefully oversimplified to provide a high-level intuition about the optimization dynamics. Suppose we have $c$ functions $f_{1},...,f_{c}$ , corresponding to the losses for each class, that are on the same scale in the sense that gradient descent with step-size $\\alpha$ makes fast progress on any $f_{i}$ . For concreteness, take $f_{i}(w)=\\textstyle\\frac{1}{2}\\|\\bar{w}\\|^{2}$ , where GD with a step-size of 1 converges in one step. Instead of running GD on each function independently, suppose we run GD on the weighted average $\\begin{array}{r}{f(w_{1},...,w_{c})\\stackrel{<}{=}\\sum_{i=1}^{c}{\\pi}_{i}f_{i}(w_{i})}\\end{array}$ with positive weights $\\pi_{1}\\ge...\\ge\\pi_{c}$ $\\textstyle\\sum_{i}\\pi_{i}=1$ , corresponding to the class frequencies. If these weights span multiple orders of magnitude, we expect a similar behavior as in Figures 1 to 5, as illustrated in Figure 6. GD makes slow progress on functions with low weights as the gradient w.r.t. $w_{k}$ is scaled by $\\pi_{k}$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\nw_{k}^{(t)}=w_{k}^{(t-1)}-\\alpha\\pi_{k}f_{k}^{\\prime}(w_{k}^{(t-1)})=(1-\\alpha\\pi_{k})^{t}w_{k}^{(0)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This slow convergence on functions with low weights cannot be fixed by increasing the step-size, as increasing it beyond $1/\\pi_{1}$ would cause instabilities on the highest-frequency \u201cclass\u201d $f_{1}$ . The problem is that we use the same step size for all functions, which have different scales. Adam and sign descent ", "page_idx": 4}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/50793836f2561c02bd154cde750446758124629e816174d509dc18ff3e48160c.jpg", "img_caption": ["Figure 5: Sign descent, as a simplified form of Adam, performs well on low-frequency classes. Training the last layer of a simplified one-layer transformer with GD, Adam, normalized GD, and sign descent, with and without momentum $(\\pm\\mathfrak{m})$ . Momentum and normalizing the magnitude help but have smaller effects than using sign descent, which recovers similar dynamics to Adam. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "are less sensitive to this problem as their updates are independent of $\\pi_{k}$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\nw_{k}^{(t)}=w_{k}^{(t-1)}-\\alpha\\frac{\\pi_{k}f_{k}^{\\prime}(w_{k}^{(t-1)})}{\\left|\\pi_{k}f_{k}^{\\prime}(w_{k}^{(t-1)})\\right|}=w_{k}^{(t-1)}-\\alpha\\,\\mathrm{sign}(f_{k}^{\\prime}(w_{k}^{(t-1)})).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "While sign descent or Adam with a fixed step-size need not converge and can oscillate around the minimum, they perform much better in early iterations, independently of $\\pi_{k}$ . ", "page_idx": 5}, {"type": "text", "text": "Another perspective is that the imbalance in the weights $\\pi_{1},...,\\pi_{c}$ makes the problem ill-conditioned. The weights not only affect the gradient of $f$ but also its Hessian, which is $\\mathrm{Diag}([\\pi_{1},...,\\pi_{c}])$ . A common intuition for Adam is that using the magnitude of the coordinates of the gradient as a preconditioner is a good proxy for the Hessian diagonal (Duchi et al., 2011; Kingma and Ba, 2015), which would also lead to larger step-sizes for coordinates with small $\\pi_{k}$ . While this does not hold in general (Kunstner et al., 2019), the gradient can be a reasonable approximation to the Hessian on this problem. The gradient is $[\\pi_{1}w_{1},...,\\pi_{c}w_{c}]$ . If the weights $\\pi_{1},...,\\pi_{c}$ vary by orders of magnitude more than the parameters $|w_{1}|,...,|w_{c}|$ , the gradient and Hessian will be correlated, and preconditioning by the gradient magnitude or Hessian diagonal will yield similar directions. ", "page_idx": 5}, {"type": "text", "text": "3.2 Correlations between the magnitude of the gradient and Hessian across coordinates ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "What is lacking to explain Adam\u2019s improved performance is an understanding of how a correlation between the gradient and Hessian arises in realistic problems. This feature has been observed on neural networks, but we do not yet know why it appears, even on the softmax linear problem. The caricature of the diagonal quadratic problem of the previous section provides some intuition, but does not directly apply to the softmax linear model of Figure 4 as that problem is neither quadratic nor separable. Nonetheless, a similar pattern emerges in the rows $\\mathbf{w}_{1},...,\\mathbf{w}_{c}$ of its parameter matrix $\\mathbf{W}\\in\\mathbb{R}^{\\dot{c}\\times d}$ ; the magnitude of the gradient and Hessian across rows and the class frequencies can become correlated during training due to class imbalance. In this section, we establish this observation empirically and provide a mechanism for how it emerges. ", "page_idx": 5}, {"type": "text", "text": "In Figure 7, we show the gradient norm against the Hessian trace with respect to each row $\\mathbf{w}_{k}$ throughout the trajectory of Adam on the softmax linear model of Figure 4. While there is no correlation at initialization, the gradient and Hessian blocks become correlated with class frequencies during training and become imbalanced. This imbalance in the diagonal blocks is the main feature of the Hessian as the than off-diagonal blocks are orders of magnitude smaller, as shown in Figure 9. Similar dynamics occur with GD, although only on high-frequency classes as GD makes little progress on low-frequency classes, see Appendix F. This correlation also appears in the last layer of large models such as GPT2-Small used in Figure 1, as shown in Figure 8. ", "page_idx": 5}, {"type": "text", "text": "To explain this behavior, we show that the impact of samples on the Hessian follows an assignment mechanism: if the model assigns samples to their correct class, the Hessian with respect to $\\mathbf{w}_{k}$ is ", "page_idx": 5}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/7c143bc486f7522e409a03474f021be075443c288abc6d5309fc180f95c00901.jpg", "img_caption": ["$\\approx10\\%$ ooff  tthhee  wweeiigghhttss,,  ssmmaalllleesstt  wweeiigghhttss $\\approx10\\%$ ooff  tthhee  wweeiigghhttss,,  llaarrggeesstt  wweeiigghhttss "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 6: Class-separation on the quadratic problem of Section 3.1 with weights $\\pi_{k}\\propto1/k$ . GD fits functions with low weights more slowly, while Adam and sign descent have the same dynamics across all functions and all the lines overlap as every parameter $w_{i}$ is initialized at $w_{i}=1$ . ", "page_idx": 6}, {"type": "text", "text": "primarily influenced by samples from class $k$ , leading to a correlation between the magnitude of the gradient, Hessian, and class frequencies. To capture this effect, we introduce some notation and a simplifying assumption. Suppose we have $n$ samples with inputs $\\mathbf{x}_{i}\\in\\mathbb{R}^{d}$ and labels $y_{i}\\in[c]$ , where class $k$ has frequency $\\pi_{k}\\,=\\,n_{k}\\big/n$ . The parameters of the linear model are $\\mathbf{W}\\in\\mathbb{R}^{\\bar{c}\\times d}$ . We write $\\mathbf{p}(\\mathbf{x})=\\sigma(\\mathbf{W}\\mathbf{x})$ for the predicted probabilities where $\\sigma$ is the softmax, and summarize the data as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l r l}&{\\bar{\\mathbf{x}}=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{x}_{i},}&{{}~\\bar{\\mathbf{x}}^{k}=\\frac{1}{n_{k}}\\sum_{i:y_{i}=k}\\mathbf{x}_{i},}&{\\bar{\\mathbf{H}}=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top},}&{\\bar{\\mathbf{H}}^{k}=\\frac{1}{n_{k}}\\sum_{i:y_{i}=k}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Assumption 1 (correct assignment). The model correctly assigns samples to class $k$ if it predicts $k$ with non-negligible probability $p$ on samples from that class $(\\mathbf{p}(\\mathbf{x}_{i})_{k}=\\mathbf{\\bar{\\boldsymbol{p}}}=\\omega(1/c)$ for $\\mathbf{x}_{i}$ from class $y_{i}=k)$ , and predicts $k$ with near-random chance otherwise $(\\mathbf{p}(\\mathbf{x}_{i})_{k}=O(^{1}/c)$ for $\\mathbf{x}_{i}$ where $y_{i}\\neq k$ ). Proposition 2. If initialized at $\\mathbf{W}_{0}=0,$ , the gradient and Hessian of the loss $\\mathcal{L}$ w.r.t. $\\mathbf{w}_{k}$ are ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{\\mathbf{w}_{k}}\\mathcal{L}(\\mathbf{W}_{0})=\\pi_{k}\\bar{\\mathbf{x}}^{k}-\\frac{1}{c}\\bar{\\mathbf{x}},\\qquad\\qquad\\nabla_{\\mathbf{w}_{k}}^{2}\\mathcal{L}(\\mathbf{W}_{0})=\\frac{1}{c}\\big(1-\\frac{1}{c}\\big)\\bar{\\mathbf{H}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "During training, if the model correctly assigns samples to class $k$ with probability $p$ (Assumption $^{\\,l}$ ), ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{\\nabla_{\\mathbf{w}_{k}}\\mathcal{L}=\\mathbf{\\Phi}(1-p)\\pi_{k}\\,\\bar{\\mathbf{x}}^{k}}&{+O\\big(\\frac{1}{c}\\big),}\\\\ {\\nabla_{\\mathbf{w}_{k}}^{2}\\mathcal{L}=p(1-p)\\pi_{k}\\,\\bar{\\mathbf{H}}^{k}+O\\big(\\frac{1}{c}\\big),}&{a n d}&{\\|\\nabla_{\\mathbf{w}_{k}}\\mathcal{L}\\|\\sim\\left(\\frac{1}{p}\\frac{\\|\\bar{\\mathbf{x}}^{k}\\|}{\\mathrm{Tr}(\\bar{\\mathbf{H}}^{k})}\\right)\\mathrm{Tr}(\\nabla_{\\mathbf{w}_{k}}^{2}\\mathcal{L})\\,a s\\,c\\to\\infty,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for classes where the frequency does not vanish too quickly, $\\pi_{k}=\\omega({^1}/c)$ . ", "page_idx": 6}, {"type": "text", "text": "The assumption that $c\\rightarrow\\infty$ is used to obtain a simple and interpretable equation in the correlation.   \nIn practice, $c>10^{3}$ appears sufficient to make the dependence on $\\pi_{k}$ appear, as in Figures 7 and 8. ", "page_idx": 6}, {"type": "text", "text": "At initialization, Equation (1) shows that the Hessian blocks are uniform across classes while the gradients depend on $\\pi_{k}$ . If the data is uniform across classes $(\\|\\bar{\\mathbf{x}}^{k}\\|\\approx\\|\\bar{\\mathbf{x}}^{k^{\\prime}}\\|)$ while the frequencies differ by orders of magnitude, the the gradient blocks will mirror the class frequencies for highfrequency classes where $\\pi_{k}\\gg{}^{1/{\\textstyle c}}$ . This confirms the pattern observed at initialization in Figures 7 and 8. During training, Equation (2) indicates a correlation between gradient norm and Hessian trace if classes have similar values of $\\lVert\\bar{\\mathbf x}^{k}\\rVert$ , $\\mathrm{Tr}(\\bar{\\bf H}^{k})$ and predicted probabilities $p$ , confirming the behavior observed during training in Figures 7 and 8 for the high frequency classes. As Adam fits low-frequency classes faster in Figure 4, they have a value of $p$ closer to 1 (shown in Appendix F) and deviate slightly from the trend in Figure 7, as expected from Equation (2). ", "page_idx": 6}, {"type": "text", "text": "We now give the main intuition and defer the derivation of the asymptotics to Appendix G. We ignore off-diagonal blocks here, as they are orders of magnitude smaller than diagonal blocks (Figure 9), and show in Appendix G.1 that they are expected to be small. ", "page_idx": 6}, {"type": "text", "text": "Proof idea. Our loss is $\\begin{array}{r}{\\mathcal{L}(\\mathbf{W})=\\frac{1}{n}\\sum_{i=1}^{n}\\ell(\\mathbf{W},\\mathbf{x}_{i},\\mathbf{y}_{i})}\\end{array}$ , where $\\ell$ is a softmax linear model, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\ell(\\mathbf{W},\\mathbf{x},y)=-\\log(\\sigma(\\mathbf{W}\\mathbf{x})_{y}),\\quad\\mathrm{with}\\quad\\sigma(\\mathbf{z})_{k}=\\frac{\\exp(\\mathbf{z}_{k})}{\\sum_{j}\\exp(\\mathbf{z}_{j})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Writing $\\mathbf{p}(\\mathbf{x})=\\sigma(\\mathbf{W}\\mathbf{x})$ for the vector predicted probabilities, the gradient and Hessian blocks are ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{w}_{k}}\\ell(\\mathbf{W},\\mathbf{x},y)=(\\mathbf{1}[y=k]-\\mathbf{p}(\\mathbf{x})_{k})\\mathbf{x},\\ \\ \\ \\ \\ \\nabla_{\\mathbf{w}_{k}}^{2}\\ell(\\mathbf{W},\\mathbf{x},y)=\\mathbf{p}(\\mathbf{x})_{k}(1-\\mathbf{p}(\\mathbf{x})_{k})\\mathbf{x}\\mathbf{x}^{\\top}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The contribution of a sample $(\\mathbf{x},y)$ to the gradient w.r.t. $\\mathbf{w}_{k}$ primarily depends on whether the sample belongs to class $k$ through the $\\mathbf{1}[y=k]$ term, while the contribution to the Hessian block depends on whether the model assigns that sample to class $k$ through $\\mathbf{p}(\\mathbf{x})_{k}$ . At initialization, $\\mathbf{p}(\\mathbf{x})_{k}\\overset{\\d}{=}1/c$ for all samples, and averaging the terms in Equation (4) yields Equation (1). Highlighting this effect during training is more challenging due to the dependency on the predictions. However, if W start to assign samples to their correct classes (Assumption 1), we can obtain a similar decomposition as Equation (1). For a given class $k$ , the probabilities for correct labels are all $p$ while the probabilities for incorrect ones are bounded by $O(1/c)$ , which vanishes in the limit of $c\\rightarrow\\infty$ . \u53e3 ", "page_idx": 6}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/5f606f7e330b89bb34764e19b4d3d42c88f467b4118d0dc3224003ee2bb76668.jpg", "img_caption": ["Figure 7: The gradient norm and Hessian trace across blocks become correlated during training, over the path taken by Adam in training the linear model of Figure 4. The blocks correspond to the rows $\\mathbf{w}_{1},...,\\mathbf{w}_{c}$ of the parameter matrix W. The color indicates the class frequency, showing that lower (higher) frequency classes have smaller (larger) gradient norm and Hessian trace. ", "lleeaasstt  ffrreeqq..  ccllaasssseess  (( $\\approx10\\%$ ssaammpplleess)) mmoosstt  ffrreeqq..  ccllaasssseess  (( $\\approx10\\%$ ssaammpplleess)) "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "This assignment mechanism explains why the gradient, Hessian, and class probabilities can become correlated on the linear model. While the gradient does not directly approximate the Hessian, the main feature of the imbalance in the Hessian comes from the weighting by the class frequencies $\\pi_{1},...,\\pi_{c}$ , which is present in both the gradient and the Hessian, as shown in Figures 7 and 9. This correlation is not a global property of the problem, as there are parameters for which the opposite pattern holds, see Appendix F, but it appears during training if the optimization algorithm makes progress. While the per-coordinate normalization of Adam or sign descent was not designed to specifically address class imbalance, they appear to benefit from this property to make faster progress. ", "page_idx": 7}, {"type": "text", "text": "Our results complement prior work on optimization with class imbalance on problems with two or few classes, which argued that the gradient is dominated by the majority class, and as a result is biased towards making progress on the majority class at the expense of the minority class (Anand et al., 1993; Ye et al., 2021; Francazi et al., 2023). While this explains why GD might not make fast progress on rare classes, it was not clear why this would lead to slow performance on average, especially under heavy-tailed imbalance where there is no \u201cmajority\u201d. Our results show that, in addition to imbalance in the gradients, class imbalance leads to optimization difficulties through imbalanced Hessians. ", "page_idx": 7}, {"type": "text", "text": "3.3 Improvement of sign-based approaches over gradient descent ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "While the above arguments provide a high-level intuition as to why the gradient might be a reasonable proxy for the Hessian, it remains difficult to formally describe this effect and prove the benefits of Adam over GD without strong assumptions. Doing so would require a fine-grained analysis of the dynamics, as the correlation only appears during training. To obtain a provable a guarantee highlighting the benefti of sign-based methods, we consider a stripped-down problem where the only difficulty lies in the class imbalance: ", "page_idx": 7}, {"type": "text", "text": "Simple imbalanced setting. Consider c classes with frequencies $\\pi_{1},...,\\pi_{c}$ where all samples from a class are the same, $\\mathbf{x}_{i}=\\mathbf{e}_{k}$ if $y_{i}=k$ , where $\\mathbf{e}_{k}$ is the kth standard basis vector in $\\mathbb{R}^{\\epsilon}$ . ", "page_idx": 7}, {"type": "text", "text": "This setting is trivial as a possible solution is $\\mathbf{W}=\\alpha\\mathbf{I}$ with $\\alpha\\to\\infty$ , or taking one step of gradient descent with an arbitrarily large step-size. However, we will see that the dynamics with small stepsizes already exhibit the separation by class frequencies observed experimentally. In this simplified setting, we show that the continuous time variant of gradient descent, gradient flow, and sign descent as a proxy for Adam, obtain qualitatively different convergence rates (proof in Appendix H). ", "page_idx": 7}, {"type": "text", "text": "Theorem 3. On the simple imbalanced setting, gradient flow and continuous time sign descent initialized at $\\mathbf{W}=0$ minimize the loss of class $k$ , $\\ell_{k}(t)=-\\log(\\sigma(\\mathbf{W}(t)\\mathbf{e}_{k})_{k})$ , at the rate ", "page_idx": 7}, {"type": "text", "text": "Gradient flow: $\\ell_{k}(t)=\\Theta(1/\\pi_{k}t)$ , Continuous time sign descent: $\\ell_{k}(t)=\\Theta\\big(e^{-c t}\\big)$ . ", "page_idx": 7}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/675ec4a044c7fcca4630b33be4e331f69829ab14e57ac2b9f1180ca04434a43d.jpg", "img_caption": ["lleeaasstt  ffrreeqq..  ccllaasssseess  (( $\\approx10\\%$ ssaammpplleess)) mmoosstt  ffrreeqq..  ccllaasssseess  (( $\\approx10\\%$ ssaammpplleess)) "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 8: The gradient-Hessian blocks also become correlated in the last layer of large models. Reproducing Figure 7 on the GPT2-Small/WikiText-103 problem of Figure 1. Evolution of the gradient norm and Hessian trace for each row $\\mathbf{w}_{c}$ of the last layer throughout optimization, over the path taken by Adam. The color indicates the class frequency, showing that lower (higher) frequency classes have smaller (larger) gradient norm and Hessian trace. ", "page_idx": 8}, {"type": "text", "text": "The difference between the sublinear rate of gradient flow $(1/t)$ and linear rate of sign descent $(e^{-t})$ is similar to existing results for separable logistic regression, where normalized updates converge faster as they keep increasing the margin despite small gradients (Nacson et al., 2019). While the setting studied here is separable, we still observe the separation across class frequencies on problems that are not separable, either because the problem has examples with different output for the same inputs, as in Figure 1, or when adding regularization, as in Appendix D.3. The novel element is that the convergence of gradient flow strongly depends on the class frequencies $\\pi$ , while the convergence of sign descent is independent of the class frequencies. ", "page_idx": 8}, {"type": "text", "text": "This setting is admittedly oversimplified and does not capture some of the features observed in our experiments. For example, in Theorem 3, the loss is monotonically decreasing for all classes. This no longer holds once we introduce a bias term and the loss from low-frequency classes will instead first increase, as can be seen for example in Figure 4. This setting is also biased towards sign descent, as the inputs are aligned with the basis vectors. Finally, the problem is inadequate to study large step-sizes, as it can be solved in one large step. On data with non-orthogonal classes, large step-sizes would lead to training instabilities and oscillations in the loss of frequent classes, as can be seen in Figures 2 to 5. Nevertheless, this result formally establishes the benefti of sign-based updates and we believe it captures the key difficulty encountered by GD under heavy-tailed class imbalance. ", "page_idx": 8}, {"type": "text", "text": "4 Discussion and limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Interaction with stochasticity. Our experiments include both stochastic and deterministic training regimes and show that stochasticity is not the cause of the slow performance of SGD on low-frequency classes, as it already appears between full batch GD and Adam. This observation is consistent with prior work showing that the performance gap between SGD and Adam on language transformers already appears with deterministic training (Kunstner et al., 2023). However, we do not attempt to quantify the interaction between stochasticity and class imbalance and leave it for future work. ", "page_idx": 8}, {"type": "text", "text": "Training performance vs. generalization. Our main focus is on optimization performance. Our observations need not generalize to the validation loss, especially in settings prone to overfitting, as good training performance may lead to overftiting on classes with few samples (Sagawa et al., 2020). However, some form of memorization might be needed in long-tailed settings (Feldman, 2020), and if SGD cannot even fti the training data, generalization cannot be good. On the transformer of Figure 1, we observe similar dynamics across frequencies on the validation loss, shown Appendix B.2. Training dynamics on the empirical and population loss are also often similar, particularly early in training (see, e.g., Nakkiran et al., 2021; Ghosh et al., 2022), and the one-pass training regime commonly used in large language models might mitigate those issues by blurring the line between train and test loss. ", "page_idx": 8}, {"type": "text", "text": "Additional difficulties due to text data. We study the effect of the distribution of the classes, the next token to be predicted, but other optimization difficulties might arise from the heavy-tailedness of text data. For example, the sequence of tokens used as inputs to the embedding layer are also heavy-tailed. This imbalance might lead to slow progress on embeddings for rare tokens with GD, giving another potential cause for a performance gap. Full sentences (Williams et al., 2015) and latent rules or mechanisms required to understand a paragraph (Michaud et al., 2023) may also display heavy tails, and Adam could be beneficial if those are captured by intermediate layers (e.g., Meng et al., 2022; Wang et al., 2023; Bietti et al., 2023). The choice of tokenization has also been shown to impact downstream performance, which has been attributed to the lack of samples on rare tokens (Gowda and May, 2020) and the improved efficiency of more uniform tokenizers (Zouhar et al., 2023). Our results indicate that tokenization also has a large impact on optimization performance. ", "page_idx": 8}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/bf8faf15bc3ebe602e8fc63b7dc27655d7ed665dd75a93fa8ee73213559bae9a.jpg", "img_caption": ["Figure 9: The diagonal Hessian blocks are orders of magnitude larger than off-diagonal blocks. Showing the magnitude of a subset of the Hessian blocks $(\\log_{10}(\\big|\\mathrm{Tr}(\\nabla_{i j}^{2}\\mathcal{L})\\big|))$ for a $[160\\times160]$ subset of the Hessian, sampling 40 classes log-uniformly and 40 input dimensions uniformly. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Difficulties due to architectures. Beyond the class distribution, additional optimization difficulties may arise from the architectures, due to depth, signal propagation (Noci et al., 2022; He et al., 2023), vanishing gradients and higher order derivatives (Liu et al., 2020; Orvieto et al., 2022). The simplified transformer of Ahn et al. (2023) also exhibits many of the difficulties observed in the literature on regression instead of a classification problem. However, a phenomenon similar to the assignment mechanism could still explain the benefti of Adam. The oscillations in the loss observed at the feature level by Rosenfeld and Risteski (2023) suggests a link between subsets of the samples and subsets of the parameters. For example, if a convolution fliter detects a specific background color and captures a specific feature of the data, the magnitude of the gradient and Hessian at intermediate layers could be influenced by the relative frequency of the feature in the data, leading to another form of imbalance. ", "page_idx": 9}, {"type": "text", "text": "Recent ablations on the benefit of Adam for language transformer. Parallel to our work, recent investigations have looked into the beneftis of Adam on language transformers. Zhang et al. (2024a) argue that the Hessian has a block-diagonal structure, with similar magnitude within blocks but very different magnitudes across blocks, and that Adam may improve performance by using a different step-size for different blocks. This hypothesis is supported by recent ablations studies. Zhang et al. (2024b) show that the element-wise preconditioning in Adam is not necessary and can be replaced by a single parameter across such blocks while maintaining performance, which they coin Adam-mini. Similarly, Zhao et al. (2024) show that the performance of Adam can be recovered by training most of the network with (S)GD, except for the last layer and LayerNorm parameters. Both approaches still need to treat the last layer separately, either using a step-size for each row $\\mathbf{w}_{c}$ of the last layer in the case of Zhang et al. (2024b) or by using Adam to train the last layer in the case of Zhao et al. (2024). These observations complement our approach, which focuses on the impact of heavy-tailed class imbalance on the last layer, and are consistent with our conclusion that one of the main benefti of Adam is to counteract the slow progress on rare classes by preconditioning the last layer. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have shown that heavy-tailed class imbalance leads to a performance gap between (S)GD and Adam. This effect is reproducible across architectures and data types, but is most salient on language tasks which naturally exhibit heavy-tailed imbalance. As vision tasks are typically more uniform, imbalance is a key differentiating feature of the training difficulties in language tasks. The correlation between entries of the gradient and Hessian that occurs due to class imbalance provides a simple setting that justifies the intuition that Adam-like algorithms can \u201cadapt to curvature\u201d. We provide an explanation for how this correlation arises during training through the assignment mechanism and prove on a simplified problem that gradient descent performs poorly on low-frequency classes while sign descent is unaffected by class frequencies. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Greg d\u2019Eon, Aaron Mishkin, Victor Sanches Portella, and Danica Sutherland for useful discussions and comments on the manuscript. This research was supported by the Canada CIFAR AI Chair Program, the Natural Sciences and Engineering Research Council of Canada (NSERC) through the Discovery Grants RGPIN-2022-03669, and was enabled by the support provided by the BC DRI Group and the Digital Research Alliance of Canada (alliancecan.ca). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Ali Jadbabaie, and Suvrit Sra (2023). \u201cLinear attention is (maybe) all you need (to understand transformer optimization)\u201d. In: arXiv preprint arXiv:2310.01082.   \nRangachari Anand, Kishan G. Mehrotra, Chilukuri K. Mohan, and Sanjay Ranka (1993). \u201cAn improved algorithm for neural network classification of imbalanced training sets\u201d. In: IEEE Transactions on Neural Networks 4.6, pp. 962\u2013969.   \nJimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton (2016). \u201cLayer Normalization\u201d. In: Neural Information Processing Systems (NeurIPS), Deep Learning Symposium.   \nLukas Balles and Philipp Hennig (2018). \u201cDissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients\u201d. In: International Conference on Machine Learning (ICML). Vol. 80, pp. 413\u2013422.   \nLucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov (2022). \u201cBetter plain ViT baselines for ImageNet-1k\u201d. In: CoRR abs/2205.01580.   \nAlberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou (2023). \u201cBirth of a Transformer: A Memory Viewpoint\u201d. In: Neural Information Processing Systems (NeurIPS).   \nTom B. Brown et al. (2020). \u201cLanguage Models are Few-Shot Learners\u201d. In: Neural Information Processing Systems (NeurIPS).   \nVivien Cabannes, Berfin Simsek, and Alberto Bietti (2024). \u201cLearning associative memories with gradient descent\u201d. In: International Conference on Machine Learning (ICML).   \nMichael Crawshaw, Mingrui Liu, Francesco Orabona, Wei Zhang, and Zhenxun Zhuang (2022). \u201cRobustness to Unbounded Smoothness of Generalized SignSGD\u201d. In: Neural Information Processing Systems (NeurIPS).   \nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei (2009). \u201cImageNet: A largescale hierarchical image database\u201d. In: Conference on Computer Vision and Pattern Recognition (CVPR).   \nJohn C. Duchi, Elad Hazan, and Yoram Singer (2011). \u201cAdaptive Subgradient Methods for Online Learning and Stochastic Optimization\u201d. In: Journal of Machine Learning Research (JMLR) 12, pp. 2121\u20132159.   \nVitaly Feldman (2020). \u201cDoes learning require memorization? a short tale about a long tail\u201d. In: Symposium on Theory of Computing (STOC), pp. 954\u2013959.   \nEmanuele Francazi, Marco Baity-Jesi, and Aur\u00b4elien Lucchi (2023). \u201cA Theoretical Analysis of the Learning Dynamics under Class Imbalance\u201d. In: International Conference on Machine Learning (ICML). Vol. 202, pp. 10285\u201310322.   \nPhilip Gage (1994). \u201cA new algorithm for data compression\u201d. In: C Users Journal 12.2, pp. 23\u201338.   \nNikhil Ghosh, Song Mei, and Bin Yu (2022). \u201cThe Three Stages of Learning Dynamics in Highdimensional Kernel Methods\u201d. In: International Conference on Learning Representations (ICLR).   \nBoris Ginsburg, Patrice Castonguay, Oleksii Hrinchuk, Oleksii Kuchaiev, Vitaly Lavrukhin, Ryan Leary, Jason Li, Huyen Nguyen, Yang Zhang, and Jonathan M. Cohen (2020). Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks. Preprint. arXiv/1905.11286.   \nThamme Gowda and Jonathan May (2020). \u201cFinding the Optimal Vocabulary Size for Neural Machine Translation\u201d. In: Findings of the Association for Computational Linguistics (EMNLP), pp. 3955\u2013 3964.   \nBobby He, James Martens, Guodong Zhang, Aleksandar Botev, Andrew Brock, Samuel L. Smith, and Yee Whye Teh (2023). \u201cDeep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation\u201d. In: International Conference on Learning Representations (ICLR).   \nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun (2016). \u201cDeep Residual Learning for Image Recognition\u201d. In: Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770\u2013778.   \nAbdolhossein Hoorfar and Mehdi Hassani (2008). \u201cInequalities on the Lambert function and hyperpower function\u201d. In: Journal of Inequalities in Pure and Applied Mathematics 9.2.   \nSergey Ioffe and Christian Szegedy (2015). \u201cBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\u201d. In: International Conference on Machine Learning (ICML). Vol. 37, pp. 448\u2013456.   \nKaiqi Jiang, Dhruv Malik, and Yuanzhi Li (2022). \u201cHow Does Adaptive Optimization Impact Local Neural Network Geometry?\u201d In: arXiv preprint arXiv:2211.02254.   \nDiederik P. Kingma and Jimmy Ba (2015). \u201cAdam: A Method for Stochastic Optimization\u201d. In: International Conference on Learning Representations (ICLR).   \nTaku Kudo (2018). \u201cSubword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates\u201d. In: Annual Meeting of the Association for Computational Linguistics (ACL), pp. 66\u201375.   \nFrederik Kunstner, Jacques Chen, Jonathan Wilder Lavington, and Mark Schmidt (2023). \u201cNoise is not the main factor behind the gap between SGD and Adam on transformers, but sign descent might be\u201d. In: International Conference on Learning Representations (ICLR).   \nFrederik Kunstner, Philipp Hennig, and Lukas Balles (2019). \u201cLimitations of the empirical Fisher approximation for natural gradient descent\u201d. In: Neural Information Processing Systems (NeurIPS), pp. 4158\u20134169.   \nYann LeCun, L\u00b4eon Bottou, Yoshua Bengio, and Patrick Haffner (1998). \u201cGradient-Based Learning Applied to Document Recognition\u201d. In: Proceedings of the IEEE. Vol. 86. 11, pp. 2278\u20132324.   \nLiyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han (2020). \u201cUnderstanding the Difficulty of Training Transformers\u201d. In: Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 5747\u20135763.   \nMitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz (1993). \u201cBuilding a Large Annotated Corpus of English: The Penn Treebank\u201d. In: Computational Linguistics 19.2, pp. 313\u2013 330.   \nJames Martens and Roger B. Grosse (2015). \u201cOptimizing Neural Networks with Kronecker-factored Approximate Curvature\u201d. In: International Conference on Machine Learning (ICML). Vol. 37, pp. 2408\u20132417.   \nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov (2022). \u201cLocating and editing factual associations in GPT\u201d. In: Neural Information Processing Systems (NeurIPS).   \nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher (2017). \u201cPointer Sentinel Mixture Models\u201d. In: International Conference on Learning Representations (ICLR).   \nEric J. Michaud, Ziming Liu, Uzay Girit, and Max Tegmark (2023). \u201cThe quantization model of neural scaling\u201d. In: Neural Information Processing Systems (NeurIPS).   \nMor Shpigel Nacson, Jason D. Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan Srebro, and Daniel Soudry (2019). \u201cConvergence of Gradient Descent on Separable Data\u201d. In: International Conference on Artificial Intelligence and Statistics (AISTATS). Vol. 89, pp. 3420\u2013 3428.   \nPreetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi (2021). \u201cThe deep bootstrap framework: Good online learners are good offline generalizers\u201d. In: International Conference on Learning Representations (ICLR).   \nLorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Antonio Orvieto, Sidak Pal Singh, and Aur\u00b4elien Lucchi (2022). \u201cSignal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse\u201d. In: Neural Information Processing Systems (NeurIPS).   \nAntonio Orvieto, Jonas Kohler, Dario Pavllo, Thomas Hofmann, and Aure\u00b4lien Lucchi (2022). \u201cVanishing Curvature in Randomly Initialized Deep ReLU Networks\u201d. In: International Conference on Artificial Intelligence and Statistics (AISTATS). Vol. 151, pp. 7942\u20137975.   \nYan Pan and Yuanzhi Li (2023). Toward Understanding Why Adam Converges Faster Than SGD for Transformers. NeurIPS 2022 Workshop on Optimization for Machine Learning. arXiv/2306.00204.   \nVardan Papyan, XY Han, and David L Donoho (2020). \u201cPrevalence of neural collapse during the terminal phase of deep learning training\u201d. In: Proceedings of the National Academy of Sciences (PNAS) 117.40, pp. 24652\u201324663.   \nAdam Paszke et al. (2019). \u201cPyTorch: An Imperative Style, High-Performance Deep Learning Library\u201d. In: Neural Information Processing Systems (NeurIPS), pp. 8024\u20138035.   \nSteven T. Piantadosi (2014). \u201cZipf\u2019s word frequency law in natural language: A critical review and future directions\u201d. In: Psychonomic bulletin & review 21, pp. 1112\u20131130.   \nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever (2019). Language Models are Unsupervised Multitask Learners. Tech. Report.   \nElan Rosenfeld and Andrej Risteski (2023). \u201cOutliers with Opposing Signals Have an Outsized Effect on Neural Network Optimization\u201d. In: arXiv preprint arXiv/2311.04163.   \nShiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang (2020). \u201cAn investigation of why overparameterization exacerbates spurious correlations\u201d. In: International Conference on Machine Learning (ICML).   \nRobin M. Schmidt, Frank Schneider, and Philipp Hennig (2021). \u201cDescending through a Crowded Valley - Benchmarking Deep Learning Optimizers\u201d. In: International Conference on Machine Learning (ICML). Vol. 139, pp. 9367\u20139376.   \nRico Sennrich, Barry Haddow, and Alexandra Birch (2016). \u201cNeural Machine Translation of Rare Words with Subword Units\u201d. In: Annual Meeting of the Association for Computational Linguistics (ACL).   \nNitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov (2014). \u201cDropout: a simple way to prevent neural networks from overfitting\u201d. In: Journal of Machine Learning Research (JMLR) 15.1, pp. 1929\u20131958.   \nAndreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer (2022). \u201cHow to train your ViT? Data, Augmentation, and Regularization in Vision Transformers\u201d. In: Transactions of Machine Learning Research (TMLR) 2022.   \nChristos Thrampoulidis, Ganesh Ramachandra Kini, Vala Vakilian, and Tina Behnia (2022). \u201cImbalance Trouble: Revisiting Neural-Collapse Geometry\u201d. In: Neural Information Processing Systems (NeurIPS).   \nTijmen Tieleman and Geoffrey Hinton (2012). RMSPROP: Divide the gradient by a running average of its recent magnitude. Lecture notes http://www.cs.toronto.edu/\\~tijmen/csc321/slides/lecture_slides_lec6.pdf.   \nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve\u00b4 J\u00b4egou (2021). \u201cTraining data-efficient image transformers & distillation through attention\u201d. In: International Conference on Machine Learning (ICML). Vol. 139, pp. 10347\u201310357.   \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin (2017). \u201cAttention is All you Need\u201d. In: Neural Information Processing Systems (NeurIPS), pp. 5998\u20136008.   \nKevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt (2023). \u201cInterpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 Small\u201d. In: International Conference on Learning Representations (ICLR).   \nJake Ryland Williams, Paul R. Lessard, Suma Desu, Eric M. Clark, James P. Bagrow, Christopher M. Danforth, and Peter Sheridan Dodds (2015). \u201cZipf\u2019s law holds for phrases, not words\u201d. In: Scientific reports 5.1, p. 12209.   \nHan-Jia Ye, De-Chuan Zhan, and Wei-Lun Chao (2021). \u201cProcrustean Training for Imbalanced Deep Learning\u201d. In: International Conference on Computer Vision (ICCV), pp. 92\u2013102.   \nJingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie (2020a). \u201cWhy Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity\u201d. In: International Conference on Learning Representations (ICLR).   \nJingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank J. Reddi, Sanjiv Kumar, and Suvrit Sra (2020b). \u201cWhy are Adaptive Methods Good for Attention Models?\u201d In: Neural Information Processing Systems (NeurIPS), pp. 15383\u201315393.   \nYushun Zhang, Congliang Chen, Tian Ding, Ziniu Li, Ruoyu Sun, and Zhi-Quan Luo (2024a). Why Transformers Need Adam: A Hessian Perspective.   \nYushun Zhang, Congliang Chen, Ziniu Li, Tian Ding, Chenwei Wu, Yinyu Ye, Zhi-Quan Luo, and Ruoyu Sun (2024b). Adam-mini: Use Fewer Learning Rates To Gain More. Preprint. arXiv/2406.16793.   \nRosie Zhao, Depen Morwani, David Brandfonbrener, Nikhil Vyas, and Sham M. Kakade (2024). Deconstructing What Makes a Good Optimizer for Language Models.   \nShuai Zheng and James T. Kwok (2019). Blockwise Adaptivity: Faster Training and Better Generalization in Deep Learning. Preprint. arXiv/1905.09899.   \nVil\u00b4em Zouhar, Clara Meister, Juan Luis Gastaldi, Li Du, Mrinmaya Sachan, and Ryan Cotterell (2023). \u201cTokenization and the Noiseless Channel\u201d. In: Annual Meeting of the Association for Computational Linguistics (ACL), pp. 5184\u20135207. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Supplementary Material ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Experimental details 15   \nB Leanngudagie xproblems 18   \nC Vision problems 20   \nD Linear models 22   \nE Alternative optimizers 25   \nF Dynamics of the gradient and Hessian 27   \nG Correlation between the gradient and Hessian across blocks 31   \nH Continuous time GD and sign descent on a simple imbalanced problem 33 ", "page_idx": 14}, {"type": "text", "text": "A Experimental details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This section documents the datasets, models, software, and experimental setup. The code is available at https://github.com/fkunstner/class-imbalance-sgd-adam. ", "page_idx": 14}, {"type": "text", "text": "A.1 Datasets ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 WikiText-103 (Merity et al., 2017), using sequences of 1 024 tokens and the BPE tokenizer (Sennrich et al., 2016), with a vocabulary of size 50 608. ", "page_idx": 14}, {"type": "text", "text": "\u2022 WikiText-2 (Merity et al., 2017) is used in Appendix B.1 to illustrate that other combinations of datasets and tokenizers lead to heavy-tailed distributions. ", "page_idx": 14}, {"type": "text", "text": "\u2022 PTB (Marcus et al., 1993), using sequences of 35 tokens built from a word-based tokenizer (basic english provided by torchtext), for a vocabulary of size 9 920. For deterministic runs, we use the validation set as a reduced training set, labeled TinyPTB. ", "page_idx": 14}, {"type": "text", "text": "\u2022 MNIST (LeCun et al., 1998).   \n\u2022 ImageNet (Deng et al., 2009). ", "page_idx": 14}, {"type": "text", "text": "A.2 Custom datasets ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 The Random Heavy-Tailed Labels dataset is a synthetic dataset exhibiting heavy-tailed class imbalance. The number of samples per class and the number of classes are picked to approximate a power-law distribution. We create $m$ \u201cgroups\u201d of classes, where each class within a group has the same relative frequency; ", "page_idx": 14}, {"type": "text", "text": "1 class with $2^{m}$ samples, 2 classes with 2m\u22121 samples, $2^{m-1}$ classes with 2 samples. gro u p 1 gro u p 2 gro up  m ", "page_idx": 14}, {"type": "text", "text": "The inputs are drawn from a uniform distribution on [0, 1], independently of the class label. The inputs are in $d=(m+1)\\,2^{m}$ dimensions, the number of samples is $n=m\\,2^{m}$ and the number of classes is $c=2^{m+1}-1$ . We use two variants of the datasets; a large one in Figure 4, Appendix $\\boldsymbol{\\mathrm E}$ $\\langle m\\,=\\,11,n\\,=\\,22\\,528,d\\,=\\,24\\,576,c\\,=\\,4\\,095\\rangle$ and a small one in Appendix D $(m=8,n=2\\,048,d=2\\,304,c=511)$ . ", "page_idx": 14}, {"type": "text", "text": "\u2022 The Barcoded MNIST dataset is a modified variant of MNIST. We start with $50\\mathrm{k}$ examples from the original MNIST dataset across 10 classes, and create 51 150 $(5\\times(10\\times2^{10}-1))$ ) new images. The new examples are copies of existing image with an added \u201cbarcode\u201d, a 10-bit number encoded in a corner of the image, as in the examples below. The class label is a combination of the original class and this barcode. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\n0123456789\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The Barcoded-only dataset contains $10\\times2^{10}$ classes with 5 samples each. To obtain an imbalanced dataset, we combine the barcoded images with the original samples from the MNIST dataset to get 101 200 examples spread across 10 250 $(10\\times2^{10}+\\mathrm{\\bar{10})}$ classes classes; 10 240 with 5 examples per class and 10 classes with $\\approx5k$ examples per class, labeled MNIST $^{\\ast}$ Barcoded \u2022 The Heavy Tailed ImageNet dataset is a subset of ImageNet (Deng et al., 2009), subsampled to exhibit heavy-tailed class imbalance. We sort the original 1000 classes by frequency and sample $\\lceil1300/k\\rceil$ images from the $k$ th class, leading to $n=10\\,217$ samples. \u2022 The Small ImageNet dataset is a uniform subset of ImageNet to contrast the with the heavy tailed variant. We sample 10 images per class to get $n=10\\,000$ samples. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A.3 Models ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2022 The 2-layer transformer used in Appendix B.3 is a transformer Vaswani et al. (2017), based on the PyTorch implementation of TransformerEncoderLayer (Paszke et al., 2019). ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{Embedding}\\rightarrow2\\times\\mathrm{[Attention\\rightarrowLinear\\rightarrowReLU\\rightarrowLinear]}\\rightarrow\\mathrm{Classifier}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The model includes LayerNorm, dropout, and skip connections (He et al., 2016; Ba et al., 2016; Srivastava et al., 2014). The embedding dimension and width of the linear layers is 1000 and the attention modules use 4 heads. ", "page_idx": 15}, {"type": "text", "text": "\u2022 The simplified transformer used in Figure 5 and Appendix B.3 does not use encoder blocks, and only uses attention: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{Embedding}\\rightarrow\\mathrm{Attention}\\rightarrow\\mathrm{Classifier}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We remove LayerNorm, dropout, and the block [Linear $\\rightarrow\\mathrm{ReLU}\\rightarrow\\mathrm{I}$ Linear] containing the non-linearity. In Figure 5, we freeze the embedding and attention layers at initialization, and only the last classification layer is trained. The model is then a linear model on a fixed feature transformation. ", "page_idx": 15}, {"type": "text", "text": "\u2022 The GPT2-Small model (Radford et al., 2019) is used in Figure 1. The blocks includes LayerNorm, residual connections, and dropout on the embedding and dense layers. We use sinusoid positional encodings as in the transformer architecture (Vaswani et al., 2017). The embedding dimension is 768, the width of the intermediate layers is 3072, and we use 12 encoder blocks with 12 self attention heads. ", "page_idx": 15}, {"type": "text", "text": "\u2022 The convolutional network used in Figure 2 and Appendix C is a 2-layer convolution ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{Conv}\\rightarrow\\mathrm{Relu}\\rightarrow\\mathrm{MaxPool}\\rightarrow\\mathrm{Conv}\\rightarrow\\mathrm{Relu}\\rightarrow\\mathrm{MaxPool}\\rightarrow\\mathrm{Linea}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "\u2022 The linear model used in Figures 4 and 7 and Appendix E uses a bias vector. ", "page_idx": 15}, {"type": "text", "text": "\u2022 The ResNet18 model (He et al., 2016) is used in Figure 3. Additionally, a variant replacing the BatchNorm layers with LayerNorm is used in Appendix C. ", "page_idx": 15}, {"type": "text", "text": "\u2022 The SimpleViT model (Beyer et al., 2022) used in Appendix C follows the architecture of a ViTS/16 (Touvron et al., 2021), based on the vit-pytorch implementation (https://github.com/ lucidrains/vit-pytorch v1.6.5). ", "page_idx": 15}, {"type": "text", "text": "A.4 Training procedures ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Our primary focus is on the performance of the optimizers on the training error, using the simplest training procedure possible. We use a constant step-size throughout training, set by grid search. We start with a sparse grid of powers of $10\\,[10^{-6},10^{-2},...,10^{1}]$ and increase the density to half-powers around the best step-size. The step-size is selected to minimize the maximum over 3 seeds of the training loss at the end of training. For some settings, this selection still produces runs that are unstable; the training loss is the smallest at the end but oscillates a lot during training, reaching loss values that are orders of magnitude worse than at initialization. For those runs, we use the next smaller step-size, which has similar performance at the end but is more stable. We use the following batch sizes with gradient accumulation (computing the gradient through multiple passes) ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "- The large transformer experiment in Figure 1 uses mini-batches of 512 sequences of 1024 tokens.   \n- The stochastic experiments with a smaller transformer in Appendix B.3 uses mini-batches of 512 sequences of 35 tokens.   \n- Both ResNet18 variants and the Simple Vision Transformer were trained using mini-batches of 1024. The training images were normalized and randomly cropped to $224\\times224$ pixels as is standard for ImageNet training.   \n- Other experiments use the entire dataset to compute updates ", "page_idx": 16}, {"type": "text", "text": "Our experiments ran on a cluster using a mix of A100, P100, V100, and H100 GPUs. The large scale experiment in Figure 1 took 3 days on a H100, while all other experiments ran in 2\u20138 hours. The total amount of compute used for this project is ${\\approx}3$ GPU-years, including preliminary experiments. ", "page_idx": 16}, {"type": "text", "text": "A.5 Optimization algorithms ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Given momentum buffers $m_{t}$ initialized at $m_{0}~=~0$ and a (possibly) stochastic gradient $\\tilde{g}_{t}$ , we implement the update of GD, normalized GD and sign descent with heavy-ball momentum as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{m_{t}=\\beta m_{t-1}+d_{t},\\qquad\\qquad\\mathrm{with}\\;d_{t}=\\left\\{\\tilde{g}_{t}/\\|\\tilde{g}_{t}\\|_{2}\\right.\\quad\\mathrm{for~afradient~descent},}\\\\ {x_{t+1}=x_{t}-\\alpha m_{t},\\qquad\\qquad\\mathrm{with}\\;d_{t}=\\left\\{\\begin{array}{r l}{\\tilde{g}_{t}}&{\\mathrm{for~gradient~descent},}\\\\ {\\tilde{g}_{t}/\\|\\tilde{g}_{t}\\|_{2}}&{\\mathrm{for~normalized~GD},}\\\\ {\\mathrm{sign}(\\tilde{g}_{t})}&{\\mathrm{for~sign~descent}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For GD and Adam, we use the standard implementation in PyTorch (Paszke et al., 2019). For all algorithms, we use either momentum with $\\beta=0.9$ ( $\\beta_{1}=0.9$ for Adam) or no momentum ( $\\beta=0$ , $\\beta_{1}=0$ ), indicated by solid lines and the legend $(+\\mathrm{m})$ for runs with momentum, and dashed lines and the legend $\\left(-\\mathbf{m}\\right)$ for runs without momentum. ", "page_idx": 16}, {"type": "text", "text": "A.6 Summary of settings used ", "text_level": 1, "page_idx": 16}, {"type": "table", "img_path": "T56j6aV8Oc/tmp/78a4c391551ed5adc0dddae4d3a003565eab65e648e9331c3024df86ba58699d.jpg", "table_caption": ["Table 1: Summary of models, datasets and batch-size used "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B Language problems ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "This section provides additional ablations on language models, showing that the impact of class imbalance holds across models of different sizes and using deterministic updates. ", "page_idx": 17}, {"type": "text", "text": "B.1 shows that the heavy-tailed distribution in text data occurs across datasets and tokenizers.   \nB.2 shows that the imbalanced training speed across frequencies translates to the validation loss.   \nB.3 shows that the imbalance training speed across frequencies and the gap between SGD and Adam can be reproduced with smaller transformers. This effect also appears when training only the last layer, and in the deterministic setting, comparing GD and Adam. ", "page_idx": 17}, {"type": "text", "text": "B.1 Class distribution for common datasets and tokenizers ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Figure 10 provides additional examples of the heavy-tailed distribution of tokens using the basic english tokenizer in torchtext (Paszke et al., 2019), Byte-Pair Encoding (BPE, Sennrich et al., 2016; Gage, 1994) and Unigram (Kudo, 2018) on the PTB and WikiText-2 datasets. The relationship between the relative frequency rank $k$ and and the relative frequency $\\pi_{k}$ is roughly $\\pi_{k}\\propto1/k$ . ", "page_idx": 17}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/fbaa7b597fb06331243e76e55dcb9ce08e2a9c830550df5ca54a2344d480839d.jpg", "img_caption": ["Figure 10: Different tokenizers and datasets lead to heavy-tailed token distributions. Comparison of word and subword tokenization (BPE, Unigram) on the PTB and WikiText2 datasets. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.2 Effect of class imbalance on validation loss ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Figure 11, we show the validation error on the same problem as Figure 1, training GPT2-Small on WikiText-103. The validation loss exhibits the same separation across class frequencies, and the faster progress of Adam on low-frequency classes is also visible. While this trend does not hold for all the settings we investigate, as some settings use smaller datasets and deterministic training to isolate the source of the training difficulties, the benefit of Adam on low-frequency classes does not immediately lead to overfitting. ", "page_idx": 17}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/ffafc412309b45c4ce502f4aa0ba31185ffdc343cb860b2ce856ec1048f2bb95.jpg", "img_caption": ["Figure 11: The class-separation behavior of Figure 1 holds on the validation loss. Same experiment as Figure 1, training GPT2-Small on WikiText-103, but showing the validation loss. (a) Distribution of the classes sorted by class frequency, split into groups corresponding to ${\\approx}10\\%$ of the data. (b) Overall validation loss. (c, d) Validation loss for each group using SGD and Adam. SGD makes little to no progress on low-frequency classes while Adam makes progress on all groups. (b) is the average of (c, d) for the respective optimizer. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.3 Smaller transformers and deterministic training ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Section 2.3, we argued that the qualitatively different behavior on low-frequency classes between SGD and Adam in Figure 1 is not due to stochasticity. In this section, we provide additional results showing that this behavior appears across multiple batch sizes on language transformers of different sizes and that it can be reproduced in the deterministic setting. ", "page_idx": 18}, {"type": "text", "text": "In Figure 12, we show that a similar qualitative behavior appears when training a smaller model (2-layer transformer) on a smaller dataset (PTB). In Figure 13, we repeat the experiment with a 1-layer transformer, trained in full batch on TinyPTB (the validation set of PTB). The separation between GD and Adam on low-frequency classes in the deterministic settings is also visible in Figures 2, 4, 5 and 7 in the main paper. These results indicate that stochasticity it is not necessary to reproduce the behavior observed in Figure 1. Finally, we repeat the experiment but freeze all the layers except the last, and still observe this behavior in Figure 14. ", "page_idx": 18}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/daa9d4ef9b844abac44502b4459dd2714942353d48f2048ec278897338404880.jpg", "img_caption": ["Figure 12: Similar behavior as Figure 1 on a smaller problem. Training a 2-layer transformer on PTB with Adam and SGD using larger batch-sizes. As in Figure 1, SGD makes little to no progress on low-frequency classes while Adam makes progress on all subsets. Subplots: (1) Distribution of the classes and subsets of the data sorted by class frequency, each corresponding to ${\\approx}10\\%$ of the samples. (2) Overall training loss. (3, 4) Training loss for each subset for SGD and Adam. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/3717f806bf472ca8cdb2c23f9747473787cdd580642fb26a7151a49777a21d3e.jpg", "img_caption": ["Figure 13: Similar behavior as Figure 1 on a one-layer transformer with deterministic updates. Trained on TinyPTB. Subplots: (1) Distribution of the classes and subsets of the data sorted by class frequency. (2) Overall training loss. (3, 4) Training loss for each subset for GD and Adam. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/785dc60a8b8943f37459deb28bdc7664e4c7b1dc250ffcab90016e9283970af8.jpg", "img_caption": ["Figure 14: Similar behavior as Figure 1 when training only the last layer. Training the last layer of a 1-layer transformer on PTB with Adam and GD with deterministic updates. Subplots: (1) Distribution of the classes and subsets of the data sorted by class frequency. (2) Overall training loss. (3, 4) Training loss for each subset for GD and Adam. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "C Vision problems ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "This section gives additional results on vision tasks to complement Section 2.1. ", "page_idx": 19}, {"type": "text", "text": "- Figure 15 shows a similar behavior on a ResNet18 with LayerNorm instead of BatchNorm.   \n- Figure 16 shows a similar behavior with a vision transformer.   \n- Figure 18 confirms that GD can solve the Barcoded MNIST variant without imbalance. ", "page_idx": 19}, {"type": "text", "text": "C.1 ResNet18 with LayerNorm ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Figure 15, we use the same settings Figure 3. training a ResNet18 on a uniform and unbalanced subset of ImageNet, but replace the normalization layers with LayerNorm (Ba et al., 2016) instead of BatchNorm (Ioffe and Szegedy, 2015). We observe a similar pattern as in Figure 3. Although Adam slightly outperforms SGD on the uniform dataset, the performance gap grows on the imbalanced one. ", "page_idx": 19}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/859d2d27fe0df7b1432f9fbfe4961955d1810d6de274d5acca22e753ed46242c.jpg", "img_caption": ["Figure 15: Adam outperforms SGD on ResNet with LayerNorm under heavy-tailed imbalance. (a) Performance on a uniform subset of ImageNet (b) and on an imbalanced subset with class frequencies $\\pi_{k}\\propto1/k$ . (c, d) Performance of GD and Adam across frequencies. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "C.2 Vision Transformers ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Figure 16, we train a vision transformer on the ImageNet dataset, without subsampling, to confirm that the training behavior is similar. While vision transformers might require more data or regularization than their ResNet counterparts to achieve comparable generalization performance, the optimization problem does not appear to be more difficult for SGD than for Adam. ", "page_idx": 19}, {"type": "text", "text": "Figure 16: Adam and SGD perform similarly training a Vision Transformer with balanced Classes. Training loss on the full ImageNet dataset (without subsampling). There is little performance in training performance. ", "page_idx": 19}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/ac12790c85b829ac3b681b113d1fde36f4be4689acfabf83a079798ab7a019b2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "In Figure 17, we train the same vision transformer on the uniform and imbalanced subsets of ImageNet. As in prior experiments with vision data, the performance of Adam appears unaffected by the change in class frequencies while the performance of SGD degrades. ", "page_idx": 19}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/1df1098ac84394a7d927646015a6c52b05220f11e7f5f8c0f1090ba0559d297c.jpg", "img_caption": ["Figure 17: Adam outperforms SGD on vision transformer under heavy-tailed imbalance. (a) Performance on a uniform subset of ImageNet (b) and on an imbalanced subset with class frequencies $\\pi_{k}\\propto1/k$ . (c, d) Performance of GD and Adam across frequencies. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "C.3 Sanity check on Barcoded MNIST ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Figure 2 in Section 2.1 showed that the performance gap between GD and Adam on the imbalanced variant of MNIST with barcoded images is larger than on plain MNIST. In this section, we verify that the training difficulties encountered on the CNN on the imbalanced MNIST dataset of Figure 2 are indeed due to class imbalance. As we create new images and new classes by adding a barcode in the corner of existing images, it could be that the dataset becomes harder to fit. ", "page_idx": 20}, {"type": "text", "text": "In Figure 18, we run Adam and GD to train the same network on the MNIST dataset only, the barcoded-only subset of the imbalanced MNIST and the combination of the two, leading to an imbalanced dataset. While Adam is faster GD on the barcoded-only dataset, both algorithms reach negligible error within 200 steps. In contrast, on the combined imbalanced dataset MNIST $\\mathbf{\\dot{+}B}$ arcoded, GD fails to make progress on the low-frequency classes and stalls. ", "page_idx": 20}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/e6d877e23bbdf74edaf70345a3a88da60259395fc7987c4a67c3e78a4612515c.jpg", "img_caption": ["Figure 18: GD optimizes on balanced barcoded data. Training a CNN on only the barcoded portion of the data, which has balanced classes. While Adam is slightly faster, both optimizers reach negligible error within 200 steps. As the level of imbalance is increased, GD performs increasingly worse than Adam. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "D Linear models ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Section 2.2 showed that GD is already slow on linear models. We give additional details here. ", "page_idx": 21}, {"type": "text", "text": "D.1 discusses the impact of the distribution of the inputs, as it is possible to construct problems exhibiting class imbalance without negatively impacting training.   \nD.2 shows that while (S)GD appears stuck in some experiments, it is not due to being stuck in a local minima. It eventually converges, although very slowly, if run long enough.   \nD.3 shows that while some of our datasets are separable, leading to weights going to $\\infty$ , class imbalance also impacts optimization when the weights remain small, e.g. when using l2 regularization. ", "page_idx": 21}, {"type": "text", "text": "D.1 Impact of input distribution ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Imbalance alone is not sufficient to induce slow performance of GD on low-frequency classes. It is possible to generate a dataset with heavy-tailed class imbalance where GD fits all classes fast, by making all inputs $\\mathbf{x}_{i}$ (close to) orthogonal, $\\langle\\mathbf{x}_{i},\\mathbf{x}_{j}\\rangle\\approx0$ for $i\\neq j$ . If all samples are orthogonal, $\\langle\\mathbf{x}_{i},\\mathbf{x}_{j}\\rangle=0\\,\\forall i\\neq j$ , a decomposition similar to that used in the proof of Theorem 3 shows that each sample is learned independently of the other, and class frequency has no impact. However, completely orthogonal data is rare. In the last layer of neural networks, we expect samples from the same class to be mapped to similar representation (Papyan et al., 2020), a phenomenon also observed under class imbalance (Thrampoulidis et al., 2022). Using a bias term also increases alignment between samples, as it is equivalent to adding a dimension where each sample has the same value. ", "page_idx": 21}, {"type": "text", "text": "In the setting of Theorem 3, class imbalance has an impact because samples from the same class are collinear, even though samples from separate classes are orthogonal. A more realistic mixture model where samples from the same class are aligned $(|\\langle\\mathbf{x}_{i},\\mathbf{x}_{j}\\rangle|>\\delta$ if $y_{i}=y_{j}$ ) but independent otherwise $(|\\langle\\mathbf{x}_{i},\\mathbf{x}_{j}\\rangle|\\,\\le\\,\\epsilon$ if $y_{i}\\neq y_{j}$ ), as the setting of Feldman (2020) would also exhibit class separation. The class imbalance appears in Figure 4 because we draw the inputs from a high-dimensional uniform distribution on $[0,1]^{d}$ , ensuring that for any two samples $\\mathbf{x}_{i},\\mathbf{x}_{j}$ , $\\langle\\mathbf{x}_{i},\\mathbf{x}_{j}\\rangle>0$ . If the data was sampled from $\\mathcal{N}(0,1)^{d}$ in sufficiently high dimension, the samples would be independent enough to avoid the slowdown due to class imbalance. We illustrate this in Figure 19, where we use a smaller synthetic data with inputs drawn from ${\\mathcal{N}}(1,1)$ and ${\\mathcal{N}}(0,1)$ . The zero-mean data is be approximately orthogonal as $d>n$ and does not exhibit a slow progress on low-frequency classes. ", "page_idx": 21}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/adc76ecb9560e6eee5d4ed1a61d55e63de5ff54317a8aac206432483c0a39327.jpg", "img_caption": ["Figure 19: The distribution of the inputs can have a large impact on optimization. Linear model on the Random Heavy-Tailed Labels dataset, with Inputs sampled from ${\\mathcal{N}}(1,1)$ (a) and ${\\mathcal{N}}(0,1)$ (b). "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "The behavior of GD on aligned data appears to be a better representation of the behavior of GD on language transformers, as we observe a performance separation per class frequency on GD, even when tuning only the last layer of a language transformer in Figure 5. Although the embedding weights are initialized to be zero-mean Gaussian noise, the representation of the tokens in a transformer are aligned, and this alignment increases with depth (Noci et al., 2022, e.g.). ", "page_idx": 22}, {"type": "text", "text": "D.2 An early iteration problem ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "As GD is slower than Adam at fitting the low-frequency classes, it might seem that GD does not fit the low-frequency classes at all. But when run for longer, GD converges and ftis all classes. We show this behavior on the linear model and the CNN on imbalanced MNIST in Figure 20. This highlight that the difference between the algorithms is primarily a difference at the start of training. However, this \u201cstart\u201d can be quite long. In the transformer of Figure 1, the average loss on $10\\%$ of the data corresponding to the least frequent classes is still higher than at initialization after 15k steps. ", "page_idx": 22}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/d3a6773a0ae72db6fe967d9a9c7314861d37ffc1f19beb66e7b28f9b1bb5c439.jpg", "img_caption": ["Figure 20: Training with GD eventually drives the loss down for all classes. Using the same step-size for different horizons (100, 1k, 10k). GD eventually drives the loss down for all classes, but the loss for the least-frequent classes only decreases below its value at initialization after 1k steps. (a) Linear model on synthetic data, (b) CNN on MNIST. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "D.3 Impact of regularization ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The data used with the linear model of Figure 4 is separable, meaning the predicted probabilities for the correct class will converge to 1 while the magnitude of weights go to $\\infty$ . This might lead to concerns that the observed behavior is tied to the weights growing without bounds. In Figure 21, we show that the gap between GD and Adam still appears with regularization limiting the magnitude of the weights. However, as regularization is increased, the L2 penalty makes it difficult to fit lowfrequency classes, the problem looks more like $\\lambda{\\frac{1}{2}}\\|\\cdot\\|^{2}$ , and the gap between the methods disappears. ", "page_idx": 23}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/f698b5659364bb37b862d13d49aa141490fc9bc35078e1814139e6222ea156c2.jpg", "img_caption": ["Figure 21: The separation between GD and Adam still appears when using $L_{2}$ regularization. Using varying levels of regularization $\\lambda$ on the linear model of Figure 4. The plots show the negative log-likelihood and do not include the $L_{2}$ penalty. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "E Alternative optimizers ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Figure 5 in Section 2.3 we compared GD and Adam to normalized GD and sign descent on the last layer of a one-module transformer on TinyPTB, showing that Adam and sign descent perform similarly. We repeat this experiment on other settings here to confirm that sign descent leads to similar benefits as Adam on low-frequency classes, and that changing the direction, as in sign descent, has more impact than just changing the magnitude, as in normalized GD. ", "page_idx": 24}, {"type": "text", "text": "We also observe this behavior on the following problems: ", "page_idx": 24}, {"type": "text", "text": "Figure 22: A linear model on Random Heavy-Tailed Labels, as in Figure 4.   \n- Figure 23: A one-module transformer on TinyPTB, as in Figure 13, training all layers.   \n- Figure 24: A CNN on MNIST $^+$ Barcoded, as in Figure 2. ", "page_idx": 24}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/e6cb5d2f75f1ca64272ad880482192a86b4f106b25d65ce850a3ec38996322ba.jpg", "img_caption": ["Figure 24: All optimizers on the CNN of Figure 2. First column: Overall training loss. Remaining: Loss by frequency groups for each optimizer, with and without momentum ( $\\mathrel{\\phantom{10}}^{\\cdot}+\\mathrm{m}$ , bottom/\u2212m, top). "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "E.1 Up-weighting low-frequency classes can improve the performance of SGD ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "To support Section 2.3, we show show that upweighting low-frequency classes helps reduce the performance gap between SGD and Adam on problems with heavy-tailed class imbalance, providing evidence that the optimization difficulties are associated with class imbalance. ", "page_idx": 25}, {"type": "text", "text": "While reweighting the loss of samples from class $k$ by $1/\\pi_{k}$ to address the class imbalance seems intuitive, optimizing the reweighted loss is no longer guaranteed to lead to progress on the original loss, especially if the weights are large. Indeed, we find that on some problems this reweighting does not improve performance (although SGD and Adam perform similarly on the reweighted loss, not shown). However, the less extreme reweighting of $1/\\sqrt{\\pi_{k}}$ appears to consistently outperform SGD. ", "page_idx": 25}, {"type": "text", "text": "In Figure 25, we run SGD on the reweighted loss with the two weighting schemes, $1/\\pi_{k}$ and $1/\\sqrt{\\pi_{k}}$ and plot its performance on the original, unweighted loss. We compare the performance of the two reweighting schemes with SGD and Adam, all with momentum, on the following 4 problems. ", "page_idx": 25}, {"type": "text", "text": "- The small transformer on PTB in Figure 12 (stochastic training) - The Linear model on synthetic data in Figure 4 (deterministic training) - The CNN on MNIST $^+$ Barcoded dataset in Figure 2 (deterministic training) - The ResNet18 on the Heavy-Tailed ImageNet dataset in Figure 3 (stochastic training) ", "page_idx": 25}, {"type": "text", "text": "We found that the combination of both Adam and reweighting did not improve over running Adam on the original loss and do no include it in Figure 25. ", "page_idx": 25}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/9159597af94f64f5b0f6882a3810fd48d13ebcdc911c2ea5569b480f0c20a7d1.jpg", "img_caption": ["Figure 25: Reweighting the loss improves the performance of (S)GD on low-frequency classes. The plots show the unweighted loss, while (S)GD and Adam optimize a reweighted loss. Reweighted (S)GD (r(S)GD) with weights $^1\\!/\\!\\sqrt{\\pi_{k}}$ consistently outperforms plain SGD, although it can lead to spikes, as on the CNN on the MNIST dataset. Reweighting with weights $1/\\pi_{k}$ is sometimes better (Linear, MNIST) but can be worse (PTB, ImageNet) as it optimizes a different objective. We use deterministic updates for the first 3 problems, labeled Epoch, and stochastic updates for the ResNet18 on heavy-tailed ImageNet. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "F Dynamics of the gradient and Hessian ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "This section provides additional details on the dynamics of (S)GD and Adam discussed in Section 3.2. ", "page_idx": 26}, {"type": "text", "text": "- Figure 26 shows the dynamics of GD and Adam on the linear model on synthetic data in Figure 4 (deterministic training). This figure complements Figure 7 which shows the dynamics over the path taken by Adam.   \n- Figure 27 and additionally shows the average predicted probabilities $p$ for each frequency group, showing that the deviation from the linear relationship for rare classes coincides with the predicted probabilities $p$ for those classes going to 1.   \n- The following figures show the correlation on additional problems, on - Figure 28 The GPT2-Small model on WikiText-103 in Figure 1 (stochastic training). This figure complements Figure 8 which shows the dynamics over the path taken by Adam. - Figure 29 The CNN on the MNIST $^{\\cdot}+$ Barcoded dataset in Figure 2 (deterministic training) - Figure 31 The small transformer on PTB in Figure 12 (stochastic training) - Figure 30 The ResNet18 on the Heavy-Tailed ImageNet dataset in Figure 3 (stochastic training)   \n- Figure 32 illustrates that this correlation does not hold globally and only emerges throughout ", "page_idx": 26}, {"type": "text", "text": "training by showing that a negative correlation can instead be found by looking at the oppositve path of the path taken by Adam, $-\\mathbf{W}_{t}$ (when $\\mathbf{W}_{t}$ are the iterates generated by Adam). ", "page_idx": 26}, {"type": "text", "text": "F.1 Linear model on synthetic data ", "text_level": 1, "page_idx": 26}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/a7f5ebd36d2d59e69b15d33b992d97287c5e2ffcf1f696fbcef8cda77267604a.jpg", "img_caption": ["Figure 26: Evolution of the gradient norm and Hessian trace through optimization. Taken over the path of GD (a) and Adam (b) on the linear problem of Figure 4. The blocks correspond to the rows $\\mathbf{w}_{1},...,\\mathbf{w}_{c}$ of the parameter matrix W. The color indicates the class frequency, showing that lower (higher) frequency classes have smaller (larger) gradient norm and Hessian trace. Figure 26b is a replication of Figure 7, given here for convenience. The deviation from the correlation is explainable by the fact that difference classes are learned at difference speed, leading to a different value of $p$ in Proposition 2, shown in Figure 27. For GD, frequent classes are learned faster than infrequent ones, while for Adam, $p$ is similar among the most frequent groups of classes while $p\\rightarrow1$ for the least frequent classes. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/ff42e4bf3510565cc7b37249fe9e25915b08f9537204d87097b3809884db7d59.jpg", "img_caption": ["Figure 27: Evolution of the predicted probabilities for the correct class. Complement to Figure 26, taken over the path of GD and Adam on the linear problem of Figure 4. For GD, frequent classes are learned faster than infrequent ones, Adam has a similar behavior on the most frequent groups of classes but also increases the predicted probability for the correct class on infrequent groups. The color indicates the class frequency, showing that lower (higher) frequency classes have smaller (larger) gradient norm and Hessian trace. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/57a9a8641a96d4c86ffc12d28a0a7d46bd39a4e9172aca81781ba35a6ed3298f.jpg", "img_caption": ["F.2 GPT2-Small on WikiText-103 ", "Figure 28: The gradient-Hessian blocks also become correlated in the last layer of large models. Reproducing Figure 7 on the GPT2-Small/WikiText-103 problem of Figure 1. Evolution of the gradient norm and Hessian trace for each row $\\mathbf{w}_{c}$ of the last layer throughout optimization, over the path taken by SGD (a) and Adam (b). The color indicates the class frequency, showing that lower (higher) frequency classes have smaller (larger) gradient norm and Hessian trace. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/1bb23140cd1678425b96597018279f67d454569f57b86796afbc15833325c458.jpg", "img_caption": ["Figure 29: Evolution of the gradient norm and Hessian trace through optimization. Taken over the path of GD and Adam on the CNN on imbalanced MNIST in Figure 2. Note that this problem only has two groups of classes with different frequencies; 10 classes have ${\\approx}5\\mathbf{k}$ samples while $10\\mathbf{k}$ classes have 5 samples. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "F.4 ResNet18 on Heavy-Tailed ImageNet ", "text_level": 1, "page_idx": 28}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/027dcf88162ca7fc8d09267c4b93eab25a599f4d92d6488f69c406bb0596eef4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 30: Evolution of the gradient norm and Hessian trace through optimization. Taken over the path of SGD and Adam on the ResNet18 on Heavy-Tailed ImageNet in Figure 3. ", "page_idx": 28}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/4d8e3b1e784f11d538a199b7fe1942bf83a2fd41857ec9add79984aa1fd0d72e.jpg", "img_caption": ["Figure 31: Evolution of the gradient norm and Hessian trace through optimization. Taken over the path of SGD and Adam on the small Transformer on PTB in Figure 12. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "F.6 The correlation depends on the path ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Proposition 2 requires that the optimizer make progress and assign samples to their correct classes. Indeed, the positive correlation observed in the previous figures is not a global property of the loss function. Not only does it not hold at initialization, where the Hessian is uniform, the correlation can even be reversed in some areas of the parameter space, as shown in Figure 32. ", "page_idx": 29}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/f7481b9426e0036d6dde9705087e0ce18b939448b8bbcfe8ac62af6389b76e1f.jpg", "img_caption": ["Figure 32: The correlation only holds while training. Correlation between the gradient and Hessian blocks through the path $\\left\\{-\\mathbf{W}_{t}\\right\\}$ , where $\\mathbf{W}_{t}$ are the iterates of Adam on the linear model of Figure 4. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "G Correlation between the gradient and Hessian across blocks ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "This section gives the proof of Proposition 2 in Section 3.2 ", "page_idx": 30}, {"type": "text", "text": "Proposition 2. If initialized at $\\mathbf{W}_{0}=0$ , the gradient and Hessian of the loss $\\mathcal{L}$ w.r.t. $\\mathbf{w}_{k}$ are ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{\\mathbf{w}_{k}}\\mathcal{L}(\\mathbf{W}_{0})=\\pi_{k}\\bar{\\mathbf{x}}^{k}-\\frac{1}{c}\\bar{\\mathbf{x}},\\qquad\\qquad\\nabla_{\\mathbf{w}_{k}}^{2}\\mathcal{L}(\\mathbf{W}_{0})=\\frac{1}{c}\\big(1-\\frac{1}{c}\\big)\\bar{\\mathbf{H}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "During training, if the model correctly assigns samples to class $k$ with probability $p$ (Assumption $^{\\,l}$ ), ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{\\nabla_{\\mathbf{w}_{k}}\\mathcal{L}=\\mathbf{\\Phi}(1-p)\\pi_{k}\\,\\bar{\\mathbf{x}}^{k}}&{+O\\big(\\frac{1}{c}\\big),}\\\\ {\\nabla_{\\mathbf{w}_{k}}^{2}\\mathcal{L}=p(1-p)\\pi_{k}\\,\\bar{\\mathbf{H}}^{k}+O\\big(\\frac{1}{c}\\big),}&{a n d}&{\\|\\nabla_{\\mathbf{w}_{k}}\\mathcal{L}\\|\\sim\\left(\\frac{1}{p}\\frac{\\|\\bar{\\mathbf{x}}^{k}\\|}{\\mathrm{Tr}(\\bar{\\mathbf{H}}^{k})}\\right)\\mathrm{Tr}(\\nabla_{\\mathbf{w}_{k}}^{2}\\mathcal{L})\\,a s\\,c\\to\\infty,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for classes where the frequency does not vanish too quickly, $\\pi_{k}=\\omega({^1}/c)$ . ", "page_idx": 30}, {"type": "text", "text": "The requirement that the class frequencies do not vanish, $\\pi_{k}\\,=\\,\\omega(1/c)$ , is necessary to make it possible to discuss class frequencies as $c\\rightarrow\\infty$ , unless the class frequencies do not depend on $c$ . While the frequencies $\\pi_{k}$ and the number of classes $c$ can be independent, for example if $\\pi_{k}$ follows an exponential decay, $\\pi_{k}\\propto2^{-k}$ , it does not hold for all distributions. While it may seem that this result only holds for relatively frequent classes, as it requires $\\pi_{k}c\\rightarrow\\infty$ , we can see that nearly all the data comes from classes where this correlation holds when the classes are distributed as $\\pi_{k}\\propto1/k$ . Denote by $\\begin{array}{r}{H(c)=\\sum_{k=1}^{c}1/k=\\Theta(\\log c)}\\end{array}$ . After normalization, we have $\\pi_{k}={1}/{k H(c)}$ . The correlation result holds as long as $\\pi_{k}c\\rightarrow\\infty$ , and so it at least holds for the first $k\\leq c/\\log(c)^{2}$ classes as $\\pi_{k}c\\geq\\log(c)\\to\\infty$ . While this only cover a $1/\\log(c)^{2}$ fraction of the classes, those classes account for nearly all the data as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{\\frac{c}{\\log(c)}\\rceil}\\pi_{k}=\\frac{H\\bigl(\\bigl\\lceil c/\\log(c)^{2}\\bigr\\rceil\\bigr)}{H(c)}=\\Theta\\biggl(\\frac{\\log(c)-2\\log\\log(c)}{\\log(c)}\\biggr)\\to1.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof of Proposition 2. We first recall the gradient and Hessian for each block $\\mathbf{w}_{1},...,\\mathbf{w}_{c}$ ; ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{w}_{k}}\\ell(\\mathbf{W},\\mathbf{x},\\mathbf{y})=(\\mathbf{1}[y=k]-\\mathbf{p}(\\mathbf{x})_{k})\\mathbf{x},\\qquad\\nabla_{\\mathbf{w}_{k}}^{2}\\ell(\\mathbf{W},\\mathbf{x},\\mathbf{y})=\\mathbf{p}(\\mathbf{x})_{k}(1-\\mathbf{p}(\\mathbf{x})_{k})\\mathbf{x}\\mathbf{x}^{\\top},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and the definitions of the moments of the data, per class and overall. ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\mathbf{x}}^{k}=\\frac{1}{n_{k}}\\sum_{i=1:y_{i}=k}^{n}\\mathbf{x}_{i},\\quad\\bar{\\mathbf{x}}=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{x}_{i},\\quad\\bar{\\mathbf{H}}^{k}=\\frac{1}{n_{k}}\\sum_{i=1:y_{i}=k}^{n}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top},\\quad\\bar{\\mathbf{H}}=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Our first step is to rewrite the sums for the gradient and Hessian to separate the influence of the samples of the correct class $k$ and the other samples. ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{\\displaystyle\\nabla_{\\mathsf{w}_{i}}\\mathcal{L}({\\mathbf{W}})=\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}(1[\\boldsymbol{\\mu}_{i}=k]-{\\mathbf{p}}(\\mathbf{x}_{i})_{i})\\mathbf{x}_{i},}\\\\ {\\displaystyle=\\frac{1}{n}\\displaystyle\\sum_{j=1}^{n}\\sum_{i_{1},i_{0}\\neq j}(1[\\boldsymbol{\\mu}_{i}=k]-{\\mathbf{p}}(\\mathbf{x}_{i})_{i})\\mathbf{x}_{i},}&{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{(Splith~yclass)}}\\\\ {\\displaystyle=\\sum_{j=1}^{n}\\boldsymbol{\\pi}_{j}\\displaystyle\\sum_{i_{1},i_{0},j_{0}=j}^{q}(1[\\boldsymbol{\\mu}_{i}=k]-{\\mathbf{p}}(\\mathbf{x}_{i})_{i})\\mathbf{x}_{i},}&{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{(Use~class~frequencies~}\\mathbf{x}_{j}=\\boldsymbol{n}_{j}/n)}\\\\ &{\\displaystyle=\\pi_{k}\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i_{1},i_{0}=1,i_{1}\\neq j}^{n}(1-{\\mathbf{p}}(\\mathbf{x}_{i})_{i})\\mathbf{x}_{i}+\\displaystyle\\sum_{j=1,j_{0},j_{0}}^{q}\\displaystyle\\frac{\\boldsymbol{\\pi}_{j}}{n}\\displaystyle\\sum_{i_{2},j_{0}=j}^{n}\\left(-[\\boldsymbol{\\Phi}(\\mathbf{x}_{i})_{i}\\mathbf{x}_{i},}\\\\ {\\nabla_{\\mathsf{w}_{i}}^{2}\\mathcal{L}({\\mathbf{W}})=\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i_{2}=1}^{n}\\sum_{j_{1}\\in\\mathcal{N}_{i}}(\\mathbf{x}_{i})_{i}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top},}&{}\\\\ {\\displaystyle=\\frac{\\boldsymbol{\\pi}_{k}}{n}\\displaystyle\\sum_{i_{1},i_{2}\\in\\mathcal{N}_{i}}\\mathbf{p}(\\mathbf{x}_{i})_{i}(1-{\\mathbf{p}}(\\mathbf{x}_{i})_{i})\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}+\\displaystyle\\sum_{j=1,j_{0},j_{0}=j}^{n}\\displaystyle\\frac{\\boldsymbol{ \n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We can simplify the first terms using the assumption that $p(\\mathbf{x}_{i})_{k}=p$ for samples of the correct class, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{\\tau_{k}}{\\i_{k}}\\sum_{\\substack{i=1:\\,y_{i}=k}}^{n}(1-\\mathbf{p}(\\mathbf{x}_{i})_{k})\\mathbf{x}_{i}=(1-p)\\pi_{k}\\bar{\\mathbf{x}}^{k},\\quad\\frac{\\pi_{k}}{n_{k}}\\sum_{\\substack{i;\\,y_{i}=k}}\\mathbf{p}(\\mathbf{x}_{i})_{k}(1-\\mathbf{p}(\\mathbf{x}_{i})_{k})\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}=p(1-p)\\pi_{k}\\bar{\\mathbf{H}}^{k}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We introduce the following shorthands for the second terms, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbf{d}_{k}=c\\sum_{j=1,j\\neq k}^{c}\\frac{\\pi_{j}}{n_{j}}\\sum_{i:y_{i}=j}(-\\mathbf{p}(\\mathbf{x}_{i})_{k})\\mathbf{x}_{i},\\;\\;\\;\\;\\;\\mathbf{D}_{k}=c\\sum_{j\\neq k}\\frac{\\pi_{j}}{n_{j}}\\sum_{i:y_{i}=j}\\mathbf{p}(\\mathbf{x}_{i})_{k}(1-\\mathbf{p}(\\mathbf{x}_{i})_{k})\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Using those simplifications, we obtain that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{w}_{k}}\\mathcal{L}(\\mathbf{W})=(1-p)\\pi_{k}\\bar{\\mathbf{x}}^{k}+\\frac{1}{c}\\mathbf{d}_{k},\\qquad\\quad\\nabla_{\\mathbf{w}_{k}}^{2}\\mathcal{L}(\\mathbf{W})=p(1-p)\\pi_{k}\\bar{\\mathbf{H}}^{k}+\\frac{1}{c}\\mathbf{D}_{k}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The terms $\\mathbf{d}_{k},\\mathbf{D}_{I}$ are averages of terms weighted by $c{\\bf p}({\\bf x}_{i})_{k}$ , which by assumption is $O(1)$ , and as such both $\\left\\|\\mathbf{d}_{k}\\right\\|$ and $\\mathrm{Tr}(\\mathbf{D}_{k})$ are $O(1)$ . The ratio between the two will be dominated by the contribution of their first term as long as \u03c0k dominates 1/c, in the sense that limc\u2192\u221e\u03c01kc \u21920, as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{c\\rightarrow\\infty}{\\operatorname*{lim}}\\,\\frac{\\Vert\\nabla_{\\mathbf{w}_{k}}\\mathcal{L}\\Vert}{\\mathrm{Tr}\\left(\\nabla_{\\mathbf{w}_{k}}^{2}\\mathcal{L}\\right)}=\\underset{c\\rightarrow\\infty}{\\operatorname*{lim}}\\,\\frac{\\left\\Vert\\left(1-p\\right)\\pi_{k}\\bar{\\mathbf{x}}^{k}+\\frac{1}{c}\\mathbf{d}_{k}\\right\\Vert}{\\mathrm{Tr}\\left(p\\left(1-p\\right)\\pi_{k}\\bar{\\mathbf{H}}^{k}+\\frac{1}{c}\\mathbf{D}_{k}\\right)}}\\\\ &{\\qquad\\qquad\\qquad=\\underset{c\\rightarrow\\infty}{\\operatorname*{lim}}\\,\\frac{\\left\\Vert\\left(1-p\\right)\\bar{\\mathbf{x}}^{k}+\\frac{1}{c\\pi_{k}}\\mathbf{d}_{k}\\right\\Vert}{\\mathrm{Tr}\\left(p\\left(1-p\\right)\\pi_{k}\\bar{\\mathbf{H}}^{k}+\\frac{1}{c\\pi_{k}}\\mathbf{D}_{k}\\right)}=\\frac{1}{p}\\frac{\\left\\Vert\\bar{\\mathbf{x}}^{k}\\right\\Vert}{\\mathrm{Tr}\\left(\\bar{\\mathbf{H}}^{k}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "G.1 Off-diagonal blocks are orders of magnitude smaller than diagonal blocks ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Our discussion Section 3.2 ignored the impact of off-diagonal blocks. In this section, we show that they are small. The diagonal and off-diagonal blocks of the matrix for $k\\neq k^{\\prime}$ . ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}{\\mathbf{H}_{k k}}&{:=\\nabla_{\\mathbf{w}_{k}}^{2}\\ell(\\mathbf{W},\\mathbf{x},y)}&{=\\mathbf{p}(\\mathbf{x})_{k}(1-\\mathbf{p}(\\mathbf{x})_{k})\\mathbf{x}\\mathbf{x}^{\\top},}\\\\ {\\mathrm{and~for~}j\\neq k,}&{\\mathbf{H}_{k j}}&{:=\\nabla_{\\mathbf{w}_{k}}\\nabla_{\\mathbf{w}_{k^{\\prime}}}\\ell(\\mathbf{W},\\mathbf{x},\\mathbf{y})}&{=\\mathbf{p}(\\mathbf{x})_{k}(\\mathbf{\\xi}-\\mathbf{p}(\\mathbf{x})_{k^{\\prime}})\\mathbf{x}\\mathbf{x}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "From this, we can see that, on average, the magnitude of the off-diagonal blocks will be smaller than that of the diagonal blocks, as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbf{H}_{k k}=-\\sum_{j=1,j\\neq k}^{c}\\mathbf{H}_{k j},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "because $\\begin{array}{r}{\\sum_{k^{\\prime}=1,k^{\\prime}\\neq k}^{c}\\mathbf{p}(\\mathbf{x})_{k}\\mathbf{p}(\\mathbf{x})_{k^{\\prime}}\\,=\\,\\mathbf{p}(\\mathbf{x})_{k}(1-\\mathbf{p}(\\mathbf{x})_{k})}\\end{array}$ , This means that the matrix $\\mathbf{T}:[c\\times c]$ formed by taking the trace of the blocks, $\\mathbf{T}_{j k}=\\mathrm{Tr}(\\mathbf{H}_{j k})$ , is diagonally dominant. ", "page_idx": 31}, {"type": "text", "text": "Figures 9 and 33 show that the magnitude of the entries of the Hessian in off-diagonal blocks is orders of magnitude smaller than those of the diagonal blocks. Instead of plotting the $\\bar{[}c d\\times c d]$ Hessian, we subsample 40 classes and 40 input dimensions and plot the resulting $[160\\times160]$ entries at different points throughout the trajectory of Adam on the problem of Figure 4. Figure 9 shows the matrices with classes sampled uniformly and Figure 33 with classes sampled log-uniformly ", "page_idx": 31}, {"type": "image", "img_path": "T56j6aV8Oc/tmp/c7ba52809c5fd0bd2bbe19bd3c9f6df0cef7c17817c817cccedcb46f0b91565b.jpg", "img_caption": ["Figure 33: The off-diagonal blocks are much smaller than the diagonal blocks. Showing the magnitude $\\log_{10}(\\left|(\\nabla^{2}\\check{\\mathcal{L}})_{i j}\\right|)$ for a $[160\\times160]$ subset of the Hessian, sampling 40 classes and 40 input dimensions uniformly. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "H Continuous time GD and sign descent on a simple imbalanced problem ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We give the proof of Theorem 3 on the simple imbalanced setting, restated here for convenience. ", "page_idx": 32}, {"type": "text", "text": "Simple imbalanced setting. Consider c classes with frequencies $\\pi_{1},...,\\pi_{c}$ where all samples from $a$ class are the same, $\\mathbf{x}_{i}=\\mathbf{e}_{k}\\;i f y_{i}=k,$ , where $\\mathbf{e}_{k}$ is the kth standard basis vector in $\\mathbb{R}^{\\epsilon}$ . ", "page_idx": 32}, {"type": "text", "text": "Theorem 3. On the simple imbalanced setting, gradient flow and continuous time sign descent initialized at $\\mathbf{W}=0$ minimize the loss of class $k$ , $\\ell_{k}(t)=-\\log(\\sigma(\\mathbf{W}(t)\\mathbf{e}_{k})_{k})$ , at the rate ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\ell_{k}(t)=\\Theta\\big(e^{-c t}\\big).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We separate the proof for gradient flow into 3 parts. Lemma 4 simplifies the dynamics into smaller, independent differential equations, Lemma 5 solves the differential equation and Lemma 6 bounds the loss. The proof uses similar tools as for the gradient flow dynamics studied by Cabannes et al. (2024), but we focus instead on the loss per class. We treat continuous time sign descent separately in Lemma 7. ", "page_idx": 32}, {"type": "text", "text": "Notation. If $\\mathbf{W}$ is a $[a\\times b]$ matrix, then $\\mathbf{w}_{1},...,\\mathbf{w}_{a}$ are the rows and $\\mathbf{w}^{1},...,\\mathbf{w}^{b}$ are the vectors, and $w_{i j}$ is the entry at the ith column, $j$ th row. For brevity, we use $z=c-1$ as the term appears often. ", "page_idx": 32}, {"type": "text", "text": "Lemma 4 (Separation of the dynamics). The dynamics of the parameter matrix W separate into $c$ 2-dimensional differential equations, $w_{k k}(t)=a_{k}(t)$ and $w_{j k}(t)=b_{k}(t)$ for $j\\neq k$ , where ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{a_{k}(0)=0,\\qquad}&{}&{\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}a_{k}=\\pi_{k}\\biggl(1-\\frac{\\exp(a_{k})}{\\exp(a_{k})+(c-1)\\exp(b_{k})}\\biggr),}\\\\ {b_{k}(0)=0,\\qquad}&{}&{\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}b_{k}=\\pi_{k}\\biggl(\\begin{array}{l l}{-\\,\\frac{\\exp(b_{k})}{\\exp(a_{k})+(c-1)\\exp(b_{k})}\\biggr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. Our goal is to simplify the dynamics starting at $\\mathbf{W}(0)=0$ and following the gradient flow $\\begin{array}{r}{\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathbf{W}=-\\nabla\\mathcal{L}(\\mathbf{W})}\\end{array}$ , where $\\mathbf{W}:[c\\times d]$ . For the simplified setting, we have that $d=c$ are the inputs are the standard basis vectors in $\\mathbb{R}^{c}$ . The derivative of $\\mathcal{L}$ w.r.t. a single element $w_{k j}$ is ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\partial_{w_{k j}}\\mathcal{L}(\\mathbf{W})=-\\pi_{k}\\mathbf{1}[k=j]+\\pi_{j}\\sigma(\\mathbf{w}^{j})_{k}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "As $\\partial_{w_{k j}}$ only depends on $\\mathbf{w}^{j}$ for all $k$ , The dynamics are independent across the columns of $\\mathbf{W}$ , giving $c$ independent equations in $\\mathbb{R}^{c}$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbf{w}^{j}(0)=0,\\qquad\\qquad\\qquad{\\frac{\\mathrm{d}}{\\mathrm{d}t}}\\mathbf{w}^{j}=\\pi_{j}(\\mathbf{e}_{j}-\\sigma(\\mathbf{w}^{j})).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "To further simplify the dynamics, we use the fact that the weights that are not associated with the correct class have the same dynamics. For any indices $i,j$ different from $k$ , $w_{i k}(t)=w_{j k}(t)$ . They have the same derivatives if they have the same value, as ", "page_idx": 32}, {"type": "equation", "text": "$$\n-\\frac{\\mathrm{d}}{\\mathrm{d}t}w_{i k}=\\pi_{k}\\sigma(\\mathbf{w}^{k})_{i}=\\pi_{k}\\frac{\\exp(w_{i k})}{\\sum_{k^{\\prime}}\\exp(w_{k^{\\prime}k})}=\\pi_{k}\\frac{\\exp(w_{j k})}{\\sum_{k^{\\prime}}\\exp(w_{k^{\\prime}k})}=\\pi_{k}\\sigma(\\mathbf{w}^{k})_{j}=-\\frac{\\mathrm{d}}{\\mathrm{d}t}w_{j k},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "so they will have the same dynamics and the equation can be reduced to a system of 2 variables, $w_{k k}=a_{k}$ and $w_{j k}=b_{k}$ for any $j\\neq k$ , with ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{a_{k}(0)=0,\\qquad}&{}&{\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}a_{k}=\\pi_{k}\\biggl(1-\\frac{\\exp(a_{k})}{\\exp(a_{k})+(c-1)\\exp(b_{k})}\\biggr),}\\\\ {b_{k}(0)=0,\\qquad}&{}&{\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}b_{k}=\\pi_{k}\\biggl(\\begin{array}{l l}{-\\,\\frac{\\exp(b_{k})}{\\exp(a_{k})+(c-1)\\exp(b_{k})}\\biggr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Lemma 5 (Solution of the dynamics). For a given class with frequency $\\pi$ , the dynamics of the parameters a and $b$ in Lemma 4 evolve as follows, using the shortcuts $f(t)=1+c\\pi t$ and $z=c-1$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\na(t)=\\frac{1}{c}\\bigg(f(t)-z W\\bigg(\\frac{1}{z}\\exp\\bigg(\\frac{1}{z}f(t)\\bigg)\\bigg)\\bigg)\\qquad\\qquad\\qquad b(t)=-\\frac{1}{z}a(t),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. We want the solution to the differential equation ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{a(0)=0}&{{}\\quad\\quad}&{\\cfrac{\\mathrm{d}}{\\mathrm{d}t}a=\\pi\\bigg(1-\\cfrac{\\exp(a)}{\\exp(a)+(c-1)\\exp(b)}\\bigg),}\\\\ {b(0)=0}&{{}\\quad\\quad}&{\\cfrac{\\mathrm{d}}{\\mathrm{d}t}b=\\pi\\bigg(\\textit{--}\\cfrac{\\exp(b)}{\\exp(a)+(c-1)\\exp(b)}\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The general solution, ignoring the initial conditions, uses the Lambert $W$ function and constants $K_{1},K_{2}$ .1For brevity, we introduce the shortcut $z=c-1$ . ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{a(t)=}&{}&{\\frac{1}{z c}\\bigg(c e^{-K_{1}}K_{2}+c z\\pi t-z^{2}W\\bigg(\\frac{1}{z}\\exp\\Bigl(\\frac{c}{z^{2}}\\bigl(z\\pi t+e^{-K_{1}}K_{2}\\bigr)-K_{1}\\Bigr)\\bigg)\\bigg),}\\\\ {b(t)=}&{}&{K_{1}-\\frac{1}{z^{2}c}\\bigg(c e^{-K_{1}}K_{2}+c z\\pi t-z^{2}W\\bigg(\\frac{1}{z}\\exp\\Bigl(\\frac{c}{z^{2}}\\bigl(z\\pi t+e^{-K_{1}}K_{2}\\bigr)-K_{1}\\Bigr)\\bigg)\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We need to set $K_{1},K_{2}$ to satisfy the initial conditions $a(0)=b(0)=0$ . As $b(t)=K_{1}-a(t)/z$ , we must have that $K_{1}=0$ , giving the simplification ", "page_idx": 33}, {"type": "equation", "text": "$$\na(t)=\\frac{1}{z c}\\bigg(c K_{2}+c z\\pi t-z^{2}W\\bigg(\\frac{1}{z}\\exp\\Bigl(\\frac{c}{z^{2}}(z\\pi t+K_{2})-K_{1}\\Bigr)\\bigg)\\bigg),\\qquad b(t)=-\\frac{1}{z}a(t).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "To set $K_{2}$ , we need to have ", "page_idx": 33}, {"type": "equation", "text": "$$\n0=z c a(0)=c K_{2}-z^{2}W\\bigg(\\frac{1}{z}\\exp\\Big(K_{2}\\frac{c}{z^{2}}\\Big)\\bigg)\\implies W\\bigg(\\frac{1}{z}\\exp\\Big(K_{2}\\frac{c}{z^{2}})\\Big)\\bigg)=\\frac{c}{z^{2}}K_{2}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Since $W(x e^{x})=x$ for $x>0$ , the equation is satisfied for $\\begin{array}{r}{K_{2}=\\frac{z}{c}}\\end{array}$ , as we get $\\begin{array}{r}{W\\Big(\\frac{1}{z}e^{\\frac{1}{z}}\\Big)=\\frac{1}{z}}\\end{array}$ , giving ", "page_idx": 33}, {"type": "equation", "text": "$$\na(t)=\\frac{1}{c}\\bigg(1+c\\pi t-z W\\bigg(\\frac{1}{z}\\exp\\bigg(\\frac{1}{z}(1+c\\pi t)\\bigg)\\bigg)\\bigg)\\qquad\\qquad b(t)=-\\frac{1}{z}a(t).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Lemma 6 (Bound for the loss). For $t$ sufficiently large such that $1+c\\pi_{k}t\\geq z\\log z+1,$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\ell_{k}(t)=\\Theta\\biggl(\\frac{1}{\\pi_{k}t}\\biggr).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Using the simplification derived in Lemma 4 and the solution of the differential equation in Lemma 5, we can rewrite the loss for a specific class as a function of time as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle L_{k}(\\mathbf{W}):=-\\log(\\sigma(\\mathbf{W}\\mathbf{e}_{k})_{k})=-\\log\\left(\\frac{\\exp(w_{k k})}{\\sum_{j=1}^{c}\\exp(w_{j k})}\\right),}\\\\ {\\displaystyle\\dot{\\mathbf{\\eta}}_{k}(t):=L_{k}(\\mathbf{W}(t))=-\\log\\left(\\frac{\\exp(a_{k}(t))}{\\exp(a_{k}(t))+(c-1)\\exp(b_{k}(t))}\\right)=\\log(1+(c-1)\\exp(c b_{k}(t))),}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the equality uses that $a_{k}(t)=(c-1)b_{k}(t)$ . For brevity, we will drop the index $k$ in $a_{k},b_{k},\\ell_{k}$ and $\\pi_{k}$ and use the shortcut $z=c-1$ , bounding the quantity ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\ell(t)=\\log(1+z\\exp(c b(t))).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Expanding the definition of $b(t)$ using Lemma 5, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\nz\\exp(c b(t))=z\\exp\\Biggl(-\\frac{1}{z}\\biggl(f(t)-z W\\biggl(\\frac{1}{z}\\exp\\biggl(\\frac{1}{z}f(t)\\biggr)\\biggr)\\biggr)\\Biggr),\\quad\\mathrm{where}\\quad f(t)=1+c\\pi t.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "To simplify the $W$ function, we use the fact that for $x>e$ (Hoorfar and Hassani, 2008, Theorem 2.7) ", "page_idx": 33}, {"type": "equation", "text": "$$\nW(x)=\\log(x)-\\log(\\log(x))+\\delta(x)\\qquad{\\mathrm{where}}\\qquad{\\frac{1}{2}}\\leq\\delta(x){\\frac{\\log(x)}{\\log(\\log(x))}}\\leq{\\frac{e}{e-1}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "To use this bound on $\\begin{array}{r}{W\\big(\\frac{1}{z}\\exp\\big(\\frac{1}{z}f(t)\\big)\\big)}\\end{array}$ , we need $\\textstyle{\\frac{1}{z}}\\exp\\!\\left({\\frac{1}{z}}f(t)\\right)\\;\\geq\\;e$ , which is satisfied for $t$ sufficiently large, once $f(t)\\geq z(\\log z+1)$ . ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(t)-z W\\biggl(\\frac{1}{z}\\exp\\biggl(\\frac{1}{z}f(t)\\biggr)\\biggr)=f(t)-z\\biggl(\\frac{1}{z}f(t)-\\log(z)-\\log\\biggl(\\frac{1}{z}f(t)-\\log(z)\\biggr)+h(t)\\biggr),}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=z(\\log(f(t)-z\\log(z))-h(t)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "giving the simplification ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z\\exp(c b(t))=z\\exp\\biggl(-\\frac{1}{z}\\biggl(f(t)-z W\\biggl(\\frac{1}{z}\\exp\\biggl(\\frac{1}{z}f(t)\\biggr)\\biggr)\\biggr)\\biggr),}\\\\ &{\\qquad\\qquad=z\\exp(-\\log(f(t)-z\\log(z))+h(t))=\\frac{z\\exp(h(t))}{f(t)-z\\log z},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This gives the average loss ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\ell(t)=\\log(1+z\\exp(c b(t)))=\\log\\biggl(1+{\\frac{z\\exp(h(t))}{f(t)-z\\log z}}\\biggr)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "To bound this expression, we can use that $\\frac{z\\exp(h(t))}{f(t)\\!-\\!z\\log z}\\geq0$ after $f(t)\\geq z\\log z$ , which we have already assumed to apply the bound on the $W$ function, and use the bounds $\\textstyle{\\frac{x}{1+x}}\\leq\\log(1+x)\\leq x$ to get ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{z\\exp(h(t))}{f(t)-z\\log z+z\\exp(h(t))}\\leq\\ell(t)\\leq\\frac{z\\exp(h(t))}{f(t)-z\\log z}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "As $h(t)$ is upper bounded by a constant and $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow\\infty}h(t)=0}\\end{array}$ , $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\exp(h(t))=1}\\end{array}$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\ell(t)=\\Theta\\!\\left({\\frac{z}{f(t)-z\\log z}}\\right)=\\Theta\\!\\left({\\frac{1}{\\pi t}}\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Lemma 7. The loss at time $t$ for continuous time sign descent is $\\ell_{k}(t)=\\log(1+(c-1)\\exp(-c t))$ Proof. The same decomposition as in Lemma 4 hold, with the dynamics ", "page_idx": 34}, {"type": "equation", "text": "$$\na_{k}(0)=0,\\ \\ \\ \\ \\frac{\\mathrm{d}}{\\mathrm{d}t}a_{k}=1,\\ \\ \\ \\ a_{k}(t)=t,\\ \\ \\ \\ \\ \\ \\ b_{k}(0)=0,\\ \\ \\ \\ \\ \\frac{\\mathrm{d}}{\\mathrm{d}t}b_{k}=-1,\\ \\ \\ \\ b_{k}(t)=-t,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "leading to the following loss ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\ell_{k}(t)=\\log(1+(c-1)\\exp(-c t))=\\Theta(z\\exp(-c t)).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The main claims of the abstract and introduction refer to the following sections. ", "page_idx": 35}, {"type": "text", "text": "\u2022 Section 2 and Figures 1 to 5 support the claim that heavy-tailed class imbalance leads to a performance gap between SGD and Adam, and that this gap can be made to appear by taking typically uniform datasets and making the class distribution heavy-tailed. \u2022 Section 3 and Figures 6 to 9 supports the analysis on a softmax linear model under heavy-tailed class imbalance, where Adam outperforms SGD. We provide a toy example where a correlation between the magnitude of the gradient and Hessian across coordinates can be argued to benefti Adam (Section 3.1), show that the class imbalance on a linear model leads to such a correlation and explain how through an assignment mechanism in (Section 3.2), and prove that, on a simplified problem and in continuous time, gradient descent performs poorly on low-frequency classes while sign descent is unaffected (Section 3.3). ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We discuss the main limitations of our results, regarding the interaction between class imbalance and stochasticity, the validity of our findings on generalization error, and additional optimization difficulties not captured by class imbalance in Section 4. Throughout the paper, we point out subtleties that are further discussed in the appendix, such that it is possible to create pathological imbalance datasets that are still easy to optimize with GD in Section 2.2, that the correlation between class frequencies, gradients and Hessian due to the assignment mechanism is not a global property and requires the model to fti the data in Section 3.2, and point out what properties of the optimization dynamics are not capture by the simple imbalanced setting in Section 3.3. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper. \u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. ", "page_idx": 35}, {"type": "text", "text": "\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The proof of Proposition 2 relies on Assumption 1 and is given in Appendix G.   \nThe proof of Theorem 3 on the simple imbalanced setting is given in Appendix H. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 36}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The experimental details needed to reproduce the main experimental results are given in Appendix A. We use standard architectures and datasets where possible and a simple training procedure for reproducibility, and the main claims of section Section 2.2 and Section 3 can easily be reproduced on linear models with synthetic data. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 36}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: The code to reproduce our experiments is uploaded to the openreview submission and will be made publicly available. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: The experimental setting are presented at a high-level in Section 2 and detailled in Appendix A. The accompagnying code provides a full specification of the experiments. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The main figures do not report error bars, as the figures show detailled trajectories for specific runs that do not lend themself to show the behavior averaged over mutliple runs. Instead, we account for factors of variability by reproducing the observed behavior, the performance gap across class frequencies, across multiple datasets, architectures, and training procedures. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified. ", "page_idx": 38}, {"type": "text", "text": "\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). \u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Appendix A.4 lists the type of compute and the estimated overall total compute budget used, including preliminary experiments. The details of per-experiment compute type and budget is available in the code, where each experiment file specifies the hardware configuration and runtime. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The authors confirm that the research was conducted conforming to the Code of Ethics. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper focuses on foundational research to understand the behavior of generic algorithms used to optimize neural networks. The paper is not tied to a particular applications and we do not see a direct path to a social impact impact. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 39}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: This paper does not release data or models. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 40}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Appendix A gives references for the datasets, models, and code used in this project, along with citations to the original papers and URLs where available. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 40}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper does not release new assets. Existing publicly available datasets are used to create variants with more classes, using the procedures are detailled in Appendix A. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: the paper does not involve research with human subjects. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 42}]