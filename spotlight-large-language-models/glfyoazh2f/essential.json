{"importance": "This paper is crucial for researchers working on controllable language models and trustworthy AI.  It introduces a novel semi-supervised approach to selective generation, improving efficiency and offering theoretical guarantees on controlling the false discovery rate. This work opens new avenues for mitigating the hallucination problem in large language models and advances the field of certified risk control methods.", "summary": "Certified selective generation controls language model hallucinations by leveraging textual entailment and a novel semi-supervised algorithm, guaranteeing a controlled false discovery rate.", "takeaways": ["A novel semi-supervised selective generation algorithm (SGenSemi) is proposed, which controls the false discovery rate while fully utilizing unlabeled data.", "Textual entailment is used as the evaluation metric to address the metric misalignment problem in language generation tasks.", "Neuro-selection functions are introduced to improve the selection efficiency of selective generation."], "tldr": "Generative Language Models (GLMs) often produce inaccurate or 'hallucinated' information, hindering their use in critical applications. Current risk mitigation methods, like selective prediction, struggle with GLMs due to the lack of an appropriate correctness metric for evaluating generated text. This is because multiple valid answers exist for many questions, making simple metrics like exact match unreliable.\nThe paper tackles this issue by proposing two selective generation algorithms (SGensup and SGenSemi). These algorithms use textual entailment to evaluate answer correctness and control the false discovery rate. SGenSup uses supervised learning with entailment-labeled data, while SGenSemi leverages unlabeled data through pseudo-labeling and conformal prediction. SGenSemi further introduces 'neuro-selection functions', allowing for a more general class of selection functions and improved efficiency.  The authors demonstrate the efficacy of their approach on various GLMs.", "affiliation": "POSTECH", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "glfYOAzh2f/podcast.wav"}