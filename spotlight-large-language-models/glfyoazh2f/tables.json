[{"figure_path": "glfYOAzh2f/tables/tables_8_1.jpg", "caption": "Table 1: Comparison results of semi-supervised methods. Here, |Z<sub>U</sub>|=10K for GPT-3.5-turbo and Alpaca-7B. The best results are highlighted in bold and results from methods that do not satisfy desired FDR-E guarantees in learning are underlined.", "description": "This table compares the performance of several semi-supervised methods for selective generation on two language models, GPT-3.5-turbo and Alpaca-7B.  The methods are evaluated based on their false discovery rate with respect to textual entailment (FDR-E) and their selection efficiency.  The table highlights the best-performing methods and indicates which methods failed to meet the desired FDR-E guarantee.", "section": "5 Experiments"}, {"figure_path": "glfYOAzh2f/tables/tables_8_2.jpg", "caption": "Table 2: Qualitative results by Alpaca7B.", "description": "This table shows qualitative results of the Alpaca7B model in terms of whether the model's generated answer is correct or not according to human judgment. It demonstrates the effectiveness of the proposed method (SGenSemi) in selecting correct answers while rejecting incorrect ones.  Specifically, it compares the results of the proposed method with the baseline method (SGENEM) on two different question-answer pairs.", "section": "5 Experiments"}, {"figure_path": "glfYOAzh2f/tables/tables_15_1.jpg", "caption": "Table 1: Comparison results of semi-supervised methods. Here, |Z\u028a| = 10K for GPT-3.5-turbo and Alpaca-7B. The best results are highlighted in bold and results from methods that do not satisfy desired FDR-E guarantees in learning are underlined.", "description": "This table compares the performance of several semi-supervised methods for selective generation.  The key metrics are the False Discovery Rate with respect to textual entailment (FDR-E) and the selection efficiency.  The table shows results for two different language models (GPT-3.5-turbo and Alpaca-7B) and uses 10,000 unlabeled samples.  The best performing methods for each model are highlighted in bold, while methods that failed to meet the desired FDR-E guarantee are underlined.", "section": "5 Experiments"}, {"figure_path": "glfYOAzh2f/tables/tables_15_2.jpg", "caption": "Table 1: Comparison results of semi-supervised methods. Here, |Z\u028a| = 10K for GPT-3.5-turbo and Alpaca-7B. The best results are highlighted in bold and results from methods that do not satisfy desired FDR-E guarantees in learning are underlined.", "description": "This table compares the performance of several semi-supervised methods for selective generation.  The methods are evaluated based on their False Discovery Rate with respect to textual entailment (FDR-E) and their efficiency (the ratio of non-abstained generations).  The experiment uses 10,000 unlabeled samples for both GPT-3.5-turbo and Alpaca-7B language models. The best-performing methods for each metric are highlighted in bold, and methods that failed to meet the desired FDR-E level are underlined.", "section": "5 Experiments"}, {"figure_path": "glfYOAzh2f/tables/tables_15_3.jpg", "caption": "Table 1: Comparison results of semi-supervised methods. Here, |Z\u028a| = 10K for GPT-3.5-turbo and Alpaca-7B. The best results are highlighted in bold and results from methods that do not satisfy desired FDR-E guarantees in learning are underlined.", "description": "This table compares the performance of several semi-supervised methods for selective generation in controlling the False Discovery Rate with respect to Textual Entailment (FDR-E).  The methods are evaluated on two language models (GPT-3.5-turbo and Alpaca-7B) using 10,000 unlabeled data points.  The table shows the achieved FDR-E and the selection efficiency for each method.  The best performing methods (lowest FDR-E) are highlighted in bold.  Methods that failed to meet the desired FDR-E guarantee are underlined.", "section": "5 Experiments"}, {"figure_path": "glfYOAzh2f/tables/tables_19_1.jpg", "caption": "Table 1: Comparison results of semi-supervised methods. Here, |Z\u028a| = 10K for GPT-3.5-turbo and Alpaca-7B. The best results are highlighted in bold and results from methods that do not satisfy desired FDR-E guarantees in learning are underlined.", "description": "This table compares the performance of several semi-supervised methods for controlling the false discovery rate via textual entailment (FDR-E).  The methods are evaluated on two large language models (GPT-3.5-turbo and Alpaca-7B) using 10,000 unlabeled samples.  The best results for FDR-E and efficiency are highlighted in bold.  Underlined results indicate methods that failed to meet the desired FDR-E guarantee during training.", "section": "5 Experiments"}, {"figure_path": "glfYOAzh2f/tables/tables_25_1.jpg", "caption": "Table 3: Comparison results of fully supervised methods. Here, we use all entailment labels, i.e., |ZE| = |Z| for GPT-3.5-turbo and Alpaca-7B. The best results are highlighted in bold, results from methods that do not satisfy desired FDR-E guarantee are underlined. In GPT-3.5-turbo and Alpaca-7B, the best efficiency values among methods that satisfy a desired FDR-E guarantee are 0.7535 and 0.2959, respectively, which serve as the best achievable efficiency results of semi-supervised methods.", "description": "This table compares the performance of fully supervised selective generation methods on two language models, GPT-3.5-turbo and Alpaca-7B.  The methods are evaluated based on their False Discovery Rate with respect to textual entailment (FDR-E) and selection efficiency.  The best results (lowest FDR-E and highest efficiency) are highlighted, and those that did not meet the desired FDR-E guarantee are indicated.  The table also notes that the best efficiency values for the fully supervised methods serve as a benchmark for comparing the semi-supervised methods.", "section": "Additional Experiments"}, {"figure_path": "glfYOAzh2f/tables/tables_26_1.jpg", "caption": "Table 4: Comparison results of semi-supervised methods. Here, |Z\u028a| = 10K for GPT-3.5-turbo and Alpaca-7B. The best results are highlighted in bold and results from methods that do not satisfy desired FDR-E guarantee are underlined. We used QA2D dataset, filtered with only SQUAD, where human transformed QA sentences exist. \u03b5 = 0.15.", "description": "This table compares the performance of several semi-supervised methods for selective generation on two language models (GPT-3.5-turbo and Alpaca-7B).  The key metrics are FDR-E (False Discovery Rate with respect to Entailment) and efficiency (percentage of instances where the model does not abstain). The table highlights the best performing methods and indicates which methods failed to meet the desired FDR-E guarantee (underlined values).  The dataset used is the QA2D dataset, filtered to include only questions from the SQUAD subset, where human-labeled entailment information is available. A stricter FDR-E threshold (\u03b5 = 0.15) was used for this evaluation.", "section": "5 Experiments"}, {"figure_path": "glfYOAzh2f/tables/tables_26_2.jpg", "caption": "Table 5: Comparison results of fully supervised methods. Here, we use all entailment labels, i.e., |ZE| = |Z| for GPT-3.5-turbo and Alpaca-7B. The best results are highlighted in bold, results from methods that do not satisfy desired FDR-E guarantee are underlined. We used QA2D dataset, filtered with only SQuAD, where human transformed QA sentences exist. \u03b5 = 0.15.", "description": "This table compares the performance of fully supervised methods for controlling the False Discovery Rate with respect to textual entailment (FDR-E).  It shows the FDR-E and efficiency achieved by different methods using two different scoring functions (fM1 and fM2) on two language models (GPT-3.5-turbo and Alpaca-7B). The dataset used is the QA2D dataset filtered to include only the SQuAD subset with human-transformed questions and answers, and the desired FDR-E level (\u03b5) is set to 0.15.  Underlined results indicate that the method did not meet the FDR-E guarantee during training.", "section": "Additional Experiments"}]