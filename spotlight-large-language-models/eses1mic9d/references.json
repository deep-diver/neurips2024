{"references": [{"fullname_first_author": "Usman Anwar", "paper_title": "Foundational challenges in assuring alignment and safety of large language models", "publication_date": "2024-04-09", "reason": "This paper is foundational to the current work, highlighting challenges in ensuring the safety and alignment of LLMs, which are directly addressed in this paper."}, {"fullname_first_author": "Andy Arditi", "paper_title": "Refusal mechanisms: initial experiments with llama-2-7b-chat", "publication_date": "2023-05-14", "reason": "This paper explores refusal mechanisms in LLMs, which are a central theme of this paper, providing insights into how models decide whether to respond to harmful queries."}, {"fullname_first_author": "Andy Arditi", "paper_title": "Refusal in LLMs is mediated by a single direction", "publication_date": "2024-05-14", "reason": "This paper further refines the understanding of refusal mechanisms, showing that a single direction in the activation space can influence a model's response to harmful queries, supporting the methodological approach of this paper."}, {"fullname_first_author": "Yuntao Bai", "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback", "publication_date": "2022-04-05", "reason": "This paper introduces a widely adopted safety training technique, reinforcement learning with human feedback, which is important context for understanding the limitations of current safety methods."}, {"fullname_first_author": "Collin Burns", "paper_title": "Discovering latent knowledge in language models without supervision", "publication_date": "2022-00-00", "reason": "This paper addresses the issue of latent misalignment in LLMs, suggesting that harmful capabilities can persist despite safety training, which serves as a theoretical foundation for the current work."}]}