[{"figure_path": "5NMbQPY7Bn/tables/tables_3_1.jpg", "caption": "Table 1: Zero-shot results on EgoSchema [39] full set. Methods that leverage closed-source LLMs are marked in gray. \u2020 denotes the model is trained with in-domain egocentric videos from Ego4D [15]. * denotes results on EgoSchema subset. Results of InternVideo and FrozenBiLM are sourced from [39]. Results of SEVILA are sourced from [45].", "description": "This table presents the zero-shot accuracy results on the EgoSchema benchmark, comparing various video understanding approaches.  It highlights the performance of TOPA against existing methods that use pre-trained models on web video-text data or adapt image-based Multimodal Large Language Models (MLLMs) to videos. The table shows the Top-1 accuracy for each method, categorized by whether they use pre-trained web video data, adapt image MLLMs, utilize LLM-based video agents, or employ TOPA's novel text-only pre-alignment method. Closed-source LLMs are indicated, as are models trained on in-domain egocentric videos.", "section": "4.1 Zero-Shot Evaluation on Multi-Choice Video QA"}, {"figure_path": "5NMbQPY7Bn/tables/tables_6_1.jpg", "caption": "Table 1: Zero-shot results on EgoSchema [39] full set. Methods that leverage closed-source LLMs are marked in gray. \u2020 denotes the model is trained with in-domain egocentric videos from Ego4D [15]. * denotes results on EgoSchema subset. Results of InternVideo and FrozenBiLM are sourced from [39]. Results of SEVILA are sourced from [45].", "description": "This table presents the zero-shot performance of various video understanding approaches on the EgoSchema benchmark's full set.  It compares different model types (pre-trained on web video-text data, adapting image MLLMs for video understanding, LLM-based video agents, and the proposed TOPA), highlighting their accuracy (Acc@1).  Closed-source LLMs are indicated, along with notes on any in-domain training and subset results for clarity.", "section": "4.1 Zero-Shot Evaluation on Multi-Choice Video QA"}, {"figure_path": "5NMbQPY7Bn/tables/tables_7_1.jpg", "caption": "Table 1: Zero-shot results on EgoSchema [39] full set. Methods that leverage closed-source LLMs are marked in gray. \u2020 denotes the model is trained with in-domain egocentric videos from Ego4D [15]. * denotes results on EgoSchema subset. Results of InternVideo and FrozenBiLM are sourced from [39]. Results of SEVILA are sourced from [45].", "description": "This table presents the results of a zero-shot evaluation on the EgoSchema benchmark. It compares the performance of various video understanding models, including those using closed-source LLMs (marked in gray), models trained with in-domain data (denoted by \u2020), and models evaluated on a subset of EgoSchema (denoted by *).  The table highlights the Top-1 accuracy achieved by each model, providing a comparison across different video understanding approaches.", "section": "4.1 Zero-Shot Evaluation on Multi-Choice Video QA"}, {"figure_path": "5NMbQPY7Bn/tables/tables_7_2.jpg", "caption": "Table 1: Zero-shot results on EgoSchema [39] full set. Methods that leverage closed-source LLMs are marked in gray. \u2020 denotes the model is trained with in-domain egocentric videos from Ego4D [15]. * denotes results on EgoSchema subset. Results of InternVideo and FrozenBiLM are sourced from [39]. Results of SEVILA are sourced from [45].", "description": "This table presents the zero-shot results on the EgoSchema benchmark, comparing various video understanding approaches.  It highlights the performance of different models, categorized by their underlying core visual language models (VLMs) and large language models (LLMs). The table distinguishes between models trained on web video-text data, those adapting image-based MLLMs for video, LLM-based video agents, and the proposed TOPA method.  The results show TOPA's competitive performance, especially considering its text-only pre-alignment approach and lack of training on real video data.", "section": "4.1 Zero-Shot Evaluation on Multi-Choice Video QA"}, {"figure_path": "5NMbQPY7Bn/tables/tables_7_3.jpg", "caption": "Table 1: Zero-shot results on EgoSchema [39] full set. Methods that leverage closed-source LLMs are marked in gray. \u2020 denotes the model is trained with in-domain egocentric videos from Ego4D [15]. * denotes results on EgoSchema subset. Results of InternVideo and FrozenBiLM are sourced from [39]. Results of SEVILA are sourced from [45].", "description": "This table presents the results of a zero-shot evaluation on the EgoSchema benchmark, comparing various video understanding approaches.  It highlights the performance (accuracy@1) of different models, categorized by their core visual language models (VLMs), core large language models (LLMs), or whether they employ image-based adaptation or video agents.  The table also indicates if a model uses closed-source LLMs and trained with in-domain egocentric videos, providing a comprehensive comparison of approaches.", "section": "4.1 Zero-Shot Evaluation on Multi-Choice Video QA"}, {"figure_path": "5NMbQPY7Bn/tables/tables_8_1.jpg", "caption": "Table 1: Zero-shot results on EgoSchema [39] full set. Methods that leverage closed-source LLMs are marked in gray. \u2020 denotes the model is trained with in-domain egocentric videos from Ego4D [15]. * denotes results on EgoSchema subset. Results of InternVideo and FrozenBiLM are sourced from [39]. Results of SEVILA are sourced from [45].", "description": "This table presents the zero-shot performance of various video understanding models on the EgoSchema benchmark.  It compares different approaches, categorizing them by their underlying methodology (e.g., web video pre-training, adapting image MLLMs, LLM-based video agents, and the proposed TOPA method). The table highlights the Top-1 accuracy achieved by each model, showing how TOPA compares to existing state-of-the-art methods, even without training on real video data.  The use of closed-source LLMs is indicated, as is the use of in-domain training data.", "section": "4.1 Zero-Shot Evaluation on Multi-Choice Video QA"}, {"figure_path": "5NMbQPY7Bn/tables/tables_9_1.jpg", "caption": "Table 5: Zero-shot video captioning results. We report CIDEr score for all benchmarks. VT denotes (video clip, text) pairs, IT denotes (image, text) pairs, and WP denotes webpages consisting of interleaved image and text data.", "description": "This table presents the zero-shot video captioning results on MSR-VTT and VATEX benchmarks, using CIDEr scores as the evaluation metric.  It compares the performance of various models, categorized into those pre-trained on web video-text data and those using a text-only pre-training approach (TOPA). The table highlights the performance improvement achieved by TOPA compared to other text-only methods and even some video-text pre-training methods.", "section": "4.3 Video Captioning"}, {"figure_path": "5NMbQPY7Bn/tables/tables_9_2.jpg", "caption": "Table 6: Blind results on EgoSchema. \u2020 denotes results sourced from [4].", "description": "This table presents the results of a blind test conducted on the EgoSchema benchmark.  The \"Blind\" setting means that the model only received the questions and choices, but not the actual video. This tests the model's ability to answer questions based solely on its pre-existing knowledge and linguistic understanding.  The table compares the performance of several LLMs (Large Language Models) under this blind condition. The performance of TOPA models (TOPA-Llama2-7B and TOPA-Llama2-13B) is also shown, demonstrating their ability to perform well even without access to the video.", "section": "4.1 Zero-Shot Evaluation on Multi-Choice Video QA"}, {"figure_path": "5NMbQPY7Bn/tables/tables_9_3.jpg", "caption": "Table 7: Ablation on video frames.", "description": "This table presents the ablation study on the number of video frames used as input to the TOPA model for the NeXT-QA and EgoSchema benchmarks.  It shows the performance of Llama2-7B and Llama2-13B models with 1, 5, and 10 frames, demonstrating how the accuracy increases with the number of frames, indicating the model's ability to capture temporal dynamics from more video information.", "section": "4.1 Zero-Shot Evaluation on Multi-Choice Video QA"}, {"figure_path": "5NMbQPY7Bn/tables/tables_17_1.jpg", "caption": "Table 8: Multi-choice video QA on EgoSchema subset and full set. \"Gap\" refers to the difference in performance between the subset and the full set", "description": "This table compares the performance of various methods on the EgoSchema benchmark's subset and full set for multi-choice video QA.  The \"Gap\" column shows the difference in accuracy between the subset and full set, highlighting the challenge of generalizing to more complex and diverse video data. The methods include those using similarity-based approaches, LLM logits, and LLM selection. TOPA is shown with and without multi-choice training to demonstrate the impact of this type of training. This table demonstrates the impact of using different evaluation methods (LLM Logits vs LLM Selection) on the performance gap between the subset and full set of the EgoSchema dataset.", "section": "A.1 Further Discussion on Multi-Choice Video QA Task"}, {"figure_path": "5NMbQPY7Bn/tables/tables_18_1.jpg", "caption": "Table 9: Ablation on the modality projection (Eq. 2). Results on EgoSchema full set.", "description": "This table presents the ablation study of the modality projection (Equation 2) in the TOPA framework. It shows the results on the EgoSchema full set for two different models, TOPA-LLama2-7B and TOPA-LLama2-13B, with and without the modality projection. The modality projection aims to bridge the gap between CLIP text features used in pre-training and CLIP image features used in inference, improving the model's performance.", "section": "A.2 The CLIP Modality Gap"}, {"figure_path": "5NMbQPY7Bn/tables/tables_25_1.jpg", "caption": "Table 1: Zero-shot results on EgoSchema [39] full set. Methods that leverage closed-source LLMs are marked in gray. \u2020 denotes the model is trained with in-domain egocentric videos from Ego4D [15]. * denotes results on EgoSchema subset. Results of InternVideo and FrozenBiLM are sourced from [39]. Results of SEVILA are sourced from [45].", "description": "This table presents the zero-shot results on the EgoSchema benchmark.  It compares various video understanding approaches, categorized by their method (web video pre-training, adapting image MLLMs, LLM-based video agents, and the proposed TOPA method).  The results are shown as accuracy at Top-1 (Acc@1), highlighting the performance of each method on the full EgoSchema dataset and, in some cases, a subset.  The table also notes which methods use closed-source LLMs and those trained on in-domain egocentric videos.", "section": "4.1 Zero-Shot Evaluation on Multi-Choice Video QA"}, {"figure_path": "5NMbQPY7Bn/tables/tables_26_1.jpg", "caption": "Table 11: Vocabulary size of Tideos generated under different prompts. We randomly sampled 20,000 global captions from each type of Tideos for comparison.", "description": "This table presents the vocabulary size of textual videos (Tideos) generated using different prompts.  The prompts are based on four different datasets: Howto100m, Ego4D, WebVid, and WordNet.  To ensure a fair comparison, a random sample of 20,000 global captions was selected from each dataset's generated Tideos, and the resulting vocabulary size was then calculated and displayed in the table. This analysis helps to understand the diversity of the language used across different sources and their impact on the generated Tideos.", "section": "E Experimental Setup"}, {"figure_path": "5NMbQPY7Bn/tables/tables_27_1.jpg", "caption": "Table 12: Training hyper-parameters.", "description": "This table details the hyperparameters used for training the various models in the paper. It includes information about the model, training dataset, number of epochs, effective batch size, base learning rate, and the optimizer used.  The table differentiates between the pre-training phase and the fine-tuning phase for the TOPA models, and also shows hyperparameters for the baseline models for comparison.", "section": "E.2 The details of training and evaluation"}]