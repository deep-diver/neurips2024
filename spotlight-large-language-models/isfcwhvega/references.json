{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational for the field of large language models (LLMs) and is cited frequently in the current paper, showcasing its significant influence on LLM development."}, {"fullname_first_author": "Zichang Liu", "paper_title": "Deja vu: Contextual sparsity for efficient LLMs at inference time", "publication_date": "2023-07-01", "reason": "This paper is highly relevant to the current research because it introduces a contextual sparsity approach for improving LLM efficiency, which serves as a direct comparison and baseline for the proposed method."}, {"fullname_first_author": "William Fedus", "paper_title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity", "publication_date": "2022-07-01", "reason": "This paper explores another crucial aspect of LLM optimization-sparsity-which is directly relevant to the research topic, serving as a significant comparison in the field."}, {"fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019-01-01", "reason": "This work is highly influential in the LLM field, demonstrating the impressive capabilities of large language models, thus setting the stage for subsequent research, including the current paper."}, {"fullname_first_author": "Yinhan Liu", "paper_title": "Roberta: A robustly optimized bert pretraining approach", "publication_date": "2019-12-01", "reason": "This work is frequently referenced due to its role as a strong baseline model in the NLU experiments in the current paper, allowing for comparative analysis of the proposed method."}]}