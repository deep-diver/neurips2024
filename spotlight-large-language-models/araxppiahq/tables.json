[{"figure_path": "ARAxPPIAhq/tables/tables_6_1.jpg", "caption": "Table 1: Method comparison on next token prediction when trained on 15B tokens from SlimPajama. Best validation perplexities within model classes, i.e., linear Transformers, RNNs, Transformers, SSMs, and XLSTMs are underlined and overall best is in bold. For each model class, the best performing methods are used in Section 4.3 for LLM training. xLSTMs with new memory (xLSTM[1:0] and xLSTM[7:1]) perform best.", "description": "This table presents a comparison of different language models trained on 15 billion tokens from the SlimPajama dataset, evaluating their performance on next-token prediction. The models are categorized into linear Transformers, RNNs, Transformers, and State Space Models (SSMs), with xLSTMs forming a separate category.  The table shows the number of parameters for each model, and their respective validation set perplexities. The best performing model in each category is underlined, with the overall best model in bold.  Note that the best-performing models in each category are selected for further large language model (LLM) training in Section 4.3.", "section": "4 Experiments"}, {"figure_path": "ARAxPPIAhq/tables/tables_7_1.jpg", "caption": "Table 1: Method comparison on next token prediction when trained on 15B tokens from SlimPajama. Best validation perplexities within model classes, i.e., linear Transformers, RNNs, Transformers, SSMs, and XLSTMs are underlined and overall best is in bold. For each model class, the best performing methods are used in Section 4.3 for LLM training. xLSTMs with new memory (xLSTM[1:0] and xLSTM[7:1]) perform best.", "description": "This table presents the results of comparing various language models on a next-token prediction task using the SlimPajama dataset.  It shows the validation perplexity achieved by each model. Models are categorized into linear Transformers, Recurrent Neural Networks (RNNs), Transformers, and State Space Models (SSMs). The best-performing model in each category and the overall best-performing model are highlighted.  The table also indicates that the best performing models from each category were selected for further large language model training in a subsequent section.", "section": "4 Experiments"}, {"figure_path": "ARAxPPIAhq/tables/tables_29_1.jpg", "caption": "Table 1: Method comparison on next token prediction when trained on 15B tokens from SlimPajama. Best validation perplexities within model classes, i.e., linear Transformers, RNNs, Transformers, SSMs, and XLSTMs are underlined and overall best is in bold. For each model class, the best performing methods are used in Section 4.3 for LLM training. xLSTMs with new memory (xLSTM[1:0] and xLSTM[7:1]) perform best.", "description": "This table compares the performance of various language models on a next-token prediction task.  Models are categorized into linear Transformers, RNNs, Transformers, and State Space Models (SSMs). The table shows the number of parameters and the validation perplexity for each model.  The best-performing model in each category is underlined, and the overall best model is shown in bold. Notably, the xLSTMs with new memory capabilities achieve the lowest perplexity scores, highlighting their effectiveness in this task. These top-performing models are then used in further experiments.", "section": "4 Experiments"}, {"figure_path": "ARAxPPIAhq/tables/tables_29_2.jpg", "caption": "Table 1: Method comparison on next token prediction when trained on 15B tokens from SlimPajama. Best validation perplexities within model classes, i.e., linear Transformers, RNNs, Transformers, SSMs, and XLSTMs are underlined and overall best is in bold. For each model class, the best performing methods are used in Section 4.3 for LLM training. xLSTMs with new memory (xLSTM[1:0] and xLSTM[7:1]) perform best.", "description": "This table compares the performance of various language models, including xLSTM variants, on a next-token prediction task using the SlimPajama dataset.  It shows the validation perplexity achieved by each model, highlighting the best-performing models within different model categories (linear Transformers, RNNs, Transformers, SSMs) and overall. The table also notes that the best-performing models from each category are used in subsequent, larger-scale experiments.", "section": "4 Experiments"}, {"figure_path": "ARAxPPIAhq/tables/tables_31_1.jpg", "caption": "Table 1: Method comparison on next token prediction when trained on 15B tokens from SlimPajama. Best validation perplexities within model classes, i.e., linear Transformers, RNNs, Transformers, SSMs, and XLSTMs are underlined and overall best is in bold. For each model class, the best performing methods are used in Section 4.3 for LLM training. xLSTMs with new memory (xLSTM[1:0] and xLSTM[7:1]) perform best.", "description": "This table compares the performance of various language models on a next-token prediction task using the SlimPajama dataset.  It shows the validation perplexity achieved by different model architectures (Transformers, RNNs, SSMs, and XLSTMs), highlighting the best-performing models within each class and overall.  The table is specifically useful for understanding the relative performance of XLSTMs compared to state-of-the-art alternatives on a large-scale language modeling task. The table also indicates that XLSTMs with new memory mechanisms perform exceptionally well.", "section": "4 Experiments"}, {"figure_path": "ARAxPPIAhq/tables/tables_35_1.jpg", "caption": "Table 3: Long Range Arena model hyperparameters. These are the model hyperparameters used in each of the Long Range Arena tasks. For each model we used the best learning rate and the better of the two learning rate schedulers.", "description": "This table shows the hyperparameters used in the Long Range Arena experiments.  It lists the number of blocks, embedding dimension, batch size, and training steps used for each of the five tasks: Retrieval, ListOps, Pathfinder, G-Image, and RGB-Image.  The hyperparameters were chosen to optimize performance on the given tasks. The table helps to reproduce these experiments.", "section": "4.2 Method Comparison and Ablation Study"}, {"figure_path": "ARAxPPIAhq/tables/tables_36_1.jpg", "caption": "Table 4: Long Range Arena test accuracy. Bold highlights the best performing model, underlined the second best. X denotes models that fail to outperform random baselines. xLSTM is the best of XLSTM[1:0], xLSTM[0:1] based on validation dataset accuracy.", "description": "This table shows the results of the Long Range Arena benchmark. The accuracy of different models on five tasks (Retrieval, ListOps, Pathfinder, G-Image, and RGB-Image) is presented. The best performing model for each task is highlighted in bold and the second-best model is underlined. A ranking of all models is also provided, indicating the overall performance of each model across all tasks.", "section": "4.2 Method Comparison and Ablation Study"}, {"figure_path": "ARAxPPIAhq/tables/tables_38_1.jpg", "caption": "Table 1: Method comparison on next token prediction when trained on 15B tokens from SlimPajama. Best validation perplexities within model classes, i.e., linear Transformers, RNNs, Transformers, SSMs, and XLSTMs are underlined and overall best is in bold. For each model class, the best performing methods are used in Section 4.3 for LLM training. xLSTMs with new memory (xLSTM[1:0] and xLSTM[7:1]) perform best.", "description": "This table compares the performance of various language models on the next token prediction task using the SlimPajama dataset (15B tokens).  Models are categorized into linear Transformers, RNNs, Transformers, and State Space Models (SSMs).  The table shows the number of parameters for each model and its validation perplexity.  The best performing models within each category are highlighted, with the overall best model shown in bold. The results indicate that XLSTM models with new memory mechanisms achieve the lowest perplexity, suggesting their superiority in next-token prediction.", "section": "4 Experiments"}, {"figure_path": "ARAxPPIAhq/tables/tables_39_1.jpg", "caption": "Table 1: Method comparison on next token prediction when trained on 15B tokens from SlimPajama. Best validation perplexities within model classes, i.e., linear Transformers, RNNs, Transformers, SSMs, and xLSTMs are underlined and overall best is in bold. For each model class, the best performing methods are used in Section 4.3 for LLM training. xLSTMs with new memory (xLSTM[1:0] and xLSTM[7:1]) perform best.", "description": "This table compares the performance of various language models on the next token prediction task using the SlimPajama dataset (15B tokens).  The models are categorized into linear Transformers, RNNs, Transformers, and State Space Models (SSMs). The table shows the number of parameters for each model and its validation perplexity.  The best performing model in each category is underlined, and the overall best-performing model is shown in bold. Notably, the xLSTMs with new memory achieve the lowest perplexity.", "section": "4 Experiments"}, {"figure_path": "ARAxPPIAhq/tables/tables_39_2.jpg", "caption": "Table 6: Ablation studies. Top: Ablation studies on the new xLSTM components, contributing the strong performance improvement of xLSTM over vanilla LSTM to both the exponential gating and the matrix memory. Bottom: Ablation studies on different gating techniques. We consider an xLSTM[1:0] with sigmoid forget gate and exponential input gate. Bias initialization \u221e means that the forget gate is set to one, [3, 6] indicates that values are taken equidistant in the respective interval, and N(0, 0.1) that values are randomly chosen from a Gaussian with mean 0 and std 0.1. PPL denotes validation perplexity. The first two lines correspond to models similar to linearized attention, line four to Retention, line five to RWKV-5, and line six to RWKV-6. Dependencies of the gates on the input lead to better performance.", "description": "This table presents ablation studies on the xLSTM architecture. The top part shows the impact of adding different xLSTM components (exponential gating and matrix memory) on the model performance. The bottom part focuses on different gating techniques, comparing various combinations of learnable gates, input-dependent gates, and bias initializations. The results highlight the contribution of each component to the model's overall performance, measured by the validation perplexity (PPL) on the SlimPajama dataset.", "section": "D.2 Method Comparison and Ablation Study on SlimPajama (15B)"}, {"figure_path": "ARAxPPIAhq/tables/tables_39_3.jpg", "caption": "Table 1: Method comparison on next token prediction when trained on 15B tokens from SlimPajama. Best validation perplexities within model classes, i.e., linear Transformers, RNNs, Transformers, SSMs, and XLSTMs are underlined and overall best is in bold. For each model class, the best performing methods are used in Section 4.3 for LLM training. xLSTMs with new memory (xLSTM[1:0] and xLSTM[7:1]) perform best.", "description": "This table compares the performance of various language models on the next token prediction task, using 15B tokens from the SlimPajama dataset for training. The models are categorized into linear Transformers, Recurrent Neural Networks (RNNs), Transformers, and State Space Models (SSMs). The table highlights the best validation perplexity within each model class, with the overall best performance shown in bold.  The best performing models from each class are then used in subsequent experiments.", "section": "4 Experiments"}, {"figure_path": "ARAxPPIAhq/tables/tables_42_1.jpg", "caption": "Table 1: Method comparison on next token prediction when trained on 15B tokens from SlimPajama. Best validation perplexities within model classes, i.e., linear Transformers, RNNs, Transformers, SSMs, and XLSTMs are underlined and overall best is in bold. For each model class, the best performing methods are used in Section 4.3 for LLM training. xLSTMs with new memory (xLSTM[1:0] and xLSTM[7:1]) perform best.", "description": "This table presents the results of comparing various language models on the next token prediction task, after training them on 15 billion tokens from the SlimPajama dataset. The models are categorized into linear Transformers, RNNs, Transformers, and SSMs, with xLSTMs forming a separate class. The table highlights the best validation perplexity within each model category and overall. It also indicates that the xLSTMs with the new memory components achieved the best performance.", "section": "4 Experiments"}, {"figure_path": "ARAxPPIAhq/tables/tables_42_2.jpg", "caption": "Table 1: Method comparison on next token prediction when trained on 15B tokens from SlimPajama. Best validation perplexities within model classes, i.e., linear Transformers, RNNs, Transformers, SSMs, and XLSTMs are underlined and overall best is in bold. For each model class, the best performing methods are used in Section 4.3 for LLM training. xLSTMs with new memory (xLSTM[1:0] and xLSTM[7:1]) perform best.", "description": "This table compares the performance of various language models on the next-token prediction task using the SlimPajama dataset (15B tokens).  Models are categorized into linear Transformers, RNNs, Transformers, and State Space Models (SSMs).  The table shows the number of parameters for each model and their validation set perplexity. The best performing model in each category is highlighted, along with the overall best-performing model.  This table informs the selection of models for further large-language model training in Section 4.3.", "section": "4 Experiments"}, {"figure_path": "ARAxPPIAhq/tables/tables_43_1.jpg", "caption": "Table 1: Method comparison on next token prediction when trained on 15B tokens from SlimPajama. Best validation perplexities within model classes, i.e., linear Transformers, RNNs, Transformers, SSMs, and XLSTMs are underlined and overall best is in bold. For each model class, the best performing methods are used in Section 4.3 for LLM training. xLSTMs with new memory (xLSTM[1:0] and xLSTM[7:1]) perform best.", "description": "This table compares the performance of various language models on a next-token prediction task, using 15B tokens from the SlimPajama dataset for training.  The models are categorized into linear Transformers, RNNs, Transformers, and State Space Models (SSMs), along with the novel xLSTMs. The table highlights the best-performing model within each category and overall, indicating the superior performance of xLSTMs with the new memory structures.", "section": "4 Experiments"}, {"figure_path": "ARAxPPIAhq/tables/tables_44_1.jpg", "caption": "Table 1: Method comparison on next token prediction when trained on 15B tokens from SlimPajama. Best validation perplexities within model classes, i.e., linear Transformers, RNNs, Transformers, SSMs, and XLSTMs are underlined and overall best is in bold. For each model class, the best performing methods are used in Section 4.3 for LLM training. xLSTMs with new memory (xLSTM[1:0] and xLSTM[7:1]) perform best.", "description": "This table compares the performance of various language models on the next token prediction task, using the SlimPajama dataset with 15B tokens. The models are categorized into linear Transformers, RNNs, Transformers, and SSMs, with xLSTMs forming a separate category.  The table shows the number of parameters for each model and its validation perplexity.  The best performing model within each category is underlined, and the overall best model is shown in bold. The table highlights that xLSTMs with novel memory mechanisms achieve the best performance, setting the stage for the scaling experiments in section 4.3.", "section": "4 Experiments"}, {"figure_path": "ARAxPPIAhq/tables/tables_45_1.jpg", "caption": "Table 1: Method comparison on next token prediction when trained on 15B tokens from SlimPajama. Best validation perplexities within model classes, i.e., linear Transformers, RNNs, Transformers, SSMs, and XLSTMs are underlined and overall best is in bold. For each model class, the best performing methods are used in Section 4.3 for LLM training. xLSTMs with new memory (xLSTM[1:0] and xLSTM[7:1]) perform best.", "description": "This table compares the performance of various language models (including xLSTM variants) on the next-token prediction task.  Models are grouped into categories (linear Transformers, RNNs, Transformers, SSMs) based on their architecture and the best performing model from each category is used in later experiments.  The table highlights the best overall performance and indicates the best-performing models in each category, showing that xLSTMs with new memory structures outperform others.", "section": "4 Experiments"}, {"figure_path": "ARAxPPIAhq/tables/tables_46_1.jpg", "caption": "Table 1: Method comparison on next token prediction when trained on 15B tokens from SlimPajama. Best validation perplexities within model classes, i.e., linear Transformers, RNNs, Transformers, SSMs, and XLSTMs are underlined and overall best is in bold. For each model class, the best performing methods are used in Section 4.3 for LLM training. xLSTMs with new memory (xLSTM[1:0] and xLSTM[7:1]) perform best.", "description": "This table compares the performance of various language models on the next token prediction task using the SlimPajama dataset.  The models are categorized into linear Transformers, RNNs, Transformers, and SSMs, with XLSTMs included as a separate category.  The table highlights the best validation perplexity results within each model category, with the overall best performance shown in bold.  The best performing models from each category are selected for further large-scale language model training in section 4.3 of the paper.  The results showcase the superior performance of XLSTMs, particularly those with the new memory mechanisms.", "section": "4 Experiments"}, {"figure_path": "ARAxPPIAhq/tables/tables_47_1.jpg", "caption": "Table 1: Method comparison on next token prediction when trained on 15B tokens from SlimPajama. Best validation perplexities within model classes, i.e., linear Transformers, RNNs, Transformers, SSMs, and XLSTMs are underlined and overall best is in bold. For each model class, the best performing methods are used in Section 4.3 for LLM training. xLSTMs with new memory (xLSTM[1:0] and xLSTM[7:1]) perform best.", "description": "This table compares the performance of various language models on the next token prediction task after training on 15 billion tokens from the SlimPajama dataset.  Models are grouped into categories: linear Transformers, RNNs, Transformers, and State Space Models (SSMs). The table shows the number of parameters for each model and its validation set perplexity. The best performing models in each category are highlighted, and the overall best-performing model is indicated in bold.  The results show xLSTMs with new memory structures achieving the lowest perplexity.", "section": "4 Experiments"}, {"figure_path": "ARAxPPIAhq/tables/tables_48_1.jpg", "caption": "Table 1: Method comparison on next token prediction when trained on 15B tokens from SlimPajama. Best validation perplexities within model classes, i.e., linear Transformers, RNNs, Transformers, SSMs, and XLSTMs are underlined and overall best is in bold. For each model class, the best performing methods are used in Section 4.3 for LLM training. xLSTMs with new memory (xLSTM[1:0] and xLSTM[7:1]) perform best.", "description": "This table compares the performance of various language models on a next-token prediction task using the SlimPajama dataset (15B tokens).  Models are categorized into linear Transformers, RNNs, Transformers, and SSMs, with XLSTMs forming a separate category. The table shows the number of parameters for each model, along with its validation perplexity (lower is better). The best performing model within each category is underlined, and the best overall model is bolded. The results highlight that the XLSTMs with novel memory architectures (xLSTM[1:0] and xLSTM[7:1]) achieve the lowest perplexity, indicating superior performance in this benchmark.", "section": "4 Experiments"}, {"figure_path": "ARAxPPIAhq/tables/tables_49_1.jpg", "caption": "Table 1: Method comparison on next token prediction when trained on 15B tokens from SlimPajama. Best validation perplexities within model classes, i.e., linear Transformers, RNNs, Transformers, SSMs, and xLSTMs are underlined and overall best is in bold. For each model class, the best performing methods are used in Section 4.3 for LLM training. xLSTMs with new memory (xLSTM[1:0] and xLSTM[7:1]) perform best.", "description": "The table compares various language models trained on 15B tokens from the SlimPajama dataset, evaluating their performance based on next-token prediction perplexity.  It highlights the best-performing models within different model categories (linear Transformers, RNNs, Transformers, SSMs, and xLSTMs).  It also notes that the best-performing models are subsequently used for large language model training in a later section of the paper.", "section": "4 Experiments"}, {"figure_path": "ARAxPPIAhq/tables/tables_50_1.jpg", "caption": "Table 1: Method comparison on next token prediction when trained on 15B tokens from SlimPajama. Best validation perplexities within model classes, i.e., linear Transformers, RNNs, Transformers, SSMs, and XLSTMs are underlined and overall best is in bold. For each model class, the best performing methods are used in Section 4.3 for LLM training. xLSTMs with new memory (xLSTM[1:0] and xLSTM[7:1]) perform best.", "description": "This table compares the performance of various language models (including xLSTM variants, Transformers, RNNs, and SSMs) on a next-token prediction task using the SlimPajama dataset with 15B tokens.  The models are grouped into classes, and the best-performing model within each class is highlighted.  The table shows the number of parameters for each model and its validation perplexity.  The xLSTMs with the novel matrix memory architecture achieve the lowest perplexity, demonstrating their superior performance on this specific task.", "section": "4 Experiments"}, {"figure_path": "ARAxPPIAhq/tables/tables_51_1.jpg", "caption": "Table 1: Method comparison on next token prediction when trained on 15B tokens from SlimPajama. Best validation perplexities within model classes, i.e., linear Transformers, RNNs, Transformers, SSMs, and XLSTMs are underlined and overall best is in bold. For each model class, the best performing methods are used in Section 4.3 for LLM training. xLSTMs with new memory (xLSTM[1:0] and xLSTM[7:1]) perform best.", "description": "This table compares the performance of various language models on the next-token prediction task using the SlimPajama dataset.  Models are categorized into linear Transformers, RNNs, Transformers, and SSMs, with XLSTMs forming a separate category.  The table shows the number of parameters for each model and its validation perplexity.  The best-performing model in each category is underlined, with the overall best model shown in bold.  The table highlights that the XLSTMs, particularly those incorporating new memory mechanisms, achieve the lowest perplexity.", "section": "4 Experiments"}, {"figure_path": "ARAxPPIAhq/tables/tables_52_1.jpg", "caption": "Table 1: Method comparison on next token prediction when trained on 15B tokens from SlimPajama. Best validation perplexities within model classes, i.e., linear Transformers, RNNs, Transformers, SSMs, and XLSTMs are underlined and overall best is in bold. For each model class, the best performing methods are used in Section 4.3 for LLM training. xLSTMs with new memory (xLSTM[1:0] and xLSTM[7:1]) perform best.", "description": "This table compares the performance of various language models on a next-token prediction task using the SlimPajama dataset (15B tokens).  Models are grouped by architecture type (linear Transformer, RNN, Transformer, SSM).  The table shows the number of parameters for each model and its validation set perplexity. The best-performing model in each class is highlighted, with the overall best model shown in bold.  The best-performing xLSTM models, which incorporate novel memory mechanisms, are emphasized.", "section": "4 Experiments"}, {"figure_path": "ARAxPPIAhq/tables/tables_54_1.jpg", "caption": "Table 1: Method comparison on next token prediction when trained on 15B tokens from SlimPajama. Best validation perplexities within model classes, i.e., linear Transformers, RNNs, Transformers, SSMs, and XLSTMs are underlined and overall best is in bold. For each model class, the best performing methods are used in Section 4.3 for LLM training. xLSTMs with new memory (xLSTM[1:0] and xLSTM[7:1]) perform best.", "description": "This table presents the results of comparing various language models on next token prediction task using 15B tokens from SlimPajama dataset. It compares different model classes such as Transformers, RNNs, and SSMs, along with the proposed xLSTMs. The table highlights the best performing model within each class and overall by underlining and bolding the best validation perplexities.  The best performing xLSTMs are those with new memory.", "section": "4 Experiments"}]