[{"figure_path": "ARAxPPIAhq/figures/figures_1_1.jpg", "caption": "Figure 1: LSTM limitations. Left: Nearest Neighbor Search problem in terms of mean squared error (MSE). Given a reference vector, a sequence is scanned sequentially for the most similar vector with the objective to return its attached value at sequence end. LSTM struggles to revise a stored value when a more similar vector is found. Our new XLSTM overcomes this limitation by exponential gating. Right: Rare Token Prediction. The perplexity (PPL) of token prediction on Wikitext-103, in partitions of token frequency. LSTM performs worse on predicting rare tokens because of its limited storage capacities, whereas our new xLSTM solves this problem via a matrix memory.", "description": "The figure demonstrates two limitations of LSTMs. The left panel shows the mean squared error (MSE) for the Nearest Neighbor Search task, highlighting LSTMs' struggle to revise stored values upon finding a more similar vector.  The right panel displays the perplexity (PPL) of token prediction on the Wikitext-103 dataset, partitioned by token frequency, revealing LSTMs' poorer performance on rare tokens due to limited storage.  The figure emphasizes that the proposed XLSTM architecture addresses both limitations.", "section": "1 Introduction"}, {"figure_path": "ARAxPPIAhq/figures/figures_2_1.jpg", "caption": "Figure 6: The extended LSTM (xLSTM) family. From left to right: 1. The original LSTM memory cell with constant error carousel and gating. 2. New sLSTM and mLSTM memory cells that introduce exponential gating. sLSTM offers a new memory mixing technique. mLSTM is fully parallelizable with a novel matrix memory cell state and new covariance update rule. 3. mLSTM and sLSTM in residual blocks yield xLSTM blocks. 4. Stacked xLSTM blocks give an xLSTM architecture.", "description": "This figure illustrates the evolution of the LSTM architecture from the original version to the extended xLSTM.  It shows the components added at each stage: exponential gating, new memory mixing in sLSTM, parallel processing capabilities of mLSTM with its matrix memory and covariance update, and finally, the combination of these into residual xLSTM blocks which can be stacked to create xLSTM architectures.", "section": "Extended Long Short-Term Memory"}, {"figure_path": "ARAxPPIAhq/figures/figures_2_2.jpg", "caption": "Figure 6: The extended LSTM (xLSTM) family. From left to right: 1. The original LSTM memory cell with constant error carousel and gating. 2. New sLSTM and mLSTM memory cells that introduce exponential gating. sLSTM offers a new memory mixing technique. mLSTM is fully parallelizable with a novel matrix memory cell state and new covariance update rule. 3. mLSTM and sLSTM in residual blocks yield xLSTM blocks. 4. Stacked xLSTM blocks give an xLSTM architecture.", "description": "This figure illustrates the evolution of LSTM to xLSTM. It starts with the original LSTM cell, highlighting its core components: constant error carousel and gating. Then, it introduces the two new variants: sLSTM and mLSTM, each with its unique improvements. sLSTM incorporates exponential gating and a new memory mixing technique for enhanced revisability. mLSTM features exponential gating, matrix memory, and a covariance update rule for parallelization. The figure then shows how both sLSTM and mLSTM are integrated into residual blocks to form xLSTM blocks, which can then be stacked to create an xLSTM architecture.", "section": "2 Extended Long Short-Term Memory"}, {"figure_path": "ARAxPPIAhq/figures/figures_3_1.jpg", "caption": "Figure 6: The extended LSTM (xLSTM) family. From left to right: 1. The original LSTM memory cell with constant error carousel and gating. 2. New sLSTM and mLSTM memory cells that introduce exponential gating. sLSTM offers a new memory mixing technique. mLSTM is fully parallelizable with a novel matrix memory cell state and new covariance update rule. 3. mLSTM and sLSTM in residual blocks yield xLSTM blocks. 4. Stacked xLSTM blocks give an xLSTM architecture.", "description": "This figure illustrates the evolution of LSTM to xLSTM. It starts with the original LSTM cell and shows how it's extended by introducing exponential gating and new memory structures (sLSTM and mLSTM).  The figure highlights the key differences between the original LSTM, sLSTM, and mLSTM, and how these components are integrated into xLSTM blocks and finally stacked into the xLSTM architecture.", "section": "2 Extended Long Short-Term Memory"}, {"figure_path": "ARAxPPIAhq/figures/figures_5_1.jpg", "caption": "Figure 1: LSTM limitations. Left: Nearest Neighbor Search problem in terms of mean squared error (MSE). Given a reference vector, a sequence is scanned sequentially for the most similar vector with the objective to return its attached value at sequence end. LSTM struggles to revise a stored value when a more similar vector is found. Our new XLSTM overcomes this limitation by exponential gating. Right: Rare Token Prediction. The perplexity (PPL) of token prediction on Wikitext-103, in partitions of token frequency. LSTM performs worse on predicting rare tokens because of its limited storage capacities, whereas our new xLSTM solves this problem via a matrix memory.", "description": "This figure illustrates two limitations of LSTMs. The left panel shows that LSTMs struggle to update their memory when encountering a more similar vector later in a sequence, resulting in higher MSE in nearest neighbor search. The right panel shows that LSTMs perform poorly on predicting rare tokens compared to Transformers, suggesting limitations in their storage capacity.  The authors' proposed XLSTM model addresses these issues with improved memory mixing (left) and enhanced storage capacity (right).", "section": "1 Introduction"}, {"figure_path": "ARAxPPIAhq/figures/figures_7_1.jpg", "caption": "Figure 3: Sequence extrapolation in language modeling. This is a comparison of 1.3B-sized, large models of xLSTM, RWKV-4, Llama, and Mamba at next token prediction on the SlimPajama validation set after training on 300B tokens from SlimPajama. Models are trained with context length 2048 (gray) and then tested for context lengths up to 16384. Left: Token perplexities evaluated at different context lengths. In contrast to other methods, xLSTM models remain at low perplexities for longer contexts. Right: Prediction quality when extrapolating to long context sizes in terms of validation perplexity (PPL). xLSTM yields the best PPL values (best in bold, second best underlined).", "description": "This figure compares the performance of xLSTM, RWKV-4, Llama, and Mamba models on a sequence extrapolation task in language modeling. The models were initially trained on sequences of length 2048.  The left panel shows how perplexity changes as the models are tested on sequences of increasing length (up to 16384).  The right panel summarizes this by showing the final validation perplexity at the longest sequence length (16384). The results demonstrate that xLSTM models maintain significantly lower perplexity compared to other models when tested on longer sequences.", "section": "4.2 Method Comparison and Ablation Study"}, {"figure_path": "ARAxPPIAhq/figures/figures_8_1.jpg", "caption": "Figure 15: Method comparison on next token prediction when trained on 15B tokens from SlimPajama. Performance measure in validation perplexity for the best methods of each model class (see Table 1) are reported. The performance degradation of XLSTM[7:1] at 2.7B is due to initially slower training convergence that leads to an especially undertrained model. xLSTM is the best method at all sizes.", "description": "This figure compares the performance of different language models on a next-token prediction task using the SlimPajama dataset (15B tokens).  It shows the validation perplexity as a function of the model size (number of parameters) for several models, including Llama, Mamba, RWKV-4, XLSTM[7:1], and XLSTM[1:0]. The results indicate that XLSTM models generally outperform the others, achieving lower perplexity scores across different model sizes.  A slight performance dip is observed for XLSTM[7:1] at the largest model size, which is attributed to slower initial training convergence.", "section": "4.2 Method Comparison and Ablation Study on SlimPajama (15B)"}, {"figure_path": "ARAxPPIAhq/figures/figures_9_1.jpg", "caption": "Figure 5: Generation Times and Maximal Throughput. Left: Generation times of different 1.3B models for a pre-fill context of 16 tokens (to mitigate cache initialization). The recurrent models (xLSTM[1:0], xLSTM[7:1], Mamba and RWKV-4) show linear behavior, whereas the Transformer (Llama) inference/decoding time is quadratic in sequence length. Right: Token throughput for different batch sizes on a A100-80GB GPU for 1.3B sized models. Note that the Transformer / Llama model goes out of memory (OOM) already for small batch sizes, whereas xLSTM and Mamba can sustain very large batch sizes. xLSTM[1:0] consistently outperforms Mamba in throughput. Beyond batch size 2048, all models go OOM.", "description": "This figure compares the performance of different 1.3B large language models in terms of generation time and maximal throughput. The left panel shows that recurrent models exhibit linear scaling with generation length, unlike the Transformer model which shows quadratic scaling.  The right panel demonstrates the throughput of the models at various batch sizes.  It highlights that xLSTM[1:0] achieves the highest throughput at large batch sizes, while the Transformer model runs out of memory at relatively small batch sizes.", "section": "4 Experiments"}, {"figure_path": "ARAxPPIAhq/figures/figures_18_1.jpg", "caption": "Figure 6: The extended LSTM (xLSTM) family. From left to right: 1. The original LSTM memory cell with constant error carousel and gating. 2. New sLSTM and mLSTM memory cells that introduce exponential gating. sLSTM offers a new memory mixing technique. mLSTM is fully parallelizable with a novel matrix memory cell state and new covariance update rule. 3. mLSTM and sLSTM in residual blocks yield xLSTM blocks. 4. Stacked xLSTM blocks give an xLSTM architecture.", "description": "This figure illustrates the evolution of the LSTM architecture to the xLSTM architecture. It starts with the original LSTM cell, showing its core components such as the constant error carousel and sigmoid gates.  It then introduces two new variants: sLSTM and mLSTM, which incorporate exponential gating and modified memory structures. The sLSTM introduces a new memory mixing technique, while the mLSTM enables parallelization using a matrix memory and a covariance update rule. Finally, it shows how these new LSTM variants are integrated into residual blocks to form xLSTM blocks, which are then stacked to create the final xLSTM architecture.", "section": "Extended Long Short-Term Memory"}, {"figure_path": "ARAxPPIAhq/figures/figures_19_1.jpg", "caption": "Figure 6: The extended LSTM (xLSTM) family. From left to right: 1. The original LSTM memory cell with constant error carousel and gating. 2. New sLSTM and mLSTM memory cells that introduce exponential gating. sLSTM offers a new memory mixing technique. mLSTM is fully parallelizable with a novel matrix memory cell state and new covariance update rule. 3. mLSTM and sLSTM in residual blocks yield xLSTM blocks. 4. Stacked xLSTM blocks give an xLSTM architecture.", "description": "This figure illustrates the evolution of LSTM architectures from the original LSTM to the extended LSTM (xLSTM). It starts with the original LSTM cell, highlighting its constant error carousel and gating mechanisms.  Then it introduces two new variations: sLSTM and mLSTM.  sLSTM incorporates exponential gating and a new memory mixing technique, while mLSTM is fully parallelizable due to the use of a matrix memory and covariance update rule.  The figure shows how these new LSTM variants are integrated into residual blocks to form xLSTM blocks, which are then stacked to create the final xLSTM architecture.", "section": "B Extended Long Short-Term Memory"}, {"figure_path": "ARAxPPIAhq/figures/figures_19_2.jpg", "caption": "Figure 6: The extended LSTM (xLSTM) family. From left to right: 1. The original LSTM memory cell with constant error carousel and gating. 2. New sLSTM and mLSTM memory cells that introduce exponential gating. sLSTM offers a new memory mixing technique. mLSTM is fully parallelizable with a novel matrix memory cell state and new covariance update rule. 3. mLSTM and sLSTM in residual blocks yield xLSTM blocks. 4. Stacked xLSTM blocks give an xLSTM architecture.", "description": "This figure illustrates the evolution of LSTM to xLSTM. It starts with the original LSTM cell, highlighting its core components like the constant error carousel and gating mechanisms.  Then, it introduces two novel variations: sLSTM (with scalar memory and exponential gating) and mLSTM (with matrix memory, exponential gating, and a covariance update rule).  Finally, it shows how these new LSTM variants are integrated into residual blocks (xLSTM blocks) and stacked to form the complete xLSTM architecture.", "section": "2 Extended Long Short-Term Memory"}, {"figure_path": "ARAxPPIAhq/figures/figures_25_1.jpg", "caption": "Figure 7: Schematic representation of an sLSTM Block \u2013 post up-projection: Embedded in a pre-LayerNorm residual structure, the input is optionally passed through a causal convolution of window size 4 that includes a Swish activation for input and forget gates. Then, for all input, forget and output gates i, f, o, and the cell update z the input is fed through a block-diagonal linear layer with four diagonal blocks or \u201cheads\u201d. These diagonal blocks coincide with the recurrent gate pre-activations from the last hidden state, which corresponds to an sLSTM with four heads depicted with the circular arrows. The resulting hidden state goes through a GroupNorm layer (Wu & He, 2018) \u2013 a head-wise LayerNorm for each of the four heads. Finally, the output is up- and down-projected using a gated MLP, with GeLU activation function and projection factor 4/3 to match parameters.", "description": "This figure shows the architecture of an sLSTM block, a key component of the XLSTM model.  It illustrates how the input data is processed through a causal convolution (optional), a block-diagonal linear layer representing four heads, Group Normalization, and a gated MLP before being outputted. This block uses pre-LayerNorm residual architecture and exponential gating.", "section": "B.4 Detailed Block Structure"}, {"figure_path": "ARAxPPIAhq/figures/figures_26_1.jpg", "caption": "Figure 8: Schematic representation of an mLSTM block \u2013 pre up-projection: Embedded in a pre-LayerNorm residual structure, the input is up-projected first with projection factor 2, once for an externalized output gate and once as input for the mLSTM cells. The mLSTM cell input is dimension-wise causally convoluted (kernel size 4), before entering a learnable skip connection. We obtain input q and k via block-diagonal projection matrices of block size 4. The values v are fed directly, skipping the convolution part. After the mLSTM sequence mixing, outputs are normalized via GroupNorm (Wu & He, 2018) \u2013 a head-wise layer norm for each of the four heads. Finally, the learnable skip input is added and the result is gated component-wise with the external output gate. The output is down-projected.", "description": "This figure depicts the architecture of an mLSTM block, a key component of the XLSTM model.  It showcases the pre-up-projection design where the input is first linearly transformed to a higher dimension before being processed by the mLSTM cell.  The mLSTM cell utilizes a matrix memory and a covariance update rule.  Key features illustrated include the causal convolution, learnable skip connections, Group Normalization, and gating mechanisms.", "section": "2 Extended Long Short-Term Memory"}, {"figure_path": "ARAxPPIAhq/figures/figures_33_1.jpg", "caption": "Figure 11: Illustration of the MQAR task. Color pairs represent key-value pairs (keys have darker shade). The first part of the sequence defines the key-value pairs for the respective sample. After that, the keys appear randomly according to a power law distribution 4. Grey tokens in the input sequence represent a zero token. The \"target\" sequence contains the value after the respective key appearance - the rest of the tokens are ignored for the accuracy and loss calculation. The model must predict the value tokens given the respective key.", "description": "This figure illustrates the Multi-Query Associative Recall (MQAR) task.  The top row shows the \"target\" sequence, where colored squares represent key-value pairs.  The bottom row displays the \"input\" sequence that the model receives, where the color coding corresponds to the key-value pairs in the target sequence.  The model needs to predict the values (colored squares) in the target sequence based on the order of keys presented in the input sequence.  The task's complexity is controlled by varying the number of key-value pairs and the length of the input sequence.", "section": "D.1.2 Test of xLSTM's Memory Capacities on Associative Recall Tasks"}, {"figure_path": "ARAxPPIAhq/figures/figures_34_1.jpg", "caption": "Figure 1: LSTM limitations. Left: Nearest Neighbor Search problem in terms of mean squared error (MSE). Given a reference vector, a sequence is scanned sequentially for the most similar vector with the objective to return its attached value at sequence end. LSTM struggles to revise a stored value when a more similar vector is found. Our new XLSTM overcomes this limitation by exponential gating. Right: Rare Token Prediction. The perplexity (PPL) of token prediction on Wikitext-103, in partitions of token frequency. LSTM performs worse on predicting rare tokens because of its limited storage capacities, whereas our new xLSTM solves this problem via a matrix memory.", "description": "This figure shows two graphs illustrating the limitations of LSTMs. The left graph shows that LSTMs struggle with the Nearest Neighbor Search problem because they cannot easily revise a stored value when a more similar vector is encountered.  The right graph shows that LSTMs perform poorly on predicting rare tokens in the Wikitext-103 dataset due to their limited storage capacity.  The figure highlights that the proposed XLSTM model addresses these limitations through exponential gating and a matrix memory.", "section": "1 Introduction"}, {"figure_path": "ARAxPPIAhq/figures/figures_35_1.jpg", "caption": "Figure 13: Result of MQAR-Experiment 2. The columns and rows correspond to different numbers of key-value pairs and the context length respectivly. The x-axis gives the model size and the y-axis the validation accuracy.", "description": "This figure shows the results of the second Multi-Query Associative Recall (MQAR) experiment.  It explores how the accuracy of various models changes as the context length and the number of key-value pairs increase. The x-axis represents the model size (dimension), while the y-axis shows the accuracy achieved on the validation set. The different columns and rows represent variations in the number of key-value pairs and context lengths respectively.", "section": "D.1.2 Test of xLSTM's Memory Capacities on Associative Recall Tasks"}, {"figure_path": "ARAxPPIAhq/figures/figures_36_1.jpg", "caption": "Figure 12: Result of MQAR-Experiment 1. The columns show different task settings (context length and key-value pairs). The rows group related models for better clarity. The x-axis gives the model size and the y-axis the validation accuracy.", "description": "This figure shows the results of the first Multi-Query Associative Recall (MQAR) experiment.  The experiment tested various models' ability to perform associative recall with different levels of difficulty. Difficulty was manipulated by varying the context length (64, 128, 256) and the number of key-value pairs that needed to be memorized (4, 8, 16). The x-axis represents the model size (model dimension), while the y-axis indicates the validation accuracy. The plot is organized to group related models (e.g., Transformers, RNNs, and xLSTMs) for easier comparison.", "section": "D.1.2 Test of xLSTM's Memory Capacities on Associative Recall Tasks"}, {"figure_path": "ARAxPPIAhq/figures/figures_38_1.jpg", "caption": "Figure 15: Method comparison on next token prediction when trained on 15B tokens from SlimPajama. Performance measure in validation perplexity for the best methods of each model class (see Table 1) are reported. The performance degradation of XLSTM[7:1] at 2.7B is due to initially slower training convergence that leads to an especially undertrained model. xLSTM is the best method at all sizes.", "description": "This figure shows the performance comparison of different language models trained on 15 billion tokens from the SlimPajama dataset. The models are compared based on their validation perplexity, which measures how well the models predict the next token in a sequence. The models included in the comparison are Llama, Mamba, RWKV-4, xLSTM[7:1], and xLSTM[1:0]. The x-axis represents the number of parameters in each model, while the y-axis shows the validation perplexity. The results indicate that xLSTM models generally outperform other models across various parameter scales, demonstrating superior performance in language modeling.", "section": "Method Comparison and Ablation Study on SlimPajama (15B)"}, {"figure_path": "ARAxPPIAhq/figures/figures_40_1.jpg", "caption": "Figure 16: Scaling laws over number of training FLOPs. We compare the Llama baseline with our XLSTM variants and compute the number of training FLOPs for the recurrent and parallel mode for 300B tokens with context length 2048.", "description": "This figure shows the scaling behavior of Llama, and two versions of XLSTM (one with only mLSTM blocks and another with a mix of sLSTM and mLSTM blocks)  as the number of training FLOPs increases.  The models were trained on 300B tokens with a context length of 2048.  The plot displays validation perplexity on the y-axis and the number of FLOPs on the x-axis.  The results suggest how the model's performance (perplexity) changes with the increasing computational cost.  Different lines represent the recurrent and parallel versions of the XLSTM models, showcasing the trade-off between performance and parallelization.", "section": "D.3 XLSTM Large Language Models - SlimPajama300B"}]