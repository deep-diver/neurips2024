[{"type": "text", "text": "Model Fusion through Bayesian Optimization in Language Model Fine-Tuning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chaeyun Jang\u2217 KAIST jcy9911@kaist.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Hyungi Lee\u2217 KAIST lhk2708@kaist.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Jungtaek Kim\u2020 University of Pittsburgh jungtaek.kim@pitt.edu ", "page_idx": 0}, {"type": "text", "text": "Juho Lee\u2020 KAIST juholee@kaist.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Fine-tuning pre-trained models for downstream tasks is a widely adopted technique known for its adaptability and reliability across various domains. Despite its conceptual simplicity, fine-tuning entails several troublesome engineering choices, such as selecting hyperparameters and determining checkpoints from an optimization trajectory. To tackle the difficulty of choosing the best model, one effective solution is model fusion, which combines multiple models in a parameter space. However, we observe a large discrepancy between loss and metric landscapes during the fine-tuning of pre-trained language models. Building on this observation, we introduce a novel model fusion technique that optimizes both the desired metric and loss through multi-objective Bayesian optimization. In addition, to effectively select hyperparameters, we establish a two-stage procedure by integrating Bayesian optimization processes into our framework. Experiments across various downstream tasks show considerable performance improvements using our Bayesian optimization-guided method. Code will be available at: https://github.com/chaeyoon-jang/bomf.git. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The field of Natural Language Processing (NLP) has significantly advanced with the pre-training of Transformer-based models on large amounts of texts without supervision. In general, these pre-trained networks are fine-tuned on supervised downstream tasks to solve particular tasks. The rise of Large Language Models (LLMs) such as GPT [50] and LLaMA [63] has increased demands for huge memory and computing during fine-tuning on downstream tasks. In response, low rank approximation methods such as Low-Rank Adaptation (LoRA) [22] and Quantized Low-Rank Adaptation (QLoRA) [11] have been adopted recently to fine-tune the LLM. However, the fine-tuning of Pretrained Language Models (PLMs) exhibits high sensitivity to marginal variations in hyperparameters such as learning rate and batch size, often leading to training failure and the performance drop of a fine-tuned model [45], while searching hyperparameters requires a vast amount of resources. ", "page_idx": 0}, {"type": "text", "text": "An effective strategy to seek an optimal model among multiple candidates is model ensembling, which yields impressive performance on generalization and robustness [33]. However, traditional ensemble methods lead to several drawbacks including the space and computational costs that linearly scale with the number of models involved. These issues are particularly pertinent for LLMs, since individual models are costly to train and test. Therefore, we can alternatively utilize model fusion to aggregate multiple models into a single proficient model on a parameter space. One of its simplest forms, known as Stochastic Weight Averaging (SWA) [25], involves taking the average of model parameters obtained during an optimization process. Despite its simplicity, SWA and its variants have proven successful across various tasks, notably in computer vision [25, 42, 6, 46]. Recent advancement in this field is the concept of Model Soups, which has been introduced by Wortsman et al. [70]. This approach weight-averages a set of models, obtained from multiple fine-tuning runs with different hyperparameters to create a powerful model that outperforms both individual and ensemble models. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The effectiveness of model fusion has predominantly been explored in the visual domain. For instance, while Model Soups have shown considerable improvements in image classification, they have not demonstrated superiority in the NLP tasks [70]. The existing averaging methods like SWA make use of their ability to encourage a fused model to locate on the center of the flatter area near local optima [25, 20], as loss landscapes are analogous to metric landscapes in computer vision tasks. Unfortunately, for PLMs, loss landscapes are substantially mismatched to metric landscapes, so that the flat loss minimum found by SWA does not necessarily correspond to the flat metric minimum making a simple averaging method fail to find a superior model. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we present a novel model fusion approach with an efficient hyperparameter selection strategy, denoted as Bayesian Optimization Model Fusion (BOMF), specifically designed to fine-tune PLMs. To motivate our method, we start by illustrating two empirical analyses. Firstly, we demonstrate that the existing model fusion techniques are not suitable for PLMs. Secondly, we highlight that the optimal hyperparameters for PLMs exhibit consistency on varying the number of frozen layers or the rank used in the LoRA setting [22]. ", "page_idx": 1}, {"type": "text", "text": "Based on these findings, we introduce our proposed method to build a single model, aggregated through the weighted combination of individual models. Supposing that evaluation metrics are nondifferentiable, we employ Bayesian Optimization (BO) [5, 18], which is a black-box optimization technique, in developing our model fusion method. To the best of our knowledge, this is the first study that utilizes BO in the context of model fusion, in order to achieve the following objectives: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Utilization of Both Metrics and Loss Functions in Model Fusion. Instead of running BO with an averaged target metric, we use Multi-Objective Bayesian Optimization (MOBO) that considers both metrics and loss functions for model fusion. Despite low correlations between loss and metric values, we find that incorporating loss values still serves as useful guidance. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Two-Stage Model Fusion. We devise our model fusion process as a two-stage BO procedure. One is for optimizing hyperparameters in fine-tuning and the other is dedicated to our model fusion method. The objective of the first stage is to maximize gains from the second stage to find hyperparameters leading to the optimal fused model after the BO of the second stage. ", "page_idx": 1}, {"type": "text", "text": "We demonstrate the effectiveness of BOMF on several NLP tasks, including both Natural Language Understanding (NLU) and Natural Language Generation (NLG), with RoBERTa, Text-to-Text Transfer Transformer (T5) and LLaMA. Through these comprehensive experiments, we assess the performance of BOMF in diverse NLP tasks and uncover the interesting properties of our approach through various ablation studies. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Problem Setup. In this paper, we explore the process of fine-tuning PLMs using two types of datasets: a downstream training dataset $\\mathcal{D}_{\\mathrm{trn}}$ and a validation dataset $\\mathcal{D}_{\\mathrm{val}}$ . Assuming that we are given a pre-trained set of weights $\\theta_{\\mathrm{init}}$ and a trainable set of weights ${\\bf w}_{\\mathrm{init}}$ for our PLM denoted as $\\mathcal{M}(\\pmb{\\theta},\\mathbf{w})$ , ${\\bf w}_{\\mathrm{init}}$ is either a subset of $\\theta_{\\mathrm{init}}$ or LoRA weights [22]. Specifically, in the former case, ${\\bf w}_{\\mathrm{init}}$ is deliberately selected from $\\theta_{\\mathrm{init}}$ . As a special case, ${\\bf w}_{\\mathrm{init}}$ will be identical to $\\theta_{\\mathrm{init}}$ if any layers or weights are not frozen. Meanwhile, if the LoRA method is employed during the fine-tuning of our model, ${\\bf w}_{\\mathrm{init}}$ will be the LoRA weights. ", "page_idx": 1}, {"type": "text", "text": "oWne  au sgei $K$ dtiasstikn. cEt amche trmicest,r idc aiss $f_{\\mathrm{metric}}^{(k)}(\\mathcal{M},\\mathcal{D})$ dfiofrf $k\\in[K]$ ,e ,t ow ehvialle uaa tdei fofuerr emntoidaebll\u2019es  lpoesrsf ofrunmcatinocne $f_{\\mathrm{metric}}^{(k)}$ $f_{\\mathrm{loss}}$ is employed for training. Assuming that $\\mathcal{D}_{\\mathrm{val}}$ is similar to the true data distribution, our goal is to find the optimal set of trainable weights $\\mathbf{w}^{\\star}$ that minimizes the following: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{w}^{\\star}=\\underset{\\mathbf{w}\\in\\mathbf{W}}{\\arg\\operatorname*{min}}\\sum_{k=1}^{K}\\bar{f}_{\\mathrm{metric}}^{(k)}(\\mathcal{M}(\\pmb{\\theta}_{\\mathrm{init}},\\mathbf{w}),\\mathcal{D}_{\\mathrm{val}}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\bar{f}_{\\mathrm{metric}}^{(k)}$ is a normalized version of $f_{\\mathrm{metric}}^{(k)}$ for all $k\\in[K]$ , and $\\mathbf{W}$ is the space of trainable weights. However, due to the non-differentiability of the f (mket)ric functions, conventional approaches resort to finding approximate solutions using gradient descent or its variants, as shown below: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\mathbf{w}}^{\\star}=\\underset{\\mathbf{w}\\in\\mathbf{W}}{\\arg\\operatorname*{min}}\\,f_{\\mathrm{loss}}(\\mathcal{M}(\\pmb{\\theta}_{\\mathrm{init}},\\mathbf{w}),\\mathcal{D}_{\\mathrm{trn}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "As will be discussed in the subsequent section, misalignment between loss and metric surfaces is more prominent in PLMs compared to computer vision models. To address this challenge, we propose a novel method to more adequately make use of $\\{f_{\\mathrm{metric}}^{(k)}\\}_{k=1}^{K}$ and $\\mathcal{D}_{\\mathrm{val}}$ by considering Equation 1. ", "page_idx": 2}, {"type": "text", "text": "Model Fusion. In the recent work [25, 70, 53, 54], there has been a growing interest in the use of weight averaging or model fusion across diverse tasks. This line of research is an effective strategy to achieve superior performance in downstream tasks, all while managing computational costs by aggregation of multiple models. In this context, the aggregation of multiple models invTohlev eosb tjehcet iivdee nitsi ftioc atdieorni voef  aa  fsueste do f $N$ gfhint e-vteucnteord, ,i nbayb luet ilwieziignhgt ,d seuncohte tdh aats $\\dot{S}~=~\\{\\mathbf{w}_{i}\\}_{i=1}^{N}$ $\\bar{\\bf w}$ $\\boldsymbol{S}$   \nall other members in $\\boldsymbol{S}$ . This can be expressed as $L_{\\mathrm{metric}}(\\bar{\\mathbf{w}})\\ \\leq\\ \\mathrm{arg}\\operatorname*{min}_{\\mathbf{w}\\in S}L_{\\mathrm{metric}}(\\mathbf{w})$ , where $\\begin{array}{r}{L_{\\mathrm{metric}}(\\mathbf{w}):=\\sum_{k=1}^{K}\\bar{f}_{\\mathrm{metric}}^{(k)}(\\mathcal{M}(\\pmb{\\theta}_{\\mathrm{init}},\\mathbf{w}),\\mathcal{D}_{\\mathrm{val}}).}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "Model fusion approaches can be categorized into two main types: 1) uniform averaging and 2) weighted averaging. Uniform averaging methods, e.g., SWA [25], Greedy Soups [70], involve the straightforward process of uniformly averaging weights within a subset $\\bar{S}\\,\\doteq\\,S$ to obtain an improved performing weight vector $\\bar{\\bf w}$ , i.e. $\\begin{array}{r}{\\mathbf{\\bar{w}}\\;=\\;\\frac{\\mathbf{\\bar{\\rho}}_{1}}{|\\bar{\\mathcal{S}}|}\\bar{\\sum}_{\\mathbf{w}\\in\\bar{\\mathcal{S}}}\\,\\mathbf{w}.}\\end{array}$ . Here, selecting a suitable subset $\\bar{S}$ is an important strategy for each method. On the other hand, weighted averaging approaches, e.g., Learned Soups [70] and Rewarded Soups [54], aim to determine an optimized weight vector w by forming a convex combination of parameters from $\\boldsymbol{S}$ , expressed as $\\begin{array}{r}{\\bar{\\mathbf{w}}=\\sum_{i=1}^{N}\\delta_{i}\\bar{\\mathbf{w}_{i}}}\\end{array}$ , where each averaging coefficient $\\delta_{i}$ satisfies $\\delta_{i}\\,\\geq\\,0$ , and $\\textstyle\\sum_{i=1}^{N}\\delta_{i}\\,=\\,1$ . While weigh ted averaging methods offer more flexibility compared to uniform aver aging, they often require additional training to determine suitable values for the coefficient set $\\delta$ through gradient descent updates based on the loss function $f_{\\mathrm{loss}}$ . However, in our proposed method, we suggest a weighted averaging technique that considers not only the loss function floss but also the metrics {f (mket)ric}kK=1. ", "page_idx": 2}, {"type": "text", "text": "Multi-Objective Bayesian Optimization. BO is a sample-efficient black-box optimization technique with probabilistic regression. Since we assume that an objective to optimize is unknown, a surrogate function, which is generally a probabilistic regression model, is estimated instead. The key desired properties of the surrogate function are attained by considering how a search space is exploited and explored through its outputs. Utilizing the surrogate function, BO eventually optimizes a specific form of optimizable function, called an acquisition function; see [5, 18] for details. ", "page_idx": 2}, {"type": "text", "text": "On top of generic BO, MOBO is used to solve an optimization problem, involved with $K$ different competing objectives: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{x}^{\\dagger}=\\underset{\\mathbf{x}}{\\arg\\operatorname*{min}}(f_{1}(\\mathbf{x}),f_{2}(\\mathbf{x}),\\ldots,f_{K}(\\mathbf{x})).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Supposing that we cannot directly access $f_{1},f_{2},\\ldots,f_{K}$ , probabilistic surrogate models, which are alternatives to unknown objectives, should be used to determine a next point to evaluate. To find a solution candidate of Equation 3 using MOBO, we can consider scalarization of either the realizations of surrogate models or acquisition functions corresponding to multiple objectives [48]. In contrast to the scalarization method, the maximization of Expected HyperVolume Improvement (EHVI), on a metric space [16] can be used: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}^{\\dagger}=\\underset{\\mathbf{x}}{\\arg\\operatorname*{max}}~\\mathrm{EHVI}(\\mathbf{x};\\mathbf{Y},\\mathbf{r}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where a hypervolume is defined as the size of space between the Pareto frontier of $n$ historical evaluations $\\mathbf{Y}\\,\\in\\,\\mathbb{R}^{n\\times K}$ and a reference point $\\mathbf{r}$ . While the scalarization determines query points by aggregating $K$ outputs with particular (potentially random) coefficients, the hypervolume improvement maximization chooses query points that widen the expected hypervolume, which is more robust to function scales without the sampling distributions of scalarization coefficients. As reported in the previous work [10, 3], compared to other existing MOBO algorithms, $q{\\mathrm{NEHVI}}$ which is a variant of the EHVI method that evaluates a batch of $q$ points in a parallel manner. Building on the powerful MOBO algorithm, our model fusion framework is capable of determining averaging coefficients efficiently reducing the number of evaluations required to find better fused PLMs. ", "page_idx": 2}, {"type": "image", "img_path": "lV4kTHTgpJ/tmp/07b4182f0a1aafa1e2022d89c2683e8309ddd206febd746b56cd1de01d920291.jpg", "img_caption": ["Figure 1: Visualization of the loss landscape over parameters (Figures 1a and 1c) and the metric landscape over parameters (Figures 1b and 1d) for both the vision task (Figures 1a and 1b) and the NLP task (Figures 1c and 1d). The metric is 1\u2212accuracy and F1 score for the vision task and the NLP task, respectively. In the vision task, we fine-tune the ResNet-50 model [21] pre-trained with ImageNet-21k [56] on the Caltech-101 dataset [35], while in the NLP task, fine-tuning was performed on the pre-trained RoBERTa model on the MRPC dataset. The members of the SWA for each figure are denoted as w1, w2, w3. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Empirical Findings ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present empirical observations motivating our model fusion strategy. In $\\S\\ 3.1$ , we initially illustrate distinct findings: unlike in computer vision tasks, in NLP tasks, there exists a significant misalignment between the loss and metric surfaces. This misalignment poses a challenge for straightforward model fusion methods when fine-tuning PLMs. In $\\S\\ 3.2$ , we find that the optimal fine-tuned hyperparameters for PLMs analogously align across different architectural configurations varying the number of frozen layers or variations in rank in the LoRA setting. ", "page_idx": 3}, {"type": "text", "text": "3.1 On Misalignment in Loss and Metric Landscapes ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The well-known success of uniform averaging, e.g., SWA and Model Soups, in image classification tasks, has been grounded on the flatness of a loss landscape. As one can see in Figure 1a, the use of uniform averaging successfully explores minima on the flatter region of the loss landscape using individual weights close to the flatter region, resulting in enhanced generalization loss on a test dataset. This generalization effect is similarly observed in the case of the metric landscape, as illustrated in Figure 1b, owing to the similarity between the loss and metric landscapes. This similarity is the consequence of the inherent similarity between the loss function and the metric [43]. However, the domain of language modeling, characterized by semantic, morphosyntactic, and pragmatic intricacies, requires the evaluation of generalization performance across a diverse array of tasks and metrics [12]. It is unlikely to precisely align these metrics with a training loss function [74, 39], leading to a misalignment that often results in more complex and less flat surfaces in language tasks compared to the loss function visually demonstrated in Figures 1c and 1d. ", "page_idx": 3}, {"type": "image", "img_path": "lV4kTHTgpJ/tmp/a2b2260cda2be83bfbfbb3f4b8da25c019995bdfad50dda3099aca11014ed919.jpg", "img_caption": ["Figure 2: Validation results on the MRPC dataset for RoBERTa: loss (shown in left panels) and F1 score (in right panels) for varying learning rates, batch sizes, and frozen layers. Optimal hyperparameters align well across different frozen layers, except when all pre-trained layers are frozen. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "In Figures 1c and 1d, we find that while uniform averaging can reach high generalization performance based on the loss function, it poorly performs concerning the metric function compared to the bestperforming weight in $\\boldsymbol{S}$ . However, Figure 1d shows that even though the uniform averaging of three weight points degrades the metric performance, better points in terms of higher metric values exist in the convex set of the three weight points. The empirical results we observe above, which are caused by the complex and misaligned surface, motivate the need to utilize weighted averaging methods and seek the optimal combination of averaging weights based on the metric. This does not agree with the previous findings in vision tasks [70] and Figure 1b which argue minimal performance difference between the weighted averaging and the uniform averaging. Refer to Appendix C.1 for numerical results that show the discrepancy between the loss and metric landscapes in PLMs. ", "page_idx": 4}, {"type": "text", "text": "3.2 On Hyperparameter Alignment ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Discovering the optimal training hyperparameters incurs significant computational costs, particularly when fine-tuning extensive foundational models [2, 45, 66]. This challenge arises since the ideal set of hyperparameters tends to vary in tandem with changes in both tasks and model structures. ", "page_idx": 4}, {"type": "text", "text": "Surprisingly, our empirical findings reveal a consistent alignment of optimal hyperparameters when fine-tuning PLMs, regardless of variations in the number of frozen layers or the rank of LoRA. As illustrated in Figure 2, the alterations in validation loss and metric resulting from changes in the learning rate or batch size exhibit a similar pattern across different numbers of frozen layers, except in the case when all pre-trained layers are frozen and only the classifier layer is trained. This proves that we can decrease computational cost for searching the optimal hyperparameters by tuning on smaller models with more frozen layers or LoRA with smaller ranks. Refer to Appendix C.2 to see the additional results when varying the adam beta, learning rate schedule, as well as the case of the LoRA. ", "page_idx": 4}, {"type": "text", "text": "Yang et al. [72] demonstrate that employing a particular model weight initialization method and learning rate scheduling method, referred to as $\\mu$ -parametrization, enables the transferability of certain training hyperparameters (such as learning rate and momentum) varying the width of the model. However, it is important to note that these results specifically pertain to scenarios where models are trained from scratch. This distinction is noteworthy as our context involves the fine-tuning of PLMs. It would be a great future research direction to theoretically analyze this phenomenon. ", "page_idx": 5}, {"type": "text", "text": "4 Bayesian Optimization Model Fusion ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, BOMF unfolds in three key steps. In $\\S\\ 4.1$ , we present the process of constructing a set of fine-tuned trainable weights $\\boldsymbol{S}$ , serving as components for model fusion. In $\\S\\ 4.2$ , we introduce a method to identify optimal hyperparameters crucial in the construction of the set $\\boldsymbol{S}$ based on the findings explained in $\\S\\ 3.2$ . Finally, we delve into how we conduct weighted averaging in $\\S\\ 4.3$ , following the insights discussed in $\\S\\ 3.1$ . ", "page_idx": 5}, {"type": "text", "text": "4.1 Fusion Member Sampling ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To improve the performance of our model through model fusion, it is crucial to carefully create the set $\\boldsymbol{S}$ by employing an appropriate weight sampling method. There are two main types of weight sampling methods: 1) sampling from multiple training trajectories [70] and 2) sampling from a single training trajectory with proper learning rate scheduling [25]. However, Wortsman et al. [70] indicate that, when applying model fusion with samples from multiple training trajectories, the performance improvement becomes less significant during the fine-tuning of PLMs compared to vision tasks. This limitation in NLP tasks is attributed to the misalignment in loss and metric surfaces, as discussed in $\\S\\ 3.1$ . Furthermore, when employing multiple training trajectories to sample fusion members, the training computation cost increases linearly in proportion to the number of fusion members. This poses a significant challenge, particularly in the context of fine-tuning PLMs. For these reasons, in our approach, we collect our fusion members from a single training trajectory. Since the fine-tuning process of PLMs involves a small number of training epochs and exhibits rapid convergence [41], we start gathering fusion members after $50\\%$ of the training epochs are completed. This timing is slightly quicker than the point described in the work [25], which begins collecting after $75\\%$ of the training epochs are concluded. Once we start collecting the fusion members, we proceed to uniformly sample 15 members throughout the remaining training epochs. Refer to Appendix A for more details on the process of collecting fusion members. ", "page_idx": 5}, {"type": "text", "text": "4.2 Hyperparameter Search via Bayesian Optimization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the construction of a set of fusion members $\\boldsymbol{S}$ from a single training trajectory, the effectiveness of the training trajectory significantly impacts the ultimate metric performance of the fused model weight w\u00af. In this context, the effectiveness of a training trajectory refers to the model\u2019s metric performance using the bestperforming weight within that trajectory on the validation dataset $\\mathcal{D}_{\\mathrm{val}}$ . The correlation in Figure 3 strongly indicates that the performance of the best-performing weight is positively correlated with the performance of the fused weight. Consequently, to achieve the best performance of the fused weight, it becomes crucial to identify the set of optimal hyperparameters $\\lambda$ that results in the most effective training trajectory. However, two primary challenges arise when searching for the optimal hyperparameters $\\lambda^{\\star}$ that yield the best metric ", "page_idx": 5}, {"type": "image", "img_path": "lV4kTHTgpJ/tmp/8b5a7fece4d36178a5cd668f9e046700d59aa11658c2a8ba14a366d9ab9a0e34.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: Correlation between the performance of best-performing weights in a training trajectory and the performance of the fused model. We fine-tune the RoBERTa model on the RTE dataset. Each point is obtained from the evaluation of a single trajectory with varying hyperparameters. ", "page_idx": 5}, {"type": "text", "text": "performance: 1) the metric functions $\\{f_{\\mathrm{metric}}^{(k)}\\}_{k=1}^{K}$ are non-differentiable and 2) we need to efficiently assign computational resources in finding better hyperparameters beyond na\u00efve methods such as grid search. To remedy these two issues simultaneously, in BOMF, we employ BO to find the optimal set ", "page_idx": 5}, {"type": "text", "text": "of hyperparameters: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\lambda^{*}=\\arg\\operatorname*{min}_{\\lambda}\\sum_{k=1}^{K}\\bar{f}_{\\mathrm{metric}}^{(k)}(\\mathcal{M}(\\pmb{\\theta}_{\\mathrm{init}},\\mathbf{w}(\\lambda)),\\mathcal{D}_{\\mathrm{val}}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathbf{w}(\\lambda)$ represents the best-performing weight within the training trajectory associated with the hyperparameter set $\\lambda$ . Here, we utilize Gaussian process (GP) regression [55] and Log-Expected Improvement [3] as a surrogate function and an acquisition function, respectively. We employ three randomly initialized sets of hyperparameters as the starting point for BO, conducting 10 iterations of computations to determine the optimal set $\\lambda^{\\star}$ . ", "page_idx": 6}, {"type": "text", "text": "The sequential nature of BO computations can lead to a substantial computational load, particularly in the context of fine-tuning PLMs. To address this issue and propose a more computationally efficient BO approach, we draw insights from the observations discussed in $\\S\\ 3.2$ . The alignment of the best hyperparameters for fine-tuning between the full model and lightweight models (e.g., frozen layers model or reduced rank LoRA) allows us to utilize the lightweight model instead of the full model when seeking the optimal set $\\lambda^{\\star}$ as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\lambda^{\\star}=\\underset{\\lambda}{\\arg\\operatorname*{min}}\\sum_{k=1}^{K}\\bar{f}_{\\mathrm{metric}}^{(k)}(\\mathcal{M}(\\pmb{\\theta}_{\\mathrm{init}},\\widehat{\\mathbf{w}}(\\lambda)),\\mathcal{D}_{\\mathrm{val}}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\widehat{\\bf w}$ is the trainable weight of the lightweight model. Refer to $\\S\\ 6$ to see how our computationally efficie n t method decreases computation time while maintaining performance. ", "page_idx": 6}, {"type": "text", "text": "4.3 Multi-Objective Bayesian Optimization for Model Fusion ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "After completing the construction of the set $\\boldsymbol{S}$ with $N$ individual models, the next stage involves selecting appropriate averaging coefficients $\\pmb{\\delta}\\in[0,1]^{N}$ to ensure the enhanced metric performance of a fused model. To achieve this, we can leverage metrics {f (mket)ric}kK=1 and apply a BO procedure to obtain optimal averaging coefficients $\\delta^{\\star}$ , similar to the optimization process for the hyperparameter set $\\lambda$ . However, restricting the consideration to metric performance solely on $\\mathcal{D}_{\\mathrm{val}}$ may result in our fused weights w\u00af overfitting to $\\mathcal{D}_{\\mathrm{val}}$ and exhibiting poor generalization to the true data distribution, this challenge, when optimizing $\\delta$ , we propose to minimize both $f_{\\mathrm{loss}}$ and $\\{f_{\\mathrm{metric}}^{(k)}\\}_{k=1}^{K}$ $\\S\\ 3.1$ employing ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{P}=\\biggl\\{\\delta^{\\star}\\mid\\delta^{\\star}=\\arg\\operatorname*{min}_{\\delta}\\left(l(\\delta),l_{1}(\\delta),\\ldots,l_{K}(\\delta)\\right)\\biggr\\},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "wNhoteer et $l(\\delta):=\\,\\bar{f}_{\\mathrm{loss}}(\\mathcal{M}(\\pmb{\\theta}_{\\mathrm{init}},\\bar{\\mathbf{w}}(\\delta)),\\mathcal{D}_{\\mathrm{val}})$ gahntsd $l_{k}(\\delta):=\\bar{f}_{\\mathrm{metric}}^{(k)}(\\mathcal{M}(\\pmb{\\theta}_{\\mathrm{init}},\\bar{\\mathbf{w}}(\\delta)),\\mathcal{D}_{\\mathrm{val}})$ ,  fi.oer. $k\\,\\in\\,[K]$ $\\bar{\\bf w}(\\pmb\\delta)$ $\\delta$ $\\bar{\\bf w}(\\pmb\\delta)=$ $\\sum_{i=1}^{N}\\delta_{i}\\mathbf{w}_{i}$ where $\\mathbf{w}_{i}\\in{\\cal S}$ for $i\\in[N]$ and $N$ is the number of models to fuse. ", "page_idx": 6}, {"type": "text", "text": "Here, we utilize the EHVI strategy, which is described in the work by Emmerich et al. [16]. The hypervolume, in this context, is defined as a volume size between $\\mathcal{P}$ and a reference point $\\mathbf{r}$ . We set the reference points as a zero vector. To enhance the optimization of the hypervolume improvement objective, we employ the logarithmic form of $q{\\mathrm{NEHVI}}$ algorithm [10, 3], which is implemented with the BoTorch framework [4]. As highlighted in $\\S~2$ , this algorithm has proven effective in practical multi-objective optimization scenarios. This makes it well-suited to handle the complex and sharp nature of our metric landscape, enabling it to successfully identify the optimal $\\delta^{\\star}$ . We run MOBO for a total of $5|\\pmb{\\delta}|=75$ iterations to find the optimal coefficients $\\delta^{\\star}$ . ", "page_idx": 6}, {"type": "text", "text": "In our case, additional constraints are in place for executing MOBO, specifically 1) equality constraints and 2) inequality constraints for $\\delta$ . To address the inequality constraints (i.e., $\\delta_{i}\\geq0$ ), we follow the work by Gardner et al. [17] to incorporate constraints into the acquisition function. To deal with the equality constraints $\\textstyle\\sum_{i=1}^{N}\\delta_{i}=1$ , we simply normalize the output of the acquisition function. Refer to Algorithm 1 in Appendix B for the summary of BOMF. ", "page_idx": 6}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Model Fusion for Pre-Trained Language Models. Due to the increasing number of model parameters in recent PLMs, there has been a significant increase in both memory requirements and computational costs [73, 8, 63]. Consequently, there is growing attention on a research direction aimed at enhancing the performance of PLMs while simultaneously managing computational costs and memory requirements through the exploration of model fusion methods [54, 71, 9]. However, most of these studies have focused on fusing the models fine-tuned on different tasks, aiming to develop a single multi-task learner. In the context of a single-task fine-tuning scenario within PLM, it has been observed that the previous simple weight-averaging approaches often yield marginal improvements [70, 27]; nevertheless, the exploration into the underlying rationale of this consequence remains limited. As mentioned in $\\S\\ 3$ , we find that uniform weight averaging does not always align generalization on the loss surface with the optimal point on the metric surface, primarily due to the discrepancy between loss and metric landscapes. To address this issue, we develope a singletask model fusion method based on MOBO, finding the optimal weight combination coefficients by considering both metrics and loss functions. ", "page_idx": 6}, {"type": "table", "img_path": "lV4kTHTgpJ/tmp/7fb9b90a36155285f0bf6cd2040d519b8c8b5bb22a109d37c150a586b47927f3.jpg", "table_caption": ["Table 1: Results on Medium-Sized Language Models. We conduct the text classification task using RoBERTa-base on a subset of the GLUE benchmark datasets, and the question-answering task using T5-base on the SQuAD2.0 dataset. ACC, F1, and EM denote accuracy, F1 score, and Exact Match, respectively. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Bayesian Optimization. BO [5, 18] is a promising strategy to optimize a black-box function. In particular, if a target objective is costly in terms of function evaluations, Specifically, BO sequentially seeks solution candidates by modeling a surrogate function and maximizing an acquisition function. In the BO community, a GP [55] is often employed as a surrogate function but diverse regression models such as Bayesian neural networks [61, 38] and tree-based models [23, 30] can be used. As a choice of acquisition function, expected improvement [26] and GP upper confidence bound [62] are often considered. Importantly, BO is more effective than other existing optimization strategies such as grid search and genetic algorithms [64]. Its efficacy has been demonstrated in a wide variety of applications such as hyperparameter optimization [58], nanostructured device design [31], and chemical reaction optimization [57]. Moreover, in the deep learning context, the necessity for efficient hyperparameter tuning via BO has risen following the increasing number of hyperparameters and parameters in models [59]. Consequently, BO is applied for hyperparameter optimization in various deep learning tasks, such as image classification [28, 34] and NLP tasks [44, 7]. ", "page_idx": 7}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we present empirical results demonstrating the effectiveness of BOMF in various NLP tasks. We compare our method to five basic algorithms aimed at finding a high-performing solution. Grid Fine-Tune is a simple fine-tuning method that selects the best-performing checkpoint using grid search. HPBO utilizes optimal hyperparameters obtained by $\\S\\ 4.2$ for fine-tuning the baselines. SWA is an optimization technique that averages model parameters obtained during training. Greedy SWA is a modified version of SWA inspired by Greedy Soups [70], sorting weights based on metric performance on $\\mathcal{D}_{\\mathrm{val}}$ and including them in $\\bar{S}$ only if they improve w\u00af\u2019s performance. Learned SWA, inspired by Learned Soup [70], learns the coefficients $\\delta$ based on the loss after fine-tuning. For medium-sized language models, we tested a variant of Transformer OTfusion [24], aligning pre-trained weights before averaging. Additionally, we experimented with TWA [36], a recent SWA variant that reconstructs $\\boldsymbol{S}$ by finding weight space basis vectors and learns $\\delta$ based on the loss. ", "page_idx": 7}, {"type": "text", "text": "Table 2: Results on Large Language Models. We compare the results of BOMF and baseline methods using the SAMSum and KorMCQA datasets for summarization and medical multiple choice questionanswering tasks with LLaMA2-7B and LLaMA3-8B. R1, R2, and RL denote Rouge-1, Rouge-2, and Rouge-L scores for summarization. Doctor, Nurse, and Pharm denote evaluation results for medical question answering in each respective field, using accuracy as the metric. ", "page_idx": 8}, {"type": "table", "img_path": "lV4kTHTgpJ/tmp/3592109ff1c844529391b5b2dde838167b155fc9bed322fc0474b197bdaede2d.jpg", "table_caption": ["(a) Summarization (SAMSum) "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "lV4kTHTgpJ/tmp/90ba5c1a42668827097c6e40ead3b512a558510d193b06d17cb6d9265cdc0bb3.jpg", "table_caption": ["(b) Korean Medical Question Answering "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "In all tables, the best performance is indicated with boldfaced underline, while the second-best value is represented with underline in each column. The final column \u2018Avg.\u2019 provides a summary of overall results for each method across various datasets or metrics. The terms \u2018Full\u2019 and \u2018Freeze\u2019 in Table 1 specify the exploration of optimal hyperparameters using either the entire model or a model with half of its weights frozen, as discussed in $\\S\\ 4.2$ . Similarly, the terms \u2018Rank 64\u2019 and \u2018Rank $\\r_{4}\\rrangle$ in Table 2a denote that we use the Rank 64 or the lightweight Rank 4 version of the LoRA model for the hyperparameter search, respectively. See Appendix A for the details of downstream datasets and hyperparameter selection. ", "page_idx": 8}, {"type": "text", "text": "6.1 Empirical Analysis on Medium-Sized Language Models ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We begin by evaluating the effectiveness of BOMF on medium-sized language models using RoBERTabase [40] and T5-base [51]. For RoBERTa-base, we performed text classification tasks using the GLUE benchmark datasets [65]. For T5-base, we carried out the question-answering task with the SQuAD2.0 [52] dataset. For both models, we fine-tuned the weights directly on the downstream datasets. ", "page_idx": 8}, {"type": "text", "text": "Table 1 shows that BOMF consistently outperforms other baselines across all model structure and datasets.3 Notably, the performance of HPBO, which uses hyperparameters obtained from $\\S\\ 4.2$ with the full model, surpasses Grid Fine-Tune for most datasets. These results demonstrate that our BO-based hyperparameter search framework effectively discovers optimal hyperparameters compared to grid search. Refer to Appendix C.4 for the performance of freeze HPBO, which uses a lightweight model for hyperparameter optimization. Freeze HPBO also clearly outperforms Grid Fine-Tune which proves the effectiveness of our BO-based hyperparameter search. Also, it is evident that model fusion methods, except BOMF, lead to performance declines compared to HPBO, as discussed in $\\S\\ 3.1$ , in certain datasets. On the contrary, BOMF consistently betters the performance compared to HPBO, yielding that our method with MOBO effectively finds optimal $\\delta^{\\star}$ even in complex and sharp metric landscapes. Refer to Table 16 for the complete results. ", "page_idx": 8}, {"type": "text", "text": "6.2 Empirical Analysis on Large Language Models ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We further validated the effectiveness of our proposed method by fine-tuning larger models using LoRA. Specifically, we experimented with LLaMA2-7B and LLaMA3-8B on tasks such as summarization using the SAMsum [19] dataset, Korean multi-choice medical question answering using the KorMCQA [32] dataset, and dialogue generation using the E2E [47] dataset. In the summarization task, while Learned SWA exhibited the best performance in terms of Rouge-2, BOMF surpassed Learned SWA in average performance across all metrics, as illustrated in Table 2a. Notably, for Rouge-L, only BOMF improved over HPBO, highlighting the effectiveness of the multi-objective approach in BOMF. Furthermore, as shown in Table 2b, our model not only outperforms other baselines but also demonstrates that fine-tuning remains essential for specific tasks despite the rise of in-context learning (ICL) [14]. This highlights the necessity of BOMF, which efficiently identifies hyperparameters and provides an effective fine-tuning solution through model fusion. The results for E2E can be found in Appendix C.4. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6.3 Ablation Study ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Number of Frozen Layers. To analyze the efficiency of memory and compute when using a lightweight model in the BO procedure to find ${\\bar{\\mathbf{\\lambda}}}^{\\star}$ we conduct a study using RoBERTa-base on the RTE and MRPC datasets. As presented in Table 3, the use of a lightweight model successfully identifies favorable hyperparameters that yield good performance while reducing the number of parameters by up to $25\\%$ and the computation time by up to $66\\%$ . This efficiency is achieved by caching outputs from the frozen layers. By systematically freezing layers from the tail of the model, we can cache the outputs from these frozen layers and reuse them during the training process. ", "page_idx": 9}, {"type": "text", "text": "Table 3: Results on the Varying Number of Frozen Layers. Comparison of the number of parameters and relative training wall-clock time per epoch when optimizing hyperparameters across different numbers of frozen layers, using RoBERTa-base fine-tuned on the RTE and MRPC datasets. ", "page_idx": 9}, {"type": "table", "img_path": "lV4kTHTgpJ/tmp/121cb2424075bfeba8ae324df2d2722ccd11c964c822b10a70e38bc8ce1bed70.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Multiple Objectives. To validate the efficacy of using multiple objectives when determining optimal $\\delta$ , we compare BOMF with single-objective baselines using T5-base on the SQuAD2.0 dataset. In this task, we consider two metrics: F1 score and Exact Match. Table 4 shows that relying on only one specific metric slightly increases the objective metric but results in a significant performance drop for the other metric. This outcome suggests ", "page_idx": 9}, {"type": "table", "img_path": "lV4kTHTgpJ/tmp/9f5cd076613988e8a670360e8502e910c01de6e5c034df74c937c49a9962f4dd.jpg", "table_caption": ["Table 4: Comparison of Using Multi-Objective and Single-Objective Approaches. Results of BOMF and single-objective BO baselines with T5-base fine-tuned on the SQuAD2.0 dataset. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "that using single-objective BO is appropriate when aiming to find a model optimized for a specific metric, while the use of MOBO is more suitable for discovering an optimal fused model that achieves high performance across various metrics. Refer to Appendix C.3 for further ablation studies. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we empirically remarked two intriguing findings on loss and metric landscapes and hyperparameter alignment. Then, motivated by the observations mentioned above, we proposed a novel BO-based BOMF algorithm for model fusion. Our method utilizes the BO and MOBO frameworks to seek optimal fine-tuning hyperparameters and averaging coefficients, respectively. We validated that our proposed method exhibits improved performance on both NLU and NLG tasks on middle- and large-scale PLMs. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Future Work. As discussed in $\\S\\ 3.2$ , compelling future research involves the theoretical analysis of the hyperparameter alignment phenomenon. Moreover, we empirically observed that when utilizing quantization-based low-rank approximation methods [11, 37], traditional uniform averaging methods and weighted averaging methods face challenges in effectively aggregating models. These challenges arise from the quantized weight values in the models that behave differently with averaging weights. Another research direction is the development of averaging methods for the quantization-based low-rank approximation methods. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partly supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (NRF-2022R1A5A708390812) and Institute of Information $\\&$ communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.RS-2019-II190075, Artificial Intelligence Graduate School Program (KAIST), No.2022- ", "page_idx": 9}, {"type": "text", "text": "0-00184, Development and Study of AI Technologies to Inexpensively Conform to Evolving Policy on Ethics, No.2022-0-00713, Meta-learning Applicable to Real-world Problems). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] A. Agarwal and A. Lavie. Meteor, M-BLEU and M-TER: Evaluation metrics for high-correlation with human rankings of machine translation output. In Proceedings of the Third Workshop on Statistical Machine Translation, June 2008. 26   \n[2] A. Aghajanyan, A. Shrivastava, A. Gupta, N. Goyal, L. Zettlemoyer, and S. Gupta. Better fine-tuning by reducing representational collapse. arXiv preprint arXiv:2008.03156, 2020. 5   \n[3] S. Ament, S. Daulton, D. Eriksson, M. Balandat, and E. Bakshy. Unexpected improvements to expected improvement for Bayesian optimization. In Advances in Neural Information Processing Systems (NeurIPS), 2023. 4, 7   \n[4] M. Balandat, B. Karrer, D. Jiang, S. Daulton, B. Letham, A. G. Wilson, and E. Bakshy. BoTorch: A framework for efficient Monte-Carlo Bayesian optimization. In Advances in Neural Information Processing Systems (NeurIPS), 2020. 7, 16   \n[5] E. Brochu, V. M. Cora, and N. de Freitas. A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599, 2010. 2, 3, 8 [6] J. Cha, S. Chun, K. Lee, H.-C. Cho, S. Park, Y. Lee, and S. Park. SWAD: domain generalization by seeking flat minima. In Advances in Neural Information Processing Systems (NeurIPS), 2021. 2   \n[7] L. Chen, J. Chen, T. Goldstein, H. Huang, and T. Zhou. Instructzero: Efficient instruction optimization for black-box large language models. arXiv preprint arXiv:2306.03082, 2023. 8   \n[8] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1\u2013113, 2023. 8   \n[9] A. Chronopoulou, M. E. Peters, A. Fraser, and J. Dodge. Adaptersoup: Weight averaging to improve generalization of pretrained language models. arXiv preprint arXiv:2302.07027, 2023. 8   \n[10] S. Daulton, M. Balandat, and E. Bakshy. Parallel Bayesian optimization of multiple noisy objectives with expected hypervolume improvement. In Advances in Neural Information Processing Systems (NeurIPS), 2021. 4, 7   \n[11] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023. 1, 10   \n[12] J. Dodge, S. Gururangan, D. Card, R. Schwartz, and N. A. Smith. Show your work: Improved reporting of experimental results. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019. 5   \n[13] B. Dolan and C. Brockett. Automatically constructing a corpus of sentential paraphrases. In Third International Workshop on Paraphrasing (IWP2005), 2005. 16   \n[14] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, L. Li, and Z. Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2023. 10   \n[15] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. 21   \n[16] M. T. M. Emmerich, K. C. Giannakoglou, and B. Naujoks. Single-and multiobjective evolutionary optimization assisted by Gaussian random field metamodels. IEEE Transactions on Evolutionary Computation, 10(4):421\u2013439, 2006. 3, 7   \n[17] J. R. Gardner, M. J. Kusner, Z. E. Xu, K. Q. Weinberger, and J. P. Cunningham. Bayesian optimization with inequality constraints. In Proceedings of the International Conference on Machine Learning (ICML), 2014. 7   \n[18] R. Garnett. Bayesian Optimization. Cambridge University Press, 2023. 2, 3, 8   \n[19] B. Gliwa, I. Mochol, M. Biesek, and A. Wawer. Samsum corpus: A human-annotated dialogue dataset for abstractive summarization. arXiv preprint arXiv:1911.12237, 2019. 9, 17   \n[20] H. He, G. Huang, and Y. Yuan. Asymmetric valleys: Beyond sharp and flat local minima. In Advances in Neural Information Processing Systems 32 (NeurIPS 2019), 2019. 2   \n[21] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 4   \n[22] E. J. Hu, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, et al. Lora: Low-rank adaptation of large language models. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. 1, 2   \n[23] F. Hutter, H. H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for general algorithm configuration. In Proceedings of the International Conference on Learning and Intelligent Optimization (LION), 2011. 8   \n[24] M. Imfeld, J. Graldi, M. Giordano, T. Hofmann, S. Anagnostidis, and S. P. Singh. Transformer fusion with optimal transport. In The Twelfth International Conference on Learning Representations, 2023. 8   \n[25] P. Izmailov, D. Podoprikhin, T. Garipov, D. Vetrov, and A. G. Wilson. Averaging weights leads to wider optima and better generalization. In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI), 2018. 2, 3, 6   \n[26] D. R. Jones, M. Schonlau, and W. J. Welch. Efficient global optimization of expensive black-box functions. Journal of Global Optimization, 13:455\u2013492, 1998. 8   \n[27] J. Kaddour, L. Liu, R. Silva, and M. J. Kusner. When do flat minima optimizers work? Advances in Neural Information Processing Systems (NeurIPS), 2022. 8   \n[28] K. Kandasamy, W. Neiswanger, J. Schneider, B. P\u00f3czos, and E. P. Xing. Neural architecture search with Bayesian optimisation and optimal transport. In Advances in Neural Information Processing Systems (NeurIPS), 2018. 8   \n[29] M. Kilickaya, A. Erdem, N. Ikizler-Cinbis, and E. Erdem. Re-evaluating automatic metrics for image captioning. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, 2017. 26   \n[30] J. Kim and S. Choi. On uncertainty estimation by tree-based surrogate models in sequential model-based optimization. In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS), 2022. 8   \n[31] J. Kim, M. Li, Y. Li, A. G\u00f3mez, O. Hinder, and P. W. Leu. Multi-BOWS: Multi-fidelity multiobjective Bayesian optimization with warm starts for nanophotonic structure design. Digital Discovery, 3(2):381\u2013391, 2024. 8   \n[32] S. Kweon, B. Choi, M. Kim, R. W. Park, and E. Choi. Kormedmcqa: Multi-choice question answering benchmark for korean healthcare professional licensing examinations. arXiv preprint arXiv:2403.01469, 2024. 9, 17   \n[33] B. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems (NeurIPS), 2017. 1 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[34] B. Letham, R. Calandra, A. Rai, and E. Bakshy. Re-examining linear embeddings for highdimensional Bayesian optimization. In Advances in Neural Information Processing Systems (NeurIPS), 2020. 8 ", "page_idx": 12}, {"type": "text", "text": "[35] F.-F. Li, M. Andreeto, M. Ranzato, and P. Perona. Caltech 101, Apr 2022. 4   \n[36] T. Li, Z. Huang, Q. Tao, Y. Wu, and X. Huang. Trainable weight averaging: Efficient training by optimizing historical solutions. In Proceedings of the International Conference on Learning Representations (ICLR), 2023. 8   \n[37] Y. Li, Y. Yu, C. Liang, P. He, N. Karampatziakis, W. Chen, and T. Zhao. Loftq: Lora-finetuning-aware quantization for large language models. arXiv preprint arXiv:2310.08659, 2023. 10   \n[38] Y. L. Li, T. G. J. Rudner, and A. G. Wilson. A study of Bayesian neural network surrogates for Bayesian optimization. arXiv preprint arXiv:2305.20028, 2023. 8   \n[39] G. Liu, Z. Yang, T. Tao, X. Liang, J. Bao, Z. Li, X. He, S. Cui, and Z. Hu. Don\u2019t take it literally: An edit-invariant sequence loss for text generation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2055\u20132078, 2022. 5   \n[40] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019. 9   \n[41] P. Lu, I. Kobyzev, M. Rezagholizadeh, A. Rashid, A. Ghodsi, and P. Langlais. Improving generalization of pre-trained language models via stochastic weight averaging. arXiv preprint arXiv:2212.05956, 2022. 6   \n[42] W. Maddox, T. Garipov, P. Izmailov, D. Vetrov, and A. G. Wilson. A simple baseline for Bayesian uncertainty in deep learning. In Advances in Neural Information Processing Systems (NeurIPS), 2019. 2   \n[43] A. Mao, M. Mohri, and Y. Zhong. Cross-entropy loss functions: Theoretical analysis and applications. In Proceedings of the International Conference on Machine Learning (ICML), 2023. 4   \n[44] G. Melis, C. Dyer, and P. Blunsom. On the state of the art of evaluation in neural language models. In Proceedings of the International Conference on Learning Representations (ICLR), 2018. 8   \n[45] M. Mosbach, M. Andriushchenko, and D. Klakow. On the stability of fine-tuning bert: Misconceptions, explanations, and strong baselines. arXiv preprint arXiv:2006.04884, 2020. 1, 5   \n[46] G. Nam, S. Jang, and J. Lee. Decoupled training for long-tailed classification with stochastic representations. In Proceedings of the International Conference on Learning Representations (ICLR), 2023. 2   \n[47] J. Novikova, O. Du\u0161ek, and V. Rieser. The e2e dataset: New challenges for end-to-end generation. arXiv preprint arXiv:1706.09254, 2017. 9, 17   \n[48] B. Paria, K. Kandasamy, and B. P\u00f3czos. A flexible framework for multi-objective Bayesian optimization using random scalarizations. In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI), 2019. 3   \n[49] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems (NeurIPS), 2019. 16   \n[50] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understanding by generative pre-training. arXiv preprint arXiv:1801.06146, 2018. 1   \n[51] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(1):5485\u20135551, 2020. 9   \n[52] P. Rajpurkar, R. Jia, and P. Liang. Know what you don\u2019t know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822, 2018. 9, 16, 17   \n[53] A. Ram\u00e9, K. Ahuja, J. Zhang, M. Cord, L. Bottou, and D. Lopez-Paz. Model ratatouille: Recycling diverse models for out-of-distribution generalization. In Proceedings of the International Conference on Machine Learning (ICML), 2023. 3   \n[54] A. Ram\u00e9, G. Couairon, M. Shukor, C. Dancette, J.-B. Gaya, L. Soulier, and M. Cord. Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. In Advances in Neural Information Processing Systems (NeurIPS), 2023. 3, 8   \n[55] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006. 7, 8   \n[56] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252, 2015. 4   \n[57] B. J. Shields, J. Stevens, J. Li, M. Parasram, F. Damani, J. I. M. Alvarado, J. M. Janey, R. P. Adams, and A. G. Doyle. Bayesian reaction optimization as a tool for chemical synthesis. Nature, 590(7844):89\u201396, 2021. 8   \n[58] J. Snoek, H. Larochelle, and R. P. Adams. Practical Bayesian optimization of machine learning algorithms. In Advances in Neural Information Processing Systems (NeurIPS), 2012. 8   \n[59] J. Snoek, O. Rippel, K. Swersky, R. Kiros, N. Satish, N. Sundaram, M. Patwary, M. Prabhat, and R. Adams. Scalable bayesian optimization using deep neural networks. In Proceedings of the International Conference on Machine Learning (ICML), 2015. 8   \n[60] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u20131642, 2013. 16   \n[61] J. T. Springenberg, A. Klein, S. Falkner, and F. Hutter. Bayesian optimization with robust Bayesian neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2016. 8   \n[62] N. Srinivas, A. Krause, S. Kakade, and M. Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the International Conference on Machine Learning (ICML), 2010. 8   \n[63] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 1, 8   \n[64] R. Turner, D. Eriksson, M. McCourt, J. Kiili, E. Laaksonen, Z. Xu, and I. Guyon. Bayesian optimization is superior to random search for machine learning hyperparameter tuning: Analysis of the black-box optimization challenge 2020. In Proceedings of the NeurIPS Competition and Demonstration Track, 2020. 8   \n[65] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the International Conference on Learning Representations (ICLR), 2018. 9, 16   \n[66] L. Wang, Y. Li, T. Miller, S. Bethard, and G. Savova. Two-stage fine-tuning for improved bias and variance for large pretrained language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), July 2023. 5   \n[67] Z. Wang, W. Hamza, and R. Florian. Bilateral multi-perspective matching for natural language sentences. arXiv preprint arXiv:1702.03814, 2017. 16   \n[68] A. Williams, N. Nangia, and S. R. Bowman. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017. 16   \n[69] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al. Huggingface\u2019s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. 16   \n[70] M. Wortsman, G. Ilharco, S. Y. Gadre, R. Roelofs, R. Gontijo-Lopes, A. S. Morcos, H. Namkoong, A. Farhadi, Y. Carmon, S. Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In Proceedings of the International Conference on Machine Learning (ICML), 2022. 2, 3, 5, 6, 8   \n[71] P. Yadav, D. Tam, L. Choshen, C. Raffel, and M. Bansal. Resolving interference when merging models. arXiv preprint arXiv:2306.01708, 2023. 8   \n[72] G. Yang, E. J. Hu, I. Babuschkin, S. Sidor, X. Liu, D. Farhi, N. Ryder, J. Pachocki, W. Chen, and J. Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466, 2022. 6   \n[73] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. 8   \n[74] V. Zhukov, E. Golikov, and M. Kretov. Differentiable lower bound for expected bleu score. arXiv preprint arXiv:1712.04708, 2017. 5 ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Details of Experiments ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Our implementation leverages key libraries, including PyTorch 2.0.1 [49], Huggingface Transformers [69], and BoTorch [4], to construct a robust framework for our experiments. These experiments are rigorously conducted on high-performance computing hardware, specifically NVIDIA RTX 3090 and NVIDIA RTX A6000 GPUs, to ensure the efficiency and scalability of our models. To further bolster the reproducibility of our results, we meticulously set and documented all experiment seeds, enabling precise replication of our experimental conditions and findings. ", "page_idx": 15}, {"type": "text", "text": "A.1 Medium-Sized Language Models ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "lV4kTHTgpJ/tmp/f5542989377392bd32c5dffe3de13c99446b6e110e8c379fbbc338097950c034.jpg", "table_caption": ["Table 5: Detailed RoBERTa experimental setup. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "For the RoBERTa model, we evaluated the performance for classification and utilized a subset of the GLUE benchmark [65]. This benchmark serves as a comprehensive evaluation of a language model\u2019s overall NLU capabilities. The Recognizing Textual Entailment (RTE) task, which employs neutral and contradiction instances to assign a not-entailment label, is a binary classification task comprising 2,490 training instances and 277 validation instances. The Microsoft Research Paraphrase Corpus (MRPC) [13] consists of sentence pairs and corresponding labels. This task involves binary classification to determine whether a pair of sentences are semantically equivalent, utilizing the F1 score as the metric due to label imbalance. This dataset contains a total of 3,668 training and 408 validation instances. The Stanford Sentiment Treebank (SST-2) [60] includes movie reviews with associated positive/negative labels. The task is binary classification to discern the sentiment of a given sentence as positive or negative, with 67,349 training and 872 validation instances. The Stanford Question Answering Dataset (QNLI) [52] is a question-answering task composed of paragraphquestion pairs, where one sentence in the paragraph contains the answer to the human-generated question. This dataset comprises 104,743 training and 5,463 validation instances. The Quora Question Pairs dataset (QQP) [67] involves determining whether two questions are semantically equivalent, again using the F1 score as the metric due to label imbalance, with 363,846 training and 40,430 test instances. Lastly, The Multi-Genre Natural Language Inference Corpus (MNLI) [68] is labeled for textual entailment across genre pairs, primarily consisting of premise and hypothesis sentence pairs. This task predicts the relationship between these sentences in three categories. The dataset includes 392,702 training and 9,815 validation instances, of which we used the matched case of the validation set. We conducted experiments by selecting two datasets from each GLUE benchmark based on their size scale. Additionally, specific details on the fine-tuning methods can be found in Table 5. ", "page_idx": 15}, {"type": "table", "img_path": "lV4kTHTgpJ/tmp/b90595e162d3012777290d117d08cd05a28a6c1fdccf2e73697925f7cdde2e51.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "For the T5-base model, we utilize the Stanford Question Answering Dataset $(\\mathrm{SQuAD}\\ 2.0)$ [52]. This dataset comprises 130,319 training pairs and 11,873 validation pairs of questions and answers. The dataset can be accessed through the Hugging Face datasets library.4 Details on our fine-tuning procedures are provided in Table 6. Furthermore, we assess the generated answers by adhering to the code established in the official SQuAD 2.0 repository.5 ", "page_idx": 16}, {"type": "text", "text": "A.2 Large Language Models ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In our experiments with the LLaMA2- $\\mathbf{\\nabla}\\cdot7\\mathbf{B}^{6}$ model, we focused on two tasks: summarization and dialogue generation. For the summarization task, we employed the Samsung Abstractive Messenger Summarization (SAMSum) dataset [19], which consists of 14,732 training samples, 818 validation samples, and 819 test samples. For the dialogue generation task, we selected the End-to-End NLG Challenge (E2E) dataset [47]. This dataset includes 42,061 training samples, 4,672 validation samples, and 4,693 test samples. Details of our fine-tuning process are provided in Table 7. Notably, in the case of the E2E dataset, the test set typically contains around five common inputs with a variety of labels. To save time, we conducted a generate process for one common input and used the different labels as multiple references to calculate the metrics. Consequently, for evaluation, the sentences generated by the model are based on a unique label, totaling 630 sentences. This accounts for the discrepancy in experimental performance between our study and that presented in the original paper of the E2E dataset [47]. All metrics including BLEU, METEOR, and ROUGE were computed using the Huggingface evaluate library.7 ", "page_idx": 16}, {"type": "text", "text": "To demonstrate that fine-tuning is still necessary in specific domains and to show the effectiveness of our method in finding the best model under these circumstances, we conducted evaluations using the Korean Medical Multiple Choice Question Answering (KorMCQA) dataset [32]. For batch learning, we used text segments with a maximum sequence length not exceeding 512 tokens. Consequently, the train, test, and validation sets for doctors contained 1,890, 285, and 164 examples, respectively; for nurses, the train, test, and validation sets contained 582, 291, and 291 examples; and for pharmacists, the train, test, and validation sets contained 692, 614, and 300 examples, respectively. For in-context learning, we provided examples within this length limit, and for classification fine-tuning, we used a linear head. For this, we used the LLaMA3-8Bs model, the latest multilingual open-source large language model. This version was downloaded from this link.8 More specific details about the model and experiments can be found in Table 8. ", "page_idx": 16}, {"type": "table", "img_path": "lV4kTHTgpJ/tmp/587177b0e815326bb8e5c4c6002ba7d65375e082a88c79b1999503a61e3d307b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "A.3 Bayesian Optimization ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Details of HPBO. In the HPBO experiments, the number of iterations varied depending on the size of each dataset. Specifically, 20 iterations were conducted for the RTE dataset, while 10 iterations were carried out for the MRPC, SST2, and QNLI datasets. For the QQP and MNLI datasets, 8 iterations were performed. In addition, the SQuAD 2.0, SAMSum, and E2E datasets each underwent 10 iterations. These iteration counts were determined based on the respective sizes of the datasets. For single metric tasks, the chosen objective was the single valid metric itself. Conversely, for multi-metric tasks, the objective was the sum of all valid metrics. ", "page_idx": 17}, {"type": "table", "img_path": "lV4kTHTgpJ/tmp/76ac46ece59597973f10d9c6fcbf59fc321b01c3ba322047ff5d06fd131f8829.jpg", "table_caption": ["Table 8: Detailed LLaMA3-8B experimental setup. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Details of Sampling Fusion Members. We collected fusion members at step intervals ranging from 0.5 to 2.0 times the point of convergence identified in the training trajectory which represented $B$ in Algorithm 1, adjusting the process to yield approximately 15 members in total. Additionally, for the RoBERTa model, we employed PyTorch\u2019s official SWA scheduler with cosine annealing. For the T5 and LLaMA models, we do not use any additional scheduler for collecting SWA members. ", "page_idx": 18}, {"type": "text", "text": "Details of MOBO. In the case of MOBO, we initially provided the length of the fusion member and conducted iterations five times the total number of fusion members. This approach follows the common practice in BO of determining the initial points and the number of iterations based on the input dimension, allowing for the option to perform more iterations for improved performance. Furthermore, due to the differing scales of the loss and each metric, we applied min-max normalization ", "page_idx": 18}, {"type": "text", "text": "Require: Training set $\\mathcal{D}_{\\mathrm{trn}}$ , validation set $\\mathcal{D}_{\\mathrm{val}}$ , initial pre-trained weights $\\theta_{\\mathrm{init}}$ , initial hyperparameters $\\lambda_{\\mathrm{init}}$ .   \nEnsure: Optimized hyperparameters $\\lambda^{*}$ , combination coefficients $\\delta^{*}$ . ", "page_idx": 19}, {"type": "text", "text": "1: Initialize model $\\mathcal{M}$ with $\\theta_{\\mathrm{init}}$ , Optionally freeze layers and cache intermediate features. 2: Initialize BO with GP model, starting with $\\lambda_{\\mathrm{init}}$ and da $\\begin{array}{r}{\\mathcal{H}_{0}=(\\lambda_{\\mathrm{init}},\\sum_{k=1}^{K}\\bar{f}_{\\mathrm{metric}}^{(k)}(\\mathcal{M}(\\pmb{\\theta}_{\\mathrm{init}},\\mathbf{w}(\\lambda_{\\mathrm{init}})),\\mathcal{D}_{\\mathrm{val}})).}\\end{array}$ 3: for $i=1$ $I$ iter do 4: Define LogEI using current GP. 5: Find $\\lambda_{i}$ by optimizing LogEI. 6: Training $\\mathcal{M}(\\pmb{\\theta}_{\\mathrm{init}},\\mathbf{w})$ with $(\\mathcal{D}_{\\mathrm{trn}},\\lambda_{i})$ . 7: Evaluate $\\begin{array}{r}{\\sum_{k=1}^{K}\\bar{f}_{\\mathrm{metric}}^{(k)}(\\mathcal{M}(\\pmb{\\theta}_{\\mathrm{init}},\\mathbf{w}(\\pmb{\\lambda}_{i})),\\mathcal{D}_{\\mathrm{val}})}\\end{array}$ . 8: Update GP model with new data $\\begin{array}{r}{(\\lambda_{i},\\sum_{k=1}^{K}\\bar{f}_{\\mathrm{metric}}^{(k)}(\\mathcal{M}(\\pmb{\\theta}_{i},\\mathbf{w}(\\pmb{\\lambda}_{i})),\\mathcal{D}_{\\mathrm{val}})).}\\end{array}$ 190::  eCnodll efoctr $\\begin{array}{r}{\\lambda^{*}=\\arg\\operatorname*{min}_{\\lambda}\\sum_{k=1}^{K}\\bar{f}_{\\mathrm{metric}}^{(k)}(\\mathcal{M}(\\pmb{\\theta}_{\\mathrm{init}},\\mathbf{w}(\\lambda)),\\mathcal{D}_{\\mathrm{val}})}\\end{array}$ 11: $\\begin{array}{r}{B^{*}\\gets\\arg\\operatorname*{min}_{B}\\bar{f}_{\\mathrm{metric}}^{(k)}(\\mathcal{M}(\\pmb{\\theta}_{\\mathrm{init}},\\mathbf{w}_{B}(\\pmb{\\lambda}^{*})),\\mathcal{D}_{\\mathrm{val}})}\\end{array}$ 12: $S\\gets\\{\\}$ . 13: for $j=1$ to $J$ step do 14: Optimize $\\mathcal{M}(\\pmb{\\theta}_{\\mathrm{init}},\\mathbf{w}_{j})$ with $(\\mathcal{D}_{\\mathrm{trn}},\\lambda^{*})$ . 15: if $j\\geq0.5B^{*}$ then 16: $S\\gets S\\cup\\mathcal{M}(\\pmb{\\theta}_{\\mathrm{init}},\\mathbf{w}_{j})$ 17: end if 18: end for 19: Set reference point $\\mathbf{r}$ . 20: $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{init}}\\leftarrow\\{\\bar{f}_{\\mathrm{loss}}(\\mathcal{M}(\\theta_{\\mathrm{init}},\\bar{\\mathbf{w}}(\\delta_{\\mathrm{init}})),\\mathcal{D}_{\\mathrm{val}}),\\bar{f}_{\\mathrm{metric}}^{(1)}(\\mathcal{M}(\\theta_{\\mathrm{init}},\\bar{\\mathbf{w}}(\\delta_{\\mathrm{init}})),\\mathcal{D}_{\\mathrm{val}}),\\cdots.}\\end{array}$ , $\\bar{f}_{\\mathrm{metric}}^{(K)}(\\mathcal{M}(\\pmb{\\theta}_{\\mathrm{init}},\\bar{\\mathbf{w}}(\\pmb{\\delta}_{\\mathrm{init}})),\\mathcal{D}_{\\mathrm{val}})\\}$ 21: Initialize MOBO with GP models, starting with $\\bar{\\bf w}_{\\mathrm{init}}$ and prior data $\\mathcal{H}_{0}=(\\bar{\\mathbf{w}}_{\\mathrm{init}},\\delta_{\\mathrm{init}},\\mathcal{L}_{\\mathrm{init}})$ . 22: Compute initial Pareto optimal set ${\\mathcal P}_{0}$ using $\\mathcal{H}_{0}$ . 23: for $m=1$ to $M$ iter do 24: Define EHVI using current GPs. 25: Find $\\delta_{m}$ by optimizing EHVI. Equation 4 26: $\\begin{array}{r}{\\mathcal{L}\\gets\\{\\bar{f}_{\\mathrm{loss}}(\\tilde{\\mathcal{M}}(\\theta_{\\mathrm{init}},\\bar{\\Psi}(\\delta_{m})),\\mathcal{D}_{\\mathrm{val}}),\\bar{f}_{\\mathrm{metric}}^{(1)}(\\mathcal{M}(\\theta_{\\mathrm{init}},\\bar{\\mathbf{w}}(\\delta_{m})),\\mathcal{D}_{\\mathrm{val}}),\\cdot\\cdot\\cdot}\\end{array}$ , $\\bar{f}_{\\mathrm{metric}}^{(K)}(\\mathcal{M}(\\pmb{\\theta}_{\\mathrm{init}},\\bar{\\mathbf{w}}(\\pmb{\\delta}_{m})),\\mathcal{D}_{\\mathrm{val}})\\}$ 27: $(\\pmb{\\delta}_{m},\\pmb{\\mathcal{L}})$ 28: Update ${\\mathcal{P}}_{m}$ . 29: end for 30: Collect $\\delta^{*}=\\arg\\operatorname*{max}_{\\delta}\\,\\mathrm{EHVI}(\\delta;\\mathcal{L},\\mathbf{r})$ ", "page_idx": 19}, {"type": "text", "text": "to adjust the scales, utilizing the lowest value of single model performance obtained after the convergence point in the trajectory collection of members, rounded to the nearest value, as the minimum. The maximum values were determined by adding 0.1 for metric and 1.0 for loss respectively to this minimum value for use as the maximum. If there is a more critical metric or criterion, one can freely modify the optimization by placing weights according to the user\u2019s intent. No additional weights were applied in our experiments. ", "page_idx": 19}, {"type": "text", "text": "B Proposed Algorithm ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we provide an overview of the complete process of BOMF encapsulated in the algorithm. Algorithm 1 systematically incorporates all three steps: 1) hyperparameter search via BO as detailed in $\\S\\ 4.2,2$ ) fusion member sampling as outlined in $\\S\\ 4.1$ , and 3) identification of optimal $\\delta^{\\star}$ and model fusion through MOBO in $\\S\\ 4.3$ . ", "page_idx": 19}, {"type": "image", "img_path": "lV4kTHTgpJ/tmp/56e7f4816ed96fb7d17db761e7fe449ddaa65ce5bb6c3efa6f1c6097cdcac0be.jpg", "img_caption": ["(a) Loss surface of ViT-B/16. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "lV4kTHTgpJ/tmp/3a6b06a5bd9ded8f2783cccce376d79ae46ff048bc991992bd8000a5b7129788.jpg", "img_caption": ["(b) Metric surface of ViT-B/16. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 4: Visualization of the loss landscape over parameters (a) and the metric landscape over parameters (b) for the vision task. The metric is accuracy error. We fine-tune the ImageNet-21k pre-trained ViT-B/16 [15] model on the Caltech-101 dataset. The members of the SWA for each figure are denoted as $w_{1},w_{2},w_{3}$ . Here we can see similar trends with the ResNet-50 case. ", "page_idx": 20}, {"type": "text", "text": "Table 9: Spearman\u2019s rank correlation coefficient value of (a) ResNet18 model on the CIFAR10 dataset, (b) ViT-B/16 model on the Caltech101 dataset, (c) RoBERTa-base model on the SST-2 dataset, (d) T5-base model on the $\\mathrm{SQuAD}2.0$ dataset, and (e) LLaMA2-7B model on the E2E dataset. Here we used 15 fine-tuned weights for each task to measure the Spearman\u2019s rank correlation. A higher value indicates a higher correlation between metric and loss value. ", "page_idx": 20}, {"type": "table", "img_path": "lV4kTHTgpJ/tmp/9407e1dac5a7752b7e1d92b22b4193cc11477d12c88243bb6e8408284989a5e6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "C Additional Experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section we demonstrate additional experiments not included in the main article. ", "page_idx": 20}, {"type": "text", "text": "C.1 Additional Experiments on Loss and Metric Landscapes ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We also explored the potential loss metric discrepancy in PLMs, which might originate from the inherent features of transformer attention or from the use of adaptive optimizers. To analyze deeper, we visualize the loss-metric surface of the ViT-B/16 model per-trained on ImageNet-21k, using Adam optimizer which is the same as the optimizer of our language model. According to Figure 4, unlike in PLMs, the optimal points for loss and performance metrics in the ViT-B/16 model were aligned, indicating a distinct behavior between language and vision transformers in this context. Additionally, we have also undertaken the measurement of Spearman\u2019s rank correlation between loss and metric values across a range of models and tasks in both NLP and CV domains. Table 9 clearly shows that the correlations in NLP tasks are less than in CV tasks. ", "page_idx": 20}, {"type": "text", "text": "C.2 Additional Experiments on Hyperparameter Alignment ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In our experiments utilizing LoRA, we aimed to verify whether the optimal hyperparameters align when using a smaller rank to reduce costs, compared to using a larger rank. Figure 5 demonstrates that the optimal batch size and learning rate align even when the number of LoRA ranks varies. In addition to these hyperparameters, we investigated the potential alignment of the beta parameter of the Adam optimizer, which is the standard optimizer for training large language models, as well as the scheduler. ", "page_idx": 20}, {"type": "text", "text": "Figure 6 and Figure 7 indicate that, with the exception of the scenario where all layers are frozen, the optimal beta parameter of the Adam optimizer consistently aligns regardless of the number of frozen layers or the LoRA rank. Figure 9 and Figure 10 suggest that the optimal points of different learning rate schedules can be aligned according to the number of frozen layers. ", "page_idx": 20}, {"type": "text", "text": "These results indicate that by employing a lightweight model, we can identify the optimal hyperparameters, thereby simplifying the hyperparameter optimization process and reducing computational time. ", "page_idx": 20}, {"type": "image", "img_path": "lV4kTHTgpJ/tmp/56ead6d30e83a6224dca67e85c22ab329b332edec7ae3a7cd8090857b86c9f43.jpg", "img_caption": ["(a) Validation metric results for the varying batch size and the number of LoRA rank. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "lV4kTHTgpJ/tmp/cec91da18db7f0340a6722d295406d33797d970e14b52510c9b9b4e507d121c1.jpg", "img_caption": ["(b) Validatoin metric results for the varying learning rate and the number of LoRA rank. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 5: Validation loss and metric (F1 score) results for the varying hyperparameter ((a) batch size, (b) learning rate) and the number of LoRA rank for the RoBERTa on MRPC dataset. (a) and (b) indicate that the optimal hyperparameters consistently align well across different numbers of LoRA rank. ", "page_idx": 21}, {"type": "image", "img_path": "lV4kTHTgpJ/tmp/4e10ad8c7b39703f9b6edc5b95a47136c37549524e81ac2df96222a920dbdf4c.jpg", "img_caption": ["(a) Validation metric (Accuracy) for varying beta parameters and frozen layers. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "lV4kTHTgpJ/tmp/e9c96b3aa4fef6eb2594946288e22e33f2bd43fe317bad255b935cf3422ccd6b.jpg", "img_caption": ["(b) Validation loss for varying beta parameters and frozen layers. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 6: Results for the RoBERTa-base model on the RTE dataset. (a) and (b) indicate that the optimal hyperparameters align well across different numbers of frozen layers, except when all pre-trained layers are frozen. ", "page_idx": 21}, {"type": "text", "text": "C.3 Additional Ablation Experiments ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Metric Function in Multi-Objective Bayesian Optimization. In the $\\S\\ 3$ , we pointed out the issues of existing methods that perform fusion based solely on loss, particularly due to the discrepancy between metric and loss in language models. We demonstrated that our method outperforms the learned-SWA method, which relies only on loss. To strengthen our claim, we additionally compared the loss and metric values between optimization processes with and without metrics. Specifically, we examined the loss and metric values of weights along the training trajectory between the initial point and the optimized point of Learned SWA and BOMF. Figures 9 and 10 show that BOMF generates complex trajectories for both loss and metric, exploring solutions based on both criteria. In contrast, Learned SWA, relying solely on the loss function, gets trapped in local minima around the starting point, failing to discover the optimal solution. This suggests that BOMF\u2019s exploration property and the inclusion of metrics help escape local minima and discover more robust solutions. ", "page_idx": 21}, {"type": "text", "text": "Loss Function in Multi-Objective Bayesian Optimization. BOMF adopts both loss and metric as objectives when optimizing $\\delta$ . This approach is based on the understanding that the loss provides a macroscopic guide for overall metric performance. As seen in Figure 1, the solution of SWA is optimal on the loss surface but not on the metric surface. In contrast, BOMF performs well on both the loss and metric surfaces. These results indicate that BOMF ensures a high correlation between loss and metric values, preventing overftiting to either loss or metric during validation and enhancing the robustness ", "page_idx": 21}, {"type": "image", "img_path": "lV4kTHTgpJ/tmp/54ffcc3c67d0a1144d4cbebe58f7c2abe0c1ab4b22a6b0b1a25778126175fd78.jpg", "img_caption": ["(a) Validation metric (Accuracy) for varying beta parameters and LoRA ranks. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "lV4kTHTgpJ/tmp/b2e3a1cd1f080335ce9d6a5900d6007f54aad942428aef62bf6438ada3b76db2.jpg", "img_caption": ["(b) Validation loss for varying beta parameters and LoRA ranks. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 7: Results for the RoBERTa-base model on the RTE dataset. (a) and (b) highlight that optimal hyperparameters align well across different numbers of LoRA ranks, emphasizing the importance of parameter tuning. ", "page_idx": 22}, {"type": "image", "img_path": "lV4kTHTgpJ/tmp/3b25de8453745f1afc4cd1bc1e0824f254cf68e6ed52282368136a7745bbbd3f.jpg", "img_caption": ["(a) Validation metric (Accuracy) for varying learning rate schedule methods and frozen layers. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "lV4kTHTgpJ/tmp/0cb73b22f793053aa5a8d9bd422d9b2269f852770f8a7125e2d7747814b85dea.jpg", "img_caption": ["(b) Validation loss for varying learning rate schedule methods and frozen layers. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 8: Results for the RoBERTa-base model on the RTE dataset. (a) and (b) demonstrate that optimal hyperparameters are consistent across different numbers of frozen layers, indicating the critical role of hyperparameter choices. ", "page_idx": 22}, {"type": "text", "text": "of the weights on the test dataset. This clearly shows that including the loss function provides more useful guidance than optimizing $\\delta$ exclusively in a complex and sharp metric landscape. ", "page_idx": 22}, {"type": "text", "text": "Moreover, as demonstrated in Tables 10 and 11, our approach, which utilizes both loss and metric, improves the correlation between them. This leads to reaching the point of optimal performance, as shown in Tables 14, 15 and 17. These findings support that BOMF outperforms Bayesian optimization using a single metric without the loss function. ", "page_idx": 22}, {"type": "text", "text": "Multiple Metric Functions in Multi-Objective Bayesian Optimization. For tasks with multiple metrics, we optimized using all the available metrics. Therefore, it is important to investigate how multiple metrics impact the optimization process. In Table 4, we validated this through performance evaluations, but we also assessed changes in correlation. Table 11 shows that optimizing with a single metric can weaken the correlation between the loss and other metrics. ", "page_idx": 22}, {"type": "text", "text": "Impact of the Better Trajectories in Model Fusion Performance BOMF aims to construct the best-performing single model through model fusion within a parameter space. As detailed in $\\S\\ 4.1$ , different combinations of fine-tuning hyperparameters (such as learning rate and batch size) yield varied generalization performances after the fine-tuning process. Therefore, to identify the bestperforming single model after the model fusion, we must first determine the optimal hyperparameters that yield the best-performing single model before the fusion. In Table 14, we can confirm that the performance of the final BOMF, executed after finding good hyperparameters through outerbo, is better than the fusion performance executed with hyperparameters obtained from grid search. ", "page_idx": 22}, {"type": "image", "img_path": "lV4kTHTgpJ/tmp/4cac2c7f68436b9e92389a89ee9b3ec1f98390bfefaa0b3184ffb462f9175fea.jpg", "img_caption": ["Figure 9: Visualization of the evolution of loss throughout the optimization phases. We conduct experiments on the RTE dataset with the RoBERTa-base model. Learned SWA, an optimization process without metrics, tends to converge to local minima close to the starting point. Conversely, BOMF exhibits a more successful exploration, incorporating metrics, and ultimately discovers a more robust solution. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "lV4kTHTgpJ/tmp/86c2b9e08e8a896566b0cfd2e8d8718653e1e612a072cf87cb99bd89cc751587.jpg", "img_caption": ["Figure 10: Visualization of the evolution of metric throughout the optimization phases. We conduct experiments on the RTE dataset with the RoBERTa-base model. Like the loss, Learned SWA tends to converge towards local minima without enhancing metric performance after the convergence. In contrast, BOMF effectively navigates through explorations to discover a high-performing solution. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Performance of Conventional Optimization Strategies for Each Task. Additionally, we conducted experiments to directly compare the original optimizers with BOMF. We found the optimal hyperparameters for the original optimizers using BO and then performed experiments on subsets of the GLUE dataset using the RoBERTa model and the SAMSum dataset using the LLaMA2-7B model. The results presented in Table 12 clearly demonstrate that BOMF outperforms the best performance achievable with any of the original optimizers. ", "page_idx": 23}, {"type": "text", "text": "Additional Experiments with BOMF Using LLM-Based Evaluation To validate BOMF\u2019s performance under diverse evaluation metrics, we conducted experiments using a ChatGPT-3.5-Turbo-based approach. This method involves scoring by asking the LLM to assess the similarity between generated responses and ground-truth answers. Using this, BOMF was benchmarked against other models. As shown in Table 13, BOMF consistently outperformed these baselines, highlighting its capability to adapt not only to traditional metrics but also to newer evaluation techniques. ", "page_idx": 23}, {"type": "text", "text": "Table 10: Spearman\u2019s rank correlation coefficient value between loss and metric for the various optimization processes in the middle scale tasks. Here, we assess the correlation of the RoBERTa-base model on the MRPC dataset (a) and the RTE dataset (b). Additionally, we examine the correlation between loss and metric of the T5-base model on the SQuAD2.0 dataset using the F1 score (c) and the EM (d) metrics. We conduct evaluations on a total of 100 sampled sets for (a) and (b), and 30 for (c) and (d). Here, Loss BO SWA and Metric BO SWA denote the approach where we exclusively employ either the loss function or the metric during the MOBO process, respectively. ", "page_idx": 24}, {"type": "table", "img_path": "lV4kTHTgpJ/tmp/1e6e9820b151097e779d7db1e232ea81e9bc2d19d94a631a16db933472e03d5d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "lV4kTHTgpJ/tmp/d32f48b8ff242a9de5730f5db0151fbff2ad43ea3d40468b06c20e06a00efba7.jpg", "table_caption": ["Table 11: Spearman\u2019s rank correlation coefficient value between loss and metric for the various optimization processes in large-scale tasks. We evaluate the LLaMA2-7B on the SAMSum dataset. Here, Loss BO SWA and Metric BO SWA denote the approach where we exclusively employ either the loss function or the metric during the MOBO process, respectively. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "lV4kTHTgpJ/tmp/fe2f17657c6a0f8d141579027d9f7a6e0c327302a99d0fb598d08f42bbf703e2.jpg", "table_caption": ["Table 12: Results of BOMF and Other Neural Network Optimization Strategies on Various Datasets. A higher value is better for all the metrics. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "This robustness is further reinforced by BOMF\u2019s design, which combines loss with multiple metrics, enhancing its generalizability across unseen metrics. Notably, BOMF leverages BO for tuning combination coefficients, optimizing based on evaluation values rather than backward processing through the metrics themselves. This approach allows BOMF to efficiently optimize coefficients across complex evaluation settings, as shown in the ChatGPT BOMF column in Table 13. ", "page_idx": 24}, {"type": "text", "text": "These findings underscore BOMF\u2019s adaptability, demonstrating its resilience across varied evaluation frameworks, including those generated by LLMs. ", "page_idx": 24}, {"type": "text", "text": "C.4 Full Experimental Results ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we present full experimental results encompassing text classification tasks for the Masked Language Model (MLM) and question answering, summarization, and dialogue generation tasks for the autoregressive LLM. In all tables, the best performance is indicated with boldfaced underline, and the second-best value is represented with underline for methods that use the same best hyperparameters by $\\S\\ 4.2$ . The final column \u2018Avg.\u2019 provides a summary of overall results for each method across various datasets or metrics. The terms \u2018Full\u2019 and \u2018Freeze\u2019 in Tables 14 and 15 specify the exploration of optimal hyperparameters using either the entire model or a model with half of its weights frozen, as discussed in $\\S\\ 4.2$ . Similarly, the terms \u2018Rank $64^{\\circ}$ and \u2018Rank $\\mathrm{4}'$ in Tables 16 and 17 denote that we use the Rank 64 or the lightweight Rank 4 version of LoRA model for the hyperparameter search, respectively. ", "page_idx": 24}, {"type": "text", "text": "Table 13: Evaluation Results Using ChatGPT-3.5-Turbo. (a) Baseline, (b) SWA, (c) Greedy SWA, (d) Learned SWA, (e) BOMF, and (f) ChatGPT BOMF. ", "page_idx": 25}, {"type": "text", "text": "EVALUATION PROMPT EXAMPLE ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "YOU ARE AN AUTOMATED GRADING ASSISTANT HELPING A TEACHER GRADE STUDENT ANSWERS. ", "page_idx": 25}, {"type": "text", "text": "THE CORRECT SUMMARY FOR THIS TEXT IS: <GROUND-TRUTH>A STUDENT SUBMITTED THE SUMMARY: <PREDICTION>", "page_idx": 25}, {"type": "text", "text": "THE STUDENT\u2019S SUMMARY MUST BE CORRECT AND SPECIFIC BUT NOT OVERCOMPLETE.SMALL DIFFERENCES IN FORMATTING SHOULD NOT BE PENALIZED. ON A SCALE FROM 0 TO100, WHERE 0 MEANS COMPLETELY INCORRECT AND 100 MEANS COMPLETELY CORRECT,HOW SIMILAR IS THE STUDENT\u2019S SUMMARY TO THE GROUND TRUTH? PLEASE PROVIDEONLY A NUMERICAL SCORE WITHOUT ANY EXPLANATION.", "page_idx": 25}, {"type": "table", "img_path": "lV4kTHTgpJ/tmp/178ed022c56df5bf3d00a0babf547d7bcb9d1cc832135334f1ecb5de2caa8654.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "lV4kTHTgpJ/tmp/21222d10e4be658f05fb8e79fc18721c68998b80924c2f81af947ef25e81e5c0.jpg", "table_caption": ["Table 14: Full Results on Text Classification Task Using RoBERTa-base. Results of BOMF and baseline methods with GLUE benchmark datasets. ACC and F1 denote metrics for each dataset, representing accuracy and F1 score, respectively. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Text Classification. Table 14 demonstrates the consistently better performance of BOMF over other baselines that employ the same best hyperparameters. These findings affirm the effectiveness of BOMF in the context of single-metric NLP tasks. ", "page_idx": 25}, {"type": "text", "text": "Question Answering. Table 15 presents the complete experimental results for the questionanswering task. BOMF consistently surpasses other baselines utilizing the same best hyperparameters. These outcomes prove the effectiveness of BOMF in the realm of multi-metric NLP tasks, improving both F1 and EM metrics concurrently. ", "page_idx": 25}, {"type": "text", "text": "Summarization. Table 16 provides empirical evidence that BOMF achieves the highest average performance across evaluated metrics. While Learned SWA and Greedy SWA exhibit the best performance results in R1 and R2 metrics, respectively, they experience declines across other metrics. However, our approach demonstrates a consistent improvement across all metrics. These results prove the efficacy of BOMF in multi-metric NLP tasks. ", "page_idx": 25}, {"type": "text", "text": "Dialogue Generation. Table 17 provides full experimental results for the dialogue generation task. The results for BOMF showcase the highest scores for all metrics in both cases of rank 64 and rank 4. Despite the conflicting correlations between BLEU and METEOR metrics [29, 1], BOMF achieves comprehensive improvements across all metrics, distinguishing itself from other baselines that fail to ", "page_idx": 25}, {"type": "text", "text": "Table 15: Full Results on Question-Answering Task with T5-base. Results of BOMF and baseline methods with SQuAD2.0 dataset. F1 and EM denote the F1 score and Exact Match, respectively. ", "page_idx": 26}, {"type": "table", "img_path": "lV4kTHTgpJ/tmp/59b59626b5d81c35967fe990fdb30b486217d5529789bb88a83576f24e56eed3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "lV4kTHTgpJ/tmp/1469c660e4804e759b1612c85e37365a6786e220327dc1277e651071ee031ca5.jpg", "table_caption": ["Table 16: Full Results on Summarization Task with LLaMA2-7B. Results of BOMF and baseline methods with SAMSum dataset. R1, R2, and RL denote Rouge-1, Rouge-2, and Rouge-L, respectively. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "lV4kTHTgpJ/tmp/095699b51b89b534ddff4fe289d9319be8c12465f53c77cac3f9db44b497d7c3.jpg", "table_caption": ["Table 17: Full Results on Dialogue Generation Task with LLaMA2-7B. Results of BOMF and baseline methods with E2E dataset. B, M, and RL denote BLEU, METEOR, and Rouge-L, respectively. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "achieve such comprehensive enhancements. This proves the efficacy of our multi-objective method in effectively considering multiple metrics with conflicting correlations. ", "page_idx": 26}, {"type": "text", "text": "D Societal impact ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "BOMF does not directly have any positive or negative societal impacts since our algorithm is for fine-tuning and model fusion. However, in the sense of developing LLMs, we can argue the societal impacts of our work. On the positive side, our work can improve the productivity of human beings, e.g., reduction of repeating tasks, and discover new scientific knowledge, e.g., artificial intelligence for scientific discovery. On the other hand, as negative societal impacts, fine-tuning LLMs on downstream tasks can still consume significant compute resources, which leads to climate change. Moreover, since our fine-tuning process aims to optimize specific metrics, there can be a potential risk of optimizing towards malicious metrics such as aggressiveness and violence. Therefore, we should be aware of potential unethical outcomes and consider responsibility in selecting and optimizing these metrics. ", "page_idx": 27}, {"type": "text", "text": "E Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We use publicly available benchmarks and open-source models widely recognized in the LLM research community. Additionally, we do not release proprietary or new datasets or models that could cause the risk of misuse. Although our work potentially has a possibility to undertake inherent misuse that is derived from public benchmarks and open-source models, we think that our method itself does not pose a high risk of misuse. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: As outlined in $\\S\\ 1$ , our approach proposes a model fusion method that leverages BO techniques. The empirical results in $\\S~6$ demonstrate that BOMF outperforms other baseline methods. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We discussed the limitations of BOMF in $\\S\\ 7$ and proposed future research to provide theoretical reasoning for our findings. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: We don\u2019t have any theoretical results in our paper. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provided a detailed explanation of the algorithm for BOMF in $\\S\\ 4$ and discussed experimental details in Appendix A. Also, we provide detailed algorithms in Appendix B. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We offer comprehensive experimental settings, including details about the data, in Appendix C, and our code is provided in the supplemental material. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide detailed experimental settings in Appendix A. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "Justification: Due to the need to fine-tune Large Language Models across various tasks, conducting experiments multiple times would be excessively computationally expensive in our work. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide our computational resources in Appendix A. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our paper aligns with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We discuss the societal impacts regarding BOMF in Appendix D. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 31}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We discuss the safeguards regarding BOMF in Appendix E. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We address the code, data, models, and packages utilized in our experiments in Appendix A. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: We do not provide any new assets. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our paper does not include crowdsourcing or research involving human subjects. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our paper does not include crowdsourcing or research involving human subjects. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 33}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]