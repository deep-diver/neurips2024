[{"figure_path": "lV4kTHTgpJ/figures/figures_3_1.jpg", "caption": "Figure 1: Visualization of the loss landscape over parameters (Figures 1a and 1c) and the metric landscape over parameters (Figures 1b and 1d) for both the vision task (Figures 1a and 1b) and the NLP task (Figures 1c and 1d). The metric is 1-accuracy and F1 score for the vision task and the NLP task, respectively. In the vision task, we fine-tune the ResNet-50 model [21] pre-trained with ImageNet-21k [56] on the Caltech-101 dataset [35], while in the NLP task, fine-tuning was performed on the pre-trained ROBERTa model on the MRPC dataset. The members of the SWA for each figure are denoted as w\u2081, w\u2082, w\u2083.", "description": "This figure visualizes the loss and metric landscapes for both computer vision and NLP tasks.  The top row shows the results for a ResNet-50 model fine-tuned on the Caltech-101 dataset (a vision task), while the bottom row presents the results for a RoBERTa model fine-tuned on the MRPC dataset (an NLP task).  The plots illustrate the discrepancy between loss and metric landscapes, highlighting the challenges of using simple averaging methods like SWA for model fusion in NLP.", "section": "On Misalignment in Loss and Metric Landscapes"}, {"figure_path": "lV4kTHTgpJ/figures/figures_4_1.jpg", "caption": "Figure 1: Visualization of the loss landscape over parameters (Figures 1a and 1c) and the metric landscape over parameters (Figures 1b and 1d) for both the vision task (Figures 1a and 1b) and the NLP task (Figures 1c and 1d). The metric is 1-accuracy and F1 score for the vision task and the NLP task, respectively. In the vision task, we fine-tune the ResNet-50 model [21] pre-trained with ImageNet-21k [56] on the Caltech-101 dataset [35], while in the NLP task, fine-tuning was performed on the pre-trained ROBERTa model on the MRPC dataset. The members of the SWA for each figure are denoted as w1, w2, w3.", "description": "This figure visualizes the loss and metric landscapes for both computer vision and NLP tasks.  The top row shows ResNet-50 on Caltech-101, with (a) showing the loss landscape and (b) showing the metric (1-accuracy and F1-score).  The bottom row shows the same for ROBERTA on MRPC, again with (c) showing the loss and (d) showing the metric.  The visualization helps illustrate the difference in landscape characteristics between vision and NLP tasks and is used to motivate the need for a new model fusion technique. ", "section": "3.1 On Misalignment in Loss and Metric Landscapes"}, {"figure_path": "lV4kTHTgpJ/figures/figures_5_1.jpg", "caption": "Figure 3: Correlation between the performance of best-performing weights in a training trajectory and the performance of the fused model. We fine-tune the ROBERTA model on the RTE dataset. Each point is obtained from the evaluation of a single trajectory with varying hyperparameters.", "description": "This figure shows the correlation between the best single model performance within a training trajectory and the final fused model performance after applying the BOMF method.  Each point represents a different fine-tuning run with varying hyperparameters. The positive correlation indicates that using better performing training trajectories leads to better fused models.", "section": "4.2 Hyperparameter Search via Bayesian Optimization"}, {"figure_path": "lV4kTHTgpJ/figures/figures_20_1.jpg", "caption": "Figure 1: Visualization of the loss landscape over parameters (Figures 1a and 1c) and the metric landscape over parameters (Figures 1b and 1d) for both the vision task (Figures 1a and 1b) and the NLP task (Figures 1c and 1d). The metric is 1-accuracy and F1 score for the vision task and the NLP task, respectively. In the vision task, we fine-tune the ResNet-50 model [21] pre-trained with ImageNet-21k [56] on the Caltech-101 dataset [35], while in the NLP task, fine-tuning was performed on the pre-trained ROBERTa model on the MRPC dataset. The members of the SWA for each figure are denoted as w\u2081, w\u2082, w\u2083.", "description": "This figure visualizes the loss and metric landscapes for both computer vision and NLP tasks.  It highlights the significant difference between the loss and metric landscapes in NLP, showing a lack of alignment between loss minima and optimal metric values. The visualization uses ResNet-50 and ROBERTA models, with the metric representing accuracy (vision) and F1 score (NLP). The figure demonstrates a key finding of the paper: the mismatch of loss and metric landscapes makes simple averaging methods less effective for NLP fine-tuning.", "section": "Empirical Findings"}, {"figure_path": "lV4kTHTgpJ/figures/figures_20_2.jpg", "caption": "Figure 1: Visualization of the loss landscape over parameters (Figures 1a and 1c) and the metric landscape over parameters (Figures 1b and 1d) for both the vision task (Figures 1a and 1b) and the NLP task (Figures 1c and 1d). The metric is 1-accuracy and F1 score for the vision task and the NLP task, respectively. In the vision task, we fine-tune the ResNet-50 model [21] pre-trained with ImageNet-21k [56] on the Caltech-101 dataset [35], while in the NLP task, fine-tuning was performed on the pre-trained ROBERTa model on the MRPC dataset. The members of the SWA for each figure are denoted as w\u2081, w\u2082, w\u2083.", "description": "This figure visualizes the loss and metric landscapes for both computer vision and NLP tasks.  It shows the discrepancy between the loss and metric surfaces for NLP models, which motivates the use of a multi-objective Bayesian optimization approach in the paper.", "section": "3.1 On Misalignment in Loss and Metric Landscapes"}, {"figure_path": "lV4kTHTgpJ/figures/figures_21_1.jpg", "caption": "Figure 5: Validation loss and metric (F1 score) results for the varying hyperparameter ((a) batch size, (b) learning rate) and the number of LORA rank for the ROBERTa on MRPC dataset. (a) and (b) indicate that the optimal hyperparameters consistently align well across different numbers of LORA rank.", "description": "This figure visualizes the validation loss and F1 score for different hyperparameters (batch size and learning rate) across various LORA ranks in a fine-tuning experiment using the ROBERTa model on the MRPC dataset.  The plots demonstrate that the optimal hyperparameters remain consistent regardless of the LORA rank used, highlighting the robustness and efficiency of the proposed method.", "section": "3.2 On Hyperparameter Alignment"}, {"figure_path": "lV4kTHTgpJ/figures/figures_21_2.jpg", "caption": "Figure 5: Validation loss and metric (F1 score) results for the varying hyperparameter ((a) batch size, (b) learning rate) and the number of LORA rank for the ROBERTa on MRPC dataset. (a) and (b) indicate that the optimal hyperparameters consistently align well across different numbers of LORA rank.", "description": "This figure shows the validation loss and F1 score for different hyperparameter settings (batch size and learning rate) and varying numbers of LORA rank during the fine-tuning of a ROBERTa model on the MRPC dataset.  The plots demonstrate that the optimal hyperparameters remain consistent across different LORA ranks, suggesting a potential for efficient hyperparameter tuning using smaller models.", "section": "3.2 On Hyperparameter Alignment"}, {"figure_path": "lV4kTHTgpJ/figures/figures_21_3.jpg", "caption": "Figure 5: Validation loss and metric (F1 score) results for the varying hyperparameter ((a) batch size, (b) learning rate) and the number of LORA rank for the ROBERTa on MRPC dataset. (a) and (b) indicate that the optimal hyperparameters consistently align well across different numbers of LORA rank.", "description": "This figure displays the validation loss and F1 score for different hyperparameters (batch size and learning rate) and LORA ranks while fine-tuning a ROBERTa model on the MRPC dataset. The plots show a consistent alignment of optimal hyperparameters across various LORA ranks, suggesting that optimizing on a smaller model with fewer parameters can effectively transfer to larger models.", "section": "3.2 On Hyperparameter Alignment"}, {"figure_path": "lV4kTHTgpJ/figures/figures_21_4.jpg", "caption": "Figure 2: Validation results on the MRPC dataset for ROBERTa: loss (shown in left panels) and F1 score (in right panels) for varying learning rates, batch sizes, and frozen layers. Optimal hyperparameters align well across different frozen layers, except when all pre-trained layers are frozen.", "description": "This figure presents the results of an experiment on the MRPC dataset using the ROBERTa model.  The experiment explores the effects of varying three hyperparameters (learning rate, batch size, and the number of frozen layers) on both the validation loss and the F1 score.  The plots show that the optimal hyperparameter values for achieving the best F1 score remain relatively consistent across different numbers of frozen layers. However, when all pre-trained layers are frozen, the optimal hyperparameters differ significantly.", "section": "3.2 On Hyperparameter Alignment"}, {"figure_path": "lV4kTHTgpJ/figures/figures_22_1.jpg", "caption": "Figure 5: Validation loss and metric (F1 score) results for the varying hyperparameter ((a) batch size, (b) learning rate) and the number of LORA rank for the ROBERTa on MRPC dataset. (a) and (b) indicate that the optimal hyperparameters consistently align well across different numbers of LORA rank.", "description": "This figure shows the validation loss and F1 score for different hyperparameters (batch size and learning rate) while varying the rank of the LORA (Low-Rank Adaptation) method during fine-tuning of the RoBERTa model on the MRPC dataset.  The results demonstrate that the optimal hyperparameters remain largely consistent across different LORA ranks, suggesting that hyperparameter tuning might be performed on a smaller model with a lower LORA rank, potentially reducing computational cost.", "section": "3.2 On Hyperparameter Alignment"}, {"figure_path": "lV4kTHTgpJ/figures/figures_22_2.jpg", "caption": "Figure 5: Validation loss and metric (F1 score) results for the varying hyperparameter ((a) batch size, (b) learning rate) and the number of LORA rank for the ROBERTa on MRPC dataset. (a) and (b) indicate that the optimal hyperparameters consistently align well across different numbers of LORA rank.", "description": "This figure displays the validation loss and F1 score for different hyperparameter settings (batch size and learning rate) across varying LORA ranks while fine-tuning the ROBERTa model on the MRPC dataset.  The results show that optimal hyperparameters remain consistent across different LORA ranks, suggesting potential computational savings by using lower-rank models during hyperparameter optimization.", "section": "3.2 On Hyperparameter Alignment"}, {"figure_path": "lV4kTHTgpJ/figures/figures_22_3.jpg", "caption": "Figure 8: Validation results for the varying learning rate schedule methods and frozen layers. Results for the ROBERTa-base model on the RTE dataset. (a) and (b) demonstrate that optimal hyperparameters are consistent across different numbers of frozen layers, indicating the critical role of hyperparameter choices.", "description": "This figure visualizes the validation metric (accuracy) and loss for different learning rate schedules (cosine, linear, step, constant) and varying numbers of frozen layers (all, 2, 4, 6) during fine-tuning of the ROBERTa-base model on the RTE dataset.  The consistent alignment of optimal hyperparameters across different numbers of frozen layers highlights their importance for achieving optimal performance.  The results show that the choice of learning rate schedule significantly impacts performance, even with varying numbers of frozen layers.", "section": "3.2 On Hyperparameter Alignment"}, {"figure_path": "lV4kTHTgpJ/figures/figures_22_4.jpg", "caption": "Figure 8: Results for the ROBERTa-base model on the RTE dataset. (a) and (b) demonstrate that optimal hyperparameters are consistent across different numbers of frozen layers, indicating the critical role of hyperparameter choices.", "description": "The figure shows the validation loss and metric (accuracy) for the ROBERTa-base model on the RTE dataset, varying the learning rate schedule (cosine, linear, step, constant) and the number of frozen layers (all, 6, 4, 2, none).  The key takeaway is that the optimal hyperparameters are largely consistent, even when different numbers of layers are frozen, highlighting the importance of proper hyperparameter tuning.", "section": "3.2 On Hyperparameter Alignment"}, {"figure_path": "lV4kTHTgpJ/figures/figures_23_1.jpg", "caption": "Figure 1: Visualization of the loss landscape over parameters (Figures 1a and 1c) and the metric landscape over parameters (Figures 1b and 1d) for both the vision task (Figures 1a and 1b) and the NLP task (Figures 1c and 1d). The metric is 1-accuracy and F1 score for the vision task and the NLP task, respectively. In the vision task, we fine-tune the ResNet-50 model [21] pre-trained with ImageNet-21k [56] on the Caltech-101 dataset [35], while in the NLP task, fine-tuning was performed on the pre-trained ROBERTa model on the MRPC dataset. The members of the SWA for each figure are denoted as w\u2081, w\u2082, w\u2083.", "description": "This figure visualizes the loss and metric landscapes for both computer vision (ResNet-50 on Caltech-101) and NLP (RoBERTa on MRPC) tasks.  It shows a clear difference in the alignment between loss and metric landscapes between the two domains. In computer vision, there is a strong correlation, whereas in NLP, there is a significant mismatch. This difference motivates the use of multi-objective Bayesian optimization in the proposed method.", "section": "3.1 On Misalignment in Loss and Metric Landscapes"}, {"figure_path": "lV4kTHTgpJ/figures/figures_23_2.jpg", "caption": "Figure 1: Visualization of the loss landscape over parameters (Figures 1a and 1c) and the metric landscape over parameters (Figures 1b and 1d) for both the vision task (Figures 1a and 1b) and the NLP task (Figures 1c and 1d). The metric is 1-accuracy and F1 score for the vision task and the NLP task, respectively. In the vision task, we fine-tune the ResNet-50 model [21] pre-trained with ImageNet-21k [56] on the Caltech-101 dataset [35], while in the NLP task, fine-tuning was performed on the pre-trained ROBERTa model on the MRPC dataset. The members of the SWA for each figure are denoted as w1, w2, w3.", "description": "This figure visualizes the loss and metric landscapes for both computer vision and NLP tasks.  It shows how the loss and metric surfaces differ between the two domains, highlighting the mismatch between loss and metric landscapes in NLP, which motivates the use of multi-objective optimization in the BOMF approach.", "section": "Empirical Findings"}]