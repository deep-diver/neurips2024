[{"type": "text", "text": "A Phase Transition between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hugo Cui Freya Behrens Statistical Physics of Computation laboratory Statistical Physics of Computation laboratory EPFL, Lausanne, Switzerland EPFL, Lausanne, Switzerland ", "page_idx": 0}, {"type": "text", "text": "Florent Krzakala Lenka Zdeborova Information, Learning and Physics laboratory Statistical Physics of Computation laboratory EPFL, Lausanne, Switzerland EPFL, Lausanne, Switzerland ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Many empirical studies have provided evidence for the emergence of algorithmic mechanisms (abilities) in the learning of language models, that lead to qualitative improvements of the model capabilities. Yet, a theoretical characterization of how such mechanisms emerge remains elusive. In this paper, we take a step in this direction by providing a tight theoretical analysis of the emergence of semantic attention in a solvable model of dot-product attention. More precisely, we consider a non-linear self-attention layer with trainable tied and low-rank query and key matrices. In the asymptotic limit of high-dimensional data and a comparably large number of training samples we provide a tight closed-form characterization of the global minimum of the non-convex empirical loss landscape. We show that this minimum corresponds to either a positional attention mechanism (with tokens attending to each other based on their respective positions) or a semantic attention mechanism (with tokens attending to each other based on their meaning), and evidence an emergent phase transition from the former to the latter with increasing sample complexity. Finally, we compare the dot-product attention layer to a linear positional baseline, and show that it outperforms the latter using the semantic mechanism provided it has access to sufficient data. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent years have seen an upheaval in our ability to learn and implement complex tasks from textual data. Instrumental in these advances is the use of self-attention layers [1], which provide an efficient method of extracting information from sentences - both the information encoded in the ordering (i.e. positions) of the words, and that encoded in the meaning (i.e. semantics) of the words. In theory, attention layers can learn to leverage both types of information, by having tokens attend to each other based on their respective positions (a mechanism called positional attention in [2]) and/or respective meanings (henceforth referred to as semantic attention). In this paper, we aim at a theoretical understanding of the emergence of these different mechanisms in attention layers, and the transitions therebetween. ", "page_idx": 0}, {"type": "text", "text": "Many empirical studies have provided evidence for the emergence of specific algorithmic mechanisms (abilities) in the learning of language models that lead to qualitative improvements of the model capabilities [3, 4, 5]. By reverse-engineering trained models into human-interpretable components [6, 7, 8] a growing body of work on mechanistic interpretability aims to empirically understand which precise algorithmic mechanisms a neural network learns. Such investigations have demonstrated that attention layers are able to implement a wide range of different algorithms, even for the same task, using both positional and semantic attributes of the inputs. We offer a particularly simple illustration of this idea in Appendix A, where we show that for a sequence modelling task involving counting different algorithmic mechanisms co-exist, each corresponding to a distinct local minimum of the empirical loss. Out of these possibilities, the precise implementation that a model learns during training is jointly affected by its architecture [9], the training procedure itself [7, 4, 10] and the available data [11, 12, 13]. It remains an open question how to theoretically characterise the conditions under which a given behaviour emerges in the model leading to said qualitative improvements. ", "page_idx": 0}, {"type": "image", "img_path": "BFWdIPPLgZ/tmp/2e0b6028eea2888f6f09a25802ff94a539eb29758b0ae1fe5ef2b6b8a304b6ef.jpg", "img_caption": ["Figure 1: A phase transition in a toy model of attention. (A) We investigate a tied low-rank attention model in a teacher-student setting. The teacher mixes the $L$ individual tokens of dimension $d$ according to a semantic (as a function of the token's content $\\mathbf{x}$ ) and a positional (as a function of the token's position) attention matrix. The student can only use positional encodings p to fit the positional properties of the teacher. $(B)$ Schematic view of the loss landscape of the teacher, which contains both a positional and a semantic minimum. $(C)$ We find that in the asymptotic high-dimensional limit and as a function of the sample complexity and the composition of the teacher, the global minimum switches, constituting a phase transition between positional and semantic learning. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "From a theoretical perspective, even the nature of this type of algorithmic emergence is unclear. It is not known whether it is simply a fast but smooth change in performance or whether the emergence is due to a sharp boundary between fundamentally different regimes of learning [14]. In our work, we take inspiration from physics, where a similar theoretical question about the nature of phase transitions was posed a century ago for models of interacting particles, such as the famous Ising model describing ferromagnetism [15, 16]. In the limit of infinitely many particles, it was shown that it is possible to theoretically deduce sharp discontinuities in some properties of interest (e.g. the magnetization of a magnet), delineating qualitatively very different regimes (e.g. magnetized or not). While mathematically, a large size limit needs to be considered to confirm the existence of sharp phase transitions, this asymptotic theory usually closely matches simulations, even for relatively moderate finite sizes. ", "page_idx": 1}, {"type": "text", "text": "In the theory of feed-forward fully connected neural networks, phase transitions in the network's generalisation ability as more training samples are available were studied as early as in [17, 18], and their existence was proven mathematically rigorously in [19]. In those works, the limit of many particles corresponds to taking the number of training samples and the dimensionality of the data to infinity at a fixed ratio. These theories rely on the property that macroscopic quantities of interest, such as the test error, become concentrated and deterministic in the high-dimensional limit. A dimension-free set of equations is then derived which predicts these deterministic quantities. Since then, a plethora of works along these lines both in statistical physics of phase transitions and in the theory of feed-forward neural networks have continued to study these phenomena, see e.g. [20, 21, 22, 23]. ", "page_idx": 1}, {"type": "text", "text": "In this work, for the first time, we bring this type of study to the analysis of neural networks with attention layers. While several previous theoretical studies of the attention mechanism considered some type of high-dimensional limit [24, 25, 26, 2], none of them identified a phase transition between different types of mechanisms that are implemented by the attention layer. Simultaneously, the finite-dimensional real-world models that are the focus of works in mechanistic interpretability do not lend themselves to a tractable definition of a high-dimensional limit, which is necessary to identify a phase transition theoretically, as explained above. We aim to fill this gap by introducing and analysing a tractable model that permits such a sharp high-dimensional characterisation for attention layers (see Fig. 1). ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We describe a model with a single self-attention layer with tied, low-rank query and key matrices. On Gaussian input data and realizable labels, we show that this model exhibits a phase transition in terms of sample complexity between a semantic and a positional mechanism. \u00b7 As the main technical result, we analyse this model in the asymptotic limit where the embedding dimension $d$ of the tokens and the number $n$ of training samples grow proportionally. We provide a tight closed-form characterization of the test error and training loss achieved at the minima of the non-convex empirical loss. Using this high-dimensional characterization, we locate the positional-semantic phase transition, thus providing the first theoretical result about the emergence of sharp phase transitions in a model of dot-product attention. \u00b7 We contrast the performance of the dot-product attention layer with that of a linear model, which can only implement positional mechanisms, and show how the former outperforms the latter once it learns the semantic mechanism, highlighting the advantage of the attention architecture for this task, when there is a sufficient amount of training data. ", "page_idx": 2}, {"type": "text", "text": "In Section 2, we discuss further related work. Section 3 defines the general version of a solvable model of tied low-rank attention, and Section 4 provides a tight characterization of the global minimum of its empirical loss. Section 5 analyses a concrete instance of dot-product attention and demonstrates that in this case the global minimum corresponds to either a semantic or positional mechanism, depending on the training data and task, with a phase transition between them. We conclude with a discussion of the limitations of our analysis in Section 6. ", "page_idx": 2}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Theory of attention  Attention models have been the object of sizeable theoretical scrutiny in recent years, with a growing body of work investigating various aspects such as their expressivity [25, 27, 28], inductive bias [29, 30, 31], training dynamics [2, 32, 33, 34], and in-context learning [26, 35, 36, 37, 38]. [39] and [25] analyze models with frozen non-trainable queries and keys, under the lens of signal propagation in such frozen models, or of their expressivity. [2] similarly studies the learning of the value matrix and positional encodings only, fixing keys and queries to identity, and shows how a transformer with a single attention head can learn spatial structure with a purely positional attention mechanism. The works of [29, 37] analyze the learning of a single layer of attention, with trainable queries and keys, assuming linear or ReLu activations - instead of the standard softmax activation. [33] provide convergence bounds for non-linear transformer models with a single attention layer, with trainable queries and keys. Because these studies are not tight, they do not allow to capture sharp changes in the behaviour of attention mechanisms such as phase transitions, and cannot for the same reason provide a theoretical description of the sudden emergence of new algorithmic mechanisms. A first tight description was provided in [24], in the context of learning a high-dimensional graphical model with a single layer of factored attention, leveraging its formal equivalence to a linear and convex learning problem. On the other hand, this model does not exhibit any emergent phenomenon. [38] analyze how induction head mechanisms can be learnt from gradient descent on the population loss, i.e. when an infinite amount of data is available. The present manuscript conducts a tight analysis of the non-convex learning of a non-linear attention model with trainable tied queries and keys from a finite train set, thereby allowing the description of sharp phase transitions in sample complexity in the behaviour and performance of the model. ", "page_idx": 2}, {"type": "text", "text": "Positional encodings  To combine the positional and semantic information in textual or general sequential data, a variety of models and input encodings have been explored. Many approaches are based on autoregressive models, e.g. recurrent architectures [40], where the positional information is provided implicitly by the order in which the input is processed. While some transformers can leverage implicit positional information through causal masks in training [41, 42, 43], in principle a dot product attention layer requires an explicit encoding of positional information as it views the input sequence in parallel, as a bag of words [1]. Several works experimentally explore different types of positional encodings with the goal of improving the downstream task performance [44, 45]. In this work, we provide a tractable model to quantify the generalization error of a single layer of attention in the presence of positional encodings. ", "page_idx": 2}, {"type": "text", "text": "Theory of phase transitions in neural networks  In supervised learning with feed-forward fully connected neural networks, phase transitions in sample complexity were identified in settings where the data consists of random Gaussian samples, and the labels are generated by a target neural networks with random weights. For a single-layer perceptron and a variety of teacher weights distributions and activation functions, a discontinuity of the optimal test error as the number of samples increases was established in [18, 17, 19]. For a two-layer neural network, [21, 20] evidenced a specialization threshold in the sample complexity below which linear regression matches the optimal test accuracy, and above which a strictly better accuracy can be reached. To our awareness, phase transitions in neural networks with attention layers have not been studied theoretically yet. ", "page_idx": 3}, {"type": "text", "text": "3  Tied low-rank attention model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Input data model  We consider a model of embedded sentences with uncorrelated (1-gram) words. More precisely, a sentence $\\pmb{x}\\in\\mathbb{R}^{L\\times d}$ where $L$ is the sentence length and $d$ represents the embedding dimension, consists of $L$ tokens $\\{\\pmb{x}_{\\ell}\\}_{1\\leq\\ell\\leq L}$ independently drawn from a Gaussian distribution $\\pmb{x}_{\\ell}\\sim\\mathcal{N}(0,\\pmb{\\Sigma}_{\\ell})$ with covariance $\\Sigma_{\\ell}\\in\\mathbb{R}^{d\\times d}$ In the following, we denote the probability distribution of $\\textbf{\\em x}$ as $p_{x}$ . Note that while this sentence model does not involve in itself statistical correlations between tokens, the task (target function) will entail interactions between different tokens. While more general data models involving inter-token correlations can also be readily analyzed such analyses come at the price of much more intricate analytical formulae. We thus choose for clarity to restrict the discussion to this simple instance, which already displays rich phenomenology, as will be explored in Section 5. We defer a discussion and an analytical treatment of the general case to Appendix C. ", "page_idx": 3}, {"type": "text", "text": "Target function  The target function (teacher) is assumed to be of the form ", "page_idx": 3}, {"type": "equation", "text": "$$\ny(x)=\\mathtt{T}\\left[\\frac{1}{\\sqrt{d}}\\pmb{x}Q_{\\star}\\right]x,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for a function $\\mathbb{T}:\\mathbb{R}^{L\\times t}\\rightarrow\\mathbb{R}^{L\\times L}$ The term $\\Gamma\\left[1/\\sqrt{d}\\pmb{x}\\pmb{Q}_{\\star}\\right]\\in\\mathbb{R}^{L\\times L}$ in (1) should be interpreted as the target attention matrix, which mixes the tokens of the input $\\textbf{\\em x}$ . This attention matrix is parametrized by the target weights $Q_{\\star}\\in\\mathbb{R}^{d\\times r_{t}}$ ", "page_idx": 3}, {"type": "text", "text": "Tied attention We consider the learning of the target (1) by a single attention layer ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{\\pmb{Q}}(\\boldsymbol{x})=\\mathbf{S}\\left[\\frac{1}{\\sqrt{d}}(\\pmb{x}+\\pmb{p})\\pmb{Q}\\right](\\pmb{x}+\\pmb{p}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In (2), $\\textbf{\\textit{p}}\\in\\~\\mathbb{R}^{L\\times d}$ is a fixed matrix, corresponding to positional encodings, and $\\textbf{Q}\\in\\ \\mathbb{R}^{d\\times r_{s}}$ is a trainable weight matrix. We denote subsequently $\\dot{\\pmb{p}}_{\\ell}~\\in~\\mathbb{R}^{d}$ the $\\ell-$ th row of $\\pmb{p}$ \uff1aLike the target (1), the parametric function (2) takes the form of a data-dependent attention matrix $\\mathbf{\\vec{s}}\\left[\\vec{1}/\\sqrt{d}(\\pmb{x}+\\pmb{p})\\pmb{\\dot{Q}}\\right]\\in\\mathbb{R}^{L\\times L}$ mixing the tokens of the input $\\textbf{\\em x}$ Note that, compared to the usual attention mechanism [1] employed in practice, (2) corresponds to setting the value weights to identity, and - since (2) is parametrized by a single matrix $Q-$ tying the key and query weights. While the assumption of tied weights is not strictly necessary, it makes for simpler and more interpretable analytical characterizations, and is thus considered in this work for clarity. We provide in Appendix $\\mathbf{C}$ a full analysis of the untied architecture for completeness. ", "page_idx": 3}, {"type": "text", "text": "Empirical risk minimization We study the learning of the attention layer (2), when a training set ${\\cal D}=\\{{\\pmb x}^{\\mu},y({\\pmb x}^{\\mu})\\}_{\\mu=1}^{n}$ With $n$ independently sampled sentences $\\{{\\pmb x}^{\\mu}\\}_{\\mu=1}^{n}$ , andthe associated labels $\\{y(\\pmb{x}^{\\mu})\\}_{\\mu=1}^{n}$ , is available. The target (1) can be learnt by carrying out an empirical risk minimization: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{Q}=\\underset{Q\\in\\mathbb{R}^{d\\times r}}{\\operatorname{argmin}}\\left[\\sum_{\\mu=1}^{n}\\frac{1}{2d}\\left\\|y(\\pmb{x}^{\\mu})-f_{Q}(\\pmb{x}^{\\mu})\\right\\|^{2}+\\frac{\\lambda}{2}\\|Q\\|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The performance of the resulting trained model $f_{\\hat{Q}}$ is measured at test time by the mean squared error (MSE) ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\epsilon_{g}\\equiv\\frac{1}{d L}\\mathbb{E}_{\\pmb{x}\\sim p_{x}}\\left\\|y(\\pmb{x})-f_{\\hat{\\pmb{Q}}}(\\pmb{x})\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "4  Closed-form characterization of the training loss ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "High-dimensional limit We analyze the learning problem (3) in the limit where the embedding dimension $d$ and the number of training samples $n$ jointly tend to infinity, while their ratio $\\alpha={}^{n}/d$ (henceforth referred to as the sample complexity) stays of order $\\Theta_{d}(1)$ . We further assume the sentence length $L$ , the ranks $r_{s},r_{t}$ of the weights $Q,Q_{\\star}$ , and the norm of the positional embeddings $\\|p\\|$ , to be $\\bar{\\Theta_{d}}(1)$ . This limit has been considered in a stream of previous works (e.g. [46, 47, 48]) and allows to derive closed-form characterization of the ERM problem (3), which we present in the next section. It also exhibits a particularly rich learning phenomenology which we further explore in Section (5). Finally, let us comment briefly on the assumption that $r_{s}^{\\bar{}}=\\Theta_{d}(1)$ , which in words implies that the weight matrix $Q$ is low-rank. While primarily motivated by technical limitations here, it is worth noting that low-rank weights are also considered in machine learning practice, in the context of model compression [49] or fine-tuning [50]. ", "page_idx": 4}, {"type": "text", "text": "The main technical result of the present work is a closed-formed characterization of the test MSE (4) and training loss (3) achieved in the high-dimensional limit when training the model (2) via the empirical risk minimization of (3). ", "page_idx": 4}, {"type": "text", "text": "Assumption 4.1.Thecovariances $\\{\\Sigma_{\\ell}\\}_{\\ell=1}^{L}$ admit a commonset of eigenvectors $\\{e_{i}\\}_{i=1}^{d}$ .We further note $\\{\\lambda_{i}^{\\ell}\\}_{i=1}^{d}$ the eigenvalues of $\\Sigma_{\\ell}$ The eigenvalues $\\{\\lambda_{i}^{\\ell}\\}_{i=1}^{d}$ and the projection of the positional embedding $\\{p_{\\ell}\\}_{\\ell=1}^{L}$ andtheteachercolumns $\\{Q_{j}^{\\star}\\}_{j=1}^{r_{t}}$ on the eigenvectors $\\{e_{i}^{\\top}p_{\\ell}\\}_{i,\\ell},$ $\\{e_{i}^{\\top}Q_{j}^{\\star}\\}_{i,j}$ are assumed to admit awell-defined joint distribution $\\nu$ as $d\\ \\to\\ \\infty\\mathrm{~-~}n a$ mely, for $\\gamma=(\\mathring{\\gamma}_{1},...,\\gamma_{L})\\in\\mathbb{R}^{L},\\pi=(\\pi_{1},...,\\pi_{r_{t}})\\in\\mathbb{R}^{r_{t}}$ and $\\tau=(\\tau_{1},...,\\tau_{L})\\in\\mathbb{R}^{L}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{1}{d}\\sum_{i=1}^{d}\\prod_{\\ell=1}^{L}\\delta\\left(\\lambda_{i}^{\\ell}-\\gamma_{\\ell}\\right)\\delta\\left(\\sqrt{d}e_{i}^{\\top}p_{\\ell}-\\tau_{\\ell}\\right)\\prod_{j=1}^{r_{t}}\\delta\\left(e_{i}^{\\top}Q_{j}^{\\star}-\\pi_{j}\\right)\\xrightarrow{d\\to\\infty}\\nu\\left(\\gamma,\\tau,\\pi\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In words, Assumption 4.1 guarantees that all parameters of the problem admit well-defined limits in the considered asymptotic limit, with the further assumption that the covariances $\\{\\Sigma_{\\ell}\\}_{\\ell=1}^{L}$ of the different tokens can be jointly diagonalized. We are now in a position to state the main technical resultofthepresentwork. ", "page_idx": 4}, {"type": "text", "text": "Result 4.2. Under Assumption 4.1, in the limit $n,d\\to\\infty$ $\\|\\pmb{p}\\|,n/d,L,r_{s},r_{t}=\\Theta_{d}(1),$ thesummary statistics ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\rho_{\\ell}\\equiv\\frac{{\\cal Q}_{\\star}^{\\top}{\\mit\\Sigma}_{\\ell}{\\cal Q}_{\\star}}{d}\\in\\mathbb{R}^{r_{t}\\times r_{t}},}&{\\qquad\\qquad q_{\\ell}\\equiv\\frac{\\hat{{\\cal Q}}^{\\top}{\\mit\\Sigma}_{\\ell}\\hat{{\\cal Q}}}{d}\\in\\mathbb{R}^{r_{s}\\times r_{s}},}\\\\ {\\displaystyle m_{\\ell}\\equiv\\frac{\\hat{{\\cal Q}}^{\\top}p_{\\ell}}{d}\\in\\mathbb{R}^{r_{s}},}&{\\qquad\\qquad\\qquad\\theta_{\\ell}\\equiv\\frac{\\hat{{\\cal Q}}^{\\top}{\\mit\\Sigma}_{\\ell}{\\cal Q}_{\\star}}{d}\\in\\mathbb{R}^{r_{s}\\times r_{t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "concentrate in probability, and are solutions of the set of finite-dimensional self-consistent equations ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left({q_{\\ell}=\\int{d\\nu(\\gamma,\\tau,\\pi)\\gamma_{\\ell}\\left(\\lambda\\mathbb{I}_{r_{s}}+\\sum_{\\kappa=1}^{L}\\gamma_{\\kappa}\\hat{V}_{\\kappa}\\right)^{-1}}\\Bigg(\\sum_{\\kappa=1}^{L}\\hat{\\eta}_{\\kappa}\\hat{q}_{\\kappa}+\\Big(\\sum_{\\kappa=1}^{L}\\hat{m}_{\\kappa}\\tau_{\\kappa}+\\gamma_{\\kappa}\\hat{\\theta}_{\\kappa}\\cdot\\pi\\Big)^{\\otimes2}\\Bigg)\\left({\\lambda\\mathbb{I}_{r_{s}}+\\sum_{\\kappa=1}^{L}\\gamma_{\\kappa}\\hat{V}_{\\kappa}}\\right)^{-1}}\\\\ &{V_{\\ell}=\\int{d\\nu(\\gamma,\\tau,\\pi)\\gamma_{\\ell}\\left(\\lambda\\mathbb{I}_{r_{s}}+\\sum_{\\kappa=1}^{L}\\gamma_{\\kappa}\\hat{V}_{\\kappa}\\right)^{-1}}}\\\\ &{m_{\\ell}=\\int{d\\nu(\\gamma,\\tau,\\pi)\\tau_{\\ell}\\left(\\lambda\\mathbb{I}_{r_{s}}+\\sum_{\\kappa=1}^{L}\\gamma_{\\kappa}\\hat{V}_{\\kappa}\\right)^{-1}\\Bigg(\\sum_{\\kappa=1}^{L}\\hat{m}_{\\kappa}\\tau_{\\kappa}+\\gamma_{\\kappa}\\hat{\\theta}_{\\kappa}\\cdot\\pi\\Bigg)}\\\\ &{\\theta_{\\ell}=\\int{d\\nu(\\gamma,\\tau,\\pi)\\gamma_{\\ell}\\left(\\lambda\\mathbb{I}_{r_{s}}+\\sum_{\\kappa=1}^{L}\\gamma_{\\kappa}\\hat{V}_{\\kappa}\\right)^{-1}\\Bigg(\\sum_{\\kappa=1}^{L}\\hat{m}_{\\kappa}\\tau_{\\kappa}+\\gamma_{\\kappa}\\hat{\\theta}_{\\kappa}\\cdot\\pi\\Bigg)\\ \\pi^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\widehat{q}_{\\ell}=\\alpha\\mathbb{E}_{\\Xi,U}V_{\\ell}^{-1}\\left(\\mathrm{prox}(\\Xi,U)_{\\ell}-q_{\\ell}^{\\frac{1}{2}}\\xi_{\\ell}-m_{\\ell}\\right)^{\\otimes2}V_{\\ell}^{-1}\\right.}\\\\ &{\\left.\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\hat{V}_{\\ell}=\\widehat{\\theta}_{\\ell}\\theta_{\\ell}^{\\top}q_{\\ell}^{-1}\\!-\\!\\alpha\\mathbb{E}_{\\Xi,U}V_{\\ell}^{-1}\\!\\left(\\!\\mathrm{prox}(\\Xi,U)_{\\ell}\\!-\\!q_{\\ell}^{\\frac{1}{2}}\\xi_{\\ell}-m_{\\ell}\\right)\\xi_{\\ell}^{\\top}q_{\\ell}^{-\\frac{1}{2}}}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\hat{m}_{\\ell}=\\alpha\\mathbb{E}_{\\Xi,U}V_{\\ell}^{-1}\\left(\\mathrm{prox}(\\Xi,U)_{\\ell}-q_{\\ell}^{\\frac{1}{2}}\\xi_{\\ell}-m_{\\ell}\\right)}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\hat{\\theta}_{\\ell}=\\alpha\\mathbb{E}_{\\Xi,U}V_{\\ell}^{-1}\\left(\\mathrm{prox}(\\Xi,U)_{\\ell}-q_{\\ell}^{\\frac{1}{2}}\\xi_{\\ell}-m_{\\ell}\\right)\\left(u_{\\ell}-\\xi_{\\ell}^{\\top}q_{\\ell}^{-1/2}\\theta_{\\ell}\\right)^{\\top}\\left(\\rho_{\\ell}-\\theta_{\\ell}^{\\top}q_{\\ell}^{-1}\\theta_{\\ell}\\right)^{-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In (7), $U=\\{u_{\\ell}\\}_{\\ell=1}^{L}$ and $\\Xi=\\{\\xi_{\\ell}\\}_{\\ell=1}^{L}$ , with $u_{\\ell}\\sim\\mathcal{N}(\\xi_{\\ell}^{\\top}q_{\\ell}^{-1/2}\\theta_{\\ell},\\rho_{\\ell}-\\theta_{\\ell}^{\\top}q_{\\ell}^{-1}\\theta_{\\ell})$ and $\\xi_{\\ell}\\sim\\mathcal{N}(0,\\mathbb{I}_{r_{s}})$ and $_{\\otimes2}$ denoteudtfwihlfnlly $\\{\\mathrm{prox}(\\Xi,U)_{\\ell}\\}_{\\ell=1}^{L}$ are defined as the minimizers of the Moreau envelope ", "page_idx": 5}, {"type": "equation", "text": "$$\nM(\\Xi,U)=\\operatorname*{inf}_{z_{1},\\ldots,z_{L}}\\!\\sum_{\\ell=1}^{L}\\mathrm{Tr}\\left[V_{\\ell}^{-1}\\left(x_{\\ell}{-}q_{\\ell}^{1/2}\\xi{-}m_{\\ell}\\right)^{\\otimes2}\\right]\\!+\\!\\mathrm{Tr}\\left[\\mathsf{S}(Z)\\rho_{\\Sigma}\\mathsf{S}(Z)^{\\top}\\right]\\!-\\!2\\mathrm{Tr}\\left[\\mathsf{T}(U)\\rho_{\\Sigma}\\mathsf{S}(Z)^{\\top}\\right]\\!\\right\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We noted $Z\\in\\mathbb{R}^{L\\times r_{s}}$ (resp. $U\\in\\mathbb{R}^{L\\times r_{t}}$ ) the matrix whose rows are $z_{\\ell}$ (resp. ue) and: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\rho_{\\Sigma}\\equiv\\mathrm{diag}\\left[\\left(\\int d\\nu(\\gamma,\\tau,\\pi)\\gamma_{\\ell}\\right)_{\\ell=1}^{L}\\right]\\in\\mathbb{R}^{L\\times L}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In the same limit, the test error (4) converges in probability to ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\epsilon_{g}=\\frac{1}{L}\\mathbb{E}_{h}\\left.\\mathrm{Tr}\\big[{\\bf S}[h]\\rho_{\\Sigma}{\\bf S}[h]^{\\top}\\big]+\\frac{1}{L}\\mathbb{E}_{h^{\\star}}\\left.\\mathrm{Tr}\\big[{\\bf T}[h^{\\star}]\\rho_{\\Sigma}{\\bf T}[h^{\\star}]^{\\top}\\right]-2\\frac{1}{L}\\mathbb{E}_{h,h^{\\star}}\\left.\\mathrm{Tr}\\big[{\\bf S}[h]\\rho_{\\Sigma}{\\bf T}[h^{\\star}]^{\\top}\\big]\\right\\vert.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the average bears on $h\\in\\mathbb{R}^{L\\times r_{s}},h^{\\star}\\in\\mathbb{R}^{L\\times r_{t}}$ with independent rows with statistics ", "page_idx": 5}, {"type": "equation", "text": "$$\n(h_{\\ell},h_{\\ell}^{\\star})\\sim\\mathcal{N}\\left[\\left(\\!\\!\\frac{m_{\\ell}}{0}\\right),\\left(\\frac{q_{\\ell}}{\\theta_{\\ell}^{\\top}}\\!\\!\\begin{array}{c}{{\\theta_{\\ell}}}\\\\ {{\\rho_{\\ell}}}\\end{array}\\!\\!\\right)\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Finally, the training loss $\\epsilon_{t}$ converges in probability to ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\epsilon_{t}=\\alpha\\mathbb{E}_{Y,\\Xi}\\mathcal{M}-\\frac{1}{2}\\sum_{\\ell=1}^{L}\\mathrm{Tr}[\\hat{q}_{\\ell}V_{\\ell}]+\\!\\frac{\\lambda}{2}\\!\\int\\!\\!d\\nu(\\gamma,\\tau)\\mathrm{Tr}\\left[\\!\\left(\\!\\lambda+\\!\\!\\frac{L}{\\varepsilon_{1}}\\gamma_{\\ell}\\dot{\\nu}_{\\ell}\\right)^{-2}\\!\\!\\left(\\!\\frac{L}{\\varepsilon_{1}}\\gamma_{\\ell}\\hat{q}_{\\ell}\\!+\\!\\left(\\!\\frac{L}{\\varepsilon_{1}^{\\perp}}\\tau_{\\ell}\\hat{m}_{\\ell}\\!+\\!\\hat{\\theta}_{\\ell}\\!\\cdot\\!\\pi\\right)^{\\otimes2}\\!\\right)\\!\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Result 4.2 provides a tight asymptotic characterization of the test error $\\epsilon_{g}$ (10) and the training loss $\\epsilon_{t}$ (12), as a function of a finite set finite-dimensional summary statistics $\\{q\\ell,V_{\\ell},m\\ell,\\theta_{\\ell}\\}_{\\ell=1}^{L}$ thereby providing a finite-dimensional description of the high-dimensional learning problem (3). These summary statistics are further characterized in closed form by the set of equations (7), in terms of the solution of a low-dimensional minimization problem (91). Intuitively, this low-dimensional problem may be viewed as a form of an effective loss averaged over the finite training set. In practice, the solution of the self-consistent equations (7) can be found by numerically iterating the equations until convergence. The resulting summary statistics can then be used to evaluate the expressions (10) and (12) to reach the asymptotic limits of the test error and train loss. In Section 5 we evaluate exactly these functions to understand the different minima in the models empirical loss landscape. Similar sharp asymptotic characterizations have been derived in the literature for other neural network architectures trained with ERM, in particular generalized linear models (see e.g. [51, 52, 48]) and auto-encoders [47]. ", "page_idx": 5}, {"type": "text", "text": "The derivation of Result 4.2 is provided in Appendix B, and is exploiting a mapping of the model (2) to a (variant of) a Generalized Linear Model (GLM) [53, 54]. The summary statistics characterized by the equations (7) (often called state evolution [55] in this context) asymptotically describe the fixed points of a Generalized Approximate Message Passing (GAMP) algorithm [56], which we state in Appendix B. The fixed points of GAMP in turn correspond to critical (zero-gradient) points of the non-convex empirical loss landscape (3). Therefore, while Result (4.2) is stated as a characterization of the global minimum of (3), which is the main concern of the present work, solutions of (7) also describe local minima and saddles. ", "page_idx": 5}, {"type": "text", "text": "This strategy has been used in many recent work to study asymptotics of a large number of highdimensional problems, see e.g. [57, 58, 59, 60, 48]. We note, however, that we importantly assume the point-wise convergence of GAMP. While we believe that this point can be rigorously justified, it would require a considerable amount of work \u2014in particular, the usual rigorous tools used in recent works fall short because of the non-convexity of the loss\u2014\u2014 and we leave this point for future studies (see the discussion in Appendix C, where we also provide an alternative derivation using the replica method from statistical physics [61]). Finally, we mention that while Result 4.2 is presented for an $\\ell_{2}$ regularization of the empirical loss (3) for clarity, similar results can be reached for generic convex regularizers, and are presented in Appendix C. In the following section, we explore the phenomenology uncovered from the study of the equations (7) of Result 4.2, for the special case of dot-product attention. ", "page_idx": 5}, {"type": "image", "img_path": "BFWdIPPLgZ/tmp/461d2b447683687a0871f24e1f924f21ee2931ea2bb8176dc5206e78546643e2.jpg", "img_caption": ["Figure 2: Mixed positional/semantic teacher for $\\omega=0.3\\$ .Setting is $r_{s}=r_{t}=1,L=2,A=$ (0.6, 0.4), (0.4, 0.6)), $\\pmb{\\Sigma}_{1}=\\pmb{\\Sigma}_{2}=0.25\\mathbb{I}_{d}$ $\\pmb{p}_{1}=\\mathbf{1}_{d}=-\\pmb{p}_{2}$ and $Q_{\\star}\\sim\\mathcal{N}(0,\\mathbb{I}_{d})$ . (left) Solid lines: difference in training loss $\\Delta\\epsilon_{t}$ between the semantic and positional solutions of (7) in Result 4.2. Markers: difference in training loss at convergence achieved by training the model (2) using gradient descent initialized resp. at $Q_{\\star}$ and at $\\pmb{p}_{1}$ . Marker color as in Fig. 3. (center) overlap $\\theta$ between the learnt weights $\\hat{Q}$ and the target weights $Q_{\\star}$ overlap $m$ between the learnt weights $\\hat{Q}$ and the positional embedding $p_{1}$ . Solid lines represent the theoretical characterization of these two summary statistics provided by Result 4.2. Only the solution of (7) corresponding to the lowest found training loss is represented (i.e. the positional solution for $\\alpha<\\alpha_{c}$ and the semantic otherwise). Markers represent experimental measures of these quantities, for gradient descent at convergence. Gradient descent was initialized at $\\mathbf{p}_{1}$ for $\\alpha<\\alpha_{c}$ and at $Q_{\\star}$ for $\\alpha>\\alpha_{c}$ . (right) We show the MSE achieved by the dense linear as $\\epsilon_{g_{\\ldots}}^{m i n}$ (Result 4.2), and MSE achieved by the dense linear baseline $\\epsilon_{g}^{l i n}$ (15) (Result 5.1). Markers indicate the MSE experimentally reached by the model (2) trained using gradient descent, initialized previously for the overlaps. All experiments were performed in $d\\,=\\,1,000$ with the Pytorch implementation of full-batch gradient descent, for $T=5,000$ epochs and learning rate $\\eta=0.15$ . All points are averaged over 24 instances of the problem each. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "5  Positional-to-semantic phase transition ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Rank one dot-product attention  In the following, we turn to a special case of tied low-rank attention (2) - namely a dot-product attention layer, which is the example from Fig. 1: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{S}\\left[\\frac{1}{\\sqrt{d}}(\\mathbf{x}+p)Q\\right]=\\mathrm{softmax}\\left(\\frac{1}{d}(x+p)Q Q^{\\top}(x+p)^{\\top}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "As in (2), we allow for positional encodings $\\pmb{p}$ in the dot-product attention parametrization (13). We further consider a specific case of target attention matrix (1) of the form ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Upsilon\\left[\\frac{1}{\\sqrt{d}}x Q_{\\star}\\right]=(1-\\omega)\\mathrm{softmax}\\left(\\frac{1}{d}x Q_{\\star}Q_{\\star}^{\\top}x^{\\top}\\right)+\\omega A.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "With $A\\in\\mathbb{R}^{L\\times L}$ a fixed matrix. In (14), the parameter $\\omega\\in[0,1]$ tunes the relative strength of the dot-product term and the fixed matrix term, and interpolates between a fully positional and a fully semantictask: ", "page_idx": 6}, {"type": "text", "text": "\u00b7For $\\omega=0$ , the target reduces to the first dot-product term, and is purely semantic, in that the $i,j$ -th element of the score matrix so $\\mathrm{\\pftmax}(^{1}/\\!d x Q_{\\star}Q_{\\star}^{\\top}x^{\\top})$ only depends on the tokens $\\mathbf{\\boldsymbol{x}}_{i},\\mathbf{\\boldsymbol{x}}_{j}$ and not explicitely on their respective placements $i,j$ inside the sentence. To learn satisfyingly the target, the learning model thus has to learn a semantic attention matrix. \u00b7For $\\omega=1$ , the target reduces to the second fixed term $A$ in (14). The attention matrix $A$ associated thereto is purely positional, in the sense that $A_{i j}$ is a function of $i,j$ but not of $\\mathbf{\\boldsymbol{x}}_{i},\\mathbf{\\boldsymbol{x}}_{j}$ . To complete the learning task, a positional mechanism then needs to be learnt. ", "page_idx": 6}, {"type": "text", "text": "The parameter $\\omega$ thus allows to tune the amount of semantic/positional content in the target (14), and thus the extent to which the task requires the model to implement semantic attention (small $\\omega{\\mathbf s})$ or rather positional attention (large $\\omega\\mathbf{s}$ ). In the following, for definiteness, we further assume $r_{s}=r_{t}=1$ and set $Q_{\\star}$ to be a fixed random Gaussian vector drawn from $\\mathcal{N}(0,\\mathbb{I}_{d})$ , and choose the positional encodings $\\pmb{p}_{1}=-\\pmb{p}_{2}=\\pmb{1}_{d}$ . Finally, for simplicity, we consider sentences with two tokens $L=2$ and isotropic token covariances $\\Sigma_{1}=\\Sigma_{2}=\\sigma^{\\hat{2}}\\mathbb{1}_{d}$ ", "page_idx": 6}, {"type": "text", "text": "Semantic and positional mechanisms  The summary statistics $\\theta_{\\ell},m_{\\ell}$ describing the global minimizer of the empirical loss minimization (3) of the dot-product attention (13) on the target (14) are captured alongside the corresponding test error (4) and training loss (3), by Result 4.2. The solution of the system of equations (7) is not unique, and different stable fixed points describe different corresponding critical points of the non-convex empirical loss landscape (3). In practice, we notably find two solutions of (7), corresponding to two minima associated with different mechanisms implemented by the dot-product attention (13) when approximating the target (14): ", "page_idx": 6}, {"type": "image", "img_path": "BFWdIPPLgZ/tmp/2ca0cde12dc0b80215ee9a6968aab6ee819525e810a2da65b7dcad3837b774c5.jpg", "img_caption": ["Figure 3: Phase transition between semantic and positional training loss. Setting and experiments were performed identical to Fig. 2. (left) Scaling $d$ and $n$ jointly for $\\alpha=1.5$ concentrates for $\\theta$ and $m$ , in different locations for the positional and semantic local minima each. We show 30 runs for each $d\\in$ [10, 15, 23, 36, 56, 87, 135, 209, 323, 500]. (center) The color map represents the difference in training loss at convergence when training the model (2) using the Pytorch implementation of full-batch gradient descent, respectively from an initialization at $\\mathbf{p}_{1}$ or at $Q_{\\star}$ . The green dashed line represents the theoretical prediction for the threshold $\\alpha_{c}(\\omega)$ above which the semantic solution of (7) in Result 4.2 has lower loss than the positional solution. (right) The color map represents the difference in test MSE at convergence when training the attention model (13) using the Pytorch implementation of full-batch gradient descent initialized at $Q_{\\star}$ , and the dense linear baseline (15). The red dashed lines indicate the theoretical prediction -following from Result 4.2 and Result $15-$ for the threshold sample complexity $\\alpha_{l}(\\omega)$ above which the dot-product attention (2) outperforms the baseline (15). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "-Positional solution One solution of (7) correspond to vanishing overlap $\\theta=0$ between the trained weights $\\hat{Q}$ and the semantic target weights $Q_{\\star}$ , and non-zero $m>0$ between the trained weights $\\hat{Q}$ and the positional embedding $p_{1}~=~-p_{2}$ .Consequently, the argument of the dot-product attention ${\\hat{Q}}(x+p)$ has a sizeable token-independent -thus positional- contribution $\\hat{Q}p$ , alongside a token-dependent semantic part $\\hat{Q}x$ . Because of the positional terms, the resulting learnt attention attention matrix softmax $\\cdot({1}/{d({\\pmb x}+{\\pmb p})}\\hat{Q}\\hat{Q}^{\\top}({\\pmb x}+{\\pmb p})^{\\top})$ implements a partly positional mechanism. -Semantic solution Another solution of the system of equations (7) is associated with a vanishing overlap $m\\ =\\ 0$ between the learnt weights $\\hat{Q}$ and the positional embeddings, and a finite overlap $\\theta~>~0$ with the target weights $Q_{\\star}$ . Therefore the resulting learnt attention matrix softmax $\\big(1/d(\\pmb{x}+\\pmb{p})\\hat{\\pmb{Q}}\\hat{\\pmb{Q}}^{\\top}(\\pmb{x}+\\pmb{p})^{\\top}\\big)\\stackrel{-}{\\approx}\\mathrm{softmax}(1/d\\pmb{x}\\hat{\\pmb{Q}}\\hat{\\pmb{Q}}^{\\top}\\pmb{x}^{\\top})$ is largely semantic. ", "page_idx": 7}, {"type": "text", "text": "While the system of self-consistent equations (7) may admit other solutions, we did not find solutions with lower training loss than the two aforedescribed fixed points. Which of these solution corresponds to the global minimum - and thus the solution of the optimization (3)- depends on the sample complexity $\\alpha$ and thepositional/semantic parameter $\\omega$ (14), as we describe in the following subsection. ", "page_idx": 7}, {"type": "text", "text": "Positional-to-semantic phase transition  For a fixed parameter $\\omega$ in (14), an analysis of equations (7), further detailed in Appendix C, reveals that for a sizeable range of $\\omega$ , in the probed setups, there exists a threshold $\\alpha_{c}$ for the sample complexity so that ", "page_idx": 7}, {"type": "text", "text": "\u00b7For $\\alpha\\,<\\,\\alpha_{c}$ , the global minimum of (3) corresponds to a positional mechanism, and is described by the positional solution of (7) of Result 4.2 with $\\theta=0,m>0$ \u00b7For $\\alpha\\,>\\,\\alpha_{c}$ , the global minimum of (3) corresponds to a semantic mechanism, and is described by the semantic solution of (7) of Result 4.2 with $\\theta>0,m=0$ ", "page_idx": 7}, {"type": "text", "text": "The dot-product attention thus displays a phase transition in sample complexity from a positional mechanism to a semantic mechanism, implementing the simpler positional mechanism when having access to small amounts of data, and only learning the semantic content of the target (14) when presented sufficient data. The critical sample complexity $\\alpha_{c}$ generically grows with the positionality $\\omega$ of the target function (14), as the semantic content - i.e. the first term of (14)- is less apparent for larger $\\omega$ , and thus requires larger amounts of data to be identified and approximated by the dot-product attention (13). An example for $\\omega=0.3$ is given in Fig. 2. In Fig. 3 (center) the difference in training loss $\\Delta\\epsilon_{t}$ between the positional and semantic solutions of (7) is represented, alongside the difference in training loss at convergence experimentally reached by gradient descent. For small (resp. large) sample complexity $\\alpha<\\alpha_{c}$ (resp. $\\alpha>\\alpha_{c}.$ ), the training loss of the positional (resp. semantic) minimum is lower, and thus corresponds to the global minimum. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Experimentally, the positional minimum can be reached for $\\alpha<\\alpha_{c}$ via gradient descent by initializing the weights $Q$ of the attention (13) close to the positional embedding $\\pmb{p}_{1}$ . By the same means, the semantic minimum can be reached from an initialization at the teacher weights $Q_{\\star}$ (14). Henceforth, we refer with a slight abuse to the minimum experimentally reached from a positional (resp. semantic) initialization as the positional (resp. semantic) minimum, even when it is not global. Note that importantly the semantic initialization is informed in nature, in that it necessitates the knowledge of the target parameters $Q_{\\star}$ . Note that even though the minima we characterize analytically are fixed points of gradient descent, a precise analysis of the dynamics of gradient descent from an agnostic (random) initialization, and ascertaining whether the optimizer reaches the global minimum, is an interesting question but falls out of the scope of the present manuscript - which is an analysis of the loss landscape. We however conduct numerical experiments from a random initialization of $Q$ in Appendix E.4, and show that the dynamics may reach either of the local minima, or get stuck in a different one. ", "page_idx": 8}, {"type": "text", "text": "In Fig. 2, we compare our analytical characterizations for different metrics at the global mimimum - the summary statistics $\\theta,m$ (middle), and the test MSE (right)-, with the corresponding experimental estimates, obtained by optimizing (3) with the Pytorch implementation of gradient descent, from a positional (resp. semantic) initialization for $\\alpha\\,<\\,\\alpha_{c}$ (resp. $\\alpha\\,>\\,\\alpha_{c},$ 0, displaying overall good agreement. In Fig. 3 (left) and Appendix E.1 we further verify that in the scaling limit of our analysis, namely $n,d\\to\\infty$ for $\\alpha=O(1)$ , the agreement improves with growing $n,d$ ", "page_idx": 8}, {"type": "text", "text": "The dot-product attention (13) thus implements a semantic mechanism when learning from sufficient amounts of data. The learning of the semantic mechanism by the dot-product attention at sample complexities $\\alpha>\\alpha_{c}$ corresponds to a noticeable drop in the generalization MSE as can be observed in Fig. 2, right. But just how essential is the learning of semantic mechanism in the ability of the dot-product attention to generalize well? We explore this question in the following subsection, by comparing the dot-product attention (13) to a purely positional attention model. ", "page_idx": 8}, {"type": "text", "text": "Purely positional baseline  In this subsection, for the same target (14), we contrast the dot-product attention model (13), analyzed in the previous subsections, to the baseline given by a linear layer ", "page_idx": 8}, {"type": "equation", "text": "$$\nf_{W}({\\boldsymbol{\\mathbf{\\mathit{x}}}})=W\\cdot{\\boldsymbol{\\mathbf{\\mathit{x}}}},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "with a trainable weight matrix $W\\in\\mathbb{R}^{L\\times L}$ . As for the dot-product attention (13), we consider the case where the weights $\\hat{W}$ are learnt by minimizing the empirical risk ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\hat{W}=\\operatorname*{argmin}_{W\\in\\mathbb R^{L\\times L}}\\sum_{\\mu=1}^{n}\\lVert y(\\pmb{x}^{\\mu})-f_{W}(\\pmb{x}^{\\mu})\\rVert^{2}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The model (15) is a natural counterpart to the dot-product architecture (13). In (15), the attention matrix is parametrized by a single fully-trainable matrix $W$ , instead of being parametrized as a dot-product attention as in (13). A seminal difference in the two parametrizations is that while the elements of softmax $(1/\\!\\dot{d}x\\dot{Q}Q^{\\top}x^{\\top})$ can depend on the input tokens $\\textbf{\\em x}$ , and therefore express semantic information, the elements $W_{i j}$ of $W$ can only depend on the positions $i,j$ . The model (15) can thus only implement positional mechanisms, while the dot-product attention (13) can implement both linear and semantic mechanisms, as discussed above. Finally, observe that the model (15) is closely related to the one analyzed by [24] in another asymptotic limit. The following result characterizes the test error achieved by the purely positional model (15): ", "page_idx": 8}, {"type": "text", "text": "Result 5.1. In the same asymptotic limit as Result (4.2), the learnt weights $\\hat{W}$ trainedbyminimizing the empirical risk (16) coincide with the minimizer of the population risk,and thus admit the compact expression ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\hat{W}=\\mathbb{E}_{\\mathbf{x}}{\\mathrm{T}}\\left[{\\frac{1}{\\sqrt{d}}}x Q_{\\star}\\right]=\\mathbb{E}_{h}{\\mathrm{T}}[h]\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where the average bears over a finite-dimensional matrix $h\\in\\mathbb{R}^{L\\times t}$ with independent rows he with statistics $h_{\\ell}\\sim\\bar{\\mathcal{N}}(0,\\rho_{\\ell})$ where $\\rho_{\\ell}$ was defined in (6) in Result (4.2). We remind that T $\\left[1/\\sqrt{d}x Q_{\\star}\\right]$ ", "page_idx": 8}, {"type": "text", "text": "corresponds to the target score matrix (1).Finally, the test MSE $1/d L\\mathbb{E}_{\\pmb{x}}\\|y(\\pmb{x})-f_{\\hat{W}}(\\pmb{x})\\|^{2}$ achieved by the trained dense linear model $f_{\\hat{W}}$ (15) admits the asymptotic characterization ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\epsilon_{g}^{\\mathrm{lin}}=\\frac{1}{L}\\,\\mathrm{Tr}\\Big[\\hat{W}\\rho_{\\Sigma}\\hat{W}^{\\top}\\Big]+\\frac{1}{L}\\mathbb{E}_{h}\\,\\mathrm{Tr}\\big[\\Upsilon[h]\\rho_{\\Sigma}\\Upsilon[h]^{\\top}\\big]-\\frac{2}{L}\\mathbb{E}_{h}\\,\\mathrm{Tr}\\Big[\\hat{W}\\rho_{\\Sigma}\\Upsilon[h]^{\\top}\\Big].\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "The MSE achieved by the baseline (15) when learning the target (14) is plotted in Fig. 2 (right) as the orange solid line, alongside the MSE achieved by the dot-product attention (13) discussed in previous subsections. Remarkably, in the setup of Fig. 2, in the positional regime $\\alpha<\\alpha_{c}$ when the dot-product attention relies on a positional mechanism $\\theta=0,m>0$ to approximate the target, the dot-product attention (13) is outperformed by the purely positional attention (15) $\\epsilon_{g}>\\epsilon_{g}^{\\mathrm{lin}}$ elin. In contrast, in the semantic regime $\\alpha>\\alpha_{c}$ where the dot-product attention learns the semantic mechanism, there exists a sample complexity $\\alpha_{l}\\geq\\alpha_{c}$ above which $\\epsilon_{g}<\\epsilon_{g}^{\\mathrm{lin}}$ , i.e. the dot-product attention (13) outperforms the dense linear baseline (15). This threshold value $\\alpha_{l}$ is plotted for various positionality strengths $\\omega$ in Fig. 3, alongside the positional-to-semantic threshold $\\alpha_{c}$ . Interestingly, we observe $\\alpha_{l}\\geq\\alpha_{c}$ in all probed settings, temptingly suggesting the natural interpretation that the dot-product attention needs to learn the semantic mechanism first (at $\\alpha=\\alpha_{c}$ ) in order to then be able to outperform the best positional approximation $f_{\\hat{W}}$ (at $\\alpha=\\alpha_{l}$ ). This highlights the importance of the semantic mechanism, enabled by the dot-product parametrization (13), in learning targets with semantic content such as (14). ", "page_idx": 9}, {"type": "text", "text": "6 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Compared to the original transformer [1] we consider a simplified model: the query and key matrices in our model are sharing weights and are of a low rank, as a value matrix we use the identity, and we employ only one head and a single layer. Further, our data model is limited to Gaussian data with sentences of 1-grams. Concerning the analysis, our characterization holds only in the high-dimensional limit, but we show that even in the finite case experimental values lie close to the theoretical prediction. Since the analysis only concerns the minima of the models loss landscape the implications for the dynamics of learning algorithms, e.g. gradient decent, are limited. This shows in our numerical experiments where we need to initialize GD close to the minima in order to arrive at them. It is yet unclear if there are scaling limits of learning algorithms which would reliably find the lower or a specific one of the minima from a random initialization. ", "page_idx": 9}, {"type": "text", "text": "Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We explored the interplay between positional and semantic attention, through the prism of tied low-rank self-attention in high dimensions. In a theoretically controlled setting, we characterized the global optimum of the empirical loss, when learning a target attention layer. This global optimum was found to correspond to either a positional or a semantic mechanism, with a phase transition between the two mechanisms occurring as the sample complexity increases. We believe the present asymptotic analysis of the inner workings of attention mechanisms opens up exciting research directions. Considering alternative attention architectures (including a readout network after the attention layer, or considering cross-attention) or training procedures (such as masked language modelling, or training with causal masks), are some possible extensions which will hopefully pave the way towards a satisfactory theoretical comprehension of attention mechanisms. Finally, elucidating under which conditions either minimum can be reached by a given optimizer from a random initialization constitutes an important future research avenue. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Luca Biggio, Federica Gerace, and Matteo Vilucchio for insightful discussions. We acknowledge funding from the Swiss National Science Foundation grant SNFS OperaGOST (grant number 200390), and SMArtNet (grant number 212049). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30, 2017.   \n[2]  Samy Jelassi, Michael Sander, and Yuanzhi Li. Vision transformers provably learn spatial structure. Advances in Neural Information Processing Systems, 35:37822-37836, 2022.   \n[3] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.   \n[4]  Aaditya K Singh, Stephanie C.Y. Chan, Ted Moskovitz, Erin Grant, Andrew M Saxe, and Felix Hill. The transient nature of emergent in-context learning in transformers. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[5]  Taylor Webb, Keith J Holyoak, and Hongjing Lu. Emergent analogical reasoning in large language models. Nature Human Behaviour, 7(9):1526-1541, 2023.   \n[6]  Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers. In International Conference on Machine Learning, pages 11080-11090. PMLR, 2021. [7] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma,_ Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfeld-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022.   \n[8] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pages 35151-35174. PMLR, 2023.   \n[9] Ziqian Zhong, Ziming Liu, Max Tegmark, and Jacob Andreas. The clock and the pizza: Two stories in mechanistic explanation of neural networks. Advances in Neural Information Processing Systems, 36, 2024.   \n[10]  Angelica Chen, Ravid Schwartz-Ziv, Kyunghyun Cho, Matthew L Leavitt, and Naomi Saphra. Sudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in mlms. arXiv preprint arXiv:2309.07311, 2023.   \n[11]  Allan Raventos, Mansheej Paul, Feng Chen, and Surya Ganguli. Pretraining task diversity and the emergence of non-bayesian in-context learning for regression. Advances in Neural Information Processing Systems, 36, 2024.   \n[12]  Gautam Reddy. The mechanistic basis of data dependence and abrupt learning in an in-context classification task. In The Twelth International Conference on Learning Representations, 2024.   \n[13] Sanjeev Arora and Anirudh Goyal. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.   \n[14] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models a mirage? Advances in Neural Information Processing Systems, 36, 2024.   \n[15] Ernst Ising. Contribution to the theory of ferromagnetism. Z. Phys, 31(1):253-258, 1925.   \n[16]  Lars Onsager. Crystal statistics. i. a two-dimensional model with an order-disorder transition. Physical Review, 65(3-4):117, 1944.   \n[17]  Haim Sompolinsky, Naftali Tishby, and H Sebastian Seung. Learning from examples in large neural networks. Physical Review Letters, 65(13):1683, 1990.   \n[18]  Geza Gyorgyi. First-order transition to perfect generalization in a neural network with binary synapses. Physical Review A, 41(12):7097, 1990.   \n[19] Jean Barbier, Florent Krzakala, Nicolas Macris, L\u00e9o Miolane, and Lenka Zdeborova. Optimal errors and phase transitions in high-dimensional generalized linear models. Proceedings of the National Academy of Sciences, 116(12):5451-5460, 2019.   \n[20] Benjamin Aubin, Antoine Maillard, Florent Krzakala, Nicolas Macris, Lenka Zdeborova, et al. The committee machine: Computational to statistical gaps in learning a two-layers neural network. Advances in Neural Information Processing Systems, 31, 2018.   \n[21]  Henry Schwarze.  Learning a rule in a multilayer neural network. Journal of Physics A: Mathematical and General, 26(21):5781, 1993.   \n[22]  Antoine Maillard, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborova. Phase retrieval in high dimensions: Statistical and computational phase transitions. Advances in Neural Information Processing Systems, 33:11071-11082, 2020.   \n[23]  Adriano Barra, Giuseppe Genovese, Peter Sollich, and Daniele Tantari. Phase transitions in restricted boltzmann machines with generic priors. Physical Review E, 96(4):042156, 2017.   \n[24]  Riccardo Rende, Federica Gerace, Alessandro Laio, and Sebastian Goldt. Optimal inference of a generalised Potts model by single-layer transformers with factored attention. arXiv preprint arXiv:2304.07235, 2023.   \n[25] Hengyu Fu, Tianyu Guo, Yu Bai, and Song Mei. What can a single attention layer learn? a study through the random features lens. Advances in Neural Information Processing Systems, 36, 2024.   \n[26]  Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. Advances in neural information processing systems, 36, 2024.   \n[27]  Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attentionmechanisms. In International Conference on Machine Leaning, pages 5793-5831. PMLR, 2022.   \n[28]  Michael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156-171, 2020.   \n[29]  Arda Sahiner, Tolga Ergen, Batu Ozturkler, John Pauly, Morteza Mardani, and Mert Pilanci. Unraveling attention via convex duality: Analysis and interpretations of vision transformers. In International Conference on Machine Learning, pages 19050-19088. PMLR, 2022.   \n[30] Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, and Samet Oymak. Max-margin token selection in attention mechanism. Advances in Neural Information Processing Systems, 36:48314-48362, 2023.   \n[31]  Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak. Transformers as support vector machines. arXiv preprint arXiv:2308.16898, 2023.   \n[32] Emmanuel Abbe, Samy Bengio, Enric Boix-Adsera, Etai Littwin, and Joshua Susskind. Transformers learn through gradual rank increase. Advances in Neural Information Processing Systems, 36, 2024.   \n[33] Hongkang Li, Meng Wang, Sijia Liu, and Pin-Yu Chen. A theoretical understanding of shallow vision transformers: Learning, generalization, and sample complexity. arXiv preprint arXiv:2302.06015, 2023.   \n[34] Yuandong Tian, Yiping Wang, Beidi Chen, and Simon S Du. Scan and snap: Understanding training dynamics and token composition in 1-layer ransformer. Advances in Neural Information Processing Systems, 36:71911-71947, 2023.   \n[35] Tianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, and Yu Bai. How do transformers learn in-context beyond simple functions? a case study on learning with representations. arXiv preprint arXiv:2310.10616, 2023.   \n[36]  Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning. In International Conference on Machine Learning, pages 19565-19594. PMLR, 2023.   \n[37] Ruiqi Zhang, Spencer Frei, and Peter L Bartlet. Trained transformers learn linear models in-context. arXiv preprint arXiv:2306.09927, 2023.   \n[38]  Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birth of a transformer: A memory viewpoint. Advances in Neural Information Processing Systems, 36, 2024.   \n[39]  Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, and Philippe Rigollet. A mathematical perspective on transformers. arXiv preprint arXiv:2312.10794, 2023.   \n[40] Jeffrey L. Elman. Finding structure in time. Cognitive Science, 14(2):179-211, 1990.   \n[41]  Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer Levy. Transformer language models without positional encodings still learn positional information. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 1382-1390, December 2022.   \n[42]  Koustuv Sinha, Amirhossein Kazemnejad, Siva Reddy, Joelle Pineau, Dieuwke Hupkes, and Adina Williams. The curious case of absolute position embeddings. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 4449-4472, 2022.   \n[43] Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[44] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-Attention with Relative Position Representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 464-468. Association for Computational Linguistics, 2018.   \n[45]  Anian Ruoss, Gregoire Del\u00e9tang, Tim Genewein, Jordi Grau-Moya, Robert Csordas, Mehdi Bennani, Shane Legg, and Joel Veness. Randomized positional encodings boost length generalization of transformers. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1889-1903, 2023.   \n[46] Francesca Mignacco, Florent Krzakala, Yue Lu, Pierfrancesco Urbani, and Lenka Zdeborova. The role of regularization in classification of high-dimensional noisy gaussian mixture. In International conference on machine learning, pages 6874-6883. PMLR, 2020.   \n[47]  Hugo Cui and Lenka Zdeborova. High-dimensional asymptotics of denoising autoencoders. Advances in Neural Information Processing Systems, 36, 2024.   \n[48]  Bruno Loureiro, Gabriele Sicuro, C\u00e9dric Gerbelot, Alessandro Pacco, Florent Krzakala, and Lenka Zdeborova. Learning gaussian mixtures with generalized linear models: Precise asymptotics in high-dimensions. Advances in Neural Information Processing Systems, 34:10144- 10157, 2021.   \n[49] Yen-Chang Hsu,Ting HuaSungen Chang,Qian Lou,Yilin Shen and HongxiaJinLanguae model compression with weighted low-rank factorization. arXiv preprint arXiv:2207.00112, 2022.   \n[50] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \n[51] Benjamin Aubin, Florent Krzakala, Yue Lu, and Lenka Zdeborova. Generalization error in high-dimensional perceptrons: Approaching bayes error with convex optimization. Advances in Neural Information Processing Systems, 33:12199-12210, 2020.   \n[52] Elisabetta Cornacchia, Francesca Mignacco, Rodrigo Veiga, Cedric Gerbelot, Bruno Loureiro, and Lenka Zdeborova. Learning curves for the multi-class teacher-student perceptron. Machine Learning: Science and Technology, 4(1):015019, 2023.   \n[53]  John Ashworth Nelder and Robert WM Wedderburn. Generalized linear models. Journal of the Royal Statistical Society Series A: Statistics in Society, 135(3):370-384, 1972.   \n[54]  Peter McCullagh. Generalized linear models. Routledge, 2019.   \n[55]  Adel Javanmard and Andrea Montanari. State evolution for general approximate message passing algorithms, with applications to spatial coupling. Information and Inference: A Journal of the IMA, 2(2):115-144, 2013.   \n[56]  Sundeep Rangan, Philip Schniter, Erwin Riegler, Alyson K Fletcher, and Volkan Cevher. Fixed points of generalized approximate message passing with arbitrary matrices. IEEE Transactions on Information Theory, 62(12):7464-7474, 2016.   \n[57]  Mohsen Bayati and Andrea Montanari. The lasso risk for gaussian matrices. IEEE Transactions on Information Theory, 58(4):1997-2017, 2011.   \n[58]  David Donoho and Andrea Montanari. High dimensional robust m-estimation: Asymptotic variance via approximate message passing. Probability Theory and Related Fields, 166:935-969, 2016.   \n[59]  Melikasadat Emami, Mojtaba Sahraee-Ardakan, Parthe Pandit, Sundeep Rangan, and Alyson Fletcher. Generalization error of generalizedlinear models in highdimensions. In International Conference on Machine Learning, pages 2892-2901. PMLR, 2020.   \n[60]  Cedric Gerbelot, Alia Abbara, and Florent Krzakala. Asymptotic errors for teacher-student convex generalized linear models (or: How to prove kabashima's replica formula). IEEE Transactions on Information Theory, 69(3):1824-1852, 2022.   \n[61] Giorgio Parisi. Toward a mean field theory for spin glasses. Physics Letters A, 73(3):203-205, 1979.   \n[62] Stephane d'Ascoli, Levent Sagun, Giulio Biroli, and Joan Bruna. Finding the needle in the haystack with convolutions: on the benefits of architectural bias. In Advances in Neural Information Processing Systems, volume 32, 2019.   \n[63]  Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. IEEE Transactions on Information Theory, 57(2):764-785, 2011.   \n[64] Lenka Zdeborova and Florent Krzakala. Statistical physics of inference: Thresholds and algorithms. Advances in Physics, 65(5):453-552, 2016.   \n[65]  Lucas Clarte, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborova. Theoretical characterization of uncertainty in high-dimensional linear classification. Machine Learning: Science and Technology, 4(2):025029, 2023.   \n[66]  Giorgio Parisi. Order parameter for spin-glasses. Physical Review Letters, 50(24): 1946, 1983.   \n[67]  Erwin Bolthausen. An iterative construction of solutions of the tap equations for the sherringtonkirkpatrick model. Communications in Mathematical Physics, 325(1):333-366, 2014.   \n[68]  Marc Mezard, Giorgio Parisi, and Miguel Angel Virasoro. Spin glass theory and beyond: An Introductionto the Replica Method and Its Applications, volume 9.World Scientifc Publishing Company, 1987.   \n[69]  Yehoram Gordon. On milman's inequality and random subspaces which escape through a mesh in rn. In Geometric Aspects of Functional Analysis: Israel Seminar (GAFA) 1986-87, pages 84-106. Springer, 1988.   \n[70]  Christos Thrampoulidis, Samet Oymak, and Babak Hassibi. The gaussian min-max theorem in the presence of convexity. arXiv preprint arXiv: 1408.4837, 2014. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "[71] Mihailo Stojnic. Meshes that trap random subspaces. arXiv preprint arXiv: 1304.0003, 2013. ", "page_idx": 14}, {"type": "text", "text": "[72]  Mihailo Stojnic. . Upper-bounding 11-optimization  weak thresholds. arXiv preprint arXiv:1303.7289, 2013.   \n[73] Hugo Cui.  High-dimensional learning of narrow neural networks. arXiv preprint arXiv:2409.13904,2024. ", "page_idx": 14}, {"type": "text", "text": "Appendix: Table of Contents ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A  The Histogram Task: An algorithmic toy example with a positional and semantic solution 17 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Phenomenology . 17   \nA.2 Dataset 18   \nA.3 Model 18   \nA.4 Training procedure and freezing model parameters 19   \nA.5 Unfreezing model parameters and SGD convergence 19 ", "page_idx": 15}, {"type": "text", "text": "B  Derivation of Result 4.2 20 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1Notations 20   \nB.2AMP algorithm 21   \nB.3 State evolution 22   \nB.4 Fixed points of GAMP are fixed points of GD 25   \nB.5 Towards a rigorous proof of result 4.2 27 ", "page_idx": 15}, {"type": "text", "text": "CDerivation of Result 4.2 with the replica method 27 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1  Replica-Symmetric ansatz 28   \nC.2 Entropic potential . . 29   \nC.3 Energetic potential: 30   \nC.4 Zero-temperature limit 30   \nC.5 Replica free entropy . . . 30   \nC.6 Saddle-point equations : general regularizer 31   \nC.7 Saddle-point equations : $\\ell_{2}$ . 31   \nC.8 test MSE . 32   \nC.9 Training loss . . . 32   \nC.10 Extensions . 34 ", "page_idx": 15}, {"type": "text", "text": "D Derivation of Result 5.1 36 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "D.1 Examples 37 ", "page_idx": 15}, {"type": "text", "text": "E Supplementary Experiments 37 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "E.1  Empirical scaling of $\\alpha=d/n$ 37   \nE.2 Alternative hyperparameters 37   \nE.3 More Complex Architectural Choices 38   \nE.4  Uninformed initialization and training via Adam . . 40 ", "page_idx": 15}, {"type": "text", "text": "F NeurIPS Paper Checklist 42 ", "text_level": 1, "page_idx": 15}, {"type": "image", "img_path": "BFWdIPPLgZ/tmp/6cf6af482c22d0b4c821565d889c34660e650dab0791f36a55f05e6e26a3ec75.jpg", "img_caption": ["Figure 4: Several solutions exist for the histogram task. Elements of attention matrices for the histogram task for local minima in the empirical loss landscape. We generated a dataset of sequences by sampling each token of the sequence i.i.d. from the uniform distribution over all tokens. The target of a a given input sequence $\\mathbf{x}=[A,D,D,C]$ is the number of occurence of each token in the complete sequence, i.e. $\\mathbf{y}=[1,2,2,1]$ . Models were trained with their respective frozen initialization using $n=35$ , 000 samples and the Adam optimizer. Top Row: The attention matrix of the positional solution is largely independent of the specific input sequence. Bottom Row: The attention matrices from the semantic solution vary based on the input token. Red squares highlight the elements of $A_{i j}$ where Ci = Cj\u00b7 "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "A  The Histogram Task: An algorithmic toy example with a positional and semantic solution ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.1 Phenomenology ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this appendix, we demonstrate that for a simple counting task two qualitatively different solutions exist in the loss landscape of a simple transformer using a dot-product attention layer with positional encodings - a practical example that further motivates our theoretical investigations in the main text. One solution corresponds to a dot-product attention matrix which is largely independent of the tokens making up the input sequence, and another strongly varies based on the tokens (and thus the semantic content of the input). Both solutions achieve a test accuracy close to $100\\%$ ", "page_idx": 16}, {"type": "text", "text": "The training task is a sequence-to-sequence counting task, referred to as the histogram task in [6]. Given an input sequence $\\pmb{x}=[x_{1},x_{2},\\cdot\\cdot\\cdot\\;,x_{L}]$ of length $L$ of tokens from a fixed alphabet, the goal is to return a sequence $\\pmb{y}=[y_{1},y_{2},\\cdot\\cdot\\cdot\\;,y_{L}]$ , where each token $y_{i}$ is the number of occurrences of the token $x_{i}$ in $\\textbf{\\em x}$ . In Fig. 1, we show an example where we consider sequences where the tokens are from the fixed alphabet $\\dot{\\boldsymbol{\\mathcal{X}}}=\\{\\boldsymbol{A},\\boldsymbol{B},\\boldsymbol{C},\\cdot\\cdot\\cdot\\}$ of size $|\\mathcal{X}|=15$ . In this setting, for instance, the sequence $\\pmb{x}=[A,B,\\bar{B},C,A,B]$ should be mapped to its histogram sequence $\\pmb{{y}}^{\\prime}\\pmb{{\\eta}}=[2,3,3,1,2,3]$ . When the input data is limited to length $L$ , the output elements $y_{i}$ thus take values up to the maximum count $L$ ", "page_idx": 16}, {"type": "text", "text": "We encode the input using token embeddings and absolute positional encodings which are trained jointly with the model weights. As an architecture we consider a small transformer made up of a single layer of dot-product attention, followed by a fully connected hidden layer and with learned embeddings for both tokens and positions. For each output position, it generates logits for the $L$ possible classes of the output alphabet; training is done using the cross-entropy loss. (Further details are provided in the remainder of this appendix). ", "page_idx": 16}, {"type": "text", "text": "We conduct experiments where we set two different sections of the model's weights to zero at the initialization of training -removing either the model's access to positional or semantic information-- and keeping the weights frozen throughout training with the Adam optimizer. After convergence, we check that the resulting configurations of weights are stable in the unconstrained loss landscape, i.e. ", "page_idx": 16}, {"type": "text", "text": "without frozen weights. More precisely, we ascertain that these weights only change marginally when further trained with SGD on the unconstrained loss, and that the qualitative behaviour of the attention layer is retained. Our experiments demonstrate that the loss landscape of the transformer has at least two qualitatively different local minimizers (or close to minimizers), subsequently referred to as the semantic and positional solution. ", "page_idx": 17}, {"type": "text", "text": "We inspect the learnt attention matrix for different input sequences in Fig. 4. The positional solution corresponds to a learnt attention matrix whose $i,j-$ th component only depend on the positions $i,j$ and little on the tokens occupying these positions. The attention matrix is thus almost independent of the input sequence. In fact, the attention matrix is similar to the identity. In this case, the attention layer simply serves to aggregate the other tokens uniformly, and the fully connected layer learns the counting. ", "page_idx": 17}, {"type": "text", "text": "In contrast, the attention matrix learnt at the semantic solution displays larger $i,j$ -thcomponent if the tokens at position $i$ and $j$ are identical. In other words, identical tokens attend more to each other. This mechanism hence does not rely on the positions, but rather on the semantic content of the tokens. Both solutions and associated attention matrices thus correspond to feasible algorithms which ultimately allow the transformer to solve the downstream task. ", "page_idx": 17}, {"type": "text", "text": "Our experimental exploration gives compelling evidence that different stable solutions exist in the empirical loss landscape of simple transformers, which correspond to different algorithmic solutions to a given task. However, it remains an interpretation of an experiment and does not allow for a precise characterization of their behaviour or of the conditions under which they are established. This stands in contrast to the example in the main paper: the model treated there is simpler, but still presents similar phenomenology and can be analyzed theoretically. ", "page_idx": 17}, {"type": "text", "text": "In the remainder of this Appendix we discuss the implementation details such as architecture and training procedure which enabled us to exhibit the different minima for the histogram task. ", "page_idx": 17}, {"type": "text", "text": "The code for reproducing the results is available at github.com/SPOC-group/positional-and-semanticattention. ", "page_idx": 17}, {"type": "text", "text": "A.2 Dataset ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We use the histogram task as proposed in [6]. We consider sequences of fixed length $L=15$ . For every input sequence $\\mathbf{x}=\\left[x_{1},x_{2},\\cdot\\cdot\\cdot\\,,x_{L}\\right]$ we sample $x_{i}$ i.i.d. and uniformly from a set of tokens $\\tau$ of size $T$ , which we set to 15 in our experiments. For visualization purposes we use capitalized letters as tokens. To obtain the target $\\mathbf{y}=[y_{1},y_{2},\\cdots\\,,y_{L}]$ , we set $\\begin{array}{r}{y_{i}=\\sum_{j=1}^{L}\\mathbb{1}(x_{i}=x_{j})}\\end{array}$ ,where $\\mathbb{1}(b)$ is 1 if the boolean statement $b$ is true and zero otherwise. Therefore, $y_{i}\\in\\{1,2,\\cdots,L\\}$ ", "page_idx": 17}, {"type": "text", "text": "A.3 Model ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In order to read the input sequence using a transformer we learn an embedding of dimension $d$ for each of the $T$ tokens and $L$ positions, stored in the matrices $E^{\\mathrm{token}}\\,=\\,[t_{A},\\boldsymbol{t_{B}},\\cdot\\cdot\\cdot]\\,\\in\\,\\mathbb{R}^{T\\times d}$ and $E^{\\mathrm{pos}}=[p_{1},p_{2},\\cdot\\cdot\\cdot]\\in\\mathbb{R}^{L\\times d}$ We convert the input sequence $\\mathbf{x}=[x_{1},x_{2},\\cdot\\cdot\\cdot\\,,x_{L}]\\in{\\mathcal{T}}^{L}$ into an embedded sequence $\\tilde{\\mathbf{x}}\\in\\mathbb{R}^{L\\times d}$ of the same length where $\\tilde{x_{i}}=t_{x_{i}}+p_{i}$ . This input sequence is then fed into the first layer of the transformer. ", "page_idx": 17}, {"type": "text", "text": "Formally, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log\\mathrm{its}_{i}(\\mathbf{x})=W_{2}R e L U(W_{1}L a y e r N o r m(A t t e n t i o n_{i}(\\Tilde{\\mathbf{x}}))+b_{1})+b_{2}\\in\\mathbb{R}^{\\mathrm{c}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with $W_{1}\\in\\mathbb{R}^{d\\times h},b_{1}\\in\\mathbb{R}^{h},W_{2}\\in\\mathbb{R}^{h\\times c},b_{2}\\in\\mathbb{R}^{c}$ The final prediction is obtained using the argmax on the logits. We have that the score matrix $A$ and the dot-product attention mechanism is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{A t t e n t i o n(\\mathbf{x})=A(\\mathbf{x})V\\mathbf{x}}\\\\ {\\qquad A(\\mathbf{x})=\\mathrm{Softmax}\\Bigl(\\frac{\\mathbf{x}Q K^{T}\\mathbf{x}^{T}}{\\sqrt{d}}\\Bigr)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $Q,K,V\\in\\mathbb{R}^{d\\times d}$ and the softmax is applied row-wise. Also, for an $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ we define ", "page_idx": 17}, {"type": "equation", "text": "$$\nL a y e r N o r m(x)=\\frac{x-\\mathbb{E}[x]}{\\sqrt{V a r(x)+\\varepsilon}}*\\gamma+\\beta\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\gamma,\\beta\\in\\mathbb{R}^{d}$ We define the empirical loss for a dataset $\\textstyle D=\\{\\mathbf{x}^{\\mu},\\mathbf{y}^{\\mu}\\}_{\\mu=1}^{n}$ as the average cross entropy loss for all output tokens $C$ ,i.e. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mathcal{D})=\\sum_{\\mu=1}^{n}\\sum_{l=1}^{L}-\\sum_{c=1}^{C}[y_{i}^{\\mu}]_{c}\\log[\\mathrm{Softmax}(\\log\\mathrm{its}_{i}(\\mathbf{x}^{\\mu}))]_{c}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Because of the way in which the histogram dataset is created for a fixed input length $L$ ,it follows $C=L$ ", "page_idx": 18}, {"type": "text", "text": "A.4  Training procedure and freezing model parameters ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To obtain the positional and semantic minima, we set some weights to zero in $E^{\\mathrm{token}},E^{\\mathrm{pos}},Q,K$ at initialization, and also freeze these zero weights during training. Note that this procedure was used in the literature to study architectural biases of varying model architectures, e.g. by [62] for convolutionalneuralnetworks. ", "page_idx": 18}, {"type": "text", "text": "With $\\cdot$ we denote the initialization that is taken as the default Pytorch initialization for a linear layer. For both semantic and positional initialization, we overwrite this initialization with zeros as follows With $i=1,\\cdots,T$ and $j=1,\\cdot\\cdot\\cdot,L$ ", "page_idx": 18}, {"type": "equation", "text": "$$\nt_{i}=\\left(\\begin{array}{l}{{\\cdot_{d/2}}}\\end{array}\\right|\\begin{array}{l}{{{\\bf0}_{d/2}}}\\end{array}\\right)\\,,\\,\\,p_{j}=\\left(\\begin{array}{l}{{{\\bf0}_{d/2}}\\end{array}\\right|\\,{\\bf\\dot{\\sigma}}_{d/2}}\\end{array}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathbf{0}_{d/2}$ is the all-zero vector of size $d/2$ . For the positional initialization we additionally set ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q=\\left(\\frac{\\cdot d/2\\times d/2}{\\mathbf{0}_{d/2\\times d/2}}\\mid\\mathbf{0}_{d/2\\times d/2}\\right)\\,,\\;K=\\left(\\frac{\\cdot d/2\\times d/2}{\\mathbf{0}_{d/2\\times d/2}}\\mid\\mathbf{0}_{d/2\\times d/2}\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and for the semantic initialization we additionally set ", "page_idx": 18}, {"type": "equation", "text": "$$\nQ=\\left(\\frac{\\mathbf{0}_{d/2\\times d/2}\\mid\\mathbf{0}_{d/2\\times d/2}}{\\mathbf{0}_{d/2\\times d/2}\\mid\\mathbf{\\epsilon}\\cdot d/2\\times d/2}\\right)\\,,\\;K=\\left(\\frac{\\mathbf{0}_{d/2\\times d/2}\\mid\\mathbf{0}_{d/2\\times d/2}}{\\mathbf{0}_{d/2\\times d/2}\\mid\\mathbf{\\epsilon}\\cdot d/2\\times d/2}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathbf{0}_{d/2\\times d/2}$ is the all-zero matrix of size $d/2$ by $d/2$ . In either case, the attention mechanism only has access to the semantic or positional part of the model. ", "page_idx": 18}, {"type": "text", "text": "Training all weights except the ones frozen to zero using Adam on a dataset of size $n=35,000$ we obtain a $>99.8\\%$ test accuracy on a test set of size $n=15,000$ for both datasets. We call these parameter configurations $\\theta_{p o s}$ and $\\theta_{s e m}$ respectively (the local minima as referred to in the main text). ", "page_idx": 18}, {"type": "text", "text": "In Fig. 4, we show that the attention layer of the two models behaves in qualitatively different ways, by showing the activations of the matrix $A$ for different input sequences. ", "page_idx": 18}, {"type": "text", "text": "A.5  Unfreezing model parameters and SGD convergence ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Finally, we want to verify that the parameter configurations $\\theta_{p o s}$ and $\\theta_{s e m}$ we obtained using the special initialization and frozen training are stable configurations in the unconstrained parameter space of the model. To show this, we perturb the given parameter configuration slightly using additive Gaussian i.i.d. noise with a scale of O.001. Subsequently, we run SGD on this perturbed configuration in the unconstrained parameter space for another 1o0 epochs. We call the parameter configurations obtained afte this step $\\tilde{\\theta}_{p o s}$ and $\\bar{\\tilde{\\theta}}_{s e m}$ ", "page_idx": 18}, {"type": "text", "text": "While this further training leads the training loss to further decrease and the test accuracy to slightly increase, each models qualitative behaviour before and after extra training remains unchanged, as shown in Fig. 5 and 6, for the examples also used in Fig. 1. The absolute difference in norm between $\\tilde{\\theta}_{p o s}$ and $\\theta_{p o s}$ (respectively $\\tilde{\\theta}_{s e m}$ and $\\theta_{s e m.}$ of parameters is also small (see Table 1). ", "page_idx": 18}, {"type": "text", "text": "This evidences, that both resulting parameter configurations are in flat regions of the full parameterization of the transformer model, and that these two regions are further qualitatively different in the same sense that the original ones were. We use this stability in the unconstrained loss landscape as a justification to refer to them as \"local minima\". ", "page_idx": 18}, {"type": "table", "img_path": "BFWdIPPLgZ/tmp/41318e7e85c36d13810b5fee7ff49398cbc76532e74f850e8e75d816a99181bd.jpg", "table_caption": ["Table 1: Parameter configurations for different types of initializations for the histogram task. Optimizer parameters as in the accompanying code. Average over 10 runs using the same dataset. "], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "BFWdIPPLgZ/tmp/94f000ea48bb86d068b5edc9c443f306612aad16324e2257f09e0ce3b72b574b.jpg", "img_caption": ["Figure 5: Comparison of the attention layer activations for different sequences for $\\theta_{p o s}$ and $\\tilde{\\theta}_{p o s}$ "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "B Derivation of Result 4.2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this Appendix, we provide a detailed derivation of Result 4.2 of the main text. In subsection B.2. we introduce a Generalized Approximate Message Passing algorithm (GAMP) [56]. Subsection B.3 then establishes that equations (7) of Result 4.2 track the dynamics of summary statistics describing the GAMP algorithm. In particular, the equations (7) describe the fixed points of GAMP. Finally, subsection B.4 shows that fixed points of GAMP correspond to critical (zero-gradient) points of the empirical loss landscape (3), thus establishing that equations 7 of Result 4.2 describe fixed points of GD. ", "page_idx": 19}, {"type": "text", "text": "B.1  Notations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For simplicity, we place ourselves in the setting $r_{s}=1$ explored in Section 5 of the main text, but allow the length $L$ of the sentences to be arbitrary, and allow a generic learning model S (2), i.e. not necessarily the dot-product attention model analyzed in Section 5. The case $r_{s}\\,\\geq\\,2$ follows identical derivation steps, modulo the replacement of all variables by tensor objects. We provide another alternative derivation of Result 4.2 in full generality in Appendix C, using the replica method from statistical physics. Let us note $\\{X_{\\ell}\\}_{1\\le\\ell\\le L}$ a series of $L\\;n\\times d$ matrices, with $X_{\\ell}$ corresponding to the $\\ell-$ th rows (tokens) of each input sentence $x^{\\mu}$ stacked vertically, and normalized by $\\sqrt{d}$ We denote $\\tilde{X}_{\\ell}\\equiv X_{\\ell}+P_{\\ell}$ , where $P\\in\\mathbb{R}^{n\\times d}$ is the matrix with all rows equal to the $\\ell-$ th positional encoding $p_{\\ell}$ . Let us further define $\\rho\\,\\in\\,\\mathbb{R}^{n\\times L\\times L}$ the tensor corresponding to the sequence of $n$ matrices $\\textstyle{\\frac{1}{d}}{\\dot{x}}^{\\mu}(x^{\\mu})^{\\top}\\in\\mathbb{R}^{L\\times L}$ . Finally, let us denote $T\\in\\mathbb{R}^{n\\times L\\times L}$ the tensor so that the $\\mu$ -th row of $T$ satisfies $y(x^{\\mu})=T^{\\mu}x^{\\mu}$ , see equation (1). In other words, $T$ corresponds to the concatenation of the target attention matrices. ", "page_idx": 19}, {"type": "image", "img_path": "BFWdIPPLgZ/tmp/624da97e69c9970e95bf541d7ab714f6ccdce43de461fb5e9ab008d36dec9c99.jpg", "img_caption": ["Figure 6: Comparison of the attention layer activations for different sequences for $\\theta_{s e m}$ and $\\tilde{\\theta}_{s e m}$ "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Before detailing the derivation, we first highlight a simplifying observation. Note that a loss item can be expanded as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{\\bar{d}}\\left\\|y(x^{\\mu})-\\mathbb{S}\\left[\\frac{1}{\\sqrt{d}}(x^{\\mu}+p)Q\\right]x^{\\mu}\\right\\|^{2}=\\|y(x^{\\mu})\\|^{2}\\!+\\!\\operatorname{Tr}\\mathbb{S}\\left[\\frac{1}{\\sqrt{d}}(x^{\\mu}+p)Q\\right]\\rho_{\\Sigma}\\mathbb{S}\\left[\\frac{1}{\\sqrt{d}}(x^{\\mu}+p)Q\\right]^{\\top}}\\\\ {-\\,2\\operatorname{Tr}\\mathbb{T}\\left[\\frac{1}{\\sqrt{d}}x_{\\ell}Q_{\\star}\\right]\\rho_{\\Sigma}\\mathbb{S}\\left[\\frac{1}{\\sqrt{d}}(x_{\\ell}^{\\mu}+p_{\\ell})Q\\right]^{\\top},\\qquad(27).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we used that with high probability in the considered asymptotic limit, for all $1\\leq\\mu\\leq n$ ", "page_idx": 20}, {"type": "equation", "text": "$$\nx x^{\\top}=(x+p)(x+p)^{\\top}=x(x+p)^{\\top}=\\rho_{\\Sigma}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since the first term of (27) does not depend on the weights $Q$ , it can be without loss of generality substracted from the loss. Without loss of generality, one can thus consider the equivalent empirical risk minimization problem ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{\\mathsf{J}}=\\underset{Q\\in\\mathbb{R}^{d\\times r}}{\\operatorname{argmin}}\\sum_{\\mu=1}^{n}\\frac{1}{2d}\\left[\\mathrm{Tr}\\,\\mathsf{s}\\!\\left[\\frac{1}{\\sqrt{d}}(x^{\\mu}\\!+\\!p)Q\\right]\\rho_{\\Sigma}\\mathsf{s}\\!\\left[\\frac{1}{\\sqrt{d}}(x^{\\mu}\\!+\\!p)Q\\right]^{\\top}\\!-2\\,\\mathrm{Tr}\\,\\mathsf{T}\\!\\left[\\frac{1}{\\sqrt{d}}x_{\\ell}Q_{\\star}\\right]\\rho_{\\Sigma}\\mathsf{s}\\!\\left[\\frac{1}{\\sqrt{d}}(x_{\\ell}^{\\mu}\\!+\\!p_{\\ell})Q\\right]^{\\top}\\right]+\\frac{\\lambda}{2}\\|Q\\|^{2}\\,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The risks (29) and (3) are equivalent, and we shall use the former in the following. ", "page_idx": 20}, {"type": "text", "text": "Finally, for arguments $T\\in\\mathbb{R}^{L\\times L},\\rho\\in\\mathbb{R}^{L\\times L},\\omega\\in\\mathbb{R}^{L},V\\in\\mathbb{R}^{L\\times L}$ we introduce the resolvent ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{arginf}_{x=\\{x_{\\ell}\\in\\mathbb{R}\\}_{\\ell=1}^{L}}\\left\\{\\sum_{\\ell,\\kappa=1}^{L}(x_{\\ell}-\\omega_{\\ell})(V^{-1})_{\\ell\\kappa}(x_{\\kappa}-\\omega_{\\kappa})-2\\,\\mathrm{Tr}\\big[\\mathtt{S}[x]\\rho T^{\\top}\\big]+\\mathrm{Tr}\\big[\\mathtt{S}[x]\\rho\\mathtt{S}[x]^{\\top}\\big]\\right\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that the latter part of the bracketed term corresponds to the simplified loss (27) derived in the beginning of Appendix C, which is the one we shall without loss of generality consider in the present appendix. For ease of presentation, we place ourselves under Assumption 4.1, where all the input covariances $\\{\\Sigma_{\\ell}\\}_{\\ell}$ are codiagonalizable. In the following, without loss of generality, we thus assume them diagonal, by placing ourselves in the common basis $\\{e_{i}\\}_{1\\leq i\\leq d}$ of Assumption 4.1. ", "page_idx": 20}, {"type": "text", "text": "B.2  AMP algorithm ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We are now in a position to state the AMP algorithm: ", "page_idx": 20}, {"type": "text", "text": "Inputs $\\colon\\{\\tilde{X}_{\\ell}\\in\\mathbb{R}^{n\\times d}\\}_{\\ell=1}^{L},T\\in\\mathbb{R}^{n\\times L\\times L},\\rho\\in\\mathbb{R}^{n\\times L\\times L}$   \nInitialive $\\hat{Q}^{0}=\\sim{\\mathcal{N}}(0,\\mathbb{I}_{d}),\\hat{c}^{0}=\\mathbb{I}_{d},\\{f_{\\ell}^{0}=0_{n}\\}_{\\ell=1}^{L}$   \nfor t \u2264 tmax do $\\begin{array}{r l}&{\\forall1\\leq\\ell,\\kappa\\leq L,\\ V_{\\ell}^{t}=(\\tilde{X}_{\\ell}\\odot\\tilde{X}_{\\kappa})\\hat{c}^{t}}\\\\ &{\\forall1\\leq\\ell\\leq L,\\ \\omega_{\\ell}^{t}=\\tilde{X}_{\\ell}\\hat{Q}^{t}-\\sum_{i=1}^{L}V_{\\ell\\kappa}^{t}f_{\\kappa}^{t-1}}\\\\ &{\\forall1\\leq\\ell\\leq L,\\ \\hat{f}_{\\ell}^{t}=\\sum(V^{-1})_{\\ell\\kappa}^{t}(\\mathrm{prox}(T,\\rho,\\omega^{t},V^{t})_{\\kappa}-\\omega_{\\kappa}^{t})}\\\\ &{\\forall1\\leq\\ell,\\kappa\\leq L,\\ g_{\\ell\\kappa}^{t}=\\partial_{\\omega_{\\ell}}f_{\\kappa}^{t}}\\\\ &{A^{t}=-\\displaystyle\\sum_{\\ell,\\kappa=1}^{L}(\\tilde{X}_{\\ell}\\odot\\tilde{X}_{\\kappa})^{\\top}\\mathfrak{g}_{\\ell\\kappa}^{t}}\\\\ &{b^{t}=\\displaystyle\\sum_{\\ell=1}^{L}\\tilde{X}_{\\ell}^{\\top}f_{\\ell}^{t}+A^{t}\\odot\\hat{Q}^{t}}\\\\ &{\\hat{Q}^{t+1}=(\\lambda\\mathbb{I}_{d}+A^{t})^{-1}b^{t}}\\\\ &{\\hat{c}_{i}^{t+1}=(\\lambda\\mathbb{I}_{d}+A^{t})^{-1}b}\\end{array}$   \nend for ", "page_idx": 21}, {"type": "text", "text": "return Estimator $\\hat{Q}$ ", "page_idx": 21}, {"type": "text", "text": "The GAMP algorithm can be derived in standard fashion from the Belief Propagation (BP) algorithm, see e.g. [56, 63] or [64] for an overview. Compared to the standard GAMP iterations for Generalized linear models, one needs to account for the fact that there exist different sources of data $X_{\\ell}$ (corresponding to the $\\ell-$ th tokens of each input sentence), and for the fact that the output of the equivalentGLMare $\\mathbb{R}^{L\\times L}$ valued atention matrices. In the following subsection, we show that the fixed points of GAMP 1 correspond to critical points of the empirical loss (3), i.e. fixed points of Gradient Descent (GD), allowing to connect Result 4.2 to our numerical experiments using GD. ", "page_idx": 21}, {"type": "text", "text": "B.3 State evolution ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section we show that the dynamics of the GAMP Algorithm 1 are tracked by the summary statistics of Result 4.2. In particular, the equations (7) describe the statistics of the GAMP fixed points. To see this, it is convenient to take as a starting point the relaxed Belief Propagation (rBP) equations, which are a step upstream in the derivation of the GAMP iterations, and which are asymptotically equivalent- see e.g. [64] for a review or e.g. [65], Appendix A, for a detailed walkthrough. The rBP equationsread ", "page_idx": 21}, {"type": "text", "text": "As conventional, we note $\\cdot_{\\mu}$ the version of a variable $\\cdot_{\\mu\\to i}$ where the summation also encompasses the index $i$ and $^{i}$ the version of a variable ${\\cdot}_{i\\to\\mu}$ where the summation also encompasses the index $\\mu$ Note that in all cases above the two variables differ by at most $\\Theta_{d}\\big(1/\\sqrt{d}\\big)$ ", "page_idx": 21}, {"type": "text", "text": "Concentration of $(V_{\\mu\\to i}^{t})_{\\ell\\kappa}$ We first study the statistics of $V_{\\mu\\to i}^{t}$ \uff0c $A_{i\\to\\mu}^{t}$ , remembering that the data $\\tilde{x}_{\\ell i}^{\\mu}\\equiv(x_{\\ell}^{\\mu})_{i}/\\sqrt{d}+(p_{\\ell})_{i}/\\sqrt{d}$ in the notation of the main text, with $(x_{\\ell}^{\\mu})_{i}=\\Theta_{d}(1)$ \uff0c $(p_{\\ell})_{i}=\\Theta_{d}({}^{1}\\!/\\!\\sqrt{d})$ Replacing in the rBP updates: ", "page_idx": 21}, {"type": "equation", "text": "$$\nV_{\\mu\\to i}^{t})_{\\ell\\kappa}=\\sum_{j\\neq i}(\\tilde{x}_{\\ell j}^{\\mu})(\\tilde{x}_{\\kappa j}^{\\mu})\\hat{c}_{j\\rightarrow\\mu}^{t}}\\\\ {=\\underbrace{\\frac{1}{d}\\sum_{j\\neq i}(x_{\\ell}^{\\mu})_{j}(x_{\\kappa}^{\\mu})_{j}\\hat{c}_{j\\rightarrow\\mu}^{t}}_{\\delta_{\\ell\\kappa}\\Theta_{d}(1)+(1-\\delta_{\\ell\\kappa})\\Theta_{d}(1/\\sqrt{d})}+\\underbrace{\\frac{1}{d}\\sum_{j\\neq i}(x_{\\ell}^{\\mu})_{j}(p_{\\kappa})_{j}\\hat{c}_{j\\rightarrow\\mu}^{t}+(\\ell\\leftrightarrow\\kappa)}_{\\Theta_{d}(1/{\\mu})}+\\underbrace{\\frac{1}{d}\\sum_{j\\neq i}(p_{\\ell})_{j}(p_{\\kappa})_{j}\\hat{c}_{j\\rightarrow\\mu}^{t}}_{\\Theta_{d}(1/{\\mu})}}\\\\ {=\\delta_{\\ell\\kappa}\\frac{1}{d}\\sum_{j}(\\Sigma_{\\ell})_{j j}\\hat{c}_{j}^{t}\\equiv V_{\\ell}^{t}}&{{(31)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Inputs Initiali $\\begin{array}{r l}&{:\\{\\tilde{X}_{\\ell}\\in\\mathbb{R}^{n\\times d}\\}_{\\ell=1}^{L},T\\in\\mathbb{R}^{n\\times L\\times L},\\rho\\in\\mathbb{R}^{n\\times L\\times L}}\\\\ &{\\mathsf{\\Pi}_{\\mathsf{c}}\\mathsf{e}\\mathbb{1}\\leq\\mu\\leq n,\\ 1\\leq i\\leq d,\\ \\hat{Q}_{i\\rightarrow\\mu}^{0}=0,\\hat{c}_{i\\rightarrow\\mu}^{0}=1,\\{f_{\\ell\\mu\\rightarrow i}^{0}=0\\}_{\\ell=1}^{L}}\\end{array}$   \nfor t \u2264 tmax do   \n$\\begin{array}{r l}&{\\forall1\\leq\\ell,\\kappa\\leq L,1\\leq\\mu\\leq n,1\\leq i\\leq d,\\ (V_{\\mu\\leftarrow i}^{\\kappa})\\varepsilon_{\\kappa}=\\sum_{j\\neq\\delta}^{\\infty}(\\tilde{x}_{\\varepsilon,j}^{\\kappa})(\\tilde{x}_{\\varepsilon,j}^{\\kappa})\\tilde{c}_{j\\rightarrow\\mu}^{\\kappa}}\\\\ &{\\forall1\\leq\\ell,1\\leq\\mu\\leq n,1\\leq i\\leq d,\\ \\omega_{\\varepsilon,\\mu\\rightarrow i}^{\\ell}=\\sum_{j\\neq\\delta}^{\\ell}\\tilde{c}_{j,\\nu}^{\\mu}\\tilde{c}_{j}}\\\\ &{\\forall1\\leq\\ell,1\\leq\\mu\\leq n,1\\leq i\\leq d,\\ \\int_{\\ell,\\mu\\rightarrow i}^{t}=\\sum_{\\kappa}^{\\infty}(V_{\\mu\\rightarrow i}^{\\kappa-1})\\omega_{\\varepsilon}(\\mathrm{Prox}(T_{\\mu},\\rho_{\\mu\\nu},\\omega_{\\mu\\rightarrow i}^{t},V_{\\mu\\rightarrow i}^{t})\\kappa-\\omega_{\\kappa,\\mu\\rightarrow i}^{t})}\\\\ &{\\forall1\\leq\\ell,\\kappa\\leq L,1\\leq\\mu\\leq n,1\\leq i\\leq d,\\ \\underbrace{p_{\\ell,\\nu\\rightarrow i-\\delta}^{t}}_{\\ell}=\\sum_{\\sigma}^{\\infty}(\\lambda_{\\sigma}\\rho_{\\kappa\\mu\\rightarrow i}^{t}-i)}\\\\ &{\\forall1\\leq\\mu\\leq n,1\\leq i\\leq d,A_{\\varepsilon\\rightarrow\\mu}^{t}=-\\sum_{\\rho}^{L}\\sum_{\\>\\rho}^{\\infty}(\\tilde{x}_{\\varepsilon}^{\\nu})(\\tilde{x}_{\\varepsilon}^{\\nu})\\tilde{g}_{\\ell\\rightarrow\\nu\\rightarrow i}^{t}}\\\\ &{\\forall1\\leq\\mu\\leq n,1\\leq i\\leq d,\\ \\nu_{\\varepsilon\\rightarrow\\mu}^{t}=\\sum_{\\varepsilon=1}^{\\sum}\\sum_{\\rho}^{x}x_{\\varepsilon}^{\\mu}f_{\\varepsilon,\\nu\\rightarrow i}^{t}}\\\\ &{\\forall1\\leq\\mu\\leq n,1\\leq i\\leq d,\\ \\tilde{\\mathcal{G}}_{i+1\\mu}^{t+1}=(\\lambda\\mathbb{I}_{d}+A_{i-\\mu}^{t})^{-1}b_{i\\rightarrow\\mu}^{t}}\\\\ &{\\forall1\\leq\\mu\\leq n,1\\leq i\\leq d,\\ \\tilde{\\mathcal{G}}_{i+1\\mu}^{t+1}=(\\lambda\\mathbb{I}_{d}+A_{i-\\mu}^{t})^{-1}}\\end{array}$   \nend for ", "page_idx": 22}, {"type": "text", "text": "return Estimator $\\hat{Q}$ ", "page_idx": 22}, {"type": "text", "text": "Distribution of w.\u03bc\u2192i Let us first introduce the teacher local field ", "page_idx": 22}, {"type": "equation", "text": "$$\nh_{\\mu,\\ell}=\\sum_{i}(x_{\\ell}^{\\mu})_{i}Q_{i}^{\\star}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Like e.g. [65], Apendix A, we frst ascertan th joint distribtion f $h_{\\mu,\\ell},\\omega_{\\ell,\\mu\\to i}^{t}$ with respect to the data. These variables have mean ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\omega_{\\ell,\\mu\\rightarrow i}^{t}]=\\frac{p_{\\ell}^{\\top}\\hat{Q}^{t}}{\\sqrt{d}}\\equiv m_{\\ell}^{t}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and respective variance ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathbb{V}[\\omega_{\\ell,\\mu\\rightarrow i}^{t}\\omega_{\\kappa,\\nu\\rightarrow j}^{t}]=\\delta_{\\mu\\nu}\\delta_{\\ell\\kappa}\\frac{1}{d}\\sum_{i,j}\\hat{Q}_{i}^{t}(\\Sigma_{\\ell})_{i j}\\hat{Q}_{j}^{t}\\equiv\\delta_{\\mu\\nu}\\delta_{\\ell\\kappa}q_{\\ell}^{t}}}\\\\ {{\\displaystyle\\mathbb{E}[h_{\\mu\\ell}h_{\\nu\\kappa}]=\\delta_{\\mu\\nu}\\delta_{\\ell\\kappa}\\frac{1}{d}\\sum_{i,j}Q_{i}^{\\star}(\\Sigma_{\\ell})_{i j}Q_{j}^{\\star}\\equiv\\delta_{\\mu\\nu}\\delta_{\\ell\\kappa}\\rho_{\\ell}}}\\\\ {{\\displaystyle\\mathbb{E}[h_{\\mu\\ell}\\omega_{\\kappa,\\nu\\rightarrow j}^{t}]=\\delta_{\\mu\\nu}\\delta_{\\ell\\kappa}\\frac{1}{d}\\sum_{i,j}Q_{i}^{\\star}(\\Sigma_{\\ell})_{i j}\\hat{Q}_{j}^{t}\\equiv\\delta_{\\mu\\nu}\\delta_{\\ell\\kappa}\\theta_{\\ell}^{t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Distribution of $b_{i\\to\\mu}^{t}$ Let us ascertain the distribution of $b_{i\\to\\mu}^{t}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{b_{i\\rightarrow\\mu}^{t}=\\displaystyle\\sum_{\\ell}\\sum_{\\nu\\neq\\mu}(\\tilde{x}_{\\ell}^{\\nu})_{i}\\underbrace{(V_{\\ell}^{t})^{-1}\\left(\\mathrm{prox}(T_{\\nu},\\rho_{\\nu},\\{\\omega_{\\kappa,\\nu\\rightarrow i}^{t}\\}_{\\kappa},V_{\\nu\\rightarrow i}^{t})_{\\ell}-\\omega_{\\ell,\\nu\\rightarrow i}^{t}\\right)}_{\\equiv\\mathrm{prox}(T_{\\nu},\\rho_{\\nu},\\{\\omega_{\\kappa,\\nu\\rightarrow i}^{t}\\}_{\\kappa},V_{\\nu\\rightarrow i}^{t})_{\\ell}}}\\\\ &{\\qquad=\\displaystyle\\sum_{\\ell}\\sum_{\\nu\\neq\\mu}1/\\sqrt{d}((x_{\\ell}^{\\nu})_{i}+(p_{\\ell})_{i})\\left[\\mathrm{prox}(\\mathbb{T}[\\{h_{\\nu\\rightarrow i,\\kappa}\\}_{\\kappa}],\\rho_{\\nu},\\{\\omega_{\\kappa,\\nu\\rightarrow i}^{t}\\}_{\\kappa},V_{\\nu\\rightarrow i}^{t})_{\\ell}\\right.}\\\\ &{\\qquad\\qquad\\left.+\\displaystyle1/\\sqrt{d}\\sum_{\\gamma}(x_{\\gamma}^{\\nu})_{i}Q_{i}^{\\star}\\partial_{h_{\\gamma}}\\mathrm{prox}(\\mathbb{T}[\\{h_{\\nu\\rightarrow i,\\kappa}\\}_{\\kappa}],\\rho_{\\nu},\\{\\omega_{\\kappa,\\nu\\rightarrow i}^{t}\\}_{\\kappa},V_{\\nu\\rightarrow i}^{t})_{\\ell}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "leading asymptotically to ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[b_{i\\rightarrow\\mu}^{t}]=\\!\\sum_{\\ell}(\\sqrt{d}p_{\\ell})_{i}\\underbrace{\\alpha\\mathbb{E}_{H=\\{h_{\\kappa}\\}\\subseteq=\\{\\xi_{\\kappa}\\}}\\mathrm{prox}(\\mathrm{T}[H],\\rho_{\\Sigma},\\{m_{\\kappa}^{t}+\\sqrt{q_{\\kappa}^{t}}\\xi_{\\kappa}\\}_{\\kappa},\\{V_{\\kappa}^{t}\\}_{\\kappa})}_{\\equiv\\hat{m}_{\\ell}^{t}}\\!\\!\\!\\ell}\\\\ &{\\quad\\quad\\quad+\\underbrace{Q_{i}^{\\star}\\sum_{\\ell}(\\Sigma_{\\ell})_{i i}}_{\\ell}\\underbrace{\\alpha\\mathbb{E}_{H,\\Xi}\\partial_{h_{\\ell}}\\mathrm{prox}(\\mathrm{T}[H],\\rho_{\\Sigma},\\{m_{\\kappa}^{t}+\\sqrt{q_{\\kappa}^{t}}\\xi_{\\kappa}\\}_{\\kappa},\\{V_{\\kappa}^{t}\\}_{\\kappa})}_{\\equiv\\hat{\\theta}_{\\ell}^{t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the expectations bear over $\\xi_{\\ell}\\sim\\mathcal{N}(0,1)$ and $h_{\\ell}\\sim\\mathcal{N}(\\xi_{\\ell}\\theta_{\\ell}^{t}/\\sqrt{q_{\\ell}^{t}},\\rho_{\\ell}-(\\theta_{\\ell}^{t})^{2}/q_{\\ell}^{t})$ . The variance is given by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{V}[b_{i}^{t},b_{j}^{t}]=\\delta_{i j}\\sum_{\\ell}(\\Sigma_{\\ell})_{i i}\\underbrace{\\alpha\\mathbb{E}_{H,\\Xi}\\mathrm{prox}(\\mathbb{T}[H],\\rho_{\\Sigma},\\{m_{\\kappa}^{t}+\\sqrt{q_{\\kappa}^{t}}\\xi_{\\kappa}\\}_{\\kappa},\\{V_{\\kappa}^{t}\\}_{\\kappa})_{\\ell}^{2}}_{\\equiv\\hat{q}_{\\ell}^{t}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Concetration of $A_{i\\to\\mu}^{t}$ Similarly to the derivation for $V_{\\mu\\to i}^{t},A_{i\\to\\mu}^{t}$ concentrates to ", "page_idx": 23}, {"type": "equation", "text": "$$\nA_{i\\rightarrow\\mu}^{t}=\\sum_{\\ell}\\underbrace{-\\alpha\\frac{1}{V_{\\ell}^{t}}\\left(\\mathbb{E}_{H,\\Xi}\\partial_{\\omega_{\\ell}}\\mathrm{prox}(\\Upsilon[H],\\rho_{\\Sigma},\\{m_{\\kappa}^{t}+\\sqrt{q_{\\kappa}^{t}}\\xi_{\\kappa}\\}_{\\kappa},\\{V_{\\kappa}^{t}\\}_{\\kappa})_{\\ell}-1\\right)}_{\\equiv\\hat{V}_{\\ell}^{t}}(\\Upsilon_{\\ell})_{\\hat{V}_{\\ell}^{t}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Recovering Result 4.2  Wrapping up, we now massage these equations to recover equations (7) from Result 4.2 of the main text. Starting from (31): ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle V_{\\ell}^{t}=\\frac{1}{d}\\sum_{j}(\\Sigma_{\\ell})_{j j}\\frac{1}{\\lambda+\\sum_{\\kappa}\\hat{V}_{\\kappa}^{t-1}(\\Sigma_{\\kappa})}}}\\\\ {{\\displaystyle\\qquad=\\int d\\nu(\\gamma,\\tau)\\gamma_{\\ell}\\left(\\lambda+\\sum_{\\kappa}\\hat{V}_{\\kappa}^{t-1}\\gamma_{\\kappa}\\right)^{-1}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Next, for $q_{\\ell}^{t}$ (34): ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\frac{t}{\\ell}=\\frac{1}{d}\\sum_{i}(\\Sigma_{\\ell})_{i i}\\left(\\left(\\sum_{\\kappa}(\\sqrt{d}(p_{\\kappa})_{i}\\hat{m}_{\\kappa}^{t-1}+Q_{i}^{\\star}(\\Sigma_{\\kappa})_{i i}\\hat{\\theta}_{\\kappa}\\right)^{2}+(\\Sigma_{\\kappa})_{i i}\\hat{q}_{\\kappa}^{t-1}\\right)\\left(\\lambda+\\sum_{\\kappa}\\hat{V}_{\\kappa}^{t-1}(\\Sigma_{\\kappa})\\right)^{-2}\\right.}}\\\\ {{\\displaystyle\\quad=\\int d\\nu(\\gamma,\\tau,\\pi)\\gamma_{\\ell}\\left(\\left(\\sum_{\\kappa}\\hat{m}_{\\kappa}^{t-1}\\tau_{\\kappa}+\\hat{\\theta}_{\\kappa}\\gamma_{\\kappa}\\pi_{\\kappa}\\right)^{2}+\\gamma_{\\kappa}\\hat{q}_{\\kappa}^{t-1}\\right)\\left(\\lambda+\\sum_{\\kappa}\\hat{V}_{\\kappa}^{t-1}\\gamma_{\\kappa}\\right)^{-2}\\qquad\\left(42\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For $\\theta_{\\ell}^{t}(34)$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\theta_{\\ell}^{t}=\\frac{1}{d}\\sum_{1}(\\Sigma_{\\ell})_{i i}Q_{i}^{\\star}\\left(\\sum_{\\kappa}(\\sqrt{d}(p_{\\kappa})_{i}\\hat{m}_{\\kappa}^{t-1}+Q_{i}^{\\star}(\\Sigma_{\\kappa})_{i i}\\hat{\\theta}_{\\kappa}\\right)\\left(\\lambda+\\sum_{\\kappa}\\hat{V}_{\\kappa}^{t-1}(\\Sigma_{\\kappa})\\right)^{-1}+o_{d}(1)\\sum_{\\kappa}\\hat{V}_{\\kappa}^{t-1}(\\hat{V}_{\\kappa}^{t})_{i i}Q_{i}^{\\star}\\right)}}\\\\ {{\\displaystyle\\quad=\\int d\\nu(\\gamma,\\tau,\\pi)\\gamma_{\\ell}\\pi_{\\ell}\\left(\\sum_{\\kappa}\\hat{m}_{\\kappa}^{t-1}\\tau_{\\kappa}+\\hat{\\theta}_{\\kappa}\\gamma_{\\kappa}\\pi_{\\kappa}\\right)\\left(\\lambda+\\sum_{\\kappa}\\hat{V}_{\\kappa}^{t-1}\\gamma_{\\kappa}\\right)^{-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Finally for $m_{\\ell}^{t}$ (33): ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle m_{\\ell}^{t}=\\frac{1}{d}\\sum_{i}(\\sqrt{d}p_{\\ell})_{i}\\left(\\sum_{\\kappa}(\\sqrt{d}(p_{\\kappa})_{i}\\hat{m}_{\\kappa}^{t-1}+Q_{i}^{\\star}(\\Sigma_{\\kappa})_{i i}\\hat{\\theta}_{\\kappa}\\right)\\left(\\lambda+\\sum_{\\kappa}\\hat{V}_{\\kappa}^{t-1}(\\Sigma_{\\kappa})\\right)^{-1}+o_{d}(1)\\sum_{\\kappa}\\hat{V}_{\\kappa}^{t-1}(\\hat{V}_{\\kappa}^{t})_{i}\\right)}}\\\\ {{\\displaystyle\\qquad=\\int d\\nu(\\gamma,\\tau,\\pi)\\tau_{\\ell}\\left(\\sum_{\\kappa}\\hat{m}_{\\kappa}^{t-1}\\tau_{\\kappa}+\\hat{\\theta}_{\\kappa}\\gamma_{\\kappa}\\pi_{\\kappa}\\right)\\left(\\lambda+\\sum_{\\kappa}\\hat{V}_{\\kappa}^{t-1}\\gamma_{\\kappa}\\right)^{-1}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For $\\hat{m}_{\\ell}^{t}$ (38): ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\hat{m}_{\\ell}^{t}=\\alpha\\mathbb{E}_{H,\\Xi}\\frac{1}{V_{\\ell}^{t}}\\left[\\mathrm{prox}_{\\ell}-\\sqrt{q_{\\ell}^{t}}\\xi_{\\ell}-m_{\\ell}^{t}\\right],\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "while for $\\hat{\\theta}_{\\ell}^{t}$ (38): ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\theta}_{\\ell}^{t}=\\alpha\\mathbb{E}_{H,\\Xi}\\frac{1}{V_{\\ell}^{t}}\\partial_{h_{\\ell}}\\left[\\mathrm{prox}_{\\ell}-\\sqrt{q_{\\ell}^{t}}\\xi_{\\ell}-m_{\\ell}^{t}\\right]}\\\\ &{\\quad=\\alpha\\mathbb{E}_{H,\\Xi}\\frac{1}{V_{\\ell}^{t}}\\frac{h_{\\ell}-\\theta_{\\ell}^{t}\\left/\\sqrt{q_{\\ell}^{t}}\\xi_{\\ell}\\right.}{\\rho_{\\ell}-(\\theta_{\\ell}^{t})^{2}\\left/q_{\\ell}^{t}\\right.}\\left[\\mathrm{prox}_{\\ell}-\\sqrt{q_{\\ell}^{t}}\\xi_{\\ell}-m_{\\ell}^{t}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now turning to $\\hat{q}_{\\ell}^{t}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{q}_{\\ell}^{t}=\\alpha\\mathbb{E}_{H,\\Xi}\\left[\\left(\\frac{1}{V_{\\ell}^{t}}\\mathrm{prox}_{\\ell}-\\sqrt{q_{\\ell}^{t}}\\xi_{\\ell}-m_{\\ell}^{t}\\right)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Finally, fr $\\hat{V}_{\\ell}^{t}$ (40): ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{V}_{\\ell}^{t}=-\\alpha\\mathbb{E}_{H,\\Xi}\\frac{1}{V_{\\ell}^{t}}\\left[\\partial_{\\omega_{\\ell}}\\mathrm{prox}_{\\ell}-1\\right]}\\\\ &{\\quad=-\\alpha\\mathbb{E}_{H,\\Xi}\\frac{1}{V_{\\ell}^{t}}\\left[\\frac{1}{\\sqrt{q_{\\ell}^{t}}}\\partial_{\\xi}(\\mathrm{prox}_{\\ell}-\\sqrt{q_{\\ell}^{t}}\\xi_{\\ell}-m_{\\ell})\\right]}\\\\ &{\\quad=\\alpha\\mathbb{E}_{H,\\Xi}\\frac{1}{\\sqrt{q_{\\ell}^{t}}V_{\\ell}^{t}}\\left[\\frac{\\theta_{\\ell}^{t}}{\\sqrt{q_{\\ell}^{t}}V_{\\ell}^{t}}\\left(\\frac{h_{\\ell}-\\sqrt{q_{\\ell}^{t}}\\xi_{\\ell}}{\\rho_{\\ell}-(\\theta_{\\ell}^{t})^{2}q_{\\ell}^{t}}-\\xi\\right)(\\mathrm{prox}_{\\ell}-\\sqrt{q_{\\ell}^{t}}\\xi_{\\ell}-m_{\\ell})\\right]}\\\\ &{\\quad=\\frac{\\theta_{\\ell}^{t}\\hat{\\theta}_{\\ell}^{t}}{q_{\\ell}^{t}}-\\alpha\\mathbb{E}_{H,\\Xi}\\frac{1}{\\sqrt{q_{\\ell}^{t}}V_{\\ell}^{t}}(\\mathrm{prox}_{\\ell}-\\sqrt{q_{\\ell}^{t}}\\xi_{\\ell}-m_{\\ell})\\xi_{\\ell}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Summary $:$ State evolution equations  The state evolution equations asymptotically describing the dynamics of the GAMP algorithm 1 thus read ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|V_{t}^{t}=\\int d\\boldsymbol{\\nu}(\\gamma,\\tau)\\gamma_{t}\\left(\\lambda+\\sum_{\\nu}\\hat{V}_{\\kappa}^{t-1}\\gamma_{\\kappa}\\right)^{-1}\\right.}\\\\ &{\\left.\\left|\\boldsymbol{q}_{t}^{t}=\\int d\\boldsymbol{\\nu}(\\gamma,\\tau)\\gamma_{t}\\left(\\left(\\sum_{\\nu}\\hat{m}_{\\kappa}^{t-1}\\tau_{\\kappa}+\\hat{\\theta}_{\\kappa}\\gamma_{\\kappa}\\boldsymbol{\\pi}_{\\kappa}\\right)^{2}+\\gamma_{\\kappa}\\hat{\\theta}_{\\kappa}^{t-1}\\right)\\left(\\lambda+\\sum_{\\nu}\\hat{V}_{\\kappa}^{t-1}\\gamma_{\\kappa}\\right)^{-2}\\right.}\\\\ &{\\left.\\left|\\theta_{\\kappa}^{t}=\\int d\\boldsymbol{\\nu}(\\gamma,\\tau)\\gamma_{t}\\sigma_{\\nu}\\left(\\sum_{\\nu}\\hat{m}_{\\kappa}^{t-1}\\tau_{\\kappa}+\\hat{\\theta}_{\\kappa}\\gamma_{\\kappa}\\boldsymbol{\\pi}_{\\kappa}\\right)\\left(\\lambda+\\sum_{\\nu}\\hat{V}_{\\kappa}^{t-1}\\gamma_{\\kappa}\\right)^{-1}\\right.}\\\\ &{\\left.\\left|m_{t}^{t}=\\int d\\boldsymbol{\\nu}(\\gamma,\\tau,\\tau)\\gamma_{t}\\left(\\sum_{\\kappa}\\hat{m}_{\\kappa}^{t-1}\\tau_{\\kappa}+\\hat{\\theta}_{\\kappa}\\gamma_{\\kappa}\\boldsymbol{\\pi}_{\\kappa}\\right)\\left(\\lambda+\\sum_{\\nu}\\hat{V}_{\\kappa}^{t-1}\\gamma_{\\kappa}\\right)^{-1}\\right.}\\\\ &{\\left.\\left|\\hat{V}_{t}^{t}=\\frac{\\theta_{\\kappa}^{t}}{\\theta_{\\kappa}^{2}}-\\alpha\\mathbb{E}_{\\boldsymbol{H}}\\mathcal{L}_{\\nu}^{-1}\\gamma_{\\kappa}^{0}\\left(\\mathrm{pros}\\epsilon-\\sqrt{q_{\\kappa}}\\xi_{\\ell}-m_{\\ell}\\right)\\xi_{\\ell}\\right.}\\\\ &{\\left.\\left|\\hat{q}_{t}^{t}=\\alpha\\mathbb{E}_{\\boldsymbol{H}}\\mathcal{L}_{\\nu}^{-1}\\left(\\left[\\hat{V}_{\\kappa}^{t}\\mathbf{pros}-\\epsilon\\hat{\\mathcal{T}}_{\\kappa}^{t}\\xi_{\\ell}-m_{\\ell}\\right]^{2}\\right)\\right.}\\\\ &{\\left.\\left|\\hat{\\theta}_{\\kappa}^{t}=\\mathbb{E}_ \n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which exactly recovers equations (7) of Result 4.2 of the main text, for the case $r_{s}=1$ considered in the present Appendix. Again, we mention that the case $r_{s}\\geq2$ should follow straightforwardly with the exact same derivation steps, using tensor variables (see e.g. [52]). This subsection has thus established that the equations (7) (with time indices) describe the summary statistics capturing the dynamics of GAMP iterations 1. In particular, (7) describe the fixed points of GAMP. The next subsection further shows that the (stable) fixed points of GAMP correspond to critical (zero-gradient) points of the empirical landscape (3), i.e. fixed points of gradient descent. Finally, we provide in Appendix C an alternative derivation of the state evolution equations (7)(49), using the replica method from statistical physics [61, 66]. ", "page_idx": 24}, {"type": "text", "text": "B.4 Fixed points of GAMP are fixed points of GD ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this subsection, we show that fixed points of GAMP 1, as asymptotically described by (7) in Result 4.2, correspond to critical (zero gradient) points of the empirical landscape (3). Again, we present the ", "page_idx": 24}, {"type": "text", "text": "result for $r_{s}=1$ for clarity, the generalization to $r_{s}\\geq2$ being straightforward (see e.g. [52]). In the previous notations, let us denote the (simplified, see (27) empirical loss as ", "page_idx": 25}, {"type": "equation", "text": "$$\nL(\\{\\tilde{X}_{\\ell}Q\\}_{\\ell})+g(Q)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we introduced the shorthands ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle{\\cal L}(\\{h_{\\ell}\\}\\in\\mathbb{R}^{n})\\equiv\\sum_{\\mu=1}^{n}\\left(-2\\,\\mathrm{Tr}\\big[\\mathtt{S}[\\{h_{\\ell}^{\\mu}\\}_{\\ell}]\\rho_{\\mu}T_{\\mu}^{\\top}\\big]+\\mathrm{Tr}\\big[\\mathtt{S}[\\{h_{\\ell}^{\\mu}\\}_{\\ell}]\\rho_{\\mu}\\mathtt{S}[\\{h_{\\ell}^{\\mu}\\}_{\\ell}]^{\\top}\\big]\\right)}}\\\\ {~~}\\\\ {{\\displaystyle g(Q)\\equiv\\frac{\\lambda}{2}\\|Q\\|^{2},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "i.e. respectively the simplified empirical loss (3) and the regularization, as functions with matrix arguments. The empirical minimization problem (3) can thus be written compactly as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\hat{Q}=\\underset{Q\\in\\mathbb{R}^{d}}{\\operatorname{argmin}}\\left\\{L(\\{\\tilde{X}_{\\ell}Q\\}_{\\ell})+r(Q)\\right\\}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "with the critical (zero-gradient) condition being given by ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{\\ell=1}^{L}\\tilde{X}_{\\ell}^{\\top}\\partial_{\\ell}L(\\{\\tilde{X}_{\\ell}Q\\}_{\\ell})+\\partial g(Q)\\stackrel{!}{=}0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Let us choose a diagonal definite $A\\,\\in\\,\\mathbb{R}^{d\\times d}$ , and a sequence $\\{V_{\\mu}\\}_{1\\le\\mu\\le n}$ of symmetric definite $L\\times L$ matrices. Group them into a block diagonal matrix $\\check{V}\\in\\mathbb{R}^{L n\\times L n}$ , so that the $\\mu-$ th block of $\\check{V}$ corresponds to $V_{\\mu}$ . It shall prove useful to further introduce the matrices $\\check{\\tilde{X}}\\in\\mathbb{R}^{d\\times n L}$ (resp. $\\check{\\partial}L(\\check{\\tilde{X}})\\,\\in\\,\\mathbb{R}^{n L})$ : defined as the concatenation of the matrices $\\tilde{X}_{1},...,\\tilde{X}_{L}$ (resp. $\\partial_{1}L,...,\\partial_{L}L)$ \uff0c viewed as $n$ blocks of length $L$ . Then without loss of generality the zero-gradient condition can be rewritten as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\check{X}^{\\top}V^{-1}\\left(V\\check{\\partial}L(\\check{X})-\\check{\\tilde{X}}Q\\right)+A(A^{-1}\\partial g(Q)+Q){\\overset{!}{=}}\\check{X}^{\\top}V^{-1}\\check{\\tilde{X}}Q+A Q.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Similarly to [52], let us introduce ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\check{\\omega}\\equiv V\\check{\\partial}L(\\check{\\tilde{X}})-\\check{\\tilde{X}}Q.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This can be written in terms of a resolvent as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\check{\\tilde{X}}Q=\\mathrm{prox}(\\check{\\omega})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname{prox}(\\check{\\omega})\\in\\mathbb{R}^{n L}=\\underset{\\check{x}\\in\\mathbb{R}^{n L}}{\\operatorname{argmin}}\\left\\{\\frac{1}{2}\\|\\check{x}-\\check{\\omega}\\|_{V}^{2}\\!+\\!L(\\check{x})\\right\\}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which corresponds to (30). Similarly, we denote ", "page_idx": 25}, {"type": "equation", "text": "$$\nb\\equiv A^{-1}\\partial g(Q)+Q\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "So that ", "page_idx": 25}, {"type": "equation", "text": "$$\nQ=\\mathrm{prox}_{g}(b)=\\underset{x\\in\\mathbb{R}^{d}}{\\operatorname{argmin}}\\left\\{\\frac{1}{2}\\|x-b\\|_{A^{-1}}^{2}{+}g(x)\\right\\}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In the particular case of an $\\ell_{2}$ regularization $g(\\cdot)=\\lambda/2\\vert\\vert\\cdot\\vert\\vert^{2}$ , note that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{prox}_{g}(b)=(\\lambda\\mathbb{I}_{d}+A)^{-1}A b.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The zero-gradient condition can now be rewritten as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\check{\\tilde{X}}^{\\top}V^{-1}\\left(\\mathrm{prox}(\\check{\\omega})-\\check{\\omega}\\right)=A(b-\\mathrm{prox}_{g}(b))\\right.}\\\\ {\\left.\\check{\\tilde{X}}\\mathrm{prox}_{g}(b)=\\mathrm{prox}(\\check{\\omega})\\right.\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "One is now in a position to expand the concatenated variables ? into a sequence of $L\\,n-$ dimensional parameters. For $u=\\mathrm{prox}(\\check{\\omega}),\\check{\\omega}$ let us denote $u_{\\mu\\ell}$ $1\\le\\mu\\le n,1\\le\\ell\\le L)$ the $\\ell-$ th component of the $\\mu$ -th block. Introduce ", "page_idx": 26}, {"type": "equation", "text": "$$\nf_{\\mu\\ell}\\equiv\\sum_{\\kappa}(V_{\\mu}^{-1})_{\\ell\\kappa}(\\mathrm{prox}(\\breve{\\omega})_{\\mu\\kappa}-\\breve{\\omega}_{\\mu\\kappa}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Denote $f_{\\ell}\\equiv(f_{\\mu\\ell})_{1\\leq\\mu\\leq n}\\in\\mathbb{R}^{n},\\omega_{\\ell}\\equiv(\\omega_{\\mu\\ell})_{1\\leq\\mu\\leq n}\\in\\mathbb{R}^{n}$ .The system of equations (63) can then be rewritten as (further redefining $b\\leftarrow A b$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\underset{\\ell}{\\sum}\\tilde{X}_{\\ell}^{\\top}f_{\\ell}=b-A(\\lambda\\mathbb{I}_{d}+A)^{-1}b\\right.}\\\\ {\\left.\\tilde{X}_{\\ell}(\\lambda\\mathbb{I}_{d}+A)^{-1}b=\\sum_{\\kappa}V_{\\ell\\kappa}f_{\\kappa}-\\omega_{\\ell}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We used the assumption that $g(\\cdot)$ is an $\\ell_{2}$ regularization. Finally, introducing ${\\hat{Q}}=\\operatorname{prox}_{g}(A^{-1}b)=$ $(\\lambda\\mathbb{I}_{d}+A)^{-1}b$ , on reaches ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\displaystyle\\sum_{\\ell}\\tilde{X}_{\\ell}^{\\top}f_{\\ell}=b-A\\hat{Q}}\\\\ {\\tilde{X}_{\\ell}\\hat{Q}=\\displaystyle\\sum_{\\kappa}V_{\\ell\\kappa}f_{\\kappa}-\\omega_{\\ell}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which corresponds to the fixed-point equations of GAMP (Algorithm 1). This finishes to show the correspondence between the fixed points of GAMP and the critical points of the empirical landscape (3). To summarize, we have shown that equations (7) describe the zero-gradient points of the empirical loss landscape (3), i.e. fixed points of GD. ", "page_idx": 26}, {"type": "text", "text": "B.5 Towards a rigorous proof of result 4.2 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "While the connection between the GAMP fixed point and the extrema of the loss is sound, and has been at the roots of many rigorous results for convex losses, see e.g. [57, 58, 59, 60, 48], there exist technical difficulties in adapting these rigorous arguments to the present setting, and a fully rigorous proof would warrant sizable work. While we leave this challenging task for future work, we wish to discuss how it can be potentially achieved. The first task would require the proof of point-wise convergence of GAMP, as indeed, the identification of the GAMP estimates with the one of the extrema of the loss function requires to be at the fixed point of the iteration. This difficulty, discussed in detail in, e.g. [67, 60, 48], can be in principle addressed by computing the convergence criterion from the state evolution equations (see [67] the discussion in Lemma 7 in [60]), a criterion sometimes called the \"replicon\" in the context of replica theory [68]. ", "page_idx": 26}, {"type": "text", "text": "Provided the replicon criterion is satisfied, all converging fixed point described by our theory thus correspond rigorously to fixed point of the loss. The last task would be to prove that the minimum of the loss is indeed the fixed point we found with minimum energy. A potential strategy to prove this would be to use the Gordon-Minimax approach of [69]. While it is used in many situations for convex problems (e.g. [70, 71, 72]), only one side would be required for our (non-convex) problem thanks to the GAMP matching bound. We hope that our results would provide inspiration for further research in this direction. ", "page_idx": 26}, {"type": "text", "text": "C  Derivation of Result 4.2 with the replica method ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In the Appendix we provide an alternative derivation of Result 4.2, which sharply characterizes the global minimum of the empirical loss (3), using the heuristic replica method from statistical physics [61, 66] in its replica-symmetric formulation. First observe that for any test function $\\phi(\\hat{Q})$ Oof the minimizer $\\hat{Q}$ of (3), ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\phi(\\hat{Q})=\\operatorname*{lim}_{\\beta\\to\\infty}\\mathbb{E}_{\\mathcal{D}}\\frac{1}{Z}\\int d Q\\phi(Q)e^{-\\beta\\mathcal{R}[Q]},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we denoted $R[Q]$ the empirical loss (3), and ", "page_idx": 26}, {"type": "equation", "text": "$$\nZ\\equiv\\int d Q e^{-\\beta\\mathcal{R}[Q]}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "the normalization factor, also known as the partition function in statistical physics. We remind that $\\mathcal{D}$ refers to the training set. In order to access key summary statistics and learning metrics associated to $\\hat{Q}$ it is therefore reasonable to seek to compute the generating function associated to the measure (67),namely $\\mathbb{E}\\ln Z$ . Such computations can be addressed using the replica method from statistical physics [61, 66], building on the identity ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\ln Z=\\operatorname*{lim}_{s\\to0}\\frac{Z^{s}-1}{s}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The backbone of the derivation thus lies in the computation of $\\mathbb{E}Z^{s}$ .Below, we detail the derivation for a generic convex regularizer $g:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}_{+}$ and later specialize to the case of $\\ell_{2}$ regularization. The replicated partition function thus reads ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}Z^{s}=\\int\\prod_{a=1}^{s}\\displaystyle d Q_{a}e^{-\\beta\\sum_{a=1}^{s}g(Q_{a})}}\\\\ {\\displaystyle\\prod_{\\mu=1}^{n}\\mathbb{E}_{x}e^{-\\beta\\sum_{a=1}^{s}\\left(\\mathrm{Tr}\\,\\mathtt{b}\\left[\\frac{1}{\\sqrt{d}}(x+p)Q_{a}\\right]\\,\\rho\\mathtt{x S}\\left[\\frac{1}{\\sqrt{d}}(x+p)Q_{a}\\right]^{\\top}-2\\,\\mathrm{Tr}\\,\\mathtt{T}\\left[\\frac{1}{\\sqrt{d}}x_{\\ell}Q_{\\star}\\right]\\,\\rho\\mathtt{x S}\\left[\\frac{1}{\\sqrt{d}}(x+p)Q_{a}\\right]^{\\top}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Introduce the local fields ", "page_idx": 27}, {"type": "equation", "text": "$$\nh^{a}\\equiv\\frac{x Q_{a}}{\\sqrt{d}}\\in\\mathbb{R}^{L\\times r},\\qquad\\qquad\\qquad h^{\\star}\\equiv\\frac{x Q_{\\star}}{\\sqrt{d}}\\in\\mathbb{R}^{L\\times t}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and the overlaps ", "page_idx": 27}, {"type": "equation", "text": "$$\nm_{a}\\equiv\\frac{p Q_{a}}{\\sqrt{d}}\\in\\mathbb{R}^{L\\times r},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "with rows $m_{a}^{\\ell}$ . These felds have statistics ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{x}[h_{\\ell}^{a}(h_{\\kappa}^{b})^{\\top}]=\\delta_{\\ell\\kappa}\\frac{Q_{a}^{\\top}\\Sigma_{\\ell}Q_{b}}{d}\\equiv q_{a b}^{\\ell}}\\\\ &{\\mathbb{E}_{x}[h_{\\ell}^{\\star}(h_{\\kappa}^{\\star})^{\\top}]=\\delta_{\\ell\\kappa}\\frac{Q_{\\star}^{\\top}\\Sigma_{\\ell}Q_{\\star}}{d}\\equiv\\rho_{\\ell}}\\\\ &{\\mathbb{E}_{x}[h_{\\ell}^{a}(h_{\\kappa}^{\\star})^{\\top}]=\\delta_{\\ell\\kappa}\\frac{Q_{a}^{\\top}\\Sigma_{\\ell}Q_{\\star}}{d}\\equiv\\theta_{a}^{\\ell}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf\\tilde{\\Delta}}=\\int{d m d\\hat{m}d\\theta d\\hat{d\\phi}\\frac{-\\partial\\sum_{\\ell}\\left[\\hat{m}_{a}^{\\ell\\,\\top}m_{a}^{\\ell\\,\\top}\\hat{m}_{a}^{\\ell\\,\\top}+\\mathrm{Tr}\\left({\\theta}_{a}^{\\ell\\,\\hat{\\phi}_{a}^{\\ell\\,\\top}}\\right)\\right]-d\\sum_{\\ell}\\sum_{1\\leq a\\leq b\\leq s}\\mathrm{Tr}\\left({q}_{a b}^{\\ell}\\hat{q}_{a b}^{\\ell\\,\\top}\\right)}}}\\\\ {{\\displaystyle{\\underbrace{\\int\\prod_{a=1}^{s}d Q_{a}e^{-\\beta\\,\\sum_{a=1}^{s}g(Q_{a})}+\\sum_{\\ell}\\sum_{\\ell}\\left(\\sqrt{d}\\hat{m}_{a}^{\\ell\\,\\top}Q_{a}^{\\top}\\,p_{\\ell}+\\mathrm{Tr}\\left[{\\theta}_{a}^{\\ell}Q_{\\cdot}^{\\top}\\Sigma_{\\ell}Q_{a}\\right]\\right)+\\sum_{1\\leq a\\leq b\\leq s}\\sum_{\\ell}\\mathrm{Tr}\\left[{q}_{a b}^{\\ell}Q_{b}^{\\top}\\Sigma_{\\ell}Q_{a}\\right]}}_{e^{a\\hat{d}\\,\\Psi_{Q}}}}}\\\\ {{\\displaystyle{\\underbrace{\\left[\\mathbb{E}_{h^{*},\\{h_{a}\\}_{a=1}^{s}}e^{-\\beta\\,\\sum_{a=1}^{s}\\left(\\mathrm{Tr}\\mathbb{S}\\left[h^{a}+m^{a}\\right]\\rho_{a\\leq\\{b}\\}\\left(h^{a}+m^{a}\\right)^{\\top}-2\\,\\mathrm{Tr}\\mathbb{T}\\left[h^{*}\\right]\\rho_{\\geq\\Xi}\\mathbb{S}\\left[h^{a}+m^{a}\\right]^{\\top}\\right)}\\right]^{\\alpha d}}_{{\\displaystyle(1\\leq a\\leq b\\leq s)\\prod_{a=1}^{s}\\left(\\mathrm{Tr}\\right)\\left(\\Omega_{a}^{\\ell}\\right)}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we decomposed the replicated free entropy into the trace, entropic and energetic potentials $\\Psi_{t},\\Psi_{Q},\\Psi_{y}$ . Note that all exponents are scaling with $d\\to\\infty$ . Therefore the integral in (76) can be computed using a Laplace saddle-point approximation. ", "page_idx": 27}, {"type": "text", "text": "C.1  Replica-Symmetric ansatz ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We have thus rephrased the analysis of the measure (67) as a optimization problem over the order parameters $\\{q_{a b}^{\\ell},\\theta_{a}^{\\ell},m_{a}\\}$ , and the associated conjugate variables. However, these still represent ", "page_idx": 27}, {"type": "text", "text": "$2L(s^{2}+1)+2s$ variables, and $s\\to0$ . In order to make progress, we assume that the maximizer is of replica-symmetric (RS) form [66, 61] ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q_{a b}^{\\ell}=(r_{\\ell}-q_{\\ell})\\delta_{a b}+q_{\\ell}}\\\\ &{m_{a}^{\\ell}=m_{\\ell}}\\\\ &{\\theta_{a}^{\\ell}=\\theta_{\\ell}}\\\\ &{\\hat{q}_{a b}^{\\ell}=-\\left(\\hat{r}_{\\ell}/2+\\hat{q}_{\\ell}\\right)+\\hat{q}_{\\ell}}\\\\ &{\\hat{m}_{a}^{\\ell}=\\hat{m}_{\\ell}}\\\\ &{\\hat{\\theta}_{a}^{\\ell}=\\hat{\\theta}_{\\ell}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The RS ansatz holds in a number of machine learning settings, notably for convex problems and Bayes-optimal settings, see e.g. [64] for a review. In the present setting, since the empirical loss (3) is non-convex, we emphasize that the RS ansatz constitutes a heuristic technical assumption of our analysis. ", "page_idx": 28}, {"type": "text", "text": "C.2  Entropic potential ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We now turn to the entropic potential $\\Psi_{w}$ . It is convenient to introduce the variance order parameter ", "page_idx": 28}, {"type": "equation", "text": "$$\n{\\hat{V}}_{\\ell}\\equiv{\\hat{r}}_{\\ell}+{\\hat{q}}_{\\ell}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The entropic potential can then be expressed as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial\\lambda\\circ\\mathcal{W}_{0}}{\\partial\\alpha}}\\\\ {\\displaystyle=\\int\\prod_{\\alpha=1}^{k}d Q_{\\alpha}e^{-\\beta\\sum_{s}g(Q^{\\prime})+\\frac{\\lambda}{\\epsilon_{\\alpha1}}\\sum_{s=1}^{k}\\frac{\\gamma}{\\Gamma}\\left(\\sqrt{d\\hbar}\\alpha_{s}^{\\top}Q_{\\alpha}^{\\top}p_{+}+\\mathrm{T}\\big[\\phi_{\\Gamma}^{\\top}\\Sigma_{:\\mathcal{U}_{0}}\\big]\\right)-\\frac{1}{2}\\sum_{s=1}^{k}\\frac{\\gamma}{\\Gamma}\\big[\\tilde{\\gamma}_{\\xi_{\\alpha},\\mathbf{x}_{\\alpha}^{\\top}Q_{\\alpha}^{\\top}}\\big]+\\frac{\\lambda}{\\int_{\\alpha=1}^{k}\\sum_{s=1}^{k}\\mathrm{Tr}\\big[\\tilde{\\gamma}_{\\xi_{\\alpha},\\mathbf{x}_{\\alpha}^{\\top}}\\big]_{s}^{\\frac{k}{2}}}}\\\\ {\\displaystyle=\\int\\prod_{\\alpha=1}^{k}D\\Xi_{\\ell}}\\\\ {\\displaystyle\\qquad\\left[\\int d Q e^{-\\beta g(Q)-\\frac{1}{2}\\mathrm{Tr}\\big[\\frac{k}{\\omega_{1}}\\tilde{\\gamma}_{\\xi_{\\alpha}^{\\top}}\\tilde{\\mathbf{x}}_{\\xi^{\\alpha},\\bar{\\mathbf{z}}^{\\top}}\\big]+\\left(\\frac{\\lambda}{\\int_{\\alpha=1}^{k}\\left(\\sqrt{d\\hbar}\\alpha_{s}^{\\top}P_{\\xi}^{\\top}+\\tilde{\\beta}_{\\xi_{\\alpha}^{\\top}}Q_{\\alpha}^{\\top}\\Sigma_{:\\alpha}\\right)+\\frac{k}{\\int_{\\alpha=1}^{k}\\Xi_{\\ell}\\left(\\phi(\\tilde{\\mathbf{k}}\\otimes\\mathbf{g}\\mathbf{g}\\mathbf{x}_{\\alpha})\\right)^{\\frac{1}{2}}}\\right)\\circ Q\\right]^{s}}}\\\\ {\\displaystyle=\\mathbb{E}_{\\Xi}\\left[\\int d Q e^{-\\beta g(Q)-\\frac{1}{2}\\mathrm{Tr}\\big[\\frac{k}{\\omega_{1}}\\tilde{\\gamma}_{\\xi_{\\alpha}^{\\top}}\\tilde{\\mathbf{x}}_{\\xi^{\\alpha}}\\mathbb{T}\\big]\\otimes Q+\\left(\\frac{k}{\\int_{\\alpha=1}^{k}\\left(\\sqrt{d\\hbar}\\alpha_{s}P_{\\xi}^{\\top}+\\tilde{\\beta}_{\\xi_{\\alpha}^{\\top}}Q_{\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Im\\Psi_{w}=\\frac{1}{d}\\int\\mathbb{E}_{\\Xi}\\ln\\left[\\int d Q e^{-\\beta g(Q)-\\frac{1}{2}Q\\odot}\\Big[\\sum_{\\ell=1}^{L}\\hat{V}_{\\ell}\\otimes\\Sigma_{\\ell}\\Big]\\circledcirc Q+\\Big(\\sum_{\\ell=1}^{L}\\big(\\sqrt{d}\\hat{m}_{\\ell}p_{\\ell}^{\\top}+\\hat{\\theta}_{\\ell}Q_{\\star}^{\\top}\\Sigma_{\\ell}\\big)+\\sum_{\\ell=1}^{L}\\Xi_{\\ell}\\odot(\\hat{q}_{\\ell}\\otimes\\Sigma_{\\ell})^{\\frac{1}{2}}\\Big)\\odot_{\\ell}\\right.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Foramatrix $\\Xi\\in\\mathbb{R}^{r\\times d}$ andtensors $A,B\\in\\mathbb{R}^{r\\times d}\\otimes\\mathbb{R}^{r\\times d}$ wedenoted $\\begin{array}{r}{(\\Xi\\odot A)_{k l}=\\sum_{i j}\\Xi^{i j}A_{i j,k l}}\\end{array}$ and $\\begin{array}{r}{(A\\odot B)_{i j,k l}=\\sum_{r s}A_{i j,r s}B_{r s,k l}}\\end{array}$ ", "page_idx": 28}, {"type": "text", "text": "C.3 Energetic potential ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The computation of the energetic potential $\\Psi_{y}$ is rather standard and follows the same lines as in e.g. [20], yieiding ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\vert\\hat{w}_{y}=\\int_{\\mathbb{R}^{d_{x}}}d Y D\\hat{z}\\int_{\\mathbb{R}^{d_{x}}}\\hat{\\left[\\left[\\ell\\cdot(\\rho_{t}-\\theta_{t}^{\\top}q_{t}^{-1}\\theta_{t})^{\\frac{1}{2}}z_{t}-\\theta_{t}^{\\top}q_{t}^{\\frac{1}{2}}\\xi_{t}\\right]}\\right.}&{}\\\\ &{\\qquad\\times\\ln\\left[\\int_{\\mathbb{R}^{d_{x}}\\times\\infty}d X\\prod_{\\ell=1}^{\\frac{L}{2}}\\frac{e^{-\\frac{1}{2}\\left(x-\\eta_{\\ell}^{-1}\\xi_{\\ell}\\right)^{\\top}V_{\\ell}-\\left(x-\\eta_{\\ell}^{-1}\\xi_{\\ell}\\right)}e^{-\\beta\\mathbb{T}\\Re z\\left[z+m\\right]\\rho_{s}\\mathbb{I}\\left[x+m\\right]^{\\top}-2\\mathbb{T}\\mathbb{T}\\mathbb{T}\\mathbb{I}\\left[y\\right]\\rho\\mathbb{S}\\mathbb{I}\\left[z+m\\right]}}{\\operatorname*{det}\\left(2\\pi V_{\\ell}\\right)}e^{-\\beta\\mathbb{T}\\Re z\\left[z+m\\right]\\rho_{s}\\mathbb{I}\\left[x+m\\right]^{\\top}-2\\mathbb{T}\\mathbb{T}\\mathbb{I}\\mathbb{I}\\rho\\mathbb{S}\\mathbb{I}\\left[z+m\\right]}\\right.}\\\\ &{=\\int_{\\mathbb{R}^{d_{x}}}d Y\\int_{\\mathbb{R}^{d_{x}}\\times\\mathbb{I}}\\frac{D\\hat{z}\\prod_{\\ell=1}^{\\frac{L}{2}}\\frac{e^{-\\frac{1}{2}\\left(y-\\theta_{\\ell}^{\\top}\\frac{q_{\\ell}^{-1}}{q_{\\ell}^{\\ell}}\\right)^{\\top}}\\left(\\pi_{\\ell}-\\theta_{\\ell}^{\\top}\\pi_{\\ell}^{-1}\\theta_{\\ell}\\right)^{-1}\\left(\\pi-\\theta_{\\ell}^{\\top}\\frac{q_{\\ell}^{-1}}{q_{\\ell}^{\\ell}}\\xi_{\\ell}\\right)}{\\operatorname*{det}\\left[2\\pi\\left(\\rho_{\\ell}-\\theta_{\\ell}^{\\top}\\frac{q_{\\ell}^{-1}}{q_{\\ell}^{\\ell}}\\right)\\right]}}\\\\ &{\\qquad\\times\\ln\\left[\\int_{\\mathbb{R}^{d_{x}}}d X\\prod_{\\ell=1}^{\\frac{L}{2}}\\frac{e^{-\\frac{\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "C.4   Zero-temperature limit ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We now take the limit $\\beta\\rightarrow\\infty$ . Rescaling ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\beta\\hat{V}_{\\ell}\\gets\\hat{V}_{\\ell},\\qquad\\frac1\\beta V_{\\ell}\\gets V_{\\ell},\\qquad\\beta\\hat{m}_{\\ell}\\gets\\hat{m}_{\\ell},\\qquad\\beta\\hat{\\theta}_{\\ell}\\gets\\hat{\\theta}_{\\ell},\\qquad\\beta^{2}\\hat{q}_{\\ell}\\gets\\hat{q}_{\\ell}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The entropic potential then reduces to ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Psi_{w}=\\displaystyle\\frac{1}{2d}\\mathbb{E}_{\\Xi}\\operatorname{Tr}\\left[\\left(\\displaystyle\\sum_{\\ell=1}^{L}\\hat{V}_{\\ell}\\otimes\\Sigma_{\\ell}\\right)\\odot\\left(\\displaystyle\\sum_{\\ell=1}^{L}\\left(\\sqrt{d}\\hat{m}_{\\ell}p_{\\ell}^{\\top}+\\hat{\\theta}_{\\ell}Q_{\\star}^{\\top}\\Sigma_{\\ell}\\right)+\\displaystyle\\sum_{\\ell=1}^{L}\\Xi_{\\ell}\\odot\\left(\\hat{q}_{\\ell}\\otimes\\Sigma_{\\ell}\\right)^{\\frac{1}{2}}\\right)^{\\otimes2}\\right]}\\\\ &{\\qquad-\\displaystyle\\frac{1}{d}\\mathbb{E}_{\\Xi}\\mathcal{M}_{g}(\\Xi)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where we defined the entropic Moreau enveloppe ", "page_idx": 29}, {"type": "equation", "text": "$$\nM_{g}(\\Xi)\\equiv\\operatorname*{inf}_{Q}\\left\\{\\frac{1}{2}\\left\\|\\bigg(\\sum_{\\ell=1}^{L}\\hat{V}_{\\ell}\\otimes\\Sigma_{\\ell}\\bigg)^{1/2}\\bigg(Q-\\bigg(\\sum_{\\ell=1}^{L}\\hat{V}_{\\ell}\\otimes\\Sigma_{\\ell}\\bigg)^{-1}\\bigg(\\sum_{\\ell=1}^{L}\\Big(\\sqrt{d}\\hat{m}_{\\ell}p_{\\ell}^{\\top}+\\hat{\\theta}_{\\ell}Q_{\\star}^{\\top}\\Sigma_{\\ell}\\Big)+\\sum_{\\ell=1}^{L}\\Xi_{\\ell}\\odot(\\hat{q}_{\\ell}\\otimes\\Sigma_{\\ell})^{\\frac{1}{2}}\\bigg)\\right)\\right\\|^{2}+\\cdots\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The energetic potential can be similarly recast into a more compact form ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\Psi_{y}=-\\mathbb{E}_{Y,\\Xi}\\mathcal{M}(Y,\\Xi)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the Moreau envelope is defined as ", "page_idx": 29}, {"type": "equation", "text": "$$\nM(Y,\\Xi)=\\operatorname*{inf}_{X}\\frac{1}{2}\\left\\{\\sum_{\\ell=1}^{L}\\mathrm{Tr}\\bigg[V_{\\ell}^{-1}\\left(x_{\\ell}-q_{\\ell}^{1/2}\\xi_{\\ell}-m_{\\ell}\\right)^{\\otimes2}\\bigg]+\\mathrm{Tr}\\big[\\{\\mathrm{X}\\}\\rho_{\\Sigma}\\mathrm{S}(X)^{\\top}\\big]-2\\,\\mathrm{Tr}\\big[\\mathrm{T}(Y)\\rho_{\\Sigma}\\mathrm{S}(X)^{\\top}\\big]\\right\\}\\;,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "C.5  Replica free entropy ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "One finally reaches an expression for the replica free entropy as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathbb{P}\\!=\\!\\frac{1}{2}\\sum_{\\ell=1}^{L}\\left(\\mathrm{Tr}\\,\\hat{V}_{\\ell}q_{\\ell}-\\mathrm{Tr}\\,\\hat{q}_{\\ell}V_{\\ell}\\right)-\\sum_{\\ell=1}^{L}\\hat{m}_{\\ell}^{\\top}\\,m_{\\ell}-\\displaystyle\\sum_{\\ell=1}^{L}\\mathrm{Tr}\\,\\hat{\\theta}_{\\ell}^{\\top}\\theta_{\\ell}-\\frac{1}{d}\\mathbb{E}_{\\Xi}{\\cal M}_{g}(\\Xi)}}\\\\ {{\\displaystyle\\quad+\\left.\\frac{1}{2d}\\mathbb{E}_{\\Xi}\\,\\mathrm{Tr}\\left[\\left(\\sum_{\\ell=1}^{L}\\hat{V}_{\\ell}\\otimes\\Sigma_{\\ell}\\right)\\odot\\left(\\left(\\sqrt{d}\\hat{m}_{\\ell}p_{\\ell}^{\\top}+\\hat{\\theta}_{\\ell}Q_{\\star}^{\\top}\\Sigma_{\\ell}\\right)+\\displaystyle\\sum_{\\ell=1}^{L}\\Xi_{\\ell}\\odot\\left(\\hat{q}_{\\ell}\\otimes\\Sigma_{\\ell}\\right)^{\\frac{1}{2}}\\right)^{\\otimes2}\\right]-\\alpha\\mathbb{E}_{\\delta}\\right.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "C.6 Saddle-point equations : general regularizer ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The extremization of the free entropy (92) yields, similarly to [47], the following system of selfconsistent equations on the summary statistics: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{V_{\\ell}=\\frac{1}{d}\\mathbb{E}\\mathbf{z}\\left[\\left(\\operatorname{prox}_{g}\\odot(\\hat{q}_{\\ell}\\otimes\\mathbf{S}_{\\ell})^{-\\frac{1}{2}}\\odot(\\mathbb{I}_{r}\\otimes\\mathbf{S}_{\\ell})\\right)\\mathbf{\\Xi}\\mathbf{\\Xi}\\mathbf{\\Xi}_{\\ell}^{\\top}\\right]\\right.}\\\\ &{\\left.\\left\\{q_{\\ell}=\\frac{1}{d}\\mathbb{E}\\mathbf{z}\\left[\\operatorname{prox}_{g}\\mathbf{S}_{\\ell}\\mathbf{prox}_{g}^{\\top}\\right]\\right.}\\\\ &{\\left.\\left[m_{\\ell}=\\frac{1}{\\sqrt{d}}\\mathbb{E}\\mathbf{z}\\left[\\operatorname{prox}_{g}\\mathbf{p}_{\\ell}\\right]\\right.\\right.}\\\\ &{\\left.\\left.\\theta_{\\ell}=\\frac{1}{\\sqrt{d}}\\mathbb{E}\\mathbf{z}\\left[\\operatorname{prox}_{g}\\Sigma_{\\ell}Q_{\\ell}\\right]\\right.\\right.}\\\\ &{\\left.\\left.\\left(\\hat{q}_{\\ell}=\\alpha\\mathbb{E}_{\\ell},Y_{\\ell}^{-1}\\left(\\operatorname{prox}_{\\ell}-q_{\\ell}^{\\frac{1}{2}}\\xi_{\\ell}-m_{\\ell}\\right)^{\\otimes2}V_{\\ell}^{-1}\\right.\\right.\\right.}\\\\ &{\\left.\\left.\\left.\\int\\hat{V}_{\\ell}=\\hat{\\theta}_{\\ell}\\theta_{\\ell}^{\\top}q_{\\ell}^{-1}-\\alpha\\mathbb{E}_{\\Xi,Y}V_{\\ell}^{-1}\\left(\\operatorname{prox}_{\\ell}-q_{\\ell}^{\\frac{1}{2}}\\xi_{\\ell}-m_{\\ell}\\right)\\xi_{\\ell}^{\\top}q_{\\ell}^{-\\frac{1}{2}}\\right.\\right.}\\\\ &{\\left.\\left.\\left.\\hat{m}_{\\ell}=\\alpha\\mathbb{E}_{\\xi,\\eta}V_{\\ell}^{-1}\\left(\\operatorname{prox}_{\\ell}-q_{\\ell}^{\\frac{1}{2}}\\xi_{\\ell}-m_{\\ell}\\right)\\left(y_{\\ell}-\\xi_{\\ell}^{\\top}q_{\\ell}^{-1/2}\\theta_{\\ell}\\right)^{\\top}\\left(\\rho_{\\ell}-\\theta_{\\ell}^{\\top}q_{\\ell}^{-1}\\theta_{\\ell}\\right)^{-1}\\right.\\right.}\\\\ &{\\left.\\left.\\left(\\hat{V}_{\\ell}=\\alpha\\mathbb{E}_{\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the proximals $\\operatorname{prox}_{g}$ and $\\mathrm{prox}_{\\ell}$ respectively refer to the arginf in $Q$ (resp. $x_{\\ell}$ ) of the envelopes $\\mathcal{M}_{g}$ (89) (resp. $\\mathcal{M}\\,91$ ", "page_idx": 30}, {"type": "text", "text": "C.7  Saddle-point equations : $\\ell_{2}$ ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We now specialize the saddle-point equations (93) to the case of an $\\ell_{2}$ regularizer $g(\\cdot)=1/2\\|\\cdot\\|$ the entropic potential admits the simple form ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\mathbb J}_{Q}=\\displaystyle{\\frac{1}{2d}}\\,\\mathrm{Tr}\\left[\\left(\\lambda\\mathbb{I}_{r}\\odot{\\mathbb J}_{d}+\\sum_{\\ell=1}^{L}\\hat{V}_{\\ell}\\otimes\\Sigma_{\\ell}\\right)^{-1}\\left(\\sum_{\\ell=1}^{L}\\hat{q}_{\\ell}\\otimes\\Sigma_{\\ell}+\\left(\\sum_{\\ell=1}^{L}(\\sqrt{d}\\hat{m}_{\\ell}p_{\\ell}^{\\top}+\\hat{\\theta}_{\\ell}Q_{\\star}^{\\top}\\Sigma_{\\ell})^{\\otimes2}\\right)\\right)\\right]}}\\\\ {{\\displaystyle\\qquad=\\frac{1}{2d}\\sum_{i=1}^{d}\\mathrm{Tr}\\left[\\left(\\lambda+\\sum_{\\ell=1}^{L}\\lambda_{i}^{\\ell}\\hat{V}_{\\ell}\\right)^{-1}\\left(\\sum_{\\ell=1}^{L}\\lambda_{i}^{\\ell}\\hat{q}_{k}+\\left(\\sum_{\\ell=1}^{L}(\\sqrt{d}\\hat{m}_{\\ell}p_{\\ell}^{\\top}e_{i}+\\hat{\\theta}_{\\ell}Q_{\\star}^{\\top}\\Sigma_{\\ell}e_{i})\\right)\\left(\\sum_{\\ell=1}^{L}(\\sqrt{d}\\hat{m}_{\\ell}p_{\\ell}^{\\top}e_{i}+\\hat{\\theta}_{\\ell}Q_{\\star}^{\\top}\\Sigma_{\\ell}e_{i})\\right)^{\\top}\\right)\\right]}}\\\\ {{\\displaystyle\\qquad=\\frac{1}{2}\\sum_{j=1}^{d}\\int d\\nu(\\gamma,\\tau,\\pi)\\,\\mathrm{Tr}\\left[\\left(\\lambda+\\sum_{\\ell=1}^{L}\\gamma_{\\ell}\\hat{V}_{\\ell}\\right)^{-1}\\left(\\sum_{\\ell=1}^{L}\\gamma_{\\ell}\\hat{q}_{\\ell}+\\left(\\sum_{\\ell=1}^{L}\\tau_{\\ell}\\hat{m}_{\\ell}+\\gamma_{\\ell}\\hat{\\theta}_{\\ell}\\cdot\\pi\\right)^{\\otimes2}\\right)\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The replica free energy thus reads ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\Phi=\\frac{1}{2}\\sum_{\\ell=1}^{L}\\left(\\mathrm{Tr}\\,\\hat{V}_{\\ell}q_{\\ell}-\\mathrm{Tr}\\,\\hat{q}_{\\ell}V_{\\ell}\\right)-\\sum_{\\ell=1}^{L}\\hat{m}_{\\ell}^{\\top}\\,m_{\\ell}-\\displaystyle\\sum_{\\ell=1}^{L}\\mathrm{Tr}\\,\\hat{\\theta}_{\\ell}^{\\top}\\theta_{\\ell}-\\alpha\\mathbb{E}_{y,\\xi}\\mathcal{M}(y,\\xi)}}\\\\ {{\\displaystyle\\qquad+\\,\\frac{1}{2}\\int d\\nu(\\gamma,\\tau,\\pi)\\,\\mathrm{Tr}\\left[\\left(\\lambda+\\sum_{\\ell=1}^{L}\\gamma_{\\ell}\\hat{V}_{\\ell}\\right)^{-1}\\left(\\sum_{\\ell=1}^{L}\\gamma_{\\ell}\\hat{q}_{\\ell}+\\left(\\displaystyle\\sum_{\\ell=1}^{L}\\tau_{\\ell}\\hat{m}_{\\ell}+\\gamma_{\\ell}\\hat{\\theta}_{\\ell}\\cdot\\pi\\right)^{\\otimes2}\\right)\\right],}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "leading to the saddle point equations ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\dot{\\hat{\\mu}}_{\\hat{\\varepsilon}}=\\partial\\mathbb{E}_{\\hat{\\varepsilon}}\\gamma_{\\varepsilon}V_{\\hat{\\varepsilon}}^{-1}\\left(\\operatorname*{mrx}_{\\hat{\\varepsilon}}-\\hat{q}_{\\hat{\\varepsilon}}^{\\dagger}\\xi-m_{\\hat{\\varepsilon}}\\right)\\right)^{2}V_{\\hat{\\varepsilon}}^{-1}}\\\\ &{\\left\\lVert\\dot{V}_{\\hat{\\varepsilon}}=\\hat{\\mu}_{\\hat{\\varepsilon}}^{(\\dagger)}q_{\\hat{\\varepsilon}}^{-1}-\\operatorname*{max}_{\\hat{\\varepsilon}}V_{\\hat{\\varepsilon}}^{-1}\\left(\\operatorname*{mrx}_{\\hat{\\varepsilon}}-q_{\\hat{\\varepsilon}}^{\\dagger}\\xi_{\\hat{\\varepsilon}}-m_{\\hat{\\varepsilon}}\\right)\\xi_{\\hat{\\varepsilon}}^{\\dagger}q_{\\hat{\\varepsilon}}^{-\\frac{1}{2}}}\\\\ &{\\left\\lVert\\dot{\\hat{\\eta}}_{\\hat{\\varepsilon}}=\\alpha\\mathbb{E}_{\\hat{\\varepsilon},\\hat{\\eta}}V_{\\hat{\\varepsilon}}^{-1}\\left(\\operatorname*{proc}_{\\varepsilon}-\\hat{q}_{\\hat{\\varepsilon}}^{\\dagger}\\xi_{\\hat{\\varepsilon}}-m_{\\hat{\\varepsilon}}\\right)\\right\\rVert}\\\\ &{\\left\\lVert\\dot{\\hat{\\theta}}_{\\hat{\\varepsilon}}=\\alpha\\mathbb{E}_{\\hat{\\varepsilon},\\hat{\\eta}}V_{\\hat{\\varepsilon}}^{-1}\\left(\\operatorname*{proc}_{\\varepsilon}-\\hat{q}_{\\hat{\\varepsilon}}^{\\dagger}\\xi_{\\hat{\\varepsilon}}-m_{\\hat{\\varepsilon}}\\right)\\left(y_{\\hat{\\varepsilon}}-\\xi_{\\hat{\\varepsilon}}^{\\dagger}q_{\\hat{\\varepsilon}}^{-1}\\theta_{\\hat{\\varepsilon}}\\right)^{-1}\\left(\\rho_{\\hat{\\varepsilon}}-\\partial_{\\hat{\\varepsilon}}^{\\dagger}q_{\\hat{\\varepsilon}}^{-1}\\theta_{\\hat{\\varepsilon}}\\right)^{-1}}\\\\ &{\\left\\lVert\\dot{q}_{\\hat{\\varepsilon}}=\\int\\partial\\phi(\\gamma,\\tau,\\tau)\\gamma_{\\varepsilon}\\left(\\lambda_{\\hat{\\varepsilon}}^{-1}\\sum_{s=1}^{\\frac{L}{\\gamma_{\\varepsilon}}}\\gamma_{s}\\tilde{V}_{s}\\right)^{-1}\\left(\\sum_{s=1}^{L}\\gamma_{s}\\tilde{\\hat{\\eta}}_{s}+\\left(\\displaystyle\\sum_{s=1}^{L}\\hat{m}_\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which finishes to recover (7). Let us finally mention that the update equations (7) for the summary statistics (6) do not describe the dynamics of gradient descent, but rather that of an Approximate Message Passing algorithm [63], which we elicit in Appendix B for completeness. \u53e3 ", "page_idx": 31}, {"type": "text", "text": "C.8 test MSE ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "The generalization performance is measured by the test error ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\epsilon_{g}\\equiv\\frac{1}{L}\\mathbb{E}_{\\mathcal{D}}\\mathbb{E}_{x}\\left\\|\\mathbf{T}\\left[\\frac{1}{\\sqrt{d}}x Q_{\\star}\\right]x-\\mathbf{S}\\left[\\frac{1}{\\sqrt{d}}(x+p)\\hat{Q}\\right](x+p)\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Expliciting this expression in terms of the correlated Gaussian variables $x Q_{\\star},x Q$ allows to straightforwardly show that $\\epsilon_{g}$ admits the sharp asymptotic characterization in terms of the summary statistics characterized by (97): ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\epsilon_{g}=\\frac{1}{L}\\mathbb{E}_{X}\\operatorname{Tr}\\big[{\\mathtt{S}}[X]\\rho{\\mathtt{\\Sigma}}{\\mathtt{S}}[X]^{\\top}\\big]+\\frac{1}{L}\\mathbb{E}_{Y}\\operatorname{Tr}\\big[{\\mathtt{T}}[Y]\\rho{\\mathtt{\\Sigma}}{\\mathtt{T}}[Y]^{\\top}\\big]-2\\frac{1}{L}\\mathbb{E}_{X,Y}\\operatorname{Tr}\\big[{\\mathtt{S}}[X]\\rho{\\mathtt{\\Sigma}}{\\mathtt{T}}[Y]^{\\top}\\big],\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the average bears on $X\\in\\mathbb{R}^{L\\times r},Y\\in\\mathbb{R}^{L\\times t}$ with independent rows with statistics ", "page_idx": 31}, {"type": "equation", "text": "$$\n(x_{\\ell},y_{\\ell})\\sim\\mathcal{N}\\left[\\left(\\!\\!\\begin{array}{c}{{m}}\\\\ {{0}}\\end{array}\\!\\!\\right),\\left(\\!\\!\\frac{q_{\\ell}}{\\theta_{\\ell}^{'}}\\end{array}\\!\\!\\mid\\theta_{\\ell}\\!\\!\\right)\\right]\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "C.9  Training loss ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We finally turn to the training loss. It is reasonable to expect, from statistical physics, that the training loss should be equal to the free energy $-\\Phi$ at zero temperature. We provide below an alternative derivation, for simplicity in the case of $\\ell_{2}$ regularization $\\dot{g}=1/2\\|\\cdot\\|^{2}$ . First note that the training loss $\\epsilon_{t}$ can be expressed as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\epsilon_{t}=-\\operatorname*{lim}_{\\beta\\rightarrow\\infty}\\partial_{\\beta}\\,\\underbrace{\\frac{1}{d}\\ln Z(\\beta)}_{\\Phi(\\beta)}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Where $\\Phi(\\beta)$ is the free entropy at finite temperature. The trace potential $\\Psi_{t}$ bears no explicit dependence on $\\beta$ . On the other hand, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle3{\\Psi}_{Q}=-\\frac{1}{2}\\ln\\operatorname*{det}\\left[\\beta\\lambda{\\mathbb I}_{r}\\otimes{\\mathbb I}_{d}+\\displaystyle\\sum_{\\ell=1}^{L}\\hat{V}_{\\ell}\\otimes\\Sigma_{\\ell}\\right]}}\\\\ {{\\displaystyle\\qquad+\\frac{1}{2d}{\\mathbb T}\\left[\\left(\\beta\\lambda{\\mathbb I}_{r}\\odot{\\mathbb I}_{d}+\\displaystyle\\sum_{\\ell=1}^{L}\\hat{V}_{\\ell}\\otimes\\Sigma_{\\ell}\\right)^{-1}\\left(\\displaystyle\\sum_{\\ell=1}^{L}\\hat{q}_{\\ell}\\otimes\\Sigma_{\\ell}+\\left(\\displaystyle\\sum_{\\ell=1}^{L}(\\sqrt{d}\\hat{m}_{\\ell}p_{\\ell}^{\\top}+\\hat{\\theta}_{\\ell}Q_{\\star}^{\\top}\\Sigma_{\\ell}\\right)^{\\otimes2}\\right)\\right)\\right]^{\\displaystyle-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Thus ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle3\\Psi_{Q})=-\\,\\frac{\\lambda}{2}\\,\\mathrm{Tr}\\,\\Bigg[\\beta\\lambda\\mathbb{I}_{r}\\otimes\\mathbb{I}_{d}+\\displaystyle\\sum_{\\ell=1}^{L}\\hat{V}_{\\ell}\\otimes\\Sigma_{\\ell}\\Bigg]^{-1}}\\\\ {\\displaystyle\\qquad-\\,\\frac{\\lambda}{2d}\\,\\mathrm{Tr}\\Bigg[\\Bigg(\\beta\\lambda\\mathbb{I}_{r}\\odot\\mathbb{I}_{d}+\\displaystyle\\sum_{\\ell=1}^{L}\\hat{V}_{\\ell}\\otimes\\Sigma_{\\ell}\\Bigg)^{-2}\\odot\\left(\\displaystyle\\sum_{\\ell=1}^{L}\\hat{q}_{\\ell}\\otimes\\Sigma_{\\ell}+\\left(\\displaystyle\\sum_{\\ell=1}^{L}(\\sqrt{d}\\hat{m}_{\\ell}p_{\\ell}^{\\top}+\\hat{\\theta}_{\\ell}Q_{\\star}^{\\top}\\Sigma_{\\ell}\\right)^{\\mathrm{c}}\\right.\\Bigg.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Finally, going through the same rescaling steps to take the $\\beta\\rightarrow\\infty$ limit, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\to\\infty}\\partial_{\\beta}\\big(\\beta\\Psi_{Q}\\big)=-\\frac{\\lambda}{2d}\\operatorname{Tr}\\left[\\left(\\lambda\\mathbb{I}_{r}\\odot\\mathbb{I}_{d}+\\sum_{\\ell=1}^{L}\\hat{V}_{\\ell}\\otimes\\Sigma_{\\ell}\\right)^{-2}\\big)\\overset{L}{\\odot}\\left(\\sum_{\\ell=1}^{L}\\hat{q}_{\\ell}\\otimes\\Sigma_{\\ell}+\\left(\\sum_{\\ell=1}^{L}(\\sqrt{d}\\hat{m}_{\\ell}p_{\\ell}^{\\top}+\\hat{\\theta}_{\\ell}Q)\\right)\\right)\\right]\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By the same token, it is straightforward to see that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\beta\\to\\infty}\\partial_{\\beta}(\\beta\\Psi_{y})=-\\mathbb{E}_{Y,\\Xi}\\left[\\mathcal{M}(Y,\\Xi)-\\frac{1}{2}\\sum_{\\ell=1}^{L}\\mathrm{Tr}\\left[\\underbrace{V_{\\ell}^{-1}\\left(x_{\\ell}-q_{\\ell}^{1/2}\\xi_{\\ell}-m_{\\ell}\\right)^{\\otimes2}}_{\\hat{q}_{\\ell}V_{\\ell}}\\right]\\right]\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We used the self-consistent equations (7) to identify the term in underbrace. Putting everything together, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle-\\epsilon_{t}=\\operatorname*{lim}_{\\beta\\rightarrow\\infty}\\partial_{\\beta}\\Psi(\\beta)=-\\,\\frac{\\lambda}{2}\\int d\\nu(\\gamma,\\tau)\\,\\mathrm{Tr}\\left[\\left(\\lambda+\\sum_{\\ell=1}^{L}\\gamma_{\\ell}\\hat{V}_{\\ell}\\right)^{-1}\\left(\\displaystyle\\sum_{\\ell=1}^{L}\\gamma_{\\ell}\\hat{q}_{\\ell}+\\left(\\displaystyle\\sum_{\\ell=1}^{L}\\tau_{\\ell}\\hat{m}_{\\ell}+\\hat{\\theta}_{\\ell}\\cdot\\pi\\right)^{\\otimes}\\right.\\right.}}\\\\ {{\\displaystyle\\left.\\left.-\\,\\alpha\\mathbb{E}_{Y,\\Xi}\\left[M(Y,\\Xi)\\right]+\\frac{1}{2}\\sum_{\\ell=1}^{L}\\mathrm{Tr}[\\hat{q}_{\\ell}V_{\\ell}].\\right.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This constitutes a sharp asymptotic characterization of the training loss $\\epsilon_{t}$ as a function of the summary statistics characterized in Result 4.2. ", "page_idx": 32}, {"type": "text", "text": "For completeness, we finally explicit the connection between $\\epsilon_{t}$ and the negative free entropy (i.e. the free energy in statistical physics). We go back to massage the expression for the free entropy ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{\\partial}_{t}\\sum_{i=1}^{\\infty}\\!\\!\\!\\!\\frac{1}{2}\\!\\!\\!\\frac{\\partial}{\\partial\\alpha_{i}}\\!\\left(\\!\\!\\nabla_{\\xi i}\\nabla_{\\xi i}\\!-\\!\\boldsymbol{\\mathrm{Tr}}_{0}\\boldsymbol{\\mathrm{d}}_{\\xi}\\!\\right)\\!\\!-\\!\\!\\frac{\\boldsymbol{\\mathrm{L}}_{\\xi}}{\\nu_{i}!}\\boldsymbol{\\mathrm{d}}_{\\xi^{\\prime}n_{i}}\\!\\!-\\!\\frac{\\boldsymbol{\\mathrm{L}}_{\\xi}}{\\nu_{i}!}\\boldsymbol{\\mathrm{d}}_{\\xi^{\\prime}\\ell_{i}}^{\\alpha}\\!-\\!\\frac{\\boldsymbol{\\mathrm{L}}_{\\xi}}{\\nu_{i}!}\\boldsymbol{\\mathrm{d}}_{\\xi^{\\prime}\\ell_{i}}^{\\alpha}\\!-\\!\\frac{\\boldsymbol{\\mathrm{d}}_{\\xi^{\\prime}\\ell}}{\\nu_{i}!}\\boldsymbol{\\mathrm{d}}_{\\xi^{\\prime}\\ell_{i}}^{\\alpha}\\!-\\!\\frac{\\boldsymbol{\\mathrm{L}}_{\\xi}}{\\nu_{i}!}\\boldsymbol{\\mathrm{d}}(\\boldsymbol{\\mathrm{Tr}}_{0}\\!-\\!\\boldsymbol{\\mathrm{Tr}}_{0}\\!+\\!\\boldsymbol{\\mathrm{Tr}}_{0}\\!)\\boldsymbol{\\mathrm{d}}(\\boldsymbol{\\mathrm{Tr}}_{\\xi})\\!-\\!\\frac{\\boldsymbol{\\mathrm{c}}}{\\nu_{i}!}}}\\\\ &{\\quad+\\frac{1}{2}\\int_{\\xi}\\!\\!\\!\\int_{\\xi}\\!\\!\\!\\int_{\\xi}\\!\\!\\!\\!\\left(\\boldsymbol{\\mathrm{x}}_{\\xi}\\!-\\!\\frac{\\boldsymbol{\\mathrm{Tr}}}{\\xi_{i}!}\\boldsymbol{\\mathrm{d}}_{\\xi}\\right)\\!\\!\\!\\!\\frac{1}{2}\\!\\!\\!\\left(\\displaystyle\\left(\\frac{\\boldsymbol{\\mathrm{L}}_{\\xi}}{\\displaystyle\\sum_{i=1}^{k}\\boldsymbol{\\mathrm{\\xi}}_{i}}\\!\\!-\\!\\frac{\\boldsymbol{\\mathrm{t}}_{\\xi}}{\\nu_{i}!}\\boldsymbol{\\mathrm{d}}_{\\xi}^{\\prime}\\right)^{-1}\\!\\left(\\displaystyle\\left(\\frac{\\boldsymbol{\\mathrm{L}}_{\\xi}}{\\displaystyle\\sum_{i=1}^{k}\\boldsymbol{\\mathrm{\\xi}}_{i}}\\!\\!+\\!\\left(\\displaystyle\\frac{\\boldsymbol{\\mathrm{L}}_{\\xi}}{\\displaystyle\\sum_{i=1}^{k}\\boldsymbol{\\mathrm \n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "C.10  Extensions ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "For the sake of clarity and definiteness, we stated the technical results for in the simplest instance, under notably the symplifying assumptions of independent tokens, tied key and query weight matrices, as exposed in Section 3 of the main text. These two assumptions can in fact be relaxed. In this final subsection of Appendix C, we discuss how the characterization of Result 4.2 can be generalized to more complex data distributions and attention architectures. ", "page_idx": 33}, {"type": "text", "text": "Data model  Let us consider a data distribution allowing for statistical correlations between different tokens. A simple model meeting this criterion, proposed in [73], consists in assuming every token (row) $x_{\\ell}$ is drawn from aGaussian mixturewith $K_{\\ell}$ withrespectiveprobabilities $\\rho_{\\ell,k}$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\pmb{x}_{\\ell},c_{\\ell}\\sim\\sum_{k=1}^{K_{\\ell}}\\rho_{\\ell,k}\\delta_{c_{\\ell},k}\\mathcal{N}(\\mu_{\\ell,k},\\Sigma_{\\ell,k}).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "In (108), the random variable $c_{\\ell}\\in[K_{\\ell}]$ indicates the index of the cluster the $\\ell-$ th token is drawn from. Note that we generically aliow for different rows to be sampled from distinct mixtures. In order to introduce correlations between different tokens, we further assume that the variables ${\\boldsymbol{c}}=(c_{1},...,c_{L})$ follow a generic joint law $\\rho(c)$ . Intuitively, the different models can model different types of words (e.g. verbs, pronouns, adjectives), and the nature of the words, as reflected by the variable $c_{\\ell}$ , is correlated across tokens. We assume that $\\|\\mu_{\\ell,k}\\|\\!=\\Theta_{d}(1)$ , i.e. are of the same order as the positional encodings. ", "page_idx": 33}, {"type": "text", "text": "Assumption C.1.Finally, similarly to Assumption 4.1,let us assume that the set of matrices $\\{\\{\\Sigma_{\\ell,k}\\}_{k=1}^{K_{\\ell}}\\}_{\\ell=1}^{L}$ admits a common set of eingevectors $\\{e_{i}\\}_{i=1}^{d}$ with eigenvalues $\\{\\lambda_{i}^{\\ell,k}\\}_{i=1}^{d}$ .The eigenvalues $\\{\\lambda_{i}^{\\ell,k}\\}_{\\ell,k,i}$ and the projection of the cluster means (to which the positional encodings are added) $\\{\\mu_{\\ell,k}+p_{\\ell}\\}_{\\ell,k}$ and the teacher columns $\\{Q_{i}^{\\star}\\}_{i=1}^{r_{t}}$ onthese eigenvectors are assumed to admit $a$ well-defined joint distribution $\\nu$ as $d\\rightarrow\\infty-n a r$ nely,for $\\gamma=(\\bar{\\gamma}\\ell,k)\\ell,k$ $\\pi=(\\pi_{1},...,\\pi_{t})\\in\\mathbb{R}^{r_{t}}$ \uff0c $\\tau=(\\tau_{\\ell,k})_{\\ell,k}$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{1}{d}\\sum_{i=1}^{d}\\prod_{\\ell=1}^{L}\\prod_{k=1}^{K_{\\ell}}\\delta\\left(\\lambda_{i}^{\\ell,k}-\\gamma_{\\ell,k}\\right)\\delta\\left(\\sqrt{d}e_{i}^{\\top}(\\mu_{\\ell,k}+p_{\\ell})-\\tau_{\\ell,k}\\right)\\prod_{j=1}^{r_{\\ell}}\\delta\\left(e_{i}^{\\top}Q_{j}^{\\star}-\\pi_{j}\\right)\\xrightarrow{d\\rightarrow\\infty}\\nu\\left(\\gamma,\\tau,\\pi\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Architecture  Again in the interest of further generality, we alleviate the architectural restrictions on the attention (2), and allow for untied key and query weight matrices. More precisely, let us consider function of theform ", "page_idx": 33}, {"type": "equation", "text": "$$\nf_{Q,K}(x)=\\Im\\left[\\frac{1}{\\sqrt{d}}(x+p)Q,\\frac{1}{\\sqrt{d}}(x+p)K\\right](x+p),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "parameterized by two set of weights $Q,K\\in\\mathbb{R}^{d\\times r_{s}}$ . In order to work with more compact expression, let us introduce the total weight matrix $W\\in\\mathbb{R}^{d\\times(r_{s}+r_{s})}$ , obtained as the horizontal concatenation of $Q,K$ . Then (110) can be written compactly as ", "page_idx": 33}, {"type": "equation", "text": "$$\nf_{K,Q}(x)=g_{W}(x)\\equiv\\mathtt{G}\\left[\\frac{1}{\\sqrt{d}}(x+p)W\\right](x+p),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "whereforanyargument $h\\in\\mathbb{R}^{L\\times(r_{s}+r_{s})}$ ,wiewed as a $1\\times2$ blockmatrixwithtwoblocks $h_{1},h_{2}\\in$ $\\mathbb{R}^{L\\times r}$ $\\mathtt{G}(h)=\\mathtt{S}(h_{1},h_{2})$ ", "page_idx": 33}, {"type": "text", "text": "Asymptotic characterization A sharp asymptotic characterization of the test error and train loss for this more general model can be derived along the same lines, as presented in Appendix C (see also [73]). For the sake of conciseness, we only report the final result. ", "page_idx": 33}, {"type": "text", "text": "Result C.2. Under Assumption C.1, in the limit $n,d\\rightarrow\\infty,\\,\\|\\pmb{p}\\|,\\stackrel{n}{n}/d,L,r_{s},r_{t}=\\Theta_{d}(1),$ thesummary statistics ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\rho_{\\ell,k}\\equiv\\frac{Q_{\\star}^{\\top}\\Sigma_{\\ell,k}Q_{\\star}}{d}\\in\\mathbb{R}^{r_{t}\\times r_{t}},}&&{\\qquad\\quad q_{\\ell,k}\\equiv\\frac{\\hat{W}^{\\top}\\Sigma_{\\ell,k}\\hat{W}}{d}\\in\\mathbb{R}^{2r_{s}\\times2r_{s}},}\\\\ &{m_{\\ell,k}\\equiv\\frac{\\hat{W}^{\\top}\\left(\\mu_{\\ell,k}+p_{\\ell}\\right)}{d}\\in\\mathbb{R}^{2r_{s}},}&&{\\qquad\\quad\\theta_{\\ell,k}\\equiv\\frac{\\hat{W}^{\\top}\\Sigma_{\\ell,k}Q_{\\star}}{d}\\in\\mathbb{R}^{2r_{s}\\times r_{t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "concentrate in probability, and are solutions of the set of finite-dimensional self-consistent equations ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\widehat{q}_{\\ell,k}=\\alpha\\mathbb{E}_{c}\\delta_{c\\ell,k}\\mathbb{E}_{\\Xi,Y}V_{\\ell,k}^{-1}\\left(\\operatorname{prox}_{\\ell}^{c}-q_{\\ell,k}^{\\frac{1}{2}}\\xi_{\\ell}-m_{\\ell,k}\\right)^{\\otimes2}V_{\\ell,k}^{-1}\\right.}\\\\ &{\\left.\\kern-\\nulldelimiterspace\\hat{V}_{\\ell,k}=\\widehat{\\theta}_{\\ell,k}\\theta_{\\ell,k}^{\\top}q_{\\ell,k}^{-1}-\\alpha\\mathbb{E}_{c}\\delta_{c_{\\ell,k}}\\mathbb{E}_{\\Xi,Y}V_{\\ell,k}^{-1}\\left(\\operatorname{prox}_{\\ell}^{c}-q_{\\ell,k}^{\\frac{1}{2}}\\xi_{\\ell}-m_{\\ell,k}\\right)\\xi_{\\ell}^{\\top}q_{\\ell,k}^{-\\frac{1}{2}}}\\\\ &{\\left.\\kern-\\nulldelimiterspace\\hat{m}_{\\ell,k}=\\alpha\\mathbb{E}_{c}\\delta_{c_{\\ell,k}}\\mathbb{E}_{\\Xi,Y}V_{\\ell,k}^{-1}\\left(\\operatorname{prox}_{\\ell}^{c}-q_{\\ell,k}^{\\frac{1}{2}}\\xi_{\\ell}-m_{\\ell,k}\\right)\\right.}\\\\ &{\\left.\\kern-\\nulldelimiterspace\\hat{\\theta}_{\\ell,k}=\\alpha\\mathbb{E}_{c}\\delta_{c_{\\ell,k}}\\mathbb{E}_{\\Xi,Y}V_{\\ell,k}^{-1}\\left(\\operatorname{prox}_{\\ell}^{c}-q_{\\ell,k}^{\\frac{1}{2}}\\xi_{\\ell}-m_{\\ell,k}\\right)\\left(y_{\\ell}-\\theta_{\\ell,k}^{\\top}q_{\\ell,k}^{-1/2}\\xi_{\\ell}\\right)^{\\top}\\left(\\rho_{\\ell,k}-\\theta_{\\ell,k}^{\\top}q_{\\ell,k}^{-1}\\theta_{\\ell,k}\\right)^{-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\begin{array}{l l}{\\displaystyle q_{\\ell,k}=\\int d\\boldsymbol{b}(\\gamma,\\tau,\\boldsymbol{\\pi})\\gamma_{\\ell,k}\\left(\\lambda\\mathrm{I2}_{\\mathcal{T}_{s}}+\\sum_{\\kappa}\\sum_{j}\\gamma_{\\kappa,j}\\hat{V}_{\\kappa,j}\\right)^{-1}}\\\\ {\\displaystyle\\left[\\left(\\sum_{\\kappa}\\sum\\hat{m}_{\\kappa,j}\\tau_{\\kappa,j}+\\gamma_{\\kappa,j}\\hat{\\theta}_{\\kappa,j}\\pi\\right)^{\\infty}\\right]+\\sum_{\\kappa}\\sum_{j}\\sum\\gamma_{\\kappa,j}\\hat{\\phi}_{\\kappa,j}\\right]\\left(\\lambda\\mathrm{I2}_{\\mathcal{T}_{s}}+\\sum_{\\kappa}\\sum\\gamma_{\\kappa,j}\\hat{V}_{\\kappa,k}\\right)^{-1}}\\\\ {\\displaystyle V_{\\ell,k}=\\int d\\boldsymbol{b}(\\gamma,\\tau,\\boldsymbol{\\pi})\\gamma_{\\ell,k}\\left(\\lambda\\mathrm{I2}_{\\mathcal{T}_{s}}+\\sum_{\\kappa}\\sum_{j}\\gamma_{\\kappa,j}\\hat{V}_{\\kappa,j}\\right)^{-1}}\\\\ {\\displaystyle m_{\\ell,k}=\\int d\\boldsymbol{b}(\\gamma,\\tau,\\boldsymbol{\\pi})\\tau_{\\ell,k}\\left(\\lambda\\mathrm{I2}_{\\mathcal{T}_{s}}+\\sum_{\\kappa}\\sum_{j}\\gamma_{\\kappa,j}\\hat{V}_{\\kappa,j}\\right)^{-1}\\left(\\sum_{\\kappa}\\sum\\hat{m}_{\\kappa,j}\\tau_{\\kappa,j}+\\gamma_{\\kappa,j}\\hat{\\theta}_{\\kappa,j}\\pi\\right)}\\\\ {\\displaystyle\\theta_{\\ell,k}=\\int d\\boldsymbol{b}(\\gamma,\\tau,\\boldsymbol{\\pi})\\gamma_{\\ell,k}\\left(\\lambda\\mathrm{I2}_{\\mathcal{T}_{s}}+\\sum_{\\kappa}\\sum_{j}\\gamma_{\\kappa,j}\\hat{V}_{\\kappa,j}\\right)^{-1}\\left(\\sum_{\\kappa}\\sum_{j}\\hat{m}_{\\kappa,j}\\tau_{\\kappa,j}+\\gamma_{\\kappa,j}\\hat{\\theta}_{\\kappa,j}\\pi\\right)\\pi^{\\top}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The averages bears over $\\Xi\\in\\mathbb{R}^{L\\times2r_{s}},Y\\in\\mathbb{R}^{L\\times r_{t}}$ , with rows $\\xi_{\\ell}\\sim\\mathcal{N}(0,\\mathbb{I}_{2r_{s}})$ independently and   \n$y_{\\ell}\\sim\\mathcal{N}(\\xi_{\\ell}^{\\top}q_{\\ell,c_{\\ell}}^{-1/2}\\theta_{\\ell,c_{\\ell}}\\rho_{\\ell,c_{\\ell}}-\\theta_{\\ell,c_{\\ell}}^{\\top}q_{\\ell,c_{\\ell}}^{-1}\\theta_{\\ell,c_{\\ell}})$ 00 mmu $c$ $\\stackrel{c}{\\boldsymbol{\\cdot}}\\!\\!\\!\\boldsymbol{\\ell}$ $\\mathcal{M}(c,Y,\\Xi)$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathrm{prox}^{c}=\\underset{X}{\\mathrm{arginf}}\\Biggl\\{\\frac{1}{2}\\sum_{\\ell=1}^{L}\\mathrm{Tr}\\biggl[V_{\\ell,c_{\\ell}}^{-1}\\left(X_{\\ell}-q_{\\ell,c_{\\ell}}^{1/2}\\xi_{\\ell}-m_{\\ell,c_{\\ell}}\\right)^{\\otimes2}\\biggr]+\\ell\\left(Y+m_{\\star}^{c},X,c\\right)\\Biggr\\}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We introduced $m_{\\star}^{c}\\in\\mathbb{R}^{L\\times r_{t}}$ , whose rows are ", "page_idx": 34}, {"type": "equation", "text": "$$\n(m_{\\star}^{c})_{\\ell}=\\frac{\\mu_{\\ell,c_{\\ell}}^{\\top}Q_{\\star}}{\\sqrt{d}},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and theshorthand ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\ell(y,x,c)=\\mathrm{Tr}\\big[\\mathsf{G}[x]\\rho_{\\Sigma}^{c}\\mathsf{G}[x]^{\\top}\\big]-2\\,\\mathrm{Tr}\\big[\\mathtt{T}[y]\\rho_{\\Sigma}^{c}\\mathsf{G}[x]^{\\top}\\big],\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\rho_{\\Sigma}^{c}\\in\\mathbb{R}^{L\\times L}$ is diagonal with elements ", "page_idx": 34}, {"type": "equation", "text": "$$\n(\\rho_{\\Sigma}^{c})_{\\ell\\ell}=\\int d\\nu(\\gamma,\\tau,\\pi)\\gamma_{\\ell,c_{\\ell}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Test error The test error admits the sharp asymptotic characterization ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\epsilon_{g}=\\frac{1}{L}\\mathbb{E}_{c}\\mathbb{E}_{h,h^{\\star}}\\left[\\mathrm{Tr}\\big[\\mathtt{T}[h^{\\star}]\\rho_{\\Sigma}^{c}\\mathtt{T}[h^{\\star}]^{\\top}\\big]+\\mathrm{Tr}\\big[\\mathtt{G}[h]\\rho_{\\Sigma}^{c}\\mathtt{G}[h]^{\\top}\\big]-2\\,\\mathrm{Tr}\\big[\\mathtt{T}[h^{\\star}]\\rho_{\\Sigma}^{c}\\mathtt{G}[h]^{\\top}\\big]\\right],\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where, conditioned on the class assignments $c,$ the average bears on $h\\in\\mathbb{R}^{L\\times2r_{s}},Y\\in\\mathbb{R}^{L\\times r_{t}}$ with independent rows with statistics ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left(h_{\\ell},h_{\\ell}^{\\star}\\right)\\sim\\mathcal{N}\\left[\\left(\\frac{m_{\\ell,c_{\\ell}}}{m_{\\ell,c_{\\ell}}^{\\star}}\\right),\\left(\\frac{q_{\\ell,c_{\\ell}}}{\\theta_{\\ell,c_{\\ell}}^{\\top}}\\left|\\begin{array}{l}{\\theta_{\\ell,c_{\\ell}}}\\\\ {\\rho_{\\ell,c_{\\ell}}}\\end{array}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Finally, the train loss $\\epsilon_{t}$ converges in probability to ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\ t=\\frac{\\lambda}{2}\\!\\int d\\nu(\\gamma,\\tau,\\pi)\\,\\mathrm{Tr}\\!\\left[\\left(\\lambda\\mathbb{I}_{2r_{s}}+\\hat{v}+\\sum_{\\kappa}\\sum_{k}\\gamma_{\\kappa,k}\\hat{V}_{\\kappa,k}\\right)^{-2}\\left[\\left(\\sum_{\\kappa}\\sum_{k}\\hat{m}_{\\kappa,k}\\tau_{\\kappa,k}\\hat{v}_{\\kappa,k}\\pi\\right)_{\\kappa}^{\\otimes2}\\!\\sum_{\\kappa}\\gamma_{\\kappa,k}\\hat{q}_{\\kappa,k}\\right.}\\right.}}\\\\ {\\displaystyle{\\left.\\ +\\,\\alpha\\mathbb{E}_{c,Y,\\Xi}\\mathcal{M}(c,Y,\\Xi)-\\frac{1}{2}\\sum_{\\ell=1}^{L}\\sum_{k=1}^{K_{\\ell}}\\mathrm{Tr}[\\hat{q}_{\\ell,k}V_{\\ell,k}].\\right.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Trainable value matrix  Finally, let us remark that it should be possible to leverage a similar derivation to further accommodate trainable value matrices $V$ , provided they are low rank, i.e. in $\\mathbb{R}^{d\\times r_{V}}$ with $r_{V}=\\Theta_{d}(1)$ as $d\\to\\infty$ . Similarly, introducing the stacked weights $W=(Q|K|V)\\in$ $\\mathbb{R}^{d\\times(2r_{s}+r_{V})}$ allows to perform the asymptotic analysis Since this architectural modification then implies that $f$ now takes values in $\\mathbb{R}^{\\check{L}\\times\\dot{r}_{V}}$ , one cannot immediately analyze this architecture for the task consideredin the presentmanuscript, as the labels are valued in $\\mathbb{R}^{\\bar{L}\\times d}$ , and the setting and questions explored would need to be entirely modified. We thus postpone a thorough analysis of the effect of a trained value matrix to future investigations. ", "page_idx": 35}, {"type": "text", "text": "D Derivation of Result 5.1 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In this Appendix we derive the asymptotic characterization of the learning performance of the dense linear baseline (15), as stated in Result 5.1. Consider the empirical risk minimization (16) ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{R}(W)=\\frac{1}{n}\\sum_{\\mu=1}^{n}\\|A^{\\star}(x^{\\mu})x^{\\mu}-W x^{\\mu}\\|^{2}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where we use the shorthand notation $A^{\\star}(x)\\equiv\\mathrm{T}[1/\\sqrt{d}x Q_{\\star}]$ for the target attention score matrix (1). The expression for the risk can be asymptotically simplified as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}(W)=\\displaystyle\\frac{1}{n}\\sum_{\\mu=1}^{n}\\left[A^{\\star}(x^{\\mu})\\rho_{\\Sigma}A^{\\star}(x^{\\mu})^{\\top}-\\left(A^{\\star}(x^{\\mu})\\rho_{\\Sigma}W^{\\top}+\\mathrm{h.c}\\right)+W\\rho_{\\Sigma}W^{\\top}+o(1/\\sqrt{a})\\right]}\\\\ &{\\quad\\quad\\approx\\mathbb{E}_{x}\\left[A^{\\star}(x)\\rho_{\\Sigma}A^{\\star}(x)^{\\top}\\right]-\\left(W\\rho_{\\Sigma}\\mathbb{E}_{x}\\left[A^{\\star}(x)\\right]^{\\top}+\\mathrm{h.c}\\right)+W\\rho_{\\Sigma}W^{\\top}}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}_{x}\\left[A^{\\star}(x)\\rho_{\\Sigma}A^{\\star}(x)^{\\top}\\right]-\\mathbb{E}_{x}\\left[A^{\\star}(x)\\right]\\rho_{\\Sigma}\\mathbb{E}_{x}\\left[A^{\\star}(x)\\right]^{\\top}+\\left\\|\\rho_{\\Sigma}^{1/2}\\left(W-\\mathbb{E}_{x}\\left[A^{\\star}(x)\\right]\\right)\\right\\|_{-1}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Therefore the learnt weight $\\hat{W}$ is simply equal to ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\hat{W}=\\mathbb{E}_{x}\\left[A^{\\star}(x)\\right]=\\mathbb{E}_{x}\\left[\\mathrm{T}\\big[1/\\sqrt{d}x Q_{\\star}\\big]\\right].\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Naming $h={^1}/\\sqrt{d}x Q_{\\star}$ the argument of the last term, the matrix $h$ possesses independent rows with statistics ", "page_idx": 35}, {"type": "equation", "text": "$$\nh_{\\ell}\\sim\\mathcal{N}(0,\\rho_{\\ell})\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where we remind that $\\rho_{\\ell}\\equiv{^1/}d Q_{\\star}^{\\top}\\Sigma_{\\ell}Q_{\\star}$ Therefore, the trained weights $\\hat{W}$ obtained by minimizing the empirical loss also coincide with the minimizer of the population loss. Intuitively, this follows from the fact that in the asymptotic limit considered $n,d\\to\\infty$ and $L=\\Theta_{d}(1)$ , training $W$ ,i.e. a number $L^{2}=\\Theta_{d}(1)$ of parameters on the empirical loss for $n\\ll L^{2}$ data points is equivalent asymptotically to directly training on the population loss. Expliciting the corresponding test MSE, one reaches the sharp asymptotic characterization ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\epsilon_{g}^{\\mathrm{lin}}=\\!\\frac{1}{L}\\operatorname{Tr}\\!\\left[\\hat{W}\\rho_{\\Sigma}\\hat{W}^{\\top}\\right]+\\frac{1}{L}\\mathbb{E}_{h}\\operatorname{Tr}\\!\\left[\\mathbb{T}[h]\\rho_{\\Sigma}\\mathbb{T}[h]^{\\top}\\right]}\\\\ &{\\qquad-\\left.\\frac{2}{L}\\mathbb{E}_{h}\\operatorname{Tr}\\!\\left[\\hat{W}\\rho_{\\Sigma}\\mathbb{T}[h]^{\\top}\\right]\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We close this Appendix by giving a few examples for definiteness. ", "page_idx": 36}, {"type": "text", "text": "D.1 Examples ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Purely positional target  Let us consider the case where the target is purely positional, i.e. $\\mathbb{T}[x]=A$ for all $x$ . This corresponds to the $\\omega=1$ limit of the target (14). It then follows from Result 5.1 that the dense linear layer recover perfectly the target weights $\\hat{W}=A$ ", "page_idx": 36}, {"type": "text", "text": "Target (14)  For the target discussed in the main text (14), ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\hat{W}=(1-\\omega)\\mathbb{E}_{h}\\mathrm{softmax}(h h^{\\top})+\\omega A\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "with $h$ and having independent rows $h_{\\ell}\\sim\\mathcal{N}(0,\\rho_{\\ell})$ ", "page_idx": 36}, {"type": "text", "text": "E Supplementary Experiments ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "E.1  Empirical scaling of $\\alpha=d/n$ ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In the following we verify that our experiments are consistent with the scaling behaviour predicted from the theory. We jointly increase $d$ and $n$ for a fixed value of $\\alpha$ . In Fig. 7 we indeed observe the expected behaviourfor an exemplaryvalue of $\\alpha=2$ . The same holds for the summary statistics $\\theta$ and $m$ , which concentrate as $d$ and $n$ jointly grow, shown in Fig. 3 (left) in the main text. ", "page_idx": 36}, {"type": "image", "img_path": "BFWdIPPLgZ/tmp/c127668629a5caf7478aae25c7e267160c70ec5b6712a0a08e6e360a95c1307f.jpg", "img_caption": ["Figure 7: Scaling $d$ and $n$ jointly for $\\alpha=1.5$ approaches the theoretical prediction of the generalization error of the positional and semantic local minima. Experimental settings as in Fig. 2, with 70 runs per datapoint. "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "E.2 Alternative hyperparameters ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We provide supplementary results for different parameter settings. Fig. 8 on the left shows more slices from the phase diagram that appears in the main Fig. 3. For the experimental section of the main text, we chose a specific $A$ for definiteness. In the following, we present the same results for a different $A$ with a stronger off-diagonal and a higher rank, ", "page_idx": 36}, {"type": "equation", "text": "$$\nA=\\binom{0.3}{0.8}\\binom{0.7}{0.2}\\,.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "In Fig. 9 we present the analogous simulations to Fig. 3 (center, right). While the global phenomena match the previous example, the details of the transitions location differ. ", "page_idx": 36}, {"type": "image", "img_path": "BFWdIPPLgZ/tmp/b2d29606a8f14d01d55fcf71c47116168772cf3235060e3c29940347767898bb.jpg", "img_caption": ["Figure 8: Alternative Parameters. Mixed positional/semantic teacher for $\\omega\\,=\\,0.3$ .Settings is $r_{s}\\,=\\,r_{t}\\,=\\,1,L\\,=\\,2,A\\,=\\,((0.6,0.4),(0.4,0.6))$ $\\Sigma_{1}\\,=\\,\\Sigma_{2}\\,=\\,0.25\\mathbb{I}_{d}$ $\\pmb{p}_{1}\\;=\\;\\mathbf{1}_{d}\\;=\\;-\\pmb{p}_{2}$ and $Q_{\\star}\\sim\\mathcal{N}(0,\\mathbb{I}_{d})$ . While keeping all other settings the same, we vary from left to right: The target positionality $\\omega$ , the student regularizer $\\lambda$ and the standard deviation $\\sigma$ (which is $0.5\\,=\\,\\sqrt{\\Sigma_{1}})$ Experiment settings as in Fig. 2. "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "BFWdIPPLgZ/tmp/86d4f5a5a5ab8d96ad58fdbd05fb73d40c7c295be1fc60c08ebd9963dc902510.jpg", "img_caption": ["Figure 9: Alternative Positional Matrix. $r_{s}=r_{t}=1,L=2$ $\\pmb{\\Sigma}_{1}=\\pmb{\\Sigma}_{2}=0.25\\mathbb{I}_{d}$ $p_{1}=-p_{2}$ and $\\bar{p_{1}},\\bar{Q}_{\\star}\\sim\\mathcal{N}(0,\\mathbb{I}_{d})$ independently. Here, we use a definite matrix $A$ from (128) , which differs form the one used in the main text. Experiments were conducted as in Fig. 2. "], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "E.3 More Complex Architectural Choices ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "As discussed previously in Section C.10, the theory developed in this work can be relaxed to incorporate arbitrary statistical correlations between tokens, changes in the (low-)rank student, generally untied key and query matrices and the presence of a low-rank trainable value matrix. This surplus in complexity comes at the cost of more intricate and heavier equations. ", "page_idx": 37}, {"type": "text", "text": "In the present section, we report experiments in the four settings above, but not the theoretical predictions. Fig. 10-13 compare the change in architecture with the setting considered in the main text, for a given $\\omega=0.5$ and varying sample complexity $\\alpha$ . The exhaustive experiments for varying $\\omega$ are shown in Fig. 14 - which immediately shows that the general idea of the phase transition from positional to semantic solution is consistent, but that the shape of the phase transition curve is influenced by the architecture. ", "page_idx": 37}, {"type": "text", "text": "For correlated inputs, the transitions seems to moves to larger values of $\\alpha$ (Fig. 10). For the case of a student with a rank-2 query matrix, and a teacher with a rank-1 query matrix, we consider three possible initilisations of the columns of $Q$ when finding minima of GD empirically. ", "page_idx": 37}, {"type": "text", "text": "Initializing both semantically, both positional, or one of them in each way, as shown in Fig. 11. Interestingly, the dual initialization seems to do approximately as well as the best of the two other initializations. The optimization generally seems more complex, as the phase diagram in the upper right corner of Fig. 14 are not entirely clean, and more noisy than empirical results from other architectures. ", "page_idx": 37}, {"type": "text", "text": "When we compare tied and untied $Q$ and $K$ weights in Fig. 12 we observe that the additional parameters we need to learn in the case of an extra $K$ come at a small cost, as the transition to the semantic solution is moved to the right. ", "page_idx": 37}, {"type": "text", "text": "Finally, for a value matrix (Fig. 13) the results seem to largely resemble the transition line we observed without the value matrix. ", "page_idx": 38}, {"type": "image", "img_path": "BFWdIPPLgZ/tmp/d856d1ec7b3866637c021ca46395804aa3dc77597c62e44ab3b0d8d42d4b9cd1.jpg", "img_caption": ["Figure 10: Uncorrelated vs. correlated inputs $(\\omega=0.3,\\sigma=0.5,\\lambda=0.001)$ . In the main text we sample all datapoints $\\mathbf{x}\\in\\mathbb{R}^{d\\times L=2}$ such that the columns are independent. We compare this setting with a correlated data structure with a hidden latent: We sample three vectors $u,v,w\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{1}),$ and set the first column of $x$ to be $(\\sigma u+\\gamma v)/\\sqrt{\\sigma+\\gamma}$ and the second one $(\\sigma w+\\gamma v)/\\sqrt{\\sigma+\\gamma}$ Experiments are repeated 5 times per data point with $d=1000$ "], "img_footnote": [], "page_idx": 38}, {"type": "image", "img_path": "BFWdIPPLgZ/tmp/63c8221177daff0b5b26d1ae5a44efceea0b1fee8baaafbd76e860c22c4e231a.jpg", "img_caption": ["Figure 11: Rank 1 vs. rank 2 student $(\\omega\\,=\\,0.3,\\sigma\\,=\\,0.5,\\lambda\\,=\\,0.001)$ .We comparedifferent initializations of the higher-rank student. Positional is when both columns of the student matrix $\\hat{Q}$ are initialized using the positional strategy. We do the same for the semantic strategy. The 'both' initialization initializes one column using the positional strategy and one using the semantic strategy. Experiments are repeated 5 times per data point with $d=1000$ "], "img_footnote": [], "page_idx": 38}, {"type": "image", "img_path": "BFWdIPPLgZ/tmp/f71c3228d65e19e96efe66f0c7d073f2794b8a55799dd87ed99bda87bc6cf9d2.jpg", "img_caption": ["Figure 12: Tied vs. independent weight $\\mathbf{Q}$ $\\mathbf{K}$ $(\\omega=0.3,\\sigma=0.5,\\lambda=0.001)$ . We compare the student setting from the paper, where the query and key matrices are bound to each other with the setting where we set them independently. We initialize with them both being either close to the positional or close to the semantic initialization. The phase transition for the semantic minimum dominating moves to the left, i.e. more samples are now needed. Experiments are repeated 5 times per data point with $d=1000$ "], "img_footnote": [], "page_idx": 38}, {"type": "image", "img_path": "BFWdIPPLgZ/tmp/95d033a1cdac928bd4b77a46917a0aa8794edf10bec2d231bf1f5f3aa97363a9.jpg", "img_caption": ["Figure 13: No value matrix vs. value matrix. $(\\omega=0.3,\\sigma=0.5,\\lambda=0.001)$ . We compare the setting from the main text with adding a value matrix, i.e. a trainable parameter $V\\in\\mathbf{R}^{d\\times d}$ This is applied to every embedding before they are averaged over using the attention matrix. We ran the experiment 5 times for each $\\alpha$ with $d=500$ , and rescaled the training error to compare to the experiments in the main, where $d=1000$ "], "img_footnote": [], "page_idx": 39}, {"type": "image", "img_path": "BFWdIPPLgZ/tmp/47e6cd0ed2252ca035ed248a6400ef0c5ed18b6c8be4bde626ef82b86c1d2a3d.jpg", "img_caption": ["Figure 14: Phase diagrams of the difference in training loss between the semantic and positional solutions from Fig. 10-13, in terms of $\\omega$ and the sample complexity $\\alpha$ : The green dashed line represents the theoretical phase transition which we obtained from the architecture as considered in the main text. "], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "E.4  Uninformed initialization and training via Adam ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "In or experiments, to obtain the empirical results, we initialize the GD optimizer in an informed fashion, i.e. initializing $Q_{\\star}$ of the student with $r=1$ as either $\\pmb{p}_{1}$ (positional) or $Q_{\\star}$ (semantics). GD then converges in the two local optima described by our theory. ", "page_idx": 39}, {"type": "text", "text": "Since our theory only ascertains that these solutions predicted are indeed fixed points of GD for large sizes, this does not have direct implications for other types of optimization algorithms. In Fig. 15 we show that indeed running the Adam optimizer from an uninformed initialization may lead one to either of the local minima for $d=100$ .For larger $d$ we observe the semantic minimum is reached less often than the positional minimum, and a considerable number of times the algorithm simply does not find either of them. ", "page_idx": 40}, {"type": "image", "img_path": "BFWdIPPLgZ/tmp/3f93abd45656b062e0920aa2ecd01fe57732c2223f2b61df984a684fe364e82c.jpg", "img_caption": ["Figure 15: Comparing $G D$ and Adam. Settings as in Fig. 2 for the sample complexity $\\alpha=2$ The student parameter $\\mathbf{Q}$ is obtained via either (left) positional and semantic informed initialization and (right) GD training from a random initialization are compared. Each point represents a single run. For the informed GD, we used the same optimization parameters as in Fig. 2 (24 runs per initialization). For Adam we trained on the same data, but for 2, 500 epochs with learning rate $\\eta=0.01$ (showing 140 runs). "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "F NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: Yes, it just matches. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 41}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: There is a dedicated section on limitations. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 41}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 41}, {"type": "text", "text": "Answer: [No] ", "page_idx": 41}, {"type": "text", "text": "Justification: At least not fully - our theoretical results are on the level of rigor of theoretical physics and hence we provide assumptions and formal derivations (which are all provided in the Appendix of this work), but not proofs. We verify the match between the theory and empirical simulations both in the main text and in the Appendix. We verify the deterministic concentration of properties in the high-dimensional limit through empirical simulations in the main text and the Appendix. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 42}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The full code is available in the supplementary material. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 42}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: The code will be made public on github as it is available with the supplementary material. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 43}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: The concrete details of how many samples per batch etc. are contained in the supplementary material. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 43}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: Error bars are provided. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 44}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 44}, {"type": "text", "text": "Answer: [No] ", "page_idx": 44}, {"type": "text", "text": "Justification: The compute needed is negligable, so it is not specified. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 44}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: It is a theoretical work that aims to advance understanding only. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 44}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 44}, {"type": "text", "text": "Answer: [No] ", "page_idx": 44}, {"type": "text", "text": "Justification: It is a foundational work that aims to advance our understanding of the model internals. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 44}, {"type": "text", "text": "\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 45}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: It is foundational research. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 'I'he answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 45}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: We do not use external original material. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 45}, {"type": "text", "text": "", "page_idx": 46}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: The code is well documented and accessible. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 46}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: This work is foundational theoretical research. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 46}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: This work is foundational theoretical research. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 46}, {"type": "text", "text": "", "page_idx": 47}]