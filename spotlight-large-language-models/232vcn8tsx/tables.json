[{"figure_path": "232VcN8tSx/tables/tables_4_1.jpg", "caption": "Table 1: Combination of models, training datasets, and evaluation datasets", "description": "This table shows the different model-training-evaluation dataset combinations used in the experiments of the paper.  It lists the language models used (MISTRAL-7B, LLAMA-2-7B, LLAMA-3-8B, and GPT-SMALL), the training datasets used for fine-tuning (LESS and ALPACA) and pretraining (OPEN WEBTEXT), and the evaluation datasets (MMLU, TYDIQA, and SAMSUM). The number of validation data points used is also specified for each combination.", "section": "5.1 Experimental Setup"}, {"figure_path": "232VcN8tSx/tables/tables_7_1.jpg", "caption": "Table 1: Combination of models, training datasets, and evaluation datasets", "description": "This table presents the experimental setup used in the paper, including the language models, training datasets, evaluation datasets, and the number of validation data points used for each experiment.  The table shows four different model-training-evaluation setups used to evaluate the proposed GREATS algorithm. The models evaluated include LLAMA-2-7B, MISTRAL-7B, LLAMA-3-8B, and GPT-SMALL. The training datasets used are LESS and ALPACA. The evaluation datasets include MMLU, TYDIQA, SAMSUM, and OPENWEBTEXT. The number of validation data points used varies from 5 to 16 depending on the specific experiment. This information is crucial for understanding the scope and reproducibility of the experimental results presented in the paper.", "section": "5.1 Experimental Setup"}, {"figure_path": "232VcN8tSx/tables/tables_7_2.jpg", "caption": "Table 1: Combination of models, training datasets, and evaluation datasets", "description": "This table shows different model-training-evaluation dataset combinations used in the paper's experiments.  It lists the specific large language models (LLMs) used (Llama-2-7B, Mistral-7B, Llama-3-8B), the training datasets employed (LESS, Alpaca), the evaluation datasets used (MMLU, TYDIQA, SAMSUM), and the number of validation data points for each combination.  These combinations are used to comprehensively evaluate the GREATS algorithm's performance across various language modeling tasks and conditions.", "section": "5.1 Experimental Setup"}, {"figure_path": "232VcN8tSx/tables/tables_9_1.jpg", "caption": "Table 2: Accuracy on MMLU (9 subjects) and TYDIQA test set for online batch selection methods.", "description": "This table presents the accuracy results of different online batch selection methods on two benchmark datasets: MMLU (Multitask Language Understanding) and TYDIQA (Typologically Diverse Information-Seeking Question Answering).  The MMLU results show the average accuracy across 9 randomly selected subjects, while TYDIQA provides a single accuracy score.  The table allows comparison of the performance of GREATS against baseline methods such as Regular training, GradNorm, and MaxLoss, highlighting the improvement in accuracy achieved by GREATS.", "section": "5.2 Performance Evaluation"}, {"figure_path": "232VcN8tSx/tables/tables_9_2.jpg", "caption": "Table 3: Efficiency comparison of different implementations of GREATS. We use throughput\u2014# training data points being processed per second\u2014as the efficiency metric.", "description": "This table compares the efficiency of different implementations of the GREATS algorithm in terms of throughput (training data points processed per second).  It contrasts the performance of GREATS using the \"ghost inner-product\" technique, a direct implementation of GREATS, and a direct implementation of GradNorm. The \"ghost inner-product\" method is shown to be significantly faster, nearly matching the speed of regular training. The direct implementations are considerably slower due to the computation of per-sample gradients.", "section": "5.3 Runtime Comparison"}]