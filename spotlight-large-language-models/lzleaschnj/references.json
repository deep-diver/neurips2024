{"references": [{"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2021-00-00", "reason": "This paper introduces LoRA, a foundational low-rank adaptation technique that is frequently compared against in the current paper's experiments."}, {"fullname_first_author": "Zeju Qiu", "paper_title": "Controlling text-to-image diffusion by orthogonal finetuning", "publication_date": "2023-00-00", "reason": "This paper introduces OFT, another foundational parameter-efficient fine-tuning method that is frequently compared against in the current paper's experiments."}, {"fullname_first_author": "Tom Henighan", "paper_title": "Scaling laws for autoregressive generative modeling", "publication_date": "2020-10-14", "reason": "This paper establishes scaling laws for autoregressive models, motivating the need for parameter-efficient fine-tuning methods such as HRA."}, {"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-08", "reason": "This paper establishes scaling laws for neural language models, highlighting the computational challenges of adapting large models, motivating the research on parameter-efficient fine-tuning."}, {"fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-00-00", "reason": "This paper introduces the MetaMathQA dataset, a key benchmark used for mathematical reasoning experiments in the paper."}]}