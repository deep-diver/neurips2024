[{"figure_path": "dA7hUm4css/tables/tables_23_1.jpg", "caption": "Table 1: Hyper-parameters for training safety-only and helpfulness-only DPO models.", "description": "This table lists the hyperparameters used for training the safety-only and helpfulness-only models using Direct Preference Optimization (DPO).  The hyperparameters cover various aspects of the training process, including the number of training epochs, maximum sequence length, batch size, learning rate schedule, weight decay, and optimizer.  Different values are used for the safety-only and helpfulness-only models, suggesting that different configurations may be optimal for different objectives.", "section": "J Training details of algorithms"}, {"figure_path": "dA7hUm4css/tables/tables_23_2.jpg", "caption": "Table 2: Hyper-parameters for training MOCAN and PECAN.", "description": "This table lists the hyperparameters used for training both MOCAN and PECAN models.  It shows that both models used similar settings for various parameters, such as the number of epochs, sequence length, batch size, learning rate, and optimizer.  However, there is one key difference: the \u03b2 (beta) parameter for KL regularization, which is set to 0.1 for MOCAN and {0.025, 0.1} for PECAN, suggesting some experimentation with this value was performed for PECAN.", "section": "J Training details of algorithms"}, {"figure_path": "dA7hUm4css/tables/tables_24_1.jpg", "caption": "Table 3: Hyper-parameters for LM Generation.", "description": "This table shows the hyperparameters used for Language Model generation.  The hyperparameters are: max_length (maximum sequence length), temperature (controls randomness of sampling), and top_p (controls randomness of sampling).", "section": "J Training details of algorithms"}, {"figure_path": "dA7hUm4css/tables/tables_27_1.jpg", "caption": "Table 4: Predicted safety margins and empirical confidence intervals for MOCAN-trained LMs using different dual variables \u03bb.", "description": "This table presents the results of using MOCAN (Model-based Constrained Alignment via dualization) with different dual variables (\u03bb).  For each \u03bb value, the table shows the predicted safety margin and the corresponding empirical 95% confidence interval obtained through bootstrapping.  The safety margin represents the expected improvement in safety achieved by the aligned language model, compared to the reference model. The confidence intervals give a measure of the uncertainty associated with the safety improvement estimates.", "section": "5.2 Experimental results"}, {"figure_path": "dA7hUm4css/tables/tables_29_1.jpg", "caption": "Table 5: Sample responses for the red-teaming experiments on social bias.", "description": "This table presents sample responses generated by different language models (SFT, Safe-RLHF, DPOH, DPOS, MOCAN, and PECAN) to a prompt designed to assess their performance on social bias. The prompt is related to the crime rate of the poor. Each model's response reflects its different strengths and weaknesses in addressing sensitive social issues and avoiding harmful biases.", "section": "5 Computational experiments"}, {"figure_path": "dA7hUm4css/tables/tables_30_1.jpg", "caption": "Table 4: Predicted safety margins and empirical confidence intervals for MOCAN-trained LMs using different dual variables \u03bb.", "description": "This table presents the predicted safety margins calculated using offline dual optimization in MOCAN.  It compares these predictions to the empirical safety improvements observed after training the language model (LM) with different dual variables (\u03bb).  The confidence intervals are also given for each dual variable's performance. The table helps evaluate the effectiveness of the dual optimization strategy in MOCAN for predicting the actual safety gains in the aligned LM.", "section": "5.2 Experimental results"}, {"figure_path": "dA7hUm4css/tables/tables_31_1.jpg", "caption": "Table 7: Sample responses for the red-teaming experiments on emotional harm.", "description": "This table shows example responses generated by different language models (SFT, Safe-RLHF, DPOH, DPOs, MOCAN, and PECAN) to a prompt designed to elicit responses that could cause emotional harm.  The purpose is to evaluate the safety and ethical considerations of each model's output in response to sensitive or potentially harmful questions.  The responses vary in their appropriateness and level of awareness of the potential harm caused by the prompt.", "section": "5 Computational experiments"}, {"figure_path": "dA7hUm4css/tables/tables_31_2.jpg", "caption": "Table 5: Sample responses for the red-teaming experiments on social bias.", "description": "This table presents sample responses generated by different language models (SFT, Safe-RLHF, DPOH, DPOS, MOCAN, and PECAN) to a prompt designed to elicit potentially biased responses related to social issues.  The goal is to illustrate how each model handles such a sensitive topic and the variations in their responses in terms of bias and safety. The prompt explores the stereotype that people from poor backgrounds are more likely to commit crimes.", "section": "5 Computational experiments"}]