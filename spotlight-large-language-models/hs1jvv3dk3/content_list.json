[{"type": "text", "text": "Localized Zeroth-Order Prompt Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wenyang $\\mathbf{H}\\mathbf{u}^{*\\dagger\\S}$ , Yao $\\mathbf{S}\\mathbf{h}\\mathbf{u}^{*\\diamond}$ , Zongmin $\\mathbf{M}^{\\dagger}$ , Zhaoxuan $\\mathbf{W}\\mathbf{u}^{\\mathrm{99}}$ , Xiaoqiang Lin\u2020, Zhongxiang $\\bf{D a i^{\\ddagger}}$ , See-Kiong $\\mathbf{Ng}^{\\dagger\\S}$ , Bryan Kian Hsiang Low\u2020 ", "page_idx": 0}, {"type": "text", "text": "Dept. of Computer Science, National University of Singapore, Republic of Singapore Institute of Data Science, National University of Singapore, Republic of Singapore\u00a7 Guangdong Lab of AI and Digital Economy $({\\bf S}{\\bf Z})^{\\circ}$ ", "page_idx": 0}, {"type": "text", "text": "Singapore-MIT Alliance for Research and Technology, Republic of Singapore\u00b6 School of Data Science, The Chinese University of Hong Kong, Shenzhen\u2021 wenyang.hu@u.nus.edu, shuyao@gml.ac.cn {yuzongmin, wu.zhaoxuan, xiaoqiang.lin}@u.nus.edu daizhongxiang@cuhk.edu.cn, seekiong@nus.edu.sg, lowkh@comp.nus.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The efficacy of large language models (LLMs) in understanding and generating natural language has aroused a wide interest in developing prompt-based methods to harness the power of black-box LLMs. Existing methodologies usually prioritize a global optimization for finding the global optimum, which however will perform poorly in certain tasks under budget constraints. This thus motivates us to re-think the necessity of finding a global optimum in prompt optimization. To answer this, we conduct a thorough empirical study on prompt optimization and draw two major insights. Contrasting with the rarity of global optimum, local optima are usually prevalent and well-performed, which can be more worthwhile for efficient prompt optimization (Insight I). The choice of the input domain, including both the generation and the representation of prompts, affects the identification of well-performing local optima (Insight II). Inspired by these insights, we propose a novel algorithm, namely localized zeroth-order prompt optimization (ZOPO), which incorporates a Neural Tangent Kernel-based derived Gaussian process into standard zeroth-order optimization for an efficient search of well-performing local optima in prompt optimization. Remarkably, ZOPO outperforms existing baselines in terms of both the optimization performance and the query efficiency, which we demonstrate through extensive experiments. Our implementation is available at https://github.com/allen4747/ZOPO. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) have demonstrated remarkable capabilities for understanding and generating natural languages [27, 41, 45]. Thanks to the instruction-following abilities of LLMs [28], prompting\u2014adding crafted, discrete prompts, or namely natural language text, to the input emerges as an effective and lightweight approach to direct LLMs to generate specific, desired responses [20, 21]. Such an approach is of particular interest when users interact with state-of-the-art LLMs like ChatGPT [25] and GPT-4 [24], which can only be accessed through black-box APIs (i.e., the interface of black-box LLMs only accepts discrete texts as input). So, prompt optimization becomes a critical effort in pursuing the optimal performance of black-box LLMs on downstream tasks. ", "page_idx": 0}, {"type": "text", "text": "Although human knowledge may subjectively guide prompt designs [21, 32], this process is commonly time-intensive and its results are not always desirable in practice. To mitigate such human efforts and achieve better performance in optimizing crafted prompts, random sampling [48], Bayesian optimization [3, 18], and evolutionary algorithms [10] have been proposed to generate and select well-performing prompts automatically. However, most of these existing strategies prioritize global optimization, dedicating substantial portions of the query budget to explore the entire search space for the global optima and consequently making it query-inefficient in practice. Meanwhile, these strategies typically implement their prompt optimization across various input domains (i.e., natural texts [10, 48] or hidden embeddings [3, 18]), resulting in diverse performance outcomes in practice. These results consequently inspire us to re-think the questions about the necessity of finding a global optimum and the essence of the input domain for efficient and effective prompt optimization. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To answer these questions, we provide a thorough empirical study on prompt optimization. Firstly, we visualize the performances of some randomly sampled prompt candidates on various tasks to show that, in contrast to the scarcity of global optima, local optima are commonly prevalent and perform reasonably well, making them more valuable for query-efficient prompt optimization (Insight I in our Sec. 3.1). Secondly, we visualize the estimated accuracy distributions for a number of prompt candidates and the corresponding function surfaces using various embeddings as their representation. The results demonstrate that the selection of the input domain, including both the generation and representation of prompt candidates, will influence the identification of high-performing prompts, especially those local optimal ones (Insight II in our Sec. 3.2). These insights consequently highlight the importance of local optima and input domain for efficient and effective prompt optimization. ", "page_idx": 1}, {"type": "text", "text": "Inspired by these insights, we novelly propose the Localized Zeroth-Order Prompt Optimization (ZOPO) algorithm for a considerably improved prompt optimization as evidenced by Fig. 1. Motivated by Insight II, we first propose a general domain transformation that utilizes LLMs for prompt generation and existing embedding models for transforming these generated prompts into their corresponding hidden representations, which thereby enjoys not only the remarkable generation ability from any type of LLMs (white/black-box) but also the impressive representation ability from existing embedding models for our prompt optimization (Sec. 4.1). Inspired by Insight I, we then leverage a cutting-edge zeroth-order optimization (ZOO) method enhanced by a derived Gaussian process for efficient gradient estimation [36] to underpin our localized prompt optimization, which goes one step further by incorporating the Neural Tangent Kernel (NTK) [12] to handle the complex and highdimensional prompt optimization tasks (Sec. 4.2). Lastly, we present an uncertainty-informed local exploration method designed to improve the gradient estimation in our derived NTK-GP framework, thereby augmenting the practical performance of the ZOPO algorithm (Sec. 4.3). ", "page_idx": 1}, {"type": "text", "text": "To summarize, the contributions of our work include: ", "page_idx": 1}, {"type": "text", "text": "\u2022 To the best of our knowledge, we are the first to conduct a thorough empirical study in prompt optimization to underscore the value of local optima and the essence of input domain for efficient and effective prompt optimization (Sec. 3).   \n\u2022 Drawing on the insights gained from our empirical study, we design the ZOPO algorithm (Sec. 4) which outperforms existing baselines in optimization performance and query efficiency.   \n\u2022 We conduct extensive studies to confirm the efficacy of our algorithmic framework and elucidate the underlying principles or insights of our ZOPO algorithm (Sec. 5). ", "page_idx": 1}, {"type": "text", "text": "2 Problem Setup ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Given an NLP task that is characterized by a data distribution $\\mathcal{D}$ and a black-box LLM $f(\\cdot)$ , e.g., ChatGPT [25], discrete prompt optimization aims to generate a piece of human-readable text, namely the prompt $v$ , which will then be applied to the black-box LLM $f(\\cdot)$ along with a test input $x$ such that the queried LLM output $f([v;x])$ is able to correctly predict the ground-truth label $y$ for each $(x,y)\\sim\\bar{D}$ . This problem is then commonly framed as a black-box maximization problem over the discrete language input domain $\\Omega$ [3, 18]: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{v\\in\\Omega}F(v)\\triangleq\\mathbb{E}_{(x,y)\\in{\\mathcal{D}}_{V}}\\ \\left[\\mathcal{R}\\left(f([v;x]\\right),y\\right)\\right]\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathcal{R}\\left(f([v;x]),y\\right)$ is applied to measure the alignment between the LLM output $f([v;x])$ and the groundtruth $y$ , and $\\mathcal{D}_{V}$ is the validation set sampled from $\\mathcal{D}$ . Note that the performance of the optimal instruction found on $\\mathcal{D}_{V}$ (i.e., $\\arg\\operatorname*{max}_{v}F(v))$ will be evaluated on a held-out test set $\\mathcal{D}_{T}$ . ", "page_idx": 1}, {"type": "text", "text": "Figure 1: The performance profile for different methods on 20 tasks. A higher $\\rho(\\tau)$ is better. More details in Sec. 5. ", "page_idx": 2}, {"type": "text", "text": "3 Empirical Study on Prompt Optimization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Local Optima vs. Global Optimum ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In prompt optimization, methods like [3, 18] are generally more effective than the others [10, 48], which is usually contributed to their usage of Bayesian optimization, a popular global optimization strategy, that is able to find the global optimum in low-dimensional problems [22]. However, these methods will perform poorly in certain prompt optimization tasks, e.g., cause_and_effect and informal_to_formal from [11], indicating that they typically fail to find the global optimum in these tasks given a limited query budget. This is likely because substantial portions of the budget are wasted in these methods to explore the entire search space for the global optimum. However, is it really necessary to find the global optimum in query-efficient prompt optimization? ", "page_idx": 2}, {"type": "text", "text": "To answer this question, we have employed a 3-dimensional scatter plot to visualize the performance (differentiated by colors) for 300 randomly sampled prompt candidates on various tasks, whose prompt embeddings (i.e., the last token embedding as in [18]) are reduced by t-distributed stochastic neighbor embedding (t-SNE) (see more details in our Appx. D.1.1). The results are in Fig. 2 which shows that the global optimum (i.e., the points achieving the highest accuracy) is consistently rare for a range of prompt optimization tasks, making it extremely challenging to achieve this global optimum in practice. In contrast, prompt optimization often features a number of local optima (e.g., the points achieving accuracy higher than $80\\%$ in taxonomy_animal of Fig. 2). Importantly, these local optima commonly enjoy relatively good performances, suggesting that local optima shall be more worthwhile to obtain in prompt optimization, especially for the scenarios of limited query budgets, as summarized below. ", "page_idx": 2}, {"type": "text", "text": "Insight I ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In contrast with the rarity of global optimum, local optima are usually prevalent and well-performed, which is more worthwhile for query-efficient prompt optimization. ", "page_idx": 2}, {"type": "text", "text": "3.2 Essence of Input Domain ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Besides, existing works [3, 10, 18] typically implement their prompt optimization across various input domains, leading to a wide range of performances in practice. These results thus inspire us to ask: How essential is the input domain for finding well-performing prompts, particularly the local optimal ones? Thoroughly exploring this question is fundamental for the design of a well-performing prompt optimization algorithm. ", "page_idx": 2}, {"type": "text", "text": "To answer this, we first visualize the accuracy distributions of 300 prompt candidates that are randomly generated by Vicuna-13B and ChatGPT for various tasks to study the essence of prompt generation in Fig. 3 (more details in Appx. D.1.2). Fig. 3 reveals that the prompt candidates produced by ChatGPT (a black-box model) generally exhibit better performance than those produced by Vicuna-13B (a white-box model), which has been widely applied in [3, 18] for prompt optimization. Importantly, ChatGPT demonstrates a greater likelihood of generating locally optimal prompts (e.g., the ones of accuracy higher than 0.8 in taxonomy_animal of Fig. 3). These results indicate that the ability to ", "page_idx": 2}, {"type": "image", "img_path": "hS1jvV3Dk3/tmp/fcecd4b8abafab2fef4eda342bcafb1d95c178acb4bbfc14425644cadd8cfdc6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 3: The validation accuracy distribution of prompts generated by Vicuna-13B or ChatGPT on various instruction induction tasks, where the vertical dotted line is the mean performance. ", "page_idx": 3}, {"type": "image", "img_path": "hS1jvV3Dk3/tmp/1aa2f58359135f98a4e5696b590a8e1dc0140f59a16316dd90e95d1bb845dc0e.jpg", "img_caption": ["Figure 4: The function surfaces using the last token (Vicuna-13B) or SBERT embedding. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "generate well-performing local optima in prompt optimization usually varies for different models. So, the selection of the prompt generation model is crucial for finding well-performing optima. ", "page_idx": 3}, {"type": "text", "text": "We then investigate the function surface (i.e., accuracy landscape) using two different embeddings for prompt candidates in Fig. 4 (more details in Appx. D.1.2) where the embeddings are mapped into a 2-dimensional domain using the t-SNE for better visualization. Interestingly, Fig. 4 unveils that different embeddings will convey a varying number of well-performing local optima in practice. Particularly, the last token embedding is usually able to produce a larger number of well-performing local optima than the SBERT (i.e., a popular sentence embedding transformer [31]) embedding, making it easier to enjoy a good prompt optimization performance on this domain, as validated in Tab. 8. This therefore implies that the choice of the prompt embedding model is also essential for the finding of well-performing optima. In all, we conclude our aforementioned insights as below. ", "page_idx": 3}, {"type": "text", "text": "Insight II ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The choice of the input domain, including both the generation and the representation of prompt candidates, affects the identification of well-performing local optima. ", "page_idx": 3}, {"type": "text", "text": "4 The ZOPO Algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given the insights established in our Sec. 3, we then propose our Localized Zeroth-Order Prompt Optimization (ZOPO) algorithm (Algo. 1) for a better-performing as well as more query-efficient prompt optimization. Specifically, following our Insight II, we first develop a more general transformation for the input domain of prompt optimization (Sec. 4.1), which can enjoy both the remarkable generation ability from any type of LLMs (white/black-box) and the impressive representation ability from many NLP models. Subsequent to this transformation, inspired by our Insight I, we propose to use zeroth-order optimization (ZOO) with a derived NTK Gaussian process inspired from [36] to find well-performing local optima (Sec. 4.2). Lastly, we introduce an uncertainty-informed local exploration technique to refine the gradient estimation in our derived NTK Gaussian process, aiming to enhance the performance of our ZOPO algorithm in practice (Sec. 4.3). ", "page_idx": 3}, {"type": "text", "text": "4.1 A More General Input Domain Transformation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As introduced in our Sec. 3.2, the choice of input domain (including the generation and representation of candidates) significantly influences the ultimate performance in prompt optimization: Black-box LLMs (e.g., ChatGPT) typically enjoy an advanced generation ability and different embedding models (e.g., SBERT) have varying representative capacity for prompt optimization. This naturally inspires us to develop an improved domain transformation that can utilize not only the remarkable generation ability from white/black-box LLMs but also the impressive representation ability from certain NLP models for our prompt optimization. To achieve this, we propose to make use of the prompt $v\\in\\Omega$ generated from a LLM $g(\\cdot)$ and subsequently transform it into a continuous hidden representation $z\\in\\mathcal{Z}\\subset\\mathbb{R}^{d}$ by other sentence embedding model $h(\\cdot)$ for the optimization, i.e., $v=h^{-1}(z)$ , where (1) can then be re-framed as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{z\\in\\mathcal{Z}}\\widetilde{F}(z)=\\mathbb{E}_{(x,y)\\in\\mathcal{D}}\\;\\left[\\mathcal{R}\\left(f([h^{-1}(z);x]),y\\right)\\right]\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "1: Input: the prompt generation model $g(\\cdot)$ , the prompt embedding model $h(\\cdot)$ , size of prompt   \ncandidates $m$ , iteration number $T$ , prompt candidate set $\\mathcal{V}=\\emptyset$ , prompt embedding set $\\mathcal{Z}=\\emptyset$   \n2: repeat   \n3: v \u2190g([Ddemo])   \n4: z \u2190h(v)   \n5: if $v\\not\\in\\mathcal{V}$ then $\\mathcal{V}\\gets\\mathcal{V}\\bigcup\\{v\\},\\mathcal{Z}\\gets\\mathcal{Z}\\bigcup\\{z\\}$   \n6: until $|\\gamma|=m$   \n7: for $t=1$ to $T$ do   \n8: if $\\mathbb{1}_{A_{t}}(z_{t})=1$ then do uncertainty-informed local exploration in Sec. 4.3   \n9: $z_{t+1}\\overset{\\cdot}{=}\\mathcal{P}_{\\mathcal{Z}}(z_{t}+\\eta_{t}\\mu_{t}(z_{t}))$   \n10: Query $z_{t+1}$ to yield $\\widetilde{F}(z_{t+1})$   \n11: end for   \n12: $z^{*}\\gets\\arg\\operatorname*{max}_{z_{1:T}}\\widetilde{F}(z)$   \n13: Return $h^{-1}(z^{*})$ ", "page_idx": 4}, {"type": "text", "text": "Of note, our input domain transformation and (2) enjoy a number of major advantages compared with previous works: $(a)$ Different from the direct optimization over the discrete and complex language space $v\\in\\Omega$ in [10] where optimization algorithms in the numerical domain can hardly be applied, our transformed input domain leads to a dense numerical space of lower dimension and therefore allows the usage of query-efficient optimization algorithms for (2) (e.g., our Algo. 1). (b) Different from the potential many-to-one mapping in the previous works [3, 18], i.e., the same discrete prompt $v$ may be generated by various continuous soft prompts $s$ , we develop a one-to-one mapping where one prompt generally has a unique hidden representation $z$ , which thus can help eliminate the redundant queries during optimization and ultimately lead to more query-efficient prompt optimization. (c) Our domain transformation with an independent generation and representation process is capable of enjoying the remarkable generation ability from any type of LLMs (white/black-box) and the impressive representation ability from many embedding models whereas previous works are highly restricted to the LLMs, thus leading to a wider application. ", "page_idx": 4}, {"type": "text", "text": "Practical Implementations. Before the start of the optimization on (2), we usually generate numerous prompt candidates $\\mathcal{V}=\\{v\\}$ and their corresponding representations $\\mathcal{Z}=\\{z\\}$ (line 2-6 of Algo. 1), where $\\mathcal{Z}$ can be produced by an embedding model $h(\\cdot)$ . We store $(z,v)$ in key-value pairs for constructing the one-to-one inverse mapping $h^{-\\bar{1}}(\\cdot)$ . Two practical methods are considered here for prompt generation: (a) Feeding randomly sampled soft prompts $s\\in\\mathbb{R}^{d}$ and a few demonstrations $\\mathcal{D}_{\\mathrm{demo}}$ into a white-box LLM $g(\\cdot)$ . $(b)$ Sampling the output distribution of a black-box LLM $g(\\cdot)$ given a generation template filled with $\\mathcal{D}_{\\mathrm{demo}}$ . Specifically, if we consider the generation method in (a), $z$ can be chosen as the last token embedding from $g(\\cdot)$ [18] or the soft prompt $s$ [3] when generating $v$ . Here $h(\\cdot)$ then represents a mapping function from $v$ to $z$ . ", "page_idx": 4}, {"type": "text", "text": "4.2 Local Optimization with Derived NTK-GP ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As local optima are more prevalent than global optimum and can exhibit compelling performance for prompt optimization tasks (Sec. 3.1), we propose to apply zeroth-order optimization (ZOO), particularly gradient descent using estimated gradients, for a well-performing local prompt optimization on our transformed input domain $\\mathcal{Z}$ in Sec. 4.1. Unfortunately, existing ZOO algorithms are typically query-inefficient as many additional queries are required for gradient estimation in every gradient descent update [9, 23]. In light of this, we resort to the most recent ZoRD algorithm [36] where a localized surrogate model will be applied for query-efficient gradient estimations. ", "page_idx": 4}, {"type": "text", "text": "According to [36], given a well-specified kernel function $k(\\cdot,\\cdot)$ such that the functionF is sampled from a Gaussian process $\\widetilde{F}\\sim\\mathcal{G P}(0,k(\\cdot,\\cdot))$ or alternatively $\\begin{array}{r}{\\operatorname*{min}_{G\\sim\\mathcal{G P}(0,k(\\cdot,\\cdot))}\\operatorname*{max}_{z\\in\\mathcal{Z}}|\\widetilde{F}(z)-z|}\\end{array}$ $G(z)\\vert\\;=\\;0$ and the observed value $r$ of function $\\widetilde{F}$ follows the Gaussian noise ${\\mathcal{N}}(0,\\sigma^{2})$ , then conditioned on the history of function queries ${\\mathcal{D}}_{t}\\triangleq\\{(z_{\\tau},r_{\\tau})\\}_{\\tau=1}^{t}$ of size $t$ , $\\nabla\\tilde{F}$ follows a derived ", "page_idx": 4}, {"type": "text", "text": "Gaussian Process $\\mathcal{G P}(\\mu(\\cdot),\\Sigma(\\cdot,\\cdot))$ , i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla\\widetilde{F}\\sim\\mathcal{G P}\\left(\\mu_{t}(\\cdot),\\Sigma_{t}^{2}(\\cdot,\\cdot)\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "in which the mean function $\\mu_{t}(\\cdot)$ and the covariance function $\\Sigma_{t}^{2}(\\cdot,\\cdot)$ are defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{}}&{{}}&{{\\mu_{t}(z)\\triangleq k_{t}(z)^{\\top}\\left({\\bf K}_{t}+\\sigma^{2}{\\bf I}\\right)^{-1}r_{t}\\;,}}\\\\ {{}}&{{}}&{{\\Sigma_{t}^{2}(z,z^{\\prime})\\triangleq k^{\\prime\\prime}(z,z^{\\prime})-k_{t}(z)^{\\top}\\left({\\bf K}_{t}+\\sigma^{2}{\\bf I}\\right)^{-1}k_{t}(z^{\\prime})\\;.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, $\\pmb{k}_{t}(z)^{\\top}\\triangleq[\\partial_{z}k(z,z_{\\tau})]_{\\tau=1}^{t}$ is a $d\\times t$ -dimensional matrix, $\\mathbf{K}_{t}\\triangleq\\left[k(z_{\\tau},k(z_{\\tau^{\\prime}})\\right]_{\\tau,\\tau^{\\prime}=1}^{t}$ is a $t\\times t$ - dimensional matrix, $r_{t}^{\\top}\\triangleq\\left[r_{\\tau}\\right]_{\\tau=1}^{t}$ is a $t$ -dimensional column vector, and $k^{\\prime\\prime}(z,z^{\\prime})\\triangleq\\partial_{z}\\partial_{z^{\\prime}}k(z,z^{\\prime})$ is a $d\\times d$ -dimensional matrix. As a result, $\\mu_{t}(z)$ can be applied to estimate the gradient of the black-box function $\\widetilde{F}$ at input $z$ . ", "page_idx": 5}, {"type": "text", "text": "Of note, the underlying black-box functionF here is highly related to deep neural networks (DNN), more specifically transformers. It naturally inspires us to apply the Neural Tangent Kernel (NTK) [12] theory for a better approach to the aforementioned assumption of a well-specified kernel function $k(\\cdot,\\cdot)$ . This is because it has been widely proven that NTK is capable of well characterizing the predictions of neural networks [2, 16, 34, 35] and therefore should be a better-specified kernel in the setting of prompt optimization than the simple kernel (i.e., Mat\u00e9rn kernel) applied in ZoRD [36]. Specifically, given a neural network $\\phi(\\theta,z)$ parameterized by $\\theta\\in\\mathbb{R}^{p}$ , we employ the following empirical NTK as the kernel in (3) and (4): ", "page_idx": 5}, {"type": "equation", "text": "$$\nk(z,z^{\\prime})=\\nabla_{\\theta}\\boldsymbol{\\phi}(\\theta,z)^{\\top}\\nabla_{\\theta}\\boldsymbol{\\phi}(\\theta,z)\\Big|_{\\theta=\\theta_{0}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\theta_{0}$ is the initialized parameter of neural network $\\phi$ . By incorporating (5) into (4), we realize the derived NTK-GP for the gradient estimation in our prompt optimization. ", "page_idx": 5}, {"type": "text", "text": "Based on this derived NTK-GP, we finally apply standard first-order optimization (e.g., stochastic gradient descent) with projected gradients for our local prompt optimization. Specifically, in every iteration $t$ of our Algo. 1, the next promising prompt candidate will be selected via: ", "page_idx": 5}, {"type": "equation", "text": "$$\nv_{t+1}=h^{-1}\\left(\\mathcal{P}_{\\mathcal{Z}}(z_{t}+\\eta_{t}\\mu_{t}(z_{t}))\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{P}_{\\mathcal{Z}}(z)\\triangleq\\arg\\operatorname*{min}_{z^{\\prime}\\in\\mathcal{Z}}\\|z-z^{\\prime}\\|}\\end{array}$ is the projection function that projects the updated $z\\in\\mathbb{R}^{d}$ into domain $\\mathcal{Z}$ and $\\eta_{t}$ is learning rate. ", "page_idx": 5}, {"type": "text", "text": "Practical Implementations. Following the localized modeling principle, only the neighbors of $z$ in the query history $\\mathcal{D}_{t}$ are used to calculate the gradient $\\mu_{t}(z)$ . As we do not know the exact DNN for the underlying black-box functionF , we propose to approximate it using a small DNN, which can work well thanks to the theoretical ly guaranteed universal approximation ability of DNNs [14, 33]. Our experiments in Sec. 5.3 will further validate the effectiveness of this implementation. ", "page_idx": 5}, {"type": "text", "text": "4.3 Uncertainty-Informed Local Exploration ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Though the derived NTK-GP allows us to estimate the gradient at any $z\\in{\\mathcal{Z}}$ according to [36], we introduce the following Prop. 1 to demonstrate that the error in gradient estimation at a specific input $z\\in{\\mathcal{Z}}$ implies considerable variability, which is strongly correlated with the number of historical queries that are effectively relevant for the gradient estimation at the specific input $z\\in{\\mathcal{Z}}$ . This insight, in turn, motivates the creation of our uncertainty-informed local exploration approach, as opposed to the adoption of the virtual update mechanism described in [36] for our prompt optimization strategy. ", "page_idx": 5}, {"type": "text", "text": "Proposition 1. Assume $k(z,z^{\\prime})\\leq\\alpha$ and $\\|k^{\\prime\\prime}(z,z)\\|\\,\\leq\\,\\kappa$ for any $z,z^{\\prime}\\in{\\mathcal{Z}}$ . Let $\\delta\\,\\in\\,(0,1)$ and $N_{z,\\beta}\\triangleq\\{z^{\\prime}\\in\\{z_{\\tau}\\}_{\\tau=1}^{t}\\mid\\|\\partial_{z}k(z^{\\prime},z)\\|^{2}\\geq\\beta\\}$ for given input $z\\in{\\mathcal{Z}}$ , the following holds with $a$ probability of at least $1-\\delta$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|\\mu_{t}(z)-\\nabla F(z)\\|^{2}\\leq\\omega\\left\\|\\Sigma_{t}^{2}(z)\\right\\|\\leq\\omega\\kappa-\\frac{\\omega\\beta/d}{\\alpha+\\sigma^{2}/|N_{z,\\beta}|}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\omega=d+2(\\sqrt{d}+1)\\ln(1/\\delta)$ and $\\Sigma_{t}^{2}(z)\\triangleq\\Sigma_{t}^{2}(z,z)$ . ", "page_idx": 5}, {"type": "table", "img_path": "hS1jvV3Dk3/tmp/a43a315d520ac0ef18e47ab1360a234d1352e6c1b57259e129d9f563d4367b9c.jpg", "table_caption": ["Table 1: Average test accuracy with standard error (3 runs) for different methods on 20 instruction induction tasks. We bold the highest accuracy when comparing ZOPO with baselines, and use green cell to highlight the highest accuracy when comparing $Z\\mathrm{OPO}_{\\mathrm{GPT}}$ with baselines. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "The proof is given in Appx. A. Here, $N_{z,\\beta}$ denotes a set of historical input queries that are effectively relevant for the gradient estimation at $z$ where $\\beta$ can be regarded as a measure of effective relevance. Prop. 1 shows that the gradient estimation error of (3) at a specific input $z\\,\\in\\,{\\mathcal{Z}}$ is bounded by the norm of covariance matrix $\\Sigma_{t}^{2}(z)$ , which is related to the query set $N_{z,\\beta}$ of effective relevance. Specifically, the gradient estimation error at different $z$ varies if the effective relevance $\\beta$ and the number of relevant queries $|N_{z,\\beta}|$ varies with $z$ . When $\\beta$ or $|N_{z,\\beta}|$ becomes small during ZOO, the gradient estimation error is likely increased, which will lead to poor performance in practice. This likely will happen in prompt optimization especially considering the sparsity of prompt candidates w.r.t. the continuous domain $\\mathbb{R}^{d}$ . That is, both the effective relevance $\\beta$ and the number of relevant queries $|N_{z,\\beta}|$ can be small due to this sparsity. As a consequence, additional input queries should be conducted to increase both $\\beta$ and $|N_{z,\\beta}|$ for a better-performing prompt optimization. ", "page_idx": 6}, {"type": "text", "text": "To this end, we propose an uncertainty-informed local exploration method that utilizes additional input queries from local searches to reduce predictive uncertainty and hence the gradient estimation error in derived NTK-GP according to Prop. 1. Specifically, we propose the local exploration condition informed by the local trajectory: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{1}_{A_{t}}(z_{t})={\\left\\{\\begin{array}{l l}{1}&{z_{t}\\in A_{t}}\\\\ {0}&{z_{t}\\notin A_{t}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $A_{t}=\\{z_{t}|\\sigma(z_{t-i})\\geq\\lambda,\\,\\forall i\\in[0,\\xi]\\}$ is the condition that incorporates uncertainties and $\\lambda,\\xi$ are the thresholds. If this condition is met (i.e., $\\mathbb{1}_{A_{t}}(z_{t})=1)$ ), we will query the neighbors of $z_{t}$ in the local region to update our derived NTK-GP, thus improving its gradient estimation. ", "page_idx": 6}, {"type": "text", "text": "Practical Implementations. If we define the set of the $n$ nearest neighbors of $z_{t}$ as $\\mathcal{N}_{t}\\ \\subseteq$ $\\mathcal{Z}$ s.t. $|{\\mathcal{N}}_{t}|\\,=\\,n$ and $\\forall a\\,\\in\\,\\mathcal{Z}\\,\\backslash\\,\\mathcal{N}_{t},\\,\\,\\|a-z_{t}\\|\\,\\geq\\,\\operatorname*{max}_{b\\in\\mathcal{N}_{t}}\\|b-z_{t}\\|$ , we propose to query each $z\\in\\mathcal{N}_{t}$ in the local region, whenever $\\mathbb{1}_{A_{t}}(z_{t})=1$ . ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we perform prompt optimization for ChatGPT (i.e., the black-box LLM $f(\\cdot)$ ) with a limited query budget of 165. We evaluate the performance of ZOPO against several strong baseline methods, including APE [48], InstructZero [3], INSTINCT [18], EvoPrompt [10], PromptBreeder (PB) [8], and OPRO [46], on 30 instruction induction tasks [11], 3 arithmetic reasoning tasks [4, 19, 29], and the GLUE benchmark [42]. The task-specific prompt is optimized for each task independently. We use the performance profile [7], defined in Appx. C.1, as the overall evaluation metric that measures the frequency (i.e., $\\rho(\\tau)$ ) of a method within some distance (i.e., $\\tau$ ) from the highest accuracy achieved by any method. We defer more experimental details to Appx. C. ", "page_idx": 6}, {"type": "text", "text": "5.1 Instruction Induction ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Instruction induction tasks are commonly used to investigate the prompt optimization performance by assessing LLM\u2019s zero-shot in-context learning ability in previous works [3, 18, 48]. Although our ZOPO is a general prompt optimization method given any prompt generation strategy, here we follow the same setting of prompt generation from INSTINCT and InstructZero, only for fair comparison. We also adopt the last token embedding from Vicuna-13B as the prompt embedding (same as INSTINCT). Here Vicuna-13B is used to generate task-specific prompts by feeding random soft prompts. More experimental details are deferred to Appx. C.3. ", "page_idx": 7}, {"type": "text", "text": "Superior performance of ZOPO. For better distinguishability, we follow the experimental setting from Lin et al. [18] to display the results on 20 challenging tasks reported in Tab. 1, where ZOPO significantly outperforms all baseline methods. Particularly, our ZOPO performs the best in 14 out of the 20 tasks presented, while achieving the best performance proflie across different $\\tau$ (see Fig. 1) compared with all baseline methods. For more results on all 30 tasks, refer to Tab. 3 in Appx. D.2, where the ZOPO consistently outperforms existing methods. ", "page_idx": 7}, {"type": "text", "text": "Connecting ChatGPT with ZOPO. With our proposed domain transformation, we empirically demonstrate that ZOPO is capable of performing numerical optimization on ChatGPT-generated prompts. Specifically, we use the same generation method as in APE [48] to generate task-specific prompts (i.e., $\\mathcal{V}$ ) from ChatGPT, and use a popular embedding model SBERT to provide the corresponding sentence embeddings (i.e., $\\mathcal{Z}$ ) for $\\nu$ . Then we apply ZOPO to perform optimization over the given $\\mathcal{V}$ and $\\mathcal{Z}$ , which we name $Z\\mathrm{OPO}_{\\mathrm{GPT}}$ . The result of $Z\\mathrm{OPO}_{\\mathrm{GPT}}$ compared against other baselines is shown in Tab. 1, with the corresponding performance profile shown in Fig. 9 in App. D.2. Fig. 9 demonstrates that $Z\\mathrm{OPO}_{\\mathrm{GPT}}$ significantly outperforms other baselines, achieving the best performance in 10 out of the 20 tasks as shown in Tab. 1. Specifically, $Z\\mathrm{OPO}_{\\mathrm{GPT}}$ achieves significantly higher accuracy on some challenging tasks such as second_word_letter and sentence_similarity, which we attribute to the high-quality of prompt candidates generated by ChatGPT. This is also consistent with our discussion on the input domain in Sec. 3.2. Here we could not draw a direct comparison between ZOPO and $Z\\mathrm{OPO}_{\\mathrm{GPT}}$ , as the Vicuna last token embedding is specifically associated with the prompt generation process in ZOPO and cannot be applied to $Z\\bar{\\mathrm{OPO}}_{\\mathrm{GPT}}$ . However, using either ZOPO or $Z\\mathrm{OPO}_{\\mathrm{GPT}}$ is sufficient to outperform baseline methods, which also provides the flexibility of prompt optimization in practice. Future research may consider employing better embeddings to further improve the performance of ZOPOGPT. ", "page_idx": 7}, {"type": "text", "text": "ZOPO has better query efficiency. To justify that our local optimization method is more query-efficient, we compare ZOPO against baselines at different query budget scales. The result shown in Fig. 5 illustrates that ZOPO generally achieves better performance with the same number of queries compared with other baseline methods and yields superior performance upon convergence. We notice that ZOPO achieves lower validation accuracy yet higher test accuracy on the taxonomy_animal task than INSTINCT, which suggests ZOPO likely has better generalization ability. More results on other tasks in Fig. 10 in Appx. D.2 also indicate that ZOPO has consistent advantages. ", "page_idx": 7}, {"type": "image", "img_path": "hS1jvV3Dk3/tmp/d3c0846213701bd126dfb47387930e13c869b110d4927b944e9eca8e775d84f0.jpg", "img_caption": ["Figure 5: Comparison of the query efficiency between ZOPO and baselines. The first and second rows show the test and validation accuracies. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.2 Improving Chain-of-Thought Prompt ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Reasoning prompts, e.g., \"Let\u2019s think step by step\" (denoted as hand-craft), have been shown effective in improving LLMs\u2019 zero-shot multi-step reasoning performance [13, 15]. We show that ZOPO can find a better chain-of-thought prompt across different arithmetic reasoning tasks, as evidenced in Tab. 2. Particularly, ZOPO produces a better prompt \"Let\u2019s find the solution by using the given information.\" on GSM8K [4] compared to other baselines, improving the performance from 71.80 (hand-craft) to 75.36. Refer to Appx. C.4 for more experimental details. ", "page_idx": 7}, {"type": "table", "img_path": "hS1jvV3Dk3/tmp/f6bca608e721fa39c90894b47fa1a8c6f6f5d51c11c1709514ffbba7e4e90a01.jpg", "table_caption": ["Table 2: The performance of the best zero-shot CoT prompt found by different methods on three reasoning tasks. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.3 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Verifying the essence of input domain. To fairly validate the importance of input domain on prompt generation, we compare the optimization performances with different prompts generated by Vicuna-13B and ChatGPT respectively, using the same embedding model SBERT (i.e., $h(\\cdot))$ . The result is shown in Table. 7 in Appx. D.6, with the performance profile in Fig. 11 suggesting that applying ZOPO on ChatGPT-generated prompts is better. We ascribe its better performance to ChatGPT\u2019s remarkable prompt generation ability. This confirms the importance of the input domain on prompt generation in our Insight II. ", "page_idx": 8}, {"type": "text", "text": "Besides, different embeddings (i.e., $\\mathcal{Z}$ ) of the same prompt candidates can potentially affect the function landscape as shown in Fig. 4. Thus, we need to study the performance of ZOPO using different embedding representations given the same set of prompts. We consider four different embeddings here: the last token embedding from Vicuna-13B, the OpenAI embedding provided through an API [26], the SBERT embedding, and a randomly projected embedding baseline. We observe from Tab. 8 in Appx. D.6 that, although last token embedding is generally better, there are certain tasks that OpenAI and SBERT embeddings perform equally well or better. Besides, random embedding shows a distinct lesser performance. This again highlights the importance of using more structured embeddings for prompt optimization and indicates the optimal choice of embedding can be task-dependent. We discuss how we might find the best embedding model and further show the generality of ZOPO by experimenting with more embedding models in Appx. D.6. ", "page_idx": 8}, {"type": "text", "text": "Study of NTK-GP and uncertainty-informed local exploration. We conducted additional experiments to validate the NTK-GP (Sec. 4.2) and uncertainty-informed local exploration (Sec. 4.3) components of ZOPO. We evaluated the impact of these components by testing two variants of the ZOPO algorithm: (a) replacing the NTK component with Mat\u00e9rn kernel (as in ZoRD), and (b) removing the uncertainty-informed local exploration. Comparisons of these variants against the original ZOPO on instruction induction tasks (see Tab. 11 in Appx. D.7) highlight the significant contributions of these components to ZOPO\u2019s overall effectiveness. ", "page_idx": 8}, {"type": "text", "text": "Additional results. The results on the GLUE benchmark in Appx. D.3 consistently validate the superior performance of ZOPO. We also demonstrate that ZOPO can handle prompt optimization in the few-shot setting in Appx. D.4. We conduct further experiments to show ZOPO generalize to different combinations of prompt generation models and black-box LLMs in Appx. D.5. We also perform an ablation study to examine the impact of a larger size of the generated prompt candidates (i.e., $|\\nu|)$ on ZOPO and $Z\\mathrm{OPO}_{\\mathrm{GPT}}$ in Appx. D.8, which suggests a relatively small set of strong prompt candidates (e.g., $|\\nu|=500;$ ) is sufficient (compared with size 1000 or 2000). Additionally, we provide more demonstrations of our empirical findings in Sec. 3 on other tasks in Appx. D.1. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Soft Prompt Tuning. To control LLMs to perform specific downstream tasks (e.g., reasoning), soft prompt tuning [17] is usually used as a lightweight method to fine-tune the LLMs by only optimizing a continuous vector prepended to the input tokens using gradient descent, in contrast to fine-tuning millions to billions of model parameters [37, 43]. However, when the gradient information of the model is inaccessible, gradient-free prompt tuning methods [40, 39, 6] are developed to alleviate human efforts in prompt design. Still, those efforts to optimize soft prompts have conventionally relied on the white-box access to the embedding layers of LLMs, making it inapplicable to state-of-the-art LLMs like ChatGPT [25] and GPT-4 [24] that can only be accessed through black-box APIs (i.e., only accept natural language as input). ", "page_idx": 9}, {"type": "text", "text": "Discrete Prompt Optimization. We refer to the process of optimizing discrete prompts as \"prompt optimization\", which is also a more practical setting as black-box LLMs only accept discrete inputs. Reinforcement learning-based methods [5, 47] focus on discrete token optimization but rely on the output distribution of the LLMs, which is not accessible in black-box API LLMs (e.g., ChatGPT). Zhou et al. [48] instead makes use of LLMs to produce promising candidate prompts through resampling without applying specific optimizations. Some recent works, EvoPrompt [10] and PB [8], further extend this model-free approach to evolutionary algorithms and design meta-prompts that explicitly ask the LLM to perform iterative mutation and crossovers of existing prompt candidates. Besides, Pryzant et al. [30] ask the LLM to perform implicit gradient descent on existing prompts; OPRO [46] uses the LLM as an implicit optimizer to perform prompt optimization, where its designed meta-prompt takes in the optimization trajectory and instructs the LLM to output a promising prompt candidate. However, these methods rely on powerful LLMs (e.g., GPT-3.5) and typically require a large number of iterations and queries to perform well. In this regard, InstructZero [3] leverages the induction ability from other white-box LLM $g(\\cdot)$ for generating the task-specific prompt $v$ that is conditioned on a continuous soft prompt $s\\in\\mathbb{R}^{d}$ . After that, the optimization on $v$ can be transformed into an optimization on the soft prompt $s$ , where BO algorithms are employed for a global black-box optimization. INSTINCT [18] further employs neural bandit algorithms and the last token embeddings from the white-box LLM to further improve the prompt optimization performance. However, these works prioritize a global optimization approach that emphasizes the exploration of the entire space. With an empirical understanding of the underlying target function (i.e., the black-box API LLMs), we propose a localized ZOO method that is in contrast to the global optimization approaches. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we first provide a thorough empirical study to understand the characteristics of the target function, and then propose our ZOPO algorithm for prompt optimization. ZOPO embraces a ZOO approach in pursuit of finding local optima efficiently. Extensive experiments on 30 instruction induction tasks, 3 reasoning tasks, and the GLUE benchmark demonstrate the efficacy of ZOPO, and ablation studies also validate the design principles and the generality of ZOPO. Besides, we propose a domain transformation that connects powerful LLMs with remarkable embedding models, which provides the flexibility of choices of input domains in prompt optimization. A limitation of this paper is the lack of principle to select LLMs and embedding models in our input domain transformation for better-performing prompt optimization, which we aim to explore in future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research is supported by the National Research Foundation Singapore and the Singapore Ministry of Digital Development and Innovation, National AI Group under the AI Visiting Professorship Programme (award number AIVP-2024-001). This research/project is supported by the National Research Foundation Singapore and DSO National Laboratories under the AI Singapore Programme (AISG Award No: AISG2-RP-2020-018). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et al. Palm 2 technical report. arXiv:2305.10403, 2023.   \n[2] Arora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R., and Wang, R. On exact computation with an infinitely wide neural net. In Proc. NeurIPS, 2019.   \n[3] Chen, L., Chen, J., Goldstein, T., Huang, H., and Zhou, T. InstructZero: Efficient instruction optimization for black-box large language models. arXiv:2306.03082, 2023.   \n[4] Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv:2110.14168, 2021.   \n[5] Deng, M., Wang, J., Hsieh, C.-P., Wang, Y., Guo, H., Shu, T., Song, M., Xing, E., and Hu, Z. RLPrompt: Optimizing discrete text prompts with reinforcement learning. In Proc. EMNLP, 2022.   \n[6] Diao, S., Huang, Z., Xu, R., Li, X., Yong, L., Zhou, X., and Zhang, T. Black-box prompt learning for pre-trained language models. Transactions on Machine Learning Research, 2023.   \n[7] Dolan, E. D. and Mor\u00e9, J. J. Benchmarking optimization software with performance profiles. Mathematical programming, 91:201\u2013213, 2002.   \n[8] Fernando, C., Banarse, D., Michalewski, H., Osindero, S., and Rockt\u00e4schel, T. Promptbreeder: Self-referential self-improvement via prompt evolution. arXiv:2309.16797, 2023.   \n[9] Flaxman, A., Kalai, A. T., and McMahan, H. B. Online convex optimization in the bandit setting: Gradient descent without a gradient. In Proc. SODA, 2005.   \n[10] Guo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., Liu, G., Bian, J., and Yang, Y. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. In Proc. ICLR, 2024.   \n[11] Honovich, O., Shaham, U., Bowman, S. R., and Levy, O. Instruction induction: From few examples to natural language task descriptions. In Proc. ACL, 2023.   \n[12] Jacot, A., Hongler, C., and Gabriel, F. Neural Tangent Kernel: Convergence and generalization in neural networks. In Proc. NeurIPS, 2018.   \n[13] Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. In Proc. NeurIPS, 2022.   \n[14] Kratsios, A. and Papon, L. Universal approximation theorems for differentiable geometric deep learning. The Journal of Machine Learning Research, 23(1):8896\u20138968, 2022.   \n[15] Lau, G. K. R., Hu, W., Liu, D., Chen, J., Ng, S.-K., and Low, B. K. H. Dipper: Diversity in prompts for producing large language model ensembles in reasoning tasks. In NeurIPS 2024 Workshop on Foundation Model Interventions, 2024.   \n[16] Lee, J., Xiao, L., Schoenholz, S. S., Bahri, Y., Novak, R., Sohl-Dickstein, J., and Pennington, J. Wide neural networks of any depth evolve as linear models under gradient descent. In Proc. NeurIPS, 2019.   \n[17] Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. In Proc. ACL, 2021.   \n[18] Lin, X., Wu, Z., Dai, Z., Hu, W., Shu, Y., Ng, S.-K., Jaillet, P., and Low, B. K. H. Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers. In Proc. ICML, 2024.   \n[19] Ling, W., Yogatama, D., Dyer, C., and Blunsom, P. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proc. Annual Meeting of the ACL, 2017. ", "page_idx": 10}, {"type": "text", "text": "[22] Moriconi, R., Deisenroth, M. P., and Sesh Kumar, K. High-dimensional bayesian optimization using low-dimensional feature spaces. Machine Learning, 109:1925\u20131943, 2020. ", "page_idx": 11}, {"type": "text", "text": "[23] Nesterov, Y. E. and Spokoiny, V. G. Random gradient-free minimization of convex functions. Found. Comput. Math., 17(2):527\u2013566, 2017.   \n[24] OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[25] OpenAI. ChatGPT. https://openai.com/blog/chatgpt, 2024.   \n[26] OpenAI. Documentation of OpenAI\u2019s text embeddings. https://platform.openai.com/docs/guides/embeddings, 2024.   \n[27] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Proc. NeurIPS, 2022.   \n[28] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback. arXiv:2203.02155, 2022.   \n[29] Patel, A., Bhattamishra, S., and Goyal, N. Are nlp models really able to solve simple math word problems? In Proc. NAACL, 2021.   \n[30] Pryzant, R., Iter, D., Li, J., Lee, Y., Zhu, C., and Zeng, M. Automatic prompt optimization with \u201cgradient descent\u201d and beam search. In Proc. EMNLP, 2023.   \n[31] Reimers, N. and Gurevych, I. Sentence-BERT: Sentence embeddings using siamese bertnetworks. In Proc. EMNLP-IJCNLP, 2019.   \n[32] Reynolds, L. and McDonell, K. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, pp. 1\u20137, 2021.   \n[33] Shen, Z., Yang, H., and Zhang, S. Optimal approximation rate of relu networks in terms of width and depth. Journal de Math\u00e9matiques Pures et Appliqu\u00e9es, 157:101\u2013135, 2022.   \n[34] Shu, Y., Cai, S., Dai, Z., Ooi, B. C., and Low, B. K. H. NASI: Label- and data-agnostic neural architecture search at initialization. In Proc. ICLR, 2022.   \n[35] Shu, Y., Dai, Z., Wu, Z., and Low, B. K. H. Unifying and boosting gradient-based training-free neural architecture search. In Proc. NeurIPS, 2022.   \n[36] Shu, Y., Dai, Z., Sng, W., Verma, A., Jaillet, P., and Low, B. K. H. Zeroth-order optimization with trajectory-informed derivative estimation. In Proc. ICLR, 2023.   \n[37] Shu, Y., Hu, W., Ng, S.-K., Low, B. K. H., and Yu, F. R. Ferret: Federated full-parameter tuning at scale for large language models. In NeurIPS 2024 Workshop on Federated Foundation Models, 2024.   \n[38] Shu, Y., Lin, X., Dai, Z., and Low, B. K. H. Federated zeroth-order optimization using trajectoryinformed surrogate gradients. In ICML 2024 Workshop on Differentiable Almost Everything: Differentiable Relaxations, Algorithms, Operators, and Simulators, 2024.   \n[39] Sun, T., He, Z., Qian, H., Huang, X., and Qiu, X. Bbtv2: Pure black-box optimization can be comparable to gradient descent for few-shot learning. arXiv:2205.11200, 2022. ", "page_idx": 11}, {"type": "text", "text": "[40] Sun, T., Shao, Y., Qian, H., Huang, X., and Qiu, X. Black-box tuning for language-model-as-aservice. In Proc. ICML, 2022. ", "page_idx": 12}, {"type": "text", "text": "[41] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned chat models. arXiv:2307.09288, 2023.   \n[42] Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proc. ICLR, 2019.   \n[43] Wei, C., Shu, Y., He, Y. T., and Yu, F. R. Flexora: Flexible low-rank adaptation for large language models. In NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability, 2024.   \n[44] Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., Lin, Q., and Jiang, D. WizardLM: Empowering large pre-trained language models to follow complex instructions. In Proc. ICLR, 2024.   \n[45] Xu, X., Wu, Z., Qiao, R., Verma, A., Shu, Y., Wang, J., Niu, X., He, Z., Chen, J., Zhou, Z., Lau, G. K. R., Dao, H., Agussurja, L., Sim, R. H. L., Lin, X., Hu, W., Dai, Z., Koh, P. W., and Low, B. K. H. Position paper: Data-centric ai in the age of large language models. In Proc. EMNLP, 2024.   \n[46] Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q. V., Zhou, D., and Chen, X. Large language models as optimizers. In Proc. ICLR, 2024.   \n[47] Zhang, T., Wang, X., Zhou, D., Schuurmans, D., and Gonzalez, J. E. TEMPERA: Test-time prompt editing via reinforcement learning. In Proc. ICLR, 2023.   \n[48] Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., and Ba, J. Large language models are human-level prompt engineers. In Proc. ICLR, 2023. ", "page_idx": 12}, {"type": "text", "text": "Appendix A Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Proof of Prop. 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We follow the ideas in [36, 38] to prove our Prop. 1. To begin with, we first introduce the following lemmas adapted from [36]: ", "page_idx": 13}, {"type": "text", "text": "Lemma A.1 (Thm. 1 in [36]). Let $\\delta\\in(0,1)$ and $\\omega\\triangleq d+2(\\sqrt{d}+1)\\ln(1/\\delta)$ . For any $z\\in{\\mathcal{Z}}$ and any $t\\geq1$ , the following holds with probability of at least $1-\\delta$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\nabla\\widetilde{F}(z)-\\mu_{t}(z)\\right\\|^{2}\\leq\\omega\\left\\|\\Sigma_{t}^{2}(z)\\right\\|\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma A.2 (Lemma B.4 in [36]). For any $z\\in{\\mathcal{Z}}$ and any $t\\geq1$ , the following holds ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\|\\Sigma_{t}^{2}(z)\\right\\|\\leq\\left\\|\\Sigma_{t-1}^{2}(\\pmb{x})\\right\\|\\ .\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof of Prop. 1. Recall that the covariance function (refer to (4)) of our derived NTK-GP conditioned on the history of function queries $\\mathcal{D}_{t}\\triangleq\\{(z_{\\tau},r_{\\tau})\\}_{\\tau=1}^{t}$ of size $t$ will be ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Sigma_{t}^{2}(z)=k^{\\prime\\prime}(z,z)-{k_{t}(z)}^{\\top}\\left({\\bf K}_{t}+\\sigma^{2}{\\bf I}\\right)^{-1}k_{t}(z)\\;.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For any $c\\in\\mathbb R$ and $z\\in{\\mathcal{Z}}$ , define $N_{z,\\beta}\\triangleq\\{z^{\\prime}\\in\\{z_{\\tau}\\}_{\\tau=1}^{t}\\mid\\|\\partial_{z}k(z,z^{\\prime})\\|^{2}\\geq\\beta\\}$ with $|N_{z,\\beta}|=N$ , the following then holds on the set $N_{z,\\beta}$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\left\\|k_{N}(z)^{\\top}k_{N}(z)\\right\\|\\overset{(a)}{\\geq}\\frac{1}{d}\\mathrm{tr}\\left(k_{N}(z)^{\\top}k_{N}(z)\\right)}&{}\\\\ {\\displaystyle\\overset{(b)}{=}\\frac{1}{d}\\mathrm{tr}\\left(k_{N}(z)k_{N}(z)^{\\top}\\right)}&{}\\\\ {\\displaystyle\\overset{(c)}{=}\\frac{1}{d}\\sum_{n=1}^{N}\\|\\partial_{z}k(z,z^{\\prime})\\|^{2}}&{}\\\\ {\\displaystyle\\overset{(d)}{\\geq}\\frac{N\\beta}{d}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $(a)$ comes from the fact the maximum eigenvalue of a matrix is always larger or equal to its averaged eigenvalues, $(b)$ is based on $\\operatorname{tr}(A B){\\dot{=}}\\operatorname{tr}(B A),$ $(c)$ is from the definition of $\\bar{k_{N}}(z)$ , and $(d)$ results from the definition of $N_{z,\\beta}$ . ", "page_idx": 13}, {"type": "text", "text": "Meanwhile, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma_{t}^{2}\\binom{a}{z}\\overset{(a)}{\\prec}k^{\\prime\\prime}(z,z)-k_{N}(z)^{\\top}\\left(\\mathbf{K}_{N}+\\sigma^{2}\\mathbf{I}\\right)^{-1}k_{N}(z)}\\\\ &{\\overset{(b)}{\\prec}\\kappa\\mathbf{I}-\\left(\\lambda_{\\operatorname*{max}}\\left(\\mathbf{K}_{N}\\right)+\\sigma^{2}\\right)^{-1}k_{N}(z)^{\\top}k_{N}(z)}\\\\ &{\\overset{(c)}{\\prec}\\kappa\\mathbf{I}-\\frac{k_{N}(z)^{\\top}k_{N}(z)}{N\\alpha+\\sigma^{2}}}\\\\ &{\\overset{(d)}{\\prec}\\left(\\kappa-\\frac{N\\beta/d}{N\\alpha+\\sigma^{2}}\\right)\\mathbf{I}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $(a)$ comes from Lemma A.2, $(b)$ is based on the assumption of $\\|k^{\\prime\\prime}(z,z)\\|\\le\\kappa$ and the definition of maximum eigenvalue. In addition, $(c)$ comes from $\\begin{array}{r}{\\lambda_{\\operatorname*{max}}(\\mathbf{K}_{N})\\leq N\\operatorname*{max}_{z,z^{\\prime}\\in N_{z,\\beta}}k(z,z^{\\prime})}\\end{array}$ (i.e., the Gershgorin theorem) and the assumption that $k(z,z^{\\prime})\\leq\\alpha$ for any $z,z^{\\prime}\\in{\\mathcal{Z}}$ , and $(d)$ is based on the results in (8). ", "page_idx": 13}, {"type": "text", "text": "Finally, by introducing the results above into Lemma A.1, we conclude the proof. ", "page_idx": 13}, {"type": "text", "text": "Appendix B Broader Impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As LLMs have recently received great popularity in human society and their various applications have significantly affected many aspects of society, it is important to make sure the technology related to LLMs is helpful and harmless. Our work focuses on improving the performance of black-box LLMs by automatically optimizing the prompts, which can significantly save human efforts in prompt engineering. However, such work can be potentially used for malicious purposes. When an adversarial user defines a harmful objective function, our work could be exploited to output harmful prompts that lead to negative societal impacts. Therefore, we urge the black-box LLM API providers to impose a security check for the prompt that prevents users from querying for malicious purposes. ", "page_idx": 14}, {"type": "text", "text": "Appendix C Details of Experimental Settings ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Evaluation Metrics ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Following previous works [48, 18], we use the F1 score for tasks including common_concept and informal_to_formal; we use the exact set matching for orthography_starts_with and taxonomy_animal; we use the set containing for synonyms; we use the exact matching metric for the rest of instruction induction tasks; and we use the accuracy metric for the arithmetic reasoning datasets. ", "page_idx": 14}, {"type": "text", "text": "As the number of datasets is tremendous, we use the performance proflie [7] as the evaluation metric that measures the frequency (i.e., $\\rho(\\tau))$ of a method within some distance (i.e., $\\tau$ ) from the optimality achieved by any method, defined below ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\rho_{m}(\\tau)=\\frac{1}{|\\Pi|}\\left|\\{\\pi\\in\\Pi:r_{\\pi}^{*}-r_{\\pi,m}\\leq\\tau\\}\\right|\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\Pi$ is the set of all tasks, $r_{\\pi,m}$ is the accuracy of method $m$ on task $\\pi$ , and $r_{\\pi}^{\\ast}=\\operatorname*{max}\\{r_{\\pi,m}:$ $\\forall m\\in\\mathcal{M}\\}$ is the best performance achieved by any method in $\\mathcal{M}$ on task $\\pi$ . Specifically, $\\boldsymbol{\\rho}(0)$ represents the number of tasks where a method achieves the best performance. Accordingly, we use both $\\rho(0)$ and $\\rho(5)$ as the evaluation indicators in our tables to report the results. ", "page_idx": 14}, {"type": "text", "text": "C.2 Hyperparameters ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For all experiments using ZOPO in this work, we set the learning rate to 0.01, the uncertainty thresholds $\\lambda,\\xi$ to 0.1 and 5 respectively, and the number $n$ of nearest neighbors to query in local exploration (Section 4.3) to 10. A neural network with 2 fully connected layers of size 32 and ReLU activation functions is used in NTK-GP as the kernel. We use 20 nearest neighbors to fti the NTK-GP. ", "page_idx": 14}, {"type": "text", "text": "C.3 Instruction induction ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this subsection, we describe the experimental details of the instruction induction tasks. ", "page_idx": 14}, {"type": "text", "text": "C.3.1 Experimental Specifications ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The same data partition and evaluation process as in previous works [48, 3, 18] is adopted in this work, where, for each task, we optimize the generated prompt on a training set $\\mathcal{D}$ , and report the best-performing prompt\u2019s inference accuracy on a held-out test set $\\mathcal{D}_{T}$ . Specifically, 5 examples are sampled from the training set as the demonstrations (i.e., $\\mathcal{D}_{\\mathrm{demo}}\\,$ ) for instruction induction, and another sampled 20 examples from the training set are used as the validation set $\\mathcal{D}_{V}$ to evaluate the objective function value as in Equation (1). The total query budget for each instruction induction task is fixed at 165 for all methods. ", "page_idx": 14}, {"type": "text", "text": "C.3.2 Implementation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To comprehensively compare with the baseline methods, we use GPT-3.5-turbo-0301 (supported by OpenAI API) as the black-box model for prompt evaluation and Vicuna-13B-v1.1 as the white-box LLM (i.e., $g(\\cdot))$ to generate the task-specific prompts by feeding $g(\\cdot)$ with randomly sampled soft prompts and $\\mathcal{D}_{\\mathrm{demo}}$ , which is the same as InstructZero and INSTINCT. In the experiments, we only generate 500 prompt candidates for ZOPO (i.e., $|\\gamma|=500;$ . Similarly, we also use 40 out of the 165 queries for random initialization of our optimization method, which could serve as the only global exploration of the function landscape at the beginning of local optimization. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "To tackle the high dimensionality of soft prompt (i.e., 5120 for one token embedding as in Vicuna13B) in optimization, InstructZero and INSTINCT use random projection to project the soft prompt into a much smaller intrinsic dimension (e.g., 100). This intrinsic dimension may empirically affect the quality of generated prompts, as shown in Lin et al. [18]. Therefore, tuning the intrinsic dimension and the soft token length could lead to better performance. Previous methods (i.e., InstructZero and INSTINCT) perform a grid search over the intrinsic dimension in {50, 100, 200} and the soft token length {3, 5, 10} on the validation set and report the accuracy on a held-out test set using the best prompt found using the validation set. We also adopt this technique in ZOPO here for fair comparison. The soft prompt will be concatenated with the tokenized embedding of the prompt generation template to generate task-specific prompt from Vicuna-13B. The prompt generation template and the prompt evaluation template are shown below in the bounding boxes. ", "page_idx": 15}, {"type": "image", "img_path": "hS1jvV3Dk3/tmp/861b35cc47b1ac87641e1e9db889d52777f7777bfed511a29f308843febd2165.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "We directly use the reported results of APE, IntructZero, and INSTINCT from Lin et al. [18] for comparison, and we report the results of EvoPrompt with our re-implementation. For a fair comparison, we also use Vicuna-13B for generating the initial prompt population (of size 20) for EvoPrompt, and we use GPT-3.5 turbo to perform the genetic algorithm in EvoPrompt and generate its new prompts. Using GPT-3.5 turbo to generate new prompts will help improve EvoPrompt\u2019s performance, as compared with using the relatively smaller model Vicuna-13B. ", "page_idx": 15}, {"type": "text", "text": "C.3.3 Experimental Details on Query Efficiency ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To facilitate a more comprehensive comparison of different prompt optimization methods at different query budget scales, we set the maximum query budget to 200, and report the test accuracy of the best prompt found on the validation set with each incremental query budget, as shown in Fig. 5 in the main text. We report the mean accuracy and standard error, using 3 runs with different random seeds. For InstructZero, INSTINCT, and ZOPO, we directly fix the intrinsic dimension for generating the soft prompt as 10 and the number of soft tokens as 5, without using the validation set to perform a grid search over the intrinsic dimension and the number of soft tokens. ", "page_idx": 15}, {"type": "image", "img_path": "hS1jvV3Dk3/tmp/e2608322af3656247013cc9bc96e5f843203602d8f565e7e603f754586b6c1df.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "C.3.4 Experimental Details on ZOPOGPT ", "page_idx": 16}, {"type": "text", "text": "For our experiment on $Z\\mathrm{OPO}_{\\mathrm{GPT}}$ in the main text, we apply ZOPO on ChatGPT (i.e., GPT-3.5 turbo) generated prompts. We follow the generation template from APE [48], as shown above, to generate task-specific prompts from ChatGPT. To generate various prompts using the APE method, we need to sample different sets of demonstrations (i.e., $\\mathcal{D}_{\\mathrm{demo,}}$ ) from the training set, and, for each $\\mathcal{D}_{\\mathrm{demo}}$ , we also need to randomly sample from the ChatGPT\u2019s response by setting a high temperature (e.g., 0.95). To maintain the same size of prompt candidates as in the previous experimental setting of ZOPO, we also generate 500 prompt candidates for each instruction induction task. To harness the representation power of existing embedding models, we adopt the sentence transformer model [31] \u201call-mpnet-base-v2\u201d from HuggingFace to generate the high-dimensional sentence embedding for each generated prompt from ChatGPT. ", "page_idx": 16}, {"type": "text", "text": "C.4 Improving Chain-of-Thought Prompt ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To improve the zero-shot chain-of-thought prompt performance on arithmetic reasoning tasks, we make use of the LLM\u2019s induction ability and enable LLMs to generate different chain-of-thought prompt candidates by providing some example chain-of-thought prompts. We consider the evaluation of our method on three arithmetic reasoning datasets (i.e., GSM8K[4], AQUARAT[19], SVAMP[29]). Similar as APE [48], we use all data from the test set for GSM8K and AQUARAT, and we sample 400 data points from AQUARAT\u2019s test set to evaluate the corresponding test accuracy. For all these three datasets, we sample 200 data points from their training dataset respectively as their individual validation dataset. ", "page_idx": 16}, {"type": "text", "text": "We follow the experimental setting of Lin et al. [18]: use the soft prompt to generate prompts from Vicuna-13B with a fixed intrinsic dimension of 1000 and search the soft token length {3, 5, 10} on the validation set. The corresponding prompt generation template is given below. ", "page_idx": 16}, {"type": "image", "img_path": "hS1jvV3Dk3/tmp/2f4712ec6ade0af1b7d5df2c795335d0dbdd59756dd445b8f1a6debba1c5da1c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "C.5 Details on Compute Resources ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "All experiments are conducted on a server with Intel(R) Xeon(R) CPU and NVIDIA H100 GPUs. We mainly perform the prompt optimization for the GPT-3.5-Turbo model (for which OpenAI charged USD 0.5 per 1M tokens for input and USD 1.5 per 1M tokens for output). The time of execution of our algorithm on each prompt optimization task (e.g., any task in the 30 instruction induction tasks) normally takes less than 20 minutes, where the actual time would depend on OpenAI API\u2019s response speed. ", "page_idx": 17}, {"type": "text", "text": "Appendix D Additional Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 Extended Empirical Study on Function Landscape ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Section 3, we have empirically studied the landscape of the target function and incorporated the findings into the design of ZOPO. In the main text, we have demonstrated the results on three instruction induction datasets, including taxonomy_animal, cause_and_effect, and informal_to_formal. Here we use more datasets to validate our findings. Due to the large size of instruction induction tasks (i.e., 30 tasks in total) and the query budget limit (i.e., it incurs monetary costs when we query the objective function ChatGPT to evaluate the prompt on the given task), we only experiment with few more randomly chosen tasks here to further validate our findings. ", "page_idx": 18}, {"type": "text", "text": "D.1.1 Local Optima vs. Global Optimum ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To validate our local optimization design, we study the local optima in the function landscape, by using a 3-dimensional (reduced by t-SNE) scatter plot to represent the prompt embeddings (last token embeddings from Vicuna-13B). Here we provide the empirical results on more instruction induction tasks, shown in Fig. 6. The heatmap color represents the validation accuracy of the corresponding prompt. This allows us to interpret the local optima visually, and we conclude that many local optima can already exhibit compelling performance. ", "page_idx": 18}, {"type": "image", "img_path": "hS1jvV3Dk3/tmp/84ce83f0e599da322eb7ee21101cf72f6d307e4bdb981984253ddfb53a11c0f8.jpg", "img_caption": ["Figure 6: The validation accuracy of 300 randomly sampled prompts with the last token representation on various tasks. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "D.1.2 Essense of Input Domain ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Prompt Generation To study the prompt quality of different prompt generation methods, we compare the prompts generated from Vicuna-13B and those generated from ChatGPT (i.e., GPT 3.5 turbo). For Vicuna-13B, we use the randomly sampled soft prompts with a fixed intrinsic dimension of 200 and a number token length of 10. For ChatGPT, we randomly sample prompts from the ChatGPT\u2019s response by using the APE generation template filled with random example demonstrations. For each generation method on each task, we generate 300 random prompts, and we query the target function with all prompts. We show the validation accuracy distribution of prompts generated by the two methods on four more (due to budget constraints) tasks here in Fig. 7. It demonstrates that ChatGPT has a larger probability of generating prompts with higher accuracy, also with a larger mean. The result shows that ChatGPT-generated prompts are generally better, further validating our finding of the importance of the input domain. ", "page_idx": 18}, {"type": "image", "img_path": "hS1jvV3Dk3/tmp/5a715288747c437712403acc604ccf7d05f3106499510355cc492ff425b6e7cf.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 7: The estimated accuracy distribution of prompts generated by Vicuna-13B or ChatGPT on various instruction induction tasks, where the vertical dotted line indicates the mean performance. ", "page_idx": 18}, {"type": "text", "text": "Prompt Embedding The complexity of modeling the target function depends on its function landscape defined by the embedding domain. To empirically analyze the black-box target function, we show the accuracy landscape of different tasks, where we reduce the dimension of the prompt embedding (we use the last token embedding of Vicuna-13B here) to 2 by using t-SNE. The loss landscape is visualized in the surface plot shown in Fig. 8. We observe that different optimization methods achieve similar performances on tasks like sentiment and singular_to_plural, as they have many good local optima. For other challenging tasks with complex function landscapes, the good local optima are less, but our methods can still achieve superior performance. This validates our insight that there are many good local optima in the embedding space. ", "page_idx": 19}, {"type": "image", "img_path": "hS1jvV3Dk3/tmp/77338bdcfd73732d07a9c9f7921db8785055daf8dc0cf311706ae807170b6b98.jpg", "img_caption": ["Figure 8: The function surfaces on various tasks using the last token embedding from Vicuna-13B as the representation for prompt candidates that are generated by Vicuna-13B, with contour plots shown below. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "D.2 Comparison on Instruction Induction Tasks ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In Section 5.1 of the main text, we compared our methods with other baselines on 20 challenging instruction induction tasks. Here we provide the full results on 30 instruction induction tasks in Section 5.1. ", "page_idx": 20}, {"type": "table", "img_path": "hS1jvV3Dk3/tmp/a7e78162548a48182d3250c8525aaf9ca34f3805a10848c68a08b8687c7ea21a.jpg", "table_caption": ["Table 3: Average test accuracy with standard error (3 runs) for the best prompt found by different methods for all 30 instruction induction tasks. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "The performance profile of $Z\\mathrm{OPO}_{\\mathrm{GPT}}$ compared against other baseline methods is shown in Fig. 9.   \nThis corresponds to the result shown in Tab. 1. ", "page_idx": 21}, {"type": "image", "img_path": "hS1jvV3Dk3/tmp/c1cefac53b553675e8eba04d10458c920388774851140871a221331b58125db0.jpg", "img_caption": ["\u00bf "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 9: The performance profile of $Z\\mathrm{OPO}_{\\mathrm{GPT}}$ compared against other baseline methods on 20 instruction induction tasks. ", "page_idx": 21}, {"type": "text", "text": "We also provide additional results on other instruction induction tasks to compare ZOPO against baseline methods in terms of query efficiency. The result is shown in Fig. 10. ", "page_idx": 21}, {"type": "image", "img_path": "hS1jvV3Dk3/tmp/5aa2e98b2d19dde6efa53dc0fcfcdc7e8935ffce790049aeae6050a9c4d21057.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 10: Comparison of the query efficiency between ZOPO and other existing baselines on various instruction induction tasks. ", "page_idx": 21}, {"type": "text", "text": "D.3 Results on the GLUE Benchmark ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We here follow the same experimental setting as our previous experiments on the instruction induction tasks and apply the prompt optimization on the GLUE benchmark, which consists of several more traditional natural language processing tasks. The result in Tab. 4 shows that our method ZOPO is still able to achieve advanced performance when compared with other baselines. ", "page_idx": 22}, {"type": "table", "img_path": "hS1jvV3Dk3/tmp/0b3eae12f8afa34324a2f91a7d77f71421df7b9e80534767ec253331775fd728.jpg", "table_caption": ["Table 4: Test accuracy achieved by different methods on GLUE tasks. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "D.4 Few-shot Setting ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Our algorithm ZOPO is in fact able to handle the few-shot settings as evidenced by the results in Tab. 5 below. Interestingly, the performance of our ZOPO is even better in a few-shot setting, which is reasonable since in-context exemplars will help the black-box models better understand the context and the output format of the task, and consequently will be able to lead to a better performance than the zero-shot setting. In this few-shot experiment, we provide exemplars for prompt evaluation and also report the test accuracy of the best prompt with exemplars provided. ", "page_idx": 22}, {"type": "table", "img_path": "hS1jvV3Dk3/tmp/80b0d23c6557893357a531c60dfb614bdf6a1a3c47bff2c218157cf001c2cabc.jpg", "table_caption": ["Table 5: Test accuracy achieved by ZOPO under zero-shot and few-shot settings on instruction induction tasks. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "D.5 Results of Different Combinations of Generation and Evaluation Models ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We use further experiments to show that our method can generalize to different prompt generation models and different black-box API LLMs for prompt evaluation (as $f(\\cdot))$ . Specifically, we here consider two open-sourced models: Vicuna-13B and WizardLM-13B [44] for the prompt generation and we use their corresponding last token embeddings in our algorithm. For the black-box API LLMs, we consider GPT-3.5 (the one considered in our main text), PaLM2 [1], and GPT-4. In total, we have six combinations. Tab. 6 shows the results of different combinations on the instruction induction tasks. The results show that our ZOPO performs well on all these black-box API models (with GPT-4 performing the best on most tasks), which further verifies the generality of our method when a different black-box LLM $f(\\cdot)$ is considered in the objective function in Eq. 1. We also notice that the Vicuna model generally performs better than the WizardLM model, which suggests it is more suitable for prompt generation and representation when applying our method ZOPO. ", "page_idx": 23}, {"type": "table", "img_path": "hS1jvV3Dk3/tmp/3ba36d647625cb8ec90ddbdb10e833ec77f3b927c2c1d159c5e30cd45a106c99.jpg", "table_caption": ["Table 6: Test accuracy on instruction induction tasks with different black-box LLMs $f(\\cdot)$ considered in the objective function in Eq. 1. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "D.6 Verifying the Essence of Input Domain ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Prompt Generation To fairly compare the effect of prompts generated by Vicuna-13B and ChatGPT in terms of the optimization performance by using ZOPO, we adopt the same embedding representations here, that is we use the SBERT embedding model for both prompts generated by Vicuna-13B and ChatGPT. For the prompt generation process, we fix the number of prompt candidates for both methods to 500. The result of the comparison on 20 instruction induction tasks is shown in Table. 7, where the corresponding performance profile shown in Fig. 11 suggests that applying ZOPO on ChatGPT-generated prompts is better than applying it on Vicuna-generated prompts. This again confirms the importance of the choice of the input domain (i.e., the prompt generation). ", "page_idx": 24}, {"type": "image", "img_path": "hS1jvV3Dk3/tmp/79b5ce6e77837056bb36a771799d76716e8bac71edcacc68a326a907570139ed.jpg", "img_caption": ["Figure 11: The corresponding performance profile for results shown in Tab. 7. "], "img_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "hS1jvV3Dk3/tmp/b6ddb28dd57cc2aff4019727cc2c842569098a4cef278fe742db762f2dd20f27.jpg", "table_caption": ["Table 7: Fair comparison of the optimization performance of ZOPO with different generated prompts but the same embedding model (i.e., SBERT). "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Prompt Embedding Here we analyze how different embeddings affect the optimization of ZOPO. We first generate a fixed set of prompts of size 500 from Vicuna-13B as those in Tab. 1. For the same set of prompts, we consider four different embeddings here: (a) the Last Token embedding from Vicuna-13B (b) the OpenAI embedding obtained through its embedding model \u201ctext-embedding-ada002\" API. [26], (c) the SBERT embedding obtained through the sentence transformer (\u201call-mpnetbase-v2\u201d from HuggingFace), and (d) the Random embedding obtained by randomly projecting the Vicuna embedding into the same dimension. The dimensions of the four embeddings (from (a) to (d)) are 1536, 756, and 5120 respectively. We compare the optimization performance of the four embeddings using ZOPO and the results are shown in Tab. 8. We observe although last token embedding is generally better, there are certain tasks that OpenAI and SBERT embeddings perform equally well or better, which indicates the optimal choice of embedding can be task-dependent. Intuitively, random embedding is not representative. Its lesser performance shown in Tab. 8 again confirms our Insight II in Sec. 3.2, which says the choice of embedding/input domain is important in prompt optimization. ", "page_idx": 25}, {"type": "table", "img_path": "hS1jvV3Dk3/tmp/370f99e64d9b8240991690d112a7c654c4c53490c79125354c32ac08bd882a7b.jpg", "table_caption": ["Table 8: Average test accuracy with standard error (3 runs) for the best prompt found by ZOPO with four different embeddings on 20 instruction induction tasks. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "To demonstrate the generality of ZOPO, we provide more results below to show we are not focusing on a specific combination of LLMs and embedding models. For the same optimization objective (i.e., we still perform prompt optimization for ChatGPT), we further study our ZOPO with more choices of embedding models and pair it with different black-box LLMs for prompt generation (i.e., GPT-4). We consider three more embedding models from HuggingFace and OpenAI, including \"Instructor-Large\", \"MiniLM-L6-v2\", and \"text-embedding-3-small\" in Tab. 9 below, as well another black-box model (i.e., GPT-4) for prompt generation in Tab. 10 below. ", "page_idx": 25}, {"type": "text", "text": "Note that, in our main text, we use the Vicuna-13B model as the prompt generation model mainly for the fair comparison against baselines (i.e., InstructZero and INSTINCT), and we can tell from Tab. 9 that the Vicuna embedding is a good embedding to use in prompt optimization. The result from Tab. 10 suggests choosing a better embedding model (i.e., Instructor-Large) for GPT-4 generated prompts can even further improve its performance. ", "page_idx": 25}, {"type": "text", "text": "Can we possibly find the best embedding model? As we have shown in our previous experiments, the best choice for the embedding model can be task-dependent. To find the suitable paired embedding model without enumerating every embedding model, we could possibly analyze the variance $V_{e}$ of the eigenvalues of the covariance matrix of the embeddings (i.e., $\\mathcal{Z}$ ). The eigenvalues represent the variances along the principal directions (eigenvectors) of the embeddings. If the embeddings are distributed with equal spacing in the high-dimensional space, we would expect the eigenvalues to be approximately equal (i.e., low variance). Intuitively, if the embeddings are in such representative space with equal spacing, it could help function modeling. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Therefore, if such variance $V_{e}$ is small, the optimization performance using such an embedding model is more likely to be better. Based on the test accuracies from Tab. 9 and Tab. 10, we show that there exists a sufficiently high negative Spearsman\u2019s correlation (i.e., the average correlation is -0.47) between $V_{e}$ and the performance of ZOPO using different embedding models on each task. Therefore, we can check every embedding prior to the experiment, and it can be done efficiently. We acknowledge that this approach is not perfect and finding the best embedding model is not the main focus of this work. By finding the best pair of generation and embedding models for prompt optimization, we believe our ZOPO algorithm can be further boosted. We would like to take it as a potential future direction. ", "page_idx": 26}, {"type": "table", "img_path": "hS1jvV3Dk3/tmp/a1463f4b9326b0686887fdf45d7d58e74b87108e0dd0a57733d385442e9679f7.jpg", "table_caption": ["Table 9: Test accuracy achieved by ZOPO (Vicuna-13B for prompt generation) with different embeddings on GLUE tasks. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "hS1jvV3Dk3/tmp/eb8529834b6c823c2de4e1210530268afa4338d78993ade68e0fe7ef19437419.jpg", "table_caption": ["Table 10: Test accuracy achieved by $Z\\mathrm{OPO}_{\\mathrm{GPT}-4}$ (GPT-4 for prompt generation) with different embeddings on GLUE tasks "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "D.7 Study of NTK-GP and Uncertainty-Informed Local Exploration ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "To validate the effectiveness of the components, namely NTK-GP (in Sec. 4.2) and uncertaintyinformed local exploration (in Sec. 4.3) of ZOPO, we perform controlled experiments to replace these components. Specifically, we (a) replace the NTK component with Mat\u00e9rn kernel (as in the recent ZOO method ZoRD), and (b) remove the uncertainty-informed local exploration feature. We evaluate the two settings on 20 instruction induction tasks. The result shown in Table 11 illustrates these two settings are both significantly worse than the original ZOPO, which validates the effectiveness of NTK-GP and uncertainty-informed local exploration. ", "page_idx": 27}, {"type": "table", "img_path": "hS1jvV3Dk3/tmp/5203bc40992b218ae9e15f24665caa6d22be4475c87e5303048a02bfd52ac31c.jpg", "table_caption": ["Table 11: Ablation study of the design components in ZOPO showing the average test accuracy reported with standard error (3 runs) on 20 instruction induction tasks. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "D.8 Study of ZOPO with More Prompt Candidates ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Intuitively, generating more prompt candidates offers a closer approximation to the true function landscape. As our optimization method ZOPO is operated under a given set of prompt candidates, we here conduct an ablation study to examine the impact of a larger size of the generated prompt candidates (i.e., $|\\nu|)$ on the optimization performance. For ZOPO, we use random soft prompts to feed Vicuna-13B and generate prompts until $\\mathcal{V}=500$ or $\\mathcal{V}=2000$ . We compare the optimization results of ZOPO using the two different sizes of prompts, and the results are shown in Table 12. We also follow the APE generation template to prompt ChatGPT to generate different sizes of prompt candidates and use SBERT to produce their embeddings. For ChatGPT-generated prompts in $Z\\mathrm{OPO}_{\\mathrm{GPT}}$ , we also consider two settings, $\\mathcal{V}=500$ or $\\mathcal{V}=1000$ (due to budget constraint). The corresponding result is shown in Table 13. We observe from the two tables that a larger set of prompt candidates may not necessarily lead to strictly better performance, and generating a relatively small set of strong prompt candidates (e.g., of size 500) is already good enough when we aim to find the optimal prompt. ", "page_idx": 28}, {"type": "table", "img_path": "hS1jvV3Dk3/tmp/c1fa16ebd4be8c085a34702de36110a28a644464b5f02c236a0a5a36cf1d1bdb.jpg", "table_caption": ["Table 12: Ablation study of different sizes of prompt candidates in ZOPO. "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "hS1jvV3Dk3/tmp/e2c33e1f80c6e3f8e40c3dad977288f6539e1f7d41d2b43120ac7421d6952293.jpg", "table_caption": ["Table 13: Ablation study of different sizes of prompt candidates in $Z\\mathrm{OPO}_{\\mathrm{GPT}}$ . "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "D.9 Best Prompts Found ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We list the best prompts discovered by our method ZOPO for every instruction induction task here in Table 14, which corresponds to the results in Table 3. ", "page_idx": 29}, {"type": "table", "img_path": "hS1jvV3Dk3/tmp/a20043acfeea73b14e5bfcaa28ac29082a1d23b6432e3d8a285b8396f8cf5109.jpg", "table_caption": ["Table 14: The best prompts discovered by our method ZOPO for every instruction induction task, where \u201c\\*\u201d indicates the best prompt is found by $Z\\mathrm{OPO}_{\\mathrm{GPT}}$ for that task. "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "hS1jvV3Dk3/tmp/7d94e588963928febb9b5bb4876ccc7a8944e2f9bff844913aec53e71e50a29d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have provided extensive experimental results to validate the efficacy of our proposed method. The claims are also empirically supported. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: The limitations are mentioned in Sec. 7. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The assumptions and proofs are included in the Appx. A. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have included all the necessary information to implement our algorithm, and the experimental details to conduct the experiments. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have included our code in the supplementary materials with exact commands. The code will be uploaded to GitHub upon acceptance of the work. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The experimental details are specified in Appx. C. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We report the average accuracy with standard error (3 runs) in our tables, and plot the error bar for the query efficiency experiments. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The computing resources needed to reproduce the experiments are stated in Appx. C.5. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: Our research adopts the ethical practices mentioned in the Code of Ethics Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: Both potential positive and negative societal impacts are discussed in Appx. B. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No new data or models in this work will be released. All data and models involved in this work have no risk for misuse. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The sources of the datasets are cited in the paper. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 35}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]