{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational for the field of large language models, introducing the concept of few-shot learning which is heavily referenced and built upon in the current paper."}, {"fullname_first_author": "Elias Frantar", "paper_title": "SparseGPT: Massive language models can be accurately pruned in one-shot", "publication_date": "2023-07-10", "reason": "This paper is directly compared against and builds upon by proposing a learnable alternative to the one-shot pruning method."}, {"fullname_first_author": "Song Han", "paper_title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "publication_date": "2016-01-01", "reason": "This paper is a seminal work on neural network pruning, providing the foundation for many of the techniques used in the current work, including structured and unstructured pruning."}, {"fullname_first_author": "Jonathan Frankle", "paper_title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks", "publication_date": "2018-03-15", "reason": "This paper introduces the lottery ticket hypothesis, a key concept in neural network pruning, which is discussed and relevant to the current work's approach to sparsity."}, {"fullname_first_author": "Eric Jang", "paper_title": "Categorical reparameterization with gumbel-softmax", "publication_date": "2016-11-07", "reason": "This paper introduces the Gumbel-softmax trick, a crucial technique used in the current work for differentiable sampling of masks, enabling the end-to-end training of the model."}]}