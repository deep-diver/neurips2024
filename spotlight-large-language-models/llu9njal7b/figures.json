[{"figure_path": "Llu9nJal7b/figures/figures_0_1.jpg", "caption": "Figure 1: Learnable N:M sparsity for Large Language Models.", "description": "This figure illustrates the concept of learnable N:M sparsity for Large Language Models (LLMs) introduced by MaskLLM.  It shows how MaskLLM learns task-specific and general masks that can be transferred to different downstream tasks, leading to lossless compression in LLMs.  The left side depicts the learnable mask generation process. The middle depicts the frozen LLM weights and learnable mask application. The right depicts the speed and memory improvements obtained. The table at the bottom summarizes the performance comparison between oneshot and MaskLLM methods.", "section": "1 Introduction"}, {"figure_path": "Llu9nJal7b/figures/figures_3_1.jpg", "caption": "Figure 2: This work introduces learnable semi-structured sparsity for LLMs. MaskLLM models mask selection as a distribution learning problem, enabling the creation of accurate masks through end-to-end training on large-scale datasets. The learned and general mask can be further transferred to downstream tasks or domains, achieving lossless compression.", "description": "The figure illustrates the MaskLLM framework.  The left side shows the end-to-end training process where a differentiable mask is learned from a mask distribution.  The right side shows how this learned mask can be transferred to different downstream tasks (e.g., French and HTML processing), resulting in lossless compression.", "section": "3 Method"}, {"figure_path": "Llu9nJal7b/figures/figures_4_1.jpg", "caption": "Figure 3: Drawing a random mask from the learnable distribution with Gumbel Softmax. Each consecutive M parameters are associated with a learnable distribution for candidate masks. All illustrated computations, including Gumbel Softmax, and the weighted averaging are differentiable.", "description": "This figure illustrates the process of sampling a mask from a learnable distribution using Gumbel Softmax.  It shows how learnable logits are transformed into a probability distribution over candidate masks.  Gumbel noise is added for differentiable sampling, resulting in a soft mask for training.  A hard mask is then derived for inference by selecting the mask with the highest probability. The entire process, from logits to final mask, is differentiable, enabling end-to-end training.", "section": "3.2 MaskLLM: Learnable Semi-Structured Sparsity"}, {"figure_path": "Llu9nJal7b/figures/figures_6_1.jpg", "caption": "Figure 4: Consumed samples vs. PPL on LLaMA-2 7B. MaskLLM requires 128 samples for the prior and outperforms SparseGPT after 1280 samples.", "description": "This figure shows the relationship between the number of consumed samples during training and the resulting perplexity (PPL) on the Wikitext-2 benchmark for the LLaMA-2 7B model using two methods: SparseGPT and MaskLLM.  The x-axis represents the number of unique samples used for training, while the y-axis represents the PPL achieved.  The plot demonstrates that MaskLLM outperforms SparseGPT, especially when a larger number of samples are used. Notably, MaskLLM only requires 128 samples to surpass SparseGPT's performance.", "section": "4.2 Learning 2:4 Sparsity in LLMs"}, {"figure_path": "Llu9nJal7b/figures/figures_7_1.jpg", "caption": "Figure 5: (a) The L1 distance of sampled masks between adjacent training steps. (b) The maximum probability of mask distribution, serving as an indicator of convergence. In our method, the randomness of mask sampling is regulated by the scaling factor \u03ba. A too-small \u03ba introduces huge randomness, resulting in slow convergence as shown in (b). And an inappropriately large \u03ba will suppress mask exploration and yield zero mask difference throughout the training process in (a).", "description": "This figure shows two graphs that illustrate the effect of the scaling factor (\u03ba) on the convergence of the mask sampling process. The first graph shows the L1 distance between sampled masks in consecutive training steps, while the second graph shows the maximum probability of the mask distribution. The results indicate that an appropriately chosen scaling factor is crucial for balancing exploration and exploitation during the mask learning process, ensuring efficient convergence without sacrificing diversity.", "section": "3.2 MaskLLM: Learnable Semi-Structured Sparsity"}, {"figure_path": "Llu9nJal7b/figures/figures_7_2.jpg", "caption": "Figure 5: (a) The L1 distance of sampled masks between adjacent training steps. (b) The maximum probability of mask distribution, serving as an indicator of convergence. In our method, the randomness of mask sampling is regulated by the scaling factor \u03ba. A too-small \u03ba introduces huge randomness, resulting in slow convergence as shown in (b). And an inappropriately large \u03ba will suppress mask exploration and yield zero mask difference throughout the training process in (a).", "description": "This figure shows two graphs, (a) and (b), that illustrate the impact of the scaling factor (\u03ba) on the learning process of MaskLLM. Graph (a) plots the L1 distance between consecutively sampled masks against the number of training steps. It demonstrates that a small \u03ba leads to high randomness and slow convergence, while a large \u03ba suppresses exploration, resulting in no change in masks. Graph (b) displays the maximum probability of mask distribution over training steps, also showing the impact of \u03ba on convergence speed.  The figure highlights the importance of selecting an appropriate \u03ba value to balance exploration and convergence in MaskLLM's learning process.", "section": "3.2 MaskLLM: Learnable Semi-Structured Sparsity"}, {"figure_path": "Llu9nJal7b/figures/figures_12_1.jpg", "caption": "Figure 1: Learnable N:M sparsity for Large Language Models.", "description": "This figure illustrates the MaskLLM approach for achieving learnable N:M sparsity in LLMs.  It shows how learnable masks are generated and transferred to downstream tasks, resulting in improved speed and memory efficiency. The diagram depicts the process of using a general mask to achieve lossless compression for different downstream tasks (e.g., French, HTML), demonstrating the transferability and efficiency gains of the MaskLLM method compared to existing approaches. ", "section": "1 Introduction"}, {"figure_path": "Llu9nJal7b/figures/figures_14_1.jpg", "caption": "Figure 6: The relative L1 norm of pruned weights compared to magnitude pruning", "description": "This figure shows the relative L1 norm of pruned weights compared to the magnitude pruning baseline. The subfigures (a) and (b) present the results for GPT-3 2B and LLaMA-2 7B, respectively.  The plots compare the L1 norm of weights after different pruning methods: magnitude pruning, learned mask (MaskLLM), Hessian-based pruning, and regularized learned masks with different regularization strengths (1e-4 and 1e-5).  The learned mask with a magnitude prior is also included for comparison. The results illustrate that the learned mask method often achieves a lower L1 norm than magnitude pruning, and that weight regularization helps maintain larger magnitudes in the remaining weights.", "section": "F Sparse Weight Regularization"}, {"figure_path": "Llu9nJal7b/figures/figures_14_2.jpg", "caption": "Figure 4: Consumed samples vs. PPL on LLaMA-2 7B. MaskLLM requires 128 samples for the prior and outperforms SparseGPT after 1280 samples.", "description": "This figure shows the relationship between the number of training samples used and the resulting perplexity (PPL) on the Wikitext benchmark for the pruned LLaMA-2 7B model.  It compares the performance of MaskLLM to SparseGPT. The results demonstrate that MaskLLM is more data-efficient, achieving good performance with fewer samples than SparseGPT. While SparseGPT shows improvement with more samples, the gains diminish beyond a certain point. MaskLLM, on the other hand, continues to improve its performance even with a larger number of samples.", "section": "4.2 Learning 2:4 Sparsity in LLMs"}, {"figure_path": "Llu9nJal7b/figures/figures_15_1.jpg", "caption": "Figure 1: Learnable N:M sparsity for Large Language Models.", "description": "This figure illustrates the concept of learnable N:M sparsity in Large Language Models (LLMs).  It shows how MaskLLM learns customized sparsity masks for different tasks (e.g., French, HTML).  The general mask, learned from a large dataset, can be transferred to new tasks, enabling efficient sparsity transfer learning. The figure also highlights the speed and memory improvements achieved by using MaskLLM for lossless compression.", "section": "1 Introduction"}]