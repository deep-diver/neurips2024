[{"figure_path": "nY0BrZdqLt/figures/figures_16_1.jpg", "caption": "Figure 1: This task is an approach to link specific highlight sentences to lines that corroborate these sentences from within a lines in an article. By using linear binary and exclusion search methods, the aim is to efficiently and accurately find sentences in the articles that support the highlights.", "description": "This figure shows the three main tasks that the paper addresses using TRLMs.  The first task is best-of-N reranking, evaluated using the AlpacaEval leaderboard.  The second task is task-based retrieval which consists of citation of answers and document retrieval. The third task is query generation for defense against jailbreaks.", "section": "3 TRLM - Time Reversed Language Models"}, {"figure_path": "nY0BrZdqLt/figures/figures_17_1.jpg", "caption": "Figure 2: This task is an approach to link specific highlight sentences to lines that corroborate these sentences from within a lines in an article. By using linear binary and exclusion search methods, the aim is to efficiently and accurately find sentences in the articles that support the highlights.", "description": "This figure illustrates the task of citation attribution.  Given a set of highlight sentences (summaries) from an article, the goal is to identify the corresponding sentences in the original article that best support each highlight.  Three search methods are employed: linear search, binary search, and exclusion search. Each method uses the TRLM (Time Reversed Language Model) to score sentences in the article based on their relevance to a given highlight. The best-scoring sentences are selected as the citations.", "section": "5.2 Citation Attribution"}, {"figure_path": "nY0BrZdqLt/figures/figures_18_1.jpg", "caption": "Figure 3: This task is used to assess the representational capability of TRLM. Here we look at how likely a document is to contain information relevant to answering a question. The language understanding of an LLM makes it likely that it produces better semantic retrieval than a simple embedding based model which is not contextual.", "description": "This figure illustrates the document retrieval task.  The goal is to find documents relevant to a given query. The method uses a TRLM model, prompted with a prefix (\"Document has an answer to\") and the query as a suffix, to achieve semantic retrieval. This approach is expected to perform better than simple embedding-based methods because of the LLM's contextual understanding.", "section": "5.3 Document Retrieval"}, {"figure_path": "nY0BrZdqLt/figures/figures_19_1.jpg", "caption": "Figure 4: Plots showing the False Negative Rate and False Positive Rate of the proposed defense strategy. Positive indicates UNSAFE response, while negative indicates SAFE response. The first plot considers 72 questions generated from the JBB dataset. The second plot considers questions from the new-HA dataset. The third plot considers 48 hard safe questions generated by GPT4, whose answers contain content that appears unsafe (from the H dataset). The fourth plot considers 49 easy safe questions from Alpaca Eval2 dataset (E dataset). TRLM-Ba (PT) - the reverse pre-trained model clearly outperforms all other cases with lower FNR rate while keeping FPR rates under check.", "description": "This figure visualizes the performance of different models in a jailbreak defense task. It shows the false negative rate (FNR) and false positive rate (FPR) for various models across four different datasets: toxic jailbreak questions (JBB), human-annotated data (HA), hard safe questions (H), and easy safe questions (E). The results demonstrate the effectiveness of the TRLM-Ba (PT) model, which achieves lower FNR while maintaining low FPR across datasets.", "section": "5.4 Defending against Jailbreak attacks"}]