[{"figure_path": "0XeNkkENuI/tables/tables_25_1.jpg", "caption": "Table 1 Hyperparameter settings for convex problems.", "description": "This table shows the hyperparameter settings used for the convex problem experiments in the paper.  It lists the number of GPUs used, batch size, number of epochs, number of random seeds used, and the beta1 parameter for the Schedule-Free optimizer.", "section": "G Experimental Setup"}, {"figure_path": "0XeNkkENuI/tables/tables_25_2.jpg", "caption": "Table 1: Hyperparameter settings for convex experiments.", "description": "This table presents the hyperparameter settings used in the convex experiments.  It shows the values used for the decay, optimizer, and beta parameters (\u03b21 and \u03b22).  These parameters are crucial components of the optimization algorithms used in the paper, and their settings influence the performance and convergence.", "section": "G.1 Convex experiments"}, {"figure_path": "0XeNkkENuI/tables/tables_25_3.jpg", "caption": "Figure 3: Deep Learning Experiments", "description": "This table presents the results of deep learning experiments comparing Schedule-Free AdamW against the baseline methods and cosine schedule for various tasks like CIFAR-10, CIFAR-100, SVHN, ImageNet, IWSLT14, fastMRI, Criteo, and OpenWebText. It demonstrates that Schedule-Free methods often outperforms other methods in terms of test accuracy or loss.", "section": "4 Experiments"}, {"figure_path": "0XeNkkENuI/tables/tables_25_4.jpg", "caption": "G.3 CIFAR-100", "description": "This table shows the hyperparameters used for the CIFAR-100 experiment.  It includes architectural details (DenseNet), training parameters (epochs, GPUs, batch size, warmup percentage), optimization settings (Schedule-Free \u03b2, learning rates for both Schedule-Free and Cosine approaches, decay, momentum), and other details like the number of seeds used.", "section": "G Experimental Setup"}, {"figure_path": "0XeNkkENuI/tables/tables_25_5.jpg", "caption": "Table 6: Comparison of the LR sensitivity of Schedule-Free training and cosine schedule training", "description": "This table compares the sensitivity of learning rate for Schedule-Free training and cosine schedule training on the ImageNet dataset using ResNet-50 architecture.  It shows the test accuracy obtained at different learning rates (0.5, 1.0, 1.5, 3.0, 5.0) for both approaches over 200 epochs.  The results highlight that schedule-free training displays a broader range of optimal learning rates, indicating robustness and less sensitivity to hyperparameter tuning.", "section": "G Experimental Setup"}, {"figure_path": "0XeNkkENuI/tables/tables_26_1.jpg", "caption": "Table 6: Comparison of the LR sensitivity of Schedule-Free training and cosine schedule training", "description": "This table compares the sensitivity of the learning rate (LR) for Schedule-Free training and cosine schedule training. It shows how different learning rates affect the performance of both methods. The comparison is important for understanding how the hyperparameters of the two methods affect their performance.", "section": "G.5 ImageNet"}, {"figure_path": "0XeNkkENuI/tables/tables_26_2.jpg", "caption": "Table 6: Comparison of the LR sensitivity of Schedule-Free training and cosine schedule training", "description": "This table compares the sensitivity of Schedule-Free and Cosine training methods to different learning rates.  It shows how the test accuracy changes for both methods with variations in the learning rate across several epochs, illustrating the relative robustness and optimal learning rate ranges for each approach.", "section": "G Experimental Setup"}, {"figure_path": "0XeNkkENuI/tables/tables_26_3.jpg", "caption": "Figure 4: Schedule-Free Adam compared to target-setting baseline on the Algoperf competition self-tuning track.", "description": "This figure compares the performance of Schedule-Free AdamW against a target-setting NAdamW baseline across various tasks in the MLCommons AlgoPerf Algorithmic Efficiency Challenge Self-Tuning track.  The plots show the normalized test metric (y-axis) against normalized time (x-axis) for each task, illustrating the relative performance of both algorithms in terms of achieving target metrics within a given timeframe.", "section": "4.1 MLCommons Algorithmic Efficiency benchmark"}, {"figure_path": "0XeNkkENuI/tables/tables_27_1.jpg", "caption": "Table 6: Comparison of the LR sensitivity of Schedule-Free training and cosine schedule training", "description": "This table shows the sensitivity analysis of learning rate for both the Schedule-Free training and cosine schedule training on ImageNet dataset. The results are presented in terms of test accuracy with respect to various learning rates.  The data demonstrates the performance of both methods across a range of learning rates, highlighting the relative robustness and effectiveness of each approach.", "section": "G.5 ImageNet"}, {"figure_path": "0XeNkkENuI/tables/tables_27_2.jpg", "caption": "Figure 5: Sensitivity to momentum values", "description": "This figure shows the sensitivity of Schedule-Free SGD performance on ImageNet to different momentum values (\u03b2).  The experiment uses a fixed learning rate of 1.5 and trains for 200 epochs. It demonstrates that the optimal momentum parameter is consistent across various training durations, indicating that it is not implicitly dependent on the training horizon.", "section": "5 Parameter Sensitivity"}, {"figure_path": "0XeNkkENuI/tables/tables_27_3.jpg", "caption": "G.10 MRI", "description": "This table shows the hyperparameter settings used for the MRI experiment.  It lists the architecture, epochs, GPUs used, batch size per GPU, acceleration factor, baseline schedule, baseline learning rate, beta2 value, low frequency lines, mask type, seeds, decay, baseline beta1, Schedule-Free learning rate, and Schedule-Free beta values.", "section": "G Experimental Setup"}, {"figure_path": "0XeNkkENuI/tables/tables_28_1.jpg", "caption": "Table G.11 Algoperf", "description": "This table shows the hyperparameter settings used for the Schedule-Free AdamW submission to the MLCommons AlgoPerf Algorithmic Efficiency Challenge Self-Tuning track.  It lists the values for learning rate, one-minus Beta1, Beta2 (default), weight decay (default), dropout rate, warmup percentage, label smoothing, and polynomial in ct average.", "section": "G Experimental Setup"}, {"figure_path": "0XeNkkENuI/tables/tables_28_2.jpg", "caption": "Table 1: Hyper-parameters for the AlgoPerf challenge self-tuning track.", "description": "This table lists the hyper-parameters used for the Schedule-Free AdamW submission to the MLCommons 2024 AlgoPerf Algorithmic Efficiency Challenge Self-Tuning track.  The self-tuning track required that a single set of hyper-parameters be used for all problems, making the choice of good defaults especially important.  The hyper-parameters listed represent a good default configuration for a broad range of deep learning problems.", "section": "G.11 Algoperf"}]