[{"type": "text", "text": "Bayesian-guided Label Mapping for Visual Reprogramming ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chengyi Cai1 Zesheng Ye1 Lei Feng2 Jianzhong Qi1 Feng Liu1\u2217 ", "page_idx": 0}, {"type": "text", "text": "1The University of Melbourne 2Singapore University of Technology and Design {chengyi.cai1,zesheng.ye,jianzhong.qi}@unimelb.edu.au feng_lei@sutd.edu.sg fengliu.ml@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Visual reprogramming (VR) leverages the intrinsic capabilities of pretrained vision models by adapting their input or output interfaces to solve downstream tasks whose labels (i.e., downstream labels) might be totally different from the labels associated with the pretrained models (i.e., pretrained labels). When adapting the output interface, label mapping methods transform the pretrained labels to downstream labels by establishing a gradient-free one-to-one correspondence between the two sets of labels. However, in this paper, we reveal that one-to-one mappings may overlook the complex relationship between pretrained and downstream labels. Motivated by this observation, we propose a Bayesian-guided Label Mapping (BLM) method. BLM constructs an iteratively-updated probabilistic label mapping matrix, with each element quantifying a pairwise relationship between pretrained and downstream labels. The assignment of values to the constructed matrix is guided by Bayesian conditional probability, considering the joint distribution of the downstream labels and the labels predicted by the pretrained model on downstream samples. Experiments conducted on both pretrained vision models (e.g., ResNeXt) and vision-language models (e.g., CLIP) demonstrate the superior performance of BLM over existing label mapping methods. The success of BLM also offers a probabilistic lens through which to understand and analyze the effectiveness of VR. Our code is available at https://github.com/tmlr-group/BayesianLM. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Repurposing pretrained models from data-rich domains [6, 28, 58] has emerged as an effective strategy to address downstream tasks without re-training a task-specific model. For visual tasks, visual reprogramming (VR) [3, 4, 48, 51]\u2013also called adversarial reprogramming [12, 38, 47]\u2013 repurposes a pretrained model for downstream tasks without changing the model. In particular, VR (full task setup detailed in Appendix A) modifies the model\u2019s input interface by adding trainable noise patterns to the images of downstream tasks. Since pretrained and downstream tasks typically have distinct label spaces, a label mapping (LM) function is needed to map outputs of the pretrained models to downstream labels. Often, existing VR methods adopt a gradient-free one-to-one LM [4, 12, 47], avoiding the computational cost of training fully-connected output layers through backpropagation. ", "page_idx": 0}, {"type": "text", "text": "However, we find that a one-to-one LM overlooks the complex many-to-many relationship between pretrained and downstream labels, which may limit the performance of VR. In Figure 1, we repurpose a model pretrained on ImageNet [45] for downstream classification tasks using a one-to-one LM strategy [4], and present statistical results. The two subfigures Figure 1a and Figure 1b, illustrate drawbacks from the perspectives of individual images and the entire dataset, respectively. Figure 1a shows the distribution of logits (i.e., model output before the softmax layer) for the most likely predicted pretrained labels of two images from downstream tasks: a \u2018Dog\u2019 image from CIFAR10 [30] and an \u2018Osteospermum\u2019 image from Flowers102 [40]. For the \u2018Dog\u2019 image, multiple pretrained labels like \u2018Chihuahua\u2019, \u2018Basenji\u2019\u2013subclasses of dogs\u2013receive high logits. Similarly, for the \u2018Osteospermum\u2019 image, pretrained labels such as \u2018Sea Urchin\u2019, \u2018Daisy\u2019, which share similar features, also score high. Despite these connections, the one-to-one LM retains only the label with the highest logit, suggesting the probabilities of other related labels are ignored. Figure 1b shows the frequency distribution of the predicted pretrained labels and the ground-truth downstream labels of downstream samples, with the diagonal representing the results derived from one-to-one LM. The \u2018Automobile\u2019 class from CIFAR10, for example, can no longer be paired with the optimal pretrained label \u2018Moving Van\u2019, which has already been greedily mapped to the label \u2018Truck\u2019, implying suboptimal label assignments. ", "page_idx": 0}, {"type": "image", "img_path": "135eKqDoRR/tmp/64edf2fc5bd972ebc2fe7986925226272df84de62c2b9d28c6d774de2c1a6963.jpg", "img_caption": ["Figure 1: Drawbacks of one-to-one LM from the perspectives of (a) individual images and (b) the entire dataset. An ImageNet-pretrained classifier is reused in downstream tasks. In (a), images \u2018Dog\u2019 and \u2018Osteospermum\u2019 from downstream tasks are mapped into only one pretrained label, respectively, ignoring other probabilities. In (b), the distribution of [predicted pretrained label $y^{\\mathrm{S}}$ , ground-truth downstream label $y^{\\mathrm{T}}]$ pairs reveals the existence of suboptimal solutions, where \u2018Automobile\u2019 cannot be paired with the optimal pretrained label \u2018Moving Van\u2019, which has already been mapped to \u2018Truck\u2019. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The above observation motivates us to go beyond these binary mappings. In Section 3, we replace the one-to-one LM function with a probabilistic LM matrix. Each matrix element is a real number that quantifies the relationship between a pretrained label and a downstream label, updated iteratively during VR optimization. This allows predictions for each downstream sample to consider diverse contributions from all pretrained labels, enabling a flexible many-to-many mapping strategy. ", "page_idx": 1}, {"type": "text", "text": "Specifically, we present Bayesian-guided label mapping (BLM) in Section 4, which assigns values to elements in the probabilistic LM matrix based on Bayesian conditional probabilities, derived from the joint distribution of the predicted pretrained labels on downstream tasks and the groundtruth downstream labels. We further extend BLM to $\\mathrm{BLM+}$ , which aggregates top- $K$ predicted probabilities instead of using a single predicted label when estimating the joint distribution, accounting for uncertainty in the predictions. We also provide a theoretical analysis that justifies the potential of probabilistic many-to-many LM to outperform deterministic one-to-one LM. ", "page_idx": 1}, {"type": "text", "text": "To show the effectiveness of BLM, experiments are conducted on 12 widely used datasets, with BLM and $\\mathrm{BLM+}$ being applied to different input VR methods\u2013padding and watermarking\u2013on pretrained ResNet and ResNeXt (see Section 5). The ablation study and parameter analysis are also included, along with visualization results and discussions of why VR is effective. BLM and $\\mathrm{BLM+}$ are also applied to vision-language models (see Appendix L) to demonstrate their general applicability. ", "page_idx": 1}, {"type": "text", "text": "In summary, both theoretical analysis and empirical findings (Tables 1-2) provide compelling evidence that BLM and $\\mathrm{BLM+}$ , grounded in Bayesian principles, facilitate VR to leverage pretrained knowledge for diverse downstream tasks. Beyond performance improvement, BLM and $\\mathrm{BLM+}$ offer insights into understanding the effectiveness of VR (Figures 3-4): revealing the relations between pretrained and downstream label spaces may guide future studies into more interpretable VR methods. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Model Reprogramming. Among cutting-edge transfer learning methods (see Appendix B), model reprogramming introduces an efficient learning framework for adapting models pretrained on largescale data to downstream tasks constrained by limited resources [7]. By changing the input or output interfaces (i.e., input or output space) purposefully, while preserving the integrity of the pretrained model, knowledge can be reused on new tasks, sidestepping exhaustive finetuning of the model. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Many recent studies focus on repurposing diverse pretrained models for downstream tasks, including pretrained vision models [1, 4, 38, 47, 48, 51] such as ResNet [17] and ViT [11], language models [15, 49] such as BERT [24], acoustic [21, 59, 60] and graph models [23]. Such repurposing encompasses several types: cross-modal (e.g., from voice to time-series [60], or vision to text [38]), different tasks within the same modality (e.g., from image classification to out-of-distribution detection [51]), and different domains within the same task (e.g., from ImageNet to medical images [47]). ", "page_idx": 2}, {"type": "text", "text": "Prompting and Input VR. Prompting incorporates meticulously designed prompts (additional parameters) into pretrained models with specific architectures to utilize pretrained models in downstream tasks. Leveraging ViT, VPT [22] integrates prompts alongside image embeddings, while EEVPT [16] further enhances VPT by embedding parameters within self-attention layers. TransHP [52] additionally learns prompt tokens to encode coarse image categories. In vision-language models such as CLIP [44], besides text-prompting methods such as CoOP [67] and CoCoOP [66], models like MaPLe [25] also learn layer-specific mapping functions that bridge vision and text. ", "page_idx": 2}, {"type": "text", "text": "Slightly different from prompt tuning, input VR offers a model-agnostic approach by introducing trainable noise to images in the input space before feeding those images into pretrained models. This process does not impact the visual effect of the images. Two prevalent techniques are paddingbased VR and watermarking-based VR. Padding-based models [4, 12, 47, 48] preserve the integrity of images while introducing trainable noise patterns to the outer frames around images, whereas watermarking-based models [1, 3, 41, 51] train noise patterns that overlay the images. ", "page_idx": 2}, {"type": "text", "text": "Output Mapping for VR. Because pretrained labels and downstream labels are often different, relying solely on input VR may be insufficient for downstream tasks. To bridge this gap, output mapping methods are introduced to facilitate alignment between different label spaces. Mainstream approaches include deep learning-based and statistical inference-based (i.e., gradient-free) LM methods. Deep learning-based methods insert a learnable fully connected layer to connect pretrained and downstream labels [27, 48]. However, for tasks with large label spaces, the additional model layers would result in extra training costs, potentially canceling the efficiency advantages of VR. ", "page_idx": 2}, {"type": "text", "text": "As for gradient-free LM methods, random label mapping (RLM) [12] establishes mappings between an equal number of randomly selected pretrained labels and downstream labels, masking out other unused ones. Frequent label mapping (FLM) [47] selects optimal one-to-one mappings using a greedy approach based on the number of pairs between pretrained and downstream labels. Iterative label mapping (ILM) [4] extends FLM by updating mappings at each epoch, refining the output label mapping as input VR patterns evolve. As depicted in Figure 1, these one-to-one mappings overlook potential probabilities and lead to suboptimal solutions. We propose BLM to address these issues. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Problem Setup. Consider a pretrained task with input and output variables $X^{\\mathrm{S}}$ and $Y^{\\mathrm{S}}$ , jointly defined over $\\bar{\\mathcal{X}}^{\\mathrm{s}}\\times\\mathcal{Y}^{\\mathrm{S}}$ , where $\\mathcal{X}^{\\mathrm{S}}\\subseteq\\mathbb{R}^{d_{\\mathrm{S}}}$ has the input dimensionality $d_{\\mathrm{S}}$ and $\\mathcal{V}^{\\mathrm{S}}=\\{1,\\dot{\\cdots},k_{\\mathrm{S}}\\}$ . We have a pretrained classifier $f_{\\mathrm{pre}}\\,:\\,\\chi^{\\mathrm{S}}\\,\\mapsto\\,\\mathbb{R}^{k_{\\mathrm{S}}}$ producing a logits vector $f_{\\mathrm{pre}}(x^{\\mathrm{S}})\\,\\in\\,\\mathbb{R}^{k_{\\mathrm{S}}}$ for each $x^{\\mathrm{S}}\\in\\chi^{\\mathrm{S}}$ . For a downstream task with input and output variables $X^{\\mathrm{T}}$ and $Y^{\\mathrm{T}}$ defined over $\\mathcal{X}^{\\mathrm{T}}\\times\\mathcal{Y}^{\\mathrm{T}}$ , where $\\mathcal{X}^{\\mathrm{T}}\\subseteq\\mathbb{R}^{d_{\\mathrm{T}}}$ has the input dimensionality $d_{\\mathrm{T}}$ and $\\mathcal{V}^{\\mathrm{T}}=\\{1,\\dots,k_{\\mathrm{T}}\\}$ , VR seeks to adapt $f_{\\mathrm{pre}}$ to the downstream task without modifying its parameters. To achieve this, VR introduces two functions: 1) input VR function $f_{\\mathrm{in}}(\\cdot|\\theta):\\mathcal{X}^{\\mathrm{T}}\\mapsto\\mathcal{X}^{\\mathrm{S}}$ with learnable parameters $\\theta$ that converts downstream inputs for compatibility with $f_{\\mathrm{pre}}$ ; and 2) output LM function $f_{\\mathrm{out}}^{\\omega}(\\cdot):\\mathbb{R}^{k_{\\mathrm{S}}}\\mapsto\\mathbb{R}^{k_{\\mathrm{T}}}$ that aligns the output logits of $f_{\\mathrm{pre}}$ with the downstream label space by a transformation $\\omega$ . Concretely, given a training dataset $\\mathcal{D}^{\\mathrm{T}}=\\{(x_{i}^{\\mathrm{T}},y_{i}^{\\mathrm{T}})\\}_{i=1}^{n}$ with $n$ training samples drawn from $\\mathcal{X}^{\\mathrm{T}}\\times\\mathcal{Y}^{\\mathrm{T}}$ for the downstream task, the training objective of VR can be formulated as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta\\in\\Theta}\\frac{1}{n}\\sum_{i=1}^{n}\\ell(\\boldsymbol{y}_{i}^{\\mathrm{T}},(f_{\\mathrm{out}}^{\\omega}\\circ f_{\\mathrm{pre}}\\circ f_{\\mathrm{in}})(\\boldsymbol{x}_{i}^{\\mathrm{T}};\\boldsymbol{\\theta})),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\ell$ is a loss function, and $f_{\\mathrm{out}}^{\\omega}\\circ f_{\\mathrm{pre}}\\circ f_{\\mathrm{in}}$ denotes the composition of input VR, pretrained model and output LM. In this study, we focus on gradient-free LM, where $f_{\\mathrm{out}}^{\\omega}$ does not introduce additional trainable parameters but strategically leverages $f_{\\mathrm{in}}$ and $f_{\\mathrm{pre}}$ to determine $\\omega$ . ", "page_idx": 2}, {"type": "text", "text": "Modeling Existing LM. As mentioned, $f_{\\mathrm{out}}^{\\omega}$ serves to find a mapping between each $y^{\\mathrm{S}}\\in\\mathcal{V}^{\\mathrm{S}}$ and $y^{\\mathrm{T}}\\in\\mathcal{Y}^{\\mathrm{T}}$ . This can be achieved by constructing an output label transformation $\\omega$ such that for each downstream sample $x_{i}^{\\mathrm{{T}}}$ , its label $\\bar{\\hat{y}}_{i}^{\\mathrm{T}}$ is predicted by arg max softmax $(\\tilde{y}_{i}^{\\mathrm{T}})$ , with: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{y}_{i}^{\\mathrm{T}}\\equiv\\left[\\begin{array}{c}{\\tilde{y}_{i}^{1}}\\\\ {\\vdots}\\\\ {\\tilde{y}_{i}^{k_{\\mathrm{T}}}}\\end{array}\\right]=f(\\boldsymbol{x}_{i}^{\\mathrm{T}})^{\\top}\\cdot\\boldsymbol{\\omega}=\\left[f(\\boldsymbol{x}_{i}^{\\mathrm{T}})_{1}\\quad\\ldots\\quad f(\\boldsymbol{x}_{i}^{\\mathrm{T}})_{k_{\\mathrm{S}}}\\right]\\left[\\begin{array}{c c c}{\\omega_{1,1}}&{\\ldots}&{\\omega_{1,k_{\\mathrm{T}}}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\omega_{k_{\\mathrm{S}},1}}&{\\ldots}&{\\omega_{k_{\\mathrm{S}},k_{\\mathrm{T}}}}\\end{array}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $f(x_{i}^{\\mathrm{{T}}})$ is shorthand for $(f_{\\mathrm{pre}}\\circ f_{\\mathrm{in}})(x_{i}^{\\mathrm{T}};\\theta)$ . $\\omega$ can be updated iteratively [4] with input VR. ", "page_idx": 3}, {"type": "text", "text": "A deterministic one-to-one relation between $\\mathrm{\\Delta\\psi^{S}}$ and $\\updownarrow{\\updownarrow}$ implies only a single \u201ccorrect\u201d $y^{\\mathrm{S}}\\in\\mathcal{Y}^{\\mathrm{S}}$ exists for each $y^{\\mathrm{T}}\\in\\mathcal{Y}^{\\mathrm{T}}$ . Formally, $\\omega$ in Eq. (2) is a binary matrix, where just a single element $\\omega_{j,k}$ is set to 1 in each column of $\\omega$ (i.e., $\\omega\\in\\{0,1\\}^{k_{\\mathrm{S}}\\times k_{\\mathrm{T}}}$ satisfying $\\begin{array}{r}{\\sum_{j=1}^{k_{\\mathrm{S}}}\\omega_{j,\\cdot}=1)}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Our Probabilistic LM. Considering aforementioned drawbacks of one-to-one mappings, we propose a probabilistic LM for VR, assigning real values to all elements in $\\omega$ (i.e., $\\omega\\in[0,\\bar{1}]^{k_{\\mathrm{S}}\\cdot}\\mathrm{\\times}k_{\\mathrm{T}}$ satisfying jkS=1 \u03c9j,\u00b7 = 1). Each element \u03c9yS,yT quantifies the relationship between yS \u2208YS and yT \u2208YT. This acknowledges contributions from all pretrained labels for the prediction of downstream samples. The flexible many-to-many LM implies the inherent complexity in label correspondence. In Section 4, we investigate how to assign values to our probabilistic LM based on Bayes\u2019 theorem. ", "page_idx": 3}, {"type": "text", "text": "4 Bayesian-guided Probabilistic Label Mapping (BLM) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 Method Demonstration ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Interpreting $p(Y^{\\mathrm{T}}|X^{\\mathrm{T}})$ . The objective of VR is to maximize $p(Y^{\\mathrm{T}}|X^{\\mathrm{T}})$ defined over the downstream task space. By using the law of total probability, we can express $p(\\dot{Y}^{\\mathrm{T}}|X^{\\mathrm{T}})$ as: ", "page_idx": 3}, {"type": "equation", "text": "$$\np(Y^{\\mathrm{T}}|X^{\\mathrm{T}})=\\sum_{y^{\\mathrm{S}}\\in\\mathcal{y}^{\\mathrm{S}}}p(Y^{\\mathrm{S}}=y^{\\mathrm{S}}|X^{\\mathrm{T}})\\,p(Y^{\\mathrm{T}}|Y^{\\mathrm{S}}=y^{\\mathrm{S}},X^{\\mathrm{T}}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Mirroring the structure of Eq. (2), Eq. (3) enables us to estimate $p(Y^{\\mathrm{T}}|X^{\\mathrm{T}})$ with the i.i.d observations $\\mathcal{D}^{\\mathrm{T}}=\\{\\bar{(x}_{i}^{\\mathrm{T}},y_{i}^{\\mathrm{T}})\\}_{i=1}^{n}$ of the downstream task, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{p}(Y^{\\mathrm{T}}|X^{\\mathrm{T}})=\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\sum_{y^{\\mathrm{S}}\\in\\mathcal{Y}^{\\mathrm{S}}}\\underbrace{p(Y^{\\mathrm{S}}=y^{\\mathrm{S}}|X^{\\mathrm{T}}=x_{i}^{\\mathrm{T}})}_{\\mathrm{\\Phi}\\mathrm{(input~VR:~}(f_{\\mathrm{pre}}\\circ f_{\\mathrm{in}})(x_{i}^{\\mathrm{T}};\\theta)}\\underbrace{p(Y^{\\mathrm{T}}=y_{i}^{\\mathrm{T}}|Y^{\\mathrm{S}}=y^{\\mathrm{S}},X^{\\mathrm{T}}=x_{i}^{\\mathrm{T}})}_{\\mathcal{O}\\mathrm{\\:output~LM:~}f_{\\mathrm{out}}^{\\omega,y^{\\mathrm{S}}}}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\circled{1}$ denotes the predicted probability of pretrained label $y^{\\mathrm{S}}$ for input $\\boldsymbol{x}_{i}^{T}$ , obtained from $f_{\\mathrm{pre}}\\circ f_{\\mathrm{in}}$ . Essentially, $\\circled{1}$ can be viewed as the standard input VR and is orthogonal to the LM methods employed; $\\circled{2}$ represents the probability that the true downstream label $y_{i}^{\\mathrm{T}}$ is mapped from the predicted $\\dot{y}^{\\mathrm{S}}$ and input $x_{i}^{\\mathrm{{T}}}$ , which amounts to estimating the output label transformation $\\omega\\in[0,1]^{k_{\\mathrm{S}}\\times k_{\\mathrm{T}}}$ . Since $\\circled{1}$ is independent of output LM, the focus now shifts to estimating $\\circled{2}$ . ", "page_idx": 3}, {"type": "text", "text": "Estimating $\\omega_{y}\\mathrm{s}_{,y}\\mathrm{r}$ Using Conditional Probability. Since $\\omega_{y}\\mathrm{s}_{,y}\\mathrm{r}$ is used to quantify the contributions from pretrained label $y^{\\mathrm{S}}$ to downstream label $y^{\\mathrm{T}}$ , we can associate it with the conditional probability: ", "page_idx": 3}, {"type": "equation", "text": "$$\np(Y^{\\mathrm{{T}}}=y^{\\mathrm{{T}}}|Y^{\\mathrm{{S}}}=y^{\\mathrm{{S}}},X^{\\mathrm{{T}}})={\\frac{p(Y^{\\mathrm{{T}}}=y^{\\mathrm{{T}}},Y^{\\mathrm{{S}}}=y^{\\mathrm{{S}}}|X^{\\mathrm{{T}}})}{p(Y^{\\mathrm{{S}}}=y^{\\mathrm{{S}}}|X^{\\mathrm{{T}}})}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "By applying $f_{\\mathrm{pre}}\\circ f_{\\mathrm{in}}$ to $\\mathcal{D}^{\\mathrm{T}}$ , we can empirically estimate the joint distribution of $p(Y^{\\mathrm{T}}=y^{\\mathrm{T}},Y^{\\mathrm{S}}=$ $y^{\\mathrm{S}}|X^{\\mathrm{T}})$ , then obtain $\\begin{array}{r}{p(Y^{\\mathrm{{S}}}=y^{\\mathrm{{S}}}|X^{\\mathrm{T}})=\\sum_{y^{\\mathrm{{T}}}\\in\\mathcal{Y}^{\\mathrm{{T}}}}p(Y^{\\mathrm{{T}}}=y^{\\mathrm{{T}}},Y^{\\mathrm{{S}}}=y^{\\mathrm{{S}}}|X^{\\mathrm{{T}}})}\\end{array}$ , and substitute them into Eq. (5). Two strategies, BLM and $\\mathrm{BLM+}$ , are presented for these estimations in this paper. To help understanding, we include a simple example to illustrate the estimation of $p(Y^{\\mathrm{{T}}}=y^{\\mathrm{{T}}},{\\dot{Y}}^{\\mathrm{{S}}}=y^{\\mathrm{{S}}}|X^{\\mathrm{{T}}})$ and $p(Y^{\\mathrm{S}}={\\bar{y}}^{\\mathrm{S}}|X^{\\mathrm{T}})$ in Appendix C. ", "page_idx": 3}, {"type": "text", "text": "BLM. Let $f(x_{i}^{\\mathrm{T}})\\,\\equiv\\,(f_{\\mathrm{pre}}\\circ f_{\\mathrm{in}})(x_{i}^{\\mathrm{T}};\\theta)$ denote the predicted logits obtained from the pretrained model for a given input $x_{i}^{\\mathrm{{T}}}$ . We define $\\begin{array}{r}{\\hat{y}_{i}^{\\mathrm{S}}=\\arg\\operatorname*{max}_{y^{\\prime}\\in\\mathcal{Y}^{\\mathrm{S}}}f(x_{i}^{\\mathrm{T}})_{y^{\\prime}}}\\end{array}$ to be the predicted pretrained label for $x_{i}^{\\mathrm{{T}}}$ and $\\mathbb{1}\\{\\cdot\\}$ to be the indicator function. Starting with the joint distribution $p(Y^{\\mathrm{T}}\\,=$ $y^{\\mathrm{T}},Y^{\\mathrm{S}}=y^{\\mathrm{S}}|X^{\\mathrm{T}})$ , we could intuitively count the frequency of $(\\hat{y}_{i}^{\\mathrm{S}}=y^{\\mathrm{S}}\\wedge y_{i}^{\\mathrm{T}}=y^{\\mathrm{T}})$ to estimate: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{p}_{\\mathrm{BLM}}(Y^{\\mathrm{T}}=y^{\\mathrm{T}},Y^{\\mathrm{S}}=y^{\\mathrm{S}}|X^{\\mathrm{T}})=\\frac{\\sum_{i=1}^{n}\\mathbb{1}\\{y_{i}^{\\mathrm{T}}=y^{\\mathrm{T}}\\}\\cdot\\mathbb{1}\\{\\hat{y}_{i}^{\\mathrm{S}}=y^{\\mathrm{S}}\\}}{n}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "135eKqDoRR/tmp/ac9b32170ce63bb1050a9b6f876a45988311623b8a60416d8a88a6201d313171.jpg", "img_caption": ["Figure 2: Learning strategy of BLM and BLM+. First, input images, incorporated with VR watermarking or padding patterns, are fed into a fixed pretrained model to obtain logits and predicted labels. Then, the true labels (of $y^{\\mathrm{T}}$ ) and predicted labels (of $y^{\\mathrm{S}}.$ ) are used to estimate $\\omega_{\\mathrm{BLM}}$ or $\\omega_{\\mathrm{BLM_{+}}}$ . Next, using $\\omega_{\\mathrm{BLM}}$ or $\\omega_{\\mathrm{BLM_{+}}}$ that reweights output logits of pretrained models for the downstream labels, the predicted results can be derived. Finally, backpropagation is performed to update the input VR. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "For $p(Y^{\\mathrm{S}}=y^{\\mathrm{S}}|X^{\\mathrm{T}})$ , in addition to summing up Eq. (6) for $y^{\\mathrm{T}}\\in\\mathcal{Y}^{\\mathrm{T}}$ , we add Laplace smoothing coefficient $\\lambda$ to ensure the denominator of Eq. (5) being non-zero, with $k_{\\mathrm{S}}$ being the size of $\\mathrm{\\Delta}{y^{\\mathrm{s}}}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{p}_{\\mathrm{BLM}}(Y^{\\mathrm{S}}=y^{\\mathrm{S}}|X^{\\mathrm{T}})=\\frac{\\sum_{y^{\\mathrm{T}}\\in\\mathcal{Y}^{\\mathrm{T}}}\\sum_{i=1}^{n}\\mathbb{I}\\left\\{y_{i}^{\\mathrm{T}}=y^{\\mathrm{T}}\\right\\}\\cdot\\mathbb{I}\\left\\{\\hat{y}_{i}^{\\mathrm{S}}=y^{\\mathrm{S}}\\right\\}+\\lambda}{n+k\\mathrm{s}\\cdot\\lambda}=\\frac{\\sum_{i=1}^{n}\\mathbb{I}\\left\\{\\hat{y}_{i}^{\\mathrm{S}}=y^{\\mathrm{S}}\\right\\}+\\lambda}{n+k\\mathrm{s}\\cdot\\lambda}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Substituting Eq. (7) and Eq. (6) back to Eq. (5) yields the estimation of $\\hat{\\omega}_{y^{\\mathrm{S}},y^{\\mathrm{T}}}$ to be $\\hat{p}_{\\mathrm{BLM}}(Y^{\\mathrm{T}}=$ $y^{\\mathrm{T}}|Y^{\\mathrm{S}}=y^{\\mathrm{S}},X^{\\mathrm{T}})$ . After column-wise sum normalization of $\\hat{\\omega}_{y^{\\mathrm{S}},y^{\\mathrm{T}}}$ to satisfy $\\begin{array}{r}{\\sum_{j=1}^{k_{\\mathrm{S}}}\\omega_{j,\\cdot}=1}\\end{array}$ (as formulated in Section 3), we obtain the final probabilistic LM, denoted as $\\omega_{\\mathrm{BLM}}$ . ", "page_idx": 4}, {"type": "text", "text": "$\\mathbf{BLM+}$ . Recall that BLM estimates $p(Y^{\\mathrm{T}}=y^{\\mathrm{T}},Y^{\\mathrm{S}}=y^{\\mathrm{S}}|X^{\\mathrm{T}})$ by frequency-counting based on a single most likely predicted label. However, this strategy disregards other high-ranking predictions that could offer valuable information. Thus, we introduce $\\mathrm{BLM+}$ , an extension of BLM that considers top$K$ predicted probabilities of the pretrained model for the estimation of $p(Y^{\\mathrm{T}}=y^{\\mathrm{T}},Y^{\\mathrm{S}}=y_{\\circ}^{\\mathrm{S}}|X^{\\mathrm{T}})$ Rather than relying solely on the tally, $\\mathrm{BLM+}$ aggregates probabilities for samples where $y^{\\mathrm{S}}$ ranks among the top- $K$ predictions. In this way, $\\mathrm{BLM+}$ acknowledges the uncertainty in $f(x_{i}^{\\mathrm{{T}}})$ and exploits other potential predictions, providing more robust estimations. ", "page_idx": 4}, {"type": "text", "text": "Let $\\mathcal{V}_{K,i}^{\\mathrm{S}}\\equiv\\{y^{\\prime}|$ arg $\\operatorname*{max}_{y_{1},\\dots y_{K}}f(x_{i}^{\\mathrm{T}})_{y^{\\prime}}\\}$ denote the set of the top- $\\mathcal{K}$ predicted pretrained labels for input $x_{i}^{\\mathrm{T}}$ , and $\\hat{p}(y^{\\mathrm{S}}|x_{i}^{\\mathrm{T}})\\equiv(\\mathrm{softmax}\\circ f)(x_{i}^{\\mathrm{T}})_{y^{\\mathrm{S}}}$ denote the predicted probability for any $y^{\\mathrm{S}}\\in\\mathcal{V}^{\\mathrm{S}}$ given $x_{i}^{\\mathrm{{T}}}$ . Then, within the $\\mathrm{BLM+}$ strategy, the joint density is approximated2 as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{p}_{\\mathrm{BLM_{+}}}(Y^{\\mathrm{T}}=y^{\\mathrm{T}},Y^{\\mathrm{S}}=y^{\\mathrm{S}}|X^{\\mathrm{T}})=\\frac{\\sum_{i=1}^{n}\\mathbb{1}\\{y_{i}^{\\mathrm{T}}=y^{\\mathrm{T}}\\}\\cdot\\hat{p}(y^{\\mathrm{S}}|x_{i}^{\\mathrm{T}})\\cdot\\mathbb{1}\\{y^{\\mathrm{S}}\\in\\mathcal{Y}_{K,i}^{\\mathrm{S}}\\}}{n}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Similar to BLM, with the Laplace smoothing coefficient being $\\lambda$ and the size of $\\mathrm{\\Delta\\mathrm{\\Omega}}^{\\mathrm{yS}}$ being $k_{\\mathrm{S}}$ $p(Y^{\\mathrm{S}}=y^{\\mathrm{S}}|X^{\\mathrm{T}})$ can be expressed by applying $\\mathrm{BLM+}$ as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{p}_{\\mathrm{BLM+}}(Y^{\\mathrm{S}}=y^{\\mathrm{S}}|X^{\\mathrm{T}})=\\frac{\\sum_{i=1}^{n}\\hat{p}(y^{\\mathrm{S}}|x_{i}^{\\mathrm{T}})\\cdot\\mathbb{1}\\{y^{\\mathrm{S}}\\in\\mathcal{Y}_{K,i}^{\\mathrm{S}}\\}+\\lambda}{n+k^{\\mathrm{S}}\\cdot\\lambda}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Combining Eq. (9) and Eq. (8) with Eq. (5), and going through all $y^{\\mathrm{T}}\\in\\mathcal{Y}^{\\mathrm{T}}$ and $y^{\\mathrm{S}}\\in\\mathcal{Y}^{\\mathrm{S}}$ , we obtain the full $\\mathrm{BLM+}$ estimation as $\\omega_{\\mathrm{BLM}_{+}}$ after column-wise sum normalization of $\\hat{\\omega}_{y^{\\mathrm{S}},y^{\\mathrm{T}}}$ , similar to BLM. In practice, we set $K=\\lfloor\\alpha\\cdot k_{\\mathrm{T}}\\rfloor$ , with ratio $\\alpha$ being a hyper-parameter that decides $K$ based on the size of downstream label space $k_{\\mathrm{T}}$ . ", "page_idx": 4}, {"type": "text", "text": "Pipeline and Learning Strategy. The learning of BLM and $\\mathrm{BLM+}$ allows for seamless integration into existing VR pipelines. It is model-agnostic (e.g., pretrained ResNet or ResNeXt) and compatible with all input VR methods (e.g., watermarking or padding). Figure 2 illustrates the learning strategy in detail. Besides, the learning pipeline of BLM is shown in Algorithm 1, while that of $\\mathrm{BLM+}$ is shown in Algorithm 2. The completed pseudocode for all LM methods (RLM, FLM, ILM, BLM, $\\mathrm{BLM+}$ ) and a more detailed discussion of involved matrix operations are in Appendix D. ", "page_idx": 4}, {"type": "table", "img_path": "135eKqDoRR/tmp/6920fba3f0ed73433331a5acb3cd75163fef39882aefd7be6505705ec8e6ad68.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "The iterative process of learning $\\omega_{\\mathrm{BLM}}$ , $\\omega_{\\mathrm{BLM+}}$ comprises these four steps: 1) Input images, with VR patterns, are fed into the fixed pretrained model to obtain output logits and predicted pretrained labels. 2) BLM and $\\mathrm{BLM+}$ replace previous LM (e.g., RLM, FLM or ILM) to estimate $\\omega$ . 3) The initial logits are reweighted using $\\omega_{\\mathrm{BLM}}$ or $\\omega_{\\mathrm{BLM_{+}}}$ , yielding refined predictions for downstream labels. 4) Loss functions (e.g., cross-entropy) and backpropagation are employed to update the input VR. ", "page_idx": 5}, {"type": "text", "text": "4.2 Theoretical Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Furthermore, we include a justification of why probabilistic many-to-many LM (e.g., BLM and $\\mathrm{BLM+})$ ) should be favored over deterministic one-to-one LM (e.g., RLM, FLM and ILM). Define the label spaces $\\smash{\\ y^{\\mathrm{S}}=\\{0,1\\}}$ and ${\\underline{{\\boldsymbol{y}}}}^{\\mathrm{T}}=\\{0,1\\}$ as binary sets3. Consider the set of potential LM functions ${\\mathcal{F}}_{\\mathrm{lm}}=\\{f_{\\mathrm{lm}}:\\mathcal{V}^{\\mathrm{S}^{\\dagger}}\\rightarrow\\mathcal{V}^{\\mathrm{T}}\\}$ , including each function $f_{\\mathrm{lm}}(y^{\\mathrm{S}})\\in\\{y^{\\mathrm{T}},1-\\dot{y}^{\\mathrm{T}}\\}$ . For any $f_{\\mathrm{lm}}\\in\\mathcal{F}_{\\mathrm{lm}}$ , the expected accuracy of $f_{\\mathrm{lm}}$ regarding the entire downstream label space is defined as4: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname{Acc}(f_{\\mathrm{lm}})=\\mathbb{E}_{\\boldsymbol{y}^{\\mathrm{T}}\\in\\boldsymbol{y}^{\\mathrm{T}}}\\left[\\sum_{\\boldsymbol{y}^{\\mathrm{S}}\\in\\boldsymbol{y}^{\\mathrm{S}}}\\boldsymbol{p}(\\boldsymbol{y}^{\\mathrm{S}})\\cdot\\boldsymbol{p}\\left(f_{\\mathrm{lm}}(\\boldsymbol{y}^{\\mathrm{S}})=\\boldsymbol{y}^{\\mathrm{T}}\\vert\\boldsymbol{y}^{\\mathrm{S}}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $p(y^{\\mathrm{S}})$ is the marginal distribution of the pretrained labels and $p\\left(f_{\\mathrm{lm}}(y^{\\mathrm{S}})=y^{\\mathrm{T}}|y^{\\mathrm{S}}\\right)$ is the conditional probability that $f_{\\mathrm{lm}}$ correctly predicts a downstream label $y^{\\mathrm{T}}$ from a pretrained label $y^{\\mathrm{S}}$ . Let $f_{\\mathrm{plm}}$ and $f_{\\mathrm{dlm}}$ denote the probabilistic LM (Definition E.1) and deterministic LM (Definition E.2), respectively. We finally prove that $\\mathrm{Acc}(f_{\\mathrm{plm}})\\geq\\mathrm{Acc}(f_{\\mathrm{dlm}})$ (Corollary E.5) in Appendix $\\mathrm{E}$ , which further verifies the effectiveness of our methods in the view of theoretical understanding. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Tasks and Baselines. Following ILM [4], we employ ResNet-18 [17] pretrained on ImageNet-1K [45] and ResNeXt pretrained on Instagram [37] to test the performance of VR. The results are evaluated on twelve downstream datasets: Flowers102 [40], DTD [9], UCF101 [46], Food101 [2], GTSRB [19], EuroSAT [18], OxfordPets [43], StanfordCars [29], SUN397 [57], CIFAR10/100 [30] and SVHN [39]. Previous gradient-free LM methods RLM [12], FLM [47] and ILM [4] are used as the baselines. The results of deep learning-based LM will also be included for reference, where LM is treated as a single-layer linear neural network connected to the output of the pretrained model for training alongside VR. More dataset and implementation details are in Appendix F. Regarding hyper-parameters of BLM, $\\lambda$ is set as 1, and the top- $K$ ratio $\\alpha$ is 0.15 (analyzed in Appendix G). ", "page_idx": 5}, {"type": "table", "img_path": "135eKqDoRR/tmp/e7c8a204e4d17341e6306bcb76930c1035e8c929bf3d4ba08d2b3835df2012d3.jpg", "table_caption": ["Table 1: Performance comparison of gradient-free output LM methods (mean $\\%\\pm\\mathrm{std}\\,\\%$ ). Ours are highlighted and the highest accuracy is in bold (with deep learning-based LM in gray for reference) "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Results for Padding-based VR. Padding-based input VR adds trainable noise to the outer frames of centered images. Table 1 shows the performance of BLM and $\\mathrm{BLM+}$ applied with padding-based input VR. BLM and $\\mathrm{BLM+}$ yield the highest accuracy across all datasets except for SVHN. On ResNet-18, compared to the SOTA (i.e., ILM), BLM achieves an average improvement of $4.7\\%$ across the 12 datasets, whereas $\\mathrm{BLM+}$ achieves a $6.1\\%$ enhancement. On ResNeXt-101, BLM and $\\mathrm{BLM+}$ achieve accuracy improvements of $3.2\\%$ and $3.8\\%$ on average, respectively. The elevation in accuracy is particularly pronounced in tasks with a higher number of classes (e.g., UCF101, CIFAR100). On SVHN, ILM performs slightly better, which could be attributed to the minimal inter-class variation and the smaller number of classes (which is 10) in SVHN, resulting in similar mapping values for different downstream labels and thus reducing our method\u2019s advantage (discussed in Appendix H). However, compared to current gradient-free LM methods, the deep learning-based LM may still have an advantage in the performance of downstream tasks due to the learning capacity of the linear layer neural network. Our proposed BLM and $\\mathrm{BLM+}$ aim to bridge the gap between gradient-free LM and deep learning-based LM. Additionally, BLM and $\\mathrm{BLM+}$ have been observed to possess greater interpretability (see Appendix I for more experiments) and fewer parameters (see Appendix J for details) compared to deep learning-based LM. ", "page_idx": 6}, {"type": "text", "text": "Table 2: Performance comparison of gradientfree LM methods for watermarking-based VR on ResNet-18 (mean $\\%~\\pm$ std $\\%$ ). Ours are highlighted and the highest accuracy is in bold ( with deep learning-based LM in gray for reference) ", "page_idx": 6}, {"type": "table", "img_path": "135eKqDoRR/tmp/e663573ce0b65ec9393c04fc5e97937cd1a124c4f1a0eb45743043a430d2ae64.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Results for Watermarking-based VR. BLM and $\\mathrm{BLM+}$ can be applied to different input VR methods. For the watermarking-based VR method, which overlays trainable noise patterns on resized images, the results of BLM and $\\mathrm{BLM+}$ with ResNet-18 as the pretrained model are shown in Table 2. Since ILM is the best-performing baseline, we only include its results here for comparison. Our BLM and $\\mathrm{BLM+}$ methods again outperform ILM, achieving an average gain in accuracy of $6.1\\%$ and $7.5\\%$ , respectively. Therefore, in the case of watermarking-based VR, BLM and $\\mathrm{BLM+}$ also close the gap between current gradient-free and deep learning-based LM. Results in Tabel 2 underscore the applicability of our output LM methods with different input VR. ", "page_idx": 6}, {"type": "image", "img_path": "135eKqDoRR/tmp/e0f7b949bb5ec9b158415961a299ab6fdef8fbf937e8e9a820de89c33336ecf4.jpg", "img_caption": ["Figure 3: Visualization results of top weighted pretrained labels $y^{\\mathrm{S}}$ and weights $\\omega_{y^{\\mathrm{S}},y^{\\mathrm{T}}}$ for some $y^{\\mathrm{T}}$ applying BLM and $\\mathrm{BLM+}$ . Downstream labels \u2018Edamame\u2019, \u2018Fibrous\u2019, and \u2018Dog\u2019 are shown as examples. ResNet-18 pretrained on ImageNet is used. More results are in Appendix K. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "cussed in Appendix L. BLM and $\\mathrm{BLM+}$ achieve the average accuracy of $79.1\\%$ and $79.3\\%$ across 12 datasets, respectively, and outperform the baseline methods on 11 datasets. ", "page_idx": 7}, {"type": "text", "text": "Ablation Study. Table 3 presents the ablation study results of BLM and $\\mathrm{BLM+}$ . For BLM, we list the results of replacing probabilistic LM with a one-to-one LM, denoted as \u2018-Bayes\u2019, and the results of calculating $\\omega_{\\mathrm{BLM}}$ only once in the first epoch without subsequent iterations, denoted as \u2018-Iter\u2019. For $\\mathrm{BLM+}$ , the removal of aggregating probabilities results in BLM; hence, we report the results of aggregating all probabilities instead of top- $K$ predicted probabilities, denoted as \u2018-Top- $\\mathbf{\\nabla}\\cdot\\mathbf{K}^{\\star}$ . Like that for BLM, \u2018-Iter\u2019 shows the results of calculating $\\omega_{\\mathrm{BLM+}}$ only once without subsequent iterations. Besides, when both \u2018-Top- $\\mathbf{\\nabla}\\cdot\\mathbf{K}^{\\star}$ and \u2018-Bayes\u2019 are applied to $\\mathrm{BLM+}$ , $\\mathrm{BLM+}$ degenerates into the same results as \u2018-Bayes\u2019 of BLM, which is displayed in the previous column of Table 3. ", "page_idx": 7}, {"type": "table", "img_path": "135eKqDoRR/tmp/c8a67bc320dab33f8f24865075a48e37beb2ebc607a8bf71f8e188334505affd.jpg", "table_caption": ["Table 3: Ablation study results of BLM and $\\mathrm{BLM+}$ , using ResNet18 as the pretrained model (showing the mean accuracies $(\\%)$ , with ours highlighted and the best in bold) "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "For BLM, employing \u2018Bayes\u2019 to compute $\\omega$ improves accuracy across most datasets, with slightly smaller gains observed for datasets with fewer classes (EuroSAT, CIFAR10, and SVHN). For $\\mathrm{BLM+}$ , applying \u2018Top- $\\mathbf{\\nabla}\\cdot\\mathbf{K}^{\\star}$ assists in filtering out redundant information, yielding positive impacts across most datasets. In particular, on the OxfordPets dataset, to classify cat and dog breeds, using top- $\\mathcal{K}$ predicted probability effectively fliters out numerous irrelevant categories in the pretrained label space, which leads to significant improvements. Furthermore, for both BLM and $\\mathrm{BLM+}$ , iterative up", "page_idx": 7}, {"type": "text", "text": "dates are crucial as the initial input VR may deviate considerably from the final iteration. The greater the disparity between the domains of downstream and pretrained tasks (GTSRB, SVHN), the more pronounced the impact of the input VR, thereby emphasizing the necessity of iteration updates. ", "page_idx": 7}, {"type": "text", "text": "Visualization Results. The probabilistic LM obtained by BLM or $\\mathrm{BLM+}$ can elucidate the connection between pretrained and downstream label spaces. Figure 3 shows the visualization results for three labels in downstream tasks, taking ResNet-18 pretrained on ImageNet-1K as an example. Each column of $\\omega$ computed using BLM or $\\mathrm{BLM+}$ is a vector with length $k_{\\mathrm{S}}=1000$ , representing the weights assigned to the 1,000 outputs\u2013one for each $y^{\\mathrm{S}}$ \u2013of the pretrained model corresponding to a downstream label $y^{\\mathrm{T}}$ . The top-weighted labels (i.e., $y^{\\mathrm{S}}$ where $\\omega_{y}\\mathrm{s}_{,y}\\mathrm{r}$ is larger) for \u2018Edamame\u2019 correspond to organisms such as snakes and artichokes, which share similarities in color and shape. Similarly, the predominant labels associated with \u2018Fibrous\u2019 from the texture dataset include roughtextured items like \u2018Hay\u2019 and \u2018Komondor\u2019. \u2018Dog\u2019 encompasses various sub-breed canines. These findings suggest that BLM and $\\mathrm{BLM+}$ establish an optimal probabilistic LM between label spaces, and handle similarity or inclusion relationship, addressing the drawbacks in Figure 1. ", "page_idx": 7}, {"type": "image", "img_path": "135eKqDoRR/tmp/75ee2f58e0f13f92c9ccd98abb6cccb174e7a601e6b796f998fe7da012f17e66.jpg", "img_caption": ["Figure 4: Visualization of input VR and top-weighted pretrained labels applying $\\mathrm{BLM+}$ . Training loss and weight changes (Euclidean norm) of probabilistic LM $\\omega_{\\mathrm{BLM+}}$ per iteration are plotted below. Pretrained ResNet-18 is used, and the downstream label \u2018Marigold\u2019 is selected as an example. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Discussion of Why VR Is Effective. From a visualization perspective, Figure 4 shows the topweighted pretrained labels and input VR patterns $\\theta$ at different iteration stages using $\\mathrm{BLM+}$ . The training loss for each iteration and changes in $\\omega$ , measured by the Euclidean norm, are also plotted. During the update of $\\omega$ and $\\theta$ , the pretrained labels with top $\\omega_{y^{\\mathrm{S}},y^{\\mathrm{T}}}$ for $y^{\\mathrm{T}}$ being \u2018Marigold\u2019 transition gradually from dissimilar labels such as \u2018Reef\u2019 and \u2018Teddy\u2019 to \u2018Cauliflower\u2019 and \u2018Pineapple\u2019 which share more similarities in color, shape and texture. Meanwhile, the training loss diminishes gradually, and $\\omega$ converges, demonstrating the effectiveness of VR and $\\mathrm{BLM+}$ . ", "page_idx": 8}, {"type": "text", "text": "Impact of Label Space Sizes $k_{\\mathrm{T}}$ . Figure 5 shows the relationship between different sizes of the downstream label space and the accuracy improvement achieved by BLM and $\\mathrm{BLM+}$ . Tasks with larger label spaces report more pronounced performance improvements. While simpler tasks with smaller label spaces might not fully showcase the power of our approach, the strength of BLM and $\\mathrm{BLM+}$ lies in unraveling the complex many-to-many relationship that often arises in tasks with more numerous classes. In such scenarios, our probabilistic LM methods demonstrate their full potential. ", "page_idx": 8}, {"type": "text", "text": "Impact of Training Dataset Sizes $n$ . Figure 6 illustrates the impact of varying training dataset sizes for the downstream task on different LM methods. Regarding CIFAR100 as the downstream task, compared with RLM and ILM, BLM and $\\mathrm{BLM+}$ yield higher accuracy consistently. With approximately a $40\\%$ fraction of the downstream training data, BLM or $\\mathrm{BLM+}$ can achieve similar accuracy compared with training on the entire dataset. ", "page_idx": 8}, {"type": "text", "text": "Other Experiments. The parameter experiments and performance analysis regarding the impact of Laplace smoothing coefficient $\\lambda$ and top- $K$ ratio $\\alpha$ for BLM and $\\mathrm{BLM+}$ are detailed in Appendix G. The visualization and analysis of LM matrices derived from gradient-free and deep learning-based methods can be found in Appendix I. Training cost analysis is discussed in Appendix J. Additional visualization results of LM methods applied to pretrained vision models are presented in Appendix K. Lastly, the application of BLM and $\\mathrm{BLM+}$ for vision-language models is explored in Appendix L. ", "page_idx": 8}, {"type": "image", "img_path": "135eKqDoRR/tmp/5ccd48a309bab8350bca2388a05abdc69a7d18012acf27d98d7c33e6c36cd99e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "135eKqDoRR/tmp/82929dade3904e6a8693a52c72b4413084fb102e73aa9cea3ae27f01a4ae2a95.jpg", "img_caption": ["Figure 6: Accuracy $(\\%)$ of methods Figure 5: Accuracy improvement $(\\%)$ of BLM and $\\mathrm{BLM+}$ when varying training dataset sizes $n$ compared with ILM given different sizes $(k_{\\mathrm{T}})$ of the down- for downstream task CIFAR100, usstream label space, using pretrained ResNet-18. ing pretrained ResNet-18. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We focus on output LM methods for VR and reveal the drawbacks in current gradient-free LM methods, which use one-to-one mappings that overly simplify the relationship between the pretrained and downstream label spaces. To address this issue, we propose BLM, which calculates probabilistic LM matrices guided by Bayes\u2019 theorem. Additionally, we aggregate the probability of top- $K$ predicted pretrained labels instead of counting a single label during the estimation of probabilistic LM matrices, yielding an improved method $\\mathrm{BLM+}$ . Both theoretical analysis and experimental results validate the effectiveness of BLM and $\\mathrm{BLM+}$ while offering insights into understanding the effectiveness of VR through a probabilistic lens. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "CYC, ZSY, and FL are supported by the Australian Research Council (ARC) with grant number DE240101089, and FL is also supported by ARC with grant number DP230101540 and the NSF&CSIRO Responsible AI program with grant number 2303037. JZQ is supported by ARC with grant number DP240101006. This research is also supported by The University of Melbourne\u2019s Research Computing Services and the Petascale Campus Initiative. We sincerely appreciate the time and dedication of the reviewers in carefully reviewing our manuscript. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola. Exploring visual prompts for adapting large-scale models. arXiv preprint arXiv:2203.17274, 2022.   \n[2] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative components with random forests. In ECCV, 2014.   \n[3] Chengyi Cai, Zesheng Ye, Lei Feng, Jianzhong Qi, and Feng Liu. Sample-specific masks for visual reprogramming-based prompting. In ICML, 2024.   \n[4] Aochuan Chen, Yuguang Yao, Pin-Yu Chen, Yihua Zhang, and Sijia Liu. Understanding and improving visual prompting: A label-mapping perspective. In CVPR, 2023.   \n[5] Hao Chen, Ran Tao, Han Zhang, Yidong Wang, Xiang Li, Wei Ye, Jindong Wang, Guosheng Hu, and Marios Savvides. Conv-adapter: Exploring parameter efficient transfer learning for convnets. In CVPR, 2024.   \n[6] Hao Chen, Jindong Wang, Ankit Shah, Ran Tao, Hongxin Wei, Xing Xie, Masashi Sugiyama, and Bhiksha Raj. Understanding and mitigating the label noise in pre-training on downstream tasks. In ICLR, 2024.   \n[7] Pin-Yu Chen. Model reprogramming: Resource-efficient cross-domain machine learning. In AAAI, 2024.   \n[8] Haoang Chi, Feng Liu, Wenjing Yang, Long Lan, Tongliang Liu, Bo Han, William Cheung, and James Kwok. Tohan: A one-step approach towards few-shot hypothesis adaptation. NeurIPS, 2021.   \n[9] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In CVPR, 2014.   \n[10] Ruijiang Dong, Feng Liu, Haoang Chi, Tongliang Liu, Mingming Gong, Gang Niu, Masashi Sugiyama, and Bo Han. Diversity-enhancing generative network for few-shot hypothesis adaptation. In ICML, 2023.   \n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2020.   \n[12] Gamaleldin F Elsayed, Ian Goodfellow, and Jascha Sohl-Dickstein. Adversarial reprogramming of neural networks. In ICLR, 2018.   \n[13] Zhen Fang, Jie Lu, Feng Liu, Junyu Xuan, and Guangquan Zhang. Open set domain adaptation: Theoretical bound and algorithm. IEEE TNNLS, 2020.   \n[14] Zhen Fang, Jie Lu, Feng Liu, and Guangquan Zhang. Semi-supervised heterogeneous domain adaptation: Theory and algorithms. IEEE TPAMI, 2022.   \n[15] Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. Warp: Word-level adversarial reprogramming. In ACL-IJCNLP, 2021.   \n[16] Cheng Han, Qifan Wang, Yiming Cui, Zhiwen Cao, Wenguan Wang, Siyuan Qi, and Dongfang Liu. E 2 vpt: An effective and efficient approach for visual prompt tuning. In ICCV, 2023.   \n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.   \n[18] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019.   \n[19] Sebastian Houben, Johannes Stallkamp, Jan Salmen, Marc Schlipsing, and Christian Igel. Detection of traffic signs in real-world images: The german traffic sign detection benchmark. In IJCNN, 2013.   \n[20] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \n[21] Yun-Ning Hung, Chao-Han Huck Yang, Pin-Yu Chen, and Alexander Lerch. Low-resource music genre classification with cross-modal neural model reprogramming. In ICASSP, 2023.   \n[22] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In ECCV, 2022.   \n[23] Yongcheng Jing, Chongbin Yuan, Li Ju, Yiding Yang, Xinchao Wang, and Dacheng Tao. Deep graph reprogramming. In CVPR, 2023.   \n[24] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, 2019.   \n[25] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Maple: Multi-modal prompt learning. In CVPR, 2023.   \n[26] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[27] Eliska Kloberdanz, Jin Tian, and Wei Le. An improved (adversarial) reprogramming technique for neural networks. In ICANN, 2021.   \n[28] Jannik Kossen, Mark Collier, Basil Mustafa, Xiao Wang, Xiaohua Zhai, Lucas Beyer, Andreas Steiner, Jesse Berent, Rodolphe Jenatton, and Effrosyni Kokiopoulou. Three towers: Flexible contrastive learning with pretrained image models. NeurIPS, 2023.   \n[29] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In ICCV workshops, 2013.   \n[30] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[31] Tao Lei, Junwen Bai, Siddhartha Brahma, Joshua Ainslie, Kenton Lee, Yanqi Zhou, Nan Du, Vincent Zhao, Yuexin Wu, Bo Li, et al. Conditional adapters: Parameter-efficient transfer learning with fast inference. NeurIPS, 2023.   \n[32] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In ICML, 2020.   \n[33] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial domain adaptation. NeurIPS, 2018.   \n[34] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adaptation with residual transfer networks. NeurIPS, 2016.   \n[35] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.   \n[36] Yadan Luo, Zijian Wang, Zi Huang, and Mahsa Baktashmotlagh. Progressive graph learning for open-set domain adaptation. In ICML, 2020.   \n[37] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In ECCV, 2018.   \n[38] Paarth Neekhara, Shehzeen Hussain, Jinglong Du, Shlomo Dubnov, Farinaz Koushanfar, and Julian McAuley. Cross-modal adversarial reprogramming. In WACV, 2022.   \n[39] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al. Reading digits in natural images with unsupervised feature learning. In NeurIPS workshop, 2011.   \n[40] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In 2008 Sixth Indian conference on computer vision, graphics & image processing. IEEE, 2008.   \n[41] Changdae Oh, Hyeji Hwang, Hee-young Lee, YongTaek Lim, Geunyoung Jung, Jiyoung Jung, Hosik Choi, and Kyungwoo Song. Blackvip: Black-box visual prompting for robust transfer learning. In CVPR, 2023.   \n[42] Junting Pan, Ziyi Lin, Xiatian Zhu, Jing Shao, and Hongsheng Li. St-adapter: Parameterefficient image-to-video transfer learning. NeurIPS, 2022.   \n[43] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In CVPR, 2012.   \n[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021.   \n[45] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 2015.   \n[46] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.   \n[47] Yun-Yun Tsai, Pin-Yu Chen, and Tsung-Yi Ho. Transfer learning without knowing: Reprogramming black-box machine learning models with scarce data and limited resources. In ICML, 2020.   \n[48] Hsi-Ai Tsao, Lei Hsiung, Pin-Yu Chen, Sijia Liu, and Tsung-Yi Ho. AutoVP: an automated visual prompting framework and benchmark. In ICLR, 2024.   \n[49] Ria Vinod, Pin-Yu Chen, and Payel Das. Reprogramming language models for molecular representation learning. NeurIPS, 2020.   \n[50] Boyu Wang, Jorge Mendez, Mingbo Cai, and Eric Eaton. Transfer learning via minimizing the performance gap between domains. NeurIPS, 2019.   \n[51] Qizhou Wang, Feng Liu, Yonggang Zhang, Jing Zhang, Chen Gong, Tongliang Liu, and Bo Han. Watermarking for out-of-distribution detection. NeurIPS, 2022.   \n[52] Wenhao Wang, Yifan Sun, Wei Li, and Yi Yang. Transhp: Image classification with hierarchical prompting. NeurIPS, 2023.   \n[53] Zixin Wang, Yadan Luo, Zhi Chen, Sen Wang, and Zi Huang. Cal-sfda: Source-free domainadaptive semantic segmentation with differentiable expected calibration error. In ACM MM, 2023.   \n[54] Wei-Hung Weng, Jonathan Deaton, Vivek Natarajan, Gamaleldin F Elsayed, and Yuan Liu. Addressing the real-world class imbalance problem in dermatology. In Machine learning for health, 2020.   \n[55] Wikipedia contributors. Cartesian product \u2014 Wikipedia, the free encyclopedia. https://en. wikipedia.org/w/index.php?title=Cartesian_product&oldid=1219343305, 2024.   \n[56] Wikipedia contributors. Cosine similarity \u2014 Wikipedia, the free encyclopedia. https://en. wikipedia.org/w/index.php?title=Cosine_similarity&oldid $=$ 1224774490, 2024.   \n[57] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In CVPR, 2010.   \n[58] Zhuoyan Xu, Zhenmei Shi, Junyi Wei, Fangzhou Mu, Yin Li, and Yingyu Liang. Towards few-shot adaptation of foundation models via multitask finetuning. In ICLR, 2024.   \n[59] Chao-Han Huck Yang, Bo Li, Yu Zhang, Nanxin Chen, Rohit Prabhavalkar, Tara N Sainath, and Trevor Strohman. From english to more languages: Parameter-efficient model reprogramming for cross-lingual speech recognition. In ICASSP, 2023.   \n[60] Chao-Han Huck Yang, Yun-Yun Tsai, and Pin-Yu Chen. Voice2series: Reprogramming acoustic models for time series classification. In ICML, 2021.   \n[61] Li Yi, Gezheng Xu, Pengcheng Xu, Jiaqi Li, Ruizhi Pu, Charles Ling, A Ian McLeod, and Boyu Wang. When source-free domain adaptation meets learning with noisy labels. arXiv preprint arXiv:2301.13381, 2023.   \n[62] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199, 2021.   \n[63] Maxime Zanella and Ismail Ben Ayed. Low-rank few-shot adaptation of vision-language models. In CVPR, 2024.   \n[64] Yichi Zhang, Yinpeng Dong, Siyuan Zhang, Tianzan Min, Hang Su, and Jun Zhu. Exploring the transferability of visual prompting for multimodal large language models. In CVPR, 2024.   \n[65] Ziyi Zhang, Weikai Chen, Hui Cheng, Zhen Li, Siyuan Li, Liang Lin, and Guanbin Li. Divide and contrast: Source-free domain adaptation via adaptive contrastive learning. NeurIPS, 2022.   \n[66] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In CVPR, 2022.   \n[67] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. IJCV, 2022.   \n[68] Yitao Zhu, Zhenrong Shen, Zihao Zhao, Sheng Wang, Xin Wang, Xiangyu Zhao, Dinggang Shen, and Qian Wang. Melo: Low-rank adaptation is better than fine-tuning for medical image diagnosis. In IEEE International Symposium on Biomedical Imaging, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "image", "img_path": "135eKqDoRR/tmp/ec03fb8374466b096c4db052a729bc00e5036663baafccb722696680f42e530b.jpg", "img_caption": ["Figure 7: The problem setting of Visual Reprogramming. The left part shows the pretrained model and corresponding dataset, while the right part shows downstream tasks. The pretrained model is fixed, whereas the input VR and output LM modules are variable. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "The task of VR reuses fixed pretrained models for downstream tasks. As illustrated in Figure 7, an input VR module operates before pretrained models, directly altering the input space of downstream tasks. Concurrently, an output LM function acts after pretrained models, taking the predicted pretrained labels as input and outputting those for downstream tasks. Hence, VR achieves the reusability of pretrained models for downstream tasks without adapting the model parameters, primarily through modifications to the input and output spaces. ", "page_idx": 14}, {"type": "text", "text": "B Recent Work in Transfer Learning ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Visual reprogramming is one type of methods that aim to obtain models on downstream tasks with the help of pretrained models. This process is similar to the aim of transfer learning which is used to leverage knowledge from a data-rich domain [13] or a pretrained model [50] to address tasks on other domains. The former is known as domain adaptation, and the latter is known as finetuning. ", "page_idx": 14}, {"type": "text", "text": "Finetuning. Given a pretrained model, finetuning uses trainable parameters to accommodate new taskspecific information of the downstream tasks. As pretrained models grow in size, recent progresses in transfer learning have prioritized parameter-efficient finetuning (PEFT) [20] to support resourcefriendly adaptation. Regarding PEFT, the prevailing methods can be categorized as follows. The most widely adopted approach is selective finetuning [45, 62], which adjusts a subset of parameters from the pretrained model while keeping the remaining components fixed, thereby reducing the total number of trainable parameters for downstream tasks. Other methods may involve adding adapters [5, 31, 42], which introduce extra trainable layers or parameters to the pretrained model and finetune only these adapters during training. Moreover, low-rank adaptation methods [20, 63, 68] have also been proposed for pretrained Vision Transformers. They apply low-rank decomposition to the parameters during training, achieving remarkable performance on downstream tasks with a significantly reduced number of parameters. Additionally, Prompt Tuning methods [16, 22, 52], similarly directed at pretrained Vision Transformers, integrate trainable parameters parallel to the features at the input and intermediate layers. The primary distinction of these methods from VR [1, 3, 4, 47, 48, 64] lies in their necessity to be designed according to different pretrained model architectures and may also involve modifying the model weights. In contrast, VR is model-agnostic and does not require alterations to pretrained model parameters. ", "page_idx": 14}, {"type": "text", "text": "Domain Adaptation. Domain adaptation (DA) aims to bridge distributional gap by aligning feature spaces of the source task to the target domain with different data distributions [14, 36, 53]. Often, DA is achieved by learning invariant representations or transforming parameters to manage domainspecific shifts of the source and target data. CDAN [33] addresses this by introducing a conditional discriminator for class label-conditioned feature adaptation, while UDA [34] leverages residual layers to capture both domain-specific and domain-shared representations. More recently, source-free DA [32, 61, 65], which seeks adaptation without access to the source data, has gained popularity due to growing concerns over data privacy and storage limitations, as well as the need for adaptation in scenarios where source data is inaccessible [8, 10]. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "C A Simple Probability Estimation Example ", "text_level": 1, "page_idx": 15}, {"type": "image", "img_path": "135eKqDoRR/tmp/00a33296f42e5120e20f9983b0300baad9da0a2862a6570947f723e9346f0bb4.jpg", "img_caption": ["Figure 8: A simple example to help understand how to estimate $p(Y^{\\mathrm{T}}=y^{\\mathrm{T}},Y^{\\mathrm{S}}=y^{\\mathrm{S}}\\mid X^{\\mathrm{T}})$ and $p(\\breve{Y}^{\\mathrm{S}}=y^{\\mathrm{S}}\\mid X^{\\hat{\\mathrm{T}}})$ . "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "We aim to estimate $p(Y^{\\mathrm{T}}=y^{\\mathrm{T}},Y^{\\mathrm{S}}=y^{\\mathrm{S}}\\mid X^{\\mathrm{T}})$ and $p(Y^{\\mathrm{S}}=y^{\\mathrm{S}}\\mid X^{\\mathrm{T}})$ for BLM and $\\mathrm{BLM+}$ in this paper. Here, we employ a simple example (without Laplace smoothing) to help understand how to estimate these two probabilities. ", "page_idx": 15}, {"type": "text", "text": "The conditional probability $p(Y^{\\mathrm{T}}=y^{\\mathrm{T}},Y^{\\mathrm{S}}=y^{\\mathrm{S}}\\mid X^{\\mathrm{T}})$ represents the joint distribution of $Y^{\\mathrm{T}}$ and $Y^{\\mathrm{S}}$ , given the input reprogramming $f_{\\mathrm{in}}(\\cdot;\\theta)$ , the pretrained model $f_{\\mathrm{pre}}(\\cdot)$ , and the variable $X^{\\mathrm{T}}$ of the downstream task. Similarly, $p(Y^{\\mathrm{S}}\\,=\\,y^{\\mathrm{S}}\\mid X^{\\mathrm{T}})$ represents the distribution of $Y^{\\mathrm{S}}$ under these conditions. ", "page_idx": 15}, {"type": "text", "text": "We consider the following example shown in Figure 8. It is assumed that $\\boldsymbol{\\mathcal{V}}^{\\mathrm{T}}=\\{\\mathsf{C a t},\\mathsf{D o g}\\}$ and $\\begin{array}{r}{\\mathcal{V}^{\\mathrm{S}}=\\left\\{\\begin{array}{r l r l}\\end{array}\\right.}\\end{array}$ {CockerSpaniel, EnglishSpringer, EgyptianCat}. The Downstream samples are ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\{(x_{1},\\mathtt{D o g}),(x_{2},\\mathtt{D o g}),(x_{3},\\mathtt{D o g}),(x_{4},\\mathtt{C a t})\\}\\in\\mathcal{X}^{\\mathrm{T}}\\times\\mathcal{Y}^{\\mathrm{T}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "If the reprogrammed predictions calculated by $f_{\\mathrm{pre}}(f_{\\mathrm{in}}(x_{i};\\theta))$ are ", "page_idx": 15}, {"type": "text", "text": "$\\{x_{1}$ : CockerSpaniel, $x_{2}$ : CockerSpaniel, $x_{3}$ : EnglishSpringer, $x_{4}:\\mathtt{E g y p t i a n C a t}\\}$ , then the joint distribution $p(Y^{\\mathrm{T}}\\,=\\,y^{\\mathrm{T}},Y^{\\mathrm{S}}\\,=\\,y^{\\mathrm{S}}\\;\\;|\\;\\;X^{\\mathrm{T}})$ can be estimated as a matrix with the following nonzero values: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(Y^{\\mathrm{T}}={\\sf D o g},{\\sf Y^{\\mathrm{S}}}={\\sf C o c k e r S p a n i e l}\\mid{\\sf X^{\\mathrm{T}}})=\\frac{1}{2},}\\\\ &{p(Y^{\\mathrm{T}}={\\sf D o g},{\\sf Y^{\\mathrm{S}}}={\\sf E n g l i s h S p r i n g e r}\\mid{\\sf X^{\\mathrm{T}}})=\\frac{1}{4},}\\\\ &{p(Y^{\\mathrm{T}}={\\sf C a t},{\\sf Y^{\\mathrm{S}}}={\\sf E g y p t i a n C a t}\\mid{\\sf X^{\\mathrm{T}}})=\\frac{1}{4},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "as is shown in Figure 8. Similarly, $p(Y^{\\mathrm{S}}=y^{\\mathrm{S}}\\mid X^{\\mathrm{T}})$ can also be estimated. ", "page_idx": 15}, {"type": "text", "text": "D Detailed Procedures of Output LM Methods ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This section provides a detailed exposition of gradient-free LM methods. Such methods, derived from data distributions, obviate the need for gradients in the output mapping phase. The pseudocode is presented below. Similar to Section 3, $\\omega$ represents the one-to-one LM or probabilistic LM. ", "page_idx": 15}, {"type": "text", "text": "D.1 Random Label Mapping (RLM) ", "text_level": 1, "page_idx": 16}, {"type": "table", "img_path": "135eKqDoRR/tmp/49a15ac0ac4c3799bd80ac3f6a8e040392df5d5e24a040fa1dd7701dca1789e0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "The process of random label mapping (RLM) is outlined in Algorithm 3. , where the computation of $\\omega$ does not involve the downstream training set. The algorithm establishes a random one-to-one mapping between the pretrained and the downstream labels, ensuring that each $y^{\\mathrm{T}}$ corresponds to a unique $\\bar{y}^{\\mathrm{S}}$ . RLM is computed once before learning the input VR $f(\\cdot;\\bar{\\theta})$ . ", "page_idx": 16}, {"type": "text", "text": "D.2 Frequent Label Mapping (FLM) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Algorithm 4 Computing Frequency Distribution Matrix of [predicted pretrained label, ground-truth downstream label] ", "page_idx": 16}, {"type": "text", "text": "1: Input: Downstream training set $\\{(x_{i}^{\\mathrm{T}},y_{i}^{\\mathrm{T}})\\}_{i=1}^{n}$ , given input VR $f_{\\mathrm{in}}(\\cdot;\\theta)$ and pretrained model   \n$\\bar{f}_{\\mathrm{pre}}(\\cdot)$ with the $j$ th dimension being $f_{\\mathrm{pre}}(\\cdot)_{j}$   \n2: Output: Frequency distribution matrix $d\\in\\mathbb{Z}^{k_{\\mathrm{S}}\\times k_{\\mathrm{T}}}$   \n3: Initialize $d\\gets\\{0\\}^{\\tilde{k}_{\\mathrm{S}}\\times k_{\\mathrm{T}}}$   \n4: # Computing frequency distribution matrix $d$   \n5: for $i=1...n$ do   \n6: $\\hat{y_{i}}^{\\mathrm{S}}\\leftarrow\\arg\\operatorname*{max}_{j}\\(f_{\\mathrm{pre}}(f_{\\mathrm{in}}(x_{i}^{\\mathrm{T}};\\theta))_{j})$   \n7: $d_{\\hat{y_{i}}^{\\mathrm{~S~}},y_{i}^{\\mathrm{~T~}}}\\gets d_{\\hat{y_{i}}^{\\mathrm{~S~}},y_{i}^{\\mathrm{~T~}}}+1$   \n8: end for   \n9: return $d$   \nAlgorithm 5 Frequent Label Mapping for VR   \n1: Input: Pretrained label space $\\mathrm{\\Delta}{y^{\\mathrm{s}}}$ with $k_{\\mathrm{S}}$ labels, downstream label space $\\upnu^{\\mathrm{T}}$ with $k_{\\mathrm{T}}$ labels,   \ndownstream training set $\\{\\overline{{(x_{i}^{\\mathrm{T}},y_{i}^{\\mathrm{T}})}}\\}_{i=1}^{n}$ , given pretrained model $f_{\\mathrm{pre}}(\\cdot)$   \n2: Output: One-to-one LM $\\omega\\in\\{0,1\\}^{k_{\\mathrm{S}}\\times k_{\\mathrm{T}}}$   \n3: Initialize $\\omega\\gets\\{0\\}^{k_{\\mathrm{S}}\\times k_{\\mathrm{T}}}$ , temp set $T\\gets\\{\\}$ to store matched pretrained labels, initialize $f_{\\mathrm{in}}(\\cdot;\\theta)$   \n$\\mathbf{\\nabla}\\theta\\gets\\mathbf{0}$ )   \n4: # Computing frequency distribution matrix $d$   \n5: Use Algorithm 4 to obtain $d$   \n6: # Computing output mapping $\\omega$   \n7: while size of $T$ is not $k_{\\mathrm{T}}$ do   \n8: Find the maximum $d_{y^{\\mathrm{S}},y^{\\mathrm{T}}}$ in $d$   \n9: $\\begin{array}{r l}&{\\omega_{y^{\\mathrm{S}},y^{\\mathrm{T}}}\\gets1}\\\\ &{d_{y^{\\mathrm{S}},t}\\gets0\\mathrm{~for~}t=1,2,...,k_{\\mathrm{T}}}\\\\ &{d_{s,y^{\\mathrm{T}}}\\gets0\\mathrm{~for~}s=1,2,...,k_{\\mathrm{S}}}\\\\ &{T\\gets T\\cup\\{y^{\\mathrm{S}}\\}}\\end{array}$   \n10:   \n11:   \n12:   \n13: end while   \n14: return $\\omega$ ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "The procedure of frequent label mapping (FLM) is outlined in Algorithm 5. Initially, it utilizes the pretrained model to obtain predicted pretrained labels for samples of the downstream task. ", "page_idx": 16}, {"type": "text", "text": "Subsequently, it computes a joint distribution matrix between the predicted pretrained labels and the ground-truth downstream labels. Finally, employing a greedy algorithm, it iteratively identifies the maximum value in the rows and columns corresponding to unmatched label pairs in the matrix to determine the one-to-one mappings. FLM is also computed prior to the training of $f_{\\mathrm{in}}(\\cdot;\\theta)$ . ", "page_idx": 17}, {"type": "text", "text": "D.3 Iterative Label Mapping (ILM) ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "135eKqDoRR/tmp/53153110c6eb48f4327cb95d9ca57b16a6b0db584c7e42cf197228bf2c95244d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "As an enhanced version of FLM, iterative label mapping (ILM) employs interleaved updates with $\\theta$ at each epoch, as outlined in Algorithm 6. Such interleaved updates take into consideration the variations in the output space induced by updates to the input VR during the training process, thereby ensuring that the output LM will be matched with the updated VR. ", "page_idx": 17}, {"type": "text", "text": "D.4 Bayesian-guided Label Mapping (BLM) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The detailed procedure of Bayesian-guided label mapping (BLM) proposed in this paper is shown in Algorithm 7. Compared to ILM, BLM replaces the previous one-to-one mapping $\\dot{\\omega}\\in\\{0,1\\}^{k_{\\mathrm{S}}\\times k_{\\mathrm{T}}}$ with probabilistic LM $\\omega\\ \\in\\ [0,1]^{k_{\\mathrm{S}}\\times k_{\\mathrm{T}}}$ , both satisfying $\\begin{array}{r}{\\sum_{j=1}^{k_{\\mathrm{S}}}\\omega_{j,\\cdot}\\;=\\;1}\\end{array}$ as stated in Section 3. Meanwhile, the process of matrix computation for BLM is based on the Bayes\u2019 theorem (detailed in Section 4) to reflect the complex relationship among label spaces, rather than determining the optimal match through the oversimplified greedy algorithm. ", "page_idx": 17}, {"type": "text", "text": "D.5 Improved Bayesian-guided Label Mapping $\\mathbf{(BLM+)}$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As mentioned in Section 4, $\\mathrm{BLM+}$ extends BLM by incorporating the aggregation of top- $K$ predicted probabilities, shown in Algorithm 9. ", "page_idx": 17}, {"type": "text", "text": "This divergence manifests in the computation process of the joint distribution matrix between predicted pretrained labels and ground-truth downstream labels. Previous methods (i.e., RLM, ILM, BLM) computed a non-negative integer matrix $d\\in\\mathbb{Z}^{k_{\\mathrm{S}}\\times k_{\\mathrm{T}}}$ based on the frequency of occurrence of samples (Algorithm 4). ", "page_idx": 17}, {"type": "text", "text": "In $\\mathrm{BLM+}$ , the calculation entails replacing the deterministic frequencies with predicted probabilities from the top $K$ pretrained labels to estimate the joint distribution matrix $\\boldsymbol{d}\\in\\mathbb{R}^{k_{\\mathrm{S}}\\times k_{\\mathrm{T}}}$ , as is shown in Algorithm 8. In the procedure, the probability aggregation substitutes the binary frequency distribution $\\{0,1\\}$ with a probability distribution within the range of $[0,1]$ , while the top- $K$ technique ", "page_idx": 17}, {"type": "text", "text": "1: Input: Pretrained label space $\\mathrm{\\Delta}{y^{\\mathrm{s}}}$ with $k_{\\mathrm{S}}$ labels, downstream label space $\\upnu^{\\mathrm{T}}$ with $k_{\\mathrm{T}}$ labels,   \ndownstream training set $\\{(x_{i}^{\\mathrm{T}},y_{i}^{\\mathrm{T}})\\}_{i=1}^{n}$ , given pretrained model $f_{\\mathrm{pre}}(\\cdot)$ , total iteration number   \n$E$ , learning rate $a$ , Laplace smoothing $\\lambda$   \n2: Output: Probabilistic LM $\\omega\\in[0,1]^{\\tilde{k}_{\\mathrm{S}}\\times k_{\\mathrm{T}}}$   \n3: Initialize $\\omega\\gets\\{0\\}^{k_{\\mathrm{S}}\\times k_{\\mathrm{T}}}$ , initialize $f_{\\mathrm{in}}(\\cdot;\\theta)$ $\\mathbf{\\nabla}\\theta\\gets\\mathbf{0}$ ), temp matrix $P=[P_{1},...,P_{k_{\\mathrm{S}}}]^{\\intercal}\\in\\mathbb{R}^{k_{\\mathrm{S}}}$   \n4: for $e=1...E$ do   \n5: # Computing frequency distribution matrix $d$   \n6: Use Algorithm 4 to obtain $d$   \n7: # Computing output mapping $\\omega$   \n8: $\\begin{array}{r}{P_{y^{\\mathrm{s}}}\\gets\\sum_{t=1}^{k_{\\mathrm{T}}}d_{y^{\\mathrm{s}},t}+\\lambda}\\end{array}$ for $y^{\\mathrm{S}}=1...k_{\\mathrm{S}}$   \n9: $\\omega_{y^{\\mathrm{S}},y^{\\mathrm{T}}}\\leftarrow d_{y^{\\mathrm{S}},y^{\\mathrm{T}}}/P_{y^{\\mathrm{S}}}$ for $y^{\\mathrm{S}}=1...k_{\\mathrm{S}},y^{\\mathrm{T}}=1...k_{\\mathrm{T}}$   \n10: # Column normalization of $\\omega$   \n11: $\\begin{array}{r}{\\omega_{y^{\\mathrm{S}},y^{\\mathrm{T}}}\\leftarrow\\omega_{y^{\\mathrm{S}},y^{\\mathrm{T}}}/\\sum_{s=1}^{k_{\\mathrm{S}}}\\omega_{s,y^{\\mathrm{T}}}}\\end{array}$ for $y^{\\mathrm{S}}=1...k_{\\mathrm{S}},y^{\\mathrm{T}}=1...k_{\\mathrm{T}}$   \n12: # Training $f_{\\mathrm{in}}(\\cdot;\\theta)$   \n13: $\\begin{array}{r}{\\theta\\leftarrow\\theta-\\stackrel{\\cdot}{a}\\cdot\\overleftarrow{\\mathbf{\\Theta}_{\\theta}}\\sum_{i=1}^{n}\\ell(y_{i}^{\\mathrm{T}},f_{\\mathrm{out}}^{\\omega}(f_{\\mathrm{pre}}(f_{\\mathrm{in}}(x_{i}^{\\mathrm{T}};\\theta))))}\\end{array}$   \n14: end for   \n15: return $\\omega$ ", "page_idx": 18}, {"type": "text", "text": "Algorithm 8 Computing Probability Aggregation Matrix by Top- $\\cal{K}$ Predicted Probabilities ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1: Input: Downstream training set $\\{(x_{i}^{\\mathrm{T}},y_{i}^{\\mathrm{T}})\\}_{i=1}^{n}$ , given input VR $f_{\\mathrm{in}}(\\cdot;\\theta)$ and pretrained model   \n$f_{\\mathrm{pre}}(\\cdot)$ with the $j$ th dimension being $f_{\\mathrm{pre}}(\\cdot)_{j}$ , Laplace smoothing $\\lambda$ , top- $\\mathcal{K}$ value $k$   \n2: Output: Probability aggregation matrix $\\bar{d^{\\prime}}\\in\\mathbb{R}^{k_{\\mathrm{S}}\\times k_{\\mathrm{T}}}$   \n3: Initialize $d^{\\prime}\\gets\\{\\boldsymbol{0}\\}^{k_{\\mathrm{S}}\\times k_{\\mathrm{T}}}$ , temp matrix $Q=[Q_{1},...,Q_{k}]^{\\intercal}\\in\\mathbb{R}^{k}$ , $K=[K_{1},...,K_{k}]^{\\intercal}\\in{\\mathbb{N}}^{+^{k}}$   \n4: # Computing aggregation distribution matrix $d^{\\prime}$   \n5: for $i=1...n$ do   \n6: $Q\\gets\\mathrm{TopK}_{j}(\\mathrm{softmax}(f_{\\mathrm{pre}}(f_{\\mathrm{in}}(x_{i}^{\\mathrm{T}};\\theta))_{j}),k)$ # top- $\\mathcal{K}$   \n7: K \u2190TopKIndices ${\\bf\\Phi}_{j}(f_{\\mathrm{pre}}(f_{\\mathrm{in}}(x_{i}^{\\mathrm{T}};\\theta))_{j},k)$   \n89:: en $d_{K_{s},y_{i}^{\\mathrm{~\\tiny~T~}}}^{\\prime}\\leftarrow d_{K_{s},y_{i}^{\\mathrm{~\\tiny~T~}}}^{\\prime}+Q_{s}$ for $s=1...k$ # Probability Aggregation   \n10: return $d^{\\prime}$ ", "page_idx": 18}, {"type": "text", "text": "serves to retain the most probable $k$ predicted labels rather than selecting only one (i.e., BLM) or all labels (denoted as \u2018-Top- $\\mathbf{\\nabla}\\cdot\\mathbf{K}^{\\star}$ in ablation studies in Section 5). ", "page_idx": 18}, {"type": "text", "text": "D.6 A Quick Version of ILM, BLM, and BLM+ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The baseline method FLM calculates the mapping $\\omega$ once and keeps it fixed, while ILM and our methods update $\\omega$ at each step. However, updating $\\omega$ does not require running the model twice to obtain current predictions for each epoch. Instead, predictions from the most recent epoch can be reused. Therefore, only in the first epoch is it necessary to run the pretrained model an additional time to initialize the weights of LM, which is the same as FLM. In subsequent epochs, these methods do not require any extra runs. More details can be found in the quick version of our released code. ", "page_idx": 18}, {"type": "text", "text": "E Detailed Theoretical Analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "E.1 Justification and Analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we investigate why probabilistic LM should be favored over deterministic one-toone mapping. This analysis assumes the existence of true correspondences between labels in the pretrained and downstream domains. We establish that, under certain conditions, probabilistic LM (Definition. E.1) outperforms deterministic LM (Definition E.2) in estimating the distribution of true label correspondences, quantified by the expected accuracy of the LM function (Eq. (10)). ", "page_idx": 18}, {"type": "text", "text": "1: Input: Pretrained label space $\\mathrm{\\Delta}{y^{\\mathrm{s}}}$ with $k_{\\mathrm{S}}$ labels, downstream label space $\\upnu^{\\mathrm{T}}$ with $k_{\\mathrm{T}}$ labels,   \ndownstream training set $\\{(x_{i}^{\\mathrm{T}},y_{i}^{\\mathrm{T}})\\}_{i=1}^{n}$ , given pretrained model $f_{\\mathrm{pre}}(\\cdot)$ with the $j$ th dimension   \nbeing $f_{\\mathrm{pre}}(\\cdot)_{j}$ , total iteration number $E$ , learning rate $a$ , Laplace smoothing $\\lambda$ , top- $K$ value $k$   \n2: Output: Probabilistic LM $\\omega\\in[0,1]^{k_{\\mathrm{S}}\\times k_{\\mathrm{T}}}$   \n3: Initialize $\\omega\\gets\\{0\\}^{k_{\\mathrm{S}}\\times k_{\\mathrm{T}}}$ , initialize $f_{\\mathrm{in}}(\\cdot;\\theta)$ $\\mathbf{\\nabla}\\theta\\gets\\mathbf{0}$ ), temp matrix $P=[P_{1},...,P_{k_{\\mathrm{S}}}]^{\\intercal}\\in\\mathbb{R}^{k_{\\mathrm{S}}}$   \n4: for $e=1...E$ do   \n5: # Computing probability aggregation matrix $d^{\\prime}$   \n6: Use Algorithm 8 to obtain $d^{\\prime}$   \n7: # Computing output mapping $\\omega$   \n8: $\\begin{array}{r}{P_{y^{\\mathrm{s}}}\\gets\\sum_{t=1}^{k_{\\mathrm{T}}}d_{y^{\\mathrm{s}},t}+\\lambda}\\end{array}$ for $y^{\\mathrm{S}}=1...k_{\\mathrm{S}}$   \n9: $\\omega_{y^{\\mathrm{S}},y^{\\mathrm{T}}}\\leftarrow d_{y^{\\mathrm{S}},y^{\\mathrm{T}}}/P_{y^{\\mathrm{S}}}$ for $y^{\\mathrm{S}}=1...k_{\\mathrm{S}},y^{\\mathrm{T}}=1...k_{\\mathrm{T}}$   \n10: # Column normalization of $\\omega$   \n11: $\\begin{array}{r l}&{\\overset{n\\ \\lor\\ \\mathrm{vatum~numanzaum~zaum~sn}}{\\omega_{y}\\mathrm{s}}\\overbrace{y^{\\mathrm{T}}}^{\\omega}=1...k_{\\mathrm{S}},y^{\\mathrm{T}}=1...k_{\\mathrm{T}}}\\\\ &{\\#\\ \\mathrm{Training}\\ \\boldsymbol{f}_{\\mathrm{in}}(\\cdot;\\theta)}\\\\ &{\\theta\\leftarrow\\theta-a\\cdot\\nabla_{\\theta}\\sum_{i=1}^{n}\\ell(y_{i}^{\\mathrm{T}},f_{\\mathrm{out}}^{\\omega}(\\boldsymbol{f}_{\\mathrm{pre}}(\\boldsymbol{f}_{\\mathrm{in}}(\\boldsymbol{x}_{i}^{\\mathrm{T}};\\theta))))}\\end{array}$   \n12:   \n13:   \n14: end for   \n15: return \u03c9 ", "page_idx": 19}, {"type": "text", "text": "This analysis focuses on the comparisons of LM. Given that the pretrained model $f_{\\mathrm{pre}}$ , input $x$ , and input transformations $f_{\\mathrm{in}}$ are the same across different LM methods, we will omit these notations below unless explicitly needed. We begin by introducing key definitions. ", "page_idx": 19}, {"type": "text", "text": "Definition E.1 (probabilistic label mapping (PLM)). Let $\\mathcal{F}_{\\mathrm{plm}}\\subset\\mathcal{F}_{\\mathrm{lm}}$ be a set of mapping functions such that for all $f_{\\mathrm{plm}}\\in\\mathcal{F}_{\\mathrm{plm}}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\np(f_{\\mathrm{plm}}(\\boldsymbol{y}^{\\mathrm{S}})=\\boldsymbol{y}^{\\mathrm{T}}|\\boldsymbol{y}^{\\mathrm{S}})=\\omega_{\\boldsymbol{y}^{\\mathrm{S}},\\boldsymbol{y}^{\\mathrm{T}}},\\;s.t.\\sum_{\\boldsymbol{y}^{\\mathrm{S}}\\in\\mathcal{Y}^{\\mathrm{S}}}\\omega_{\\boldsymbol{y}^{\\mathrm{S}},\\boldsymbol{y}^{\\mathrm{T}}}=1,\\forall\\boldsymbol{y}^{\\mathrm{T}}\\in\\mathcal{Y}^{\\mathrm{T}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here, $p(f_{\\mathrm{plm}}(y^{\\mathrm{S}})=y^{\\mathrm{T}}|y^{\\mathrm{S}})$ is the conditional probability that a pretrained label $y^{\\mathrm{S}}$ is mapped to $a$ downstream label $y^{\\mathrm{T}}$ . ", "page_idx": 19}, {"type": "text", "text": "Definition E.2 (deterministic label mapping (DLM)). Let $\\mathcal{F}_{\\mathrm{dlm}}\\subset\\mathcal{F}_{\\mathrm{lm}}$ be a set of mapping functions, defined by $f_{\\mathrm{dlm}}(y^{\\mathrm{S}})\\,=\\,g(y^{\\mathrm{S}})$ for all $\\mathbf{\\boldsymbol{y}}^{\\mathrm{S}}\\in\\boldsymbol{\\mathcal{Y}}^{\\mathrm{S}}$ , where $g(y^{\\mathrm{S}})$ specifies a deterministic rule, either $g(y^{\\mathrm{S}})=\\dot{y}^{\\mathrm{S}}$ for identity mapping; or $\\begin{array}{r}{\\bar{g}(y^{\\mathrm{S}})=1-y^{\\mathrm{S}}}\\end{array}$ for flip mapping, respectively. Then, deterministic label mapping is defined as: $\\forall f_{\\mathrm{dlm}}\\in\\mathcal{F}_{\\mathrm{dlm}}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\np(f_{\\mathrm{dlm}}(y^{\\mathrm{S}})=y^{\\mathrm{T}}|y^{\\mathrm{S}})=\\delta_{y^{\\mathrm{S}},g(y^{\\mathrm{S}})},\\;w i t h\\;\\delta_{y^{\\mathrm{S}},g(y^{\\mathrm{S}})}=\\left\\{\\begin{array}{l l}{1}&{i f\\,g(y^{\\mathrm{S}})=y^{\\mathrm{T}}\\;,}\\\\ {0}&{o t h e r w i s e}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\delta$ is the Kronecker delta function, ensuring $y^{\\mathrm{T}}$ is uniquely mapped from a pretrained label $y^{\\mathrm{S}}$ . ", "page_idx": 19}, {"type": "text", "text": "Then, we demonstrate the conditions where $\\operatorname{Acc}(f_{\\mathrm{plm}})\\geq\\operatorname{Acc}(f_{\\mathrm{dlm}})$ . Since DLM is defined by $g$ , following either identity mapping or filp mapping exclusively, each case will be discussed separately. ", "page_idx": 19}, {"type": "text", "text": "Lemma E.3. Given a collection of paired labels $\\{(y^{\\mathrm{S}},y^{\\mathrm{T}})\\}_{i=1}^{n}$ . If the aggregate conditional probabilities $p(y^{\\mathrm{S}}=1|y^{\\mathrm{T}}=0)\\geq p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0)$ and $p(y^{\\mathrm{\\Large{S}}}=0|y^{\\mathrm{\\mathrm{T}}}=1)\\geq p(y^{\\mathrm{\\Large{S}}}=1|y^{\\mathrm{\\mathrm{T}}}=1)$ hold true, and considering $f_{\\mathrm{dlm}}$ is defined by identity mapping as outlined in Definition E.2, then it follows that $\\operatorname{Acc}(f_{\\mathrm{plm}})\\geq\\operatorname{Acc}(f_{\\mathrm{dlm}})$ . ", "page_idx": 19}, {"type": "text", "text": "Lemma E.3 (proof in Appendix E.2) implies that PLM achieves at least as high expected accuracy as DLM defined by identity mapping, under the following conditions: for downstream samples with $y^{\\mathrm{T}}=0$ , the inequality is satisfied when they are more likely to correspond to pretrained samples with $y^{\\mathrm{S}}=1$ than those with $y^{\\mathrm{S}}=0$ ; for downstream samples with $y^{\\mathrm{T}}=\\dot{1}$ , the inequality is satisfied when the corresponding pretrained samples are more likely to have $y^{\\mathrm{S}}=0$ than $y^{\\dot{\\mathrm{S}}}=1$ . ", "page_idx": 19}, {"type": "text", "text": "Uncertainty in Label Inter-Dependencies. Essentially, the conditions above reflect potential complex patterns of label correspondence that arise when inter-dependencies between the labels exist across domains. While this \u201clabel mismatch\u201d problem has been discussed in binary settings, it can be generalized to multi-class settings without loss of generality. Unlike DLM, which merely relies on a static mapping rule and hence may fail when true label correspondence conflicts with this predefined rule, PLM captures the conditional probabilities of $y^{\\mathrm{T}}$ given $y^{\\mathrm{S}}$ . By harnessing the inherent uncertainty encoded in the probabilistic form of $\\omega$ , PLM is expected to achieve more robust label mapping predictions. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Next, we compare PLM with DLM using the flip mapping rule. ", "page_idx": 20}, {"type": "text", "text": "Lemma E.4. Given a collection of paired labels $\\{(y^{\\mathrm{S}},y^{\\mathrm{T}})\\}_{i=1}^{n}$ . If the aggregate conditional probabilities $p(y^{\\mathrm{s}}=0|y^{\\mathrm{T}}=0)\\leq p(y^{\\mathrm{s}}=1|y^{\\mathrm{T}}=0)$ and $p(y^{\\mathrm{{S}}}={\\mathsf{0}}|y^{\\mathrm{{T}}}=1)\\leq p(y^{\\mathrm{{S}}}=1|y^{\\mathrm{{T}}}=1)$ , and $f_{\\mathrm{dlm}}$ is defined by flip mapping as outlined in Definition $E.2$ , then $\\operatorname{Acc}(f_{\\mathrm{plm}})\\geq\\operatorname{Acc}(f_{\\mathrm{dlm}})$ . ", "page_idx": 20}, {"type": "text", "text": "Lemma E.4 (proof in Appendix E.2) establishes another sufficient condition under which PLM could achieve an expected accuracy at least as high as DLM defined by filp mapping. The condition applies to all downstream samples, regardless of their labels (both $y^{\\mathrm{T}}=0$ or $y^{\\mathrm{T}}=\\bar{1},$ ), stating that it is more likely that their corresponding pretrained label being $y^{\\mathrm{S}}=1$ rather than $y^{\\mathrm{S}}=0$ . ", "page_idx": 20}, {"type": "text", "text": "Bias in Label Correspondences. The bias in Label correspondence refers to a phenomenon where a disproportionate number of downstream samples correspond to pretrained samples with a specific label. For example, consider a medical diagnosis task where both pretrained and downstream data come from populations with low disease prevalence, the label correspondences may exhibit this bias [54]. While this bias may be overlooked by DLM, it could be captured and even exploited by PLM, which flexibly adjusts the weighting schemes, e.g., assigning higher value to $\\omega_{1,0}$ than $\\omega_{0,0}$ for samples where $y^{\\mathrm{T}}=0$ , and to $\\omega_{1,1}$ over $\\omega_{0,1}$ for samples where $y^{\\mathrm{T}}=1$ . ", "page_idx": 20}, {"type": "text", "text": "Corollary E.5. Let $f_{\\mathrm{plm}}$ and $f_{\\mathrm{dlm}}$ denote the label mapping functions defined in Definition E.1 and Definition $E.2$ , respectively. Given pretrained and downstream label spaces $\\mathcal{D}^{\\mathrm{S}}=\\{0,1\\}$ and ${\\mathcal{V}}^{\\mathrm{T}}=\\dot{\\{0,1\\}}$ , if for any joint distribution over $\\mathcal{V}^{\\mathrm{S}}\\times\\mathcal{V}^{\\mathrm{T}}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\exists\\,a\\in\\{0,1\\}\\;s.t.\\;p(y^{\\mathrm{S}}=a|y^{\\mathrm{T}}=\\bar{a})\\geq p(y^{\\mathrm{S}}=\\bar{a}|y^{\\mathrm{T}}=\\bar{a}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where \u00afa is the opposite label of $a_{\\scriptscriptstyle-}$ , then we have $\\operatorname{Acc}(f_{\\mathrm{plm}})\\geq\\operatorname{Acc}(f_{\\mathrm{dlm}}).$ . ", "page_idx": 20}, {"type": "text", "text": "Remark E.6. Corollary E.5 implies a theoretical foundation for preferring PLM over DLM in scenarios where the label mapping relationship between two domains is uncertain, biased and potentially deviates from a deterministic one-to-one mapping assumption. This finding holds importance in label mappings for VR, as the label spaces may encompass multi-class settings. Furthermore, in VR settings, the pretrained labels derived from $f_{\\mathrm{pre}}$ predictions, are subject to increased uncertainties and biases influenced by the quality and distribution of the pretrained model and dataset5. ", "page_idx": 20}, {"type": "text", "text": "E.2 Completed Proof of Lemma E.3 and Lemma E.4 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Lemma E.7 (cf. Lemma E.3). Given a collection of paired labels $\\{(y^{\\mathrm{S}},y^{\\mathrm{T}})\\}_{i=1}^{n}$ . If the aggregate conditional probabilities $p(y^{\\mathrm{s}}=1|y^{\\mathrm{T}}=0)\\geq p(y^{\\mathrm{s}}=0|y^{\\mathrm{T}}=0)$ and $p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=1)\\geq p(y^{\\mathrm{S}}=$ $1|y^{\\mathrm{T}}=1)$ hold true, and $f_{\\mathrm{dlm}}$ is defined by identity mapping as outlined in Definition $E.2$ , then $\\mathrm{Acc}(f_{\\mathrm{plm}})\\geq\\mathrm{Acc}(f_{\\mathrm{dlm}})$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. Expand Eq. (10) by taking all possibilities of $y^{\\mathrm{T}}$ , we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Acc}(f_{\\mathrm{Im}})=\\mathbb{E}_{y^{\\mathrm{T}}\\in\\mathcal{Y}^{\\mathrm{T}}}\\left[\\displaystyle\\sum_{y^{8}\\in\\mathcal{Y}^{8}}p(y^{8})\\cdot p\\left(f_{\\mathrm{Im}}(y^{8})=y^{\\mathrm{T}}|y^{8}\\right)\\right]}\\\\ &{\\phantom{=}\\displaystyle\\sum_{y^{\\mathrm{T}}\\in\\mathcal{Y}^{\\mathrm{T}}}p(y^{\\mathrm{T}})\\left[\\displaystyle\\sum_{y^{8}\\in\\mathcal{Y}^{8}}p(y^{8}|y^{\\mathrm{T}})\\cdot p\\left(f_{\\mathrm{Im}}(y^{8})=y^{\\mathrm{T}}|y^{8},y^{\\mathrm{T}}\\right)\\right]}\\\\ &{\\phantom{=}\\displaystyle\\sum_{y^{\\mathrm{T}}\\in\\mathcal{Y}^{\\mathrm{T}}}p(y^{\\mathrm{T}})\\left[\\displaystyle\\sum_{y^{8}\\in\\mathcal{Y}^{8}}p(y^{8}|y^{\\mathrm{T}})\\cdot p\\left(f_{\\mathrm{Im}}(y^{8})=y^{\\mathrm{T}}|y^{8}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "5For the analysis purpose, in this section we simplify the setting and operate with ground-truth $y^{\\mathrm{s}}$ . In practice, VR does not have access to true $y^{\\mathrm{S}}$ but must rely on the predicted $y^{\\mathrm{S}}$ from the well-trained $f_{\\mathrm{pre}}$ instead. ", "page_idx": 20}, {"type": "text", "text": "Note that the conditional independence holds since the output of $f_{\\mathrm{lm}}$ relies solely on the input $y^{\\mathrm{S}}$ . For DLM defined by identity mapping, $p(f_{\\mathrm{dlm}}(y^{\\mathrm{S}})=y^{\\mathrm{T}}|y^{\\mathrm{S}})=1$ if $y^{\\mathrm{S}}=y^{\\mathrm{T}}$ , and 0 otherwise. Taking into account all the samples, the expected accuracy $\\operatorname{Acc}(f_{\\mathrm{dlm}})$ can then be expressed by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Acc}(f_{\\mathrm{dlm}})=\\displaystyle\\sum_{y^{\\mathrm{T}}\\in\\mathcal{Y}^{\\mathrm{T}}}p(y^{\\mathrm{T}})\\,p(y^{\\mathrm{S}}=y^{\\mathrm{T}}|y^{\\mathrm{T}})}\\\\ &{\\qquad\\qquad=p(y^{\\mathrm{T}}=0)p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0)+p(y^{\\mathrm{T}}=1)p(y^{\\mathrm{S}}=1|y^{\\mathrm{T}}=1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "As for PLM, the expected accuracy can be rewritten as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathrm{Acc}(f_{\\mathrm{plm}})=\\sum_{y^{\\mathrm{T}}\\in\\mathcal{Y}^{\\mathrm{T}}}p(y^{\\mathrm{T}})\\sum_{y^{\\mathrm{S}}\\in\\mathcal{Y}^{\\mathrm{S}}}\\omega_{y^{\\mathrm{S}},y^{\\mathrm{T}}}\\cdot p(y^{\\mathrm{S}}|y^{\\mathrm{T}})}}\\\\ &{}&{=p(y^{\\mathrm{T}}=0)\\left[\\omega_{0,0}\\cdot p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0)+\\omega_{1,0}\\cdot p(y^{\\mathrm{S}}=1|y^{\\mathrm{T}}=0)\\right]}\\\\ &{}&{+\\,p(y^{\\mathrm{T}}=1)\\left[\\omega_{0,1}\\cdot p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=1)+\\omega_{1,1}\\cdot p(y^{\\mathrm{S}}=1|y^{\\mathrm{T}}=1)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\omega_{0,0}$ stands for $\\omega_{y}\\mathrm{s}\\!=\\!0,y^{\\mathrm{T}}\\!=\\!0$ , and similarly for the remaining $\\omega_{0,1},\\omega_{1,0},\\omega_{1,1}$ ", "page_idx": 21}, {"type": "text", "text": "To evaluate the expected accuracy of $f_{\\mathrm{plm}}$ and $f_{\\mathrm{dlm}}$ , we look into the comparison separately for each $y^{\\mathrm{T}}$ . Specifically, for the samples with $y^{\\mathrm{T}}=0$ , we aim to show that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p({\\boldsymbol y}^{\\mathrm{{T}}}=0)\\left[\\omega_{0,0}\\cdot p({\\boldsymbol y}^{\\mathrm{{S}}}=0|{\\boldsymbol y}^{\\mathrm{{T}}}=0)+\\omega_{1,0}\\cdot p({\\boldsymbol y}^{\\mathrm{{S}}}=1|{\\boldsymbol y}^{\\mathrm{{T}}}=0)\\right]}\\\\ &{\\qquad\\qquad\\geq p({\\boldsymbol y}^{\\mathrm{{T}}}=0)p({\\boldsymbol y}^{\\mathrm{{S}}}=0|{\\boldsymbol y}^{\\mathrm{{T}}}=0).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Given the constraints $\\omega_{0,0}+\\omega_{1,0}=1$ and $p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0)+p(y^{\\mathrm{S}}=1|y^{\\mathrm{T}}=0)=1$ , the LHS of Eq. (17) becomes ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad p(y^{\\mathrm{T}}=0)\\left[\\omega_{0,0}\\cdot p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0)+\\omega_{1,0}\\cdot p(y^{\\mathrm{S}}=1|y^{\\mathrm{T}}=0)\\right]}\\\\ &{=p(y^{\\mathrm{T}}=0)\\left[\\left(\\omega_{0,0}\\cdot p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0)+\\omega_{1,0}(1-p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0))\\right]}\\\\ &{=p(y^{\\mathrm{T}}=0)\\left[\\left(\\omega_{0,0}-\\omega_{1,0}\\right)\\cdot p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0)+\\omega_{1,0}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The inequality we need to show is then simplified to $p(y^{\\mathrm{T}}=0)[(\\omega_{0,0}-\\omega_{1,0})\\cdot p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=$ $0)+\\omega_{1,0}\\bigr]\\geq p(y^{\\mathrm{T}}=0)p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0)$ . This inequality holds if $p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0)\\leq p(y^{\\mathrm{S}}=$ $1|y^{\\mathrm{T}}=0|$ ). ", "page_idx": 21}, {"type": "text", "text": "Similarly, for samples with $y^{\\mathrm{T}}=1$ , the inequality of interest is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p({\\boldsymbol y}^{\\mathrm{{T}}}=1)\\left[\\omega_{1,0}\\cdot p({\\boldsymbol y}^{\\mathrm{{S}}}=1|{\\boldsymbol y}^{\\mathrm{{T}}}=0)+\\omega_{1,1}\\cdot p({\\boldsymbol y}^{\\mathrm{{S}}}=1|{\\boldsymbol y}^{\\mathrm{{T}}}=1)\\right]}\\\\ &{\\qquad\\qquad\\geq p({\\boldsymbol y}^{\\mathrm{{T}}}=1)p({\\boldsymbol y}^{\\mathrm{{S}}}=1|{\\boldsymbol y}^{\\mathrm{{T}}}=1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This holds if $p(y^{\\mathrm{S}}=1|y^{\\mathrm{T}}=1)\\leq p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=1)$ . ", "page_idx": 21}, {"type": "text", "text": "Both conditions can be satisfied without conflict. Thus, we can confirm Lemma E.3 by evaluating these conditions jointly. ", "page_idx": 21}, {"type": "text", "text": "Lemma E.8 (cf. Lemma E.4). Given a collection of paired labels $\\{(y^{\\mathrm{S}},y^{\\mathrm{T}})\\}_{i=1}^{n}$ . If the aggregate conditional probabilities $p(y^{\\mathrm{s}}=0|y^{\\mathrm{T}}=0)\\leq p(y^{\\mathrm{s}}=1|y^{\\mathrm{T}}=0)$ and $p(y^{\\mathrm{S}}={\\bar{0}}|{\\bar{y^{\\mathrm{T}}}}=1)\\leq p(y^{\\mathrm{S}}=$ $1|y^{\\mathrm{T}}\\,=\\,1)$ , and $f_{\\mathrm{dlm}}$ is defined by flip mapping as outlined in Definition E.2, then $\\operatorname{Acc}(f_{\\mathrm{plm}})\\ge$ $\\operatorname{Acc}(f_{\\mathrm{dlm}})$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. When defined deterministically by flip mapping, DLM can be equivalently expressed as $p(f_{\\mathrm{dlm}}(y^{\\mathrm{S}})=y^{\\mathrm{T}}|y^{\\mathrm{S}})=1$ if $y^{\\mathrm{S}}\\neq y^{\\mathrm{T}}$ , and 0 otherwise. This allows the expected accuracy of DLM to be expanded as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Acc}(f_{\\mathrm{dlm}})=\\displaystyle\\sum_{y^{\\mathrm{r}}\\in\\mathcal{Y}^{\\mathrm{T}}}p(y^{\\mathrm{T}})\\,p(y^{\\mathrm{S}}\\ne y^{\\mathrm{T}}|y^{\\mathrm{T}})}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{y^{\\mathrm{r}}\\in\\mathcal{Y}^{\\mathrm{T}}}p(y^{\\mathrm{T}})\\,\\left(1-p(y^{\\mathrm{S}}=y^{\\mathrm{T}}|y^{\\mathrm{T}})\\right)}\\\\ &{\\qquad=p(y^{\\mathrm{T}}=0)\\left(1-p(y^{\\mathrm{S}}=1|y^{\\mathrm{T}}=0)\\right)+p(y^{\\mathrm{T}}=1)\\left(1-p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=1)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Meanwhile, the expected accuracy of PLM remains consistent as in Eq. (16). Again, to show that $\\mathrm{Acc}(f_{\\mathrm{plm}})\\,\\geq\\,\\mathrm{Acc}(f_{\\mathrm{dlm}})$ holds, we compare the expected accuracy with respect to different $y^{\\mathrm{T}}$ samples separately. ", "page_idx": 22}, {"type": "text", "text": "For samples with $y^{\\mathrm{T}}=0$ , we need to show ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p({\\boldsymbol y}^{\\mathrm{{T}}}=0)\\left[\\omega_{0,0}\\cdot p({\\boldsymbol y}^{\\mathrm{{S}}}=0|{\\boldsymbol y}^{\\mathrm{{T}}}=0)+\\omega_{1,0}\\cdot p({\\boldsymbol y}^{\\mathrm{{S}}}=1|{\\boldsymbol y}^{\\mathrm{{T}}}=0)\\right]}\\\\ &{\\qquad\\qquad\\geq p({\\boldsymbol y}^{\\mathrm{{T}}}=0)p({\\boldsymbol y}^{\\mathrm{{T}}}=0)\\left(1-p({\\boldsymbol y}^{\\mathrm{{S}}}=1|{\\boldsymbol y}^{\\mathrm{{T}}}=0)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Given the constraints that $\\omega_{0,0}+\\omega_{1,0}=1$ and $p(y^{\\mathrm{S}}=1|y^{\\mathrm{T}}=0)=1-p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0)$ , the LHS of Eq. (21) can be expressed by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad p(y^{\\mathrm{T}}=0)\\left[\\omega_{0,0}\\cdot p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0)+\\omega_{1,0}\\cdot p(y^{\\mathrm{S}}=1|y^{\\mathrm{T}}=0)\\right]}\\\\ &{=p(y^{\\mathrm{T}}=0)\\left[(\\omega_{0,0}-\\omega_{1,0})\\cdot p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0)+\\omega_{1,0}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We rearrange the terms: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\quad p(y^{\\mathrm{T}}\\,\\overset{<}{=}0)\\left[(\\omega_{0,0}-\\omega_{1,0})\\cdot p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0)+\\omega_{1,0}\\right]\\geq p(y^{\\mathrm{T}}=0)\\left(1-p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0)\\right)}&\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(\\omega_{0,0}-\\omega_{1,0})\\cdot p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0)+\\omega_{1,0}\\geq1-p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0)}&\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\frac{1-p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0)-\\omega_{1,0}+\\omega_{1,0}\\cdot p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0)}{p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0)}\\leq\\omega_{0,0}}&\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0|y^{\\mathrm{T}}=0)}&\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0)-\\omega_{1,0}\\cdot(1-p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0))}&\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0)}&\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\frac{(1-\\omega_{1,0})\\cdot(1-p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0))}{p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0)}\\leq\\omega_{0,0}.}&\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "It is then concluded that Eq. (23) holds if $p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=0)\\leq p(y^{\\mathrm{S}}=1|y^{\\mathrm{T}}=0)$ . ", "page_idx": 22}, {"type": "text", "text": "As with $y^{\\mathrm{T}}=1$ samples, a similar derivation is performed to satisfy the inequality ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p({\\boldsymbol y}^{\\mathrm{T}}=1)\\left[\\omega_{0,1}\\cdot p({\\boldsymbol y}^{\\mathrm{S}}=0|{\\boldsymbol y}^{\\mathrm{T}}=1)+\\omega_{1,1}\\cdot p({\\boldsymbol y}^{\\mathrm{S}}=1|{\\boldsymbol y}^{\\mathrm{T}}=1)\\right]}\\\\ &{\\ \\geq p({\\boldsymbol y}^{\\mathrm{T}}=0)p({\\boldsymbol y}^{\\mathrm{T}}=0)\\left(1-p({\\boldsymbol y}^{\\mathrm{S}}=1|{\\boldsymbol y}^{\\mathrm{T}}=0)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Resembling $y^{\\mathrm{T}}=0$ samples, the derivation yields the condition $p(y^{\\mathrm{S}}=0|y^{\\mathrm{T}}=)\\leq p(y^{\\mathrm{S}}=1|y^{\\mathrm{T}}=$ 1). ", "page_idx": 22}, {"type": "text", "text": "Notably, the condition $p(y^{\\mathrm{s}}\\,=\\,0|y^{\\mathrm{T}}\\,=\\,0)\\,\\leq\\,p(y^{\\mathrm{s}}\\,=\\,1|y^{\\mathrm{T}}\\,=\\,0)$ does not conflict with $p(y^{\\mathrm{S}}=$ $0|y^{\\mathrm{T}}=)\\leq p(y^{\\mathrm{S}}=1|y^{\\bar{\\mathrm{T}}}=1)$ , and both conditions can be jointly satisfied. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "F Training Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "F.1 Dataset Information ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Additional dataset information is presented in Table 4. For a fair comparison, we adhere to the data partitioning scheme employed by ILM [4] through all datasets. The batch size for Oxfordpets and DTD is set to be 64 while 256 for the remaining datasets. ", "page_idx": 22}, {"type": "text", "text": "F.2 Parameter Information ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Consistent training settings are maintained to ensure a fair comparison. For training input VR patterns, we apply the Adam optimizer [26] with an initial learning rate of 0.01. The number of epochs is 200, with the learning rate decay being 0.1, scheduled at epochs 100 and 145. All experiments are conducted on a single A100 GPU and the average accuracy of three distinct random seeds are reported. ", "page_idx": 22}, {"type": "text", "text": "G Parameter Analysis ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "G.1 Choosing Hyper-parameters ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "As described in Section 4, the ratio $\\alpha$ is used in calculating $k=\\lfloor\\alpha\\cdot k_{\\mathrm{T}}\\rfloor$ . The experimental results to tune hyper-parameters $\\alpha$ and $\\lambda$ are reported in Table 5. $\\alpha$ is chosen among [0.01, 0.05, 0.15, 0.5, 1], ", "page_idx": 22}, {"type": "table", "img_path": "135eKqDoRR/tmp/a7b3e8235ff2a5f7fc5dd5d85dce8b4c98b057963b2a953aeaf594f97b9597ba.jpg", "table_caption": ["Table 4: Detailed dataset information "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "135eKqDoRR/tmp/e9539ff626b9a6618602cc4fc678d3b4baa31b03150da4408e4ca904292d5640.jpg", "table_caption": ["Table 5: Tuning ratio $\\alpha$ and Laplace $\\lambda$ (ResNet-18, Flowers102, average accuracy $(\\%)$ ) "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "while $\\lambda$ is chosen among $[0.01,0.1,1,10,100,1000]$ . The optimized $\\lambda$ is determined first to be 1 by the average accuracy of different $\\alpha$ values, followed by deriving an optimal $\\alpha=0.15$ . ", "page_idx": 23}, {"type": "text", "text": "While the same hyper-parameters may not necessarily be optimal across different datasets, for the sake of consistency and fairness, this paper employs identical hyper-parameters for all datasets. ", "page_idx": 23}, {"type": "text", "text": "G.2 Analyzing Hyper-parameters ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Figures 9 and Figure 10 illustrate the impact of $\\lambda$ and $\\alpha$ on accuracy. It is observed that the optimal hyper-parameters vary across different datasets. ", "page_idx": 23}, {"type": "text", "text": "In general, as $\\lambda$ increases, the test accuracy initially rises and then declines. This parameter is used to balance the contributions of individual pretrained labels. An over-small $\\lambda$ might overly rely on the distribution of pretrained labels obtained from pretrained models, while a too-large one might overlook differences among pretrained labels. Meanwhile, with an increase in $\\alpha$ , accuracy first increases, then plateaus or slightly decreases. This is because excessively small or large $\\alpha$ values may lead to the neglect of certain crucial labels or the emphasis on redundant ones during the estimation of the probability aggregation matrix. Therefore, choosing moderate values for $\\lambda$ and $\\alpha$ appears to be more appropriate. ", "page_idx": 23}, {"type": "text", "text": "G.3 Task-specific Hyper-parameters ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We used universal hyper-parameters to show that BLM and $\\mathrm{BLM+}$ \u2019s performance gains over baselines are not sensitive to hyper-parameters. However, we assume that the dataset-specific tuning for hyper-parameters could yield more optimized results. ", "page_idx": 23}, {"type": "text", "text": "Additional experiments are conducted using a validation set and training set split of $30\\%$ and $70\\%$ to find optimal hyper-parameters for each dataset. Results are shown in Table 6. We observe that optimal hyper-parameters tailored for each dataset achieve better performance compared to using shared hyper-parameters, which matches our assumption. ", "page_idx": 23}, {"type": "image", "img_path": "135eKqDoRR/tmp/22f91edde9b48fd37d9337e62fa83e7c193818324a13c68b260f1114f26da384.jpg", "img_caption": ["Figure 9: Accuracy with different Laplace $\\lambda$ . "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "135eKqDoRR/tmp/4cdae3d563a7bbfae442b2ae1e17564f56e13707ed4d4f8f7541e0dd6aa9e798.jpg", "img_caption": ["Figure 10: Accuracy with different Ratio $\\alpha$ . "], "img_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "135eKqDoRR/tmp/f0dd7468c1f4658b8f459124a6a8ded1083cc547bc0bd7cfda454f212a895628.jpg", "table_caption": ["Table 6: Difference between task-specific parameters and shared parameters "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "H Limitations of BLM and $\\mathbf{BLM+}$ ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Less effective for tasks with very few classes. As shown in Table 1, when the number of classes (i.e., size of the label space) in downstream tasks is smaller (10 classes in SVHN and 10 classes in EuroSAT) and the original task is relatively simple, the advantage of BLM and $\\mathrm{BLM+}$ is not very pronounced. This is because BLM and $\\mathrm{BLM+}$ replace the one-to-one mapping with a pairwiseconnected probabilistic LM. While this optimization yields positive results in most tasks, for a small subset of simple tasks, the one-to-one mapping may better reflect the relationship between the pretrained label space and the downstream label space. For such tasks, BLM and $\\mathrm{BLM+}$ no longer exhibit significant effects. ", "page_idx": 24}, {"type": "text", "text": "Not solving the cases where VR is not applicable for downstream tasks. For example, in the case of the StanfordCars dataset in vision models, as shown in Table 1 and Table 2, the accuracy of the downstream task remains consistently low $(<\\!10\\%)$ through learning using input VR. While applying BLM and $\\mathrm{BLM+}$ in such scenarios yields better results compared to using one-to-one mapping, it still cannot significantly enhance VR performance to the extent of being comparable to finetuning the entire model. ", "page_idx": 24}, {"type": "text", "text": "I Visualization of Label Mapping Matrices ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Based on the example of ResNet-18 pretrained on ImageNet-1K applying to the downstream task CIFAR10, Figure 11 depicts the visualization results of LM matrices. The first row in Figure 11 shows the results of gradient-free methods BLM and $\\mathrm{BLM+}$ , while the second row shows deep learning-based methods which learn a linear neural network for $f_{\\mathrm{out}}^{\\omega}$ . \u2018Deep Learning\u2019 refers to a single-layer neural network without constraints (i.e., $\\omega\\in\\mathbb{R}^{k_{\\mathrm{S}}\\times k_{\\mathrm{T}}})$ , while \u2018Deep Learning $^+$ Sigmoid\u2019 refers to applying the Sigmoid function to restrict $\\omega\\in[0,1]^{k_{\\mathrm{S}}\\times k_{\\mathrm{T}}}$ aligning with the range of $\\omega_{\\mathrm{BLM}}$ and $\\omega_{\\mathrm{BLM+}}$ . The right part of Figure 11 depicts the specific pretrained and downstream labels corresponding to these matrices. ", "page_idx": 24}, {"type": "image", "img_path": "135eKqDoRR/tmp/63b080167f8525c6ff2a6a6e0d8cb2877e68ea1466670503e508b79ed6798fb9.jpg", "img_caption": ["Figure 11: The visualization results of the LM matrices. Using the example of ResNet-18 pretrained on ImageNet-1K applied to the downstream task CIFAR10, the left figure displays the first 10 rows and 10 columns of the LM matrices (including the result matrix of the first 10 pretrained and downstream labels), while the right figure presents specific labels. Compared to gradient-free LM methods (i.e., BLM and $\\mathrm{BLM+}$ ), deep learning-based methods (i.e., a single-layer unrestricted neural network $\\boldsymbol{\\omega}\\in\\mathbb{R}^{k_{\\mathrm{S}}\\times k_{\\mathrm{T}}}$ and a single-layer neural network with Sigmoid $\\omega\\in[0,\\dot{1}]^{k_{\\mathrm{S}}\\times k_{\\mathrm{T}}},$ demonstrate less interpretability in revealing the relationship between labels. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "It is observed that BLM and $\\mathrm{BLM+}$ are good at revealing similarities between pretrained and downstream labels. For example, for the downstream label \u2018Airplane\u2019, which visually resembles \u2018Great White Shark\u2019, \u2018Hammerhead\u2019 and \u2018Stingray\u2019, the weights in $\\omega_{\\mathrm{BLM}}$ or $\\omega_{\\mathrm{BLM+}}$ tend to be higher. Conversely, for dissimilar labels like \u2018Truck\u2019 and \u2018Ostrich\u2019, the weights will be approaching 0. However, the weight matrices obtained from deep learning-based methods fail to capture such clear-label relationship. The results demonstrate the advantages of BLM and $\\mathrm{BLM+}$ in terms of interpretability. ", "page_idx": 25}, {"type": "text", "text": "J Training Cost Analysis ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The Required Number of Epochs. Different label mapping methods require varying numbers of epochs to converge. We initially used 200 epochs as with [4] to ensure a fair comparison with the baseline methods. Additional experiments are conducted to assess the impact of different epoch numbers from [60, 100, 150] on our BLM and $\\mathrm{BLM+}$ model, using ResNet-18 as the pretrained model. The results are shown in Table 7. ", "page_idx": 25}, {"type": "text", "text": "We found that running 100 epochs yields results comparable to those achieved with 200 epochs. This demonstrates that BLM and BLM $^+$ require less convergence time, highlighting their efficiency. ", "page_idx": 25}, {"type": "table", "img_path": "135eKqDoRR/tmp/cbb5d40b276bee2d438ff92a2633e9b17892e18cfa73dfa2afd0eac63a4dd22e.jpg", "table_caption": ["Table 7: Impact of epoch numbers on different label mapping methods "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "135eKqDoRR/tmp/3bb46877b8a0031f4845db8753242ca9c8fcc74624569b24297de4671f1814ab.jpg", "table_caption": ["Table 8: Training cost analysis of LM & VR and none-VR finetuning (on Flowers102) "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Overall Time Consumption. Table 8 presents a comparison of different output mapping methods in terms of computational resources, utilizing the Flowers102 dataset as the downstream task. Gradientfree LM refers to estimating output mappings using statistical methods, while deep learning-based LM treats label mapping as a single linear layer neural network attached after the pretrained models. $\\mathbf{\\nabla}^{*}\\mathbf{B}\\mathbf{L}\\mathbf{M}^{*}{}^{,}$ and $\\mathbf{\\dot{\\Phi}}\\mathbf{B}\\mathbf{L}\\mathbf{M}\\mathbf{+}^{\\ast}\\mathbf{\\Phi}$ refer to training with only 100 epochs as is shown in Table 7. It should be noted that the running times for ILM, BLM, and $\\mathrm{BLM+}$ are measured using the quick version (see Appendix D.6 for details). Apart from VR methods, which fix the pretrained model, the time costs associated with directly finetuning pretrained models are also listed. Here, the term \u2018Linear\u2019 refers to finetuning the final layer of the pretrained model, while \u2018Fully\u2019 refers to finetuning the entire model. ", "page_idx": 26}, {"type": "text", "text": "Besides, regarding the performance of finetuning methods on downstream tasks compared with VR, please refer to [4] for more discussion. Since we mainly focus on LM methods for VR in this paper, which has a different problem setting with finetuning, the performance comparison of VR and finetuning will not be addressed here. ", "page_idx": 26}, {"type": "text", "text": "We therefore analyze the efficiency of BLM and $\\mathrm{BLM+}$ from three perspectives: ", "page_idx": 26}, {"type": "text", "text": "\u2022 Extra Consumption of Calculating the Mapping Matrix Compared with One-to-One Mapping: Compared to the baseline method ILM, the additional cost for BLM and $\\mathrm{BLM+}$ primarily involves the gradient-free multiplication and division within the mapping matrix (which is sized according to the source and target label spaces, $1000\\times102$ in this case). This additional cost is minimal, as shown in Table 8.   \n\u2022 Time Consumption of Updating the Mapping Matrix per Epoch: Compared with FLM, updating $\\omega$ in ILM, BLM, and $\\mathrm{BLM+}$ does not require running the model to obtain current predictions for each epoch. Instead, predictions from the most recent epoch can be reused (see Appendix D.6). As a result, there is no noticeable time overhead for updating $\\omega$ per epoch, as indicated by Table 8.   \n\u2022 Time Consumption of LM and VR Compared with Deep Learning-based Methods: It is observed that methods based on deep learning introduce a substantial number of extra parameters (which would further increase with larger downstream label space and higher ", "page_idx": 26}, {"type": "text", "text": "pretrained model complexity) along with the necessity of backpropagation for gradient computation. Conversely, the gradient-free LM methods along with VR emphasized in this study do not encounter these challenges. ", "page_idx": 27}, {"type": "text", "text": "K More Results on Visual Classification Tasks ", "text_level": 1, "page_idx": 27}, {"type": "image", "img_path": "135eKqDoRR/tmp/21d5456674dfa32d97f9e7e122f3999b82f7cdd6e17dc73db976bd9dc52121ba.jpg", "img_caption": ["Figure 12: Label mapping results of ILM, BLM, BLM+ for VR on Flowers102 dataset. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "135eKqDoRR/tmp/47813a1f5a13a3beb193b5df998589129e8d94ce5b09def6fd1dbd483f0ab0eb.jpg", "img_caption": ["Figure 13: Label mapping results of ILM, BLM, BLM $\\pm$ for VR on DTD dataset. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figures 12-16 illustrate the visualization results of label mapping using ILM (one-to-one mapping), BLM, and $\\mathrm{BLM+}$ for VR on various datasets with pretrained ResNet-18. For BLM and $\\mathrm{BLM+}$ , the top three contributing pretrained labels corresponding to the downstream label are presented, along with their respective weights. ", "page_idx": 27}, {"type": "text", "text": "Results when the pretrained and downstream labels exhibit appearance resemblance. Figures 12 and 15 respectively depict the outcomes on Flowers102 and Food101 datasets, each about classification tasks of various flowers and food. $\\mathrm{BLM+}$ is adept at assigning higher weights to pretrained labels with a greater resemblance to the downstream label in terms of color, shape, and intricate features. In terms of color, as evidenced in Figure 15, the top-weighted labels for \u2018Edamame\u2019 comprise \u2018Green Snake\u2019, \u2018Artichoke\u2019, and \u2018Green Mamba\u2019, all sharing a green hue. Regarding shape, in Figure 12, the \u2018Gazania\u2019 with petal stripes corresponds to top weighted labels such as \u2018Banana\u2019 and \u2018Zucchini\u2019, which exhibit similar striping patterns. As for intricate features, in Figure 12, the \u2018Globe Thistle\u2019 with needle-like appearance aligns with top weighted labels including \u2018Sea Urchin\u2019, \u2018Porcupine\u2019, and \u2018Cardoon\u2019, which possess akin prickly characteristics. ", "page_idx": 27}, {"type": "image", "img_path": "135eKqDoRR/tmp/553d5c5985080bee55c333cd7fab05f98d4baf64962f149bb44f44b31fd85779.jpg", "img_caption": ["Figure 14: Label mapping results of ILM, BLM, BLM $^+$ for VR on UCF101 dataset. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "135eKqDoRR/tmp/f0a0f14a5d5c5f924ce585e7aad55d36be2d4fc7fdfc3c83e4f18bcc6130e472.jpg", "img_caption": ["Figure 15: Label mapping results of ILM, BLM, $\\mathrm{BLM+}$ for VR on Food101 dataset. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Results when the pretrained and downstream labels exhibit similarities in texture. Figure 13 presents the results on the DTD dataset, which pertains to the classification of various textures. Both $\\mathrm{BLM+}$ and BLM assign higher weights to labels sharing akin textures. For example, \u2018Spiralled\u2019 corresponds to top-weighted labels embodying spiral-shaped entities such as \u2018Snail\u2019, \u2018Centipede\u2019, and \u2018Coil\u2019, while \u2018Fibrous\u2019 aligns with entities possessing a rough and fibrous texture, including \u2018Hay\u2019, \u2018Komondor\u2019, and \u2018Porcupine\u2019. ", "page_idx": 28}, {"type": "text", "text": "Results when the pretrained and downstream labels exhibit similarities in backgrounds. Figure 14 illustrates the results on the UCF101 dataset, a dataset for action classification. In this task, both BLM and $\\mathrm{BLM+}$ tend to assign higher weights to pretrained labels with backgrounds or environments akin to the downstream labels. For example, the action \u2018Apply Lipstick\u2019 often involves the presence of a human face; hence, pretrained labels such as applying \u2018Lipstick\u2019, eating \u2018Ice Lolly\u2019, and spraying \u2018Hair Spray\u2019 contribute significantly. Likewise, labels closely associated with \u2018Baseball pitch\u2019 include \u2018Ballplayer\u2019 and \u2018Scoreboard\u2019, featuring backgrounds of vast grass fields. ", "page_idx": 28}, {"type": "text", "text": "Results when pretrained and downstream labels exhibit inclusion relationship. Figure 16 illustrates the results on the CIFAR10 dataset, which comprises images broadly categorized into ten main classes, with each category corresponding to several subcategories within the pretrained domain. It is noted that unlike the singular class selection of ILM, both BLM and $\\mathrm{BLM+}$ allocate similar weights to multiple subcategories. For example, \u2018Dog\u2019 corresponds to different breeds such as \u2018Cocker Spaniel\u2019, \u2018English Springer\u2019, and \u2018English Setter\u2019, while \u2018Bird\u2019 encompasses subcategories including \u2018Peacock\u2019, \u2018Albatross\u2019, and \u2018Little Blue Heron\u2019. Hence, the learning framework of BLM and $\\mathrm{BLM+}$ demonstrates effective handling of the inclusion relationship between label spaces. ", "page_idx": 28}, {"type": "image", "img_path": "135eKqDoRR/tmp/25dd293e39d93df1551d782a44ee15e6477d79ee81f21950817c2e96fb3312cb.jpg", "img_caption": ["Figure 16: Label mapping results of ILM, BLM, BLM $^+$ for VR on CIFAR10 dataset. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "L Applications on Vision-Language Models ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "L.1 Learning Framework ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The distinction between Vision-Language Models (VLM) and vision models lies in (1) vision models take a single image as input, whereas VLMs take a pair of text and images as input; and (2) vision models have fixed pretrained labels, with model outputs being logits, while VLMs lack pretrained labels, with model outputs being the cosine similarity [56] between images and text embeddings. As a result, when applying BLM and $\\mathrm{BLM+}$ to VLM, it is necessary to design an input text set to replace the original pretrained labels in vision models. ", "page_idx": 29}, {"type": "text", "text": "In this paper, we follow a previous work [4] and construct the input texts set by taking the Cartesian product [55] of the downstream label set and the prompt set. BLM and $\\mathrm{BLM+}$ can be applied to compute the joint frequency distribution (for BLM) or aggregated predicted probability (for $\\mathrm{BLM+}$ ) between the input texts and the downstream ground-truth labels. This enables the mapping from candidate input texts to probable classification results. ", "page_idx": 29}, {"type": "text", "text": "The full process of learning $\\omega_{\\mathrm{BLM}}$ , $\\omega_{\\mathrm{BLM+}}$ for vision models or VLMs is illustrated in Figure 17. Besides, the pipeline and algorithm details are the same as BLM and $\\mathrm{BLM+}$ for vision models shown in Figure 2, Algorithm 7 and 9. ", "page_idx": 29}, {"type": "text", "text": "L.2 Performance Results ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Table 9 presents the performance of BLM and $\\mathrm{BLM+}$ applied to VLMs across 12 datasets. For a fair comparison, we follow the previous work [1] to employ CLIP as the pretrained model and a watermarking-based VR with an outer frame size of 30. We utilized an initial learning rate of 40 and a Cosine Annealing learning rate schedule [35], with a total of 200 epochs. An SGD optimizer with a momentum of 0.9 was employed for learning the Input VR. Results without label mapping (denoted by \u2018None\u2019) and one-to-one mapping served as the baseline, and the average accuracy was computed over three different random seeds. ", "page_idx": 29}, {"type": "text", "text": "From the performance results, it can be observed that except for the EuroSAT dataset, which has a small number of classes and simpler tasks (this limitation will be discussed in detail in Appendix H), BLM or $\\mathrm{BLM+}$ achieves improvements across all other tasks. They achieve the average accuracy of $79.1\\%$ and $79.3\\%$ , respectively, without increasing the number of model parameters to be trained. This empirical evidence demonstrates that BLM and $\\mathrm{BLM+}$ can also be effectively applied to VLMs. ", "page_idx": 29}, {"type": "image", "img_path": "135eKqDoRR/tmp/fe1d518398fe6044cf34ece41db5f6cf1f9acf2b1eab6620bd32f57c787bf41f.jpg", "img_caption": ["Figure 17: The framework of learning $\\omega_{\\mathrm{BLM}}$ or $\\omega_{\\mathrm{BLM+}}$ for pretrained vision models (upper) or VLMs (lower). As described in Section 4, for vision models, $\\omega_{\\mathrm{BLM}}$ or $\\omega_{\\mathrm{BLM+}}$ is derived from the frequency distribution (in BLM) or probability aggregation matrix (in $\\mathrm{BLM+}$ ) where pairs of [predicted pretrained label, ground-truth downstream label] are calculated. Nevertheless, for VLMs, the predicted pretrained label is replaced by possible text inputs from the Cartesian product of the downstream label set, and the prompt set. The cosine similarities of images and text embedding are calculated in VLMs to replace the output logits in vision models. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "L.3 Visualization Results ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Figures 18-22 show the visualization results of top-weighted input texts on different datasets applying BLM and $\\mathrm{BLM+}$ . It is evident that, unlike the single optimal text input in one-to-one mapping, BLM and $\\mathrm{BLM+}$ assign different weights to many possible descriptions. For example, in CIFAR10, an image of a bird may be described as \u2018a low-resolution photo of a bird\u2019, \u2018a close-up photo of the bird\u2019, or \u2018this is a photo of a bird\u2019, among others. Such methods affirm different expressions instead of only one description using one-to-one LM. ", "page_idx": 30}, {"type": "text", "text": "These experiments further demonstrate that BLM and $\\mathrm{BLM+}$ can be used to enhance the performance of input VR in VLMs while providing reasonable explanations for why input VR in VLMs can effectively work. ", "page_idx": 30}, {"type": "table", "img_path": "135eKqDoRR/tmp/f5e1d8a4857295584fe854eee0324ebccc26a1c60b31ed2870a2276c2b91de7f.jpg", "table_caption": ["Table 9: Performance comparison on VLMs (mean $\\%\\pm\\mathrm{std}\\,\\%$ ) "], "table_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "135eKqDoRR/tmp/2529e78760e7c6167342f7498e869254e9c841115e8f810b49f6d33e7986ff89.jpg", "img_caption": ["Figure 18: Results of ILM, BLM, $\\mathrm{BLM+}$ for VR on Flowers102 dataset. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "135eKqDoRR/tmp/17ce4f9ac2e0a5b62d19fd16ea26185770de89c41552ac339c7587b30e0c0473.jpg", "img_caption": ["Figure 19: Results of ILM, BLM, $\\mathrm{BLM+}$ for VR based on CLIP on DTD Dataset. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "135eKqDoRR/tmp/f8d727e93a3a063a2835c00f5c8ee3b383fc7d58dd418ed35e06cf6a2369514b.jpg", "img_caption": ["Figure 20: Results of ILM, BLM, $\\mathrm{BLM+}$ for VR based on CLIP on UCF101 dataset. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "135eKqDoRR/tmp/532ebef1b12ff8f0125f5a03994449d76cf68c2602a1fd75f5c35cede36f9402.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "135eKqDoRR/tmp/0ad8f69045176509b376d103127477e65999079216449621cc32e16391ec728e.jpg", "img_caption": ["Figure 22: Results of ILM, BLM, BLM $^+$ for VR based on CLIP on CIFAR10 dataset. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The claims made in the abstract are detailed in the introduction part, and further discussed in each section. Each paragraph in the introduction is provided with a corresponding link to certain sections. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: The limitations are discussed in Appendix H. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: All assumptions and proof are included in Appendix E. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: All implementation details (described in Appendix F) have been included. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The link to the code has been provided in the abstract. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Detailed dataset information is included in Section 5 and Appendix F. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Experiments are run on three seed with the standard divination being reported in Table 1, 2 and 9, and also shown in strip areas in Figure 9-10. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The computer resources have been discussed in Appendix F and detailed in Appendix J. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The research conducted in the paper conform with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Since methods proposed in this paper are used to improve the performance of VR in downstream classification tasks, there is no potential societal impact. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 37}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not release data or models that have a high risk for misuse. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: Creators or original owners of code or data have been cited. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]