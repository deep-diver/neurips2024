[{"figure_path": "47loYmzxep/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of (d) E2E-MFD with existing MF-OD task paradigms (a) Two-Stage (Separate Cascaded), (b) Two-stage (Joint Cascaded) and (c) Multi-stage (Joint Cascaded).", "description": "This figure compares different MF-OD (Multimodal Fusion-Object Detection) task paradigms.  (a) shows a two-stage approach where multimodal fusion and object detection are performed sequentially as separate tasks. (b) illustrates another two-stage method but with joint cascading, where the object detection network guides the fusion process. (c) presents a multi-stage joint cascade, offering more complex interactions.  Finally, (d) highlights the proposed E2E-MFD, which performs multimodal fusion and object detection simultaneously in a single end-to-end framework, streamlining the process and avoiding suboptimal solutions from separate task optimizations.", "section": "1 Introduction"}, {"figure_path": "47loYmzxep/figures/figures_3_1.jpg", "caption": "Figure 2: An overview of the proposed E2E-MFD framework, which consists of a backbone, nodes, and branches. The backbone is utilized to extract multimodal image features. A fine-grained fusion network (ORPPT) and diffusion-based object detection network (CFDP) are optimized by synchronous joint optimization (GMTA) in an end-to-end manner.", "description": "This figure provides a detailed illustration of the E2E-MFD framework's architecture.  It showcases the backbone responsible for extracting features from multimodal images, the Object-Region-Pixel Phylogenetic Tree (ORPPT) for fine-grained image fusion, and the Coarse-to-Fine Diffusion Process (CFDP) for object detection.  The diagram highlights the synchronous joint optimization and Gradient Matrix Task-Alignment (GMTA) techniques employed for end-to-end optimization of both tasks.  The interplay between these components, their functions, and how they synergistically work together is explicitly shown in the figure.", "section": "3.2 Architecture"}, {"figure_path": "47loYmzxep/figures/figures_5_1.jpg", "caption": "Figure 4: Visual results of object detection on M3FD.", "description": "This figure presents a comparison of object detection results on the M3FD dataset using different multimodal image fusion methods.  The top row shows the visible and infrared input images, followed by the fusion results from several state-of-the-art methods and the proposed E2E-MFD.  The yellow bounding boxes indicate the objects detected by the YOLOv5s object detector, highlighting the effectiveness of each method's fusion in enabling accurate object localization. The bottom row presents the ground truth bounding boxes for comparison.", "section": "4.2 Main Results"}, {"figure_path": "47loYmzxep/figures/figures_7_1.jpg", "caption": "Figure 4: Visual results of object detection on M3FD.", "description": "This figure shows a comparison of object detection results on the M3FD dataset using different image fusion methods.  It visually demonstrates how various methods perform in detecting objects (cars, buses, motorcycles, etc.) in visible and infrared images. The ground truth bounding boxes are also shown for comparison, allowing for a qualitative assessment of each method's accuracy and ability to handle challenging conditions like occlusions or low light.", "section": "4.2 Main Results"}, {"figure_path": "47loYmzxep/figures/figures_9_1.jpg", "caption": "Figure 5: Visualization of task dominance and conflicting gradients in joint learning of OD and MF.", "description": "This figure visualizes the gradient values of shared parameters computed by the object detection (OD) loss function (blue) and the multimodal fusion (MF) loss function (orange) during the training process.  It demonstrates that without Gradient Matrix Task Alignment (GMTA), the gradients of the OD task dominate, potentially hindering the learning of the MF task.  With GMTA, a better balance is achieved, mitigating the impact of conflicting gradients and leading to a more effective optimization of both tasks. The plots show the gradient values over the course of 60,000 training iterations.", "section": "4.3 Ablation Studies"}, {"figure_path": "47loYmzxep/figures/figures_9_2.jpg", "caption": "Figure 6: Feature map visualization of various branches in the ORPPT", "description": "This figure visualizes the feature maps generated by different branches of the Object-Region-Pixel Phylogenetic Tree (ORPPT) in the E2E-MFD model.  The ORPPT is a novel component designed to extract features at multiple granularities (from coarse to fine). Each branch represents a different level of granularity, allowing the model to capture both global context and detailed local information. The figure showcases how the feature maps change across different branches, illustrating the ORPPT's ability to capture a multi-scale representation of the input images which is important for both image fusion and object detection tasks.", "section": "3.2 Architecture"}, {"figure_path": "47loYmzxep/figures/figures_15_1.jpg", "caption": "Figure 2: An overview of the proposed E2E-MFD framework, which consists of a backbone, nodes, and branches. The backbone is utilized to extract multimodal image features. A fine-grained fusion network (ORPPT) and diffusion-based object detection network (CFDP) are optimized by synchronous joint optimization (GMTA) in an end-to-end manner.", "description": "This figure illustrates the architecture of the E2E-MFD framework. It shows how multimodal images are processed through a backbone network to extract features.  These features are then fed into two parallel networks: a fine-grained fusion network (ORPPT) and a diffusion-based object detection network (CFDP). Both networks are jointly optimized using a Gradient Matrix Task-Alignment (GMTA) technique, enabling end-to-end learning. The ORPPT focuses on detailed image fusion, while CFDP handles object detection.  This synchronous optimization aims for improved performance in both tasks.", "section": "3.2 Architecture"}, {"figure_path": "47loYmzxep/figures/figures_16_1.jpg", "caption": "Figure 4: Visual results of object detection on M3FD.", "description": "This figure shows a comparison of object detection results on the M3FD dataset using different image fusion methods.  The top row displays the ground truth bounding boxes for the objects in the images.  The bottom row displays the object detection results obtained by using the YOLOv5s detector on images produced by various image fusion methods, including the proposed E2E-MFD method. The figure visually demonstrates how the quality of the fused images impacts the accuracy of object detection. E2E-MFD produces images that yield more accurate object detection, especially in challenging scenarios with occlusion and overlapping objects.", "section": "4.2 Main Results"}, {"figure_path": "47loYmzxep/figures/figures_17_1.jpg", "caption": "Figure 12: Visual results of object detection on M3FD.", "description": "This figure shows a comparison of object detection results on the M3FD dataset using various image fusion methods.  Each row represents a different scene. The first two columns show the visible and infrared images respectively. The subsequent columns demonstrate the fused images produced by different methods (U2Fusion, Tardal, SwinFusion, PIAFusion, DIDFuse, CDDFuse, MetaFusion, YOLOv5s with E2E-MFD fusion, and finally the E2E-MFD method). The red bounding boxes indicate the detected objects.  The figure highlights the improved object detection accuracy achieved by the E2E-MFD method, especially for small or occluded objects, compared to other state-of-the-art methods. ", "section": "4.2 Main Results"}, {"figure_path": "47loYmzxep/figures/figures_18_1.jpg", "caption": "Figure 3: Visual results of image fusion on M3FD.", "description": "This figure shows a qualitative comparison of image fusion results from different methods on the M3FD dataset.  Each row represents a different image pair (visible and infrared).  The first two columns show the original visible and infrared images. The remaining columns display the fused images generated by several state-of-the-art multimodal fusion techniques, including the proposed E2E-MFD method. The figure visually demonstrates the performance of each method in terms of detail preservation, contrast enhancement, and overall visual quality of the fused image.", "section": "4 Experiments and Analysis"}, {"figure_path": "47loYmzxep/figures/figures_18_2.jpg", "caption": "Figure 4: Visual results of object detection on M3FD.", "description": "This figure displays visual comparisons of object detection results on the M3FD dataset, using different image fusion methods. The results demonstrate that the proposed E2E-MFD approach produces superior object detection outcomes, with clearer object boundaries and reduced missed detections compared to existing methods. This highlights the advantage of the end-to-end synchronous fusion and detection framework in improving the overall detection performance.", "section": "4.2 Main Results"}, {"figure_path": "47loYmzxep/figures/figures_19_1.jpg", "caption": "Figure 4: Visual results of object detection on M3FD.", "description": "This figure shows a comparison of object detection results on the M3FD dataset using different image fusion methods.  Each row represents a different scene, with the first two columns showing the visible and infrared images, respectively,  followed by fusion images from various methods (U2Fusion, Tardal, SwinFusion, PIAFusion, DIDFuse, CDDFuse, MetaFusion, YOLOv5s using E2E-MFD fused image and finally E2E-MFD). The last column displays the ground truth bounding boxes for comparison. The results visually demonstrate how different fusion techniques affect the object detection performance, highlighting the strengths of the E2E-MFD approach in providing clearer object boundaries and improved detection accuracy.", "section": "4.2 Main Results"}]