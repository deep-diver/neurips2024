{"references": [{"fullname_first_author": "Alexei Baevski", "paper_title": "wav2vec 2.0: A framework for self-supervised learning of speech representations", "publication_date": "2020-12-01", "reason": "This paper introduces wav2vec 2.0, a framework for self-supervised learning of speech representations, which is used as a pretrained feature extractor in the current work."}, {"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-12-01", "reason": "This paper introduces denoising diffusion probabilistic models, which are used as the foundation for the diffusion model in the current work."}, {"fullname_first_author": "Ian Goodfellow", "paper_title": "Generative adversarial nets", "publication_date": "2014-12-01", "reason": "This paper introduces Generative Adversarial Networks (GANs), a generative model that has influenced many subsequent works, including the current work."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces a method for learning transferable visual models from natural language supervision, which is used to train a new metric for evaluating audio-pose alignment in the current work."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduces the Transformer architecture, which is used as the backbone for the sequence generation task in the current work."}]}