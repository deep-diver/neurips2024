[{"figure_path": "x7pjdDod6Z/figures/figures_0_1.jpg", "caption": "Figure 1: Given a sparse set (e.g., 6) of multi-view RGB images and their normal maps as input, MeshFormer reconstructs high-quality 3D textured meshes with fine-grained, sharp geometric details in a feed-forward pass of just a few seconds. Here, ground truth multi-view RGB and normal images are used as input.", "description": "This figure shows example results of the MeshFormer model.  Given a small number (e.g., six) of RGB images and their corresponding normal maps from different viewpoints, MeshFormer generates a high-quality 3D mesh in just a few seconds. The generated meshes are textured and exhibit fine details and sharp features.  The input for this example uses ground truth (perfect) RGB images and normal maps for demonstration purposes.", "section": "Abstract"}, {"figure_path": "x7pjdDod6Z/figures/figures_3_1.jpg", "caption": "Figure 2: Pipeline Overview. MeshFormer takes a sparse set of multi-view RGB and normal images as input, which can be estimated using existing 2D diffusion models. We utilize a 3D feature volume representation, and submodules Voxel Former and Sparse Voxel Former share a similar novel architecture, detailed in the gray region. We train MeshFormer in a unified single stage by combining mesh surface rendering and 5123 SDF supervision. MeshFormer learns an additional normal texture, which can be used to further enhance the geometry and generate fine-grained sharp geometric details.", "description": "This figure provides a detailed overview of the MeshFormer pipeline.  It shows how sparse multi-view RGB and normal images are processed by 2D encoders. The features are then fed into a novel 3D architecture combining transformers and 3D convolutions (Voxel Former and Sparse Voxel Former). This architecture processes the data through a coarse-to-fine approach, generating a high-resolution sparse feature volume.  Finally, this volume is used with MLPs to generate the SDF, color texture, and normal texture. The SDF is used for mesh extraction with a geometry enhancement step and used for losses along with rendered images.", "section": "3 Method"}, {"figure_path": "x7pjdDod6Z/figures/figures_6_1.jpg", "caption": "Figure 3: Qualitative Examples of Single Image to 3D (GSO dataset). Both the textured and textureless mesh renderings are shown. Please zoom in to examine details and mesh quality, and refer to the supplemental material for results of One-2-3-45++ [31] and CRM [77].", "description": "This figure shows qualitative comparison results of several single-image-to-3D methods on the GSO dataset.  The figure displays both textured and textureless mesh renderings for each method, allowing for a visual comparison of the quality and detail of the generated 3D models.  The caption suggests referring to supplementary material for additional results from two specific methods: One-2-3-45++ and CRM.", "section": "4 Experiments"}, {"figure_path": "x7pjdDod6Z/figures/figures_8_1.jpg", "caption": "Figure 2: Pipeline Overview. MeshFormer takes a sparse set of multi-view RGB and normal images as input, which can be estimated using existing 2D diffusion models. We utilize a 3D feature volume representation, and submodules Voxel Former and Sparse Voxel Former share a similar novel architecture, detailed in the gray region. We train MeshFormer in a unified single stage by combining mesh surface rendering and 5123 SDF supervision. MeshFormer learns an additional normal texture, which can be used to further enhance the geometry and generate fine-grained sharp geometric details.", "description": "This figure shows the pipeline of MeshFormer, a 3D reconstruction model.  It takes sparse multi-view RGB and normal images as input. These images can be predicted by 2D diffusion models.  The model uses a 3D feature volume representation.  Two submodules, Voxel Former and Sparse Voxel Former, share a similar architecture.  The training process combines mesh surface rendering with SDF supervision.  Finally, MeshFormer learns an additional normal texture to improve geometry and details.", "section": "3 Method"}, {"figure_path": "x7pjdDod6Z/figures/figures_9_1.jpg", "caption": "Figure 5: Geometry enhancement generates sharper details. Please zoom in to see the details.", "description": "This figure shows the effect of geometry enhancement on the quality of generated 3D meshes. The top row displays the meshes before enhancement, while the bottom row shows the same meshes after enhancement. Zooming in on the highlighted areas reveals that the geometry enhancement process sharpens the fine details of the meshes, leading to significantly improved visual quality.", "section": "3 Method"}, {"figure_path": "x7pjdDod6Z/figures/figures_16_1.jpg", "caption": "Figure 2: Pipeline Overview. MeshFormer takes a sparse set of multi-view RGB and normal images as input, which can be estimated using existing 2D diffusion models. We utilize a 3D feature volume representation, and submodules Voxel Former and Sparse Voxel Former share a similar novel architecture, detailed in the gray region. We train MeshFormer in a unified single stage by combining mesh surface rendering and 5123 SDF supervision. MeshFormer learns an additional normal texture, which can be used to further enhance the geometry and generate fine-grained sharp geometric details.", "description": "This figure illustrates the pipeline of Meshformer, a model that reconstructs high-quality 3D textured meshes from sparse multi-view RGB and normal images.  It highlights the model's architecture, which combines 3D convolutions and transformers to process 3D voxel features. The training process integrates mesh surface rendering and SDF supervision. Notably, it details the use of a normal texture for geometry enhancement, leading to higher quality mesh outputs.", "section": "3 Method"}, {"figure_path": "x7pjdDod6Z/figures/figures_16_2.jpg", "caption": "Figure 7: The triplane-based method MeshLRM [79] has difficulty capturing words on objects, even when ground truth multi-view RGB images are used as input.", "description": "This figure compares the performance of MeshLRM and the proposed method in capturing fine details, specifically text on the label of a creatine bottle.  MeshLRM, which uses a triplane representation, struggles to render the text clearly, while the proposed method produces a much sharper and more accurate rendering of the text. This highlights one of the advantages of using a 3D voxel representation over a triplane representation for detailed 3D reconstruction.", "section": "A.2 Triplane Artifacts"}, {"figure_path": "x7pjdDod6Z/figures/figures_17_1.jpg", "caption": "Figure 8: Ablation study on input normal maps. Evaluated on the GSO dataset [11]. \u201cw/o normal\u201d indicates that the model is trained with multi-view RGB images only. \u201cw/ predicted normal\u201d indicates that the model is trained with ground truth normal maps but evaluated with predicted normals by Zero123++ [59]. \u201cw/ GT normal\u201d indicates that the model is trained and tested with ground truth normals.", "description": "This ablation study compares the performance of Meshformer when trained with different types of normal maps as input. The three conditions are: no normal maps, predicted normal maps from Zero123++, and ground truth normal maps. The figure shows that using ground truth normal maps yields the best results, as expected.", "section": "4.3 Ablation Study"}, {"figure_path": "x7pjdDod6Z/figures/figures_19_1.jpg", "caption": "Figure 9: Qualitative Results of One-2-3-45++ [31] and CRM [77] on Single Image to 3D. Both the textured and textureless mesh renderings are shown.", "description": "This figure shows a comparison of the 3D reconstruction results from three different methods: One-2-3-45++, CRM, and the proposed MeshFormer.  The input is a single image for each object. Both textured and untextured mesh renderings are presented for each method. The figure demonstrates that the proposed MeshFormer method outperforms the others in terms of mesh quality and detail preservation.", "section": "4. Experiments"}]