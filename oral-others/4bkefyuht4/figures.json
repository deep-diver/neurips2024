[{"figure_path": "4bKEFyUHT4/figures/figures_0_1.jpg", "caption": "Figure 1: Gate count vs. accuracy plot on the CIFAR-10 data set. Our models (\u2605) are substantially above the pareto-front of the SOTA baselines. Gate counts are proportional to chip area. Our models are more efficient than the SOTA by factors of \u2265 29x. Note that the x-axis (gate count) is on a log-scale.", "description": "This figure shows a comparison of different neural network architectures on the CIFAR-10 dataset, plotting accuracy against the number of logic gates used.  The authors' models significantly outperform existing state-of-the-art (SOTA) models in terms of efficiency (fewer gates for higher accuracy). The x-axis uses a logarithmic scale to accommodate the wide range of gate counts.", "section": "1 Introduction"}, {"figure_path": "4bKEFyUHT4/figures/figures_1_1.jpg", "caption": "Figure 2: Architecture of a randomly connected LGN. Each node corresponds to one logic gate. During training, the distribution over choices of logic gates (bottom, 16 options) is learned for each node.", "description": "This figure illustrates the architecture of a randomly connected Logic Gate Network (LGN).  Each node in the network represents a single logic gate (e.g., AND, NAND, XOR). The network's function is determined by the choice of logic gate at each node and the connections between them.  The bottom part of the diagram shows that during training, the network learns the optimal combination of logic gates for each node by selecting from a distribution of 16 possible gates.  The example given in the figure shows how an LGN processes binary inputs representing image pixels (of a panda and a polar bear) to classify them.", "section": "2 Background"}, {"figure_path": "4bKEFyUHT4/figures/figures_2_1.jpg", "caption": "Figure 3: Conventional convolutional neural networks (a) compared to convolutional logic gate networks (b). The images illustrate the first and second to last kernel placements. The nodes correspond to weighted sums (a), and binary logic gates f1, f2, f3 (b), respectively. The weights / choices of logic gates are shared between kernel placements. For visual simplicity, only a single input channel and kernel (output channel) is displayed.", "description": "This figure compares the conventional convolutional neural networks with the proposed convolutional logic gate networks.  The left side (a) shows a conventional CNN where kernel weights are summed. The right side (b) shows the proposed convolutional logic gate network which uses logic gates (f1, f2, f3) instead of weighted sums.  Both illustrations depict shared weights/logic gate choices across kernel placements for spatial efficiency. Only one input and output channel is shown for clarity.", "section": "3 Convolutional Logic Gate Networks"}, {"figure_path": "4bKEFyUHT4/figures/figures_3_1.jpg", "caption": "Figure 4: Plot of the density of activations for the second convolutional block of an or-pooling based convolutional LGN. It shows that training implicitly enforces that the outputs of the block have the activation level of a no-pooling network (i.e., with pure stride).", "description": "This figure shows the activation level during training for three different scenarios: with pre-or-pooling, with post-or-pooling, and without or-pooling. It demonstrates that, even without explicit regularization, training implicitly leads to the activation levels of the no-or-pooling scenario when using or-pooling.", "section": "3.1 Logical Or Pooling"}, {"figure_path": "4bKEFyUHT4/figures/figures_4_1.jpg", "caption": "Figure 3: Conventional convolutional neural networks (a) compared to convolutional logic gate networks (b). The images illustrate the first and second to last kernel placements. The nodes correspond to weighted sums (a), and binary logic gates f1, f2, f3 (b), respectively. The weights / choices of logic gates are shared between kernel placements. For visual simplicity, only a single input channel and kernel (output channel) is displayed.", "description": "This figure compares the architecture of conventional convolutional neural networks (CNNs) with the proposed convolutional logic gate networks (CLGNs).  In CNNs, each kernel performs a weighted sum of the inputs, while in CLGNs, kernels consist of binary logic gates (f1, f2, f3) arranged in a tree structure. The weights in CNNs are replaced by the choices of logic gates in CLGNs, which are learned during training. The figure highlights that the logic gate choices are shared across different locations within the image, mimicking the weight sharing in CNNs. The simplified representation uses a single input and output channel for clarity.", "section": "3 Convolutional Logic Gate Networks"}, {"figure_path": "4bKEFyUHT4/figures/figures_5_1.jpg", "caption": "Figure 6: LogicTreeNet architecture. The logical architectures of the layers / blocks are illustrated on a per neuron basis. Circles indicate a logic gate that can be learned while the logical ors remain fixed. During training, for the trainable nodes, we use probabilistic relaxations of logic gates, which we parameterize via a softmax distribution over operators (Eq. 1/3). For the fixed logical ors, we use the continuous maximum t-conorm relaxation.", "description": "This figure shows the architecture of the LogicTreeNet used in the paper.  It's a convolutional neural network specifically designed for efficient inference using logic gates. The architecture consists of convolutional blocks, each containing logic gate trees, followed by or-pooling layers to reduce dimensionality.  The final layers are fully connected using randomly connected logic gates, ultimately leading to a group sum for classification. The diagram visually depicts the structure, highlighting the learnable logic gates (circles) and fixed or-gates.", "section": "3.4 LogicTreeNet Architecture"}, {"figure_path": "4bKEFyUHT4/figures/figures_9_1.jpg", "caption": "Figure 1: Gate count vs. accuracy plot on the CIFAR-10 data set. Our models (\u2605) are substantially above the pareto-front of the SOTA baselines. Gate counts are proportional to chip area. Our models are more efficient than the SOTA by factors of \u2265 29x. Note that the x-axis (gate count) is on a log-scale.", "description": "This figure shows the trade-off between the number of logic gates and accuracy on the CIFAR-10 dataset.  The plot compares the performance of the proposed Convolutional Differentiable Logic Gate Networks (CDLGNs) with several state-of-the-art (SOTA) baselines.  The authors' models significantly outperform the existing methods, achieving higher accuracy with considerably fewer logic gates. The x-axis is logarithmic, highlighting the substantial efficiency gains.", "section": "1 Introduction"}, {"figure_path": "4bKEFyUHT4/figures/figures_9_2.jpg", "caption": "Figure 8: Distributions of choices of logic gates in a trained MNIST model, comparing Gaussian (left) and residual (right) initializations. The row number indicates the layer and the column indicates the logic gate.", "description": "This figure compares the distribution of logic gates chosen during training for a MNIST model using two different initialization methods: Gaussian and Residual. Each cell in the heatmaps represents the probability of a specific logic gate being selected for a particular layer and gate position.  The Gaussian initialization shows a more uniform distribution across the gates in most layers, indicating a less biased training process.  In contrast, the Residual initialization demonstrates a strong bias towards the identity gate ('A') in many layers, potentially stemming from the intentional bias used in this initialization method to improve training stability and mitigate vanishing gradients. The color intensity represents the probability; darker colors mean lower probability.", "section": "3.2 Residual Initialization"}, {"figure_path": "4bKEFyUHT4/figures/figures_15_1.jpg", "caption": "Figure 6: LogicTreeNet architecture. The logical architectures of the layers / blocks are illustrated on a per neuron basis. Circles indicate a logic gate that can be learned while the logical ors remain fixed. During training, for the trainable nodes, we use probabilistic relaxations of logic gates, which we parameterize via a softmax distribution over operators (Eq. 1/3). For the fixed logical ors, we use the continuous maximum t-conorm relaxation.", "description": "This figure shows the architecture of the LogicTreeNet model for CIFAR-10.  The architecture is composed of convolutional blocks with or-pooling layers, followed by randomly connected layers and a group sum for classification.  Each block reduces the spatial size of the feature maps. The figure highlights the use of logic gate trees, where circles represent learnable logic gates, while the logical OR gates for pooling are fixed.  The training process involves learning the probability distributions over logic gates using a softmax function and applying a continuous maximum t-conorm relaxation to the fixed OR gates.", "section": "3.4 LogicTreeNet Architecture"}, {"figure_path": "4bKEFyUHT4/figures/figures_15_2.jpg", "caption": "Figure 10: CIFAR-10 training and test accuracy plot. The discretization error, i.e., the difference between the inference (hard) mode accuracy and the differentiable training mode accuracy is very small during late training.", "description": "This figure shows the training and testing accuracy curves for a convolutional LGN model trained on the CIFAR-10 dataset.  Three curves are presented: training accuracy in inference mode (discretized), testing accuracy in inference mode (discretized), and testing accuracy during differentiable training. The plot highlights that the discrepancy between differentiable training accuracy and inference accuracy is minimal towards the end of training, indicating a successful relaxation and discretization process.", "section": "A.4 Train/Test Accuracy and Discretization Error"}, {"figure_path": "4bKEFyUHT4/figures/figures_16_1.jpg", "caption": "Figure 11: Test accuracy of an MNIST model with different choices of z3 for the residual initialization, in steps of 0.5. Averaged over 5 seeds.", "description": "This figure shows the results of an ablation study on the hyperparameter z3, which controls the strength of the residual initialization in an MNIST model.  The x-axis represents different values of z3, and the y-axis shows the corresponding test accuracy. The plot reveals that the model performs well when z3 is greater than or equal to 2, achieving high accuracy around z3=5.  Values of z3 below 2 lead to significantly lower accuracy. The error bars represent the average over 5 different random seeds used for training, indicating the variability in performance.", "section": "A.5 Ablation of the Residual Initialization Hyperparameter z3"}]