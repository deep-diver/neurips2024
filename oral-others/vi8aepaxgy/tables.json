[{"figure_path": "Vi8AepAXGy/tables/tables_7_1.jpg", "caption": "Table 1: Comparison between our SVA and other aggregation approaches. The SVA module consistently outperforms other baselines and excels in aggregating high-resolution vision information.", "description": "This table compares the performance of the proposed Spatial Vision Aggregator (SVA) against other methods for aggregating features from multiple vision encoders.  The SVA module significantly improves performance across all benchmark categories, especially excelling at aggregating high-resolution vision data.  The table highlights that the SVA's dynamic and spatially-aware approach effectively integrates vision features with LLMs while mitigating information loss, a key challenge in handling high-resolution visual data.", "section": "3 Spatial Vision Aggregator (SVA): A New Connector Design"}, {"figure_path": "Vi8AepAXGy/tables/tables_9_1.jpg", "caption": "Table 4: Comparison of Cambrian-1 with other leading MLLMs. Cambrian-1 outperforms other open-source models and achieves competitive performance, compared to proprietary models such as GPT-4V, Gemini, and Grok-1.5. Despite using only 576 visual tokens, Cambrian-1 performs better on OCR & Chart and Vision-Centric benchmarks compared to Mini-Gemini-HD and LLaVA-NeXT, which use 2880 tokens.", "description": "This table compares the performance of Cambrian-1, a new family of multimodal LLMs (MLLMs), to other leading MLLMs across various benchmark categories.  It highlights Cambrian-1's superior performance compared to other open-source models and its competitiveness with proprietary models, particularly given its use of significantly fewer visual tokens (576) than some of the others (2880). The results showcase its strength in OCR & Chart and Vision-Centric tasks.", "section": "5 State of the Art Performance"}, {"figure_path": "Vi8AepAXGy/tables/tables_20_1.jpg", "caption": "Table 5: Breakdown of the 2D and 3D tasks evaluated in the Cambrian Vision-Centric Benchmark (CV-Bench). The examples are sourced from ADE20K [154], COCO [79], and Omni3D [18].", "description": "This table details the breakdown of tasks in the Cambrian Vision-Centric Benchmark (CV-Bench).  It categorizes tasks into 2D and 3D types. Each task type then lists the specific tasks, their descriptions, the source datasets used to generate them, and the number of samples available for each task.  This benchmark is specifically designed for vision-centric multimodal LLMs, and these tasks aim to assess various aspects of 2D and 3D understanding of an MLLM.", "section": "2.2 Cambrian Vision-Centric Benchmark (CV-Bench)"}, {"figure_path": "Vi8AepAXGy/tables/tables_22_1.jpg", "caption": "Table 6: Catalog of all vision backbones tested. \u2020 denotes that the visual tokens have been interpolated down to the specified length.", "description": "This table lists the vision backbones used in the experiments.  It details their architecture (e.g., ViT-L, ConvNeXt-L), patch size, resolution, number of tokens, and hidden size.  The table is categorized by supervision type (language-supervised, self-supervised, etc.) and method. The \u2020 symbol indicates that the number of tokens for some models was adjusted via interpolation to match the specified number.", "section": "F Vision Models in MLLMs"}, {"figure_path": "Vi8AepAXGy/tables/tables_22_2.jpg", "caption": "Table 7: Linear Probing Results of Different Vision Backbones", "description": "This table shows the linear probing results for different vision backbones.  Linear probing is a technique used to evaluate the quality of learned visual representations by assessing their performance when used as input features for a linear classifier.  The table lists various vision models along with their architectures, patch size, resolution, number of tokens (used as input), and their respective linear probing accuracy (%). The higher the accuracy, the better the quality of the learned visual representation.", "section": "F Vision Models in MLLMs"}, {"figure_path": "Vi8AepAXGy/tables/tables_23_1.jpg", "caption": "Table 8: Benchmark performance rankings for MLLMs built upon language-supervised and self-supervised vision encoders across all benchmarks (All), and across general (G), knowledge (K), OCR & chart (O), and vision-centric (V) benchmark categories. Full results for all models on each benchmark are tabulated in Table 11.", "description": "This table presents the ranking of various Multimodal Large Language Models (MLLMs) based on their performance across different benchmark categories. The benchmarks assess various capabilities, including general understanding, knowledge-based reasoning, OCR and chart processing, and vision-centric tasks. The table highlights the relative strengths and weaknesses of different MLLMs built using either language-supervised or self-supervised vision encoders.  The full results for all models on each benchmark can be found in Table 11 of the paper.", "section": "2 Evaluating Visual Representations through MLLMs"}, {"figure_path": "Vi8AepAXGy/tables/tables_23_2.jpg", "caption": "Table 6: Catalog of all vision backbones tested. \u2020 denotes that the visual tokens have been interpolated down to the specified length.", "description": "This table lists various vision backbones used in the experiments, categorized by supervision type (Language-Supervised, Self-Supervised, Other), and provides details about their architecture (e.g., ViT-L, ConvNeXt-L), patch size, resolution, number of tokens, and hidden size.  The \"\u2020\" symbol indicates that the number of visual tokens has been adjusted (interpolated) to match the specified value.", "section": "F Vision Models in MLLMs"}, {"figure_path": "Vi8AepAXGy/tables/tables_24_1.jpg", "caption": "Table 6: Catalog of all vision backbones tested. \u2020 denotes that the visual tokens have been interpolated down to the specified length.", "description": "This table lists the vision backbones used in the experiments, categorized by supervision type (Language-Supervised, Self-Supervised, Other, Class Labels).  For each backbone, the architecture, patch size, resolution, number of tokens, and hidden size are specified.  The '\u2020' symbol indicates that the number of visual tokens has been reduced via interpolation.", "section": "F Vision Models in MLLMs"}, {"figure_path": "Vi8AepAXGy/tables/tables_24_2.jpg", "caption": "Table 4: Comparison of Cambrian-1 with other leading MLLMs. Cambrian-1 outperforms other open-source models and achieves competitive performance, compared to proprietary models such as GPT-4V, Gemini, and Grok-1.5. Despite using only 576 visual tokens, Cambrian-1 performs better on OCR & Chart and Vision-Centric benchmarks compared to Mini-Gemini-HD and LLaVA-NeXT, which use 2880 tokens.", "description": "This table compares the performance of Cambrian-1 with other leading Multimodal Large Language Models (MLLMs) across various benchmarks.  It shows that Cambrian-1 surpasses open-source models and achieves competitive results against proprietary models like GPT-4V, Gemini, and Grok-1.5.  A key finding is that even with a significantly lower number of visual tokens (576 compared to 2880 in Mini-Gemini-HD and LLaVA-NeXT), Cambrian-1 demonstrates superior performance on tasks related to Optical Character Recognition (OCR), charts, and vision-centric challenges.", "section": "5 State of the Art Performance"}, {"figure_path": "Vi8AepAXGy/tables/tables_25_1.jpg", "caption": "Table 6: Catalog of all vision backbones tested. \u2020 denotes that the visual tokens have been interpolated down to the specified length.", "description": "This table lists the vision backbones used in the experiments.  It details the type of supervision (language-supervised, self-supervised, other, class labels), the method used to train the model, the architecture, patch size, resolution, number of tokens, and hidden size for each backbone. The  \u2020 symbol indicates that the number of visual tokens for that model was interpolated down to the specified number.", "section": "F Vision Models in MLLMs"}, {"figure_path": "Vi8AepAXGy/tables/tables_25_2.jpg", "caption": "Table 4: Comparison of Cambrian-1 with other leading MLLMs. Cambrian-1 outperforms other open-source models and achieves competitive performance, compared to proprietary models such as GPT-4V, Gemini, and Grok-1.5. Despite using only 576 visual tokens, Cambrian-1 performs better on OCR & Chart and Vision-Centric benchmarks compared to Mini-Gemini-HD and LLaVA-NeXT, which use 2880 tokens.", "description": "This table compares the performance of Cambrian-1 against other leading multimodal large language models (MLLMs), both open-source and proprietary.  It highlights Cambrian-1's superior performance, especially considering its efficient use of visual tokens (576) compared to other models using significantly more (2880). The comparison is broken down by benchmark category, showing Cambrian-1's strengths in OCR & Chart and Vision-Centric tasks.", "section": "5 State of the Art Performance"}, {"figure_path": "Vi8AepAXGy/tables/tables_26_1.jpg", "caption": "Table 4: Comparison of Cambrian-1 with other leading MLLMs. Cambrian-1 outperforms other open-source models and achieves competitive performance, compared to proprietary models such as GPT-4V, Gemini, and Grok-1.5. Despite using only 576 visual tokens, Cambrian-1 performs better on OCR & Chart and Vision-Centric benchmarks compared to Mini-Gemini-HD and LLaVA-NeXT, which use 2880 tokens.", "description": "This table compares the performance of Cambrian-1 against other leading multi-modal LLMs (MLLMs) across various benchmarks.  It highlights Cambrian-1's competitive performance compared to both open-source and proprietary models, particularly in OCR & Chart and Vision-Centric tasks.  A key point is that Cambrian-1 achieves this performance despite using significantly fewer visual tokens (576) than some of its competitors (e.g., Mini-Gemini-HD and LLaVA-NeXT, which use 2880).", "section": "State of the Art Performance"}, {"figure_path": "Vi8AepAXGy/tables/tables_27_1.jpg", "caption": "Table 6: Catalog of all vision backbones tested. \u2020 denotes that the visual tokens have been interpolated down to the specified length.", "description": "This table lists the various vision backbones used in the experiments.  It details the type of supervision (language-supervised, self-supervised, other, or class labels), the method used to train the model, the architecture of the model (e.g., ViT, ConvNeXt), the patch size, resolution, number of tokens, and hidden size for each backbone.  The  \u2020 symbol indicates that the number of visual tokens was adjusted through interpolation.", "section": "F Vision Models in MLLMs"}, {"figure_path": "Vi8AepAXGy/tables/tables_29_1.jpg", "caption": "Table 6: Catalog of all vision backbones tested. \u2020 denotes that the visual tokens have been interpolated down to the specified length.", "description": "This table lists the various vision backbones used in the experiments, categorized by supervision type (language-supervised, self-supervised, other, class labels), along with details such as the method, architecture, patch size, resolution, number of tokens, and hidden size.  The '\u2020' symbol indicates that the number of tokens for some models were adjusted through interpolation to match the target number of tokens used in the experiments.", "section": "F Vision Models in MLLMs"}, {"figure_path": "Vi8AepAXGy/tables/tables_31_1.jpg", "caption": "Table 4: Comparison of Cambrian-1 with other leading MLLMs. Cambrian-1 outperforms other open-source models and achieves competitive performance, compared to proprietary models such as GPT-4V, Gemini, and Grok-1.5. Despite using only 576 visual tokens, Cambrian-1 performs better on OCR & Chart and Vision-Centric benchmarks compared to Mini-Gemini-HD and LLaVA-NeXT, which use 2880 tokens.", "description": "This table compares the performance of Cambrian-1 with other leading Multimodal Large Language Models (MLLMs) across various benchmarks.  It highlights Cambrian-1's superior performance compared to other open-source models and its competitiveness against proprietary models like GPT-4V and Gemini.  A key observation is that Cambrian-1 achieves better results on OCR & Chart and Vision-Centric tasks despite using significantly fewer visual tokens (576) than its competitors (Mini-Gemini-HD and LLaVA-NeXT, which use 2880 tokens). This suggests that Cambrian-1's design is particularly efficient and effective in processing visual information.", "section": "5 State of the Art Performance"}, {"figure_path": "Vi8AepAXGy/tables/tables_33_1.jpg", "caption": "Table 19: Number of leaked test set images. Using image hashing, we assess the overlap of test images across three training datasets: Cambrian10M Data Engine 161k subset (\u201cData Eng.\u201d), Cambrian10M, and LLaVA-665k. We list the number of images in each test set, as well as the number of matching images and percentage of overlap for each training set in blue. Our Data Engine finds a neglible 0.06% of test images, dispelling any concerns that it is targeting the test sets. The full Cambrian10M training set contains 7,244 test set images, whereas LLaVA-665k contains 1,034. Despite being a 15x larger dataset, Cambrian10M only has 6x more overlapping images. Such overlap is inevitable since many test sets use validation images from standard benchmarks (like COCO). It is worth highlighting: although exact image matches are found, this does not mean that exact image-question pairs have been found. Unlike in prior unimodal paradigms of computer vision research, in the multimodal setting, a single data point is composed of an image-text (question) pair, not just the image itself. Thus, seeing a test image during training is not equivalent to \u201ctraining on the test set", "description": "This table presents the results of an analysis to determine the extent of overlap between test images and images from three training datasets: Cambrian10M Data Engine (161k subset), Cambrian10M, and LLaVA-665k.  Image hashing was used to identify overlapping images. The table shows the number of images in each test set, and the number and percentage of matching images found for each training dataset.  The results indicate minimal overlap (0.06%), suggesting that the training data does not contain significant leakage from the test datasets.", "section": "G.6 Test Image Leakage in Visual Instruction Training Data"}, {"figure_path": "Vi8AepAXGy/tables/tables_34_1.jpg", "caption": "Table 1: Comparison between our SVA and other aggregation approaches. The SVA module consistently outperforms other baselines and excels in aggregating high-resolution vision information.", "description": "This table compares the performance of the Spatial Vision Aggregator (SVA) against other methods for aggregating features from multiple vision encoders.  The results show that the SVA consistently achieves better performance across various benchmark categories, particularly excelling when handling high-resolution vision information.  The comparison involves several alternative aggregation techniques, highlighting the superior performance of the SVA architecture.", "section": "3 Spatial Vision Aggregator (SVA): A New Connector Design"}, {"figure_path": "Vi8AepAXGy/tables/tables_34_2.jpg", "caption": "Table 1: Comparison between our SVA and other aggregation approaches. The SVA module consistently outperforms other baselines and excels in aggregating high-resolution vision information.", "description": "This table compares the performance of the Spatial Vision Aggregator (SVA) against other methods for aggregating visual features in a multimodal large language model.  The SVA consistently achieves better results across different benchmark categories, particularly excelling at handling high-resolution visual inputs.  The comparison highlights SVA's advantage in efficiently integrating information from multiple visual encoders while minimizing information loss during the aggregation process.  The table shows the performance of each method on four benchmark categories: General, Knowledge, OCR & Chart, and Vision-Centric.", "section": "3 Spatial Vision Aggregator (SVA): A New Connector Design"}, {"figure_path": "Vi8AepAXGy/tables/tables_35_1.jpg", "caption": "Table 22: Attention distribution studies. The attention distribution among different vision encoders varies with different image categories.", "description": "This table shows the percentage of attention weights assigned to different vision encoders (SigLIP, CLIP, DINOV2, and ConvNext) when processing images from three different benchmark categories: GQA (general visual question answering), DocVQA (document visual question answering), and ScienceQA (science visual question answering).  The results show that the attention distribution varies depending on the image category, reflecting the relative importance of different visual features for different types of questions.", "section": "F Vision Models in MLLMs"}, {"figure_path": "Vi8AepAXGy/tables/tables_35_2.jpg", "caption": "Table 6: Catalog of all vision backbones tested. \u2020 denotes that the visual tokens have been interpolated down to the specified length.", "description": "This table lists the details of the 23 vision backbones used in the Cambrian-1 experiments.  For each backbone, the table specifies the supervision type (language-supervised, self-supervised, depth-supervised, or other), the training method (e.g., contrastive, masked), the architecture (e.g., ViT-L, ConvNeXt), the patch size, resolution, number of tokens, and hidden dimension size.  The '\u2020' symbol indicates that the visual tokens were interpolated to match the specified number of tokens.", "section": "F Vision Models in MLLMs"}, {"figure_path": "Vi8AepAXGy/tables/tables_36_1.jpg", "caption": "Table 6: Catalog of all vision backbones tested. \u2020 denotes that the visual tokens have been interpolated down to the specified length.", "description": "This table lists various vision backbones used in the experiments.  It provides details on the type of supervision used to train each model (language-supervised, self-supervised, other), the specific model architecture, patch size, resolution, number of tokens, and hidden size.  The \u2020 symbol indicates that the number of visual tokens has been reduced through interpolation.", "section": "F Vision Models in MLLMs"}, {"figure_path": "Vi8AepAXGy/tables/tables_37_1.jpg", "caption": "Table 4: Comparison of Cambrian-1 with other leading MLLMs. Cambrian-1 outperforms other open-source models and achieves competitive performance, compared to proprietary models such as GPT-4V, Gemini, and Grok-1.5. Despite using only 576 visual tokens, Cambrian-1 performs better on OCR & Chart and Vision-Centric benchmarks compared to Mini-Gemini-HD and LLaVA-NeXT, which use 2880 tokens.", "description": "This table compares the performance of Cambrian-1 with other leading multimodal large language models (MLLMs), including both open-source and proprietary models.  It shows the performance of each model across various benchmark categories (General, Knowledge, OCR & Chart, and Vision-Centric), highlighting Cambrian-1's competitive performance, particularly its superior performance on OCR & Chart and Vision-Centric tasks despite using significantly fewer visual tokens (576) compared to other models (2880).", "section": "5 State of the Art Performance"}, {"figure_path": "Vi8AepAXGy/tables/tables_38_1.jpg", "caption": "Table 4: Comparison of Cambrian-1 with other leading MLLMs. Cambrian-1 outperforms other open-source models and achieves competitive performance, compared to proprietary models such as GPT-4V, Gemini, and Grok-1.5. Despite using only 576 visual tokens, Cambrian-1 performs better on OCR & Chart and Vision-Centric benchmarks compared to Mini-Gemini-HD and LLaVA-NeXT, which use 2880 tokens.", "description": "This table compares the performance of Cambrian-1 against other leading Multimodal Large Language Models (MLLMs) across various benchmarks.  It highlights Cambrian-1's superior performance over open-source alternatives while showing competitive results against proprietary models like GPT-4V, Gemini, and Grok-1.5. Notably, despite using significantly fewer visual tokens (576 vs 2880), Cambrian-1 surpasses Mini-Gemini-HD and LLaVA-NeXT on OCR & Chart and Vision-Centric tasks.", "section": "5 State of the Art Performance"}, {"figure_path": "Vi8AepAXGy/tables/tables_39_1.jpg", "caption": "Table 6: Catalog of all vision backbones tested. \u2020 denotes that the visual tokens have been interpolated down to the specified length.", "description": "This table lists the details of various vision backbones used in the experiments, categorized by their supervision type (Language-Supervised, Self-Supervised, Other, Class Labels), method (Language, Contrastive, Masked, Depth, Diffusion), architecture (ViT, ConvNeXt, VAE+UNet, ViT-B), patch size, resolution, number of tokens, and hidden size.  The symbol \u2020 indicates that the number of visual tokens has been interpolated to the specified number.", "section": "F Vision Models in MLLMs"}]