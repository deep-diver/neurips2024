[{"figure_path": "8qu52Fl1Dt/figures/figures_2_1.jpg", "caption": "Figure 1: The overall framework of NeuroClips. The red lines represent the inference process.", "description": "NeuroClips is an fMRI-to-video reconstruction framework that consists of three essential components: Perception Reconstructor (PR), Semantics Reconstructor (SR), and Inference Process. PR generates a blurry but continuous rough video from the perceptual level, while SR reconstructs a high-quality keyframe image from the semantic level. The Inference Process employs a T2V diffusion model and combines the reconstructions from PR and SR to reconstruct the final video with high fidelity, smoothness, and consistency.", "section": "3 Method"}, {"figure_path": "8qu52Fl1Dt/figures/figures_5_1.jpg", "caption": "Figure 2: Visualization of Multi-fMRI fusion. With the semantic relevance measure, we can generate video clips up to 6s long without any additional training.", "description": "This figure demonstrates the effectiveness of the proposed Multi-fMRI fusion method for generating longer videos (up to 6 seconds).  The top row shows the ground truth video frames. The middle row shows the results from reconstructing videos using single fMRI scans, showing limitations in generating consistent and longer videos.  The bottom row showcases the results obtained using Multi-fMRI fusion, indicating improved generation of longer, continuous video clips by leveraging semantic relevance between adjacent fMRI frames.", "section": "3.4 Multi-fMRI Fusion"}, {"figure_path": "8qu52Fl1Dt/figures/figures_7_1.jpg", "caption": "Figure 3: Video reconstruction on the cc2017 dataset. On the left are the results of the comparison with previous studies, and on the right are additional comparisons with previous SOTA methods. Best viewed with zoom-in. As shown in the leftmost figure group, Mind-Video's reconstruction fails to go for detail consistency on the character's face, but our NeuroClips achieves an extremely high consistency.", "description": "This figure compares video reconstruction results of NeuroClips with several other state-of-the-art methods on the cc2017 dataset. The left side shows comparisons with earlier methods, highlighting the improvements in detail and consistency achieved by NeuroClips. The right side offers further comparisons with more recent top-performing methods, again emphasizing NeuroClips' superior performance, particularly its ability to maintain detail consistency (e.g., facial features) that other methods lack.  The image demonstrates NeuroClips' ability to reconstruct videos with high-fidelity and smoothness.", "section": "5 Results"}, {"figure_path": "8qu52Fl1Dt/figures/figures_8_1.jpg", "caption": "Figure 4: Visualization of ablation study.", "description": "This figure visualizes the ablation study by comparing the video reconstruction results with different components removed.  It shows the impact of keyframes, blurry videos, and keyframe captioning on the final video quality.  The results highlight the trade-offs between semantic and perceptual reconstruction and the importance of each component for achieving high-fidelity and smooth video reconstruction.", "section": "6 Ablations"}, {"figure_path": "8qu52Fl1Dt/figures/figures_9_1.jpg", "caption": "Figure 5: Visualization of voxel weights for the first ridge regression layer for subject 1, with each voxel's weight averaged and normalized to between 0 and 1 and we set the colorbar to 0.25-0.75 for a clear comparison.", "description": "This figure visualizes the voxel-level weights learned by the model for both semantic and perceptual reconstruction tasks on a brain flatmap for subject 1.  The color intensity represents the weight magnitude, showing which brain regions contributed most strongly to the respective tasks.  Warmer colors (reddish-orange) indicate higher weights. The left panel shows the weights for semantic reconstruction, demonstrating higher activation in higher-level visual areas. The right panel shows weights for perceptual reconstruction, highlighting activation in lower-level visual areas. This visualization provides insights into the model's neural interpretability by illustrating which brain regions were crucial for each task.", "section": "7 Interpretation Results"}, {"figure_path": "8qu52Fl1Dt/figures/figures_14_1.jpg", "caption": "Figure 6: The detailed architecture of Temporal Upsampling module.", "description": "This figure shows the detailed architecture of the Temporal Upsampling module used in the Perception Reconstructor of the NeuroClips framework.  The module consists of four main components: a Spatial Layer, a Temporal Attention mechanism, a learnable Residual Connection, and an Upsampling layer. The input is a five-dimensional fMRI embedding (Ey).  The Spatial Layer processes this embedding, followed by a learnable residual connection, then Temporal Attention is applied, with another residual connection.  Finally, the result is upsampled to the target dimensions. This multi-step process is designed to effectively align fMRI data with the VAE's pixel space while maintaining temporal consistency and preventing overfitting to noise. Each layer's input and output dimensions are also shown, along with the equations for the residual connections (using a mixing coefficient \u03b7).", "section": "3.1 Perception Reconstructor"}, {"figure_path": "8qu52Fl1Dt/figures/figures_16_1.jpg", "caption": "Figure 7: The reconstructed keyframe and its corresponding text. Best viewed with zoom in.", "description": "This figure displays four pairs of ground truth keyframes and their corresponding reconstructed keyframes generated by the model. Each pair is accompanied by a text description that matches the visual content of the keyframes.  The figure aims to demonstrate the model's ability to reconstruct keyframes that accurately reflect the semantic content and visual details of the original video frames.", "section": "C.1 Visualizing Reconstructed Keyframe and Text"}, {"figure_path": "8qu52Fl1Dt/figures/figures_17_1.jpg", "caption": "Figure 3: Video reconstruction on the cc2017 dataset. On the left are the results of the comparison with previous studies, and on the right are additional comparisons with previous SOTA methods. Best viewed with zoom-in.", "description": "This figure shows a visual comparison of video reconstruction results obtained using NeuroClips and other state-of-the-art methods on the cc2017 dataset. The left side compares NeuroClips with earlier methods, highlighting the improvement in detail and consistency.  The right side provides additional comparisons with other top-performing methods, further demonstrating NeuroClips' superior performance in generating high-fidelity and smooth videos.", "section": "5 Results"}, {"figure_path": "8qu52Fl1Dt/figures/figures_18_1.jpg", "caption": "Figure 3: Video reconstruction on the cc2017 dataset. On the left are the results of the comparison with previous studies, and on the right are additional comparisons with previous SOTA methods. Best viewed with zoom-in.", "description": "This figure shows a comparison of video reconstruction results from different methods on the cc2017 dataset.  The left side compares NeuroClips's results to those of several earlier methods, highlighting its improved performance in terms of detail and consistency. The right side provides additional comparisons against state-of-the-art (SOTA) methods, further emphasizing NeuroClips' superiority in reconstructing high-fidelity videos from fMRI data.", "section": "5 Results"}, {"figure_path": "8qu52Fl1Dt/figures/figures_18_2.jpg", "caption": "Figure 3: Video reconstruction on the cc2017 dataset. On the left are the results of the comparison with previous studies, and on the right are additional comparisons with previous SOTA methods. Best viewed with zoom-in.", "description": "This figure displays visual comparisons of video reconstruction results.  The left side shows NeuroClips' results against several earlier methods, highlighting improvements in detail and consistency. The right side provides additional comparisons with state-of-the-art (SOTA) methods, further emphasizing NeuroClips' superior performance in terms of high-fidelity reconstruction and smoothness.", "section": "5 Results"}, {"figure_path": "8qu52Fl1Dt/figures/figures_19_1.jpg", "caption": "Figure 11: Visualization of ablation study.", "description": "This figure visualizes the ablation study on the blurry video. The top row shows the ground truth video frames of the Eiffel Tower. The second row displays the video frames reconstructed by NeuroClips. The third row shows the blurry video frames generated by the Perception Reconstructor. The bottom row shows the video frames reconstructed without the blurry video. The figure demonstrates that the blurry video plays a crucial role in ensuring the smoothness and structural consistency in video reconstruction.", "section": "6 Ablations"}, {"figure_path": "8qu52Fl1Dt/figures/figures_19_2.jpg", "caption": "Figure 5: Visualization of voxel weights for the first ridge regression layer for subject 1, with each voxel's weight averaged and normalized to between 0 and 1 and we set the colorbar to 0.25-0.75 for a clear comparison.", "description": "This figure visualizes the voxel-level weights learned by the model for both semantic and perceptual reconstruction tasks.  It shows the distribution of weights across the brain's cortical surface for subject 1.  The colormap indicates the magnitude of the weights, with warmer colors representing higher weights.  The visualization helps understand which brain regions are most important for the model's performance in reconstructing different aspects of the video.  Higher weights in the higher visual cortex are observed for semantic reconstruction, and higher weights in the lower visual cortex are observed for perceptual reconstruction, which aligns with our understanding of how the brain processes visual information.", "section": "Interpretation Results"}, {"figure_path": "8qu52Fl1Dt/figures/figures_20_1.jpg", "caption": "Figure 13: Visualisation of the generalization ability of SDXL unCLIP.", "description": "This figure shows the results of using SDXL unCLIP to generate images from both COCO and cc2017 datasets.  The left side shows the process using an image from COCO dataset. The right side shows the same process using an image from cc2017 dataset.  Both sides show the input image, the embedding generated by ViT/bigG-14, and the final image generated by SDXL unCLIP. The consistency across different datasets demonstrates that SDXL unCLIP has strong generalization capabilities, which is crucial for the accurate generation of keyframes in the NeuroClips framework.", "section": "D Is NeuroClips Credible?"}]