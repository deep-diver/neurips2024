[{"figure_path": "EKdk4vxKO4/tables/tables_2_1.jpg", "caption": "Table 1: Comparison between our framework and previous methods (Solo and Group). Among these works, MDAgents is the only one to perform all key dimensions of LLM decision-making.", "description": "This table compares MDAgents with several existing single-agent and multi-agent LLM frameworks.  The comparison highlights key differences in interaction type, the presence of multiple roles for agents, the use of early stopping mechanisms, refinement techniques, explicit complexity checks, multi-party chat capabilities, and the flexibility of the conversation pattern. It demonstrates that MDAgents uniquely incorporates all these crucial features for effective LLM decision-making, making it distinct from prior approaches.", "section": "Related Works"}, {"figure_path": "EKdk4vxKO4/tables/tables_5_1.jpg", "caption": "Table 2: Accuracy (%) on Medical benchmarks with Solo/Group/Adaptive setting. Bold represents the best and Underlined represents the second best performance for each benchmark and model. All benchmarks except for MedVidQA were evaluated with GPT-4(V) and MedVidQA was evaluated with Gemini-Pro(Vision). Full experimental results with other models are listed in Table 9-12 in Appendix.", "description": "This table presents the accuracy results of different methods (Solo, Group, and Adaptive) on ten medical benchmarks.  The benchmarks are categorized into medical knowledge retrieval and clinical reasoning/diagnostic tasks.  The table highlights the best performing method for each benchmark, indicating the effectiveness of the adaptive approach compared to single-agent and multi-agent baselines.  Detailed results with additional models are available in the appendix.", "section": "Experiments and Results"}, {"figure_path": "EKdk4vxKO4/tables/tables_8_1.jpg", "caption": "Table 3: Ablations for the impact of moderator's review and MedRAG. The Accuracy were averaged accuracy across all datasets.", "description": "This table presents the ablation study results, showing the impact of adding external medical knowledge (MedRAG) and moderator reviews to the MDAgents framework.  It shows the average accuracy improvement across all datasets when incorporating these additions individually and together.", "section": "4.3 Ablation Studies"}, {"figure_path": "EKdk4vxKO4/tables/tables_18_1.jpg", "caption": "Table 2: Accuracy (%) on Medical benchmarks with Solo/Group/Adaptive setting. Bold represents the best and Underlined represents the second best performance for each benchmark and model. All benchmarks except for MedVidQA were evaluated with GPT-4(V) and MedVidQA was evaluated with Gemini-Pro(Vision). Full experimental results with other models are listed in Table 9-12 in Appendix.", "description": "This table presents the accuracy results of different methods (Solo, Group, and Adaptive) on ten medical benchmarks.  The benchmarks cover various medical tasks including question answering, diagnosis, and visual interpretation.  The table highlights the superior performance of the MDAgents (Adaptive) approach compared to solo and group methods, indicating the effectiveness of the adaptive collaboration strategy.", "section": "4 Experiments and Results"}, {"figure_path": "EKdk4vxKO4/tables/tables_22_1.jpg", "caption": "Table 5: Accuracy (%) on entire MedQA 5-options dataset with GPT-40 mini", "description": "This table presents a comprehensive evaluation of various methods on the complete MedQA 5-options dataset using the GPT-40 mini model.  It compares the accuracy of different single-agent and multi-agent approaches, including MDAgents, highlighting the superior performance of MDAgents in achieving an accuracy of 83.6%.", "section": "D Additional Results"}, {"figure_path": "EKdk4vxKO4/tables/tables_22_2.jpg", "caption": "Table 2: Accuracy (%) on Medical benchmarks with Solo/Group/Adaptive setting. Bold represents the best and Underlined represents the second best performance for each benchmark and model. All benchmarks except for MedVidQA were evaluated with GPT-4(V) and MedVidQA was evaluated with Gemini-Pro(Vision). Full experimental results with other models are listed in Table 9-12 in Appendix.", "description": "This table presents the accuracy results of different methods (Solo, Group, and Adaptive) on ten medical benchmarks.  It shows the performance comparison of several baseline methods and MDAgents under different settings.  The results highlight the superior performance of MDAgents, particularly on benchmarks requiring medical knowledge and multi-modal reasoning.", "section": "Experiments and Results"}, {"figure_path": "EKdk4vxKO4/tables/tables_23_1.jpg", "caption": "Table 3: Ablations for the impact of moderator's review and MedRAG. The Accuracy were averaged accuracy across all datasets.", "description": "This table presents ablation study results, showing the impact of adding a moderator's review and/or MedRAG (Retrieval-Augmented Generation) to the MDAgents framework. It shows that both methods improve accuracy, and combining them yields the highest accuracy.", "section": "4.3 Ablation Studies"}, {"figure_path": "EKdk4vxKO4/tables/tables_23_2.jpg", "caption": "Table 8: Impact of collaboration settings on high-complexity image+text tasks", "description": "This table presents the accuracy results for various collaborative settings in handling high-complexity image+text tasks.  It compares sequential vs. parallel processing approaches, with and without discussion among agents. The results highlight the significant impact of enabling discussion in both sequential and parallel settings, leading to improved accuracy.", "section": "4.3 Ablation Studies"}, {"figure_path": "EKdk4vxKO4/tables/tables_24_1.jpg", "caption": "Table 2: Accuracy (%) on Medical benchmarks with Solo/Group/Adaptive setting. Bold represents the best and Underlined represents the second best performance for each benchmark and model. All benchmarks except for MedVidQA were evaluated with GPT-4(V) and MedVidQA was evaluated with Gemini-Pro(Vision). Full experimental results with other models are listed in Table 9-12 in Appendix.", "description": "This table presents the accuracy of different methods (Solo, Group, and Adaptive) on various medical benchmarks.  It shows the performance of different LLMs (GPT-3.5, GPT-4, and Gemini) using several techniques (zero-shot, few-shot, chain-of-thought, self-consistency, ensemble refinement, and MedPrompt).  The adaptive method (MDAgents) is compared against single-agent and multi-agent baselines.  Bold indicates the best performance for each benchmark, and underlined indicates the second-best.", "section": "Experiments and Results"}, {"figure_path": "EKdk4vxKO4/tables/tables_25_1.jpg", "caption": "Table 2: Accuracy (%) on Medical benchmarks with Solo/Group/Adaptive setting. Bold represents the best and Underlined represents the second best performance for each benchmark and model. All benchmarks except for MedVidQA were evaluated with GPT-4(V) and MedVidQA was evaluated with Gemini-Pro(Vision). Full experimental results with other models are listed in Table 9-12 in Appendix.", "description": "This table presents the accuracy results of different methods (Solo, Group, and Adaptive) on ten medical benchmarks.  The benchmarks cover various tasks, including medical knowledge retrieval, clinical reasoning, and medical visual interpretation.  The table highlights the best-performing method for each benchmark and shows the impact of different model settings and the adaptive approach.", "section": "4 Experiments and Results"}, {"figure_path": "EKdk4vxKO4/tables/tables_25_2.jpg", "caption": "Table 2: Accuracy (%) on Medical benchmarks with Solo/Group/Adaptive setting. Bold represents the best and Underlined represents the second best performance for each benchmark and model. All benchmarks except for MedVidQA were evaluated with GPT-4(V) and MedVidQA was evaluated with Gemini-Pro(Vision). Full experimental results with other models are listed in Table 9-12 in Appendix.", "description": "This table presents the accuracy results achieved by various methods (Solo, Group, and Adaptive) across ten different medical benchmarks.  The benchmarks are categorized into Medical Knowledge Retrieval and Clinical Reasoning & Diagnostic datasets.  The table highlights the best-performing method for each benchmark, indicating the effectiveness of the adaptive approach in comparison to traditional single-agent and fixed multi-agent methods.", "section": "Experiments and Results"}, {"figure_path": "EKdk4vxKO4/tables/tables_26_1.jpg", "caption": "Table 2: Accuracy (%) on Medical benchmarks with Solo/Group/Adaptive setting. Bold represents the best and Underlined represents the second best performance for each benchmark and model. All benchmarks except for MedVidQA were evaluated with GPT-4(V) and MedVidQA was evaluated with Gemini-Pro(Vision). Full experimental results with other models are listed in Table 9-12 in Appendix.", "description": "This table presents the accuracy results of different methods (Solo, Group, and Adaptive) on ten medical benchmarks.  It shows the performance of various methods, including baseline methods and the proposed MDAgents framework, for each benchmark. The best and second-best performing models are highlighted for each benchmark and method.  The table also notes the specific LLMs used for each benchmark.", "section": "4 Experiments and Results"}, {"figure_path": "EKdk4vxKO4/tables/tables_35_1.jpg", "caption": "Table 2: Accuracy (%) on Medical benchmarks with Solo/Group/Adaptive setting. Bold represents the best and Underlined represents the second best performance for each benchmark and model. All benchmarks except for MedVidQA were evaluated with GPT-4(V) and MedVidQA was evaluated with Gemini-Pro(Vision). Full experimental results with other models are listed in Table 9-12 in Appendix.", "description": "This table presents the accuracy of different methods (Zero-shot, Few-shot,  CoT, CoT-SC, ER, Medprompt, Majority Voting, Weighted Voting, Borda Count, MedAgents, Meta-Prompting, Reconcile, AutoGen, DyLAN, and MDAgents) on 10 medical benchmarks categorized into Medical Knowledge Retrieval and Clinical Reasoning & Diagnosis.  The results show the performance of each method across three settings: Solo (single LLM agent), Group (multiple LLMs collaborating), and Adaptive (MDAgents, dynamically adjusting the collaboration structure).  Different LLMs (GPT-4, Gemini) were used depending on the benchmark.  Bold values show the best performance for each benchmark and model.", "section": "4 Experiments and Results"}]