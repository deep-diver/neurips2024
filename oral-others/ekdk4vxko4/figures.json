[{"figure_path": "EKdk4vxKO4/figures/figures_1_1.jpg", "caption": "Figure 1: Medical Decision-Making Agents (MDAgents) framework. Given a medical query from different medical datasets, the framework performs 1) medical complexity check, 2) recruitment, 3) analysis and synthesis, 4) final decision-making steps.", "description": "The MDAgents framework is depicted, illustrating its four main stages.  First, a medical query is subjected to a complexity check to determine its difficulty level (low, moderate, or high).  Based on this assessment, the system recruits agents: a single primary care clinician for low-complexity queries, a multidisciplinary team (MDT) for moderate-complexity queries, or an integrated care team (ICT) for high-complexity queries.  The selected agents then perform analysis and synthesis, using various methods including prompting, collaborative discussion, or report generation depending on the complexity level.  Finally, the framework arrives at a final decision by synthesizing the outputs from different stages, generating a final answer.", "section": "1 Introduction"}, {"figure_path": "EKdk4vxKO4/figures/figures_3_1.jpg", "caption": "Figure 1: Medical Decision-Making Agents (MDAgents) framework. Given a medical query from different medical datasets, the framework performs 1) medical complexity check, 2) recruitment, 3) analysis and synthesis, 4) final decision.", "description": "The figure illustrates the MDAgents framework, which takes a medical query as input and goes through four steps to reach a final decision.  First, it checks the complexity of the query (low, moderate, or high). Second, it recruits appropriate agents (LLMs) based on the complexity, ranging from a single primary care clinician for simple queries to multidisciplinary or integrated care teams for complex queries. Third, the agents analyze and synthesize information.  Finally, a decision is made and reported.", "section": "1 Introduction"}, {"figure_path": "EKdk4vxKO4/figures/figures_6_1.jpg", "caption": "Figure 3: Experiment with the MedQA dataset (N=25 randomly sampled questions). (a) LLM's capability to classify complexity. (b-d) Evaluating 25 medical problems by solving each one 10 times at various complexity levels. The x-axis represents the accuracy achieved for each problem, while the y-axis shows the number of problems that reached that level of accuracy.", "description": "This figure shows the results of an experiment using the MedQA dataset.  Part (a) illustrates the LLM's ability to correctly classify the complexity of medical questions. Parts (b), (c), and (d) show the accuracy of the LLM's responses for questions of low, moderate, and high complexity, respectively. Each question was attempted 10 times, and the figure shows the accuracy distribution.", "section": "4.2 Results"}, {"figure_path": "EKdk4vxKO4/figures/figures_7_1.jpg", "caption": "Figure 4: Our method outperforms Solo and Group settings across different medical benchmarks.", "description": "This figure presents a bar chart comparing the accuracy of the proposed MDAgents method against the baseline Solo and Group methods across multiple medical benchmarks. The results visually demonstrate the superior performance of MDAgents in achieving higher accuracy compared to the single-agent and multi-agent baselines.  The x-axis represents the different approaches (Ours, Solo, Group), while the y-axis displays the accuracy percentages achieved on the benchmark datasets. The chart highlights the significant improvements obtained by MDAgents, providing a clear visual summary of the performance gains achieved by the adaptive approach.", "section": "4 Experiments and Results"}, {"figure_path": "EKdk4vxKO4/figures/figures_8_1.jpg", "caption": "Figure 5: Impact of complexity selection of the query. Accuracy of each ablation on text-only (left), text+image (center) and text+video (right) benchmarks are reported.", "description": "This figure shows the impact of the adaptive complexity selection method on the accuracy of the model across three different data modalities: text-only, image+text, and video+text.  It compares the performance of the adaptive method to three static complexity settings (Low, Moderate, High). The results demonstrate that the adaptive method achieves higher accuracy compared to static settings across all modalities, highlighting the effectiveness of dynamically adjusting the complexity level based on the input query.", "section": "4.3 Ablation Studies"}, {"figure_path": "EKdk4vxKO4/figures/figures_9_1.jpg", "caption": "Figure 6: Our method outperforms Solo and Group settings across different medical benchmarks.", "description": "This figure displays the results of experiments comparing the performance of three different settings (Solo, Group, and Ours - Adaptive) across various medical benchmarks. The x-axis represents the number of agents used, while the y-axis in (a) shows the accuracy achieved and in (b) displays the number of API calls made.  The results demonstrate that the adaptive method (Ours) consistently outperforms both the solo and group methods in terms of accuracy, while also maintaining efficiency by requiring fewer API calls. The chart (c) illustrates the robustness of the adaptive approach across different temperatures, indicating a better performance under higher temperatures.", "section": "4.2 Results"}, {"figure_path": "EKdk4vxKO4/figures/figures_9_2.jpg", "caption": "Figure 7: An illustration of consensus entropy in group collaboration process of MDAgents (w/ Gemini-Pro (Vision), N=30 for each dataset) on medical benchmarks with different modality inputs.", "description": "This figure shows the entropy (a measure of uncertainty or disagreement) over time during the collaborative discussion phase of the MDAgents framework.  The lines represent the average entropy for different data modalities (text-only, image+text, video+text).  The shaded areas represent the standard deviation around the average. The figure demonstrates how the entropy decreases over time (steps 0-5), indicating a convergence of agent opinions and reaching a consensus.  The speed of convergence varies based on the data modality, with video+text showing the fastest convergence and text-only the slowest.", "section": "4.6 Convergence Trends in Consensus Dynamics"}, {"figure_path": "EKdk4vxKO4/figures/figures_27_1.jpg", "caption": "Figure 8: Complexity Distribution for each dataset classified by GPT-4(V) and Gemini-Pro (Vision) (for MedVidQA). The plot illustrates the varying levels of medical complexity across datasets, reflecting the diverse nature of medical question answering, diagnostic reasoning, and medical visual interpretation tasks. For instance, MedQA is categorized under Medical Knowledge Retrieval due to their focus on text-based questions and literature synthesis, while MIMIC-CXR, categorized under Clinical Reasoning and Diagnostic tasks, shows a high complexity distribution due to the need for interpreting detailed radiographic images (See Section in Section 4.1 for the task categorization)", "description": "This figure shows the distribution of low, moderate, and high complexity questions in different medical datasets as classified by GPT-4 and Gemini.  The complexity levels reflect the difficulty of the questions based on their textual nature, clinical reasoning involved and the inclusion of image or video data. It highlights the diversity in complexity across datasets, indicating the need for an adaptive approach like MDAgents.", "section": "4 Experiments and Results"}, {"figure_path": "EKdk4vxKO4/figures/figures_28_1.jpg", "caption": "Figure 8: Complexity Distribution for each dataset classified by GPT-4(V) and Gemini-Pro (Vision) (for MedVidQA). The plot illustrates the varying levels of medical complexity across datasets, reflecting the diverse nature of medical question answering, diagnostic reasoning, and medical visual interpretation tasks. For instance, MedQA is categorized under Medical Knowledge Retrieval due to their focus on text-based questions and literature synthesis, while MIMIC-CXR, categorized under Clinical Reasoning and Diagnostic tasks, shows a high complexity distribution due to the need for interpreting detailed radiographic images (See Section in Section 4.1 for the task categorization)", "description": "This figure shows the complexity distribution for each dataset as classified by GPT-4(V) and Gemini-Pro (Vision).  It highlights the varying levels of complexity across different types of medical tasks, from simple text-based questions (low complexity) to complex tasks involving image and video interpretation (high complexity). The differences reflect the diverse nature of medical question answering, diagnostic reasoning, and medical visual interpretation.", "section": "4 Experiments and Results"}, {"figure_path": "EKdk4vxKO4/figures/figures_28_2.jpg", "caption": "Figure 10: Simplified agent structure examples assigned during the expert recruitment process ranging from (a) A Primary Care Clinician (PCC), (b) Multi-disciplinary Team (MDT), (C) MDT w/hierarchy to (d) Integrated Care Team (ICT).", "description": "This figure illustrates the different agent structures used in the MDAgents framework depending on the complexity of the medical query. (a) shows a single Primary Care Clinician for low-complexity queries. (b) depicts a Multi-disciplinary Team (MDT) for moderate complexity, where multiple specialists collaborate. (c) presents a hierarchical MDT for more complex scenarios. (d) illustrates an Integrated Care Team (ICT), the most complex structure, involving multiple teams and specialists for high-complexity queries.", "section": "3 MDAgents: Medical Decision-making Agents"}, {"figure_path": "EKdk4vxKO4/figures/figures_32_1.jpg", "caption": "Figure 1: Medical Decision-Making Agents (MDAgents) framework. Given a medical query from different medical datasets, the framework performs 1) medical complexity check, 2) recruitment, 3) analysis and synthesis, 4) final decision.", "description": "The MDAgents framework is shown, illustrating its four key steps.  First, the complexity of the medical query is checked. Then, based on the complexity, agents (LLMs) are recruited; a single agent for low-complexity queries, or teams of agents (MDT or ICT) for moderate or high-complexity queries, respectively.  Next, analysis and synthesis occur within the recruited agents, followed by a final decision and report generation. This dynamic process mimics the way human clinicians approach medical decision-making.", "section": "1 Introduction"}, {"figure_path": "EKdk4vxKO4/figures/figures_33_1.jpg", "caption": "Figure 1: Medical Decision-Making Agents (MDAgents) framework. Given a medical query from different medical datasets, the framework performs 1) medical complexity check, 2) recruitment, 3) analysis and synthesis, 4) final decision.", "description": "This figure illustrates the MDAgents framework's four main steps for medical decision-making.  It starts by checking the complexity of the medical query. Based on this complexity, the appropriate team of LLMs (Primary Care Clinician, Multidisciplinary Team, or Integrated Care Team) is recruited to analyze and synthesize information to arrive at a final decision. The framework adapts its approach based on the complexity of the task, mirroring real-world medical decision-making processes.", "section": "1 Introduction"}, {"figure_path": "EKdk4vxKO4/figures/figures_34_1.jpg", "caption": "Figure 1: Medical Decision-Making Agents (MDAgents) framework. Given a medical query from different medical datasets, the framework performs 1) medical complexity check, 2) recruitment, 3) analysis and synthesis, 4) final decision.", "description": "This figure illustrates the MDAgents framework, which consists of four main steps: 1) assessing the complexity of a given medical query; 2) recruiting a team of LLMs (Large Language Models) tailored to the query's complexity (a solo LLM for simple queries, a multidisciplinary team (MDT) for moderate queries, and an integrated care team (ICT) for complex queries); 3) analyzing and synthesizing information from various sources using the recruited LLMs; and 4) making a final decision based on the integrated information.", "section": "1 Introduction"}]