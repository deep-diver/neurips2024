[{"figure_path": "NPKZF1WDjZ/tables/tables_7_1.jpg", "caption": "Table 1: Overall results of our DeAR Framework on three intricate reasoning datasets. (* : p < 0.05).", "description": "This table presents the overall performance of the DeAR framework on three different reasoning datasets (ScienceQA, StrategyQA, and GSM8K).  It compares DeAR's accuracy against several baseline methods (Few-shot, CoT, ToT, GoT, Least-to-most, and SelfCheck) across three different LLMs (GPT-3.5, LLAMA2, and ChatGLM3).  The asterisk (*) indicates statistically significant improvements (p < 0.05) compared to the baselines.", "section": "5 Experiments"}, {"figure_path": "NPKZF1WDjZ/tables/tables_7_2.jpg", "caption": "Table 2: Characteristics of T in different datasets.", "description": "This table presents a quantitative analysis of the reasoning trees (T) generated by the DeAR model across three different datasets: ScienceQA, StrategyQA, and GSM8K.  It provides key statistics for each dataset, including the average branching factor (Avg Branch), average depth (Avg Depth), and average length of the rationale (Avg Length of R). These metrics offer insights into the complexity of the questions within each dataset and how the DeAR model approaches them.", "section": "5.3 Analyses of the Reasoning Tree"}, {"figure_path": "NPKZF1WDjZ/tables/tables_8_1.jpg", "caption": "Table 3: ROSCOE evaluation results of rationales generated by Tree-of-Thoughts (ToT), Graph-of-Thoughts (GoT) and DeAR on different datasets. SC = Source-Consistency; RA = Reasoning Alignment.", "description": "This table presents the results of evaluating the logical coherence of rationales generated by three different methods: Tree-of-Thoughts (ToT), Graph-of-Thoughts (GoT), and the proposed DeAR method.  The evaluation uses the ROSCOE suite, specifically focusing on Source Consistency (SC) and Reasoning Alignment (RA) metrics across three different datasets: ScienceQA, StrategyQA, and GSM8K. Higher scores in both SC and RA indicate better logical coherence and alignment with ground truth.", "section": "5.4 Logical Coherence of the Generated Rationales"}, {"figure_path": "NPKZF1WDjZ/tables/tables_8_2.jpg", "caption": "Table 1: Overall results of our DeAR Framework on three intricate reasoning datasets. (* : p < 0.05).", "description": "This table presents the overall performance of the DeAR framework on three benchmark datasets: ScienceQA, StrategyQA, and GSM8K.  The results compare DeAR against several baseline methods (Few-shot, CoT, ToT, GoT, Least-to-most, and SelfCheck) across three different LLMs (GPT-3.5, LLaMA2, and ChatGLM3).  The '*' indicates statistically significant improvements (p < 0.05) over baseline methods.  The table shows that DeAR consistently outperforms all baseline methods across all datasets and LLMs.", "section": "5 Experiments"}, {"figure_path": "NPKZF1WDjZ/tables/tables_15_1.jpg", "caption": "Table 1: Overall results of our DeAR Framework on three intricate reasoning datasets. (* : p < 0.05).", "description": "This table presents the overall performance of the DeAR framework on three benchmark datasets: ScienceQA, StrategyQA, and GSM8K.  It compares DeAR against several baseline methods (Few-shot, CoT, ToT, GoT, Least-to-most, and SelfCheck) across three different large language models (LLMs): GPT-3.5, LLaMA2, and ChatGLM3. The results show the accuracy of each method on each dataset and LLM, indicating that DeAR consistently outperforms the baselines. The asterisk (*) denotes statistically significant improvements (p < 0.05).", "section": "5 Experiments"}, {"figure_path": "NPKZF1WDjZ/tables/tables_15_2.jpg", "caption": "Table 1: Overall results of our DeAR Framework on three intricate reasoning datasets. (* : p < 0.05).", "description": "This table presents the overall performance of the DeAR framework on three benchmark datasets: ScienceQA, StrategyQA, and GSM8K.  It compares DeAR's accuracy against several baseline methods, including few-shot prompting, Chain-of-Thought (CoT), Tree-of-Thoughts (ToT), Graph-of-Thoughts (GoT), Least-to-most prompting, and SelfCheck, across three different LLMs (GPT-3.5, LLaMA2, and ChatGLM3).  The results show DeAR achieves significant improvements over the baseline methods on all three datasets and across all LLMs tested, indicated by the asterisks denoting statistically significant differences (p < 0.05).", "section": "5 Experiments"}, {"figure_path": "NPKZF1WDjZ/tables/tables_16_1.jpg", "caption": "Table 1: Overall results of our DeAR Framework on three intricate reasoning datasets. (* : p < 0.05).", "description": "This table presents the overall performance comparison of the proposed DeAR framework against several state-of-the-art baselines on three complex reasoning benchmarks: ScienceQA, StrategyQA, and GSM8K.  The results are broken down by LLM model (GPT-3.5, LLaMA2, and ChatGLM3) and show DeAR's significant accuracy improvements across all models and datasets. The * indicates statistically significant differences (p<0.05).", "section": "5 Experiments"}, {"figure_path": "NPKZF1WDjZ/tables/tables_17_1.jpg", "caption": "Table 1: Overall results of our DeAR Framework on three intricate reasoning datasets. (* : p < 0.05).", "description": "This table presents the overall performance of the DeAR framework on three benchmark datasets: ScienceQA, StrategyQA, and GSM8K.  The results are broken down by Large Language Model (LLM) backbone used (GPT-3.5, LLaMA2, and ChatGLM3) and compared to several baseline methods (Few-shot, CoT, ToT, GoT, Least-to-most, SelfCheck).  The asterisk (*) indicates statistically significant improvements (p < 0.05) compared to the baseline methods.", "section": "5 Experiments"}, {"figure_path": "NPKZF1WDjZ/tables/tables_18_1.jpg", "caption": "Table 10: Performance comparison of DeAR with and without self-check on the ScienceQA dataset.", "description": "This table presents the results of an ablation study comparing the performance of the DeAR framework with and without the self-check mechanism.  The study is conducted using the ScienceQA dataset and three different large language models (LLMs): GPT-3.5, LLaMA2-7B, and ChatGLM3-6B.  The accuracy (ACC) is reported for each LLM and framework configuration. The purpose is to demonstrate the impact of the self-check on DeAR's overall accuracy.", "section": "B.3 Ablation Study of the Self-Check Method in the Analyze Stage"}, {"figure_path": "NPKZF1WDjZ/tables/tables_19_1.jpg", "caption": "Table 1: Overall results of our DeAR Framework on three intricate reasoning datasets. (* : p < 0.05).", "description": "This table presents the overall performance of the DeAR framework on three benchmark datasets: ScienceQA, StrategyQA, and GSM8K.  It compares DeAR's accuracy against several baseline methods (Few-shot, CoT, ToT, GoT, Least-to-most, and SelfCheck) across three different LLMs (GPT-3.5, LLaMA2, and ChatGLM3).  The asterisk (*) indicates statistically significant improvements (p < 0.05) compared to the baseline methods.", "section": "5 Experiments"}]