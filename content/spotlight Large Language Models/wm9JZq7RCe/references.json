{"references": [{"fullname_first_author": "Zaid Alyafeai", "paper_title": "Evaluating various tokenizers for arabic text classification", "publication_date": "2023-00-00", "reason": "This paper is cited for its empirical study of tokenizers in different languages, showing the variety of tokenizers used across various domains."}, {"fullname_first_author": "Jonathan H Clark", "paper_title": "Canine: Pre-training an efficient tokenization-free encoder for language representation", "publication_date": "2022-00-00", "reason": "This paper is cited for its investigation into the possibility of circumventing tokenization in language models, an area of active research."}, {"fullname_first_author": "Benjamin L Edelman", "paper_title": "The evolution of statistical induction heads: In-context learning markov chains", "publication_date": "2024-00-00", "reason": "This paper is cited for its study on the learning trajectory of transformers on Markov data and the observation of a surprising phenomenon where models trained on higher-order Markov data may reduce to simpler models."}, {"fullname_first_author": "Philip Gage", "paper_title": "A new algorithm for data compression", "publication_date": "1994-00-00", "reason": "This paper introduced the BPE algorithm, which is a widely used tokenization algorithm in modern LLMs, making it a highly relevant foundational paper for the current work."}, {"fullname_first_author": "Vil\u00e9m Zouhar", "paper_title": "Tokenization and the noiseless channel", "publication_date": "2023-00-00", "reason": "This paper is mentioned for its theoretical investigation of the relationship between tokenization and the cross-entropy loss in language models, forming a strong theoretical basis for the current research."}]}