[{"figure_path": "wm9JZq7RCe/figures/figures_2_1.jpg", "caption": "Figure 1: 2-state switching process. The above state diagram describes the distribution of X\u2099 conditioned on X\u2099\u22121. kth-order extension: the conditional probability of X\u2099 only depends on X\u2099\u2212k through the kernel, Pr(X\u2099 = 1|X\u2099\u2212k = 0) = p and Pr(X\u2099 = 0|X\u2099\u2212k = 1) = q.", "description": "This figure shows a state diagram for a 2-state Markov chain.  The states are labeled 0 and 1, and the transition probabilities are p (from state 0 to state 1) and q (from state 1 to state 0).  The caption also describes how this model can be generalized to a kth-order Markov chain, where the probability of transitioning to a state depends only on the state k time steps prior. This is done by defining transition probabilities Pr(X\u2099 = 1|X\u2099\u2212k = 0) = p and Pr(X\u2099 = 0|X\u2099\u2212k = 1) = q.", "section": "2 Formulation"}, {"figure_path": "wm9JZq7RCe/figures/figures_3_1.jpg", "caption": "Figure 2: Token distribution returned by the transformer tokenized by a learnt BPE encoder with a dictionary size of 20. A test sequence is generated from the stochastic source and encoded into a token sequence t. Each narrow vertical column represents the distribution over next tokens returned by the transformer when the first x tokens of t are fed into the model, where x is varied from 0 to the length of t. For most values of x, the model appears to predict the same distribution over the next token.", "description": "This figure visualizes the token distribution predicted by a transformer model at various positions within a test sequence. The transformer is trained on tokenized data using a learned BPE encoder with a dictionary size of 20.  Each vertical column represents the probability distribution of the next token, given the preceding tokens.  The relative uniformity of the columns across the x-axis (sequence length) suggests the model learns a unigram model, even though the data source is a more complex kth-order Markov process.", "section": "3 Unigram models under tokenization"}, {"figure_path": "wm9JZq7RCe/figures/figures_4_1.jpg", "caption": "Figure 3: Transformers trained on the order-2 switching Markov process (Figure 1) with p = q = 0.8. On the left we have the model trained without tokenization and on the right the model uses BPE with a dictionary of size 10 learnt from data.", "description": "This figure compares the performance of transformers trained on a simple Markov process with and without tokenization.  The left panel shows that without tokenization, the model fails to learn the data distribution, converging to a unigram model instead.  In contrast, the right panel shows that with the addition of BPE tokenization, the transformer is able to learn the target distribution successfully, achieving near-optimal cross-entropy loss. This illustrates the significant impact of tokenization on the performance of transformers on non-i.i.d data.", "section": "Experimental Results"}, {"figure_path": "wm9JZq7RCe/figures/figures_4_2.jpg", "caption": "Figure 3: Transformers trained on the order-2 switching Markov process (Figure 1) with p = q = 0.8. On the left we have the model trained without tokenization and on the right the model uses BPE with a dictionary of size 10 learnt from data.", "description": "This figure compares the training performance of transformers on an order-2 Markov process with and without tokenization. The left panel shows that without tokenization, the transformer's loss converges to that of the best unigram model, failing to learn the true Markov process.  The right panel demonstrates that with BPE tokenization (dictionary size 10), the transformer achieves near-optimal cross-entropy loss, demonstrating that tokenization helps transformers effectively model higher-order dependencies in data.", "section": "Experimental Results"}, {"figure_path": "wm9JZq7RCe/figures/figures_7_1.jpg", "caption": "Figure 3: Transformers trained on the order-2 switching Markov process (Figure 1) with p = q = 0.8. On the left we have the model trained without tokenization and on the right the model uses BPE with a dictionary of size 10 learnt from data.", "description": "This figure shows the training loss curves for transformer models trained on a second-order Markov process with and without tokenization.  The left panel displays the loss for a model trained without tokenization, showing it plateaus at the performance of a unigram model. The right panel shows the loss for a model trained with BPE tokenization, demonstrating that it reaches near-optimal performance.", "section": "Experimental Results"}, {"figure_path": "wm9JZq7RCe/figures/figures_7_2.jpg", "caption": "Figure 3: Transformers trained on the order-2 switching Markov process (Figure 1) with p = q = 0.8. On the left we have the model trained without tokenization and on the right the model uses BPE with a dictionary of size 10 learnt from data.", "description": "This figure compares the training performance of transformer models with and without tokenization on a second-order Markov process.  The left panel shows that without tokenization, the model fails to surpass the performance of a unigram model. The right panel demonstrates that with BPE tokenization (dictionary size 10), the model achieves near-optimal performance, indicating the importance of tokenization for transformer models trained on data with dependencies.", "section": "Experimental Results"}, {"figure_path": "wm9JZq7RCe/figures/figures_8_1.jpg", "caption": "Figure 5: Performance vs. dictionary size. Tokenizers are trained on the Wikitext-103 dataset. For all other tokenizers we train unigram models while for the the character-level tokenizer, we train k-gram models for k \u2208 {1,2,3,4}. Likelihood models are trained on the GLUE dataset. The parentheses indicates the number of distinct observed k-grams, which lower bounds the k-gram model complexity.", "description": "This figure compares the performance of different tokenizers on the GLUE dataset in terms of cross-entropy loss. The x-axis shows the size of the dictionary used, and the y-axis represents the cross-entropy loss. The figure shows that tokenizers trained on Wikitext-103 significantly outperform the character-level tokenizer, highlighting the impact of tokenization on model performance. The different colored lines represent the different tokenizers (LZW, BPE, Unigram, Wordpiece, and Character), and the different markers within each line shows the performance of k-gram models (k=1, 2, 3, 4).", "section": "Experimental Results"}, {"figure_path": "wm9JZq7RCe/figures/figures_8_2.jpg", "caption": "Figure 3: Transformers trained on the order-2 switching Markov process (Figure 1) with p = q = 0.8. On the left we have the model trained without tokenization and on the right the model uses BPE with a dictionary of size 10 learnt from data.", "description": "This figure shows the training loss curves for transformers trained on a simple Markov process. The left plot displays the results when no tokenization is used, while the right plot shows the improved results when using a Byte Pair Encoding (BPE) tokenizer with a dictionary of size 10. The figure highlights the significant improvement in convergence speed and final loss when tokenization is applied.", "section": "Experimental Results"}, {"figure_path": "wm9JZq7RCe/figures/figures_17_1.jpg", "caption": "Figure 6: The circled nodes indicates substrings which are tokens in Dict. Red nodes indicate the set of \u201cmaximal tokens\u201d, which are the set of tokens which the greedy encoder assigns, leaving out those which can only be assigned as the last token of some string. Tokens like \u201cb\u201d are never assigned by the greedy encoder (save as the last token of the encoding of a string) since any sufficiently long trajectory starting with b must have a longer prefix which is also a token, namely, one of ba, bc, bba, bbb or bbc. The vertices of the tree which are assigned by the greedy encoder as tokens (together with all their prefixes) forms a cut of the tree, which marks the dotted red line. The heavy hitting property asserts that this cut is uniformly far away from the root node (\u00d8, and that every vertex s marked red has P(s) \u2264 1/d\u03b2.", "description": "This figure is a tree diagram illustrating the heavy-hitting property of a dictionary. Each node represents a substring.  The circled nodes show substrings that are tokens. Red nodes show substrings that are *maximal* tokens; that is, tokens that are not prefixes of any other tokens in the dictionary. The red dotted line marks a boundary above which tokens are never selected by the greedy encoder, illustrating the fact that these maximal tokens have a low probability of appearing. The heavy hitting property is that these maximal tokens have a probability that is less than 1/d\u03b2.", "section": "A.6 Heavy-hitter dictionaries and a proof of Theorem 3.6"}, {"figure_path": "wm9JZq7RCe/figures/figures_25_1.jpg", "caption": "Figure 7: Jointly generating a sequence and its greedy encoding: In this example we use the greedy encoder under the dictionary composed of all the substrings shadowed red. The first character (a) is sampled from the stationary distribution. Then an infinite string is sampled on the tree with a as root (green path). The last substring on this path which is a token (t\u2081 = abb) is returned by the greedy encoder. Then the next character x = b is sampled from the source conditioned on the previous character (b) and further conditioned on t\u2081x \u2209 Dict. Finally, another infinite string is sampled on the tree with x = b as root (purple path) and the last substring on this path which is a token (t2 = ba) is returned by the greedy encoder. Repeating this process, we can generate a string, here, abbba\u2026, as well as its greedy encoding, (abb, ba,\u2026).", "description": "This figure illustrates the process of jointly generating a sequence and its greedy encoding using a dictionary of substrings.  It starts by sampling a character from the stationary distribution and then iteratively builds a sequence and its encoding by selecting the longest token-prefix from an infinite trajectory on the tree, ensuring that the selected token's extension is not already present in the dictionary. This process continues until a complete sequence and its encoding are generated.", "section": "B.1 Analysis of Algorithm 1"}, {"figure_path": "wm9JZq7RCe/figures/figures_26_1.jpg", "caption": "Figure 6: The circled nodes indicates substrings which are tokens in Dict. Red nodes indicate the set of \u201cmaximal tokens\u201d, which are the set of tokens which the greedy encoder assigns, leaving out those which can only be assigned as the last token of some string. Tokens like \u201cb\u201d are never assigned by the greedy encoder (save as the last token of the encoding of a string) since any sufficiently long trajectory starting with b must have a longer prefix which is also a token, namely, one of ba, bc, bba, bbb or bbc. The vertices of the tree which are assigned by the greedy encoder as tokens (together with all their prefixes) forms a cut of the tree, which marks the dotted red line. The heavy hitting property asserts that this cut is uniformly far away from the root node (\u00d8, and that every vertex s marked red has P(s) \u2264 1/d\u03b2.", "description": "This figure shows a tree structure representing substrings generated from a given alphabet. Nodes represent substrings and edges represent the addition of a character. Red nodes represent maximal tokens in the dictionary, meaning that no longer substring containing the token is also a token in the dictionary. The figure illustrates the heavy-hitting property, which ensures that maximal tokens have low probabilities and that the greedy encoder assigns tokens far from the root node.", "section": "A.6 Heavy-hitter dictionaries and a proof of Theorem 3.6"}, {"figure_path": "wm9JZq7RCe/figures/figures_27_1.jpg", "caption": "Figure 9: A pictorial representation of the proof of Lemma B.6", "description": "This figure illustrates the proof of Lemma B.6 by contradiction. It shows that if two tokens decode to the same character substring, then at each step in the merging process, these occurrences of the substring must map to the same sequence of tokens, which leads to a contradiction. This demonstrates that tokens assigned by Algorithm 1 must have distinct character representations.", "section": "B Additional Theoretical Results I: A sequential variant of BPE"}, {"figure_path": "wm9JZq7RCe/figures/figures_41_1.jpg", "caption": "Figure 1: 2-state switching process. The above state diagram describes the distribution of Xn conditioned on Xn\u22121. kth-order extension: the conditional probability of Xn only depends on Xn\u2212k through the kernel, Pr(Xn = 1|Xn\u2212k = 0) = p and Pr(Xn = 0|Xn\u2212k = 1) = q.", "description": "This figure shows a state diagram of a 2-state Markov chain.  The probability of transitioning from state 0 to state 1 is \u03b4, and from state 1 to state 0 is also \u03b4.  The self-transition probabilities (remaining in the same state) are 1-\u03b4.  The caption also mentions a kth-order extension where the state transitions depend only on the kth previous state, with probabilities p and q for transitioning to state 1 from state 0, and state 0 from state 1, respectively.", "section": "2 Formulation"}]