{"references": [{"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-01", "reason": "This paper introduces the concept of scaling laws for large language models, which is fundamental to the paper's investigation of optimal training strategies for protein language models."}, {"fullname_first_author": "Jordan Hoffmann", "paper_title": "Training compute-optimal large language models", "publication_date": "2022-03-01", "reason": "This paper directly addresses the optimization of compute budgets in training large language models, providing a framework relevant to the paper's focus on compute-optimal protein language models."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-01-01", "reason": "This paper establishes the effectiveness of large language models as few-shot learners, a concept extended to protein language models in the current work."}, {"fullname_first_author": "Alexander Rives", "paper_title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences", "publication_date": "2021-01-01", "reason": "This paper demonstrates the success of large-scale unsupervised learning for protein sequences, influencing the approach and dataset choices in the current study."}, {"fullname_first_author": "Ali Madani", "paper_title": "Progen: Language modeling for protein generation", "publication_date": "2020-04-01", "reason": "This paper introduces a pioneering protein language model (ProGen), which is directly compared to and improved upon by the models developed in this research."}]}