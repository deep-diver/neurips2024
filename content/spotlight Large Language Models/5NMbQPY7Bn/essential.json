{"importance": "This paper is important because it presents a novel approach to video understanding that avoids the computationally expensive and data-hungry process of traditional video-text pre-training. By using large language models and a text-only pre-alignment technique, researchers can achieve promising results on challenging video understanding benchmarks without the need for extensive video data. This opens up new possibilities for efficient video understanding research, especially for those with limited computational resources or access to large video datasets.  The approach also tackles the issues with inconsistent web-collected video-text data.  The proposed method addresses current limitations in the field, paving the way for further advancements in video-language research.", "summary": "TOPA: Extending LLMs for video understanding using only text data.", "takeaways": ["TOPA pre-trains large language models for video understanding using only text data, eliminating the need for real video data and reducing computational costs.", "TOPA introduces a novel text-only pre-alignment framework using automatically generated textual videos to simulate real video-text pairs.", "Experiments show TOPA outperforms existing video-text pre-training methods on challenging long-form video understanding benchmarks, even without training on real videos."], "tldr": "Current video understanding methods struggle with the complexity of videos and noisy web video-text data, requiring extensive pre-training on large-scale video data. This is computationally expensive and limits accessibility for researchers.  This paper introduces a novel approach, TOPA, which tackles these challenges. \n\nTOPA uses Large Language Models (LLMs) to automatically generate \"textual videos\" with corresponding annotations, simulating real video-text pairs.  It then uses these simulated pairs for pre-alignment with the video modality via CLIP model. This method successfully extends LLMs to video understanding without pre-training on real video data and achieves state-of-the-art results on challenging video benchmarks like EgoSchema.", "affiliation": "Zhejiang University", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "5NMbQPY7Bn/podcast.wav"}