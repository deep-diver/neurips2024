[{"type": "text", "text": "QTIP: Quantization with Trellises and Incoherence Processing ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Albert Tseng Qingyao Sun David Hou Cornell University Cornell University dhou@alumni.caltech.edu albert@cs.cornell.edu qs234@cornell.edu ", "page_idx": 0}, {"type": "text", "text": "Christopher De Sa Cornell University cdesa@cs.cornell.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Post-training quantization (PTQ) reduces the memory footprint of LLMs by quantizing weights to low-precision datatypes. Since LLM inference is usually memorybound, PTQ methods can improve inference throughput. Recent state-of-the-art PTQ approaches use vector quantization (VQ) to quantize multiple weights at once, which improves information utilization through better shaping. However, VQ requires a codebook with size exponential in the dimension. This limits current VQbased PTQ works to low VQ dimensions $(\\le8)$ that in turn limit quantization quality. Here, we introduce QTIP, which instead uses trellis coded quantization (TCQ) to achieve ultra-high-dimensional quantization. TCQ uses a stateful decoder that separates the codebook size from the bitrate and effective dimension. QTIP introduces a spectrum of lookup-only to computed lookup-free trellis codes designed for a hardware-efficient \u201cbitshift\u201d trellis structure; these codes achieve state-of-theart results in both quantization quality and inference speed. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) have accelerated advancements in fields ranging from natural language processing [34] to scientific modeling [28]. However, the largest LLMs have hundreds of billions of parameters that can take over a terabyte of memory to load in half-precision; this size poses significant challenges for the practical deployment of LLMs [33, 17, 2]. For example, smallbatch autoregressive decoding, a common form of inference for LLMs, is memory bound [35]. Even on a modern datacenter GPU with $\\approx3\\mathrm{TB/s}$ memory bandwidth, a large LLM $(>200\\mathrm{GB})$ can only be directly run at $<20$ tokens per second and may require multiple devices [35]. One way to accelerate inference is by compressing LLMs. This directly reduces the memory footprint of the model and increases the theoretical maximum inference throughput on any given machine. ", "page_idx": 0}, {"type": "text", "text": "One form of compression, weight-only post-training quantization (PTQ), quantizes trained model weights to lower precision datatypes [9, 35, 5]. The latest state-of-the-art weight-only PTQ methods, $\\mathrm{QulP\\#}$ and AQLM, use vector quantization (VQ) to achieve high-quality 2-bit models [35, 11]. In VQ, a vector $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ is quantized to one of $2^{k d}$ vectors in $\\mathbb{R}^{d}$ that form a codebook $C\\in\\mathbb{R}^{2^{k d}\\times d}$ . A higher vector dimension $d$ allows for better codebook shaping and packing density, improving information utilization [19]. However, unstructured VQ requires exponential time and space in both the bitrate and dimension, limiting its practicality. During quantization, VQ costs $O(2^{k d}\\dot{d})$ time to perform nearestneighbor rounding to $C$ , and during inference, $C$ must fit in hardware cache for fast lookups. This exponential scaling limits how high $d$ can be and thus the advantages of VQ over scalar quantization. ", "page_idx": 0}, {"type": "image", "img_path": "7sdkLVuYCU/tmp/7aa106827b1243261bac85aadbf1750b04ba46b59161b8933a793535d881d4a7.jpg", "img_caption": ["Figure 1: QTIP performs ultra-high dimensional $(>100)$ quantization by using Trellis Coded Quantization, which has linear cost in dimension. This enables QTIP to outperform Vector Quantizationbased approaches $(\\mathrm{QulP\\#}$ , AQLM) that are limited to low dimensions. With QTIP, 2 bit models scale better than theoretically optimal 4 bit models. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "To address this limitation, we propose QTIP, which uses trellis-coded quantization (TCQ) to enable tractable ultra-high-dimensional $(>100)$ quantization and improve quantization quality over prior VQ-based approaches. In the simplest scalar form of TCQ, a length- $\\mathbf{\\nabla}\\cdot\\mathbf{\\mathcal{T}}$ sequence $S$ is statefully quantized using a trellis \u2013 a directed graph $G$ with $2^{L}$ nodes, each with $2^{k}$ incoming and outgoing edges and a scalar value [24]. The reconstructed sequence $\\hat{S}$ corresponds to the node values of a length- $\\boldsymbol{\\cdot}\\boldsymbol{T}$ walk on $G$ , and quantization finds the walk that minimizes some distortion metric on $S$ and $\\hat{S}$ . Since neighboring entries in $\\hat{S}$ are connected by one of $2^{k}$ edges, we only need to store which $e d g e$ an entry came from, which takes $k$ bits. For additive distortion metrics such as squared error, the optimal $\\hat{S}$ can be found with the Viterbi algorithm, which runs in $O(2^{L}T)$ time [13, 24]. This means that the cost of quantization is independent of the bitrate $k$ and linear in the sequence dimension $T$ , enabling tractable high dimensional quantization. ", "page_idx": 1}, {"type": "text", "text": "However, TCQ is not free. During inference, vanilla TCQ requires storing both $G$ and the size $2^{L}\\times V$ node value codebook, which can be too large to fit in cache. TCQ-quantized sequences also cannot generally be decoded in parallel, as $t$ th elment of $\\hat{S}$ could depend on up to the first $t k$ encoded bits. In QTIP, we solve these issues by introducing a series of fast compute-based Gaussian codes designed for the hardware-efficient \u201cbitshift trellis.\u201d Specifically, the bitshift trellis supports parallel decoding, does not require storing $G$ , and our compute-based codes eliminate needing to store a large node value codebook. This enables high-quality quantization of Gaussian sources while supporting fast inference, and we adopt incoherence processing with the random Hadamard transform to ensure that LLM weights are approximately i.i.d Gaussian distributed. Altogether, QTIP ", "page_idx": 1}, {"type": "text", "text": "1. Achieves a state-of-the-art combination of weight-only LLM PTQ quality and fast inference through hardware-efficient trellis and codebook design.   \n2. Introduces multiple novel hardware-efficient $\\leq4$ instructions per weight) compute-based random Gaussian codes for TCQ on i.i.d. Gaussian sources. ", "page_idx": 1}, {"type": "text", "text": "2 Background and Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We focus on weight-only post-training quantization (PTQ) of LLMs in this work; other modelcompression approaches include quantization-aware training (QAT) and pruning. These methods are not strictly orthogonal to each other, as one could both prune and quantize a model. Since QTIP is a weight-only PTQ method, the rest of this section focuses on this area. Most current state-of-the-art PTQ methods round to minimize the per-layer proxy loss from Nagel et al. [27]. ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\ell({\\hat{W}})=\\mathbb{E}_{x}\\left[\\|({\\hat{W}}-W)x\\|^{2}\\right]=\\operatorname{tr}\\left(({\\hat{W}}-W)H({\\hat{W}}-W)^{T}\\right)\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Here, $\\hat{W}\\,\\in\\,\\mathbb{R}^{m\\times n}$ is the quantized weight matrix, $\\boldsymbol{x}\\in\\mathbb{R}^{n}$ is an input activation, and $H\\;=$ $\\mathbb{E}_{x}\\left[x x^{T}\\right]\\,\\in\\,\\mathbb{R}^{n\\times n}$ is interpreted as a proxy Hessian matrix. This objective is defined per-layer, making it tractable for very large models. However, minimizing it is difficult due to the nondifferentiable nature of quantization. Instead many works have proposed algorithms such as Hessianbased adaptive rounding, alternating optimization, and even coordinate descent to approximately minimize the proxy error [11, 5, 35, 14]. ", "page_idx": 2}, {"type": "text", "text": "2.1 Incoherence Processing ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The effectiveness of these methods depends on properties of $W$ . For example, many works have observed that weight and activation outliers cause poor quantization quality [10, 20, 29]. In QuIP, Chee et al. [5] proposed that incoherence was important for quantifying this effect. ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 (Chee et al. [5]). A Hessian $H\\in\\mathbb{R}^{n\\times n}$ is $\\mu$ -incoherent if its eigendecomposition ${\\cal H}=Q\\Lambda Q^{T}$ has $\\begin{array}{r}{\\operatorname*{max}_{i,j}~|Q_{i j}|=\\operatorname*{max}_{i,j}~|e_{i}^{T}Q e_{j}|\\leq\\mu/\\sqrt{n}}\\end{array}$ . A weight matrix $\\bar{W}\\in\\mathbb{R}^{m\\times n}$ is $\\mu$ - incoherent if $\\mathrm{max}_{i,j}$ $|W_{i j}|=\\operatorname*{max}_{i,j}$ $|e_{i}^{T}W e_{j}|\\leq\\mu\\|W\\|_{F}/\\sqrt{m n}$ . ", "page_idx": 2}, {"type": "text", "text": "Essentially, incoherence means the weights and important rounding directions (Hessian eigenvectors) are not too large in any direction, aiding quantization. To make $W,H$ incoherent (small $\\dot{\\mu_{,}}$ ), one can perform incoherence processing (IP) by conjugating $W,H$ with random orthogonal matrices $U,V$ : $\\bar{W}\\leftarrow U W V^{T},\\tilde{H}\\leftarrow V H V^{T}$ . $\\mathrm{QulP\\#}$ introduced IP with the random Hadamard transformation (RHT), which performs $\\Tilde{W}\\leftarrow V_{m}S_{m}W S_{n}V_{n}^{T}$ , $\\tilde{H}\\gets V_{n}S_{n}H S_{n}V_{n}^{T}$ where $V_{k}$ is a $k\\times k$ Hadamard matrix and $S_{k}$ is a length $k$ random sign vector. The RHT achieves, with probability $\\geq1-\\delta.$ , $\\mu_{\\tilde{W}}=$ $2\\log(4m n/\\delta)$ , meaning that $\\Tilde{W}$ \u2019s entries are approximately independently Gaussian distributed, which can aid quantization [35, 3]. We choose to build on incoherence processing here because the independent Gaussian-like weights it produces are suitable inputs for trellis coding [23]. ", "page_idx": 2}, {"type": "text", "text": "2.2 Vector Quantization (VQ) for LLM PTQ ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "$k$ -bit VQ quantizes a $d$ dimensional vector $S$ to one of $2^{k d}\\ d$ -dimensional vectors that form a codebook $\\bar{C}\\in\\mathbb{R}^{2^{k d}\\times d}$ [1]. Since $C$ is an unstructured collection of arbitrary vectors, VQ enables better shaping and packing density than scalar product quantization (SPQ), where each entry in $S$ is quantized independently [19]. However, this also comes at the cost of exponential time quantization and exponential space inference: finding the nearest neighbor in $C$ requires $O(2^{k d}d)$ time, and storing $C$ requires $O(2^{k d}d)$ space. The current crop of state-of-the-art LLM PTQ methods, $\\mathrm{QulP\\#}$ and AQLM, both use VQ to achieve high-quality 2-bit models. Since the shaping advantage of VQ comes from high dimensionality, both $\\mathrm{QulP\\#}$ and AQLM attempt to maximize dimensionality. AQLM\u2019s uses a large 8D codebook (1MiB) that does not fit in L1 cache. $\\mathrm{QulP\\#}$ uses an 8D compressible codebook based on the $E_{8}$ lattice, which is highly symmetric. This codebook is compressible by $256\\times$ and barely ftis in L1 cache. In either case, the VQ dimension is effectively hardware-limited to $\\leq8$ , motivating methods that enable even higher-dimensional quantization. ", "page_idx": 2}, {"type": "text", "text": "2.3 Trellis-Coded Quantization (TCQ) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "TCQ was first proposed by Marcellin and Fischer [24] to apply the beneftis of trellis coded modulation, a conceptually dual problem, to quantization. Define a $(L,k,V)$ trellis $G$ as a directed graph with $2^{L}$ nodes, each of which has $2^{k V}$ incoming and outgoing edges and a value $\\in\\mathbb{R}^{V}$ ; these values form a codebook $C\\in\\mathbb{R}^{2^{L}\\times V}$ . To quantize a length- $\\mathbf{\\nabla}\\cdot\\mathbf{\\mathcal{T}}$ sequence $S\\in\\mathbb{R}^{T}$ , each contiguous length- $V$ subsequence of $S$ is assigned to a node $\\in G$ , with the restriction that the assigned nodes form a walk. The reconstruction $\\hat{S}$ of $S$ is then given by concatenating node values in the walk. When $V=1$ , this setup describes Marcellin and Fischer [24]\u2019s original scalar TCQ. When $V>1$ , this describes TCVQ, which applies TCQ to vectors [12, 37]. ", "page_idx": 2}, {"type": "text", "text": "Finding the optimal $\\hat{S}$ under an additive distortion metric can be done with the Viterbi algorithm in $O(2^{L}\\bar{T})$ time. This is linear in sequence length, enabling ultra-high dimensional quantization. For exposition, we briefly describe the Viterbi algorithm here. Concretely, if we want to quantize a $T$ - length scalar sequence reinterpreted as a sequence of vectors $s_{1},s_{2},\\dotsc,s_{T/V}\\in\\mathbb{R}^{V}$ using a trellis code with graph $G$ and codebook $C$ , this corresponds to solving the optimization problem ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "T/V minimize \u2225Cxi \u2212si\u22252 over $x_{1},x_{2},\\ldots,x_{T/V}$ the vertex sequence of a walk on graph $G$ . i=1   \nThis optimization problem can be solved exactly with dynamic programming via the value function   \nVt(x) = min si\u2225 x1, x2, . . . , xt the vertex sequence of a walk on $G$ and $x_{t}=x\\Bigg\\}$ i=1 ", "page_idx": 3}, {"type": "text", "text": "using the update rule ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nu_{t}(y)=\\operatorname*{min}_{(x,y)\\in G}\\nu_{t-1}(x)+\\|C_{y}-s_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This Viterbi approach clearly takes time linear in $T$ and in the number of edges of $G$ ; with a few simple optimizations this can be brought to $O(2^{L}T)$ . In comparison, brute-force-searching all possible $2^{k T}$ codes\u2014which is what we would need to do for an unstructured $k$ -bit $T$ -dimensional codebook\u2014 would take time proportional to $2^{L T/V}$ . The ability to tractably find the closest representable vector in $\\mathbb{R}^{T}$ , even for large $T$ , is in some sense the \u201cmain benefit\u201d of trellis coding. For i.i.d sources, as $L$ increases, TCQ efficiently approaches the infinite-length distortion-rate $D_{R}$ , which lower bounds the attainable distortion of a $k$ -bit quantizer [19]. As shown in Table 1, when quantizing an i.i.d. Gaussian with $k=2$ , the scalar Lloyd-Max quantizer attains 0.118 MSE, $\\mathrm{QulP\\#}$ \u2019s 8D E8P codebook 0.089 MSE, our (QTIP) 256D $L=16$ TCQ quantizer 0.069 MSE, and $D_{R}=0.063$ [21, 25, 35, 8]. ", "page_idx": 3}, {"type": "text", "text": "3 QTIP ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Quantizing with TCQ requires storing both the codebook $(2^{L}\\times V)$ and trellis structure $(2^{L}\\times2^{k V})$ during inference. These components are too large for fast inference when $L\\gtrapprox12$ , which is necessary for high quality. Furthermore, for a generic trellis, recovering the state (and so the decoded value) at step tth requires a graph walk using the first $k t$ bits: this prevents parallel decoding. QTIP solves these problems with a novel combination of incoherence processing, a hardware-efficient \u201cbitshift trellis,\u201d and fast compute-based random Gaussian codes. Incoherence processing makes $W$ approximatelly i.i.d Gaussian, which reduces quantization to Gaussian source coding. The bitshift trellis removes needing to store the trellis structure during decoding and also enables parallel decoding. Finally, the fast compute-based random Gaussian codes remove the need to store the full codebook, completing the equation for fast inference. On the quality side, the fast random Gaussian codes enable the simple bitshift trellis to match complicated trellises and achieve state-of-the-art quantization quality. ", "page_idx": 3}, {"type": "text", "text": "The main focus of QTIP is on what to quantize with (i.e. TCQ) and not how to quantize (e.g. adaptive rounding or descent methods). The general construction of QTIP can be used as a drop-in replacement for VQ in any rounding framework. In the following sections, we first describe the \u201cbitshift\u201d trellis (Section 3.1). Then, we describe a series of fast compute-based codes for i.i.d Gaussian sources, aligning with different types of hardware (Sections 3.1.1 and 3.1.2). Finally, we give an approximation for the tail-biting trellis problem, which lets us more efficiently load weights in hardware (Section 3.2). ", "page_idx": 3}, {"type": "text", "text": "3.1 \u201cBitshift\u201d Trellis and Codebook Design ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The bitshift trellis was introduced by Mao and Gray [23] as part of the \u201crandom permutation trellis coder\u201d (RPTC). In the bitshift trellis, node $i$ has an edge to node $j$ if $\\exists c\\in\\mathbb{Z},\\mathbf{\\bar{0}}\\,\\leq\\,c\\,<\\,2^{k V}$ , s.t. $j=(i2^{\\stackrel{.}{k}V}\\mathrm{~mod~}2^{L})+c$ . Essentially, the top $L-k V$ bits of $j$ equal the bottom $L-k V$ bits of $i$ . This means that the first group of $V$ weights depends only on the bits at positions $\\{1,2,\\ldots,L\\}$ , the second only on bit positions $\\left\\{k V+1,k\\bar{V}+2,\\bar{.}..\\,,k V+L\\right\\}$ , and in general the tth on bit positions $\\{(t-1)k\\dot{V}+1,\\dot{\\dots},(t-1)\\dot{k}V+L\\}$ . During inference, obtaining the next compressed group of $V$ weights in a sequence only requires bitshifting by $k V$ bits, which is supported on virtually all hardware. Furthermore, since each group of $V$ weights only depends on a contiguous window of $L$ bits in $\\hat{S}$ , decoding can be parallelized. Figure 2 shows a simple $\\left(L=2,k=1,V=1\\right)$ bitshift trellis. Note that edges only exist between nodes that overlap by 1 bit, and storing the quantized length 6 $\\hat{S}$ indeed only requires 6 bits (plus the initial state). ", "page_idx": 3}, {"type": "text", "text": "Quantizing an i.i.d. source with the bitshift trellis is nontrivial because neighboring groups of weights sharing many bits can potentially lead to strong correlations (Figure $3\\ \\mathrm{LL}_{\\rightmoon,}$ ). The RPTC permutes ", "page_idx": 3}, {"type": "image", "img_path": "7sdkLVuYCU/tmp/1777598ac489036de502fd4b27c943fd354566d57d30d809b2aa2e45a66b43f3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: A bitshift trellis code with $L=2,k=1,V=1$ . Nodes 0, 1, 2, and 3 have code values 0.5, 0.1, 0.8, and 0.3, respectively. Each node can only transition to the $2^{k V}=2$ nodes that share their top $L-k V=1$ bit with its bottom $L-k V=1$ bit. In this example, $\\hat{S}$ can be stored as 0010110. $\\hat{S}$ is also tail-biting, so the last $L-k V=1$ bits can be dropped to give $\\hat{S}=001011$ . ", "page_idx": 4}, {"type": "text", "text": "Table 1: QTIP\u2019s compute-based codes (1MAD, 3INST, HYB) achieve similar distortion rates as a pure-lookup random Gaussian trellis code (RPTC) when quantizing an i.i.d Gaussian source to 2 bits. All TCQ methods $\\mathcal{L}=16$ ) outperform SQ and VQ and are significantly closer to the infinite-length distortion rate $D_{R}$ , which lower bounds the distortion a $k$ -bit quantizer can attain. ", "page_idx": 4}, {"type": "table", "img_path": "7sdkLVuYCU/tmp/cebeeb7406c12cf9de70f7f4c171aa3955df6ad3e2a2ccefe3def738196a2b75.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "the codebook to decorrelate neighboring weight groups (Figure 3 RR) [23]. However, this requires storing the codebook or storing and applying the permutation, both of which are prohibitively expensive during decoding. Instead, QTIP introduces a series of compute-based codes to produce a psuedorandom code, which has the same decorrelating effect and admits fast inference. To match approximately i.i.d. Gaussian RHT-transformed matrices, these codes produce psuedorandom approximate Gaussians in as few as 2 instructions per weight (see Table 1 and Figure 3). To the best of our knowledge, these code constructions alone are novel and we are the first to propose a lookupfree Gaussian trellis code. ", "page_idx": 4}, {"type": "text", "text": "3.1.1 Lookup-Free Computed Codes ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Here, we present two pure-computed lookup-free codes that produce a pseudorandom approximately Gaussian number from a $L$ bit word, enabling fast decoding on cache-limited hardware. These codes avoid strong correlations and can be implemented in $\\leq4$ hardware instructions per weight on NVIDIA GPUs. We present two codes here to illustrate that multiple such codes are possible: in practice a lookup-free code can be designed to use the instructions available on whatever hardware we want to run on. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 (1MAD) first runs a linear congruential generator (LCG) to produce a pseudorandom 32-bit word [32]. This requires 2 instructions (MAD and &). It then sums the 32-bit word as four 8- bit unsigned integers; this sum is approximately Gaussian distributed. This requires 1 instruction (vabsdiff4). Finally, this sum must be scaled and shifted (another MAD). Although there are only $2^{10}$ representable values even when $L>10$ , this does not empirically affect quantization quality. 1MAD requires choosing $a$ and $b$ to avoid strong correlations; we set $a\\,=\\,34038481$ and $b=76625530$ (Figure 3 LC). ", "page_idx": 4}, {"type": "text", "text": "Algorithm 2 (3INST) also first runs an LCG to produce a random 32-bit word $X$ . Then, it XORs the bottom 16 bits of $X$ with the mantissa bits, bottom two exponent bits, and sign bit of a magic FP16 number $m$ to produce an FP16 number $m_{1}$ . It then repeats this with the top 16 bits of $X$ to produce $m_{2}$ and returns $m_{1}+m_{2}$ . This entire process can be implemented in 3 ALU instructions1 with a MAD for the LCG, a lop3 to mask and XOR with a packed duplicated $m$ , and then summing $m_{1}$ and $m_{2}$ . ", "page_idx": 4}, {"type": "image", "img_path": "7sdkLVuYCU/tmp/8b21a0fda8f391f5118b90e60db34bdd4bece986392fa0f3e718d9d884f52a47.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: Set of representable neighboring values in a bitshift trellis with $L=16,k=2,V=1$ for (far left) a code with strong correlations, (left center) algorithm 1 (\u201c1MAD\u201d), (right center) algorithm 2 (\u201c3INST\u201d), and (far right) a random Gaussian code. Note that while 1MAD has minor correlations, both 1MAD and 3INST are close to a random Gaussian, resulting in good quantization quality. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Computed Gaussian Code \u201c1MAD\u201d ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "nput $L$ -bit 0 left-padded integer $x$ , ${\\tt u i n t32}\\;a,b$ . $x\\leftarrow(a x+b)$ mod $2^{32}$ {run LCG to get uniform random $x$ }   \n{sum $x$ as four 8-bit unsigned integers, this is approximately Gaussian} $\\bar{x}\\leftarrow(x\\And255)+((x\\Bar{>}5\\end{array}8)\\ \\bar{\\ x}\\ ^{2}55)+((x\\Bar{>}5\\ 16)\\ \\bar{\\ x}\\ 255)+((x\\Bar{>}5\\ 24)\\ \\bar{\\ x}\\ 255)$ $x\\leftarrow(x-510)/147.8$ ", "page_idx": 5}, {"type": "text", "text": "output Pseudorandom approximate Gaussian $x$ . ", "page_idx": 5}, {"type": "text", "text": "$m_{1}+m_{2}$ is approximately distributed by the sum of two mirrored exponential distributions, which is close to Gaussian. Like with Algorithm 1, $a,b$ , and $m$ must be chosen to avoid correlations; we used $a=89226354$ , $b=64248484$ , $m=0.922$ (Figure 3 right). ", "page_idx": 5}, {"type": "text", "text": "3.1.2 Hybrid Lookup-Computed Codes ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Here, we describe a hybrid computed-lookup code that computes a pseudorandom (or hashed) index into a 2D vector codebook $\\mathbf{\\nabla}V=2$ ). This code is tailored for modern GPUs, which have enough cache for a small in-memory LUT\u2014one benefit of using such a LUT over a purely computed codebook is that a LUT can be fine-tuned after quantization. Algorithm 3 first performs the hash $X\\leftarrow X^{2}+X$ to mix the lower order and upper order bits of $X$ [18]. Then, it takes bits $(14-Q+1)-14$ (0 indexed) as an index into a $2^{Q}\\times\\dot{2}$ LUT to get two 16-bit floats. (The reason why we chose a 2D codebook here is that shared memory on NVIDIA GPUs is accessed in 32-bit-word elements, and each such word can contain two 16-bit floats.) Finally, it XORs bit 15 of $X$ to flip the sign of the second entry of the codebook vector. Algorithm 3 can be implemented with MAD, bitshift, mask, and lop3, giving an amortized 2 instructions per weight. This effectively assigns a $L$ bit word to one of $2^{Q\\bar{+}1}$ 2D vectors, each of which can be fine-tuned to improve quality. Algorithm 3 can also be implemented to XOR bit 31 alongside bit 15 (this is free in the lop3) to give an effectively $2^{Q+2}$ -sized codebook, which can improve quantization quality. We only realized this after running all the experiments, so the numbers in this paper use the \u201cone sign flip\u201d version of Algorithm 3. In QTIP, we initialize the LUT using K-means on an empirical 2D i.i.d. Gaussian distribution. ", "page_idx": 5}, {"type": "text", "text": "3.2 Tail-Biting Trellises ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Directly quantizing a length- $\\cdot T$ sequence to a $(L,k,V)$ trellis results in a total of $k T+L-k V$ bits since the starting state takes an additional $L-k V$ bits to store. If we run inference on a machine with $w$ -bit words where $w|k T$ , we must read an extra $\\begin{array}{r}{\\lceil\\frac{L-k V}{w}\\rceil w-(L-k V)}\\end{array}$ wasted bits per sequence. For common $w$ (e.g. 32), setting $L=k V+w$ makes the Viterbi algorithm intractable. One way to solve this is by enforcing that the start and end state share $L-k V$ bits, i.e. the trellis is tail-biting [4]. Exactly solving the tail-biting trellis problem via dynamic programming takes time quadratic in the state space $(2^{L}\\bar{)}$ , making this problem intractable for reasonable $L\\protect\\geq12$ [30]. However, since RHT-processed weights are approximately i.i.d., simple algorithms can be effective for approximately solving the tail-biting problem. We propose Algorithm 4, which first rotates the sequence by $T/\\dot{2}$ , quantizes it, and then extracts the overlap between the rotated start and end states. It then requantizes the original sequence with this overlap as the tail-biting overlap. This only requires two Viterbi calls in total. Table 2 shows that in practice, Algorithm 4 can find close-to-optimal tail-biting sequences while being significantly cheaper to run than other tail-biting approximation algorithms [30]. ", "page_idx": 5}, {"type": "table", "img_path": "7sdkLVuYCU/tmp/05f7e645e2ed76a8a6b3b34fa5d8f6535c4fa89c6d05fcb0196341bd3128fb93.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "7sdkLVuYCU/tmp/e19923cfeca2789700b1c4f44933c5376dd75b05966d6ee6f7d46d9e38dfe567.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Here, we present experiments quantizing the Llama family of models with QTIP [33, 34, 26]. These models offer strong performance across a wide range of sizes, allowing us to compare how different quantization methods perform and scale. We primarily compare QTIP against $\\mathrm{QulP\\#}$ and AQLM. For Llama 1, we include GPTVQ-2D instead of AQLM since AQLM does not publish Llama 1 numbers [36]. GPTVQ-2D performs 2D VQ inside GPTQ and offers strong performance. These methods outperform scalar quantization methods including GPTQ, AWQ, and OmniQuant; comparisons to those methods can be found in $\\mathrm{QulP\\#}$ and AQLM [20, 14, 29, 35, 11]. We mainly focus on the hybrid code (Section 4.2) since it is tailored for modern GPUs, and present a full suite of results for it. For the computed codes (Section 4.1), we present results for Llama 2. ", "page_idx": 6}, {"type": "text", "text": "Since the proxy error is not an additive distortion metric, we cannot minimize it by quantizing $W$ as one sequence. Instead, for all experiments, we use QTIP as a quantizer in $\\mathrm{QulP\\#}$ \u2019s BlockLDLQ, which allows us to simultaneously achieve high dimensionality and low proxy error [35]. Specifically, we quantize a block of $T_{x}\\times T_{y}$ weights as a sequence, where $T_{x}$ and $T_{y}$ span the output and input dimensions of $W$ , respectively. Since BlockLDLQ only specifies feedback along the input dimension, this is equivalent to BlockLDLQ with $g=T_{y}$ but a vector dimension of $T_{x}T_{y}\\gg T_{y}$ . This has the benefti of limiting the effect of $g$ in BlockLDLQ\u2019s error bound $g m\\mu^{2}\\sigma^{2}\\mathrm{tr}(H^{1/2})^{2}/n$ while achieving a high dimension for TCQ. Algorithm 5 in the Appendix describes this in more detail. ", "page_idx": 6}, {"type": "text", "text": "4.1 Lookup-Free Computed Codes ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Here, we use 1MAD and 3INST with $L=16,V=1,T_{x}=T_{y}=16.$ . Setting $T_{x}=T_{y}=16$ enables using a $16\\times16$ MMA tile per trellis sequence to perform matrix multiplication during inference. $16\\times16$ MMA tiles form the basis of many types of \u201cAI hardware,\u201d making fast decoding relatively simple [6]. We do not perform fine-tuning since the codes themselves are not tunable, but these codes are fully compatible with $\\mathrm{QulP\\#}$ -style fine-tuning (recall that $\\mathrm{QulP\\#}$ \u2019s codebook is also not tunable). Table 3 shows that both 1MAD and 3INST significantly outperform $\\mathrm{QulP\\#}$ without fine-tuning (AQLM does not have numbers without fine-tuning). Even at 4 bits, where all methods are close to lossless, QTIP results in significant improvements. Notably, the computed-code QTIP variants without fine-tuning outperforms both $\\mathrm{QulP\\#}$ and AQLM with fine-tuning on almost all models and sizes, showing that fine-tuning is not a silver bullet. ", "page_idx": 6}, {"type": "text", "text": "4.2 Hybrid Lookup-Computed Codes ", "text_level": 1, "page_idx": 6}, {"type": "table", "img_path": "7sdkLVuYCU/tmp/d3368d52f8f511a503e47aa564e08af9be476179fd327780a62b460e6d745bb2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "7sdkLVuYCU/tmp/3979f0d9f9ad638ccf1838c7c728196b5c6e48ac1eba6a6d8f314671a4660470.jpg", "table_caption": ["Table 2: Quantizing 4K $T=256$ i.i.d Gaussian seqs. with a tail-biting $(12,k,1)$ trellis. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 3: Wikitext2 and C4 perplexity $\\left(\\downarrow\\right)$ , ctx. 4096, QTIP with pure-computed codes. Even without fine-tuning, pure-computed QTIP outperforms $\\mathrm{QulP\\#}$ and AQLM, both of which use fine-tuning, at almost all models sizes. ", "page_idx": 7}, {"type": "table", "img_path": "7sdkLVuYCU/tmp/d1dd633f180421b4082c456587ebb411ca4df1b834c244e415280742c5aa96ae.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Here, we use the hybrid lookup-computed code with $L=16$ , $V=2$ , $T_{x}=T_{y}=16$ , $Q=9$ . Setting $Q=9$ gives a 2KiB codebook, which fits in L1 cache even after duplication for bank conflicts $(32\\times)$ on modern GPUs. This codebook is differentiable, so we can finetune it: to evaluate this, we fine-tune using $\\mathrm{QulP\\#}$ \u2019s methodology, tuning both the codebook entries and the as-yet-unquantized weights in a blockwise fashion. Table 5 shows the perplexity of quantized Llama 1 and 2 models. In all cases, QTIP outperforms the other vector quantization-based methods. Even at 3 and 4 bits, where $\\mathrm{QulP\\#}$ and AQLM are close to lossless, ", "page_idx": 7}, {"type": "table", "img_path": "7sdkLVuYCU/tmp/1a9e485f65701ca8a8f5fb2e2eddcda1c30f64726883eafe119fa11e3eeff4d4.jpg", "table_caption": ["Table 4: Batch size 1 decoding throughput on a RTX6000 Ada (960GB/s mem. BW). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "QTIP roughly halves the perplexity gap. These results also show the importance of dimensionality. Note that the 3- and 4-bit Llama 2 70B numbers here match those in 3. Since Table 3 uses a purecomputed code without fine-tuning, fine-tuning has no effect in these regimes and the improvement over $\\mathrm{QulP\\#}$ is purely from dimensionality. ", "page_idx": 7}, {"type": "text", "text": "Table 6 shows zeroshot results computed with LM Eval, which are slightly random; QTIP generally matches or exceeds $\\mathrm{QulP\\#}$ and AQLM on these tasks [15]. Table 7 contains results on Llama 3. Like other works, we have observed that Llama 3 (especially 70B base) is harder to quantize than Llama 2 [16]. Since the contribution and focus of this work is what to round with (TCQ) and not how to round (BlockLDLQ), we only compare against the proximal baseline $\\mathrm{QulP\\#}$ , which uses BlockLDLQ with VQ. QTIP significantly improves upon $\\mathrm{QulP\\#}$ at all model sizes and bitrates, once again showing the dimensionality advantage of TCQ over VQ. Table 8 shows results for Llama 3.1 instruct-tuned models, including Llama 3.1 405B. At all sizes, QTIP achieves strong results. Notably, QTIP is able to match or exceed PV-Tuning, a recent quantization method that focuses on better finetuning algorithms [22]. However, PV-Tuning is based off of AQLM and inherits its slow inference speed, making it significantly slower than QTIP. Finally, Table 9 shows results for quantizing Llama 3.2 instruct-tuned models to 4 bits. Since the embedding layers are very large relative to the decoder layers for small Llama 3 models $(\\approx500-750\\mathrm{MB})$ , quantizing the decoder layers to fewer than 4 bits does not make a significant difference on the final model size. Here, QTIP is still able to achieve a meaningful end-to-end compression rate (2.5-3X) without degrading the final model. ", "page_idx": 7}, {"type": "text", "text": "Table 5: Wikitext2 and C4 perplexity $\\left(\\downarrow\\right)$ , QTIP with the hybrid-computed code. QTIP enables highdimensional quantization and outperforms state-of-the-art vector quantization approaches. ", "page_idx": 8}, {"type": "table", "img_path": "7sdkLVuYCU/tmp/9ad50a1f0c279d12112559abac0c1fe80a8e02ec6fa7d258b934cac77569708c.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "7sdkLVuYCU/tmp/739602fad6662c19217f7335824faa22f7f93bf16283bea9957544afce988271.jpg", "table_caption": ["Table 6: Zeroshot accuracy (\u2191), QTIP with the hybrid-computed code. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3 Inference Speed ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Table 4 shows the batch size 1 inference speed of QTIP, $\\mathrm{QulP\\#}$ , and AQLM on Llama 2 7B and 70B with matrix fusion. Here, the design choices of QTIP and $\\mathrm{QulP\\#}$ become apparent. Whereas AQLM uses a codebook that is too large to fit in cache and thus prevents fast inference, both QTIP and $\\mathrm{QulP\\#}$ achieve significant speedups over FP16. Furthermore, while it is impressive that both $\\mathrm{QulP\\#}$ and QTIP are $>2\\times$ faster than AQLM, it is even more impressive that QTIP is able to match QuIP#\u2019s throughput with an effective dimension size of 256, or $32\\times$ larger than $\\mathrm{QulP\\#}^{*}$ s. This means that the improved quantization quality of QTIP comes with no additional inference-time cost. Although our empirical throughput numbers were timed on NVIDIA GPUs, QTIP can be fast on a broad class of accelerators due to its flexibility. QTIP only requires generating a pseudorandom Gaussian efficiently, and can work on devices with no cache as well as devices with lookup hardware. For example, if we were using a ARMv8 CPU, we could use the vqtbl4q_u8 NEON intrinsic to look up 16 indices in a 64-entry codebook. This would let us use a 6 bit 1D codebook with the HYB code $(\\mathrm{Q}{=}6,\\mathrm{V}{=}1)$ ). Quantizing Llama 2 7B to 2 bits with this setup and w/out fine-tuning gives 6.89 Wikitext2 perplexity \u2013 essentially the same state-of-the-art quality as 3INST. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We present QTIP, a weight-only post-training quantization algorithm that achieves state-of-theart results through the use of trellis-coded quantization (TCQ). TCQ enables tractable ultra-high dimensional quantization, significantly reducing quantization distortion over vector quantization (VQ). However, naive TCQ does not admit fast inference due to sequential bottlenecks during decoding and needing to store a large codebook. QTIP solves this problem through a novel combination of incoherence processing, the hardware-efficient bitshift trellis, and fast computed codes. Specifically, QTIP introduces a series of compute-based pseudorandom Gaussian codes that, when used in ", "page_idx": 8}, {"type": "text", "text": "Table 7: QTIP vs. $\\mathrm{QulP\\#}$ , Llama 3 (ctx. 8192 for perplexity). Although Hessian-based rounding generally underperforms on Llama 3, the focus of this work is on what to quantize with (TCQ vs. VQ). Here, the high-dimensionality of TCQ in QTIP improves over the low-dimensional VQ in $\\mathrm{QulP\\#}$ . ", "page_idx": 9}, {"type": "table", "img_path": "7sdkLVuYCU/tmp/b984b85c71a9031803312e1b9795d3da6e5ef53da6620e0b1a0fc74257789647.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 8: Llama 3.1 instruct-tuned model results (ctx. 8192 for perplexity). QTIP performs well at all model sizes and generally outperforms PV-Tuning, a recent quantization method that focuses on finetuning algorithms. The zeroshot results in this table use LM Eval 0.4.4 and the \u201cstandard\u201d versions of each task instead of the Meta versions in [26]. ", "page_idx": 9}, {"type": "table", "img_path": "7sdkLVuYCU/tmp/a2bd50846b6c04a8eb61cd891f66f33fe74eff9a413b1f4e3d5ca41d40b7f8b4.jpg", "table_caption": ["PPL. (\u2193) ZEROSHOT (\u2191) "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 9: Llama 3.2 instruct-tuned results when quantizing to 4 bits (ctx. 8192 for perplexity). Even on extremely small models, QTIP is still able to achieve meaningful compression without sacrificing quality. This table uses the same LM Eval setup as Table 8. ", "page_idx": 9}, {"type": "table", "img_path": "7sdkLVuYCU/tmp/30558bea128d46e22d2857db250e66b83e983bbfd28b7fa286cdf6b6657b4e0d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "conjunction with the bitshift trellis and incoherence processing, simultaneously achieves state-ofthe-art PTQ quality and fast inference. QTIP improves quantization quality at all tested bitrates over the latest VQ-based PTQ methods, $\\mathrm{QulP\\#}$ and AQLM, further pushing the boundary of LLM PTQ. QTIP\u2019s codes use as few as 2 instructions per weight during decoding, enabling matrix-vector multiplication to run at over $80\\%$ of peak memory bandwidth on modern GPUs. Altogether, our results indicate that high dimensional quantization is necessary for high-quality compression, and QTIP is the first LLM PTQ method to scale to ultra-high dimensions while supporting fast inference. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "C.D. was supported by NSF-2046760 CAREER. We thank Together AI for compute resources. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] An algorithm for vector quantizer design. IEEE Transactions on communications, 28(1):84\u201395, 1980.   \n[2] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, M\u00e9rouane Debbah, \u00c9tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. The falcon series of open language models, 2023.   \n[3] Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms, 2024.   \n[4] A.R. Calderbank, G.D. Forney, and A. Vardy. Minimal tail-biting trellises: the golay code and more. IEEE Transactions on Information Theory, 45(5):1435\u20131455, 1999. doi: 10.1109/18. 771145.   \n[5] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. QuIP: 2-bit quantization of large language models with guarantees. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id $\\cdot$ xrk9g5vcXR.   \n[6] Jack Choquette, Wishwesh Gandhi, Olivier Giroux, Nick Stam, and Ronny Krashinsky. Nvidia a100 tensor core gpu: Performance and innovation. IEEE Micro, 41(2):29\u201335, 2021. doi: 10.1109/MM.2021.3061394.   \n[7] Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023. URL https://github.com/togethercomputer/RedPajama-Data.   \n[8] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing). Wiley-Interscience, USA, 2006. ISBN 0471241954.   \n[9] Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 7750\u20137774. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/dettmers23a. html.   \n[10] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix multiplication for transformers at scale, 2022.   \n[11] Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, and Dan Alistarh. Extreme compression of large language models via additive quantization, 2024.   \n[12] T.R. Fischer, M.W. Marcellin, and M. Wang. Trellis-coded vector quantization. IEEE Transactions on Information Theory, 37(6):1551\u20131566, 1991. doi: 10.1109/18.104316.   \n[13] G.D. Forney. The viterbi algorithm. Proceedings of the IEEE, 61(3):268\u2013278, 1973. doi: 10.1109/PROC.1973.9030.   \n[14] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: Accurate quantization for generative pre-trained transformers. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=tcbBPnfwxS.   \n[15] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPof,i Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac\u2019h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/ 10256836.   \n[16] Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao Lv, Hong Chen, Jie Luo, Xiaojuan Qi, Xianglong Liu, and Michele Magno. How good are low-bit quantized llama3 models? an empirical study, 2024.   \n[17] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mixtral of experts, 2024.   \n[18] Alexander Klimov and Adi Shamir. A new class of invertible mappings. In Cryptographic Hardware and Embedded Systems-CHES 2002: 4th International Workshop Redwood Shores, CA, USA, August 13\u201315, 2002 Revised Papers 4, pages 470\u2013483. Springer, 2003.   \n[19] Victoria Kostina and Sergio Verdu. Fixed-length lossy compression in the finite blocklength regime. IEEE Transactions on Information Theory, 58(6):3309\u20133338, 2012. doi: 10.1109/TIT. 2012.2186786.   \n[20] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration, 2023.   \n[21] Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28 (2):129\u2013137, 1982.   \n[22] Vladimir Malinovskii, Denis Mazur, Ivan Ilin, Denis Kuznedelev, Konstantin Burlachenko, Kai Yi, Dan Alistarh, and Peter Richtarik. Pv-tuning: Beyond straight-through estimation for extreme llm compression, 2024. URL https://arxiv.org/abs/2405.14852.   \n[23] Mark Z. Mao and Robert M. Gray. Stationary and trellis encoding for iid sources and simulation. In 2010 Data Compression Conference, pages 3\u201312, 2010. doi: 10.1109/DCC.2010.8.   \n[24] M.W. Marcellin and T.R. Fischer. Trellis coded quantization of memoryless and gauss-markov sources. IEEE Transactions on Communications, 38(1):82\u201393, 1990. doi: 10.1109/26.46532.   \n[25] J. Max. Quantizing for minimum distortion. IRE Transactions on Information Theory, 6(1): 7\u201312, 1960. doi: 10.1109/TIT.1960.1057548.   \n[26] meta llama. llama3. https://github.com/meta-llama/llama3, 2024.   \n[27] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? Adaptive rounding for post-training quantization. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 7197\u20137206. PMLR, 13\u201318 Jul 2020. URL https://proceedings.mlr.press/v119/nagel20a.html.   \n[28] Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, Stefano Ermon, Stephen A. Baccus, and Chris R\u00e9. Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution. 2023.   \n[29] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantization for large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=8Wuvhh0LYW.   \n[30] Y. Shao.Rose, Shu Lin, and Marc P. C. Fossorier. Tail biting trellis representation of codes: Decoding and construction. 1999. URL https://api.semanticscholar.org/CorpusID: 3727533.   \n[31] Neil Sloane. Hadamard Matrices \u2014 neilsloane.com. http://neilsloane.com/hadamard/. [Accessed 02-02-2024]. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[32] W. E. Thomson. A Modified Congruence Method of Generating Pseudo-random Numbers. The Computer Journal, 1(2):83\u201383, 01 1958. ISSN 0010-4620. doi: 10.1093/comjnl/1.2.83. URL https://doi.org/10.1093/comjnl/1.2.83. ", "page_idx": 12}, {"type": "text", "text": "[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. ", "page_idx": 12}, {"type": "text", "text": "[34] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. ", "page_idx": 12}, {"type": "text", "text": "[35] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. Quip#: Even better llm quantization with hadamard incoherence and lattice codebooks, 2024. ", "page_idx": 12}, {"type": "text", "text": "[36] Mart van Baalen, Andrey Kuzmin, Markus Nagel, Peter Couperus, Cedric Bastoul, Eric Mahurin, Tijmen Blankevoort, and Paul Whatmough. Gptvq: The blessing of dimensionality in llm quantization. arXiv preprint arXiv:2402.15319, 2024. ", "page_idx": 12}, {"type": "text", "text": "[37] H.S. Wang and N. Moayeri. Trellis coded vector quantization. IEEE Transactions on Communications, 40(8):1273\u20131276, 1992. doi: 10.1109/26.156629. ", "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Additional Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1.1 Ablations on Trellis Size ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table 10 shows an ablation on $L$ for quantizing Llama 2 7B with $K=2,V=1$ , the bitshift trellis, a pure-lookup codebook, and no fine-tuning. $L=8$ is the largest $L$ achievable if we had to store the trellis and codebook in the same amount of cache as the HYB code (2KiB). $L=10$ is the largest $L$ achievable if we only had to store the codebook. As expected, increasing $L$ improves quality. Table 10 also shows very little difference between an equal-sized LUT codebook and QTIP\u2019s codes, meaning that QTIP isn\u2019t sacrificing quality for speed. However, an equal-sized LUT would need $>10\\times$ more cache than the latest GPUs have, making the bitshift trellis and compute-based codes necessary to achieve both quality and speed. Table 11 shows an ablation on $V$ with $L=12$ and 16, $K=2$ , and the same settings as Table 10. Increasing $V$ generally decreases quality, but this can be recovered with a larger $L$ . It is hard to measure $V$ \u2019s impact on decoding speed since this is highly implementation and hardware dependent, so $V$ is more of a user-chosen hyperparameter. ", "page_idx": 13}, {"type": "text", "text": "Table 10: Ablation on $L$ when quantizing Llama 2 7B to 2 bits ( $K=2$ and $V=1$ ). ", "page_idx": 13}, {"type": "table", "img_path": "7sdkLVuYCU/tmp/45ce72ed8955930e63e75ed06f9384ba5abf8638cc3edb0834f4064b4c005bd6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "table", "img_path": "7sdkLVuYCU/tmp/2bfc17ca4b7ba83cee9f9acfbf8dd7f5c68065a63ee0fba121fdb7a2c18b6bcb.jpg", "table_caption": ["Table 11: Ablation on $V$ when quantizing Llama 2 7B to 2 bits $(K=2)$ ). "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.1.2 Zeroshot Results ", "text_level": 1, "page_idx": 13}, {"type": "table", "img_path": "7sdkLVuYCU/tmp/9d9e01dbb7dc3b8abc1c8a0edb8dacdf16a4a995edabeb4f7c13394f7ad01ad3.jpg", "table_caption": ["Table 12: Zeroshot results for the 1MAD code. "], "table_footnote": [], "page_idx": 13}, {"type": "table", "img_path": "7sdkLVuYCU/tmp/d981216a10579a8ba30163ae3ea0748858e27e98ecc9a8f7beff3645422ba4fb.jpg", "table_caption": ["Table 13: Zeroshot results for the 3INST code. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "7sdkLVuYCU/tmp/0b9a8886df4d413ef733b35d3d82ee754440c85f326fd175594a0f477cd4c57e.jpg", "table_caption": ["Table 14: Llama 1 Zeroshot results for the Hybrid code "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.1.3 Lookup-Only Codes", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table 15: Wikitext2 and C4 perplexity (\u2193), ctx. 4096, QTIP with a size $2^{14}$ LUT codebook. This codebook is too large (32KB) for current GPU L1 caches, but could fit on near-future hardware. ", "page_idx": 14}, {"type": "table", "img_path": "7sdkLVuYCU/tmp/33e71add8963e45a2eba584ba50a4f3a67a3c9186104f2c22c8c3d28ded6dac2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Here, we use a pure-lookup code $\\sim\\mathcal{N}(0,1)$ with $L=14,V=1,T_{x}=32$ , $T_{y}=8$ , and $\\mathrm{QulP\\#}$ \u2019s fine-tuning scheme. These parameters show what performance QTIP could achieve if we did not care about fast inference today. Specifically, a pure-lookup codebook is tunable, and setting $T_{y}=8$ reduces the BlockLDLQ group size while maintaining high dimensionality (256). This codebook uses 32KB; this only ftis in GPU L1 cache with bank confilcts. Setting $T_{x}=32,T_{y}=8$ corresponds to using a larger MMA tile size than current GPUs allow for. The largest tile size is usually 16 in the $T_{x}$ dimension, meaning that a $32\\times8$ trellis needs two tiles. Thankfully, hardware required to serve ", "page_idx": 14}, {"type": "text", "text": "Table 16: Wikitext2 and C4 zeroshot accuracy (\u2191), QTIP with a size $2^{14}$ LUT codebook. This codebook is too large (32KB) for current GPU L1 caches, but could fit on near-future hardware. ", "page_idx": 15}, {"type": "table", "img_path": "7sdkLVuYCU/tmp/c6eaed830c77b3449e63565ee5473bf70d98db806eaebac39f32c414fd00acd2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "such a model quickly is likely only a few years away, as these parameters are only slightly outside of what today\u2019s hardware is capable of. ", "page_idx": 15}, {"type": "text", "text": "Table 15 shows that QTIP outperforms both $\\mathrm{QulP\\#}$ and AQLM at all compression ratios, with 3 bit QTIP achieving similar quality as 4 bit AQLM. While it is not fair to compare this QTIP setup with $\\mathrm{QulP\\#}$ , since $\\mathrm{QulP\\#}$ was designed for fast inference, we note that AQLM\u2019s VQ codebook uses $2^{16}\\times8\\times2=1$ MiB. This is 32 times larger than the QTIP codebook here, and would require 32 MiB of L1 cache to read from without bank conflicts. Not only is this orders of magnitude larger than current L1 caches (256KB on the H100), it is even larger than many L2 caches! ", "page_idx": 15}, {"type": "text", "text": "A.1.4 Decoding Speed on Different GPUs ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "7sdkLVuYCU/tmp/2bc539ba3ba76d163a1152d74d98352d0adbfc2bde0aed60e9cd26aed082b932.jpg", "table_caption": ["Table 17: Decoding speed on different Ampere and Lovelace GPUs. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.2 QTIP with BlockLDLQ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here, we detail how we use TCQ within BlockLDLQ to produce our experimental setup. Essentially, QTIP is used as a high dimensional $T_{x}T_{y}$ quantizer within BlockLDLQ and is a drop-in replacement for vector quantization in BlockLDLQ. The regular blockLDLQ step $Q(W+(W-{\\hat{W}})A)$ is exactly the same, and the only difference is in how $Q$ rounds. Instead of rounding each row of $x=\\dot{W}\\!+\\!(W\\!-$ $\\hat{W})A$ independently, it groups $T_{x}$ rows into a block to round as $m/T_{x}$ high-dimensional sequences. ", "page_idx": 15}, {"type": "text", "text": "A.3 Implementation Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.3.1 Code ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Our code is available at https://github.com/Cornell-RelaxML/qtip. ", "page_idx": 15}, {"type": "text", "text": "A.3.2 Hessian Generation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Hessian matrices were generated with 6144 sequences of length 2048 for Llama 1, 6144 sequences of length 2048 for Llama 2, 4096 sequences of 8192 for Llama 3, and 4096 sequences of 8192 for Llama 3.1 except for 405B, which only used 2048 sequences due to time constraints. All sequences were sampled from the RedPajama dataset [7]. ", "page_idx": 15}, {"type": "text", "text": "input $W\\in\\mathbb{R}^{m\\times n},H\\in\\mathbb{R}^{n\\times n},T_{x},T_{y},L,k,V,\\mathrm{code}\\,C.$ W\u02c6 \u21900m,n $L D L^{T}\\leftarrow T_{y}$ -block LDL decomposition of $H$ $A\\leftarrow L-I$ for $j\\in\\{n/T_{y}-1,n/T_{y}-2,...,0\\}$ do $x\\leftarrow W_{:,j T_{y}:(j+1)T_{y}}+(W_{:,j T_{y}:}-\\hat{W}_{:,j T_{y}:})A_{j T_{y}:,j T_{y}:(j+1)T_{y}}$ x \u2190x.reshape $(m/T_{x},T_{x}T_{y})$ $\\hat{x}\\gets\\mathrm{viterbi}(x,(L,k,V)$ bitshift trellis, $C$ ) (row-wise) er $\\hat{W}_{:,j T_{y}:(j+1)T_{y}}\\gets\\hat{x}$ .reshape $(m,T_{y})$   \noutput Quantized $\\hat{W}$ . ", "page_idx": 16}, {"type": "text", "text": "A.3.3 Hadamard Matrices ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We use Hadamard matrices from Neil Sloane\u2019s website [31]. ", "page_idx": 16}, {"type": "text", "text": "A.3.4 Perplexity ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We use the OPTQ \u201cWikitext2\u201d and \u201cC4\u201d test splits to calculate perplexity [14]. ", "page_idx": 16}, {"type": "text", "text": "A.3.5 Fine Tuning ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We use the same fine-tuning hyperparemters as QuIP#, except that we do not perform matrix fusion [35]. This results in having to quantize 7 matrices (q, k, v, o, up, gate, down) instead of 4 (qkv, o, upgate, down). We quantize and fine tune in the following order: (v, q, k, o, up gate, down). ", "page_idx": 16}, {"type": "text", "text": "A.4 Example Generations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The following are some sample generations from Llama 2 70B (not instruction tuned) quantized to 2 bits with QTIP, truncated to 256 tokens. ", "page_idx": 16}, {"type": "text", "text": "Prompt: It is a truth universally acknowledged ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "QTIP: It is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a wife. However little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding residents, that he is considered the rightful property of some one or other of their daughters. \u201cMy dear Mr. Bennet,\u201d said his lady to him one day, \u201chave you heard that Netherfield Park is let at last?\u201d \u201cBut it is,\u201d returned she; \u201cfor Mrs. Long has just been here, and she told me that Netherfield is taken by a young man of large fortune from the north of England; that he came down on Monday in a chaise and four to see the place, and was so much delighted with it that he agreed with Mr. Morris immediately; that he is to take possession before Michaelmas, and that he is now in town in search of a wife.\u201d \u201cI see no occasion for that. You and the girls may go, or you may send them by themselves, which do you prefer? Only let me know what you are meaning to do, for I shall certainly not be at home. You ", "page_idx": 16}, {"type": "text", "text": "Prompt: Call me Ishamel ", "page_idx": 17}, {"type": "text", "text": "QTIP: Call me Ishmael. Some years ago\u2014never mind how long precisely\u2014having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world. It is a way I have of driving off the spleen, and regulating the circulation. Whenever I find myself growing grim about the mouth; whenever it is a damp, drizzly November in my soul; whenever I find myself involuntarily pausing before coffin warehouses, and bringing up the rear of every funeral I meet; and especially whenever my hypochondriacal palate is out of sorts, and I am desperately dull, and in the dumps, and have such an overpowering sense of fatality that I find myself involuntarily gravitating toward Cemeteries\u2014and especially toward those open ones which are occasionally cheered by the gleam of a bonfire, or a cigar, or a pipe, or a lantern, or a candle, or a torch, or the moon, I then take to the ship. There is nothing surprising in this, ", "page_idx": 17}, {"type": "text", "text": "The following is a sample generation from Llama 3.1 405B Instruct quantized to 2 bits with QTIP. ", "page_idx": 17}, {"type": "text", "text": "Prompt: Give a proof for why the square root of 2 is irrational: ", "page_idx": 17}, {"type": "text", "text": "QTIP: The proof that the square root of 2 is irrational is as follows: Assume, for the sake of contradiction, that $\\sqrt{2}$ is rational. This means it can be expressed as the ratio of two integers, $a$ and $b$ , where $b$ is non-zero: ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\sqrt{2}}=a/b\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We can further assume that $a$ and $b$ are not both even (i.e., they do not share a common factor o\u221af 2), because if they were, we could divide both by 2 and obtain a smaller representation of $\\sqrt{2}$ . Squaring both sides of the equation, we get: ", "page_idx": 17}, {"type": "equation", "text": "$$\n2=a^{2}/b^{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Multiplying both sides by $b^{2}$ , we get: ", "page_idx": 17}, {"type": "equation", "text": "$$\n2b^{2}=a^{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This means that $a^{2}$ is even, because it is equal to 2 times an integer $(b^{2})$ . Therefore, $a$ must also be even, because the square of an odd number is always odd. Since $a$ is even, we can write $a=2c$ , where $c$ is an integer. Substituting this into the equation above, we get: ", "page_idx": 17}, {"type": "equation", "text": "$$\n2b^{2}=(2c)^{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Expanding the right-hand side, we get: ", "page_idx": 17}, {"type": "equation", "text": "$$\n2b^{2}=4c^{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Dividing both sides by 2, we get: ", "page_idx": 17}, {"type": "equation", "text": "$$\nb^{2}=2c^{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This means that $b^{2}$ is even, and therefore $b$ is also even. However, this contradicts\u221a our assumption that $a$ and $b$ are \u221anot both even. Therefore, our original assumption that $\\sqrt{2}$ is rational must be false, and $\\sqrt{2}$ is actually irrational. This proof is often attributed to the ancient Greek mathematician Euclid\u221a, and it is still considered one of the most elegant and simple proofs of the irrationality of $\\sqrt{2}$ . ", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction reflect the paper\u2019s contributions and scope. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: The main text describes limitations of the work that the authors were able perceive at submission time. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: This paper\u2019s method is well documented and can be easily reproduced. The code will be made publicly available at a later date as well. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The code will be made publicly available at a later date. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: Yes, details are listed in the appendix. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: It is standard practice in LLM quantization papers to not report error bars on metrics. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: See the appendix for details. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We are not aware of any violations of the NeurIPS Code of Ethics. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper concerns foundational research on LLM quantization. There is not a direct path to any negative applications. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All such instances were properly cited. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 22}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper does not involve human subjects. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper does not involve human subjects. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]