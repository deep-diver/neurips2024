[{"figure_path": "8jB6sGqvgQ/figures/figures_1_1.jpg", "caption": "Figure 1: We propose continuous adversarial training (AT) to address the large computational requirements of existing discrete AT approaches [6]. We demonstrate that robustness against continuous attacks successfully extrapolates to discrete threats, such as suffix and jailbreaking attacks while being considerably faster to compute.", "description": "This figure illustrates the proposed continuous adversarial training (CAT) method.  The left side shows the training loop where the input prompt is converted into embeddings.  Instead of applying discrete adversarial attacks to the tokens directly, CAT applies continuous attacks to the embeddings. The right side shows that the model's robustness achieved against continuous attacks in the embedding space extrapolates to robustness against discrete attacks, such as suffix attacks using Greedy Coordinate Gradient (GCG) and jailbreak attacks using AutoDAN and PAIR. This is achieved with significantly faster computation.", "section": "1 Introduction"}, {"figure_path": "8jB6sGqvgQ/figures/figures_6_1.jpg", "caption": "Figure 2: Trade-off between utility and robustness for CAT (Eq. 4), CAPO (Eq. 5), and R2D2 [6], compared to their non-adversarially fine-tuned models. The objective is a small loss in utility and a large improvement in attack robustness. Larger is better for MMLU, ARC-E, ARC-C, MT-BENCH (left of dashed line). Smaller is better for GCG, AUTODAN, and PAIR (right of dashed line). MT-BENCH score is multiplied by 10 to see the change in performance on this y-axis. Additional results are included in App. B.", "description": "This figure shows the trade-off between utility and robustness for three different adversarial training methods (CAT, CAPO, and R2D2) across five different LLMs.  It compares the performance of these methods against several benchmarks measuring both utility (MMLU, ARC-E, ARC-C, MT-BENCH) and robustness (GCG, AUTODAN, PAIR) against various attacks.  The results illustrate that the proposed methods (CAT and CAPO) achieve significantly better robustness with a minor decrease in utility compared to the baseline and R2D2.", "section": "Results"}, {"figure_path": "8jB6sGqvgQ/figures/figures_8_1.jpg", "caption": "Figure 2: Trade-off between utility and robustness for CAT (Eq. 4), CAPO (Eq. 5), and R2D2 [6], compared to their non-adversarially fine-tuned models. The objective is a small loss in utility and a large improvement in attack robustness. Larger is better for MMLU, ARC-E, ARC-C, MT-BENCH (left of dashed line). Smaller is better for GCG, AUTODAN, and PAIR (right of dashed line). MT-BENCH score is multiplied by 10 to see the change in performance on this y-axis. Additional results are included in App. B.", "description": "This figure shows the results of experiments comparing three adversarial training methods (CAT, CAPO, and R2D2) across five different language models.  The goal is to evaluate the trade-off between model utility (measured by performance on MMLU, ARC-E, ARC-C, and MT-BENCH) and robustness against adversarial attacks (GCG, AutoDAN, and PAIR).  The figure demonstrates that CAT and CAPO achieve significantly higher robustness than R2D2 with only a small decrease in utility, suggesting that these methods are effective for improving the robustness of LLMs against attacks.", "section": "Results"}, {"figure_path": "8jB6sGqvgQ/figures/figures_17_1.jpg", "caption": "Figure 5: (a-b) Cross entropy loss of an embedding attack performed in an e-ball around the instruction embeddings. The same e as during training is used. For the base models, we use e = 0.05. (c) For unconstrained attacks, the loss converges to 0 for all models, showing that gradient obfuscation is not an issue during attack optimization. The black dashed line indicates the threshold, where an affirmative response is achieved for all toxic queries.", "description": "This figure shows the results of an embedding attack performed on two different models, PHI-3-MINI and GEMMA.  The attacks are performed within an e-ball around the instruction embeddings, using the same epsilon value as during training.  Subfigure (a) and (b) show the cross-entropy loss for each attack iteration, demonstrating that adversarial training improves the models' robustness against these attacks. Subfigure (c) shows the results of an unconstrained attack, illustrating that even without constraints, gradient obfuscation is not a significant factor and the models still ultimately fail when the attacks are unconstrained.", "section": "Experimental Details"}]