[{"figure_path": "Li2rpRZWjy/tables/tables_1_1.jpg", "caption": "Table 1: Formal languages used in our paper: The languages are categorized according to the Chomsky hierarchy, and they can be considered as the intersection of two rules: (R1) and (R2)", "description": "This table lists six formal languages used in the paper, categorized according to the Chomsky hierarchy (regular, context-free, context-sensitive). Each language is defined by the intersection of two rules (R1 and R2).  The table is crucial for understanding the experimental setup, as these languages are used to evaluate the models' ability to extrapolate rules in out-of-distribution scenarios.", "section": "3 Experimental setup"}, {"figure_path": "Li2rpRZWjy/tables/tables_5_1.jpg", "caption": "Table 2: Test loss and rule-following accuracies for the regular language L\u2081 = {ba}: the LSTM can extrapolate (R1) the best. The column R2 is left out as it is satisfied by design.", "description": "This table presents the results of evaluating different language models on a regular language (L1 = {ba}).  The models were assessed based on their test loss, their ability to follow rule 1 (R1) in the in-distribution (ID) and out-of-distribution (OOD) settings, and their ability to follow rule 2 (R2) in the OOD setting. Note that R2 is inherently satisfied by design for the in-distribution set and thus omitted from this section of the table. The LSTM model exhibits the highest accuracy in extrapolating rule 1 to the OOD data.", "section": "4 Results"}, {"figure_path": "Li2rpRZWjy/tables/tables_5_2.jpg", "caption": "Table 3: Test loss and rule-following accuracies for the regular language L2 = {b<sup>n</sup>a<sup>2m</sup>: n,m>0}: the LSTM and the xLSTM can extrapolate (R1) the best, closely followed by Mamba", "description": "This table presents the test loss and rule-following accuracies for the regular language L2, where the models are evaluated on their ability to extrapolate rule 1 (R1). The LSTM and XLSTM models achieve the highest accuracies in extrapolating R1, followed closely by the Mamba model.  The table also includes results for rule 2 (R2) completion, which is not directly comparable as it measures performance on a task designed to always satisfy R2.", "section": "4 Results"}, {"figure_path": "Li2rpRZWjy/tables/tables_5_3.jpg", "caption": "Table 4: Test loss and rule-following accuracies for the context-free language L3 = {a<sup>n</sup>b<sup>n</sup>}: the Transformer can extrapolate (R1) the best.", "description": "This table presents the results of evaluating different language models on a context-free language (L3 = {a<sup>n</sup>b<sup>n</sup>}).  The models were tested on their ability to extrapolate rule 1 (R1) which is that the number of 'a's equals the number of 'b's, when rule 2 (R2) is violated, meaning the 'a's do not precede the 'b's.  The table shows the test loss, the accuracy of following rule 1 in the in-distribution data, the accuracy of following rule 2 in the in-distribution data, the accuracy of extrapolating rule 1 in the out-of-distribution data, and the accuracy of completing sequences while satisfying rule 2 in the out-of-distribution data. The Transformer model achieves the highest accuracy in extrapolating rule 1, indicating its superior ability to generalize this specific rule to out-of-distribution scenarios.", "section": "4 Results"}, {"figure_path": "Li2rpRZWjy/tables/tables_6_1.jpg", "caption": "Table 5: Test loss and rule-following accuracies for the context-free Dyck language L4: the Transformer can extrapolate (R1) the best.", "description": "This table presents the results of evaluating five different models (Linear, LSTM, Mamba, Transformer, and XLSTM) on a context-free Dyck language (L4).  The models were evaluated on their ability to follow two rules (R1 and R2), both in-distribution (ID) and out-of-distribution (OOD).  The \"Test loss\" column shows the model's performance during training.  The \"ID R1\" and \"ID R2\" columns indicate the accuracy of the models in adhering to rules R1 and R2, respectively, on in-distribution data.  Conversely, the \"OOD R1\" and \"OOD R2 completion\" columns show the accuracy of the models in following rules R1 and R2 on out-of-distribution data, where R2 is intentionally violated. The results reveal the Transformer model's superior performance in extrapolating rule R1.", "section": "4 Results"}, {"figure_path": "Li2rpRZWjy/tables/tables_6_2.jpg", "caption": "Table 6: Test loss and rule-following accuracies for the context-sensitive language L5 = {anbncn}: the Transformer can extrapolate (R1) the best", "description": "This table presents the results of the experiment on the context-sensitive language L5.  It shows the test loss and the accuracy of the models in following rules R1 and R2, both in-distribution (ID) and out-of-distribution (OOD). The OOD setting violates rule R2, and the accuracy is measured in how well the models complete the sequences so that rule R1 still holds. The Transformer shows the best performance in extrapolating rule R1.", "section": "4 Results"}, {"figure_path": "Li2rpRZWjy/tables/tables_6_3.jpg", "caption": "Table 7: Test loss and rule-following accuracies for the context-sensitive Dyck language L6 = {sequences of paired, but not necessarily nested parentheses and brackets}: the Transformer and the LSTM can extrapolate the best", "description": "This table presents the results of evaluating the performance of five different sequence models (Linear, LSTM, Mamba, Transformer, and XLSTM) on a context-sensitive Dyck language (L6). The models were trained on sequences of paired parentheses and brackets where nesting is allowed.  The table shows the test loss achieved by each model, along with their accuracy in following rules R1 (brackets are paired) and R2 (parentheses are paired) for both in-distribution (ID) and out-of-distribution (OOD) prompts. OOD prompts violate rule R2, but still adhere to rule R1, allowing the assessment of rule extrapolation ability. The table shows that the Transformer and LSTM models perform best in extrapolating the rules.", "section": "4 Results"}, {"figure_path": "Li2rpRZWjy/tables/tables_15_1.jpg", "caption": "Table 8: General parameters", "description": "This table presents the hyperparameters used for training the different models in the experiments.  It lists the values used for parameters such as the maximum length of training data, prompt prediction cutoff length, batch size, optimizer, learning rate scheduler, learning rate, and the number of epochs.", "section": "3 Experimental setup"}, {"figure_path": "Li2rpRZWjy/tables/tables_15_2.jpg", "caption": "Table 9: Linear model parameters", "description": "This table lists the hyperparameters used for the linear model in the experiments.  It shows that a linear model was used, the dimension of the model was 256, and a bias term was included.", "section": "3 Experimental setup"}, {"figure_path": "Li2rpRZWjy/tables/tables_16_1.jpg", "caption": "Table 10: LSTM parameters", "description": "This table lists the hyperparameters used for the LSTM model in the experiments.  It shows the model type as a standard LSTM, the number of layers (5), the embedding dimension (16), the hidden dimension (64), and the dropout probability (0.4). These settings were used to train and evaluate the LSTM's performance on rule extrapolation tasks in formal languages.", "section": "3 Experimental setup"}, {"figure_path": "Li2rpRZWjy/tables/tables_16_2.jpg", "caption": "Table 11: Transformer parameters", "description": "This table lists the hyperparameters used for the Transformer model in the experiments.  It shows the model architecture, including the number of layers, the model dimension, the number of attention heads, the feedforward dimension, dropout probability, layer normalization epsilon, and the activation function used.", "section": "3 Experimental setup"}, {"figure_path": "Li2rpRZWjy/tables/tables_16_3.jpg", "caption": "Table 12: Mamba parameters", "description": "This table lists the hyperparameters used for the Mamba model in the experiments.  It specifies the model architecture, including the number of layers, model dimension, dimension of the convolutional layer, and the dimension of the state space.", "section": "3 Experimental setup"}, {"figure_path": "Li2rpRZWjy/tables/tables_16_4.jpg", "caption": "Table 13: xLSTM parameters\u00b3", "description": "This table lists the hyperparameters used for training the XLSTM model in the rule extrapolation experiments.  It specifies the model architecture, including the number of blocks, embedding dimensions, and various parameters within the MLSTM and SLSTM components.  These parameters control aspects like kernel sizes in convolutional layers, the number of attention heads, and activation functions.", "section": "3 Experimental setup"}, {"figure_path": "Li2rpRZWjy/tables/tables_19_1.jpg", "caption": "Table 14: Human pilot study OOD accuracies: humans in our study performed better than chance, though they could not beat the LSTM on L\u2081 and the Transformer on L3", "description": "This table presents the results of a small-scale human study designed to evaluate human performance on out-of-distribution (OOD) rule extrapolation tasks, comparing human performance with the results obtained from the LSTM and the Transformer models in the main study.  The study examined two formal languages, L1 and L3, each having two rules, and human subjects were tasked with extrapolating rule 1 (R1) and rule 2 (R2) in an OOD setting (i.e., when rule 2 was intentionally violated in the prompt). The table shows that human performance exceeded chance level on both languages, although it did not surpass the performance of the LSTM model on language L1 or the Transformer model on language L3.", "section": "B.6 Human pilot study details"}]