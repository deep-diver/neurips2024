[{"figure_path": "zeYyq0GpXO/tables/tables_2_1.jpg", "caption": "Table 1: The compared model variants. Full attention is denoted as Full and window attention with a window size of W tokens is denoted as Window (W). We abbreviate TinyLLaMA as TL.", "description": "This table presents the different model variants used in the experiments.  Each model is based on the TinyLlama architecture and varies in its positional encoding (PE) method and attention mechanism.  Models are categorized by whether they employ no positional encoding (NoPE), Rotary Positional Embeddings (RoPE), or the ALiBi method.  Attention mechanisms are either full attention (where each token attends to all others) or window attention with a specified window size (e.g., 512 or 80 tokens).", "section": "3.1 Experimental Settings"}, {"figure_path": "zeYyq0GpXO/tables/tables_4_1.jpg", "caption": "Table 2: Results of removing different components in attention. Sim denotes the cosine similarity between original and new positional vectors, and PPL denotes perplexity on RedPajama.", "description": "This table presents the results of an ablation study on the effects of removing different components (value, positional vector, positional basis, and semantic vector) from the attention mechanism in two transformer models: TL-NoPE and TL-RoPE.  The study evaluates both the cosine similarity between original and modified positional vectors (Sim) and the perplexity (PPL) on the RedPajama dataset.  It shows the impact of each component on positional vector formation and overall model performance, indicating the relative importance of each component for maintaining positional information and achieving good generalization performance. Removing positional vectors and their basis from initial tokens leads to significantly lower similarity and higher PPL, highlighting their crucial roles in attention mechanisms.", "section": "3.2 Formation and Effect of Positional Vectors within Context Window"}, {"figure_path": "zeYyq0GpXO/tables/tables_6_1.jpg", "caption": "Table 3: The interpolation results of positional vectors, where Factor (= Target Length/C) is the expansion factor of the context window, Ratio is the effective interpolation ratio of positional vectors (detailed in Appendix C), and Similarity is the average cosine similarity between the scaled positional vector and the original most similar positional vector by averaging all layers.", "description": "This table shows the results of positional vector interpolation experiments using two methods: Attention Scaling and Initial Scaling, applied to TL-NoPE and TL-ROPE models with different target lengths (4096 and 8192 tokens).  The table presents the expansion factor of the context window, the effective interpolation ratio of positional vectors, the average cosine similarity between the interpolated and original vectors, and the resulting perplexity (PPL) and change in perplexity (\u0394PPL).  The results demonstrate the effectiveness of interpolation methods in improving the model's performance on longer sequences, showing the relationship between interpolation ratio, similarity, and resulting perplexity.", "section": "3.3 Effect of Positional Vectors beyond Context Window"}, {"figure_path": "zeYyq0GpXO/tables/tables_13_1.jpg", "caption": "Table 1: The compared model variants. Full attention is denoted as Full and window attention with a window size of W tokens is denoted as Window (W). We abbreviate TinyLLaMA as TL.", "description": "This table presents the different model variants used in the paper's experiments.  The models vary in their attention mechanisms (full attention vs. windowed attention with different window sizes) and positional encoding schemes (no positional encoding, RoPE, and ALiBi).  The table provides a concise overview of the model configurations used for comparison in the empirical analysis.", "section": "3.1 Experimental Settings"}, {"figure_path": "zeYyq0GpXO/tables/tables_17_1.jpg", "caption": "Table 6: Results of removing different components in attention.", "description": "This table presents the results of an ablation study investigating the impact of removing different components (value, positional vector, positional basis, and semantic vector) from the attention mechanism on the cosine similarity (Sim) between positional vectors and perplexity (PPL) scores. The study examines two token groups: initial tokens (0-4) and subsequent tokens (32-256) in Llama-3, Yi-9B, Qwen-1.5-7B, and TL-NoPE-new.  The results illustrate how the removal of different components affects both positional vector similarity and model performance (as measured by PPL).", "section": "3.2 Formation and Effect of Positional Vectors within Context Window"}, {"figure_path": "zeYyq0GpXO/tables/tables_18_1.jpg", "caption": "Table 7: Resuls of PPL and change of positional vectors during direct extrapolation.", "description": "This table presents the results of perplexity (PPL) and the cosine similarity of positional vectors during direct extrapolation. Three models with different context window sizes are evaluated: Llama-3-8B, Yi-9B, and TL-NoPE-new. For each model, the PPL at the original context window size (C) and twice the context window size (2C) are reported, along with the cosine similarity between positional vectors within and beyond the context window (Simi(2C)). The results demonstrate that direct extrapolation generally leads to a significant increase in PPL, and this increase is accompanied by a decrease in the similarity of positional vectors.", "section": "3.3 Effect of Positional Vectors beyond Context Window"}, {"figure_path": "zeYyq0GpXO/tables/tables_18_2.jpg", "caption": "Table 8: Change of attention sinks and output logits beyond context window.", "description": "This table presents the results of an experiment that investigates the effect of exceeding the context window on the attention mechanism and the output logits in various LLMs.  It displays the attention sink (a measure of the attention assigned to the initial tokens) and the similarity of logits (a measure of the consistency of the output) within and beyond the context window. The results demonstrate a significant decrease in attention sink and similarity of logits as the input length goes beyond the context window.", "section": "3.3 Effect of Positional Vectors beyond Context Window"}, {"figure_path": "zeYyq0GpXO/tables/tables_19_1.jpg", "caption": "Table 9: Results of language modeling in PG-19 with TL-NoPE-new", "description": "This table presents the results of language modeling experiments conducted on the PG-19 dataset using the TL-NoPE-new model.  It compares the performance of three different context window extension methods:  no method (baseline), Attention Scaling, and Positional Vector Replacement.  The results show perplexity (PPL) scores for each method across different target lengths (2048, 4096, 6144, and 8192 tokens).  The 'Factor' column indicates the extent of context window extension applied.", "section": "4.3 Results on Language Modeling"}]