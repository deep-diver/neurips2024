{"references": [{"fullname_first_author": "Tom B. Brown", "paper_title": "Language Models are Few-Shot Learners", "publication_date": "2020-00-00", "reason": "This paper introduces GPT-3, a foundational large language model whose training relies on the Adam optimizer, which is the central subject of the current paper."}, {"fullname_first_author": "Diederik P. Kingma", "paper_title": "Adam: A Method for Stochastic Optimization", "publication_date": "2015-00-00", "reason": "This paper introduces the Adam optimizer, whose performance is compared against gradient descent in the current work."}, {"fullname_first_author": "Frederik Kunstner", "paper_title": "Noise is not the main factor behind the gap between SGD and Adam on transformers, but sign descent might be", "publication_date": "2023-00-00", "reason": "This paper is a related work that explores the performance gap between SGD and Adam on transformers, informing the current research."}, {"fullname_first_author": "Jingzhao Zhang", "paper_title": "Why are Adaptive Methods Good for Attention Models?", "publication_date": "2020-00-00", "reason": "This paper investigates the benefits of adaptive optimization methods like Adam for attention models, providing a relevant context for the current study."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is All you Need", "publication_date": "2017-00-00", "reason": "This paper introduces the Transformer architecture, a key component of many large language models including those used in experiments within the current paper."}]}