[{"figure_path": "FXJDcriMYH/figures/figures_1_1.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure compares the training loss curves of two 7B LLMs. One model was trained from scratch, while the other utilized the Gdirect (Gstack) method, a depthwise stacking operator. The x-axis represents the training FLOPS (floating-point operations), and the y-axis shows the training loss.  The inset shows a zoomed-in view of the region where the models converge. The key observation is that Gstack achieves the same loss as the conventionally trained model (trained from scratch), but it uses significantly fewer FLOPs. Specifically, at 300 billion tokens, Gstack is 54.6% faster than the conventionally trained model. This highlights the efficiency gains of the Gstack method in accelerating LLM pre-training.", "section": "4 Delving Deeper Into Depthwise Stacking (Gstack)"}, {"figure_path": "FXJDcriMYH/figures/figures_3_1.jpg", "caption": "Figure 2: The simplified illustration of four growth operators Gdirect, Glearn, Gzero and Grandom, each of which can grow along widthwise (intra-layer) G\u2192 or depthwise (layer-wise) G\u2191. Wn is the parameters before growth, while Dn, Rn and O are the growth parameters derived from the old, randomly initialized, and zero-initialized respectively. Except Gdirect, other three operators only illustrates the widthwise growth.", "description": "This figure illustrates four different methods for expanding the parameters of a neural network model.  Each method involves different ways of generating new parameters (or neurons) based on existing parameters.  \n\n* **Gdirect:** Directly duplicates or stacks existing layers (depthwise) or splits existing neurons (widthwise).\n* **Glearn:** Uses a learned hypernetwork to generate new parameters based on existing ones.\n* **Gzero:** Initializes new parameters to zero.\n* **Grandom:** Initializes new parameters randomly.\n\nThe figure visually shows how each method modifies the existing network structure, either by adding layers (depthwise growth) or adding neurons within a layer (widthwise growth).", "section": "3.1 Growing LLMs with Growth Operators"}, {"figure_path": "FXJDcriMYH/figures/figures_5_1.jpg", "caption": "Figure 4: Training 3B LLMs with 300B tokens. Gstack significantly outperforms scratch in (a) loss and (b) average accuracy across NLP benchmarks. At 180B and 240B tokens, Gstack accelerates by 48.6% and 54.5% compared to scratch.", "description": "This figure shows the training loss and average accuracy across multiple NLP benchmarks for two 3B LLMs: one trained from scratch and another using the Gstack method. The Gstack model demonstrates significantly lower training loss and higher average accuracy compared to the model trained from scratch, achieving speedups of 48.6% at 180B tokens and 54.5% at 240B tokens.  This highlights the effectiveness of the Gstack approach in accelerating the training of large language models.", "section": "4.1 Scaling Gstack"}, {"figure_path": "FXJDcriMYH/figures/figures_5_2.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure compares the training loss curves of two 7-billion parameter LLMs. One model was trained from scratch, while the other used the Gstack method, a depthwise stacking operator. The x-axis represents the FLOPs (floating-point operations) in units of 1e+20, and the y-axis shows the training loss. The inset graph zooms in on the region where the two curves diverge, highlighting that Gstack achieves a 54.6% speedup at 300 billion tokens compared to the conventional training method.", "section": "4 Delving Deeper Into Depthwise Stacking (Gstack)"}, {"figure_path": "FXJDcriMYH/figures/figures_5_3.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure compares the training loss curves of two 7B Large Language Models (LLMs). One LLM was trained conventionally from scratch, while the other used the Gstack method, a model growth technique.  The x-axis represents the FLOPs (floating point operations) during training, and the y-axis represents the training loss.  The Gstack model achieves the same level of training loss as the conventionally trained model but uses significantly fewer FLOPs (54.6% fewer in this case), indicating a substantial speedup in training time. The red dashed box highlights the point where Gstack achieves its 54.6% speedup.", "section": "4 Delving Deeper Into Depthwise Stacking (Gstack)"}, {"figure_path": "FXJDcriMYH/figures/figures_5_4.jpg", "caption": "Figure 4: Training 3B LLMs with 300B tokens. Gstack significantly outperforms scratch in (a) loss and (b) average accuracy across NLP benchmarks. At 180B and 240B tokens, Gstack accelerates by 48.6% and 54.5% compared to scratch.", "description": "This figure shows the training loss and average accuracy of 3B LLMs trained with 300B tokens using two different methods: conventional training from scratch and the proposed Gstack method.  The Gstack method consistently shows lower training loss and higher average accuracy across eight standard NLP benchmarks, demonstrating a significant speedup in training. The speedup is more pronounced at higher token counts (180B and 240B tokens).", "section": "4.1 Scaling Gstack"}, {"figure_path": "FXJDcriMYH/figures/figures_6_1.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure shows the training loss curves for two 7B Large Language Models (LLMs). One LLM was trained from scratch, while the other utilized the Gstack method.  The x-axis represents the number of training tokens (in billions), and the y-axis represents the training loss. The Gstack method demonstrates a significant speedup, converging to a similar loss as the model trained from scratch with fewer tokens (300B tokens for Gstack vs. more for the from-scratch model).  The figure highlights that at 300 billion tokens, Gstack achieves a 54.6% speedup compared to training the model from scratch.", "section": "4 Delving Deeper Into Depthwise Stacking (Gstack)"}, {"figure_path": "FXJDcriMYH/figures/figures_6_2.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure shows a comparison of the training loss curves for two 7B Large Language Models (LLMs). One LLM was trained from scratch, while the other used the Gstack method, a depth-wise stacking operator.  The x-axis represents the number of tokens (in billions) used for pre-training, and the y-axis represents the training loss. The Gstack model achieves the same training loss as the conventionally trained model, but with significantly fewer tokens (194B tokens versus 300B tokens), representing a 54.6% reduction in training time.", "section": "4 Delving Deeper Into Depthwise Stacking (Gstack)"}, {"figure_path": "FXJDcriMYH/figures/figures_6_3.jpg", "caption": "Figure 4: Training 3B LLMs with 300B tokens. Gstack significantly outperforms scratch in (a) loss and (b) average accuracy across NLP benchmarks. At 180B and 240B tokens, Gstack accelerates by 48.6% and 54.5% compared to scratch.", "description": "This figure shows the training loss and average accuracy of 3B LLMs trained with 300B tokens using both conventional training (scratch) and the proposed Gstack method.  The results demonstrate that Gstack significantly reduces training loss and improves average accuracy across multiple NLP benchmarks.  Specifically, at 180B and 240B tokens, Gstack achieves a 48.6% and 54.5% speedup, respectively, compared to the scratch model.", "section": "4.1 Scaling Gstack"}, {"figure_path": "FXJDcriMYH/figures/figures_7_1.jpg", "caption": "Figure 8: In 410M, 1.1B, and 3B LLMs, we plot smoothed loss curves for different growth timing d given a set of FLOPs to form IsoFLOP figures. We find a clear valley in loss, indicating that for a given FLOP budget, there exists an optimal growth timing d for the Gstack operation.", "description": "This figure shows the relationship between training loss, FLOPs, and growth timing (d) for three different sizes of LLMs (410M, 1.1B, and 3B parameters).  Each subplot displays multiple curves representing different FLOPs. The curves show that for each FLOP value, there's a minimal loss associated with an optimal growth timing (d). This suggests a way to determine the best time to initiate the growth operation given a particular computational budget.", "section": "4.2 Determining Growth Timing and Growth Factor for Using Gstack"}, {"figure_path": "FXJDcriMYH/figures/figures_7_2.jpg", "caption": "Figure 9: We fit a contour figure for predicting d given C and N. These optimal growth timing d fit the figure well.", "description": "This figure visualizes the relationship between the optimal growth timing (d), computational budget (C), and number of parameters (N) for the Gstack operator in LLM pre-training.  It shows a contour plot where lines of constant FLOPs are plotted against growth timing. Each line represents a particular computational budget, and the valley along each line indicates the optimal growth timing (d) for a given computational budget (C) and target model size (N).  The plot demonstrates the existence of a logarithmic equation linking these three variables.", "section": "4.2 Determining Growth Timing and Growth Factor for Using Gstack"}, {"figure_path": "FXJDcriMYH/figures/figures_8_1.jpg", "caption": "Figure 4: Training 3B LLMs with 300B tokens. Gstack significantly outperforms scratch in (a) loss and (b) average accuracy across NLP benchmarks. At 180B and 240B tokens, Gstack accelerates by 48.6% and 54.5% compared to scratch.", "description": "This figure displays the training loss and average accuracy of 3B LLMs trained with 300B tokens, comparing the performance of Gstack against training from scratch. Gstack demonstrates significant improvements in both loss and accuracy across various NLP benchmarks, resulting in notable speedups (48.6% and 54.5% at 180B and 240B tokens, respectively).", "section": "4.1 Scaling Gstack"}, {"figure_path": "FXJDcriMYH/figures/figures_22_1.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure shows the training loss curves for two 7-billion parameter large language models (LLMs). One LLM was trained conventionally from scratch, while the other utilized the Gstack method.  The x-axis represents the cumulative floating-point operations (FLOPs), a measure of computational cost, and the y-axis displays the training loss.  The figure demonstrates that Gstack achieves a 54.6% speedup in training compared to the conventional approach, reaching the same loss with significantly fewer FLOPs.", "section": "1 Introduction"}, {"figure_path": "FXJDcriMYH/figures/figures_22_2.jpg", "caption": "Figure 11: Training Loss on Slimpajama.", "description": "This figure shows the training loss curves for four different growth operators (Gdirect, Glearn, Gzero, Grandom) and training from scratch on the Slimpajama dataset.  The top two sub-figures show the depth-wise growth from small models trained on 10B and 50B tokens respectively, while the bottom two sub-figures depict the width-wise growth from the same small models. Each sub-figure compares the training loss of the different methods in terms of FLOPs. This visualization helps to assess the efficiency of each growth operator in accelerating LLM pre-training, showing how quickly they achieve lower training loss compared to starting from scratch.", "section": "3.2 Pre-Training 1.1B LLMs"}, {"figure_path": "FXJDcriMYH/figures/figures_22_3.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure shows a comparison of the training loss curves for two 7B LLMs. One model was trained from scratch, while the other utilized the Gdirect (Gstack) method.  The graph demonstrates that the Gstack model achieves a significant speedup of 54.6% at 300 billion tokens compared to the conventionally trained model. This highlights the efficiency gains of the Gstack approach in LLM pre-training.", "section": "1 Introduction"}, {"figure_path": "FXJDcriMYH/figures/figures_22_4.jpg", "caption": "Figure 3: We evaluate operators using training loss and Lambada [30], ARC-c [31], ARC-e [31], Logiqa [32], PIQA [33], Sciq [34], Winogrande [35] and Wikitext PPL [36] totaling eight standard NLP benchmarks. After 8 \u00d7 10<sup>20</sup> FLOPs of training, G<sub>direct</sub> demonstrates a significant speedup.", "description": "This figure shows the results of training loss and eight NLP benchmark evaluation metrics on four growth operators (G<sub>direct</sub>, G<sub>learn</sub>, G<sub>zero</sub>, G<sub>random</sub>) in both depthwise and widthwise directions. The depthwise stacking operator (G<sub>direct</sub>) consistently outperforms other operators in accelerating LLM pre-training.  The results demonstrate the effectiveness of G<sub>direct</sub> and its significant speedup compared to training from scratch, as evidenced by the substantial reduction in training loss and improvement in various NLP benchmark scores.", "section": "3 Systematically Assessing Model Growth for LLM Pre-Training"}, {"figure_path": "FXJDcriMYH/figures/figures_23_1.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure compares the training loss curves for two 7 Billion parameter Language Models (LLMs). One LLM was trained conventionally from scratch, while the other used the Gstack method, a model growth technique. The y-axis represents the training loss, and the x-axis represents the number of training tokens (in billions). The figure shows that the Gstack model achieves the same loss as the conventionally trained model but with significantly fewer tokens (194B vs 300B), resulting in a 54.6% speedup.", "section": "1 Introduction"}, {"figure_path": "FXJDcriMYH/figures/figures_23_2.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure compares the training loss curves of two 7-billion parameter LLMs. One model was trained conventionally from scratch, while the other utilized the Gstack method, a depth-wise stacking operator.  The plot shows that at 300 billion tokens, the Gstack model converges to the same loss as the scratch model but with a significant reduction in the number of training tokens, resulting in a 54.6% speedup.", "section": "4 Delving Deeper Into Depthwise Stacking (Gstack)"}, {"figure_path": "FXJDcriMYH/figures/figures_24_1.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure shows a comparison of the training loss curves for two 7B LLMs. One model was trained from scratch, while the other used the Gdirect (Gstack) method, which leverages smaller pre-trained models to accelerate training.  The graph clearly demonstrates that Gstack achieves significantly faster convergence, reaching the same loss level as the model trained from scratch with 105.4 billion fewer tokens (a 54.6% reduction in tokens needed). This highlights the efficiency gains offered by the Gstack model growth technique. ", "section": "4 Delving Deeper Into Depthwise Stacking (Gstack)"}, {"figure_path": "FXJDcriMYH/figures/figures_24_2.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure shows a comparison of the training loss curves for two 7B LLMs. One model was trained conventionally from scratch, while the other was trained using the Gstack method (a depth-wise stacking operator). The results demonstrate that Gstack achieves a 54.6% speedup in training compared to the conventional training method when reaching the same loss level at 300B tokens. The graph visually represents the substantial training time reduction that Gstack provides.", "section": "1 Introduction"}, {"figure_path": "FXJDcriMYH/figures/figures_25_1.jpg", "caption": "Figure 3: We evaluate operators using training loss and Lambada [30], ARC-c [31], ARC-e [31], Logiqa [32], PIQA [33], Sciq [34], Winogrande [35] and Wikitext PPL [36] totaling eight standard NLP benchmarks. After 8 \u00d7 10<sup>20</sup> FLOPs of training, G<sub>direct</sub> demonstrates a significant speedup.", "description": "This figure shows the results of comparing four different growth operators for LLMs on eight standard NLP benchmarks. The operators are evaluated on their training loss and accuracy. The results show that the depthwise growth operator G<sub>direct</sub> (Gstack) significantly outperforms the other operators and a model trained from scratch in terms of speed and performance.", "section": "3 Systematically Assessing Model Growth for LLM Pre-Training"}, {"figure_path": "FXJDcriMYH/figures/figures_25_2.jpg", "caption": "Figure 4: Training 3B LLMs with 300B tokens. Gstack significantly outperforms scratch in (a) loss and (b) average accuracy across NLP benchmarks. At 180B and 240B tokens, Gstack accelerates by 48.6% and 54.5% compared to scratch.", "description": "This figure shows the training loss and average accuracy of two 3B LLMs, one trained from scratch and the other using the Gstack method.  The Gstack model converges faster, reaching the same loss with fewer tokens, demonstrating a significant speedup in training. The average accuracy across eight NLP benchmarks further supports the superior performance of the Gstack method.", "section": "4.1 Scaling Gstack"}, {"figure_path": "FXJDcriMYH/figures/figures_26_1.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure shows the training loss curves for two 7B LLMs: one trained from scratch and another trained using the Gdirect (Gstack) method.  The plot demonstrates that Gstack achieves a significantly faster convergence rate than training from scratch. Specifically, at 300 billion tokens, Gstack shows a 54.6% speedup compared to the conventionally trained model, indicating substantial improvements in training efficiency.", "section": "4 Delving Deeper Into Depthwise Stacking (Gstack)"}, {"figure_path": "FXJDcriMYH/figures/figures_26_2.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure shows a comparison of the training loss curves for two 7-billion parameter LLMs. One model was trained from scratch, while the other utilized the Gdirect (Gstack) method, a depth-wise stacking operator that leverages pre-trained smaller models to accelerate training.  The graph demonstrates that, at the 300 billion token mark, the Gstack model achieves a 54.6% speedup compared to the conventionally trained model, indicating that Gstack significantly accelerates training for large language models.", "section": "4 Delving Deeper Into Depthwise Stacking (Gstack)"}, {"figure_path": "FXJDcriMYH/figures/figures_27_1.jpg", "caption": "Figure 4: Training 3B LLMs with 300B tokens. Gstack significantly outperforms scratch in (a) loss and (b) average accuracy across NLP benchmarks. At 180B and 240B tokens, Gstack accelerates by 48.6% and 54.5% compared to scratch.", "description": "This figure shows the results of training 3B parameter LLMs using 300B tokens.  It compares the training loss and average accuracy across several NLP benchmarks for two approaches: training from scratch and using the Gstack method.  The results demonstrate that Gstack significantly outperforms training from scratch, achieving substantial speedups (48.6% and 54.5% at 180B and 240B tokens respectively).", "section": "4.1 Scaling Gstack"}, {"figure_path": "FXJDcriMYH/figures/figures_27_2.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure compares the training loss curves of two 7-billion parameter LLMs. One model was trained from scratch, while the other utilized the Gstack method, a depthwise stacking operator.  The plot shows that Gstack achieves a 54.6% speedup in training time compared to the model trained from scratch, reaching the same loss level with significantly fewer training tokens.", "section": "1 Introduction"}, {"figure_path": "FXJDcriMYH/figures/figures_28_1.jpg", "caption": "Figure 9: We fit a contour figure for predicting d given C and N. These optimal growth timing d fit the figure well.", "description": "This figure visualizes the relationship between three key hyperparameters in the Gstack model growth technique: growth timing (d), computational budget (C), and number of parameters in the target model (N).  It shows a contour plot where each curve represents a constant value of training loss (IsoFLOP).  The lowest loss (optimal d) is indicated by the valley along each IsoFLOP curve.  The plot suggests a logarithmic relationship between these hyperparameters, which is formalized in equation (2) in the paper.", "section": "4.2 Determining Growth Timing and Growth Factor for Using Gstack"}, {"figure_path": "FXJDcriMYH/figures/figures_29_1.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure compares the training loss curves of two 7B LLMs. One model was trained from scratch, while the other used the Gstack method (a depth-wise stacking operator).  The x-axis represents the number of tokens processed during training (in billions), and the y-axis represents the training loss. The figure shows that the Gstack model reaches the same training loss as the from-scratch model but with significantly fewer tokens (194B vs 300B), resulting in a 54.6% speedup. This demonstrates the efficiency gains achieved by Gstack during LLM pre-training.", "section": "4 Delving Deeper Into Depthwise Stacking (Gstack)"}, {"figure_path": "FXJDcriMYH/figures/figures_30_1.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure compares the training loss curves of two 7B Large Language Models (LLMs). One LLM was trained conventionally from scratch, while the other utilized the Gstack method, a depthwise stacking operator.  The x-axis represents the number of training tokens (in billions), and the y-axis shows the training loss.  The graph demonstrates that the Gstack model achieves the same training loss as the conventionally trained model but using significantly fewer tokens (194B vs 300B), resulting in a 54.6% reduction in training time.", "section": "1 Introduction"}, {"figure_path": "FXJDcriMYH/figures/figures_30_2.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure shows a comparison of the training loss curves for two 7B Large Language Models (LLMs). One LLM was trained from scratch, while the other used the Gstack method.  The x-axis represents the number of tokens (in billions) used during training, and the y-axis represents the training loss. The Gstack method, a depthwise stacking operator, achieved a 54.6% speedup compared to training from scratch when reaching the same loss level at 300B tokens. This demonstrates the efficiency gains of the Gstack method in LLM pre-training.", "section": "4 Delving Deeper Into Depthwise Stacking (Gstack)"}, {"figure_path": "FXJDcriMYH/figures/figures_31_1.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure compares the training loss curves of two 7-billion parameter LLMs. One model was trained conventionally from scratch, while the other utilized the Gstack method, a depthwise stacking operator.  The graph demonstrates that Gstack achieves a 54.6% speedup by converging to the same loss level with fewer training tokens (194B vs. 300B) than the model trained from scratch. This highlights Gstack's efficiency in accelerating LLM pre-training.", "section": "1 Introduction"}, {"figure_path": "FXJDcriMYH/figures/figures_31_2.jpg", "caption": "Figure 4: Training 3B LLMs with 300B tokens. Gstack significantly outperforms scratch in (a) loss and (b) average accuracy across NLP benchmarks. At 180B and 240B tokens, Gstack accelerates by 48.6% and 54.5% compared to scratch.", "description": "This figure shows the training loss and average accuracy across various NLP benchmarks for two 3B LLMs: one trained from scratch and the other using the Gstack method.  The Gstack model demonstrates significantly lower training loss and higher accuracy than the model trained from scratch, indicating a substantial speedup in training time.  Specific speedup percentages are shown for token counts of 180B and 240B, highlighting the significant performance improvement achieved using Gstack.", "section": "4.1 Scaling Gstack"}, {"figure_path": "FXJDcriMYH/figures/figures_32_1.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure demonstrates the training loss curves for two 7B LLMs. One model was trained conventionally from scratch, while the other used the Gstack method, a depthwise stacking operator.  The Gstack model achieved a 54.6% speedup in training compared to the scratch model when both reached 300 billion tokens, indicating significant efficiency gains. The figure also shows the training FLOPS (floating point operations per second) for each model.", "section": "4 Delving Deeper Into Depthwise Stacking (Gstack)"}, {"figure_path": "FXJDcriMYH/figures/figures_32_2.jpg", "caption": "Figure 4: Training 3B LLMs with 300B tokens. Gstack significantly outperforms scratch in (a) loss and (b) average accuracy across NLP benchmarks. At 180B and 240B tokens, Gstack accelerates by 48.6% and 54.5% compared to scratch.", "description": "This figure shows the training loss and average accuracy on NLP benchmarks for two 7B LLMs: one trained from scratch and the other using the Gstack method. The Gstack model achieves the same loss with fewer tokens (194B vs 300B), resulting in a 54.6% speedup. The figure also shows the average accuracy on eight standard NLP benchmarks, demonstrating Gstack's consistent superior performance.", "section": "4.1 Scaling Gstack"}, {"figure_path": "FXJDcriMYH/figures/figures_33_1.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure compares the training loss curves of two 7-billion parameter large language models (LLMs). One LLM was trained from scratch, while the other utilized the Gstack method, a depth-wise stacking operator.  The plot shows that Gstack achieves the same loss with significantly fewer training tokens (194B instead of 300B), resulting in a 54.6% reduction in training time. This demonstrates the effectiveness of Gstack in accelerating LLM pre-training.", "section": "1 Introduction"}, {"figure_path": "FXJDcriMYH/figures/figures_33_2.jpg", "caption": "Figure 4: Training 3B LLMs with 300B tokens. Gstack significantly outperforms scratch in (a) loss and (b) average accuracy across NLP benchmarks. At 180B and 240B tokens, Gstack accelerates by 48.6% and 54.5% compared to scratch.", "description": "This figure shows the training loss and average accuracy of 3B LLMs trained with and without Gstack, across eight standard NLP benchmarks.  The results demonstrate that using Gstack significantly improves training speed and model performance compared to training from scratch.  Specifically, at 180B and 240B tokens, Gstack achieves a 48.6% and 54.5% speedup, respectively, while also improving average accuracy.", "section": "4.1 Scaling Gstack"}, {"figure_path": "FXJDcriMYH/figures/figures_34_1.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure compares the training loss curves of two 7-billion parameter language models (LLMs). One model was trained from scratch, while the other used the Gstack method, a technique that leverages smaller pre-trained models to accelerate the training of larger ones. The graph shows that, at the 300 billion token mark, the Gstack model achieves a 54.6% speedup compared to the model trained from scratch, indicating improved efficiency in LLM pre-training. ", "section": "4 Delving Deeper Into Depthwise Stacking (Gstack)"}, {"figure_path": "FXJDcriMYH/figures/figures_34_2.jpg", "caption": "Figure 3: We evaluate operators using training loss and Lambada [30], ARC-c [31], ARC-e [31], Logiqa [32], PIQA [33], Sciq [34], Winogrande [35] and Wikitext PPL [36] totaling eight standard NLP benchmarks. After 8 \u00d7 10<sup>20</sup> FLOPs of training, G<sub>direct</sub> demonstrates a significant speedup.", "description": "This figure presents the results of training 1.1B LLMs using four different growth operators: G<sub>direct</sub>, G<sub>learn</sub>, G<sub>zero</sub>, and G<sub>random</sub>.  Both widthwise and depthwise growth are evaluated.  The key finding is that depthwise stacking (G<sub>direct</sub>) significantly outperforms other methods and training from scratch across all eight NLP benchmarks (Lambada, ARC-c, ARC-e, Logiqa, PIQA, Sciq, Winogrande, Wikitext) and training loss. This highlights the effectiveness of depthwise stacking for accelerating LLM pre-training.", "section": "3 Systematically Assessing Model Growth for LLM Pre-Training"}, {"figure_path": "FXJDcriMYH/figures/figures_35_1.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure compares the training loss curves of two 7-billion parameter large language models (LLMs). One LLM was trained from scratch, while the other utilized the Gstack method (a depthwise stacking operator).  The x-axis represents the number of tokens processed during pre-training (in billions). The y-axis shows the training loss.  The results show that the Gstack method achieves a significantly lower training loss than the conventional training method (54.6% faster at 300 billion tokens).", "section": "4 Delving Deeper Into Depthwise Stacking (Gstack)"}, {"figure_path": "FXJDcriMYH/figures/figures_36_1.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure compares the training loss curves for two 7B parameter LLMs. One model was trained from scratch, while the other used the Gstack method.  The graph shows the training loss plotted against the number of tokens processed (in billions).  The key takeaway is that Gstack achieves the same training loss as the scratch-trained model, but with significantly fewer tokens (194B tokens compared to 300B tokens), resulting in a 54.6% speedup in training time.", "section": "4 Delving Deeper Into Depthwise Stacking (Gstack)"}, {"figure_path": "FXJDcriMYH/figures/figures_36_2.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure shows the training loss curves for two 7B LLMs. One model was trained from scratch, while the other used the Gdirect (Gstack) method, which leverages smaller models to accelerate training.  The graph demonstrates that the Gstack model converges to the same loss with fewer tokens (194B vs 300B), representing a 54.6% reduction in training time.", "section": "1 Introduction"}, {"figure_path": "FXJDcriMYH/figures/figures_37_1.jpg", "caption": "Figure 4: Training 3B LLMs with 300B tokens. Gstack significantly outperforms scratch in (a) loss and (b) average accuracy across NLP benchmarks. At 180B and 240B tokens, Gstack accelerates by 48.6% and 54.5% compared to scratch.", "description": "This figure presents the results of training 3B parameter LLMs using 300B tokens with and without Gstack. It shows that Gstack significantly improves both the training loss and average accuracy on various NLP benchmarks.  Specifically, at 180B and 240B tokens, Gstack achieves a 48.6% and 54.5% speedup, respectively, compared to training from scratch.", "section": "4.1 Scaling Gstack"}, {"figure_path": "FXJDcriMYH/figures/figures_38_1.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure displays the training loss curves for two 7-billion parameter LLMs. One model was trained conventionally from scratch, while the other employed the Gstack method, a depthwise stacking operator for model growth.  The graph demonstrates that the Gstack model reaches the same training loss as the conventionally trained model but using significantly fewer tokens (194B vs 300B).  This translates to a 54.6% speedup in pre-training time, highlighting the efficiency gains of the Gstack method.", "section": "4 Delving Deeper Into Depthwise Stacking (Gstack)"}, {"figure_path": "FXJDcriMYH/figures/figures_39_1.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure shows a comparison of the training loss curves for two 7B Large Language Models (LLMs). One LLM was trained conventionally from scratch, while the other used the Gstack method, a depth-wise stacking operator. The x-axis represents the number of training tokens in billions, and the y-axis represents the training loss.  The Gstack method significantly outperforms the conventional training method, achieving the same loss with fewer tokens (194B vs 300B). This translates to a 54.6% speedup in training time.", "section": "4 Delving Deeper Into Depthwise Stacking (Gstack)"}, {"figure_path": "FXJDcriMYH/figures/figures_40_1.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure compares the training loss curves for two 7-billion parameter large language models (LLMs) trained using different methods. One LLM was trained from scratch, while the other was trained using the Gstack method, which leverages smaller trained models to accelerate training.  The figure shows that at the 300 billion token mark, the Gstack method achieves a 54.6% speedup in training compared to training from scratch.", "section": "4 Delving Deeper Into Depthwise Stacking (Gstack)"}, {"figure_path": "FXJDcriMYH/figures/figures_40_2.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure compares the training loss curves for two 7-billion parameter large language models (LLMs). One LLM was trained from scratch, while the other utilized the Gstack method, a depthwise stacking growth operator. The x-axis represents the number of tokens (in billions) used for pre-training, and the y-axis shows the training loss.  The figure demonstrates that Gstack achieves the same training loss as the model trained from scratch but using significantly fewer tokens (194B tokens instead of 300B), resulting in a 54.6% reduction in training time.", "section": "4 Delving Deeper Into Depthwise Stacking (Gstack)"}, {"figure_path": "FXJDcriMYH/figures/figures_41_1.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure shows a comparison of the training loss curves for two 7B LLMs: one trained from scratch and another trained using the Gstack method (a depthwise stacking operator). The Gstack model achieves a 54.6% speedup compared to the conventionally trained model at 300B tokens, demonstrating the efficiency gains provided by this model growth technique.  The y-axis represents the training loss, and the x-axis represents the number of tokens used during training (in billions).", "section": "4 Delving Deeper Into Depthwise Stacking (Gstack)"}, {"figure_path": "FXJDcriMYH/figures/figures_41_2.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure compares the training loss curves of two 7B Large Language Models (LLMs). One LLM was trained conventionally from scratch, while the other used the Gstack method, a model growth technique.  The graph shows that at 300 billion tokens, the Gstack model achieves the same loss as the scratch model but with significantly fewer training tokens (194B), representing a 54.6% speedup in training.", "section": "1 Introduction"}, {"figure_path": "FXJDcriMYH/figures/figures_42_1.jpg", "caption": "Figure 1: The training loss for two 7B LLMs, trained from scratch and with Gdirect (Gstack). At 300B tokens, Gstack accelerates by 54.6% compared to scratch.", "description": "This figure compares the training loss curves of two 7B LLMs. One model was trained from scratch, while the other used the Gstack method (a depthwise stacking operator).  The x-axis represents the number of tokens (in billions) used during pre-training, and the y-axis shows the training loss.  The Gstack model achieves the same training loss as the scratch-trained model using significantly fewer tokens (194B vs 300B), demonstrating a 54.6% speedup in training.", "section": "1 Introduction"}]