{"references": [{"fullname_first_author": "S. Hochreiter", "paper_title": "Long short-term memory", "publication_date": "1997-08-01", "reason": "This paper introduced the core concepts of LSTM, forming the foundation upon which the current work builds and significantly advances."}, {"fullname_first_author": "J. Ba", "paper_title": "Layer normalization", "publication_date": "2016-07-06", "reason": "Layer normalization is a crucial technique used for stabilizing training in deep learning models, and its application is critical for the success of the proposed XLSTM architecture."}, {"fullname_first_author": "A. Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-05", "reason": "The Transformer architecture, introduced in this paper, is a major competitor to LSTMs, and this work directly addresses the challenges of scaling LSTMs to compete with Transformers."}, {"fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This work established benchmarks for large language models that the current work aims to match using LSTMs, highlighting the importance of scaling in the field."}, {"fullname_first_author": "G. Del\u00e9tang", "paper_title": "Neural networks and the Chomsky hierarchy", "publication_date": "2023-05-01", "reason": "This paper provides a framework of formal language tasks to rigorously evaluate the capabilities of different neural network architectures, which is used to benchmark the XLSTM model."}]}