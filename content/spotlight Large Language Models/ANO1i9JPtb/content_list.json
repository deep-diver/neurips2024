[{"type": "text", "text": "Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ling $\\mathbf{Yang}^{1*\\dagger}$ , Zhaochen $\\mathbf{Y}\\mathbf{u}^{1}$ ,\u2217 Tianjun Zhang2, Shiyi $\\mathbf{Cao^{2}}$ , Minkai $\\mathbf{X}\\mathbf{u}^{3}$ , Wentao Zhang1, Joseph E. Gonzalez2, Bin $\\mathbf{Cui}^{1}$ \u2020 ", "page_idx": 0}, {"type": "text", "text": "1Peking University, 2UC Berkeley, 3Stanford University Project: https://github.com/YangLing0818/buffer-of-thought-llm Extension: https://github.com/YangLing0818/SuperCorrect-llm ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We introduce Buffer of Thoughts (BoT), a novel and versatile thought-augmented reasoning approach for enhancing accuracy, efficiency and robustness of large language models (LLMs). Specifically, we propose meta-buffer to store a series of informative high-level thoughts, namely thought-template, distilled from the problem-solving processes across various tasks. Then for each problem, we retrieve a relevant thought-template and adaptively instantiate it with specific reasoning structures to conduct efficient reasoning. To guarantee the scalability and stability, we further propose buffer-manager to dynamically update the meta-buffer, thus enhancing the capacity of meta-buffer as more tasks are solved. We conduct extensive experiments on 10 challenging reasoning-intensive tasks, and achieve significant performance improvements over previous SOTA methods: $11\\%$ on Game of 24, $20\\%$ on Geometric Shapes and $51\\%$ on Checkmate-in-One. Further analysis demonstrate the superior generalization ability and model robustness of our BoT, while requiring only $12\\%$ of the cost of multi-query prompting methods (e.g., tree/graph of thoughts) on average. Notably, we find that our Llama3- $^{\\cdot8\\mathrm{B}+}$ BoT has the potential to surpass Llama3-70B model. Our project is available at https://github.com/YangLing0818/buffer-of-thought-llm ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A series of Large Language Models (LLMs) [1\u20135] like GPT-4 [3], PaLM [2] and LLaMA [6, 7] have showcased the impressive performance in various reasoning tasks. In addition to scaling up the model size to improve the reasoning performance, there are more effective prompting methods that further enhance the functionality and performance of LLMs. We divide these methods into two categories: (i) single-query reasoning: these methods [8\u201310] usually focus on prompt engineering and their reasoning process can be finished within a single query, such as CoT [8] that appends the input query with \u2019Let\u2019s think step by step\u2019 to produce rationales for increasing reasoning accuracy, and Few-shot Prompting [11, 12, 9, 13] which provides task-relevant exemplars to assist the answer generation; (ii) multi-query reasoning: these methods [14, 15] focus on leveraging multiple LLM queries to elicit different plausible reasoning paths, thus decomposing a complex problem into a series of simpler sub-problems, such as Least-to-Most [16], ToT [14] and GoT [17]. ", "page_idx": 0}, {"type": "text", "text": "However, both kinds of methods face some limitations: (1) single-query reasoning usually requires prior assumption or relevant exemplars of reasoning process, which makes it impractical to manually design them task by task, thus lacking universality and generalization; (2) Due to the recursive expansion of reasoning paths, multi-query reasoning is usually computationally-intensive when finding a unique intrinsic structure underlying the reasoning process for each specific task; (3) Both single-query and multi-query reasoning processes are limited by their designed exemplars and reasoning structures, and they neglect to derive general and high-level guidelines or thoughts from previously-completed tasks, which are informative for improving efficiency and accuracy when solving similar problems. ", "page_idx": 0}, {"type": "image", "img_path": "ANO1i9JPtb/tmp/6fb60fb77e0e74bf299fbd4fd1794b144b421b40b92d0abf3a321bf9c5daf390.jpg", "img_caption": ["Figure 1: Comparison between single-query [8, 11], multi-query [14, 17], and (c) our BoT methods. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address these limitations, we propose Buffer of Thoughts (BoT), a novel and versatile thoughtaugmented reasoning framework aimed at enhancing reasoning accuracy, efficiency and robustness of LLMs across various tasks. Specifically, we design meta-buffer, a lightweight library housing a series of universal high-level thoughts (thought-template), which are distilled from different problem-solving processes and can be shared across tasks. Then, for each problem, we retrieve a relevant thoughttemplate and instantiate it with specific reasoning structure for efficient thought-augmented reasoning. In order to guarantee the scalability and stability of our BoT, we further propose buffer-manager to dynamically update the meta-buffer, which effectively enhances the capacity of meta-buffer as more tasks are solved. ", "page_idx": 1}, {"type": "text", "text": "Our method has three critical advantages: (i) Accuracy Improvement: With the shared thoughttemplates, we can adaptively instantiate high-level thoughts for addressing different tasks, eliminating the need to build reasoning structures from scratch, thereby improving reasoning accuracy. (ii) Reasoning Efficiency: Our thought-augmented reasoning could directly leverage informative historical reasoning structures to conduct reasoning without complex multi-query processes, thus improving reasoning efficiency. (iii) Model Robustness: The procedure from thought retrieval to thought instantiation is just like the human thought process, enabling LLMs to address similar problems in a consistent way, thus significantly enhancing the model robustness of our method. Our empirical studies demonstrate that Buffer of Thoughts significantly improves precision, efficiency, and robustness over a diverse array of tasks. Here, we summarize our contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We propose a novel thought-augmented reasoning framework Buffer of Thoughts (BoT) for improving the accuracy, efficiency and robustness of LLM-based reasoning.   \n2. We propose meta-buffer for store informative high-level thoughts distilled from different problems, and adaptively instantiate each thought template to address each specific task.   \n3. We design buffer-manager to distill thought-templates from various solutions, and is continually improves the capacity of meta-buffer as more tasks are solved.   \n4. We conduct extensive experiments on 10 challenging reasoning-intensive tasks. Our BoT achieves significant performance improvements over previous SOTA methods: ${\\bf11\\%}$ on Game of 24, $2\\mathbf{0}\\%$ on Geometric Shapes and ${\\bf51\\%}$ on Checkmate-in-One, while requiring only ${\\bf1}2\\%$ of the cost of multi-query prompting methods on average. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work and Discussions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Retrieval-Augmented Language Models The retrieval-augmented (Large) Language Model is introduced as a solution to mitigate the phenomenon of hallucination and enhance the output quality of language models [18\u201322]. When presented with an input question, the retrieval-augmented LLM first queries an external database with billion-level tokens [23] for retrieving a subset of the text corpus to help generating the final answer. Notably, the retrieval-augmented LLM achieves superior question-answering performance using fewer parameters compared to conventional LLMs [19], and it has found application across various downstream tasks [24\u201326], including multi-modal generation [24, 22, 23, 25] and biomedical applications [26, 27]. In this paper, we construct a novel category of retrieval database, termed meta-buffer, which contains a series of high-level thoughts rather than specific instances, aiming to universally address various tasks for LLM-based reasoning. ", "page_idx": 2}, {"type": "text", "text": "Prompt-based Reasoning with Large Language Models Prompting techniques have significantly enahnced the arithmetic and commonsense reasoning capabilities of LLMs. Chain-of-Thought (CoT) prompting [8] and its variants [28\u201330], such as Least-to-Most [16], Decomposed Prompting [31], and Auto-CoT [13]\u2014prompt LLMs to break down complex questions into simpler subtasks and systematically solve them before summarizing a final answer. Numerous studies [32\u201337] have demonstrated the effectiveness of these prompting methods across a wide range of tasks and benchmarks. Innovations like Tree-of-Thought [14] and Graph-of-Thought [17], have further advanced this field by exploring dynamic, non-linear reasoning pathways to expand heuristic capabilities of LLMs [38, 39]. However, they suffer from increased resource demands and greater time complexity, depend on manual prompt crafting, and are often tailored to specific task types. Recent meta prompting methods [15, 40] utilize a same task-agnostic form of prompting for various tasks and recursively guide a single LLM to adaptively addressing different input queries. Nevertheless, such a long meta prompt may require a considerable context window, and these methods fail to leverage historical informative guidelines or thoughts for potential similar tasks. ", "page_idx": 2}, {"type": "text", "text": "Analogical Reasoning Analogical reasoning is a useful technique for natural language reasoning [41\u201345]. Recent works demonstrate that LLMs can perform analogical reasoning just like humans [46, 47, 12, 48, 49]. For example, Analogical Prompting [12] and Thought Propagation [48] prompt LLMs to self-generate a set of analogous problems, and then utilize the results of analogous problems to produce a solution for input problem. However, the specific solutions for self-explored problems may introduce additional noise and cause error accumulation. Recent Thought-Retriever [49] uses the intermediate thoughts generated when solving past user to address analogous queries, but it only focuses on textual comprehension/generation instead of general reasoning problems. Thus, a more high-level and general analogical approach for LLM complex reasoning is still lacking. ", "page_idx": 2}, {"type": "text", "text": "3 Buffer of Thoughts ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Overview of Buffer of Thoughts In this section, we introduce our Buffer of Thoughts in detail and we also illustrate our core thought-augmented reasoning process in Figure 2. Given a specific task, we utilize our problem-distiller (Section 3.1) to extract critical task-specific information along with relevant constraints. Based on the distilled information, we search in meta-buffer (Section 3.2) that contains a series of high-level thoughts (thought-template) and retrieve a most relevant thoughttemplate for the task. Subsequently, we instantiate the retrieved thought-template with more taskspecific reasoning structures and conduct reasoning process. Finally, we employs a buffer-manager (Section 3.3) for summarizing the whole problem-solving process and distilling high-level thoughts for imcreasing the capacity of meta-buffer. ", "page_idx": 2}, {"type": "text", "text": "3.1 Problem Distiller ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Most of complex tasks contain implicit constraints, complex object relationships, and intricate variables and parameters within their contexts. Consequently, during the reasoning stage, LLMs need to overcome three main challenges: extracting vital information, recognizing potential constraints, and performing accurate reasoning. These challenges would impose a significant burden on a single LLM. Therefore, we separate the extraction and comprehension stages of task information from the final reasoning stage, through prepending a problem distiller to the reasoning process. More concretely, we design a meta prompt $\\phi$ to first distill and formalize the task information. The distilled task information could be denoted as: ", "page_idx": 2}, {"type": "image", "img_path": "ANO1i9JPtb/tmp/f50d268367427323c469c7e13134bbdbaade35314a050a29f9a2a4aada47403f.jpg", "img_caption": ["Figure 2: Illustration of different reasoning process. Buffer of Thoughts enables large language models to tackle complex reasoning tasks through our thought-augmented reasoning process. Thought template is marked in orange and instantiated thought is marked in blue. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nx_{d}=L L M(\\phi(x)),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $x$ is the task statement. Due to the page limit, we put the detailed meta prompt for problemdistiller in Appendix B.2. ", "page_idx": 3}, {"type": "text", "text": "Problem Condensation and Translation We use the problem distiller to extract key elements from input tasks, focusing on: (1). Essential parameters and variables for problem-solving; (2). The objectives of the input tasks and their corresponding constraints. We then re-organize this distilled information into a clear, comprehensible format for the subsequent reasoning stage. We then translate the specific problems into high-level concepts and structures. This translation procedure decomposes complex real-world problems, like intricate mathematical application scenarios, into simpler, multi-step calculations, making it easier for later retrieval of high-level thought. ", "page_idx": 3}, {"type": "text", "text": "3.2 Thought-Augmented Reasoning with Meta Buffer ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Motivation Human often summarize and induce higher-level guidelines when solving problems and then apply them to relevant problems. Motivated by this, we propose meta-buffer, a lightweight library that contains a series of high-level thoughts (thought-template) for addressing various types of problems. Unlike traditional methods [11, 46, 12, 36, 9] that require specific instructions or exemplars, our high-level thought-templates can be adaptively instantiated when solving different problems, thereby enhancing LLMs with superior precision and flexibility. ", "page_idx": 3}, {"type": "text", "text": "Thought Template As a kind of high-level guideline, our thought-template is stored in metabuffer , and is obtained from various problem-solving processes by our buffer-manager. The details about acquiring thought-templates would be introduced in Section 3.3. Since our BoT aims to provide a general reasoning approach for various tasks, we correspondingly classify the thoughttemplates into six categories: Text Comprehension, Creative Language Generation, Common Sense Reasoning, Mathematical Reasoning, Code Programming and Application Scheduling. We provide some example thought-templates in Appendix B.1. Such classification of thought-templates can facilitate the template retrieval for finding most suitable solutions to different problems. Here we denote thought template, template description and its corresponding category as $(\\bar{T}_{i},D_{T_{i}},C_{k})$ , where $i$ denotes the index of meta-template, $\\bar{k_{\\mathrm{~}}}\\in\\mathbb{Z}^{+}$ and $1\\leq k\\leq6$ , which means $C_{k}$ is in one of the six categories, and $D_{T_{i}}$ is the description of thought template. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Template Retrieval For each task, our BoT retrieves a thought-template $T_{i}$ that is highly similar to the distilled problem $x_{d}$ by calculating the embedding similarity between the description $D_{T_{i}}$ and $x_{d}$ . The retrieval process can be formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nj=\\mathrm{argmax}_{i}(\\mathrm{Sim}(f(x_{d}),\\{f(D_{T_{i}})\\}_{i=1}^{N})),\\quad\\mathrm{where}\\quad\\mathrm{Sim}(f(x_{d}),\\{f(D_{T_{i}})\\}_{i=0}^{n})>=\\delta,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "$N$ is the size of the meta-buffer, $f(\\cdot)$ is a normal text embedding model, and $T_{j}$ denotes the retrieved thought template. We set a threshold $\\delta$ $(0.5{\\sim}0.7\\$ is recommended) to determine whether the current task is new. Therefore, if $\\mathrm{Sim}(f(x_{d}),\\{f(D_{T_{i}})\\}_{i=0}^{n})<\\delta$ , we identify the task $x$ as a new task. ", "page_idx": 4}, {"type": "text", "text": "Instantiated Reasoning For each specific task, we discuss two situations for the instantiated reasoning, depending on whether the current task is new: The first situation is that we successfully retrieve a thought-template $T_{j}$ for the task. In this case, as presented in Figure 2, our thoughtaugmented reasoning will be adaptively instantiated to suitable reasoning structures with our designed instantiation prompt (in Appendix B.3). For example, in a Checkmate-in-One problem, we instantiate the template of updating chess board state to solve the problem step by step. Thus we conduct the instantiated reasoning for task $x$ using the distilled information $x_{d}$ and the retrieved template $T_{j}$ , and produce its solution $S_{x}$ as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nS_{x}=L L M_{\\mathrm{instantiation}}(x_{d},T_{j}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $L L M_{\\mathrm{instantiation}}$ denotes the instantiated reasoner with a LLM. ", "page_idx": 4}, {"type": "text", "text": "In the second situation, the task is identified as a new task. To enable proper instantiated reasoning, we prepare three general coarse-grained thought-templates for utilization. Based on the distilled task information $x_{d}$ , our BoT would automatically assign a suitable thought-template to the reasoning process. The detailed pre-defined thought-templates are included in Appendix B.3). ", "page_idx": 4}, {"type": "text", "text": "3.3 Buffer Manager ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We propose buffer-manager to summarize the high-level guidelines and thoughts that are gained from each problem-solving process. It can generalize each specific solution to more problems, storing the critical distilled knowledge in the form of thought-templates within the meta buffer. In contrast to methods that temporarily generate exemplars or instructions for each problem, our buffer-manager can ensure permanent advancements in accuracy, efficiency, and robustness for LLM-based reasoning. ", "page_idx": 4}, {"type": "text", "text": "Template Distillation To extract a general though-template, we propose a three-step approach: (1) Core task summarization: identifying and describing basic types and core challenges of problems; (2) Solution steps description: summarize the general steps for solving a problem; (3) General answering template: based on the above analysis, propose a solution template or approach that can be widely applied to similar problems. Additionally, to boost the generalization ability and stability of template distillation, we carefully design two types of in-context examples of how to generate thoughttemplate\u2014in-task and cross-task examples. Cross-task means we choose the template distilled from one task to tackle the problem of other tasks, such as addressing a mathematical problem with a code-related thought-template. The new template distilled from input task $x$ can be denoted as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nT_{n e w}=L L M_{\\mathrm{distill}}(x_{d},S_{x}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $L L M_{\\mathrm{distill}}$ is the LLM-based template distiller initialized with the following prompt: ", "page_idx": 4}, {"type": "text", "text": "Prompt for Template Distillation:   \nUser: [Problem Description] $^+$ [Solution Steps or Code]   \nTo extract and summarize the high-level paradigms and general approaches for solving such problems, please follow these steps in your response:   \n1. Core task summarization:   \nIdentify and describe the basic type and core challenges of the problem, such as classifying it as a mathematical problem (e.g., solving a quadratic equation), a data structure problem (e.g., array sorting), an algorithm problem (e.g., search algorithms), etc. And analyze the most efficient way to solve the problem.   \n2. Solution Steps Description:   \nOutline the general solution steps, including how to define the problem, determine variables, list key equations or constraints, choose appropriate solving strategies and methods, and how to verify the correctness of the results.   \n3. General Answer Template:   \nBased on the above analysis, propose a template or approach that can be widely applied to this type of problem, including possible variables, functions, class definitions, etc. If it is a programming problem, provide a set of base classes and interfaces that can be used to construct solutions to specific problems.   \nPlease ensure that your response is highly concise and structured, so that specific solutions can be transformed into generalizable methods.   \n[Optional] Here are some exemplars of the thought-template: (Choose cross-task or in-task exemplars based on the analysis of the Core task summarization.) ", "page_idx": 5}, {"type": "text", "text": "Dynamic Update of Meta-Buffer After template distillation, we need to consider whether the distilled template should be updated into the meta-buffer. If we initialize an empty meta-buffer or encounter a problem without a proper thought-template, the distilled thought-templates will be directly stored in the meta-buffer. If we solve problem with a retrieved thought-template, new insights may arise during the instantiation of a certain thought-template. Therefore, to avoid the redundancy of the meta-buffer while maintaining newly-generated informative thoughts, we will calculate the similarity between the embedding vectors of $D_{T_{n e w}}$ and $\\{D_{T_{i}}\\}_{i=0}^{n}$ and update the meta-buffer with the following rule: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{Max}(\\mathrm{Sim}(f(D_{T_{n e w}}),\\{f(D_{T_{i}})\\}_{i=0}^{n}))<\\delta.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Otherwise, it means the meta-buffer has already possessed the necessary knowledge to solve this task and does not need to perform the update. Our dynamic update strategy effectively reduces the computational burden of template retrieval while ensuring the lightweight property of our meta-buffer. We further conduct ablation study to analyze it in Section 4 and Appendix A. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets and Tasks To evaluate the efficacy of our proposed Buffer of Thoughts and compare with previous methods, we consider a diverse set of tasks and datasets that require varying degrees of mathematical and algorithmic reasoning, domain-specific knowledge, and literary creativity: (a). The Game of 24 from ToT [14], where the objective is to form an arithmetic expression that equals 24 using each of four given numbers exactly once; (b). Three BIG-Bench Hard (BBH) [35] tasks: Geometric Shapes, Multi-Step Arithmetic Two, and Word Sorting; (c). Three reasoning tasks directly obtained from the BIG-Bench suite [50]: Checkmate-in-One, Penguins\u2014where the task is to answer questions about penguins\u2019 attributes based on a given table and additional natural language information, and DateUnderstanding\u2014a task that involves inferring dates from natural language descriptions, performing arithmetic operations on dates, and utilizing global knowledge such as the number of days in February; (d). Python Programming Puzzles (P3) [51, 52], a collection of challenging programming puzzles written in Python with varying difficulty levels; (e). Multilingual Grade School Math (MGSM) [33], a multilingual version of the GSM8K dataset [53] featuring translations of a subset of examples into ten typologically diverse languages, including Bengali, Japanese, and Swahili; (f). Shakespearean Sonnet Writing from meta-prompting [15], a novel task where the goal is to write a sonnet following the strict rhyme scheme \"ABAB CDCD EFEF GG\" and incorporating three provided words verbatim. ", "page_idx": 5}, {"type": "table", "img_path": "ANO1i9JPtb/tmp/8229816d259f72dad9be95319edf0d2dc2ac1dff11d50f9e599d5cc59989cd5a.jpg", "table_caption": ["Table 1: Comparing BoT with previous methods across various tasks. We denote the best score in blue , and the second-best score in green . Our BoT significantly outperforms other methods on all tasks, especially on general reasoning problems. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Implementation and Baselines For the fair comparisons with previous methods, we use GPT-4 as the base model of our BoT, including the main experiment and the ablation study. We also use Llama3-8B and Llama3-70B in our analysis part on NVIDIA A100-PCIE-40GB GPU. We compare our Buffer of Thoughts with the following prompting methods: 1. Standard Prompting: This is our most basic baseline, where an LLM is asked to generate a response directly from the input query, without any specific guiding input-output examples or additional instructions beyond the task description included in the query. ", "page_idx": 6}, {"type": "text", "text": "2. Single-query Method: This includes Zero-shot CoT [8] and PAL [10], which use the LLM to analyze natural language problems and generate intermediate reasoning steps. We also include Expert Prompting [9], which creates an expert identity tailored to the specific context of the input query, and then integrates this expert profile into the input to generate a well-informed response. ", "page_idx": 6}, {"type": "text", "text": "3. Multi-query Method: This includes ToT [14] and GoT [17], which enable LLMs to make deliberate decisions by considering multiple reasoning paths and self-evaluating choices to determine the next course of action. These methods also allow for looking ahead or backtracking when necessary to make global decisions. Additionally, we include Meta Prompting [15], which employs an effective scaffolding technique designed to enhance the functionality of LLMs. ", "page_idx": 6}, {"type": "text", "text": "4.1 BoT Achieves Better Accuracy, Efficiency and Robustness ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Reasoning Accuracy As shown in Table 1, our BoT consistently outperforms all previous prompting methods across multiple kinds of challenging benchmarks, particularly demonstrated in complicated reasoning tasks such as Game of 24 and Checkmate-in-One. Taking GPT-4 as a baseline, our method achieves an astonishing $79.4\\%$ accuracy improvement in Game of 24, and compared to ToT, which has a good performance on this task, we also achieve an $8.4\\%$ accuracy improvement. What\u2019s more, compared to recent Meta-prompting method [15], we see significant accuracy improvements: ${}^{23\\%}$ on Game of 24, ${\\bf20}\\,\\%$ on Geometric Shapes and ${\\bf51\\%}$ on Checkmate-in-One. Existing methods need complex, iterative, and heuristic search strategies to address these problems on a case-by-case basis. Conversely, our BoT leverages the historical insights and informative guidelines from thought-templates, and further adaptively instantiate a more optimal reasoning structure for addressing these complex problems. ", "page_idx": 6}, {"type": "text", "text": "Reasoning Efficiency In addition to significant improvements in accuracy, as a multi-query method, our BoT can achieve comparable reasoning time to single-query method across various tasks, while being considerably less than conventional multi-query method like ToT [14] as shown in Figure 3. For example, in Game of 24, both single-query and multi-query methods necessitate iterative and heuristic searches to identify feasible solutions. This process is particularly time-consuming and inefficient, especially for the multi-query method, which involves conducting multi-query search and backtrace phases. In contrast, our BoT directly retrieves a thought-template in code format, thus a program is instantiated to traverse combinations of numbers and symbols, thereby eliminating the need to build the reasoning structure from scratch. This allows for solving the problem with just one query after invoking the problem-distiller, significantly reducing the time required for complex reasoning. Notably, our BoT requires only ${\\bf1}2\\%$ of the cost of multi-query methods (e.g., tree of thoughts and meta-prompting) on average. ", "page_idx": 6}, {"type": "image", "img_path": "ANO1i9JPtb/tmp/e30b6cc6d5c68f25c7ea524b654ed460d658013df33990d854858dfc97ba5376.jpg", "img_caption": ["Comparison of the inference time ", "Figure 3: Comparison of logarithmic inference time between our Buffer of Thoughts and GPT4 [3], GPT4 $^+$ CoT [8], Expert-prompting [9], PAL [10], ToT [14] across different benchmarks. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "ANO1i9JPtb/tmp/0a1ec2fa545d855646e3dd6ed34b67087ca7eabb7f4f092e06981c5fb71b63ff.jpg", "img_caption": ["Figure 4: Comparison of reasoning robustness between our Buffer of Thoughts and GPT4 [3], GPT4+CoT [8], Expert-prompting [9], PAL [10], ToT [14] across different benchmarks. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Reasoning Robustness To better evaluate our BoT, we devise a new evaluation metric: success rate, which is used to assess the reasoning robustness. We randomly sample 1000 examples from various benchmarks as a test subset and evaluate different methods on this subset. As shown in Figure 4, we repeat this evaluation process 10 times and take the average accuracy as the success rate of different methods on each benchmark. Compared with other methods, our BoT consistently maintains a higher success rate across various tasks, surpassing the second-best by $10\\%$ in average success rate. We attribute our outstanding robustness to the great generalization ability of our distilled thought-templates during reasoning across different tasks. By offering high-level thought from the suitable thought-templates, the stability of our method across different tasks is greatly enhanced. ", "page_idx": 7}, {"type": "text", "text": "5 Model Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Distribution Analysis of Thought-Templates As depicted in the left figure of Figure 5, we choose six different benchmarks, each sampled with 100 distinct tasks. We update the meta-buffer from scratch, and after completing all sampled tasks, we display the number of derived thought-templates. We can observe that our BoT generates a greater number of thought-templates in the MGSM tasks that contain more diverse scenarios. In tasks with relatively simple requirements, such as Checkmatein-One and Penguins, BoT produces more fixed thought-templates tailored for those specific issues. The distribution of templates indicates that our BoT can effectively discover appropriate thought templates for different benchmarks. ", "page_idx": 7}, {"type": "image", "img_path": "ANO1i9JPtb/tmp/4f4124e65ab35da6593c5569462691c76300508d16f91f71ff8bcd2b15e06185.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 5: Distribution Analysis of Thought-Templates and Time. Left: Distribution Analysis of Thought-Templates. Right: Time Distribution of BoT. ", "page_idx": 8}, {"type": "text", "text": "Distribution Analysis of Time Cost As illustrated in Figure 5, we measured the average time cost for each component of BoT\u2019s reasoning framework across different tasks. The time required for distilling task information and template retrieval is relatively short, whereas instantiated reasoning takes longer. Overall, considering the complexity of different components, our BoT achieves a relatively balanced distribution of time cost, demonstrating the efficiency of our BoT framework. ", "page_idx": 8}, {"type": "text", "text": "Better Trade-off between Model Size and Performance As depicted in Figure 6, on Game of 24, word list sorting and Checkmate-in-One, Llama3-8B and Llama-70B models [6] may result in poor outcomes. However, equipped with our BoT, both models demonstrate a substantial accuracy improvement. Notably, BoT+Llama3-8B has the potential to surpass single Llama3-70B model. Our BoT enables smaller models to exhibit the capabilities that approximate or even surpass larger models, significantly bridging the gap between their reasoning abilities. Furthermore, it greatly diminishes the inference cost required by large language models when tackling complex problems. ", "page_idx": 8}, {"type": "image", "img_path": "ANO1i9JPtb/tmp/c1302c20ac3b8ffe4e7a9f98a9af522bebe7f7d97d6b3dbd0c28188e1106e97c.jpg", "img_caption": ["Trade-off between model size and performance ", "Figure 6: We evaluate the trade-off between model size and performance with Llama3-8B and Llama3-70B models on three challenging benchmarks. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Quanlity of Automatically-Induced Template The success of the proposed approach critically depends on the quality of the automatically induced template. While this previous experiments has shown promising empirical performance on downstream tasks, it remains unclear how good the templates themselves are. Thus we make a comparison between the automatically generated task templates with manually prepared templates for more complex reasoning tasks on MATH dataset [54], with randomly sampled 500 problems. From the results, we can find that our automatic template boosts the reasoning ability of LLMs, demonstrating its generalization ability. ", "page_idx": 8}, {"type": "text", "text": "Impact of Buffer-Manager We further conduct ablation study on our buffer-manager, where we divide the entire process into four rounds. In each round, we randomly sample 50 questions from each benchmark and conduct reasoning. In the subsequent round, we continue to randomly sample another 50 questions from each benchmark. As depicted in Figure 7, with the increase of the number of rounds, the model with the buffer-manager continually expands the meta-buffer while also utilizing the thought-templates obtained from previously solved problems to help addressing subsequent similar problems. Therefore, we can observe that the accuracy of BoT steadily improves with each round. In contrast, the model without the buffer-manager fails to exhibit an upward trend. ", "page_idx": 8}, {"type": "table", "img_path": "ANO1i9JPtb/tmp/b5d2afbddee9cb0923fc08381cde8d9f247c5d6c22255d71a425321b21e33fad.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Additionally, we also demonstrate the superiority of our buffer-manager on the reasoning efficiency as depicted in Figure 10. when the number of rounds increases, the model with the buffer-manager will experience a continual improvement in reasoning efficiency. This is because, with the continual expansion of the meta-buffer, the likelihood of retrieving suitable thought-templates also increases. Consequently, models can avoid constructing reasoning structures from scratch, thereby enhancing the inference efficiency accordingly. More ablation study can be found in Appendix A. ", "page_idx": 9}, {"type": "image", "img_path": "ANO1i9JPtb/tmp/90ec5f273384d8abd834fb5dc551dfacefa07a78c3d56b4e27788ab79b269fca.jpg", "img_caption": ["Ablation study of buffer-manager -- Accuracy ", "Figure 7: We conduct ablation study on buffer-manager regarding reasoning accuracy across four tasks, employing Llama3-70B and GPT-4 as the base models. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduce Buffer of Thoughts, a novel beffered reasoning framework that employs LLMs to utilize pre-accumulated experiences and methodologies from prior tasks for progressively raising the LLM\u2019s reasoning capacity. our BoT brings out a set of future directions: (1). integrating external resources with BoT to build a open-domain system like agent models [55, 56]. (2). making the distillation of thought-templates optimizable, which may significantly enhance their template qualities for more complex tasks. (3). incorporating BoT into LLM training for eliciting more fine-grained and accurate reasoning process, like SuperCorrect [57]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by National Natural Science Foundation of China (U23B2048, U22B2037), Beijing Municipal Science and Technology Project (Z231100010323002), research grant No. SH2024JK29, the Fund of Kunpeng and Ascend Center of Excellence (Peking University), and Highperformance Computing Platform of Peking University. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., \u201cLanguage models are few-shot learners,\u201d Advances in neural information processing systems, vol. 33, pp. 1877\u20131901, 2020. [2] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al., \u201cPalm 2 technical report,\u201d arXiv preprint arXiv:2305.10403, 2023. [3] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al., \u201cGpt-4 technical report,\u201d arXiv preprint arXiv:2303.08774, 2023.   \n[4] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang, \u201cGlm: General language model pretraining with autoregressive blank infilling,\u201d in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 320\u2013335, 2022. [5] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al., \u201cMixtral of experts,\u201d arXiv preprint arXiv:2401.04088, 2024. [6] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, et al., \u201cLlama: Open and efficient foundation language models,\u201d arXiv preprint arXiv:2302.13971, 2023. [7] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., \u201cLlama 2: Open foundation and fine-tuned chat models,\u201d arXiv preprint arXiv:2307.09288, 2023. [8] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al., \u201cChain-of-thought prompting elicits reasoning in large language models,\u201d Advances in neural information processing systems, vol. 35, pp. 24824\u201324837, 2022.   \n[9] B. Xu, A. Yang, J. Lin, Q. Wang, C. Zhou, Y. Zhang, and Z. Mao, \u201cExpertprompting: Instructing large language models to be distinguished experts,\u201d arXiv preprint arXiv:2305.14688, 2023.   \n[10] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig, \u201cPal: Program-aided language models,\u201d in International Conference on Machine Learning, pp. 10764\u201310799, PMLR, 2023.   \n[11] X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, S. Narang, A. Chowdhery, and D. Zhou, \u201cSelfconsistency improves chain of thought reasoning in language models,\u201d in The Eleventh International Conference on Learning Representations, 2022.   \n[12] M. Yasunaga, X. Chen, Y. Li, P. Pasupat, J. Leskovec, P. Liang, E. H. Chi, and D. Zhou, \u201cLarge language models as analogical reasoners,\u201d International Conference on Learning Representations, 2024.   \n[13] Z. Zhang, A. Zhang, M. Li, and A. Smola, \u201cAutomatic chain of thought prompting in large language models,\u201d in The Eleventh International Conference on Learning Representations, 2022.   \n[14] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Grifftihs, Y. Cao, and K. Narasimhan, \u201cTree of thoughts: Deliberate problem solving with large language models,\u201d Advances in Neural Information Processing Systems, vol. 36, 2024.   \n[15] M. Suzgun and A. T. Kalai, \u201cMeta-prompting: Enhancing language models with task-agnostic scaffolding,\u201d arXiv preprint arXiv:2401.12954, 2024.   \n[16] D. Zhou, N. Sch\u00e4rli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, C. Cui, O. Bousquet, Q. V. Le, et al., \u201cLeast-to-most prompting enables complex reasoning in large language models,\u201d in The Eleventh International Conference on Learning Representations, 2022.   \n[17] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Podstawski, L. Gianinazzi, J. Gajda, T. Lehmann, H. Niewiadomski, P. Nyczyk, et al., \u201cGraph of thoughts: Solving elaborate problems with large language models,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, pp. 17682\u201317690, 2024.   \n[18] A. Asai, S. Min, Z. Zhong, and D. Chen, \u201cRetrieval-based language models and applications,\u201d in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts), pp. 41\u201346, 2023.   \n[19] G. Mialon, R. Dessi, M. Lomeli, C. Nalmpantis, R. Pasunuru, R. Raileanu, B. Roziere, T. Schick, J. Dwivedi-Yu, A. Celikyilmaz, et al., \u201cAugmented language models: a survey,\u201d Transactions on Machine Learning Research, 2023.   \n[20] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettlemoyer, and W.-t. Yih, \u201cReplug: Retrieval-augmented black-box language models,\u201d arXiv preprint arXiv:2301.12652, 2023.   \n[21] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, and H. Wang, \u201cRetrieval-augmented generation for large language models: A survey,\u201d arXiv preprint arXiv:2312.10997, 2023.   \n[22] P. Zhao, H. Zhang, Q. Yu, Z. Wang, Y. Geng, F. Fu, L. Yang, W. Zhang, and B. Cui, \u201cRetrieval-augmented generation for ai-generated content: A survey,\u201d arXiv preprint arXiv:2402.19473, 2024.   \n[23] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark, et al., \u201cImproving language models by retrieving from trillions of tokens,\u201d in International conference on machine learning, pp. 2206\u20132240, PMLR, 2022.   \n[24] M. Yasunaga, A. Aghajanyan, W. Shi, R. James, J. Leskovec, P. Liang, M. Lewis, L. Zettlemoyer, and W.-T. Yih, \u201cRetrieval-augmented multimodal language modeling,\u201d in International Conference on Machine Learning, pp. 39755\u201339769, PMLR, 2023.   \n[25] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick, J. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, \u201cAtlas: Few-shot learning with retrieval augmented language models,\u201d Journal of Machine Learning Research, vol. 24, no. 251, pp. 1\u201343, 2023.   \n[26] Z. Wang, W. Nie, Z. Qiao, C. Xiao, R. Baraniuk, and A. Anandkumar, \u201cRetrieval-based controllable molecule generation,\u201d in The Eleventh International Conference on Learning Representations, 2022.   \n[27] L. Yang, Z. Huang, X. Zhou, M. Xu, W. Zhang, Y. Wang, X. Zheng, W. Yang, R. O. Dror, S. Hong, et al., \u201cPrompt-based 3d molecular diffusion models for structure-based drug design,\u201d 2023.   \n[28] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, \u201cLarge language models are zero-shot reasoners,\u201d Advances in neural information processing systems, vol. 35, pp. 22199\u201322213, 2022.   \n[29] O. Press, M. Zhang, S. Min, L. Schmidt, N. A. Smith, and M. Lewis, \u201cMeasuring and narrowing the compositionality gap in language models,\u201d in Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 5687\u20135711, 2023.   \n[30] S. Arora, A. Narayan, M. F. Chen, L. Orr, N. Guha, K. Bhatia, I. Chami, and C. Re, \u201cAsk me anything: A simple strategy for prompting language models,\u201d in The Eleventh International Conference on Learning Representations, 2022.   \n[31] T. Khot, H. Trivedi, M. Finlayson, Y. Fu, K. Richardson, P. Clark, and A. Sabharwal, \u201cDecomposed prompting: A modular approach for solving complex tasks,\u201d in The Eleventh International Conference on Learning Representations, 2022.   \n[32] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, et al., \u201cEmergent abilities of large language models,\u201d Transactions on Machine Learning Research, 2022.   \n[33] F. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W. Chung, Y. Tay, S. Ruder, D. Zhou, et al., \u201cLanguage models are multilingual chain-of-thought reasoners,\u201d in The Eleventh International Conference on Learning Representations, 2022.   \n[34] Y. Fu, H. Peng, A. Sabharwal, P. Clark, and T. Khot, \u201cComplexity-based prompting for multi-step reasoning,\u201d in The Eleventh International Conference on Learning Representations, 2022.   \n[35] M. Suzgun, N. Scales, N. Sch\u00e4rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. Le, E. Chi, D. Zhou, et al., \u201cChallenging big-bench tasks and whether chain-of-thought can solve them,\u201d in Findings of the Association for Computational Linguistics: ACL 2023, pp. 13003\u201313051, 2023.   \n[36] H. S. Zheng, S. Mishra, X. Chen, H.-T. Cheng, E. H. Chi, Q. V. Le, and D. Zhou, \u201cTake a step back: Evoking reasoning via abstraction in large language models,\u201d arXiv preprint arXiv:2310.06117, 2023.   \n[37] P. Zhou, J. Pujara, X. Ren, X. Chen, H.-T. Cheng, Q. V. Le, E. H. Chi, D. Zhou, S. Mishra, and H. S. Zheng, \u201cSelf-discover: Large language models self-compose reasoning structures,\u201d arXiv preprint arXiv:2402.03620, 2024.   \n[38] W. Chen, X. Ma, X. Wang, and W. W. Cohen, \u201cProgram of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks,\u201d Transactions on Machine Learning Research, 2023.   \n[39] X. Ning, Z. Lin, Z. Zhou, Z. Wang, H. Yang, and Y. Wang, \u201cSkeleton-of-thought: Large language models can do parallel decoding,\u201d in The Twelfth International Conference on Learning Representations, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[40] Y. Zhang, \u201cMeta prompting for agi systems,\u201d arXiv preprint arXiv:2311.11482, 2023. ", "page_idx": 12}, {"type": "text", "text": "[41] J. Chen, R. Xu, Z. Fu, W. Shi, Z. Li, X. Zhang, C. Sun, L. Li, Y. Xiao, and H. Zhou, \u201cE-kar: A benchmark for rationalizing natural language analogical reasoning,\u201d in Findings of the Association for Computational Linguistics: ACL 2022, pp. 3941\u20133955, 2022.   \n[42] O. Sultan and D. Shahaf, \u201cLife is a circus and we are the clowns: Automatically finding analogies between situations and processes,\u201d in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 3547\u20133562, 2022.   \n[43] N. Zhang, L. Li, X. Chen, X. Liang, S. Deng, and H. Chen, \u201cMultimodal analogical reasoning over knowledge graphs,\u201d in The Eleventh International Conference on Learning Representations, 2022.   \n[44] B. Bhavya, J. Xiong, and C. Zhai, \u201cAnalogy generation by prompting large language models: A case study of instructgpt,\u201d in Proceedings of the 15th International Conference on Natural Language Generation, pp. 298\u2013312, 2022.   \n[45] B. Bhavya, J. Xiong, and C. Zhai, \u201cCam: A large language model-based creative analogy mining framework,\u201d in Proceedings of the ACM Web Conference 2023, pp. 3903\u20133914, 2023.   \n[46] Z. Zhang, A. Zhang, M. Li, and A. Smola, \u201cAutomatic chain of thought prompting in large language models,\u201d in The Eleventh International Conference on Learning Representations, 2022.   \n[47] T. Webb, K. J. Holyoak, and H. Lu, \u201cEmergent analogical reasoning in large language models,\u201d Nature Human Behaviour, vol. 7, no. 9, pp. 1526\u20131541, 2023.   \n[48] J. Yu, R. He, and Z. Ying, \u201cThought propagation: An analogical approach to complex reasoning with large language models,\u201d in International Conference on Learning Representations, 2024.   \n[49] T. Feng, P. Han, G. Lin, G. Liu, and J. You, \u201cThought-retriever: Don\u2019t just retrieve raw data, retrieve thoughts,\u201d in ICLR 2024 Workshop: How Far Are We From AGI.   \n[50] B. bench authors, \u201cBeyond the imitation game: Quantifying and extrapolating the capabilities of language models,\u201d Transactions on Machine Learning Research, 2023.   \n[51] T. Schuster, A. Kalyan, A. Polozov, and A. T. Kalai, \u201cProgramming puzzles,\u201d in Thirty-ffith Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021.   \n[52] A. T. K. Patrick Haluptzok, Matthew Bowers, \u201cLanguage models can teach themselves to program better,\u201d in Eleventh International Conference on Learning Representations (ICLR), 2023.   \n[53] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman, \u201cTraining verifiers to solve math word problems,\u201d arXiv preprint arXiv:2110.14168, 2021.   \n[54] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt, \u201cMeasuring mathematical problem solving with the math dataset,\u201d arXiv preprint arXiv:2103.03874, 2021.   \n[55] G. Chen, S. Dong, Y. Shu, G. Zhang, J. Sesay, B. F. Karlsson, J. Fu, and Y. Shi, \u201cAutoagents: A framework for automatic agent generation,\u201d arXiv preprint arXiv:2309.17288, 2023.   \n[56] Q. Wu, G. Bansal, J. Zhang, Y. Wu, S. Zhang, E. Zhu, B. Li, L. Jiang, X. Zhang, and C. Wang, \u201cAutogen: Enabling next-gen llm applications via multi-agent conversation framework,\u201d arXiv preprint arXiv:2308.08155, 2023.   \n[57] L. Yang, Z. Yu, T. Zhang, M. Xu, J. E. Gonzalez, B. Cui, and S. Yan, \u201cSupercorrect: Supervising and correcting language models with error-driven insights,\u201d arXiv preprint arXiv:2410.09008, 2024.   \n[58] Y. Xu, H. Cao, W. Du, and W. Wang, \u201cA survey of cross-lingual sentiment analysis: Methodologies, models and evaluations,\u201d Data Science and Engineering, vol. 7, no. 3, pp. 279\u2013299, 2022.   \n[59] S. R. Khope and S. Elias, \u201cCritical correlation of predictors for an efficient risk prediction framework of icu patient using correlation and transformation of mimic-iii dataset,\u201d Data Science and Engineering, vol. 7, no. 1, pp. 71\u201386, 2022.   \n[60] L. Guo, D. Chen, and K. Jia, \u201cKnowledge transferred adaptive filter pruning for cnn compression and acceleration,\u201d Science China. Information Sciences, vol. 65, no. 12, p. 229101, 2022.   \n[61] Z. Zhang, Y. Zhang, D. Guo, S. Zhao, and X. Zhu, \u201cCommunication-efficient federated continual learning for distributed learning system with non-iid data,\u201d Science China Information Sciences, vol. 66, no. 2, p. 122102, 2023. ", "page_idx": 12}, {"type": "text", "text": "A More Ablation Studies ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Impact of Problem-Distiller ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "As illustrated in Figure 8, when the problem-distiller is disabled, both Llama3-70B and GPT-4 experience a certain degree of accuracy decline. More complex problems, such as Game of 24 and Checkmate-in-One, show a more significant accuracy reduction, whereas relatively simpler problems like word list sorting and MGSM exhibit smaller decreases. This is because LLMs can more easily extract key information in simpler tasks, making the impact of the problem-distiller less noticeable. In contrast, extracting key information and potential constraints in complex problems is more challenging, making the role of our problem-distiller more prominent, thereby explaining the differences depicted in the figure. ", "page_idx": 13}, {"type": "image", "img_path": "ANO1i9JPtb/tmp/2c2b0d174d94f323367cb0e6531075f08c6cd8bf8e0b2ed52abd8487e103a371.jpg", "img_caption": ["Ablation study of problem-distiller ", "Figure 8: We conduct ablation study on problem-distiller across four benchmarks, employing Llama3- 70B and GPT-4 as the base models. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.2 Impact of Meta-Buffer ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "As illustrated in Figure 9, when the meta-buffer is disabled, both Llama3-70B and GPT-4 models exhibit a noticeable decline in performance, particularly in benchmarks requiring complex reasoning, such as Game of 24 and Checkmate-in-One. This further underscores the superiority of our metabuffer in addressing complex problems. We would extend such mechanism to more scenarios [57\u201361]. ", "page_idx": 13}, {"type": "image", "img_path": "ANO1i9JPtb/tmp/980b88dd9fb6849d72c29d9d78324cede026d1dbd6c1171e336387e37aa72535.jpg", "img_caption": ["Figure 9: We conduct ablation study on meta-buffer across four benchmarks, employing Llama3-70B and GPT-4 as the base models. "], "img_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "ANO1i9JPtb/tmp/20e660595bfc4081294b29b4365478b315b4a5f435d7d0332264ab23525eb1f0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 10: We conduct ablation study on buffer-manager regarding reasoning efficiency across four tasks, employing Llama3-70B and GPT-4 as the base models. ", "page_idx": 14}, {"type": "text", "text": "B Additional Method Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Six Kinds of Detailed Thought-Templates ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1. Text Comprehension ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Task Description: ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The task involves analyzing a table with various attributes of penguins, such as name, age, height, and weight, and answering questions about these attributes. The table may be updated with new entries, and additional context or comparisons may be provided in natural language. ", "page_idx": 14}, {"type": "text", "text": "Solution Description: ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To accurately answer questions about the penguins\u2019 attributes, one must be able to interpret the data presented in tabular form, understand any additional information provided in natural language, and apply logical reasoning to identify the correct attribute based on the question asked. ", "page_idx": 14}, {"type": "text", "text": "Thought Template: ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Step 1: Parse the initial table, extracting the header information and each penguin\u2019s attributes into a structured format (e.g., a list of dictionaries).   \nStep 2: Read and integrate any additional natural language information that updates or adds to the table, ensuring the data remains consistent.   \nStep 3: Identify the attribute in question (e.g., oldest penguin, heaviest penguin) and the corresponding column in the table.   \nStep 4: Apply logical reasoning to compare the relevant attribute across all entries to find the correct answer (e.g., the highest age for the oldest penguin).   \nStep 5: Select the answer from the provided options that matches the result of the logical comparison. ", "page_idx": 14}, {"type": "text", "text": "2. Creative Language Generation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Task Description: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The task is to generate a sonnet that adheres to the traditional English sonnet rhyme scheme of \"ABAB CDCD EFEF GG\" and includes three specific words verbatim in the text. ", "page_idx": 15}, {"type": "text", "text": "Solution Description: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Writing a sonnet involves crafting 14 lines of poetry that follow a specific rhyme pattern. The lines are typically in iambic pentameter, though flexibility in rhythm can be allowed for creative reasons. The given rhyme scheme dictates the end sounds of each line, ensuring a structured poetic form. Incorporating the three provided words verbatim requires strategic placement within the lines to maintain the poem\u2019s coherence and thematic unity. ", "page_idx": 15}, {"type": "text", "text": "Thought Template: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Step 1: Identify the three words that must be included in the sonnet. ", "page_idx": 15}, {"type": "text", "text": "Step 2: Understand the rhyme scheme \"ABAB CDCD EFEF GG\" and prepare a list of rhyming words that could be used. ", "page_idx": 15}, {"type": "text", "text": "Step 3: Develop a theme or story for the sonnet that can naturally incorporate the three provided words. ", "page_idx": 15}, {"type": "text", "text": "Step 4: Begin drafting the sonnet by writing the first quatrain (four lines) following the \"ABAB\" rhyme scheme, ensuring one or more of the provided words are included. ", "page_idx": 15}, {"type": "text", "text": "Step 5: Continue with the second quatrain \"CDCD,\" the third quatrain \"EFEF,\" and finally the closing couplet \"GG,\" each time incorporating the provided words as needed. ", "page_idx": 15}, {"type": "text", "text": "Step 6: Review the sonnet for coherence, flow, and adherence to the rhyme scheme, making adjustments as necessary. ", "page_idx": 15}, {"type": "text", "text": "3. Common Sense Reasoning ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Task Description: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Given a specific date and an event, such as a holiday or historical event, determine the following date. ", "page_idx": 15}, {"type": "text", "text": "Solution Description: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To determine the next date, we need to consider the structure of the calendar, the number of days in each month, and whether it\u2019s a leap year. Typically, the number of days in a month is fixed, except February may vary due to leap years. The next day in a year is usually the date increased by one day unless it\u2019s the end of the month, then the next day will be the first day of the following month. For the end of the year, the next day will be January 1st of the following year. ", "page_idx": 15}, {"type": "text", "text": "Thought Template: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Step 1: Identify the given date\u2019s month and day number.   \nStep 2: Check if it\u2019s the end of the month; if so, confirm the start date of the next month.   \nStep 3: If it\u2019s not the end of the month, simply add one to the day number. ", "page_idx": 15}, {"type": "text", "text": "Step 4: Pay special attention to the end of the year, ensuring the year increments. ", "page_idx": 15}, {"type": "text", "text": "4. Code Programming ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Task Description:   \nWhen given a list of numbers, try to utilize 4 basic mathematical operations $(+-^{*}/)$ to get a target number.   \nThought Template: Listing 1: Python template   \nfrom itertools import permutations , product   \ndef perform_operation (a, b, operation): # Define the operation logic (e.g., addition , subtraction , etc.). pass   \ndef evaluate_sequence (sequence , operations): # Apply operations to the sequence and check if the result meets the criteria. pass   \ndef generate_combinations (elements , operations): # Generate all possible combinations of elements and operations. pass   \ndef format_solution (sequence , operations): # Format the sequence and operations into a human -readable string. pass   \ndef find_solution(input_elements , target_result ): # Data Input Handling # Validate and preprocess input data if necessary. # Core Algorithm Logic for sequence in permutations( input_elements ): for operation_combination in generate_combinations ( sequence , operations): try: if evaluate_sequence (sequence , operation_combination ) $==$ target_result : # Data Output Formatting return format_solution (sequence , operation_combination ) except Exception as e: # Error Handling # Handle specific exceptions that may occur during evaluation. continue # If no solution is found after all iterations , return a default message. # return No solution found message return   \n# Example usage:   \ninput_elements $=$ [1, 7, 10, 3]   \ntarget_result $=$ 24   \nprint(find_solution(input_elements , target_result )) ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "5. Application Scheduling ", "text_level": 1, "page_idx": 17}, {"type": "image", "img_path": "ANO1i9JPtb/tmp/a6204bf33d51afc00f94c395ee3d85e116d4b7da3a6cf476d0a3087f6b68ca29.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Task Description:   \nSolve an quadratic equation of the form $a x^{2}+b x+c=0$ considering any situations. Solution Description:   \nTo solve any quadratic equation of the form $a x^{2}+b x+c=0$ , we can follow a general approach based on the method described. Here is the structured template for solving such equations:   \nThought Template:   \nStep 1: Calculate the Discriminant   \n- Compute the discriminant $D$ using the formula $D=b^{2}-4a c$ .   \nStep 2: Determine the Nature of the Roots   \n- If $D>0$ , the equation has two distinct real roots.   \n- If $D=0$ , the equation has exactly one real root (also known as a repeated or double root). - If $D<0$ , the equation has two complex roots.   \nStep 3: Compute the Roots - For D \u22650, calculate the roots using the formula x = \u2212b2\u00b1aD. - For $D<0$ \u221a, calculate the real and imaginary parts of the complex roots using the formula x = 2\u2212ab \u00b1 2\u2212aD i, where i is the imaginary unit. ", "page_idx": 18}, {"type": "text", "text": "B.2 Prompt for Problem Distiller ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "[Problem Distiller]: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "As a highly professional and intelligent expert in information distillation, you excel at extracting essential information to solve problems from user input queries. You adeptly transform this extracted information into a suitable format based on the respective type of the issue. ", "page_idx": 18}, {"type": "text", "text": "Please categorize and extract the crucial information required to solve the problem from the user\u2019s input query, the distilled information should include. ", "page_idx": 18}, {"type": "image", "img_path": "ANO1i9JPtb/tmp/cde419fd640c4c2e27e9cccb701425efd9aa6ca6dd5d8fffbf59f4390e780fa1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Values and information of key variables extracted from user input, which will be handed over to the respective expert for task resolution, ensuring all essential information required to solve the problem is provided. ", "page_idx": 18}, {"type": "text", "text": "2. Restrictions: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The objective of the problem and corresponding constraints. 3. Distilled task: ", "page_idx": 18}, {"type": "text", "text": "Extend the problem based on 1 and 2, summarize a meta problem that can address the user query and handle more input and output variations. Incorporate the real-world scenario of the extended problem along with the types of key variables and information constraints from the original problem to restrict the key variables in the extended problem. After that, use the user query input key information as input to solve the problem as an example. ", "page_idx": 18}, {"type": "text", "text": "[Meta Reasoner]   \nYou are a Meta Reasoner who are extremely knowledgeable in all kinds of fields including Computer Science, Math, Physics, Literature, History, Chemistry, Logical reasoning, Culture, Language..... You are also able to find different high-level thought for different tasks. Here are three reasoning sturctures:   \ni) Prompt-based structure:   \nIt has a good performance when dealing with problems like Common Sense Reasoning, Application Scheduling   \nii) Procedure-based structure   \nIt has a good performance when dealing with creative tasks like Creative Language Generation, and Text Comprehension   \niii) Programming-based:   \nIt has a good performance when dealing with Mathematical Reasoning and Code Programming, it can also transform real-world problems into programming problem which could be solved efficiently.   \n(Reasoning instantiation)   \nYour task is:   \n1. Deliberately consider the context and the problem within the distilled respond from problem distiller and use your understanding of the question within the distilled respond to find a domain expert who are suitable to solve the problem.   \n2. Consider the distilled information, choose one reasoning structures for the problem. 3. If the thought-template is provided, directly follow the thought-template to instantiate for the given problem. ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Please refer to the abstract in the main paper. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: Please refer to the Section 6 in the paper. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Please refer to the Section 4 in the paper. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Please refer to the Section 4 in the paper. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Please refer to the Section 4 in the paper. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Error bars are not reported because it would be too computationally expensive. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Please refer to the Section 4 in the paper. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Please check out the paper. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: Please refer to the ?? in our paper. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 23}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We ensure all papers and codebases used or relevant to this work are properly cited. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]