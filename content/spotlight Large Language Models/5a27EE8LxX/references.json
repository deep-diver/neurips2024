{"references": [{"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper is foundational to the concept of safety alignment in LLMs, which is central to the paper's approach to toxicity detection."}, {"fullname_first_author": "Zi Lin", "paper_title": "ToxicChat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation", "publication_date": "2023-01-01", "reason": "This paper introduces the ToxicChat dataset, which is the main dataset used for evaluation in the paper, providing a benchmark for toxicity detection methods."}, {"fullname_first_author": "Hakan Inan", "paper_title": "Llama Guard: LLM-based input-output safeguard for human-AI conversations", "publication_date": "2023-12-01", "reason": "LlamaGuard is a state-of-the-art toxicity detector that the paper compares against, representing a strong baseline for performance comparison."}, {"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-01", "reason": "The GPT-4 model is used as the base language model in the paper's experiments, making this model's technical report a crucial reference."}, {"fullname_first_author": "Todor Markov", "paper_title": "A holistic approach to undesired content detection in the real world", "publication_date": "2023-01-01", "reason": "This paper provides a comprehensive evaluation dataset for toxicity detection, used to validate the effectiveness of the proposed method in a realistic setting."}]}