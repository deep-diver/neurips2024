[{"figure_path": "5a27EE8LxX/tables/tables_4_1.jpg", "caption": "Table 1: Effectiveness of the toy models", "description": "This table presents the performance of several toy models designed to detect toxic prompts.  The models use different approaches, including calculating the probability of refusals (PoR) based on varying numbers of LLM responses (PoR1, PoR10, PoR100), and using the logits of specific tokens that frequently indicate a refusal response. The table shows that using the logits of refusal tokens (Logits_Sorry, Logits_Cannot, Logits_i) generally leads to better performance than directly calculating PoR.  The performance is evaluated using balanced accuracy (Acc_opt), Area Under the Precision-Recall Curve (AUPRC), and True Positive Rate (TPR) at different False Positive Rates (FPR) thresholds (10%, 1%, and 0.1%). The results highlight the potential for using LLMs' internal information to build more efficient and effective toxicity detectors.", "section": "4 Toy models"}, {"figure_path": "5a27EE8LxX/tables/tables_6_1.jpg", "caption": "Table 2: Results on ToxicChat", "description": "This table presents the performance comparison of different toxicity detection methods on the ToxicChat dataset.  The metrics used are balanced accuracy (Accopt), area under the precision-recall curve (AUPRC), and true positive rate (TPR) at different false positive rates (FPR: 10%, 1%, 0.1%, 0.01%).  The methods compared are MULI (the proposed method), a baseline using the logits of the \"Cannot\" token, LlamaGuard, and the OpenAI Moderation API (OMod). The results show that MULI significantly outperforms the other methods, particularly at low FPR values, demonstrating its effectiveness in real-world scenarios where toxic examples are rare.", "section": "6.2 Main results"}, {"figure_path": "5a27EE8LxX/tables/tables_6_2.jpg", "caption": "Table 3: Results on LMSYS-Chat-1M", "description": "This table presents the performance comparison of different toxicity detection methods on the LMSYS-Chat-1M dataset.  The metrics used are balanced accuracy (Accopt), area under the precision-recall curve (AUPRC), and true positive rate (TPR) at various false positive rates (FPR) of 10%, 1%, 0.1%, and 0.01%. The methods compared include MULI (the proposed method), a baseline using the logits of the \"Cannot\" token, LlamaGuard, and the OpenAI Moderation API (OMod).  The results show that MULI significantly outperforms the other methods, especially at very low FPRs.", "section": "6.2 Main results"}, {"figure_path": "5a27EE8LxX/tables/tables_7_1.jpg", "caption": "Table 4: Cross-dataset performance", "description": "This table presents the results of the MULI model and baseline models when tested on datasets different from the training data. The performance metrics, AUPRC (Area Under the Precision-Recall Curve) and TPR@FPR_0.1% (True Positive Rate at a False Positive Rate of 0.1%), show how well the models generalize to unseen data.  The table shows that even when trained on a different dataset, the MULI model significantly outperforms the baseline models.", "section": "6.2 Main results"}, {"figure_path": "5a27EE8LxX/tables/tables_8_1.jpg", "caption": "Table 5: Results on OpenAI Moderation API Evaluation dataset", "description": "This table presents the results of various toxicity detection models evaluated on the OpenAI Moderation API Evaluation dataset.  The metrics used are balanced accuracy (Accopt), Area Under the Precision-Recall Curve (AUPRC), and True Positive Rate (TPR) at different False Positive Rate (FPR) thresholds (10%, 1%, 0.1%, and 0.01%). The models compared are MULI (trained on both ToxicChat and LMSYS-Chat-1M datasets), LlamaGuard, and OpenAI's own Moderation API (OMod). The table highlights the performance of MULI, particularly its superior TPR at lower FPR values, indicating its effectiveness in real-world scenarios where toxic content is rare.", "section": "6. Experiments"}, {"figure_path": "5a27EE8LxX/tables/tables_9_1.jpg", "caption": "Table 6: Ablation study", "description": "This table presents the results of an ablation study on different functions and regularization techniques used in the MULI model.  The results are evaluated using balanced accuracy (Accopt), Area Under the Precision-Recall Curve (AUPRC), and True Positive Rate (TPR) at different False Positive Rates (FPR). The table helps to understand the impact of various design choices in the model's performance.", "section": "6 Experiments"}, {"figure_path": "5a27EE8LxX/tables/tables_12_1.jpg", "caption": "Table 1: Effectiveness of the toy models", "description": "This table presents the performance of the two toy models proposed in the paper for toxicity detection. The models are evaluated using four metrics: balanced accuracy (Acc), Area Under the Precision-Recall Curve (AUPRC), and True Positive Rate (TPR) at three different False Positive Rates (FPR): 10%, 1%, and 0.1%.  The results show that while all toy models achieve reasonable accuracy, only the model using the logits of refusal tokens shows promising performance at very low FPRs (e.g., 0.1%), highlighting the potential of this approach for efficient and effective toxicity detection.", "section": "4 Toy models"}, {"figure_path": "5a27EE8LxX/tables/tables_13_1.jpg", "caption": "Table 2: Results on ToxicChat", "description": "This table presents the performance of different toxicity detection models on the ToxicChat dataset.  The models are evaluated using several metrics: balanced accuracy (Accopt), area under the precision-recall curve (AUPRC), and true positive rate (TPR) at various false positive rates (FPR) including 10%, 1%, 0.1%, and 0.01%. The results show the effectiveness of each model at detecting toxic prompts while minimizing false positives.  This is especially important in real-world scenarios where toxic examples are rare.", "section": "6.2 Main results"}, {"figure_path": "5a27EE8LxX/tables/tables_13_2.jpg", "caption": "Table 3: Results on LMSYS-Chat-1M", "description": "This table presents the performance comparison of different toxicity detection models on the LMSYS-Chat-1M dataset.  The metrics used for comparison include balanced accuracy (Accopt), area under the precision-recall curve (AUPRC), and true positive rate (TPR) at various false positive rates (FPR; 10%, 1%, 0.1%, 0.01%). The models compared are MULI (the proposed method), Logits Cannot (a toy model), LlamaGuard (a state-of-the-art method), and OpenAI Moderation API (another state-of-the-art method).  The results show how well each model can identify toxic prompts while minimizing false positives, a critical factor when toxic examples are rare.", "section": "6 Experiments"}, {"figure_path": "5a27EE8LxX/tables/tables_13_3.jpg", "caption": "Table S4: Rank of certain tokens among all SLR weights (%).", "description": "This table presents the rank of specific tokens (refusal and affirmative) within the weights of the Sparse Logistic Regression (SLR) model. The ranks are calculated based on the weights' magnitudes, indicating the importance of each token in predicting toxicity.  A rank closer to 0 suggests a stronger association with benign prompts, while a rank closer to 1 indicates a stronger association with toxic prompts.  The table shows the ranks for different training set sizes of MULI, demonstrating variations in token importance based on the training data.", "section": "A.6 Token Ranks"}]