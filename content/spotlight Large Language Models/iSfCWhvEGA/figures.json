[{"figure_path": "iSfCWhvEGA/figures/figures_0_1.jpg", "caption": "Figure 1: Learn-To-be-Efficient (LTE) performs efficiency-aware training to construct structured model contextual sparsity for fast inference. Activated neurons are in orange.", "description": "The figure illustrates the Learn-To-be-Efficient (LTE) method.  The left side shows a standard Feed Forward Network (FFN) with its activations represented by orange squares. The activations are then passed through the LTE training process (represented by a circular diagram with interconnected nodes), which optimizes for structured sparsity. The right side shows the resulting FFN after LTE training; the activations are still represented by orange squares but are now more sparsely distributed, indicating improved efficiency.  The overall process aims to reduce computational cost by activating fewer neurons during inference without significantly sacrificing performance.", "section": "1 Introduction"}, {"figure_path": "iSfCWhvEGA/figures/figures_2_1.jpg", "caption": "Figure 2: Models trained with noisy top-K Softmax routing experience severe accuracy drops, even at very low levels of sparsity.", "description": "This figure shows the accuracy drop of two models, MRPC (ROBERTa-base) and QNLI (ROBERTa-base), when using noisy top-K Softmax routing at different sparsity levels.  The accuracy of both models significantly decreases as sparsity increases, even at very low sparsity levels. This demonstrates a limitation of the noisy top-K Softmax routing method for achieving sparsity in language models.", "section": "3.2 Limitations of Noisy Top-K Softmax Routing"}, {"figure_path": "iSfCWhvEGA/figures/figures_4_1.jpg", "caption": "Figure 3: Expert distribution w/ and w/o separability loss. The expert scores in the model trained with the separability loss are much more separable than the model trained with-out the separability loss.", "description": "This figure compares the distribution of expert scores in two models: one trained without the separability loss and another trained with it.  The separability loss is a component of the LTE training algorithm, designed to improve the selection of experts by making their scores more distinct.  The graph clearly shows that adding the separability loss leads to a more clearly separated distribution of expert scores, making it easier to choose experts based on a simple threshold.", "section": "4.2.2 Two-stage LTE Training"}, {"figure_path": "iSfCWhvEGA/figures/figures_5_1.jpg", "caption": "Figure 4: Structural sparsity provided by LTE in FFN layer. After selecting neurons, unselected rows and columns won't be loaded and used.", "description": "This figure illustrates how LTE achieves structured sparsity in the feed-forward neural network (FFN) layers of a transformer.  The left side shows the input vector x multiplied by the weight matrix W<sub>up</sub>.  LTE selects a subset of neurons (shown in orange), creating a sparse representation. The matrix multiplication is then performed only on the selected neurons, which drastically reduces the computational load. The result of this sparse multiplication is then multiplied by the weight matrix W<sub>down</sub>, again with the same sparsity pattern, producing a final output vector x<sub>out</sub>. This structured sparsity is key to LTE's efficiency gains.", "section": "4.3 Hardware-efficient Implementation"}, {"figure_path": "iSfCWhvEGA/figures/figures_6_1.jpg", "caption": "Figure 5: LTE consistently outperforms other baselines on 4 NLU datasets.", "description": "This figure displays the performance of LTE and baseline models on four different natural language understanding (NLU) tasks from the GLUE benchmark.  The x-axis represents sparsity levels, ranging from 0 to 1 (100%). The y-axis shows accuracy.  LTE consistently achieves higher accuracy than the other methods (Deja Vu, ReLU + MoEfication) at various sparsity levels, demonstrating its effectiveness in maintaining good performance even with high sparsity.", "section": "5.2 End-to-end Performance Comparison"}, {"figure_path": "iSfCWhvEGA/figures/figures_7_1.jpg", "caption": "Figure 6: Performance comparison across three NLG datasets (each column). We compare LTE (Ours) with other baselines on two decoder-based models: GPT2-M and LLaMA-7B (each row).", "description": "This figure compares the performance of LTE against other baselines (Full Model, Deja Vu, MoEfication, and R-LLaMA+MoEfication) across three different NLG (Natural Language Generation) datasets (XSum, E2E, and WikiText-103).  The comparison is shown for two different decoder-based models: GPT2-Medium and LLaMA-7B.  Each subfigure shows the performance metric (ROUGE-L for XSum and E2E, Perplexity for WikiText-103) plotted against sparsity levels.  This visualization helps to understand how the different methods perform at various levels of sparsity and which model is more resilient to sparsity.", "section": "5.2 End-to-end Performance Comparison"}, {"figure_path": "iSfCWhvEGA/figures/figures_7_2.jpg", "caption": "Figure 7: LTE outperforms other baselines on 5-shot MMLU benchmark.", "description": "This figure shows the performance comparison of LTE with other baselines on a 5-shot MMLU benchmark. The x-axis represents the sparsity, and the y-axis represents the MMLU accuracy (5-shot).  The results demonstrate that LTE consistently achieves higher accuracy than other methods across different sparsity levels. This highlights LTE's effectiveness in improving model performance without significantly impacting efficiency.", "section": "5.2 End-to-end Performance Comparison"}, {"figure_path": "iSfCWhvEGA/figures/figures_7_3.jpg", "caption": "Figure 8: Wall-clock time latency comparison on dense FFN blocks and our custom Triton kernel (Left). End-to-end generation latency comparison between dense model and LTE with 50% sparsity (Right).", "description": "The figure shows two subfigures. The left one presents a line graph comparing the FFN inference latency of a dense model and a model with LTE using a custom Triton kernel and PyTorch indexing across different sparsity levels. The right one displays a bar chart comparing the end-to-end generation latency of the dense model and the LTE model (at 50% sparsity) for various sequence lengths.", "section": "5.3 Translate Sparsity to Wall-clock Time Speedup"}, {"figure_path": "iSfCWhvEGA/figures/figures_8_1.jpg", "caption": "Figure 9: Performance comparison between different experts grouping algorithms.", "description": "The figure shows a comparison of the performance of different expert grouping algorithms used in the Learn-To-be-Efficient (LTE) model.  The algorithms compared are random grouping, co-activation graph split, and K-means clustering.  The performance metric is perplexity on the WikiText-103 dataset, and the x-axis represents the sparsity of the model.  The results indicate that co-activation graph split and K-means clustering achieve similar and better performance compared to random grouping.", "section": "5.5 Ablation Study"}, {"figure_path": "iSfCWhvEGA/figures/figures_9_1.jpg", "caption": "Figure 12: Performance comparison between random routers and routers trained with LTE. Models with LTE-trained routers consistently outperform models with random routers.", "description": "This figure compares the performance of models using randomly initialized routers versus routers trained with the Learn-To-be-Efficient (LTE) algorithm.  The left panel shows accuracy on the MRPC dataset using a RoBERTa-base model, while the right panel displays perplexity on the WikiText-103 dataset using a GPT2-Medium model.  Both panels demonstrate that models with LTE-trained routers consistently achieve better results across various sparsity levels, highlighting the effectiveness of LTE in training stable and efficient routers.", "section": "5.2 End-to-end Performance Comparison"}, {"figure_path": "iSfCWhvEGA/figures/figures_9_2.jpg", "caption": "Figure 6: Performance comparison across three NLG datasets (each column). We compare LTE (Ours) with other baselines on two decoder-based models: GPT2-M and LLaMA-7B (each row).", "description": "This figure compares the performance of LTE with other baselines on three different NLG (Natural Language Generation) tasks using two different decoder-based language models, GPT2-Medium and LLaMA-7B.  Each column represents a different NLG task (XSum, E2E, and WikiText-103), while each row shows the results for a different language model. The x-axis shows the FFN (Feed-Forward Network) sparsity, and the y-axis shows the performance metrics (ROUGE-L for XSum and E2E, and perplexity for WikiText-103).  The figure visually demonstrates the performance of LTE across various sparsity levels compared to other methods (Full Model, Deja Vu, MoEfication, and R-LLaMA+MoEfication).", "section": "5.2 End-to-end Performance Comparison"}, {"figure_path": "iSfCWhvEGA/figures/figures_15_1.jpg", "caption": "Figure 5: LTE consistently outperforms other baselines on 4 NLU datasets.", "description": "This figure shows the performance comparison of LTE against other baselines (Deja Vu and ReLU + MoEfication) on four different Natural Language Understanding (NLU) datasets from the GLUE benchmark.  The x-axis represents the sparsity level, and the y-axis represents the accuracy.  The results demonstrate that LTE consistently achieves higher accuracy than the baselines across various sparsity levels, indicating its effectiveness in improving model efficiency without significant performance loss.", "section": "5.2 End-to-end Performance Comparison"}, {"figure_path": "iSfCWhvEGA/figures/figures_15_2.jpg", "caption": "Figure 15: Union sparsity changes w.r.t. batch size.", "description": "This figure shows how union sparsity changes with respect to batch size for different values of efficiency loss (\u03b7). Union sparsity represents the proportion of unique neurons activated across multiple input batches. As the batch size increases, more neurons tend to be activated, leading to a decrease in union sparsity.  The figure demonstrates that the increase in activated neurons is not linear with the batch size.  This suggests a non-uniform distribution of parameter access across different inputs. The non-linear relationship implies that even with larger batch sizes, significant sparsity can still be achieved. This observation is relevant for the application of LTE in high-throughput scenarios, where batching inputs activating similar neurons might help achieve a high union sparsity.", "section": "C.2 Further Analysis on Sparsity Provided by LTE"}, {"figure_path": "iSfCWhvEGA/figures/figures_15_3.jpg", "caption": "Figure 16: Sparsity across different layers. We present the sparsity of different layers in an encoder-based model (RoBERTa-base) for MRPC and a decoder-based model (GPT2-M) for WikiText-103. Different layers learn to have different sparsity, and RoBERTa-base and GPT2-M have different sparsity patterns across layers.", "description": "This figure shows the distribution of sparsity across different layers in two different models: RoBERTa-base (for MRPC task) and GPT2-M (for WikiText-103 task).  It highlights that LTE doesn't apply a uniform sparsity level across all layers but rather adapts it depending on the layer's importance for the task.  The pattern of sparsity distribution also differs between the two model architectures, reflecting their distinct designs and how information is processed through the layers.", "section": "5.3 Translate Sparsity to Wall-clock Time Speedup"}]