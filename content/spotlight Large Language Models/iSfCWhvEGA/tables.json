[{"figure_path": "iSfCWhvEGA/tables/tables_6_1.jpg", "caption": "Table 1: GFLOPs per token for LLaMA-7B on different datasets with permitting quality drops. FLOPs of routers are also included in our method. N/A stands for the method failing to achieve the expected performance.", "description": "This table compares the GFLOPs per token for different methods (Full, Deja Vu, MoEfication, R-LLaMA+MoE, and LTE) on three NLG datasets (XSum, E2E, Wiki) using the LLaMA-7B model.  The results show the computational efficiency gains achieved by each method, with LTE demonstrating significant improvements in reducing FLOPs while maintaining acceptable performance.  Note that N/A indicates methods failed to reach expected performance levels.", "section": "5.2 End-to-end Performance Comparison"}, {"figure_path": "iSfCWhvEGA/tables/tables_8_1.jpg", "caption": "Table 2: Performance comparison with model pruning. We apply Wanda [36] on WikiText103 fine-tuned LLaMA-7B with the author-implemented code. Besides using C4 as the calibration data suggested by the Wanda paper, we also try to use WikiText for calibration to improve this baseline. The sparsity reported here is the overall sparsity of the entire model, which is different from the FFN sparsity. The evaluation results show that LTE achieves a lower perplexity than Wanda.", "description": "This table compares the performance of LTE and Wanda model pruning methods on the WikiText103 dataset.  It shows perplexity (PPL) results for both methods at varying sparsity levels (50% and 52%).  Different calibration data (C4 and WikiText) were used for Wanda, showing its impact on performance.  LTE consistently demonstrates lower perplexity than Wanda.  It highlights the difference between overall model sparsity and FFN layer sparsity.", "section": "5.4 Addtional Comparison"}, {"figure_path": "iSfCWhvEGA/tables/tables_16_1.jpg", "caption": "Table 3: ROBERTa-base hyperparameter for LTE training on GLUE dataset.", "description": "This table shows the hyperparameters used for training the ROBERTa-base model using the Learn-To-be-Efficient (LTE) algorithm on the GLUE benchmark dataset.  It lists the learning rate, training batch size, training epochs, weight decay, and warm-up ratio for both stages of the LTE training process (LTE-Stage 1 and LTE-Stage 2).  The values provided are either single values or a range of values indicating that multiple experiments were run with different settings to find the optimal hyperparameters.", "section": "5.1 Experimental Setting"}, {"figure_path": "iSfCWhvEGA/tables/tables_16_2.jpg", "caption": "Table 4: ROBERTa-large hyperparameter for LTE training on GLUE dataset.", "description": "This table shows the hyperparameters used for training the large version of RoBERTa model using the Learn-To-be-Efficient (LTE) algorithm on the GLUE benchmark dataset. It specifies the hyperparameters for both stages of the LTE training process: Stage 1 (model-router training) and Stage 2 (model adaptation).  For each stage, hyperparameters such as the learning rate, training batch size, training epochs, weight decay, and warm-up ratio are listed.  Braces {} indicate a range of values tested; the best performing value was selected. ", "section": "5.1 Experimental Setting"}, {"figure_path": "iSfCWhvEGA/tables/tables_16_3.jpg", "caption": "Table 5: GPT2-Medium hyperparameter for fine-tuning and LTE training on XSum, E2E, and WikiText-103 datasets. (Numbers in the bracket correspond to three datasets in order.)", "description": "This table shows the hyperparameters used for fine-tuning and training with LTE for the GPT2-Medium model on three different datasets: XSum, E2E, and WikiText-103.  It details the learning rate, training batch size, number of training epochs, weight decay, and warm-up ratio for each stage of the training process (fine-tuning, LTE Stage 1, and LTE Stage 2). Note that the number of training epochs varies depending on the dataset, reflecting the different training requirements of each dataset.", "section": "5.1 Experimental Setting"}, {"figure_path": "iSfCWhvEGA/tables/tables_16_4.jpg", "caption": "Table 6: LLaMA hyperparameter for fine-tuning and LTE training on XSum, E2E, and WikiText-103 datasets.", "description": "This table shows the hyperparameters used for fine-tuning and LTE training (both stage 1 and stage 2) on three different datasets: XSum, E2E, and WikiText-103.  The hyperparameters include learning rate, training batch size, training epochs, weight decay, and warm-up ratio. Note that the values in parentheses correspond to the three datasets in order.", "section": "5.1 Experimental Setting"}, {"figure_path": "iSfCWhvEGA/tables/tables_16_5.jpg", "caption": "Table 7: Hyperparameters for instruction tunning with Tulu dataset.", "description": "This table presents the hyperparameters used for fine-tuning and two-stage LTE training on the Tulu instruction tuning dataset.  It shows the learning rate, training batch size, training epochs, weight decay, and warm-up ratio used in each stage of the training process.  The fine-tuning stage prepares the base model, followed by LTE-Stage 1 (model-router training) and LTE-Stage 2 (model adaptation).", "section": "5.2 End-to-end Performance Comparison"}]