[{"figure_path": "Llu9nJal7b/tables/tables_6_1.jpg", "caption": "Table 1: Evaluation of 2:4 Sparsity with frozen weights (SparseGPT does perform the weight update step). One-shot pruning methods are calibrated with C4 and evaluated on Wikitext-2 following [12]. More results for Llama-3 [1] or other SOTA methods can be found in Table 12 and 13 of the appendix.", "description": "This table compares the performance of MaskLLM against several baseline methods for achieving 2:4 sparsity in various LLMs.  The metrics used are perplexity on the Wikitext-2 dataset and accuracy scores across various downstream tasks (HellaSwag, RACE, PIQA, Winograd, ARC-E, ARC-C, OBQA).  The table demonstrates that MaskLLM achieves better performance than other state-of-the-art methods (SparseGPT and Wanda) while only learning the masks and not updating the model weights.  Additional results for Llama-3 and other baselines can be found in the appendix.", "section": "4.2 Learning 2:4 Sparsity in LLMs"}, {"figure_path": "Llu9nJal7b/tables/tables_7_1.jpg", "caption": "Table 2: The effectiveness of transfer learning with prior masks. We report the Wikitext PPL of both prior and learned masks. The learned masks use the corresponding prior for initialization and refine the logits through end-to-end training. All results are obtained with frozen weights.", "description": "This table demonstrates the effectiveness of transfer learning when using pre-computed masks as priors for initializing the learnable masks in MaskLLM.  It shows the Wikitext-2 Perplexity (PPL) for several large language models (LLMs) using different prior masks (Magnitude, SparseGPT, Wanda) and without a prior. The PPL values for the learned masks illustrate how initializing with a prior can improve the model's performance and how the end-to-end training refines these initial masks.", "section": "4.3 How to Learn a Good Mask for LLMs"}, {"figure_path": "Llu9nJal7b/tables/tables_7_2.jpg", "caption": "Table 3: Weight Regularization on remaining weights helps mask learning", "description": "This table shows the impact of sparse weight regularization on the performance of the MaskLLM model across three different scenarios: mask-only evaluation, sub-domain evaluation, and fine-tuning.  The \"w/o Reg.\" column shows results without regularization, while the \"w/ Reg.\" column presents results with the addition of a regularization term designed to maintain the magnitude of the remaining weights after pruning. The results demonstrate that regularization is beneficial to improving downstream task performance.", "section": "4.3 How to Learn a Good Mask for LLMs"}, {"figure_path": "Llu9nJal7b/tables/tables_8_1.jpg", "caption": "Table 4: Learning customized masks for downstream tasks with frozen LLM weights.", "description": "This table presents the results of applying MaskLLM to various downstream tasks with frozen weights.  It shows the average task perplexity (PPL) achieved by different methods, including MaskLLM, on several downstream domains (e.g., C#, HTML, Pascal, Story, French, Japanese, Chinese, Open Web, CUDA, VHDL, Javascript, BigScience, Reddit-Plus, Book, Arxiv, MedAbs). The results demonstrate MaskLLM's ability to learn domain-specific masks and achieve high-quality results even without updating the base model's weights.", "section": "4 Experiments"}, {"figure_path": "Llu9nJal7b/tables/tables_8_2.jpg", "caption": "Table 5: Transfer learning is effective for downstream tasks.", "description": "This table presents the average task perplexity (PPL) for different mask types when applied to downstream tasks using a 2B parameter language model.  It compares the performance of a dense model (no pruning) to three different approaches for learning sparsity masks: using a general mask learned on a large dataset, learning a separate (scratch) mask for each task, and transferring a pre-learned general mask to each downstream task.  The results show that transferring a pre-trained general mask yields comparable performance to the dense model, highlighting the effectiveness of the transfer learning approach implemented in MaskLLM.", "section": "4.4 Learning N:M Sparsity for Downstream Tasks"}, {"figure_path": "Llu9nJal7b/tables/tables_8_3.jpg", "caption": "Table 6: Storage and inference cost of of llama-2 7B for downstream tasks", "description": "This table compares the storage requirements and inference speed of fine-tuning versus using learned 2:4 sparsity masks for downstream tasks using the Llama-2 7B model.  Fine-tuning requires 16 bits per parameter and maintains 100% of the model size, resulting in 1.0x speed. In contrast, using learned 2:4 sparsity masks reduces storage to only 0.65 bits per parameter (a 25x reduction) and uses only 73% of the model size in memory. This results in a 1.4x speed improvement.", "section": "4.4 Learning N:M Sparsity for Downstream Tasks"}, {"figure_path": "Llu9nJal7b/tables/tables_12_1.jpg", "caption": "Table 7: Training details and hyper-parameters for mask training", "description": "This table lists the training details and hyperparameters used for training the MaskLLM model on various large language models.  It shows the optimizer used (AdamW), the number of training steps, the initialization of logits (drawn from a normal distribution), the scaling factor and range for the Gumbel softmax, the temperature range, and the prior strength and sparse regularization used. The parameters were tuned to achieve optimal results for learning high-quality sparsity masks.", "section": "4.1 Implementation Details"}, {"figure_path": "Llu9nJal7b/tables/tables_13_1.jpg", "caption": "Table 1: Evaluation of 2:4 Sparsity with frozen weights (SparseGPT does perform the weight update step). One-shot pruning methods are calibrated with C4 and evaluated on Wikitext-2 following [12]. More results for Llama-3 [1] or other SOTA methods can be found in Table 12 and 13 of the appendix.", "description": "This table compares the performance of MaskLLM against several other methods for achieving 2:4 sparsity in several large language models.  The evaluation metrics are perplexity and accuracy on the Wikitext-2 dataset.  It highlights MaskLLM's ability to achieve lower perplexity than other methods while only learning masks and keeping model weights frozen.", "section": "4.2 Learning 2:4 Sparsity in LLMs"}, {"figure_path": "Llu9nJal7b/tables/tables_13_2.jpg", "caption": "Table 12: Wikitext-2 PPL of 2:4 LLaMA-3 8B, with the sequence length of 4096. We took the SparseGPT mask as the prior and learned the mask on the C4 dataset.", "description": "This table shows the results of applying different sparsity methods (Magnitude Pruning, SparseGPT, Wanda, and MaskLLM) to the LLaMA-3 8B model with 2:4 sparsity.  The Wikitext-2 Perplexity (PPL) is used as a metric to evaluate the performance of the pruned models. The sequence length was 4096 for all experiments. Notably, the MaskLLM method used the SparseGPT mask as a prior. The experiments were conducted using the C4 dataset for both calibration and mask learning.", "section": "4.2 Learning 2:4 Sparsity in LLMs"}, {"figure_path": "Llu9nJal7b/tables/tables_13_3.jpg", "caption": "Table 1: Evaluation of 2:4 Sparsity with frozen weights (SparseGPT does perform the weight update step). One-shot pruning methods are calibrated with C4 and evaluated on Wikitext-2 following [12]. More results for Llama-3 [1] or other SOTA methods can be found in Table 12 and 13 of the appendix.", "description": "This table presents the results of evaluating different 2:4 sparsity methods on the Llama-2 7B and 13B, Nemotron-4 15B, and GPT-3 843M and 2B language models.  The Wikitext-2 perplexity (PPL) and accuracy on several downstream tasks (HellaSwag, RACE, PIQA, Winogrande, ARC-E, ARC-C, OBQA) are reported for each method.  The table compares the performance of MaskLLM against baselines like Magnitude Pruning, SparseGPT, and Wanda, highlighting MaskLLM's improved performance with frozen weights.  Additional results for Llama-3 and other state-of-the-art methods are referenced in the appendix.", "section": "4.2 Learning 2:4 Sparsity in LLMs"}, {"figure_path": "Llu9nJal7b/tables/tables_14_1.jpg", "caption": "Table 14: Average Gradient Norm over the First 500 Training Steps of GPT-3 2B, with Varying Levels of Sparse Weight Regularization. In this study, we use a regularization strength of 1e-5 for mask learning, as it offers a stable gradient while imposing minimal constraints on the search space.", "description": "This table shows the impact of different levels of sparse weight regularization on the average gradient norm during the first 500 training steps of the GPT-3 2B model.  The results indicate that a regularization strength of 1e-5 provides a good balance between gradient stability and avoiding overly restrictive constraints on the search space during mask learning.", "section": "F Sparse Weight Regularization"}, {"figure_path": "Llu9nJal7b/tables/tables_14_2.jpg", "caption": "Table 15: Comparison to SOTA 2:4 pruning methods on LLaMA-2 13B, with all results collected from original papers or official implementations.", "description": "This table compares the proposed MaskLLM method with other state-of-the-art (SOTA) 2:4 pruning methods on the LLaMA-2 13B model.  It shows the Wikitext-2 perplexity achieved by each method, indicating the performance of each pruning technique. The table also notes whether each method involves weight updates during the pruning process.  MaskLLM demonstrates superior performance, even in comparison to methods that employ weight updates. The results highlight MaskLLM's effectiveness in achieving high-quality, sparse models.", "section": "E Comparison to More Pruning Methods for LLMs"}, {"figure_path": "Llu9nJal7b/tables/tables_15_1.jpg", "caption": "Table 16: Benchmarking LLaMA-2 7B and 13B on A6000 with TensorRT-LLM.", "description": "This table presents the benchmark results of LLaMA-2 7B and 13B models with 2:4 sparsity on an A6000 GPU using the TensorRT-LLM framework.  It shows the throughput (tokens processed per second) for various input and output sequence lengths, comparing the dense model's performance to that of the sparse model. The speedup factor is also calculated, demonstrating the performance improvement gained through sparsity.", "section": "H Throughput of 2:4 LLAMA-2 7B"}, {"figure_path": "Llu9nJal7b/tables/tables_16_1.jpg", "caption": "Table 17: MaskLLM for Vision Transformers", "description": "This table presents the results of applying MaskLLM and several baseline methods for pruning a Vision Transformer (ViT-B/16) model.  The top-1 accuracy on the ImageNet-1K dataset is reported for various sparsity patterns (dense, 2:4) and with or without weight updates during pruning.  It demonstrates MaskLLM's ability to achieve high accuracy with sparsity, even surpassing methods that utilize weight updates.", "section": "4.4 Learning N:M Sparsity for Downstream Tasks"}]