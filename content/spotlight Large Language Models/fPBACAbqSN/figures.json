[{"figure_path": "fPBACAbqSN/figures/figures_0_1.jpg", "caption": "Figure 1: Attention weights, especially in long-context LLMs, exhibit up to 96.8% sparsity in contexts of 128K. We propose MInference, leveraging dynamic sparse attention to accelerate the pre-filling stage of long-context LLM inference. It achieves up to 10x speedup for 1M contexts on a single A100, as shown in (b), and matches or surpasses baselines, as demonstrated by Needle In A Haystack [Kam23] in (a) on LLaMA-3-8B-1M [Gra24].", "description": "This figure shows two plots. Plot (a) is a bar chart comparing the latency of MInference and FlashAttention-2 on the Needle in a Haystack benchmark using the LLaMA-3-8B-1M model. It demonstrates that MInference achieves comparable accuracy with significantly lower latency. Plot (b) shows a line chart illustrating the speedup achieved by MInference across various context window sizes, highlighting a 10x speedup for 1M context.", "section": "Introduction"}, {"figure_path": "fPBACAbqSN/figures/figures_2_1.jpg", "caption": "Figure 2: (a) Latency breakdown of the pre-filling stage. (b) How much attention scores can top-k (k=4096) columns cover in a 128k context. (c) Less attention scores are retrieved when reusing the top-k indices from another examples, indicating its dynamic nature. Visualizations are based on LLaMa-3-8B with a single A100.", "description": "This figure shows the breakdown of latency during the pre-filling stage of a long-context LLM. It highlights the significant cost of attention computation (a).  It then demonstrates the sparsity of attention weights, showing that a small subset of attention scores captures most of the information (b). Finally, it illustrates the dynamic nature of attention sparsity, showing that patterns observed in one example do not generalize well to others (c).  The visualizations are based on the LLaMA-3-8B model using a single A100 GPU.", "section": "2 Attention Heads: Dynamic, Sparse, and Characteristic"}, {"figure_path": "fPBACAbqSN/figures/figures_2_2.jpg", "caption": "Figure 3: (a) Visualization of attention weights from different attention heads. For different prompts and tasks, the pattern of the same head is relatively consistent, but the sparse indices are dynamically changing.(b) Distance of the top-10 nearest non-zero element in the attention matrix. (c) Attention recall distribution using our identified patterns, where FLOPs in the kernel refer to the real FLOPs required for sparse attention computing using on GPUs. Here, a 1x64 block size is used for the Vertical-Slash pattern, and a 64x64 block size is used for others on GPUs. All visualization are based on LLaMA-3-8B-Instruct-262K [Gra24].", "description": "This figure visualizes the three sparse attention patterns identified in the paper: A-shape, Vertical-Slash, and Block-Sparse.  Panel (a) shows example attention weight matrices for each pattern, highlighting their distinct structures. Panel (b) illustrates the spatial clustering of non-zero attention weights across different layers, supporting the existence of these patterns. Panel (c) compares the recall (percentage of attention scores captured) of these patterns against the computational cost (dense FLOPs/FLOPs in kernel), demonstrating their efficiency compared to a dense approach.", "section": "2 Attention Heads: Dynamic, Sparse, and Characteristic"}, {"figure_path": "fPBACAbqSN/figures/figures_3_1.jpg", "caption": "Figure 4: The three sparse methods in MInference.", "description": "This figure shows the three sparse methods used in MInference: A-shape, vertical-slash, and block-sparse.  Each method shows how a subset of the attention matrix is computed to reduce the computational cost. The A-shape focuses on the top-left corner, the vertical-slash on diagonal and vertical lines, and the block-sparse on selected blocks.  The figure highlights the different ways MInference approximates the full attention matrix to achieve efficiency while maintaining accuracy.", "section": "3 MInference 1.0"}, {"figure_path": "fPBACAbqSN/figures/figures_7_1.jpg", "caption": "Figure 5: Perplexity results on PG-19 [RPJ+20] using different models and methods.", "description": "The figure shows the perplexity results on the PG-19 language modeling task for different context window sizes using several models (LLaMA-3-8B-Instruct-262K and Yi-9B-200K).  It compares the performance of MInference against several baseline methods including StreamingLLM, StreamingLLM with dilated and strided attention, InfLLM, and FlashAttention-2.  The graph demonstrates MInference's superior performance in maintaining low perplexity even with large context window sizes while significantly outperforming baselines.", "section": "Experiments"}, {"figure_path": "fPBACAbqSN/figures/figures_7_2.jpg", "caption": "Figure 1: Attention weights, especially in long-context LLMs, exhibit up to 96.8% sparsity in contexts of 128K. We propose MInference, leveraging dynamic sparse attention to accelerate the pre-filling stage of long-context LLM inference. It achieves up to 10x speedup for 1M contexts on a single A100, as shown in (b), and matches or surpasses baselines, as demonstrated by Needle In A Haystack [Kam23] in (a) on LLaMA-3-8B-1M [Gra24].", "description": "This figure shows two graphs. Graph (a) compares the performance of MInference and FlashAttention-2 on the Needle in a Haystack benchmark using the LLaMA-3-8B-1M model with a 1M context. The results demonstrate that MInference matches or surpasses the baseline. Graph (b) illustrates the latency speedup achieved by MInference across various context window sizes, showcasing a significant improvement in efficiency.", "section": "Introduction"}, {"figure_path": "fPBACAbqSN/figures/figures_19_1.jpg", "caption": "Figure 7: The dynamic sparse mask for the vertical-slash pattern using LLaMA-3-8B in the summarization task [ZCH+24]. Yellow areas indicate the computed parts. Slash lines use 64 \u00d7 64 blocks, while vertical lines use 1 \u00d7 64 blocks.", "description": "This figure shows a visualization of the dynamic sparse mask used in the Vertical-Slash pattern of the MInference method.  The yellow regions highlight the non-zero elements (or computed parts) of the attention matrix, while the purple areas represent the zero elements (not computed).  It demonstrates how the method identifies and utilizes specific patterns (vertical and slash lines) in the attention matrix to reduce computation during the inference stage. The different block sizes used for vertical and slash lines are also shown.", "section": "Kernel Implementation"}, {"figure_path": "fPBACAbqSN/figures/figures_19_2.jpg", "caption": "Figure 1: Attention weights, especially in long-context LLMs, exhibit up to 96.8% sparsity in contexts of 128K. We propose MInference, leveraging dynamic sparse attention to accelerate the pre-filling stage of long-context LLM inference. It achieves up to 10x speedup for 1M contexts on a single A100, as shown in (b), and matches or surpasses baselines, as demonstrated by Needle In A Haystack [Kam23] in (a) on LLaMA-3-8B-1M [Gra24].", "description": "This figure demonstrates the sparsity of attention weights in long-context LLMs and the performance of MInference.  (a) shows the attention weight sparsity in the Needle in A Haystack benchmark using LLaMA-3-8B-1M. It highlights that even with a million tokens, a significant portion of the attention weights are near zero. (b) illustrates the speedup achieved by MInference compared to FlashAttention-2 across varying context window sizes. MInference achieves a substantial speedup, especially with longer contexts.  The figure supports the paper's claim that MInference efficiently accelerates pre-filling for long-context LLMs by leveraging dynamic sparse attention.", "section": "Introduction"}, {"figure_path": "fPBACAbqSN/figures/figures_20_1.jpg", "caption": "Figure 1: Attention weights, especially in long-context LLMs, exhibit up to 96.8% sparsity in contexts of 128K. We propose MInference, leveraging dynamic sparse attention to accelerate the pre-filling stage of long-context LLM inference. It achieves up to 10x speedup for 1M contexts on a single A100, as shown in (b), and matches or surpasses baselines, as demonstrated by Needle In A Haystack [Kam23] in (a) on LLaMA-3-8B-1M [Gra24].", "description": "This figure shows the sparsity of attention weights in long-context LLMs and the speedup achieved by MInference. (a) demonstrates the performance of MInference on the Needle In A Haystack benchmark compared to FlashAttention-2, showing that MInference matches or surpasses the baseline while maintaining high sparsity. (b) illustrates the latency speedup achieved by MInference for different context window sizes, showing up to a 10x speedup for 1M contexts on a single A100 GPU.", "section": "Abstract"}, {"figure_path": "fPBACAbqSN/figures/figures_21_1.jpg", "caption": "Figure 1: Attention weights, especially in long-context LLMs, exhibit up to 96.8% sparsity in contexts of 128K. We propose MInference, leveraging dynamic sparse attention to accelerate the pre-filling stage of long-context LLM inference. It achieves up to 10x speedup for 1M contexts on a single A100, as shown in (b), and matches or surpasses baselines, as demonstrated by Needle In A Haystack [Kam23] in (a) on LLaMA-3-8B-1M [Gra24].", "description": "This figure shows two plots. Plot (a) compares the performance of MInference and FlashAttention-2 on the Needle in a Haystack benchmark using the LLaMA-3-8B-1M model with 1M context. It demonstrates that MInference achieves similar performance to FlashAttention-2 while exhibiting significantly reduced latency. Plot (b) illustrates the speedup achieved by MInference in terms of latency reduction.  For a context window of 1M tokens, it shows a speedup of approximately 10 times compared to other methods. The figure highlights the effectiveness of MInference in accelerating the pre-filling stage of long-context LLMs by leveraging the sparsity of attention weights.", "section": "Introduction"}, {"figure_path": "fPBACAbqSN/figures/figures_22_1.jpg", "caption": "Figure 1: Attention weights, especially in long-context LLMs, exhibit up to 96.8% sparsity in contexts of 128K. We propose MInference, leveraging dynamic sparse attention to accelerate the pre-filling stage of long-context LLM inference. It achieves up to 10x speedup for 1M contexts on a single A100, as shown in (b), and matches or surpasses baselines, as demonstrated by Needle In A Haystack [Kam23] in (a) on LLaMA-3-8B-1M [Gra24].", "description": "This figure shows two graphs. Graph (a) presents the performance comparison of MInference and FlashAttention-2 on the Needle In A Haystack benchmark using the LLaMA-3-8B-1M model. It demonstrates that MInference maintains accuracy while achieving similar performance. Graph (b) illustrates the speedup in latency provided by MInference across various context window sizes, showcasing significant improvements.", "section": "Introduction"}, {"figure_path": "fPBACAbqSN/figures/figures_22_2.jpg", "caption": "Figure 1: Attention weights, especially in long-context LLMs, exhibit up to 96.8% sparsity in contexts of 128K. We propose MInference, leveraging dynamic sparse attention to accelerate the pre-filling stage of long-context LLM inference. It achieves up to 10x speedup for 1M contexts on a single A100, as shown in (b), and matches or surpasses baselines, as demonstrated by Needle In A Haystack [Kam23] in (a) on LLaMA-3-8B-1M [Gra24].", "description": "The figure shows two plots. Plot (a) compares the performance of MInference and FlashAttention-2 on the Needle in A Haystack benchmark using the LLaMA-3-8B-1M model with a 1M context. The plot shows that MInference achieves comparable performance to FlashAttention-2 while significantly reducing latency. Plot (b) illustrates the speedup achieved by MInference compared to a baseline approach. The plot shows that MInference achieves up to a 10x speedup for 1M contexts on a single A100 GPU.", "section": "Introduction"}, {"figure_path": "fPBACAbqSN/figures/figures_23_1.jpg", "caption": "Figure 3: (a) Visualization of attention weights from different attention heads. For different prompts and tasks, the pattern of the same head is relatively consistent, but the sparse indices are dynamically changing.(b) Distance of the top-10 nearest non-zero element in the attention matrix. (c) Attention recall distribution using our identified patterns, where FLOPs in the kernel refer to the real FLOPs required for sparse attention computing using on GPUs. Here, a 1x64 block size is used for the Vertical-Slash pattern, and a 64x64 block size is used for others on GPUs. All visualization are based on LLaMA-3-8B-Instruct-262K [Gra24].", "description": "This figure visualizes three key aspects of the attention mechanism in long-context LLMs. (a) shows the distinct patterns (A-shape, Vertical-Slash, Block-Sparse) observed in attention weight matrices for different attention heads. These patterns remain consistent across various prompts and tasks, despite the dynamic nature of the sparse indices. (b) illustrates spatial clustering in the attention matrix by showing the distances between non-zero attention weights and their nearest non-zero neighbors. This demonstrates that attention weights are not randomly distributed but exhibit spatial patterns. Finally, (c) compares the recall and computational efficiency (FLOPs) of different sparse patterns, highlighting the superiority of the proposed method in retrieving important attention scores with minimal computational cost.", "section": "2 Attention Heads: Dynamic, Sparse, and Characteristic"}]