[{"figure_path": "fykjplMc0V/figures/figures_1_1.jpg", "caption": "Figure 1: Parameter count vs. performance for LoReFT and other PEFTs across four benchmarks when applied to LLaMA, Llama-2, Llama-3, and RoBERTa models. Despite training far fewer parameters than existing PEFTs, LoReFT achieves competitive or even state-of-the-art performance on all tasks. Its value is most apparent for the largest models in our evaluations. Note: FT is full-parameter finetuning, which is not a PEFT or ReFT method. Additional results are in section 4.", "description": "This figure compares the performance and parameter efficiency of LoReFT against other parameter-efficient fine-tuning (PEFT) methods across four benchmarks: commonsense reasoning, arithmetic reasoning, instruction tuning, and GLUE.  It shows that LoReFT achieves competitive performance while using significantly fewer parameters than other PEFTs, especially for larger language models. The performance is measured on the y-axis while the number of trainable parameters relative to the base model's total number of parameters is shown on the x-axis.  'FT' represents full finetuning (not a PEFT or ReFT method).", "section": "4 Experiments"}, {"figure_path": "fykjplMc0V/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of ReFT. (1) The left panel depicts an intervention I: the intervention function I is applied to hidden representations at positions P in layer l. (2) The right panel depicts the intervention function used in LoReFT, which finds an edit vector that only modifies the representation in the linear subspace spanned by the rows of R. Specifically, we show how a rank-2 LoReFT operates on 3-dimensional hidden representations.", "description": "This figure illustrates the ReFT intervention and LoReFT methods. The left panel shows a general ReFT intervention where an intervention function is applied to hidden representations at specific positions in a layer. The right panel illustrates the LoReFT method, a specific instance of ReFT, which uses a low-rank projection matrix to modify hidden representations within a linear subspace.  It shows how a rank-2 LoReFT modifies 3-dimensional hidden representations by applying an edit vector to the subspace.", "section": "3 ReFT"}, {"figure_path": "fykjplMc0V/figures/figures_18_1.jpg", "caption": "Figure 1: Parameter count vs. performance for LoReFT and other PEFTs across four benchmarks when applied to LLaMA, Llama-2, Llama-3, and RoBERTa models. Despite training far fewer parameters than existing PEFTs, LoReFT achieves competitive or even state-of-the-art performance on all tasks. Its value is most apparent for the largest models in our evaluations. Note: FT is full-parameter finetuning, which is not a PEFT or ReFT method. Additional results are in section 4.", "description": "This figure compares the performance of LoReFT against other parameter-efficient fine-tuning (PEFT) methods across four NLP benchmarks using different language models (LLaMA, Llama-2, Llama-3, and RoBERTa).  It shows that LoReFT achieves comparable or superior performance while using significantly fewer parameters than other PEFTs, particularly when used with larger language models.", "section": "4 Experiments"}, {"figure_path": "fykjplMc0V/figures/figures_18_2.jpg", "caption": "Figure 1: Parameter count vs. performance for LoReFT and other PEFTs across four benchmarks when applied to LLaMA, Llama-2, Llama-3, and RoBERTa models. Despite training far fewer parameters than existing PEFTs, LoReFT achieves competitive or even state-of-the-art performance on all tasks. Its value is most apparent for the largest models in our evaluations. Note: FT is full-parameter finetuning, which is not a PEFT or ReFT method. Additional results are in section 4.", "description": "This figure compares the performance and the number of parameters of LoReFT against other PEFT methods (Parameter-Efficient Fine-Tuning) on four different NLP benchmarks using various language models like LLaMA, Llama-2, Llama-3, and RoBERTa.  It shows that LoReFT achieves comparable or superior performance while using significantly fewer parameters, especially with larger language models.  Full finetuning (FT) is included as a baseline for comparison but is not a PEFT method.", "section": "1 Introduction"}, {"figure_path": "fykjplMc0V/figures/figures_32_1.jpg", "caption": "Figure 1: Parameter count vs. performance for LoReFT and other PEFTs across four benchmarks when applied to LLaMA, Llama-2, Llama-3, and RoBERTa models. Despite training far fewer parameters than existing PEFTs, LoReFT achieves competitive or even state-of-the-art performance on all tasks. Its value is most apparent for the largest models in our evaluations. Note: FT is full-parameter finetuning, which is not a PEFT or ReFT method. Additional results are in section 4.", "description": "This figure compares the performance of LoReFT against other parameter-efficient fine-tuning (PEFT) methods across four different benchmarks (Commonsense, Arithmetic, Instruction-tuning, and GLUE).  The x-axis represents the number of parameters used (as a percentage of the base model's parameters), while the y-axis represents the performance achieved.  The figure demonstrates that LoReFT achieves competitive or superior performance while using significantly fewer parameters than other PEFT methods, particularly on larger models.", "section": "4 Experiments"}, {"figure_path": "fykjplMc0V/figures/figures_32_2.jpg", "caption": "Figure 1: Parameter count vs. performance for LoReFT and other PEFTs across four benchmarks when applied to LLaMA, Llama-2, Llama-3, and RoBERTa models. Despite training far fewer parameters than existing PEFTs, LoReFT achieves competitive or even state-of-the-art performance on all tasks. Its value is most apparent for the largest models in our evaluations. Note: FT is full-parameter finetuning, which is not a PEFT or ReFT method. Additional results are in section 4.", "description": "This figure compares the performance and parameter count of LoReFT against other PEFT methods across four different NLP benchmarks using various language models. It shows that LoReFT achieves competitive or better performance while using significantly fewer parameters, especially when applied to large models.", "section": "4 Experiments"}, {"figure_path": "fykjplMc0V/figures/figures_32_3.jpg", "caption": "Figure 1: Parameter count vs. performance for LoReFT and other PEFTs across four benchmarks when applied to LLaMA, Llama-2, Llama-3, and RoBERTa models. Despite training far fewer parameters than existing PEFTs, LoReFT achieves competitive or even state-of-the-art performance on all tasks. Its value is most apparent for the largest models in our evaluations. Note: FT is full-parameter finetuning, which is not a PEFT or ReFT method. Additional results are in section 4.", "description": "This figure compares the performance and parameter count of LoReFT against other parameter-efficient fine-tuning (PEFT) methods across four different benchmarks and model sizes (LLaMA, Llama-2, Llama-3, and RoBERTa).  It shows that LoReFT achieves state-of-the-art or competitive performance while using significantly fewer parameters than other PEFTs, particularly on the larger language models.", "section": "4 Experiments"}, {"figure_path": "fykjplMc0V/figures/figures_32_4.jpg", "caption": "Figure 1: Parameter count vs. performance for LoReFT and other PEFTs across four benchmarks when applied to LLaMA, Llama-2, Llama-3, and RoBERTa models. Despite training far fewer parameters than existing PEFTs, LoReFT achieves competitive or even state-of-the-art performance on all tasks. Its value is most apparent for the largest models in our evaluations. Note: FT is full-parameter finetuning, which is not a PEFT or ReFT method. Additional results are in section 4.", "description": "The figure compares the performance and the number of parameters used by LoReFT and other parameter-efficient fine-tuning (PEFT) methods across four benchmarks using different language models (LLaMA, Llama-2, Llama-3, and RoBERTa). LoReFT consistently achieves competitive or better performance while using significantly fewer parameters than other PEFTs, especially for larger language models.", "section": "4 Experiments"}, {"figure_path": "fykjplMc0V/figures/figures_32_5.jpg", "caption": "Figure 1: Parameter count vs. performance for LoReFT and other PEFTs across four benchmarks when applied to LLaMA, Llama-2, Llama-3, and RoBERTa models. Despite training far fewer parameters than existing PEFTs, LoReFT achieves competitive or even state-of-the-art performance on all tasks. Its value is most apparent for the largest models in our evaluations. Note: FT is full-parameter finetuning, which is not a PEFT or ReFT method. Additional results are in section 4.", "description": "This figure compares the parameter efficiency and performance of LoReFT against other PEFT methods across four different benchmarks using various language models.  It showcases LoReFT's ability to achieve competitive or even state-of-the-art results while using significantly fewer parameters than existing methods, especially when applied to larger models.  The benchmarks include commonsense reasoning, arithmetic reasoning, instruction tuning, and GLUE.", "section": "4 Experiments"}, {"figure_path": "fykjplMc0V/figures/figures_32_6.jpg", "caption": "Figure 1: Parameter count vs. performance for LoReFT and other PEFTs across four benchmarks when applied to LLaMA, Llama-2, Llama-3, and RoBERTa models. Despite training far fewer parameters than existing PEFTs, LoReFT achieves competitive or even state-of-the-art performance on all tasks. Its value is most apparent for the largest models in our evaluations. Note: FT is full-parameter finetuning, which is not a PEFT or ReFT method. Additional results are in section 4.", "description": "The figure compares the performance and parameter count of LoReFT against other parameter-efficient fine-tuning (PEFT) methods across four NLP benchmarks using different language models (LLaMA, Llama-2, Llama-3, and RoBERTa). LoReFT consistently achieves state-of-the-art performance while using significantly fewer parameters than existing PEFT methods, highlighting its efficiency and effectiveness, particularly with larger models.", "section": "4 Experiments"}, {"figure_path": "fykjplMc0V/figures/figures_34_1.jpg", "caption": "Figure 1: Parameter count vs. performance for LoReFT and other PEFTs across four benchmarks when applied to LLaMA, Llama-2, Llama-3, and RoBERTa models. Despite training far fewer parameters than existing PEFTs, LoReFT achieves competitive or even state-of-the-art performance on all tasks. Its value is most apparent for the largest models in our evaluations. Note: FT is full-parameter finetuning, which is not a PEFT or ReFT method. Additional results are in section 4.", "description": "This figure compares the parameter efficiency and performance of LoReFT against other parameter-efficient fine-tuning (PEFT) methods across four different NLP benchmarks (Commonsense, Arithmetic, Instruction-tuning, and GLUE) and using four different language models (LLaMA, Llama-2, Llama-3, and RoBERTa).  The results show that LoReFT achieves competitive or better results than other PEFTs while using significantly fewer parameters, especially for larger language models.  The y-axis represents performance, and the x-axis represents the number of parameters used (as a percentage of the total parameters in the base model).", "section": "4 Experiments"}, {"figure_path": "fykjplMc0V/figures/figures_38_1.jpg", "caption": "Figure 1: Parameter count vs. performance for LoReFT and other PEFTs across four benchmarks when applied to LLaMA, Llama-2, Llama-3, and RoBERTa models. Despite training far fewer parameters than existing PEFTs, LoReFT achieves competitive or even state-of-the-art performance on all tasks. Its value is most apparent for the largest models in our evaluations. Note: FT is full-parameter finetuning, which is not a PEFT or ReFT method. Additional results are in section 4.", "description": "This figure compares the parameter efficiency and performance of LoReFT against other PEFT methods (Parameter-Efficient Fine-Tuning) on four different NLP benchmarks using various language models (LLaMA, Llama-2, Llama-3, and RoBERTa).  The x-axis represents the percentage of parameters trained relative to the full model, showing LoReFT's significant parameter efficiency. The y-axis shows the performance on each benchmark. The results demonstrate that despite using drastically fewer parameters, LoReFT achieves performance comparable to, or even better than, existing PEFT methods, particularly with larger language models.", "section": "4 Experiments"}, {"figure_path": "fykjplMc0V/figures/figures_39_1.jpg", "caption": "Figure 1: Parameter count vs. performance for LoReFT and other PEFTs across four benchmarks when applied to LLaMA, Llama-2, Llama-3, and RoBERTa models. Despite training far fewer parameters than existing PEFTs, LoReFT achieves competitive or even state-of-the-art performance on all tasks. Its value is most apparent for the largest models in our evaluations. Note: FT is full-parameter finetuning, which is not a PEFT or ReFT method. Additional results are in section 4.", "description": "This figure compares the performance and parameter efficiency of LoReFT against other parameter-efficient fine-tuning (PEFT) methods across four different benchmarks and model sizes. LoReFT consistently achieves competitive results despite using significantly fewer parameters, demonstrating its efficiency, especially with larger models.", "section": "4 Experiments"}, {"figure_path": "fykjplMc0V/figures/figures_39_2.jpg", "caption": "Figure 1: Parameter count vs. performance for LoReFT and other PEFTs across four benchmarks when applied to LLaMA, Llama-2, Llama-3, and RoBERTa models. Despite training far fewer parameters than existing PEFTs, LoReFT achieves competitive or even state-of-the-art performance on all tasks. Its value is most apparent for the largest models in our evaluations. Note: FT is full-parameter finetuning, which is not a PEFT or ReFT method. Additional results are in section 4.", "description": "This figure compares the performance and the number of parameters used by LoReFT and other PEFT methods across four different NLP benchmarks and model sizes.  It demonstrates that LoReFT achieves performance comparable to other PEFT methods while using significantly fewer parameters, with the difference particularly pronounced for larger models.  The figure highlights LoReFT's parameter efficiency.", "section": "1 Introduction"}, {"figure_path": "fykjplMc0V/figures/figures_40_1.jpg", "caption": "Figure 1: Parameter count vs. performance for LoReFT and other PEFTs across four benchmarks when applied to LLaMA, Llama-2, Llama-3, and RoBERTa models. Despite training far fewer parameters than existing PEFTs, LoReFT achieves competitive or even state-of-the-art performance on all tasks. Its value is most apparent for the largest models in our evaluations. Note: FT is full-parameter finetuning, which is not a PEFT or ReFT method. Additional results are in section 4.", "description": "This figure compares the performance and parameter efficiency of LoReFT against other parameter-efficient fine-tuning (PEFT) methods across four different benchmarks (Commonsense, Arithmetic, Instruction-tuning, and GLUE) and various language models (LLaMA, Llama-2, Llama-3, and RoBERTa).  It demonstrates that LoReFT achieves competitive or superior performance while using significantly fewer parameters than other PEFT methods, especially when applied to larger language models.  The y-axis represents performance, and the x-axis shows the number of parameters used (as a percentage of the total model parameters).", "section": "4 Experiments"}, {"figure_path": "fykjplMc0V/figures/figures_45_1.jpg", "caption": "Figure 1: Parameter count vs. performance for LoReFT and other PEFTs across four benchmarks when applied to LLaMA, Llama-2, Llama-3, and RoBERTa models. Despite training far fewer parameters than existing PEFTs, LoReFT achieves competitive or even state-of-the-art performance on all tasks. Its value is most apparent for the largest models in our evaluations. Note: FT is full-parameter finetuning, which is not a PEFT or ReFT method. Additional results are in section 4.", "description": "This figure compares the parameter efficiency and performance of LoReFT against other parameter-efficient fine-tuning (PEFT) methods across four NLP benchmarks.  The results show LoReFT achieves competitive or superior performance while using significantly fewer parameters, particularly beneficial for larger language models.", "section": "4 Experiments"}, {"figure_path": "fykjplMc0V/figures/figures_46_1.jpg", "caption": "Figure 1: Parameter count vs. performance for LoReFT and other PEFTs across four benchmarks when applied to LLaMA, Llama-2, Llama-3, and RoBERTa models. Despite training far fewer parameters than existing PEFTs, LoReFT achieves competitive or even state-of-the-art performance on all tasks. Its value is most apparent for the largest models in our evaluations. Note: FT is full-parameter finetuning, which is not a PEFT or ReFT method. Additional results are in section 4.", "description": "This figure compares LoReFT's performance against other parameter-efficient fine-tuning (PEFT) methods across four benchmarks using different language models (LLaMA, Llama-2, Llama-3, and RoBERTa).  It shows that LoReFT, despite using significantly fewer parameters, achieves comparable or superior performance to other PEFTs, especially when using larger language models.", "section": "4 Experiments"}]