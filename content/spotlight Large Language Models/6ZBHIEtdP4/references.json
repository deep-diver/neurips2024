{"references": [{"fullname_first_author": "Edward J Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2021-06-09", "reason": "This paper introduces the Low-Rank Adaptation (LoRA) method, which is the basis of the PiSSA method presented in this paper, forming the core of the comparative analysis."}, {"fullname_first_author": "Tim Dettmers", "paper_title": "Qlora: Efficient finetuning of quantized llms", "publication_date": "2024-00-00", "reason": "This paper presents QLoRA, a quantized version of LoRA, which is directly compared to and improved upon by the proposed QPiSSA method in this paper."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-00-00", "reason": "This paper details the instruction-following methodology using human feedback, a crucial component in evaluating and benchmarking the large language models (LLMs) used in this paper's experiments."}, {"fullname_first_author": "Haipeng Luo", "paper_title": "Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct", "publication_date": "2023-08-09", "reason": "This paper introduces the WizardMath dataset, one of the key benchmarks used in this paper to evaluate the performance of the proposed PiSSA method."}, {"fullname_first_author": "Longhui Yu", "paper_title": "Metamath: Bootstrap your own mathematical questions for large language models", "publication_date": "2023-09-12", "reason": "This paper introduces the MetaMathQA dataset, another important benchmark used in this paper's experiments to assess the mathematical problem-solving capabilities of the evaluated LLMs."}]}