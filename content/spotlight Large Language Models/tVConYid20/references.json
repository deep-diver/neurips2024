{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, the foundation upon which FlashAttention-3 builds and improves."}, {"fullname_first_author": "Tri Dao", "paper_title": "FlashAttention: Fast and memory-efficient exact attention with IO-awareness", "publication_date": "2022-12-01", "reason": "This paper introduced FlashAttention, the precursor to the algorithm presented in this paper."}, {"fullname_first_author": "Tri Dao", "paper_title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning", "publication_date": "2023-07-01", "reason": "This paper introduced FlashAttention-2, a significant improvement over the original FlashAttention algorithm, and directly improved upon by this paper's algorithm."}, {"fullname_first_author": "Krzysztof Marcin Choromanski", "paper_title": "Rethinking attention with performers", "publication_date": "2021-01-01", "reason": "This paper proposed an alternative attention mechanism (Performers) that this paper compares against and also aims to outperform."}, {"fullname_first_author": "Nikita Kitaev", "paper_title": "Reformer: The efficient transformer", "publication_date": "2020-01-01", "reason": "This paper introduced Reformer, another important alternative attention mechanism that the authors benchmark and compare their proposed solution against."}]}