[{"figure_path": "LzLeAscHnj/figures/figures_1_1.jpg", "caption": "Figure 1: (a) An illustration of our HRA method. (b) Comparisons for various methods on GLUE benchmark [50]. The x-axis corresponds to the number of trainable parameters (M), and the y-axis corresponds to the average score (%). (c) Comparisons for various methods on the ratio of trainable parameters and accuracy (%) when adapting LLaMA2-7B [46] in mathematical reasoning tasks.", "description": "This figure demonstrates the Householder Reflection Adaptation (HRA) method and compares its performance to other state-of-the-art methods. Subfigure (a) shows a schematic of the HRA method, illustrating how it adapts a pre-trained model by multiplying each frozen weight matrix with an orthogonal matrix constructed using a chain of Householder reflections. Subfigure (b) presents a comparison of HRA's performance against other methods on the GLUE benchmark, showing superior performance with fewer parameters. Finally, subfigure (c) shows the performance comparison for mathematical reasoning tasks using LLaMA2-7B.", "section": "1 Introduction"}, {"figure_path": "LzLeAscHnj/figures/figures_1_2.jpg", "caption": "Figure 1: (a) An illustration of our HRA method. (b) Comparisons for various methods on GLUE benchmark [50]. The x-axis corresponds to the number of trainable parameters (M), and the y-axis corresponds to the average score (%). (c) Comparisons for various methods on the ratio of trainable parameters and accuracy (%) when adapting LLaMA2-7B [46] in mathematical reasoning tasks.", "description": "This figure presents a visual comparison of the proposed Householder Reflection Adaptation (HRA) method against other state-of-the-art methods.  Panel (a) shows a schematic of the HRA approach, illustrating how it applies a series of Householder reflections to modify pre-trained model weights.  Panel (b) compares HRA's performance to other methods on the GLUE benchmark, plotting average GLUE score against the number of trainable parameters.  Finally, Panel (c) shows a similar comparison of HRA's performance on mathematical reasoning tasks, this time using LLaMA2-7B as the base model.", "section": "1 Introduction"}, {"figure_path": "LzLeAscHnj/figures/figures_1_3.jpg", "caption": "Figure 1: (a) An illustration of our HRA method. (b) Comparisons for various methods on GLUE benchmark [50]. The x-axis corresponds to the number of trainable parameters (M), and the y-axis corresponds to the average score (%). (c) Comparisons for various methods on the ratio of trainable parameters and accuracy (%) when adapting LLaMA2-7B [46] in mathematical reasoning tasks.", "description": "This figure presents a comprehensive evaluation of the proposed Householder Reflection Adaptation (HRA) method.  Panel (a) illustrates the architecture of HRA, showing how it adapts a pre-trained model by multiplying its weight matrices with a chain of orthogonal matrices (Householder reflections). Panel (b) compares HRA's performance against several state-of-the-art parameter-efficient fine-tuning (PEFT) methods on the GLUE benchmark, demonstrating superior performance with fewer trainable parameters. Panel (c) shows a similar comparison for adapting a large language model (LLaMA2-7B) on mathematical reasoning tasks, reinforcing HRA's effectiveness.", "section": "1 Introduction"}, {"figure_path": "LzLeAscHnj/figures/figures_4_1.jpg", "caption": "Figure 2: A 2D illustration indicating that when the reflection planes H1 and H2 are orthogonal, the distance ||H2H1w - w||2 is maximized.", "description": "This figure illustrates the impact of orthogonality on the effectiveness of the Householder Reflection Adaptation (HRA) method.  It shows a 2D representation of the effect of applying two successive Householder reflections (H1 and H2) to a weight vector w. When the reflection planes (hyperplanes) defined by H1 and H2 are orthogonal, the resulting vector H2H1w is maximally distant from the original vector w. This maximal distance implies that the HRA method can maximize the adaptation capacity when the reflection planes are orthogonal. The non-orthogonal case (dashed lines) is also shown for comparison, highlighting that the distance is smaller when the planes are not orthogonal. This supports the paper's argument that orthogonality of Householder reflections influences model capacity.", "section": "3 Proposed Method"}, {"figure_path": "LzLeAscHnj/figures/figures_6_1.jpg", "caption": "Figure 3: The robustness of HRA (r = 8) to \u03bb on MRPC.", "description": "This figure displays the robustness of the Householder Reflection Adaptation (HRA) method, with r=8, to variations in the orthogonality regularizer (\u03bb) when evaluated on the MRPC (Microsoft Research Paraphrase Corpus) dataset.  The x-axis represents the values of \u03bb, ranging from 10\u207b\u2077 to 10\u207b\u00b3, and the y-axis shows the accuracy achieved on the MRPC task.  The bars show that the performance of HRA remains relatively stable across a wide range of \u03bb values, indicating its robustness to variations in this hyperparameter.", "section": "4.1 Natural Language Understanding"}, {"figure_path": "LzLeAscHnj/figures/figures_6_2.jpg", "caption": "Figure 1: (a) An illustration of our HRA method. (b) Comparisons for various methods on GLUE benchmark [50]. The x-axis corresponds to the number of trainable parameters (M), and the y-axis corresponds to the average score (%). (c) Comparisons for various methods on the ratio of trainable parameters and accuracy (%) when adapting LLaMA2-7B [46] in mathematical reasoning tasks.", "description": "This figure contains three subfigures. Subfigure (a) shows the architecture of the Householder Reflection Adaptation (HRA) method, illustrating how it modifies the weight matrix of a pre-trained model by multiplying it with a chain of Householder reflections. Subfigure (b) presents a comparison of HRA's performance against other state-of-the-art parameter-efficient fine-tuning (PEFT) methods on the GLUE benchmark. It shows that HRA achieves superior performance with fewer parameters. Subfigure (c) displays a comparison of different PEFT methods, including HRA, on mathematical reasoning tasks using the LLaMA2-7B model. It further demonstrates HRA's efficiency in adapting large language models.", "section": "1 Introduction"}, {"figure_path": "LzLeAscHnj/figures/figures_8_1.jpg", "caption": "Figure 1: (a) An illustration of our HRA method. (b) Comparisons for various methods on GLUE benchmark [50]. The x-axis corresponds to the number of trainable parameters (M), and the y-axis corresponds to the average score (%). (c) Comparisons for various methods on the ratio of trainable parameters and accuracy (%) when adapting LLaMA2-7B [46] in mathematical reasoning tasks.", "description": "This figure consists of three subfigures. Subfigure (a) illustrates the architecture of the proposed Householder Reflection Adaptation (HRA) method, showing how it adapts a pre-trained model by multiplying its weight matrices with orthogonal matrices constructed using a chain of Householder reflections. Subfigure (b) presents a comparison of HRA's performance against other state-of-the-art parameter-efficient fine-tuning (PEFT) methods on the GLUE benchmark, plotting average accuracy against the number of trainable parameters. Lastly, subfigure (c) shows a similar comparison but specifically for adapting the LLaMA2-7B model on mathematical reasoning tasks.", "section": "1 Introduction"}, {"figure_path": "LzLeAscHnj/figures/figures_9_1.jpg", "caption": "Figure 1: (a) An illustration of our HRA method. (b) Comparisons for various methods on GLUE benchmark [50]. The x-axis corresponds to the number of trainable parameters (M), and the y-axis corresponds to the average score (%). (c) Comparisons for various methods on the ratio of trainable parameters and accuracy (%) when adapting LLaMA2-7B [46] in mathematical reasoning tasks.", "description": "This figure presents a comparison of the proposed Householder Reflection Adaptation (HRA) method with other state-of-the-art methods for model adaptation.  Subfigure (a) shows a schematic of the HRA method, illustrating its mechanism using Householder reflections. Subfigure (b) displays a performance comparison on the GLUE benchmark, plotting average scores against the number of trainable parameters.  Finally, subfigure (c) shows a similar comparison, but specifically for adapting the LLaMA2-7B model for mathematical reasoning tasks.", "section": "1 Introduction"}, {"figure_path": "LzLeAscHnj/figures/figures_17_1.jpg", "caption": "Figure 1: (a) An illustration of our HRA method. (b) Comparisons for various methods on GLUE benchmark [50]. The x-axis corresponds to the number of trainable parameters (M), and the y-axis corresponds to the average score (%). (c) Comparisons for various methods on the ratio of trainable parameters and accuracy (%) when adapting LLaMA2-7B [46] in mathematical reasoning tasks.", "description": "This figure presents a comparison of the proposed Householder Reflection Adaptation (HRA) method with other state-of-the-art methods for model adaptation.  Subfigure (a) illustrates the HRA method's architecture, showing how it multiplies a pre-trained weight matrix with a chain of orthogonal matrices constructed by Householder reflections. Subfigure (b) shows the performance comparison on the GLUE benchmark, highlighting HRA's superior performance with fewer trainable parameters.  Finally, subfigure (c) shows the performance comparison on mathematical reasoning tasks using LLaMA2-7B, again demonstrating HRA's efficiency and effectiveness.", "section": "1 Introduction"}, {"figure_path": "LzLeAscHnj/figures/figures_18_1.jpg", "caption": "Figure 1: (a) An illustration of our HRA method. (b) Comparisons for various methods on GLUE benchmark [50]. The x-axis corresponds to the number of trainable parameters (M), and the y-axis corresponds to the average score (%). (c) Comparisons for various methods on the ratio of trainable parameters and accuracy (%) when adapting LLaMA2-7B [46] in mathematical reasoning tasks.", "description": "This figure shows three subfigures. Subfigure (a) illustrates the architecture of the proposed Householder Reflection Adaptation (HRA) method. Subfigure (b) presents a comparison of the performance of HRA against other state-of-the-art methods on the GLUE benchmark dataset, showing the average score on the y-axis against the number of trainable parameters on the x-axis. Subfigure (c) provides a similar comparison, but this time for adapting the LLaMA2-7B model to perform mathematical reasoning tasks.", "section": "1 Introduction"}, {"figure_path": "LzLeAscHnj/figures/figures_19_1.jpg", "caption": "Figure 1: (a) An illustration of our HRA method. (b) Comparisons for various methods on GLUE benchmark [50]. The x-axis corresponds to the number of trainable parameters (M), and the y-axis corresponds to the average score (%). (c) Comparisons for various methods on the ratio of trainable parameters and accuracy (%) when adapting LLaMA2-7B [46] in mathematical reasoning tasks.", "description": "This figure presents a comparison of the proposed Householder Reflection Adaptation (HRA) method with other state-of-the-art parameter-efficient fine-tuning methods.  Subfigure (a) illustrates the mechanism of HRA, showing how it adapts a pre-trained model by multiplying each weight matrix with an orthogonal matrix created by a chain of Householder reflections. Subfigure (b) shows a performance comparison on the GLUE benchmark, plotting average accuracy against the number of trainable parameters, demonstrating HRA's efficiency and effectiveness.  Finally, subfigure (c) presents a similar comparison focusing on adapting LLaMA2-7B for mathematical reasoning tasks.", "section": "1 Introduction"}, {"figure_path": "LzLeAscHnj/figures/figures_20_1.jpg", "caption": "Figure 1: (a) An illustration of our HRA method. (b) Comparisons for various methods on GLUE benchmark [50]. The x-axis corresponds to the number of trainable parameters (M), and the y-axis corresponds to the average score (%). (c) Comparisons for various methods on the ratio of trainable parameters and accuracy (%) when adapting LLaMA2-7B [46] in mathematical reasoning tasks.", "description": "This figure presents a comprehensive overview of the Householder Reflection Adaptation (HRA) method proposed in the paper. Subfigure (a) illustrates the architectural scheme of HRA, which involves multiplying frozen weight matrices by an orthogonal matrix constructed from a chain of Householder reflections. Subfigure (b) compares the performance of HRA with other state-of-the-art parameter-efficient fine-tuning methods on the GLUE benchmark, highlighting HRA's superior performance with fewer trainable parameters.  Finally, subfigure (c) demonstrates HRA's effectiveness in adapting large language models (LLMs) for mathematical reasoning tasks, showcasing its performance advantage in terms of both accuracy and parameter efficiency compared to competing methods.", "section": "1 Introduction"}, {"figure_path": "LzLeAscHnj/figures/figures_21_1.jpg", "caption": "Figure 1: (a) An illustration of our HRA method. (b) Comparisons for various methods on GLUE benchmark [50]. The x-axis corresponds to the number of trainable parameters (M), and the y-axis corresponds to the average score (%). (c) Comparisons for various methods on the ratio of trainable parameters and accuracy (%) when adapting LLaMA2-7B [46] in mathematical reasoning tasks.", "description": "This figure presents a comparison of different parameter-efficient fine-tuning (PEFT) methods.  Subfigure (a) illustrates the Householder Reflection Adaptation (HRA) method, showing how it modifies the weight matrix of a pre-trained model. Subfigures (b) and (c) show performance comparisons on the GLUE benchmark and mathematical reasoning tasks, respectively, demonstrating HRA's effectiveness in adapting large language models with fewer parameters and achieving higher accuracy.", "section": "1 Introduction"}]