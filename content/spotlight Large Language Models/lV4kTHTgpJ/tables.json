[{"figure_path": "lV4kTHTgpJ/tables/tables_7_1.jpg", "caption": "Table 1: Results on Medium-Sized Language Models. We conduct the text classification task using ROBERTa-base on a subset of the GLUE benchmark datasets, and the question-answering task using T5-base on the SQuAD2.0 dataset. ACC, F1, and EM denote accuracy, F1 score, and Exact Match, respectively.", "description": "This table presents the results of different model fusion methods and baselines on two NLP tasks: text classification using ROBERTA-base and question answering using T5-base.  The GLUE benchmark datasets are used for text classification, while the SQuAD2.0 dataset is used for question answering.  Performance is measured using accuracy (ACC), F1 score (F1), and exact match (EM) as appropriate for each task.  The table allows for comparison of various methods to enhance model fusion techniques.", "section": "6 Experiments"}, {"figure_path": "lV4kTHTgpJ/tables/tables_8_1.jpg", "caption": "Table 2: Results on Large Language Models. We compare the results of BOMF and baseline methods using the SAMSum and KorMCQA datasets for summarization and medical multiple choice question-answering tasks with LLAMA2-7B and LLaMA3-8B. R1, R2, and RL denote Rouge-1, Rouge-2, and Rouge-L scores for summarization. Doctor, Nurse, and Pharm denote evaluation results for medical question answering in each respective field, using accuracy as the metric.", "description": "This table compares the performance of BOMF against several baseline methods on two tasks using large language models: summarization (SAMSum dataset) and Korean medical multiple choice question answering (KorMCQA dataset).  For summarization, Rouge scores (R1, R2, RL) are reported. For the question-answering task, accuracy is reported for three different professions (doctor, nurse, pharmacist). The results are shown for two different large language models: LLAMA2-7B and LLAMA3-8B.", "section": "6.2 Empirical Analysis on Large Language Models"}, {"figure_path": "lV4kTHTgpJ/tables/tables_8_2.jpg", "caption": "Table 2: Results on Large Language Models. We compare the results of BOMF and baseline methods using the SAMSum and KorMCQA datasets for summarization and medical multiple choice question-answering tasks with LLAMA2-7B and LLaMA3-8B. R1, R2, and RL denote Rouge-1, Rouge-2, and Rouge-L scores for summarization. Doctor, Nurse, and Pharm denote evaluation results for medical question answering in each respective field, using accuracy as the metric.", "description": "This table presents a comparison of the performance of BOMF and several baseline methods on two tasks using large language models: summarization (SAMSum dataset) and medical multiple-choice question answering (KorMCQA dataset).  The models used are LLAMA2-7B and LLAMA3-8B.  For summarization, performance is measured using Rouge-1, Rouge-2, and Rouge-L scores. For the medical question answering task, accuracy is reported for three categories: Doctor, Nurse, and Pharmacist. The table helps assess BOMF's effectiveness compared to established methods in these specific contexts.", "section": "6.2 Empirical Analysis on Large Language Models"}, {"figure_path": "lV4kTHTgpJ/tables/tables_9_1.jpg", "caption": "Table 1: Results on Medium-Sized Language Models. We conduct the text classification task using ROBERTa-base on a subset of the GLUE benchmark datasets, and the question-answering task using T5-base on the SQuAD2.0 dataset. ACC, F1, and EM denote accuracy, F1 score, and Exact Match, respectively.", "description": "This table presents the results of different model fusion techniques and baselines on two NLP tasks: text classification using the RoBERTa-base model and question answering using the T5-base model.  The GLUE benchmark datasets are used for text classification, and the SQuAD2.0 dataset is used for question answering.  The table shows performance metrics for each method across various datasets, including accuracy (ACC), F1 score (F1), and Exact Match (EM).", "section": "6.1 Empirical Analysis on Medium-Sized Language Models"}, {"figure_path": "lV4kTHTgpJ/tables/tables_9_2.jpg", "caption": "Table 1: Results on Medium-Sized Language Models. We conduct the text classification task using ROBERTa-base on a subset of the GLUE benchmark datasets, and the question-answering task using T5-base on the SQuAD2.0 dataset. ACC, F1, and EM denote accuracy, F1 score, and Exact Match, respectively.", "description": "This table compares the performance of different model fusion techniques and fine-tuning methods on medium-sized language models (RoBERTa-base and T5-base) for text classification and question answering tasks.  It shows the accuracy (ACC), F1 score (F1), and exact match (EM) for each method across different datasets.  The results highlight the effectiveness of the proposed BOMF (Bayesian Optimization Model Fusion) method compared to several baselines.", "section": "6.1 Empirical Analysis on Medium-Sized Language Models"}, {"figure_path": "lV4kTHTgpJ/tables/tables_15_1.jpg", "caption": "Table 1: Results on Medium-Sized Language Models. We conduct the text classification task using ROBERTa-base on a subset of the GLUE benchmark datasets, and the question-answering task using T5-base on the SQuAD2.0 dataset. ACC, F1, and EM denote accuracy, F1 score, and Exact Match, respectively.", "description": "This table compares the performance of different model fusion methods (BOMF and baselines) on medium-sized language models (ROBERTa-base and T5-base).  The models were fine-tuned on text classification tasks from the GLUE benchmark and a question answering task from SQuAD2.0.  The table shows the accuracy (ACC), F1 score, and Exact Match (EM) for each model and dataset.  The results demonstrate the effectiveness of BOMF in improving model performance.", "section": "6 Experiments"}, {"figure_path": "lV4kTHTgpJ/tables/tables_16_1.jpg", "caption": "Table 1: Results on Medium-Sized Language Models. We conduct the text classification task using ROBERTa-base on a subset of the GLUE benchmark datasets, and the question-answering task using T5-base on the SQuAD2.0 dataset. ACC, F1, and EM denote accuracy, F1 score, and Exact Match, respectively.", "description": "This table compares the performance of different model fusion methods (BOMF and baselines) on two medium-sized language models: RoBERTa-base and T5-base.  The models were fine-tuned on various text classification tasks from the GLUE benchmark and a question answering task using the SQuAD2.0 dataset. The table reports accuracy (ACC), F1 score, and exact match (EM) for each method across multiple datasets.  This provides a quantitative assessment of the proposed BOMF method against well-established baselines. ", "section": "6.1 Empirical Analysis on Medium-Sized Language Models"}, {"figure_path": "lV4kTHTgpJ/tables/tables_17_1.jpg", "caption": "Table 1: Results on Medium-Sized Language Models. We conduct the text classification task using ROBERTa-base on a subset of the GLUE benchmark datasets, and the question-answering task using T5-base on the SQuAD2.0 dataset. ACC, F1, and EM denote accuracy, F1 score, and Exact Match, respectively.", "description": "This table presents the results of experiments conducted on medium-sized language models (ROBERTa-base and T5-base) for text classification and question answering tasks.  It compares the performance of BOMF (the proposed method) against several baseline methods across various datasets (a subset of GLUE and SQuAD2.0).  The results are reported using accuracy (ACC), F1-score (F1), and Exact Match (EM) as evaluation metrics, providing a comprehensive comparison of the model's performance on different tasks.", "section": "6.1 Empirical Analysis on Medium-Sized Language Models"}, {"figure_path": "lV4kTHTgpJ/tables/tables_18_1.jpg", "caption": "Table 1: Results on Medium-Sized Language Models. We conduct the text classification task using ROBERTa-base on a subset of the GLUE benchmark datasets, and the question-answering task using T5-base on the SQuAD2.0 dataset. ACC, F1, and EM denote accuracy, F1 score, and Exact Match, respectively.", "description": "This table presents the results of different model fusion methods and baselines on medium-sized language models (RoBERTa-base and T5-base) for text classification (using a subset of GLUE benchmark datasets) and question answering (using the SQuAD2.0 dataset).  It compares the performance of BOMF (the proposed method) against several baseline methods like Grid Fine-Tuning, HPBO, SWA, OTfusion, Greedy SWA, Learned SWA, and TWA.  The evaluation metrics used are Accuracy (ACC), F1 score (F1), and Exact Match (EM).", "section": "6.1 Empirical Analysis on Medium-Sized Language Models"}, {"figure_path": "lV4kTHTgpJ/tables/tables_20_1.jpg", "caption": "Table 1: Results on Medium-Sized Language Models. We conduct the text classification task using ROBERTa-base on a subset of the GLUE benchmark datasets, and the question-answering task using T5-base on the SQuAD2.0 dataset. ACC, F1, and EM denote accuracy, F1 score, and Exact Match, respectively.", "description": "This table presents the results of experiments conducted on medium-sized language models (RoBERTa-base and T5-base).  It compares the performance of the proposed BOMF method against several baseline methods across various text classification (using GLUE benchmark datasets) and question answering (using SQuAD2.0) tasks.  The results are reported in terms of accuracy (ACC), F1 score (F1), and exact match (EM) metrics, providing a comprehensive performance comparison.", "section": "6.1 Empirical Analysis on Medium-Sized Language Models"}, {"figure_path": "lV4kTHTgpJ/tables/tables_24_1.jpg", "caption": "Table 1: Results on Medium-Sized Language Models. We conduct the text classification task using ROBERTa-base on a subset of the GLUE benchmark datasets, and the question-answering task using T5-base on the SQuAD2.0 dataset. ACC, F1, and EM denote accuracy, F1 score, and Exact Match, respectively.", "description": "This table presents the results of different model fusion and fine-tuning methods on medium-sized language models (ROBERTa-base and T5-base).  It compares the performance of BOMF against several baseline methods across multiple text classification and question-answering tasks, using standard evaluation metrics (accuracy, F1 score, exact match).  The results highlight the effectiveness of BOMF in achieving state-of-the-art results on these tasks.", "section": "6.1 Empirical Analysis on Medium-Sized Language Models"}, {"figure_path": "lV4kTHTgpJ/tables/tables_24_2.jpg", "caption": "Table 11: Spearman's rank correlation coefficient value between loss and metric for the various optimization processes in large-scale tasks. We evaluate the LLAMA2-7B on the SAMSum dataset. Here, Loss BO SWA and Metric BO SWA denote the approach where we exclusively employ either the loss function or the metric during the MOBO process, respectively.", "description": "This table shows the Spearman's rank correlation between loss and metrics (R1, R2, RL) for different optimization strategies on the SAMSum dataset using the LLAMA2-7B model.  It compares the baseline (HPBO), using only loss for optimization; Loss BO SWA, using only loss in the MOBO process; Metric BO SWA, only using metrics in the MOBO process; and BOMF, using both loss and metrics. The results highlight the impact of incorporating multiple objectives into the optimization process.", "section": "Additional Ablation Experiments"}, {"figure_path": "lV4kTHTgpJ/tables/tables_24_3.jpg", "caption": "Table 1: Results on Medium-Sized Language Models. We conduct the text classification task using ROBERTa-base on a subset of the GLUE benchmark datasets, and the question-answering task using T5-base on the SQuAD2.0 dataset. ACC, F1, and EM denote accuracy, F1 score, and Exact Match, respectively.", "description": "This table presents the performance comparison of different optimization strategies (including the proposed BOMF) on two medium-sized language models: ROBERTa-base for text classification tasks using the GLUE benchmark, and T5-base for question answering using the SQuAD2.0 dataset.  The results are shown for various metrics: accuracy (ACC), F1 score (F1), and exact match (EM).", "section": "6.1 Empirical Analysis on Medium-Sized Language Models"}, {"figure_path": "lV4kTHTgpJ/tables/tables_25_1.jpg", "caption": "Table 13: Evaluation Results Using ChatGPT-3.5-Turbo. (a) Baseline, (b) SWA, (c) Greedy SWA, (d) Learned SWA, (e) BOMF, and (f) ChatGPT BOMF.", "description": "This table compares the performance of different model fusion methods, including BOMF, using a ChatGPT-based evaluation approach.  The evaluation involves a human-like grading task of the similarity between student-submitted answers and the ground truth, providing a numerical score for each model.", "section": "6.3 Ablation Study"}, {"figure_path": "lV4kTHTgpJ/tables/tables_25_2.jpg", "caption": "Table 1: Results on Medium-Sized Language Models. We conduct the text classification task using ROBERTa-base on a subset of the GLUE benchmark datasets, and the question-answering task using T5-base on the SQuAD2.0 dataset. ACC, F1, and EM denote accuracy, F1 score, and Exact Match, respectively.", "description": "This table presents the results of the text classification task using the RoBERTa-base model on a subset of the GLUE benchmark datasets and the question-answering task using the T5-base model on the SQuAD2.0 dataset.  It compares the performance of the BOMF method to several baseline methods (Grid Fine-Tune, HPBO (Full), SWA, OTFUSION, Greedy SWA, Learned SWA, TWA) across different metrics (accuracy, F1 score, exact match).  The results show the effectiveness of BOMF compared to other baselines.", "section": "6.1 Empirical Analysis on Medium-Sized Language Models"}, {"figure_path": "lV4kTHTgpJ/tables/tables_26_1.jpg", "caption": "Table 1: Results on Medium-Sized Language Models. We conduct the text classification task using ROBERTa-base on a subset of the GLUE benchmark datasets, and the question-answering task using T5-base on the SQuAD2.0 dataset. ACC, F1, and EM denote accuracy, F1 score, and Exact Match, respectively.", "description": "This table compares the performance of different model fusion techniques (BOMF and baselines) on two NLP tasks: text classification using ROBERTa-base and question answering using T5-base.  It shows the accuracy (ACC), F1 score (F1), and Exact Match (EM) for each method across various datasets.  The results highlight the improved performance of the proposed BOMF method.", "section": "6.1 Empirical Analysis on Medium-Sized Language Models"}, {"figure_path": "lV4kTHTgpJ/tables/tables_26_2.jpg", "caption": "Table 1: Results on Medium-Sized Language Models. We conduct the text classification task using ROBERTa-base on a subset of the GLUE benchmark datasets, and the question-answering task using T5-base on the SQuAD2.0 dataset. ACC, F1, and EM denote accuracy, F1 score, and Exact Match, respectively.", "description": "This table presents the performance comparison of different model fusion methods and baselines on two medium-sized language models: ROBERTA-base for text classification (using GLUE benchmark datasets), and T5-base for question answering (using SQuAD2.0 dataset).  The results show accuracy (ACC), F1-score (F1), and Exact Match (EM) for each method and dataset.  It helps to evaluate the effectiveness of various model fusion strategies in improving the performance of pre-trained language models on downstream NLP tasks.", "section": "6.1 Empirical Analysis on Medium-Sized Language Models"}, {"figure_path": "lV4kTHTgpJ/tables/tables_26_3.jpg", "caption": "Table 1: Results on Medium-Sized Language Models. We conduct the text classification task using ROBERTa-base on a subset of the GLUE benchmark datasets, and the question-answering task using T5-base on the SQuAD2.0 dataset. ACC, F1, and EM denote accuracy, F1 score, and Exact Match, respectively.", "description": "This table presents the results of the proposed BOMF method and several baseline methods on two medium-sized language models: RoBERTa-base for text classification on the GLUE benchmark and T5-base for question answering on SQuAD2.0.  The table compares the performance across different datasets using metrics like accuracy (ACC), F1 score (F1), and Exact Match (EM).", "section": "6.1 Empirical Analysis on Medium-Sized Language Models"}]