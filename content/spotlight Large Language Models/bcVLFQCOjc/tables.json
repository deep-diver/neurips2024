[{"figure_path": "bcVLFQCOjc/tables/tables_2_1.jpg", "caption": "Table 1: Breakdown of the number of unique TikZ graphics in DATIKZv2 compared to its predecessor DATIKZv1.", "description": "This table shows the number of unique TikZ graphics included in the DATIKZv2 dataset, broken down by source (curated, TeX.SE, arXiv, artificial).  It also provides a comparison to the number of graphics in the previous version of the dataset, DATIKZv1, highlighting the significant increase in size of the DATIKZv2 dataset.", "section": "Datasets"}, {"figure_path": "bcVLFQCOjc/tables/tables_5_1.jpg", "caption": "Table 2: System-level scores for output-driven inference (DETIKZIFY abbreviated as DT). Bold and underlined values indicate the best and second-best scores for each metric column, respectively. Cell shading reflects the relative score magnitudes across input types. Arrows indicate metric directionality.", "description": "This table presents the results of an automatic evaluation of DETIKZIFY and several baselines on the task of generating TikZ code from images.  The evaluation focuses on output-driven inference (OI), where the models generate code until a successful compilation is achieved. The table shows various metrics for evaluating the generated code, including: Mean Token Efficiency (MTE), which measures the efficiency of code generation; CrystalBLEU (cBLEU), which measures the similarity between generated and reference code;  TEX Edit Distance (TED), measuring the edit distance between generated and reference code; DREAMSIM, SELFSIM, and SSIM, which are perceptual similarity metrics comparing generated and reference images; and Kernel Inception Distance (KID), which measures the distribution difference between the generated and reference images. Higher scores for MTE, cBLEU, DSIM, SSIM, and AVG are better, while lower scores for TED and KID are preferable.  The table breaks down the results for models using either reference figures or synthetic sketches as input. ", "section": "6.1 Automatic Evaluation"}, {"figure_path": "bcVLFQCOjc/tables/tables_6_1.jpg", "caption": "Table 3: System-level scores for time-budgeted inference, displaying relative changes for metrics shared with output-driven inference (Table 2; colored green for improvements and red for declines) and absolute scores for independent metrics. Bold and underlined values indicate the best and second-best absolute scores for each metric column, respectively. Arrows indicate metric directionality.", "description": "This table presents the results of a time-budgeted inference experiment, comparing the performance of four different DETIKZIFY models (with varying sizes and training data) on two tasks: generating TikZ code from reference figures and from synthetic sketches.  It shows both relative changes (compared to the output-driven inference results in Table 2) and absolute scores for various metrics, including code similarity (CBLEU, TED), image similarity (DSIM, SSIM, KID), and overall average similarity (AVG).  The table highlights the best performing models for each metric and input type (figures vs. sketches).", "section": "6.1 Automatic Evaluation"}, {"figure_path": "bcVLFQCOjc/tables/tables_8_1.jpg", "caption": "Table 5: Correlations of image similarity metrics with humans at the segment and system level.", "description": "This table shows the correlation between image similarity metrics (LPIPS, DISTS, DSIM, SSIM) and human judgments at both segment and system levels.  The higher the correlation value, the better the metric aligns with human perception of similarity.  The table highlights that SELFSIM shows the strongest correlation at the segment level, while DREAMSIM has the highest correlation at the system level, indicating their relative effectiveness in evaluating image similarity.", "section": "Analysis"}, {"figure_path": "bcVLFQCOjc/tables/tables_20_1.jpg", "caption": "Table 2: System-level scores for output-driven inference (DETIKZIFY abbreviated as DT). Bold and underlined values indicate the best and second-best scores for each metric column, respectively. Cell shading reflects the relative score magnitudes across input types. Arrows indicate metric directionality.", "description": "This table presents the performance comparison of different models on the task of generating TikZ code from images.  The models compared include CLAUDE 3, GPT-4V, and several variations of the DETIKZIFY model (with different sizes and training configurations).  The metrics used to evaluate the generated TikZ code are Mean Token Efficiency (MTE), CrystalBLEU (cBLEU), TEX Edit Distance (TED), DREAMSIM (DSIM), Structural Similarity Index (SSIM), Kernel Inception Distance (KID), and the average of all similarity metrics (AVG). The table shows results for both reference figures and synthetic sketches as input to the models.", "section": "6.1 Automatic Evaluation"}, {"figure_path": "bcVLFQCOjc/tables/tables_21_1.jpg", "caption": "Table 7: Ablation study results for DT-TL1.1B (OI), showing the relative impact on test set performance when either sketch-based training or connector pretraining is omitted, compared to full training. Improvements are highlighted in green, and declines in red, with reference scores taken from Table 2.", "description": "This table presents the results of an ablation study conducted on the DETIKZIFY-TL1.1B model using output-driven inference. It investigates the impact of removing either sketch-based training or connector pre-training from the model's training process.  The table shows the relative changes in various metrics (MTE, cBLEU, TED, DSIM, SSIM, KID) for both reference figures and synthetic sketches as input when comparing the full training to the models trained without sketch-based training or without connector pre-training.  Positive changes are highlighted in green, while negative changes are in red.  Reference scores are taken from Table 2.", "section": "6 Experiments"}, {"figure_path": "bcVLFQCOjc/tables/tables_26_1.jpg", "caption": "Table 2: System-level scores for output-driven inference (DETIKZIFY abbreviated as DT). Bold and underlined values indicate the best and second-best scores for each metric column, respectively. Cell shading reflects the relative score magnitudes across input types. Arrows indicate metric directionality.", "description": "This table presents the quantitative results of the DETIKZIFY model's performance on the output-driven inference task.  It compares DETIKZIFY against two baseline models (CLAUDE 3 and GPT-4V) across various metrics. These metrics assess both the code quality (MTE, cBLEU, TED) and the visual similarity between the generated and reference figures (DSIM, SSIM, KID, AVG).  The table highlights the superior performance of DETIKZIFY, particularly the larger variants, in generating high-quality and visually accurate TikZ code from both reference figures and synthetic sketches.", "section": "6.1 Automatic Evaluation"}]