[{"figure_path": "qGiZQb1Khm/figures/figures_0_1.jpg", "caption": "Figure 1: Bob fine-tunes his LLM on data with a fraction coming from Alice's LLM. This leaves traces in Bob's model that Alice can detect reliably, provided that her text was watermarked. Thus, a side effect of Alice's watermark, intended for machine-generated text detection, is to reveal what data Bob's model was fine-tuned on.", "description": "The figure illustrates the concept of \"radioactivity\" in LLMs. Alice uses a watermarked LLM to generate text. Bob then fine-tunes his own LLM using a dataset that includes some of Alice's watermarked text.  Alice can then detect the presence of her watermarked text in Bob's fine-tuned LLM by using a statistical test, thus detecting whether Bob trained his model using any of her LLM's outputs. This demonstrates how watermarked data used in training a model leaves detectable traces in the resulting model.", "section": "1 Introduction"}, {"figure_path": "qGiZQb1Khm/figures/figures_2_1.jpg", "caption": "Figure 1: Bob fine-tunes his LLM on data with a fraction coming from Alice's LLM. This leaves traces in Bob's model that Alice can detect reliably, provided that her text was watermarked. Thus, a side effect of Alice's watermark, intended for machine-generated text detection, is to reveal what data Bob's model was fine-tuned on.", "description": "This figure illustrates the core concept of the paper:  Alice's watermarked LLM outputs are used as part of Bob's training data for his LLM.  Even though the proportion of watermarked data is small, the watermark leaves detectable traces in Bob's model, allowing Alice to reliably determine whether Bob trained his model using her data. This demonstrates the \"radioactivity\" of LLMs.", "section": "1 Introduction"}, {"figure_path": "qGiZQb1Khm/figures/figures_3_1.jpg", "caption": "Figure 2: Detection performance mainly depends on p = |DA|/|D| and d = |DA|/|\u010eA|, where D is the fine-tuning dataset used by Bob, \u010eA are the outputs from Alice's model, and DA the intersection of both.", "description": "This figure is a Venn diagram showing the relationship between three sets of data: D, \u010eA, and DA.  D represents the entire dataset used by Bob to fine-tune his language model.  \u010eA represents the set of all outputs generated by Alice's language model. DA represents the intersection of D and \u010eA \u2013 the portion of Bob's training data that originated from Alice's model.  The diagram visually explains that the detection performance (radioactivity) primarily depends on two factors: (1) p, which represents the proportion of Bob's training data derived from Alice's model; and (2) d, which represents the degree of supervision or the extent to which Alice knows Bob's training data. ", "section": "3 Problem Formulation"}, {"figure_path": "qGiZQb1Khm/figures/figures_4_1.jpg", "caption": "Figure 1: Bob fine-tunes his LLM on data with a fraction coming from Alice's LLM. This leaves traces in Bob's model that Alice can detect reliably, provided that her text was watermarked. Thus, a side effect of Alice's watermark, intended for machine-generated text detection, is to reveal what data Bob's model was fine-tuned on.", "description": "This figure illustrates the concept of \"radioactivity\" in LLMs. Alice fine-tunes her language model (LLM) with watermarked text. Bob then fine-tunes his own LLM using a dataset that includes some of Alice's watermarked outputs.  Alice can then use her watermark detection method to detect traces of her original watermarked data in Bob's fine-tuned model, even though Bob may not have intentionally included her data and is unaware of the watermark. This demonstrates the \"radioactive\" nature of the watermarked data, leaving detectable traces in models trained on it.", "section": "1 Introduction"}, {"figure_path": "qGiZQb1Khm/figures/figures_6_1.jpg", "caption": "Figure 1: Bob fine-tunes his LLM on data with a fraction coming from Alice's LLM. This leaves traces in Bob's model that Alice can detect reliably, provided that her text was watermarked. Thus, a side effect of Alice's watermark, intended for machine-generated text detection, is to reveal what data Bob's model was fine-tuned on.", "description": "This figure illustrates the concept of \"radioactivity\" in LLMs. Alice's LLM generates watermarked text. Bob then fine-tunes his LLM using a dataset that includes some of Alice's watermarked text.  Alice can then detect the presence of her watermarked text in Bob's fine-tuned model, even if only a small portion of Bob's training data came from her model. This demonstrates that the watermark leaves detectable traces in models trained on the watermarked data, revealing information about the training data used.", "section": "1 Introduction"}, {"figure_path": "qGiZQb1Khm/figures/figures_7_1.jpg", "caption": "Figure 5: Radioactivity detection results. Average of log10(p) over 10 runs (\u2193 is better). Bars indicate standard deviations. The detection methods are detailed in Sec. 4. In the supervised closed-model setting, our tests detect radioactivity (p < 10\u22125) when only 1% of training data is watermarked. In the absence of watermarked data, all tests output random p-values.", "description": "This figure shows the results of radioactivity detection experiments under various conditions.  The x-axis represents the percentage of watermarked training data used in fine-tuning model B. The y-axis shows the average base-10 logarithm of the p-value obtained from the statistical test for radioactivity. Lower values indicate stronger evidence of radioactivity. The results are shown separately for four scenarios: open model with supervision, open model without supervision, closed model with supervision, and closed model without supervision. The error bars represent the standard deviations across 10 runs of the experiment. The figure demonstrates that even a small percentage of watermarked data can lead to detectable radioactivity, particularly in open-model settings. When no watermarked data is used, the p-values are randomly distributed around 0.", "section": "Detection results"}, {"figure_path": "qGiZQb1Khm/figures/figures_7_2.jpg", "caption": "Figure 6: Influence of the filter on scored tokens. log10(p) as a function of the number of generated tokens in the supervised closed-model setting with p = 1%. We perform the watermark detection test on text generated by B with prompts from \u010eA. When filtering, we only score k-grams that were part of \u010eA.", "description": "This figure shows the impact of using a filter on the performance of the radioactivity detection test.  The test is performed in a closed-model setting where only 1% of the training data is watermarked.  The x-axis represents the number of tokens generated by model B, and the y-axis shows the log10 of the p-value.  Two lines are plotted: one for the test without a filter and one with a filter. The filtered test shows a significantly lower p-value (stronger evidence of radioactivity) for the same number of generated tokens.", "section": "4.2 Radioactivity detection in practice"}, {"figure_path": "qGiZQb1Khm/figures/figures_17_1.jpg", "caption": "Figure 7: Box plot for the log10(p) in the open/unsupervised setting with varying p, the proportion of B\u2019s fine-tuning data watermarked. This corresponds to the values presented in Fig. 5 where the means are reported.", "description": "This box plot visualizes the distribution of log10(p-values) obtained from the radioactivity detection test in the open-model and unsupervised setting.  The x-axis represents different proportions (p) of watermarked data used for fine-tuning model B, while the y-axis shows the log10(p-value). Each box represents the interquartile range (IQR), the median is marked by the orange line, and the mean is marked by the red cross. The whiskers extend to the maximum and minimum values, excluding outliers. The plot demonstrates how the distribution of p-values shifts toward stronger evidence of radioactivity (lower p-values) as the proportion of watermarked data increases.", "section": "D.2 Correctness"}, {"figure_path": "qGiZQb1Khm/figures/figures_19_1.jpg", "caption": "Figure 7: Box plot for the log10(p) in the open/unsupervised setting with varying p, the proportion of B\u2019s fine-tuning data watermarked. This corresponds to the values presented in Fig. 5 where the means are reported.", "description": "This figure shows the results of the radioactivity detection test in the open/unsupervised setting. The y-axis represents the log10(p-value), and the x-axis represents the proportion of watermarked training data (p).  The box plot shows the distribution of log10(p-values) across multiple runs. The mean log10(p-value) is shown as a solid line, and the standard deviation is represented by a shaded area. For each proportion of watermarked data, ten runs of the experiment were performed, and these results are displayed as box plots.", "section": "D.2 Correctness"}, {"figure_path": "qGiZQb1Khm/figures/figures_19_2.jpg", "caption": "Figure 7: Box plot for the log10(p) in the open/unsupervised setting with varying p, the proportion of B's fine-tuning data watermarked. This corresponds to the values presented in Fig. 5 where the means are reported.", "description": "This figure shows the distribution of p-values obtained from the radioactivity detection test in an open-model, unsupervised setting, across various proportions (p) of watermarked data used in fine-tuning model B.  The box plots illustrate the median, mean, and standard deviation of the log10(p) values.  It visually represents the data presented in Figure 5, where only the mean values were shown.", "section": "D.2 Correctness"}, {"figure_path": "qGiZQb1Khm/figures/figures_20_1.jpg", "caption": "Figure 10: Comparative analysis of membership inference and watermarking for radioactivity detection, in the open-model setup. (Left) MIA aims to detect the difference between the two distributions. It gets harder as d decreases, since the actual fine-tuning data is mixed with texts that Bob did not use. (Right) Therefore, for low degrees of supervision (< 2%), MIA is no longer effective, while WM detection gives p-values lower than 10-5.", "description": "This figure compares the performance of membership inference attacks (MIA) and watermarking-based methods for detecting radioactivity in a language model.  The left panel shows the distributions of calibrated loss from model B (the suspect model) for data that was and wasn't used in training.  The difference in distributions becomes less pronounced as the degree of supervision (d) decreases, indicating that MIA becomes less effective with less overlap between training and test data. The right panel illustrates that the watermarking method consistently achieves high detection confidence (p-value < 10^-5) even with low supervision (d < 2%), demonstrating its superior performance compared to MIA in detecting model imitation.", "section": "E Discussion on Other Approaches"}, {"figure_path": "qGiZQb1Khm/figures/figures_20_2.jpg", "caption": "Figure 10: Comparative analysis of membership inference and watermarking for radioactivity detection, in the open-model setup. (Left) MIA aims to detect the difference between the two distributions. It gets harder as d decreases, since the actual fine-tuning data is mixed with texts that Bob did not use. (Right) Therefore, for low degrees of supervision (< 2%), MIA is no longer effective, while WM detection gives p-values lower than 10-5.", "description": "This figure compares two methods for detecting model imitation: Membership Inference Attacks (MIA) and watermark detection.  The left panel shows the distribution of perplexity scores for models trained with and without data from a source model.  The difference in distributions is used to detect imitation. However, the effectiveness of MIA diminishes as the proportion of imitated data decreases (low supervision). The right panel shows that watermark detection remains effective even at low supervision levels, providing more reliable detection of model imitation.", "section": "E Discussion on Other Approaches"}, {"figure_path": "qGiZQb1Khm/figures/figures_22_1.jpg", "caption": "Figure 11: Detailed results for the influence of the number of epochs when B is fine-tuned on p = 100% of watermarked data. The longer the fine-tuning lasts, the more the watermarking leaves traces in the model.", "description": "This figure shows how the number of epochs during the fine-tuning process affects the detection of watermarks. As the number of epochs increases, the watermark becomes more prominent, leading to a stronger detection signal (lower p-value).", "section": "5. Radioactivity in Instruction Datasets"}, {"figure_path": "qGiZQb1Khm/figures/figures_22_2.jpg", "caption": "Figure 12: Detailed influence of the filter. Box plots of the log10(p) in the closed-model setting with p = 10%. We perform the watermark detection test on text generated by B. The baseline uses the default scoring (no filters). In the unsupervised scenario, scoring is confined to k-grams generated in new watermarked data produced by A. In the supervised scenario, scoring is limited to k-grams present in DA.", "description": "This figure shows box plots illustrating the impact of using a filter on the log10(p)-values obtained from a radioactivity detection test in a closed-model setting, where only 10% of the fine-tuning data was watermarked. Three scenarios are compared: a baseline without filtering, an unsupervised setting where the filter is constructed from newly generated watermarked data, and a supervised setting where the filter is built using the known watermarked data from the training set (DA).  The plots display the median and mean log10(p)-values for each scenario, providing a visual representation of how the filtering method enhances the accuracy of radioactivity detection by focusing the analysis on specific k-grams.", "section": "F.6 Impact of the filter"}, {"figure_path": "qGiZQb1Khm/figures/figures_23_1.jpg", "caption": "Figure 6: Influence of the filter on scored tokens. log10(p) as a function of the number of generated tokens in the supervised closed-model setting with p = 1%. We perform the watermark detection test on text generated by B with prompts from \u010eA. When filtering, we only score k-grams that were part of \u010eA.", "description": "This figure shows how the p-value (a measure of statistical significance) changes as more tokens are scored in a closed-model setting where only 1% of the training data was watermarked.  The x-axis shows the number of generated tokens, and the y-axis represents the log10 of the p-value. Two lines are displayed, one for when a filter is used and one without.  The filter helps focus the analysis on specific tokens, making it easier to detect the watermark even with a small amount of watermarked data in the training set. A smaller p-value indicates stronger evidence that the model was indeed trained on watermarked data.", "section": "4.2 Radioactivity detection in practice"}, {"figure_path": "qGiZQb1Khm/figures/figures_25_1.jpg", "caption": "Figure 14: Bit accuracy when watermarked instruction data are generated with MPAC [Yoo et al., 2024], against the number of scored tokens generated by the fine-tuned model. This is done under the supervised/closed-model setup, for various lengths (n=8, 16, 32) of the message.", "description": "This figure shows the bit accuracy of watermark extraction using the MPAC method under different conditions.  The x-axis represents the number of scored tokens, and the y-axis represents the bit accuracy.  There are three plots, one for each proportion (p) of watermarked data used in fine-tuning: 0%, 10%, and 100%.  Each plot shows curves for different message lengths (8, 16, and 32 bits).  The results indicate that bit accuracy increases with the number of scored tokens and the proportion of watermarked data, but decreases with message length.  This suggests that even multi-bit watermarking schemes exhibit radioactivity.", "section": "G.2 Multi-bit scenario"}]