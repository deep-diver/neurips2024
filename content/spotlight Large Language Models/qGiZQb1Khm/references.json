{"references": [{"fullname_first_author": "Scott Aaronson", "paper_title": "Watermarking GPT outputs", "publication_date": "2023-02-01", "reason": "This paper introduces a watermarking method for large language model outputs, which is the core technology whose radioactivity is analyzed in this paper."}, {"fullname_first_author": "John Kirchenbauer", "paper_title": "A watermark for large language models", "publication_date": "2023-01-01", "reason": "This paper presents a watermarking method used in the experiments to evaluate the radioactivity of watermarking in LLMs, providing a practical implementation for the core concept."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper introduces instruction fine-tuning, a key technique used in the experiments to train LLMs which are then evaluated for radioactivity."}, {"fullname_first_author": "Rohan Taori", "paper_title": "Stanford Alpaca: An instruction-following LLaMA model", "publication_date": "2023-03-01", "reason": "This paper provides details on the fine-tuning method used in the experiments, which is crucial for understanding how the models were prepared."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-01", "reason": "This paper describes Llama-2, a large language model used as a source of watermarked data in the experiments, and its characteristics are crucial for understanding the results."}]}