[{"type": "text", "text": "Context and Geometry Aware Voxel Transformer for Semantic Scene Completion ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhu Yu1 Runmin Zhang1 Jiacheng Ying1 Junchen Yu1 Xiaohai Hu3 Lun Luo4 Si-Yuan Cao2,1\u2217 Hui-Liang Shen1\u2217 ", "page_idx": 0}, {"type": "text", "text": "1Zhejiang University 2Ningbo Innovation Center, Zhejiang University 3University of Washington 4HAOMO.AI Technology Co., Ltd. https://github.com/pkqbajng/CGFormer ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vision-based Semantic Scene Completion (SSC) has gained much attention due to its widespread applications in various 3D perception tasks. Existing sparseto-dense approaches typically employ shared context-independent queries across various input images, which fails to capture distinctions among them as the focal regions of different inputs vary and may result in undirected feature aggregation of cross-attention. Additionally, the absence of depth information may lead to points projected onto the image plane sharing the same 2D position or similar sampling points in the feature map, resulting in depth ambiguity. In this paper, we present a novel context and geometry aware voxel transformer. It utilizes a context aware query generator to initialize context-dependent queries tailored to individual input images, effectively capturing their unique characteristics and aggregating information within the region of interest. Furthermore, it extend deformable crossattention from 2D to 3D pixel space, enabling the differentiation of points with similar image coordinates based on their depth coordinates. Building upon this module, we introduce a neural network named CGFormer to achieve semantic scene completion. Simultaneously, CGFormer leverages multiple 3D representations (i.e., voxel and TPV) to boost the semantic and geometric representation abilities of the transformed 3D volume from both local and global perspectives. Experimental results demonstrate that CGFormer achieves state-of-the-art performance on the SemanticKITTI and SSCBench-KITTI-360 benchmarks, attaining a mIoU of 16.87 and 20.05, as well as an IoU of 45.99 and 48.07, respectively. Remarkably, CGFormer even outperforms approaches employing temporal images as inputs or much larger image backbone networks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Semantic Scene Completion (SSC) aims to jointly infer the complete scene geometry and semantics. It serves as a crucial step in a wide range of 3D perception tasks such as autonomous driving [11, 24], robotic navigation [42, 15], mapping and planning [34]. SSCNet [40] initially formulates the semantic scene completion task. Subsequently, many LiDAR-based approaches [5, 51, 38] have been proposed, but these approaches usually suffer from high-cost sensors. ", "page_idx": 0}, {"type": "text", "text": "Recently, there has been a shift towards vision-based SSC solutions. MonoScene [3] lifts the input 2D images to 3D volumes by densely assigning the same 2D features to both visible and occluded regions, leading to many ambiguities. With advancements in bird\u2019s-eye-view (BEV) perception [24, 54], transformer-based approaches [13, 47, 23] achieve feature lifting by projecting 3D queries from 3D ", "page_idx": 0}, {"type": "image", "img_path": "9bu627mTfs/tmp/76aa5a3491ef598b001a9c2ecfb369ea38d4d4a8a16579f651d8365bd792c1b5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Comparison of feature aggregation. (a) VoxFormer [23] employs a set of shared contextindependent queries for different input images, which fails to capture distinctions among them and may lead to undirected feature aggregation. Besides, due to the ignorance of depth information, multiple 3D points may be projected to the same 2D point, causing depth ambiguity. (b) Our CGFormer initializes the voxel queries based on individual input images, effectively capturing their unique features and aggregating information within the region of interest. Furthermore, the deformable cross-attention is extended from 2D to 3D pixel space, enabling the points with similar image coordinates to be distinguished based on their depth coordinates. ", "page_idx": 1}, {"type": "text", "text": "space to image plane and aggregating 3D features through deformable attention mechanisms [61]. Among these, VoxFormer [23] introduces a sparse-to-dense architecture, which first aggregates 3D information for the visible voxels using the depth-based queries and then completes the 3D information for non-visible regions by leveraging the reconstructed visible areas as starting points. Building upon VoxFormer [23], the following approaches further improve performance through self-distillation training strategy [43], extracting instance features from images [14], or incorporating an image-conditioned cross-attention module [60]. ", "page_idx": 1}, {"type": "text", "text": "Despite significant progress, existing sparse-to-dense approaches typically employ shared voxel queries (referred to as context-independent queries) across different input images. These queries are defined as a set of learnable parameters that are independent of the input context. Once the training process is completed, these parameters remain constant for all input images during inference, failing to capture distinctions among various input images, as the focal regions of different images vary. Additionally, these queries also encounter the issue of undirected feature aggregation in crossattention, where the sampling points may fall in irrelevant regions. Besides, when projecting the 3D points onto the image plane, many points may end with the same 2D position with similar sampling points in the 2D feature map, causing a crucial depth ambiguity problem. An illustrative diagram is presented in Fig. 1(a). ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose a context and geometry aware voxel transformer (CGVT) to lift the 2D features. We observe that context-dependent query tend to aggregate information from the points within the region of interest. Fig. 3 presents an example of the sampling points of the contextdependent queries. Thus, before the aggregation of cross-attention, we first utilizes a context aware query generator to predict context-aware queries from the input individual images. Additionally, we extend deformable cross-attention from 2D to 3D pixel space, which allows points ending with similar image coordinates to be distinguished by their depth coordinates, as illustrated in Fig. 1(b). Furthermore, we propose a simple yet efficient depth refinement block to enhance the accuracy of estimated depth probability. This involves incorporating a more precise estimated depth map from a pretrained stereo depth estimation network [39], avoiding the heavy computational burden as observed in StereoScene [16]. ", "page_idx": 1}, {"type": "text", "text": "Based on the aforementioned module, we devise a neural network, named as CGFormer. To enhance the obtained 3D volume from the view transformation we integrate multiple representation, i.e., voxel and tri-perspective view (TPV). The TPV representation offers a global perspective that encompasses more high-level semantic information, while the voxel representation focuses more on the fine-grained structures. Drawing from the above analyses, we propose a 3D local and global encoder (LGE) that dynamically fuses the results of the voxel-based and TPV-based branches, to further enhance the 3D features from both local and global perspectives. ", "page_idx": 1}, {"type": "text", "text": "Our contributions can be summarized as follows: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a context and geometry aware voxel transformer (CGVT) to improve the performance of semantic scene completion. This module initializes the queries based on the ", "page_idx": 1}, {"type": "text", "text": "context of individual input images and extends the deformable cross-attention from 2D to 3D pixel space, thereby improving the performance of feature lifting. \u2022 We introduce a simple yet effective depth refinement block to enhance the accuracy of estimated depth probability with only introducing minimal computational burden. \u2022 We devise a 3D local and global encoder (LGE) to strengthen the semantic and geometric discriminability of the 3D volume. This encoder employs various 3D representations (voxel and TPV) to encode the 3D features, capturing information from both local and global perspectives. \u2022 Beneftiing from the aforementioned modules, our CGFormer attains state-of-the-art results with a mIoU of 16.63 and an IoU of 44.41 on SemanticKITTI, as well as a mIoU of 20.05 and an IoU of 48.07 on SSCBench-KITTI-360. Notably, our method even surpasses methods employing temporal images as inputs or using much larger image backbone networks. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Vision-based 3D Perception ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Vision-based 3D perception [24, 25, 31, 20, 12, 45, 62, 17, 44, 19, 58, 48, 52, 53, 56] has received extensive attention due to its ease of deployment, cost-effectiveness, and the preservation of intricate visual attributes, emerging as a crucial component in the autonomous driving. Current research efforts focus on constructing unified 3D representations (e.g., BEV, TPV, voxel) from input images. LiftSplat [36] lifts image features by performing outer product between the 2D image features and their estimated depth probability to generate a frustum-shaped pseudo point cloud of contextual features. The pseudo point cloud features are then splatted to predefined 3D anchors through a voxel-pooling operation. Building upon this, BEVDet [12] extends LiftSplat to 3D object detection, while BEVDepth [21] further enhances performance by introducing ground truth supervision for the estimated depth probability. With the advancements in attention mechanisms [4, 61, 57, 33, 45], BEVFormer [24, 54] transforms image features into BEV features using point deformable cross-attention [61]. Additionally, many other methods, such as OFT [37], Petr [28, 29], Inverse Perspective Mapping (IPM) [10], have also been presented to transform 2D image features into a 3D representation. ", "page_idx": 2}, {"type": "text", "text": "2.2 Semantic Scene Completion ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "SSCNet [40] initially introduces the concept of semantic scene completion, aiming to infer the semantic voxels. Following methods [38, 18, 51, 5] commonly utilize explicit depth maps or LIDAR point clouds as inputs. MonoScene [3] is the pioneering method for directly predicting the semantic occupancy from the input RGB image, which presents a FLoSP module for 2D-3D feature projection. StereoScene[16] introduces explicit epipolar constraints to mitigate depth ambiguity, albeit with heavy computational burden for correlation. TPVFormer [13] introduces a tri-perspective view (TPV) representation to describe the 3D scene, as an alternative to the BEV representation. The elements fall into the field of view aggregates information from the image features using deformable cross-attention [24, 61]. Beginning from depth-based [23] sparse proposal queries, VoxFormer [23] construct the 3D representation in a coarse-to-fine manner. The 3D information from the visible queries are diffused to the overall 3D volume using deformable self-attention, akin to the masked autoencoder (MAE)[8]. HASSC [43] introduces a self-distillation training strategy to improve the performance of VoxFormer [23], while MonoOcc [60] further enhance the 3D volume with an imageconditioned cross-attention module. Symphonize [14] extracts high level instance features from the image feature map, serving as the key and value of the cross-attention. ", "page_idx": 2}, {"type": "text", "text": "3 CGFormer ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As shown in Fig. 2, the overall framework of CGFormer is composed of four parts: feature extraction to extract 2D image features, view transformation (Section 3.2) to lift the 2D features to 3D volumes, ", "page_idx": 2}, {"type": "image", "img_path": "9bu627mTfs/tmp/162dea3053f5def7babb648eefe257349b3e77b4005246f418a351ab7262796e.jpg", "img_caption": ["Figure 2: Schematics and detailed architectures of CGFormer. (a) The framework of the proposed CGFormer for camera-based semantic scene completion. The pipeline consists of the image encoder for extracting 2D features, the context and geometry aware voxel (CGVT) transformer for lifting the 2D features to 3D volumes, the 3D local and global encoder (LGE) for enhancing the 3D volumes and a decoding head to predict the semantic occupancy. (b) Detailed structure of the context and geometry aware voxel transformer. (c) Details of the Depth Net. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3D encoder (Section 3.3) to further enhance the semantic and geometric discriminability of the 3D features, and a decoding head to infer the final result. ", "page_idx": 3}, {"type": "text", "text": "Image Encoder. The image encoder consists of a backbone network for extracting multi-scale features and a feature pyramid network (FPN) to fuse them. We adopt $\\mathbf{F}^{\\mathrm{2D}}\\in\\mathbb{R}^{H\\times W\\times\\widecheck{C}}$ to represent the extracted 2D image feature, where $C$ is the channel number, and $(H,W)$ refers to the resolution. ", "page_idx": 3}, {"type": "text", "text": "Depth Estimator. In alignment with VoxFormer [23], we use an off-the-shelf stereo depth estimation model [39] to predict the depth $\\mathbf{Z}(u,v)$ for each image pixel $(u,v)$ . The resulting estimated depth map is then employed to define the visible voxels located on the surface, serving as query proposals. Additionally, it is also used to refine the depth probability for lifting the 2D features. ", "page_idx": 3}, {"type": "text", "text": "3.2 View Transformation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A detailed diagram of our context and geometry aware voxel transformer is presented in Fig. 2(b). The process begins with a context aware query generator, which takes the context feature map to generate the context-dependent queries. Subsequently, the visible query proposals are located by the depth map from the pretrained depth estimation network. These proposals then attend to the image features to aggregate semantic and geometry information based on the 3D deformable cross-attention. Finally, the aggregated 3D information is further diffused from the updated proposals to the overall 3D volume via deformable self-attention. ", "page_idx": 3}, {"type": "text", "text": "Context-dependent Query Generation. Previous coarse-to-fine approaches [23, 43, 46, 60] typically employ shared context-independent queries for all the inputs. However, these approaches may overlook the differences among different images and aggregate information from irrelevant areas. In contrast, CGFormer first generates context-dependent queries from the image features using a context aware query generator. To elaborate, the extracted $\\mathbf{F}^{2D}$ is fed to the context net and the depth net [21] to generate the context feature $\\mathbf{C}\\in\\mathbb{R}^{H\\times W\\times C}$ and depth probability $\\mathbf{D}\\in\\mathbb{R}^{H\\times W\\times D}$ , respectively. Then the query generator $f$ takes $\\mathbf{C}$ and $\\mathbf{D}$ as inputs to generate voxel queries $\\mathbf{V_{Q}}\\in\\mathbb{R}^{X\\times Y\\times Z\\times C}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{V_{Q}}=f(\\mathbf{C},\\mathbf{D}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\left(X,Y,Z\\right)$ denotes the spatial resolution of the 3D volume. Benefiting from the initialization, the sampling points of the context-dependent queries usually locate within the region of interest. An example of the deformable sampling points is displayed in Fig. 3. ", "page_idx": 3}, {"type": "text", "text": "Depth-based Query Proposal. Following VoxFormer [23], we determine the visible voxels through the conversion of camera coordinates to world coordinates using the pre-estimated depth map, except that we do not employ an additional occupancy network [38]. A pixel $(u,v)$ will be transformed to a 3D point $\\left(x,y,z\\right)$ , utilizing the camera intrinsic and extrinsic matrices $\\mathcal{K}\\in\\mathbb{R}^{4\\times4}$ , $E\\in\\mathbb{R}^{4\\times4})$ . ", "page_idx": 3}, {"type": "image", "img_path": "9bu627mTfs/tmp/fd3dd4486fa9c0c500d69cc5abc628218d8f8e02077c5e90cd46d9bdd661559e.jpg", "img_caption": ["Figure 3: Visualization of the sampling locations for different small objects. The yellow dot represents the query point, while the red dots indicate the locations of the deformable sampling points. The sampling points of the context-dependent query (a) tend to be distributed within the regions of interest. Beneficial from this, CGFormer achieve better performance than previous methods. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "The synthetic point cloud is further converted into a binary voxel grid map $\\mathbf{M}$ , where each voxel is masked as 1 if occupied by at least one point and 0 otherwise. The query proposals are selected based on the binary $\\mathbf{M}$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{Q}=\\mathbf{V_{Q}}[\\mathbf{M}].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3D Deformable Cross-Attention. The query proposals then attend to the image features to aggregate the visual features of the 3D scene. We first expand the dimension of $\\mathbf{C}$ from 2D to 3D pixel space by conducting the outer product between $\\mathbf{D}$ and $\\mathbf{C}$ , formulated as $\\mathbf{F}=\\mathbf{C}\\otimes\\mathbf{D}$ . Here, the operator $\\otimes$ refers to the outer product conducted at the last dimension. Taking $\\mathbf{F}$ as key and value, for a 3D query $\\mathbf{Q}_{p}$ located at position $p$ , the 3D deformable cross-attention mechanism can be formulated as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{Q}}_{p}=3\\mathrm{D-DCA}(\\mathbf{Q}_{p},\\mathbf{F},p)=\\sum_{n=1}^{N}A_{n}W\\phi(\\mathbf{F},\\mathcal{P}(p)+\\Delta p),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $n$ indexes the sampled point from a total of $N$ points, $\\mathcal{P}(p)$ denotes camera projection function to obtain the reference points in the 3D pixel space, $A_{n}\\in[0,1]$ is the learnable attention weight, and $W$ denotes the projection weight. $\\bar{\\Delta p}\\in\\mathbb{R}^{3}$ is the predicted offset to the reference point $p$ , and $\\phi({\\bf F},{\\mathcal{P}}_{d}(p)+\\Delta p_{d})\\;$ indicates the trilinear interpolation used to sample features in the expanded 3D feature map $\\mathbf{F}$ . $\\hat{\\mathbf{Q}}$ denotes the updated query proposals. For simplicity, we only show the formulation of single-head attention. ", "page_idx": 4}, {"type": "text", "text": "Deformable Self-Attention. After several layers of deformable cross-attention, we merge $\\hat{\\mathbf{Q}}$ and the invisible elements of $\\mathbf{V_{Q}}$ , indicated as $\\mathbf{F}^{3\\tilde{\\mathrm{D}}}$ . The information of the updated query proposals is diffused to the overall 3D volume via deformable self-attention and $\\hat{\\mathbf{F}}^{3\\mathrm{D}}$ indicates the updated 3D volume. This process of a query located at position $p$ can be formulated as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{F}}_{p}^{\\mathrm{3D}}=\\mathrm{DSA}(\\mathbf{F}_{p}^{\\mathrm{3D}},\\mathbf{F}^{\\mathrm{3D}},p)=\\sum_{n=1}^{N}A_{n}W\\phi(\\mathbf{F}^{\\mathrm{3D}},p+\\Delta p).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Depth Net. The depth probability serves as the inputs of both the context aware query generator and 3D deformable cross-attention, whose accuracy greatly influences the performance. The results of single image depth estimation are usually unsatisfactory. Incorporating epipolar constraint can help estimate more accurate depth map. However, computing correlation to achieve this may introduce much computation burden during the estimation of the semantic voxels. Instead, we introduce a simple yet effective depth refinement strategy. The detailed architecture of the depth net is shown in Fig. 2(c). We first estimate monocular depth feature ${\\bf{D}}_{\\bf{M}}$ from $\\mathbf{F}^{2\\mathrm{D}}$ . The depth map generated from the stereo method [39] is encoded as stereo feature ${\\bf D}_{\\mathrm{S}}$ by several convolutions. These two features are further processed by cross-attention blocks for information interaction. To save memory, the window size of cross-attention is restricted within a range of the nearest neighboring pixels [7], which can be formulated as ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathbf{D}}_{\\mathrm{M}}=\\psi(\\mathbf{D}_{\\mathrm{M}},\\mathbf{D}_{\\mathrm{S}},\\mathbf{D}_{\\mathrm{S}}),}\\\\ {\\hat{\\mathbf{D}}_{\\mathrm{S}}=\\psi(\\mathbf{D}_{\\mathrm{S}},\\mathbf{D}_{\\mathrm{M}},\\mathbf{D}_{\\mathrm{M}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\hat{\\mathbf{D}}_{\\mathbf{M}}$ and $\\hat{\\mathbf{D}}_{\\mathrm{S}}$ denote the enhanced ${\\bf{D}}_{\\bf{M}}$ and ${\\bf D}_{\\mathrm{S}}$ , and $\\psi$ denotes the neighborhood attention [7]. We set the window size to $5\\times5$ . Finally, these two volumes are concatenated and fed to estimate the final depth probability. This strategy can boost the accuracy of the depth probability a lot with only minimal computation cost. ", "page_idx": 5}, {"type": "text", "text": "3.3 3D Local and Global Encoder ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The TPV representation is derived by compressing the voxel representation along its three axes. TPV focuses more on the global high-level semantic information, whereas the voxel emphasizes more on the details of intricate structures. We combine these two representations to enhance representation capability of the 3D features. As shown in Fig. 2(a), our 3D local and global encoder consists of the voxel-based and TPV-based branches. With the input 3D volume, the two branches first aggregate features in parallel. The dual-branch outputs are then fused dynamically and fed to the final decoding block. ", "page_idx": 5}, {"type": "text", "text": "Voxel-based Branch. The voxel-based branch aims to enhance the fine-grained structure features of the 3D volume. We adopt a lightweight ResNet [9] with 3D convolutions to extract multi-scale voxel features from $\\hat{\\mathbf{F}}^{3\\mathrm{D}}$ and merge them into $\\mathbf{F}_{\\mathrm{voxel}}^{3\\mathrm{D}}\\in\\mathbb{R}^{X\\times Y\\times Z\\times C}$ using a 3D feature pyramid network. ", "page_idx": 5}, {"type": "text", "text": "TPV-based Branch. To obtain three-plane features of the TPV representation, we apply spatial pooling to the voxel features along three axes. Directly performing MaxPooling may cause the loss of the fine-grained details. Considering the balance between performance and efficiency, we apply the group spatial to channel operation [50, 25, 62] to transform the voxel representation to TPV representation. Specifically, we split the voxel features along the pooling axis into $K$ groups and apply the MaxPooling within each group. Then we concatenate each group features along the channel dimension and use convolutions to reduce the channel dimension into $C$ . Denote the 2D view features as $\\mathbf{F}_{X Y}^{3\\mathrm{D}}\\in\\mathbb{R}^{X\\times Y\\times C}$ , $\\mathbf{F}_{X Z}^{\\mathrm{3D}}\\in\\mathbb{R}^{X\\times Z\\times C}$ , $\\mathbf{F}_{Y Z}^{3\\mathrm{D}}\\in\\mathbb{R}^{Y\\times Z\\times C}$ , the above process can be formulated as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{F}_{X Y}^{\\mathrm{3D}}=\\operatorname{Conv}_{X Y}(\\operatorname{Concat}(\\{\\operatorname{Pooling}_{\\{Z,i\\}}(\\hat{\\mathbf{F}}^{\\mathrm{3D}})\\}_{i=1}^{K})),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{\\gamma}:=\\operatorname{Conv}_{Y Z}(\\operatorname{Concat}(\\{\\operatorname{Pooling}_{\\{X,i\\}}(\\hat{\\mathbf{F}}^{\\mathrm{3D}})\\}_{i=1}^{K})),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{F}_{X Z}^{\\mathrm{3D}}=\\operatorname{Conv}_{X Z}(\\operatorname{Concat}(\\{\\mathrm{Pooling}_{\\{Y,i\\}}(\\hat{\\mathbf{F}}^{\\mathrm{3D}})\\}_{i=1}^{K}))\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "After obtaining the TPV features, we employ a 2D image backbone to process each plane to produce corresponding multi-scale features, which are then fused by a 2D feature pyramid network. For simplicity, we leverage $\\mathbf{F}_{X Y}^{\\mathrm{3D}},\\mathbf{F}_{X Z}^{\\mathrm{3D}},\\mathbf{F}_{Y Z}^{\\mathrm{3D}}$ to represent the outputs of these networks, as well. ", "page_idx": 5}, {"type": "text", "text": "Dynamic Fusion. The dual-path outputs are fused dynamically. We generate aggregation weights $\\dot{\\mathbf{W}}\\in\\mathbb{R}^{X\\times Y\\times Z\\times4}$ from F3voDxel using a convolution with a softmax on the last dimension. The final 3D feature volume $\\mathbf{F}_{\\mathrm{final}}^{3\\mathrm{D}}$ is computed as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{F}_{\\mathrm{final}}^{3\\mathrm{D}}=\\sum_{i}^{4}\\mathbf{W}_{i}\\odot\\mathbf{F}_{i}^{3\\mathrm{D}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r l r}{\\mathbf{W}_{i}}&{{}\\in}&{\\mathbb{R}^{X\\times Y\\times Z\\times1}}\\end{array}$ is a slice of the $\\mathbf{W}$ and $\\mathbf{F}_{i}^{3\\mathrm{D}}$ is an element of the set $[\\mathbf{F}_{\\mathrm{voxel}}^{\\mathrm{3D}},\\mathbf{F}_{X Y}^{\\mathrm{3D}},\\mathbf{F}_{X Z}^{\\mathrm{3D}},\\mathbf{F}_{Y Z}^{\\mathrm{3D}}]$ . The operation $\\odot$ refers to the element multiplication and the dimension broadcasting of the features from TPV-based branches is omitted in the equation for conciseness. $\\mathbf{F}_{\\mathrm{final}}^{3\\mathrm{D}}$ tios  tfhinea lsluyp fpelde tmoe tnhtae rdy ecmoadtienriga lb floorc km toor ien fdeert atihlee dc oscmhpelmetaet isccse nofe  tgheeo 3mDe tlroyc aaln da nsde mglaonbtiacl s.e nPcloedaseer. ", "page_idx": 5}, {"type": "text", "text": "3.4 Training Loss ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To train the proposed CGFormer, cross-entropy loss weighted by class frequencies $\\mathcal{L}_{c e}$ is leveraged to optimize the network. Following MonoScene [3], we also utilize affinity loss $\\mathcal{L}_{s c a l}^{g e o}$ and $\\mathcal{L}_{s c a l}^{s e m}$ to ", "page_idx": 5}, {"type": "image", "img_path": "9bu627mTfs/tmp/1405a452816dfb8081449fe7bed58e83ef3c7f62fb9f9325bdec2fe574f54d21.jpg", "img_caption": ["Table 1: Quatitative results on SemanticKITTI [1] test set. \u2217represents the reproduced results in [13, 59]. The best and the second best results are in bold and underlined, respectively. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 2: Quantitative results on SSCBench-KITTI360 test set. The results for counterparts are provided in [22]. The best and the second best results for all camera-based methods are in bold and underlined, respectively. The best results from the LiDAR-based methods are in red. ", "page_idx": 6}, {"type": "image", "img_path": "9bu627mTfs/tmp/9b462292e539f9d32ff61062ccdf5cc057ee7c64967170a8e31c4e880ae72aeb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "optimize the scene-wise and class-wise metrics (geometric IoU and semantic mIoU). We also employ explicit depth loss $\\mathcal{L}_{d}$ [24] to supervise the estimated depth probability of the depth net. Hence, the overall loss function can be formulated as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}=\\lambda_{d}\\mathcal{L}_{d}+\\mathcal{L}_{c e}+\\mathcal{L}_{s c a l}^{g e o}+\\mathcal{L}_{s c a l}^{s e m},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where we set the weight of depth loss $\\lambda_{d}$ to 0.001. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Quantitative Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conducted experiments to evaluate the performance of our CGFormer with the latest state-ofthe-art methods on the SemanticKITTI [1] and SSCBench-KITTI-360 [22]. Refer to the supplement material for information of datasets, metrics and detailed implementation details. ", "page_idx": 6}, {"type": "text", "text": "We list the results on SemanticKITTI hidden test set in Table 1. CGFormer achieves a mIoU of 16.63 and an IoU of 44.41. Notably, CGFormer outperforms all competing methods in terms of mIoU, demonstrating its superior performance. Regarding IoU, CGFormer surpasses all other methods except DepthSSC [55]. While ranking 2nd in terms of IoU, CGFormer exhibits only a slight marginal difference of 0.17 in comparison to DepthSSC [55]. However, CGFormer significantly outperforms DepthSSC [55] by a substantial margin of 3.52 in terms of mIoU. Additionally, compared to VoxFormer-T [23], HASSC-T [43], H2GFormer-t [46], and MonoOcc-L [3], although they utilize temporal stereo image pairs as inputs or larger image backbone networks, our CGFormer exceeds them a lot in terms of both metrics. For specific instances, CGFormer achieves the best or second-best performance on most of the classes, such as road, sidewalk, parking, and building. ", "page_idx": 6}, {"type": "text", "text": "Table 3: Ablation study of the architectural components on SemanticKITTI [1] validation set. CGVT: context and geometry aware voxel transformer. LGE: local and global encoder. 3D-DCA: 3D deformable cross attention. CAQG: context aware query generator. LB: local voxel-based branch. $\\mathcal{T}_{X Y}$ , $\\mathcal{T}_{Y Z}$ , $\\mathcal{T}_{X Z}$ : planes of the TPV-based branch. DF: dynamic fusion. There are 32M predefined parameters. ", "page_idx": 7}, {"type": "table", "img_path": "9bu627mTfs/tmp/ff2c60de017ba0435ed9906c64ce22c2351f97368e893a1ea3232c7bfd982347.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "9bu627mTfs/tmp/b83d67b1a4536aae8b087202ee8ed83d1a63af31b3e3aa708f1fc6ba1f32d308.jpg", "table_caption": ["Table 4: Ablation on the choices of context aware query generator. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "9bu627mTfs/tmp/558d5912de8ae8b7bb6c6c3d7de4bf746fcfc74df0033fa9dd26a564a2b578d1.jpg", "table_caption": ["Table 5: Ablation on the depth refinement block of the depth net. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "To demonstrate the versatility of our model, we also conduct experiments on the SSCBench-KITTI360 [40] dataset and list the results in Table 2. It is observed that CGFormer surpasses all the published camera-based methods by a margin of $3.95\\:\\mathrm{IoU}$ and 1.67 mIoU. Notably, CGFormer even outperforms the two LiDAR-based methods in terms of mIoU. The above analyses further verifies the effectiveness and excellent performance of CGFormer. ", "page_idx": 7}, {"type": "text", "text": "4.2 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct ablation analyses on the components of CGFormer on the SemanticKITTI validation set. ", "page_idx": 7}, {"type": "text", "text": "Ablation on the context and geometry aware voxel transformer (CGVT). Table 3 presents a detailed analysis of various architectural components within CGFormer. The baseline can be considered as a simplified version of VoxFormer [23], without utilizing the extra occupancy network [38] to generate coarse occupancy masks. It begins with an image encoder to extract image features, a depth-based query proposal layer to define the visible queries, the deformable cross-attention layer to aggregate features for visible areas, the deformable self-attention layer to diffuse features from the visible regions to the invisible ones. Extending the cross-attention to 3D deformable cross-attention (a) brings a notable improvement of $1.63\\;\\mathrm{mIoU}$ and $2.15\\;\\mathrm{IoU}$ . The performance is further enhanced by giving the voxel queries a good initialization by introducing the context ware query generator (b). ", "page_idx": 7}, {"type": "text", "text": "Ablation on the local and global encoder (LGE). After obtaining the 3D features, CGFormer incorporates multiple representation to refine the 3D volume form both local and global perspectives. Through experimentation, we validate that the voxel-based and TPV-based branches collectively contribute to performance improvement with a suitable fusion strategy. Specifically, we compate the results of solely utilizing the local voxel-based branch (c), simply adding the results of dual branches (d), and dynamically fusing the dual-branch outputs (h), as detailed in Table 3. The accuracy is notably elevated to an IoU of 45.99 and mIoU of 16.87 by dynamically fusing the dual-branch outputs. Additionally, we conduct an ablation study on the three TPV planes utilized in the TPV-based branch (e,f,g). The results demonstrate that any individual plane improves performance compared to the model with only the local branch. Combining the information from all three planes into the TPV representation achieves superior performance, underscoring the complementary nature of the three TPV planes in effectively representing complex 3D scenes. ", "page_idx": 7}, {"type": "text", "text": "Ablation on the context aware query generator. We present the ablation analysis of the contextaware query generator in Table 4. We remove the CAQG and increase the number of attention layers, where the results of the previous layers can be viewed as a initialization of the queries for the later layers. This configuration (6 cross-attention layers and 4 self-attention layers) significantly improves IoU but only marginally lifts mIoU, and it requires much more training memory. Employing ", "page_idx": 7}, {"type": "image", "img_path": "9bu627mTfs/tmp/54d2e4719c3cb9e4e3d9e80414916bc9a3c0a1ced7eefb2404163ecf40b64c20.jpg", "img_caption": ["Figure 4: Qualitative visualization results on the SemanticKITTI [1] validation set. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "FLoSP [3] with the occupancy-aware depth module [35] can improve performance without much additional computational burden. By replacing it with the voxel pooling [36], the model achieves the best performance. Thus, we employ voxel pooling as our context aware query generator. ", "page_idx": 8}, {"type": "text", "text": "Ablation on the depth refinement block. Table 5 presents analyses of the impact of each module within the depth net. By removing the stereo feature $\\mathbf{D}_{S}$ and employing the same structure as BEVDepth [21], we observe a performance drop of 1.42 IoU and $1.79\\,\\mathrm{mIoU}$ . When incorporating the stereo feature $\\mathbf{D}_{S}$ but fusing it with a simple concatenate operation without using the neighborhood attention, the model achieves a mIoU of 45.72 and an IoU of 16.26. These results emphasize that deactivating any component of the depth net leads to a decrease of the accuracy of the full network. Additionally, we replace the depth refinement module with the dense correlation module from StereoScene [16]. Compared to this, our depth refinement module achieves comparable results while using significantly fewer parameters and less training memory. ", "page_idx": 8}, {"type": "text", "text": "4.3 Qualitative Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Fig. 4 presents visualizations of predicted results on the SemanticKITTI validation set obtained from MonoScene[3], VoxFormer [23], OccFormer [59], and our proposed CGFormer. Notably, CGFormer outperforms other methods by effectively capturing the semantic scene and inferring previously invisible regions. The predictions generated by CGFormer exhibit distinctly clearer geometric structures and improved semantic discrimination, particularly for classes like cars and the overall scene layout. This enhancement is attributed to the precision achieved through the context and geometry aware voxel transformer applied in our proposed approach. In contrast, the instances generated by the comparison methods appear to be influenced by depth ambiguity, resulting in vague shapes. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we present CGFormer, a novel neural network for Semantic Scene Completion. CGFormer dynamically generates distinct voxel queries, which serve as a good starting point for the attention layers, capturing the unique characteristics of various input images. To improve the accuracy of the estimated depth probability, we propose a simple yet efficient depth refinement module, with minimal computational burden. To boost the semantic and geometric representation abilities, CGFormer incorporates multiple representations (voxel and TPV) to encode the 3D volumes from both local and global perspectives. We experimentally show that our CGFormer achieves state-of-the-art performance on the SemanticKITTI and SSCBench-KITTI-360 benchmarks. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by the National Key Research and Development Program of China under Grant No. 2023YFB3209800, in part by Zhejiang Provincial Natural Science Foundation of China under Grant No. LD24F020003, and in part by the National Natural Science Foundation of China under Grant No. 62301484. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel, Sven Behnke, Cyrill Stachniss, and J\u00fcrgen Gall. Semantickitti: A dataset for semantic scene understanding of lidar sequences. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9297\u20139307, 2019. 7, 8, 9, 14, 15, 16, 17, 18   \n[2] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Adabins: Depth estimation using adaptive bins. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4009\u20134018, 2021. 14, 15   \n[3] Anh-Quan Cao and Raoul de Charette. Monoscene: Monocular 3d semantic scene completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3981\u20133991, 2022. 1, 3, 6, 7, 8, 9, 14, 16, 17, 18   \n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In Proceedings of the European Conference on Computer Vision, pages 213\u2013229, 2020. 3   \n[5] Ran Cheng, Christopher Agia, Yuan Ren, Xinhai Li, and Liu Bingbing. S3cnet: A sparse semantic scene completion network for lidar point clouds. In Conference on Robot Learning, pages 2148\u20132161, 2021. 1, 3   \n[6] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3354\u20133361, 2012. 14   \n[7] Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi. Neighborhood attention transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6185\u20136194, 2023. 5, 6   \n[8] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16000\u201316009, 2022. 3   \n[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 770\u2013778, 2016. 6, 14   \n[10] Anthony Hu, Zak Murez, Nikhil Mohan, Sof\u00eda Dudas, Jeffrey Hawke, Vijay Badrinarayanan, Roberto Cipolla, and Alex Kendall. Fiery: Future instance prediction in bird\u2019s-eye-view from surround monocular cameras. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15273\u201315282, 2021. 3   \n[11] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17853\u201317862, 2023. 1   \n[12] Junjie Huang, Guan Huang, Zheng Zhu, Yun Ye, and Dalong Du. Bevdet: High-performance multi-camera 3d object detection in bird-eye-view. arXiv preprint arXiv:2112.11790, 2021. 3   \n[13] Yuanhui Huang, Wenzhao Zheng, Yunpeng Zhang, Jie Zhou, and Jiwen Lu. Tri-perspective view for vision-based 3d semantic occupancy prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9223\u20139232, 2023. 1, 3, 7, 14, 15, 16   \n[14] Haoyi Jiang, Tianheng Cheng, Naiyu Gao, Haoyang Zhang, Wenyu Liu, and Xinggang Wang. Symphonize 3d semantic scene completion with contextual instance queries. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 2, 3, 7, 14, 15, 16   \n[15] Bu Jin, Yupeng Zheng, Pengfei Li, Weize Li, Yuhang Zheng, Sujie Hu, Xinyu Liu, Jinwei Zhu, Zhijie Yan, Haiyang Sun, et al. Tod3cap: Towards 3d dense captioning in outdoor scenes. arXiv preprint arXiv:2403.19589, 2024. 1   \n[16] Bohan Li, Yasheng Sun, Xin Jin, Wenjun Zeng, Zheng Zhu, Xiaoefeng Wang, Yunpeng Zhang, James Okae, Hang Xiao, and Dalong Du. Stereoscene: Bev-assisted stereo matching empowers 3d semantic scene completion. arXiv preprint arXiv:2303.13959, 2023. 2, 3, 7, 8, 9, 15, 16   \n[17] Hongyang Li, Hao Zhang, Zhaoyang Zeng, Shilong Liu, Feng Li, Tianhe Ren, and Lei Zhang. Dfa3d: 3d deformable attention for 2d-to-3d feature lifting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6684\u20136693, 2023. 3   \n[18] Jie Li, Kai Han, Peng Wang, Yu Liu, and Xia Yuan. Anisotropic convolutional networks for 3d semantic scene completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3348\u20133356, 2020. 3   \n[19] Pengfei Li, Ruowen Zhao, Yongliang Shi, Hao Zhao, Jirui Yuan, Guyue Zhou, and Ya-Qin Zhang. Lode: Locally conditioned eikonal implicit scene completion from sparse lidar. In IEEE International Conference on Robotics and Automation, pages 8269\u20138276. IEEE, 2023. 3   \n[20] Yinhao Li, Han Bao, Zheng Ge, Jinrong Yang, Jianjian Sun, and Zeming Li. Bevstereo: Enhancing depth estimation in multi-view 3d object detection with temporal stereo. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1486\u20131494, 2023. 3   \n[21] Yinhao Li, Zheng Ge, Guanyi Yu, Jinrong Yang, Zengran Wang, Yukang Shi, Jianjian Sun, and Zeming Li. Bevdepth: Acquisition of reliable depth for multi-view 3d object detection. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1477\u20131485, 2023. 3, 4, 9   \n[22] Yiming Li, Sihang Li, Xinhao Liu, Moonjun Gong, Kenan Li, Nuo Chen, Zijun Wang, Zhiheng Li, Tao Jiang, Fisher Yu, Yue Wang, Hang Zhao, Zhiding Yu, and Chen Feng. Sscbench: A large-scale 3d semantic scene completion benchmark for autonomous driving. arXiv preprint arXiv:2306.09001, 2023. 7, 14, 16   \n[23] Yiming Li, Zhiding Yu, Christopher B. Choy, Chaowei Xiao, Jos\u00e9 M. \u00c1lvarez, Sanja Fidler, Chen Feng, and Anima Anandkumar. Voxformer: Sparse voxel transformer for camera-based 3d semantic scene completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9087\u20139098, 2023. 1, 2, 3, 4, 5, 7, 8, 9, 14, 15, 16, 17, 18   \n[24] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer: Learning bird\u2019s-eye-view representation from multi-camera images via spatiotemporal transformers. In Proceedings of the European Conference on Computer Vision, pages 1\u201318, 2022. 1, 3, 7   \n[25] Tingting Liang, Hongwei Xie, Kaicheng Yu, Zhongyu Xia, Zhiwei Lin, Yongtao Wang, Tao Tang, Bing Wang, and Zhi Tang. Bevfusion: A simple and robust lidar-camera fusion framework. Advances in Neural Information Processing Systems, pages 10421\u201310434, 2022. 3, 6   \n[26] Yiyi Liao, Jun Xie, and Andreas Geiger. Kitti-360: A novel dataset and benchmarks for urban scene understanding in 2d and 3d. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 3292\u20133310, 2022. 14   \n[27] Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2117\u20132125, 2017. 14   \n[28] Yingfei Liu, Tiancai Wang, Xiangyu Zhang, and Jian Sun. Petr: Position embedding transformation for multi-view 3d object detection. arXiv preprint arXiv:2203.05625, 2022. 3   \n[29] Yingfei Liu, Junjie Yan, Fan Jia, Shuailin Li, Qi Gao, Tiancai Wang, Xiangyu Zhang, and Jian Sun. Petrv2: A unified framework for 3d perception from multi-camera images. arXiv preprint arXiv:2206.01256, 2022. 3   \n[30] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10012\u201310022, 2021. 14   \n[31] Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela L Rus, and Song Han. Bevfusion: Multi-task multi-sensor fusion with unified bird\u2019s-eye-view representation. In IEEE International Conference on Robotics and Automation, pages 2774\u20132781, 2023. 3   \n[32] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 14   \n[33] Wenyu Lv, Shangliang Xu, Yian Zhao, Guanzhong Wang, Jinman Wei, Cheng Cui, Yuning Du, Qingqing Dang, and Yi Liu. Detrs beat yolos on real-time object detection. arXiv preprint arXiv:2304.08069, 2023. 3   \n[34] Jianbiao Mei, Yu Yang, Mengmeng Wang, Junyu Zhu, Xiangrui Zhao, Jongwon Ra, Laijian Li, and Yong Liu. Camera-based 3d semantic scene completion with sparse guidance network. arXiv preprint arXiv:2312.05752, 2023. 1   \n[35] Ruihang Miao, Weizhou Liu, Mingrui Chen, Zheng Gong, Weixin Xu, Chen Hu, and Shuchang Zhou. Occdepth: A depth-aware method for 3d semantic scene completion. arXiv preprint arXiv:2302.13540, 2023. 9   \n[36] Jonah Philion and Sanja Fidler. Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d. In Proceedings of the European Conference on Computer Vision, pages 194\u2013210, 2020. 3, 9   \n[37] Thomas Roddick, Alex Kendall, and Roberto Cipolla. Orthographic feature transform for monocular 3d object detection. arXiv preprint arXiv:1811.08188, 2018. 3   \n[38] Luis Rold\u00e3o, Raoul de Charette, and Anne Verroust-Blondet. Lmscnet: Lightweight multiscale 3d semantic completion. In Proceedings of the International Conference on 3D Vision, pages 111\u2013119, 2020. 1, 3, 4, 7, 8, 16   \n[39] Faranak Shamsafar, Samuel Woerz, Rafia Rahim, and Andreas Zell. Mobilestereonet: Towards lightweight deep networks for stereo matching. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2417\u20132426, 2022. 2, 4, 5, 15   \n[40] Shuran Song, Fisher Yu, Andy Zeng, Angel X. Chang, Manolis Savva, and Thomas A. Funkhouser. Semantic scene completion from a single depth image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 190\u2013198, 2017. 1, 3, 7, 8, 16   \n[41] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning, pages 6105\u20136114, 2019. 14   \n[42] Xiaoyu Tian, Tao Jiang, Longfei Yun, Yucheng Mao, Huitong Yang, Yue Wang, Yilun Wang, and Hang Zhao. Occ3d: A large-scale 3d occupancy prediction benchmark for autonomous driving. In Advances in Neural Information Processing Systems, pages 64318\u201364330, 2023. 1   \n[43] Song Wang, Jiawei Yu, Wentong Li, Wenyu Liu, Xiaolu Liu, Junbo Chen, and Jianke Zhu. Not all voxels are equal: Hardness-aware semantic scene completion with self-distillation. arXiv preprint arXiv:2404.11958, 2024. 2, 3, 4, 7, 16   \n[44] Xiaofeng Wang, Zheng Zhu, Wenbo Xu, Yunpeng Zhang, Yi Wei, Xu Chi, Yun Ye, Dalong Du, Jiwen Lu, and Xingang Wang. Openoccupancy: A large scale benchmark for surrounding semantic occupancy perception. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 17850\u201317859, 2023. 3   \n[45] Yue Wang, Vitor Guizilini, Tianyuan Zhang, Yilun Wang, Hang Zhao, , and Justin M. Solomon. Detr3d: 3d object detection from multi-view images via 3d-to-2d queries. In Conference on Robot Learning, pages 180\u2013191, 2021. 3   \n[46] Yu Wang and Chao Tong. H2gformer: Horizontal-to-global voxel transformer for 3d semantic scene completion. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 5722\u20135730, 2024. 4, 7, 16   \n[47] Yi Wei, Linqing Zhao, Wenzhao Zheng, Zheng Zhu, Jie Zhou, and Jiwen Lu. Surroundocc: Multi-camera 3d occupancy prediction for autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21729\u201321740, 2023. 1, 7, 14, 16   \n[48] Yuan Wu, Zhiqiang Yan, Zhengxue Wang, Xiang Li, Le Hui, and Jian Yang. Deep height decoupling for precise vision-based 3d occupancy prediction. arXiv preprint arXiv:2409.07972, 2024. 3   \n[49] Haihong Xiao, Hongbin Xu, Wenxiong Kang, and Yuqiong Li. Instance-aware monocular 3d semantic scene completion. IEEE Transactions on Intelligent Transportation Systems, 2024. 7, 16   \n[50] Enze Xie, Zhiding Yu, Daquan Zhou, Jonah Philion, Anima Anandkumar, Sanja Fidler, Ping Luo, and Jose M Alvarez. Mbev: Multi-camera joint 3d detection and segmentation with unified birds-eye-view representation. arXiv preprint arXiv:2204.05088, 2022. 6   \n[51] Xu Yan, Jiantao Gao, Jie Li, Ruimao Zhang, Zhen Li, Rui Huang, and Shuguang Cui. Sparse single sweep lidar point cloud segmentation via learning contextual shape priors from scene completion. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 3101\u20133109, 2021. 1, 3   \n[52] Zhiqiang Yan, Yuankai Lin, Kun Wang, Yupeng Zheng, Yufei Wang, Zhenyu Zhang, Jun Li, and Jian Yang. Tri-perspective view decomposition for geometry-aware depth completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4874\u20134884, 2024. 3   \n[53] Zhiqiang Yan, Kun Wang, Xiang Li, Zhenyu Zhang, Jun Li, and Jian Yang. Rignet: Repetitive image guided network for depth completion. In Proceedings of the European Conference on Computer Vision, pages 214\u2013230, 2022. 3   \n[54] Chenyu Yang, Yuntao Chen, Hao Tian, Chenxin Tao, Xizhou Zhu, Zhaoxiang Zhang, Gao Huang, Hongyang Li, Yu Qiao, Lewei Lu, et al. Bevformer v2: Adapting modern image backbones to bird\u2019s-eye-view recognition via perspective supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17830\u201317839, 2023. 1, 3   \n[55] Jiawei Yao and Jusheng Zhang. Depthssc: Depth-spatial alignment and dynamic voxel resolution for monocular 3d semantic scene completion. arXiv preprint arXiv:2311.17084, 2023. 7, 16   \n[56] Zhu Yu, Zehua Sheng, Zili Zhou, Lun Luo, Si-Yuan Cao, Hong Gu, Huaqi Zhang, and Hui-Liang Shen. Aggregating feature point cloud for depth completion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8732\u20138743, 2023. 3   \n[57] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M. Ni, and HeungYeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2302.14325, 2022. 3   \n[58] Jiahui Zhang, Hao Zhao, Anbang Yao, Yurong Chen, Li Zhang, and Hongen Liao. Efficient semantic scene completion network with spatial group convolution. In Proceedings of the European Conference on Computer Vision, pages 733\u2013749, 2018. 3   \n[59] Yunpeng Zhang, Zheng Zhu, and Dalong Du. Occformer: Dual-path transformer for visionbased 3d semantic occupancy prediction. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9433\u20139443, 2023. 5, 7, 9, 15, 16, 17, 18   \n[60] Yupeng Zheng, Xiang Li, Pengfei Li, Yuhang Zheng, Bu Jin, Chengliang Zhong, Xiaoxiao Long, Hao Zhao, and Qichao Zhang. Monoocc: Digging into monocular semantic occupancy prediction. arXiv preprint arXiv:2403.08766, 2024. 2, 3, 4, 7, 16   \n[61] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020. 2, 3   \n[62] Sicheng Zuo, Wenzhao Zheng, Yuanhui Huang, Jie Zhou, and Jiwen Lu. Pointocc: Cylindrical tri-perspective view for point-based 3d semantic occupancy prediction. arXiv preprint arXiv:2308.16896, 2023. 3, 6 ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix / Supplemental Material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In the appendix, we mainly provide implementation details and more experiment results. ", "page_idx": 13}, {"type": "text", "text": "A.1 Datasets and Metrics ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Datasets. We evaluate our CGFormer on two datasets: SemanticKITTI [1] and SSC-Bench-KITTI360 [22]. These datasets are derived from the KITTI Odometry [6] and KITTI-360 [26] Benchmarks, respectively. The evaluation focuses on a specific spatial volume: $51.2m$ in front of the car, $25.6m$ to the left and right sides, and $6.4m$ above the car. Voxelization of this volume results in a set of 3D voxel grids with a resolution of $256\\times256\\times32$ , where each voxel measures $0.2m\\times0.2m\\times0.2m$ . SemanticKITTI provides RGB images with dimensions of $1226\\times370$ as inputs, encompassing 20 unique semantic classes (19 semantic classes and 1 free class). The dataset includes 10 sequences for training, 1 sequence for validation, and 11 sequences for testing. SSC-Bench-KITTI-360 [22] offers 7 sequences for training, 1 sequence for validation, and 1 sequence for testing. It contains 19 unique semantic classes (18 semantic classes and 1 free class), with input RGB images having a resolution of $1408\\times376$ . ", "page_idx": 13}, {"type": "text", "text": "Metrics. Following previous methods [3, 23, 13], we report the intersection over union (IoU) and mean IoU (mIoU) metrics for occupied voxel grids and voxel-wise semantic predictions, respectively. The interplay between IoU and mIoU offers a comprehensive perspective on the model\u2019s effectiveness in capturing both geometry and semantic aspects of the scene. ", "page_idx": 13}, {"type": "text", "text": "A.2 Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Network Structures. Consistent with previous researches [13, 3, 47], we utilize a 2D UNet based on a pretrained EfficientNetB7 [41] as the image backbone. The CGVT generates a 3D feature volume with dimensions of $128\\times128\\times16$ and 128 channels. The numbers of deformable attention layers for cross-attention and self-attention are 3 and 2 respectively. We use 8 sampling points around each reference point for the cross and self-attention head. The voxel-based branch of the LGE comprises 3 stages with 2 residual blocks [9] each. SwinT [30] is employed as the 2D backbone in the TPV-based branch. Both are followed by feature pyramid networks (FPNs) [27] to aggregate multi-scale features for dynamic fusion. The final prediction has dimensions of $128\\times128\\times16$ and is upsampled to $256\\times256\\times32$ through trilinear interpolation to align the resolution with the ground truth. ", "page_idx": 13}, {"type": "text", "text": "Training Setup. We train CGFormer for 25 epochs on 4 NVIDIA 4090 GPUs, with a batch size of 4. It approximately consumes 19 GB of GPU memory on each GPU during the training phase. We employ the AdamW [32] optimizer with $\\beta_{1}=0.9$ , $\\beta_{2}=0.99$ and set the maximum learning rate to $3\\times10^{-4}$ . The cosine annealing learning rate strategy is adopted for the learning rate decay, where the cosine warmup strategy is applied for the first $5\\%$ iterations. ", "page_idx": 13}, {"type": "text", "text": "A.3 Results Using Monocular Inputs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In alignment with previous methods [23, 14], we evaluate the performance of our CGFormer using only a monocular RGB image as input. We replace the depth estimation network with AdaBins [2] and present the results on the semantickitti validation set in the table 7. To better demonstrate the advantage of our CGFormer, we also include the results of VoxFormer, Symphonize, and OccFormer. Compared to the stereo-based methods when using only a monocular image (VoxFormer, Symphonize), CGFormer achieves superior performance in terms of both IoU and mIoU. Furthermore, our method also surpasses OccFormer, the state-of-the-art monocular method. ", "page_idx": 13}, {"type": "table", "img_path": "9bu627mTfs/tmp/f5de20c6064a201fdc438319d05799353227d3e69b528242251dd88a6c28b267.jpg", "table_caption": ["Table 6: The performance of the CGFormer with more lightweight backbone networks. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Table 7: Comparison of the performance using monocular inputs. For stereo-based methods, we replace the MobileStereoNet [39] with Adabins [2]. ", "page_idx": 14}, {"type": "table", "img_path": "9bu627mTfs/tmp/fc15fe39322ae755bc0d4c4c946c23c07d1d41f800e31a1d53df5b187ac9b10e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "9bu627mTfs/tmp/aabe513734d787e26943d74d547a3674b4e0d8f20372cc8dec7376e147fe5063.jpg", "table_caption": ["Table 8: Comparison of training memory and inference time with SOTA methods on the and SemanticKITTI test set. These metrics were measured on the NVIDIA 4090 GPU. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.4 Reults with More Lightweight Backbone Networks ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We reanalyze the components of CGFormer, finding that replacing EfficientNetB7, used as the image backbone, and the Swin blocks, used in the TPV branch backbone, with more lightweight ResNet50 and residual blocks, respectively, can significantly reduce the number of parameters of our network. Besides, we also remove the predefined parameters as we find it doesn\u2019t influence the final performance. The results on the semantickitti validation set are presented in the Table 6. Compared to the original architecture, CGFormer maintains stable performance regardless of the backbone networks used for the image encoder and TPV branch encoder, underscoring its effectiveness, robustness, and potential. ", "page_idx": 14}, {"type": "text", "text": "A.5 Additional Quantitative Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For more comprehensive comparison, we list the results with input modality and image backbones in Table 9 and Table 10. Table 11 presents the comparison results of CGFormer with the state-of-the-art methods on the SemanticKITTI validation set. CGFormer outperforms all other methods in terms of both IoU and mIoU. Additionally, it ranks either first or second on most of the classes, demonstrating consistent performance across various semantic categories, as indicated in previous tables. ", "page_idx": 14}, {"type": "text", "text": "A.6 Computational Cost ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Table 8, we display the training memory and inference time of CGFormer, along with those of the comparison methods. Additionally, the table includes the corresponding IoU and mIoU metrics for comprehensive comparison. As shown in the table, CGFormer achieves the best performance in terms of both IoU and mIoU, with comparable training memory and inference time. ", "page_idx": 14}, {"type": "text", "text": "A.7 Additional Qualitative Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We offer additional visualization results in Fig.6 and Fig.7. These examples are randomly selected from the SemanticKITTI [1] validation set. ", "page_idx": 14}, {"type": "text", "text": "A.8 Failure Cases ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We provide two failure cases in Fig. 5. ", "page_idx": 14}, {"type": "text", "text": "A.9 Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "While CGFormer exhibits strong performance on benchmarks, but the accuracy on most of the categories (e.g., person, bicyclist, other vehicle) is unsatisfactory. Improving the performance on these instances could be beneficial for the downstream application tasks. Furthermore, there is a need to explore designing depth estimation networks under multi-view scenarios to extend the geometry-aware view transformation to these scenes. Despite these limitations, we are confident that CGFormer will contribute to advancing the field of 3D perception. ", "page_idx": 14}, {"type": "text", "text": "Table 9: Quatitative results on SemanticKITTI [1] test set. \u2217represents the reproduced results in [13, 59]. The best and the second best results are in bold and underlined, respectively. Our CGFormer outperforms temporal stereo-based (Stereo-T) methods or those methods with larger image backbones in terms of IoU and mIoU. ", "page_idx": 15}, {"type": "table", "img_path": "9bu627mTfs/tmp/37b24a5f32ec502f1c736f6ff726fec9a559b7cf5fce0b2635dc19edaeaf3445.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 10: Quantitative results on SSCBench-KITTI360 test set. The results for counterparts are provided in [22]. The best and the second best results for all camera-based methods are in bold and underlined, respectively. The best results from the LiDAR-based methods are in red. ", "page_idx": 15}, {"type": "table", "img_path": "9bu627mTfs/tmp/9dba99e9778e416becbc046056a7b448e660605a86d7c85e15f44a801f9cfd9f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 11: Quatitative results on SemanticKITTI [1] validation set. \u2217represents the reproduced results in [13, 59, 55]. The best and the second best results are in bold and underlined, respectively. ", "page_idx": 15}, {"type": "image", "img_path": "9bu627mTfs/tmp/146f4b447eedcdb106ca18428bebf1a0aae26bd1dede3645762e4f092d40d9ba.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "9bu627mTfs/tmp/3c6b54fe222eceedb331eaaf2b89bd443b5a9d16bacd1fb04c2a058167049ff2.jpg", "img_caption": ["Figure 5: Failure cases. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "9bu627mTfs/tmp/aa6dc36d6d065410fc53509fda5e024c59ee81b122b9e555108e32fcf642a4e3.jpg", "img_caption": ["(a) MonoScene [3] (b) VoxFormer [23] (c) OccFormer [59] (d) CGFormer (ours) (e) Ground Truth ", "Figure 6: More qualitative comparison results on the SemanticKITTI [1] validation set. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "9bu627mTfs/tmp/a4c6cdd221c39d39be25112b8cec016769a00524865df154fd164dc8d930792a.jpg", "img_caption": ["(a) MonoScene [3] (b) VoxFormer [23] (c) OccFormer [59] (d) CGFormer (ours) (e) Ground Truth ", "Figure 7: More qualitative comparison results on the SemanticKITTI [1] validation set. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We describe the motivation of our work at the beginning of the abstract. Then we describe the methodology we employed to tackle the presented issues. Our contributions are summarized in the last of introduction. Refer to the introduction to validate it. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We provide a subsection of limitation in the supplement material ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: We provide formulas for better understanding our job, but we don\u2019t propose new theorems. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We provide the detailed implementation in the supplement material. Code is also provided to validate it. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide code as an additional zip file. However, as the checkpoint of the model is too large to upload, we provide a anonymous link for downloading the pretrained checkpoints and scripts for retraining the model. Code will be publicly available. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We provide the details of datasets and metrics, which includes the data splits and evaluation metrics. Hyper-parameters and optimizer can be found in the implementation details in the supplement material. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: Following previous methods, we obtain the results by submitting results to the leaderboard. And all of our experiments are conducted using the same seed 7240. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our model is trained on 4 NVIDIA 4090 GPUs, with a batch size of 4. It approximately consumes 19GB of GPU memory on each GPU during the training phase. Refer to the implementation details for detailed information. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Yes, the research conducted in the paper conforms with the NeurIPS Code of Ethics. The provided code and link are both anonymous. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We mentioned that we are confident that CGFormer will contribute to advancing the field of 3D perception and autonomous driving at the end of the conclusion, while we also pointed that the accuracy should be further improved for actual application. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: No, our paper did not utilize any high-risk data or models. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We will thank all the assets we refer to when the code is publicly available.   \nWe respect these methods and cite them in our paper. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]