[{"type": "text", "text": "DiffLight: A Partial Rewards Conditioned Diffusion Model for Traffic Signal Control with Missing Data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hanyang Chen1,2, Yang Jiang1,2, Shengnan $\\mathbf{Guo}^{1,2}$ ,\u2217 Xiaowei Mao1,2, Youfang Lin1,2, Huaiyu Wan1,2 ", "page_idx": 0}, {"type": "text", "text": "1School of Computer Science and Technology, Beijing Jiaotong University, China 2Beijing Key Laboratory of Traffic Data Analysis and Mining, Beijing, China {hanyangchen, jiangyang, guoshn, maoxiaowei, yflin, hywan}@bjtu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The application of reinforcement learning in traffic signal control (TSC) has been extensively researched and yielded notable achievements. However, most existing works for TSC assume that traffic data from all surrounding intersections is fully and continuously available through sensors. In real-world applications, this assumption often fails due to sensor malfunctions or data loss, making TSC with missing data a critical challenge. To meet the needs of practical applications, we introduce DiffLight, a novel conditional diffusion model for TSC under datamissing scenarios in the offline setting. Specifically, we integrate two essential sub-tasks, i.e., traffic data imputation and decision-making, by leveraging a Partial Rewards Conditioned Diffusion (PRCD) model to prevent missing rewards from interfering with the learning process. Meanwhile, to effectively capture the spatial-temporal dependencies among intersections, we design a Spatial-Temporal transFormer (STFormer) architecture. In addition, we propose a Diffusion Communication Mechanism (DCM) to promote better communication and control performance under data-missing scenarios. Extensive experiments on five datasets with various data-missing scenarios demonstrate that DiffLight is an effective controller to address TSC with missing data. The code of DiffLight is released at https://github.com/lokol5579/DiffLight-release. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "With the acceleration of urbanization, the surge in the number of vehicles in cities has led to increasingly severe traffic congestion and pollution problems [1]. Intersections, where traffic congestion often occurs, become a key in addressing these problems. To this end, solving the traffic signal control (TSC) problem is crucial for reducing traffic congestion in intersections by controlling traffic lights. Over the years, many approaches have been developed to tackle the TSC problem, which can be categorized into conventional approaches and reinforcement learning-based (RL-based) approaches. Conventional approaches, like Fixed-time [2], SCOOT [3] and SCATS [4], have been widely deployed in different cities. However, these approaches struggle to adapt to the inherent stochasticity and highly dynamic nature of real-time traffic conditions, limiting their effectiveness in responding to dynamic traffic demands. ", "page_idx": 0}, {"type": "text", "text": "Recently, reinforcement learning (RL) is introduced into TSC to enable adaptive traffic signal control [5, 6, 7, 8, 9, 10, 11, 12, 13]. Unlike conventional approaches, RL-based approaches for TSC deploy a learnable agent at each intersection, allowing traffic signals to be adjusted dynamically based on real-time traffic conditions. However, most existing RL-based approaches for TSC assume that traffic data from all surrounding intersections is fully and continuously available through deployed sensors. In practice, this assumption is often unrealistic. Due to budget constraints, not all intersections can be equipped with sufficient sensors. Even if necessary sensors are deployed, malfunctions or errors could lead to incomplete data collection. Therefore, the research on TSC with missing data is more in line with the needs of actual scenarios but has not been studied sufficiently yet. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Furthermore, existing RL-based approaches for TSC can be categorized into two types, i.e., online approaches and offline approaches. Most RL-based approaches for TSC rely on the online setting, interacting with the environment frequently. Specific to data-missing scenarios, MissLight [14] composed of the traffic data imputation stage and decision-making stage has been proposed in the online setting. However, frequent interaction with the real-world traffic environment is challenging and potentially unsafe, especially when dealing with incomplete data. As an alternative, training using offline traffic data with missing values offers a safer and more practical solution. Therefore, we focus on the offline setting in this paper. Similar to the online setting, traffic data imputation and decision-making for TSC with missing data are two sub-tasks we must confront in the offline setting. ", "page_idx": 1}, {"type": "text", "text": "Recently, diffusion models [15] have been introduced into offilne RL due to their powerful generative ability [16, 17, 18, 19, 20]. These approaches frame sequential decision-making as conditional generative modeling and utilize the generative ability of the diffusion model to capture complex policy distribution in offline datasets to make better decisions. Additionally, in the context of TSC with missing data, traffic data imputation is equally critical. Inspired by existing works [21, 22, 23], we approach traffic data imputation as a conditional generative problem, similar to decision-making. Considering the similarity of the two sub-tasks, we propose to unify traffic data imputation and decision-making for TSC with missing data by utilizing the powerful generative ability of the diffusion model. ", "page_idx": 1}, {"type": "text", "text": "There are several challenges that must be addressed to integrate the two sub-tasks mentioned above effectively. Firstly, in RL-based approaches for TSC, rewards which are typically vehicle queue length, are critical for the performance of controllers. However, due to the absence of traffic data, only partial rewards are available. A straightforward solution might be to flil in the missing rewards with padded values, which could confuse the imputed rewards with the actual ones, leading to a negative impact on performance. Secondly, relying solely on traffic data of the local intersection makes it challenging to capture the dynamic and spatial-temporal dependencies in the traffic network for traffic data imputation and decision-making tasks. The complexity of traffic flow often requires a broader context, as traffic data from the local intersection may not adequately reflect the behaviors and interactions occurring across the entire network. The absence of traffic data from neighboring intersections may further exacerbate this issue, hindering the ability to capture these dependencies and potentially leading to a decline in performance. ", "page_idx": 1}, {"type": "text", "text": "To tackle these challenges, we introduce DiffLight, a novel conditional diffusion model for TSC with missing data. We propose a Partial Rewards Conditioned Diffusion (PRCD) model for both traffic data imputation and decision-making under data-missing scenarios to prevent missing rewards from interfering with the learning process. Meanwhile, to effectively capture the spatial-temporal dependencies among intersections, we design the noise model as a Spatial-Temporal transFormer (STFormer) architecture. In addition, we propose a Diffusion Communication Mechanism (DCM) to enable communication and promote the capture of spatio-temporal dependencies in the traffic network through the propagation of generated observations, facilitating better control in scenarios with missing data. Extensive experiments on five datasets with various data-missing scenarios are conducted to evaluate the effectiveness of DiffLight. The experimental results indicate that DiffLight is highly competitive for TSC with missing data. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Partially Observable Markov Decision Process ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider a partially observable Markov decision process (POMDP) in the offline setting, defined as a tuple $\\langle S,A,\\mathcal{P},\\mathcal{R},\\Omega,\\mathcal{O},\\gamma\\rangle$ . $\\boldsymbol{S}$ is the state space, and $s_{t}\\in\\mathcal S$ denotes the state at time $t$ . $\\boldsymbol{\\mathcal{A}}$ is the set of available actions and $a_{t}\\in\\mathcal A$ denotes the action of an agent at time $t$ . The observation $o_{t}\\in\\Omega$ observed by the agent is part of the state $s_{t}$ and can be derived from the function $O(s_{t})$ . $r_{t}=\\mathcal{R}(s_{t})$ is the immediate reward of an agent at time $t$ . $\\mathcal{P}$ and $\\gamma$ denote the transition probability function and the discount factor separately. The optimization objective is to learn a policy $\\pi$ for agents to maximize the expected return $\\mathbb{E}_{s_{t},a_{t}}[R_{t}]$ , where $\\begin{array}{r}{R_{t}=\\dot{\\sum_{t}}\\,\\gamma^{t}r_{t}}\\end{array}$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.2 Traffic Signal Control with Missing Data ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We formulate TSC with missing data as POMDP, and consider TSC in a traffic network with several intersections. Agents are deployed at each intersection of the traffic network. As illustrated in Figure 1, for a fourway intersection, there are twelve traffic movements from the intersection\u2019s entrance lane $l_{\\mathrm{in}}$ to the departure lane $l_{\\mathrm{out}}$ , and four pairs of traffic movements without conflict comprise four traffic signal phases, i.e., A, B, C, D. For example, the traffic signal phase of the intersection in Figure 1 is phase-A which involves movement-2 $=\\{l_{\\mathrm{in}}^{2}\\stackrel{}{\\rightarrow}l_{\\mathrm{out}}^{7},l_{\\mathrm{in}}^{2}\\stackrel{\\cdot}{\\rightarrow}l_{\\mathrm{out}}^{8},l_{\\mathrm{in}}^{2}\\stackrel{}{\\rightarrow}l_{\\mathrm{out}}^{9}\\}$ and movement-8 in in $=\\{l_{\\mathrm{in}}^{\\updelta^{-}}\\rightarrow l_{\\mathrm{out}}^{1},l_{\\mathrm{in}}^{\\updelta^{-}}\\rightarrow l_{\\mathrm{out}}^{2},l_{\\mathrm{in}}^{\\updelta^{-}}\\rightarrow l_{\\mathrm{out}}^{3}\\}$ . ", "page_idx": 2}, {"type": "text", "text": "In this paper, the observation $o_{t}$ contains the number of vehicles $L_{\\mathrm{num}}$ and queue length $L_{\\mathrm{queue}}$ in every entrance lane of the local intersection. Available actions are four phases. The immediate reward $r_{t}$ is defined as the sum of the queue length $\\sum_{l_{\\mathrm{in}}}L_{\\mathrm{queue}}$ . Due to the lack or error of sensors resulting in missing traffic data, $o_{t}$ and $r_{t}$ , which are derived from traffic data, could be missing in a particular missing pattern. We consider random missing and kriging missing patterns detailed in Appendix C.2. To simplify the problem, we assume that $o_{t}$ and $r_{t}$ in an intersection are missing simultaneously. ", "page_idx": 2}, {"type": "image", "img_path": "A969ouPqEs/tmp/a4ab7a3ad4c4f6900683255fa8ca44bf550eaca2803e3017f0aeffd3e5ee81bb.jpg", "img_caption": ["Figure 1: Illustration of a four-way intersection with 12 traffic movements and 4 traffic signal phases. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.3 Diffusion Models for Reinforcement Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion model. Diffusion models [15, 24, 25], as a powerful generative model, provide a framework to model the data generative process as a discrete diffusion process. Diffusion models consist of two processes: the forward process and the reverse process. In this paper, the forward process is defined as $q(x^{k}|x^{k-1}):=\\mathcal{N}(\\sqrt{1-\\beta^{k}}x^{k-1},\\beta^{k}\\mathbf{I})$ by the Markov process, where $\\beta^{k}$ is the variance of the noise at timestep $k$ . We adopt DDIM sampler [24] to sample in the reverse process in order to accelerate sampling. DDIM sampler is parameterized with $p_{\\theta}(\\dot{x}^{k-1}|x^{k},x^{0}):=\\dot{\\mathcal{N}}(\\mu_{\\theta}(x^{k},k),(\\sigma^{k})^{2}\\mathbf{I})$ , which can be optimized by a simplified surrogate loss, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta):=\\mathbb{E}_{k\\sim\\mathcal{U}(1,K),\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})}[\\|\\epsilon_{\\theta}(x^{k},k)-\\epsilon\\|^{2}].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The reverse process b\u221aegins by samp\u221aling an initial noise $x_{K}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ . The estim\u221aated mean of Gauss\u03b1ika n =i s $\\mu_{\\theta}(x^{k},k)=\\sqrt{\\bar{\\alpha}^{k-1}}\\hat{x}_{0}\\ +\\sqrt{1-\\bar{\\alpha}^{k-1}}\\epsilon_{\\theta}(x^{k},k)$ wusheedr et $\\begin{array}{r}{\\hat{x}_{0}=\\frac{1}{\\sqrt{\\bar{\\alpha}^{k}}}(x^{k}-\\sqrt{1-\\bar{\\alpha}^{k}}\\epsilon_{\\theta}(x^{k},k))}\\end{array}$ $\\bar{\\alpha}^{k}=\\prod_{k=1}^{K}\\alpha^{k}$ $\\epsilon_{\\theta}(x^{k},\\boldsymbol{k})$ ", "page_idx": 2}, {"type": "text", "text": "Diffusing decision-making. Recently, many diffusion-based approaches have been proposed to address decision-making problems in RL. Among existing works, Diffuser [16] chooses to diffuse on an observation-action trajectory with returns as the condition to generate actions directly. Decision Diffuser [16] focuses solely on diffusing the observation trajectory conditioned on returns. By avoiding direct diffusion over actions, Decision Diffuser enhances performance in scenarios with discrete actions. In this paper, considering the discrete nature of actions in TSC, we focus on introducing the approach diffusing on the observation trajectory $\\tau$ sampled from offline dataset $\\mathcal{D}$ . We denote the $k$ -step denoised output of the diffusion model as $x^{k}(\\tau)$ . The observation trajectory would be diffused to generate $o_{t+1}$ . However, only diffusing on observation trajectory is not enough to make decisions. An inverse dynamics model $f_{\\phi}$ is adapted to generate the action $a_{t}$ that makes the observation transit from $o_{t}$ to $o_{t+1}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\na_{t}:=f_{\\phi}\\big(o_{t},o_{t+1}\\big).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Classifier-free guidance. Classifier-free guidance [26] aims to learn the conditional distribution $q(x(\\tau)|y(\\tau))$ . It learns both a conditional $\\bar{\\epsilon_{\\theta}}(x^{k}(\\tau),\\bar{y}(\\bar{\\tau}),k)$ and an unconditional $\\epsilon_{\\theta}(x^{k}(\\tau),\\phi,k)$ for the noise. Then, the perturbed noise $\\epsilon_{\\theta}(x^{k}(\\tau),\\phi,k)+\\omega\\big(\\epsilon_{\\theta}(x^{k}(\\tau),y(\\tau),k)-\\epsilon_{\\theta}(x^{k}(\\tau),\\phi,k)\\big)$ can be used to generate samples, where $\\omega$ is the guidance scale. ", "page_idx": 3}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In order to effectively unify traffic data imputation and decision-making for TSC with missing data, we consider both of them as a conditional generative modeling problem via diffusion models, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\theta}\\mathbb{E}_{\\tau\\sim\\mathcal{D}}[\\log p_{\\theta}(x^{0}(\\tau)|y(\\tau))],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $p_{\\theta}$ is a learnable model distribution to estimate the conditional data distribution of trajectory $x^{0}(\\tau)$ , conditioned on $y(\\tau)$ . We construct our generative model according to the conditional diffusion process, ", "page_idx": 3}, {"type": "equation", "text": "$$\nq(x^{k}(\\tau)|x^{k-1}(\\tau)),\\qquad p_{\\theta}(x^{k-1}(\\tau)|x^{k}(\\tau),x^{0}(\\tau),y(\\tau)),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with conditions as, ", "page_idx": 3}, {"type": "equation", "text": "$$\ny(\\tau):=[r(\\tau),y^{\\prime}(\\tau)],\\qquad y^{\\prime}(\\tau):=[\\tau_{\\mathrm{obs}}^{\\prime},\\tau_{\\mathrm{nei}}^{\\prime}].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To facilitate a better understanding of the symbol definitions, we categorize the trajectory segments into three types. The observed trajectory consists of data collected by sensors up to time $t$ . The missing trajectory includes data that have not been collected during this period. The observable trajectory encompasses both the observed trajectory and the data that could potentially be collected in the future. In this context, $r(\\tau)$ is the observable reward trajectory, and $\\tau_{\\mathrm{obs}}^{\\prime}$ is the observed part of trajectory $\\tau$ from the local intersection. $\\tau_{\\mathrm{nei}}^{\\prime}=\\cup_{N}f_{\\mathrm{nei}}(\\tau^{i})$ is the observed observations from neighboring intersections, where $\\tau^{i}$ denotes the observation trajectory of the neighboring intersection $i$ , $N$ is total number of neighboring intersections, $f_{\\mathrm{nei}}(\\cdot)$ represents the observed observations from entrance lanes of all neighboring intersections that feed into the entrance lanes of the local intersection, as shown in Figure 1. Due to discrete and high-frequent actions in TSC, we choose to diffuse solely on observations and utilize the inverse dynamic model to generate actions, which demonstrates a better performance proven in Appendix F.1 and F.6. We define the observation trajectory $\\tau$ under data-missing scenarios as, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tau:=\\big[\\hat{o}_{t-C+1},o_{t-C+2},\\cdots\\,,\\hat{o}_{t},\\hat{o}_{t+1},\\cdot\\cdot\\cdot\\,,\\hat{o}_{t+H}\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $o_{t}$ is the observation collected by sensors, $\\hat{O}_{t}$ is the uncollected observation up to time $t$ or a potentially collectible observation in the future, $C$ is the length of historical observations and $H$ is the horizon of future observations. It should be noted that we generate $H$ -step future observations to enable effective long-horizon planning [16]. ", "page_idx": 3}, {"type": "text", "text": "Figure 2 shows the overview of DiffLight, which consists of the Partial Rewards Conditioned Diffusion (PRCD), a noise model with a Spatial-Temporal transFormer structure (STFormer), and the Diffusion Communication Mechanism (DCM). We introduce each of them in the following sections. ", "page_idx": 3}, {"type": "text", "text": "3.1 Partial Rewards Conditioned Diffusion ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Under data-missing scenarios, sensors deployed to collect traffic data for rewards may malfunction or be lacking. The absence of partial rewards makes it challenging to calculate returns. Therefore, we adopt partial rewards as the condition instead of returns [16, 17, 19], which can be expressed as, ", "page_idx": 3}, {"type": "equation", "text": "$$\nr(\\tau):=[\\tilde{r}_{t-C+1},r_{t-C+2},\\cdot\\cdot\\cdot\\,,\\tilde{r}_{t},r_{t+1},\\cdot\\cdot\\cdot\\,,r_{t+H}],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $r_{t}$ is the collected reward or a potentially collectible reward in the future, and $\\tilde{r}_{t}$ is the uncollected reward. ", "page_idx": 3}, {"type": "image", "img_path": "A969ouPqEs/tmp/2d65a4c43b30f35db014aaa319d27a8bd043ca7bb140798ccc144f487887464b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: An overview of DiffLight. We demonstrate the signal control process of an intersection in random missing. Traffic data is collected by sensors to derive rewards and observations. Missing rewards and observations are masked. Only the observed part of the observation trajectory and observable rewards from the local intersection, and observation trajectories from neighboring intersections would be input into PRCD with STFormer. In the inference process, DCM would work with STFormer to generate observations. The inverse dynamics model is used to generate actions to control the traffic signal. ", "page_idx": 4}, {"type": "text", "text": "There are two ways to handle the missing part of rewards: conditioning on the rewards with padded values or conditioning on the partial observable rewards directly. Padding a specific value in the missing part is a feasible way. However, padded values could confuse the imputed rewards with the actual ones, leading to a negative impact on performance proven in Section 4.3. For better decision-making, we choose to condition on only partial observable rewards. We assume that the observable part and the missing part of the trajectory are collected by real sensors and virtual sensors separately, and the distribution of traffic data collected by two kinds of sensors is independent. In this case, the distribution in Equation 3 can be factorized as, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{\\theta}(x^{0}(\\tau)|y(\\tau))=p_{\\theta}(x^{0}(\\tau_{\\mathrm{obs}})|r(\\tau),y^{\\prime}(\\tau))\\cdot p_{\\theta}(x^{0}(\\tau_{\\mathrm{mis}})|y^{\\prime}(\\tau)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\tau_{\\mathrm{obs}}$ is the observable part of the trajectory $\\tau$ from the local intersection, and $\\tau_{\\mathrm{mis}}$ is the missing part. We parameterize Equation 8 as the same form of locally conditioned diffusion [27, 28], propose partial rewards conditioned diffusion (PRCD) with classifier-free guidance [26] and introduce it into TSC with missing data. Given condition set $\\mathbf{c}=\\{\\phi,r(\\tau)\\}$ and binary non-overlapping mask set $\\mathbf{m}=\\{m_{\\mathrm{mis}},m_{\\mathrm{obs}}\\}$ , PRCD assigns partial rewards to corresponding observation sub-trajectory masked by $\\mathbf{m}$ , which can be formulated as, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\epsilon}_{\\boldsymbol{\\theta}}(x^{k}(\\tau),k,y^{\\prime}(\\tau),\\mathbf{c},\\mathbf{m}):=\\!m_{\\mathrm{obs}}\\odot\\hat{\\epsilon}_{\\boldsymbol{\\theta}}(x^{k}(\\tau),k,y^{\\prime}(\\tau),r(\\tau))\\!+}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad m_{\\mathrm{mis}}\\odot\\hat{\\epsilon}_{\\boldsymbol{\\theta}}(x^{k}(\\tau),k,y^{\\prime}(\\tau),\\boldsymbol{\\phi}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\hat{\\epsilon}_{\\theta}(x^{k}(\\tau),k,y^{\\prime}(\\tau),c_{i})$ could be expressed using classifier-free guidance, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\epsilon}_{\\theta}(x^{k}(\\tau),k,y^{\\prime}(\\tau),c_{i}):=\\!\\epsilon_{\\theta}(x^{k}(\\tau),k,y^{\\prime}(\\tau),\\phi)+}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\omega(\\epsilon_{\\theta}(x^{k}(\\tau),k,y^{\\prime}(\\tau),c_{i})-\\epsilon_{\\theta}(x^{k}(\\tau),k,y^{\\prime}(\\tau),\\phi)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $c_{i}\\in\\mathbf{c}$ . We provide a derivation for the feasibility of PRCD in Appendix E. ", "page_idx": 4}, {"type": "text", "text": "3.2 Diffusing with Spatial-Temporal Transformer ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The noise model with a U-Net structure is widely applied in image generation [15, 24, 29, 30], control [16, 17, 19] and other fields. However, it is hard to be applied to capture the spatial-temporal dependencies in TSC. The emergence of Transformer [31] and its applications on spatial-temporal modeling [32, 33, 34, 35] provide a promising solution to deal with it. In this section, we design a Spatial-Temporal transFormer (STFormer) structure to effectively model the spatial-temporal dependencies in TSC, which includes a data embedding layer, stacked $L$ spatial-temporal encoder layers, and an output layer. Data embedding layer embeds different inputs into embeddings, including diffusion timestep, trajectory timestep, rewards, trajectory of the local intersection, and neighboring intersections. Spatial-Temporal Encoder layer (STE) is composed of Communication Cross-Attention module (CCA), Spatial Self-Attention module (SSA), and Temporal Self-Attention module (TSA). CCA is designed to capture the spatial-temporal dependencies between the local intersection and neighboring intersections. SSA and TSA are designed to capture the spatial dependencies and temporal dependencies at the local intersection separately. Output layer is used to convert the output of STE into the noise we desire to predict. We detail the structure of STFormer in Appendix B.2. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3.3 Diffusion Communication Mechanism ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Observations of neighboring intersections are crucial for TSC with missing data [14]. However, due to the possible absence of observations from neighboring intersections, the traffic signal could be controlled ineffectively. For instance, we assume an extreme situation where there is no available observation in both the local intersection and neighboring intersections. In this case, observation trajectories of intersections are all masked by noise, leading to difficulty in decision-making at the local intersection. Therefore, we propose a Diffusion Communication Mechanism (DCM) to disseminate observation information generated by the noise model in the reverse process among intersections. Formally, we formulate DCM as, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tau_{\\mathrm{nei}}^{\\prime}=\\left\\{\\begin{array}{l l}{\\cup_{N}\\;f_{\\mathrm{nei}}(\\tau^{i}),}&{k=K,}\\\\ {\\cup_{N}\\;f_{\\mathrm{nei}}(\\frac{1}{\\sqrt{\\bar{\\alpha}^{k}}}(x^{k}(\\tau^{i})-\\sqrt{1-\\bar{\\alpha}^{k}}\\hat{\\epsilon}_{\\theta}(x^{k}(\\tau),k,y^{\\prime}(\\tau),\\mathbf{c},\\mathbf{m}))),}&{k<K.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The reverse process begins by inputting original observations of neighboring intersections with missing data. During diffusing, we predict $\\hat{x}^{\\tilde{0}}(\\tau^{i})$ , which is the same in Section 2.3. With the help of DCM, generated observations of neighboring intersections could be spread from neighboring intersections for better decisions of the agent at the local intersection. Note that we train our model with ground-truth values collected from neighboring intersections and only use DCM in the inference process. ", "page_idx": 5}, {"type": "text", "text": "3.4 Training and Inference of DiffLight ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Training process. Given an offline dataset $\\mathcal{D}$ which consists of observation trajectories, rewards and actions, we train the reverse process $p_{\\theta}$ parameterized through the noise model $\\epsilon_{\\theta}$ , and the inverse dynamics model $f_{\\phi}$ in DiffLight with the following loss, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(\\theta):=\\!\\!\\mathbb{E}_{(o,a,o^{\\prime})\\in\\mathcal{D}}[\\|a-f_{\\phi}(o,o^{\\prime})\\|^{2}\\cdot\\mathbb{1}(o,o^{\\prime})]\\!+}\\\\ &{\\qquad\\quad\\mathbb{E}_{k,\\epsilon,\\beta\\sim\\mathrm{Bern}(p)}[\\|\\hat{\\epsilon}_{\\theta}(x^{k}(\\tau),k,y^{\\prime}(\\tau),(1-\\beta)r(\\tau)+\\beta\\phi,\\mathbf{m})-\\epsilon\\|^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For each trajectory $\\tau$ , we sample noise $\\epsilon\\,\\mathbf{\\Omega}\\sim\\,\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ and a diffusion timestep $k\\;\\sim\\mathcal{U}(1,K)$ , construct a masked noise array of observations $x^{k}(\\tau)$ with $\\tau_{\\mathrm{obs}}^{\\prime}$ , and predict the noise $\\hat{\\epsilon}_{\\theta}(x^{k}(\\tau),k,y^{\\prime}(\\tau),r(\\tau),\\mathbf{m})$ . Missing values in condition $\\tau_{\\mathrm{nei}}^{\\prime}$ are padded with zeros. It should be noted that we ignore the rewards condition $r(\\tau)$ with probability $p$ and the inverse dynamics is trained with individual transitions without missing observation $o$ or $o^{\\prime}$ . For the training process of DiffLight, due to the inaccessibility of the ground-truth of missing data, we consider it self-supervised learning. In random missing, given a trajectory $\\tau$ and conditions, we can separate the observed part into two parts and set one of them to miss. In kriging missing, we randomly mask the whole trajectories of one observed intersection. Then, we can train the noise model $\\hat{\\epsilon}_{\\theta}$ by solving Equation 12. ", "page_idx": 5}, {"type": "text", "text": "Inference process. DiffLight is deployed to every intersection in the traffic network in the inference process. Given rewards $r(\\tau)$ , a $C$ -step observed trajectory of local intersection $\\tau_{\\mathrm{obs}}^{\\prime}$ with missing data and trajectories of neighboring intersections $\\tau_{\\mathrm{nei}}^{\\prime}$ , the agent can impute the missing observations of local intersection, predict the observations in the future and generate next action with Equation 2, 9 and 10. In order to sample a high reward trajectory, the rewards from time $t+1$ to $t+H$ are considered in $r(\\tau)$ , which is set to 1. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Experiment Settings We conduct our experiments on CityFlow [36], a traffic simulator widely used in various RL-based methods. Similar to existing work in [13], we set the phase number as four and the minimum action duration as 15 seconds. ", "page_idx": 6}, {"type": "text", "text": "Datasets The datasets consist of two parts: offline datasets with missing data and real-world traffic flow datasets with traffic networks. We apply five real-world traffic flow datasets [9, 37] for comparison, including Hangzhou1 $\\mathcal{D}_{\\mathrm{HZ}}^{1})$ , Hangzh ${\\boldsymbol{\\mathrm{{)}}}\\boldsymbol{\\mathrm{{u}}}}_{2}$ $\\mathcal{D}_{\\mathrm{HZ}}^{2})$ , Jinan1 $(\\mathcal{D}_{\\mathbf{JN}}^{1})$ , Jinan2 $(\\mathcal{D}_{\\mathrm{JN}}^{2})$ and Jinan3 $(\\mathcal{D}_{\\mathrm{JN}}^{3})$ . Corresponding offline datasets are composed of training trajectories of three online methods. We detail the datasets in Appendix C.1. To simulate data-missing scenarios, we adopt masks of different missing rates in different missing patterns, including random missing (RM) and kriging missing (KM), to mask observations and rewards. We describe the details of two patterns and missing rates in Appendix C.2. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics We use the average travel time (ATT) as the main metric for evaluation, which is widely used to evaluate the performance of TSC. It calculates the average time all the vehicles spend between entering and leaving the traffic network during simulation, which is formulated as, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{ATT}=\\frac{1}{N}\\sum_{i=1}^{N}\\left(t_{i}^{l}-t_{i}^{e}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $N$ is the total number of vehicles entering the road network, $t_{i}^{e}$ and $t_{i}^{l}$ are the entering time and leaving time for the $i$ -th vehicle respectively. The lower ATT indicates a better control performance. ", "page_idx": 6}, {"type": "text", "text": "Compared Methods We compare our method with Behavior Cloning (BC) [38] and offline approaches, including CQL [39], $\\mathrm{TD}3\\substack{+\\mathrm{BC}}$ [40], Decision Transformer (DT) [41], Diffuser [16], Decision Diffuser (DD) [17]. Similar to the existing work in [14], we adopt store-and-forward method (SFM) [42], a rule-based method that has generally more stable performances, to impute missing observations and rewards for these approaches. We detail these approaches and SFM in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "4.2 Performance under Data-Missing Scenarios ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We train and test our method on all five datasets with different missing patterns and missing rates, and compare our method with all baselines. DiffLight performs the best on most of the datasets. We analyze experiments of different missing patterns below. ", "page_idx": 6}, {"type": "text", "text": "Random missing. In random missing, we can see that DiffLight achieves optimal or sub-optimal performance compared with baselines in all datasets in Table 1. Diffusion-based approaches, including Diffuser and DD, show a better performance than other baselines in most datasets. These diffusionbased approaches utilize a noise model to predict noise and generate actions or observations, which helps mitigate the disturbance caused by imputed observations and rewards during the diffusion process. Compared with diffusion-based approaches, the performance of other baselines without the diffusion process is disturbed by imputed observations and rewards more seriously. ", "page_idx": 6}, {"type": "table", "img_path": "A969ouPqEs/tmp/dc9eb766d2e6bfc63eaf4f68ecc5605f474e30a903d141cc58c7e3d742e29d9f.jpg", "table_caption": ["Table 1: Comparing ATT for DiffLight and baselines in random missing. We report the mean and the standard error for three trials. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Kriging missing. In kriging missing, DiffLight shows the best performance in most datasets in Table 2. Unlike the results of random missing, DD does not perform well in kriging missing compared with other baselines. Since DD must impute missing observations with the SFM model first, generate future observations with the diffusion model, and then use the inverse dynamics to generate actions, which leads to serious error accumulation. While other baselines generate actions directly, requiring only roughly imputed observations and rewards. ", "page_idx": 7}, {"type": "table", "img_path": "A969ouPqEs/tmp/1d6eefd9af6433557df27bc071d45e27bfbe2976f049f74391b6263752d15cf0.jpg", "table_caption": ["Table 2: Comparing ATT for DiffLight and baselines in kriging missing. We report the mean and the standard error for three trials. "], "table_footnote": ["We provide the overall performance of DiffLight and baselines without missing data as well, which is detailed in Appendix F.1. In addition, we provide further experiments on the influence of unobserved locations of intersections in Appendix F.2, the limit of missing rates in Appendix F.3, and the scalability of the approach in Appendix F.4. "], "page_idx": 7}, {"type": "text", "text": "4.3 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We further evaluate the effectiveness of different parts in DiffLight with the following variants. (1) U-Net: this variant replaces STFormer with U-Net as the noise model and missing rewards input are zero-padded. (2) STFormer: this variant uses STFormer as the noise model and keeps missing rewards zero-padded. (3) STFormer+PRCD: this is equal to DiffLight which uses STFormer as the noise model and is conditioned on partial rewards. It should be noted that DCM is adopted in both STFormer and STFormer $^+$ PRCD and all missing observations would not be imputed by the SFM model but are masked with Gaussian noise in order to be inpainted in the reverse process in ablation experiments. ", "page_idx": 8}, {"type": "table", "img_path": "A969ouPqEs/tmp/a0c405d8871eacb78b4ebafbb5d1a1d4553dfc35e7b5e27073760447c037e74d.jpg", "table_caption": ["Table 3: Ablation study on Hangzhou1 and Jinan1. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 3 shows the comparison of these variants on $\\mathrm{{Hangzhou_{1}}}$ and Jinan1 with random missing and kriging missing. Based on the results, we can find that STFormer which takes spatial-temporal dependencies into consideration leads to a great performance improvement over U-Net. It shows that capturing spatial-temporal dependencies is important in TSC. STFormer+PRCD performs better than STFormer, indicating that padding values in rewards could be confused with the ground-truth rewards and only conditioning on partial rewards could have a better performance. Due to the space limitation, we provide further experiments on DCM in Appendix F.5 and the inverse dynamics in Appendix F.6. ", "page_idx": 8}, {"type": "text", "text": "4.4 Model Generalization ", "text_level": 1, "page_idx": 8}, {"type": "image", "img_path": "A969ouPqEs/tmp/254c94eb7bc7f1eafe188ff114dca27c82c3121df44ee4644ef811c580a1ece4.jpg", "img_caption": ["Figure 3: The relative generalization performance of DiffLight in different missing rates. The $\\mathbf{X}_{\\mathrm{}}$ -axis is the missing rate during testing and the y-axis is the missing rate during training. The formula used to calculate the relative generalization performance is depicted below. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "We evaluate the generalization performance of DiffLight on Hangzhou and $\\operatorname{Jinan}_{1}$ with different missing rates. We train our method in a specific missing rate and test it on the same dataset with other missing rates. To better compare the generalization performance among models trained in different missing rates, we formulate the relative generalization performance as, ", "page_idx": 8}, {"type": "equation", "text": "$$\nP_{\\mathrm{r}}=P_{\\mathrm{g}}/P_{\\mathrm{o}},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $P_{\\mathrm{r}}$ is the relative generalization performance of the current missing rate, $P_{\\mathrm{g}}$ is the performance of the model trained in other missing rates, and $P_{\\mathrm{{o}}}$ is the performance of the model trained in the current missing rate. The results in Figure 3 demonstrate that the generalization performance of DiffLight is excellent in most situations. If we train DiffLight in a high missing rate and test it in a lower missing rate, the performance of DiffLight remains stable. In contrast, if we train DiffLight in a low missing rate and test it in a higher missing rate, the performance of DiffLight would decrease slightly. As data with a higher missing rate has more complex missing situations, making it difficult for models to handle these situations. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Traffic Signal Control TSC approaches can be categorized into conventional and RL-based methods. Conventional approaches like Fixed-time [2], SCOOT [3] and SCATS [4] have been widely deployed in different cities. In recent years, RL-based approaches for TSC get more attention. DQN algorithm is introduced into TSC in [5, 6, 7] for dynamic real-time control. Attention mechanisms are applied to promote inter-agent communication [9] and build universal models [10]. Max-pressure [8] and advanced-MP [13] are proposed to promote the performance of existing methods. TSC with missing data is considered with the help of the imputation model in the online setting. However, there is no existing work to solve this problem in the offline setting. ", "page_idx": 9}, {"type": "text", "text": "Diffusion-based Reinforcement Learning There are various works applying the diffusion model to offilne RL recently. Diffusion and deep q-learning are combined [18], demonstrating the potential of diffusion in RL. The state-action trajectory is encoded in latent space [43], enhancing credit assignment and reward propagation. In addition to methods relying on TD-learning, Diffuser [16] and Decision Diffuser [17] are proposed as planners to generate the trajectory with a conditional diffusion model. However, they are all studied under scenarios without missing data, while we model TSC with missing data. ", "page_idx": 9}, {"type": "text", "text": "Traffic Data Imputation With the development of deep learning, RNN-based methods [44, 45, 46] show good performance for traffic data imputation. In subsequent studies, diffusion models are utilized to learn the complex distribution in traffic data [21, 22]. For TSC, store-and-forward method (SFM) [42] is proven to have a more stable performance than neural networks [14]. In this paper, we adopt SFM to impute observations and rewards for baselines. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion and Limitation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Conclusion In this paper, we introduce DiffLight, a novel conditional diffusion model designed for TSC in scenarios with missing data. Our approach centers on the Partial Rewards Conditioned Diffusion (PRCD) model, which addresses both traffic data imputation and decision-making in the presence of incomplete data. This model helps prevent missing rewards from disrupting the learning process. We address the challenge of capturing spatial-temporal dependencies across intersections by designing a Spatial-Temporal transFormer (STFormer) architecture as the noise model. Additionally, to enhance communication and control performance, we propose a Diffusion Communication Mechanism (DCM) that facilitates the propagation of generated observations. We conduct extensive experiments on different datasets and settings to demonstrate that DiffLight is an effective controller to address TSC with missing data. ", "page_idx": 9}, {"type": "text", "text": "Limitation In this work, we only consider two missing patterns: random missing and kriging missing. While in the real world, the missing pattern in the traffic network is more complex. Meanwhile, we just adopt SFM which is similar to $\\mathbf{k}$ -nearest neighbor (KNN) to impute the traffic data for baselines. In addition, our approach is conditioned on partial rewards instead of returns which could lead to the short-sightedness of agents. Future work could explore the influence on performance in more different missing patterns even mixed missing patterns, adopt more different imputation methods, and find out a more far-sighted method to control the traffic signals under data-missing scenarios. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Natural Science Foundation of China (No. 62372031). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Juan Lu, Bin Li, He Li, and Abdo Al-Barakani. Expansion of city scale, traffic modes, traffic congestion, and air pollution. Cities, 108:102974, 2021.   \n[2] Fo Vo Webster. Traffic signal settings. Technical report, 1958.   \n[3] PB Hunt, DI Robertson, RD Bretherton, and M Cr Royle. The scoot on-line traffic signal optimisation technique. Traffic Engineering & Control, 23(4), 1982.   \n[4] PR Lowrie. Scats, sydney co-ordinated adaptive traffic system: A traffic responsive method of controlling urban traffic. 1990.   \n[5] Elise Van der Pol and Frans A Oliehoek. Coordinated deep reinforcement learners for traffic light control. Proceedings of learning, inference and control of multi-agent systems (at NIPS 2016), 8:21\u201338, 2016.   \n[6] Li Li, Yisheng Lv, and Fei-Yue Wang. Traffic signal timing via deep reinforcement learning. IEEE/CAA Journal of Automatica Sinica, 3(3):247\u2013254, 2016.   \n[7] Hua Wei, Guanjie Zheng, Huaxiu Yao, and Zhenhui Li. Intellilight: A reinforcement learning approach for intelligent traffic light control. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, pages 2496\u20132505, 2018.   \n[8] Hua Wei, Chacha Chen, Guanjie Zheng, Kan Wu, Vikash Gayah, Kai Xu, and Zhenhui Li. Presslight: Learning max pressure control to coordinate traffic signals in arterial network. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 1290\u20131298, 2019.   \n[9] Hua Wei, Nan Xu, Huichu Zhang, Guanjie Zheng, Xinshi Zang, Chacha Chen, Weinan Zhang, Yanmin Zhu, Kai Xu, and Zhenhui Li. Colight: Learning network-level cooperation for traffic signal control. In Proceedings of the 28th ACM international conference on information and knowledge management, pages 1913\u20131922, 2019.   \n[10] Afshin Oroojlooy, Mohammadreza Nazari, Davood Hajinezhad, and Jorge Silva. Attendlight: Universal attention-based reinforcement learning model for traffic signal control. Advances in Neural Information Processing Systems, 33:4079\u20134090, 2020.   \n[11] Chacha Chen, Hua Wei, Nan Xu, Guanjie Zheng, Ming Yang, Yuanhao Xiong, Kai Xu, and Zhenhui Li. Toward a thousand lights: Decentralized deep reinforcement learning for large-scale traffic signal control. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 3414\u20133421, 2020.   \n[12] Xinshi Zang, Huaxiu Yao, Guanjie Zheng, Nan Xu, Kai Xu, and Zhenhui Li. Metalight: Value-based meta-reinforcement learning for traffic signal control. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 1153\u20131160, 2020.   \n[13] Liang Zhang, Qiang Wu, Jun Shen, Linyuan L\u00fc, Bo Du, and Jianqing Wu. Expression might be enough: representing pressure and demand for reinforcement learning based traffic signal control. In International Conference on Machine Learning, pages 26645\u201326654. PMLR, 2022.   \n[14] Hao Mei, Junxian Li, Bin Shi, and Hua Wei. Reinforcement learning approaches for traffic signal control under missing data. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, pages 2261\u20132269, 2023.   \n[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[16] Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. In International Conference on Machine Learning, pages 9902\u20139915. PMLR, 2022.   \n[17] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua B Tenenbaum, Tommi S Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need for decision making? In The Eleventh International Conference on Learning Representations, 2022.   \n[18] Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class for offilne reinforcement learning. In The Eleventh International Conference on Learning Representations, 2022.   \n[19] Zhengbang Zhu, Minghuan Liu, Liyuan Mao, Bingyi Kang, Minkai Xu, Yong Yu, Stefano Ermon, and Weinan Zhang. Madiff: Offline multi-agent learning with diffusion models. arXiv preprint arXiv:2305.17330, 2023.   \n[20] Haoran He, Chenjia Bai, Kang Xu, Zhuoran Yang, Weinan Zhang, Dong Wang, Bin Zhao, and Xuelong Li. Diffusion model is an effective planner and data synthesizer for multi-task reinforcement learning. Advances in neural information processing systems, 36, 2023.   \n[21] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional score-based diffusion models for probabilistic time series imputation. Advances in Neural Information Processing Systems, 34:24804\u201324816, 2021.   \n[22] Mingzhe Liu, Han Huang, Hao Feng, Leilei Sun, Bowen Du, and Yanjie Fu. Pristi: A conditional diffusion framework for spatiotemporal imputation. In 2023 IEEE 39th International Conference on Data Engineering (ICDE), pages 1927\u20131939. IEEE, 2023.   \n[23] Qianru Zhang, Haixin Wang, Cheng Long, Liangcai Su, Xingwei He, Jianlong Chang, Tailin Wu, Hongzhi Yin, Siu-Ming Yiu, Qi Tian, et al. A survey of generative techniques for spatial-temporal data mining. arXiv preprint arXiv:2405.09592, 2024.   \n[24] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2020.   \n[25] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR, 2015.   \n[26] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.   \n[27] Ryan Po and Gordon Wetzstein. Compositional 3d scene generation using locally conditioned diffusion. In 2024 International Conference on 3D Vision (3DV), pages 651\u2013663. IEEE, 2024.   \n[28] Jiawei Ren, Mengmeng Xu, Jui-Chieh Wu, Ziwei Liu, Tao Xiang, and Antoine Toisoul. Move anything with layered scene diffusion. arXiv preprint arXiv:2404.07178, 2024.   \n[29] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 8162\u20138171. PMLR, 2021.   \n[30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[32] Shengnan Guo, Youfang Lin, Ning Feng, Chao Song, and Huaiyu Wan. Attention based spatial-temporal graph convolutional networks for traffic flow forecasting. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 922\u2013929, 2019.   \n[33] Shengnan Guo, Youfang Lin, Huaiyu Wan, Xiucheng Li, and Gao Cong. Learning dynamics and heterogeneity of spatial-temporal graph data for traffic forecasting. IEEE Transactions on Knowledge and Data Engineering, 34(11):5415\u20135428, 2021.   \n[34] Aosong Feng and Leandros Tassiulas. Adaptive graph spatial-temporal transformer network for traffic forecasting. In Proceedings of the 31st ACM international conference on information & knowledge management, pages 3933\u20133937, 2022.   \n[35] Jiawei Jiang, Chengkai Han, Wayne Xin Zhao, and Jingyuan Wang. Pdformer: Propagation delay-aware dynamic long-range transformer for traffic flow prediction. In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 4365\u20134373, 2023.   \n[36] Huichu Zhang, Siyuan Feng, Chang Liu, Yaoyao Ding, Yichen Zhu, Zihan Zhou, Weinan Zhang, Yong Yu, Haiming Jin, and Zhenhui Li. Cityflow: A multi-agent reinforcement learning environment for large scale city traffic scenario. In The world wide web conference, pages 3620\u20133624, 2019.   \n[37] Guanjie Zheng, Yuanhao Xiong, Xinshi Zang, Jie Feng, Hua Wei, Huichu Zhang, Yong Li, Kai Xu, and Zhenhui Li. Learning phase competition for traffic signal control. In Proceedings of the 28th ACM international conference on information and knowledge management, pages 1963\u20131972, 2019.   \n[38] Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. In Proceedings of the 27th International Joint Conference on Artificial Intelligence, pages 4950\u20134957, 2018.   \n[39] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative $\\mathbf{q}$ -learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33:1179\u20131191, 2020.   \n[40] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offilne reinforcement learning. Advances in neural information processing systems, 34:20132\u201320145, 2021.   \n[41] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084\u201315097, 2021.   \n[42] Konstantinos Aboudolas, Markos Papageorgiou, and Elias Kosmatopoulos. Store-and-forward based methods for the signal control problem in large-scale congested urban road networks. Transportation Research Part C: Emerging Technologies, 17(2):163\u2013174, 2009.   \n[43] Siddarth Venkatraman, Shivesh Khaitan, Ravi Tej Akella, John Dolan, Jeff Schneider, and Glen Berseth. Reasoning with latent diffusion in offilne reinforcement learning. In The Twelfth International Conference on Learning Representations, 2023.   \n[44] Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. Recurrent neural networks for multivariate time series with missing values. Scientific reports, 8(1):6085, 2018.   \n[45] Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. Brits: Bidirectional recurrent imputation for time series. Advances in neural information processing systems, 31, 2018.   \n[46] Jinsung Yoon, William R Zame, and Mihaela van der Schaar. Estimating missing data in temporal data streams using multi-directional recurrent neural networks. IEEE Transactions on Biomedical Engineering, 66(5):1477\u20131490, 2018.   \n[47] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[48] Qiang Wu, Liang Zhang, Jun Shen, Linyuan L\u00fc, Bo Du, and Jianqing Wu. Efficient pressure: Improving efficiency for signalized intersections. arXiv preprint arXiv:2112.02336, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Broad Impacts ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Our proposed method demonstrates effective abilities in TSC with missing data. It can handle different missing patterns and different missing rates when controlling traffic signals. Even in an intersection where there are no observed neighboring intersections, DiffLight can perform competitively against baselines with SFM. Moreover, the fast inference speed with fewer sampling steps and stable performance indicates that DiffLight can be deployed to achieve real-time control in the real world. However, a potential negative impact of this work is that bad decisions could lead to a collapse in the traffic network. ", "page_idx": 13}, {"type": "text", "text": "B The Details of DiffLight ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Hyperparameters of DiffLight ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we describe the details of hyperparameters, ", "page_idx": 13}, {"type": "text", "text": "\u2022 We set the batch size as 64 and each sample contains the trajectory of the whole intersections in the traffic network. We train our model using Adam optimizer [47] with $2e^{-4}$ learning rate for $1.5e^{5}$ train steps.   \n\u2022 We train DiffLight on NVIDIA GeForce RTX A5000 for around 15 hours and test it on the same GPU.   \n\u2022 We choose the probability $p$ of removing the condition information to be 0.25 and guidance scale $\\alpha=1.2$ .   \n\u2022 In DiffLight, we choose the length of historical observations $C\\,=\\,5$ and the planning horizon of observation trajectory $H=3$ .   \n\u2022 We use $K=100$ for diffusion steps. ", "page_idx": 13}, {"type": "text", "text": "B.2 Structure of STFormer ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "STFormer is composed of a data embedding layer, stacked $L$ spatial-temporal encoder layers, and an output layer. We detail each of them as follows. ", "page_idx": 13}, {"type": "text", "text": "Data embedding layer. Different inputs are embedded into embeddings $^e$ with the same dimension $D$ by the data embedding layer which consist of separate MLPs $f_{\\mathrm{MLP}}(\\cdot)$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\boldsymbol e}_{\\mathrm{dt}}:=f_{\\mathrm{MLP}}({\\boldsymbol k}),\\quad{\\boldsymbol e}_{\\mathrm{tt}}:=f_{\\mathrm{MLP}}(t_{0:T-1}),\\quad{\\boldsymbol e}_{\\mathrm{r}}:=f_{\\mathrm{MLP}}(R({\\boldsymbol\\tau})),}\\\\ &{{\\boldsymbol e}_{\\mathrm{ctr}}:=f_{\\mathrm{MLP}}({\\boldsymbol\\mathrm{x}}^{k}({\\boldsymbol\\tau})),\\quad{\\boldsymbol e}_{\\mathrm{ntr}}:=f_{\\mathrm{MLP}}({\\boldsymbol\\tau}_{\\mathrm{nei}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $t_{0:T-1}$ is the timestep of trajectory, $e_{\\mathrm{dt}}$ $_{\\mathrm{t},\\ e_{\\mathrm{tt},\\ e_{\\mathrm{r},}}}$ $e_{\\mathrm{ctr}}$ and $e_{\\mathrm{ntr}}$ represent the embedding of diffusion timestep, trajectory timestep, rewards, trajectory of local intersection and partial trajectory of neighboring intersection separately. ", "page_idx": 13}, {"type": "text", "text": "Spatial-temporal encoder layer. The spatial-temporal encoder layer, abbreviated as STE, is composed of Communication Cross-Attention module, Spatial Self-Attention module and Temporal Self-Attention module. We adopt the vanilla attention operator [31] in modules, represented as $f_{\\mathrm{Att}}(Q,\\bar{K},V)$ . The following slice notations are used to formulate attention modules. For the embedding of local intersection\u2019s trajectory $e_{\\mathrm{ctr}}\\in$ $\\mathbb{R}^{T\\times L\\times D}$ where $L$ is the number of entrance lanes, the $t$ slice is $\\pmb{e}_{\\mathrm{ctr}}^{t::}\\in\\mathbb{R}^{L\\times D}$ and the $l$ slice is $\\bar{e}_{\\mathrm{ctr}}^{:l:}\\in\\mathbb{R}^{T\\times D}$ For the embedding of neighboring intersections\u2019 partial trajectories $e_{\\mathrm{ntr}}\\;\\in\\;\\mathbb{R}^{L\\times(T\\cdot L^{\\prime})\\times D}$ where $L^{\\prime}$ is the number of neighboring intersections\u2019 entrance lanes taken into consideration, the $l$ slice is $e_{\\mathrm{ntr}}^{l:}\\in\\mathbb{R}^{(T\\cdot L^{\\prime})\\times D}$ . ", "page_idx": 13}, {"type": "text", "text": "The Communication Cross-Attention module, abbreviated as CCA, is designed to capture the spatial-temporal dependencies between the local intersection and neighboring intersections. As illustrated in Figure 1, $e_{\\mathrm{ntr}}^{l::}$ contains information of entrance lanes from neighboring intersections feeding into lane $l$ . This module can be formulated as, ", "page_idx": 13}, {"type": "equation", "text": "$$\nf_{\\mathrm{CCA}}(e_{\\mathrm{ctr}}^{;l;},e_{\\mathrm{ntr}}^{l;:}):=f_{\\mathrm{Att}}(e_{\\mathrm{ctr}}^{;l;},e_{\\mathrm{ntr}}^{l;:},e_{\\mathrm{ntr}}^{l;:})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\ne_{\\mathrm{ctr}}^{\\prime:l:}=f_{\\mathrm{CCA}}(e_{\\mathrm{ctr}}^{:l:},e_{\\mathrm{ntr}}^{l::})+e_{\\mathrm{ctr}}^{:l:}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The Spatial Self-Attention module, abbreviated as SSA, and Temporal Self-Attention module, abbreviated as TSA, are designed to capture the spatial dependencies and temporal dependencies separately in the local intersection, which can be formulated as, ", "page_idx": 13}, {"type": "equation", "text": "$$\nf_{\\mathrm{SSA}}(e_{\\mathrm{ctr}}^{\\prime h::}):=f_{\\mathrm{Att}}(e_{\\mathrm{ctr}}^{\\prime t:},e_{\\mathrm{ctr}}^{\\prime t:},e_{\\mathrm{ctr}}^{\\prime t::}),\\quad f_{\\mathrm{TSA}}(e_{\\mathrm{ctr}}^{\\prime:l:}):=f_{\\mathrm{Att}}(e_{\\mathrm{ctr}}^{\\prime:l:},e_{\\mathrm{ctr}}^{\\prime:l:},e_{\\mathrm{ctr}}^{\\prime:l:})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, the spatial-temporal encoder layer can be expressed as, ", "page_idx": 14}, {"type": "equation", "text": "$$\nf_{\\mathrm{STE}}(e_{\\mathrm{ctr}},e_{\\mathrm{ntr}}):=f_{\\mathrm{MLP}}(f_{\\mathrm{SSA}}(e_{\\mathrm{ctr}})+f_{\\mathrm{TSA}}(e_{\\mathrm{ctr}}))+e_{\\mathrm{ctr}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To simplify the expression, we omit the embedding of diffusion timestep, trajectory timestep and rewards in Equation 16, 17, 18 and 19. In the implementation, the embedding of diffusion timestep and trajectory timestep are added to every input of CCA, SSA and TSA, and the embedding of rewards is added to every input of SSA and TSA. ", "page_idx": 14}, {"type": "text", "text": "Output layer. We use an MLP layer as the output layer to convert the output of the spatial-temporal encoder layers into the noise we desire to predict. ", "page_idx": 14}, {"type": "text", "text": "C Datasets ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Detials of Datasets ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We apply five real-world traffic flow datasets in three cities of different sizes including Hangzhou and Jinan: (1) Hangzhou datasets: the road network contains 16 $(4\\!\\times\\!4)$ intersections with two traffic flow datasets, including Hangzhou1 and Hangzhou2. (2) Jinan datasets: the road network contains 12 $(3\\!\\times\\!4)$ intersections with three traffic flow datasets, including $\\operatorname{Jinan}_{1}$ , $\\operatorname{Jinan}_{2}$ and Hangzhou3. All these datasets are accessible in https://traffic-signal-control.github.io/. ", "page_idx": 14}, {"type": "text", "text": "We train AttendLight [10], Efficient-CoLight [48] and Advanced-CoLight [13] in isolation for each dataset from scratch until convergence. Then we collect all transitions in the replay buffer for each dataset during training. We present the converged performance of three methods in Table 4. ", "page_idx": 14}, {"type": "table", "img_path": "A969ouPqEs/tmp/a209651d55c237190b34b9af71b8d6ac0d8399c331e1f66f267a93dab9b7aa07.jpg", "table_caption": ["Table 4: Converged performance of methods used to collect offline datasets. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "C.2 Missing Pattern ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In TSC with missing data, missing patterns have a significant impact on the performance of control. In this paper, as shown in Figure 4, we conduct our experiments on random missing and kriging missing: (1) Random missing: traffic data collected by sensors for rewards and observations in every intersection is randomly masked with $10\\%{\\sim}50\\%$ probability. (2) Kriging missing: there is always no traffic data collected in certain intersections with $6.25\\%{\\sim}25.00\\%$ (1-intersection ${\\sim}4$ -intersection) probability in $\\mathrm{{Hangzhou}_{1}}$ and $\\mathrm{{Hangzhou_{2}}}$ , $8.33\\%{\\sim}25.00\\%$ (1- intersection ${\\sim}3$ -intersection) probability in $\\operatorname{Jinan}_{1}$ , $\\operatorname{Jinan}_{2}$ and Jinan3, and all neighboring intersections surround missing intersections are observable. ", "page_idx": 14}, {"type": "image", "img_path": "A969ouPqEs/tmp/e8994af8279f6eac9e72d792a7f3696c1abd2cba5920546b1e161c780b4ff8a0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 4: Illustration of random missing and kriging missing pattern. Each node represents an intersection in the road network and blocks with masks are traffic data with missing value. ", "page_idx": 14}, {"type": "text", "text": "D Baseline Methods ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we make a brief introduction to baseline approaches and the Store-and-forward method (SFM). ", "page_idx": 15}, {"type": "text", "text": "Baseline approaches. We adopt six baseline approaches in experiments as follows: ", "page_idx": 15}, {"type": "text", "text": "\u2022 BC is a type of imitation learning, where the agent learns to mimic the behavior of an expert demonstration. The agent is trained on a dataset of state-action pairs from the expert, and the goal is to learn a policy that can replicate the expert\u2019s actions given the same states.   \n\u2022 CQL is an RL algorithm that aims to learn a conservative Q-function, which provides a lower bound on the true Q-values. This helps to address the issue of overestimation of Q-values, which can lead to poor performance in practice.   \n\u2022 $\\mathbf{TD3+BC}$ combines the TD3 algorithm with behavioral cloning. By incorporating BC into TD3, the agent can leverage expert demonstrations to accelerate learning and improve sample efficiency. $\\mathrm{TD3+BC}$ offers the benefits of both TD3\u2019s stability and BC\u2019s ability to learn from expert demonstrations.   \n\u2022 DT is a sequence-to-sequence model that casts reinforcement learning as a sequence modeling problem. It takes as input a sequence of past states, actions, and rewards, and it outputs a sequence of future actions that maximize the expected cumulative reward. We build DT based on the code https://github.com/kzl/decision-transformer/.   \n\u2022 Diffuser is a diffusion-based approach for decision-making. Diffuser focuses on generating sequences of actions that lead to desirable outcomes by iteratively refining these sequences. We build Diffuser based on the code https://github.com/jannerm/diffuser.   \n\u2022 DD is a diffusion-based approach for decision-making. DD diffuses over state trajectories and planning with an inverse dynamics model. We build DD based on the code https://github.com/ anuragajay/decision-diffuser. ", "page_idx": 15}, {"type": "text", "text": "To avoid non-convergence caused by missing data, we train baselines on datasets without missing data and test them under data-missing scenarios with observations and rewards imputed by SFM. ", "page_idx": 15}, {"type": "text", "text": "Store-and-forward method. Since baselines cannot handle the data-missing scenarios, we adopt a rulebased SFM to impute observations and rewards for baselines. It is proved that SFM has more stable performance compared to learning neural networks [14]. In this paper, we model current observation as: $f(\\mathcal{V}_{t-1},k)=$ Concat $\\left(\\cup_{l_{i}}f^{\\prime}(l_{i},k)\\right)$ and $\\begin{array}{r}{f^{\\prime}(l_{i},k)=\\frac{1}{k}\\sum_{l_{j}}o_{t-1}^{l_{j}}}\\end{array}$ , where $\\mathcal{V}_{t-1}^{k}$ is the intersection at time $t$ , $l_{i}\\in\\mathcal{V}_{t-1}^{k}$ is a lane and $l_{j}$ is the $k$ \u2019s neighboring lane connected by traffic movements. We set $k$ as 12 in this paper. ", "page_idx": 15}, {"type": "text", "text": "E Proof of Partial Rewards Conditioned Diffusion ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To prove that the aim of partial rewards conditioned diffusion is the same as the goal in Equation 3, we assume that the observable part of the trajectory and the missing part of the trajectory are collected by real sensors and virtual sensors separately, and the distribution of traffic data collected by two kinds of sensors are independent. Thus, the distribution in Equation 3 can be factorized as follows, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(x^{0}(\\tau)|\\mathrm{y}(\\tau))=p(x^{0}(\\tau_{\\mathrm{obs}}),x^{0}(\\tau_{\\mathrm{mis}})|y(\\tau))}\\\\ &{\\qquad\\qquad\\qquad=p(x^{0}(\\tau_{\\mathrm{obs}})|y(\\tau))\\cdot p(x^{0}(\\tau_{\\mathrm{mis}})|y(\\tau))}\\\\ &{\\qquad\\qquad\\qquad=p(x^{0}(\\tau_{\\mathrm{obs}})|r(\\tau),y^{\\prime}(\\tau))\\cdot p(x^{0}(\\tau_{\\mathrm{mis}})|r(\\tau),y^{\\prime}(\\tau))}\\\\ &{\\qquad\\qquad\\qquad=p(x^{0}(\\tau_{\\mathrm{obs}})|r(\\tau),y^{\\prime}(\\tau))\\cdot p(x^{0}(\\tau_{\\mathrm{mis}})|y^{\\prime}(\\tau))}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The distribution without rewards condition $p(x^{0}(\\tau)|y^{\\prime}(\\tau))$ can be regarded as the marginal distribution of that with rewards condition $p(x^{0}(\\tau)|y(\\tau))=\\bar{p}(\\dot{x}^{0}(\\dot{\\tau})|\\ddot{r}(\\tau),\\dot{y}^{\\prime}(\\tau))$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\np(x^{0}(\\tau)|y^{\\prime}(\\tau))=\\int p(r(\\tau))p(x^{0}(\\tau)|r(\\tau),y^{\\prime}(\\tau))d r(\\tau)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In this case, we can adopt the same diffusion model with classifier-free guidance to model $p_{\\theta}(x^{0}(\\tau_{\\mathrm{obs}})|r(\\tau),y^{\\prime}(\\tau))$ and $\\dot{p}_{\\theta}(x^{0}(\\tau_{\\mathrm{mis}})|y^{\\prime}(\\tau))$ . ", "page_idx": 15}, {"type": "text", "text": "F Additional Experiments Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "F.1 Performance without Missing Data ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We train and test our method on all five datasets and compare our method with all baselines under no data-missing scenarios. DiffLight performs the best on over half of the datasets. Meanwhile, DD demonstrates a better performance than Diffuser, which shows that diffusing only on observations is a better choice in TSC. ", "page_idx": 16}, {"type": "table", "img_path": "A969ouPqEs/tmp/33e2c8b69556fdfe768c92b0993aec6d17e8e583acb94968eceeeb9f8c03fd60.jpg", "table_caption": ["Table 5: Overall performance in scenarios without missing data. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "F.2 Influence of Unobserved Locations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In previous experiments, the unobserved intersections in kriging missing are not adjacent. In this section, we study the influence of unobserved locations. We provide another mask of $\\mathcal{D}_{\\mathrm{HZ}}^{1}$ with a missing rate of $25\\%$ in kriging missing, which contains a missing intersection where all neighboring intersections are missing. The performance of this experiment is shown in Table 6. DiffLight still demonstrates the best performance. ", "page_idx": 16}, {"type": "table", "img_path": "A969ouPqEs/tmp/59e922b0523a4f8eac97a4342a2906040c26cd20c97172ccb6c6e82d6dc730f2.jpg", "table_caption": ["Table 6: Performance in different unobserved locations. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "F.3 Limit of Missing Rates ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To further explore limits on missing proportions, we conduct experiments on the selected datasets in random missing with missing rates of $70\\%$ and $90\\%$ . In the experiment, DiffLight remains an acceptable performance at the missing rate of $70\\%$ . When the missing rate rises to $90\\%$ , the performance of DiffLight drops rapidly, which shows that the limit for the missing rate is around $70\\%$ . ", "page_idx": 16}, {"type": "table", "img_path": "A969ouPqEs/tmp/bdad3e8928d2f41aec98f02829215f4a203306d2fbf867c9a049eea1045b3575.jpg", "table_caption": ["Table 7: Limit of missing rates in random missing. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "F.4 Scalability of DiffLight ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To further evaluate the efficacy and validate the performance of our approach, we conduct experiments on the New York dataset, which includes 48 intersections. In the experiment on the New York dataset, DiffLight achieves the best performance in most scenarios, demonstrating its ability to deal with complex traffic scenarios and control traffic signals in a larger-scale traffic network. In contrast, the performance of most baselines drops rapidly, due to the cumulative effect of errors in state imputation and decision-making at more intersections. ", "page_idx": 16}, {"type": "table", "img_path": "A969ouPqEs/tmp/9c3051eea3751fafd21095e8272cf1e1bd13bab72595fa5f83c46b10bf53f25b.jpg", "table_caption": ["Table 8: Scalability of DiffLight in random missing. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "A969ouPqEs/tmp/dac8b20f88b087063194aa4504c73532d8a18e43ecef859c6eba3d4ce15fb919.jpg", "table_caption": ["Table 9: Scalability of DiffLight in kriging missing. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "F.5 Additional Ablation Study on Diffusion Communication Mechanism ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We further evaluate the effectiveness of DCM with DiffLight w/ DCM and DiffLight w/o DCM. Table 10 shows the comparison of these variants on $\\mathrm{{Hangzhou_{1}}}$ and Jinan1. It should be noted that we adopt another mask of $\\mathcal{D}_{\\mathrm{HZ}}^{1}$ used in Appendix F.2, which contains a missing intersection where all neighboring intersections are missing. Based on the results, we can find that DiffLight w/ DCM shows better performance in kriging missing and the performance of DiffLight w/ DCM is close to the performance of DiffLight w/o DCM in random missing. It is proven that DCM sharing generated observations among intersections can promote the performance of TSC with missing data effectively. ", "page_idx": 17}, {"type": "table", "img_path": "A969ouPqEs/tmp/3f82e3b44a76066a188a0ac5cf26877706bb715b8b143b492198649b1f8078be.jpg", "table_caption": ["Table 10: Ablation study on Diffusion Communication Mechanism. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "F.6 Additional Ablation Study on the Inverse Dynamics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We further evaluate the effectiveness of the inverse dynamics (ID) with DiffLight w/ ID and DiffLight w/o ID. For DiffLight w/o inverse dynamics, we remove the inverse dynamics and extend the dimension of the noise model to generate both observations and actions. Table 11 shows the comparison of these variants on Hangzhou1 and $\\operatorname{Jinan}_{1}$ . Based on the results, we can find that DiffLight w/ inverse dynamics shows better performance in both random missing and kriging missing. ", "page_idx": 17}, {"type": "table", "img_path": "A969ouPqEs/tmp/c1d923c30d6963310a4716fdeb34e00b9ab567d0f3c09ad01866e1179b4ed34a.jpg", "table_caption": ["Table 11: Ablation study on the inverse dynamics. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "F.7 Time Cost ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To effectively demonstrate the usability of DiffLight, we conduct experiments to study the relationship between inference speed and performance in Table 12 and 13. We adopt models trained with 100 steps and test them on ", "page_idx": 17}, {"type": "text", "text": "different sampling steps. We can see that with the decrease in sampling steps, the performance of DiffLight remains stable. It is proven that DiffLight is able to handle the TSC task in an acceptable time with good performance. ", "page_idx": 18}, {"type": "table", "img_path": "A969ouPqEs/tmp/7f7fd03b538d178228e60bbf282e96d2b09e7bdbe84b25ccfa792d11f10acbd6.jpg", "table_caption": ["Table 12: Performance of DiffLight on different sampling steps. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "A969ouPqEs/tmp/ef245b8f40cc1dce2b782a001dfe1bcfb4826bd4811e6d3805d147144528c85f.jpg", "table_caption": ["Table 13: Inference time cost of DiffLight on different sampling steps. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "G Discussion on MissLight ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To better clarify the core differences between DiffLight and MissLight [14], we compare them from the following two aspects. ", "page_idx": 18}, {"type": "text", "text": "Model training. MissLight is an online method with a state imputation model and a reward imputation model, which means that interaction with the environment is necessary. In the online setting, if the method is trained in the physical environment, safety problems must be taken into consideration. If the method is trained in a simulated environment, the difference between the physical environment and the simulated environment could affect the performance of the method to some extent when the online method is going to be employed in the physical environment. In contrast, our approach, DiffLight, is an offline method based on the diffusion model. In the offline setting, our method is trained using the collected dataset without interaction with the environment, which avoids the problems mentioned above. ", "page_idx": 18}, {"type": "text", "text": "Model composition. MissLight is a two-stage method. In the first stage, state imputation and reward imputation models are used to fill in the missing data. In the second stage, the DQN algorithm is employed to complete the training process based on the imputed data. This approach suffers from the problem of error accumulation during the training process. However, our proposed DiffLight model, which incorporates both a diffusion model and an inverse dynamics model, can simultaneously train on missing data and collaboratively achieve traffic signal control with missing data. ", "page_idx": 18}, {"type": "text", "text": "To better compare with MissLight, we implement the SDQN-SDQN (model-based) in [14] and replace the DQN algorithm with the CQL algorithm to adapt the offilne setting. We replaced the DQN algorithm with different algorithms in the offilne setting and imputed the states with the SFM model. Note that all the baselines in Section 4.2 were implemented with reference to the SDQN-SDQN (transferred) method in MissLight. To distinguish the baseline of CQL implemented in Section 4.2, the new baseline is named CQL (model-based). We provide the performance of CQL (model-based) in Table 14 and Table 15. ", "page_idx": 18}, {"type": "table", "img_path": "A969ouPqEs/tmp/7877b6622bc2a5c7b7544bf80fc30699bdb8487080c0a5c9d1ba00f38611140d.jpg", "table_caption": ["Table 14: Performance of CQL (model-based) in random missing. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "A969ouPqEs/tmp/e02ca29c279eefb3195e23c2e42d0434be286ed56970a46c855ebe69a4733e57.jpg", "table_caption": ["Table 15: Performance of CQL (model-based) in kriging missing. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "DiffLight achieves competitive performance compared with CQL (transferred) and CQL (model-based). The possible reason why DiffLight has better performance is that CQL (model-based) suffers from error accumulation caused by the reward imputation model while DiffLight can directly make decisions with Partial Rewards Conditioned Diffusion (PRCD). ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See Abstract and Section 1. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See Appendix 6. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See Appendix E. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: See Section 4.1, Appendix B and Appendix C. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We have submitted our data and code on: https://github.com/lokol5579/ DiffLight-release ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: See Section 4.1 and Appendix B. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: See Section 4 and Appendix F. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: See Appendix B.1 and Appendix F.7. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We ensured that the study complied with the NeurIPS Code of Ethics in all respects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: See Appendix A. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not suffer from this risk. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not release new assets. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]