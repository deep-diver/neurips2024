[{"type": "text", "text": "Enhancing Zero-Shot Vision Models by Label-Free Prompt Distribution Learning and Bias Correcting ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xingyu Zhu1\u22c6 Beier Zhu2\u22c6 Yi Tan1 Shuo Wang1\u2020 Yanbin Hao1 Hanwang Zhang2 ", "page_idx": 0}, {"type": "text", "text": "1University of Science and Technology of China 2Nanyang Technological University xingyuzhu@mail.ustc.edu.cn, shuowang.edu@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vision-language models, such as CLIP, have shown impressive generalization capacities when using appropriate text descriptions. While optimizing prompts on downstream labeled data has proven effective in improving performance, these methods entail labor costs for annotations and are limited by their quality. Additionally, since CLIP is pre-trained on highly imbalanced Web-scale data, it suffers from inherent label bias that leads to suboptimal performance. To tackle the above challenges, we propose a label-Free prompt distribution learning and bias correction framework, dubbed as Frolic, which boosts zero-shot performance without the need for labeled data. Specifically, our Frolic learns distributions over prompt prototypes to capture diverse visual representations and adaptively fuses these with the original CLIP through confidence matching. This fused model is further enhanced by correcting label bias via a label-free logit adjustment. Notably, our method is not only training-free but also circumvents the necessity for hyper-parameter tuning. Extensive experimental results across 16 datasets demonstrate the efficacy of our approach, particularly outperforming the state-of-the-art by an average of $2.6\\%$ on 10 datasets with CLIP ViT-B/16 and achieving an average margin of $1.5\\%$ on ImageNet and its five distribution shifts with CLIP ViT-B/16. Codes are available in https://github.com/zhuhsingyuu/Frolic. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vision-language models (VLMs), such as CLIP [29], which are pre-trained on large-scale datasets using contrastive loss, effectively align visual and textual representations within a shared feature space. This capability enables the zero-shot inference on downstream tasks through prompting and achieves remarkable performance. For example, using a selection of 80 hand-crafted prompts, a zero-shot CLIP ViT-B/16 achieves an accuracy of $68.7\\%$ , and with prompts generated by language models [27], the accuracy increases to $69.9\\%$ . ", "page_idx": 0}, {"type": "text", "text": "The success of zero-shot capabilities heavily relies on the appropriate text descriptions of the classes, which has gained research interest in improving prompts. Recent studies propose learning prompts from a small set of labeled images in the downstream data [46, 45, 47]. Among these studies, Lu et al. [18] and Wang et al. [38] have found that learning the distribution of diverse prompts, which better captures the variance in visual representations, leads to improved performance. Although these methods have achieved significant improvements, they still depend on artificial prior knowledge for labeling downstream data and are limited by the quality of manual annotations, which may restrict the scalability of the original model. ", "page_idx": 0}, {"type": "image", "img_path": "OJximyClit/tmp/50da81f2899d217beb40c46dac5cadde292d87b4c6e0e3bb53348ec06c18d9d2.jpg", "img_caption": ["Figure 1: Illustration of prompt distribution learning and label bias correction on ImageNet using CLIP ViT-B/16. (a) Existing zero-shot models [1, 27]. (b) Our prompt distribution learning (c) Average probability prediction of original CLIP. (d) Average probability prediction of our Frolic. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Another significant approach to enhancing zero-shot performance involves correcting the label bias inherent in skewed web-scale pre-training data [1, 25, 49]. This bias leads to highly imbalanced predictions and suboptimal performance. As illustrated in Figure 1(c), the average predicted probability on ImageNet using ViT-B/16 reveals an imbalanced distribution: the highest class probability exceeds 0.002, whereas the lowest is below 0.0005. Existing methods correct this bias by allowing access to a portion of the pre-training data [1, 25], or by using labeled downstream data [49]. However, the pre-training data is often inaccessible due to privacy or copyright concerns, and debiasing without labeled data is challenging. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce a label-Free prompt distribution learning and bias correction framework, dubbed as Frolic, which eliminates the need for data annotations to enhance zero-shot performance. First, unlike previous methods [1, 27, 46, 39, 43], which use a single class prototype for each class to define the decision boundary (as shown in Figure 1(a)), our approach employs Gaussian distributions to model the varied visual representations of text prototypes, as illustrated in Figure 1(b). It is worth noting that estimating such a distribution is non-trivial, since classical maximum likelihood estimation requires the annotation of each sample. Fortunately, we demonstrate that it is possible to infer distribution for each class directly from the first and second moments of the marginal distribution of downstream data without label information. Second, to prevent the use of pre-training data or labeled samples in downstream tasks, we develop a bias estimation mechanism, which transitions the sampling process from the pre-training data distribution to a class-conditional sampling from downstream distribution. By incorporating the estimated label bias into zero-shot models, we can achieve a balanced prediction, as illustrated in Figure 1 (d). Furthermore, we explore the possibility of combining the original CLIP predictions with those from the Gaussian-based models to enhance zeroshot performance. To this end, we have developed a confidence-matching technique that dynamically balances the contributions of the two models, eliminating the need for hyperparameter tuning. Notably, our framework is training-free, which enhances both flexibility and ease of implementation. ", "page_idx": 1}, {"type": "text", "text": "The main contributions of this work are: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We enhance zero-shot performance by estimating a distribution over prompt prototypes to capture the variance in visual appearances. We demonstrate that this process can be implemented entirely without labels.   \n\u2022 We propose a confidence matching technique that fuses the original CLIP model with a Gaussian distribution-based model to further enhance zero-shot performance. This process eliminates the need for hyper-parameter searching, in stark contrast to previous studies.   \n\u2022 We develop an unsupervised method to correct pre-training label bias. Unlike existing methods that require access to pre-training data, our Proposition 2 suggests that we can avoid sampling from the pre-training distribution for estimating and correcting this bias. Instead, our method utilizes only downstream images.   \n\u2022 We demonstrate the effectiveness of our proposed method Frolic by conducting experiments across 16 datasets, which has a consistent and significant improvement over existing baselines. For example, our method surpasses the state-of-the-art zero-shot models by a margin of $2.6\\%$ on average with CLIP ViT-B/16. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Zero-shot vision models. Vision models pre-trained with auxiliary language supervision, such as CLIP [29] and OpenCLIP [6], facilitate zero-shot inference through prompting. Enhancing zero-shot performance has gained increasing research interest: (1) One approach involves prompt engineering, which includes designing hand-crafted prompts based on human priors [29] or automatically generating prompts via language models [35]. (2) Another promising approach seeks to improve classifiers, e.g., ZPE [1] scores the importance of candidate prompts for prompt ensembling. InMaP [28] reduces the modality gap between vision and text. Several studies [32, 31] optimize prompt at test time by encouraging consistent predictions across augmented samples. Our work aims to enhance zero-shot models by learning the prompt distribution and mitigating the pre-training label bias. ", "page_idx": 2}, {"type": "text", "text": "Prompt distribution learning. Automatically learning prompts from downstream data has shown potential in improving zero-shot models [46, 45, 47]. These methods typically optimize prompts via minimizing the classification loss on the target task. However, as pointed out in Lu et al. [18], learning prototype prompts overlook the diversity of visual representations. To this end, they estimate a distribution over the prompts to capture the variance of visual representations. Recently, Wang et al. [38] propose training-free prompt distribution learning to improve efficiency. Contrary to existing methods [18] that estimate distributions through supervised approaches, our method circumvents the necessity for labels by inferring the variance of distributions from the statistics of unlabeled data. ", "page_idx": 2}, {"type": "text", "text": "Correcting label bias. Label bias generally occurs in the presence of skewed or imbalanced training data. In response to this challenge, Logit Adjustment (LA) [34, 14, 21, 49] has emerged as a prominent technique in long-tailed learning, specifically designed to adjust the decision boundary of classifiers to mitigate label bias. Menon et al. [21] derives the theoretically optimal adjustment for logits. Zhu et al. [49] extents LA to fine-tune zero-shot models by removing the pre-trained label bias. Unlike approaches that rely on the label distribution of the training set [34, 14, 21, 48] or the labels of fine-tuning data [49], our method adjusts the logits using unlabeled test data. ", "page_idx": 2}, {"type": "text", "text": "3 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we present our prompt distribution learning, adaptive fusion, and logit adjustment techniques for adapting zero-shot models. Without loss of generality, we adopt CLIP [29] as our zero-shot model. To begin with, we emphasize three advantages of our framework: ", "page_idx": 2}, {"type": "text", "text": "Training-free: Our Frolic is training-free without optimizing the backbone of the zero-shot models, enhancing both flexibility and ease of implementation. ", "page_idx": 2}, {"type": "text", "text": "Label-free: Our method Frolic requires no external labeled data, making it suitable for zero-shot scenarios. ", "page_idx": 2}, {"type": "text", "text": "No hyper-parameters searching: Our method Frolic eliminates hyper-parameter tuning on validation datasets, in stark contrast to [38, 44] ", "page_idx": 2}, {"type": "text", "text": "3.1 Setup", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The zero-shot model consists of a visual encoder $\\Phi_{v}(\\cdot)$ and a text encoder $\\Phi_{\\mathrm{t}}(\\cdot)$ . Given a set of unlabeled image data $\\{x_{i}\\}_{i=1}^{N}$ and the unique text set of the class description $\\{z_{j}\\}_{j=1}^{K}$ , their visual and text representation can be computed as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{x}_{i}=\\Phi_{\\mathsf{v}}(x_{i});\\quad\\mathbf{z}_{j}=\\Phi_{\\mathsf{t}}(z_{j}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{x}_{i}$ and $\\mathbf{z}_{j}$ share the same dimension $(\\mathbf{x},\\mathbf{z}\\in\\mathbb{R}^{d})$ . $N$ is the sample size and $K$ is the class size. $\\mathbf{z}_{j}$ can be considered as the prototype for class $j$ . With an image $\\mathbf{x}$ and all prototypes $\\{\\mathbf{z}_{j}\\}_{j=1}^{K}$ zero-shot CLIP predicts the label as: ", "page_idx": 2}, {"type": "equation", "text": "$$\ny=\\underset{j}{\\mathrm{argmax}}\\,f_{\\mathsf{c}}(\\mathbf{x})_{j}=\\underset{j}{\\mathrm{argmax}}\\,\\mathbf{z}_{j}^{\\top}\\mathbf{x},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $f_{\\mathsf{c}}(\\mathbf{x})_{j}=\\mathbf{z}_{j}^{\\top}\\mathbf{x}$ is the score for class $j$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Label-Free Prompt Distribution Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In order to express the diverse visual variations, our approach aims to learn the distribution of the class prototypes. Previous studies [18, 38] show that the Gaussian distribution is effective to model the distribution of the CLIP features and achieves impressive improvement. However, these methods require extra labeled training data, which is not applicable to our zero-shot setting. ", "page_idx": 3}, {"type": "text", "text": "Specifically, we follow [38] to assume $\\mathcal{N}(\\mathbf{z}_{1:K},\\Sigma)$ with identical covariance is the underlying distribution. In classical maximum likelihood estimation [3], the shared covariance $\\Sigma$ is computed by averaging the empirical covariances of K classes: \u03a3\u02c6 = K1 j \u03a3\u02c6j, where \u03a3\u02c6j =|Cj1|\u22121 x\u2208Cj(x \u2212 $\\mathbf z_{j})(\\mathbf x-\\mathbf z_{j})^{\\top}$ . Here, one need the label information of each image to compute $\\hat{\\Sigma}_{j}$ . Fortunately, to avoid using label information, we can infer $\\Sigma$ directly from the expectation and the second order moment of the marginal distribution $\\mathbb{P}(\\mathbf{x})$ .1 Using a Gaussian mixture model with the priors $\\{\\pi_{j}\\}_{j=1}^{K}$ , $\\mathbb{P}(\\mathbf{x})$ is given by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathbf{x})=\\sum_{j=1}^{K}\\pi_{j}\\mathcal{N}(\\mathbf{x};\\mathbf{z}_{j},\\Sigma),\\quad\\mathcal{N}(\\mathbf{x};\\mathbf{z}_{j},\\Sigma)=\\frac{1}{\\sqrt{(2\\pi)^{d}|\\Sigma|}}\\exp\\{-\\frac{1}{2}(\\mathbf{x}-\\mathbf{z}_{j})^{\\top}\\Sigma^{-1}(\\mathbf{x}-\\mathbf{z}_{j})\\}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Denote the second moment of $\\mathbf{x}$ as $M$ , we have (proof in Section A.1): ", "page_idx": 3}, {"type": "equation", "text": "$$\nM=\\Sigma+\\sum_{j}\\pi_{j}\\mathbf{z}_{j}\\mathbf{z}_{j}^{\\top}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Denote $\\pmb{\\pi}\\,=\\,[\\pi_{1},..,\\pi_{K}]^{\\intercal}$ , $Z\\,=\\,[{\\bf z}_{1},..,{\\bf z}_{K}]^{\\top}$ , and the expectation of $\\mathbf{x}$ as $\\pmb{\\mu}$ , the prior over the unlabeled data distribution can be estimated by (proof in Section A.2): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{\\pi}=Z^{-1}\\pmb{\\mu}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We estimate the expectation and the second order moment as $\\begin{array}{r c l}{\\hat{\\pmb{\\mu}}}&{=}&{\\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{x}_{i}}\\end{array}$ and $\\hat{\\boldsymbol{M}}\\,\\,=$ $\\begin{array}{r}{\\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}}\\end{array}$ , which are unbiased and consistent. In practice, given that test benchmarks are generally class-balanced, we use a uniform prior over the data distribution, i.e., $\\textstyle\\pi_{j}={\\frac{1}{K}}$ . Combining with Eq. (4), the estimated shared covariance $\\hat{\\Sigma}$ can be written as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\Sigma}=\\hat{M}-\\frac{1}{K}\\sum_{j}\\mathbf{z}_{j}\\mathbf{z}_{j}^{\\top}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Let $\\mathbf{w}_{j}=\\hat{\\Sigma}^{-1}\\mathbf{z}_{j}$ and $\\begin{array}{r}{b_{j}=-\\frac{1}{2}\\mathbf{z}_{j}^{\\top}\\mathbf{w}_{j}}\\end{array}$ , the Gaussian discriminant analysis predicts the label for an image $\\mathbf{x}$ as follows (proof in Section A.3): ", "page_idx": 3}, {"type": "equation", "text": "$$\ny=\\mathop{\\operatorname{argmax}}_{j}f_{\\mathbf{g}}(\\mathbf{x})_{j}=\\mathop{\\operatorname{argmax}}_{j}\\mathbf{w}_{j}^{\\top}\\mathbf{x}+b_{j}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $f_{\\mathbf{g}}(\\mathbf{x})_{j}=\\mathbf{w}_{j}^{\\top}\\mathbf{x}+b_{j}$ is the score for class $j$ . ", "page_idx": 3}, {"type": "text", "text": "3.3 Prediction Fusion via Adaptive Calibration. ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As a rule of thumb, combining the zero-shot predictions with the ones from the learned model can further improve performance for CLIP adaptations [44, 35, 40, 47, 38, 50]. Previous studies commonly employ a mixing coefficient, $\\alpha$ , to balance the contributions of two models, e.g., $f(\\mathbf{x})=$ $f_{\\mathsf{c}}(\\mathbf{x})+\\alpha f_{\\mathsf{g}}(\\mathbf{x})$ . Typically, this hyper-parameter $\\alpha$ is optimized on labeled data to maximize accuracy. However, in our context, labels are unavailable, it is not possible to search for the optimal value of $\\alpha$ . It is imperative to develop a mechanism that balances the prediction fusion without relying on the label. ", "page_idx": 3}, {"type": "image", "img_path": "OJximyClit/tmp/821bf5cce8251f3578822fc7ad0e0eca72ae84f626d6c4bdc02fedac92ca5aa3.jpg", "img_caption": ["Figure 2: Comparison of confidence. "], "img_footnote": [], "page_idx": 3}, {"type": "table", "img_path": "OJximyClit/tmp/a6a1a9cabd9855aae7fcd062d1ef73a5b1f8ff5eb7b31c11af9c9dfe4069d374.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "The key in our prediction fusion lies in aligning the average confidence of the two models. Formally, the average confidence over the dataset $\\{\\bar{\\mathbf{x}_{i}}\\}_{i=1}^{N}$ scaled by a temperature $\\tau$ is given by the average of the model\u2019s probability for its prediction: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname{conf}(f,\\tau)=\\frac{1}{N}\\sum_{i=1}^{N}\\operatorname*{max}_{j}\\operatorname{softmax}_{{j}}(f(\\mathbf{x}_{i})/\\tau)_{j}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Ideally, a model\u2019s average confidence should reflect the predicted accuracy, which is called a wellcalibrated model. Suppose we have the oracle well-calibrated models, denoted by $f_{\\mathrm{c}}^{\\prime}(\\cdot)$ and $f_{\\mathrm{g}}^{\\prime}(\\cdot)$ , Kumart et al. [17] prove that the optimal strategy is to fuse the two predictions equally, i.e., $f_{\\mathsf{f}}(\\mathbf{x})=$ $f_{\\mathrm{c}}^{\\prime}(\\mathbf{x})\\!+\\!f_{\\mathrm{g}}^{\\prime}(\\mathbf{x})$ . However, as shown in Figure 2, $f_{\\mathrm{g}}$ is much overconfident than $f_{\\mathsf{c}}$ . Let $f_{\\mathrm{g}}(\\mathbf{x})=C f_{\\mathrm{g}}^{\\prime}(\\mathbf{x})$ for large $\\overline{{C}}\\in\\mathbb{R}^{+}$ (an overconfident model magnifies its logits) and suppose $f_{\\mathsf{c}}(\\mathbf{x})\\approx f_{\\mathsf{c}}^{\\prime}(\\mathbf{x})$ . The fused predictions are given by $f_{\\mathbf{f}}(\\mathbf{x})=C f_{\\mathrm{g}}^{\\prime}(\\mathbf{x})\\overset{\\cdot}{+}f_{\\mathrm{c}}^{\\prime}(\\mathbf{x})$ . For very large $C$ , $f_{\\mathsf{f}}(\\mathbf{x})$ and $f_{\\mathrm{g}}(\\mathbf{x})$ have the same predictions, i.e., $f_{\\mathsf{f}}(\\mathbf{x})$ is biased towards the $f_{\\mathrm{g}}(\\mathbf{x})$ . As we do not have the label to compute accuracy, we cannot apply classical calibration methods [19, 10] to calibrate $f_{\\mathrm{g}}(\\mathbf{x})$ and $f_{\\mathsf{c}}(\\mathbf{x})$ . As our desideratum is to automatically balance the contribution of the two models, we can optimize $\\tau_{\\mathrm{g}}$ to make the confidence of $f_{\\mathrm{g}}$ to match up the one of $f_{\\mathsf{c}}$ , which circumvent the need of labels: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tau_{\\mathbf{g}}=\\underset{\\tau_{\\mathbf{g}}}{\\mathrm{argmin}}\\left|\\mathrm{conf}(f_{\\mathbf{g}},\\tau_{\\mathbf{g}})-\\mathrm{conf}(f_{\\mathbf{c}},\\tau_{\\mathbf{c}})\\right|\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Specifically, we implement this by binary search, as the confidence monotonically decreases as the temperature increases. $\\tau_{\\mathsf{c}}=0.01$ is fixed and learned by CLIP. The fused logits are given by: ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{\\mathrm{f}}(\\mathbf{x})=f_{\\mathrm{g}}(\\mathbf{x})/\\tau_{\\mathrm{g}}+f_{\\mathrm{c}}(\\mathbf{x})/\\tau_{\\mathrm{c}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.4 Correcting Pre-training Label Bias via Label-Free Logit Adjustment ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Pre-training datasets typically exhibit a long-tailed concept distribution, leading to biased performance in zero-shot models [49, 25, 5, 1]. This bias occurs because zero-shot models reflect the posterior probability $\\mathbb{P}(y|\\mathbf{x})$ derived from the pre-training distribution. According to Bayes\u2019 rule, this posterior probability is influenced by the pre-training label distribution $\\mathbb{P}(y)$ , as $\\mathbb{P}(y|\\mathbf{x})\\propto\\mathbb{P}(\\mathbf{x}|y)\\mathbb{P}(y).$ . If the prior probability of class $j$ is significantly larger than that of other classes (e.g., $\\mathbb{P}(j)\\gg\\mathbb{P}(i)$ , $\\forall i\\in$ $[K],i\\neq j)$ , the predictions will be biased toward class $j$ . ", "page_idx": 4}, {"type": "text", "text": "Prior research [21, 14] has identified a theoretical optimal solution to address this label bias: let $\\beta_{y}$ denote the prior probability of class $y$ , i.e., $\\beta_{y}=\\mathbb{P}(\\bar{y})$ . The debiased logit of $f_{\\mathsf{f}}(\\mathbf{x})$ for class $y$ should be (proof in Section A.4): ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{\\mathsf{d}}(\\mathbf{x})_{y}=f_{\\mathsf{f}}(\\mathbf{x})_{y}-\\ln\\beta_{y}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Previous methods estimate $\\beta$ either by accessing the pre-training data [25, 1] or counteract the influence of the prior by optimizing on labeled downstream data [49]. However, these approaches are often impractical due to inaccessible pre-training labels due to privacy or copyright concerns or the necessity for labeled downstream data. In this work, we address label bias using only the unlabeled downstream data $\\{\\mathbf{x}_{i}\\}_{i=1}^{N}$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Let $s(\\mathbf{x})=\\operatorname{softmax}(f_{\\mathsf{f}}(\\mathbf{x}))$ represent the softmax outputs of $f_{\\mathbf{f}}(\\mathbf{x})$ , where $s(\\mathbf{x})_{y}=\\hat{\\mathbb{P}}(y|\\mathbf{x})$ is the predicted probability for class $y$ . Define $\\mathbf{s}_{j}=\\mathbb{E}_{\\mathbf{x}}[s(\\mathbf{x})|Y=j]$ as the expected posterior probability over the image distribution of class $j$ , and let $S\\;=\\;[{\\bf s}_{1},...,{\\bf s}_{K}]\\;\\in\\;\\mathbb{R}^{K\\times K}$ . We prove that the pre-training label prior $\\beta=[\\beta_{1},...,\\beta_{K}]^{\\intercal}\\in\\mathbb{R}^{K}$ must satisfy the following linear equation system: ", "page_idx": 5}, {"type": "equation", "text": "$$\n(S-I)\\beta={\\bf0}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Remark. The key point in Eq. (12) is that we avoid sampling from the pre-training data distribution; instead, we sample from $\\mathbb{P}(\\mathbf{x}|y)$ , which is available from the downstream data. We provide the proof in Section A.5 and the numerical power solver for $\\beta$ in Section A.6. ", "page_idx": 5}, {"type": "text", "text": "We iteratively refine the estimation of $S$ and solve for $\\beta$ using updated pseudo-labels generated by $f_{\\mathsf{d}}(\\mathbf{x})$ . As $f_{\\mathsf{d}}(\\mathbf{x})$ becomes more precise, it yields more accurate pseudo-labels for $\\mathbf{x}$ , which in turn enhances the accuracy of our estimation of $\\beta$ . Specifically, we initialize ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\beta^{0}=[1/K,...,1/K]^{\\top},\\ f_{\\mathsf{d}}^{0}=f_{\\mathsf{f}},\\ s_{j}^{0}=\\frac{1}{|\\mathcal{C}_{j}^{0}|}\\sum_{\\mathbf{x}\\in\\mathcal{C}_{j}^{0}}s(\\mathbf{x}),\\ \\mathrm{and}\\ S^{0}=[\\mathbf{s}_{1}^{0},...,\\mathbf{s}_{K}^{0}]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{x}\\in\\mathcal{C}_{j}^{0}$ if $\\mathbf{x}$ is classified as $j$ by $f_{\\mathsf{d}}^{0}(\\mathbf{x})$ . We proceed by solving for $\\beta^{1}$ using Eq. (12), refining $f_{\\mathsf{d}}^{1}(\\mathbf{x})$ using $\\mathrm{Eq}\\ 11$ and reassign the pseudo label using $f_{\\mathsf{d}}^{1}(\\mathbf{x})$ to estimate the updated $\\mathbf{s}_{j}^{1}$ . This process is repeated $t$ times until the relative change in $\\beta$ satisfies the convergence criterion: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\|\\beta^{t}-\\beta^{t-1}\\|_{1}}{\\|\\beta^{t-1}\\|_{1}}=\\|\\beta^{t}-\\beta^{t-1}\\|_{1}<\\epsilon,\\quad\\|\\beta^{t-1}\\|=1\\mathrm{~by~definition}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\epsilon$ is a predefined threshold for relative error tolerance. We summarize the algorithm for solving $\\beta$ in Algorithm 2 and provide the overall pipeline in Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "Discussion: Comparison with Other Prior Estimation Methods. We compare existing methods for estimating pre-training label priors and demonstrate their in-applicability or flaws in our setting. ", "page_idx": 5}, {"type": "text", "text": "(1) Explicit method: the explicit method directly measures the frequency of each class in pre-training data, e.g., $\\begin{array}{r}{\\beta_{y}\\,=\\,\\frac{N_{y}}{N}}\\end{array}$ , where $N_{y}$ is the sample size for class $y$ and $N$ is the total sample size. Most long-tail learning algorithms, e.g., LA and PC [21, 14], are based on this method because they can access the training data. However, estimating such frequency is complex due to the free-form texts, as opposed to a pre-defined label set. In addition, the pre-training dataset is often inaccessible, making the method inapplicable in our case. ", "page_idx": 5}, {"type": "text", "text": "(2) Implicit method: [1, 25] allow access to a portion of the pre-training data $\\mathcal{D}_{\\mathrm{pt}}$ and use the law of total probability to estimate the prior: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\beta_{y}=\\mathbb{P}(y)=\\int_{\\mathbf{x}}\\mathbb{P}_{\\mathrm{pt}}(\\mathbf{x})\\mathbb{P}(y|\\mathbf{x})\\mathrm{d}\\mathbf{x}=\\mathbb{E}_{\\mathbf{x}\\sim\\mathbb{P}_{\\mathrm{pt}}(\\mathbf{x})}[\\mathbb{P}_{\\mathrm{pt}}(y|\\mathbf{x})]\\approx{\\frac{1}{|\\mathcal{D}_{\\mathrm{pt}}|}}\\sum_{\\mathbf{x}\\in\\mathcal{D}_{\\mathrm{pt}}}\\hat{\\mathbb{P}}_{\\mathrm{pt}}(y|\\mathbf{x})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\hat{\\mathbb{P}}_{\\mathsf{p t}}(y|\\mathbf{x})$ denotes the zero-shot model. However, in our setting, we do have access to the pretraining data or a portion of it. Wang et al. [37] replace the pre-training data $\\mathcal{D}_{\\mathrm{pt}}$ with the downstream data $\\mathcal{D}_{\\mathsf{d s}}$ in Eq. (15) to debias. However, this method neglects the distribution discrepancies between the pre-training and downstream data. In Section 4.3, we show that our debiasing significantly outperforms this implicit method. ", "page_idx": 5}, {"type": "text", "text": "(3) TDE [34]: Tang et al. [34] debias by removing features along a global direction, retaining only those orthogonal to it. Specifically, the global feature is estimated by |D1pt| x\u2208Dpt x. Given a test sample $\\mathbf{x}$ , TDE decomposes it into parallel and orthogonal directions to $\\bar{\\bf x}$ : ${\\bf x}={\\bf x}_{\\parallel}+{\\bf x}_{\\perp}$ . Then, only the orthogonal component is used for classification: $\\hat{\\mathbb{P}}_{\\mathsf{p t}}(y|\\mathbf{x}_{\\perp})$ . While TDE does not require labels for the samples, we cannot apply it because it requires sampling from the pre-training data. In Section 4.3, we replace $\\mathcal{D}_{\\mathrm{pt}}$ with downstream data $\\mathcal{D}_{\\mathsf{d s}}$ and demonstrate its inferior performance. ", "page_idx": 5}, {"type": "text", "text": "(4) GLA [49]: Zhu et al. [49] propose to estimate the pre-training prior from the downstream data using the Bayes optimal criterion. The pre-training prior $\\beta$ is solved by optimizing: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\beta=\\arg\\operatorname*{min}_{\\beta}\\mathbb{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}_{\\mathsf{d s}}}[\\ell_{\\mathsf{c e}}(f_{\\mathsf{p t}}(\\mathbf{x})-\\ln\\beta,y)],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "table", "img_path": "OJximyClit/tmp/aa0b07a6e1d91191e8e98377071bfe48f717a82bccabd2681fb1dceb0e929d49.jpg", "table_caption": ["Table 1: Comparison of accuracy $(\\%)$ on 10 datasets for CLIP ViT-B/16 and ViT-L/14. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "where $\\ell_{\\mathsf{c e}}$ is the cross-entropy loss and $f_{\\mathsf{p t}}(\\mathbf{x})$ is the logit of the zero-shot model. While this method circumvents the need for pre-training data access, it is inapplicable because it requires labels for each downstream sample. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Setup", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. We conduct experiments on 16 image classification benchmarks, covering diverse range categories including generic objects (ImageNet [8], Caltech [9]), scenes (SUN [42]), textures (DTD [7]), satellite images (EuroSAT [11]), actions (UCF [33]) and fine-grained categories (Pets [26], Cars [16], Flowers [23], Food [4], Aircraft [20]). Additionally, we evaluate on five ImageNet distribution shifted datasets [8]: ImageNetV2 (IN-V2) [30], ImageNet-Sketch (IN-Sketch) [36], ImageNet-A (IN-A) [13], ImageNet-R (IN-R) [12] and ObjectNet [2]. ", "page_idx": 6}, {"type": "text", "text": "Implementation details. We adopt CLIP [29] ViT-B/16 and ViT-L/14 as our pre-trained models. The default model for ablation studies is CLIP ViT-B/16. We use the same text descriptions as $\\mathrm{SuS-X}$ [35] and CuPL [27], and adhere to the InMaP [28] settings to include all test images. $\\tau_{\\mathsf{c}}=0.01$ is provided by CLIP. $\\epsilon$ in Algorithm 2 is set to 0.01. All experiments are conducted on a single NVIDIA 3090 GPU if not specified. Note that our algorithm does not require any hyper-parameter searching. ", "page_idx": 6}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We compare our method with several state-of-art methods, including CLIP [29], TPT [32], PromptAlign [31], SuS-X-DS [35], TDA [15], GPT4-Prompt [41], CuPL-CLIP [27], and InMaP [28]. Both TPT and TDA utilize a stream of unlabeled test images. For TPT, TDA and InMaP, we produce the results of ViT-L/14 by executing the official released code and maintaining the same hyper-parameters. ", "page_idx": 6}, {"type": "text", "text": "Results on 10 datasets. In Table 1, we summarize the accuracy across all datasets, excluding ImageNet and its shifts (denoted as 10-datasets). Our method consistently shows superior performance across the datasets and backbones, significantly surpassing GPT4-Prompt, which is known for generating high-quality prompts. By integrating our method with InMaP, our Frolic achieves the highest performance, with an average improvement of $2.6\\%$ with ViT-B/16 and $2.0\\%$ with ViT-L/14. ", "page_idx": 6}, {"type": "table", "img_path": "OJximyClit/tmp/3773ebee97376ca76793279b224f76e83315ae7b7a2fc2de149deb8495a65a53.jpg", "table_caption": ["Table 2: Comparison of accuracy $(\\%)$ on ImageNet and its variants for CLIP ViT-B/16 and ViT-L/14. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "OJximyClit/tmp/9142d0c177db44cb4d3232a4d6aad3195bc55454fe25a567c3f400db2561e6f9.jpg", "table_caption": ["Table 3: Accuracy $(\\%)$ of different models on 10-datasets, ImageNet and its five variant datasets. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Results on ImageNet and associated five shifts. In Table 2, our Frolic again surpasses the comparison methods, achieving the average accuracy of $64.4\\%$ and $75.1\\%$ with ViT-B/16 and ViT-L/14, respectively. Additionally, we observe improvements on the distribution shift datasets: IN-V2, INSketch, IN-A, and IN-R with ViT-B/16, and on IN-A and IN-R with ViT-L/14, when our Frolic is combined with InMaP. However, these results still lag behind the original performance of our Frolic. This discrepancy may stem from the hyper-parameters in InMaP being optimized specifically for ImageNet; applying them unchanged to its shifted datasets could lead to over-fitting. ", "page_idx": 7}, {"type": "text", "text": "4.3 Ablation Studies and Further Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Effectiveness of the prompt distribution learning. In Table 3 (Row (1) & (3)), we compare the performance of the original CLIP model $f_{\\mathsf{c}}$ with our prompt distribution learning model $f_{\\mathrm{g}}$ . We observe that modeling the underlying distribution of the text prototypes results in notable performance gains. For example, $\\bar{3}.7\\%$ accuracy improvement on 10-datasets using ViT-B/16. ", "page_idx": 7}, {"type": "text", "text": "Effectiveness of the prediction fusion. As described in Eq.(10), our Frolic fuses the original CLIP $f_{\\mathsf{c}}$ and the prompt distribution learning model $f_{\\mathrm{g}}$ via confidence matching. We compare the simple fusion $f_{\\mathrm{c}}\\!+\\!f_{\\mathrm{g}}$ and our adaptive fusion $f_{\\mathrm{f}}={f_{\\mathrm{c}}}/{\\tau_{\\mathrm{c}}}{+}\\bar{f}_{\\mathrm{g}}/{\\tau_{\\mathrm{g}}}$ in Table 3 (Row (4) & (5)). We show that our fusion technique outperforms the simple fusion by a large margin. Recall that our adaptive fusion method addresses situations where $f_{\\mathrm{g}}$ is more overconfident than $f_{\\mathsf{c}}$ . In Figure 3, we illustrate the relationship between performance gains over simple fusion\u2014i.e., $\\mathsf{A c c}(f_{\\mathsf{c}}/\\tau_{\\mathsf{c}}+f_{\\mathsf{g}}/\\tau_{\\mathsf{g}})-\\mathsf{A c c}(f_{\\mathsf{c}}+f_{\\mathsf{g}}).$ \u2014and the confidence difference\u2014i.e., $|\\mathrm{conf}(f_{\\mathrm{g}},1)-\\mathrm{conf}(f_{\\mathrm{c}},\\tau_{\\mathrm{c}})|$ . We present this as a scatter plot where each point represents a dataset, and we have ftited these points with a line. As expected, larger confidence differences correlate with more significant improvements. ", "page_idx": 7}, {"type": "text", "text": "Effectiveness of the bias correction. Row (2) and (6) in Table 3 demonstrate the effectiveness of our debiasing method, which can further improve the base CLIP model $f_{\\mathsf{c}}$ and the fusion model $f_{\\mathsf{d}}$ across various backbones and datasets. We also compare our debiasing method with other label bias correction methods in Table 4. The descriptions of TDE [34] and the Implicit method can be found in Section 3.4. The results reveal that TDE [34] does not consistently perform well across all datasets. In contrast, while the implicit method using downstream data enhances zero-shot performance, it underperforms compared to our debiasing method, which shows an average gain of $1.6\\%$ over the implicit method. To further assess our method\u2019s potential, we replaced pseudo-labeling with ground truth labels. The results reveal that the maximum achievable accuracy surpasses our method by $1.0\\%$ , highlighting the importance of our iterative approach for more accurate pseudo-labeling. ", "page_idx": 7}, {"type": "table", "img_path": "OJximyClit/tmp/fd1e0efd11736b8b5681bdb35f391b0de5b674dd8c96dcc3e5a4c7c39c5808c3.jpg", "table_caption": ["Table 4: Comparison of accuracy $(\\%)$ between our Frolic and other label bias correcting methods for CLIP ViT-B/16. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "OJximyClit/tmp/6e2568d82705c89f37072e4629354396f65b4415e1a49ab69f974395657e8152.jpg", "img_caption": ["Figure 3: Relation between gains and confidence differences. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "OJximyClit/tmp/a584179df4141f64e1bca4894c5f3045b22b3f7a9e815e778bfacc34026e59fa.jpg", "img_caption": ["Figure 4: Convergence of accuracy and $\\ell_{1}$ error of on ImageNet. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Convergence of Algorithm 1. Our method Frolic, as described in Algorithm 2, iteratively solves for the prior $\\beta$ . In Figure 4, we examine the convergence by displaying the errors $\\ell_{1}=\\|\\pmb{\\beta}^{\\dot{t}}-\\beta^{t-1}\\|_{1}$ and the accuracy across iterations. We find that the resultant accuracy saturates after only 6 steps, and the relative $\\ell_{1}$ error decreases to less than $\\epsilon=0.01$ after 10 steps. ", "page_idx": 8}, {"type": "text", "text": "Comparison with other prompt-based methods. The popular prompt-based methods, such as CoOp [46] and CoCoOp [45], require a training procedure with labeled samples while our method does not involve any training. To ensure a fair comparison, we compare our Frolic with CoOp and CoCoOp on across-dataset results, where the $\\mathrm{CoOp}$ and $\\mathrm{CoCoOp}$ are trained only with the labeled samples from the ImageNet dataset and then directly tested on the remaining datasets. The results shown in Table 5 demonstrate that our Frolic not only avoids the complexities of training but also exhibits superior generalization performance compared to these methods. ", "page_idx": 8}, {"type": "table", "img_path": "OJximyClit/tmp/228b65da0d369ccfe70cc442bfa586277177e26d4715cadaf3ac032f027d6f8b.jpg", "table_caption": ["Table 5: Comparison of accuracy $(\\%)$ between our Frolic and prompt-based methods for CLIP ViT-B/16. $^*$ denotes our method built upon InMaP [28] "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Comparison with adapter-based methods. The adapter-based methods, e.g., LFA [24] and TipAdapter [44] boost the CLIP\u2019s generalization using labeled training samples. In contrast, our Frolic doesn\u2019t require any labeled samples. We evaluate our method with LFA and Tip-Adapert on the ", "page_idx": 8}, {"type": "table", "img_path": "OJximyClit/tmp/f9174abf20590599e18d752950097c2b670aaadbf99b1a2652231818c2b529c5.jpg", "table_caption": ["Table 6: Comparison of accuracy $(\\%)$ between our Frolic and adapter-based distribution methods for CLIP ViT-B/16. $^*$ denotes our method built upon InMaP [28] "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "ImageNet and its variants dataset, where the LFA and Tip-Adapter only utilize the labeled samples from the ImageNet dataset. The results in Table 6 show that our method achieves the best performance across all datasets with nearly $3\\%$ improvements in averaged accuracy over LFA. ", "page_idx": 9}, {"type": "text", "text": "Running time. Our method Frolic is completely trainingfree, unlike prompt tuning approaches such as TPT [32] and TDA [15], which involve back-propagating through an expensive encoder during optimization. We assess the wall-clock time of Frolic, TPT, and TDA in Table 7, using the CLIP ViT-B/16 model on ImageNet. These evaluations are conducted on a single NVIDIA A100 GPU. The results indicate that our method not only requires less time but also delivers superior performance. ", "page_idx": 9}, {"type": "table", "img_path": "OJximyClit/tmp/41bf2657601ff66e67d6ff6da1f55be8dc1c2103d6dc3eb859c89f4a7edae05e.jpg", "table_caption": ["Table 7: Comparison of running time on ImageNet with ViT-B/16. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Societal Impact, Limitation and Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Societal impact and limitation. Models pre-trained on large-scale web-crawled datasets may incorporate knowledge from noisy or malicious samples. ", "page_idx": 9}, {"type": "text", "text": "Limitation. Our approach assumes that the feature representations follow a mixture of Gaussian; however, this assumption may not always hold. On the other hand, the quality and distribution of data used in pre-training can significantly impact the performance of pre-trained models. Our method relies on the capabilities of pre-trained models for downstream tasks, if the pre-trained knowledge differs from the downstream tasks, the efficacy of our method may be limited. ", "page_idx": 9}, {"type": "text", "text": "Conclusion. In this work, we propose label-Free prompt distribution learning and bias correction, dubbed as Frolic, framework to boost the performance of zero-shot models. Our Frolic models each class prototype via a Gaussian distribution and fuses the learned model with the original CLIP [29] via confidence matching. The proposed framework further effectively removes the label bias without accessing to the pre-training data. Extensive experiments across various datasets demonstrate the effectiveness of our approach. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The work is supported by the National Natural Science Foundation of China (Grants No. 62202439), and the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-PhD-2021-01-002). This work is also supported by the advanced computing resources provided by the Supercomputing Center of the USTC. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] James Urquhart Allingham, Jie Ren, Michael W. Dusenberry, Xiuye Gu, Yin Cui, Dustin Tran, Jeremiah Zhe Liu, and Balaji Lakshminarayanan. A simple zero-shot prompt weighting technique to improve prompt ensembling in text-image models. In ICML, 2023.   \n[2] Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. In NeurIPS, 2019.   \n[3] Christopher M Bishop. Pattern recognition and machine learning. Springer, 2:200\u2013202.   \n[4] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 - mining discriminative components with random forests. In ECCV, 2014.   \n[5] Hao Chen, Bhiksha Raj, Xing Xie, and Jindong Wang. On catastrophic inheritance of large foundation models. arXiv preprint arXiv:2402.01909, 2024.   \n[6] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In CVPR, 2023.   \n[7] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In CVPR, 2014.   \n[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.   \n[9] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In CVPR Workshops, 2004.   \n[10] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In ICML, 2017.   \n[11] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE J. Sel. Top. Appl. Earth Obs. Remote. Sens., 12(7):2217\u20132226, 2019.   \n[12] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In ICCV, 2021.   \n[13] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. 2021.   \n[14] Youngkyu Hong, Seungju Han, Kwanghee Choi, Seokjun Seo, Beomsu Kim, and Buru Chang. Disentangling label distribution for long-tailed visual recognition. In CVPR, 2021.   \n[15] Adilbek Karmanov, Dayan Guan, Shijian Lu, Abdulmotaleb El Saddik, and Eric Xing. Efficient test-time adaptation of vision-language models. In CVPR, 2024.   \n[16] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In ICCV Workshops, 2013.   \n[17] Ananya Kumar, Tengyu Ma, Percy Liang, and Aditi Raghunathan. Calibrated ensembles can mitigate accuracy tradeoffs under distribution shift. In UAI, 2022.   \n[18] Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu, and Xinmei Tian. Prompt distribution learning. In CVPR, 2022.   \n[19] Rachel Luo, Shengjia Zhao, Jiaming Song, Jonathan Kuck, Stefano Ermon, and Silvio Savarese. Privacy preserving recalibration under domain shift. CoRR, abs/2008.09643, 2020.   \n[20] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew B. Blaschko, and Andrea Vedaldi. Finegrained visual classification of aircraft. CoRR, abs/1306.5151, 2013.   \n[21] Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and Sanjiv Kumar. Long-tail learning via logit adjustment. In ICLR, 2021.   \n[22] RV Mises and Hilda Pollaczek-Geiringer. Praktische verfahren der gleichungsaufl\u00f6sung. ZAMMJournal of Applied Mathematics and Mechanics/Zeitschrift f\u00fcr Angewandte Mathematik und Mechanik, 9(1):58\u201377, 1929.   \n[23] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In ICVGIP, 2008.   \n[24] Yassine Ouali, Adrian Bulat, Brais Mart\u00ednez, and Georgios Tzimiropoulos. Black box few-shot adaptation for vision-language models. CoRR, abs/2304.01752, 2023.   \n[25] Shubham Parashar, Zhiqiu Lin, Tian Liu, Xiangjue Dong, Yanan Li, Deva Ramanan, James Caverlee, and Shu Kong. The neglected tails of vision-language models. arXiv preprint arXiv:2401.12425, 2024.   \n[26] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR, 2012.   \n[27] Sarah M. Pratt, Ian Covert, Rosanne Liu, and Ali Farhadi. What does a platypus look like? generating customized prompts for zero-shot image classification. In ICCV, 2023.   \n[28] Qi Qian, Yuanhong Xu, and Juhua Hu. Intra-modal proxy learning for zero-shot visual categorization with CLIP. In NeurIPS, 2023.   \n[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021.   \n[30] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In ICML, 2019.   \n[31] Jameel Abdul Samadh, Hanan Gani, Noor Hussein, Muhammad Uzair Khattak, Muzammal Naseer, Fahad Shahbaz Khan, and Salman H. Khan. Align your prompts: Test-time prompting with distribution alignment for zero-shot generalization. In NeurIPS, 2023.   \n[32] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. In NeurIPS, 2022.   \n[33] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A dataset of 101 human actions classes from videos in the wild. CoRR, abs/1212.0402, 2012.   \n[34] Kaihua Tang, Jianqiang Huang, and Hanwang Zhang. Long-tailed classification by keeping the good and removing the bad momentum causal effect. In NeurIPS, 2020.   \n[35] Vishaal Udandarao, Ankush Gupta, and Samuel Albanie. Sus-x: Training-free name-only transfer of vision-language models. CoRR, abs/2211.16198, 2022.   \n[36] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems, pages 10506\u201310518, 2019.   \n[37] Xudong Wang, Zhirong Wu, Long Lian, and Stella X Yu. Debiased learning from naturally imbalanced pseudo-labels. In CVPR, 2022.   \n[38] Zhengbo Wang, Jian Liang, Lijun Sheng, Ran He, Zilei Wang, and Tieniu Tan. A hard-to-beat baseline for training-free CLIP-based adaptation. In ICLR, 2024.   \n[39] Zhicai Wang, Yanbin Hao, Tingting Mu, Ouxiang Li, Shuo Wang, and Xiangnan He. Bidirectional distribution alignment for transductive zero-shot learning. In CVPR, 2023.   \n[40] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt. Robust fine-tuning of zero-shot models. In CVPR, 2022.   \n[41] Wenhao Wu, Huanjin Yao, Mengxi Zhang, Yuxin Song, Wanli Ouyang, and Jingdong Wang. Gpt4vis: What can GPT-4 do for zero-shot visual recognition? CoRR, abs/2311.15732, 2023.   \n[42] Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. SUN database: Large-scale scene recognition from abbey to zoo. In CVPR, 2010.   \n[43] Xuanyu Yi, Jiajun Deng, Qianru Sun, Xian-Sheng Hua, Joo-Hwee Lim, and Hanwang Zhang. Invariant training 2d-3d joint hard samples for few-shot point cloud recognition. In ICCV, 2023.   \n[44] Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free adaption of CLIP for few-shot classification. In ECCV, 2022.   \n[45] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In CVPR, 2022.   \n[46] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. IJCV, 130(9):2337\u20132348, 2022.   \n[47] Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Hanwang Zhang. Prompt-aligned gradient for prompt tuning. In ICCV, 2023.   \n[48] Beier Zhu, Yulei Niu, Xian-Sheng Hua, and Hanwang Zhang. Cross-domain empirical risk minimization for unbiased long-tailed classification. In AAAI, 2022.   \n[49] Beier Zhu, Kaihua Tang, Qianru Sun, and Hanwang Zhang. Generalized logit adjustment: Calibrating fine-tuned models by removing label bias in foundation models. In NeurIPS, 2023.   \n[50] Xingyu Zhu, Beier Zhu, Yi Tan, Shuo Wang, Yanbin Hao, and Hanwang Zhang. Selective vision-language subspace projection for few-shot CLIP. In ACM MM, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1 Introduction ", "page_idx": 13}, {"type": "text", "text": "2 Related Works 3 ", "page_idx": 13}, {"type": "text", "text": "3 Methods 3 ", "page_idx": 13}, {"type": "text", "text": "3.1 Setup 3   \n3.2 Label-Free Prompt Distribution Learning 4   \n3.3 Prediction Fusion via Adaptive Calibration. 4   \n3.4 Correcting Pre-training Label Bias via Label-Free Logit Adjustment 5 ", "page_idx": 13}, {"type": "text", "text": "4 Experiments 7 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "4.1 Setup   \n4.2 Main Results 7   \n4.3 Ablation Studies and Further Analysis 8 ", "page_idx": 13}, {"type": "text", "text": "5 Societal Impact, Limitation and Conclusion 10 ", "page_idx": 13}, {"type": "text", "text": "A Theoretical Analysis 15 ", "page_idx": 13}, {"type": "text", "text": "A.1 Proof of Eq. (6): Estimation of Class Covariance from Marginal Second Order Moment 15   \nA.2 Proof of Eq. (5): Estimation of the Priors of Gaussian Mixture Models . . . . . . 16   \nA.3 Proof of Eq. (7): Parameters of our Learned Model . . 17   \nA.4 Proof of Eq. (11): Debiased Classifier for Downstream Data . . 17   \nA.5 Proof of Eq. (12): Equation to Estimate Pre-training Priors . . 18   \nA.6 Power Method to Estimate Pretraining Priors . . . . 18 ", "page_idx": 13}, {"type": "text", "text": "B Details of ImageNet Variant Datasets 18 ", "page_idx": 13}, {"type": "text", "text": "C Licenses 19 ", "page_idx": 13}, {"type": "text", "text": "A Theoretical Analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Proof of Eq. (6): Estimation of Class Covariance from Marginal Second Order Moment ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We first derive the second order moments for a multivariate Gaussian and then for a Gaussian mixture, corresponding to the marginal distribution of $\\mathbb{P}(\\mathbf{x})$ . ", "page_idx": 14}, {"type": "text", "text": "For a class $j$ with parameters $\\mathbf{z}_{j}$ and $\\Sigma$ , the conditional probability density function is given by: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{N}(\\mathbf{x};\\mathbf{z}_{j},\\Sigma)=\\frac{1}{\\sqrt{(2\\pi)^{d}|\\Sigma|}}\\exp\\{-\\frac{1}{2}(\\mathbf{x}-\\mathbf{z}_{j})^{\\top}\\Sigma^{-1}(\\mathbf{x}-\\mathbf{z}_{j})\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The second order moment generating function for class $j$ is: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{M_{j}=\\mathbb{E}_{\\mathbf{x}\\in\\mathcal{C}_{j}}[\\mathbf{xx}^{\\top}]=\\int_{\\mathbf{x}}\\!\\!\\!\\!K(\\mathbf{x};\\mathbf{z}_{j},\\Sigma)\\mathbf{x}\\mathbf{x}^{\\top}\\mathrm{d}\\mathbf{x}}\\\\ &{\\quad=\\frac{1}{\\sqrt{(2\\pi)^{d}|\\mathcal{Z}|}}\\int_{\\mathbf{x}}\\exp\\{-\\frac{1}{2}(\\mathbf{x}-\\mathbf{z}_{j})^{\\top}\\Sigma^{-1}(\\mathbf{x}-\\mathbf{z}_{j})\\}\\mathbf{x}\\mathbf{x}^{\\top}\\mathrm{d}\\mathbf{x}}\\\\ &{\\quad\\overset{(a)}{=}\\frac{1}{\\sqrt{(2\\pi)^{d}|\\mathcal{Z}|}}\\int_{\\mathbf{y}}\\exp\\{-\\frac{1}{2}\\mathbf{y}^{\\top}\\Sigma^{-1}\\mathbf{y}\\}(\\mathbf{y}+\\mathbf{z}_{j})(\\mathbf{y}+\\mathbf{z}_{j})^{\\top}\\mathrm{d}\\mathbf{y}}\\\\ &{\\quad=\\frac{1}{\\sqrt{(2\\pi)^{d}|\\mathcal{Z}|}}\\int_{\\mathbf{y}}\\exp\\{-\\frac{1}{2}\\mathbf{y}^{\\top}\\Sigma^{-1}\\mathbf{y}\\}(\\mathbf{y}\\mathbf{y}^{\\top}+\\underbrace{\\mathbf{y}\\Sigma_{j}^{\\top}}_{\\forall u\\land b\\land y\\operatorname*{sumet}}+\\mathbf{z}_{j}\\mathbf{z}_{j}^{\\top})\\mathrm{d}\\mathbf{y}}\\\\ &{\\quad\\overset{(b)}{=}\\frac{1}{\\sqrt{(2\\pi)^{d}|\\mathcal{Z}|}}\\int_{\\mathbf{y}}\\exp\\{-\\frac{1}{2}\\mathbf{y}^{\\top}\\Sigma^{-1}\\mathbf{y}\\}(\\mathbf{y}\\mathbf{y}^{\\top}+\\mathbf{z}_{j}\\mathbf{z}_{j}^{\\top})\\mathrm{d}\\mathbf{y}}\\\\ &{\\quad\\overset{(c)}{=}\\mathbf{z}_{j}\\mathbf{z}_{j}^{\\top}+\\frac{1}{\\sqrt{(2\\pi)^{d}|\\mathcal{Z}|}}\\int_{\\mathbf{y}}\\exp\\{-\\frac{1}{2}\\mathbf{y}^{\\top}\\Sigma^{-1}\\mathbf{y}\\}(\\mathbf{y}\\mathbf{y}^{\\top})\\mathrm{d}\\mathbf{y}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "(a=) holds as we change the integral variables y = x \u2212zj. We have(b=) because the exp(\u00b7) function is an even function of y and the factors $\\mathbf{y}\\mathbf{z}_{j}^{\\top}$ and $\\mathbf{z}_{j}\\mathbf{y}^{\\top}$ will vanish during integral by symmetry. For $\\stackrel{\\left(c\\right)}{=}$ , we take the term $\\mathbf{z}_{j}\\mathbf{z}_{j}^{\\top}$ outside of the integral as they are constant. ", "page_idx": 14}, {"type": "text", "text": "The covariance matrix $\\Sigma$ and its inverse matrix $\\Sigma^{-1}$ the can be expressed through an expansion in terms of its eigenvalues $\\{\\lambda_{i}\\}_{i=1}^{d}$ and eigenvectors $\\{{\\mathbf{u}}_{i}\\}_{i=1}^{d}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Sigma=\\sum_{i=1}^{d}\\lambda_{i}\\mathbf{u}_{i}\\mathbf{u}_{i}^{\\top},\\quad\\Sigma^{-1}=\\sum_{i=1}^{d}\\frac{1}{\\lambda_{i}}\\mathbf{u}_{i}\\mathbf{u}_{i}^{\\top}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Similarly, we can decompose $\\mathbf{y}$ using the set of eigenvectors: $\\begin{array}{r}{{\\bf y}=\\sum_{j=1}^{d}e_{j}{\\bf u}_{j}}\\end{array}$ , where $e_{j}={\\bf u}_{j}^{T}{\\bf y}$ (We temporarily abuse the subscript here. It does not represent class $j$ until we reach Eq. (32)) We have the following expression: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf y}{\\bf y}^{\\top}=\\sum_{i=1}^{d}\\sum_{j=1}^{d}e_{i}e_{j}{\\bf u}_{i}{\\bf u}_{j}^{\\top}}\\ ~}\\\\ {{\\displaystyle{\\bf y}^{\\top}\\Sigma^{-1}{\\bf y}=\\sum_{i=1}^{d}e_{i}{\\bf u}_{i}^{\\top}\\sum_{k=1}^{d}\\frac{1}{\\lambda_{k}}{\\bf u}_{k}{\\bf u}_{k}^{\\top}\\sum_{j=1}^{d}e_{j}{\\bf u}_{j}\\stackrel{(d)}{=}\\sum_{k=1}^{d}(\\frac{e_{k}}{\\sqrt{\\lambda_{k}}})^{2}}\\ ~}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We obtain $\\underline{{\\underline{{(d)}}}}$ due to the property of eigenvalues, i.e., $\\mathbf{u}_{i}^{\\top}\\mathbf{u}_{i}=1$ and $\\mathbf{u}_{i}^{\\top}\\mathbf{u}_{j}=0$ , for $i\\neq j$ . Denote $U=[\\mathbf{u}_{1},...,\\mathbf{u}_{d}]^{\\top}$ , we have $\\mathbf{e}=U\\mathbf{y}$ . As the determinant $|U|=1$ , the probability density after transformed remains unchanged: $\\mathbb{P}(\\mathbf{e})\\,=\\,|U|^{-1}\\mathbb{P}(\\mathbf{y})\\,=\\,\\mathbb{P}(\\mathbf{y})$ . Apply Eq. (25) and Eq. (26) into ", "page_idx": 14}, {"type": "text", "text": "Eq. (23), we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{1}{\\sqrt{(2\\pi)^{d}|\\sum_{i=1}^{d}}}\\int_{\\phi}\\exp\\{-\\frac{1}{2}\\mathbf{y}^{\\top}\\Sigma^{-1}\\mathbf{y}\\}(\\mathbf{y}^{\\top})\\mathrm{d}\\mathbf{y}}\\\\ &{=\\frac{1}{\\sqrt{(2\\pi)^{d}|\\sum_{i=1}^{d}{\\sum_{j=1}^{d}{\\mathbf{u}_{i}\\equiv\\sqrt{j}}\\int_{\\phi}\\exp\\{\\frac{\\mathbf{1}}{k-1}-\\frac{1}{2}(\\frac{\\mathbf{c}_{k}}{\\sqrt{\\lambda_{k}}})^{2}\\}\\boldsymbol{\\epsilon}_{i}\\boldsymbol{\\epsilon}_{j}\\mathrm{d}\\mathbf{\\hat{e}}}}}\\\\ &{=\\frac{1}{\\sqrt{(2\\pi)^{d}|\\sum_{i=1}^{d}{\\sum_{j=1}^{d}{\\mathbf{u}_{i}\\equiv\\sqrt{j}}\\int_{\\phi}\\int_{\\mathbf{k}=\\Gamma}^{d}\\exp\\{-\\frac{1}{2}(\\frac{\\mathbf{c}_{k}}{\\sqrt{\\lambda_{k}}})^{2}\\}\\boldsymbol{\\epsilon}_{i}\\boldsymbol{\\epsilon}_{j}\\mathrm{d}\\mathbf{e}}}}\\\\ &{\\stackrel{(c)}{=}\\frac{1}{\\sqrt{(2\\pi)^{d}|\\sum_{i=1}^{d}{\\mathbf{u}_{i}\\equiv\\sqrt{j}}\\int_{\\phi_{i}}\\exp\\{-\\frac{1}{2}(\\frac{\\mathbf{c}_{i}}{\\sqrt{\\lambda_{k}}})^{2}\\}\\boldsymbol{\\epsilon}_{i}^{2}\\mathrm{d}\\mathbf{\\hat{e}}}}\\\\ &{\\stackrel{(d)}{\\underset{i=1}{\\sum}}\\mathbf{u}_{i}\\mathbf{u}_{i}^{\\top}\\int_{\\phi}\\frac{1}{\\sqrt{2\\pi\\lambda_{i}}}\\exp\\{-\\frac{1}{2}(\\frac{\\mathbf{c}_{i}}{\\sqrt{\\lambda_{i}}})^{2}\\}\\boldsymbol{\\epsilon}_{i}^{2}\\mathrm{d}\\mathbf{\\hat{e}}_{i}}\\\\ &{\\stackrel{(d)}{=}\\frac{1}{\\sqrt{2\\pi}}\\mathbf{u}_{i}\\mathbf{u}_{i}^{\\top}\\sum_{i=1}^{d}\\sum}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$\\mathrm{For}\\stackrel{(e)}{=}$ , the terms $i\\neq j$ disappear by symmetry similar to $\\underline{{\\underline{{\\left(b\\right)}}}}$ . We make use of $|\\Sigma|=\\prod_{i=1}^{d}\\lambda_{i}$ for $\\underline{{\\underline{{(f)}}}}$ . We have $\\underline{{\\underline{{(g)}}}}$ because we regard $e_{i}\\sim\\mathcal{N}(0,\\sqrt{\\lambda_{i}})$ and note that $\\mathbb{E}[e_{i}^{2}]={\\mathsf{v a r}}[e_{i}]\\,\\!+\\!\\mathbb{E}[e_{i}]^{2}=\\lambda_{i}\\!+\\!0=\\lambda_{i}$ . Combining Eq. (32) with Eq. (23), we have the second order moment for class $j$ is: ", "page_idx": 15}, {"type": "equation", "text": "$$\nM_{j}=\\mathbf{z}_{j}\\mathbf{z}_{j}^{\\top}+\\Sigma.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using a Gaussian mixture model with the priors $\\{\\pi_{j}\\}_{j}^{K},\\mathbb{P}(\\mathbf{x})$ is given by: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathbf{x})=\\sum_{j=1}^{K}\\pi_{j}\\mathcal{N}(\\mathbf{x};\\mathbf{z}_{j},\\Sigma).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The second order moment for the marginal distribution $\\mathbb{P}(\\mathbf{x})$ is: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle M=\\mathbb{E}[{\\bf x x}^{\\top}]=\\int_{\\mathbf{x}}\\sum_{j=1}^{K}\\pi_{j}\\mathcal{N}({\\bf x};{\\bf z}_{j},{\\bf\\Sigma})\\mathrm{d}{\\bf x}{\\bf x}^{\\top}{\\bf x}}}\\\\ {~~}\\\\ {{\\displaystyle~~=\\sum_{j=1}^{K}\\pi_{j}\\int_{\\mathbf{x}}N({\\bf x};{\\bf z}_{j},{\\bf\\Sigma}){\\bf x}{\\bf x}^{\\top}\\mathrm{d}{\\bf x}}}\\\\ {~~}\\\\ {{\\displaystyle~~=\\sum_{j=1}^{K}\\pi_{j}M_{j}=\\sum_{j=1}^{K}\\pi_{j}({\\bf z}_{j}{\\bf z}_{j}^{\\top}+\\Sigma)}}\\\\ {~~}\\\\ {{\\displaystyle~~=\\sum_{j=1}^{K}\\pi_{j}{\\bf z}_{j}{\\bf z}_{j}^{\\top}+(\\sum_{j=1}^{K}\\pi_{j})\\Sigma=\\sum_{j=1}^{K}\\pi_{j}{\\bf z}_{j}{\\bf z}_{j}^{\\top}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.2 Proof of Eq. (5): Estimation of the Priors of Gaussian Mixture Models ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The expectation of $\\mathbf{x}$ is defined as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{x}]=\\int_{\\mathbf{x}}\\sum_{j=1}^{K}\\pi_{j}\\mathcal{N}(\\mathbf{x};\\mathbf{z}_{j},\\Sigma)\\mathbf{x}\\mathbf{dx}=\\sum_{j=1}^{K}\\pi_{j}\\int_{\\mathbf{x}}\\mathcal{N}(\\mathbf{x};\\mathbf{z}_{j},\\Sigma)\\mathbf{x}\\mathbf{dx}=\\sum_{j=1}^{K}\\pi_{j}\\mathbf{z}_{j}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Denote $\\pmb{\\pi}=[\\pi_{1},..,\\pi_{K}]^{\\top}$ , $Z=[\\mathbf{z}_{1},..,\\mathbf{z}_{K}]^{T}$ , and the expectation of $\\mathbf{x}$ as $\\pmb{\\mu}$ , Eq. (39) can be rewrite as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pmb{\\mu}=Z\\pmb{\\pi}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, the priors can be solve by $\\pi=Z^{-1}\\pmb{\\mu}$ . ", "page_idx": 15}, {"type": "text", "text": "A.3 Proof of Eq. (7): Parameters of our Learned Model ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The posterior of classes $\\mathbb{P}(y|\\mathbf x)$ can be expression as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}(y|\\mathbf{x})=\\frac{\\mathbb{P}(\\mathbf{x}|y)\\mathbb{P}(y)}{\\mathbb{P}(\\mathbf{x})}\\propto\\mathcal{N}(\\mathbf{x};\\mathbf{z}_{y},\\boldsymbol{\\Sigma})\\pi_{y}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To classify $\\mathbf{x}$ , we seek the class $y$ that maximizes this posterior. Since the term $\\mathbb{P}(\\mathbf{x})$ does not depend on $y$ , we can simplify our task to maximizing $\\mathcal{N}(\\mathbf{x};\\mu_{y},\\Sigma)\\pi_{y}$ . Taking natural logarithm gives: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ln\\!\\mathcal{N}(\\mathbf{x};\\mathbf{z}_{y},\\boldsymbol{\\Sigma})\\pi_{y}=\\ln\\frac{1}{\\sqrt{(2\\pi)^{d}|\\boldsymbol{\\Sigma}|}}\\exp\\{-\\frac{1}{2}(\\mathbf{x}-\\mathbf{z}_{y})^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\mathbf{z}_{y})\\}\\pi_{y}}\\\\ &{\\qquad\\qquad\\qquad=\\ln\\frac{1}{\\sqrt{(2\\pi)^{d}|\\boldsymbol{\\Sigma}|}}-\\frac{1}{2}(\\mathbf{x}-\\mathbf{z}_{y})^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\mathbf{z}_{y})+\\ln\\pi_{y}}\\\\ &{\\qquad\\qquad\\qquad=c_{1}-\\frac{1}{2}\\mathbf{x}^{T}\\boldsymbol{\\Sigma}^{-1}\\mathbf{x}+\\mathbf{z}_{y}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\mathbf{x}-\\frac{1}{2}\\mathbf{z}_{y}^{T}\\boldsymbol{\\Sigma}^{-1}\\mathbf{z}_{y}+c_{2}}\\\\ &{\\qquad\\qquad\\qquad=\\mathbf{z}_{y}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\mathbf{x}-\\frac{1}{2}\\mathbf{z}_{y}^{T}\\boldsymbol{\\Sigma}^{-1}\\mathbf{z}_{y}+c}\\\\ &{\\qquad\\qquad\\qquad=\\mathbf{w}_{y}^{T}\\mathbf{x}+b_{y}+c}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The first term in Equation (43) is constant; we incorporate it using a constant $c_{1}$ . Consider that most test benchmarks are generally class-balanced, we use a uniform prior $c_{2}$ to incorporate $\\ln\\pi_{y}$ . In Eq. (45), we use $c$ to absorb all constant terms, including $c_{1},c_{2}$ and $-\\frac{1}{2}\\mathbf{x}^{T}\\Sigma^{-1}\\mathbf{x}$ . Let $\\mathbf{w}_{j}=\\hat{\\Sigma}^{-1}\\mathbf{z}_{j}$ and $\\begin{array}{r}{b_{j}=-\\frac{1}{2}\\mathbf{z}_{j}^{\\top}\\mathbf{w}_{j}}\\end{array}$ , we get Eq. (46). \u53e3 ", "page_idx": 16}, {"type": "text", "text": "A.4 Proof of Eq. (11): Debiased Classifier for Downstream Data ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proposition 1. (Modified from Theorem 1 in [14]). Let $\\mathbb{P}_{\\mathsf{p t}}(y|\\mathbf{x})$ and $\\mathbb{P}_{{\\sf d s}}(y|{\\bf x})$ be the distributions of the pre-train and downstream data, respectively. Let $\\beta_{y}=\\mathbb{P}_{\\mathsf{p t}}(y)$ and $\\pi_{y}=\\mathbb{P}_{\\mathsf{d s}}(y)$ denote the priors of the pre-train and the downstream data, respectively. Assume the likelihood $\\mathbb{P}(\\mathbf{x}|y)$ is unchanged between pre-train and downstream data, i.e., $\\mathbb{P}(\\mathbf{x}|y)=\\mathbb{P}_{\\mathtt{p t}}(\\mathbf{x}|y)=\\mathbb{P}_{\\mathsf{d s}}(\\mathbf{x}|y)$ . If $f_{\\mathsf{p t}}(\\mathbf{x})_{y}$ is the logit of class $y$ from the softmax model to estimate $\\mathbb{P}_{\\mathsf{p t}}(y|\\mathbf{x})$ , then the estimated $\\mathbb{P}_{{\\sf d s}}(y|{\\bf x})$ is formulated as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathsf{d s}}(y|\\mathbf{x})=\\mathrm{softmax}(f_{\\mathsf{p t}}(\\mathbf{x})-\\ln\\beta+\\ln\\pi)_{y},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\beta=[\\beta_{1},...,\\beta_{K}]$ and $\\pmb{\\pi}=[\\pi_{1},...,\\pi_{K}]$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}_{\\mathbf{a}}(y|\\mathbf{x})=\\frac{\\mathbb{P}_{\\mathbf{a}}(\\mathbf{x}|\\mathbf{y})\\mathbb{P}_{\\mathbf{a}}(\\mathbf{y})}{\\mathbb{P}_{\\mathbf{a}}(\\mathbf{x})}=\\frac{\\mathbb{P}_{\\mathbf{p}}(\\mathbf{x}|\\mathbf{y})\\mathbb{P}_{\\mathbf{a}}(\\mathbf{y})}{\\mathbb{P}_{\\mathbf{a}}(\\mathbf{x})}}\\\\ &{=\\frac{\\mathbb{P}_{\\mathbf{p}}(\\mathbf{x}|\\mathbf{y})\\mathbb{P}_{\\mathbf{p}}(\\mathbf{y})}{\\mathbb{P}_{\\mathbf{p}}(\\mathbf{x})}\\frac{\\mathbb{P}_{\\mathbf{a}}(\\mathbf{y})}{\\mathbb{P}_{\\mathbf{p}}(\\mathbf{y})}\\frac{\\mathbb{P}_{\\mathbf{p}}(\\mathbf{x})}{\\mathbb{P}_{\\mathbf{a}}(\\mathbf{x})}}\\\\ &{\\overset{(a)}{=}\\frac{\\mathbb{P}_{\\mathbf{p}}(\\mathbf{y})\\mathbb{P}_{\\mathbf{p}}(\\mathbf{y})\\mathbb{P}_{\\mathbf{p}}^{\\perp}}{\\mathbb{P}_{\\mathbf{p}}(\\mathbf{y})}\\frac{\\mathbb{P}_{\\mathbf{p}}(\\mathbf{z})}{\\mathbb{P}_{\\mathbf{p}}^{\\perp}}=\\operatorname{sot}{\\operatorname{max}}(f_{\\mathbf{p}}(\\mathbf{x}))\\frac{\\pi_{y}}{\\delta_{y}}\\frac{1}{Z}}\\\\ &{=\\frac{\\exp(f_{\\mathbf{p}}(\\mathbf{x}))}{Z\\sum_{i}^{j}-\\mathbb{P}_{\\mathbf{p}}(\\mathbf{y})}\\frac{\\exp(\\ln(\\mathbf{\\pi}_{y}))}{\\mathbb{P}_{\\mathbf{a}}(\\mathbf{x})_{j}}\\exp(\\ln(\\mathbf{\\pi}_{y}))}\\\\ &{=\\frac{\\exp(f_{\\mathbf{p}}(\\mathbf{x}))-\\ln(\\mathbf{\\pi}_{y})}{Z\\sum_{i}^{j}-\\mathbb{P}_{\\mathbf{p}}(\\mathbf{y})}\\frac{\\ln(\\mathbf{\\pi}_{y})}{f_{\\mathbf{p}}(\\mathbf{x})_{j}}}\\\\ &{\\overset{(b)}{=}\\frac{\\exp(f_{\\mathbf{p}}(\\mathbf{x}))-\\ln(\\mathbf{\\pi}_{y})+\\ln(\\mathbf{\\pi}_{y})}{\\sum_{i}^{j}-\\mathbb{P}_{\\mathbf{p}}(\\mathbf \n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For(=a), we denote the term that is not related to y as Z1 = PPdpst((xx)) . We derive $\\underline{{\\underline{{\\left(b\\right)}}}}$ from the requirement that $\\mathbb{P}_{{\\sf d s}}(y|{\\bf x})$ , being a probability, must sum to 1 across all possible classes $y\\in[K]$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{K}\\mathbb{P}_{\\mathsf{d s}}(i|\\mathbf{x})=\\frac{\\sum_{i=1}^{K}\\exp(f_{\\mathsf{p t}}(\\mathbf{x})_{i}-\\ln\\beta_{i}+\\ln\\pi_{i})}{Z\\sum_{j=1}^{K}\\exp(f_{\\mathsf{p t}}(\\mathbf{x})_{j})}=1.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, we hav $\\begin{array}{r}{Z\\sum_{j=1}^{K}\\exp(f_{\\mathsf{p t}}(\\mathbf{x})_{j})=\\sum_{i=1}^{K}\\exp(f_{\\mathsf{p t}}(\\mathbf{x})_{i}-\\ln\\beta_{i}+\\ln\\pi_{i})}\\end{array}$ . In our context, the $f_{\\mathsf{p t}}$ is equivalent to our $f_{\\mathsf{f}}$ . ", "page_idx": 17}, {"type": "text", "text": "A.5 Proof of Eq. (12): Equation to Estimate Pre-training Priors ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proposition 2. Let $s(\\mathbf{x})\\;=\\;[\\mathbb{P}(Y\\;=\\;1|\\mathbf{x}),...,\\mathbb{P}(Y\\;=\\;K|\\mathbf{x})]^{\\intercal}\\;\\in\\;\\mathbb{R}^{K}$ be the likelihood vector, $\\mathbf{s}_{j}=\\mathbb{E}_{\\mathbf{x}|Y=j}[s(\\mathbf{x})]$ and $S=[\\mathbf{s}_{1},...,\\mathbf{s}_{K}]\\in\\mathbb{R}^{K\\times K}$ . The pretraining prior $\\beta=[\\beta_{1},...,\\beta_{K}]^{\\intercal}\\in\\mathbb{R}^{K}$ must satisfy the linear system: ", "page_idx": 17}, {"type": "equation", "text": "$$\n(S-I)\\beta={\\bf0}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\beta_{y}=\\int_{\\mathbf{x}}\\mathbb{P}_{\\mathrm{pt}}(\\mathbf{x})\\mathbb{P}_{\\mathrm{pt}}(y|\\mathbf{x})\\mathrm{d}\\mathbf{x}=\\int_{\\mathbf{x}}\\sum_{y^{\\prime}\\in[K]}\\mathbb{P}(\\mathbf{x}|y^{\\prime})\\beta_{y^{\\prime}}\\mathbb{P}_{\\mathrm{pt}}(y|\\mathbf{x})\\mathrm{d}\\mathbf{x}}}\\\\ &{=\\displaystyle\\sum_{y^{\\prime}\\in[K]}\\beta_{y^{\\prime}}\\int_{\\mathbf{x}}\\mathbb{P}(\\mathbf{x}|y^{\\prime})\\mathbb{P}_{\\mathrm{pt}}(y|\\mathbf{x})\\mathrm{d}\\mathbf{x}}\\\\ &{=\\displaystyle\\sum_{y^{\\prime}\\in[K]}\\beta_{y^{\\prime}}\\mathbb{E}_{\\mathbf{x}|Y=y^{\\prime}}[\\mathbb{P}_{\\mathrm{pt}}(y|\\mathbf{x})]}\\\\ &{=\\displaystyle\\sum_{y^{\\prime}\\in[K]}\\beta_{y^{\\prime}}\\mathbb{E}_{\\mathbf{x}|Y=y^{\\prime}}[s(\\mathbf{x})]_{y},}\\\\ &{=\\displaystyle\\sum_{y^{\\prime}\\in[K]}S_{y y^{\\prime}}\\beta_{y^{\\prime}}}\\\\ &{=\\displaystyle\\sum_{y^{\\prime}\\in[K]}S_{y y^{\\prime}}\\beta_{y^{\\prime}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that Equation (61) precisely represents the matrix multiplication given by: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\beta=S\\beta\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By moving the RHS term to the LHS, Eq. (56) is obtained. ", "page_idx": 17}, {"type": "text", "text": "A.6 Power Method to Estimate Pretraining Priors ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The solution to Equation (62) involves finding the eigenvector corresponding to the eigenvalue of 1 for the matrix $S$ . We can apply SVD decomposition to find the solution; however, we find that the results might be numerically unstable. Instead, we adopt power iteration from [22]. Like the Jacobi and Gauss-Seidel methods, the power method for approximating eigenvalues is iterative. We first initialize $\\begin{array}{r}{\\beta_{0}=[\\frac{1}{K},...,\\frac{1}{K}]}\\end{array}$ of a uniform distribution. Then, we perform the sequence: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\beta}_{t}=S\\beta_{t-1}}\\\\ &{\\beta_{t}=\\frac{\\bar{\\beta}_{t}}{\\|\\bar{\\beta}_{t}\\|_{1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We repeat the sequence until the relative change is small: $\\|\\beta_{t}-\\beta_{t-1}\\|<\\epsilon$ . ", "page_idx": 17}, {"type": "text", "text": "B Details of ImageNet Variant Datasets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "ImageNet-V2 [30]: sampling from the original ImageNet and including 10,000 images of 1,000 ImageNet categories. ", "page_idx": 17}, {"type": "text", "text": "ImageNet Sketch [36]: including 138 50,000 images and covering 1,000 ImageNet categories. ", "page_idx": 17}, {"type": "text", "text": "ImageNet-R [12]: containing renditions (e.g., art, cartoons, grafftii) for ImageNet classes, comprising 30,000 images from 200 ImageNet categories. ", "page_idx": 17}, {"type": "text", "text": "ImageNet-A [13]: collecting real-world images that are misclassified by ResNet-50, totaling 7,500 images from 200 of ImageNet categories. ", "page_idx": 17}, {"type": "text", "text": "ObjectNet: [2] including 50,000 test images with rotation, background, and viewpoint, and overlapping 113 classes with ImageNet. ", "page_idx": 17}, {"type": "text", "text": "C Licenses ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "All the datasets we considered are publicly available, we list their licences and URLs as follows: ", "page_idx": 18}, {"type": "text", "text": "\u2022 Caltech101 [9]: Non-commercial, https://data.caltech.edu/records/mzrjq-6wc02.   \n\u2022 OxfordPets [26]: MIT License, https://www.robots.ox.ac.uk/\\~vgg/data/pets/.   \n\u2022 StanfordCars [16]: MIT License, https://www.kaggle.com/datasets/ jessicali9530/stanford-cars-dataset   \n\u2022 Flowers102 [23]: MIT License, https://www.robots.ox.ac.uk/\\~vgg/data/flowers/ 102/index.html.   \n\u2022 Food101 [4]: Non-commercial,https://data.vision.ee.ethz.ch/cvl/datasets_ extra/food-101/   \n\u2022 FGVCAircraft [20]: Non-commercial, https://www.robots.ox.ac.uk/\\~vgg/data/ fgvc-aircraft/.   \n\u2022 SUN397 [42]: Non-commercial, https://vision.princeton.edu/projects/2010/ SUN/.   \n\u2022 DTD [7]: Non-commercial, https://www.robots.ox.ac.uk/\\~vgg/data/dtd/   \n\u2022 EuroSAT [11]: MIT License, https://github.com/phelber/EuroSAT   \n\u2022 UCF101 [33]: Non-commercial,https://www.crcv.ucf.edu/data/UCF101.php   \n\u2022 ImageNet [8]: Non-commercial, http://image-net.org.   \n\u2022 ImageNetV2 [30]: MIT License, https://github.com/modestyachts/ImageNetV2.   \n\u2022 ImageNet-R [12]: MIT License, https://github.com/hendrycks/imagenet-r.   \n\u2022 ImageNet-Sketch [36]: MIT License,https://github.com/HaohanWang/ ImageNet-Sketch.   \n\u2022 ImageNet-A [13]: MIT License, https://github.com/hendrycks/ natural-adv-examples.   \n\u2022 ObjectNet [2]: Creative Commons Attribution 4.0, https://objectnet.dev. ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We point out that existing prompt learning methods can not capture the various image representations and ignore the label bias in CLIP. We propose Frolic to address these problems. Experiments on various datasets demonstrate the effectiveness of our method. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: The limitations have been discussed in Section 5. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We have provided the proof related to our method in Section A. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We have provided the Frolic Algorithm in Algorithm 1 and Algorithm 2, and included the implementation details in Section 4.1. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We have uploaded the codes in supplemental materials. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We have introduced the testing details in Section 4. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: Given the zero-shot models, the process of our method is deterministic.   \nRunning multiple times will not introduce randomness. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have introduced the GPU type to reproduce results in Section 4. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We have discussed the societal impact on Section 5. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our method aims to improve the generalization ability of zero-shot models, which poses no such risks to the best of our knowledge. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have provided the licenses of each dataset in Section C. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 23}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: We do not release new assets. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: We do not involve such experiments. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]