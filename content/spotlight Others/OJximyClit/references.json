{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-18", "reason": "This paper introduces CLIP, a vision-language model that is fundamental to the research presented, serving as the base model for enhancement."}, {"fullname_first_author": "Aditya Krishna Menon", "paper_title": "Long-tail learning via logit adjustment", "publication_date": "2021-06-01", "reason": "This paper details the logit adjustment technique for correcting label bias, a crucial method used in the proposed Frolic framework."}, {"fullname_first_author": "Yuning Lu", "paper_title": "Prompt distribution learning", "publication_date": "2022-06-01", "reason": "This paper introduces prompt distribution learning, which is a key concept and technique employed in Frolic's approach to enhancing zero-shot performance."}, {"fullname_first_author": "Kaiyang Zhou", "paper_title": "Learning to prompt for vision-language models", "publication_date": "2022-09-01", "reason": "This paper explores learning to generate effective prompts, a technique related to the prompt distribution learning that is central to Frolic's design."}, {"fullname_first_author": "James Urquhart Allingham", "paper_title": "A simple zero-shot prompt weighting technique to improve prompt ensembling in text-image models", "publication_date": "2023-07-18", "reason": "This paper provides a relevant baseline method for comparison and demonstrates the use of prompt weighting for zero-shot image classification, a technique that Frolic further develops."}]}