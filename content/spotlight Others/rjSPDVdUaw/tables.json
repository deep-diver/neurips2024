[{"figure_path": "rjSPDVdUaw/tables/tables_7_1.jpg", "caption": "Table 1: Down-stream readout performance from frozen representations. For DINO and VideoMAE v2 baselines we highlight what ViT configuration their encoder is based on: (S) ~20M parameters, (B) ~80M parameters, (G) ~1000M parameters. MooG uses fewer than 35M parameters for encoder, corrector and predictor combined, which suggests a comparison to B-sized models.", "description": "This table presents the performance of different models on three downstream tasks (point tracking, depth estimation, and bounding box detection) using frozen representations.  It compares MooG against several baselines including DINO and VideoMAE v2 with different ViT encoder sizes (small, base, giant), and grid-based versions of MooG.  The table highlights the number of parameters for each model, showing that MooG uses considerably fewer parameters than the larger baselines.", "section": "4.2 End-to-End Training and Quantitative Analysis"}, {"figure_path": "rjSPDVdUaw/tables/tables_8_1.jpg", "caption": "Table 2: Down-stream readout performance trained in an end-to-end manner.", "description": "This table presents the quantitative results of different models on three downstream tasks: point tracking, depth estimation, and object tracking.  The models were trained end-to-end, meaning the readout decoder was trained alongside the main model. The results are shown for three datasets: MOVi-E, Davis, and Waymo, each using different metrics appropriate for the specific task.  The table allows a comparison of MooG against several grid-based baselines (Grid, Grid Rec., DINOv1, DINOv2) highlighting MooG's improved performance.", "section": "4.2 End-to-End Training and Quantitative Analysis"}, {"figure_path": "rjSPDVdUaw/tables/tables_9_1.jpg", "caption": "Table 3: Points comparison. End to end.", "description": "This table presents a comparison of the performance of MooG and other methods on the point tracking task, specifically using the Davis dataset.  The comparison is broken down for sequences of 8 frames and for full sequences. The methods compared include MooG, TAP-Net, and TAPIR.  The metric used is the average Jaccard index (AJ), which measures the accuracy of point tracking, considering both occlusion and positional errors.", "section": "4.2 End-to-End Training and Quantitative Analysis"}, {"figure_path": "rjSPDVdUaw/tables/tables_9_2.jpg", "caption": "Table 1: Down-stream readout performance from frozen representations. For DINO and VideoMAE v2 baselines we highlight what ViT configuration their encoder is based on: (S) ~20M parameters, (B) ~80M parameters, (G) ~1000M parameters. MooG uses fewer than 35M parameters for encoder, corrector and predictor combined, which suggests a comparison to B-sized models.", "description": "The table compares the performance of MooG and several baselines on three downstream tasks: point tracking, depth estimation, and bounding box detection.  The results are presented using metrics appropriate for each task (Average Jaccard for point tracking, Absolute Relative Error for depth, and Intersection over Union for bounding boxes).  The baselines include other self-supervised methods (DINO and VideoMAE) and the capacity of each model is indicated.  Note that the MooG model uses substantially fewer parameters than the larger baselines, suggesting it is more efficient.", "section": "4.2 End-to-End Training and Quantitative Analysis"}, {"figure_path": "rjSPDVdUaw/tables/tables_17_1.jpg", "caption": "Table 1: Down-stream readout performance from frozen representations. For DINO and VideoMAE v2 baselines we highlight what ViT configuration their encoder is based on: (S) ~20M parameters, (B) ~80M parameters, (G) ~1000M parameters. MooG uses fewer than 35M parameters for encoder, corrector and predictor combined, which suggests a comparison to B-sized models.", "description": "This table shows the quantitative results of different models on three downstream tasks: point tracking, depth estimation, and bounding box prediction.  The results are obtained using frozen representations, meaning the pre-trained weights of the models were not updated during the downstream task training.  The table compares MooG against various grid-based baselines (Grid, Grid Rec., DINOv1, DINOv2, VideoMAEv2), highlighting the performance differences across different datasets (Waymo, MOVi-E, DAVIS).  The model sizes are noted for easier comparison.", "section": "4.2 End-to-End Training and Quantitative Analysis"}, {"figure_path": "rjSPDVdUaw/tables/tables_17_2.jpg", "caption": "Table 1: Down-stream readout performance from frozen representations. For DINO and VideoMAE v2 baselines we highlight what ViT configuration their encoder is based on: (S) ~20M parameters, (B) ~80M parameters, (G) ~1000M parameters. MooG uses fewer than 35M parameters for encoder, corrector and predictor combined, which suggests a comparison to B-sized models.", "description": "This table presents the performance of different models on downstream tasks using frozen representations.  It compares MooG against grid-based baselines (Grid, Grid Recurrent, DINOv1, DINOv2, VideoMAEv2) and indicates the size of the Vision Transformer (ViT) encoder used for each baseline. Note that the MooG model is significantly smaller than the largest baselines.", "section": "4.2 End-to-End Training and Quantitative Analysis"}, {"figure_path": "rjSPDVdUaw/tables/tables_17_3.jpg", "caption": "Table 1: Down-stream readout performance from frozen representations. For DINO and VideoMAE v2 baselines we highlight what ViT configuration their encoder is based on: (S) ~20M parameters, (B) ~80M parameters, (G) ~1000M parameters. MooG uses fewer than 35M parameters for encoder, corrector and predictor combined, which suggests a comparison to B-sized models.", "description": "This table presents the quantitative results of different downstream tasks using frozen representations.  It compares the performance of MooG against various grid-based baselines (Grid, Grid Rec, DINOv1, DINOv2, VideoMAE v2) across three datasets (Waymo, MOVi-E, DAVIS).  The performance metrics used vary depending on the task (Average Jaccard for points, Absolute Relative error for depth, and Intersection over Union for bounding boxes).  The table also notes the approximate number of parameters in each model's encoder to provide context for the comparison.", "section": "4.2 End-to-End Training and Quantitative Analysis"}]