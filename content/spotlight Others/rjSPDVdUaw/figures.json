[{"figure_path": "rjSPDVdUaw/figures/figures_2_1.jpg", "caption": "Figure 1: MooG is a recurrent, transformer-based, video representation model that can be unrolled through time. MooG learns a set of \u201coff-the-grid", "description": "This figure illustrates the MooG model architecture. MooG is a recurrent model that processes video frames sequentially, learning a set of \"off-the-grid\" latent representations.  The process involves three main networks: a predictor, which forecasts the next predicted state; a corrector, which encodes the current frame and refines the prediction; and a decoder, which reconstructs the current frame from the predicted state. The model iteratively updates its representation as new frames are observed. This \"off-the-grid\" approach allows the model to track scene elements consistently even when they move across the image plane.", "section": "3 Method"}, {"figure_path": "rjSPDVdUaw/figures/figures_3_1.jpg", "caption": "Figure 1: MooG is a recurrent, transformer-based, video representation model that can be unrolled through time. MooG learns a set of \u201coff-the-grid", "description": "This figure illustrates the architecture of the MooG model, a recurrent transformer-based video representation model.  The model predicts a future state based on the past state and the current observation.  The current observation is encoded and cross-attended to using the predicted state. During training, the model reconstructs the current frame using the predicted state, minimizing the prediction error.  The key innovation is that the learned representation allows tokens to \"move off the grid,\" disentangling the representation structure from the image structure and enabling consistent representation of scene elements even as they move across the image plane through time. The figure shows the process unrolled through time, highlighting the iterative prediction and correction steps.", "section": "3 Method"}, {"figure_path": "rjSPDVdUaw/figures/figures_4_1.jpg", "caption": "Figure 3: Readout decoders overview: for grid-based readouts (e.g. pixels), we use a simple per-frame cross-attention architecture with spatial coordinates as queries, whereas for set-based readouts (points, boxes), we adopt a recurrent readout architecture.", "description": "This figure illustrates the two types of readout decoders used in the MooG model for different downstream tasks. Grid-based readouts, such as for pixel-level predictions (e.g., RGB or depth), utilize a simple per-frame cross-attention mechanism with spatial coordinates as queries. In contrast, set-based readouts, like those for point or box tracking, employ a recurrent architecture to maintain consistency over time.  The recurrent architecture processes sequences of queries, updating latent representations with cross-attention, before finally decoding into the desired outputs (points or boxes).", "section": "3.2 Readout decoders for downstream tasks"}, {"figure_path": "rjSPDVdUaw/figures/figures_5_1.jpg", "caption": "Figure 4: Qualitative analysis of MooG trained on natural videos, shown here are every 4 frames of the original 36 frame long sequence. From top to bottom: Ground truth frames, predicted frames, example MooG token attention map super-imposed on the ground truth frames, example token attention from the recurrent grid-based baseline (see text for details). As can be seen the model is able to predict the next frame well, blurring when there is fast motion or unknown elements enter the scene. The MooG attention map indicates that the visualized token tracks the scene element it binds to across the full range of motion. In contrast, the grid-based token attention map demonstrates how these tokens end up being associated with a specific image location that does not track the scene content. Please see the supplementary material (and website) for other representative examples.", "description": "The figure shows a qualitative comparison of MooG and a grid-based baseline on a video sequence.  It illustrates how MooG's off-the-grid tokens consistently track scene elements through motion, while the grid-based tokens remain fixed to spatial locations. The figure includes ground truth frames, model predictions, and attention maps highlighting the token-element associations for both methods.", "section": "4.1 Self-supervised Training and Qualitative Results"}, {"figure_path": "rjSPDVdUaw/figures/figures_6_1.jpg", "caption": "Figure 4: Qualitative analysis of MooG trained on natural videos, shown here are every 4 frames of the original 36 frame long sequence. From top to bottom: Ground truth frames, predicted frames, example MooG token attention map super-imposed on the ground truth frames, example token attention from the recurrent grid-based baseline (see text for details). As can be seen the model is able to predict the next frame well, blurring when there is fast motion or unknown elements enter the scene. The MooG attention map indicates that the visualized token tracks the scene element it binds to across the full range of motion. In contrast, the grid-based token attention map demonstrates how these tokens end up being associated with a specific image location that does not track the scene content. Please see the supplementary material (and website) for other representative examples.", "description": "This figure shows a qualitative comparison of MooG's performance against a grid-based baseline on a natural video sequence.  It demonstrates that MooG's off-the-grid tokens consistently track scene elements across time, even as they move, unlike grid-based tokens which are tied to fixed spatial locations. The top row displays ground truth frames, the second row shows frames predicted by the model, the third row shows MooG token attention maps overlaid on the ground truth frames, and the bottom row shows the attention maps from a grid-based baseline.", "section": "4.1 Self-supervised Training and Qualitative Results"}, {"figure_path": "rjSPDVdUaw/figures/figures_20_1.jpg", "caption": "Figure 6: We vary several key hyper-parameters of MooG and report results for the end-to-end point tracking setup.", "description": "The left bar chart shows the effect of varying the number of readout layers in the decoder on the end-to-end point tracking task.  The right chart demonstrates the impact of altering the number of tokens in the MooG model on the same task. Both charts display results for the MOVi-E and DAVIS-8 datasets, allowing for a comparison of performance across different model configurations and datasets.", "section": "4.2 End-to-End Training and Quantitative Analysis"}, {"figure_path": "rjSPDVdUaw/figures/figures_21_1.jpg", "caption": "Figure 7: MooG can be instantiated with a different number of tokens at test time without retraining. Because the model architecture is independent of number of tokens and image resolution, we are free to choose the number of tokens used at evaluation time. The model depicted here was trained with 1024 tokens. We instantiate the model with 256, 512 and 1024 tokens. As can be seen the model has no problem in adapting to a different number of tokens, producing good predictions while tokens cover more area in the image as needed.", "description": "This figure shows the flexibility of MooG to adapt to different numbers of tokens.  Three versions of the model, each using a different number of tokens (256, 512, and 1024), were tested on the same video sequence.  The results demonstrate that the model successfully predicts future frames even when the number of tokens is changed, showcasing its adaptability and robustness.  When using fewer tokens, each token represents a larger portion of the image.", "section": "4.1 Self-supervised Training and Qualitative Results"}]