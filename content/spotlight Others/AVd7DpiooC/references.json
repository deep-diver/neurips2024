{"references": [{"fullname_first_author": "Wolfgang Maass", "paper_title": "Networks of spiking neurons: the third generation of neural network models", "publication_date": "1997-00-00", "reason": "This paper is foundational, introducing the concept of Spiking Neural Networks (SNNs) as a third generation of neural network models, providing the theoretical groundwork for the current research on SNNs."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-00-00", "reason": "This paper introduced the Transformer architecture, a key component in the QKFormer model and crucial to modern natural language processing and computer vision."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-00-00", "reason": "This paper demonstrated the effectiveness of the Transformer architecture for image recognition, influencing the design of vision transformers and providing a benchmark for evaluating the QKFormer model."}, {"fullname_first_author": "Zhaokun Zhou", "paper_title": "Spikformer: When spiking neural network meets transformer", "publication_date": "2023-00-00", "reason": "This paper introduced Spikformer, a prior SNN model that uses spiking self-attention, establishing a baseline against which the performance of QKFormer is measured."}, {"fullname_first_author": "Wei Fang", "paper_title": "Deep Residual Learning in Spiking Neural Networks", "publication_date": "2021-00-00", "reason": "This paper introduced techniques for training deeper SNNs, which address a key challenge in the field and are relevant to the training methods employed in QKFormer."}]}