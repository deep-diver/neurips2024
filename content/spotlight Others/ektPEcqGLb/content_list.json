[{"type": "text", "text": "Poisson Variational Autoencoder ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hadi Vafaii1 Dekel Galor1 Jacob L. Yates1 vafaii@berkeley.edu galor@berkeley.edu yates@berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "1UC Berkeley ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Variational autoencoders (VAE) employ Bayesian inference to interpret sensory inputs, mirroring processes that occur in primate vision across both ventral [1] and dorsal [2] pathways. Despite their success, traditional VAEs rely on continuous latent variables, which deviates sharply from the discrete nature of biological neurons. Here, we developed the Poisson VAE ( $\\mathcal{P}$ -VAE), a novel architecture that combines principles of predictive coding with a VAE that encodes inputs into discrete spike counts. Combining Poisson-distributed latent variables with predictive coding introduces a metabolic cost term in the model loss function, suggesting a relationship with sparse coding which we verify empirically. Additionally, we analyze the geometry of learned representations, contrasting the $\\mathcal{P}$ -VAE to alternative VAE models. We find that the $\\mathcal{P}$ -VAE encodes its inputs in relatively higher dimensions, facilitating linear separability of categories in a downstream classification task with a much better $(5\\times)$ sample efficiency. Our work provides an interpretable computational framework to study brain-like sensory processing and paves the way for a deeper understanding of perception as an inferential process. ", "page_idx": 0}, {"type": "image", "img_path": "ektPEcqGLb/tmp/af9822586d8de03b078508297d8245a88e734305e9f034bf318914601ac3ffba.jpg", "img_caption": ["Figure 1: Graphical abstract. Introducing the Poisson Variational Autoencoder ( $\\mathcal{P}$ -VAE), which draws on key concepts in neuroscience. When trained on natural image patches, $\\mathcal{P}$ -VAE with a linear decoder develops Gabor-like feature selectivity, reminiscent of Sparse Coding [3]. In sharp contrast, the standard Gaussian VAE learns the principal components [4]. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The study of artificial neural networks (ANN) and neuroscience has always been closely linked, driving advancements in both fields [5\u201310]. Despite the close proximity of the two fields, most ANN models deviate substantially from biological brains [11, 12]. A major challenge is designing models that not only perform well computationally but also exhibit \u201cbrain-like\u201d structure and function. This is seen both as a goal for improving ANNs [13\u201315], and better understanding biological brains [8, 9, 16\u201319], which has recently been referred to as the neuroconnectionist research programme [20]. ", "page_idx": 1}, {"type": "text", "text": "Drawing from neuroscience, a major guiding idea is that perception is a process of inference [21, 22], where the brain constructs a representation of the external world by inferring the causes of sensory inputs [23\u201326]. This concept is mirrored in \u201cgenerative AI\u201d where models learn the generative process underlying their inputs [27\u201329]. However, in this vein, there is a tension between small well-understood models that are directly inspired by cortex, such as sparse coding [3] and predictive coding [30], and deep generative models that perform well [31\u201334]. ", "page_idx": 1}, {"type": "text", "text": "The variational autoencoder (VAE; [35, 36]) model family is a promising candidate for neuroconnectionist goals for multiple reasons. First, VAEs learn probabilistic generative models of their inputs and are grounded in Bayesian probability theory, providing a solid theoretical foundation that directly incorporates the concept of perceptual inference [10, 22]. Second, the VAE model family, specifically hierarchical VAEs, is broad with other generative models, such as diffusion models, understood as special cases of hierarchical VAEs [37\u201339]. Finally, VAEs learn representations that are similar to cortex [1, 2, 40], exhibit cortex-like topographic organization [41, 42], and make perceptual errors that mimic those of humans [43], indicating a significant degree of neural, organizational, and psychophysical alignment with the brain. ", "page_idx": 1}, {"type": "text", "text": "However, standard VAEs diverge from brains in the way they encode information. Biological neurons fire all-or-none action potentials [44], and are thought to represent information via firing rate [45\u201349]. These firing rates must be positive and generate discrete \u201cspike\u201d counts, which exhibit conditionally Poisson-like statistics in small counting windows [49\u201351]. In contrast, VAEs are typically parameterized with real-valued, continuous, Gaussian distributions [52]. ", "page_idx": 1}, {"type": "text", "text": "Contributions. In this work, we address this discrepancy by introducing the Poisson Variational Autoencoder $\\mathcal{P}$ -VAE), a novel architecture that combines perceptual inference with two other inspirations from neuroscience (Fig. 1). First, that information is encoded in the rates of discrete spike counts, which are approximately Poisson-distributed on short time intervals. And second, that feedforward connections encode deviations from expectations contained in feedback connections (Fig. 2a; [30, 53]). We introduce a reparameterization trick for Poisson samples (Algorithm 1), and derive the evidence lower bound (ELBO) objective for the $\\mathcal{P}$ -VAE (eq. (3)). Overall, we believe $\\mathcal{P}$ -VAE introduces a promising new model at the intersection of computational neuroscience and machine learning that offers several appealing features over existing VAE architectures: ", "page_idx": 1}, {"type": "text", "text": "\u2022 The $\\mathcal{P}$ -VAE loss derivation (eq. (3)) naturally results in a metabolic cost term that penalizes high firing rates, such that $\\mathcal{P}$ -VAE with a linear decoder implements amortized sparse coding (Fig. 2b). We validate this prediction empirically. \u2022 $\\mathcal{P}$ -VAE largely avoids the prevalent posterior collapse issue, maintaining many more active latents compared to alternative VAE models (Table 1), especially the continuous ones. \u2022 $\\mathcal{P}$ -VAE encodes its inputs in relatively higher dimensions, facilitating linear separability of categories in a downstream classification task with a much better $(5\\!\\times\\!)$ sample efficiency. ", "page_idx": 1}, {"type": "text", "text": "We evaluate these results on two natural image datasets and MNIST. The $\\mathcal{P}$ -VAE paves the way for the future development of interpretable hierarchical models that perform \u201cbrain-like\u201d inference. ", "page_idx": 1}, {"type": "text", "text": "2 Background & Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Perception as inference: connections to neuroscience and machine learning. A centuries-old idea [21, 22], \u201cperception as inference\u201d argues that coherent perception of the world results from the unconscious inference over the causes of the senses. In other words, the brain learns a generative model of the sensory inputs. This has led to fruitful theoretical work in neuroscience [23, 54\u201356] and machine learning [57, 58], including VAEs [52]. See Marino [10] for a review. ", "page_idx": 1}, {"type": "text", "text": "Efficient, predictive, and sparse coding. Another longstanding idea in neuroscience is that brains are adapted to the statistics of the environment. Efficient coding states that brains represent as much information about the environment as possible while minimizing neural resource use [59, 60]. ", "page_idx": 2}, {"type": "text", "text": "Predictive coding [30, 61, 62] postulates that the brain generates a statistical prediction of its inputs, with feedforward networks carrying only the prediction errors or unexplained information [63]. More recently, ANNs based on predictive coding have been shown to capture a wide range of phenomena in biological neurons across the visual system [64, 65]. More broadly, prediction in time has emerged as an objective that lends itself to brain-like representations [66, 67]. ", "page_idx": 2}, {"type": "text", "text": "Sparse coding (SC) is directly inspired by efficient coding, aiming to explain inputs as sparsely as possible [47, 68]. SC was the first unsupervised model to learn representations closely resembling the receptive fields of V1 neurons [3] and predicts an array of empirical features of neural activity [69\u201379]. SC is formalized with a generative model where neural activations $_{\\textit{z}}$ are sampled from a sparsity-inducing prior, $z\\sim p(z)$ , and the input image $\\textbf{\\em x}$ is reconstructed as a linear combination of basis vectors $\\Phi$ , plus additive Gaussian noise, ${\\hat{\\pmb x}}={\\Phi}z+{\\varepsilon}$ . The SC loss is as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{SparseCoding}}\\left(\\pmb{x};\\pmb{\\Phi},\\pmb{z}\\right)=\\left\\|\\pmb{x}-\\pmb{\\Phi}\\pmb{z}\\right\\|_{2}^{2}+\\beta\\left\\|\\pmb{z}\\right\\|_{1}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Commonly used algorithms for sparse coding include the locally competitive algorithm (LCA; [80]), which is a biologically plausible algorithm to optimize eq. (1), and iterative shrinkage-thresholding algorithm (ISTA; [81, 82]), which has shown robust performance in learning sparse codes given a fixed dictionary $\\Phi$ . ", "page_idx": 2}, {"type": "text", "text": "VAE objective. VAEs define a probabilistic generative model $p(\\pmb{x},z)$ , where $\\textbf{\\em x}$ denotes the observed data and $_{\\textit{z}}$ are some latent variables. The generative process samples $_{\\textit{z}}$ from a prior distribution $p(z)$ and then generates the observed data $\\textbf{\\em x}$ from the conditional distribution $p_{\\pmb{\\theta}}(\\bar{\\bf x}|\\pmb{z})$ , also known as the \u201cdecoder\u201d. The \u201cencoder\u201d, $q_{\\phi}(z|x)$ , performs approximate inference on the inputs. Model parameters are learned by maximizing the evidence lower bound (ELBO) objective, which is derived from variational inference (see appendix A for the full set of derivations). The ELBO is given by: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log p({\\pmb x})\\geq\\mathbb{E}_{q_{\\phi}({\\pmb z}\\mid{\\pmb x})}\\left[\\log p_{\\theta}({\\pmb x}\\mid{\\pmb z})\\right]-\\mathcal{D}_{\\mathrm{KL}}\\Big(q_{\\phi}({\\pmb z}\\mid{\\pmb x})\\,\\|\\,p({\\pmb z})\\Big)=\\mathcal{L}_{\\mathrm{VAE}}({\\pmb x};\\theta,\\phi).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The first term captures the reconstruction performance of the decoder, and the second term, the $^{\\bullet\\bullet}\\mathrm{KL}$ term,\u201d captures the divergence of the approximate posterior from the prior. ", "page_idx": 2}, {"type": "text", "text": "The specific form of these distributions is up to the practitioner. In standard VAEs, factorized Gaussians are typically used: $q=\\mathcal{N}(z;\\pmb{\\mu}(\\pmb{x}),\\bar{\\pmb{\\sigma}}^{2}(\\pmb{x}))$ and $p=\\mathcal{N}(z;\\mathbf{0},\\mathbf{1})$ . The likelihood, $p_{\\theta}(x|z)$ , is also typically modeled as a Gaussian conditioned on a parameterized neural network $\\operatorname{dec}_{\\theta}(z)$ . ", "page_idx": 2}, {"type": "text", "text": "Amortized inference in VAEs. A major contribution of VAEs is the idea of amortizing inference over the latents $_{\\textit{z}}$ with a black box ANN [83, 84]. \u201cAmortized\u201d inference borrows a term from finance to capture the idea of spreading out costs\u2014here, the cost of performing inference over multiple samples. In amortized inference, a neural network learns (during training) how to map a data sample to a distribution over latent variables given the sample. The cost is paid during training, but the trained model can then be used to perform inference on future samples efficiently. It has been argued that the brain performs amortized inference for computational efficiency [85]. ", "page_idx": 2}, {"type": "text", "text": "VAEs connection to biology. VAEs have been shown to contain individual latents that resemble neurons, capturing a wide range of the phenomena observed in visual cortical areas [40] and human perceptual judgments [43]. Like many other ANN models [86, 87], VAEs have been found to learn representations that are predictive of single-neuron activity in both the ventral [1] and dorsal [2] streams. However, unlike most ANNs, the mapping from certain VAEs to neural activity is incredibly sparse, even one-to-one in some cases [1, 2]. ", "page_idx": 2}, {"type": "text", "text": "Discrete VAEs. VAEs with discrete latent spaces, such as VQ-VAE [88] and Categorical VAE [89], are designed to capture complex data structures by mapping inputs to a finite set of latent variables. Unlike traditional VAEs that use continuous latent spaces, these models leverage discrete representations to enhance interpretability and can yield high performance with lower capacity [90]. ", "page_idx": 2}, {"type": "table", "img_path": "ektPEcqGLb/tmp/22ba8b744415aebad3c0960e425816ffa0b3d48229732298342bfb7515be6d11.jpg", "table_caption": ["Algorithm 1 Reparameterized sampling (rsample) for Poisson distribution. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "VAEs connection to sparse coding. Previous work has attempted to connect sparse coding and VAEs directly [91\u201393], with each approaching the problem differently. Geadah et al. [91] introduced sparsity-inducing priors (such as Laplace or Cauchy) and a linear decoder with an overcomplete latent space. Tonolini et al. [92] introduced a spike and slab prior into a modified ELBO, and Xiao et al. [93] added a sparse coding layer learned by ISTA to the latent space of a VQ-VAE. Notably, none of the three ended up minimizing the sparse coding loss. Two of the three maintain the linear generative model with an overcomplete latent space, but the ELBO in both requires an additional approximation step for the KL term [91, 92]. ", "page_idx": 3}, {"type": "text", "text": "3 Poisson Variational Autoencoder ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our main contribution is integrating Poisson-distributed latents into VAEs, where both the approximate posterior and the prior are parameterized as Poisson distributions. Critically, the latents $_{z}$ are no longer continuous variables, but rather they are discrete spike counts. To perform inference over discrete latents, we introduce a Poisson reparameterization trick. We then derive the KL term and obtain the full $\\mathcal{P}$ -VAE objective. ", "page_idx": 3}, {"type": "image", "img_path": "ektPEcqGLb/tmp/7d7b59a454f14a0e5d50a08d9d0c229ce95d3821fefcd555c495135af8b13a59.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Poisson reparameterization trick. For a homogeneous Poisson process [94\u201396], given a window size $\\Delta t=1$ , and rate $\\lambda$ , we can generate Poisson distributed counts by drawing randomly distributed wait-times from an exponential distribution with mean $1/\\bar{\\lambda}$ and counting all events where the cumulative time is less than 1. Because the exponential distribution is trivially reparameterized [35], and PyTorch contains an implementation [97], we need only to approximate the hard threshold for comparing cumulative wait times with the window size. We accomplish this by replacing the indicator function with a sigmoid as in refs. [89, 98]. ", "page_idx": 3}, {"type": "text", "text": "Figure 2: (a) Model architecture. Colored shapes indicate learnable model parameters, including the prior firing rates, $_r$ . We color code the model\u2019s inference and generative components using red and blue, respectively. The $\\mathcal{P}$ -VAE encodes its inputs in discrete spike counts, $_{z}$ , significantly enhancing its biological realism. (b) \u201cAmortized Sparse Coding\u201d is a special case within the $\\mathcal{P}$ -VAE model family: it\u2019s a $\\mathcal{P}$ -VAE with a linear decoder and an overcomplete latent space. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 demonstrates the steps: Given a matrix of rates $\\lambda$ , sample $n\\_\\mathrm{exp}$ wait times $t_{1},t_{2},...t_{n_{-}\\mathrm{exp}}$ for each element of $\\lambda$ by sampling from an exponential distribution with mean $1/\\lambda(i)$ . We then calculate the cumulative event times $\\begin{array}{r}{S(n\\mathrm{-exp})=\\sum_{j=1}^{n\\mathrm{-exp}}t_{j}}\\end{array}$ , pass them through a sigmoid $\\scriptstyle\\sigma\\left({\\frac{1-S}{\\mathrm{temperature}}}\\right)$ , and sum over samples to get event counts, $_{z}$ . The temperature controls the sharpness of the thresholding. We adaptively scale the number of samples, $n\\_\\mathrm{exp}$ , by keeping track of the maximum rate in each batch, $\\lambda_{\\operatorname*{max}}$ , and then use the inverse cumulative density function (cdf) for Poisson to find the number of samples, $n\\_\\mathrm{exp}$ , such that c $\\mathrm{df}(n_{-}\\mathrm{exp};\\lambda_{\\mathrm{max}})=0.99999$ . ", "page_idx": 4}, {"type": "text", "text": "At non-zero temperatures, our parameterization algorithm provides a continuous relaxation of the Poisson distribution. Figure 3 shows histograms of samples drawn using Algorithm 1 for rate $\\lambda\\ =\\ 1$ and temperatures ", "page_idx": 4}, {"type": "image", "img_path": "ektPEcqGLb/tmp/9096e8214f099a97d11a2f16195bf9beea5fb8144a79f20bf41a7cbcdef6602b.jpg", "img_caption": ["Figure 3: Relaxed Poisson distribution. Samples are drawn using Algorithm 1 for $\\lambda\\,=\\,1$ . At non-zero temperatures, samples are non-integer, but approach the true Poisson distribution as $T\\rightarrow0$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "$T=1.0,0.1,0.01$ , and 0. The latter case $T=0$ , true Poisson) is equivalent to torch.poisson(). ", "page_idx": 4}, {"type": "text", "text": "$\\mathcal{P}$ -VAE architecture. The architecture of $\\mathcal{P}$ -VAE captures the interactions between feedforward and feedback connections that are present in all visual cortical areas [99]. Feedforward areas carry sensory information and feedback connections are thought to carry modulatory signals such as attention [53] or prediction [30], which interact multiplicatively with feedforward inputs [53, 100]. ", "page_idx": 4}, {"type": "text", "text": "$\\mathcal{P}$ -VAE embodies this idea by having the posterior rates depend on the prior, such that $r_{\\mathrm{prior}}=r$ and $r_{\\mathrm{post.}}=r\\odot\\delta r(\\mathbf{x})$ , where $\\odot$ is the Hadamard (element-wise) product. The prior rates, $r\\in\\mathbb{R}^{K}$ , are learnable parameters that capture expectations about the statistics of the input. The encoder outputs, $\\delta r(\\pmb{x})\\,\\in\\,\\mathbf{\\dot{R}}^{K}$ , capture deviations from the prior. Thus, $\\mathcal{P}$ -VAE models the interaction between prior expectations, and deviations from them, in a multiplicative and symmetric way. This results in a posterior, $q(z|\\pmb{x})=\\mathcal{P}\\mathrm{ois}(z;r\\odot\\delta r(\\pmb{x}))$ , and prior, $p(z)=\\mathcal{P}\\mathrm{ois}(z;r)$ , where $_{\\textit{z}}$ is the spike count variable. Notably, this multiplicative relationship is maximally general, as any pair of positive variables, $r_{\\mathrm{prior}}$ , and $r_{\\mathrm{post.}}$ . can be expressed as a base variable, $r:=r_{\\mathrm{prior}}$ , multiplied by their relative ratio, $\\delta r:=r_{\\mathrm{post.}}/r$ . The general model architecture is shown in Fig. 2a. ", "page_idx": 4}, {"type": "text", "text": "$\\mathcal{P}$ -VAE loss function. For a comprehensive derivation of the $\\mathcal{P}$ -VAE objective, see appendix A. Here, we report the final result: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\boxed{\\mathcal{L}_{\\mathrm{PVAE}}=\\mathbb{E}_{z\\sim\\mathcal{P}\\mathrm{ois}(z;r\\odot\\delta r)}\\left[\\left\\|\\pmb{x}-\\mathrm{dec}(\\pmb{z})\\right\\|_{2}^{2}\\right]+\\sum_{i=1}^{K}r_{i}f(\\delta r_{i}),}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\operatorname*{dec}(\\cdot)$ is the decoder neural network, and $f(y):=1-y+y\\log y$ (see supplementary Fig. 6). ", "page_idx": 4}, {"type": "text", "text": "$\\mathcal{P}$ -VAE relationship to sparse coding. The KL term in eq. (3) penalizes firing rates. Both $r$ and $\\delta r$ are positive by definition, and $f(y)\\geq0$ , strongly resembling the sparsity penalty in Olshausen and Field [3]. To make this connection more explicit, we make two additional assumptions (Fig. 2b): ", "page_idx": 4}, {"type": "text", "text": "1. The decoder is a linear generative model: x\u02c6 = \u03a6z, with x \u2208RM and \u03a6 \u2208RM\u00d7K.   \n2. The latent space is overcomplete: $K>M$ . ", "page_idx": 4}, {"type": "text", "text": "Because both $\\mathbb{E}_{z\\sim\\mathcal{P}\\mathrm{ois}(z;\\lambda)}[z_{i}]$ and $\\mathbb{E}_{z\\sim\\mathcal{P}\\mathrm{ois}(z;\\lambda)}\\big[z_{i}z_{j}\\big]$ have closed-form solutions (eq. (21)), the reconstruction term in eq. (3) can be computed analytically for a linear decoder, resulting in: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\boxed{\\mathcal{L}_{\\mathrm{SC-PVAE}}\\left(x;\\delta r,r,\\Phi\\right)=\\left\\|x-\\Phi\\lambda\\right\\|_{2}^{2}+\\lambda^{T}\\mathrm{diag}(\\Phi^{T}\\Phi)+\\beta\\sum_{i=1}^{K}r_{i}f(\\delta r_{i}).}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda=r\\odot\\delta r({\\pmb x})$ are the posterior firing rates, $f(y)$ is defined as above, and $\\beta$ is a hyperparameter that scales the contribution of the KL term, and changes the sparsity penalty for the $\\mathcal{P}$ -VAE. ", "page_idx": 4}, {"type": "table", "img_path": "ektPEcqGLb/tmp/4158ebfa5694e73770fbfa06bc9deb9f0e993088abd7275346d941e4e6135b8e.jpg", "table_caption": ["Table 1: Models considered in this paper. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "The relationship between the linear $\\mathcal{P}$ -VAE loss (eq. (4)) and the sparse coding loss (eq. (1)) can now be seen. Both contain a term that minimizes the squared error of the reconstruction and a term (two terms for $\\mathcal{P}$ -VAE) that penalizes non-zero firing rates. Unlike prior work that directly implemented amortized sparse coding [91, 92], here the activity penalty naturally emerges from the derivations, and the only additional assumption was an overcomplete linear generative model. The inference is accomplished using a parameterized feed-forward neural network, $\\delta r(x)$ , thus, it is amortized. We call this specific case of $\\mathcal{P}$ -VAE \u201cAmortized Sparse Coding\u201d (Fig. 2b). ", "page_idx": 5}, {"type": "text", "text": "Note that a closed-form derivation of the reconstruction term is possible for any VAE with a linear decoder and a generating distribution that has a mean and variance (see eq. (22)). ", "page_idx": 5}, {"type": "text", "text": "This closed-form expression of the loss given a linear decoder is useful because we can see how different parameters contribute to the loss. Furthermore, we can compute gradients of the whole loss exactly, and use this to evaluate our Poisson reparameterization. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To evaluate the $\\mathcal{P}$ -VAE, we perform three sets of experiments. First, we utilize the theoretical results for a linear decoder (eqs. (4) and (22)) to test the effectiveness of our reparameterization algorithm. We compare to alternative VAE models with established reparameterization tricks (e.g., Gaussian). ", "page_idx": 5}, {"type": "text", "text": "Second, to confirm $\\mathcal{P}$ -VAE with a linear decoder not only resembles amortized sparse coding, but practically performs like sparse coding, we compare to standard and well-established sparse coding algorithms such as the locally competitive algorithm (LCA; [80]) and the widely-used iterative shrinkage-thresholding algorithm (ISTA; [81, 82]) to see if $\\mathcal{P}$ -VAE reproduces their results. ", "page_idx": 5}, {"type": "text", "text": "Third, we test the $\\mathcal{P}$ -VAE in a generic representation learning context and evaluate the geometry of learned representations for downstream tasks. For these experiments, both the encoder and decoder\u2019s architecture is a ResNet (see appendix B for full architecture and training details). ", "page_idx": 5}, {"type": "text", "text": "Alternative models. We compare $\\mathcal{P}$ -VAE to both discrete and continuous VAEs (Table 1). Other than the traditional Gaussian, we compare to Laplace-distributed VAEs because previous work found the Laplace distribution supported robust sparse representations [40, 91]. Additionally, we compare to Categorical VAEs, trained using the Gumbel-Softmax trick [89, 98]. We use PyTorch\u2019s implementation which is based on Maddison et al. [98]. ", "page_idx": 5}, {"type": "text", "text": "Finally, we test models where latents are Gaussian passed through an activation function before passing to the decoder. We call these models $\\mathcal{G}{\\mathrm{-}}\\mathrm{VAE}_{\\mathrm{+act}}$ , where act $\\in\\{\\mathrm{relu,exp}\\}$ , but they capture other families of distributions (truncated Gaussian and log-normal). We include these to test the hypothesis that positive constraints (and not discrete latents) are the key contribution of Poisson [101]. ", "page_idx": 5}, {"type": "text", "text": "Datasets. For sparse coding results, we use 101 natural images from the van Hateren dataset [102]. We tile the images to extract $16\\times16$ patches and apply whitening and contrast normalization, as is typically done in sparse coding literature [3, 103]. To test the generalizability of our sparse coding results, we repeat these steps on CIFAR10 [104], a dataset we call $\\mathrm{CIFAR}_{16\\times16}$ . For the general representation learning results, we use MNIST. See appendix B for additional details. ", "page_idx": 5}, {"type": "text", "text": "Statistical tests. In the VAE literature, it is known that random seeds can have a large effect compared to architecture or regularization [105]. Therefore, we train each configuration using 5 different random initializations. We report $99\\%$ confidence intervals throughout, and perform paired $t$ -tests, reporting significance for $p<0.01$ (FDR corrected using the Benjamini-Hochberg method). ", "page_idx": 5}, {"type": "image", "img_path": "ektPEcqGLb/tmp/b0d9e152bda1b8f572cac77bb2ac9f81cabf743e199da894ddfa2c27390c2aaf.jpg", "img_caption": ["Figure 4: Learned basis elements, $K\\;=\\;512$ total, each made of $16\\,\\times16\\,=\\,256$ pixels (i.e., $\\bar{\\Phi}^{^{\\prime}}\\in\\mathbb{R}^{256\\times512},$ ). These results are from fully linear VAEs (both the encoder and decoder were linear). Features are ordered from top-left to bottom-right, in ascending order of their associated KL divergence $\\mathcal{P}$ -VAE, $\\mathcal{G}$ -VAE, $\\mathcal{L}$ -VAE), or the magnitude of posterior logits ( $\\mathcal{C}$ -VAE). The sparse coding results (LCA and ISTA) are ordered randomly. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Training details. Because we considered a variety of architectures, training time is variable. We trained 195 VAE models, $n=5$ seeds each, resulting in a total of $195\\times5=975$ VAEs. For sparse coding models, we ran ISTA [81, 82] and LCA [80] with 270 hyperparameter combinations each. See appendix B for more details. Training all models took roughly a week on 8 RTX 6000 Ada GPUs. ", "page_idx": 6}, {"type": "text", "text": "Evaluating Poisson reparameterization. $\\mathcal{P}$ -VAE with a linear decoder has a closed form solution eq. (4), which lets us evaluate how well our reparameterized gradients perform compared to the exact ones. We compare our results to the gold-standard Gaussian as well as Categorical and Laplace VAEs. Table 4 shows the results. Monte-Carlo sampling with Poisson reparameterization closely matches exact inference just like established methods for Gaussian and Laplace. In contrast, the straight-through (ST; [106]) estimator performs poorly (see also supplementary Fig. 7). ", "page_idx": 6}, {"type": "text", "text": "The $\\mathcal{P}$ -VAE learns basis vectors similar to those from sparse coding. A major result from sparse coding is that it learns basis vectors (dictionaries) that resemble the \u201cGabor-like\u201d receptive fields of cortical neurons [3]. Inspecting the dictionaries learned by different models demonstrates this is not trivial (Fig. 4). As expected from theoretical results [4], $\\mathcal{G}$ -VAE (top left) learn probabilistic PCA, but with many noisy elements. As demonstrated previously [40, 91], $\\mathcal{L}$ -VAE (lower left) learn Gabor-like elements. However, there are a large number of noisy basis vectors. It is of note that previous work did not show complete dictionaries for their results with Laplace priors [40, 91]. In contrast, $\\mathcal{P}$ -VAE (top middle) learns Gabor-like fliters that cover space, orientation, and spatial frequency. The quality is comparable to sparse coding dictionaries learned with LCA/ISTA (top/lower right panels). $\\mathcal{C}$ -VAE also learns Gabors, although there are significantly more noisy basis elements. ", "page_idx": 6}, {"type": "text", "text": "The $\\mathcal{P}$ -VAE avoids posterior collapse. A striking feature of Fig. 4 is the sheer number of noisy basis vectors for both continuous VAEs ( $\\mathcal{G}$ -VAE, $\\mathcal{L}$ -VAE). We suspected this reflected dead neurons with vanishing KL, which is indicative of a collapsed latent dimension that\u2019s no longer encoding information. To quantify this, we binned the distribution of KL values and thresholded the resulting distribution at discontinuous points (see supplemental Fig. 8). Table 2 shows the results of this analysis for all VAEs with valid KL terms. Across all datasets, both continuous VAEs suffered from large numbers of dead neurons, whereas $\\mathcal{P}$ -VAE largely avoided this problem. On both natural image datasets, $\\mathcal{P}$ -VAE had $\\sim\\!2\\%$ dead neurons compared to $\\sim\\!80\\%$ for $\\mathcal{G}$ -VAE and $\\mathcal{L}$ -VAE. Having a more expressive encoder slightly increases this percentage, but a dramatic difference between $\\mathcal{P}$ -VAE and continuous VAEs ( $\\mathcal{G}$ -VAE, $\\mathcal{L}$ -VAE) persists. ", "page_idx": 6}, {"type": "text", "text": "The $\\mathcal{P}$ -VAE learns sparse representations. To quantify whether $\\mathcal{P}$ -VAE learns sparse representations, we compared our VAE models to sparse coding trained with LCA and ISTA and quantified the lifetime sparsity [69]. The lifetime sparsity of the $j$ -th latent is: ", "page_idx": 6}, {"type": "table", "img_path": "ektPEcqGLb/tmp/7aee25a690ec8dde813789669ccdf0b2f53c857c6e2bb355c0e4f50c0f47d047.jpg", "table_caption": ["Table 2: Proportion of active neurons. All models considered in this table had a latent dimensionality of $K=512$ . The decoders were linear, and the encoders were either linear or convolutional. "], "table_footnote": [], "page_idx": 7}, {"type": "equation", "text": "$$\ns_{j}=\\left(1-\\frac{1}{N}\\right)^{-1}\\left(1-\\frac{1}{N}\\frac{(\\sum_{i}z_{i j})^{2}}{\\sum_{i}z_{i j}^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $N$ is the number of images, and $z_{i j}$ is sampled from the posterior for the $i$ -th image. Intuitively, $s_{j}=1$ whenever neuron $j$ responds to a single stimulus out of the entire set (highly selective). In contrast, $s_{j}=0$ whenever the neuron responds equally well to all stimuli indiscriminately. ", "page_idx": 7}, {"type": "text", "text": "Fig. 5a shows the reconstruction performance (MSE) compared to lifetime sparsity (s, eq. (5)) for all VAEs. The $\\mathcal{G}$ -VAE finds good reconstructions $(\\mathrm{MSE}=71.49)\\$ ) but with low sparsity $s=0.37\\$ ). The $\\mathcal{P}$ -VAE finds much sparser solutions $s=0.94\\$ ), but at a cost of reconstruction quality $(\\mathrm{MSE}=$ 102.68). Because the $\\mathcal{P}$ -VAE KL term explicitly penalizes rate (eq. (3)), we explored different $\\beta$ values for $\\mathcal{P}$ -VAE with a linear encoder and decoder (Fig. 5a, blue curve). This maps out a ratedistortion curve, allowing us to compare what level of sparsity $\\mathcal{P}$ -VAE matches $\\mathcal{G}$ -VAE performance. Even with a simpler (linear) encoder, $\\mathcal{P}$ -VAE matches $\\mathcal{G}$ -VAE in performance with a $1.6\\times$ sparser solution (at $\\beta=0.6)$ . The addition of a relu activation to $\\mathcal{G}$ -VAE increased the sparsity $s=0.69)$ ). By comparing the $\\mathcal{P}$ -VAE with a linear encoder to $\\mathcal{P}$ -VAE with a convolutional encoder, we find that increasing decoder complexity for the same $\\beta=1$ maintains the same MSE but increases sparsity (blue open circle), suggesting amortization quality can significantly shift this curve [33, 107, 108]. ", "page_idx": 7}, {"type": "image", "img_path": "ektPEcqGLb/tmp/da4a3fefc12be5005c061a69fe599d20b47eb228c595e6435a7b89858b8d97e3.jpg", "img_caption": ["Figure 5: Reconstruction performance versus sparsity of representations. (a) Results for VAE model family. The curve is sigmoid fit to a $\\mathcal{P}$ -VAE with both linear encoder and decoder, and varying $\\beta$ values (this is $\\beta$ from eq. (4)). (b) Amortization gap of $\\mathcal{P}$ -VAE (blue open circle) compared to sparse coding (LCA/ISTA). Solid points are obtained from LCA inference applied to the $\\mathcal{P}$ -VAE basis vectors with different sparsity levels $\\mathrm{\\Delta\\beta_{LCA}}$ is the one from eq. (1)). The curve is a sigmoid fit. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Does $\\mathcal{P}$ -VAE match the performance of traditional sparse coding trained with LCA or ISTA? Figure 5b compares $\\mathcal{P}$ -VAE to sparse coding models that were trained using a wide range of hyperparameters, and the best models were selected for each class (appendix B). $\\mathcal{P}$ -VAE achieves a similar sparsity to LCA and ISTA $\\mathit{\\Pi}_{s}\\,=\\,0.94,0.91\\$ , and 0.96, respectively), but the best LCA model drastically outperforms $\\mathcal{P}$ -VAE on MSE for similar levels of sparsity. This suggests our convolutional encoder is struggling to close the amortization gap. To test this hypothesis, we performed LCA inference on basis elements learned by $\\mathcal{P}$ -VAE (Fig. 5b curve/solid points). We explored a range of hyperparameters to determine whether the MSE improved for similar sparsity levels. Indeed, LCA inference using $\\mathcal{P}$ -VAE dictionary, was able to nearly match the performance of sparse coding LCA for similar levels of sparsity. This confirms our hypothesis that a large amortization gap remains for the specific encoder architectures we tested, highlighting the need for improved inference algorithms/architectures [108]. ", "page_idx": 7}, {"type": "table", "img_path": "ektPEcqGLb/tmp/da6c6f626f5ddc2a6abfb8bb56bcba1182021a49434f1e2f9e4fa7e2ef8f5779.jpg", "table_caption": ["Table 3: Geometry of representations $K=10$ only; see Table 5 for the full set of results). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "The $\\mathcal{P}$ -VAE is more sample efficient in downstream tasks. We test the unsupervised learned representations on a downstream classification task. We first trained all VAE models with $K=10$ dimensional latent space and a convolutional encoder and decoder on the MNIST training set (see supplementary Fig. 9 for the quality of generated samples and reconstruction performance). Next, we used the trained encoders to extract representations and evaluated their performance on classifying MNIST digits. We define VAE representation to be the output of the encoder. Following conventions in the VAE literature [105], we used the mean vectors $\\pmb{\\mu}$ for continuous VAEs $\\mathcal{G}$ -VAE, $\\mathcal{L}$ -VAE). For the $\\mathcal{P}$ -VAE, we used $\\log\\delta r$ , and for the $\\mathcal{C}$ -VAE, we used logits. ", "page_idx": 8}, {"type": "text", "text": "We split the MNIST validation set into two 5,000 sample sets, used as train/test sets for this task. We train a K-nearest neighbors (KNN) classifier with a varying number of limited supervised samples $(N=200,1000,5000)$ drawn without replacement from the first set (train), to measure classification accuracy on the withheld set (test). KNN is nonparametric, and its performance is directly influenced by the geometry of representations by explicitly capturing the distance between encoded samples [109]. We find that using only $N\\,=\\,200$ samples, $\\mathcal{P}$ -VAE achieves $\\sim82\\%$ accuracy in held out data; whereas, $\\mathcal{G}$ -VAE achieves the same level of accuracy at $N=1000$ samples (Table 3). By this measure, $\\mathcal{P}$ -VAE is $5\\times$ more sample efficient. But from Alleman et al. [110], we know that the choice of activation function changes the geometry of learned representations. Therefore, we also tested $\\mathcal{G}$ -VAE models with an activation function (relu and exp) applied to latents after sampling from the posterior. This biological constraint improved $\\mathcal{G}$ -VAE, but it still underperformed $\\mathcal{P}$ -VAE (Table 3). We also found this result held for higher dimensional latent spaces (supplementary Table 5). ", "page_idx": 8}, {"type": "text", "text": "The $\\mathcal{P}$ -VAE learns representations with higher dimensional geometry. The preceding results are indicative of substantial differences in the geometry of the representations learned by $\\mathcal{P}$ -VAE compared to other VAE families (Table 3). To test this more explicitly, we calculated the \u201cshattering dimensionality\u201d of the latent space [111\u2013113]. Shattering dim measures the average accuracy over all possible pairwise classification tasks. This is called \u201cshattering\u201d because if the model shatters data points around into a high dimensional space, they will become more linearly separable. For MNIST with 10 classes, there are $\\binom{10}{5}=252$ possible classifications. We trained logistic regression on the entire training set to classify each of the 252 arbitrary splits and measured the average performance on the entire validation set. The far right column of Table 3 shows the measured shattering dims. For $K=10$ , the shattering dim was significantly higher for discrete VAEs $\\mathcal{P}$ -VAE, $\\mathcal{C}$ -VAE). For higher dimensional latent spaces $\\mathcal{P}$ -VAE strongly outperformed alternative models (Table 5). ", "page_idx": 8}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we describe the $\\mathcal{P}$ -VAE, which performs posterior inference encoded in discrete spike counts. We introduce a Poisson reparameterization trick and derive the ELBO for Poisson-distributed VAEs. The $\\mathcal{P}$ -VAE loss results in a KL term that penalizes firing rates like sparse coding [3]. We show that $\\mathcal{P}$ -VAE with a linear generative model reduces to amortized sparse coding. ", "page_idx": 8}, {"type": "text", "text": "When trained on natural image patches, $\\mathcal{P}$ -VAE with a linear decoder learns sparse solutions with Gabor-like basis vectors resembling sparse coding, both in the form of the learned basis as well as the lifetime sparsity of the latents. We evaluated the representations on downstream classification tasks and found that $\\mathcal{P}$ -VAE encodes its inputs in a higher dimensional space that enabled good linear separability between classes. ", "page_idx": 8}, {"type": "text", "text": "Limitations and future directions. $\\mathcal{P}$ -VAE samples Poisson latents. Although this is inspired by the statistics of spike counts in the brain over short time intervals [50], there are deviations from Poisson throughout the cortex over longer time windows [51]. Extending $\\mathcal{P}$ -VAE to hierarchical architectures [33] will make the latents conditionally Poisson, but not marginally Poisson (as they are modulated by top-down rates). Further extensions could implement doubly-stochastic spike generation [51, 114]. A second limitation is the amortization gap we observed between our current implementation of $\\mathcal{P}$ -VAE and traditional sparse coding. This could likely be closed with more expressive encoders [115] or through iterative inference [116], but it is an open area of research [108]. ", "page_idx": 9}, {"type": "text", "text": "Overall, the $\\mathcal{P}$ -VAE is a promising step towards learning brain-like representations in deep hierarchical generative models. ", "page_idx": 9}, {"type": "text", "text": "6 Code & Data ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our code, data, and model checkpoints are available here: https://github.com/hadivafaii/PoissonVAE. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Institute of Health under award number NEI EY032179. Additionally, this material is based upon work supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE-1752814 (DG). Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. We thank our anonymous reviewers for their helpful comments, and the developers of the software packages used in this project, including PyTorch [97], NumPy [117], SciPy [118], scikit-learn [119], pandas [120], matplotlib [121], and seaborn [122]. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Irina Higgins et al. \u201cUnsupervised deep learning identifies semantic disentanglement in single inferotemporal face patch neurons\u201d. In: Nature Communications 12.1 (2021), p. 6456. DOI: 10.1038/s41467-021-26751-5.   \n[2] Hadi Vafaii et al. \u201cHierarchical VAEs provide a normative account of motion processing in the primate brain\u201d. In: Thirty-seventh Conference on Neural Information Processing Systems. 2023. URL: https://openreview.net/forum?id $\\equiv$ 1wOkHN9JK8.   \n[3] Bruno A Olshausen and David J Field. \u201cEmergence of simple-cell receptive field properties by learning a sparse code for natural images\u201d. In: Nature 381.6583 (1996), pp. 607\u2013609. DOI: 10.1038/381607a0.   \n[4] Michael E. Tipping and Christopher M. Bishop. \u201cProbabilistic Principal Component Analysis\u201d. In: Journal of the Royal Statistical Society Series B: Statistical Methodology 61.3 (Jan. 1999), pp. 611\u2013622. ISSN: 1369-7412. DOI: 10.1111/1467-9868.00196.   \n[5] Warren S McCulloch and Walter Pitts. \u201cA logical calculus of the ideas immanent in nervous activity\u201d. In: The bulletin of mathematical biophysics 5 (1943), pp. 115\u2013133. DOI: 10.1007/ BF02478259.   \n[6] Patricia S Churchland and Terrence J Sejnowski. \u201cPerspectives on cognitive neuroscience\u201d. In: Science 242.4879 (1988), pp. 741\u2013745. DOI: 10.1126/science.3055294.   \n[7] Michael SC Thomas and James L McClelland. \u201cConnectionist models of cognition\u201d. In: The Cambridge handbook of computational psychology (2008), pp. 23\u201358. URL: http: //www7.bbk.ac.uk/psychology/dnl/wp-content/uploads/2023/10/ThomasMcClelland-proof.pdf.   \n[8] Nikolaus Kriegeskorte. \u201cDeep neural networks: a new framework for modeling biological vision and brain information processing\u201d. In: Annual Review of Vision Science 1 (2015), pp. 417\u2013446. DOI: 10.1101/029876.   \n[9] Grace W. Lindsay. \u201cConvolutional Neural Networks as a Model of the Visual System: Past, Present, and Future\u201d. In: Journal of Cognitive Neuroscience 33.10 (Sept. 2021), pp. 2017\u2013 2031. ISSN: 0898-929X. DOI: 10.1162/jocn_a_01544.   \n[10] Joseph Marino. \u201cPredictive coding, variational autoencoders, and biological connections\u201d. In: Neural Computation 34.1 (2022), pp. 1\u201344. DOI: 10.1162/neco_a_01458.   \n[11] Jeffrey S Bowers et al. \u201cDeep problems with neural network models of human vision\u201d. In: Behavioral and Brain Sciences 46 (2023), e385. DOI: 10.1017/S0140525X22002813.   \n[12] Felix A. Wichmann and Robert Geirhos. \u201cAre Deep Neural Networks Adequate Behavioral Models of Human Visual Perception?\u201d In: Annual Review of Vision Science 9.Volume 9, 2023 (2023), pp. 501\u2013524. ISSN: 2374-4650. DOI: 10.1146/annurev-vision-120522- 031739.   \n[13] Anthony Zador et al. \u201cCatalyzing next-generation Artificial Intelligence through NeuroAI\u201d. In: Nature Communications 14.1 (2023), p. 1597. DOI: 10.1038/s41467-023-37180-x.   \n[14] Fabian H Sinz et al. \u201cEngineering a less artificial intelligence\u201d. In: Neuron 103.6 (2019), pp. 967\u2013979. DOI: 10.1016/j.neuron.2019.08.034.   \n[15] Demis Hassabis et al. \u201cNeuroscience-inspired artificial intelligence\u201d. In: Neuron 95.2 (2017), pp. 245\u2013258. DOI: 10.1016/j.neuron.2017.06.011.   \n[16] Nancy Kanwisher et al. \u201cUsing artificial neural networks to ask \u2018why\u2019 questions of minds and brains\u201d. In: Trends in Neurosciences (2023). DOI: 10.1016/j.tins.2022.12.008.   \n[17] Blake Richards et al. \u201cThe application of artificial intelligence to biology and neuroscience\u201d. In: Cell 185.15 (2022), pp. 2640\u20132643. DOI: 10.1016/j.cell.2022.06.047.   \n[18] Blake A Richards et al. \u201cA deep learning framework for neuroscience\u201d. In: Nature Neuroscience 22.11 (2019), pp. 1761\u20131770. DOI: 10.1038/s41593-019-0520-2.   \n[19] David GT Barrett et al. \u201cAnalyzing biological and artificial neural networks: challenges with opportunities for synergy?\u201d In: Current Opinion in Neurobiology 55 (2019), pp. 55\u201364. DOI: 10.1016/j.conb.2019.01.007.   \n[20] Adrien Doerig et al. \u201cThe neuroconnectionist research programme\u201d. In: Nature Reviews Neuroscience (2023), pp. 1\u201320. DOI: 10.1038/s41583-023-00705-w.   \n[21] Ibn al-Haytham. Book of optics (Kitab Al-Manazir). 1011\u20131021 AD.   \n[22] Hermann Von Helmholtz. Handbuch der physiologischen Optik. Vol. 9. Voss, 1867.   \n[23] Tai Sing Lee and David Mumford. \u201cHierarchical Bayesian inference in the visual cortex\u201d. In: JOSA A 20.7 (2003), pp. 1434\u20131448. DOI: 10.1364/JOSAA.20.001434.   \n[24] Bruno A. Olshausen. \u201cPerception as an Inference Problem\u201d. In: The Cognitive Neurosciences (5th edition) (2014). Ed. by Michael Gazzaniga and George R. Mangun. DOI: 10.7551/ mitpress/9504.003.0037. URL: http://rctn.org/bruno/papers/perceptionas-inference.pdf.   \n[25] Edwin Garrigues Boring. \u201cPerception of objects\u201d. In: American Journal of Physics (1946). DOI: 10.1119/1.1990807.   \n[26] Karl Friston. \u201cThe free-energy principle: a unified brain theory?\u201d In: Nature Reviews Neuroscience 11.2 (2010), pp. 127\u2013138. DOI: 10.1038/nrn2787.   \n[27] Sam Bond-Taylor et al. \u201cDeep generative modelling: A comparative review of vaes, gans, normalizing flows, energy-based and autoregressive models\u201d. In: IEEE transactions on pattern analysis and machine intelligence 44.11 (2021), pp. 7327\u20137347. DOI: 10.1109/ TPAMI.2021.3116668.   \n[28] Stanley H. Chan. Tutorial on Diffusion Models for Imaging and Vision. 2024. arXiv: 2403. 18103 [cs.LG].   \n[29] Wayne Xin Zhao et al. A Survey of Large Language Models. 2023. arXiv: 2303.18223 [cs.CL].   \n[30] Rajesh PN Rao and Dana H Ballard. \u201cPredictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects\u201d. In: Nature Neuroscience 2.1 (1999), pp. 79\u201387. DOI: 10.1038/4580.   \n[31] Robin Rombach et al. \u201cHigh-Resolution Image Synthesis With Latent Diffusion Models\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). June 2022, pp. 10684\u201310695.   \n[32] Tero Karras et al. \u201cA Style-Based Generator Architecture for Generative Adversarial Networks\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). June 2019. URL: https://openaccess.thecvf.com/content_CVPR_ 2019/html/Karras_A_Style-Based_Generator_Architecture_for_Generative_ Adversarial_Networks_CVPR_2019_paper.html.   \n[33] Arash Vahdat and Jan Kautz. \u201cNVAE: A Deep Hierarchical Variational Autoencoder\u201d. In: Advances in Neural Information Processing Systems. Vol. 33. Curran Associates, Inc., 2020, pp. 19667\u201319679. URL: https://papers.nips.cc/paper_files/paper/2020/hash/ e3b21256183cf7c2c7a66be163579d37-Abstract.html.   \n[34] Rewon Child. \u201cVery Deep {VAE}s Generalize Autoregressive Models and Can Outperform Them on Images\u201d. In: International Conference on Learning Representations. 2021. URL: https://openreview.net/forum?id $\\cdot^{=}$ RLRXCV6DbEJ.   \n[35] Diederik P Kingma and Max Welling. \u201cAuto-encoding variational bayes\u201d. In: (2014). arXiv: 1312.6114v11 [stat.ML].   \n[36] Danilo Jimenez Rezende et al. \u201cStochastic backpropagation and approximate inference in deep generative models\u201d. In: International Conference on Machine Learning. PMLR. 2014, pp. 1278\u20131286. URL: https://proceedings.mlr.press/v32/rezende14.html.   \n[37] Diederik P Kingma and Ruiqi Gao. \u201cUnderstanding Diffusion Objectives as the ELBO with Simple Data Augmentation\u201d. In: Thirty-seventh Conference on Neural Information Processing Systems. 2023. URL: https://openreview.net/forum?id $\\cdot$ NnMEadcdyD.   \n[38] Karsten Kreis et al. NeurIPS 2023 Tutorial on Latent Diffusion Models. https : / / neurips2023-ldm-tutorial.github.io/. 2023.   \n[39] Diederik Kingma et al. \u201cVariational diffusion models\u201d. In: Advances in neural information processing systems 34 (2021), pp. 21696\u201321707.   \n[40] Ferenc Csikor et al. \u201cTop-down perceptual inference shaping the activity of early visual cortex\u201d. In: bioRxiv (2023). DOI: 10.1101/2023.11.29.569262.   \n[41] T. Anderson Keller et al. \u201cModeling Category-Selective Cortical Regions with Topographic Variational Autoencoders\u201d. In: SVRHM 2021 Workshop @ NeurIPS. 2021. URL: https: //openreview.net/forum?id $\\cdot$ yGRq_lW54bI.   \n[42] T. Anderson Keller and Max Welling. \u201cTopographic VAEs learn Equivariant Capsules\u201d. In: Advances in Neural Information Processing Systems. Ed. by M. Ranzato et al. Vol. 34. Curran Associates, Inc., 2021, pp. 28585\u201328597. URL: https://proceedings.neurips.cc/ paper/2021/hash/f03704cb51f02f80b09bffba15751691-Abstract.html.   \n[43] Katherine R Storrs et al. \u201cUnsupervised learning predicts human perception and misperception of gloss\u201d. In: Nature Human Behaviour 5.10 (2021), pp. 1402\u20131417. DOI: 10.1038/s41562- 021-01097-6.   \n[44] Edgar Adrian. The activity of the nerve fibres. https://www.nobelprize.org/prizes/ medicine/1932/adrian/lecture/. 1932.   \n[45] Edgar Douglas Adrian and Yngve Zotterman. \u201cThe impulses produced by sensory nerveendings: Part II. The response of a Single End-Organ\u201d. In: The Journal of Physiology (1926), pp. 151\u201371. DOI: 10.1113/jphysiol.1926.sp002281.   \n[46] Donald H Perkel and Theodore H Bullock. \u201cNeural coding\u201d. In: Neurosciences Research Program Bulletin (1968). URL: https://ntrs.nasa.gov/citations/19690022317.   \n[47] Horace B Barlow. \u201cSingle units and sensation: a neuron doctrine for perceptual psychology?\u201d In: Perception 1.4 (1972), pp. 371\u2013394. DOI: 10.1068/p010371.   \n[48] Ehud Zohary et al. \u201cCorrelated neuronal discharge rate and its implications for psychophysical performance\u201d. In: Nature 370.6485 (1994), pp. 140\u2013143. DOI: 10.1038/370140a0.   \n[49] Fred Rieke et al. Spikes: exploring the neural code. MIT press, 1999.   \n[50] Malvin C Teich. \u201cFractal character of the auditory neural spike train\u201d. In: IEEE Transactions on Biomedical Engineering 36.1 (1989), pp. 150\u2013160.   \n[51] Robbe LT Goris et al. \u201cPartitioning neuronal variability\u201d. In: Nature neuroscience 17.6 (2014), pp. 858\u2013865.   \n[52] Diederik P Kingma and Max Welling. \u201cAn introduction to variational autoencoders\u201d. In: Foundations and Trends\u00ae in Machine Learning 12.4 (2019), pp. 307\u2013392. DOI: 10.1561/ 2200000056.   \n[53] Charles D Gilbert and Wu Li. \u201cTop-down influences on visual processing\u201d. In: Nature Reviews Neuroscience 14.5 (2013), pp. 350\u2013363.   \n[54] David C Knill and Alexandre Pouget. \u201cThe Bayesian brain: the role of uncertainty in neural coding and computation\u201d. In: Trends in Neurosciences 27.12 (2004), pp. 712\u2013719. DOI: 10.1016/j.tins.2004.10.007.   \n[55] Horace Barlow. \u201cRedundancy reduction revisited\u201d. In: Network: computation in neural systems 12.3 (2001), p. 241. DOI: 10.1080/net.12.3.241.253.   \n[56] Karl Friston. \u201cThe free-energy principle: a rough guide to the brain?\u201d In: Trends in cognitive sciences 13.7 (2009), pp. 293\u2013301.   \n[57] Peter Dayan et al. \u201cThe Helmholtz machine\u201d. In: Neural Computation 7.5 (1995), pp. 889\u2013 904. DOI: 10.1162/neco.1995.7.5.889.   \n[58] William Lotter et al. \u201cDeep Predictive Coding Networks for Video Prediction and Unsupervised Learning\u201d. In: International Conference on Learning Representations. 2017. URL: https://openreview.net/forum?id $\\equiv$ B1ewdt9xe.   \n[59] Fred Attneave. \u201cSome informational aspects of visual perception.\u201d In: Psychological review 61.3 (1954), p. 183. DOI: 10.1037/h0054663.   \n[60] Horace B. Barlow. \u201cPossible principles underlying the transformation of sensory messages\u201d. In: Sensory communication 1.01 (1961), pp. 217\u2013233. URL: https://www.cnbc.cmu.edu/ \\~tai/microns_papers/Barlow-SensoryCommunication-1961.pdf.   \n[61] Mandyam Veerambudi Srinivasan et al. \u201cPredictive coding: a fresh view of inhibition in the retina\u201d. In: Proceedings of the Royal Society of London. Series B. Biological Sciences 216.1205 (1982), pp. 427\u2013459. DOI: 10.1098/rspb.1982.0085.   \n[62] Karl Friston. \u201cA theory of cortical responses\u201d. In: Philosophical transactions of the Royal Society B: Biological Sciences 360.1456 (2005), pp. 815\u2013836. DOI: 10.1098/rstb.2005. 1622.   \n[63] Andy Clark. \u201cWhatever next? Predictive brains, situated agents, and the future of cognitive science\u201d. In: Behavioral and brain sciences 36.3 (2013), pp. 181\u2013204. DOI: 10.1017/ S0140525X12000477.   \n[64] William Lotter et al. \u201cA neural network trained for prediction mimics diverse features of biological neurons and perception\u201d. In: Nature machine intelligence 2.4 (2020), pp. 210\u2013219. DOI: 10.1038/s42256-020-0170-9.   \n[65] Beren Millidge et al. \u201cPredictive coding networks for temporal prediction\u201d. In: PLOS Computational Biology 20.4 (2024), e1011183.   \n[66] Yosef Singer et al. \u201cHierarchical temporal prediction captures motion processing along the visual pathway\u201d. In: Elife 12 (2023), e52599.   \n[67] Pierre-\u00c9tienne Fiquet and Eero Simoncelli. \u201cA polar prediction model for learning to represent visual transformations\u201d. In: Advances in Neural Information Processing Systems 36 (2024).   \n[68] Bruno A Olshausen and David J Field. \u201cSparse coding of sensory inputs\u201d. In: Current opinion in neurobiology 14.4 (2004), pp. 481\u2013487. DOI: 10.1016/j.conb.2004.07.007.   \n[69] William E Vinje and Jack L Gallant. \u201cSparse coding and decorrelation in primary visual cortex during natural vision\u201d. In: Science 287.5456 (2000), pp. 1273\u20131276. DOI: 10.1126/ science.287.5456.1273.   \n[70] Alison L Barth and James FA Poulet. \u201cExperimental evidence for sparse firing in the neocortex\u201d. In: Trends in neurosciences 35.6 (2012), pp. 345\u2013355. DOI: 10.1016/j.tins.2012. 03.008.   \n[71] R Quian Quiroga et al. \u201cSparse but not \u2018grandmother-cell\u2019coding in the medial temporal lobe\u201d. In: Trends in cognitive sciences 12.3 (2008), pp. 87\u201391. DOI: 10.1016/j.tics.2007. 12.003.   \n[72] Tom\u00e1\u0161 Hrom\u00e1dka et al. \u201cSparse representation of sounds in the unanesthetized auditory cortex\u201d. In: PLoS biology 6.1 (2008), e16. DOI: 10.1371/journal.pbio.0060016.   \n[73] Cindy Poo and Jeffry S Isaacson. \u201cOdor representations in olfactory cortex: \u201csparse\" coding, global inhibition, and oscillations\u201d. In: Neuron 62.6 (2009), pp. 850\u2013861. DOI: 10.1016/j. neuron.2009.05.022.   \n[74] Jason Wolfe et al. \u201cSparse and powerful cortical spikes\u201d. In: Current opinion in neurobiology 20.3 (2010), pp. 306\u2013312. DOI: 10.1016/j.conb.2010.03.006.   \n[75] Ben DB Willmore et al. \u201cSparse coding in striate and extrastriate visual cortex\u201d. In: Journal of neurophysiology 105.6 (2011), pp. 2907\u20132919. DOI: 10.1152/jn.00594.2010.   \n[76] Bilal Haider et al. \u201cSynaptic and network mechanisms of sparse and reliable visual cortical activity during nonclassical receptive field stimulation\u201d. In: Neuron 65.1 (2010), pp. 107\u2013121. DOI: 10.1016/j.neuron.2009.12.005.   \n[77] Sylvain Crochet et al. \u201cSynaptic mechanisms underlying sparse coding of active touch\u201d. In: Neuron 69.6 (2011), pp. 1160\u20131175. DOI: 10.1016/j.neuron.2011.02.022.   \n[78] Carl CH Petersen. \u201cSensorimotor processing in the rodent barrel cortex\u201d. In: Nature Reviews Neuroscience 20.9 (2019), pp. 533\u2013546. DOI: 10.1038/s41583-019-0200-y.   \n[79] Emmanouil Froudarakis et al. \u201cPopulation code in mouse V1 facilitates readout of natural scenes through increased sparseness\u201d. In: Nature neuroscience 17.6 (2014), pp. 851\u2013857. DOI: 10.1038/nn.3707.   \n[80] Christopher J Rozell et al. \u201cSparse coding via thresholding and local competition in neural circuits\u201d. In: Neural Computation 20.10 (2008), pp. 2526\u20132563. DOI: 10.1162/neco.2008. 03-07-486.   \n[81] I. Daubechies et al. \u201cAn iterative thresholding algorithm for linear inverse problems with a sparsity constraint\u201d. In: Communications on Pure and Applied Mathematics 57.11 (2004), pp. 1413\u20131457. DOI: 10.1002/cpa.20042.   \n[82] Amir Beck and Marc Teboulle. \u201cA Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems\u201d. In: SIAM Journal on Imaging Sciences 2.1 (2009), pp. 183\u2013202. DOI: 10.1137/080716542.   \n[83] Ankush Ganguly et al. \u201cAmortized Variational Inference: A Systematic Review\u201d. In: Journal of Artificial Intelligence Research 78 (2023), pp. 167\u2013215. DOI: 10.1613/jair.1.14258.   \n[84] Brandon Amos. \u201cTutorial on Amortized Optimization\u201d. In: Foundations and Trends\u00ae in Machine Learning 16.5 (2023), pp. 592\u2013732. ISSN: 1935-8237. DOI: 10.1561/2200000102.   \n[85] Samuel Gershman and Noah Goodman. \u201cAmortized inference in probabilistic reasoning\u201d. In: Proceedings of the annual meeting of the cognitive science society. Vol. 36. 36. 2014. URL: https://escholarship.org/uc/item/34j1h7k5.   \n[86] Colin Conwell et al. \u201cWhat can 1.8 billion regressions tell us about the pressures shaping high-level visual representation in brains and machines?\u201d In: bioRxiv (2023). DOI: 10.1101/ 2022.03.28.485868.   \n[87] Eric Elmoznino and Michael F Bonner. \u201cHigh-performing neural network models of visual cortex benefit from high latent dimensionality\u201d. In: bioRxiv (2022), pp. 2022\u201307. DOI: 10.1101/2022.07.13.499969.   \n[88] Aaron Van Den Oord, Oriol Vinyals, et al. \u201cNeural discrete representation learning\u201d. In: Advances in neural information processing systems 30 (2017).   \n[89] Eric Jang et al. \u201cCategorical Reparameterization with Gumbel-Softmax\u201d. In: International Conference on Learning Representations. 2017. URL: https://openreview.net/forum? id=rkE3y85ee.   \n[90] Hiromichi Kamata et al. \u201cFully spiking variational autoencoder\u201d. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. 6. 2022, pp. 7059\u20137067.   \n[91] Victor Geadah et al. \u201cSparse-Coding Variational Auto-Encoders\u201d. In: bioRxiv (2024). DOI: 10.1101/399246.   \n[92] Francesco Tonolini et al. \u201cVariational sparse coding\u201d. In: Uncertainty in Artificial Intelligence. PMLR. 2020, pp. 690\u2013700.   \n[93] Pan Xiao et al. \u201cSC-VAE: Sparse Coding-based Variational Autoencoder with Learned ISTA\u201d. In: (2024). arXiv: 2303.16666 [cs.CV].   \n[94] David Roxbee Cox and Valerie Isham. Point processes. Vol. 12. CRC Press, 1980.   \n[95] Oleksandr Shchur et al. \u201cFast and Flexible Temporal Point Processes with Triangular Maps\u201d. In: Advances in Neural Information Processing Systems. Ed. by H. Larochelle et al. Vol. 33. Curran Associates, Inc., 2020, pp. 73\u201384. URL: https://proceedings.neurips.cc/ paper_files/paper/2020/hash/00ac8ed3b4327bdd4ebbebcb2ba10a00-Abstract. html.   \n[96] Oleksandr Shchur. \u201cModeling Continuous-time Event Data with Neural Temporal Point Processes\u201d. PhD thesis. Technische Universit\u00e4t M\u00fcnchen, 2022.   \n[97] Adam Paszke et al. \u201cPyTorch: An Imperative Style, High-Performance Deep Learning Library\u201d. In: Advances in Neural Information Processing Systems. Vol. 32. Curran Associates, Inc., 2019. URL: https://papers.nips.cc/paper_files/paper/2019/hash/ bdbca288fee7f92f2bfa9f7012727740-Abstract.html. [98] Chris J. Maddison et al. \u201cThe Concrete Distribution: A Continuous Relaxation of Discrete Random Variables\u201d. In: International Conference on Learning Representations. 2017. URL: https://openreview.net/forum?id=S1jE5L5gl. [99] Daniel J Felleman and David C Van Essen. \u201cDistributed hierarchical processing in the primate cerebral cortex\u201d. In: Cerebral Cortex 1.1 (1991), pp. 1\u201347. DOI: 10.1093/CERCOR/1.1.1.   \n[100] Anita A Disney. \u201cNeuromodulatory control of early visual processing in macaque\u201d. In: Annual Review of Vision Science 7 (2021), pp. 181\u2013199.   \n[101] James C. R. Whittington et al. \u201cDisentanglement with Biological Constraints: A Theory of Functional Cell Types\u201d. In: The Eleventh International Conference on Learning Representations. 2023. URL: https://openreview.net/forum?id=9Z_GfhZnGH.   \n[102] J Hans Van Hateren and Arjen van der Schaaf. \u201cIndependent component filters of natural images compared with simple cells in primary visual cortex\u201d. In: Proceedings of the Royal Society of London. Series B: Biological Sciences 265.1394 (1998), pp. 359\u2013366.   \n[103] Victor Boutin et al. \u201cSparse deep predictive coding captures contour integration capabilities of the early visual system\u201d. In: PLoS computational biology 17.1 (2021), e1008629.   \n[104] Alex Krizhevsky, Geoffrey Hinton, et al. \u201cLearning multiple layers of features from tiny images\u201d. In: (2009).   \n[105] Francesco Locatello et al. \u201cChallenging common assumptions in the unsupervised learning of disentangled representations\u201d. In: international conference on machine learning. PMLR. 2019, pp. 4114\u20134124. URL: https://proceedings.mlr.press/v97/locatello19a. html.   \n[106] Yoshua Bengio et al. \u201cEstimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation\u201d. In: (2013). arXiv: 1308.3432 [cs.LG].   \n[107] Alexander Alemi et al. \u201cFixing a Broken ELBO\u201d. In: Proceedings of the 35th International Conference on Machine Learning. Ed. by Jennifer Dy and Andreas Krause. Vol. 80. Proceedings of Machine Learning Research. PMLR, July 2018, pp. 159\u2013168. URL: https: //proceedings.mlr.press/v80/alemi18a.html.   \n[108] Chris Cremer et al. \u201cInference suboptimality in variational autoencoders\u201d. In: International Conference on Machine Learning. PMLR. 2018, pp. 1078\u20131086.   \n[109] Kilian Q Weinberger and Lawrence K Saul. \u201cDistance metric learning for large margin nearest neighbor classification.\u201d In: Journal of machine learning research 10.2 (2009).   \n[110] Matteo Alleman et al. \u201cTask structure and nonlinearity jointly determine learned representational geometry\u201d. In: The Twelfth International Conference on Learning Representations. 2024. URL: https://openreview.net/forum?id=k9t8dQ30kU.   \n[111] Mattia Rigotti et al. \u201cThe importance of mixed selectivity in complex cognitive tasks\u201d. In: Nature 497.7451 (2013), pp. 585\u2013590. DOI: 10.1038/nature12160.   \n[112] Silvia Bernardi et al. \u201cThe geometry of abstraction in the hippocampus and prefrontal cortex\u201d. In: Cell 183.4 (2020), pp. 954\u2013967.   \n[113] Matthew T Kaufman et al. \u201cThe implications of categorical and category-free mixed selectivity on representational geometries\u201d. In: Current opinion in neurobiology 77 (2022), p. 102644.   \n[114] Cina Aghamohammadi et al. \u201cA doubly stochastic renewal framework for partitioning spiking variability\u201d. In: bioRxiv (2024), pp. 2024\u201302.   \n[115] Karol Gregor and Yann LeCun. \u201cLearning fast approximations of sparse coding\u201d. In: Proceedings of the 27th international conference on international conference on machine learning. 2010, pp. 399\u2013406.   \n[116] Joe Marino et al. \u201cIterative Amortized Inference\u201d. In: Proceedings of the 35th International Conference on Machine Learning. Ed. by Jennifer Dy and Andreas Krause. Vol. 80. Proceedings of Machine Learning Research. PMLR, July 2018, pp. 3403\u20133412. URL: https: //proceedings.mlr.press/v80/marino18a.html.   \n[117] Charles R. Harris et al. \u201cArray programming with NumPy\u201d. In: Nature 585.7825 (Sept. 2020), pp. 357\u2013362. DOI: 10.1038/s41586-020-2649-2.   \n[118] Pauli Virtanen et al. \u201cSciPy 1.0: Fundamental Algorithms for Scientific Computing in Python\u201d. In: Nature Methods 17 (2020), pp. 261\u2013272. DOI: 10.1038/s41592-019-0686-2.   \n[119] Fabian Pedregosa et al. \u201cScikit-learn: Machine learning in Python\u201d. In: the Journal of machine Learning research 12 (2011), pp. 2825\u20132830. DOI: 10.5555/1953048.2078195.   \n[120] The pandas development team. pandas-dev/pandas: Pandas. Version latest. Feb. 2020. DOI: 10.5281/zenodo.3509134.   \n[121] John D Hunter. \u201cMatplotlib: A 2D graphics environment\u201d. In: Computing in science & engineering 9.03 (2007), pp. 90\u201395. DOI: 10.1109/MCSE.2007.55.   \n[122] Michael L Waskom. \u201cSeaborn: statistical data visualization\u201d. In: Journal of Open Source Software 6.60 (2021), p. 3021. DOI: 10.21105/joss.03021.   \n[123] Ashish Vaswani et al. \u201cAttention is All you Need\u201d. In: Advances in Neural Information Processing Systems. Ed. by I. Guyon et al. Vol. 30. Curran Associates, Inc., 2017. URL: https : / / papers . nips . cc / paper _ files / paper / 2017 / hash / 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.   \n[124] Prajit Ramachandran et al. \u201cSearching for Activation Functions\u201d. In: International Conference on Learning Representations. 2018. URL: https://openreview.net/forum?id $=$ SkBYYyZRZ.   \n[125] Stefan Elfwing et al. \u201cSigmoid-weighted linear units for neural network function approximation in reinforcement learning\u201d. In: Neural Networks 107 (2018), pp. 3\u201311. DOI: 10.1016/j. neunet.2017.12.012.   \n[126] Diederik P Kingma and Jimmy Ba. \u201cAdam: A method for stochastic optimization\u201d. In: (2014). arXiv: 1412.6980 [cs.LG].   \n[127] Ilya Loshchilov and Frank Hutter. \u201cSGDR: Stochastic Gradient Descent with Warm Restarts\u201d. In: International Conference on Learning Representations. 2017. URL: https: //openreview.net/forum?id $\\cdot$ Skq89Scxx.   \n[128] Casper Kaae S\u00f8nderby et al. \u201cLadder Variational Autoencoders\u201d. In: Advances in Neural Information Processing Systems. Vol. 29. Curran Associates, Inc., 2016. URL: https://papers. nips.cc/paper_files/paper/2016/hash/6ae07dcb33ec3b7c814df797cbda0f87- Abstract.html.   \n[129] Samuel R. Bowman et al. \u201cGenerating Sentences from a Continuous Space\u201d. In: Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning. Berlin, Germany: Association for Computational Linguistics, Aug. 2016, pp. 10\u201321. DOI: 10.18653/ v1/K16-1002.   \n[130] Hao Fu et al. \u201cCyclical Annealing Schedule: A Simple Approach to Mitigating KL Vanishing\u201d. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Minneapolis, Minnesota: Association for Computational Linguistics, June 2019, pp. 240\u2013250. DOI: 10.18653/v1/N19-1021.   \n[131] Michael Teti. LCA-PyTorch. [Computer Software] https://doi.org/10.11578/dc. 20230728.4. June 2023. DOI: 10.11578/dc.20230728.4. URL: https://doi.org/10. 11578/dc.20230728.4.   \n[132] Shakir Mohamed et al. \u201cMonte Carlo Gradient Estimation in Machine Learning\u201d. In: Journal of Machine Learning Research 21.132 (2020), pp. 1\u201362. URL: http://jmlr.org/papers/ v21/19-346.html. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Full derivations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we provide a self-contained and pedagogical introduction to VAEs, derive the $\\mathcal{P}$ -VAE loss function, and highlight how combining Poisson-distributed latents with predictive coding leads to the emergence of a metabolic cost term in the $\\mathcal{P}$ -VAE loss. For the case of a linear decoder, the reconstruction loss assumes a closed-form solution. This means we can compute the gradients analytically, which we can then use to evaluate the Poisson reparameterization trick. ", "page_idx": 16}, {"type": "text", "text": "A.1 Deriving the evidence lower bound (ELBO) loss ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For completeness, let\u2019s first go over the basics. This section will provide a quick refresher on variational inference and how to derive the VAE loss from scratch. Assume the data $\\pmb{x}\\in\\mathbb{R}^{M}$ and $K$ -dimensional latent variables $_{z}$ are jointly distributed as $p(\\pmb{x},z)$ , with the data generated by the following process: ", "page_idx": 16}, {"type": "equation", "text": "$$\np({\\pmb x})=\\int p({\\pmb x},{\\pmb z})\\,d{\\pmb z}=\\int p({\\pmb x}|{\\pmb z})p({\\pmb z})\\,d{\\pmb z},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In Bayesian posterior inference, the goal is to identify which latents $_{z}$ are likely given data $\\textbf{\\em x}$ . In other words, we want to approximate $P(z|x)$ , the optimal but intractable posterior distribution. ", "page_idx": 16}, {"type": "text", "text": "Variational inference and VAE loss function. To achieve approximate Bayesian inference, a common approach is to define a family of variational densities $\\mathcal{Q}$ and find a member $q(\\pmb{z}|\\pmb{x})\\in\\mathcal{Q}$ such that it sufficiently approximates the optimal posterior. We call $q(z|x)$ the approximate posterior. The general aim of variational inference (VI) can be summarized as follows: ", "page_idx": 16}, {"type": "text", "text": "The goodness of our approximate posterior, or its closeness to the true posterior, is measured using the Kullback-Leibler (KL) divergence: ", "page_idx": 16}, {"type": "equation", "text": "$$\nq^{*}=\\operatorname*{argmin}_{q\\,\\in\\,\\mathcal{Q}}\\mathcal{D}_{\\mathbb{K L}}\\Big(q(z|x)\\,\\big\\|\\,p(z|x)\\Big).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We cannot directly optimize eq. (8), because $p(z|x)$ is intractable. Instead, we rearrange some terms and arrive at the following loss function: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{NELBO}}(q)=-\\mathbb{E}_{z\\sim q(z\\mid x)}\\left[\\log p(x|z)\\right]+\\mathcal{D}_{\\mathrm{KL}}\\Big(q(z|x)\\,\\|\\,p(z)\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "NELBO stands for negative ELBO, also known as \u201cvariational free energy.\u201d Notably, finding a $q\\in\\mathcal{Q}$ that minimizes $\\mathcal{L}_{\\mathrm{NELBO}}(q)$ in eq. (9) is equivalent to finding the optimal $q^{*}$ in eq. (8). ", "page_idx": 16}, {"type": "text", "text": "The first term in eq. (9), often called the reconstruction term, captures the likelihood of the observed data $\\textbf{\\em x}$ , given latents $_{z}$ , under the approximate posterior. For all our VAE models, we approximate the reconstruction term as the mean squared error between input data and their reconstructed version, as is typically done in the literature. The second term, known as the KL term, is more interesting. This term can assume very different forms depending on the distribution used. ", "page_idx": 16}, {"type": "text", "text": "A.2 The KL term ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we will derive closed-form expressions for the KL term for different choices of the distributions $q(z|x)$ and $p(z)$ . Specifically, we will focus on Gaussian and Poisson parameterizations. ", "page_idx": 16}, {"type": "text", "text": "Predictive coding assumption. We will draw inspiration from predictive coding and assume that the bottom-up inference pathway only encodes the residual information relative to the top-down, or predicted information. We will apply this idea to both Gaussian and Poisson cases, and find that only in the Poisson case, the outcome becomes interpretable and resembles sparse coding. ", "page_idx": 16}, {"type": "text", "text": "Gaussian. Let $q(z|\\pmb{x})=\\mathcal{N}(z;\\mu_{q}(\\pmb{x}),\\sigma_{q}(\\pmb{x}))$ and $p(z)\\,=\\mathcal{N}(z;\\mu_{p},\\sigma_{p})$ , where the mean and variance are either outputs of the encoder network (red) or parameters of the decoder network (blue). ", "page_idx": 17}, {"type": "text", "text": "Now, let us implement the predictive coding assumption, where the encoder only keeps track of residual information that is not already contained in the prior information. Mathematically, this idea can be formalized as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mu_{p}\\to\\mu,}&{{}\\quad\\mu_{q}\\to\\mu+\\delta\\mu}\\\\ {\\sigma_{p}\\to\\sigma,}&{{}\\quad\\sigma_{q}\\to\\sigma\\cdot\\delta\\sigma}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "With these modifications, the Gaussians KL term becomes: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{D}_{\\mathrm{KL}}\\left(q\\parallel p\\right)=\\frac{1}{2}\\Big(\\frac{\\delta\\mu^{2}}{\\sigma^{2}}+\\delta\\sigma^{2}-\\log\\delta\\sigma^{2}-{\\bf1}\\Big).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In standard Gaussian VAEs, the prior has no learnable parameter. Instead, we have $\\mu\\rightarrow{\\bf0}$ and $\\sigma\\rightarrow\\mathbf{1}$ . Therefore, the final form of the KL term for a standard Gaussian VAE is: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{D}_{\\mathrm{KL}}\\left(q\\,\\|\\,\\mathcal{N}({\\bf0},{\\bf1})\\right)=\\frac{1}{2}\\Big(\\delta\\mu^{2}+\\delta\\sigma^{2}-\\log\\delta\\sigma^{2}-{\\bf1}\\Big).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We observe that the KL term vanishes when $\\delta\\mu\\rightarrow\\mathbf{0}$ and $\\delta\\sigma\\rightarrow\\mathbf{1}$ . This happens whenever no new information is propagated through the encoder, a phenomenon known as posterior collapse. ", "page_idx": 17}, {"type": "text", "text": "Other than this trivial observation, eq. (12) does not really lend itself to interpretation. In contrast, will show below that a Poisson parameterization of VAEs leads to a much more interpretable outcome for the KL term. ", "page_idx": 17}, {"type": "text", "text": "Poisson. Now suppose $q(z|\\pmb{x})=\\mathscr{P}\\mathrm{ois}(z;r\\delta r(\\pmb{x}))$ , and $p(z)=\\mathscr{P}\\mathrm{ois}(z;r)$ , where $z$ is literally the spike count of a single latent dimension\u2014or shall we say, neuron? ", "page_idx": 17}, {"type": "text", "text": "In the Poisson case, the $\\mathtt{K L}$ term becomes more interpretable, as we will show below. Recall that the Poisson distribution for a single variable $z$ , given rate $\\lambda\\in\\mathbb{R}_{>0}$ , is given by: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{P}\\mathrm{ois}(z;\\lambda)=\\frac{\\lambda^{z}e^{-\\lambda}}{z!}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Plug this expressions into the KL divergence definition to get: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{D}_{\\mathbf{R}}(q\\,|\\,p)=\\mathbb{E}_{q\\sim r}\\left[\\log\\frac{r}{r}\\right]}\\\\ {\\;}&{=\\mathbb{E}_{q\\sim r}\\left[\\log\\frac{(r\\,\\delta\\,)^{\\dagger}\\,e^{-r\\,\\delta r/2}\\,\\delta^{\\dagger}}{r^{2}\\,e^{-r\\,\\delta r/2}}\\right]}\\\\ {\\;}&{=\\mathbb{E}_{q\\sim r}\\left[\\log\\left(\\left(\\frac{r\\,\\delta\\,)^{\\dagger}}{r}\\right)^{\\,\\alpha\\,-1}e^{-r\\,\\delta r/2}\\right]}\\\\ {\\;}&{=\\mathbb{E}_{q\\sim r}\\left[\\log\\delta^{\\dagger\\,r/2}+\\log e^{-r\\,\\delta r/2}\\right]}\\\\ {\\;}&{=\\mathbb{E}_{q\\sim r}\\left[\\log\\delta^{\\dagger}\\,e^{-r\\,\\delta r/2}+r\\right]}\\\\ {\\;}&{=\\mathbb{E}_{q\\sim r}\\left[\\frac{1}{2}\\log\\delta r-r\\delta r+r\\right]}\\\\ {\\;}&{=\\mathbb{E}_{q\\sim r}\\left[\\frac{1}{2}\\log\\delta r-r\\delta r+r\\right.}\\\\ {\\;}&{=r\\delta r\\log\\delta r-r\\delta r+}\\\\ {\\left.}&{=r\\,\\delta r+\\delta r\\log\\delta r\\right]}\\\\ {\\;}&{=r\\,\\delta r\\delta r-\\delta r\\delta r.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we have define $f(y):=1-y+y\\log y$ . ", "page_idx": 17}, {"type": "text", "text": "To examine the behavior of the Poisson KL term, we assume $\\delta r=1+\\epsilon.$ , where $\\epsilon\\ll1$ , then Taylor expand $f$ . Calculating the first and second derivatives of $f(y)=1-y+y\\log y$ gives $f^{\\prime}(y)=\\log y$ and $f^{\\prime\\prime}(y)=1/y$ . Thus: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f(1+\\epsilon)=f(1)+\\epsilon f^{\\prime}(1)+\\displaystyle\\frac{\\epsilon^{2}}{2!}f^{\\prime\\prime}(1)+\\mathcal{O}(\\epsilon^{3})}}\\\\ {{\\phantom{\\hat{f}}=0+0+\\displaystyle\\frac{\\epsilon^{2}}{2!}+\\mathcal{O}(\\epsilon^{3})}}\\\\ {{\\phantom{\\hat{f}}\\approx\\displaystyle\\frac{1}{2}\\epsilon^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Plug this back into eq. (14) to get: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\mathcal D}_{\\mathrm{KL}}\\left(q\\parallel p\\right)=r f(\\delta r)}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~=r f(1+\\epsilon)}}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~\\approx\\frac{1}{2}r\\epsilon^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For small deviations $\\epsilon$ , the KL term simplifies to the product of the prior firing rate, $r$ , and $\\epsilon^{2}$ . See Fig. 6 for a visualization of the full function, $f(\\delta r)=1-\\delta r+\\delta r\\log\\delta r$ , along with its quadratic approximation near $\\delta r=1$ . ", "page_idx": 18}, {"type": "image", "img_path": "ektPEcqGLb/tmp/f0dfdc8b11829134b87795add0620c67832af59b38245367f4a5aca0f2ca8990.jpg", "img_caption": ["Figure 6: Left, residual term $f(\\delta r)$ from eq. (14). Right, quadratic approximation of $f$ from eq. (15). "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "In general, there are two ways to minimize the KL term: ", "page_idx": 18}, {"type": "text", "text": "Together with the reconstruction loss, the NELBO for a 1-dimensional $\\mathcal{P}$ -VAE reads: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{PVAE}}\\left(r,\\delta r\\right)=\\mathcal{L}_{\\mathrm{recon.}}\\left(r,\\delta r\\right)+r\\left(1-\\delta r+\\delta r\\log\\delta r\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, it is easy to show that for $K$ -dimensional latent space, eq. (14) generalizes to: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{D}_{\\mathrm{KL}}\\Big(\\mathcal{P}\\mathrm{ois}(z;r\\odot\\delta r(\\pmb{x}))\\,\\|\\,\\mathcal{P}\\mathrm{ois}(z;r)\\Big)=r\\cdot f(\\delta r),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\odot$ and $\\cdot$ denote the Hadamard (element-wise) and vector products, respectively. ", "page_idx": 18}, {"type": "text", "text": "A.3 Connection to sparse coding ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Equation (17) mirrors sparse coding due to the presence of the firing rate in the objective function. Furthermore, it follows the principle of predictive coding by design. Thus, our Poisson formulation of VAEs effectively unifies these two major themes in theoretical neuroscience. Let\u2019s explore this curious connection to sparse coding more closely below. ", "page_idx": 18}, {"type": "text", "text": "A.4 Statistically independent neurons ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Suppose our $\\mathcal{P}$ -VAE has $K$ statistically independent neurons, and $z\\in\\mathbb{Z}_{\\geq0}^{K}$ is the spike count variable, where $\\mathbb{Z}_{\\geq0}=\\{0,1,2,\\ldots\\}$ is the set of non-negative integers. Let us use bold font $r$ and $\\delta r$ to refer to the firing rate vectors of the representation and error units, respectively. Recall that we allowed these variables to interact in a multiplicative way to construct the posterior rates, $\\lambda_{i}({\\pmb x})=r_{i}\\delta r_{i}({\\pmb x})$ . More explicitly, we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle q(\\boldsymbol{z}|\\boldsymbol{x})=\\mathcal{P}\\mathrm{ois}(z;r\\delta r)=\\displaystyle\\prod_{i=1}^{K}\\mathcal{P}\\mathrm{ois}(z_{i};r_{i}\\delta r_{i})=\\displaystyle\\prod_{i=1}^{K}\\frac{\\lambda_{i}^{z_{i}}e^{-\\lambda_{i}}}{z_{i}!},}\\\\ {\\displaystyle p(\\boldsymbol{z})=\\mathcal{P}\\mathrm{ois}(z;r)=\\displaystyle\\prod_{i=1}^{K}\\mathcal{P}\\mathrm{ois}(z_{i};r_{i})=\\displaystyle\\prod_{i=1}^{K}\\frac{r_{i}^{z_{i}}e^{-r_{i}}}{z_{i}!}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that, unlike a standard Gaussian VAE, the prior in $\\mathcal{P}$ -VAE is parameterized using $^r$ , which is learned from data along with the other parameters. Similar to standard Gaussian VAEs, $\\delta r({\\pmb x})$ is parameterized as a neural network. ", "page_idx": 19}, {"type": "text", "text": "A.5 Linear decoder ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Following the sparse coding literature, we will now assume our decoder generates the input image $\\pmb{x}\\in\\mathbb{R}^{M}$ as a linear sum of $K$ basis elements, $\\Phi\\in\\mathbb{R}^{M\\times K}$ . We approximate the reconstruction loss as the mean squared error between the input $\\textbf{\\em x}$ , and its reconstruction $\\Phi z$ . Given these assumptions, the VAE reconstruction loss becomes: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{recon.}}\\left(\\pmb{x};\\boldsymbol{q}\\right)=\\mathbb{E}_{\\pmb{z}\\sim\\boldsymbol{q}\\left(Z|X=\\pmb{x}\\right)}\\left[\\left|\\pmb{|x}-\\Phi\\pmb{z}\\right|\\right|_{2}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For a linear decoder, the reconstruction term $\\left\\|\\pmb{x}-\\Phi\\pmb{z}\\right\\|_{2}^{2}$ contains only the first and second moments of $_{z}$ . Consequently, the expectation in eq. (20) can be analytically resolved. This yields the loss function, and consequently, its gradients in closed form. Specifically, for the Poisson case, we only need to know the following expectation values: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{z\\sim\\mathcal{P}\\mathrm{ois}(z;\\lambda)}\\left[z_{i}\\right]=\\lambda_{i},}\\\\ &{\\mathbb{E}_{z\\sim\\mathcal{P}\\mathrm{ois}(z;\\lambda)}\\left[z_{i}z_{j}\\right]=\\lambda_{i}\\lambda_{j}+\\delta_{i j}\\lambda_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "More generally, whenever the VAE decoder is linear, the following result holds: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boxed{\\mathcal{L}_{\\mathrm{recon.}}\\left(\\pmb{x};q,\\Phi\\right)=\\|\\pmb{x}-\\Phi\\,\\mathbb{E}_{q}[Z]\\|_{2}^{2}+\\mathrm{Var}_{q}[Z]^{T}\\mathrm{diag}(\\Phi^{T}\\Phi).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that a linear decoder is the only assumption we needed to obtain this closed-form solution. There are no restrictions on the form of the encoder: it can be linear, or as complicated as we want. We only have to compute the mean and variance of the posterior. Here are the reconstruction losses for both Poisson and Gaussian VAEs with linear decoders, put side-by-side for comparison: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\boxed{\\mathrm{Poisson:}}}&{\\qquad\\mathcal{L}_{\\mathrm{recon.}}(x;\\lambda,\\Phi)=\\|x-\\Phi\\lambda\\|_{2}^{2}+\\lambda^{T}\\mathrm{diag}(\\Phi^{T}\\Phi),}\\\\ {\\mathrm{Gaussian:}}&{\\quad\\mathcal{L}_{\\mathrm{recon.}}(x;\\mu,\\sigma,\\Phi)=\\|x-\\Phi\\mu\\|_{2}^{2}+(\\sigma^{2})^{T}\\mathrm{diag}(\\Phi^{T}\\Phi).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Given these assumptions, the NELBO (eq. (17)) for $\\mathcal{P}$ -VAE with a linear decoder becomes: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\boxed{\\mathcal{L}_{\\mathrm{SC-PVAE}}\\left(x;\\delta r,r,\\Phi\\right)=\\left\\|x-\\Phi\\lambda\\right\\|_{2}^{2}+\\lambda^{T}\\mathrm{diag}(\\Phi^{T}\\Phi)+\\beta\\sum_{i=1}^{K}r_{i}f(\\delta r_{i}).}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Recall that we have $f(y)=1-y+y\\log y$ (see Fig. 6). We introduced the $\\beta$ term here to control the trade-off between the reconstruction and the KL term. Additionally, we dropped the explicit dependence of $\\delta r({\\pmb x})$ on the input image $\\textbf{\\em x}$ to increase readability. ", "page_idx": 20}, {"type": "text", "text": "A.6 Linear encoder ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We can further simplify the $\\mathcal{P}$ -VAE architecture by making the encoder also linear. Let\u2019s use $W\\in\\mathbb{R}^{K\\times M}$ to denote encoder weights, and assume an exponential link function mapping the input into the residual firing rates. In other words, we have $\\delta r=\\exp(W\\mathbf{x})$ . To obtain the final form of the loss function, we start from eq. (24), plug in $\\log(\\delta r)=W\\mathbf{\\hat{x}}$ , and rearrange some terms to find: ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\mathrm{VAE}}=\\lambda^{T}\\Phi^{T}\\Phi\\lambda+\\lambda^{T}{\\mathrm{diag}}(\\Phi^{T}\\Phi-\\beta I)+\\lambda^{T}(\\beta W-2\\Phi^{T}){\\pmb x}+\\beta\\sum_{i=1}^{K}r_{i}+{\\pmb x}^{T}{\\pmb x}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "B Architecture, training, and hyperparameter details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "B.1 Datasets: additional details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We consider three datasets in this paper. We tile up the van Hateren dataset of natural images [102] and CIFAR10 into $16\\times16$ patches and apply whitening and contrast normalization using the code made available by Boutin et al. [103]. This operation results in the following total number of samples: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{n}\\;\\mathbf{Hateren}{:}}&{{}\\;\\neq\\operatorname{train}=107,520,\\quad\\#\\mathrm{validation}=28,224,}\\\\ {\\mathbf{FAR}_{16\\times16}{:}}&{{}\\;\\neq\\operatorname{train}=200,000,\\quad\\#\\mathrm{validation}=40,000.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We use the MNIST dataset primarily for the downstream classification task. After the training is done, we use the following train/validation split to evaluate the models: ", "page_idx": 20}, {"type": "text", "text": "\u2022 K-nearest neighbor classification (tables 3 and 5): For this task, we only make use of the validation set for both training and testing of the classifier. We divide up the $N=10\\small{,}000$ validation samples into two disjoint sets of $N\\,=\\,5{,}000$ samples each. We then draw random samples (without replacement) from the first half and use them for training the KNN classifier. We then test the performance on the other half. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Shattering dimensionality (tables 3 and 5, last column): We use the entire MNIST training set ( $N=60{,}000$ samples) to train logistic regression classifiers on extracted representations. We then test the results using the entire validation set $\\mathit{N}=10\\small{,}000$ samples). ", "page_idx": 20}, {"type": "text", "text": "B.2 Architecture details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For sparse coding results, we focused on models with linear decoders. For the fully linear models (Figs. 4 and 8) both the encoder and decoder were linear layers, without bias. ", "page_idx": 20}, {"type": "text", "text": "For the convolutional components, we use residual layers without batch norm. For van Hateren and $\\mathrm{CIFAR}_{16\\times16}$ datasets, the encoders had 5 layers $\\mathrm{2\\timesconv}$ each). The decoders had 8 convolutional layers ( $\\mathrm{1\\timesconv}$ each). For the MNIST dataset, the encoders had 7 layers $\\mathrm{2\\timesconv}$ each). The decoders had 10 convolutional layers $\\mathrm{1\\timesconv}$ each). For all convolutional encoders, the output from ResNet was followed by a learned pooling layer. The pooled output was then fed into a feed-forward layer inspired by Transformers [123], which includes a layer norm as the final operation, the output of which was fed into a linear layer that projects features into posterior distribution parameters. For all convolutional decoders, nearest neighbor upsampling was performed to scale up the spatial dimension of reconstructions. ", "page_idx": 20}, {"type": "text", "text": "We experimented with both leaky_relu and swish activation functions [124, 125], and found that swish consistently outperformed leaky_relu in all our experiments across datasets and VAE models. ", "page_idx": 20}, {"type": "text", "text": "Please see our code for the full architecture details. ", "page_idx": 20}, {"type": "text", "text": "B.3 Training: VAE models ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We used a variety of learning rates and batch sizes, depending on the dataset and architecture. For linear models, we used $l r=0.005$ , and for fully conv models we used $l r=0.002$ . All models were trained using the AdaMax optimizer [126] with a cosine learning rate schedule [127]. Please see our code for the full details of training hyperparameters. ", "page_idx": 21}, {"type": "text", "text": "KL annealing. For all VAE models, we annealed the KL term during the first half of the training, which is known to be an effective trick in training VAEs [2, 33, 128\u2013130]. ", "page_idx": 21}, {"type": "text", "text": "Temperature annealing. For discrete VAEs ( $\\mathcal{P}$ -VAE, $\\mathcal{C}$ -VAE), we also annealed the temperature from a large value to a smaller value during the same first half of training. We found that the specific functional form of temperature annealing (e.g., linear, exponential, etc) did not matter as much as the final temperature. For both $\\mathcal{P}$ -VAE and $\\mathcal{C}$ -VAE, we start from $T_{\\mathrm{start}}=1.0$ and anneal down to $T_{\\mathrm{stop}}=0.05$ for $\\mathcal{P}$ -VAE, and $T_{\\mathrm{stop}}=0.1$ for $\\mathcal{C}$ -VAE. We found that the $\\mathcal{C}$ -VAE performance was not very sensitive to the choice of $T_{\\mathrm{stop}}$ , corroborating previous reports [89, 98]. ", "page_idx": 21}, {"type": "text", "text": "The $\\mathcal{P}$ -VAE was relatively more sensitive to the value of $T_{\\mathrm{stop}}$ , and we found marginal improvements when we went lower than $T_{\\mathrm{stop}}=0.1$ to $T_{\\mathrm{stop}}=0.05$ . A possible future direction involves defining separate temperatures for the forward and backward passes. For example, setting the forward $T_{\\mathrm{stop}}$ to zero after the annealing period might further improve performance. We leave this as future work. ", "page_idx": 21}, {"type": "text", "text": "B.4 Training: sparse coding models ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To fit LCA and ISTA models, we explored a combination of 6 $\\beta$ schedules (same $\\beta$ as in eq. (1)), 3 numbers of iteration (for inference), 3 learning rates, and 5 different seeds (for dictionary initialization). The code for LCA was obtained from the public python library \u201clca-pytorch\u201d ([131]), and the code for ISTA was obtained from public \u201csparsecoding\u201d repository of the Redwood Center for Theoretical Neuroscience (with added clipping of coefficients to be nonnegative, following the thresholding step). ", "page_idx": 21}, {"type": "text", "text": "We explored learning rates of $1\\times10^{-1}$ , $1\\times10^{-2}$ , and $1\\times10^{-3}$ . We trained all models for 100 epochs. We scheduled the $\\beta$ parameters linearly, starting from $\\beta_{\\mathrm{start}}$ , and stepped it up every five epochs by $\\beta_{\\mathrm{step}}$ , until it reached $\\beta_{\\mathrm{end}}$ . We explored the following $\\beta$ schedules (expressed as $\\beta_{\\mathrm{start}};\\beta_{\\mathrm{end}};\\beta_{\\mathrm{step}})$ : ", "page_idx": 21}, {"type": "text", "text": "We also explored the inference iteration limits of 100, 500, and 900 iterations. We selected the best fits to include in the main results shown in Figs. 4 and 5. ", "page_idx": 21}, {"type": "text", "text": "C Supplementary results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we include additional results that further support those reported in the main paper, including: ", "page_idx": 21}, {"type": "text", "text": "\u2022 Table 4: Contains the negative ELBO values for all VAE models with a linear decoder. This table reveals a comparable performance between using Monte-Carlo samples to estimate gradients, versus optimizing the exact loss (see eqs. (4) and (22) to (24)), highlighting the effectiveness of our Poisson reparameterization algorithm.   \n\u2022 Figure 7: Uses the same data from the Table 4 to visualize the effects.   \n\u2022 Table 5: Contains the full set of downstream classification results. Related to Table 3.   \n\u2022 Figure 8: Shows how the distribution of KL values (or the norm of decoder weights in the case of linear decoders) can be used to determine dead neurons that don\u2019t contribute to the encoding of information.   \n\u2022 Figure 9: Shows MNIST samples generated from the latent space of different VAE models, as well as their reconstruction performance. ", "page_idx": 21}, {"type": "table", "img_path": "ektPEcqGLb/tmp/63c8e54dd4ca53f898117ade9ec2037d44f509ff68d568f6fcf9950cd7f56d70.jpg", "table_caption": ["Table 4: The reparameterized gradient estimators work as well as exact ones, across datasets and encoder architectures (linear vs. conv). Note that exact gradients are only computable for linear decoders (see eqs. (22) to (24)). The values are negative ELBO (lower is better), shown as mean $\\pm99\\%$ confidence interval calculated from $n=5$ different random initializations. EX, exact, MC, MonteCarlo, ST, straight-through [106]. "], "table_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "ektPEcqGLb/tmp/6a824a0274b1850a018d3a88308fecd3ac295d50f085821b9640cfc9fe3cfe84.jpg", "img_caption": ["Figure 7: Performance drop relative to the best fti. Blue circles are $\\mathcal{P}$ -VAE, and red ones are $\\mathcal{G}$ -VAE. There are $n=5$ circles in each condition, corresponding to 5 random initializations. Using MonteCarlo samples [132] and our Poisson reparameterization trick (Algorithm 1) to estimate gradients performs comparably to the situation where exact gradients are available (see eqs. (22) to (24)). EX, exact, MC, Monte-Carlo, ST, straight-through [106]. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "ektPEcqGLb/tmp/6410fa3c557c72de6304502f0b78a2e62a8fbc5b2dc698d8789f593268dd293d.jpg", "img_caption": ["Figure 8: Identifying dead neurons using a histogram-based method. We bin the KL values and determine the gap between small values and larger ones. We identify neurons with KL values lower than the identified threshold (black dashed lines) and pronounce them dead. The figure shows the distribution of KL values over all neurons $\\langle K=512\\rangle$ ) for $\\mathcal{P}$ -VAE, $\\mathcal{G}$ -VAE, and $\\mathcal{L}$ -VAE. The KL term is a single number for the $\\mathcal{C}$ -VAE because its latent space consists of a single one-hot categorical distribution with $K=512$ categories. Therefore, for the $\\mathcal{C}$ -VAE, we use the distribution of decoder weight norms instead. These are the same models shown in Fig. 4, where both encoder and decoder are linear. Table 2 uses this method to quantify the proportion of active neurons for VAEs across different datasets and choice of encoder architectures. "], "img_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "ektPEcqGLb/tmp/5d7c84cdfc3579619b0fd9fd2056ae1bf159c06627cce694aac44c127938466b.jpg", "table_caption": ["Table 5: Geometry of representations. Full set of results. Related to Table 3. "], "table_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "ektPEcqGLb/tmp/df24e2683c5fcd718e43199f22021446b8deb9036e0ecfdaeb0e8cc9bd134a9d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "", "img_caption": [], "img_footnote": ["Figure 9: Generated samples (left) and reconstruction performance (right). These results shown here are from fully convolutional (both encoder and decoder) models with a latent dimensionality of $K=10$ . "], "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: we provide comprehensive theoretical and empirical evidence to support our claims of (1) introducing the $\\mathcal{P}$ -VAE and its reparameterization trick; (2) $\\mathcal{P}$ -VAE containing amortized sparse coding as a special case; (3) $\\mathcal{P}$ -VAE largely avoiding posterior collapse; and (4) $\\mathcal{P}$ -VAE facilitating linear separability of categories at better sample efficiency, in sections 3 and 4, and supplemental appendices A to $\\mathbf{C}$ . ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The limitations of (1) Poisson possibly not being a perfect description of cortical activity, and (2) amortization gap, are shown explicitly and thoroughly discussed in sections 4 and 5. Specifically, we have a dedicated paragraph for limitations in section 5. We evaluated our claims using multiple well-known datasets such as the van Hateren natural images [102], CIFAR10, and MNIST, on tasks such as reconstruction, sparse coding, and classification. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 25}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide the full derivation of the $\\mathcal{P}$ -VAE loss function, which is selfcontained in the paper (section 3) and supplement (appendix A). ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We disclose all details relating to the algorithm, including the optimization objective (eq. (3)), architecture and training details (appendix B), and pseudo-code for Poisson reparameterized sampling (Algorithm 1). In addition, we intend to release all code and data needed for replicating our work. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our code, data, and model checkpoints are available from the following GitHub repository: https://github.com/hadivafaii/PoissonVAE. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All details about how the data was used for training and testing, as well as which hyperparameters were used, are available at appendix B. In addition, the provided code replicates our results and therefore contains all details of implementation. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: As stated in section 4, the paper reports confidence intervals and $t$ -test significance tests, using false discovery rate (FDR) correction for multiple comparisons. The exact implementation details are included in the provided code for reproducibility. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide details about our compute resources (GPUs), and duration of training in section 4. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our paper follows the code of ethics, including preserving anonymity (such as in releasing code anonymously). ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our paper is considered foundational research, and does not target practical tasks that can be deployed outside of the research field. Thus we do not anticipate negative social impacts from this work. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our paper utilizes publicly domain datasets (not scraped), and poses no safety risks. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We properly cite papers that introduce algorithms (such as LCA), datasets (such as MNIST, CIFAR10, van Hateren), and code (such as LCA and ISTA). ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: New assets introduced in the paper consist of our codebase which includes notebooks to replicate our experiments and analyses, and contains documentation. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]