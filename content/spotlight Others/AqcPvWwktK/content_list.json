[{"type": "text", "text": "Semi-supervised Multi-label Learning with Balanced Binary Angular Margin Loss ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ximing $\\mathbf{Li}^{1,2}$ Silong Liang1,2 Changchun $\\mathbf{L}\\mathbf{i}^{1,2,*}$ Pengfei Wang3,4 Fangming $\\mathbf{Gu^{1,2}}$ ", "page_idx": 0}, {"type": "text", "text": "1College of Computer Science and Technology, Jilin University, China   \n2Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education, Jilin University, China 3Computer Network Information Center, Chinese Academy of Sciences, China 4University of Chinese Academy of Sciences, Chinese Academy of Sciences, China   \nliximing86@gmail.com, changchunli93@gmail.com, liangsl23@mails.jlu.edu.cn, pfwang@cnic.cn, gufm@jlu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Semi-supervised multi-label learning (SSMLL) refers to inducing classifiers using a small number of samples with multiple labels and many unlabeled samples. The prevalent solution of SSMLL involves forming pseudo-labels for unlabeled samples and inducing classifiers using both labeled and pseudo-labeled samples in a self-training manner. Unfortunately, with the commonly used binary type of loss and negative sampling, we have empirically found that learning with labeled and pseudo-labeled samples can result in the variance bias problem between the feature distributions of positive and negative samples for each label. To alleviate this problem, we aim to balance the variance bias between positive and negative samples from the perspective of the feature angle distribution for each label. Specifically, we extend the traditional binary angular margin loss to a balanced extension with feature angle distribution transformations under the Gaussian assumption, where the distributions are iteratively updated during classifier training. We also suggest an efficient prototype-based negative sampling method to maintain high-quality negative samples for each label. With this insight, we propose a novel SSMLL method, namely Semi-Supervised Multi-Label Learning with Balanced Binary Angular Margin loss $\\mathbf{[S^{2}\\bar{M}L^{2}}$ -BBAM). To evaluate the effectiveness of $\\mathrm{S^{2}M L^{2}}$ - BBAM, we compare it with existing competitors on benchmark datasets. The experimental results validate that $\\bar{\\mathbf{S}^{2}}\\mathbf{M}\\mathbf{L}^{2}$ -BBAM can achieve very competitive performance. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multi-label learning (MLL) refers to the classification problem where each training sample can be associated with multiple labels [1]. For example, in text categorization, a text can involve a certain number of topics simultaneously [2, 3]; and in image annotation, an image can contain multiple objects of interest in one scene [4, 5]. Compared with single-label learning, MLL is a more prevalent paradigm in real-world scenarios, and it has been widely used in many applications such as information retrieval [6, 7] and recommendation systems [8, 9]. ", "page_idx": 0}, {"type": "image", "img_path": "AqcPvWwktK/tmp/5a63de586c0f8abc946758b2d0123b7af16f96791a6e5892f7ffba02e981dae1.jpg", "img_caption": ["Figure 1: The variance difference between feature distributions (VDFD) of positive and negative samples computed in semi-supervised and supervised manners across labels $\\{6,7,14,17\\}$ of VOC2012. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Despite the successful application of MLL, the competitive performance of most MLL methods heavily depends on the large volume of training samples with precise supervision [4, 10, 11]. Unfortunately, it is expensive to manually annotate each sample, so it is naturally time-consuming to collect loads of labeled training samples. Accordingly, the community has turned to alternative candidates to MLL, and raised the question of whether one can induce robust MLL classifiers with a small number of labeled samples and a large number of unlabeled samples, which are cheaper to collect. This concept gives birth to the emerging research topic of semi-supervised multi-label learning (SSMLL), and many attempts have been recently proposed [12, 13, 14, 15, 16, 17]. ", "page_idx": 1}, {"type": "text", "text": "Generally, the topic of SSMLL, as its name suggests, is in parallel inherited from semi-supervised learning (SSL) and MLL. The current prevalent ideas are estimating pseudo-labels of unlabeled samples with SSL techniques and inducing MLL classifiers with both labeled and pseudo-labeled samples in a self-training manner. Following the prior arts [18, 19], the binary kind of losses, e.g. binary cross-entropy loss and asymmetric loss [20], are commonly used to optimize MLL classifiers, where those are equivalent to optimizing the binary loss between the positive and negative samples for each label. To alleviate the imbalanced issue between positive and negative samples, especially for the scenarios with massive labels, the negative sampling tricks are often employed [21, 22, 23]. Unfortunately, in our preliminary experiments, we found such training paradigms suffer from the variance bias problem by using the labeled and pseudo-labeled samples in the context of SSMLL, since it is difficult to guarantee estimating accurate pseudo-labels. To be specific, the problem implies that for each label, in SSMLL the variance difference between feature distributions of positive and negative samples is often larger than the ones in fully supervised learning, as illustrated in Fig.1. In this situation, each trained binary boundary tends to keep away from the Bayesian optimal one, resulting in performance degradation. ", "page_idx": 1}, {"type": "text", "text": "To tackle this problem, we propose a novel SSMLL method, namely Semi-Supervised Multi-Label Learning with Balanced Binary Angular Margin loss $\\mathbf{S^{2}M L^{2}}$ -BBAM). The basic insight of $\\mathrm{S^{2}M L^{2}}$ - BBAM is to balance the variance bias between positive and negative samples from the perspective of the feature angle distribution for each label. To be specific, we extend the binary angular margin (BAM) loss, which measures the prediction loss by using the angle between the feature and binary boundary for each label. We suppose that for each label these feature angles of positive and negative samples are drawn from label-specific \u201cpositive\u201d and \u201cnegative\u201d Gaussian distributions, which are estimated by employing both labeled and pseudo-labeled samples during classifier training. Therefore, we can apply some linear Gaussian transformations over these feature angle distributions, so as to balance the variance bias between positive and negative samples for each label. Upon this idea, we design a new balanced binary angular margin (BBAM) loss and construct a novel $\\mathrm{S^{2}M L^{2}}$ -BBAM method based on the designed BBAM loss and self-training manner. We also suggest an efficient prototype-based negative sampling method to maintain high-quality negative samples for each label. We evaluate the proposed $\\mathrm{S^{2}M L^{2}}$ -BBAM by comparing the most recent competitors on benchmark datasets. Experimental results indicate the superior performance of $\\mathrm{S^{2}M L^{2}}$ -BBAM. ", "page_idx": 1}, {"type": "text", "text": "In summary, the main contributions of this paper are listed as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We develop a novel SSMLL method, namely $\\mathrm{S^{2}M L^{2}}$ -BBAM, by balancing the variance bias between positive and negative samples from the perspective of the feature angle distribution for each label. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We design a new BBAM loss by extending the traditional binary angular margin loss with feature angle distribution transformations under the Gaussian assumption, and suggest an efficient prototype-based negative sampling method to maintain high-quality negative samples for each label.   \n\u2022 We construct extensive experiments to evaluate $\\mathrm{S^{2}M L^{2}}$ -BBAM, and experimental results demonstrate the effectiveness of $\\mathrm{S^{2}M L^{2}}$ -BBAM. ", "page_idx": 2}, {"type": "text", "text": "2 Formulation and Analysis ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "By convention, we use $\\mathbf{x}$ to denote the sample feature vector and $\\mathbf{y}\\in\\{0,1\\}^{K}$ the label indicator vector of $K$ pre-defined classes, where $0/1$ implies a sample is irrelevant/relevant to the category. In the task of SSMLL, we are formally given a collection of training samples $\\boldsymbol{\\mathcal{D}}=\\{\\mathcal{D}_{l},\\mathcal{D}_{u}\\}$ , where $\\mathcal{D}_{l}\\,=\\,\\{(\\mathbf{x}_{i}^{l},\\mathbf{y}_{i}^{l})\\}_{i=1}^{i=N_{l}}$ and $\\mathcal{D}_{u}\\;=\\;\\{\\mathbf{x}_{j}^{u}\\}_{j=1}^{j=N_{u}}$ are the collections of $N_{l}$ labeled and $N_{u}$ unlabeled samples, respectively. The goal of SSMLL is to induce a classifier $f_{\\mathbf{W}}(\\mathbf{x})$ , parameterized by $\\mathbf{W}$ , from $\\mathcal{D}$ and use the classifier $f_{\\mathbf{W}}(\\mathbf{x})$ to predict the label indicator vectors for future samples. ", "page_idx": 2}, {"type": "text", "text": "Broadly speaking, the classifier $f_{\\mathbf{W}}(\\mathbf{x})$ typically consists of a backbone encoder and a classification layer, parameterized by ${\\bf W}^{e}$ and ${\\bf W}^{c}$ , respectively (i.e. $\\mathbf{W}=\\{\\mathbf{W}^{e},\\mathbf{W}^{c}\\}$ . Specifically, the backbone encoder transforms any original feature vector $\\mathbf{x}$ into a more discriminative latent feature $\\mathbf{z}=f_{\\mathbf{W}^{e}}(\\mathbf{x})$ ; the classification layer applies $\\mathbf{z}$ to generate its corrsponding predictive logits $\\mathbf{p}=f_{\\mathbf{W}^{c}}(\\mathbf{z})$ . Given an SSMLL training dataset $\\mathcal{D}$ , the classifier $f_{\\mathbf{W}}(\\mathbf{x})$ is commonly optimized by minimizing the following generic self-training objective concerning $\\mathbf{W}$ on $B_{l}$ -sized labeled and $B_{u}$ -sized unlabeled batches: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mathbf{W})=\\frac{1}{B_{l}K}\\sum_{i=1}^{B_{l}}\\sum_{k=1}^{K}\\ell(p_{i k}^{l},y_{i k}^{l})+\\frac{\\lambda}{B_{u}K}\\sum_{i=1}^{B_{u}}\\sum_{k=1}^{K}\\ell(p_{i k}^{u},y_{i k}^{u}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\ell(\\cdot,\\cdot)$ is a binary loss function; $\\lambda$ is the coefficient parameter; $\\mathbf{p}_{i}^{l}=f_{\\mathbf{W}}(\\mathbf{x}_{i}^{l})$ and $\\mathbf{p}_{i}^{u}=f_{\\mathbf{W}}(\\mathbf{x}_{i}^{u})$ are the predictive logits of labeled and unlabeled samples, respectively; $\\mathbf{y}_{i}^{u}$ is the pseudo-label of unlabeled samples induced from its current classifier prediction $\\mathbf{p}_{i}^{u}$ . ", "page_idx": 2}, {"type": "text", "text": "2.2 How Variance Bias Affects the Performance ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As shown in Fig.1, we have observed that the generic self-training objective of SSMLL may suffer from the variance bias problem. Here, we discuss how it will affect the classification performance. We treat SSMLL as $K$ independent semi-supervised binary classification (SSBC) tasks. For each SSBC task, let $\\{(\\mathbf{x}_{i},y_{i}^{*})\\}\\cup\\{\\mathbf{x}_{i}\\}$ be the training data, where $\\textbf{x}\\in\\mathbb{R}^{d}$ and $y^{*}\\in\\{-1,+1\\}$ is the ground-truth label. Besides, let $\\hat{y}\\in\\{-1,+1\\}$ be the pseudo-label. For clarity and conciseness, we study the SSBC training data drawn from a mixture Gaussian distribution $\\mathcal{P}^{*}$ , which can be defined by the following distribution over $\\left(\\mathbf{x},y\\right)\\in\\mathbb{R}^{d}\\times\\{\\pm1\\}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\ny=\\left\\{\\!\\!\\begin{array}{l l}{+1,\\ p=\\alpha,}\\\\ {-1,\\ p=1-\\alpha,}\\end{array}\\right.\\quad\\mathbf{x}\\sim\\left\\{\\!\\!\\begin{array}{l l}{N(\\mu,\\Sigma_{+}^{2})}&{\\mathrm{if}\\ y=+1;}\\\\ {N(-\\mu,\\Sigma_{-}^{2})}&{\\mathrm{if}\\ y=-1,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\Sigma_{-}\\,=\\,\\mathrm{diag}\\bigl(\\{\\sigma_{-}^{(1)},\\dots,\\sigma_{-}^{(d)}\\}\\bigr)$ $\\alpha$ is the prior probability of class , $\\mu_{i},\\sigma_{-}^{(i)},\\sigma_{+}^{(i)}\\,>\\,0\\,\\,\\forall i\\,\\in\\,[d]$ $\"+1\"$ , $\\pmb{\\mu}=\\{\\mu_{1},\\dots,\\mu_{d}\\}^{\\top}$ , and $\\begin{array}{r}{\\sum_{i=1}^{d}(\\sigma_{+}^{(i)})^{2}:\\sum_{i=1}^{d}(\\sigma_{-}^{(i)})^{2}=1:}\\end{array}$ , $\\Sigma_{+}=\\mathrm{diag}(\\{\\sigma_{+}^{(1)},\\dots,\\sigma_{+}^{(d)}\\})$ $M^{2}$ with $M>0,M\\neq1$ . We concentrate on analyzing the effect of the varian ce proportion $M$ of the distribution $\\mathcal{D}^{*}$ on the performance of the linear model $f_{s s l}(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w},\\mathbf{x}\\rangle+b)$ , where the parameters $\\mathbf{w}\\in\\mathbb{R}^{d},b\\in\\mathbb{R}$ , and $\\mathrm{sign}(t)$ evaluates to $+1$ if scalar $t\\geq0$ and to $-1$ otherwise. For simplicity, we denote ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{L}(f,+1)=\\mathbb{E}_{(\\mathbf{x},\\mathfrak{y})\\sim\\mathcal{P}^{*}}[\\mathbb{1}(f(\\mathbf{x})=-1)|\\boldsymbol{y}=+1],\\ \\mathcal{R}(f,-1)=\\mathbb{E}_{(\\mathbf{x},\\mathfrak{y})\\sim\\mathcal{P}^{*}}[\\mathbb{1}(f(\\mathbf{x})=+1)|\\boldsymbol{y}=-1],}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbb{1}(t)$ is the indicator function that takes 1 where t is true and 0 otherwise. We have the following theorems, whose proof can be found in the Appendix A. ", "page_idx": 2}, {"type": "text", "text": "Theorem 2.1. Given an SSBC dataset with pseudo-labels $S=\\{(\\mathbf{x}_{i},y_{i})\\}=\\{(\\mathbf{x}_{i},y_{i}^{*})\\}\\cup\\{(\\mathbf{x}_{i},\\widehat{y}_{i})\\},$ , the optimal linear classifier $f_{s s l}$ minimizing the average standard classification error is given b  y: ", "page_idx": 2}, {"type": "equation", "text": "$$\nf_{s s l}=\\underset{f}{\\arg\\operatorname*{min}}\\,\\mathbb{E}_{(\\mathbf{x},y)\\sim S}[\\mathbb{1}(f(\\mathbf{x})\\neq y)].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "When $M>1$ , it has the intra-class standard classification errors for the two classes : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}(f_{s s l},+1)=\\Phi\\big(A-M\\sqrt{A^{2}+q(M,\\alpha,\\epsilon_{-},\\epsilon_{+})}\\big),}\\\\ &{\\mathcal{R}(f_{s s l},-1)=\\Phi\\big({-M\\cdot A+\\sqrt{A^{2}+q(M,\\alpha,\\epsilon_{-},\\epsilon_{+})}}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and when $M<1$ , they are given by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}(f_{s s l},+1)=\\Phi\\big(A+M\\sqrt{A^{2}+q(M,\\alpha,\\epsilon_{-},\\epsilon_{+})}\\big),}\\\\ &{\\mathcal{R}(f_{s s l},-1)=\\Phi\\big({-M\\cdot A-\\sqrt{A^{2}+q(M,\\alpha,\\epsilon_{-},\\epsilon_{+})}}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Phi(\\cdot)$ is the cumulative distribution function $(c.d.f.)$ of standard Gaussian distribution ${\\mathcal{N}}(0,1)$ , $\\begin{array}{r}{A\\ =\\ \\frac{2\\mu}{(M^{2}-1)\\Sigma},\\quad q(M,\\alpha,\\epsilon_{-},\\epsilon_{+})\\ =\\ \\frac{2\\log M+2C}{M^{2}-1},\\ C\\ =\\ \\log\\bigl(\\frac{\\alpha(2-\\epsilon_{-}-2\\epsilon_{+})}{(1-\\alpha)(2-2\\epsilon_{-}-\\epsilon_{+})}\\bigr)}\\end{array}$ , $\\mu\\;=\\;\\sum_{i=1}^{i=d}\\mu_{i}$ $\\Sigma\\;=\\;\\sqrt{\\sum_{i=1}^{i=d}(\\sigma_{+}^{(i)})^{2}}$ , and $\\{\\epsilon_{-},\\epsilon_{+}\\}$ are the proportions of negative instances being treated as positive ones and positive instances being treated as negative ones within pseudo-labels, respectively. $\\begin{array}{r}{I\\!\\!f\\sum_{i=1}^{d}(\\sigma_{+}^{(i)})^{2}=\\sum_{i=1}^{d}(\\sigma_{-}^{(i)})^{2}}\\end{array}$ , i.e. $M=1$ , the intra-class standard classification errors for the two classes can be expressed as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{R}(f_{s s l},+1)=\\Phi\\big(\\frac{-2\\mu^{2}-C\\Sigma^{2}}{2\\mu\\Sigma}\\big),\\quad\\mathcal{R}(f_{s s l},-1)=\\Phi\\big(\\frac{-2\\mu^{2}+C\\Sigma^{2}}{2\\mu\\Sigma}\\big).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Following [24, 25, 26], We employ variance of class-wise accuracy (VCA) to quantitatively measure the model fairness and present the definition of VCA below. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.2. (VCA) Given a classifier $f:\\mathcal{X}\\to\\mathcal{Y}$ where $\\mathcal{V}=\\{1,2,3,\\cdots\\,,K\\}$ , the variance of class-wise accuracy of $f$ is defined as $\\begin{array}{r}{V C A(f)=\\frac{1}{K}\\sum_{i=1}^{K}(p(i)-\\Bar{p})}\\end{array}$ , where $p(i)=\\mathbb{P}[f(\\mathbf{x})=$ $i|y=i|=1-\\mathbb{P}[f(\\mathbf{x})\\neq i|y=i]$ and $\\begin{array}{r}{\\bar{p}=\\frac{1}{K}\\sum_{i=1}^{K}p(i)}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Theorem 2.3. Given an trained linear SSBC model $f_{s s l}$ in Eq.(3), the variance of class-wise accuracy $V C A(f_{s s l})$ is increasing when $M\\to\\infty$ for $M>1$ and $M\\rightarrow0$ for $M<1$ . Suppose $\\begin{array}{r}{\\log\\bigl(\\frac{\\alpha(2-\\epsilon_{-}-2\\epsilon_{+})}{(1-\\alpha)(2-2\\epsilon_{-}-\\epsilon_{+})}\\bigr)=0,}\\end{array}$ , then when $M=1$ , $\\mathcal{R}(f_{s s l},+1)=\\mathcal{R}(f_{s s l},-1)$ and $V C A(f_{s s l})=0$ . ", "page_idx": 3}, {"type": "text", "text": "Remark 2.4. According to Theorem 2.3, the bigger or smaller value of $M$ will result in the increase of the variance of class-wise accuracy $V C A(f_{s s l})$ , which implies that the SSBC classifier $f_{s s l}$ induced by Eq.(3) is unfair. Note that $M$ is the variance proportion of feature distributions of positive and negative samples as defined in (2). Therefore, to improve the fairness of the induced classifier, we propose to balance the variance bias of positive and negative samples for each label from the feature angle distribution perspective, leading to our $\\mathrm{S^{2}M L^{2}}$ -BBAM. ", "page_idx": 3}, {"type": "text", "text": "3 Proposed $\\mathbf{S}^{2}\\mathbf{M}\\mathbf{L}^{2}$ -BBAM Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we introduce the proposed SSMLL method named $\\mathbf{S}^{2}\\mathbf{M}\\mathbf{L}^{2}$ -BBAM. ", "page_idx": 3}, {"type": "text", "text": "3.1 Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Generally, our $\\mathrm{S^{2}M L^{2}}$ -BBAM is built on the generic self-training objective of SSMLL formulated by Eq.1. Specifically, we propose a novel Balanced Binary Angular Margin (BBAM) loss $\\ell_{\\mathrm{BBAM}}(\\cdot,\\cdot)$ , aiming to balance the variance bias of positive and negative samples for each label from the feature angle distribution perspective with the Gaussian assumption. By applying our proposed BBAM loss to the generic SSMLL self-training objective in Eq.1, the objective of $\\mathrm{S^{2}M L^{2}}$ -BBAM can be formulated as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mathbf{W})=\\frac{1}{B_{l}K}\\sum_{i=1}^{B_{l}}\\sum_{k=1}^{K}\\beta_{i k}\\ell_{\\mathrm{BBAM}}(p_{i k}^{l},y_{i k}^{l})+\\frac{\\lambda}{B_{u}K}\\sum_{i=1}^{B_{u}}\\sum_{k=1}^{K}\\beta_{i k}\\ell_{\\mathrm{BBAM}}(p_{i k}^{u},y_{i k}^{u}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\beta_{i k}=\\left\\{\\begin{array}{l l}{1}&{\\mathrm{if}\\,\\left(\\mathbf{x}_{i},\\mathbf{y}_{i}\\right)\\in\\Omega_{k};}\\\\ {1}&{\\mathrm{if}\\,\\,y_{i k}=1;}\\\\ {0}&{\\mathrm{otherwise},}\\end{array}\\right.\\quad\\forall k\\in[K],\\ \\forall i\\in[N_{l}]\\mathrm{\\or\\}[N_{u}],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and {\u2126k}kk==1K denotes high-quality negative sample sets constructed by negative sampling. ", "page_idx": 4}, {"type": "text", "text": "Here, pseudo-labels of unlabeled data $\\{\\mathbf{y}_{i}^{u}\\}_{i=1}^{i=N_{u}}$ are produced by employing the Class-Aware Pseudolabeling (CAP) trick [16], which drives their label distribution towards the prior one that is estimated with the labeled samples. Specifically, given the current classifier predictions $\\{\\mathbf{p}_{i}^{u}\\}_{i=1}^{i=N_{u}}$ of unlabeled samples, {yiu }i=1 are given by: ", "page_idx": 4}, {"type": "equation", "text": "$$\ny_{i k}^{u}=\\left\\{\\begin{array}{l l}{1\\quad\\mathrm{if}\\ p_{i k}^{u}>=\\delta_{k};}\\\\ {0\\quad\\mathrm{if}\\ p_{i k}^{u}<=\\gamma_{k};}\\\\ {-1\\quad\\mathrm{otherwise},}\\end{array}\\right.\\quad\\quad\\forall k\\in[K],\\ \\forall i\\in[N_{u}],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the class-aware thresholds $\\{\\delta_{k}\\}_{k=1}^{k=K}$ and $\\{\\gamma_{k}\\}_{k=1}^{k=K}$ are calculated by solving the equations: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\frac{\\sum_{i=1}^{N_{u}}\\mathbb{1}(p_{i k}^{u}>=\\delta_{k})}{N_{u}}=\\frac{\\sum_{i=1}^{N_{l}}\\mathbb{1}(y_{i k}^{l}=1)}{N_{l}},}\\\\ {\\sum_{i=1}^{N_{u}}\\mathbb{1}(p_{i k}^{u}\\!<\\!=\\!\\gamma_{k})}\\\\ {\\frac{\\sum_{i=1}^{N_{u}}\\mathbb{1}(p_{i k}^{u}\\!<\\!=\\!\\gamma_{k})}{N_{u}}=\\frac{\\sum_{i=1}^{N_{l}}\\mathbb{1}(y_{i k}^{l}=0)}{N_{l}},}\\end{array}\\right.\\quad\\forall k\\in[K],\\ \\forall i\\in[N_{u}],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and $y_{i k}^{u}=-1$ means that it will not be used for the classifier training. ", "page_idx": 4}, {"type": "text", "text": "3.2 BBAM loss ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we introduce the proposed BBAM loss. As its name suggests, our BBAM loss is extended from the Binary Angular Margin (BAM) loss, which measures the label-specific prediction risk by using the angle between the latent feature and boundary. Formally, for a training sample $\\left(\\mathbf{x}_{i},\\mathbf{y}_{i}\\right)$ , the BAM loss can be formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{BAM}}(p_{i k},y_{i k})=\\left\\{\\begin{array}{l l}{-\\log(\\frac{1}{1+e^{-s*(p_{i k}-m)}})}&{\\mathrm{if}\\ y_{i k}=1;}\\\\ {-\\log(1-\\frac{1}{1+e^{-s*(p_{i k}-m)}})}&{\\mathrm{if}\\ y_{i k}=0,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{p_{i k}=\\cos(\\theta_{i k})=\\frac{\\mathbf{z}_{i}^{\\top}\\mathbf{W}_{k}^{c}}{\\|\\mathbf{z}_{i}\\|_{2}\\|\\mathbf{W}_{k}^{c}\\|_{2}}}\\end{array}$ , $\\lVert\\cdot\\rVert_{2}$ is the $\\ell_{2}$ -norm of vectors; $\\mathbf{z}_{i}$ and $\\mathbf{W}_{k}^{c}$ denote the latent feature of sample $i$ and the weight vector of the classification layer for category $k$ , respectively; is the angle between $\\mathbf{z}_{i}$ and $\\mathbf{W}_{k}^{c}$ ; $s$ and $m$ are the parameters used to control the rescaled norm and magnitude of cosine margin, respectively. ", "page_idx": 4}, {"type": "text", "text": "Reviewing the BAM loss in Eq.6, one can observe that it calculates the loss by employing the label angles of samples for each category. We consider that its trained binary boundary tends to deviate from the Bayesian optimal one for each category in SSMLL, where for most categories, the differences between feature distribution variances of corresponding positive and negative samples are much larger than ones in fully supervised learning. To address this issue, for each category $k$ , we suppose that label angles of its positive samples and ones of its negative samples are drawn from a label-specific \u201cpositive\u201d Gaussian distribution $\\mathcal{N}(\\mu_{k}^{\\left(p\\right)},\\left(\\sigma_{k}^{2}\\right)^{\\left(p\\right)})$ and a label-specific \u201cnegative\u201d one $\\mathcal{N}(\\mu_{k}^{(n)},(\\sigma_{k}^{2})^{(n)})$ , respectively. According to the properties of Gaussian distribution, we can easily transfer them into ones N(\u00b5(kp ), \u03c3 k2) and N(\u00b5(kn ), \u03c3 k2) with balanced variance \u03c3 k2 = (\u03c3k2)(p)+2(\u03c3k2)(n), by performing the following Gaussian linear transformations on those label angles: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\psi_{k}^{(p)}(\\theta_{i k})=a_{k}^{(p)}\\theta_{i k}+b_{k}^{(p)},\\quad\\psi_{k}^{(n)}(\\theta_{i k})=a_{k}^{(n)}\\theta_{i k}+b_{k}^{(n)},}\\\\ {a_{k}^{(p)}=\\displaystyle\\frac{\\widehat{\\sigma}_{k}}{\\sigma_{k}^{(p)}},\\quad b_{k}^{(p)}=(1-a_{k}^{(p)})\\mu_{k}^{(p)},\\quad a_{k}^{(n)}=\\displaystyle\\frac{\\widehat{\\sigma}_{k}}{\\sigma_{k}^{(n)}},\\quad b_{k}^{(n)}=(1-a_{k}^{(n)})\\mu_{k}^{(n)},\\quad\\forall k\\in[K].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "With these linear transformation pairs $\\{(\\psi_{k}^{(p)}(\\cdot),\\psi_{k}^{(n)}(\\cdot))\\}$ , for each category, label angles of both positive and negative samples can be refined into ones drawn from balanced angular distributions with one same variance, e.g. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\psi_{k}^{(p)}(\\theta_{i k})\\sim\\mathcal{N}(\\mu_{k}^{(p)},\\widehat\\sigma_{k}^{2})\\quad\\mathrm{if}\\;y_{i k}=1;\\quad\\psi_{k}^{(n)}(\\theta_{i k})\\sim\\mathcal{N}(\\mu_{k}^{(n)},\\widehat\\sigma_{k}^{2})\\quad\\mathrm{if}\\;y_{i k}=0.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Accordingly, the BAM loss in Eq.6 can be rewritten as the following BBAM loss: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{BBAM}}(p_{i k},y_{i k})=\\left\\{\\begin{array}{l l}{-\\log\\bigl(\\frac{1}{1+e^{-s*(\\cos(\\psi_{k}^{(p)}(\\theta_{i k}))-m)}}\\bigr)}&{\\mathrm{if}\\;y_{i k}=1;}\\\\ {-\\log\\bigl(1-\\frac{1}{1+e^{-s*(\\cos(\\psi_{k}^{(n)}(\\theta_{i k}))-m)}}\\bigr)}&{\\mathrm{if}\\;y_{i k}=0.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Estimating label angle variances. As mentioned above, we concentrate on estimating label-specific \u201cpositive\u201d and \u201cnegative\u201d angular distributions, i.e. $\\{\\mathcal{N}(\\mu_{k}^{(p)},(\\sigma_{k}^{2})^{(p)})\\}_{k=1}^{k=K}$ and $\\{\\mathcal{N}(\\mu_{k}^{(n)},(\\sigma_{k}^{2})^{(n)})\\}_{k=1}^{k=K}$ e,s  foofr  itesa ccohr rceastpeognordiyn gw phoossieti vder aawnsd  nareeg atthivee  asnagmlepsl ebs,e trewsepeenc tiitvse llya. bHele rper, otwoe$\\mathbf{c}_{k}$   \napproximate $\\{(\\mu_{k}^{(p)},(\\sigma_{k}^{2})^{(p)})\\}_{k=1}^{k=K}$ , $\\{(\\mu_{k}^{(n)},(\\sigma_{k}^{2})^{(n)})\\}_{k=1}^{k=K}$ , and $\\{{\\bf c}_{k}\\}_{k=1}^{k=K}$ with labeled and pseudolabeled samples per-epoch. ", "page_idx": 5}, {"type": "text", "text": "For convenience, we denote $\\mathfrak{D}\\,=\\,\\{(\\mathbf{z}_{i},\\mathbf{y}_{i})\\}_{i=1}^{i=N_{l}+N_{u}}\\,=\\,\\{(\\mathbf{z}_{i}^{l},\\mathbf{y}_{i}^{l})\\}_{i=1}^{i=N_{l}}\\,\\cup\\,\\{(\\mathbf{z}_{i}^{u},\\mathbf{y}_{i}^{u})\\}_{i=1}^{i=N_{u}}$ as the couple set of latent features and labels or pseudo-labels of training samples $\\mathcal{D}$ in the current epoch. We calculate label prototypes $\\{{\\bf c}_{k}\\}_{k=1}^{k=K}$ by averaging latent features of positive samples in $\\mathfrak{D}$ as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{c}_{k}=\\frac{\\sum_{i=1}^{N_{l}+N_{u}}\\mathbb{1}\\big(y_{i k}=1\\big)\\mathbf{z}_{i}}{\\sum_{i=1}^{N_{l}+N_{u}}\\mathbb{1}\\big(y_{i k}=1\\big)},\\;\\forall k\\in[K].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Consequently, the label angles between label prototypes and latent features of samples are given by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\phi_{i k}=\\operatorname{arccos}({\\frac{{\\mathbf{z}_{i}}^{\\top}\\mathbf{c}_{k}}{\\|\\mathbf{z}_{i}\\|_{2}\\|\\mathbf{c}_{k}\\|_{2}}}),\\;\\forall k\\in[K],\\;\\forall i\\in[N_{l}+N_{u}],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Accordingly, the estimations of $\\{(\\mu_{k}^{(p)},(\\sigma_{k}^{2})^{(p)})\\}_{k=1}^{k=K}$ and $\\{(\\mu_{k}^{(n)},(\\sigma_{k}^{2})^{(n)})\\}_{k=1}^{k=K}$ based on the current negative sample sets $\\{\\Omega_{k}\\}_{k=1}^{k=K}$ can be formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{k}^{(p)}=\\frac{\\sum_{i=1}^{N_{l}+N_{u}}\\mathbb{1}(y_{i k}=1)\\phi_{i k}}{\\sum_{i=1}^{N_{l}+N_{u}}\\mathbb{1}(y_{i k}=1)},\\qquad(\\sigma_{k}^{2})^{(p)}=\\frac{\\sum_{i=1}^{N_{l}+N_{u}}\\mathbb{1}(y_{i k}=1)(\\phi_{i k}-\\mu_{k}^{(p)})^{2}}{\\sum_{i=1}^{N_{l}+N_{u}}\\mathbb{1}(y_{i k}=1)-1},}\\\\ &{\\mu_{k}^{(n)}=\\frac{\\sum_{i=1}^{N_{l}+N_{u}}\\beta_{i k}\\mathbb{1}(y_{i k}=0)\\phi_{i k}}{\\sum_{i=1}^{N_{l}+N_{u}}\\beta_{i k}\\mathbb{1}(y_{i k}=0)},\\qquad(\\sigma_{k}^{2})^{(n)}=\\frac{\\sum_{i=1}^{N_{l}+N_{u}}\\beta_{i k}\\mathbb{1}(y_{i k}=0)(\\phi_{i k}-\\mu_{k}^{(n)})^{2}}{\\sum_{i=1}^{N_{l}+N_{u}}\\beta_{i k}\\mathbb{1}(y_{i k}=0)-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Besides, to avoid the misleading effect of false positive or negative samples, we also employ moving average with a learning rate $\\rho$ over $\\{(\\mu_{k}^{(p)},(\\sigma_{k}^{2})\\bar{(p)})\\}_{k=1}^{k=K}$ , $\\{(\\stackrel{\\cdot}{\\mu}_{k}^{(n)},(\\sigma_{k}^{2})\\stackrel{\\cdot}{^(n)})\\}_{k=1}^{k=K}$ , and $\\{{\\bf c}_{k}\\}_{k=1}^{k=K}$ . ", "page_idx": 5}, {"type": "text", "text": "3.3 Negative Sampling ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "For efficiency, we suggest a prototype-based negative sampling method. Specifically, for each label, we tend to select those negative samples that are more similar to its positive samples, because they are more difficult to discriminate and would be more informative for the classifier training [21, 22]. To achieve this, for each category, we measure similarity scores of negative samples based on label prototypes $\\{{\\bf c}_{k}\\}_{k=1}^{k=K}$ , and construct the nearest neighbor negative sample sets $\\{\\widetilde{\\Omega}_{k}\\}_{k=1}^{k=K}$ as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widetilde{\\Omega}_{k}=\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})|d(\\mathbf{z}_{i},\\mathbf{c}_{k})\\in\\mathrm{Rank}(\\{d(\\mathbf{z}_{i},\\mathbf{c}_{k})\\}_{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\in\\widehat{\\Omega}_{k}}),(\\mathbf{x}_{i},\\mathbf{y}_{i})\\in\\widehat{\\Omega}_{k}\\}\\quad\\forall k\\in[K],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $d(\\cdot)$ is the vector distance (e.g. cosine distance), $\\mathrm{{Rank}(\\cdot)}$ outputs a set of samples with the top- $M$ minmum distance values; and $\\{\\widehat{\\Omega}_{k}\\}_{k=1}^{k=K}$ is the negative sample set of category $k$ defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{\\Omega}_{k}=\\{(\\mathbf{x}_{i}^{l},\\mathbf{y}_{i}^{l})|(\\mathbf{x}_{i}^{l},\\mathbf{y}_{i}^{l})\\in\\mathcal{D}_{l},y_{i k}^{l}=0\\}\\cup\\{(\\mathbf{x}_{i}^{u},\\mathbf{y}_{i}^{u})|\\mathbf{x}_{i}^{u}\\in\\mathcal{D}_{u},y_{i k}^{u}=0\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Accordingly, the final negative sample sets $\\{\\Omega_{k}\\}_{k=1}^{k=K}$ are generated by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Omega_{k}=\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})|(\\mathbf{x}_{i},\\mathbf{y}_{i})\\sim\\mathrm{Uniform}(\\widetilde\\Omega_{k})\\}\\quad\\forall k\\in[K],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with size $\\{|\\Omega_{k}|=\\eta N_{k}\\}_{k=1}^{k=K}$ , where $\\begin{array}{r}{N_{k}=\\sum_{i=1}^{N_{l}}\\mathbb{1}(y_{i k}^{l}=1)+\\sum_{i=1}^{N_{u}}\\mathbb{1}(y_{i k}^{u}=1)}\\end{array}$ , $\\eta$ controls the proportion of positive and negative samples  of each category. And  we update those negative sample sets $\\{\\Omega_{k}\\}_{k=1}^{k=K^{\\star}}$ per-epoch for efficiency. ", "page_idx": 5}, {"type": "table", "img_path": "AqcPvWwktK/tmp/1aad470526c8cd70960a05a5f54775fdeaf8b795ca910d442a1d866a671018ef.jpg", "table_caption": ["Table 1: Summary of the dataset statistics "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "3.4 Model Training Summary ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We describe the full training process of $\\mathrm{S^{2}M L^{2}}$ -BBAM. To avoid inaccurate pseudo-labels in the early training stage, following [16], we warm up the classifier $f_{\\mathbf{W}}(\\cdot)$ with the BAM loss of Eq.6 over labeled samples $\\mathcal{D}_{l}$ by $T_{0}$ epochs. Given the initialized $f_{\\mathbf{W}}(\\cdot)$ , we continue to train it with the BBAM loss of Eq.8 over labeled samples $\\mathcal{D}_{l}$ and unlabeled samples $\\mathcal{D}_{u}$ by $T_{t}$ epochs. At each epoch, we update pseudo labels $\\{\\mathbf{y}_{i}^{u}\\}_{i=1}^{i=N_{u}}$ by using Eq.5, label prototypes $\\{{\\bf c}_{k}\\}_{k=1}^{k=K}$ , $\\{(\\mu_{k}^{(p)},(\\sigma_{k}^{2})^{(p)})\\}_{k=1}^{k=K}$ and $\\{(\\mu_{k}^{(n)},(\\sigma_{k}^{2})^{(n)})\\}_{k=1}^{k=K}$ by using Eqs.9 and 10, and perform the negative sampling by using Eq.11. For clarity, the full training process is outlined in Appendix B. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. We employ 5 widely used MLL datasets, including image datasets Pascal VOC-2012 (VOC) [27], MS-COCO2014 (COCO) [28] and Animals with Attributes2 (AWA) [29], text datasets Ohsumed [30] and AAPD [31]. For clarity, the detailed characteristics of these datasets are displayed in Table 1. Following [16], we transform these datasets into SSL versions. For each dataset, we randomly select $\\pi$ training samples as labeled ones, and the remaining as unlabeled ones. We set $\\pi\\;\\in\\;\\{5\\%,10\\%,15\\%,\\bar{2}0\\%\\}$ , to explore the performance of our method under different data proportions. The image size is resized to 224 for all datasets. ", "page_idx": 6}, {"type": "text", "text": "Baselines. We employ 5 baseline methods for comparisons, including SoftMatch [32], FlatMatch [33], MIME [34], DRML [15], and CAP [16]. DRML and CAP are SSMLL methods; SoftMatch and FlatMatch are SSL methods; MIME is a single-positive multi-label learning (SPMLL) method. For SSL and SPMLL methods, we follow CAP to apply them to SSMLL tasks. ", "page_idx": 6}, {"type": "text", "text": "Evaluation metrics. We employ 5 evaluation metrics, including Micro-F1, Macro-F1, mean average precision (mAP), Hamming Loss and One Loss [1], and compute them with the Scikit-Learn tool.2 ", "page_idx": 6}, {"type": "text", "text": "Implementation details. We use the pre-trained ResNet-50 [35] as the backbone for image datasets and BERT-base-uncased model [36] for text datasets. We set the decay of EMA as 0.9997. The batch size is 32 for VOC, 128 for AWA and 64 for COCO, Ohsumed and AAPD. The warm-up epoch $T_{0}$ is 12. The $s$ and $m$ are 20 and 0.4 in VOC, 20 and 0.3 in COCO, 10 and 0.2 in AWA, Ohsumed and AAPD. The parameters for negative sampling $\\eta$ are set to 5. ", "page_idx": 6}, {"type": "text", "text": "4.2 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The experimental results are presented in Table 2 and Table 3. Overall, our method achieves good performance on all metrics. Our model ranks $\\mathbf{\\nabla}_{l s t}$ on average on five datasets and has a significant advantage over baselines. The detailed analyses are presented as follows. ", "page_idx": 6}, {"type": "text", "text": "Comparing with SSMLL methods: We can observe that $\\mathrm{S^{2}M L^{2}}$ -BBAM has advantages over recent SSMLL methods. Especially in the Micro-F1 and Macro-F1, our method has significant improvement. On both VOC and COCO, our F1 and mAP values increase by an average of 0.1 and 0.01. Furthermore, on Ohsumed and AAPD, we surprised to discover from the results that our method also has good results. In all data proportions, the average improvement on the mAP is 0.11, 0.14 on Macro-F1 and 0.19 on Micro-F1. This result is foreseeable because our method balanced angle variance using ", "page_idx": 6}, {"type": "table", "img_path": "AqcPvWwktK/tmp/db24ea4db99bc9d0b23716a9f66a52a4d4a540866b94606375e5245f928f86c7.jpg", "table_caption": ["Table 2: Experimental results on images datasets. The best results are highlighted in boldface. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "AqcPvWwktK/tmp/6bc015bee124ea5ebef16208d526c5901212848501ff8280dc07cbe7c5501f43.jpg", "table_caption": ["Table 3: Experimental results on text datasets. The best results are highlighted in boldface. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Gaussian transformation, making the prediction of pseudo labels more accurate. Since Ohsumed and AAPD are text datasets, this result also demonstrates the good universality of our method. ", "page_idx": 7}, {"type": "text", "text": "Comparing with SSL methods: Our $\\mathrm{S^{2}M L^{2}}$ -BBAM improves in both F1 and mAP metrics. For example, at $\\pi=5\\%$ , the mAP of $\\mathrm{S^{2}M L^{2}}$ -BBAM is $0.07\u20130.16$ higher than SoftMatch and 0.05-0.14 higher than FlatMatch across all datasets. We believe that this is because both methods are applied to multi classification tasks. So during the training process, it is more inclined to make single label classification decisions. Therefore, it doesn\u2019t perform as well as the SSMLL method. It can be inferred that it is important to set a dedicated method for SSMLL tasks. ", "page_idx": 7}, {"type": "text", "text": "Comparing with SPMLL methods: We observe that the performance of $\\mathrm{S^{2}M L^{2}}$ -BBAM is better than MIME in all aspects. When $\\pi=5\\%$ , the average improvement on the mAP is 0.11, 0.42 on Macro-F1 and 0.41 on Micro-F1. We believe that this is because SPMLL is primarily designed to address the issue of incomplete labels. However, there is a large amount of unlabeled data in the setting of SSMLL tasks. This leads to the MIME method being unable to obtain single positive observation labels for these data, resulting in a significant loss of information. Therefore, the performance of MIME has declined. ", "page_idx": 7}, {"type": "image", "img_path": "AqcPvWwktK/tmp/3b5595e46e6cddffdca20b2e4231b12eaf46b10c66d1f50893442e7d8010335a.jpg", "img_caption": ["Figure 2: Comparison of VDFD on VOC2012. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "AqcPvWwktK/tmp/09b3c29f7b2dd33f91a6e212f5b646e568e3523e6aa5f67ffe22bab4fd6e9706.jpg", "table_caption": ["Table 4: Results of the ablative study on VOC2012 and COCO. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To evaluate the effectiveness of the proposed BBAM loss, we perform several ablative studies by replacing it with the BAM loss (denoted by \u201cw/o BBAM\u201d) on VOC2012 and COCO. The results of the classification performance and VDFD are present in Table 4 and Fig.2, respectively. It clearly demonstrates that the proposed BBAM loss can significantly improve the classification performance and reduce variance differences between feature distributions. These results are expected because the BBAM loss can balance the variance bias between positive and negative samples from the perspective of the feature angle distribution for each label, leading to a fairer MLL classifier. Besides, we can observe that the VDFD of our $\\mathrm{S^{2}M L^{2}}$ -BBAM is much lower than those SSMLL baselines during the training procedure, further proving the effectiveness of the BBAM loss in balancing the variance bias. ", "page_idx": 8}, {"type": "text", "text": "4.4 Parameter Evaluation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct experiments on our method under different parameter settings. The experimental results are shown in Fig.3. We fix the $m$ value to 0.4 and set the $s$ values to $\\{1,10,20,30,40,50\\}$ respectively. When $s$ is set between 1 and 10, the performance increases with the $s$ . And when $s$ is set between 10 and 50, there is no significant change in the performance. One possible reason for this situation is that when the is small, the convergence speed of the model is too slow. So by the end of training, the model is not yet at its optimal state. We also explore the best accuracy by setting different cosine margins. We fix the value of $s$ to 20 and set the values of $m$ to $\\{0.0,0.2,0.4,0.6,0.8,1.0\\}$ respectively. We find that the performance is at its optimum at $m=0.4$ ", "page_idx": 8}, {"type": "image", "img_path": "AqcPvWwktK/tmp/3f702ede2a4a65f76c0ec5cae9239a8d86440ecf670b17fae88b2f00be62c91c.jpg", "img_caption": ["Figure 3: The sensitivity analysis of the rescaled norm and magnitude $\\{s,m\\}$ of cosine margin on VOC2012 with $\\pi=5\\%$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Related Works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "SSMLL methods. Recently, SSMLL has received a lot of attention. In the latest methods, most focus on how to use the connections between labels to guild the training on unlabeled data. For instance, SMILE [37] calculates the relationships between labels by constructing adjacency graphs, while COIN [14] applies the well-known co-training method and learns under inductive settings. DRML [15] constructs the relationship network between labels by designing two classifiers and adopting domain adaptation strategies. At the same time, how to effectively align pseudo labels with real labels is also an important issue. CAP [16] developed a class-distribution-aware thresholding strategy to control the assignment of positive and negative pseudo-labels. However, the current SSMLL methods have not paid attention to the variance bias problem, which affects the performance of the methods. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "MLL methods. MLL has multiple research directions. Some methods focus on the model structure. For instance, [38] proposed a graph convolutional networks model to improve the performance of multi-label image recognition. [4] proposes a unified framework that combines CNNs and RNNs. Some others focus on exploiting label correlations to improve performance. LSF-CI[39] calculates instance correlation in the feature space and label correlation in the label space through a probabilistic neighborhood graph model and cosine similarity. Due to the complete label information of the training samples, the MLL method can theoretically achieve Bayesian optimal classifier boundaries. However, in semi supervised learning, incorrect pseudo labels may provide incorrect guidance for classification boundaries. ", "page_idx": 9}, {"type": "text", "text": "SSL methods. Pseudo Label [40] is one of the earliest semi-supervised learning methods for neural networks. It generates pseudo labels for unlabeled data and continuously improves the accuracy of pseudo labels as the model is optimized. As data augmentation technology has advanced, an increasing number of SSL methods are incorporating this technology [41, 42, 43, 44, 45, 46]. Further research has been conducted on the threshold issue of pseudo labels in [47, 48, 49]. By developing dynamic threshold strategies, they have been able to obtain more accurate pseudo labels, effectively enhancing the performance of the SSL methods. In order to utilize pseudo labels with low confidence but correct classification, [32] proposes an effective method that fits the confidence distribution of truncated Gaussian functions. Moreover, [33] discovered that the generalization ability of SSL models is impacted by disconnection between labeled data and unlabeled data, and proposed the FlatMatch method to address this issue. However, it\u2019s important to note that these SSL methods are designed to handle multi-class single-label tasks [50, 51] and cannot be directly applied to multi-label learning scenarios. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we proposed a novel SSMLL method, namely $\\mathrm{S^{2}M L^{2}}$ -BBAM. Our $\\mathrm{S^{2}M L^{2}}$ -BBAM balances the variance bias between positive and negative samples from the perspective of the feature angle distribution for each label. To achieve this, we design a novel balanced binary angular margin loss by extending the traditional binary angular margin loss with feature angle distribution transformations under the Gaussian assumption, where the distributions are iteratively updated during classifier training. We also suggest an efficient prototype-based negative sampling method to maintain high-quality negative samples for each label. Empirical results demonstrate that our $\\mathrm{{S^{2}M L^{2}}}$ -BBAM outperforms current SSMLL baseline methods. ", "page_idx": 9}, {"type": "text", "text": "Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "From the empirical results, we found that $\\mathrm{S^{2}M L^{2}}$ -BBAM suffers from slightly lower mAP scores on the benchmarks VOC and COOC when increasing the proportion of labeled training samples. This may restrict the range of applications and scenarios in which $\\mathrm{S^{2}M L^{2}}$ -BBAM can be effectively used. And we will further exploit it in our future works. ", "page_idx": 9}, {"type": "text", "text": "Broader Impacts ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The paper focuses solely on the technical aspects of SSMLL algorithms. Therefore, this work can benefti a wide range of machine learning researchers. Also, we do not expect our efforts to have any negative consequences. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to acknowledge support for this project from the National Science and Technology Major Project of China (No.2021ZD0112500), the National Natural Science Foundation of China (No.62276113), and China Postdoctoral Science Foundation (No.2022M721321). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Zhang, M., Z. Zhou. A review on multi-label learning algorithms. IEEE TKDE, 26(8):1819\u2013 1837, 2014.   \n[2] Fujino, A., H. Isozaki, J. Suzuki. Multi-label text categorization with model combination based on f1-score maximization. In IJCNLP, pages 823\u2013828. 2008.   \n[3] Wu, H., S. Qin, R. Nie, et al. Effective collaborative representation learning for multilabel text categorization. IEEE TNNLS, 33(10):5200\u20135214, 2021.   \n[4] Wang, J., Y. Yang, J. Mao, et al. Cnn-rnn: A unified framework for multi-label image classification. In CVPR, pages 2285\u20132294. 2016.   \n[5] Lanchantin, J., T. Wang, V. Ordonez, et al. General multi-label image classification with transformers. In CVPR, pages 16478\u201316488. 2021.   \n[6] Zhao, F., Y. Huang, L. Wang, et al. Deep semantic ranking based hashing for multi-label image retrieval. In CVPR, pages 1556\u20131564. 2015.   \n[7] Lai, H., P. Yan, X. Shu, et al. Instance-aware hashing for multi-label image retrieval. IEEE TIP, 25(6):2469\u20132479, 2016.   \n[8] Zhang, D., S. Zhao, Z. Duan, et al. A multi-label classification method using a hierarchical and transparent representation for paper-reviewer recommendation. ACM TOIS, 38(1):1\u201320, 2020.   \n[9] Izadi, M., A. Heydarnoori, G. Gousios. Topic recommendation for software repositories using multi-label classification algorithms. Empirical Software Engineering, 26(5):93, 2021.   \n[10] Zhu, F., H. Li, W. Ouyang, et al. Learning spatial regularization with image-level supervisions for multi-label image classification. In CVPR, pages 5513\u20135522. 2017.   \n[11] Guo, H., K. Zheng, X. Fan, et al. Visual attention consistency under image transforms for multi-label image classification. In CVPR, pages 729\u2013739. 2019.   \n[12] Wang, B., Z. Tu, J. K. Tsotsos. Dynamic label propagation for semi-supervised multi-class multi-label classification. In ICCV, pages 425\u2013432. 2013.   \n[13] Zhao, F., Y. Guo. Semi-supervised multi-label learning with incomplete labels. In IJCAI, pages 4062\u20134068. 2015.   \n[14] Zhan, W., M. Zhang. Inductive semi-supervised multi-label learning with co-training. In SIGKDD, pages 1305\u20131314. 2017.   \n[15] Wang, L., Y. Liu, C. Qin, et al. Dual relation semi-supervised multi-label learning. In AAAI, pages 6227\u20136234. 2020.   \n[16] Xie, M.-K., J.-H. Xiao, H.-Z. Liu, et al. Class-distribution-aware pseudo-labeling for semisupervised multi-label learning. In NeurIPS. 2023.   \n[17] Zaitian, W., W. Pengfei, L. Kunpeng, et al. A comprehensive survey on data augmentation. arXiv preprint arXiv:2405.09591, 2024.   \n[18] Cole, E., O. M. Aodha, T. Lorieul, et al. Multi-label learning from single positive labels. In CVPR, pages 933\u2013942. 2021.   \n[19] Baruch, E. B., T. Ridnik, I. Friedman, et al. Multi-label classification with partial annotations using class-aware selective loss. In CVPR, pages 4764\u20134772. 2022.   \n[20] Ridnik, T., E. B. Baruch, N. Zamir, et al. Asymmetric loss for multi-label classification. In ICCV, pages 82\u201391. 2021.   \n[21] Jiang, T., D. Wang, L. Sun, et al. Lightxml: Transformer with dynamic negative sampling for high-performance extreme multi-label text classification. In AAAI, pages 7987\u20137994. 2021.   \n[22] Dahiya, K., D. Saini, A. Mittal, et al. Deepxml: A deep extreme multi-label learning framework applied to short text documents. In WSDM, pages 31\u201339. 2021.   \n[23] Qaraei, M., R. Babbar. Meta-classifier free negative sampling for extreme multilabel classification. Machine Learning, pages 1\u201323, 2023.   \n[24] Ma, X., Z. Wang, W. Liu. On the tradeoff between robustness and fairness. In NeurIPS. 2022.   \n[25] Caton, S., C. Haas. Fairness in machine learning: A survey. ACM Computing Surveys, 56(7):166:1\u2013166:38, 2024.   \n[26] Mehrabi, N., F. Morstatter, N. Saxena, et al. A survey on bias and fairness in machine learning. ACM Computing Surveys, 54(6):115:1\u2013115:35, 2022.   \n[27] Everingham, M., S. M. A. Eslami, L. V. Gool, et al. The pascal visual object classes challenge: A retrospective. IJCV, 111(1):98\u2013136, 2015.   \n[28] Lin, T., M. Maire, S. J. Belongie, et al. Microsoft coco: Common objects in context. In ECCV, pages 740\u2013755. 2014.   \n[29] Lampert, C. H., H. Nickisch, S. Harmeling. Attribute-based classification for zero-shot visual object categorization. IEEE TPAMI, 36(3):453\u2013465, 2013.   \n[30] Moschitti, A., R. Basili. Complex linguistic features for text classification: A comprehensive study. In ECIR, pages 181\u2013196. 2004.   \n[31] Yang, P., X. Sun, W. Li, et al. Sgm: Sequence generation model for multi-label classification. In COLING, pages 3915\u20133926. 2018.   \n[32] Chen, H., R. Tao, Y. Fan, et al. Softmatch: Addressing the quantity-quality trade-off in semi-supervised learning. ICLR, 2023.   \n[33] Huang, Z., L. Shen, J. Yu, et al. Flatmatch: Bridging labeled data and unlabeled data with cross-sharpness for semi-supervised learning. In NeurIPS. 2023.   \n[34] Liu, B., N. Xu, J. Lv, et al. Revisiting pseudo-label for single-positive multi-label learning. In ICML, pages 22249\u201322265. 2023.   \n[35] He, K., X. Zhang, S. Ren, et al. Deep residual learning for image recognition. In CVPR, pages 770\u2013778. 2016.   \n[36] Devlin, J., M. Chang, K. Lee, et al. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, pages 4171\u20134186. 2019.   \n[37] Tan, Q., Y. Yu, G. Yu, et al. Semi-supervised multi-label classification using incomplete label information. Neurocomputing, 260:192\u2013202, 2017.   \n[38] Chen, Z., X. Wei, P. Wang, et al. Multi-label image recognition with graph convolutional networks. In CVPR, pages 5177\u20135186. 2019.   \n[39] Zhang, J., C. Li, D. Cao, et al. Multi-label learning with label-specific features by resolving label correlations. KBS, 159:148\u2013157, 2018.   \n[40] Dong-Hyun, L., et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In ICML Workshop, page 896. 2013.   \n[41] Berthelot, D., N. Carlini, I. J. Goodfellow, et al. Mixmatch: A holistic approach to semisupervised learning. In NeurIPS, pages 5050\u20135060. 2019.   \n[42] Zhang, H., M. Ciss\u00e9, Y. N. Dauphin, et al. mixup: Beyond empirical risk minimization. In ICLR. 2018.   \n[43] Berthelot, D., N. Carlini, E. D. Cubuk, et al. Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring. ICLR, 2020.   \n[44] Sohn, K., D. Berthelot, N. Carlini, et al. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. In NeurIPS, pages 596\u2013608. 2020.   \n[45] Li, C., X. Li, L. Feng, et al. Who is your right mixup partner in positive and unlabeled learning. In ICLR. 2022.   \n[46] Li, C., Y. Dai, L. Feng, et al. Positive and unlabeled learning with controlled probability boundary fence. In ICML. 2024.   \n[47] Xu, Y., L. Shang, J. Ye, et al. Dash: Semi-supervised learning with dynamic thresholding. In ICML, pages 11525\u201311536. 2021.   \n[48] Guo, L., Y. Li. Class-imbalanced semi-supervised learning with adaptive thresholding. In ICML, pages 8082\u20138094. 2022.   \n[49] Wang, Y., H. Chen, Q. Heng, et al. Freematch: Self-adaptive thresholding for semi-supervised learning. ICLR, 2023.   \n[50] Li, C., X. Li, J. Ouyang. Semi-supervised text classification with balanced deep representation distributions. In ACL-IJCNLP, pages 5044\u20135053. 2021.   \n[51] Li, X., Y. Jiang, C. Li, et al. Learning with partial labels from semi-supervised perspective. In AAAI, pages 8666\u20138674. 2023.   \n[52] Xu, H., X. Liu, Y. Li, et al. To be robust or to be fair: Towards fairness in adversarial training. In ICML, pages 11492\u201311501. 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Proof of Theoretical Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. Proof of Theorem 2.1. For any linear classifier $f(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w},\\mathbf{x}\\rangle+b)$ , we first calculate its risk: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{s s l}(f)}\\\\ &{=\\mathbb{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}[\\mathbb{I}(f(\\mathbf{x})\\neq y)]}\\\\ &{\\propto\\mathbb{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}^{*}}[\\mathbb{I}(f(\\mathbf{x})\\neq y)]+(1-\\epsilon_{-}-\\epsilon_{+})\\mathbb{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}^{*}}[\\mathbb{I}(f(\\mathbf{x})\\neq y)]+}\\\\ &{\\quad\\epsilon_{-}\\mathbb{E}_{(\\mathbf{x},-1)\\sim\\mathcal{D}^{*}}[\\mathbb{I}(f(\\mathbf{x})\\neq+1)]+\\epsilon_{+}\\mathbb{E}_{(\\mathbf{x},+1)\\sim\\mathcal{D}^{*}}[\\mathbb{I}(f(\\mathbf{x})\\neq-1)]}\\\\ &{=(2-\\epsilon_{-}-\\epsilon_{+})\\mathbb{P}_{(\\mathbf{x},y)\\sim\\mathcal{D}^{*}}[f(\\mathbf{x})\\neq\\epsilon_{-}\\mathbb{P}(\\mathbf{x},-1)\\sim\\mathcal{P}^{*}[f(\\mathbf{x})\\neq+1]+\\epsilon_{+}\\mathbb{P}_{(\\mathbf{x},+1)\\sim\\mathcal{D}^{*}}[f(\\mathbf{x})\\neq-1]}\\\\ &{=(2-\\epsilon_{-}-\\epsilon_{+})\\cdot(\\mathbb{P}[y=+1]\\cdot\\mathbb{P}[f(\\mathbf{x})=-1]y=+1]+\\mathbb{P}[y=-1]\\cdot\\mathbb{P}[f(\\mathbf{x})=+1|y=-1])+}\\\\ &{\\quad\\epsilon_{-}\\mathbb{P}[y=-1]\\cdot\\mathbb{P}[f(\\mathbf{x})=-1|y=-1]+\\epsilon_{+}\\mathbb{P}[y=+1]\\cdot\\mathbb{P}[f(\\mathbf{x})=+1|y=+1]}\\\\ &{=(2-\\epsilon_{-}-\\epsilon_{+})\\cdot\\alpha\\cdot\\mathcal{R}(f,+1)+(2-\\epsilon_{-}-\\epsilon_{+})\\cdot(1-\\alpha)\\cdot\\mathcal{R}(f,-1)+}\\\\ &{\\quad\\epsilon_{-}\\cdot(1-\\alpha)\\cdot\\mathbb{P}[f(\\mathbf{x})=-1]+\\epsilon_{+}\\cdot\\alpha\\cdot\\mathbb{P}[f(\\mathbf{x})=+1|y=+1]}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Denote $\\mathbf{x}\\,=\\,[x_{1},\\cdots\\,,x_{d}]^{\\top}$ and $\\mathbf{w}\\,=\\,[w_{1},\\cdots\\,,w_{d}]^{\\top}$ , we can explicitly calculate $\\mathcal{R}(f,+1)$ and $\\mathbb{P}[f(\\mathbf{x})=+1|y=+1]$ as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{R}(f,+1)=\\mathbb{P}[f(\\mathbf{x})=-1|y=+1]=\\mathbb{P}[\\langle\\mathbf{w},\\mathbf{x}\\rangle+b<0|y=+1]=\\mathbb{P}[\\sum_{i=1}^{d}w_{i}x_{i}+b<0]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}[f(\\mathbf{x})=+1|y=+1]=\\mathbb{P}[\\langle\\mathbf{w},\\mathbf{x}\\rangle+b>0|y=+1]=\\mathbb{P}[\\sum_{i=1}^{d}w_{i}x_{i}+b>0]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $x_{1},\\cdot\\cdot\\cdot\\,,x_{d}$ are i.i.d. drawn from Gaussian distributions $\\{\\mathcal{N}(\\mu_{i},(\\sigma_{+}^{i})^{2})\\}_{i=1}^{d}$ according to the definition of $\\mathcal{P}^{*}$ in Eq.(2). ", "page_idx": 13}, {"type": "text", "text": "Similar to $\\mathcal{R}(f,+1)$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{R}(f,-1)=\\mathbb{P}[\\sum_{i=1}^{d}w_{i}x_{i}+b>0],\\;\\mathbb{P}[f(\\mathbf{x})=-1|y=-1]=\\mathbb{P}[\\sum_{i=1}^{d}w_{i}x_{i}+b<0],\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $x_{1},\\cdot\\cdot\\cdot\\,,x_{d}$ are i.i.d. drawn from Gaussian distributions $\\{\\mathcal{N}(-\\mu_{i},(\\sigma_{-}^{i})^{2})\\}_{i=1}^{d}$ . Denote $f_{s s l}(\\mathbf{x})=\\langle\\mathbf{w}^{*},\\mathbf{x}\\rangle+b^{*}$ . According to the method of [52], we can prove $w_{1}^{*}=\\cdots=w_{d}^{*}=1$ by contradiction. Based on the properties of Gaussian distribution, $\\mathcal{R}(f_{s s l},+1),\\mathbb{P}[f_{s s l}(\\mathbf{x})=+1|y=+1]$ , $\\mathcal{R}(f_{s s l},-1)$ and $\\mathbb{P}[f_{s s l}(\\bar{\\mathbf{x}})\\stackrel{-}{=}-1|y=-1]$ can be expressed as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}(f_{s s l},+1)=\\mathbb{P}[\\underset{i=1}{\\overset{d}{\\sum}}x_{i}+b^{*}<0]=\\mathbb{P}\\Bigg[\\frac{\\sum_{i=1}^{d}(x_{i}-\\mu_{i})}{\\sqrt{\\sum_{i=1}^{i=d}(\\sigma_{+}^{(i)})^{2}}}<\\frac{-b^{*}-\\sum_{i=1}^{d}\\mu_{i}}{\\sqrt{\\sum_{i=1}^{i=d}(\\sigma_{+}^{(i)})^{2}}}\\Bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\Phi\\bigg({-\\frac{b^{*}+\\mu}{\\Sigma}}\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}[f_{s s l}(\\mathbf{x})=+1|y=+1]=\\mathbb{P}[\\displaystyle\\sum_{i=1}^{d}x_{i}+b^{*}>0]=\\mathbb{P}\\Bigg[\\frac{\\sum_{i=1}^{d}(x_{i}-\\mu_{i})}{\\sqrt{\\sum_{i=1}^{i=d}(\\sigma_{+}^{(i)})^{2}}}>\\frac{-b^{*}-\\sum_{i=1}^{d}\\mu_{i}}{\\sqrt{\\sum_{i=1}^{i=d}(\\sigma_{+}^{(i)})^{2}}}\\Bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=1-\\Phi\\Bigg(\\!-\\!\\frac{b^{*}+\\mu}{\\Sigma}\\Bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}(f_{s s l},-1)=\\mathbb{P}[\\underset{i=1}{\\overset{d}{\\sum}}x_{i}+b^{*}>0]=\\mathbb{P}\\bigg[\\frac{\\sum_{i=1}^{d}\\left(x_{i}-\\left(-\\mu_{i}\\right)\\right)}{\\sqrt{\\sum_{i=1}^{i=d}\\!\\left(\\sigma_{-}^{(i)}\\right)^{2}}}>\\frac{-b^{*}+\\sum_{i=1}^{d}\\mu_{i}}{\\sqrt{\\sum_{i=1}^{i=d}\\!\\left(\\sigma_{-}^{(i)}\\right)^{2}}}\\bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=1-\\Phi\\bigg(\\frac{-b^{*}+\\mu}{M\\Sigma}\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}[f_{s s l}(\\mathbf{x})=-1|y=-1]=\\mathbb{P}[\\underset{i=1}{\\overset{d}{\\sum}}x_{i}+b^{*}<0]=\\mathbb{P}\\Bigg[\\frac{\\sum_{i=1}^{d}(x_{i}-(-\\mu_{i}))}{\\sqrt{\\underset{i=1}{\\overset{i=d}{\\sum}}(\\sigma_{-}^{(i)})^{2}}}<\\frac{-b^{*}+\\sum_{i=1}^{d}\\mu_{i}}{\\sqrt{\\sum_{i=1}^{i=d}(\\sigma_{-}^{(i)})^{2}}}\\Bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\Phi\\Bigg(\\frac{-b^{*}+\\mu}{M\\Sigma}\\Bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\Phi$ is c.d.f. of normal Gaussian distribution $\\mathcal{N}(0,1)$ . Then, we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\mathcal{R}_{s s l}(f_{s s l})=\\displaystyle\\alpha(2-\\epsilon_{-}-\\epsilon_{+})\\Phi\\biggl(-\\frac{b^{*}+\\mu}{\\Sigma}\\biggr)+(1-\\alpha)(2-\\epsilon_{-}-\\epsilon_{+})\\Phi\\biggl(\\frac{b^{*}-\\mu}{M\\Sigma}\\biggr)+}}\\\\ {{(1-\\alpha)\\epsilon_{-}\\Phi\\biggl(\\frac{-b^{*}+\\mu}{M\\Sigma}\\biggr)+\\alpha\\epsilon_{+}\\Phi\\biggl(\\frac{b^{*}+\\mu}{\\Sigma}\\biggr)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We will find the optimal $b^{*}$ which minimizes the overall standard classification error $\\mathcal{R}_{s s l}(f_{s s l})$ by taking $\\begin{array}{r}{\\frac{d\\mathcal{R}_{s s l}\\left(f_{s s l}\\right)}{d b^{*}}=0}\\end{array}$ . In detail, it is: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\frac{d\\mathcal{R}_{s s l}(f_{s s l})}{d b^{*}}=\\alpha(2-\\epsilon_{-}-\\epsilon_{+})\\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{1}{2}(\\frac{b^{*}+\\mu}{\\Sigma})^{2})\\frac{-1}{\\Sigma}+}}\\\\ {{\\displaystyle(1-\\alpha)(2-\\epsilon_{-}-\\epsilon_{+})\\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{1}{2}(\\frac{b^{*}-\\mu}{M\\Sigma})^{2})\\frac{1}{M\\Sigma}+}}\\\\ {{\\displaystyle(1-\\alpha)\\epsilon_{-}\\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{1}{2}(\\frac{b^{*}-\\mu}{M\\Sigma})^{2})\\frac{-1}{M\\Sigma}+\\alpha\\epsilon_{+}\\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{1}{2}(\\frac{b^{*}+\\mu}{\\Sigma})^{2})\\frac{1}{\\Sigma}=0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which can be reformulated as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n(\\frac{b^{*}+\\mu}{\\Sigma})^{2}-(\\frac{b^{*}-\\mu}{M\\Sigma})^{2}=2\\log\\left(\\frac{M\\alpha(2-\\epsilon_{-}-2\\epsilon_{+})}{(1-\\alpha)(2-2\\epsilon_{-}-\\epsilon_{+})}\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Denote $\\begin{array}{r}{B=\\log\\biggl(\\frac{M\\alpha(2-\\epsilon_{-}-2\\epsilon_{+})}{(1-\\alpha)(2-2\\epsilon_{-}-\\epsilon_{+})}\\biggr).}\\end{array}$ Without loss of generality, we assume $B>0$ and obtain: ", "page_idx": 14}, {"type": "equation", "text": "$$\n(M^{2}-1)\\Sigma^{2}(b^{*})^{2}+2(M^{2}+1)\\mu\\Sigma^{2}b^{*}+(M^{2}-1)\\Sigma^{2}\\mu^{2}=2B M^{2}\\Sigma^{4}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Consequently, $b^{*}$ can be given by selecting the smaller absolute value: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{b^{\\ast}=\\left\\{\\begin{array}{l l}{\\frac{-(M^{2}+1)\\mu+2M\\mu\\sqrt{1+B\\frac{(M^{2}-1)\\Sigma^{2}}{2\\mu^{2}}}}{M^{2}-1}}&{\\mathrm{if~}M>1,}\\\\ {-(M^{2}+1)\\mu-2M\\mu\\sqrt{1+B\\frac{(M^{2}-1)\\Sigma^{2}}{2\\mu^{2}}}}&{\\mathrm{if~}M<1,}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then when $M>1$ , the class-wise standard classification errors are: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}(f_{s s l},+1)=\\Phi\\big(A-M\\sqrt{A^{2}+q(M,\\alpha,\\epsilon_{-},\\epsilon_{+})}\\big),}\\\\ &{\\mathcal{R}(f_{s s l},-1)=\\Phi\\big({-M\\cdot A+\\sqrt{A^{2}+q(M,\\alpha,\\epsilon_{-},\\epsilon_{+})}}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "when $M<1$ , they are given by: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}(f_{s s l},+1)=\\Phi\\big(A+M\\sqrt{A^{2}+q(M,\\alpha,\\epsilon_{-},\\epsilon_{+})}\\big),}\\\\ &{\\mathcal{R}(f_{s s l},-1)=\\Phi\\big({-M\\cdot A}-\\sqrt{A^{2}+q(M,\\alpha,\\epsilon_{-},\\epsilon_{+})}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where ", "page_idx": 14}, {"type": "equation", "text": "$$\nA=\\frac{2\\mu}{(M^{2}-1)\\Sigma},\\quad q(M,\\alpha,\\epsilon_{-},\\epsilon_{+})=\\frac{2\\log\\frac{M\\alpha(2-\\epsilon_{-}-2\\epsilon_{+})}{(1-\\alpha)(2-2\\epsilon_{-}-\\epsilon_{+})}}{M^{2}-1}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "When $\\begin{array}{r}{\\sum_{i=1}^{d}(\\sigma_{+}^{(i)})^{2}=\\sum_{i=1}^{d}(\\sigma_{-}^{(i)})^{2}=\\Sigma^{2}}\\end{array}$ , i.e. $M=1$ , Eq.(12) can be rewritten as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n4\\mu b^{*}=2\\log\\Biggl(\\frac{\\alpha(2-\\epsilon_{-}-2\\epsilon_{+})}{(1-\\alpha)(2-2\\epsilon_{-}-\\epsilon_{+})}\\Biggr)\\Sigma^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In this case, $b^{*}$ can be expressed as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\nb^{*}=\\frac{\\log\\bigl(\\frac{\\alpha(2-\\epsilon_{-}-2\\epsilon_{+})}{(1-\\alpha)(2-2\\epsilon_{-}-\\epsilon_{+})}\\bigr)\\Sigma^{2}}{2\\mu},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and corresponding class-wise standard classification errors are given by: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{R}(f_{s s l},+1)=\\Phi\\bigg(\\frac{-2\\mu^{2}-\\log\\big(\\frac{\\alpha(2-\\epsilon_{-}-2\\epsilon_{+})}{(1-\\alpha)(2-2\\epsilon_{-}-\\epsilon_{+})}\\big)\\Sigma^{2}}{2\\mu\\Sigma}\\bigg),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Proof of Theorem 2.3. According to the results of Theorem 2.1, we can formulate the class-wise accuracy as: ", "page_idx": 15}, {"type": "equation", "text": "$$\np(+1)=1-\\mathcal{R}(f_{s s l},+1),\\quad p(-1)=1-\\mathcal{R}(f_{s s l},-1).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Accordingly, the variance of class-wise accuracy can be expressed as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V C A(f_{s s l})=\\operatorname{Var}(p(+1),p(-1))=\\operatorname{Var}(1-\\mathcal{R}(f_{s s l},+1),1-\\mathcal{R}(f_{s s l},-1))}\\\\ &{\\quad\\quad\\quad\\quad=\\operatorname{Var}(\\mathcal{R}(f_{s s l},+1),\\mathcal{R}(f_{s s l},-1))}\\\\ &{\\quad\\quad\\quad\\quad=\\frac{\\left(\\mathcal{R}(f_{s s l},+1)-\\mathcal{R}(f_{s s l},-1)\\right)^{2}}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For convenience, we assume $\\begin{array}{r}{\\log\\bigl(\\frac{\\alpha(2-\\epsilon_{-}-2\\epsilon_{+})}{(1-\\alpha)(2-2\\epsilon_{-}-\\epsilon_{+})}\\bigr)=0}\\end{array}$ , and the conclusion will also hold when $\\begin{array}{r}{M\\ >\\ \\operatorname*{max}(\\frac{\\alpha(2-\\epsilon_{-}-2\\epsilon_{+})}{(1-\\alpha)(2-2\\epsilon_{-}-\\epsilon_{+})},1)}\\end{array}$ and $\\begin{array}{r}{M\\ <\\ \\operatorname*{min}(\\frac{\\alpha(2-\\epsilon_{-}-2\\epsilon_{+})}{(1-\\alpha)(2-2\\epsilon_{-}-\\epsilon_{+})},1)}\\end{array}$ . When $M~>~1$ , it has $\\mathcal{R}(f_{s s l},-1)>\\mathcal{R}(f_{s s l},+1)$ because $q(M,\\alpha,\\epsilon_{-},\\epsilon_{+})>0$ and $A>0$ . Then according to Lagrange\u2019s Mean Value Theorem, there exists some $\\xi$ such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}(f_{s s l},-1)-\\mathcal{R}(f_{s s l},+1)}\\\\ &{\\;=\\Phi\\bigl(-M\\cdot A+\\sqrt{A^{2}+q(M,\\alpha,\\epsilon_{-},\\epsilon_{+})}\\bigr)-\\Phi\\bigl(A-M\\sqrt{A^{2}+q(M,\\alpha,\\epsilon_{-},\\epsilon_{+})}\\bigr)}\\\\ &{\\;=\\Phi^{\\prime}(\\xi)\\bigl(-M\\cdot A+\\sqrt{A^{2}+q(M,\\alpha,\\epsilon_{-},\\epsilon_{+})}-A+M\\sqrt{A^{2}+q(M,\\alpha,\\epsilon_{-},\\epsilon_{+})}\\bigr)}\\\\ &{\\;=\\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{\\xi^{2}}{2})(M+1)\\bigl(\\sqrt{A^{2}+q(M,\\alpha,\\epsilon_{-},\\epsilon_{+})}-A\\bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By analyzing the variation of $q(M,\\alpha,\\epsilon_{-},\\epsilon_{+})$ , we can easily verify that $\\mathcal{R}(f_{s s l},-1)-\\mathcal{R}(f_{s s l},+1)$ is increasing when $M\\to\\infty$ . Similarly, we can prove that $\\mathcal{R}(f_{s s l},+1)>\\mathcal{R}(f_{s s l},-1)$ when $M<1$ and $\\mathcal{R}(f_{s s l},+1)-\\mathcal{R}(f_{s s l},-1)$ is increasing when $M\\rightarrow0$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "B The training procedure of the model ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The Algorithm 1 provides a detailed description of the training process of the model. ", "page_idx": 16}, {"type": "table", "img_path": "AqcPvWwktK/tmp/b28ee9d7cbadd879d0be76be90e295fa1fe10f328f3626cee65c9167144c7256.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "C Time cost comparison ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To examine the efficiency of $\\mathrm{S^{2}M L^{2}}$ -BBAM, we perform efficiency comparisons over our $\\mathrm{S^{2}M L^{2}}$ - BBAM, SSL baselines (SoftMatch and FlatMatch) and SSMLL baselines (DRML and CAP) on VOC and COCO. Table 5 shows the running time averaged 100 epochs. From Table 5, it can be seen that our method is competitive with the current SSMLL methods in the time efficiency and costs less time than the SSL baselines in practice. ", "page_idx": 16}, {"type": "table", "img_path": "AqcPvWwktK/tmp/77dedf352af7c4cbe7c6e5cf8dc26990a73bee51a358ed73a442679155097bc1.jpg", "table_caption": ["Table 5: Time cost (second, s) of each training epoch on VOC and COCO. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: See 6. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Please refer to 3 ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The code will be submitted later. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper specify all the training and test details. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: Error bars were too small to have any visual impact. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We have used a single NVIDIA GeForce RTX 3090 GPU. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Please refer to 6. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: No such risks ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 21}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not release new assets ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]