[{"type": "text", "text": "Bridge the Points: Graph-based Few-shot Segment Anything Semantically ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anqi Zhang1, Guangyu Gao1,\u2217 Jianbo Jiao2, Chi Harold Liu1, and Yunchao Wei3 ", "page_idx": 0}, {"type": "text", "text": "1School of Computer Science, Beijing Institute of Technology   \n2The MIx group, School of Computer Science, University of Birmingham   \n3WEI Lab, Institute of Information Science, Beijing Jiaotong University andy_zaq@outlook.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The recent advancements in large-scale pre-training techniques have significantly enhanced the capabilities of vision foundation models, notably the Segment Anything Model (SAM), which can generate precise masks based on point and box prompts. Recent studies extend SAM to Few-shot Semantic Segmentation (FSS), focusing on prompt generation for SAM-based automatic semantic segmentation. However, these methods struggle with selecting suitable prompts, require specific hyperparameter settings for different scenarios, and experience prolonged one-shot inference time due to the overuse of SAM, resulting in low efficiency and limited automation ability. To address these issues, we propose a simple yet effective approach based on graph analysis. In particular, a Positive-Negative Alignment module dynamically selects the point prompts for generating masks, especially uncovering the potential of the background context as the negative reference. Another subsequent Point-Mask Clustering module aligns the granularity of masks and selected points as a directed graph, based on mask coverage over points. These points are then aggregated by decomposing the weakly connected components of the directed graph in an efficient manner, constructing distinct natural clusters. Finally, the positive and overshooting gating, benefiting from graph-based granularity alignment, aggregate high-confident masks and filter out the false-positive masks for final prediction, without relying on additional hyperparameters and redundant mask generation. Extensive experimental analysis across tasks including the standard FSS, One-shot Part Segmentation, and Cross Domain FSS validate the effectiveness and efficiency of the proposed approach, surpassing state-of-the-art generalist models with a mIoU of $58.7\\%$ on $\\mathrm{COCO}{-}20^{\\mathrm{i}}$ and $35.2\\%$ on LVIS-92i. The project page of this work is: https://andyzaq.github.io/GF-SAM/. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Previous semantic segmentation methods [1\u20138], which rely on the pixel-level classification, often struggle with generalization and overftiting due to limited labeled data. In addition, recent approaches, such as MaskFormer [9], have shifted the paradigm to mask-based classification, offering a more flexible approach to improving the segmentation performance by exploiting the consistency and completeness of generated class-agnostic masks. The Segment Anything Model (SAM) [10] further marks a significant advancement by utilizing extensive pre-training on huge-scale dataset SA-1B to achieve more robust, class-agnostic segmentation capabilities. SAM excels in producing precise masks across various domains using simple prompts such as points, boxes, and coarse masks. While the boundaries of these masks can closely align with object boundaries, the lack of semantic ", "page_idx": 0}, {"type": "image", "img_path": "jYypS5VIPj/tmp/b91c767b74189284a53e6323cd4de6ce42cc456e31b0536f2cbc4fe6ae42bbdd.jpg", "img_caption": ["(a) Performance-efficiency comparison of FSS models. The (b) Comparison with previous generalist and numbers inside the points represent the numbers of parameters. specialist models on various FSS datasets. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Performance comparisons of our approach against previous state-of-the-art methods regarding efficiency and generalized capabilities in Few-shot Semantic Segmentation. Figure 1(a) illustrates our approach\u2019s superior performance in efficiency and effectiveness across various model sizes. Figure 1(b) demonstrates the generalizability of our approach across different domains. ", "page_idx": 1}, {"type": "text", "text": "understanding and the requirement for manual prompts prevent SAM from being used in automatic semantic segmentation applications. ", "page_idx": 1}, {"type": "text", "text": "Recent studies have attempted to automate this process in the Few-shot Semantic Segmentation (FSS), using a few reference images and a fine-grained external backbone network (e.g., DINOv2 [11]) to guide SAM in segmenting target semantic objects. However, these methods face two main challenges: achieving suitable points for precise and full coverage of the target object, and handling the ambiguity of SAM-generated masks, from partial to complete coverage. Specifically, they either utilize the most similar candidate point prompts for iterative mask generation and refinement [12], or build a restrictively selected set of point prompts for heuristically weighted mask merging based on manually designed metrics [13], outperforming both previous specialist methods [14\u201319] and generalist methods without SAM [20, 21]. However, these methods overlooked the underlying relationships between points (derived from fine-grained features) and masks (generated by SAM in a coarse-grained manner). This oversight led to low efficiency (as indicated in Fig. 1(a)) and limited automation capabilities. Alignment between these two types of granularity could uncover the potential of simple decision-making on masks, which can eliminate redundant refinement and manual hyperparameter selection for complicated metrics. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we explicitly explore the relationship between point prompts and corresponding masks from SAM, and present a simple yet effective parameter-free framework with only one-time mask generation to segment anything semantically, in a graph-based few-shot manner. We first introduce a Positive-Negative Alignment (PNA) module to dynamically select point prompts using foreground and background references from reference images. Unlike existing methods, our approach combines different granularity by constructing a directed graph according to mask coverage over points. Then, we perform connectivity analysis on the constructed graph to obtain several weakly connected components as automatic clustering of point prompts, which bridges points and masks as well as fine-grained and coarse-grained features. To mitigate the inevitable introduction of false positives in the PNA module, we further leverage weakly connected component clusters and limited semantic information in selected points, to more accurately fliter and merge masks that mismatch in different granularities. In particular, our proposed method involves two post-gating based on weakly connected clusters: the positive gating retains masks capturing a greater proportion of potential target areas, while the overshooting gating screens out outlier points near object boundary, with coverage self-consistency consideration. ", "page_idx": 1}, {"type": "text", "text": "Extensive experimental analysis on Few-shot Semantic Segmentation demonstrates both the efficiency and effectiveness of our approach, as shown in Fig. 1(b). We first conduct the experiments on generalized FSS datasets, including Pascal- $.5^{\\mathrm{i}}$ [22], COCO- $20^{\\mathrm{i}}$ [23], FSS-1000 [24] and LVIS-92i [13]. Our approach surpasses existing state-of-the-art approaches on these datasets, with $5.8\\%$ and $2.2\\%$ of improvement respectively on more challenging COCO- $\\cdot20^{\\mathrm{i}}$ and $\\mathrm{LVIS-92^{i}}$ . As for the challenging One-shot Part Segmentation, our approach still exceeds previous methods with $1.6\\%$ of mIoU on both PACO-Part and PASCAL-Part. Furthermore, to demonstrate the ability of our approach across different domains, we perform an evaluation on several specific datasets, including Deepglobe [25], ISIC [26], and iSAID- ${\\bar{\\,}}{\\bar{\\,}}^{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!$ [27]. The proposed approach establishes new state-of-the-art performance on mIoU with $49.5\\%$ on Deepglobe, $4\\bar{8}.7\\%$ on ISIC, and $47.3\\%$ on iSAID- $5^{\\mathrm{i}}$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Overall, our contributions are summarized as follows: ", "page_idx": 2}, {"type": "text", "text": "\u2022 We present, to our knowledge, the first graph-based approach for SAM-based few-shot semantic segmentation, modeling the relationship of SAM-generated masks in an automatic clustering manner. \u2022 We propose a positive-negative alignment module and a post-gating strategy based on the weakly connected graph components, enabling a hyperparameter-free pipeline. \u2022 Extensive experimental comparisons and analysis across several datasets over various settings show the effectiveness and efficiency of the proposed method. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Few-shot semantic segmentation. Few-shot Semantic Segmentation (FSS) [22] aims to segment the target object using only a limited number of annotated reference samples for guidance. Previous FSS methods are mainly categorized into two types, namely the methods based on prototype matching [28\u2013 34] and methods based on pixel-wise matching [35\u201340]. The methods based on prototype matching, e.g. PFENet [31], BAM [41], SSP [42], use the Mask Average Pooling operation from SGOne [43] to generate a prototype as a global representation of the reference features, and compare the target features with the prototypes. The methods based on pixel-wise matching compute the correlation of all pixels between target and reference features. Then different methods address the correlations through distinct mechanisms, such as 4D Convolution (e.g., HSNet [14]) and Transformer (e.g., HDMNet [44], AMFormer [45]). Although these specialist models perform significantly on specific tasks, they are prone to overfitting the training samples and often struggle to adapt to domain shifts. ", "page_idx": 2}, {"type": "text", "text": "SAM-based semantic segmentation. Recently, Segment Anything Model (SAM) [10] has shown remarkable zero-shot class-agnostic segmentation capabilities using prompts like points, boxes, and coarse masks. However, the coarse-grained feature representation of SAM limits its effectiveness for fine-grained semantic segmentation tasks. Several approaches have been proposed to extend SAM for semantic segmentation. For example, Semantic-SAM [46] jointly train the model on SA-1B and other semantic aware segmentation datasets to enhance granularity. OV-SAM [47] combines SAM and CLIP [48] for open-vocabulary semantic segmentation. Moreover, some methods introduce SAM into FSS tasks. PerSAM and PerSAM-F [12] leverage SAM for personalized segmentation with one-shot guidance. Matcher [13] uses a SAM-based training-free structure, achieving impressive performance in both FSS and One-shot Part Segmentation. VRP-SAM [49] trains an external Visual Reference Prompt Encoder to automatically generate prompts from reference images using points, scribble, box, or masks. However, previous training-free methods struggled to balance performance and efficiency, often relying on excessive external manual hyperparameters. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Few-shot Semantic Segmentation (FSS) aims to segment target objects in an image with a few annotated reference images. Consider a scenario where each group of samples contains a target image xt and a reference image set R = {xrk, ykr}kK=1 with the size of $H\\times W$ , where $\\v x_{k}^{r}$ and $y_{k}^{r}$ mean the $k_{t h}$ reference image and its corresponding mask. Focusing on the 1-shot case, where $K=1$ , it begins with a feature extraction backbone network $f_{B}(\\cdot)$ , which encodes both $x^{t}$ and $x^{r}$ into semantic features $F^{t}$ and $F^{r}$ in $\\mathbb{R}^{h w\\times c}$ , where $h$ and $w$ denote the height and width of the feature maps, and $c$ is the feature dimension. Subsequent few-shot processes utilize these feature maps to generate a predicted segmentation $\\tilde{y}\\in\\mathbb{R}^{H\\times W^{\\prime}}$ for $x^{t}$ . This prediction is then compared to the Ground Truth (GT) $y^{t}$ for evaluation. ", "page_idx": 2}, {"type": "text", "text": "The Segment Anything Model (SAM) is a generalized foundation segmentation model adept at generating precise masks based on varied prompts of points, boxes, and coarse masks. Built around ", "page_idx": 2}, {"type": "image", "img_path": "jYypS5VIPj/tmp/57170bf15a358bea8fda41e40eeac5b1e69dcd812521ac11478ef956e3ee86b6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Overview of our approach, where the Positive-Negative Alignment module recognizes the correlation between target features and reference features for point selection, the Point-Mask Clustering module efficiently clusters the points based on the coverage of corresponding masks, and Post-Gating filters out the false-positive masks for generating final prediction. ", "page_idx": 3}, {"type": "text", "text": "a core architecture that includes an image encoder, a prompt encoder, and a mask decoder, SAM effectively processes input images $x^{t}$ and prompts $P$ to produce detailed segmentation masks $\\hat{y}$ . These masks accurately delineate specific objects or regions within the images, based on the guidance provided by the prompts. ", "page_idx": 3}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Diverging from traditional methods, we use a directed graph to exploit the natural relationships between points and their corresponding masks, representing fine-grained and coarse-grained features, respectively. As shown in Fig. 2, our approach mainly comprises the Positive-Negative Alignment (PNA) module, Point-Mask Clustering (PMC) module, and Post-Gating strategy. The PNA module leverages semantic features from the backbone network to sort pixel-wise correlations into similarity maps, enabling precise point selection. The PMC module then clusters masks based on these selected points, while Post-Gating strategy refines the selection, enhancing the accuracy and reliability of the final prediction. ", "page_idx": 3}, {"type": "text", "text": "4.1 Positive-Negative Alignment for Point Selection ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The PNA module efficiently selects point prompts to balance the number of points and coverage of target objects. Using the semantic features $F^{r}$ and $F^{t}$ from the reference and target images respectively (with e.g., DINOv2 [11]), we get the pixel-wise correlation matrix $C\\in\\mathbb{R}^{h\\bar{w}\\times h w}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nC(i,j)=R e L U\\left(\\frac{F^{t}(i)\\cdot F^{r}(j)}{\\|F^{t}(i)\\|\\cdot\\|F^{r}(j)\\|}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $C(i,j)$ represents the similarity between the $i$ -th pixel of target features $F^{t}(i)$ and the $j$ -th pixel of reference features $F^{r}(j)$ . ", "page_idx": 3}, {"type": "text", "text": "To minimize hyperparameter reliance, we leverage background features typically overlooked in FSS, indicated by the negative mask $y^{\\tilde{r}}\\,=\\,\\neg y^{r}$ of the reference image. According to $y^{r}$ and $y^{\\tilde{r}}$ , we divide $C$ into $C^{+}$ and $C^{-}$ in $\\mathbb{R}^{h w\\times h w}$ for foreground and background features, respectively. We then introduce two positive similarity maps in mean and max aspects respectively: ", "page_idx": 3}, {"type": "equation", "text": "$$\nS_{m e a n}^{+}(i)=\\frac{\\sum_{j=1}^{h w}C^{+}(i,j)}{\\sum_{j=1}^{h w}2(y^{r})_{j}},\\quad S_{m a x}^{+}(i)=m a x(C^{+}(i)),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{T}$ resizes $y^{r}$ to the same resolution as $F^{r}$ and then flatten it into a vector, $m a x(\\cdot)$ finds the maximum value in the $i$ -th row of $C^{+}$ . The mean positive similarity map $S_{m e a n}^{+}\\in\\mathbb{R}^{h w}$ captures global similarity towards the reference object but may blur distinct internal features, reducing accuracy for complex objects. In contrast, the max positive similarity map $S_{m a x}^{+}\\in\\mathbb{R}^{h w}$ focuses on the most similar regions, enhancing recall but also increasing noise. To maintain distinctiveness while reducing noise, we introduce the mixture similarity map $S_{m i x}^{+}=S_{m e a n}^{+}\\odot S_{m a x}^{+}$ using the Hadamard product. This method boosts target region distinctiveness by merging the strengths of both maps, while diminishing noise through the more stable global similarity. ", "page_idx": 4}, {"type": "text", "text": "To select prompt points, we also use the mean negative similarity map $S_{m e a n}^{-}$ , which reflects background similarity, noting that similar objects typically share higher background similarity values. We then align $S_{m i x}^{+}$ and $S_{m e a n}^{-}$ by min-max normalization $\\mathcal{M}$ to get: ", "page_idx": 4}, {"type": "equation", "text": "$$\nS_{p}(i)=\\mathcal{M}(S_{m i x}^{+})(i)\\cdot\\mathbf{1}_{\\{\\mathcal{M}(S_{m i x}^{+})(i)>\\mathcal{M}(S_{m e a n}^{-})(i)\\}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $S_{p}\\in\\mathbb{R}^{h w}$ is the filtered map for point selection, and $\\mathbf{1}_{\\{\\cdot\\}}$ is 1 if the condition is true and 0 otherwise. Although we minimize false negatives, noise points remain. To select suitable points from $S_{p}$ without hyperparameters, we define the sum of elements in $S_{p}$ as the number $N$ of points to be selected. We then pick the N highest-value points from Sp as the point prompt set P = {Pl}lN=1. ", "page_idx": 4}, {"type": "text", "text": "4.2 Point-Mask Clustering with Graph Connectivity ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We utilize point prompts from $_{P}$ to generate masks with SAM. Each point $P_{l}$ in $_{P}$ corresponds to a unique mask $\\hat{y}_{l}\\in\\mathbb{R}^{H\\times W}$ . As our point selection strategy prioritizes the coverage of objects, false-negative masks are unavoidable. Moreover, mask coverage can vary significantly within the same region, ranging from partial to full object coverage. This necessitates understanding the internal relationships among coarse-grained masks and points from fine-grained feature comparison to ensure those covering the same target are accurately gathered. ", "page_idx": 4}, {"type": "text", "text": "To address this, we design the Point-Mask Clustering (PMC) module, which clusters points and their corresponding masks based on mask coverage over points. Following the principles of efficiency and automation, the PMC module is based on a directed graph $G=(V,E)$ with the vertex $v_{l}$ in $V$ representing point $P_{l}$ and its corresponding mask $\\hat{y}_{l}$ . Edges in $E$ are established based on mask coverage over other points; an edge $e_{l,m}$ exists if mask $\\hat{y}_{l}$ covers points $P_{m}$ (with $m\\neq l$ ). Specifically, we do not establish edges for masks covering their corresponding points to avoid creating loops. ", "page_idx": 4}, {"type": "text", "text": "The graph $G$ is a directed simple graph, allowing us to cluster vertices by identifying weakly connected components. This clustering process is hyperparameter-free, ensuring that every pair of vertices $u,v\\in V$ within the same component has a directed path between them. Each weakly connected component encompasses a set of points $\\hat{P}_{p}$ (with $P=\\{\\hat{P}_{p}\\}\\}$ ) that are all covered by the union of their masks in $\\hat{M}_{p}$ , where $p$ indexes the clusters. ", "page_idx": 4}, {"type": "text", "text": "The advanced SAM plays a crucial role in maintaining the precision of the generated masks. The precision of high-quality masks typically ensures non-overlapping between masks and prompting points of adjacent regions, especially those of different categories. This is the precondition for the efficacy of our PMC module, as even slight errors could significantly impact the clustering accuracy. ", "page_idx": 4}, {"type": "text", "text": "4.3 Post-Gating on Weakly Connected Components ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our PNA module, while efficient in selecting points, inadvertently includes false positives, as detailed in Sec. 4.1. To mitigate this, we further develop two gating strategies targeting distinct types of false positives based on clusters formed from weakly connected components. ", "page_idx": 4}, {"type": "text", "text": "Positive gating. Despite the method in Sec. 4.1 diminishing the noise points outside the target region, there are still a few remaining noise points. These issues may have minimal impact on traditional segmentation methods, but under the SAM framework, masks derived from these noise points can significantly degrade accuracy. Moreover, some clusters of masks may extend beyond their intended target regions due to inaccuracies in SAM-generated masks or because the targeted object is part of a larger entity. Thus, we propose a Positive Gating strategy to address these issues. ", "page_idx": 4}, {"type": "text", "text": "This strategy prioritizes mask effectiveness by assessing whether a mask contains more positive than negative pixels, thereby facilitating a specialized designed mask growth for final prediction. The focus of mask growth is to enhance coverage of the target area rather than multiple objects, while minimizing background inclusion. Firstly, this method utilizes a parameter-free gating mechanism tahnadt $S_{m e a n}^{-}$ ,i ansa tdese sbcertiwbeede ni np iSxeelc .p 4o.l1a.r itTieos ,a cbhaiseevde  otnh itsh,e  wpeo suittiilviez ea $S_{m e a n}^{+}$ taivned $S_{m e a n}^{-}$ i,t ya lomnagp s,w $S_{m e a n}^{+}$ median of Sm+e $S_{m e a n}^{+}$ (i.e., the midpoint between the maximum and minimum values of $S_{m e a n}^{+}$ ), to constructs the polarity map $\\bar{R}$ as follows: ", "page_idx": 4}, {"type": "image", "img_path": "jYypS5VIPj/tmp/1b947790ed6d97d66fd9b7a65205829453901f23acbdb0d0785fc4512716967a.jpg", "img_caption": ["Figure 3: Illustration of the Overshooting Gating strategy. The outer ring of points in the second image indicates the most similar cluster of corresponding points, i.e., points with different outside and inside colors do not satisfy the self-consistency. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\bar{R}(i)=\\binom{1,\\ \\ \\ S_{m e a n}^{+}(i)\\times S_{m e a n}^{+}(i)>s_{m i d}\\times S_{m e a n}^{-}(i),}{-1,\\ \\ e l s e.}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then, using the polarity map $\\bar{R}$ , we calculate the number of positive pixels of the $l^{t h}$ mask as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\ns_{l}^{+}=\\sum_{i=1}^{h w}\\bar{R}(i)\\odot\\mathcal{T}(\\hat{y}_{l})(i),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{T}$ resizes and flattens $\\hat{y}_{l}$ to the feature map dimensions. Subsequently, for each cluster $\\hat{M}_{p}$ of weakly connected components, we sort the masks according to the ratio of positive pixel numbers to their areas. The indices of these sorted masks are denoted by $Q$ . We then initialize a blank pseudo mask $\\ddot{y}_{p}\\in\\mathbb{R}^{H\\times W}$ and a set of positive points $P^{+}$ . Following this, we apply a Mask Growth algorithm as outlined in Sec. A.1 and Alg. 1 for maintaining positive masks. This algorithm iteratively evaluates whether the region of ${\\hat{y}}_{q}$ outside the pseudo mask $\\ddot{y}_{p}$ is positive, updates $\\ddot{y}_{p}$ with the identified positive mask, and adds its corresponding point into $P^{+}$ . ", "page_idx": 5}, {"type": "text", "text": "Overshooting gating. The fine-grained semantic features from $f(\\cdot)$ are reliable for locating target objects, yet the point coverage of the target areas varies, leading to both under-coverage and overcoverage. SAM effectively addresses under-coverage; however, over-coverage, which extends beyond target boundaries, often produces false-positive masks. These overshooting points, while semantically similar to the target areas in $F^{t}$ , typically derive masks that cover areas outside the target, resulting in a mismatch of representations between the granularity of points and masks. Thus, these points cannot be clustered with points inside the target areas. ", "page_idx": 5}, {"type": "text", "text": "Hence, we devise an overshooting gating strategy with consideration of self-consistency to eliminate overshooting points and their associated masks. As shown in Fig. 3, We assess the similarity between the features of each point $P_{l}$ and the union mask $\\hat{y}_{p}\\,\\in\\,\\mathbb{R}^{H\\times W}$ from each mask cluster $\\hat{M}_{p}$ . The similarity computation for estimating self-consistency is performed as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\ns^{s c}(l,p)=\\frac{\\sum_{i=1}^{h w}S i m(F^{t}(P_{l}),(F^{t}\\odot\\mathbb{Z}(\\hat{y}_{p}))(i))}{\\sum_{i=1}^{h w}\\mathbb{Z}(\\hat{y}_{p})\\cdot d i s t(l,p)},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $S i m(\\cdot,\\cdot)$ refers to the correlation calculation mentioned in Eq. 1. We introduce an external function $d i s t(\\cdot,\\cdot)$ to measure the distance in $F^{t}$ between each point $P_{l}$ and the nearest selected point in $\\hat{P}_{p}$ . This measure helps confine comparison to neighboring clusters, minimizing interference from other instances. We then identify the cluster most similar to the points and retain those in the point set $P^{s c}$ that are more similar to their respective clusters. ", "page_idx": 5}, {"type": "table", "img_path": "jYypS5VIPj/tmp/24a3ea131e7f8d6655df4a4cb51ee72bca236a26c06d460d519b75cc98194da8.jpg", "table_caption": ["Table 1: Performance on Few-shot Semantic Segmentation datasets of Pascal- $.5^{\\mathrm{i}}$ , $\\mathrm{COCO-}20^{\\mathrm{i}}$ , FSS1000, and LVIS-92i. Gray means the in-domain trained results. The best results are shown in bold. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "jYypS5VIPj/tmp/4405160f729a4d38ea5102728be8d78e7dc5e1a8b10f26028909cf11d3fa64b1.jpg", "table_caption": ["Table 2: Performance on One-shot Part Segmentation datasets and Cross Domain Few-shot Semantic Segmentation datasets. The best results are shown in bold. ", "Mask Merging. Finally, we obtain two distinct sets of points, namely $P^{+}$ and $P^{s c}$ . We then union the masks corresponding to points that are common to both $P^{+}$ and $P^{s c}$ . The merged masks form the final prediction, denoted as $\\tilde{y}$ . "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5 Experimental Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To illustrate the Few-shot Semantic Segmentation ability and generalization capacity, we conduct three types of sub-tasks, i.e. standard Few-shot Semantic Segmentation, One-shot Part Segmentation, and Cross Domain Few-shot Semantic Segmentation. The datasets for these tasks are as follows: ", "page_idx": 6}, {"type": "text", "text": "Pascal- ${\\bf5^{i}}$ , COCO-20i, FSS-1000, and LVIS-92i are standard FSS datasets. Pascal- $.5^{\\mathrm{i}}$ [22] is based on the Pascal VOC 2012 [53] and SDS [54]. The 20 classes are separated into 4 folds of 5 classes. COCO$20^{\\mathrm{i}}$ [23] is an 80-class dataset from MSCOCO [55], which has 4 folds with each fold containing 20 classes. FSS-1000 [24] contains 1000 classes. The training, validation, and testing folds contain 520, 240, and 240 classes, respectively. LVIS- $92^{\\mathrm{i}}$ [13] is more challenging for evaluating generalist models, which select 920 classes with more than 2 images and divide these classes into 10 folds. ", "page_idx": 6}, {"type": "text", "text": "PASCAL-Part and PACO-Part [13] are One-shot Part Segmentation datasets. PASCAL-Part [56, 57] contains 56 different object parts in 4 superclasses. PACO-Part is built based on the PACO dataset [58], which has 456 object part classes. The 303 classes with at least 2 samples in PACO-Part are divided into four folds following Matcher [13]. ", "page_idx": 6}, {"type": "text", "text": "Deepglobe, ISIC2018, and iSAID- $.5^{\\mathbf{i}}$ are Cross Domain FSS datasets. The Deepglobe [25] contains satellite images of geographic categories including urban, agriculture, rangeland, forest, water, and barren. The ISIC2018 [26] is a skin lesion analysis dataset with three classes. The iSAID- $\\cdot5^{\\mathrm{i}}$ [27] evenly split 3 folds for 15 classes based on the remote sensing dataset iSAID [59]. ", "page_idx": 6}, {"type": "table", "img_path": "jYypS5VIPj/tmp/73f61c2a120dea1cdb47eb22ecc82e73cdcb4ff2e99349668640d2d75304e28c.jpg", "table_caption": ["Table 3: Ablation study of Point Selection. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "jYypS5VIPj/tmp/933a556d501c241f394c5a2ea2be20352b67d9e3f9ed47f21609ad4d44fed1cd.jpg", "table_caption": ["Table 4: Ablation study of PMC and Post-Gating. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "jYypS5VIPj/tmp/d20d44c74dfc281c7f474b141052e4845a0146552fe774a4794f19f49903753a.jpg", "table_caption": ["Table 5: Ablation study of positive gating on each cluster. M.G. represents the Mask Growth algorithm. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "jYypS5VIPj/tmp/5a4cd599dbcc236d3b367914fe2c88a2ee46cc906a69bdfbccd884e172746c9c.jpg", "table_caption": ["Table 6: Ablation on the strategies of SelfConsistency measurement. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.2 Implementation Details ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Following the settings of PerSAM [12] and Matcher [13] for a fair comparison, we use DINOv2 [11] with a ViT-L/14 [60] as our feature extraction backbone, and SAM [10] with ViT-H as the mask generator. The input image sizes are set to $518\\!\\times\\!518$ for DINOv2 and $1024\\!\\times\\!1024$ for SAM following Matcher [13]. Except for the default hyperparameters of SAM and DINOv2, our approach does not have any external hyperparameter. We apply the mean Intersection over Union (mIoU) metric for evaluating the performance. All experiments are conducted on a single NVIDIA RTX2080Ti. ", "page_idx": 7}, {"type": "text", "text": "5.3 Comparison with State-Of-The-Arts ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Comparison on the standard FSS datasets. We compared our approach with other state-of-the-art specialist and generalist models. As shown in Tab. 1, our approach achieves $72.1\\%$ mIoU on the Pascal- $.5^{\\mathrm{i}}$ dataset and $58.7\\%$ mIoU on $\\mathrm{COCO}{-}20^{\\mathrm{i}}$ dataset, which surpasses all previous specialist and generalist state-of-the-art models. Our approach reaches $35.2\\%$ mIoU in the more challenging dataset of LVIS-92i, with $2.2\\%$ of improvement compared to the previous training-free method Matcher. The performance remains competitive on the FSS-1000 compared with specialist models. The 5-shot result of Pascal- $.5^{\\mathrm{i}}$ , $\\mathrm{COCO-}\\bar{2}0^{\\mathrm{i}}$ , and LVIS-92i further extends the lead, which shows that our approach can effectively handle the few-shot scenario. ", "page_idx": 7}, {"type": "text", "text": "Comparison on the One-shot Part Segmentation datasets. The One-shot Part Segmentation tasks evaluate the ability to fetch the target part from the whole object. The results in Tab. 2 show that our approach achieves the mIoU of $44.5\\%$ and $36.3\\%$ on both datasets of PASCAL-Part and PACO-Part, respectively. Our approach outperforms the state-of-the-art generalist model Matcher with $1.6\\%$ on both datasets. Given that Matcher employs specific hyperparameters to enhance part segmentation, our superior performance demonstrates the adaptability of our approach across both object and part segmentation contexts. ", "page_idx": 7}, {"type": "text", "text": "Comparison on the Cross Domain FSS datasets. The Cross Domain FSS tasks validate the performance on different domains. Our approach achieves state-of-the-art performance in datasets of Deepglobe, ISIC, and iSAID- $.5^{\\mathrm{i}}$ among other specialist domain models and generalist models. Especially within the context of the skin lesion analysis dataset ISIC and remote sensing dataset iSAID- $5^{\\mathrm{i}}$ , our approach outperforms Matcher by margins of $10.1\\%$ and $13.8\\%$ respectively. ", "page_idx": 7}, {"type": "image", "img_path": "jYypS5VIPj/tmp/3b987981102a4f161f7a4b89116d203395d5dd85ef83146ad482b2d0de7bf9ea.jpg", "img_caption": ["Figure 4: Qualitative analysis of Matcher, Baseline, $\\scriptstyle\\mathrm{B+PG}$ , $\\scriptstyle\\mathrm{B+PG+OG}$ . B, PG, and OG respectively represent Baseline, Positive Gating, and Overshooting Gating. Masks in ref. image are shown in blue. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.4 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Point selection. We evaluate the impact of various similarity maps and the parameter-free selection of top $_\\mathrm{N}$ points on performance, as detailed in Sec. 4.1. As shown in Tab. 3, using either $S_{m e a n}^{+}$ or $\\bar{S}_{m a x}^{+}$ alone leads to a performance drop of up to $5.6\\%$ compared to using both. This decline is due to the inherent limitations of $S_{m e a n}^{+}$ and $S_{m a x}^{+}$ discussed in $\\mathbf{Sec}\\ 4.1$ . Additionally, the evaluation confirms that picking the top-N points based on similarity, which is parameter-free and requires no additional settings, simplifies the process and increases accuracy by $\\bar{2}.1\\%$ . ", "page_idx": 8}, {"type": "text", "text": "Clustering method. We compare our PMC module using weakly connected components with the PMC module using strong connected components, which provides finer clustering results. According to our experiment results in Tab. 4, the clusters from weakly connected components provide better performance on $\\mathrm{COCO}{-}20^{\\mathrm{i}}$ and LVIS-92i for both gating, as these clusters of masks have ideal coverage of the objects. Simply flitering the masks without clustering-based gating can only achieve $44.0\\%$ mIoU on COCO- $\\it{20^{i}}$ and $24.2\\%$ mIoU on LVIS-92i, which is significantly lower than the performance achieved with clustering-based gating. Furthermore, our dynamic hyperparameter-free clustering method outperforms the $\\boldsymbol{\\mathrm{k}}$ -means+ $^{\\cdot+}$ with $1.2\\%$ on both datasets. Note that k of $\\mathbf{k}$ -means $^{++}$ is set to 10 following Matcher [13]. ", "page_idx": 8}, {"type": "text", "text": "Positive gating. Our approach compares the number of positive points and negative points in ${\\hat{S}}^{+}$ (Num) to judge whether the mask is positive. We conduct experiments for the strategy of comparing the sum of positive and negative values (Sum). The results in Tab. 5 demonstrate the Num strategy yields better performance, as comparing the number of pixels mitigates the influence of a few excessively high similarity values. Furthermore, the utilization of the Mask Growth algorithm improves both FSS and Part Segmentation performance by carefully retaining the positive regions. However, it weakens the improvement of Num due to their similar effects. ", "page_idx": 8}, {"type": "text", "text": "Overshooting gating. Our Overshooting gating aims to filter out the overshooting points closely neighboring to target regions, thus having a less remarkable improvement of $1.6\\%$ compared to Positive Gating, as shown in Tab. 6. This performance still surpasses the mean similarity of comparing points with regions of clustered points (Point Sim.) or the prototypes from Masked Average Pooling [43] with union masks (MAP Sim.). More importantly, the distance function avoids the gating from $9.6\\%$ of performance decline. It ensures each cluster only affects its neighboring points. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5.5 Qualitative Analysis ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Here we present the qualitative results of Matcher, Baseline $\\mathrm{\\Delta}^{\\mathrm{[\\,st}}$ row in Tab. 4), Baseline $\\mathbf{+PG}$ $\\!2^{\\mathrm{nd}}$ row in Tab. 4) and our approach in Fig. 4. The bipartite matching of Matcher has a negative influence when the areas of the target object in reference and target images have significant differences, as shown in the $1^{\\mathrm{st}}$ and $3^{\\mathrm{rd}}$ rows. The positive gating with clustering fliters out the noise masks in the $3^{\\mathrm{rd}}$ row, while the Overshooting Gating further removes the masks belonging to overshooting points in the $2^{\\mathrm{nd}}$ and $4^{\\mathrm{th}}$ rows. More qualitative analyses please refer to the appendix. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we proposed an efficient, training-free SAM-based FSS approach that requires no external hyperparameters. As an automatic SAM-based semantic segmentation pipeline, our approach balanced candidate points and object coverage in the Positive-Negative Alignment (PNA) module, then used SAM-generated masks in the Point-Mask Clustering (PMC) module to enhance Post Gating. Extensive experiments validated the superior performance of our approach, advancing semantic segmentation without extensive parameter tuning or training. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Natural Science Foundation of China under No. 62472033, No. U23A20314, and No. 61972036. J. Jiao is supported by the Royal Society Short Industry Fellowship (SIF\\R1\\231009) and the Amazon Research Award. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] J. Long, E. Shelhamer, and T. Darrell, \u201cFully convolutional networks for semantic segmentation,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431\u20133440, 2015.   \n[2] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, \u201cSemantic image segmentation with deep convolutional nets and fully connected crfs,\u201d arXiv preprint arXiv:1412.7062, 2014.   \n[3] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, \u201cRethinking atrous convolution for semantic image segmentation,\u201d arXiv preprint arXiv:1706.05587, 2017.   \n[4] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, \u201cDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs,\u201d IEEE Transactions on Pattern Recognition and Machine Intelligence, vol. 40, no. 4, pp. 834\u2013848, 2017.   \n[5] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, \u201cSegformer: Simple and efficient design for semantic segmentation with transformers,\u201d Advances in neural information processing systems, vol. 34, pp. 12077\u201312090, 2021.   \n[6] Z. Zhang, G. Gao, Z. Fang, J. Jiao, and Y. Wei, \u201cMining unseen classes via regional objectness: A simple baseline for incremental segmentation,\u201d Advances in neural information processing systems, vol. 35, pp. 24340\u201324353, 2022.   \n[7] Z. Zhang, G. Gao, J. Jiao, C. H. Liu, and Y. Wei, \u201cCoinseg: Contrast inter-and intra-class representations for incremental segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 843\u2013853, 2023.   \n[8] A. Zhang and G. Gao, \u201cBackground adaptation with residual modeling for exemplar-free class-incremental semantic segmentation,\u201d Proc. European Conference on Computer Vision, 2024.   \n[9] B. Cheng, A. Schwing, and A. Kirillov, \u201cPer-pixel classification is not all you need for semantic segmentation,\u201d Advances in neural information processing systems, vol. 34, pp. 17864\u201317875, 2021.   \n[10] A. Kirillov, E. Mintun, and et al., \u201cSegment anything,\u201d in Proc. IEEE International Conference on Computer Vision, pp. 4015\u20134026, 2023.   \n[11] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, et al., \u201cDinov2: Learning robust visual features without supervision,\u201d arXiv preprint arXiv:2304.07193, 2023.   \n[12] R. Zhang, Z. Jiang, Z. Guo, S. Yan, J. Pan, H. Dong, Y. Qiao, P. Gao, and H. Li, \u201cPersonalize segment anything model with one shot,\u201d in Proc. International Conference on Learning Representations, 2024.   \n[13] Y. Liu, M. Zhu, H. Li, H. Chen, X. Wang, and C. Shen, \u201cMatcher: Segment anything with one shot using all-purpose feature matching,\u201d in Proc. International Conference on Learning Representations, 2024.   \n[14] J. Min, D. Kang, and M. Cho, \u201cHypercorrelation squeeze for few-shot segmentation,\u201d in Proc. IEEE International Conference on Computer Vision, pp. 6941\u20136952, 2021.   \n[15] Q. Xu, W. Zhao, G. Lin, and C. Long, \u201cSelf-calibrated cross attention network for few-shot segmentation,\u201d in Proc. IEEE International Conference on Computer Vision, pp. 655\u2013665, 2023.   \n[16] Y. Sun, Q. Chen, X. He, J. Wang, H. Feng, J. Han, E. Ding, J. Cheng, Z. Li, and J. Wang, \u201cSingular value fine-tuning: Few-shot segmentation requires few-parameters fine-tuning,\u201d Advances in neural information processing systems, vol. 35, pp. 37484\u201337496, 2022.   \n[17] Y. Liu, N. Liu, Q. Cao, X. Yao, J. Han, and L. Shao, \u201cLearning non-target knowledge for few-shot semantic segmentation,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 11573\u201311582, 2022.   \n[18] Z. Fang, G. Gao, Z. Zhang, and A. Zhang, \u201cHierarchical context-agnostic network with contrastive feature diversity for one-shot semantic segmentation,\u201d Journal of Visual Communication and Image Representation, vol. 90, p. 103754, 2023.   \n[19] G. Gao, Z. Fang, C. Han, Y. Wei, C. H. Liu, and S. Yan, \u201cDrnet: Double recalibration network for few-shot semantic segmentation,\u201d IEEE Transactions on Image Processing, vol. 31, pp. 6733\u2013 6746, 2022.   \n[20] X. Wang, X. Zhang, Y. Cao, W. Wang, C. Shen, and T. Huang, \u201cSeggpt: Towards segmenting everything in context,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1130\u20131140, 2023.   \n[21] X. Wang, W. Wang, Y. Cao, C. Shen, and T. Huang, \u201cImages speak in images: A generalist painter for in-context visual learning,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6830\u20136839, 2023.   \n[22] A. Shaban, S. Bansal, Z. Liu, I. Essa, and B. Boots, \u201cOne-shot learning for semantic segmentation,\u201d in Proc. British Machine Vision Conference, pp. 167.1\u2013167.13, 2017.   \n[23] K. Nguyen and S. Todorovic, \u201cFeature weighting and boosting for few-shot segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 622\u2013631, 2019.   \n[24] X. Li, T. Wei, Y. P. Chen, Y.-W. Tai, and C.-K. Tang, \u201cFss-1000: A 1000-class dataset for few-shot segmentation,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2869\u20132878, 2020.   \n[25] I. Demir, K. Koperski, D. Lindenbaum, G. Pang, J. Huang, S. Basu, F. Hughes, D. Tuia, and R. Raskar, \u201cDeepglobe 2018: A challenge to parse the earth through satellite images,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 172\u2013181, 2018.   \n[26] N. Codella, V. Rotemberg, P. Tschandl, M. E. Celebi, S. Dusza, D. Gutman, B. Helba, A. Kalloo, K. Liopyris, M. Marchetti, et al., \u201cSkin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (isic),\u201d arXiv preprint arXiv:1902.03368, 2019.   \n[27] X. Yao, Q. Cao, X. Feng, G. Cheng, and J. Han, \u201cScale-aware detailed matching for few-shot aerial image semantic segmentation,\u201d IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 1\u201311, 2021.   \n[28] C. Zhang, G. Lin, F. Liu, R. Yao, and C. Shen, \u201cCanet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 5217\u20135226, 2019.   \n[29] K. Wang, J. H. Liew, Y. Zou, D. Zhou, and J. Feng, \u201cPanet: Few-shot image semantic segmentation with prototype alignment,\u201d in Proc. IEEE International Conference on Computer Vision, pp. 9197\u20139206, 2019.   \n[30] G. Li, V. Jampani, L. Sevilla-Lara, D. Sun, J. Kim, and J. Kim, \u201cAdaptive prototype learning and allocation for few-shot segmentation,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 8334\u20138343, 2021.   \n[31] Z. Tian, H. Zhao, M. Shu, Z. Yang, R. Li, and J. Jia, \u201cPrior guided feature enrichment network for few-shot segmentation,\u201d IEEE Transactions on Pattern Recognition and Machine Intelligence, vol. 44, no. 2, pp. 1050\u20131065, 2020.   \n[32] M. Boudiaf, H. Kervadec, Z. I. Masud, P. Piantanida, I. Ben Ayed, and J. Dolz, \u201cFew-shot segmentation without meta-learning: A good transductive inference is all you need?,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 13979\u201313988, 2021.   \n[33] A. Okazawa, \u201cInterclass prototype relation for few-shot segmentation,\u201d in Proc. European Conference on Computer Vision, pp. 362\u2013378, 2022.   \n[34] J.-W. Zhang, Y. Sun, Y. Yang, and W. Chen, \u201cFeature-proxy transformer for few-shot segmentation,\u201d in Advances in neural information processing systems, pp. 6575\u20136588, 2022.   \n[35] X. Shi, D. Wei, Y. Zhang, D. Lu, M. Ning, J. Chen, K. Ma, and Y. Zheng, \u201cDense cross-queryand-support attention weighted mask aggregation for few-shot segmentation,\u201d in Proc. European Conference on Computer Vision, pp. 151\u2013168, 2022.   \n[36] H. Liu, P. Peng, T. Chen, Q. Wang, Y. Yao, and X.-S. Hua, \u201cFecanet: Boosting few-shot semantic segmentation with feature-enhanced context-aware network,\u201d vol. 25, pp. 8580\u20138592, 2023.   \n[37] Y. Wang, R. Sun, and T. Zhang, \u201cRethinking the correlation in few-shot segmentation: A buoys view,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 7183\u20137192, 2023.   \n[38] Y. Liu, N. Liu, X. Yao, and J. Han, \u201cIntermediate prototype mining transformer for few-shot semantic segmentation,\u201d vol. 35, pp. 38020\u201338031, 2022.   \n[39] S. Jiao, G. Zhang, S. Navasardyan, L. Chen, Y. Zhao, Y. Wei, and H. Shi, \u201cMask matching transformer for few-shot segmentation,\u201d Advances in neural information processing systems, vol. 35, pp. 823\u2013836, 2022.   \n[40] G. Zhang, G. Kang, Y. Yang, and Y. Wei, \u201cFew-shot segmentation via cycle-consistent transformer,\u201d Advances in neural information processing systems, vol. 34, pp. 21984\u201321996, 2021.   \n[41] C. Lang, G. Cheng, B. Tu, and J. Han, \u201cLearning what not to segment: A new perspective on few-shot segmentation,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 8057\u20138067, 2022.   \n[42] Q. Fan, W. Pei, Y.-W. Tai, and C.-K. Tang, \u201cSelf-support few-shot semantic segmentation,\u201d in Proc. European Conference on Computer Vision, pp. 701\u2013719, 2022.   \n[43] X. Zhang, Y. Wei, Y. Yang, and T. S. Huang, \u201cSg-one: Similarity guidance network for one-shot semantic segmentation,\u201d IEEE Transactions on Cybernetics, vol. 50, no. 9, pp. 3855\u20133865, 2020.   \n[44] B. Peng, Z. Tian, X. Wu, C. Wang, S. Liu, J. Su, and J. Jia, \u201cHierarchical dense correlation distillation for few-shot segmentation,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 23641\u201323651, 2023.   \n[45] Y. Wang, N. Luo, and T. Zhang, \u201cFocus on query: Adversarial mining transformer for few-shot segmentation,\u201d Advances in neural information processing systems, vol. 36, pp. 31524\u201331542, 2023.   \n[46] F. Li, H. Zhang, P. Sun, X. Zou, S. Liu, J. Yang, C. Li, L. Zhang, and J. Gao, \u201cSemantic-sam: Segment and recognize anything at any granularity,\u201d arXiv preprint arXiv:2307.04767, 2023.   \n[47] H. Yuan, X. Li, C. Zhou, Y. Li, K. Chen, and C. C. Loy, \u201cOpen-vocabulary sam: Segment and recognize twenty-thousand classes interactively,\u201d arXiv preprint, 2024.   \n[48] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al., \u201cLearning transferable visual models from natural language supervision,\u201d in International conference on machine learning, pp. 8748\u20138763, PMLR, 2021.   \n[49] Y. Sun, J. Chen, S. Zhang, X. Zhang, Q. Chen, G. Zhang, E. Ding, J. Wang, and Z. Li, \u201cVrp-sam: Sam with visual reference prompt,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2024.   \n[50] S. Hong, S. Cho, J. Nam, S. Lin, and S. Kim, \u201cCost aggregation with 4d convolutional swin transformer for few-shot segmentation,\u201d in Proc. European Conference on Computer Vision, pp. 108\u2013126, 2022.   \n[51] J. Su, Q. Fan, G. Lu, F. Chen, and W. Pei, \u201cDomain-rectifying adapter for cross-domain few-shot segmentation,\u201d 2024.   \n[52] Q. Cao, Y. Chen, C. Ma, and X. Yang, \u201cFew-shot rotation-invariant aerial image semantic segmentation,\u201d IEEE Transactions on Geoscience and Remote Sensing, vol. 62, pp. 1\u201313, 2023.   \n[53] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman, \u201cThe pascal visual object classes (voc) challenge,\u201d International Journal on Computer Vision, vol. 88, pp. 303\u2013338, 2010.   \n[54] B. Hariharan, P. Arbel\u00e1ez, L. Bourdev, S. Maji, and J. Malik, \u201cSemantic contours from inverse detectors,\u201d in Proc. IEEE International Conference on Computer Vision, pp. 991\u2013998, 2011.   \n[55] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick, \u201cMicrosoft coco: Common objects in context,\u201d in Proc. European Conference on Computer Vision, pp. 740\u2013755, 2014.   \n[56] X. Chen, R. Mottaghi, X. Liu, S. Fidler, R. Urtasun, and A. Yuille, \u201cDetect what you can: Detecting and representing objects using holistic models and body parts,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1971\u20131978, 2014.   \n[57] K. Morabia, J. Arora, and T. Vijaykumar, \u201cAttention-based joint detection of object and semantic part,\u201d arXiv preprint arXiv:2007.02419, 2020.   \n[58] V. Ramanathan, A. Kalia, V. Petrovic, Y. Wen, B. Zheng, B. Guo, R. Wang, A. Marquez, R. Kovvuri, A. Kadian, et al., \u201cPaco: Parts and attributes of common objects,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7141\u20137151, 2023.   \n[59] S. Waqas Zamir, A. Arora, A. Gupta, S. Khan, G. Sun, F. Shahbaz Khan, F. Zhu, L. Shao, G.-S. Xia, and X. Bai, \u201cisaid: A large-scale dataset for instance segmentation in aerial images,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 28\u201337, 2019.   \n[60] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al., \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d Proc. International Conference on Learning Representations, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 More Details for Mask Growth Algorithm ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We mention the Mask Growth algorithm in Sec. 4.3. The Mask Growth algorithm is designed for each cluster of masks $\\hat{M}_{w e a k,p}$ . The details of the algorithm are shown in Alg. 1. We first initialize an empty set $P^{+}$ and a blank pseudo mask $\\ddot{y}_{p}$ . Then, we start an iterative process and get the current mask ${\\hat{y}}_{q}$ based on the sorted sequence of indices $Q$ . The parts of the current mask ${\\hat{y}}_{q}$ overlapping with $\\ddot{y}_{p}$ are removed. We compute the positive value $s_{q}^{+}$ of the remaining parts. If $s_{q}^{+}$ is positive, the mask ${\\hat{y}}_{q}$ is updated into the $\\ddot{y}_{p}$ and its corresponding point $P_{q}$ is added into $P^{+}$ . As soon as the iterative process finishes, the set of positive points $P^{+}$ is established. ", "page_idx": 13}, {"type": "text", "text": "Algorithm 1 Mask Growth for each cluster   \nInput: $\\hat{M}_{p},\\ddot{y}_{p},Q,P^{+}$ for $n=1$ to $|Q|$ do $q\\leftarrow Q(n)$ y\u02c6q \u2190M\u02c6p(q) $\\hat{y_{q}}=\\hat{y}_{q}\\bar{\\&}\\sim\\ddot{y}_{p}$ $\\begin{array}{r}{s_{q}^{+}\\gets\\sum_{i=1}^{h w}\\hat{S}^{+}(i)\\odot\\mathcal{Z}(\\hat{y}_{q})(i)}\\end{array}$ if $s_{q}^{+}>0$ then Add $P_{q}$ to $P^{+}$ . $\\ddot{y}_{p}=\\hat{y_{q}}\\vee\\ddot{y}_{p}$ end if end for   \nOutput: $P^{+}$ ", "page_idx": 13}, {"type": "text", "text": "A.2 Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Our approach has impressive performance on Few-shot Semantic Segmentation tasks. However, due to the resolution of features $F^{t}$ from DINOv2 not aligning with the required resolution for prompting the SAM, we directly map the coordinates of points in $F^{\\check{t}}$ to coordinates for prompting. This results in coordinate bias for small objects, as the gap between neighboring points can reach approximately 28 pixels. Our future work will focus on locating small objects. ", "page_idx": 13}, {"type": "text", "text": "A.3 Societal Impacts ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "As a completely automatic SAM-based few-shot semantic segmentation approach without external hyperparameters, our method is capable of handling various scenarios of semantic segmentation, as demonstrated by our extensive experiments. The efficiency and generalizability of our method ensure a wide range of applications. Furthermore, since our training-free method is constructed upon the widely used open-source foundation models, we have not identified the negative societal impact to date. ", "page_idx": 13}, {"type": "text", "text": "A.4 Details of Current SAM-based FSS Methods ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Our approach aims to address several issues present in previous SAM-based FSS methods to achieve an automatic SAM-based model. These issues include the requirement of excessive external hyperparameters, overusing the mask generator of SAM, prolonged inference times, etc. The Tab. 7 shows the difference between our approach and previous SAM-based FSS methods. Fig. 5 shows the difference in using SAM as the mask generator between our approach and previous methods. Fig. 5(a) presents the iterative refinement of PerSAM, which involves generating masks from SAM 3 times. Fig. 5(b) exhibits that Matcher introduces an external Automatic Mask Generator, which automatically prompts for generating all mask proposals in the image. Our approach in Fig. 5(c) only utilizes the standard Mask Generator of SAM and generates the masks with our prompts only once. ", "page_idx": 13}, {"type": "table", "img_path": "", "table_caption": ["Table 7: Details of the current SAM-based FSS methods. "], "table_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "jYypS5VIPj/tmp/58f9675020d9724669c623db0f0d9f6cd04800e41c08704504dec8561b80c0bd.jpg", "img_caption": ["Figure 5: Comparison of the pipeline between the previous methods and our approach. (a) PerSAM [12] iteratively uses the Mask Generator to refine the mask. (b) Matcher [13] introduced an external Automatic Mask Generator [10] with automatic prompting to excessively generate masks from the whole image. (c) The effectiveness of the PMC module and Post-Gating ensures that our approach uses Mask Generator with our prompts only once. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.5 Discussion of SAM ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.5.1 Features from ViT Encoder of SAM ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Previous state-of-the-art generalist FSS methods $[]$ use DINOv2 or ResNet-50, instead of the default ViT encoder of SAM, for fine-grained features. We visualize the representative samples of Pascal- $5^{\\mathrm{i}}$ in Fig. 6. The $3^{\\mathrm{rd}}$ column of maps represents the self-similarity of the $F_{S A M}^{t}$ . We introduce the $3\\times3$ average pooling for F St $F_{S A M}^{t}$ followed by computing the cosine similarity between the pooled features and $\\bar{F}_{S A M}^{t}$ . The maps illustrate that F StAM can accurately identify the regions of objects within the image, where the features within each object region are nearly identical, while features between neighboring different objects are distinct. ", "page_idx": 14}, {"type": "text", "text": "Although the characteristics of $F_{S A M}^{t}$ ensure the generation of high-quality masks, the coarse-grained features are not suitable for locating the objects, as shown in the $4^{\\mathrm{th}}$ column. The similarity between F StAM and F Sr $F_{S A M}^{r}$ cannot effectively distinguish the target object well compared to $S_{m e a n}^{+}$ from DINOv2. Therefore, we follow the previous methods using DINOv2 for fine-grained features. ", "page_idx": 14}, {"type": "text", "text": "A.5.2 Masks analysis for Point-Mask Clustering ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our Point-Mask Clustering module introduces a parameter-free clustering method by constructing a graph of coverage. The effectiveness of the method primarily relies on the high-quality masks, whose boundaries mostly align with the object boundaries. We roughly analyze the coverage of masks generated from the points in the ground truth foreground region using 4000 samples from Pascal- $.5^{\\mathrm{i}}$ . In particular, we get the union of masks from the foreground points as $\\hat{y}_{f o r e}$ , and visualize three distributions, including the distribution of IoU between $\\hat{y}_{f o r e}$ and union of masks from background points $\\hat{y}_{b a c k}$ in Fig. 7, the ratio between the number of background points and all points covered by the $\\hat{y}_{f o r e}$ in Fig. 8, the number of background points covered by $\\hat{y}_{f o r e}$ in Fig. 9. The distribution charts demonstrate that most of the samples have acceptable coverage on the error points for Point-Mask Clustering. Given the limited precision of ground truth annotations, the analysis is for reference only. The effectiveness of our Point-Mask Clustering is validated in our ablation study in Tab. 4. ", "page_idx": 14}, {"type": "image", "img_path": "jYypS5VIPj/tmp/2dc50c104e500e897a9cffb576e8289d385dc5909c30083308841322d0498cb5.jpg", "img_caption": ["Figure 6: Analysis of the features from default ViT encoder of SAM. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "jYypS5VIPj/tmp/430632ef0c50cf4dd2eaa82312e3d37003af7e0a7bbfcdf751ed2538dd533b19.jpg", "img_caption": ["Figure 7: The distribution of IoU between masks from foreground points and from background points. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.6 Additional Experiment Results. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.6.1 Performance of Different Foundation Model Sizes ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Tab. 8 shows the experiment results of our approach with different scales of SAM and DINOv2. Compared to the previous training-free method Matcher, our approach still achieves a better performance with SAM-Large and DINOv2-Base. The fair comparison with SAM-Huge and DINOv2-Large further demonstrates the effectiveness of our approach. ", "page_idx": 15}, {"type": "text", "text": "A.6.2 Detailed Results of Evaluation Datasets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We present the detailed results on different Few-shot Semantic Segmentation datasets, including Pascal- $\\cdot5^{\\mathrm{i}}$ in Tab. 9, COCO- $\\cdot20^{\\mathrm{i}}$ in Tab. 10, LVIS-92i in Tab. 11, PASCAL-Part and PACO-Part in ", "page_idx": 15}, {"type": "image", "img_path": "jYypS5VIPj/tmp/26c67aa2333e5a82259ac7d7e3b14f0dfa5af3aec3df8a4cecf8cc3ecb4342ca.jpg", "img_caption": ["Figure 8: The distribution of the ratio between the number of background points and all points covered by the masks from foreground points. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "jYypS5VIPj/tmp/0eceae09cd9c68344fa64e1bf85e4dd0231ff35c56fa85fcc4c32176043f0043.jpg", "img_caption": ["Figure 9: The distribution of the number of background points covered by the masks from foreground points. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Tab. 12, iSAID- $.5^{\\mathrm{i}}$ in Tab. 13. The results show that our approach has remarkable performance in each fold of the datasets, demonstrating its generalized effectiveness in various scenarios. ", "page_idx": 16}, {"type": "text", "text": "A.6.3 Multiple Random Seeds Experiment ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Previous state-of-the-art methods, including both specialist methods and generalist methods, typically do not conduct multiple random seed experiments to evaluate the robustness. To demonstrate the robustness of our approach, we randomly set 5 different random seeds and conducted the experiments on the datasets that were not fully evaluated in the standard evaluation. As shown in Fig. 10, despite variations in random seeds, our approach consistently exhibits better performance compared to previous methods that were not evaluated with random seeds. ", "page_idx": 16}, {"type": "text", "text": "A.7 Additional Ablation Study ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.7.1 Ablation Study of Pivots for Positive Gating ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We apply both $s_{m i d}$ and $S_{m e a n}^{-}$ as the pivots for Positive Gating in Sec. 4.3, aiming to leverage both the pivots from the $S_{m e a n}^{+}$ itself and the negative similarity. As shown in Tab. 14, combining these two pivots for Positive Gating yields a significant improvement compared to using only one pivot. Moreover, the combination method of $\\times$ shows a $0.3\\%$ mIoU enhancement compared to $^+$ . ", "page_idx": 16}, {"type": "text", "text": "Table 8: Evaluation of our approach with different sizes of SAM and DINOv2. ", "page_idx": 17}, {"type": "table", "img_path": "jYypS5VIPj/tmp/b95cdb8bba8f7929d91b2594061205a7bb8534df6ac3d8414e5422eca6d4e3c7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "jYypS5VIPj/tmp/ab0d1a8c59bc84e3aa096803536c728caf6d7437ad4526e71b4d5789a217f4f1.jpg", "table_caption": ["Table 9: Detail results of Pascal- $.5^{\\mathrm{i}}$ . "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "jYypS5VIPj/tmp/03cc49206921574d17b46b3bf83418da5e82830783c1f7ab02ac2aa1753de225.jpg", "table_caption": ["Table 10: Detail results of $\\mathrm{COCO-20^{i}}$ . "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "jYypS5VIPj/tmp/224452ad9f65bac266b6e57f0fd31af9442e0e5cd372ced99fb8f24376960de6.jpg", "table_caption": ["Table 11: Detail results of LVIS-92i "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "jYypS5VIPj/tmp/23c7e10cef91e61980709e8a675e6a953f77592f838aa31acd0c5bc4d396ed3c.jpg", "table_caption": ["Table 12: Detail results of PASCAL-Part and PACO-Part. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "jYypS5VIPj/tmp/82a1b17274309275eb8f4431f83f6afeedb96e580818c483d34d6a81bfb4124f.jpg", "table_caption": ["Table 13: Detail results of iSAID- $.5^{\\mathrm{i}}$ . "], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "jYypS5VIPj/tmp/6f40b21970df223270b7f74887b5d98469893a6147d42353b180b9e727655227.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 10: Results of our approach in multiple random seeds experiment. The bars in the chart represent the result under the previous standard evaluation. The error bar depicts the boundaries of our performance. ", "page_idx": 18}, {"type": "table", "img_path": "jYypS5VIPj/tmp/d264efc281f7860de4c785c3d5d37070d39595506c43f7e82523cb4c195e463a.jpg", "table_caption": ["Table 14: Ablation study of pivots and combination operations in Positive Gating. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "jYypS5VIPj/tmp/899071a58e55f253dc19d29b1fed2d3ae425e52fac44b65240dda013e703d2a9.jpg", "table_caption": ["Table 15: Ablation study of different strategies for Positive Gating of the masks. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "A.7.2 Ablation Study of Other Strategies for Positive Gating ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Sec. 4.3 and Sec. A.1, we introduce the Mask Growth algorithm as our strategy for judging whether the mask is positive. We compare the strategy to separately judging each mask in Tab. 5 and judging the union mask of each cluster in Tab. 15. While simply judging the union mask shows better performance on $\\mathrm{COCO}{-}20^{\\mathrm{i}}$ and LVIS- $\\cdot92^{\\mathrm{i}}$ that require complete coverage of objects, its performance on One-shot Part Segmentation has a significant decline. Considering the generalizability of our approach, we select the Mask Growth algorithm as our strategy. ", "page_idx": 18}, {"type": "text", "text": "A.8 Additional Qualitative Analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We conduct additional qualitative analysis to better present the result of our approach. Fig. 11 further compare the Matcher, Baseline, $\\scriptstyle\\mathrm{B+PJ}$ , and $\\mathbf{B}\\mathbf{+}\\mathbf{P}\\mathbf{J}\\mathbf{+}\\mathbf{O}\\mathbf{J}$ (Ours) following Sec. 5.5. Fig. 12 illustrate the intermediate contents in the Post Gating. Moreover, we provide additional visualization results of standard FSS in Fig. 13, One-shot Part Segmentation in Fig. 14, and Cross Domain FSS in Fig. 15. These qualitative results demonstrate the effectiveness of our approach. Notably, some of the results are even better than the corresponding annotations. ", "page_idx": 18}, {"type": "image", "img_path": "jYypS5VIPj/tmp/3d9f4e597c4c3b8718f0de152848b25b2dd6034c41dd8b5540d92d2c8d527d52.jpg", "img_caption": ["Ref. Image ", "Figure 11: More Qualitative results on COCO-20ifor comparison among Matcher, Baseline, $\\scriptstyle\\mathrm{B+PG}$ , $\\scriptstyle\\mathrm{B+PG+OG}$ . "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "jYypS5VIPj/tmp/5bd6e9c31c9e4c4f43016a494ad18826148981d5b3cae14b9478ba4bfef67ceb.jpg", "img_caption": ["Figure 12: Qualitative analysis of the contents in gating. Different colors of points in the images in column \u201cClusters\" represent different clusters. The green points in images in columns \u201cPG\" and \u201cOG\" denote the points satisfying the gating criteria, while the red points denote those not satisfying. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "jYypS5VIPj/tmp/219b7761c482390755562f3a428aec5639d67bce8d15fb0d5eb8a053d77df565.jpg", "img_caption": ["Figure 13: Qualitative analysis of the results on Pascal- $\\cdot5^{\\mathrm{i}}$ , FSS-1000, and LVIS-92i. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "jYypS5VIPj/tmp/53b6fb3f353b7243ee6512e73a30a9d0637920a0b6acc99fe368f898b6e4fbf0.jpg", "img_caption": ["Figure 14: Qualitative analysis of the results on PASCAL-Part and PACO-Part. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "jYypS5VIPj/tmp/ac607433286542d175f20ede9da12cf588908ae7683c298820b5606a013d0902.jpg", "img_caption": ["Figure 15: Qualitative analysis of the results on Deepglobe and iSAID- $.5^{\\mathrm{i}}$ . "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our main claims made in the abstract and the last two paragraphs of the introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We discuss the limitations in the appendix. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: We do not introduce theory assumptions and proofs. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Our paper fully discloses all the information needed to reproduce the main experimental results of the paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our code and instructions are included in the supplementary material. The data we use for the experiments are all from open-access datasets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our paper specifies all the test details for our training-free approach in the section of experiments. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We set 5 random seeds to evaluate the large datasets and evaluate all samples of the small datasets in the appendix. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Computer resources are described in Implementation Details of the experiment section. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Our research conforms with the NeurIPS Code of Ethics in every respect. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We discuss both potential positive societal impacts and negative societal impacts of our work in the appendix. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our paper has no such risks. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The existing assets used in our paper, i.e., SAM and DINOv2, are released on GitHub under Apache License 2.0. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 28}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our new assets introduced in the paper are well documented in the supplementary material. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: There is no crowdsourcing and research with human subjects in our paper. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]