[{"figure_path": "jYypS5VIPj/figures/figures_1_1.jpg", "caption": "Figure 1: Performance comparisons of our approach against previous state-of-the-art methods regarding efficiency and generalized capabilities in Few-shot Semantic Segmentation. Figure 1(a) illustrates our approach's superior performance in efficiency and effectiveness across various model sizes. Figure 1(b) demonstrates the generalizability of our approach across different domains.", "description": "This figure presents a comparison of the proposed method's performance against other state-of-the-art methods in few-shot semantic segmentation.  Subfigure (a) shows a performance-efficiency comparison, highlighting the superior performance and efficiency of the proposed method across different model sizes. Subfigure (b) demonstrates the generalizability of the approach by showcasing its performance across various datasets.", "section": "1 Introduction"}, {"figure_path": "jYypS5VIPj/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of our approach, where the Positive-Negative Alignment module recognizes the correlation between target features and reference features for point selection, the Point-Mask Clustering module efficiently clusters the points based on the coverage of corresponding masks, and Post-Gating filters out the false-positive masks for generating final prediction.", "description": "This figure illustrates the workflow of the proposed approach for few-shot semantic segmentation. It starts by using a backbone network to extract features from both the reference and target images.  The Positive-Negative Alignment (PNA) module then leverages these features to select suitable point prompts for the Segment Anything Model (SAM), using both positive (foreground) and negative (background) context. The Point-Mask Clustering (PMC) module then groups the resulting masks from SAM based on their coverage of points, creating natural clusters. Finally, a Post-Gating mechanism refines the results, filtering out false positives to arrive at the final segmentation prediction.", "section": "4 Method"}, {"figure_path": "jYypS5VIPj/figures/figures_5_1.jpg", "caption": "Figure 3: Illustration of the Overshooting Gating strategy. The outer ring of points in the second image indicates the most similar cluster of corresponding points, i.e., points with different outside and inside colors do not satisfy the self-consistency.", "description": "This figure illustrates the Overshooting Gating strategy. The outer ring of points highlights the points that are most similar to their respective clusters. Points with different outside and inside colors do not meet the self-consistency criteria. This gating mechanism helps to filter out false positive masks that extend beyond their intended target regions.", "section": "4 Method"}, {"figure_path": "jYypS5VIPj/figures/figures_8_1.jpg", "caption": "Figure 11: More Qualitative results on COCO-20\u00b9 for comparison among Matcher, Baseline, B+PG, B+PG+OG.", "description": "This figure shows a qualitative comparison of the results obtained using different methods on the COCO-20 dataset.  The methods compared include Matcher (a state-of-the-art baseline method), a Baseline (the proposed method without the Positive and Overshooting Gating modules), B+PG (the proposed method with Positive Gating), and B+PG+OG (the full proposed method, including both Positive and Overshooting Gating). For each image, the reference image, target image, and the segmentation masks generated by each method are displayed.  The comparison visually demonstrates the improvements achieved by incorporating the Positive and Overshooting Gating modules into the proposed approach, particularly in terms of accuracy and robustness. The improved masks generated using the full method (B+PG+OG) show better alignment with the object boundaries and reduce the number of false positives compared to the other methods.", "section": "5.5 Qualitative Analysis"}, {"figure_path": "jYypS5VIPj/figures/figures_14_1.jpg", "caption": "Figure 2: Overview of our approach, where the Positive-Negative Alignment module recognizes the correlation between target features and reference features for point selection, the Point-Mask Clustering module efficiently clusters the points based on the coverage of corresponding masks, and Post-Gating filters out the false-positive masks for generating final prediction.", "description": "This figure illustrates the workflow of the proposed GF-SAM approach.  It starts with a backbone network that extracts features from both reference and target images. The Positive-Negative Alignment module then leverages these features to intelligently select point prompts, considering both positive (foreground) and negative (background) context.  These points are then fed to SAM to generate initial masks. The Point-Mask Clustering module groups masks based on their coverage over the points, creating distinct clusters. Finally, a Post-Gating module filters out low-confidence masks and merges the remaining masks to produce the final segmentation output.  The whole process is hyperparameter-free.", "section": "4 Method"}, {"figure_path": "jYypS5VIPj/figures/figures_15_1.jpg", "caption": "Figure 6: Analysis of the features from default ViT encoder of SAM.", "description": "This figure visualizes the features extracted from the default ViT encoder of SAM using the DINOv2 as the objective samples of Pascal-5i dataset. It demonstrates the self-similarity of FSAM, and that the features within each object region are nearly identical, while the features between neighboring different objects are distinct. Although FSAM can accurately identify the regions of objects, its coarse-grained features are not suitable for locating the objects well compared to Smean from DINOv2.", "section": "A.5 Discussion of SAM"}, {"figure_path": "jYypS5VIPj/figures/figures_15_2.jpg", "caption": "Figure 7: The distribution of IoU between masks from foreground points and from background points.", "description": "This figure shows the distribution of Intersection over Union (IoU) values between the masks generated from foreground points and those from background points.  The x-axis represents the IoU, and the y-axis shows the number of images and the cumulative percentage. The plot illustrates how well the masks generated from the foreground points cover the actual foreground, compared to the overlap with the background regions.  A higher concentration of images at higher IoU values indicates better performance in identifying and segmenting foreground objects.", "section": "A.6 Additional Experiment Results"}, {"figure_path": "jYypS5VIPj/figures/figures_16_1.jpg", "caption": "Figure 1: Performance comparisons of our approach against previous state-of-the-art methods regarding efficiency and generalized capabilities in Few-shot Semantic Segmentation. Figure 1(a) illustrates our approach's superior performance in efficiency and effectiveness across various model sizes. Figure 1(b) demonstrates the generalizability of our approach across different domains.", "description": "This figure compares the performance of the proposed approach with other state-of-the-art methods in few-shot semantic segmentation.  Subfigure (a) shows a performance-efficiency comparison, highlighting the superior efficiency and effectiveness of the proposed approach across different model sizes.  Subfigure (b) demonstrates the generalizability of the proposed approach across various datasets.", "section": "1 Introduction"}, {"figure_path": "jYypS5VIPj/figures/figures_16_2.jpg", "caption": "Figure 10: Results of our approach in multiple random seeds experiment. The bars in the chart represent the result under the previous standard evaluation. The error bar depicts the boundaries of our performance.", "description": "This figure compares the performance of the proposed method against previous state-of-the-art generalist and specialist models across various datasets using multiple random seeds. The bars show the average mIoU for each dataset, while the error bars indicate the variability of the results across different random seeds. This demonstrates the robustness of the proposed method.", "section": "5 Experimental Results"}, {"figure_path": "jYypS5VIPj/figures/figures_18_1.jpg", "caption": "Figure 1: Performance comparisons of our approach against previous state-of-the-art methods regarding efficiency and generalized capabilities in Few-shot Semantic Segmentation. Figure 1(a) illustrates our approach's superior performance in efficiency and effectiveness across various model sizes. Figure 1(b) demonstrates the generalizability of our approach across different domains.", "description": "This figure presents a comparison of the proposed method's performance against state-of-the-art approaches in few-shot semantic segmentation.  Subfigure (a) shows a performance-efficiency trade-off comparison, highlighting the proposed method's superior efficiency at various model sizes. Subfigure (b) demonstrates the generalizability of the proposed approach across multiple datasets.", "section": "1 Introduction"}, {"figure_path": "jYypS5VIPj/figures/figures_19_1.jpg", "caption": "Figure 11: More Qualitative results on COCO-20\u00b9 for comparison among Matcher, Baseline, B+PG, B+PG+OG.", "description": "This figure shows a qualitative comparison of the proposed method (B+PG+OG) against Matcher, Baseline (B), and Baseline+Positive Gating (B+PG) on the COCO-20 dataset.  Each row presents a reference image, a target image, and the segmentation masks generated by each method. The goal is to visually demonstrate the improvement in segmentation accuracy and the effectiveness of the proposed Positive Gating and Overshooting Gating modules in reducing false positives and refining mask boundaries.  The blue masks in the reference image shows the ground truth for that image.", "section": "5.5 Qualitative Analysis"}, {"figure_path": "jYypS5VIPj/figures/figures_20_1.jpg", "caption": "Figure 12: Qualitative analysis of the contents in gating. Different colors of points in the images in column \"Clusters\" represent different clusters. The green points in images in columns \"PG\" and \"OG\" denote the points satisfying the gating criteria, while the red points denote those not satisfying.", "description": "This figure provides a qualitative visualization of the intermediate steps within the proposed Positive-Negative Alignment (PNA) and Post-Gating modules.  It shows how the method dynamically selects point prompts by analyzing correlations between target and reference features, then clusters points and masks based on mask coverage.  The green and red points highlight whether points pass the positive and overshooting gating stages respectively, demonstrating the filtering of false positive masks.", "section": "4 Method"}, {"figure_path": "jYypS5VIPj/figures/figures_21_1.jpg", "caption": "Figure 13: Qualitative analysis of the results on Pascal-5\u00b9, FSS-1000, and LVIS-92.", "description": "This figure shows a qualitative comparison of the results obtained using the proposed method on three different datasets: Pascal-5, FSS-1000, and LVIS-92. For each dataset, it displays the reference image, the target image, and the segmentation result produced by the model. The qualitative analysis demonstrates the effectiveness of the proposed approach in handling various scenarios and datasets, illustrating its capacity to accurately and precisely segment objects across different domains and levels of complexity.", "section": "5 Experimental Results"}, {"figure_path": "jYypS5VIPj/figures/figures_22_1.jpg", "caption": "Figure 11: More Qualitative results on COCO-20\u00b9 for comparison among Matcher, Baseline, B+PG, B+PG+OG.", "description": "This figure provides a qualitative comparison of the results obtained using different methods on the COCO-20 dataset.  The methods compared are Matcher, Baseline, B+PG (Baseline + Positive Gating), and B+PG+OG (Baseline + Positive Gating + Overshooting Gating). Each row shows a reference image, a target image, and the segmentation masks produced by each of the four methods. The figure aims to visually demonstrate the effectiveness of the proposed approach (B+PG+OG) compared to the other methods.", "section": "5.5 Qualitative Analysis"}, {"figure_path": "jYypS5VIPj/figures/figures_23_1.jpg", "caption": "Figure 15: Qualitative analysis of the results on Deepglobe and iSAID-51.", "description": "This figure shows a qualitative comparison of the results obtained using the proposed method on two different datasets: Deepglobe and iSAID-5.  For each dataset, several example images are shown.  Each example includes three columns: the reference image, the target image, and the segmentation result generated by the model.  The qualitative analysis demonstrates the model's ability to accurately segment objects across different domains, including satellite imagery (Deepglobe) and aerial imagery (iSAID-5).", "section": "5 Experimental Results"}]