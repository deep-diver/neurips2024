{"references": [{"fullname_first_author": "J.-B. Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "publication_date": "2022-12-31", "reason": "This paper introduces Flamingo, a visual language model that serves as the foundation for the proposed Voila-A model architecture."}, {"fullname_first_author": "A. Awadalla", "paper_title": "Openflamingo: An open-source framework for training large autoregressive vision-language models", "publication_date": "2023-08-01", "reason": "This work provides the OpenFlamingo framework, a key component utilized for training and evaluating Voila-A, enhancing its accessibility and reproducibility."}, {"fullname_first_author": "J. Pont-Tuset", "paper_title": "Connecting vision and language with localized narratives", "publication_date": "2020-08-28", "reason": "This paper introduces Localized Narratives, a dataset that was utilized as a source for creating the Voila-COCO dataset, significantly contributing to data annotation efficiency."}, {"fullname_first_author": "Z. Peng", "paper_title": "Kosmos-2: Grounding multimodal large language models to the world", "publication_date": "2023-06-30", "reason": "This work introduces Kosmos-2, a comparative model against which Voila-A's performance was benchmarked and analyzed in the study."}, {"fullname_first_author": "B. Li", "paper_title": "Otter: A multi-modal model with in-context instruction tuning", "publication_date": "2023-05-31", "reason": "This paper introduces Otter, a pre-trained model that served as the foundational model upon which Voila-A was built, heavily influencing Voila-A's training process."}]}