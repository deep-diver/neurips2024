[{"figure_path": "dheDf5EpBT/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of our proposal vs. previous unlearning methods on erasing concept 'nudity' in diffusion models [11, 12]. Conventional methods seek the steepest descent within an Euclidean ball, often compromising general capabilities. In contrast, we reach the region around retraining along a remain-preserving manifold. To address the large cost of Hessian, we implicitly approximate the up-to-date salient unlearning direction.", "description": "This figure compares the proposed method with previous unlearning methods.  It illustrates how conventional methods, using Euclidean metrics, may compromise overall model capabilities during the unlearning process. The proposed method, in contrast, operates within a \"remain-preserving manifold,\" which implicitly approximates the Hessian, to more effectively remove unwanted information while preserving the model's performance on the retained data.", "section": "1 Introduction"}, {"figure_path": "dheDf5EpBT/figures/figures_7_1.jpg", "caption": "Figure 2: Image generations for class-wise forgetting tasks on CIFAR-10 using DDPM by baselines and our proposed SFR-on along with ablation variants. The forgetting class is 'cat', 'I' refers to the generated image sample from this class, and 'C' denotes the remaining class name. More results can be found in Appendix F.7.", "description": "This figure shows the image generation results of different methods on CIFAR-10 class-wise forgetting tasks using DDPM.  The top row shows images from the pre-trained model, while the second row shows images after retraining (RT) without the 'cat' class. The subsequent rows display the output of the different unlearning methods (SA, SalUn, and SFR-on with different ablation configurations). Each row represents a different method, and within each row, the images are grouped into 'Forgetting Class' ('cat') and 'Non-forgetting Classes' (other classes). The results demonstrate the effectiveness of the proposed SFR-on method in removing the target class ('cat') while preserving the quality of the other classes.", "section": "Experiments"}, {"figure_path": "dheDf5EpBT/figures/figures_8_1.jpg", "caption": "Figure 3: Class-wise forgetting of \u2018golden retriever\u2019 in image generations of ImageNet with DiT, comparing baselines and our proposed SFR-on. RT\u00b9 feeds autoencoder with random latent embeddings for the forgetting class, due to the computational constraints, rather than full RT. \u2018I\u2019 denotes image samples from forgetting, and \u2018C\u2019 refers to other remaining class name, e.g. \u2018Cacatua galerita\u2019 (C1). More results can be found in Appendix F.7.", "description": "This figure shows the results of class-wise forgetting experiments on the ImageNet dataset using the Diffusion Transformer (DiT) model.  The goal is to remove images of \u2018golden retrievers\u2019 while preserving the quality of other generated images. The figure compares different machine unlearning methods (Pretrain, RT\u00b9, SA, SalUn, and SFR-on). RT\u00b9 is a simulated retrained model due to computational constraints.  Each row represents a method, with the first two columns showing generated images of \u2018golden retrievers\u2019 (the forgetting class) and the following columns displaying images from other classes (non-forgetting classes). The figure highlights that the SFR-on method effectively removes \u2018golden retriever\u2019 images while better maintaining the quality of the other generated images.", "section": "Experiments"}, {"figure_path": "dheDf5EpBT/figures/figures_23_1.jpg", "caption": "Figure 1: Overview of our proposal vs. previous unlearning methods on erasing concept \u2018nudity\u2019 in diffusion models [11, 12]. Conventional methods seek the steepest descent within an Euclidean ball, often compromising general capabilities. In contrast, we reach the region around retraining along a remain-preserving manifold. To address the large cost of Hessian, we implicitly approximate the up-to-date salient unlearning direction.", "description": "This figure compares the proposed method with existing unlearning methods. The proposed method focuses on preserving the capabilities of the model during unlearning, while conventional methods may compromise the general capabilities of the model in order to achieve steepest descent in Euclidean space. The large cost associated with computing the Hessian is addressed by implicitly approximating the unlearning direction.", "section": "1 Introduction"}, {"figure_path": "dheDf5EpBT/figures/figures_23_2.jpg", "caption": "Figure 1: Overview of our proposal vs. previous unlearning methods on erasing concept 'nudity' in diffusion models [11, 12]. Conventional methods seek the steepest descent within an Euclidean ball, often compromising general capabilities. In contrast, we reach the region around retraining along a remain-preserving manifold. To address the large cost of Hessian, we implicitly approximate the up-to-date salient unlearning direction.", "description": "This figure compares the proposed method with existing unlearning methods. The focus is on removing the concept of 'nudity' from diffusion models.  Existing methods use Euclidean geometry and may compromise overall model capabilities. In contrast, the proposed approach leverages the remaining geometry (manifold) of the data to more efficiently and effectively unlearn the target concept while preserving the performance on other concepts.", "section": "1 Introduction"}, {"figure_path": "dheDf5EpBT/figures/figures_26_1.jpg", "caption": "Figure A1: Performance of SFR-on with different \u03bb in adaptive coefficients vs RT on CIFAR-10 using ResNet-18. The settings and metrics follow Tab. 2. The points closer to RT and with lower DKL are better.", "description": "This figure shows the performance of the proposed SFR-on method compared to the retraining baseline (RT) on the CIFAR-10 dataset using ResNet-18. The experiment focuses on the effect of the temperature scalar \u03bb, which controls the smoothness of the adaptive coefficients in the weighted forgetting gradient ascent part of the algorithm.  The plot shows four key metrics: Forgetting Accuracy (FA), Remaining Accuracy (RA), Testing Accuracy (TA), and Kullback-Leibler Divergence (DKL). Each metric is plotted against different values of \u03bb. The goal is to find a \u03bb value that minimizes the DKL (i.e., the difference between the model's output distribution and that of the retrained model), while simultaneously maintaining high FA, RA, and TA.  Points closer to the Retraining baseline and with lower DKL values are preferred as they signify a more effective unlearning process.", "section": "Appendix"}, {"figure_path": "dheDf5EpBT/figures/figures_27_1.jpg", "caption": "Figure 1: Overview of our proposal vs. previous unlearning methods on erasing concept \u2018nudity\u2019 in diffusion models [11, 12]. Conventional methods seek the steepest descent within an Euclidean ball, often compromising general capabilities. In contrast, we reach the region around retraining along a remain-preserving manifold. To address the large cost of Hessian, we implicitly approximate the up-to-date salient unlearning direction.", "description": "The figure illustrates the difference between the proposed method and previous unlearning methods.  Previous methods focus on finding the steepest descent in Euclidean space, potentially sacrificing overall model performance. The proposed method, however, aims for a remain-preserving manifold approach to achieve efficient unlearning while preserving capabilities. This is achieved via implicit online Hessian approximation.", "section": "1 Introduction"}, {"figure_path": "dheDf5EpBT/figures/figures_28_1.jpg", "caption": "Figure A4: More class-wise unlearning results on classifier-free guidance DDPM on CIFAR-10. The forgetting class is marked with a red color. (More results will be shown in Fig. A5 and Fig. A6)", "description": "This figure shows several examples of class-wise unlearning results using classifier-free guidance diffusion probabilistic models (DDPMs) on the CIFAR-10 dataset.  Each sub-figure represents a different class to be forgotten ('Airplane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck').  The images in the top and bottom rows represent successfully generated images *after* unlearning, while the central rows show the images of the target forgotten class.  The red color highlights the failed generation of the forgotten classes.", "section": "Additional Experimental Results"}, {"figure_path": "dheDf5EpBT/figures/figures_29_1.jpg", "caption": "Figure A4: More class-wise unlearning results on classifier-free guidance DDPM on CIFAR-10. The forgetting class is marked with a red color. (More results will be shown in Fig. A5 and Fig. A6)", "description": "This figure shows the results of class-wise forgetting experiments using classifier-free guidance diffusion probabilistic models on the CIFAR-10 dataset.  Each sub-figure presents the results for a different class being forgotten. The images generated by the model after unlearning are shown, with images of the forgotten class highlighted in red.  This illustrates the model's ability (or lack thereof) to effectively remove the specified class from its generation capabilities while maintaining the generation of other classes.", "section": "Additional Experimental Results"}, {"figure_path": "dheDf5EpBT/figures/figures_29_2.jpg", "caption": "Figure 3: Class-wise forgetting of 'golden retriever' in image generations of ImageNet with DiT, comparing baselines and our proposed SFR-on. RT\u00b9 feeds autoencoder with random latent embeddings for the forgetting class, due to the computational constraints, rather than full RT. 'I' denotes image samples from forgetting, and 'C' refers to other remaining class name, e.g. 'Cacatua galerita' (C1). More results can be found in Appendix F.7.", "description": "This figure shows the results of class-wise forgetting experiments on ImageNet using the Diffusion Transformer (DiT).  The goal is to remove the \"Golden Retriever\" class from the model's generated images.  The figure compares several baselines to the proposed SFR-on method.  Because training a fully retrained model (RT) on ImageNet is computationally expensive, the authors used a proxy for RT (RT\u00b9) that replaces the Golden Retriever class with random embeddings. The figure demonstrates that SFR-on is able to effectively remove the Golden Retriever class while maintaining the quality of images in other classes, performing better than baselines such as Saliency-based unlearning (SalUn) and Selective Amnesia (SA).", "section": "Experiments"}, {"figure_path": "dheDf5EpBT/figures/figures_30_1.jpg", "caption": "Figure 3: Class-wise forgetting of \u2018golden retriever\u2019 in image generations of ImageNet with DiT, comparing baselines and our proposed SFR-on. RT\u00b9 feeds autoencoder with random latent embeddings for the forgetting class, due to the computational constraints, rather than full RT. \u2018I\u2019 denotes image samples from forgetting, and \u2018C\u2019 refers to other remaining class name, e.g. \u2018Cacatua galerita\u2019 (C1). More results can be found in Appendix F.7.", "description": "This figure shows the results of a class-wise forgetting experiment on the ImageNet dataset using the Diffusion Transformer (DiT) model.  The goal is to remove the \u2018golden retriever\u2019 class from the model\u2019s output while preserving the quality of images generated for other classes.  The figure compares several baselines with the proposed SFR-on method. The results demonstrate that SFR-on effectively removes the target class ('golden retriever') while maintaining the generation quality for the other classes. Because a full retraining (RT) is computationally expensive, the researchers used a simulated RT (RT\u00b9) where random latent embeddings were used instead of retraining the model. The figure demonstrates the advantage of SFR-on over the baseline methods.", "section": "Experiments"}]