[{"figure_path": "2HvgvB4aWq/figures/figures_1_1.jpg", "caption": "Figure 1: (a) An example task graph encoding dependencies in a \u201cmix eggs\u201d procedure. (b) We learn a task graph which encodes a partial ordering between actions (left), represented as an adjacency matrix Z (center), from input action sequences (right). The proposed Task Graph Maximum Likelihood (TGML) loss directly supervises the entries of the adjacency matrix Z generating gradients to maximize the probability of edges from past nodes (K3, K\u2081) to the current node (K2), while minimizing the probability of edges from past nodes to future nodes (K4, K5) in a contrastive manner.", "description": "This figure illustrates the task graph learning process. (a) shows an example task graph representing a simple procedure. (b) details how the proposed Task Graph Maximum Likelihood (TGML) loss function learns the task graph from action sequences. The TGML loss function optimizes the weights of the adjacency matrix to accurately reflect the dependencies between actions by maximizing the likelihood of edges from previous actions to the current one and minimizing the likelihood of edges connecting previous actions to future ones.", "section": "3.1 Task Graph Maximum Likelihood Learning Framework"}, {"figure_path": "2HvgvB4aWq/figures/figures_4_1.jpg", "caption": "Figure 2: Given a sequence < S, A, B, D, C, E >, and a graph G with adjacency matrix Z, our goal is to estimate the likelihood P(< S, A, B, D, C, E > |Z), which can be done by factorizing the expression into simpler terms. The figure shows an example of computation of probability P(D|S, A, B, Z) as the ratio of the \u201cfeasibility of sampling key-step D, having observed key-steps S, A, and B", "description": "This figure illustrates how the likelihood of observing a sequence of key-steps given a task graph is computed. The likelihood is factorized into the probability of observing each key-step given the previously observed key-steps and the constraints imposed by the graph. For a weighted graph, the feasibility of sampling a given key-step is calculated as the sum of the weights of the edges from the previously observed key-steps to the given key-step. The probability of observing a key-step is then calculated as the ratio of its feasibility to the sum of the feasibilities of all unobserved key-steps.", "section": "3.1 Task Graph Maximum Likelihood Learning Framework"}, {"figure_path": "2HvgvB4aWq/figures/figures_5_1.jpg", "caption": "Figure 3: Our Task Graph Transformer (TGT) takes as input either D-dimensional text embeddings extracted from key-step names or video embeddings extracted from key-step segments. In both cases, we extract features with a pre-trained EgoVLPv2 model. For video embeddings, multiple embeddings can refer to the same action, so we randomly select one for each key-step (RS blocks). Learnable start (S) and end (E) embeddings are also included. Key-step embeddings are processed using a transformer encoder and regularized with a distinctiveness cross-entropy to prevent representation collapse. The output embeddings are processed by our relation head, which concatenates vectors across all (n + 2)2 possible node pairs, producing (n + 2) \u00d7 (n + 2) \u00d7 2D relation vectors. These vectors are then processed by a relation transformer, which progressively maps them to an (n+2) \u00d7 (n+2) adjacency matrix. The model is supervised with input sequences using our proposed Task Graph Maximum Likelihood (TGML) loss.", "description": "This figure illustrates the architecture of the Task Graph Transformer (TGT) model.  The model takes either text or video embeddings as input, processes them using a transformer encoder, and then uses a relation head and relation transformer to predict the adjacency matrix of a task graph.  A distinctiveness loss prevents overfitting, and the TGML loss ensures the predicted graph accurately reflects the input sequences.", "section": "3.2 Models"}, {"figure_path": "2HvgvB4aWq/figures/figures_9_1.jpg", "caption": "Figure 4: To further investigate the effect of noise, we conducted an analysis based on the controlled perturbation of ground truth action sequences, with the aim to simulate noise in the action detection process. At inference, we perturbed each key-step with a probability \u03b1 (the \"perturbation rate\"), with three kinds of perturbations: insert (inserting a new key-step with a random action class), delete (deleting a key-step), or replace (randomly changing the class of a key-step). The plots show the trend of the F1 score (Average, Correct, and Mistake) as the perturbation rate increases in the case of Assembly101-O (left) and EPIC-Tent-O (right). Results suggest that the proposed approach can still bring benefits even in the presence of imperfect action detections, with the average F1 score dropping down 10-15 points with a moderate noise level of 20%.", "description": "This figure shows the results of an experiment designed to test the robustness of the proposed online mistake detection method to noisy action sequences.  The authors simulated noise by randomly inserting, deleting, or replacing key-steps in ground truth sequences at varying rates (perturbation rate).  The resulting F1 scores (for average performance, correct predictions, and mistake predictions) are plotted against the perturbation rate for two datasets (Assembly101-O and EPIC-Tent-O). The results demonstrate that the method remains relatively effective even with a moderate level of noise.", "section": "4.2 Online Mistake Detection"}, {"figure_path": "2HvgvB4aWq/figures/figures_13_1.jpg", "caption": "Figure 5: Example of a task graph where each node represents a key-step in the procedure, with directed edges indicating the necessary preconditions for each step.", "description": "This figure shows a simple task graph representing the steps involved in a procedural task, such as making a dish. Each node (circle) represents a single step in the process, like \"Get a Bowl\", \"Add Water\", etc. The directed edges (arrows) show the dependencies between the steps, illustrating the order in which they must be performed. For example, \"Mix Eggs\" requires that you have already \"Add Water\", \"Add Milk\", and \"Crack Egg\". This visual representation makes it easy to understand the structure and prerequisites of a procedural activity.", "section": "A Task Graph"}, {"figure_path": "2HvgvB4aWq/figures/figures_19_1.jpg", "caption": "Figure 6: An example of transitive dependency between nodes. In (a) node A depends on B and C, but B depends on C, in this case, we can remove the edge between A and C for transitivity and we obtain the graph in (b).", "description": "This figure shows an example of how transitive dependencies in a graph can be simplified. In the original graph (a), node A depends on both nodes B and C, and node B depends on node C.  Since B's dependency on C implies that C must also be a precondition for A, the dependency between A and C is redundant. The simplified graph (b) removes this redundant edge, resulting in a more efficient representation of the relationships between nodes.", "section": "C.8 Graph Post-processing"}, {"figure_path": "2HvgvB4aWq/figures/figures_19_2.jpg", "caption": "Figure 4: To further investigate the effect of noise, we conducted an analysis based on the controlled perturbation of ground truth action sequences, with the aim to simulate noise in the action detection process. At inference, we perturbed each key-step with a probability \u03b1 (the \"perturbation rate\"), with three kinds of perturbations: insert (inserting a new key-step with a random action class), delete (deleting a key-step), or replace (randomly changing the class of a key-step). The plots show the trend of the F1 score (Average, Correct, and Mistake) as the perturbation rate increases in the case of Assembly101-O (left) and EPIC-Tent-O (right). Results suggest that the proposed approach can still bring benefits even in the presence of imperfect action detections, with the average F1 score dropping down 10-15 points with a moderate noise level of 20%.", "description": "This figure shows the effect of noise on the performance of the proposed online mistake detection method. The noise was simulated by introducing controlled perturbations (insertion, deletion, and replacement) to the ground truth action sequences at different rates. The plots show that the average F1 score decreases as the perturbation rate increases, indicating that the method is robust to noise but its performance degrades with significant noise level.", "section": "4.2 Online Mistake Detection"}, {"figure_path": "2HvgvB4aWq/figures/figures_19_3.jpg", "caption": "Figure 2: Given a sequence < S, A, B, D, C, E >, and a graph G with adjacency matrix Z, our goal is to estimate the likelihood P(< S, A, B, D, C, E > |Z), which can be done by factorizing the expression into simpler terms. The figure shows an example of computation of probability P(D|S, A, B, Z) as the ratio of the \u201cfeasibility of sampling key-step D, having observed key-steps S, A, and B", "description": "This figure illustrates how the likelihood of a sequence given a graph is computed.  The likelihood is factorized into simpler probabilities for each step, considering the feasibility of sampling a key-step given previously observed steps and the graph's constraints. The example focuses on calculating P(D|S, A, B, Z), showing the ratio of favorable cases (where D's preconditions are met) to possible cases.", "section": "3.1 Task Graph Maximum Likelihood Learning Framework"}, {"figure_path": "2HvgvB4aWq/figures/figures_20_1.jpg", "caption": "Figure 2: Given a sequence < S, A, B, D, C, E >, and a graph G with adjacency matrix Z, our goal is to estimate the likelihood P(< S, A, B, D, C, E > |Z), which can be done by factorizing the expression into simpler terms. The figure shows an example of computation of probability P(D|S, A, B, Z) as the ratio of the \u201cfeasibility of sampling key-step D, having observed key-steps S, A, and B", "description": "This figure illustrates how to compute likelihood of a sequence given a weighted graph by factorizing the expression into simpler terms. It gives an example of how probability of observing a key-step is estimated based on the feasibility of sampling that key-step given observed key-steps and the constraints of the graph. The feasibility is defined as the sum of all weights of edges between observed and current key-steps. The figure also shows how these probabilities are combined to estimate the likelihood of the whole sequence given the graph.", "section": "3.1 Task Graph Maximum Likelihood Learning Framework"}, {"figure_path": "2HvgvB4aWq/figures/figures_20_2.jpg", "caption": "Figure 2: Given a sequence < S, A, B, D, C, E >, and a graph G with adjacency matrix Z, our goal is to estimate the likelihood P(< S, A, B, D, C, E > |Z), which can be done by factorizing the expression into simpler terms. The figure shows an example of computation of probability P(D|S, A, B, Z) as the ratio of the \u201cfeasibility of sampling key-step D, having observed key-steps S, A, and B", "description": "This figure illustrates how the likelihood of a sequence given a task graph is calculated. It shows how the probability of observing a specific key-step at a given position in the sequence can be factorized into simpler terms based on previously observed key-steps and the constraints encoded in the task graph. The example illustrates the calculation of P(D|S, A, B, Z), the probability of observing key-step D given that S, A, and B have already been observed.", "section": "3.1 Task Graph Maximum Likelihood Learning Framework"}, {"figure_path": "2HvgvB4aWq/figures/figures_20_3.jpg", "caption": "Figure 2: Given a sequence < S, A, B, D, C, E >, and a graph G with adjacency matrix Z, our goal is to estimate the likelihood P(< S, A, B, D, C, E > |Z), which can be done by factorizing the expression into simpler terms. The figure shows an example of computation of probability P(D|S, A, B, Z) as the ratio of the \u201cfeasibility of sampling key-step D, having observed key-steps S, A, and B", "description": "This figure illustrates the process of calculating the probability of a sequence given a graph. It breaks down the calculation into simpler terms using factorization.  The example shows how the probability of observing key-step D, given that S, A, and B have already been observed, is calculated as a ratio of its feasibility score to the sum of feasibility scores for all unobserved key-steps.  This exemplifies the core idea behind the Task Graph Maximum Likelihood (TGML) loss function introduced in the paper.", "section": "3.1 Task Graph Maximum Likelihood Learning Framework"}, {"figure_path": "2HvgvB4aWq/figures/figures_20_4.jpg", "caption": "Figure 6: An example of transitive dependency between nodes. In (a) node A depends on B and C, but B depends on C, in this case, we can remove the edge between A and C for transitivity and we obtain the graph in (b).", "description": "This figure illustrates the concept of transitivity in directed acyclic graphs (DAGs) which are used to represent task graphs in the paper.  In the left panel (a), there is a redundancy where node A depends on both B and C, but B depends on C. The right panel (b) shows the simplified, equivalent DAG resulting from removing the redundant edge A-C, thus maintaining the correct precedence relationships.", "section": "C.8 Graph Post-processing"}, {"figure_path": "2HvgvB4aWq/figures/figures_21_1.jpg", "caption": "Figure 2: Given a sequence < S, A, B, D, C, E >, and a graph G with adjacency matrix Z, our goal is to estimate the likelihood P(< S, A, B, D, C, E >|Z), which can be done by factorizing the expression into simpler terms. The figure shows an example of computation of probability P(D|S, A, B, Z) as the ratio of the \u201cfeasibility of sampling key-step D, having observed key-steps S, A, and B", "description": "This figure illustrates how the likelihood of a sequence given a graph can be calculated by factorizing the probability into simpler terms. It gives an example that shows how to compute the probability of observing a key-step given previously observed key-steps and the constraints encoded in the graph. The feasibility of sampling a key-step is defined as the sum of all weights of edges between observed key-steps and the current key-step. This feasibility value is used to estimate the probability of the key-step appearing given the observed preconditions, which are represented in the graph.", "section": "3.1 Task Graph Maximum Likelihood Learning Framework"}, {"figure_path": "2HvgvB4aWq/figures/figures_21_2.jpg", "caption": "Figure 6: An example of transitive dependency between nodes. In (a) node A depends on B and C, but B depends on C, in this case, we can remove the edge between A and C for transitivity and we obtain the graph in (b).", "description": "This figure shows an example of how transitive dependencies are handled in the task graph.  In the left graph (a), node A depends on both B and C, while node B depends on C. This is redundant because if B is a precondition for A and C is a precondition for B, then C is implicitly a precondition for A.  The right graph (b) shows the simplified graph after removing the redundant dependency, making the graph more efficient and easier to understand.", "section": "3.2 Models"}, {"figure_path": "2HvgvB4aWq/figures/figures_21_3.jpg", "caption": "Figure 2: Given a sequence < S, A, B, D, C, E >, and a graph G with adjacency matrix Z, our goal is to estimate the likelihood P(< S, A, B, D, C, E > |Z), which can be done by factorizing the expression into simpler terms. The figure shows an example of computation of probability P(D|S, A, B, Z) as the ratio of the \u201cfeasibility of sampling key-step D, having observed key-steps S, A, and B", "description": "This figure illustrates how the likelihood of a sequence of key-steps given a task graph is calculated. It uses a factorization approach, breaking down the likelihood into simpler terms representing the probability of observing each key-step given the preceding ones and the constraints encoded in the graph's adjacency matrix. An example calculation of P(D|S, A, B, Z) is shown, highlighting the concept of 'feasibility' \u2013 the probability of sampling a key-step given its pre-conditions have been met.", "section": "3.1 Task Graph Maximum Likelihood Learning Framework"}, {"figure_path": "2HvgvB4aWq/figures/figures_22_1.jpg", "caption": "Figure 1: (a) An example task graph encoding dependencies in a \u201cmix eggs\u201d procedure. (b) We learn a task graph which encodes a partial ordering between actions (left), represented as an adjacency matrix Z (center), from input action sequences (right). The proposed Task Graph Maximum Likelihood (TGML) loss directly supervises the entries of the adjacency matrix Z generating gradients to maximize the probability of edges from past nodes (K3, K\u2081) to the current node (K2), while minimizing the probability of edges from past nodes to future nodes (K4, K5) in a contrastive manner.", "description": "This figure illustrates the core concept of the paper: learning task graphs from sequences of actions.  (a) shows a sample task graph representing the dependencies between actions in a simple \"mix eggs\" procedure. (b) details the proposed learning method, Task Graph Maximum Likelihood (TGML), which directly optimizes the weights of the task graph's adjacency matrix (Z) using a contrastive loss function. This loss encourages the model to learn accurate dependencies by emphasizing the likelihood of edges connecting preceding actions to subsequent ones while suppressing the likelihood of edges from past to future actions.", "section": "1 Introduction"}, {"figure_path": "2HvgvB4aWq/figures/figures_22_2.jpg", "caption": "Figure 3: Our Task Graph Transformer (TGT) takes as input either D-dimensional text embeddings extracted from key-step names or video embeddings extracted from key-step segments. In both cases, we extract features with a pre-trained EgoVLPv2 model. For video embeddings, multiple embeddings can refer to the same action, so we randomly select one for each key-step (RS blocks). Learnable start (S) and end (E) embeddings are also included. Key-step embeddings are processed using a transformer encoder and regularized with a distinctiveness cross-entropy to prevent representation collapse. The output embeddings are processed by our relation head, which concatenates vectors across all (n + 2)\u00b2 possible node pairs, producing (n + 2) \u00d7 (n + 2) \u00d7 2D relation vectors. These vectors are then processed by a relation transformer, which progressively maps them to an (n+2) \u00d7 (n+2) adjacency matrix. The model is supervised with input sequences using our proposed Task Graph Maximum Likelihood (TGML) loss.", "description": "The figure illustrates the architecture of the Task Graph Transformer (TGT) model. This model takes as input either text or video embeddings of key steps, processes them using a transformer encoder and a relation head, and outputs an adjacency matrix representing the task graph. The TGML loss is used to supervise the learning process.", "section": "3.2 Models"}, {"figure_path": "2HvgvB4aWq/figures/figures_22_3.jpg", "caption": "Figure 2: Given a sequence < S, A, B, D, C, E >, and a graph G with adjacency matrix Z, our goal is to estimate the likelihood P(< S, A, B, D, C, E >|Z), which can be done by factorizing the expression into simpler terms. The figure shows an example of computation of probability P(D|S, A, B, Z) as the ratio of the \u201cfeasibility of sampling key-step D, having observed key-steps S, A, and B", "description": "This figure illustrates how to estimate the likelihood of a sequence given a task graph represented as an adjacency matrix.  It breaks down the calculation into simpler terms by considering the probability of observing each key-step given the previously observed key-steps and the constraints encoded in the graph.  It uses the concept of 'feasibility' to quantify the likelihood of observing a specific key-step given its preconditions. The example shows how to compute the probability P(D|S, A, B, Z), representing the probability of observing key-step D after observing S, A, and B.", "section": "3.1 Task Graph Maximum Likelihood Learning Framework"}, {"figure_path": "2HvgvB4aWq/figures/figures_23_1.jpg", "caption": "Figure 6: An example of transitive dependency between nodes. In (a) node A depends on B and C, but B depends on C, in this case, we can remove the edge between A and C for transitivity and we obtain the graph in (b).", "description": "This figure shows an example of how transitive dependencies are handled in the task graph construction.  In graph (a), node A depends on both B and C, while B depends on C. Because the dependency of A on C is implied through the dependency of A on B and B on C (transitivity), the edge between A and C is redundant and can be removed, resulting in the simplified graph (b). This simplification maintains the accuracy of the task graph while reducing complexity.", "section": "3.2 Models"}, {"figure_path": "2HvgvB4aWq/figures/figures_23_2.jpg", "caption": "Figure 5: Example of a task graph where each node represents a key-step in the procedure, with directed edges indicating the necessary preconditions for each step.", "description": "This figure shows a simple example of a task graph.  A task graph is a directed acyclic graph (DAG) where each node represents a step in a procedure, and directed edges show the dependencies between steps. In other words, an edge from node A to node B indicates that step A must be completed before step B can begin.  This particular example illustrates a common workflow for making scrambled eggs.", "section": "A Task Graph"}, {"figure_path": "2HvgvB4aWq/figures/figures_23_3.jpg", "caption": "Figure 2: Given a sequence < S, A, B, D, C, E >, and a graph G with adjacency matrix Z, our goal is to estimate the likelihood P(< S, A, B, D, C, E > |Z), which can be done by factorizing the expression into simpler terms. The figure shows an example of computation of probability P(D|S, A, B, Z) as the ratio of the \u201cfeasibility of sampling key-step D, having observed key-steps S, A, and B", "description": "This figure illustrates how the likelihood of a sequence of key-steps given a graph is calculated. It shows how the probability of observing a key-step at a given position in a sequence depends on the previously observed key-steps and the constraints encoded in the graph's adjacency matrix. The figure breaks down the computation of the conditional probability P(y|Z) into simpler terms by showing an example of calculating the probability P(D|S, A, B, Z), which is interpreted as the ratio of the \"feasibility of sampling key-step D having observed S, A, and B\" to the sum of all feasibility scores for unobserved symbols.", "section": "3.1 Task Graph Maximum Likelihood Learning Framework"}, {"figure_path": "2HvgvB4aWq/figures/figures_24_1.jpg", "caption": "Figure 6: An example of transitive dependency between nodes. In (a) node A depends on B and C, but B depends on C, in this case, we can remove the edge between A and C for transitivity and we obtain the graph in (b).", "description": "This figure shows an example of how transitive dependencies can be simplified in a task graph.  In the first graph (a), node A depends on both B and C, while B depends on C. Since B's fulfillment automatically implies C's fulfillment for A, the connection between A and C is redundant. The second graph (b) shows the simplified graph after removing this redundant edge. This simplification ensures the resulting graph maintains a directed acyclic graph (DAG) structure, crucial for representing procedural activities in the proposed framework.", "section": "3.2 Models"}, {"figure_path": "2HvgvB4aWq/figures/figures_24_2.jpg", "caption": "Figure 2: Given a sequence < S, A, B, D, C, E >, and a graph G with adjacency matrix Z, our goal is to estimate the likelihood P(< S, A, B, D, C, E > |Z), which can be done by factorizing the expression into simpler terms. The figure shows an example of computation of probability P(D|S, A, B, Z) as the ratio of the \u201cfeasibility of sampling key-step D, having observed key-steps S, A, and B", "description": "This figure illustrates the calculation of likelihood for a given sequence in a weighted graph. The likelihood is factorized into simpler terms, and the probability of observing a key-step is estimated based on the feasibility of sampling that key-step given the observed key-steps and the graph structure. The feasibility score is computed by summing weights of edges from the key-step to all observed key-steps. The example shows how to compute the probability of observing key-step D given that S, A, and B have already been observed.", "section": "3.1 Task Graph Maximum Likelihood Learning Framework"}, {"figure_path": "2HvgvB4aWq/figures/figures_25_1.jpg", "caption": "Figure 2: Given a sequence < S, A, B, D, C, E >, and a graph G with adjacency matrix Z, our goal is to estimate the likelihood P(< S, A, B, D, C, E > |Z), which can be done by factorizing the expression into simpler terms. The figure shows an example of computation of probability P(D|S, A, B, Z) as the ratio of the \u201cfeasibility of sampling key-step D, having observed key-steps S, A, and B", "description": "This figure illustrates how to calculate the likelihood of a sequence given a graph, showing the factorization of the probability into simpler terms.  It also demonstrates how the probability of a specific key-step (D) is computed given previously observed key-steps (S, A, B) and the constraints encoded in the graph's adjacency matrix (Z).  The \"feasibility\" calculation represents the sum of edge weights from observed nodes to the current node under consideration.", "section": "3.1 Task Graph Maximum Likelihood Learning Framework"}, {"figure_path": "2HvgvB4aWq/figures/figures_25_2.jpg", "caption": "Figure 3: Our Task Graph Transformer (TGT) takes as input either D-dimensional text embeddings extracted from key-step names or video embeddings extracted from key-step segments. In both cases, we extract features with a pre-trained EgoVLPv2 model. For video embeddings, multiple embeddings can refer to the same action, so we randomly select one for each key-step (RS blocks). Learnable start (S) and end (E) embeddings are also included. Key-step embeddings are processed using a transformer encoder and regularized with a distinctiveness cross-entropy to prevent representation collapse. The output embeddings are processed by our relation head, which concatenates vectors across all (n + 2)\u00b2 possible node pairs, producing (n + 2) \u00d7 (n + 2) \u00d7 2D relation vectors. These vectors are then processed by a relation transformer, which progressively maps them to an (n+2) \u00d7 (n+2) adjacency matrix. The model is supervised with input sequences using our proposed Task Graph Maximum Likelihood (TGML) loss.", "description": "This figure illustrates the architecture of the Task Graph Transformer (TGT) model, which takes either text or video embeddings as input and predicts a task graph represented as an adjacency matrix. It uses a transformer encoder, a relation head, and a relation transformer to process the embeddings and predict the adjacency matrix. The model is trained with the proposed TGML loss function.", "section": "3.2 Models"}, {"figure_path": "2HvgvB4aWq/figures/figures_25_3.jpg", "caption": "Figure 6: An example of transitive dependency between nodes. In (a) node A depends on B and C, but B depends on C, in this case, we can remove the edge between A and C for transitivity and we obtain the graph in (b).", "description": "This figure demonstrates a graph simplification process that handles transitive dependencies.  The original graph (a) shows node A depending on both nodes B and C, while node B in turn depends on node C. Because B's existence implies C's, the direct dependency between A and C is redundant. The simplified graph (b) removes this unnecessary edge, resulting in a more efficient and accurate representation of the relationships.", "section": "3.1 Task Graph Maximum Likelihood Learning Framework"}, {"figure_path": "2HvgvB4aWq/figures/figures_26_1.jpg", "caption": "Figure 6: An example of transitive dependency between nodes. In (a) node A depends on B and C, but B depends on C, in this case, we can remove the edge between A and C for transitivity and we obtain the graph in (b).", "description": "This figure shows an example of how transitive dependencies can be simplified in a directed acyclic graph (DAG). The figure depicts two graphs: (a) shows a scenario where node A depends on both nodes B and C, and node B depends on node C; (b) shows the simplified graph after removing the redundant edge between A and C because the dependency is already implied by the path A -> B -> C. This simplification is done to ensure that the resulting graph remains a DAG.", "section": "3.1 Task Graph Maximum Likelihood Learning Framework"}, {"figure_path": "2HvgvB4aWq/figures/figures_26_2.jpg", "caption": "Figure 2: Given a sequence < S, A, B, D, C, E >, and a graph G with adjacency matrix Z, our goal is to estimate the likelihood P(< S, A, B, D, C, E > |Z), which can be done by factorizing the expression into simpler terms. The figure shows an example of computation of probability P(D|S, A, B, Z) as the ratio of the \u201cfeasibility of sampling key-step D, having observed key-steps S, A, and B", "description": "This figure illustrates how the likelihood of a sequence given a graph is computed. It breaks down the calculation into smaller, more manageable parts and shows how the probability of each step is influenced by the previous steps and the structure of the graph.  The figure highlights the concept of 'feasibility' in determining the probability of observing a specific key-step at a given point in the sequence. It showcases a practical example to clarify the process.", "section": "3.1 Task Graph Maximum Likelihood Learning Framework"}, {"figure_path": "2HvgvB4aWq/figures/figures_27_1.jpg", "caption": "Figure 6: An example of transitive dependency between nodes. In (a) node A depends on B and C, but B depends on C, in this case, we can remove the edge between A and C for transitivity and we obtain the graph in (b).", "description": "This figure shows an example of how to simplify a task graph by removing redundant edges.  In the first graph (a), node A depends on both nodes B and C, and node B depends on node C. Because of the transitive nature of dependencies, the link between A and C is unnecessary. The simplified graph (b) removes this redundancy.", "section": "C.8 Graph Post-processing"}, {"figure_path": "2HvgvB4aWq/figures/figures_27_2.jpg", "caption": "Figure 6: An example of transitive dependency between nodes. In (a) node A depends on B and C, but B depends on C, in this case, we can remove the edge between A and C for transitivity and we obtain the graph in (b).", "description": "This figure shows an example of how transitive dependencies between nodes in a graph can be simplified. In the first graph (a), node A depends on both nodes B and C, while node B depends on node C.  Since the dependency is transitive (B is a precondition for A, and C is a precondition for B, therefore C is also a precondition for A), the edge between A and C is redundant. Removing this redundant edge simplifies the graph to graph (b). This process of removing redundant edges helps to ensure that the graph is a directed acyclic graph (DAG), representing the partial ordering of tasks in a clear and concise manner.", "section": "C.8 Graph Post-processing"}, {"figure_path": "2HvgvB4aWq/figures/figures_28_1.jpg", "caption": "Figure 6: An example of transitive dependency between nodes. In (a) node A depends on B and C, but B depends on C, in this case, we can remove the edge between A and C for transitivity and we obtain the graph in (b).", "description": "This figure shows an example of how transitive dependencies are handled in the task graph.  The graph in (a) shows node A depending on nodes B and C, while node B depends on node C.  Since the dependency of A on C is implied through the dependency of A on B and B on C, the edge between A and C is redundant and can be removed to simplify the graph, resulting in the graph shown in (b). This process ensures that the task graph remains a directed acyclic graph (DAG), maintaining the correctness of the partial ordering of key-steps.", "section": "3.1 Task Graph Maximum Likelihood Learning Framework"}]