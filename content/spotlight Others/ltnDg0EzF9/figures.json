[{"figure_path": "ltnDg0EzF9/figures/figures_0_1.jpg", "caption": "Figure 1: We describe a purely data-driven image relighting model. Our model recovers latent variables representing scene intrinsic properties from one image, latent variables representing lighting from another, then applies the lighting to the intrinsics to produce a relighted scene (top row). There is no physical model of intrinsics, extrinsics or their interaction. Our model relights images of real scenes with SOTA accuracy and is more accurate than current supervised methods. Note how, for the chrome ball detail in top center, the specular reflections on the chrome ball (which give an approximate environment map) change when the extrinsics are changed. Note how our model ascribes lighting to visible luminaires when it can (top right), despite the absence of any physical model. A physical model accounts only for effects in that model, and most physical models of surfaces are approximate; in contrast, a latent intrinsic model accounts for whatever produces substantial effects in training data. Latent intrinsics yield albedo in a natural fashion (light the scene with an appropriate illuminant). Bottom row shows SOTA albedo estimates recovered from our latent intrinsics.", "description": "This figure demonstrates the relighting capabilities of the proposed model. The top row shows the input image, the reference image (with different lighting), and the relighted image generated by the model.  The bottom row displays the albedo (surface color) estimated by the model. The figure highlights the model's ability to accurately relight real-world scenes without relying on a physical model of scene properties or light interactions.", "section": "Introduction"}, {"figure_path": "ltnDg0EzF9/figures/figures_3_1.jpg", "caption": "Figure 2: The network diagram of our relighting model. The model functions as an autoencoder, comprising an encoder E and a decoder D. Left Half: The encoder E maps input image I, captured under scene s and lighting l, to low-dimensional extrinsic features L' and set of intrinsic features map {S}i. The decoder D then generates new images based on these intrinsic and extrinsic representations. Right Half: We employ constrained scaling for the injection of L', utilizing 0 < a < 1 to regularize the information passed from L', thereby enforcing a low-dimensional parameterization of the extrinsic features. We train our system to relight target images given input paired with images captured under the same scene s. During inference, our model demonstrates the ability to generalize to arbitrary reference images for relighting and can estimate albedo for free.", "description": "This figure illustrates the architecture of the proposed relighting model, which is an autoencoder.  The left half shows how the encoder processes input images to extract both low-dimensional extrinsic (lighting) and intrinsic (scene) features. The right half demonstrates the decoder's function, particularly the constrained scaling technique that limits the influence of extrinsic features, allowing for both relighting and albedo estimation during inference.", "section": "3 Learning Latent Intrinsic from Relighting"}, {"figure_path": "ltnDg0EzF9/figures/figures_5_1.jpg", "caption": "Figure 3: Our method outperforms all other approaches in estimating light and rendering the scene. The Unsupervised SA-AE [22] method fails by incorporating intrinsic elements from reference images. The S3Net [51] approach struggles with rendering when using unpaired reference images. Right: A zoomed-in view of the chrome ball was used as a probe to evaluate detail preservation in the environment map. Our method effectively retains the intricate room layout and accurately renders the appropriate lighting effects.", "description": "This figure compares the results of the proposed relighting model with other state-of-the-art methods.  It shows that the proposed model is superior in accurately rendering scenes under different lighting conditions, especially when using unpaired reference images. A close-up view of a chrome ball highlights the model's ability to accurately preserve fine details and specular reflections.", "section": "4.2 Evaluating image relighting"}, {"figure_path": "ltnDg0EzF9/figures/figures_6_1.jpg", "caption": "Figure 4: Latent extrinsics can be interpolated successfully; leftmost and rightmost columns are images from the multi-illumination dataset, and intermediate images are obtained by linear interpolation on the latent extrinsics (light-dependent representations), then decoding. Note how the light seems to \"move\" across space.", "description": "This figure demonstrates the ability of the model to smoothly interpolate between different lighting conditions.  The leftmost and rightmost images are from the multi-illumination dataset, representing real-world scenes under different lighting. The intermediate images are generated by linearly interpolating the latent extrinsic representations (which capture the lighting information) and then decoding the result.  The visualization showcases the model's ability to generate realistic and continuous changes in lighting.", "section": "4 Experiments"}, {"figure_path": "ltnDg0EzF9/figures/figures_7_1.jpg", "caption": "Figure 3: Our method outperforms all other approaches in estimating light and rendering the scene. The Unsupervised SA-AE [22] method fails by incorporating intrinsic elements from reference images. The S3Net [51] approach struggles with rendering when using unpaired reference images. Right: A zoomed-in view of the chrome ball was used as a probe to evaluate detail preservation in the environment map. Our method effectively retains the intricate room layout and accurately renders the appropriate lighting effects.", "description": "This figure compares the performance of the proposed method against other state-of-the-art relighting methods.  It showcases the superior quality of relighted images produced by the proposed model, particularly highlighting its ability to accurately render scenes with intricate details like reflections on a chrome ball and maintain overall scene consistency.  The right half zooms in on these details for a closer comparison.", "section": "4.2 Evaluating image relighting"}, {"figure_path": "ltnDg0EzF9/figures/figures_7_2.jpg", "caption": "Figure 3: Our method outperforms all other approaches in estimating light and rendering the scene. The Unsupervised SA-AE [22] method fails by incorporating intrinsic elements from reference images. The S3Net [51] approach struggles with rendering when using unpaired reference images. Right: A zoomed-in view of the chrome ball was used as a probe to evaluate detail preservation in the environment map. Our method effectively retains the intricate room layout and accurately renders the appropriate lighting effects.", "description": "This figure shows a comparison of the proposed relighting model against other state-of-the-art methods.  The top row shows input images, reference images, and results from the proposed method and other methods. The bottom row focuses on a detailed comparison, zooming in on the specular highlights of a chrome ball to illustrate the model's superior accuracy in detail preservation. The caption highlights the strengths of the proposed method over existing techniques and points out the weaknesses of the compared models.", "section": "4.2 Evaluating image relighting"}, {"figure_path": "ltnDg0EzF9/figures/figures_8_1.jpg", "caption": "Figure 7: Qualitative Comparison of Emergent Albedo from Latent Intrinsics on the IIW Dataset. Although our model has never been trained on any albedo-like maps, it effectively removes the effects of external light and dark shadows from the input. In contrast, Intrinsic Diffusion [29], a supervised method trained on large computer graphics data, often produces color-drifted estimations, likely due to the domain shift between CG data and real images. Observe the subdued lighting around the mirrors (top row, right) in our recovered albedo. Also, pay attention to all the details inside the refrigerator, which are visible in our recovered albedos (bottom row; right) compared to intrinsic diffusion. For comparison, we also display naive flattening (in the second column), which by itself cannot effectively reduce the strong lighting effects.", "description": "This figure compares the albedo estimations from the proposed model and a supervised method (Intrinsic Diffusion) on the IIW dataset. The proposed method effectively removes lighting effects and produces color-consistent results, while the supervised method shows color drift due to domain shift. Naive flattening is also compared, which shows its ineffectiveness in reducing lighting effects.", "section": "4.3 Zero-shot albedo evaluation"}, {"figure_path": "ltnDg0EzF9/figures/figures_9_1.jpg", "caption": "Figure 3: Our method outperforms all other approaches in estimating light and rendering the scene. The Unsupervised SA-AE [22] method fails by incorporating intrinsic elements from reference images. The S3Net [51] approach struggles with rendering when using unpaired reference images. Right: A zoomed-in view of the chrome ball was used as a probe to evaluate detail preservation in the environment map. Our method effectively retains the intricate room layout and accurately renders the appropriate lighting effects.", "description": "This figure compares the performance of the proposed relighting model against other state-of-the-art methods.  It shows examples of relighting results on various scenes with different lighting conditions.  The results highlight the superior performance of the proposed model in accurately estimating and rendering lighting and scene details, even when using unpaired reference images. A close-up of a chrome ball in one of the scenes demonstrates the superior detail preservation capability of the method.", "section": "4.2 Evaluating image relighting"}, {"figure_path": "ltnDg0EzF9/figures/figures_14_1.jpg", "caption": "Figure 3: Our method outperforms all other approaches in estimating light and rendering the scene. The Unsupervised SA-AE [22] method fails by incorporating intrinsic elements from reference images. The S3Net [51] approach struggles with rendering when using unpaired reference images. Right: A zoomed-in view of the chrome ball was used as a probe to evaluate detail preservation in the environment map. Our method effectively retains the intricate room layout and accurately renders the appropriate lighting effects.", "description": "This figure compares the image relighting results of the proposed method with other state-of-the-art methods.  It shows that the proposed method produces more accurate and realistic relighted images, especially when dealing with complex scenes and unpaired reference images. The close-up of the chrome ball highlights the superior detail preservation achieved by the proposed method.", "section": "4.2 Evaluating image relighting"}]