[{"type": "text", "text": "Skinned Motion Retargeting with Dense Geometric Interaction Perception ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zijie Ye1,2,\u2217 Jia-Wei Liu3, Jia Jia1,2,\u2020 Shikun Sun1,2, Mike Zheng Shou3 ", "page_idx": 0}, {"type": "text", "text": "1 Department of Computer Science and Technology, BNRist, Tsinghua University 2 Key Laboratory of Pervasive Computing, Ministry of Education 3 Show Lab, National University of Singapore ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Capturing and maintaining geometric interactions among different body parts is crucial for successful motion retargeting in skinned characters. Existing approaches often overlook body geometries or add a geometry correction stage after skeletal motion retargeting. This results in conflicts between skeleton interaction and geometry correction, leading to issues such as jittery, interpenetration, and contact mismatches. To address these challenges, we introduce a new retargeting framework, MeshRet, which directly models the dense geometric interactions in motion retargeting. Initially, we establish dense mesh correspondences between characters using semantically consistent sensors (SCS), effective across diverse mesh topologies. Subsequently, we develop a novel spatio-temporal representation called the dense mesh interaction (DMI) field. This field, a collection of interacting SCS feature vectors, skillfully captures both contact and non-contact interactions between body geometries. By aligning the DMI field during retargeting, MeshRet not only preserves motion semantics but also prevents self-interpenetration and ensures contact preservation. Extensive experiments on the public Mixamo dataset and our newly-collected ScanRet dataset demonstrate that MeshRet achieves state-of-the-art performance. Code available at https://github.com/abcyzj/MeshRet. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Skinned character animation is prevalent in virtual reality [16], game development [21], and various other fields. However, animating these characters often presents significant challenges due to differences in body proportions between the motion source and the target character, leading to issues such as loss of motion semantics, mesh interpenetration, and contact mismatches. Consequently, motion retargeting is essential to adjust for these discrepancies in body proportions. This process is crucial for maintaining the integrity of the source motion\u2019s characteristics in the animation of the target character. ", "page_idx": 0}, {"type": "text", "text": "Motion retargeting presents challenges due to the complex interactions among character limbs and the wide range of body geometries. Accurately preserving these interactions is crucial, as incorrect interactions can result in mesh interpenetration and contact mismatches. Prior research has typically addressed these interactions from two perspectives: skeleton interactions and geometry corrections. Early methods [1, 29, 15] employ cycle-consistency to implicitly align skeleton interaction semantics, yet they do not address the complexities of geometric interactions between different body parts. Villegas et al. [28] introduced mesh self-contact modeling; however, their approach does not extend to non-contact interactions. More recently, Zhang et al. [32] implemented a two-stage pipeline that first aligns skeleton interaction semantics and then corrects geometric artifacts. Nonetheless, the inherent confilct between preserving skeleton interaction semantics and correcting geometry leads to jittery movements, severe interpenetration and imprecise contacts. Zhang et al. [30] subsequently proposed adding a stage that aligns visual semantics with a visual language model, but this requires detailed pair-by-pair finetuning due to the loss of spatial information when projecting 3D motion into 2D images. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To resolve the conflict between skeleton interaction and geometry correction, we propose a new approach: focusing solely on dense geometric interaction for motion retargeting. Character animation videos, rendered from the skinned mesh, rely on geometric interactions to shape user perception. Skeleton interaction, in contrast, merely represents a simplified, sparse form of geometric interaction. Therefore, maintaining correct interactions between different body part geometries not only preserves motion semantics but also prevents mesh interpenetration and ensures contact preservation, as illustrated in Figure 1. ", "page_idx": 1}, {"type": "text", "text": "Given the significance of geometric interactions, we propose a new framework, named MeshRet, for skinned motion retargeting. In contrast to earlier methods that adjust skeletal motion retargeting outcomes, our approach models the intricate interactions among character meshes without depending on predefined vertex correspondences. ", "page_idx": 1}, {"type": "text", "text": "The design of MeshRet necessitates several technical innovations. Initially, there is a requirement for dense mesh correspondence across different characters. Drawing inspiration from the medial axis inverse transform (MAIT) [22], we have devised a technique, termed semantically consistent sensors (SCS), to automatically derive dense mesh correspondence from sparse skeleton correspondence. This technique enables us to sample a point cloud of sensors on the mesh to represent each character. Following this, to illustrate dense mesh interaction between body parts, we employ interacting mesh sensor pairs, maintaining generality. These pair-wise interactions are encoded within a novel spatialtemporal representation termed the Dense Mesh Interaction (DMI) field. The DMI field adeptly encapsulates both contact and non-contact interaction semantics. Finally, we proceed to learn a motion manifold that aligns with the target character geometry and the source motion DMI field. ", "page_idx": 1}, {"type": "text", "text": "To align our evaluation process more closely with real animation production, we gathered an inthe-wild motion dataset, termed ScanRet, characterized by abundant contact semantics and minimal mesh interpenetration. ScanRet consists of 100 human actors ranging from bulky to skinny, each performing 83 motion clips scrutinized by human animators. The MeshRet model is trained on both the ScanRet dataset and the widely used Mixamo [2] dataset. We assessed our method across a large variety of motions and a diverse array of target characters. Both qualitative and quantitative analyses show that our MeshRet model significantly outperforms existing methods. ", "page_idx": 1}, {"type": "text", "text": "To summarize, we present the following contributions: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce MeshRet, a pioneering solution that facilitates geometric interaction-aware motion retargeting across varied mesh topologies in a single pass.   \n\u2022 We present the SCS and the novel DMI field to guide the training of MeshRet, effectively encapsulating both contact and non-contact interaction semantics.   \n\u2022 We develop ScanRet, a novel dataset specifically tailored for assessing motion retargeting technologies, which includes detailed contact semantics and ensures smooth mesh interaction.   \n\u2022 Our experiments demonstrate that MeshRet delivers exceptional performance, marked by accurate contact preservation and high-quality motion. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Skeletal motion retargeting Motion retargeting seeks to preserve the characteristics of source motions when transferring them to a different target character. Skeletal motion retargeting primarily addresses the challenge of differing bone ratios. Gleicher [8] initially formulated motion retargeting as a spatio-temporal optimization problem, using source motion features as kinematic constraints. Subsequent researches [5, 7, 14] have focused on optimization-based approaches with various constraints. However, these methods, while requiring extensive optimization, often yield suboptimal results. Consequently, recent studies have explored learning-based motion retargeting algorithms. Jang et al. [11] trained a motion retargeting network using a U-Net [26] architecture on paired motion data. Villegas et al. [29] introduced a recurrent neural network combined with cycle-consistency [35] for unsupervised motion retargeting. Lim, Chang, and Choi [15] propose to learn frame-by-frame poses and overall movements separately. Aberman et al. [1] develop differentiable operators for cross-structural motion retargeting among homeomorphic skeletons. However, these methods generally neglect the geometry of characters, leading to frequent contact mismatches and severe mesh interpenetrations. ", "page_idx": 1}, {"type": "image", "img_path": "v1BIm8wESL/tmp/2af7d8b551aab355c8a6626488df99462c06e5c0df9542017638ab7574bf81c0.jpg", "img_caption": ["Figure 1: Comparison with the existing method. Contrary to the earlier retargeting-correction approach [32], which suffer from internal contradictions leading to interpenetration, jitter, and contact mismatches, our pipeline leverages the DMI field to accurately model complex geometric interactions. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Geometry-aware motion retargeting Previous studies have generally processed character geometries through two approaches: contact preservation and interpenetration avoidance. Lyard and Magnenat-Thalmann [19] developed a heuristic optimization algorithm to maintain character selfcontact, while Ho, Komura, and Tai [9] proposed to maintain character interactions by minimizing the deformation of interaction meshes. Ho and Shum [10] introduced a spatio-temporal optimization framework to prevent self-collisions in robot motion retargeting. Jin, Kim, and Lee [12] employed a proxy volumetric mesh to preserve spatial relationships during retargeting. Subsequently, Basset et al. [4] combined both attraction and repulsion terms in an optimization-based method to avoid interpenetration and preserve contact. However, these methods necessitate per-vertex correspondence and involve costly optimization processes. More recently, Villegas et al. [28] attempted to retarget skinned motion through optimization in a latent space of a pretrained network, although their method does not accommodate non-contact interactions. Zhang et al. [32] implemented a two-stage pipeline that initially aligns skeleton interaction semantics and subsequently corrects geometric artifacts. Nevertheless, the inherent confilct between maintaining skeleton interaction semantics and correcting geometry often results in jittery movements and imprecise contacts. In a later study, Zhang et al. [30] added a stage that aligns visual semantics using a visual language model, but this approach requires extensive pair-by-pair fine-tuning due to the loss of spatial information when projecting 3D motion into 2D images. ", "page_idx": 2}, {"type": "text", "text": "Existing geometry-aware motion retargeting methods either require expensive optimization or employ multi-stage strategies for skeleton and geometry semantics, resulting in a contradiction between stages that often leads to unsatisfactory results. In contrast, our method processes both contact and non-contact semantics using a dense mesh interaction field in a single stage. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We introduce a novel geometric interaction-aware motion retargeting framework MeshRet, as illustrated in Figure 2. Unlike previous methods that either overlook character geometries [1, 29, 15] or apply geometry correction after skeleton retargeting [32, 30], our framework directly addresses dense geometric interactions with the Dense Mesh Interaction (DMI) field. This provides a detailed representation of the interactions within skinned character motions, preserving motion semantics by preventing mesh interpenetration and ensuring precise contact preservation. ", "page_idx": 2}, {"type": "text", "text": "Motion & geometry representations Assume the motion sequence has $T$ frames and the character has $N$ skeletal joints. The motion sequence m is represented by the global root translation $\\mathbf{X}\\in\\mathbb{R}^{T\\times3}$ and the local joint rotation $\\mathbf{Q}\\in\\mathbb{R}^{T\\times N\\times6}$ , where we adopt the 6D representation [34] for the joint rotations. The rest-pose geometry $\\mathbf{G}$ of the character is represented by the rest-pose mesh $\\mathbf{O}$ and the rest-pose joint locations $\\mathbf{\\bar{J}}\\in\\mathbb{R}^{N\\times3}$ . ", "page_idx": 2}, {"type": "image", "img_path": "v1BIm8wESL/tmp/b32b7b7860bfafdd4b8a1b37f45928273655edd926ddc66f5503139da34976ff.jpg", "img_caption": ["Figure 2: Overview of the proposed MeshRet. The pipeline begins with the extraction of the DMI field using sensor forward kinematics, denoted as $\\mathcal{F}_{k}$ , and pairwise interaction feature selection, represented by $\\mathcal{F}_{c}$ . This DMI field, in conjunction with geometric features derived from ${\\mathcal{F}}_{g}$ , is fed into an encoder-decoder network. The network predicts the target motion sequence, which is aligned with the target character\u2019s geometry and the original DMI field. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Task definition Given the source motion sequence $\\mathbf{m_{A}}$ , and the geometries $\\mathbf{G}_{\\mathrm{A}}$ and $\\mathbf{G}_{\\mathrm{B}}$ of the source and target characters in their T-poses, our objective is to generate the motion $\\mathbf{m}_{\\mathrm{B}}$ for the target character. This process aims to retain essential aspects of the source motion, including its semantics, contact preservation, and the avoidance of interpenetration. ", "page_idx": 3}, {"type": "text", "text": "Following the definition of the task, our MeshRet model initially derives Semantically Consistent Sensors (SCS) $\\textbf{S}\\in\\mathbb{R}^{S\\times4\\times3}$ , which provide dense geometric correspondences essential for the retargeting process, where $\\mathbf{S}=\\mathcal{F}_{\\mathrm{s}}(\\mathbf{G})$ . S captures the sensor location and the sensor tangent space matrix, facilitating an enhanced perception of the geometry surface. Subsequently, we conduct sensor forward kinematics (FK) and pairwise interaction extraction to generate the source DMI field $\\mathbf{D}_{\\mathrm{A}}=\\mathcal{F}_{\\mathrm{d}}(\\mathbf{m}_{\\mathrm{A}},\\mathbf{S}_{\\mathrm{A}})$ , where $\\mathbf{D}_{\\mathrm{A}}\\in\\mathbb{R}^{\\dot{T}\\times K\\times L\\times P}$ . Here, $K$ is the number of SCS in the DMI field, $L$ represents a hyper-parameter of feature selection, and $P$ indicates the feature dimension of the DMI. Lastly, a transformer-based network [27] ingests $\\mathbf{m_{\\mathrm{A}}}$ , $\\mathbf{D}_{\\mathrm{A}}$ , $\\mathbf{S}_{\\mathrm{A}}$ , and $\\mathbf{S_{B}}$ , and predicts a target motion sequence $\\mathbf{m}_{\\mathrm{B}}$ that aligns with the target character\u2019s geometry and the source DMI field. The entire pipeline is denoted as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{m}_{\\mathrm{B}}=\\mathcal{F}_{\\mathrm{r}}(\\mathbf{m}_{\\mathrm{A}},\\mathbf{D}_{\\mathrm{A}},\\mathbf{S}_{\\mathrm{A}},\\mathbf{S}_{\\mathrm{B}})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.2 Semantically consistent sensors ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To facilitate dense geometric interactions, our MeshRet framework necessitates establishing dense mesh correspondence between source and target characters. Previous studies have typically derived correspondence from vertex coordinates [33], virtual sensor [31] or through a bounding mesh [12]; however, these methods are confined to template meshes sharing identical topology, such as MANO [25] or SMPL [18]. Villegas et al. [28] suggested determining vertex correspondence using nearest neighbor searches on predefined feature vectors. Nevertheless, this approach often lacks precision and brevity, resulting in inaccurate contact representations and substantial optimization burdens. ", "page_idx": 3}, {"type": "text", "text": "In this study, we introduce Semantically Consistent Sensors (SCS) that are effective across various mesh topologies while ensuring precise semantic correspondence. Our approach draws inspiration from the Medial Axis Inverse Transform (MAIT) [22]. We conceptualize the skeleton bones of each character as approximate medial axes of their limbs and torso. For each bone, a MAIT-like transform is applied to generate the corresponding SCS. This involves casting rays from the bone axis across a plane perpendicular to it. The origin parameter $l$ and direction parameter $\\phi$ of the rays, combined with the bone index $b$ , establish the semantic coordinates of the SCS. The semantic coordinates describe connection between the sensor and the skeleton bones. A sensor is deemed valid if its ray intersects the mesh linked to the bone; otherwise, it is considered invalid. Through this method, we establish a dense geometric correspondence based on sparse skeletal correspondence. The procedure for deriving SCS is illustrated in Figure 3. Given a unified set of SCS semantic coordinates $\\left\\{(b_{1},l_{1},\\phi_{1}),(b_{2},l_{2},\\bar{\\phi}_{2}),\\cdot\\cdot\\cdot,(b_{S},l_{S},\\phi_{S})\\right\\}$ , we can derive SCS feature $\\mathbf{S}\\,=\\,\\{\\mathbf{s}_{1},\\mathbf{s}_{2},\\cdots,\\mathbf{s}_{S}\\}$ for each character. Further details can be found in Algorithm 1. ", "page_idx": 3}, {"type": "image", "img_path": "v1BIm8wESL/tmp/06cf78d8817b575621d2a77814ed7374d2333a4881d22164bd483c459aa84a98.jpg", "img_caption": ["Figure 3: Left: Illustration of the method to derive a sensor feature s from the semantic coordinate $(b,l,\\phi)$ across different characters. The red line represents the projected ray. The feature s encompasses the sensor\u2019s location and its tangent space matrix. Right: The DMI field effectively captures both contact and non-contact interactions. Red lines represent $\\mathbf{d}^{t,i,j}$ in the DMI field. In the second example, the body sensors (yellow points) are located in the tangent plane of the hand sensors (blue points), signifying a contact interaction. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.3 Dense mesh interaction field ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To effectively represent the interactions between character limbs and the torso, we have developed the DMI field. Based on SCS detailed in Section 3.2, the DMI field comprehensively captures both contact and non-contact interactions across different body part geometries. Utilizing the DMI field allows for dense geometry interaction-aware motion retargeting, thereby eliminating the need for a geometry correction stage. ", "page_idx": 4}, {"type": "text", "text": "Sensor forward kinematics For a given motion sequence, denoted as $\\mathbf{m}$ , we initially conduct forward kinematics (FK) on $\\mathbf{S}$ to derive sensor features $\\mathbf{S}^{1:T}\\in\\mathbb{R}^{T\\times S\\times4\\times3}$ . Each $\\mathbf{S}^{t}$ encompasses the locations and tangent matrices for $S$ sensors at frame $t$ . The FK transformation for an individual sensor is expressed as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{s}_{i}^{t}=\\sum_{n=1}^{N}\\omega(\\mathbf{p}_{i})_{n}G_{n}(\\mathbf{Q}^{t})\\cdot\\mathbf{s}_{i},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $G_{n}(\\mathbf{Q}^{t})\\in S E(3)$ is the global transformation matrix for bone $n$ , derived from its local rotation matrix, and $\\omega(\\mathbf{p}_{i})_{n}$ represents the linear blend skinning (LBS) weight for sensor $\\mathbf{s}_{i}$ , determined through barycentric interpolation of its adjacent mesh vertices. ", "page_idx": 4}, {"type": "text", "text": "Pairwise interaction feature Next, we model the geometric interactions as pairwise interaction features between sensors. Ideally, for each frame, we obtain a comprehensive DMI field, $\\overline{{\\mathbf{D}}}^{t}$ , representing pairwise vectors across $K^{2}$ sensor pairs: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{d}^{t,i,j}=\\mathbf{t}_{i}^{-1}(\\mathbf{p}_{j}^{t}-\\mathbf{p}_{i}^{t}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\overline{{\\mathbf{D}}}^{t}=\\{(\\mathbf{d}^{t,i,j},b_{i},b_{j},l_{i},l_{j},\\phi_{i},\\phi_{j})\\}_{i=1:S}^{j=1:S},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{t}_{i}\\in\\mathbb{R}^{3\\times3}$ is the tangent matrix if sensor $i$ , and $\\mathbf{d}^{t,i,j}$ represents the relative position of target sensor $j$ in the tangent space of observation sensor $i$ . $\\overline{{\\mathbf{D}}}^{t}$ is composed of two components: the relative position of the sensor pair and the semantic coordinates of both the observation and target sensors. The use of semantic rather than spatial coordinates is essential, as it obviates the need for actual sensor positions, thereby making DMI suitable for motion retargeting applications. ", "page_idx": 5}, {"type": "text", "text": "However, $\\overline{{\\mathbf{p}}}^{t}\\in\\mathbb{R}^{S\\times S\\times P}$ exhibits quadratic growth with respect to $S$ because it includes $S^{2}$ sensor pairs, rendering it impractical when managing thousands of sensors. To address this, we implement two sparsification strategies for $\\overline{{\\mathbf{D}}}^{t}$ . Initially, we restrict interactions to critical body parts only, such as arm-torso, arm-head, arm-arm, and leg-leg, rather than between all sensor pairs, thereby restricting our focus to $K$ observation sensors. Subsequently, for each observation sensor, we select $L$ target sensors from each relevant body part, where $L$ is a predetermined hyper-parameter. Specifically, we empirically choose $L/2$ nearest and $L/2$ furthest target sensors. We find that proximate sensor pairs are crucial for minimizing interpenetration and maintaining contact, while distant pairs delineate the overall spatial relationships between body parts, as shown in Figure 3. These strategies lead to the formulation of the final DMI field $\\mathbf{D}\\in\\mathbb{R}^{\\mathbf{\\tilde{K}}\\times L\\times\\dot{P}}$ , with selected sensor pairs indicated by the sparse DMI mask $\\mathbf{M}_{\\mathrm{src}}\\in\\mathbb{R}^{S\\times S}$ shown in Figure 2. ", "page_idx": 5}, {"type": "text", "text": "3.4 Geometry interaction-aware motion retargeting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To avoid the conflict between skeleton interaction and geometric correction, the proposed MeshRet employs the DMI field to model geometric interactions directly. As shown in Figure 2, MeshRet initially extracts the DMI field $\\mathbf{D}_{\\mathrm{A}}$ from the source motion sequence $\\mathbf{m_{A}}$ , as described in Section 3.3. The field $\\mathbf{D}_{\\mathrm{A}}$ encapsulates interactions among various body parts within the source motion, encompassing both contact and non-contact interactions, further depicted in Figure 3. The DMI field, composed of sensor pair feature vectors, possesses the unordered characteristics of a point cloud. Consequently, we implement a PointNet-like architecture [24] for our DMI encoder, which is divided into two components: the per-sensor encoder and the per-frame encoder. Given DA \u2208RT \u00d7K\u00d7L\u00d7P , the per-sensor encoder initially processes it as $T*K$ separate point clouds, producing representations $\\mathbf{H}_{\\mathrm{A}}^{\\mathrm{s}}\\,^{'}\\in\\mathbb{R}^{T\\times K\\times D_{\\mathrm{model}}}$ for each observation sensor, where $D_{\\mathrm{model}}$ denotes the feature dimension. Subsequently, the per-frame encoder generates per-frame representations $\\mathbf{H}_{\\mathrm{A}}^{\\mathrm{f}}\\in\\mathbb{R}^{T\\times D_{\\mathrm{model}}}$ by encoding these $T$ point clouds. ", "page_idx": 5}, {"type": "text", "text": "Since DMI field $\\mathbf{D}_{\\mathrm{A}}$ lacks geometric information about characters, we introduced a geometry encoder ${\\mathcal{F}}_{g}$ to extract geometric features from their SCS. For each sensor, we form a feature vector by concatenating its rest-pose feature $\\mathbf{s}_{i}$ with its semantic coordinates $(b_{i},l_{i},\\phi_{i})$ . The resultant geometric features are represented as $\\mathbf{C}_{\\mathrm{A}}\\in\\mathbb{R}^{S_{\\mathrm{A}}\\times C}$ for character A and $\\mathbf{C}_{\\mathrm{B}}\\in\\mathbb{R}^{S_{\\mathrm{B}}\\times C}$ for character B. The semantic coordinates of sensors act as intermediaries linking the DMI field to character geometry. The geometry encoder employs a PointNet-like architecture [24] to transform the geometric features $\\mathbf{C}$ into a geometric latent code $\\mathbf{H^{\\mathrm{g}}}\\in\\mathbb{R}^{D_{\\mathrm{model}}}$ . ", "page_idx": 5}, {"type": "text", "text": "The transformer-based retargeting network processes input features including the source DMI feature $\\mathbf{H}_{\\mathrm{A}}^{\\mathrm{f}}$ , source joint rotation $\\mathbf{Q}_{\\mathrm{A}}$ , source geometry latent $\\mathbf{H}_{\\mathrm{A}}^{\\tilde{\\mathrm{g}}}$ , and target geometry latent $\\mathbf{H}_{\\mathrm{B}}^{\\mathrm{g}}$ . Specifically, the encoder processes $\\mathbf{H}_{\\mathrm{A}}^{\\mathrm{f}}$ and $\\mathbf{H}_{\\mathrm{B}}^{\\mathrm{g}}$ , while the decoder processes $\\mathbf{Q}_{\\mathrm{A}}$ and $\\mathbf{H}_{\\mathrm{A}}^{\\mathrm{g}}$ . The latents $\\mathbf{H}_{\\mathrm{A}}^{\\mathrm{g}}$ and $\\mathbf{H}_{\\mathrm{B}}^{\\mathrm{g}}$ serve as the initial tokens in the sequence, enabling both the encoder and decoder to operate over a sequence of length $T+1$ . The output sequence\u2019s final $T$ frames are represented as $\\hat{\\mathbf{Q}}_{\\mathrm{B}}$ . ", "page_idx": 5}, {"type": "text", "text": "Due to the lack of paired ground-truth data, we employ the unsupervised method described by Lim, Chang, and Choi [15]. Our network utilizes four loss functions for training: reconstruction loss, DMI consistency loss, adversarial loss, and end-effector loss. Supervision signals are derived from the source motion. We maintain geometric interactions by aligning the source DMI field $\\mathbf{D}_{\\mathrm{A}}$ with the target DMI field $\\hat{\\mathbf{D}}_{\\mathrm{B}}$ . The target DMI field $\\hat{\\mathbf{D}}_{\\mathrm{B}}$ is generated by first applying sensor forward kinematics to $\\hat{\\mathbf{Q}}_{\\mathrm{B}}$ , followed by selecting sensor pairs using the target sparse DMI mask $\\mathbf{M}_{\\mathrm{tgt}}\\in\\mathbb{R}^{S\\times S}$ . This mask, $\\mathbf{M}_{\\mathrm{tgt}}$ , is derived by excluding invalid sensors of the target character from $\\mathbf{M}_{\\mathrm{src}}$ . The DMI consistency loss is quantified as the cosine similarity loss between pair-wise relative positions in $\\hat{\\mathbf{D}}_{\\mathrm{B}}$ and $\\mathbf{D}_{\\mathrm{A}}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{dmi}}=\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{k=1}^{K}\\sum_{l=1}^{L}c(k,l)\\frac{\\mathbf{d}_{\\mathrm{A}}^{t,k,l}\\cdot\\hat{\\mathbf{d}}_{\\mathrm{B}}^{t,k,l}}{||\\mathbf{d}_{\\mathrm{A}}^{t,k,l}||_{2}\\cdot||\\hat{\\mathbf{d}}_{\\mathrm{B}}^{t,k,l}||_{2}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $c(k,l)$ takes the value 1 if sensor pair $(k,l)$ is valid in both $\\mathbf{M}_{\\mathrm{src}}$ and $\\mathbf{M}_{\\mathrm{tgt}}$ , and 0 otherwise. The reconstruction loss serves as a regularization mechanism to minimize motion alterations during retargeting, defined as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{rec}}=||\\hat{\\mathbf{Q}}_{\\mathrm{B}}-\\mathbf{Q}_{\\mathrm{A}}||_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "To facilitate realistic motion retargeting, a discriminator, denoted as $\\delta(\\cdot)$ , is employed. The adversarial loss is subsequently defined as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{adv}}=\\mathbb{E}_{\\mathbf{Q}\\sim p_{\\mathrm{real}}}[\\log\\delta(\\mathbf{Q})]+\\mathbb{E}_{\\mathbf{Q}\\sim p(\\hat{\\mathbf{Q}}_{\\mathrm{B}})}[\\log(1-\\delta(\\mathbf{Q}))].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We observed that the global orientation of end-effectors significantly influences user experience. Consequently, we introduced an end-effector loss to promote consistent orientations of end-effectors in the retargeted motion. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{ef}}=\\frac{1}{T|\\mathcal{X}|}\\sum_{t=1}^{T}\\sum_{i\\in\\mathcal{X}}||R(\\mathbf{Q}_{\\mathrm{A}}^{t},i)-R(\\hat{\\mathbf{Q}}_{\\mathrm{B}}^{t},i)||,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $R(\\cdot)$ transforms local joint rotations into global rotations for joint $i$ along the kinematic chain and $\\mathcal{X}$ represents the set of end-effectors. Our MeshRet is trained by: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{total}}=\\lambda_{\\mathrm{rec}}\\mathcal{L}_{\\mathrm{rec}}+\\lambda_{\\mathrm{dmi}}\\mathcal{L}_{\\mathrm{dmi}}+\\lambda_{\\mathrm{adv}}\\mathcal{L}_{\\mathrm{adv}}+\\lambda_{\\mathrm{ef}}\\mathcal{L}_{\\mathrm{ef}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets We trained and evaluated our method using the Mixamo dataset [2] and the newly curated ScanRet dataset. We downloaded 3,675 motion clips performed by 13 cartoon characters from the Mixamo dataset contains, while the ScanRet dataset consists of 8,298 clips executed by 100 human actors. Notably, the Mixamo dataset frequently features corrupted data due to interpenetration and contact mismatches. To overcome these issues, we created the ScanRet dataset, which provides detailed contact semantics and improved mesh interactions, with each clip being scrutinized by human animators. The training set comprises $90\\%$ of the motion clips from both datasets, involving nine characters from Mixamo and 90 from ScanRet. Our experiments tested the motion retargeting capabilities between cartoon characters and real humans, aligning closely with typical retargeting workflows. During inference, we adopted four data splits based on character and motion visibility: unseen character with unseen motion $(\\mathrm{UC}\\mathrm{+UM})$ , unseen character with seen motion $\\mathrm{(UC+SM)}$ ), seen character with unseen motion $(\\mathbf{S}\\mathbf{C}\\mathbf{+}\\mathbf{U}\\mathbf{M})$ , and seen character with seen motion $({\\mathrm{SC}}\\mathbf{+}{\\mathrm{SM}})$ , as delineated by Zhang et al. [32]. We present the average results across these splits. Additional details available in Appendix A. ", "page_idx": 6}, {"type": "text", "text": "Implementation details The hyper-parameters $\\lambda_{\\mathrm{rec}},\\,\\lambda_{\\mathrm{dmi}},\\,\\lambda_{\\mathrm{adv}},\\,\\lambda_{\\mathrm{ef}},$ and $L$ were empirically set to 1.0, 5.0, 1.0, 1.0, and 20, respectively. We use $\\{0,1,\\cdot\\cdot\\cdot,N_{\\mathrm{body}}\\mathrm{~-~}1\\}\\times\\{0,0.25,0.5,0.75\\}\\times$ $\\{0,0.5\\pi,\\pi,1.5\\pi\\}$ as the SCS semantic coordinates set, where $N_{\\mathrm{body}}=18$ is the number of body bones and $\\times$ represents the Cartesian product. We employed the Adam optimizer [13] with a learning rate of $10^{-4}$ to optimize our network. The training process required 36 epochs. For further details, please refer to Appendix C. ", "page_idx": 6}, {"type": "text", "text": "Evaluation metrics We assess the effectiveness of our method through three metrics: joint accuracy, contact preservation, and geometric interpenetration. Joint accuracy is quantified by calculating the Mean Squared Error (MSE) between the retargeted joint positions and the ground-truth data provided by animators in ScanRet. This analysis considers both global and local joint positions, normalized by the character heights. Contact preservation is evaluated by measuring the Contact Error, defined as the mean squared distance between sensors that were originally in contact in the source motion clip. Geometric interpenetration is determined by the ratio of penetrated limb vertices to the total limb vertices per frame. Further details are available in Appendix B. ", "page_idx": 6}, {"type": "text", "text": "4.2 Comparison with state-of-the-arts ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Qualitative results Figure 4 demonstrates the performance of skinned motion retargeting across characters with diverse body shapes, where the motion sequences are novel to the target characters during training. Most baseline methods, except $\\mathrm{R^{2}E T}$ [32], fail to consider the geometry of characters, leading to significant geometric interpenetration and contact mismatches. Unlike these methods, $\\mathrm{R^{2}E T}$ [32] includes a geometry correction phase after skeleton-aware retargeting. However, this creates a confilct between the two stages, resulting in oscillations in $\\mathrm{R^{2}}$ ET\u2019s outcomes, which manifest as alternating contact misses and severe interpenetrations, as shown in the first two rows. Additionally, these oscillations appear variably across different frames within the same motion clip, producing jittery motion, as illustrated in Figure 1 and Figure 8. A further limitation of $\\mathrm{R^{2}E T}$ is its neglect of hand contacts. In contrast, our method employs the innovative DMI field to preserve such detailed interactions, such as those observed in the \u201cPraying\u201d pose in the third row. Demo video available. ", "page_idx": 6}, {"type": "image", "img_path": "v1BIm8wESL/tmp/a3af7e73940132ff031bfd172fb9fffe2bed23daae3196b2237aa884898acac5.jpg", "img_caption": ["Figure 4: Qualitative comparison with baseline methods. Our method ensures precise contact preservation and minimal geometric interpenetration. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "v1BIm8wESL/tmp/a3fe214e32388c1d445d09859f8b07c57cd0ed08ae73de077cbe5b9c3372fc05.jpg", "table_caption": ["Table 1: Quantitative comparison between our method and state-of-the-arts. Mixamo $^+$ represents the mixed dataset of Mixamo and ScanRet. $\\boldsymbol{\\mathrm{MSE}^{l c}}$ denotes the local MSE. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Quantitative results Table 1 presents a comparison between our methods and state-of-the-arts. We initially measure the joint location error using MSE and $\\boldsymbol{\\mathbf{M}}\\boldsymbol{\\mathbf{S}}\\boldsymbol{\\mathbf{E}}^{l c}$ on ScanRet. The ground truth in ScanNet is established by human animators. Our observations indicate that human animators typically retarget motions by initially replicating joint rotations and subsequently modifying frames that display incorrect interactions. Conversely, our method modifies the entire motion sequence, resulting in a higher MSE compared to the Copy strategy. Nevertheless, MSE remains a valuable auxiliary reference. In comparison to PMnet [15], $\\mathrm{R^{2}E\\bar{T}}$ [32], and SAN [1], our method achieves MSE reductions of $65\\%$ , $29\\%$ , and $8\\%$ , respectively. These results demonstrate that our approach more closely aligns with the outputs produced by human animators. ", "page_idx": 7}, {"type": "image", "img_path": "v1BIm8wESL/tmp/25a7a4d85881d972148990694ed02f2b7fce05caa88984e72b3b25b9c833655f.jpg", "img_caption": ["Figure 5: Qualitative comparison of ablation studies. A red circle highlights areas of interpenetration, while a red rectangle identifies errors in non-contact semantics. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "As shown in Table 1, PMnet [15] and SAN [1], exhibit high interpenetration ratios and contact errors due to their neglect of character geometries. $\\mathrm{R^{2}E T}$ [32] effectively reduces interpenetration through a geometry correction stage; nonetheless, it still encounters high contact errors stemming from confilcts between the retargeting and correction stages. Our approach explicitly models geometry interactions and thereby achieves low contact error and penetration ratio, illustrating the effectiveness of our proposed MeshRet in generating high-quality retargeted motions with detailed contact semantics and smooth mesh interactions. Additionally, we observe that retargeting using the mixed Mixamo+ dataset is more challenging than with the ScanRet dataset, attributable to significant body shape variations between cartoon characters and real person characters. ", "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conducted ablation studies to demonstrate the significance of pairwise interaction feature selection and the implementation of DMI similarity loss. Initially, we evaluated the performance of a model trained exclusively with the nearest $L$ sensor pairs, denoted as $\\mathrm{Ours}_{c l s}$ , and another model trained solely with the farthest $L$ sensor pairs, referred to as ${\\mathrm{Ours}}_{f a r}$ . As indicated in Table 1 and Figure 5, $O\\mathrm{urs}_{f a r}$ compromises contact semantics and leads to significant interpenetration, while $\\mathrm{Ours}_{c l s}$ also exhibits inferior performance. This outcome suggests that proximal sensor pairs are essential for minimizing interpenetration and preserving contact, whereas distal pairs provide insights into the noncontact spatial relationships among body parts. Further, we investigated the effect of incorporating a distance matrix loss, as proposed by Zhang et al. [32], on our sensor pairs, designated as ${\\mathrm{Ours}}_{d m}$ . The results imply that the distance matrix loss fails to yield meaningful supervisory signals, likely because distance is non-directional and insufficient to discern the relative spatial positions among numerous sensors. ", "page_idx": 8}, {"type": "table", "img_path": "v1BIm8wESL/tmp/381db0041639b4e22c097425fcbbf4228079d21cfd33b2c52beb5bb687a7f137.jpg", "table_caption": ["Table 2: Human preferences between our method and baselines. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.4 User study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conducted a user study to assess the performance of our MeshRet model in comparison with the Copy strategy, PMnet [15], SAN [1], and $\\mathrm{R^{2}E T}$ [32]. Fifteen sets of motion videos were presented to participants, each consisting of one source skinned motion and five anonymized skinned results. Participants were requested to rate their preferences based on three criteria: semantic preservation, contact accuracy, and overall quality. Users were recruited from Amazon Mechanical Turk [3], resulting in a total of 600 comparative evaluations. As indicated in Table 2, approximately $81\\%$ of the comparisons favored our results. Details can be found in Appendix D ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduce a novel framework for geometric interaction-aware motion retargeting, named MeshRet. This framework explicitly models the dense geometric interactions among various body parts by first establishing a dense mesh correspondence between characters using semantically consistent sensors. We then develop a unique spatio-temporal representation, termed the DMI field, which adeptly captures both contact and non-contact interactions between body geometries. By aligning this DMI field, MeshRet achieves detailed contact preservation and seamless geometric interaction. Performance evaluations using the Mixamo dataset and our newly compiled ScanRet dataset confirm that MeshRet offers state-of-the-art results. ", "page_idx": 9}, {"type": "text", "text": "Limitations The primary limitation of MeshRet is its dependence on inputs with clean contact; motion clips exhibiting severe interpenetration yield poor outcomes. Consequently, it is unable to process noisy inputs effectively. Refer to Figure 12 and Figure 13 for failure cases under noisy inputs. Future efforts will focus on enhancing its robustness to noisy data. Additionally, SCS extraction can be compromised by noisy meshes, particularly those with complex clothing. A potential solution is to employ a Laplacian-smoothed proxy mesh for SCS extraction. Lastly, the method cannot handle characters with missing limbs. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by the National Key R&D Program of China under Grant No. 2024QY1400, the National Natural Science Foundation of China No. 62425604, and the Tsinghua University Initiative Scientific Research Program. Mike Shou does not receive any funding for this work. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Kfir Aberman et al. \u201cSkeleton-aware networks for deep motion retargeting\u201d. In: ACM Trans. Graph. 39.4 (2020), p. 62. [2] Adobe. Mixamo. https://www.mixamo.com/. 2018. [3] Amazon. Amazon Mechanical Turk. https://www.mturk.com/.   \n[4] Jean Basset et al. \u201cContact preserving shape transfer: Retargeting motion from one shape to another\u201d. In: Computers & Graphics 89 (2020), pp. 11\u201323.   \n[5] Antonin Bernardin et al. \u201cNormalized Euclidean distance matrices for human motion retargeting\u201d. In: Proceedings of the 10th International Conference on Motion in Games. 2017, pp. 1\u20136. [6] Yingruo Fan et al. \u201cFaceformer: Speech-driven 3d facial animation with transformers\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022, pp. 18770\u201318780. [7] Andrew Feng et al. \u201cAutomating the transfer of a generic set of behaviors onto a virtual character\u201d. In: Motion in Games: 5th International Conference, MIG 2012, Rennes, France, November 15-17, 2012. Proceedings 5. Springer. 2012, pp. 134\u2013145. [8] Michael Gleicher. \u201cRetargetting motion to new characters\u201d. In: Proceedings of the 25th annual conference on Computer graphics and interactive techniques. 1998, pp. 33\u201342.   \n[9] Edmond S. L. Ho, Taku Komura, and Chiew-Lan Tai. \u201cSpatial relationship preserving character motion adaptation\u201d. In: ACM Trans. Graph. 29.4 (2010), 33:1\u201333:8.   \n[10] Edmond SL Ho and Hubert PH Shum. \u201cMotion adaptation for humanoid robots in constrained environments\u201d. In: 2013 IEEE International Conference on Robotics and Automation. IEEE. 2013, pp. 3813\u20133818.   \n[11] Hanyoung Jang et al. \u201cA variational u-net for motion retargeting\u201d. In: SIGGRAPH Asia 2018 Posters. 2018, pp. 1\u20132.   \n[12] Taeil Jin, Meekyoung Kim, and Sung-Hee Lee. \u201cAura mesh: Motion retargeting to preserve the spatial relationships between skinned characters\u201d. In: Computer Graphics Forum. Vol. 37. 2. Wiley Online Library. 2018, pp. 311\u2013320.   \n[13] Diederik P. Kingma and Jimmy Ba. \u201cAdam: A Method for Stochastic Optimization\u201d. In: 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. Ed. by Yoshua Bengio and Yann LeCun. 2015.   \n[14] Jehee Lee and Sung Yong Shin. \u201cA hierarchical approach to interactive motion editing for human-like figures\u201d. In: Proceedings of the 26th annual conference on Computer graphics and interactive techniques. 1999, pp. 39\u201348.   \n[15] Jongin Lim, Hyung Jin Chang, and Jin Young Choi. \u201cPMnet: Learning of Disentangled Pose and Movement for Unsupervised Motion Retargeting.\u201d In: BMVC. Vol. 2. 6. 2019, p. 7.   \n[16] Jinghuai Lin and Marc Erich Latoschik. \u201cDigital body, identity and privacy in social virtual reality: A systematic review\u201d. In: Frontiers in Virtual Reality 3 (2022), p. 974652.   \n[17] Matthew Loper, Naureen Mahmood, and Michael J Black. \u201cMoSh: motion and shape capture from sparse markers.\u201d In: ACM Trans. Graph. 33.6 (2014), pp. 220\u20131.   \n[18] Matthew Loper et al. \u201cSMPL: a skinned multi-person linear model\u201d. In: ACM Trans. Graph. 34.6 (2015), 248:1\u2013248:16.   \n[19] Etienne Lyard and Nadia Magnenat-Thalmann. \u201cMotion adaptation based on character shape\u201d. In: Computer Animation and Virtual Worlds 19.3-4 (2008), pp. 189\u2013198.   \n[20] Naureen Mahmood et al. \u201cAMASS: Archive of motion capture as surface shapes\u201d. In: Proceedings of the IEEE/CVF international conference on computer vision. 2019, pp. 5442\u2013 5451.   \n[21] Lucas Mourot et al. \u201cA survey on deep learning for skeleton-based human animation\u201d. In: Computer Graphics Forum. Vol. 41. 1. Wiley Online Library. 2022, pp. 122\u2013157.   \n[22] Henning Na\u00df et al. \u201cMedial axis (inverse) transform in complete 3-dimensional Riemannian manifolds\u201d. In: 2007 International Conference on Cyberworlds (CW\u201907). IEEE. 2007, pp. 386\u2013 395.   \n[23] Adam Paszke et al. \u201cPytorch: An imperative style, high-performance deep learning library\u201d. In: Advances in neural information processing systems 32 (2019).   \n[24] Charles R Qi et al. \u201cPointnet: Deep learning on point sets for 3d classification and segmentation\u201d. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2017, pp. 652\u2013660.   \n[25] Javier Romero, Dimitrios Tzionas, and Michael J. Black. \u201cEmbodied hands: modeling and capturing hands and bodies together\u201d. In: ACM Trans. Graph. 36.6 (2017), 245:1\u2013245:17.   \n[26] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. \u201cU-net: Convolutional networks for biomedical image segmentation\u201d. In: Medical image computing and computer-assisted intervention\u2013MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18. Springer. 2015, pp. 234\u2013241.   \n[27] Ashish Vaswani et al. \u201cAttention is all you need\u201d. In: Advances in neural information processing systems 30 (2017).   \n[28] Ruben Villegas et al. \u201cContact-aware retargeting of skinned motion\u201d. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021, pp. 9720\u20139729.   \n[29] Ruben Villegas et al. \u201cNeural kinematic networks for unsupervised motion retargetting\u201d. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2018, pp. 8639\u20138648.   \n[30] Haodong Zhang et al. \u201cSemantics-aware Motion Retargeting with Vision-Language Models\u201d. In: arXiv preprint arXiv:2312.01964 (2023).   \n[31] He Zhang et al. \u201cManipNet: neural manipulation synthesis with a hand-object spatial representation\u201d. In: ACM Trans. Graph. 40.4 (2021), 121:1\u2013121:14.   \n[32] Jiaxu Zhang et al. \u201cSkinned Motion Retargeting with Residual Perception of Motion Semantics & Geometry\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 13864\u201313872.   \n[33] Keyang Zhou et al. \u201cToch: Spatio-temporal object-to-hand correspondence for motion refinement\u201d. In: European Conference on Computer Vision. Springer. 2022, pp. 1\u201319.   \n[34] Yi Zhou et al. \u201cOn the continuity of rotation representations in neural networks\u201d. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019, pp. 5745\u20135753.   \n[35] Jun-Yan Zhu et al. \u201cUnpaired image-to-image translation using cycle-consistent adversarial networks\u201d. In: Proceedings of the IEEE international conference on computer vision. 2017, pp. 2223\u20132232. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Dataset Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "ScanRet details The primary motivation for collecting the ScanRet dataset stemmed from two main concerns. First, the data quality in the Mixamo [2] dataset was relatively low, suffering from significant issues such as interpenetration and contact mismatch. Second, the Mixamo dataset exclusively contained cartoon characters, whose body type distributions differed markedly from those of real human motion capture actors. In response, we developed the ScanRet dataset. We recruited 100 participants, evenly split between males and females, representing common ranges of height and BMI. Each participant underwent a 3D scan to create a T-pose mesh. We intentionally did not collect texture information for the body or face to protect privacy. Subsequently, we used motion capture equipment to build a library of 83 actions characterized by extensive physical contact. We enlisted human animators to map each action onto the 100 T-pose meshes, ensuring both semantic integrity and correct physical contact were maintained. All participants and animators received fair compensation. After discarding some invalid data, we compiled a total of 8,298 motion data entries. The ScanRet dataset is designed to simulate data obtained from real human motion capture, such as the MoSh [20, 17] algorithm, thus enhancing the realism of our evaluation process in the context of actual animation production workflows. ", "page_idx": 12}, {"type": "image", "img_path": "v1BIm8wESL/tmp/5557ebeb945347380a0c01122b89412d4e0e1d7a5369afec39f69c26c9ff0583.jpg", "img_caption": ["Figure 6: Left: Characters of varying body types in the Mixamo dataset do not always maintain reasonable hand contact during clapping actions. Right: In our ScanRet dataset, characters of diverse body types consistently maintain appropriate hand contact while performing the same clapping actions. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "Data splits We collected motion data for 13 characters from the Mixamo website, totaling 3,675 motion sequences, with each character having approximately the same number of sequences. The characters are: Aj, Amy, Kaya, Mousey, Ortiz, Remy, Sporty Granny, Swat, The Boss, Timmy, X Bot, and Y Bot. Among them, Ortiz, Kaya, X Bot, and Amy were not encountered by the network during training. Overall, our training set included motion data for 9 Mixamo characters and 90 randomly selected characters from the ScanNet dataset, where $90\\%$ of the motion sequences was randomly chosen from both datasets. Details regarding the train/test split for specific motion sequences and characters are provided in the code. ", "page_idx": 12}, {"type": "text", "text": "B Evaluation metric details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We evaluate the performance of our method from three perspectives: joint accuracy, contact preservation, and geometric interpenetration. In terms of joint accuracy, we calculate the Mean Squared Error (MSE) between the ground-truth joint positions $X_{g t}$ and the retargeted joint positions X\u02c6, normalized by the character\u2019s height $h$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\nM S E=\\frac{1}{h}||X_{g t}-\\hat{X}||_{2}^{2}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Previous work [32] assessing the accuracy of self-contact measurements merely utilized the distance between hand vertices and the body surface to determine contact presence. Such experimental metrics fail to accurately reflect the precision of the contact location. Therefore, we adopted a metric similar to the vertex contact mean squared error (MSE) proposed by Villegas et al. [28], termed \u201cContact Error\u201d. Specifically, we first identified sensor pairs where the distance between hand and body sensors in the source action was less than the arm\u2019s diameter $d_{s r c}$ . We then located the same sensor pairs in the retargeted motion. If the distance between these sensor pairs in the retargeted motion exceeded that in the source action, we calculated the MSE of the distance differences; otherwise, the contact error was zero. The formula is as follows: ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Contact\\:Error}=\\Bigg\\{(||\\frac{\\mathbf{d}_{\\mathrm{A}}^{t,k,l}}{R_{\\mathrm{A}}}||_{2}-||\\frac{\\mathbf{d}_{\\mathrm{B}}^{t,k,l}}{R_{\\mathrm{B}}}||_{2})^{2},\\quad}&{\\mathrm{if}||\\frac{\\mathbf{d}_{\\mathrm{A}}^{t,k,l}}{R_{\\mathrm{A}}}||_{2}>||\\frac{\\mathbf{d}_{\\mathrm{B}}^{t,k,l}}{R_{\\mathrm{B}}}||_{2}}\\\\ &{\\quad\\mathrm{otherwise},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathbf{d}_{\\mathrm{A}}^{t,k,l}$ indicates the contact sensor pairs with $||\\mathbf{d}_{\\mathrm{A}}^{t,k,l}||_{2}<d_{s r c}$ , while $R_{\\mathrm{{A}}}$ and $R_{\\mathrm{B}}$ represent the radius of each character\u2019s arms. ", "page_idx": 13}, {"type": "text", "text": "For geometric interpenetration, we assess the percentage of interpenetration, calculated as the ratio of penetrated vertices to the total vertices per frame. A lower ratio signifies reduced interpenetration. In our evaluation, we calculate the interpenetration ratio between arms (including hands) and the body. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{{Penetration}=\\frac{\\Delta N u m b e r\\;\\mathrm{{of}\\;\\mathrm{{penetrated}\\;a r m\\;v e r t i c e s}}}{\\Delta T o t a l\\;\\mathrm{{number\\;of}\\;a r m\\;v e r t i c e s}}.}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "C Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "SCS details As introduced in Section 3.2, we establish semantic correspondences between character meshes with different topologies using semantically consistent sensors. Specifically, given the semantic coordinates $(b,l,\\phi)$ of a sensor, we can identify semantically consistent sensor positions on the meshes of different roles and obtain the feature vectors of the sensors. This process is detailed in Algorithm 1. ", "page_idx": 13}, {"type": "table", "img_path": "v1BIm8wESL/tmp/c6604962e7e35f8e5c77c0850cb3fb344b9cc0e4ead45ccd22851c7e2edc5a1d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Network architecture The network architectures of both our DMI Encoder and Geometry Encoder resemble the structure of PointNet. However, since all our data is inherently situated within the canonical space, we have eliminated the T-Net from PointNet to reduce network complexity. Before being input into the encoder, sensor features pass through a sensor group embedding layer, which converts the bone index $b$ into an 8-dimensional embedding vector. This embedding vector is updated during training. The Geometry Encoder consists of six PointNet layers with $D_{\\mathrm{model}}$ set at 256, and there is a distinct Geometry Encoder for the body, head, arms, and legs. The DMI Encoder comprises a per-sensor encoder and a per-frame encoder, each built with six PointNet layers, with each interaction pair having its own encoder. Specific interaction pairs include: [(Left Arm), (Right Arm, Head, Torso)], [(Right Arm), (Left Arm, Head, Torso)], [(Left Leg), (Right Leg, Torso)], and [(Right Leg), (Left Leg, Torso)]. The Motion Encoder is a multilayer perceptron (MLP). Both the ", "page_idx": 13}, {"type": "image", "img_path": "v1BIm8wESL/tmp/7b3ed4aad7088bdcc6761dc60f3fc9227e3a9a3a369df43fcaaea8dba16482f2.jpg", "img_caption": ["Figure 7: User interface presented to participants during the user study. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Transformer Encoder and Transformer Decoder have eight layers, with the number of heads set to four and the feed-forward size to 256. Between the Transformer Encoder and Transformer Decoder, we employ an alignment mask proposed by Fan et al. [6], which ensures that each frame feature in the decoder attends only to the corresponding DMI frame and initial token, thereby aligning the network\u2019s output motion sequence with the input features. ", "page_idx": 14}, {"type": "text", "text": "Training details We implemented our network using PyTorch [23], running on a machine equipped with an NVIDIA RTX A6000 GPU and an AMD EPYC 9654 CPU. The dataset was uniformly processed at a frame rate of 30 fps. During training, we randomly clipped a sequence of 30 frames from the dataset. The target character was set to be the same as the source character with a $50\\%$ probability, and different with a $50\\%$ probability, selected randomly from the dataset. On our system, training for 36 epochs required approximately 40 hours. During inference, our MeshRet model can achieve performance exceeding 30 fps. ", "page_idx": 14}, {"type": "text", "text": "D User study details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We recruited participants via the Amazon Mechanical Turk [3] platform to partake in a user study. As shown in Figure 7, during each session, subjects were presented with one source video and two retargeted motion videos: Video A and Video B. Participants were asked to watch all three videos and then compare Video A and Video B. At the conclusion of the viewing, they were requested to answer the following three questions: ", "page_idx": 14}, {"type": "text", "text": ". Which video better matches the source motion in terms of the overall meaning and intent of the motion? ", "page_idx": 14}, {"type": "text", "text": "2. Which video has more accurate and detailed motion? Look for less self-interpenetration and better self-contact precision. 3. Considering all factors, which video do you think is better overall? ", "page_idx": 15}, {"type": "text", "text": "For each question answered, participants received a compensation of $\\mathbb{S}0.04$ . We collected 600 comparison results in the end. ", "page_idx": 15}, {"type": "text", "text": "E Additional results ", "text_level": 1, "page_idx": 15}, {"type": "image", "img_path": "v1BIm8wESL/tmp/dea6cae0bbfb45fce1cb92feee09f1685c683783d5b698e26e30b9d58d4ca336.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 8: Left: We visualized three consecutive frames within an motion sequence. It is evident that while there was no jitter in the motion source, significant jitter occurred in the $t$ -th frame of the $\\mathrm{R^{2}E T}$ [32] results, which was not the case with our method. Right: We visualize the corresponding right-hand height for this segment of the sequence. The results indicate that the jitter in the $\\mathbf{R}^{2}\\mathrm{E}\\bar{\\mathbf{T}}$ output was pronounced. ", "page_idx": 15}, {"type": "text", "text": "Motion jitter comparison To better illustrate the jitter issue present in the results from the $\\mathrm{R^{2}E T}$ [32] method, we visualized consecutive frames generated by $\\bar{\\mathsf{R}}^{2}\\mathrm{ET}$ and our method in Figure 8, and provided a line graph depicting the variations in height of the right-hand joint over time. These results demonstrate that $\\mathrm{\\overline{{R}}^{2}E\\hat{T}}$ is adversely affected by contradictions between skeletal retargeting and geometry correction phases, leading to significant motion jitter. In contrast, our method successfully avoids this problem. ", "page_idx": 15}, {"type": "image", "img_path": "v1BIm8wESL/tmp/f5c6d89b62b91e04f8623d1216ab55becc48ffb5961bfc9b57f9a2c102097ea2.jpg", "img_caption": ["Figure 9: Qualitative comparison with Zhang et al. [30]. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Qualitative comparison with Zhang et al. [30] Since Zhang et al. [30] did not open-source their code, we were unable to conduct a complete and fair comparison of their method with ours in our experiments. However, we endeavored to locate several examples presented in their paper and applied our MeshRet to the same motion sequences. The comparative results are displayed in Figure 9. As observed in these examples, our method maintains the semantic integrity of the source motions, and it performs better in the Fireball case (the second motion sequence shown). This indicates that our method can achieve, and even surpass, the performance of their approach. ", "page_idx": 15}, {"type": "text", "text": "Metrics across different data splits Tables 3 and 4 present the contact error and penetration ratio of our method compared to the baseline method across four different data splits. A consistent pattern observed is that performance improves for seen characters or motions. It is evident that our method outperforms the baseline across all data splits. ", "page_idx": 15}, {"type": "text", "text": "Ablation studies on ratios of proximal sensor pairs The full approach can be considered a mixed version of ${\\mathrm{Ours}}_{f a r}$ and $\\mathrm{Ours}_{c l s}$ , utilizing an equal distribution of proximal and distal sensor pairs. To better illustrate this balance, we provide additional experimental results by testing different ratios of proximal to distal sensor pairs. Table 5 compares our method\u2019s performance with varying percentages of proximal sensor pairs under the Mixamo $^+$ setting. As the percentage of proximal sensor pairs decreases, the interpenetration ratio fluctuates mildly, while the contact error initially decreases and then increases. Finally, with no proximal pairs (equivalent to the \"far\" version), the performance drops significantly. In Figure 10, we present a qualitative comparison of our methods using different proximal sensor pair ratios. Except for the $100\\%$ Proximal version (equivalent to $\\mathrm{Ours}_{c l s}.$ ) and the $0\\%$ Proximal version (equivalent to ${\\mathrm{Ours}}_{f a r.}$ ), our method demonstrates fair robustness to the proximal sensor ratio in the $25\\%-75\\%$ interval. Based on these results, we conclude that choosing $50\\%$ proximal sensor pairs strikes a reasonable balance for achieving good performance. ", "page_idx": 15}, {"type": "table", "img_path": "v1BIm8wESL/tmp/65b705123d338afecf2ec037f554898312edb92260c8d9f0e077d3096f9e2b55.jpg", "table_caption": ["Table 3: Contact errors of MeshRet and baselines across all data splits on Mixamo+. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "v1BIm8wESL/tmp/e910f279ecc0e8f18d109f5c21cf56384cf55ad3d6518fc3f85ed0f2b76adbbc.jpg", "table_caption": ["Table 4: Penetration ratios of MeshRet and baselines across all data splits on Mixamo+. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "table", "img_path": "v1BIm8wESL/tmp/ac9c284b7aa5886719dd5137f79a0a6aaf443c6aa9b9176560b4d4ec5e74e53e.jpg", "table_caption": ["Table 5: Quantitative comparison between our methods with varing percentages of proximal sensor pairs under the Mixamo $^+$ setting. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Ablation studies on different sensor arragements We conducted further ablation studies on different sensor arrangements. Specifically, we evaluated the performance of a model trained with half the sample points in the $\\phi$ space in SCS, denoted as ${\\mathrm{Ours}}_{\\phi}$ , and another model trained with half the sample points in the $l$ space in SCS, referred to as Oursl. As shown in Table 6, ${\\mathrm{Ours}}_{\\phi}$ compromises the interpenetration ratio, indicating that sufficient sample points in the space are crucial for avoiding interpenetration. We also found that both models introduce artifacts; please refer to Figure 11. ", "page_idx": 16}, {"type": "text", "text": "Failure cases with noisy inputs We provide resutls with clean and noisy inputs in Figure12 and Figure13. The results of MeshRet exhibit interpenetration with noisy inputs. ", "page_idx": 16}, {"type": "image", "img_path": "v1BIm8wESL/tmp/728e67fdf83e479710e5002ca588f465999898b8eba9737af5d09f0bfe7712ec.jpg", "img_caption": ["Figure 10: Qualitative results with different proximal sensor pair ratios. "], "img_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "v1BIm8wESL/tmp/4ed6cc61149f46b0c01c958f1e5503d5d0df47308887e09e3e487742cedf02d0.jpg", "table_caption": ["Table 6: Quantitative comparison between methods with different sensor arrangements. "], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "v1BIm8wESL/tmp/fbaa8570fbd279aaf586b90d8585dfb899d8dcd1b060e52f5db67186f92d1935.jpg", "img_caption": ["Figure 11: Qualitative comparison of additional ablation studies on sensor arrangements. The red rectangles identify artifacts introduced by different sensor arrangements. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "v1BIm8wESL/tmp/c7787a5b331bf8b27e84bf2152e905598eea49af1016f8ef3ec0f25c315b0b3d.jpg", "img_caption": ["Figure 12: Qualitative results on the Mixamo dataset with clean and noisy inputs. A red rectangle indicates interpenetration. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "v1BIm8wESL/tmp/5b4e67a255a75122e79bd53da0ed4a178b9ff77d158ede6257d004214bf960d2.jpg", "img_caption": ["Figure 13: Qualitative results on the Mixamo dataset with ScanRet characters as targets. A red rectangle indicates interpenetration. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "More cases We present additional cases to validate the effectiveness of our MeshRet. Figures 14, 15, 16, and 17 depict four motion sequences retargeted from the source character to distinct target characters. These examples illustrate that our MeshRet is capable of generating high-quality motion sequences on target characters with diverse body shapes. Please watch our demo video in the supplementary material for a better comparison of the performance across different methods. ", "page_idx": 18}, {"type": "image", "img_path": "v1BIm8wESL/tmp/eb252f3d86c653e294d74cfdf38ce6350e959b80eee04a4202d2358b28d2deab.jpg", "img_caption": ["Figure 14: Snapshots of motion sequence 4 in ScanRet, retargeted from the source character to three distinct characters. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "v1BIm8wESL/tmp/758b5ca69f9e7122fc9580c173c1df395b0195a4ee7a9f5207a4b9868c0aff95.jpg", "img_caption": ["Figure 15: Snapshots of motion sequence 43 in ScanRet, retargeted from the source character to three distinct characters. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "v1BIm8wESL/tmp/50f491e898e0a53a20506e0152698f6581c7a4c1b6c7913f15e11aecf1dc6a60.jpg", "img_caption": ["Figure 16: Snapshots of motion sequence 9 in ScanRet, retargeted from the source character to three distinct characters. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "v1BIm8wESL/tmp/3c6804f8016913310c0297c0a1f6f547c06bd23590e899169ae5a6d31f5801c4.jpg", "img_caption": ["Figure 17: Snapshots of motion sequence 45 in ScanRet, retargeted from the source character to three distinct characters. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "F Broader impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Our work can provide animation professionals with enhanced results in motion retargeting, thereby alleviating their workload and increasing productivity in fields such as virtual reality, game development, and animation production. Regarding potential negative social impacts, we believe the likelihood of misuse of our work is minimal. This is because our work is situated in the midstream phase of the animation production pipeline, whereas privacy-invading forgeries, such as DeepFake, primarily occur during the downstream rendering phase. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our results clearly support our main claims made in the abstract and introduction. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We discuss the limitations in Conclusion. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper does not contain theoretical proof. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We provide experimental details in Section 4.1 and Appendix A. Code and data will be made public if this paper gets accepted. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The Mixamo dataset is accessible to the public. Our ScanRet dataset will also be made public if this paper gets accepted. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide detailed experimental settings in Section 4.1, Appendix A, and Appendix C. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [No] ", "page_idx": 23}, {"type": "text", "text": "Justification: We omitted error bars from our analysis due to the excessive computational expense involved in enumerating all data splits. For example, calculating the penetration ratio solely using ScanRet requires dozens of hours. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide resource requirements in Appendix C. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We removed textures from ScanRet to help preserve anonymity of participants.   \nWe acquired consent from every participant. All the participants are fairly paid. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We discuss potential societal impacts in Appendix F ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our work does not have such risk. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We properly cited papers and sources for existing assets. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Due to the limited capacity of currently available network storage tools that support anonymous link sharing, we are unable to host our entire dataset during the anonymous review phase. However, we have placed a single data example in the \u2018artifact/scanret\u2019 folder of our code in the supplementary material, which can be loaded using Python\u2019s pickle library. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We provide these information in the Appendix D. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We performed our research in accordance with local laws. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}]