[{"type": "text", "text": "Reverse Transition Kernel: A Flexible Framework to Accelerate Diffusion Inference ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xunpeng Huang Difan Zou Hanze Dong HKUST HKU Salesforce AI Research xhuangck@connect.ust.hk dzou@cs.hku.hk hanze.dong@salesforce.com ", "page_idx": 0}, {"type": "text", "text": "Yi Zhang Yian Ma Tong Zhang HKU UC San Diego UIUC yizhang101@connect.hku.hk yianma@ucsd.edu tongzhang@tongzhang-ml.org ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "To generate data from trained diffusion models, most inference algorithms, such as DDPM [16], DDIM [30], and other variants, rely on discretizing the reverse SDEs or their equivalent ODEs. In this paper, we view such approaches as decomposing the entire denoising diffusion process into several segments, each corresponding to a reverse transition kernel (RTK) sampling subproblem. Specifically, DDPM uses a Gaussian approximation for the RTK, resulting in low per-subproblem complexity but requiring a large number of segments (i.e., subproblems), which is conjectured to be inefficient. To address this, we develop a general RTK framework that enables a more balanced subproblem decomposition, resulting in $\\tilde{O}(1)$ subproblems, each with strongly log-concave targets. We then propose leveraging two fast sampling algorithms, the Metropolis-Adjusted Langevin Algorithm (MALA) and Underdamped Langevin Dynamics (ULD), for solving these strongly log-concave subproblems. This gives rise to the RTK-MALA and RTK-ULD algorithms for diffusion inference. In theory, we further develop the convergence guarantees for RTK-MALA and RTK-ULD in total variation (TV) distance: RTK-ULD can achieve $\\epsilon$ target error within $\\tilde{\\mathcal{O}}(d^{1/2}\\epsilon^{-1})$ under mild conditions, and RTKMALA enjoys a $\\bar{\\mathcal{O}}(d^{2}\\log(d/\\epsilon))$ convergence rate under slightly stricter conditions. These theoretical results surpass the state-of-the-art convergence rates for diffusion inference and are well supported by numerical experiments. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Generative models have become a core task in modern machine learning, where the neural networks are employed to learn the underlying distribution from training examples and generate new data points. Among various generative models, denoising diffusions have produced state-of-the-art performance in many domains, including image and text generation [13, 2, 27, 28], text-to-speech synthesis [26], and scientific tasks [33, 36, 5]. The fundamental idea involves incrementally adding noise and gradually transform the data distribution to a prior distribution that is easier to sample from, e.g., Gaussian distribution. Then, diffusion models parameterize and learn the score of the noised distributions to progressively denoise samples from priors and recover the data distribution [35, 31]. ", "page_idx": 0}, {"type": "text", "text": "Under this paradigm, generating data in denoising diffusion models involves solving a series of sampling subproblems, i.e., generating samples from the distribution after one-step denoising. To this end, DDPM [16], one of the most popular sampling methods in diffusion models, has been developed for this purpose. DDPM uses the Gaussian transition with carefully designed mean and covariance to approximate the solutions to these sampling subproblems. By sequentially stacking a series of Gaussian transitions, DDPM successfully generates high-quality samples that follow the data distribution. The empirical success of DDPM has immediately triggered various followup work [32, 24], aiming to accelerate the inference process and improve the generation quality. Alongside rapid empirical research on diffusion models and DDPM-like sampling algorithms [18, 38], theoretical studies have emerged to analyze the convergence and sampling error of DDPM. In particular, [20, 23, 8, 7, 3, 9] have established polynomial convergence bounds, in terms of dimension $d$ and target sampling error $\\epsilon$ , for the generation process under various assumptions. Previous work usually assume the score estimation error to construct the sampling analysis [8, 14, 3, 18]. A typical bound under minimal data assumptions on the score of the data distribution is provided by [8, 3], which establishes an $\\tilde{\\mathcal{O}}(d\\epsilon^{-2})$ score estimation guarantees to sample from data distribution within $\\epsilon$ -sampling error in the Total Variation (TV) distance. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In essence, the denoising diffusion process can be approached through various decompositions of sampling subproblems, where the overall complexity depends on the number of these subproblems multiplied by the complexity of solving each one. Within this framework, DDPM can be regarded as a specific solver for the denoising diffusion process that heavily prioritizes the simplicity of subproblems over their quantity. In particular, it adopts simple one-step Gaussian approximations for the subproblems, with $O(1)$ computation complexity, but needs to deal with a relatively large number-approximately $O(d\\epsilon^{-2})$ -of target subproblems to ensure the cumulative sampling error isboundedby $\\epsilon$ in TV distance. This imbalance raises the question of whether the DDPM-like approaches stand as the most efficient algorithm, considering the extensive potential subproblem decompositions of the denoising diffusion process. We therefore aim to: ", "page_idx": 1}, {"type": "text", "text": "accelerate the inference of diffusion models via a more balanced subproblem decompositioninthedenoisingprocess. ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this work, we propose a novel framework called reverse transition kernel (RTK) to achieve exactly that. Our approach considers a generalized subproblem decomposition of the denoising process, where the difficulty of each sampling subproblem and the total number of subproblems are determined by the step size parameter $\\eta$ . Unlike DDPM, which requires setting $\\eta=\\epsilon^{2}$ , resulting in approximately $\\tilde{\\mathcal{O}}(1/\\bar{\\eta})=\\tilde{\\mathcal{O}}(1/\\epsilon^{2})$ subproblems, our framework allows $\\eta$ to be feasible in a broader range. Furthermore, we demonstrate that a more balanced subproblem decomposition can be attained by carefully selecting $\\eta=\\Theta(1)$ as a constant, resulting in approximately $\\bar{O}(1)^{\\!\\!{\\phantom{+}}}$ sampling subproblems, with each target distribution being strongly log-concave. This nice property further enables us to efficiently solve the sampling subproblems using well-established acceleration techniques, such as Metropolis Hasting step and underdamped discretization, without encountering many subproblems. Consequently, our proposed framework facilitates the design of provably faster algorithms than DDPM for performing diffusion inference. Our contributions can be summarized as follows. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We propose a fexible framework that enhances the effciency of diffusion inference by balancing the quantity and hardness of RTK sampling subproblems used to segment the entire denoising diffusion process. Specifically, we demonstrate that with a carefully designed decomposition, the number of sampling subproblems can be reduced to approximately $\\bar{O}(1)^{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\bar{}}$ , while ensuring that all RTK targets exhibit strong log-concavity. This capability allows us to seamlessly integrate a range of well-established sampling acceleration techniques, thereby enabling highly efficient algorithms for diffusion inference. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 Building upon the developed framework, we implement the RTK using the Metropolis-Adjusted Langevin Algorithm (MALA), making it the first attempt to adapt this highly accurate sampler for diffusion inference. Under slightly stricter assumptions on the estimation errors of the energy difference and score function, we demonstrate that RTK-MALA can achieve linear convergence with respect to the sampling error $\\epsilon$ , specifically ${\\mathcal{O}}(\\log(1/\\epsilon))$ , which significantly outperforms the $\\tilde{\\mathcal{O}}(1/\\epsilon^{2})$ convergence rate of DDPM [8, 3]. Additionally, we consider the practical diffusion model where only the score function is accessible and develop a score-only RTK-MALA algorithm. We further prove that the score-only RTK-MALA algorithm can achieve an error $\\epsilon$ with a complexity of $\\tilde{\\mathcal{O}}(\\epsilon^{-2/(u-1)}\\cdot2^{u})$ where $u$ can be an arbitrarilylarge constant, provided the energy function satisfies the $u$ -th order smoothness condition. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We further implement Underdamped Langevin Dynamics (ULD) within the RTK framework. The resulting RTK-ULD algorithm achieves a state-of-the-art complexity of $\\tilde{\\mathcal{O}}(d^{1/2}\\epsilon^{-1})$ forboth $d$ and $\\epsilon$ dependence under minimal data assumptions, i.e., Lipschitz condition for the ground truth score function. Compared with the $\\tilde{\\mathcal{O}}(d\\epsilon^{-2})$ complexity guarantee for DDPM, it improves the complexity with an $\\bar{\\tilde{O}}(d^{1/2}\\epsilon^{-1})$ factor. This result also matches the state-of-the-art convergence rate of the ODE-based methods [9], though those methods require Lipschitz conditions for both the ground truth score function and the score neural network. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first introduce the notations used in subsequent sections. Then, we present several distinct Markov processes to demonstrate the procedures for adding noise to existing data and generating new data. Besides, we specify the assumptions required for the target distribution in our algorithms and analysis. ", "page_idx": 2}, {"type": "text", "text": "Notations. We say a complexity $h\\colon\\mathbb{R}\\to\\mathbb{R}$ to be $h(n)={\\mathcal{O}}(n^{k})$ or $h(n)=\\tilde{\\mathcal{O}}(n^{k})$ if the complexity satisfies $h(n)\\,\\leq\\,c\\cdot n^{k}$ or $h(n)\\,\\leq\\,c\\cdot n^{k}[\\log(n)]^{k^{\\prime}}$ for absolute contant $c,k$ and $k^{\\prime}$ . We use the lowercase bold symbol $\\mathbf{x}$ to denote a random vector, and the lowercase italicized bold symbol $\\textbf{\\em x}$ represents a fixed vector. The standard Euclidean norm is denoted by $\\|\\cdot\\|$ . The data distribution is presented as $p_{*}\\propto\\exp(-f_{*})$ . Besides, we define two Markov processes $\\mathbb{R}^{d}$ , i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\left\\{\\mathbf{x}_{t}\\right\\}_{t\\in[0,T]},\\quad\\left\\{\\mathbf{x}_{k\\eta}^{\\leftarrow}\\right\\}_{k\\in\\{0,1,\\dots,K\\}},\\quad\\mathrm{where}\\quad T=K\\eta.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In the above notations, $T$ presents the mixing time required for the data distribution to converge to specific priors, $K$ denotes the iteration number of the generation process, and $\\eta$ signifies the corresponding step size. Further details of the two processes are provided below. ", "page_idx": 2}, {"type": "text", "text": "Adding noise to data with the forward process. The first Markov process $\\left\\{{\\bf x}_{t}\\right\\}$ corresponds to generating progressively noised data from $p_{*}$ . In most denoising diffusion models, $\\left\\{{\\bf x}_{t}\\right\\}$ is an Ornstein-Uhlenbeck (OU) process shown as follows ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}_{t}=-\\mathbf{x}_{t}\\mathrm{d}t+\\sqrt{2}\\mathrm{d}B_{t}\\quad\\mathrm{where}\\quad\\mathbf{x}_{0}\\sim p_{*}\\propto\\exp(-f_{*}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "If we denote underlying distribution of $\\mathbf{x}_{t}$ as $p_{t}\\,\\propto\\,\\exp(-f_{t})$ meaning $f_{0}\\,=\\,f_{*}$ , the forward OU process provides an analytic form of the transition kernel, i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{t^{\\prime}|t}(x^{\\prime}|x)=\\frac{p_{t^{\\prime},t}(x^{\\prime},x)}{p_{t}(x)}=\\left(2\\pi\\left(1-e^{-2(t^{\\prime}-t)}\\right)\\right)^{-d/2}\\cdot\\exp\\left[\\frac{-\\left\\|x^{\\prime}-e^{-(t^{\\prime}-t)}x\\right\\|^{2}}{2\\left(1-e^{-2(t^{\\prime}-t)}\\right)}\\right]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for any $t^{\\prime}\\geq t$ where $p_{t^{\\prime},t}$ denotes the joint distribution of $\\left({{\\bf{x}}_{t^{\\prime}}},{\\bf{x}}_{t}\\right)$ .According to the Foker-Planck equation, we know the stationary distribution for SDE. (1) is the standard Gaussian distribution. ", "page_idx": 2}, {"type": "text", "text": "Denoising generation with a reverse SDE. Various theoretical works [20, 23, 8, 7, 3] based on DDPM [16] consider the generation process of diffusion models as the reverse process of SDE. (1) denoted as $\\{\\mathbf{x}_{t}^{\\leftarrow}\\}$ .According to the Doob's $h$ Transform, the reverse SDE, i.e., $\\bar{\\{\\mathbf{x}_{t}^{\\leftarrow}\\}}$ ,followsfrom ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}_{t}^{\\leftarrow}=\\left(\\mathbf{x}_{t}^{\\leftarrow}+2\\nabla\\ln p_{T-t}(\\mathbf{x}_{t}^{\\leftarrow})\\right)\\mathrm{d}t+\\sqrt{2}\\mathrm{d}B_{t},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "whose underlying distribution $p_{t}^{\\leftarrow}$ satisfies $p_{T-t}=p_{t}^{\\leftarrow}$ . Similar to the definition of transition kernel shown in Eq. (2), we define $\\bar{p_{t^{\\prime}|t}^{\\leftarrow}}(\\pmb{x}^{\\prime}|\\pmb{x})=p_{t^{\\prime},t}^{\\leftarrow}(\\pmb{x}^{\\prime},\\pmb{\\bar{x}})/p_{t}^{\\leftarrow}(\\pmb{x})$ for any $t^{\\prime}\\ge t\\ge0$ and name it as reverse transition kernel (RTK). ", "page_idx": 2}, {"type": "text", "text": "To implement SDE. (3), diffusion models approximate the score function $\\nabla\\ln{p_{t}}$ with a parameterized neural network model, denoted by $s_{\\theta,t}$ ,where $\\pmb{\\theta}$ denotes the network parameters. Then, SDE. (3) can be practically implemented by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}\\overline{{\\mathbf{x}}}_{t}=\\left(\\overline{{\\mathbf{x}}}_{t}+2\\mathbf{s}_{\\theta,T-k\\eta}(\\overline{{\\mathbf{x}}}_{k\\eta})\\right)\\mathrm{d}t+\\sqrt{2}\\mathrm{d}B_{t}\\quad\\mathrm{for}\\quad t\\in[k\\eta,(k+1)\\eta)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with a standard Gaussian initialization, $\\overline{{\\mathbf{x}}}_{0}\\sim\\mathcal{N}(\\mathbf{0},\\pmb{I})$ . Eq. (4) has the following closed solution ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\overline{{\\mathbf{x}}}_{(k+1)\\eta}=e^{\\eta}\\cdot\\overline{{\\mathbf{x}}}_{k\\eta}-2(1-e^{\\eta})s_{\\theta,T-k\\eta}(\\overline{{\\mathbf{x}}}_{k\\eta})+\\sqrt{e^{2\\eta}-1}\\cdot\\xi\\quad\\mathrm{and}\\quad\\xi\\sim\\mathcal{N}(\\mathbf{0},I),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which is exactly the DDPM algorithm ", "page_idx": 2}, {"type": "text", "text": "DDPM approximately samples the reverse transition kernel. DDPM can also be viewed as an approximated sampler for RTK, i.e., $p_{t^{\\prime}|t}^{\\leftarrow}(x^{\\prime}|x)$ forsome $t^{\\prime}>t$ In particular, the update of DDPM at the iteration $k$ applies the Gaussian-type transition, i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\overline{{p}}_{(k+1)\\eta\\vert k\\eta}(\\cdot\\vert x)=\\mathcal{N}\\left(e^{\\eta}x-2(1-e^{\\eta})s_{\\theta,T-k\\eta}(x),(e^{2\\eta}-1)\\cdot I\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "to approximate the distribution $p_{(k+1)\\eta|k\\eta}^{\\leftarrow}(\\cdot|\\pmb{x})$ [16]. Specifcally, by the chain rule of KL divergence, the gap between the data distribution $p_{*}$ and the generated distribution $\\overline{{p}}_{T}$ satisfies ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{KL}\\left(\\overline{{p}}_{T}\\Vert p_{*}\\right)\\leq\\mathrm{KL}\\left(\\overline{{\\mathbf{x}}}_{0}\\Vert p_{0}^{\\leftarrow}\\right)+\\sum_{k=0}^{K-1}\\mathbb{E}_{\\overline{{\\mathbf{x}}}\\sim\\overline{{p}}_{k\\eta}}\\left[\\mathrm{KL}\\left(\\overline{{p}}_{\\left(k+1\\right)\\eta\\left\\vert k\\eta}\\left(\\cdot\\vert\\overline{{\\mathbf{x}}}\\right)\\right\\vert\\left\\vert p_{\\left(k+1\\right)\\eta\\left\\vert k\\eta}^{\\leftarrow}\\left(\\cdot\\vert\\overline{{\\mathbf{x}}}\\right)\\right)\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $K=T/\\eta$ is the total number of iterations. For DDPM, to guarantee a small sampling error, Wwe need to use a smal tep size $\\eta$ to ensure that $\\overline{{p}}_{(k+1)\\eta|k\\eta}$ is sufcientlyclose to $p_{(k+1)\\eta|k\\eta}^{\\leftarrow}$ Then, the required iteration numbers $K=T/\\eta$ will be large and dominate the computational complexity. In Chen et al. [8, 10], it was shown that one needs to set $\\eta=\\tilde{\\mathcal{O}}(\\epsilon^{2})$ to achieve $\\epsilon$ sampling error in TV distance (assuming no score estimation error) and the total complexity is $K=\\tilde{\\mathcal{O}}(1/\\epsilon^{2})$ ", "page_idx": 3}, {"type": "text", "text": "Intuition for General Reverse Transition Kernel. As previously mentioned, DDPM approximately solves RTK sampling subproblems using a small step size $\\eta$ While this allows for efficient one-step implementation, it necessitates a large number of RTK sampling problems. This naturally creates a trade-off between the quantity of RTK sampling problems and the complexity of solving them. To address this, one can consider a larger step size $\\eta$ , which results in a relatively more challenging RTK sampling target $p_{(k+1)\\eta|k\\eta}^{\\leftarrow}$ and areduced nuber of sampling problems $(K=T/\\eta)$ . By examining a general choice for the step size $\\eta$ , the generation process of diffusion models can be depicted through a comprehensive framework of reverse transition kernels, which will be explored in depth in the following section. This framework enables the design of various decompositions for RTK sampling problems and algorithms to solve them, resulting in an extensive family of generation algorithms for diffusion models (that encompasses DDPM). Consequently, this also offers the potential to develop faster algorithms with lower computational complexities, e.g., applying fast sampling algorithms for sampling the RTK, i.e. $p_{(k+1)\\eta|k\\eta}^{\\leftarrow}$ with reasonably large $\\eta$ ", "page_idx": 3}, {"type": "text", "text": "General Assumptions. Similar to the analysis of DDPM [8, 7], we make the following assumptions On the data distribution $p_{*}$ that will be utilized in the theory. ", "page_idx": 3}, {"type": "text", "text": "[A1]  For all $t\\geq0$ , the score $\\nabla\\ln{p_{t}}$ is $L$ -Lipschitz. ", "page_idx": 3}, {"type": "text", "text": "[A2]  The second moment of the target distribution $p_{*}$ is upper bounded, i.e., $\\mathbb{E}_{p_{*}}\\left[\\left\\Vert\\cdot\\right\\Vert^{2}\\right]=m_{2}^{2}$ ", "page_idx": 3}, {"type": "text", "text": "Assumption [A1] is standard one in diffusion literature and has been used in many prior works [4, 10, 20, 9]. Moreover, we do not require the isoperimetric conditions, e.g., the establishment of the log-Sobolev inequality and the Poincare inequality for the data distribution $p_{*}$ as [20], and the convex conditions for the energy function $f_{*}$ as [4]. Therefore, our assumptions cover a wide range of highly non-log-concave data distributions. We emphasize that Assumption [A1] may be relaxed only to assume the target distribution is smooth rather than the entire OU process, based on the technique in [7] (see rough calculations in their Lemmas 12 and 14). We do not include this additional relaxation in this paper to clarify our analysis. Assumption [A2] is one of the weakest assumptions being adopted for the analysis of posterior sampling. ", "page_idx": 3}, {"type": "text", "text": "3  General Framework of Reverse Transition Kernel ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section introduces the general framework of Reverse Transition Kernel (RTK). As mentioned in the previous section, the framework is built upon the general setup of segmentation: each segment has length $\\eta$ ; within each segment, we generate samples according to the RTK target distributions. Then, the entire generation process in diffusion models can be considered as the combination of a series of sampling subproblems. In particular, the inference process via RTK is displayed in Alg. 1. ", "page_idx": 3}, {"type": "text", "text": "The implementation of RTK framework.We begin with a new Markov process $\\{\\hat{\\mathbf{x}}_{k\\eta}\\}_{k=0,1,...,K}$ satisfying $K\\eta=T$ , where the number of segments $K$ , segment length $\\eta$ , and length of the entire process $T$ correspond to the definition in Section 2. Consider the Markov process $\\{\\hat{\\mathbf{x}}_{k\\eta}\\}$ as the ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 INFERENCE WITH REVERSE TRANSITION KERNEL (RTK) ", "page_idx": 4}, {"type": "text", "text": "1: Input: Initial particle $\\hat{\\mathbf{x}}_{0}$ sampled from the standard Gaussian distribution, Iteration number $K$ Step size $\\eta$ , required convergence accuracy $\\epsilon$   \n2: for $k=0$ to $K-1$ do   \n3:  Dra w sample $\\hat{\\mathbf{x}}_{(k+1)\\eta}$ with MCMCs from $\\hat{p}_{(k+1)\\eta|k\\eta}(\\cdot|\\hat{\\mathbf{x}}_{k\\eta})$ which approximates the groundtruth reverse transition kernel, i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{(k+1)\\eta|k\\eta}^{\\leftarrow}(z|\\hat{\\mathbf{x}}_{k\\eta})\\propto\\exp\\left(-g(z)\\right):=\\exp\\left(-f_{(K-k-1)\\eta}(z)-\\frac{\\|\\hat{\\mathbf{x}}_{k\\eta}-z\\cdot e^{-\\eta}\\|^{2}}{2(1-e^{-2\\eta})}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "generation process of diffusion models with underlying distributions $\\{\\hat{p}_{k\\eta}\\}$ , we require $\\hat{p}_{0}=\\mathcal{N}(\\mathbf{0},\\pmb{I})$ and $\\hat{p}_{K\\eta}\\approx\\,p_{*}$ , which is similar to the Markov process $\\{\\mathbf{x}_{k\\eta}^{\\leftarrow}\\}$ . In order to make the underlying distribution of output particles close to the data distribution, we can generate $\\hat{\\mathbf{x}}_{k\\eta}$ with Alg. 1, which is equivalent to the following steps: ", "page_idx": 4}, {"type": "text", "text": "\u00b7 Initialize $\\hat{\\mathbf{x}}_{0}$ with an easy-to-sample distribution, eg. $\\mathcal{N}(\\mathbf{0},\\boldsymbol{I})$ , which is close to $p_{K\\eta}$ ", "page_idx": 4}, {"type": "text", "text": "Update particles by drawing samples from $\\hat{p}_{(k+1)\\eta|k\\eta}(\\cdot|\\hat{\\mathbf{x}}_{k\\eta})$ , which satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{p}_{(k+1)\\eta|k\\eta}(\\cdot|\\hat{\\mathbf{x}}_{k\\eta})\\approx p_{(k+1)\\eta|k\\eta}^{\\leftarrow}(\\cdot|\\hat{\\mathbf{x}}_{k\\eta}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Under these conditions, if $\\hat{p}_{k\\eta}(z)\\approx p_{(K-k)\\eta}(z)$ , then we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{p}_{(k+1)\\eta}(z)=\\left\\langle\\hat{p}_{(k+1)\\eta|k\\eta}(z|\\cdot),\\hat{p}_{k\\eta}(\\cdot)\\right\\rangle\\approx\\left\\langle p_{(k+1)\\eta|k\\eta}^{\\leftarrow}(z|\\cdot),p_{k\\eta}^{\\leftarrow}(\\cdot)\\right\\rangle=p_{(k+1)\\eta}(z)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for any $k\\,\\in\\,\\{0,1,\\ldots,K\\}$ . This means we can implement the generation of diffusion models by solving a series of sampling subproblems with target distributions $p_{(k+1)\\eta|k\\eta}^{\\leftarrow}(\\cdot|\\hat{\\mathbf{x}}_{k\\eta})$ ", "page_idx": 4}, {"type": "text", "text": "The close form of reverse transition kernels.  To implement Alg. 1, the most critical problem is determining the analytic form of RTK $p_{t^{\\prime}|t}^{\\leftarrow}(x^{\\prime}|x)$ for and $t^{\\prime}\\geq t\\geq0$ which is shown in the following lemma whose proof is deferred to Appendix B. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.1. Suppose a Markov process $\\left\\{{\\bf x}_{t}\\right\\}$ with SDE. 1, then for any $t^{\\prime}>t$ we have ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{T-t|T-t^{\\prime}}^{\\leftarrow}(x|x^{\\prime})=p_{t|t^{\\prime}}(x|x^{\\prime})\\propto\\exp\\left(-f_{t}(x)-\\frac{\\left\\|x^{\\prime}-x\\cdot e^{-(t^{\\prime}-t)}\\right\\|^{2}}{2(1-e^{-2(t^{\\prime}-t)})}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The first critical property shown in this Lemma is that RTK $p_{t|t^{\\prime}}$ is a perturbation of $p_{t}$ with a $l_{2}$ regularization. This means if the score of $p_{t}$ , i.e., $\\nabla f_{t}$ , can be well-estimated, the score of RTK, i.e., $\\nabla\\log{p_{t|t^{\\prime}}}$ can also be approximated with high accuracy. Moreover, in the diffusion model, $\\nabla f_{t}=\\nabla\\log{p_{t}}$ is exactly the score function at time $t$ , which is approximated by the score network function ${\\pmb s}_{\\theta,t}({\\pmb x})$ , then ", "page_idx": 4}, {"type": "equation", "text": "$$\n-\\nabla\\log p_{t|t^{\\prime}}(x|x^{\\prime})=\\nabla f_{t}(x)+\\frac{e^{-2(t^{\\prime}-t)}x-e^{-(t^{\\prime}-t)}x^{\\prime}}{1-e^{-2(t^{\\prime}-t)}}\\approx s_{\\theta,t}(x)+\\frac{e^{-2(t^{\\prime}-t)}x-e^{-(t^{\\prime}-t)}x^{\\prime}}{1-e^{-2(t^{\\prime}-t)}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which can be directly calculated with a single query of ${\\pmb s}_{\\theta,t}({\\pmb x})$ . The second critical property of RTK is that we can control the spectral information of its score by tuning the gap between $t^{\\prime}$ and $t$ Specifically, considering the target distribution, i.., $p(K\\!-\\!k\\!-\\!1)\\eta|(K\\!-\\!k)\\eta$ for the $k$ -th transition, the Hessian matrix of its energy function satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n-\\nabla^{2}\\log p_{(K-k-1)\\eta|(K-k)\\eta}=\\nabla^{2}f_{(K-k-1)\\eta}({\\pmb x})+\\frac{e^{-2\\eta}}{1-e^{-2\\eta}}\\cdot{\\cal I}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "According to Assumption [A1], the Hessian $\\nabla^{2}f_{(K-k-1)\\eta}({\\pmb x})=-\\nabla^{2}\\log p_{(K-k-1)\\eta}$ can be lower bounded by $-L I$ which implies that RTK $p(K\\!-\\!k\\!-\\!1)\\eta|(K\\!-\\!k)\\eta$ Will be $L$ strongly log-concave and $3L$ -smooth when the step size is set $\\eta=1/2\\cdot\\log(1+1/2L)$ . This further implies that the targets ", "page_idx": 4}, {"type": "text", "text": "of all subsampling problems in Alg. 1 will be strongly log-concave, which can be sampled very efficiently by various posterior sampling algorithms. ", "page_idx": 5}, {"type": "text", "text": "Sufficient conditions for the convergence.  According to Pinsker's inequality and Eq. (7), we can obtain the following lemma that establishes the general error decomposition for Alg. 1. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}\\left(\\hat{p}_{K\\eta},p_{*}\\right)\\le\\sqrt{(1+L^{2})d+\\left\\Vert\\nabla f_{*}(\\mathbf{0})\\right\\Vert^{2}}\\cdot\\exp(-K\\eta)}\\\\ &{\\qquad\\qquad\\qquad+\\;\\sqrt{\\frac{1}{2}\\displaystyle\\sum_{k=0}^{K-1}\\mathbb{E}_{\\hat{\\mathbf{x}}\\sim\\hat{p}_{k\\eta}}\\left[\\mathrm{KL}\\left(\\hat{p}_{(k+1)\\eta\\vert k\\eta}(\\cdot\\vert\\hat{\\mathbf{x}})\\right\\vert\\middle\\vert p_{(k+1)\\eta\\vert k\\eta}^{\\leftarrow}(\\cdot\\vert\\hat{\\mathbf{x}})\\right)\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for any $K\\in\\mathbb{N}_{+}$ and $\\eta\\in\\mathbb{R}_{+}$ ", "page_idx": 5}, {"type": "text", "text": "It is worth noting that the choice of $\\eta$ represents a trade-off between the number of subproblems divided throughout the entire process and the difficulty of solving these subproblems. By considering the choice $\\bar{\\eta}\\bar{=}1/2{\\cdot}\\mathrm{log}(1{+}1\\bar{/}2L)$ , we can observe two points: (1) the sampling subproblems in Alg. 1 tend to be simple, as all RTK targets, presented in Lemma 3.1, can be provably strongly log-concave; (2) the total number of subproblems is $K=T/\\eta=\\tilde{\\mathcal{O}}(1)$ , which is not large. Conversely, when considering a larger $\\eta$ that satisfies $\\eta\\gg\\log(1+1/L)$ , the RTK target will no longer be guaranteed to be log-concave, resulting in high computational complexity, potentially even exponential in $d$ when solving the corresponding sampling subproblems. On the other hand, if a much smaller step size $\\eta=o(1)$ is considered, the target distribution of the sampling subproblems can be easily solved, even with a one-step Gaussian transition. However, this will increase the total number of sampling subproblems, potentially leading to higher computational complexity. ", "page_idx": 5}, {"type": "text", "text": "Therefore, we will consider the setup $\\eta=1/2\\!\\cdot\\!\\log(1\\!+\\!1/2L)$ in the remaining part of this paper. Now, the remaining task, which will be discussed in the next section, would be designing and analyzing the sampling algorithms for implementing all iterations of Alg. 1, i.e., solving the subproblems of RTK. ", "page_idx": 5}, {"type": "text", "text": "4  Implementation of RTK inner loops ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we outline the implementation of Step 3 in the RTK algorithm, which aims to solve the sampling subproblems with strong log-concave targets, i.e., $p_{(k+1)\\eta|k\\eta}^{\\leftarrow}(\\cdot|\\hat{\\mathbf{x}}_{k\\eta})\\propto\\exp(-g)$ Specifically, we employ two MCMC algorithms, i.e., the Metropolis-adjusted Langevin algorithm (MALA) and underdamped Langevin dynamics (ULD). For each algorithm, we will first introduce the detailed implementation, combined with some explanation about notations and settings to describe the inner sampling process. After that, we will provide general convergence results and discuss them in several theoretical or practical settings. Besides, we will also compare our complexity results with the previous ones when achieving the convergence of TV distance to show that the RTK framework indeed obtains a better complexity by balancing the number and complexity of sampling subproblems. ", "page_idx": 5}, {"type": "text", "text": "RTK-MALA. Alg. 2 presents a solution employing MALA for the inner loop. When it is used to solve the $k$ -th sampling subproblem of Alg. 1, $\\pmb{x}_{0}$ is equal to $\\hat{\\mathbf{x}}_{k\\eta}$ defined in Section 3 and used to initialize partile ierating in Alg. 2. In Alg. 2, we consider the proces $\\{\\mathbf{z}_{s}\\}_{s=0}^{S}$ whose underlying distribution is denoted as $\\bar{\\{\\mu_{s}\\}}_{s=0}^{S}$ . Although we expect $\\mu_{S}$ to be close to the target distribtion $p_{(k+1)\\eta|k\\eta}^{\\leftarrow}(\\cdot|\\pmb{x}_{0})$ inral practice,the output particles $\\mathbf{z}_{S}$ can onlyapproximately follow $p_{(k+1)\\eta|k\\eta}^{\\leftarrow}(\\cdot|\\pmb{x}_{0})$ due to inevitable errors. Therefore, these errors should be explained in order to conduct a meaningful complexity analysis of the implementable algorithm. Specifically, Alg. 2 introduces two intrinsic errors: ", "page_idx": 5}, {"type": "text", "text": "[E1] Estimation error of the score function: we assume a score estimator, e.g., a well-trained diffusion model, $s_{\\theta}$ , which can approximate the score function with an $\\epsilon_{\\mathrm{score}}$ error, i.e., $\\|s_{\\theta,t}(z)-\\nabla\\log p_{t}(z)\\|\\le\\epsilon_{\\mathrm{score}}$ for all $z\\in\\mathbb{R}^{d}$ and $t\\in[0,T]$ ", "page_idx": 5}, {"type": "text", "text": "[E2] Estimation error of the energy function difference: we assume an energy difference estimator $r$ which can approximate energy difference with an Eenergy error, i.e., $|r_{t}(z^{\\prime},z)+\\log p_{t}(z^{\\prime})-\\log p_{t}(z)|\\leq\\epsilon_{\\mathrm{energy}}$ for all $z,z^{\\prime}\\in\\mathbb{R}^{d}$ ", "page_idx": 5}, {"type": "text", "text": "Under these settings, we provide a general convergence theorem for Alg. 2. To clearly convey the convergence properties, we only show an informal version. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 2 MALA/PROJECTED MALA FOR RTK INFERENCE ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "1: Input: Returned particle of the previous iteration $\\pmb{x}_{0}$ , current iteration number $k$ , inner iteration number $S$ , inner step size $\\tau$ , required convergence accuracy $\\epsilon$   \n2: Draw the initial particle $\\mathbf{z}_{\\mathrm{0}}$ from ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{\\mu_{0}(\\mathrm{d}z)}{\\mathrm{d}z}\\propto\\exp\\left(-L\\|z\\|^{2}-\\frac{\\left\\|x_{0}-e^{-\\eta}z\\right\\|^{2}}{2(1-e^{-2\\eta})}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "3: for $s=0$ to $S-1$ do ", "page_idx": 6}, {"type": "text", "text": "5: if $z_{s+1}\\not\\in B(z_{s},r)\\cap B(\\mathbf{0},R)$ then   \n6: $z_{s+1}=z_{s}$ $\\triangleright$ This condition step is only activated for Projected MALA.   \n7: continue; ", "page_idx": 6}, {"type": "text", "text": "8: Calculate the accept rate as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad a(\\tilde{z}_{s}-(z_{s}-\\tau\\cdot s_{\\theta}(z_{s})),z_{s})}\\\\ &{=\\operatorname*{min}\\left\\{1,\\exp\\left(r_{g}(z_{s},\\tilde{z}_{s})+\\frac{\\left\\|\\tilde{z}_{s}-z_{s}+\\tau\\cdot s_{\\theta}(z_{s})\\right\\|^{2}-\\left\\|z_{s}-\\tilde{z}_{s}+\\tau\\cdot s_{\\theta}(\\tilde{z}_{s})\\right\\|^{2}}{4\\tau}\\right)\\right\\};}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "9: Update the particle $z_{s+1}=\\tilde{z}_{s}$ with probability $a$ , otherwise $z_{s+1}=z_{s}$ ", "page_idx": 6}, {"type": "text", "text": "10: return $\\mathbf{z}_{S}$ ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.1 (Informal version of Theorem C.17). Under Assumption [A1]-[A2], for Alg. 1, we choose ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\eta=\\frac{1}{2}\\log\\frac{2L+1}{2L}\\quad\\mathrm{and}\\quad K=4L\\cdot\\log\\frac{(1+L^{2})d+\\left\\Vert\\nabla f_{*}(\\mathbf{0})\\right\\Vert^{2}}{\\epsilon^{2}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and implement Step 3 of Alg. $^{\\,l}$ with Alg. 2. Suppose the score [E1], energy $[E2J]$ estimation errors and the inner step size $\\tau$ satisfy ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\epsilon_{\\mathrm{score}}=\\mathcal{O}(\\rho d^{-1/2}),\\quad\\epsilon_{\\mathrm{energy}}=\\mathcal{O}(\\rho\\tau^{1/2}),\\quad\\mathrm{and}\\quad\\tau=\\tilde{\\mathcal{O}}\\left(L^{-2}\\cdot(d+m_{2}^{2}+Z^{2})^{-1}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and the hyperparameters, i.e., $R$ and $r$ , are chosen properly. We have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(\\hat{p}_{K\\eta},p_{*}\\right)\\leq\\tilde{\\mathcal{O}}(\\epsilon)+\\exp\\left(\\mathcal{O}(L(d+m_{2}^{2}))\\right)\\cdot\\left(1-\\frac{\\rho^{2}}{4}\\cdot\\tau\\right)^{s}+\\tilde{\\mathcal{O}}\\left(\\frac{L d^{1/2}\\epsilon_{\\mathrm{score}}}{\\rho}\\right)+\\tilde{\\mathcal{O}}\\left(\\frac{L\\epsilon_{\\mathrm{energy}}}{\\rho\\tau^{1/2}}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\rho$ is the Cheeger constant of a truncated inner target distribution $\\exp(-g(z))\\mathbf{1}[z\\in B(\\mathbf{0},R)]$ and $Z$ denotesthemaximal $l_{2}$ norm of particles appearing in outer loops (Alg. 1). ", "page_idx": 6}, {"type": "text", "text": "It should be noted that the choice of $\\eta$ choice ensures the $L$ strong log-concavity of target distribution $\\exp(-g(z))$ , which means its Cheeger constant is also $L$ . Although the Cheeger constant $\\rho$ in the second term of Eq. 9 corresponding to truncated $\\exp(-g(z))$ should also be near $L$ intuitively, current techniques can only provide a loose lower bound at an $O(\\sqrt{L/d})$ -level (proven in Corollary C.8). While in both cases above, the Cheeger constant is independent with $\\epsilon$ . Combining this fact with an $\\epsilon_{}$ -independent choice of inner step sizes $\\tau$ , the second term of Eq. 9 will converge linearly with respect to $\\epsilon$ . As for the diameter $Z$ of particles used to upper bound $\\tau$ , though it may be unbounded in the standard implementation of Alg. 2, Lemma C.18 can upper bound it with $\\tilde{\\mathcal{O}}\\left({\\dot{L}}^{3/2}(d+m_{2}^{2})\\rho^{-1}\\right)$ under the projected version of Alg. 2. ", "page_idx": 6}, {"type": "text", "text": "Additionally, to require the final sampling error to satisfy $\\mathrm{TV}\\left(\\hat{p}_{K\\eta},p_{*}\\right)\\le\\tilde{\\mathcal{O}}(\\epsilon)$ , Eq. 9 shows that the score and energy difference estimation errors should be $\\epsilon_{}$ -dependent and sufficiently small, where $\\epsilon_{\\mathrm{score}}$ corresponding to the training loss can be well-controlled. However, obtaining a highly accurate energy difference estimation (requiring a small $\\epsilon_{\\mathrm{{energy}}},$ is hard with only diffusion models. To solve this problem, we can introduce a neural network energy estimator similar to [37] to construct $r(z^{\\prime},z,t)$ , which induces the following complexity describing the calls of the score estimation. ", "page_idx": 6}, {"type": "text", "text": "Corollary 4.2 (Informal version of Corollary C.19). Suppose the estimation errors of score and energydifferencesatisfy ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\epsilon_{\\mathrm{score}}\\leq\\frac{\\rho\\epsilon}{L d^{1/2}}\\quad\\mathrm{and}\\quad\\epsilon_{\\mathrm{energy}}\\leq\\frac{\\rho\\epsilon}{L^{2}\\cdot(d^{1/2}+m_{2}+Z)},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "1: Input: Returned particle of the previous iteration $\\pmb{x}_{0}$ , current iteration number $k$ , inner iteration number $S$ , inner step size $\\tau$ , velocity diffusion coefficient $\\gamma$ , required convergence accuracy $\\epsilon$   \n2: Initialize the particle and velocity pair, i.e., $(\\hat{z}_{0},\\hat{v}_{0})$ with a Gaussian type product measure, i.e., $\\mathcal{N}(\\mathbf{0},e^{2\\eta}-\\Bar{1})\\otimes\\mathcal{N}(\\mathbf{0},I)$   \n3: for $t=s$ to $S-1$ do   \n4: Draw noise sample pair $(\\xi_{s}^{z},\\xi_{s}^{v})$ from a Gaussian type distribution.   \n5: $\\hat{z}_{s+1}=\\hat{z}_{s}+\\gamma^{-1}(\\bar{1}-e^{-\\bar{\\gamma}\\tau})\\bar{\\hat{v}}_{s}^{'}-\\gamma^{-1}(\\tau-\\gamma^{-1}(\\bar{1}-e^{-\\gamma\\tau}))s_{\\theta}(\\hat{z}_{s})+\\xi_{s}^{z}$   \n6: $\\hat{v}_{s+1}=e^{-\\gamma\\tau}\\hat{v}_{s}-\\gamma^{-1}(1-e^{-\\gamma\\tau})s_{\\theta}(\\hat{z}_{s})+\\xi_{t}^{v}$ ", "page_idx": 7}, {"type": "text", "text": "7: return $z_{S}$ .\uff0c", "page_idx": 7}, {"type": "text", "text": "If we implement Alg. 1 with the projected version of Alg. 2 with the same hyperparameter settings as Theorem 4.1, it has TV $\\left(\\hat{p}_{K\\eta},p_{*}\\right)\\le\\tilde{\\mathcal{O}}(\\epsilon)$ with an $\\mathcal{O}(\\bar{L}^{4}\\rho^{-2}\\cdot\\left(d+m_{2}^{2}\\right)^{2}\\bar{Z}^{2}\\cdot\\mathrm{log}(d/\\epsilon))$ complexity. Considering the loose bound for both $\\rho$ and $Z$ , the complexity will be at most ${\\tilde{\\mathcal{O}}}(L^{5}(d+m_{2}^{2})^{6})$ which is the first linear convergence w.r.t. $\\epsilon$ result for the diffusion inference process. ", "page_idx": 7}, {"type": "text", "text": "Score-only RTK-MALA. However, the parametric energy function may not always exist in real practice. We consider a more practical case where only the score estimation is accessible. In this case, we will make use of estimated score functions to approximate the energy difference, leading to the score-only RTK-MALA algorithm. In particular, recall that the energy difference function takes the following form: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\iota(z^{\\prime})-g(z)=-\\log p_{(K-k-1)\\eta}(z^{\\prime})+\\frac{\\left\\|x_{0}-z^{\\prime}\\cdot e^{-\\eta}\\right\\|^{2}}{2\\big(1-e^{-2\\eta}\\big)}+\\log p_{(K-k-1)\\eta}(z)-\\frac{\\left\\|x_{0}-z\\cdot e^{-\\eta}\\right\\|^{2}}{2\\big(1-e^{-2\\eta}\\big)}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Since the quadratic term can be obtained exactly, we only need to estimate the energy difference. Then let $f(z)=-\\log p_{(K-k-1)\\eta}(z)$ anddenote $h(t)=f\\left((z^{\\prime}-z)\\cdot t+z\\right)$ , the energy difference $g(z^{\\prime})-g(z)$ can be reformulated as ", "page_idx": 7}, {"type": "equation", "text": "$$\nh(1)-h(0)=\\sum_{i=1}\\frac{h^{(i)}(0)}{i!}\\quad\\mathrm{and}\\quad h^{(i)}(t):=\\frac{\\mathrm{d}^{i}h(t)}{(\\mathrm{d}t)^{i}},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where we perform the standard Taylor expansion at the point $t=0$ . Then, we only need the derives of $h^{i}(0)$ , which can be estimated using only the score function. For instance, the $h^{(1)}(t)$ canbe estimated with score estimations: ", "page_idx": 7}, {"type": "equation", "text": "$$\nh^{(1)}(t)=\\nabla f((z^{\\prime}-z)\\cdot t+z)\\cdot(z^{\\prime}-z)\\approx\\tilde{h}^{(1)}(t):=s_{\\theta}((z^{\\prime}-z)\\cdot t+z)\\cdot(z^{\\prime}-z).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Moreover, regarding the high-order derivatives, we can recursively perform the approximation: $\\tilde{h}^{(i+1)}(0)\\ =\\ \\tilde{(h^{(i)}(\\bar{\\Delta}t)-\\tilde{h}^{(i)}(0))}/\\Delta t$ Consider performing the approximation up to $u$ -order derivatives, we can get the approximation of the energy difference: ", "page_idx": 7}, {"type": "equation", "text": "$$\nr_{(K-k-1)\\eta}(z^{\\prime},z):=\\sum_{i=1}^{u}\\frac{\\tilde{h}^{(i)}(0)}{i!}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Then, the following corollary states the complexity of the score-only RTK-MALA algorithm. ", "page_idx": 7}, {"type": "text", "text": "Corollary 4.3. Suppose the estimation errors of the score satisfies Escore $\\ll\\rho\\epsilon/(L d^{1/2})$ , and the log-likelihood function of $\\mathit{\\Delta}_{p_{t}}$ has a bounded $u$ -order derivative, e.g., $\\left\\|\\nabla^{(u)}f(z)\\right\\|\\leq L_{\\cdot}$ we have a nonparametric estimation for log-likelihood to make we have TV $^{\\7}\\left(\\hat{p}_{K\\eta},p_{*}\\right)\\le\\tilde{\\mathcal{O}}(\\epsilon)$ with a complexity shown as follows ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{O}}\\left(L^{4}\\rho^{-3}\\cdot\\left(d+m_{2}^{2}\\right)^{2}Z^{3}\\cdot\\epsilon^{-2/(u-1)}\\cdot2^{u}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "This result implies that if the energy function is infinite-order Lipschitz, we can nearly achieve any polynomial order convergence w.r.t. $\\epsilon$ with the non-parametric energy difference estimation. ", "page_idx": 7}, {"type": "text", "text": "RTK-ULD. Alg. 3 presents a solution employing ULD for the inner loop, which can accelerate the convergence of the inner loop due to the better discretization of the ULD algorithm. When it is used ", "page_idx": 7}, {"type": "table", "img_path": "C2xCLze1kS/tmp/983644aafe7ae1b5d06d3d94cdb50cbf0fbca3d13d2dedf7d1a40334993b56db.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparison with prior works for RTK-based methods. The complexity denotes the number of calls for the score estimation to achieve TV $\\left.\\left(\\hat{p}_{K\\eta},p_{*}\\right)\\le\\tilde{\\mathcal{O}}(\\epsilon)\\right.$ $d$ and $\\epsilon$ mean the dimension and error tolerance. Compared with the state-of-the-art result, RTK-ULD achieves the best dependence for both $d$ and $\\epsilon$ .Though RTK-MALA requires slightly stricter assumptions and worse dimension dependence, a linear convergence w..t. $\\epsilon$ makes it suit high-accuracy sampling tasks. "], "page_idx": 8}, {"type": "text", "text": "tosolvethe $k$ -th sampling subproblem of Alg. 1, $\\pmb{x}_{0}$ isequal to $\\hat{\\mathbf{x}}_{k\\eta}$ defined in Section 3 and used to initialize particles iterating in Alg. 2. Besides, the underlying distribution of noise sample pair is ", "page_idx": 8}, {"type": "equation", "text": "$$\n(\\xi_{s}^{z},\\xi_{s}^{v})\\sim\\mathcal{N}\\left(\\mathbf{0},\\left[\\overset{2}{\\gamma}\\left(\\tau-\\frac{2}{\\gamma}\\left(1-e^{-\\gamma\\tau}\\right)\\right)+\\frac{1}{2\\gamma}\\left(1-e^{-2\\gamma\\tau}\\right)\\quad\\overset{1}{\\gamma}\\left(1-2e^{-\\gamma\\tau}+e^{-2\\gamma\\tau}\\right)\\right]\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "In Alg. 3, we consider the process $\\{(\\hat{\\mathbf{z}}_{s},\\hat{\\mathbf{v}}_{s})\\}_{s=0}^{S}$ whose underlying distribution is denoted as $\\hat{\\{\\pi}_{s}\\}_{s=0}^{S}$ We expect the $_{z}$ marginal distribution of $\\hat{\\pi}_{S}$ to be close to the target distribution presented in Eq. 8. Unlike MALA, we only need to consider the error from score estimation in an expectation perspective, which is the same as that shown in [8]. ", "page_idx": 8}, {"type": "text", "text": "[E3] Estimation error of the score function: we assume a score estimator, e.g., a well-trained diffusion model, $s_{\\theta}$ , which can approximate the score function with an $\\epsilon_{\\mathrm{score}}$ error, i.e., $\\mathbb{E}_{p_{t}}\\left\\|s_{\\theta,t}(z)-\\nabla\\log p_{t}(z)\\right\\|^{2}\\leq\\epsilon_{\\mathrm{score}}^{2}$ forany $t\\in[0,T]$ ", "page_idx": 8}, {"type": "text", "text": "Under this condition, the complexity of RTK-ULD to achieve the convergence of TV distance is provided as follows, and the detailed proof is deferred to Theorem D.6. Besides, we compare our theoretical results with the previous in Table 1. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.4. Under Assumptions [A1]-[A2] and [E3], for Alg. 1, we choose ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\eta=1/2\\cdot\\log[(2L+1)/2L]\\quad\\mathrm{and}\\quad K=4L\\cdot\\log[((1+L^{2})d+\\|\\nabla f_{*}(\\mathbf{0})\\|^{2})^{2}\\cdot\\epsilon^{-2}]\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "andimplementStep $^3$ of Alg. 1 with projected Alg. 3. For the $k$ -th run of Alg. 3, we require Gaussian-type initialization and high-accurate score estimation, i.e., ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\hat{\\pi}_{0}=\\mathcal{N}(\\pmb{x}_{0},e^{2\\eta}-1)\\otimes\\mathcal{N}(\\mathbf{0},\\pmb{I})\\quad\\mathrm{and}\\quad\\epsilon_{\\mathrm{score}}=\\tilde{\\mathcal{O}}(\\epsilon/\\sqrt{L}).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "If we set the hyperparameters of inner loops as follows. the step size and the iteration number as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\tau=\\tilde{\\Theta}\\left(\\epsilon d^{-1/2}L^{-1/2}\\cdot\\left(\\log\\left[\\frac{L(d+m_{2}^{2}+\\|{\\pmb x}_{0}\\|^{2})}{\\epsilon^{2}}\\right]\\right)^{-1/2}\\right)}\\\\ {\\displaystyle S=\\tilde{\\Theta}\\left(\\epsilon^{-1}d^{1/2}\\cdot\\left(\\log\\left[\\frac{L(d+m_{2}^{2}+\\|{\\pmb x}_{0}\\|^{2})}{\\epsilon^{2}}\\right]\\right)^{1/2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "It can achieve $\\mathrm{TV}\\left(\\hat{p}_{K\\eta},p_{*}\\right)\\lesssim\\epsilon$ with an $\\tilde{\\mathcal{O}}\\left(L^{2}d^{1/2}\\epsilon^{-1}\\right)$ gradient complexity. ", "page_idx": 8}, {"type": "text", "text": "5  Conclusion and Limitation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This paper presents an analysis of a modified version of diffusion models. Instead of focusing on the discretization of the reverse SDE, we propose a general RTK framework that can produce a large class of algorithms for diffusion inference, which is formulated as solving a sequence of RTK sampling subproblems. Given this framework, we develop two algorithms called RTK-MALA and RTK-ULD, which leverage MALA and ULD to solve the RTK sampling subproblems. We develop theoretical guarantees for these two algorithms under certain conditions on the score estimation, and demonstrate their faster convergence rate than prior works. Numerical experiments support our theory. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "We would also like to point out several limitations and future work. One potential limitation of this work is the lack of large-scale experiments. The main focus of this paper is the theoretical understanding and rigorous analysis of the diffusion process. Implementing large-scale experiments requires GPU resources and practitioner support, which can be an interesting direction for future work. Besides, though we provided a score-only RTK-MALA algorithm, the $\\tilde{\\mathcal{O}}(1/\\epsilon)$ convergence rate can only be achieved by the RTK-MALA algorithm (Alg. 2). However, this faster algorithm requires a direct approximation of the energy difference, which is not accessible in the existing pretrained diffusion model. Developing practical energy difference approximation algorithms and incorporating them with Alg. 2 for diffusion inference are also very interesting future directions. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The research is partially supported by the NSF awards: SCALE MoDL-2134209, CCF-2112665 (TILOS). It is also supported, DARPA AIE program, the U.S. Department of Energy, Offce of Science, the Facebook Research Award, as well as CDC-RFA-FT-23-0069 from the CDC's Center for Forecasting and Outbreak Analytics. Difan Zou is supported in part by Guangdong NSF 2024A1515012444, NSFC 62306252, and the central fund from HKU IDS. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1]  Jason M Altschuler and Sinho Chewi.  Faster high-accuracy log-concave sampling via algorithmic warm starts. In 2023 IEEE 64th Annual Symposium on Foundations of Computer Science (FOCS), pages 2169-2176. IEEE, 2023.   \n[2] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981-17993, 2021.   \n[3] Joe Benton, Valentin De Bortoli, Arnaud Doucet, and George Deligiannidis. Nearly $d$ -linear convergence bounds for diffusion models via stochastic localization. In The Twelfth International Conference on Learning Representations, 2024. URL https : //openreview .net /forum? id=r5njV3BsuD.   \n[4]  Adam Block, Youssef Mroueh, and Alexander Rakhlin. Generative modeling with denoising auto-encoders and Langevin sampling. arXiv preprint arXiv:2002.00107, 2020.   \n[5]  Nicholas M. Boffi and Eric Vanden-Eijnden. Probability flow solution of the Fokker-Planck equation, 2023.   \n[6] Peter Buser. A note on the isoperimetric constant. In Annales scientifiques de l'Ecole normale sup\u00e9rieure, volume 15, pages 213-230, 1982.   \n[7]  Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions. In International Conference on Machine Learning, pages 4735-4763. PMLR, 2023.   \n[8]  Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. In International Conference on Learning Representations, 2023.   \n[9] Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu, and Adil Salim. The probability flow ODE is provably fast. Advances in Neural Information Processing Systems, 36, 2024.   \n[10] Yongxin Chen, Sinho Chewi, Adil Salim, and Andre Wibisono. Improved analysis for a proximal algorithm for sampling. In Conference on Learning Theory, pages 2984-3014. PMLR, 2022. ", "page_idx": 9}, {"type": "text", "text": "[11]  Xiang Cheng and Peter Bartlett. Convergence of langevin mcmc in kl-divergence. In Algorithmic Learning Theory, pages 186-211. PMLR, 2018. ", "page_idx": 10}, {"type": "text", "text": "[13] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780-8794, 2021.   \n[14] Hanze Dong, Xi Wang, LIN Yong, and Tong Zhang. Particle-based variational inference with preconditioned functional gradient fow. In The Eleventh International Conference on Learning Representations, 2023. URL https : //openreview.net/forum?id $\\equiv$ 60phWWAE3cS.   \n[15] Raaz Dwivedi, Yuansi Chen, Martin J Wainwright, and Bin Yu. Log-concave sampling: Metropolis-Hastings algorithms are fast. Journal of Machine Learning Research, 20(183):1-42, 2019.   \n[16]  Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840-6851, 2020.   \n[17] Xunpeng Huang, Hanze Dong, Yifan Hao, Yian Ma, and Tong Zhang. Monte Carlo sampling without isoperimetry: A reverse diffusion approach, 2023.   \n[18] Xunpeng Huang, Hanze Dong, Yifan HAO, Yian Ma, and Tong Zhang. Reverse diffusion monte carlo. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $=$ kIPEyMSdFV.   \n[19] Holden Lee, Andrej Risteski, and Rong Ge. Beyond log-concavity: Provable guarantees for sampling multi-modal distributions using simulated tempering Langevin Monte Carlo. Advances in neural information processing systems, 31, 2018.   \n[20] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with polynomial complexity. arXiv preprint arXiv:2206.06227, 2022.   \n[21]  Yin Tat Lee and Santosh S Vempala. Convergence rate of Riemannian Hamiltonian Monte Carlo and faster polytope volume computation. In Proceedings of the 5Oth Annual ACM SIGACT Symposium on Theory of Computing, pages 1115-1121, 2018.   \n[22]  Yin Tat Lee and Santosh Srinivas Vempala. Eldan's stochastic localization and the KLS hyperplane conjecture: an improved lower bound for expansion. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 998-1007. IEEE, 2017.   \n[23] Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Towards non-asymptotic convergence for diffusion-based generative models. In The Twelfth International Conference on Learning Representations, 2023.   \n[24] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:5775-5787, 2022.   \n[25] Yi-An Ma, Niladri S Chatterji, Xiang Cheng, Nicolas Flammarion, Peter L Bartlett, and Michael I Jordan. Is there an analog of Nesterov acceleration for gradient-based MCMC? Bernoulli, 27(3), 2021.   \n[26]  adim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Gradtts: A diffusion probabilistic modelfor text-to-speech. In International Conference on Machine Learning, pages 8599-8608. PMLR, 2021.   \n[27]  Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arxiv 2022. arXiv preprint arXiv:2204.06125, 2022.   \n[28] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:36479-36494, 2022.   \n[29]  Ohad Shamir. A variant of azuma's inequality for martingales with subgaussian tails. arXiv preprint arXiv:1110.2392, 2011.   \n[30] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.   \n[31]  Yang Song and Stefano Ermon.  Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.   \n[32] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International ConferenceonLearningRepresentations,2020.   \n[33] Brian L. Trippe, Jason Yim, Doug Tischer, David Baker, Tamara Broderick, Regina Barzilay, and Tommi S. Jaakkola. Diffusion probabilistic modeling of protein backbones in 3d for the motifscaffolding problem. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id $\\equiv$ 6TxBxqNME1Y.   \n[34]  Santosh Vempala and Andre Wibisono. Rapid convergence of the unadjusted langevin algorithm: Isoperimetry suffices. Advances in neural information processing systems, 32, 2019.   \n[35] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computation, 23(7):1661-1674, 2011.   \n[36] Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo design of protein structure and function with rfdiffusion. Nature, 620(7976):1089-1100, 2023.   \n[37]  Xingyu Xu and Yuejie Chi.  Provably robust score-based diffusion posterior sampling for plug-and-play image reconstruction. arXiv preprint arXiv:2403.17042, 2024.   \n[38] Dinghuai Zhang, Ricky T. Q. Chen, Cheng-Hao Liu, Aaron Courville, and Yoshua Bengio. Diffusion generative flow samplers: Improving learning signals through partial trajectory optimization. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=0Isahq1UYC.   \n[39] Shunshi Zhang, Sinho Chewi, Mufan Li, Krishna Balasubramanian, and Murat A Erdogdu. Improved discretization analysis for underdamped Langevin Monte Carlo. In The Thirty Sixth Annual Conference on Learning Theory, pages 36-71. PMLR, 2023.   \n[40] Difan Zou, Pan Xu, and Quanquan Gu. Faster convergence of stochastic gradient langevin dynamics for non-log-concave sampling. In Uncertainty in Artificial Intelligence, pages 1152-- 1162. PMLR, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "1Introduction ", "page_idx": 12}, {"type": "text", "text": "2  Preliminaries 3   \n3 General Framework of Reverse Transition Kernel 4   \n4  Implementation of RTK inner loops 6   \n5  Conclusion and Limitation 9   \nA  Numerical Experiments 14   \nB  Inference process with reverse transition kernel framework 17   \nC Implement RTK inference with MALA 24   \nC.1 Control the error from the projected transition kernel 27   \nControl the error from the approximation of score and energy 31   \nC.3 Control the error from Inner MALA to its stationary 35   \nC.3.1 The Cheeger isoperimetric inequality of $\\tilde{\\mu}_{*}$ 36   \nC.3.2 The conductance properties of $\\tilde{\\mathcal{T}}_{*}$ 37   \nC.4  Main Theorems of InnerMALA implementation 42   \nC.5 Control the error from Energy Estimation 47   \nD  Implement RTK inference with ULD 50 ", "page_idx": 12}, {"type": "text", "text": "E  Auxiliary Lemmas 56 ", "page_idx": 12}, {"type": "text", "text": "A Numerical Experiments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we conduct experiments when the target distribution $p_{*}$ is a Mixture of Gaussian (MoG) and compare RTK-based methods with traditional DDPM. Specifically, we are considering a forward process from an MoG distribution to a normal distribution in the following ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}_{t}=-\\frac{1}{2}\\mathbf{x}_{t}\\mathrm{d}t+\\mathrm{d}\\mathbf{}B_{t}\\quad\\mathrm{and}\\quad\\mathbf{x}_{0}\\sim\\frac{1}{K}\\sum_{k=1}^{K}\\mathcal{N}(\\boldsymbol{\\mu}_{k},\\boldsymbol{\\sigma}_{k}^{2}\\cdot\\boldsymbol{I}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $K$ is the number of Gaussian components, $\\pmb{\\mu}_{k}$ and $\\sigma_{k}^{2}$ are the means and variances of the Gaussian components, respectively. The solution of the SDE follows ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t}=\\mathbf{x}_{0}e^{-\\frac{1}{2}t}+{\\sqrt{1-e^{-t}}}\\cdot\\xi\\quad{\\mathrm{where}}\\quad\\xi\\sim{\\mathcal{N}}(0,I).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since $\\mathbf{x}_{\\mathrm{0}}$ and $\\xi$ are both sampled from Gaussian distributions, their linear combination $\\mathbf{x}_{t}$ alsoforms a Gaussian distribution, i.e., ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t}\\sim\\frac{1}{K}\\sum_{k=1}^{K}\\mathcal{N}(\\pmb{\\mu}_{k}e^{-\\frac{1}{2}t},(\\sigma_{k}^{2}e^{-t}+1-e^{-t})\\cdot\\pmb{I}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\begin{array}{l}{\\displaystyle\\nabla p(\\pmb{x}_{t})={\\frac{1}{K}}\\sum_{i=1}^{K}\\nabla_{\\pmb{x}_{t}}\\left[{\\frac{1}{2}}({\\frac{1}{\\sqrt{2\\pi}(\\sigma_{i}^{2}e^{-t}+1-e^{-t})}})\\cdot\\exp(-{\\frac{1}{2}}({\\frac{x_{t}-\\mu_{i}e^{-{\\frac{1}{2}}t}}{\\sigma_{i}^{2}e^{-t}+1-e^{-t}}})^{2})\\right]}\\\\ {\\displaystyle={\\frac{1}{K}}\\sum_{i=1}^{K}p_{i}(\\pmb{x}_{t})\\cdot\\nabla_{\\pmb{x}_{t}}\\left[-{\\frac{1}{2}}({\\frac{x_{t}-\\mu_{i}e^{-{\\frac{1}{2}}t}}{\\sigma_{k}^{2}e^{-t}+1-e^{-t}}})^{2}\\right]}\\\\ {\\displaystyle={\\frac{1}{K}}\\sum_{i=1}^{K}p_{i}(\\pmb{x}_{t})\\cdot{\\frac{-(\\pmb{x}_{t}-\\mu_{i}e^{-{\\frac{1}{2}}t})}{\\sigma_{i}^{2}e^{-t}+1-e^{-t}}}.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We can also calculate the score of $\\pmb{x}_{t}$ , i.e., ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\nabla\\log p(\\pmb{x}_{t})=\\frac{\\nabla p(\\pmb{x}_{t})}{p(\\pmb{x}_{t})}=\\frac{1/K\\cdot\\sum_{i=1}^{K}p_{i}(\\pmb{x}_{t})\\cdot\\left(\\frac{-\\left(\\pmb{x}_{t}-\\pmb{\\mu}_{i}e^{-\\frac{1}{2}t}\\right)}{\\sigma_{i}^{2}e^{-t}+1-e^{-t}}\\right)}{1/K\\cdot\\sum_{i=1}^{K}p_{i}(\\pmb{x}_{t})}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We consider a MoG consisting of 12 Gaussian distributions, each with 10 dimensions, as shown in Fig. 2 (f). The means of the 12 Gaussian distributions are uniformly distributed along the circumference of a circle with a radius of one in the first and second dimensions, while the remaining dimensions are centered at the origin. Each component of the mixture has an equal probability and a variance of 0.oo7 across all dimensions. ", "page_idx": 13}, {"type": "text", "text": "We evaluate Alg. 1 with unadjust Langevin algorithm (ULA), which leads to RTK-ULA, Alg 2, 3 implementations, and DDPM under the same Number of Function Evaluations (NFE). Specifically, while DDPM models $\\mathbf{x}_{\\eta}$ across a sequence of $\\eta$ timesteps spanning from O to $T$ in increments of $0.001\\times T$ (i.e., $[0,0.00\\dot{1}T,0.002T,.\\;.\\;.\\;,T])$ , we execute Alg. 1, 2, and 3 at fewer timesteps within $\\mathbf{x}_{[0,0.2T,0.4T,0.6T,0.8T]}$ , and we distribute the NFE uniformly to these timesteps for MCMC. The experiments are taken on a single NVIDIA GeForce RTX 4090 GPU. We evaluate the sampling quality using marginal accuracy, i.e., ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{Marginal\\Accuracy}(\\hat{p},p)=1-0.5\\times\\frac{1}{d}\\sum_{i=1}^{d}T V(\\hat{p}_{i},p_{i}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\hat{p}_{i}(x)$ is the empirical marginal distribution of the $i$ -th dimension obtained from the sampled data, $p_{i}(x)$ is the true marginal distribution of the $i$ -th dimension, and $d$ is the total number of dimensions. ", "page_idx": 13}, {"type": "image", "img_path": "C2xCLze1kS/tmp/68c24d91e20eedcfb4765282c30b64dbfa37a3fac0228205034298b796f8b84a.jpg", "img_caption": ["Figure 1: (a) Mariginal accuracy of the sampled MoG by different algorithms along NFE. (b-f) The histograms along a certain direction of sampled MoG by different algorithms. The plots labeled by \u2018ULA', \u201cULD', \u201cMALA', \u201cMALA_ES' correspond to RTK-ULA, RTK-ULD, RTK-MALA, scoreonly RTK-MALA, respectively. The histogram is oriented along the second dimension when the first dimension is constrained within (0.75, 1.25). "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Fig. 1 (a) shows the marginal accuracies of our RTK sampling algorithms and DDPM along NFE. We observe that all algorithms using RTK converge quickly. Among all RTK algorithms, RTK-MALA achieves the highest marginal accuracy. Score-only RTK-MALA is worse than RTK-MALA since the estimated energy contains errors, yet it is still slightly better than RTK-ULD. Along all RTK algorithms, RTK-ULA demonstrates the lowest performance in terms of marginal accuracy, but it still outperforms DDPM with a large margin especially when NFE is small. ", "page_idx": 14}, {"type": "text", "text": "Fig. 1 (b-f) shows the histograms of sampled MoG by DDPM and RTK-based methods. We observe that DDPM cannot reconstruct the local structure of MoG. ULA can roughly reconstruct the MoG structure, but it is still weak in complex regions, specifically around the peaks and valleys. In contrast, RTK-ULD, score-only RTK-MALA, and RTK-MALA can reconstruct more fine-grained structures in complex regions. ", "page_idx": 14}, {"type": "text", "text": "Fig. 2 (a-e) shows the clusters sampled by DDPM and RTK-based methods. We observe that DDPM fails to accurately reconstruct the ground truth distribution. In contrast, all methods based on RTK can generate distributions that closely approximate the ground truth. Additionally, RTK-MALA shows superior performance in accurately reconstructing the distribution in regions of low probability. ", "page_idx": 14}, {"type": "text", "text": "We perform other experiments on various MoG settings.Figure 3 and 4 shows the experiments on a spiral-shaped and chessboard-shaped MoG as the ground truth distribution, respectively. In these experiments, we also compared Annealed Langevin Dynamics (ALD) [] with our RTK-based method. In these figures, we observe that compared to DDPM and ALD, our RTK-based methods achieve significantly better marginal accuracy when NFE is small. Besides, we find that our RTK-based methods have a much better estimation on low-probability region. Furthermore, in Figure 5, we evaluated the methods using the Wasserstein Distance metric, which corresponds to Fr\u00e9chet Inception Distance. Our results indicate that our RTK-based methods have a lower Wasserstein Distance compared to DDPM and ALD, especially when NFE is small. ", "page_idx": 14}, {"type": "text", "text": "Furthermore, we conducted experiments on the MNIST dataset, as shown in Figure 6. We first trained a score model following the typical variance-preserving noise schedule and then compared different sampling methods using the Fr\u00e9chet Inception Distance (FID) evaluation criterion. Figure 6 (a) shows that compared with DDPM, our RTK-based methods achieve better FID scores than DDPM, particularly when NFE is small. Figure 6 (b) shows that our RTK-based methods generate images of higher quality than DDPM when NFE is small. ", "page_idx": 14}, {"type": "image", "img_path": "C2xCLze1kS/tmp/fd54441215c285f87d0d675ccf0ffb4d93fad8bb9bf6095a61f295d808a29678.jpg", "img_caption": ["Figure 2: (a-e) Clusters sampled by DDPM, RTK-ULA, RTK-ULD, score-only RTK-MALA, and RTK-MALA, respectively. (f) Clusters sampled by the ground truth distribution. These $2D$ clusters represent the projection of the original $10D$ data onto the first two dimensions. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "C2xCLze1kS/tmp/f88b878146a93094c501e544fd168ed75c7a99d70cf3a39f7743338ab9fa8769.jpg", "img_caption": ["Figure 3: (a) Ground truth clusters sampled by a spiral-shaped MoG distribution. $\\mathbf{(b-g)}$ Clusters sampled using 205 NFE by DDPM, ALD, ULA, ULD, MALA_ES, and MALA, respectively. (h) Marginal accuracy of the sampled MoG by different algorithms along NFE. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Overall, these numerical experiments demonstrate the benefit of the RTK framework for developing faster algorithms than DDPM in diffusion inference. Besides, experimental results also well support our theory, showing that RTK-MALA achieves faster convergence than RTK-ULA and RTK-ULD, even with estimated energy difference via score functions. ", "page_idx": 15}, {"type": "image", "img_path": "C2xCLze1kS/tmp/c0dba4c3ade0406f6486880f4f0ed77a5f26ef7f1500165e4c7be467b3e4ee04.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 4: (a) Ground truth clusters sampled by a chessboard-shaped MoG distribution. (b-g) Clusters sampled using 205 NFE by DDPM, ALD, ULA, ULD, MALA_ES, and MALA, respectively. (h) Marginal accuracy of the sampled MoG by different algorithms along NFE. ", "page_idx": 16}, {"type": "image", "img_path": "C2xCLze1kS/tmp/1ea8c354814c208fbd1f43be2b89bd3913b3c544fd9264f35b0c06b057e5482a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 5: Wasserstein distance of the sampled MoG by different algorithms along NFE. (a) and (b) are the Wasserstein distance plot for spiral-shaped and chessboard shaped MoG distribution, respectively. ", "page_idx": 16}, {"type": "text", "text": "B  Inference process with reverse transition kernel framework ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 3.1. According to Bayes theorem, the following equation should be validated for any $\\dot{\\mathbf{x}}\\in\\mathbb{R}^{d}$ and $t^{\\prime}>t$ ", "page_idx": 16}, {"type": "equation", "text": "$$\np_{t}(\\pmb{x})=\\int p_{t|t^{\\prime}}(\\pmb{x}|\\pmb{x}^{\\prime})\\cdot p_{t^{\\prime}}(\\pmb{x}^{\\prime})\\mathrm{d}\\pmb{x}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To simplify the notation, we suppose the normalizing constant of $p_{t}$ ,i.e., ", "page_idx": 16}, {"type": "equation", "text": "$$\nZ_{t}:=\\int\\exp(-f_{t}(\\pmb{x}))\\mathrm{d}\\pmb{x}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Besides, the forward OU process, i.e., SDE. 1, has a closed transition kernel, i.e., ", "page_idx": 16}, {"type": "equation", "text": "$$\np_{t^{\\prime}|t}(\\pmb{x}^{\\prime}|\\pmb{x})=\\left(2\\pi\\left(1-e^{-2(t^{\\prime}-t)}\\right)\\right)^{-d/2}\\cdot\\exp\\left[\\frac{-\\left\\|\\pmb{x}^{\\prime}-e^{-(t^{\\prime}-t)}\\pmb{x}\\right\\|^{2}}{2\\left(1-e^{-2(t^{\\prime}-t)}\\right)}\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{t^{\\prime}}(\\pmb{x}^{\\prime})=\\displaystyle\\int p_{t}(\\pmb{y})p_{t^{\\prime}|t}(\\pmb{x}^{\\prime}|\\pmb{y})\\mathrm{d}\\pmb{y}}\\\\ &{\\qquad=\\displaystyle\\int Z_{t}^{-1}\\cdot\\exp(-f_{t}(\\pmb{y}))\\cdot\\left(2\\pi\\left(1-e^{-2(t^{\\prime}-t)}\\right)\\right)^{-d/2}\\cdot\\exp\\left[\\frac{-\\left\\|\\pmb{x}^{\\prime}-e^{-(t^{\\prime}-t)}\\pmb{y}\\right\\|^{2}}{2\\left(1-e^{-2(t^{\\prime}-t)}\\right)}\\right]\\mathrm{d}\\pmb{y}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "image", "img_path": "C2xCLze1kS/tmp/f261067fea20393e9f00097339167777ce5998989b3874a012bd765f3a604bab.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 6: (a) FID of the sampled MNIST data by different algorithms along NFE. (b-d) Sampled MNIST data by ULD, score-only RTK-MALA, and DDPM respectively, when NFE is 20. ", "page_idx": 17}, {"type": "text", "text": "Plugging this equation into Eq. 10, and we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{3}q,\\,\\mathrm{10}=\\displaystyle\\int p_{t|t^{\\prime}}(\\pmb{x}|\\pmb{x}^{\\prime})\\cdot p_{t^{\\prime}}(\\pmb{x}^{\\prime})\\mathrm{d}\\pmb{x}^{\\prime}}\\\\ &{\\qquad=\\displaystyle\\int p_{t|t^{\\prime}}(\\pmb{x}|\\pmb{x}^{\\prime})\\cdot\\int Z_{t}^{-1}\\cdot\\exp(-f_{t}(\\pmb{y}))\\cdot\\left(2\\pi\\left(1-e^{-2(t^{\\prime}-t)}\\right)\\right)^{-d/2}\\cdot\\exp\\left[\\frac{-\\left\\|\\pmb{x}^{\\prime}-e^{-(t^{\\prime}-t)}\\pmb{y}\\right\\|^{2}}{2\\left(1-e^{-2(t^{\\prime}-t)}\\right)}\\right]<\\pmb{x}^{\\prime}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Moreover, when we plug the reverse transition kernel ", "page_idx": 17}, {"type": "equation", "text": "$$\np_{t|t^{\\prime}}(\\pmb{x}|\\pmb{x}^{\\prime})\\propto\\exp\\left(-f_{t}(\\pmb{x})-\\frac{\\left|\\left|\\pmb{x}^{\\prime}-\\pmb{x}\\cdot e^{-(t^{\\prime}-t)}\\right|\\right|^{2}}{2(1-e^{-2(t^{\\prime}-t)})}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "into the previous equation and have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{q_{4}\\,10=\\int_{\\frac{\\exp\\left(-f_{i}(x)-\\frac{\\left|e^{-\\pi\\,e^{-\\pi\\,e^{-\\pi\\,e^{-\\pi}}}}\\right|^{2}}{2(1-e^{-\\pi\\,e^{-\\pi\\,e^{-\\pi}}})})}}}\\,\\mathrm{d}x}\\,}&{}\\\\ &{\\,\\int_{\\frac{\\exp\\left(-f_{i}(x)-\\frac{\\left|e^{-\\pi\\,e^{-\\pi\\,e^{-\\pi\\,e^{-\\pi}}}}\\right|^{2}}{2(1-e^{-\\pi\\,e^{-\\pi\\,e^{-\\pi}}})}\\right)}}{\\int_{\\frac{\\exp\\left(-f_{i}(x)\\right)}{2}\\cdot\\left(2\\pi\\right)^{3}\\cdot\\left(2\\pi\\right)^{4}}\\cdot\\exp\\left\\{\\frac{-\\left\\|e^{-\\pi\\,e^{-\\pi\\,e^{-\\pi}}}\\right\\|^{2}}{2\\left(1-e^{-\\pi\\,e^{-\\pi}}\\right)}\\right\\}\\mathrm{d}y}\\mathrm{d}z^{\\prime}}\\\\ &{=\\frac{f_{-1}^{-1}\\cdot\\exp\\left(-f_{i}(x)\\right)\\cdot\\int_{\\exp\\left(-\\frac{\\left\\|e^{-\\pi\\,e^{-\\pi\\,e^{-\\pi}}}\\right\\|^{2}}{2(1-e^{-\\pi\\,e^{-\\pi}})}\\right)}}{\\int_{\\frac{\\exp\\left(-f_{i}(x)-\\frac{\\left\\|e^{-\\pi\\,e^{-\\pi}}\\right\\|^{2}}{2(1-e^{-\\pi\\,e^{-\\pi}})}\\right)}{2\\left(1-e^{-\\pi\\,e^{-\\pi}}\\right)}}\\mathrm{d}y}\\,\\mathrm{d}z^{\\prime}}\\\\ &{\\quad\\Bigg[\\int_{\\frac{\\exp\\left(-f_{i}(x)-\\frac{\\left\\|e^{-\\pi\\,e^{-\\pi\\,e^{-\\pi}}}\\right\\|^{2}}{2(1-e^{-\\pi\\,e^{-\\pi\\,e^{-\\pi}}})}\\right)}{\\int_{\\frac{\\exp\\left(-f_{i}(x)-\\frac{\\left\\|e^{-\\pi\\,e^{-\\pi}}\\right\\|^{2}}{2(1-e^{-\\pi\\,e^{-\\pi}})}\\right)}{2\\pi}\\mathrm{d}x}}\\mathrm{d}y\\Bigg]\\,\\mathrm{d}z^{\\prime}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, the proof is completed. ", "page_idx": 17}, {"type": "text", "text": "Lemma B.1 (Chain rule of TV). Consider four random variables, $\\mathbf{x},\\mathbf{z},\\tilde{\\mathbf{x}},\\tilde{\\mathbf{z}},$ whose underlying distributions are denoted as $p_{x},p_{z},q_{x},q_{z}$ .Suppose $p_{x,z}$ and $q_{x,z}$ denotes the densities of joint distributions of $(\\mathbf{x},\\mathbf{z})$ and $(\\Tilde{{\\mathbf x}},\\Tilde{{\\mathbf z}})$ ,which we write in terms of the conditionals and marginals as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{x,z}(\\pmb{x},z)=p_{x|z}(\\pmb{x}|z)\\cdot p_{z}(z)=p_{z|x}(z|\\pmb{x})\\cdot p_{x}(\\pmb{x})}\\\\ &{q_{x,z}(\\pmb{x},z)=q_{x|z}(\\pmb{x}|z)\\cdot q_{z}(z)=q_{z|x}(z|\\pmb{x})\\cdot q_{x}(\\pmb{x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "then we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{TV}\\left(p_{x,z},q_{x,z}\\right)\\leq\\operatorname*{min}\\left\\lbrace\\mathrm{TV}\\left(p_{z},q_{z}\\right)+\\mathbb{E}_{\\mathbf{z}\\sim p_{z}}\\left[\\mathrm{TV}\\left(p_{x|z}(\\cdot|\\mathbf{z}),q_{x|z}(\\cdot|\\mathbf{z})\\right)\\right],\\right.\\qquad}\\\\ {\\left.\\qquad\\qquad\\qquad\\mathrm{TV}\\left(p_{x},q_{x}\\right)+\\mathbb{E}_{\\mathbf{x}\\sim p_{x}}\\left[\\mathrm{TV}\\left(p_{z|x}(\\cdot|\\mathbf{x}),q_{z|x}(\\cdot|\\mathbf{x})\\right)\\right]\\right\\rbrace.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Besides, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(p_{x},q_{x}\\right)\\leq\\mathrm{TV}\\left(p_{x,z},q_{x,z}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. According to the definition of the total variation distance, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{(p_{x,z},q_{x,z})=\\frac{1}{2}\\int\\int|p_{x,z}(\\pmb{x},z)-q_{x,z}(\\pmb{x},z)|\\,\\mathrm{d}z\\mathrm{d}x}}\\\\ &{=\\frac{1}{2}\\int\\int\\left|p_{z}(z)p_{x|z}(\\pmb{x}|z)-p_{z}(z)q_{x|z}(\\pmb{x}|z)+p_{z}(z)q_{x|z}(\\pmb{x}|z)-q_{z}(z)q_{x|z}(\\pmb{x}|z)\\right|\\mathrm{d}z\\mathrm{d}x}\\\\ &{\\leq\\frac{1}{2}\\int p_{z}(z)\\int\\left|p_{x|z}(\\pmb{x}|z)-q_{x|z}(\\pmb{x}|z)\\right|\\mathrm{d}\\mathbf{x}\\mathrm{d}z+\\frac{1}{2}\\int|p_{z}(z)-q_{z}(z)|\\int q_{x|z}(\\pmb{x}|z)\\mathrm{d}\\mathbf{x}\\mathrm{d}z}\\\\ &{=\\mathbb{E}_{\\pm\\sim p_{z}}\\left[\\mathrm{TV}\\left(p_{x|z}(\\cdot|\\pmb{x}),q_{x|z}(\\cdot|\\pmb{x})\\right)\\right]+\\mathrm{TV}\\left(p_{z},q_{z}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "With a similar technique, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(p_{x,z},q_{x,z}\\right)\\leq\\mathrm{TV}\\left(p_{x},q_{x}\\right)+\\mathbb{E}_{\\mathbf{x}\\sim p_{x}}\\left[\\mathrm{TV}\\left(p_{z\\mid x}(\\cdot|\\mathbf{x}),q_{z\\mid x}(\\cdot|\\mathbf{x})\\right)\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence, the first inequality of this Lemma is proved. Then, for the second inequality, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathrm{TV}\\left(p_{x},q_{x}\\right)=\\frac{1}{2}\\int\\left|p_{x}(\\pmb{x})-q_{x}(\\pmb{x})\\right|\\mathrm{d}\\pmb{x}}\\\\ {\\displaystyle\\qquad\\qquad=\\frac{1}{2}\\int\\left|\\int p_{x,z}(\\pmb{x},z)\\mathrm{d}z-\\int q_{x,z}(\\pmb{x},z)\\mathrm{d}z\\right|\\mathrm{d}\\pmb{x}}\\\\ {\\displaystyle\\qquad\\qquad\\leq\\frac{1}{2}\\int\\int\\left|p_{x,z}(\\pmb{x},z)-q_{x,z}(\\pmb{x},z)\\right|\\mathrm{d}z\\mathrm{d}\\pmb{x}\\,=\\mathrm{TV}\\left(p_{x,z},q_{x,z}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence, the proof is completed. ", "page_idx": 18}, {"type": "text", "text": "Lemma B.2. For Alg $^{l}$ ,we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}\\left(\\hat{p}_{K\\eta},p_{*}\\right)\\leq\\!\\!\\sqrt{(1+L^{2})d+\\left\\Vert\\nabla f_{*}(\\mathbf{0})\\right\\Vert^{2}}\\cdot\\exp(-K\\eta)}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\displaystyle\\sum_{k=0}^{K-1}\\mathbb{E}_{\\hat{\\mathbf{x}}\\sim\\hat{p}_{k\\eta}}\\left[\\mathrm{TV}\\left(\\hat{p}_{(k+1)\\eta\\vert k\\eta}(\\cdot\\vert\\hat{\\mathbf{x}}),p_{(k+1)\\eta\\vert k\\eta}^{\\leftarrow}(\\cdot\\vert\\hat{\\mathbf{x}})\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for any $K\\in\\mathbb{N}_{+}$ and $\\eta\\in\\mathbb{R}_{+}$ ", "page_idx": 18}, {"type": "text", "text": "Proof. For any $k\\in\\{0,1,\\ldots,K-1\\}$ let $\\hat{p}_{(k+1)\\eta,k\\eta}$ and $p_{(k+1)\\eta,k\\eta}^{\\leftarrow}$ denote th join distribution of $(\\hat{\\mathbf{x}}_{(k+1)\\eta},\\hat{\\mathbf{x}}_{k\\eta})$ and $(\\mathbf{x}_{(k+1)\\eta}^{\\leftarrow},\\mathbf{x}_{k\\eta}^{\\leftarrow})$ , which we write in term of the conditionals and marginals as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{p}_{(k+1)\\eta,k\\eta}(\\pmb{x}^{\\prime},\\pmb{x})=\\hat{p}_{(k+1)\\eta|k\\eta}(\\pmb{x}^{\\prime}|\\pmb{x})\\cdot\\hat{p}_{k\\eta}(\\pmb{x})=\\hat{p}_{k\\eta|(k+1)\\eta}(\\pmb{x}|\\pmb{x}^{\\prime})\\cdot\\hat{p}_{(k+1)\\eta}(\\pmb{x}^{\\prime})}\\\\ &{p_{(k+1)\\eta,k\\eta}^{\\leftarrow}(\\pmb{x}^{\\prime},\\pmb{x})=p_{(k+1)\\eta|k\\eta}^{\\leftarrow}(\\pmb{x}^{\\prime}|\\pmb{x})\\cdot p_{k\\eta}^{\\leftarrow}(\\pmb{x})=p_{k\\eta|(k+1)\\eta}^{\\leftarrow}(\\pmb{x}|\\pmb{x}^{\\prime})\\cdot p_{(k+1)\\eta}^{\\leftarrow}(\\pmb{x}^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Under this condition, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\GammaV}\\left(\\hat{p}_{K\\eta},p_{*}\\right)=\\mathrm{TV}\\left(\\hat{p}_{K\\eta},p_{K\\eta}^{\\leftarrow}\\right)\\leq\\mathrm{TV}\\left(\\hat{p}_{K\\eta,(K-1)\\eta},p_{K\\eta,(K-1)\\eta}^{\\leftarrow}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\mathrm{TV}\\left(\\hat{p}_{(K-1)\\eta},p_{(K-1)\\eta}^{\\leftarrow}\\right)+\\mathbb{E}_{\\hat{\\mathbf{x}}\\sim\\hat{p}_{(K-1)\\eta}}\\left[\\mathrm{TV}\\left(\\hat{p}_{K\\eta|(K-1)\\eta}(\\cdot|\\hat{\\mathbf{x}}),p_{*}^{\\leftarrow}\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the inequalities follow from Lemma B.1. By using the inequality recursively, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}\\left(\\hat{p}_{K\\eta},p_{*}\\right)\\leq\\mathrm{TV}\\left(\\hat{p}_{0},p_{0}^{\\leftarrow}\\right)+\\displaystyle\\sum_{k=0}^{K-1}\\mathbb{E}_{\\hat{\\mathbf{x}}\\sim\\hat{p}_{k\\eta}}\\left[\\mathrm{TV}\\left(\\hat{p}_{(k+1)\\eta\\vert k\\eta}(\\cdot\\vert\\hat{\\mathbf{x}}),p_{(k+1)\\eta\\vert k\\eta}^{\\leftarrow}(\\cdot\\vert\\hat{\\mathbf{x}})\\right)\\right]}\\\\ &{\\qquad\\qquad=\\underbrace{\\mathrm{TV}\\left(p_{\\infty},p_{K\\eta}\\right)}_{\\mathrm{Tem~1}}+\\displaystyle\\sum_{k=0}^{K-1}\\mathbb{E}_{\\hat{\\mathbf{x}}\\sim\\hat{p}_{k\\eta}}\\left[\\mathrm{TV}\\left(\\hat{p}_{(k+1)\\eta\\vert k\\eta}(\\cdot\\vert\\hat{\\mathbf{x}}),p_{(k+1)\\eta\\vert k\\eta}^{\\leftarrow}(\\cdot\\vert\\hat{\\mathbf{x}})\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $p_{\\infty}$ denotes the stationary distribution of the forward process. In this analysis, $p_{\\infty}$ is the standard since the forward SDE. 1, whose negative log density is 1-strongly convex and also satisfies LSI with constant 1 due to Lemma E.9. ", "page_idx": 18}, {"type": "text", "text": "For Term 1. we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}\\left(p_{\\infty},p_{K\\eta}\\right)\\le\\sqrt{\\frac{1}{2}\\mathrm{KL}\\left(p_{K\\eta}\\big\\|p_{\\infty}\\right)}\\le\\sqrt{\\frac{1}{2}\\cdot\\exp\\left(-2K\\eta\\right)\\cdot\\mathrm{KL}\\left(p_{0}\\big\\|p_{\\infty}\\right)}}\\\\ &{\\qquad\\qquad\\qquad\\le\\sqrt{(1+L^{2})d+\\|\\nabla f_{*}(\\mathbf{0})\\|^{2}}\\cdot\\exp(-K\\eta)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the first inequality follows from Pinsker's inequality, the second one follows from Lemma E.1, and the last one follows from Lemma E.2. It should be noted that the smoothness of $p_{0}$ required in Lemma E.2 is given by [A1]. ", "page_idx": 19}, {"type": "text", "text": "Plugging this inequality into Eq. 11, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}\\left(\\hat{p}_{K\\eta},p_{*}\\right)\\leq\\!\\!\\sqrt{(1+L^{2})d+\\left\\Vert\\nabla f_{*}(\\mathbf{0})\\right\\Vert^{2}}\\cdot\\exp(-K\\eta)}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\displaystyle\\sum_{k=0}^{K-1}\\mathbb{E}_{\\hat{\\mathbf{x}}\\sim\\hat{p}_{k\\eta}}\\left[\\mathrm{TV}\\left(\\hat{p}_{(k+1)\\eta\\vert k\\eta}(\\cdot\\vert\\hat{\\mathbf{x}}),p_{(k+1)\\eta\\vert k\\eta}^{\\leftarrow}(\\cdot\\vert\\hat{\\mathbf{x}})\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, the proof is completed. ", "page_idx": 19}, {"type": "text", "text": "Corollary B.3. For Alg 1, if we set ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\eta=\\frac{1}{2}\\cdot\\log\\frac{2L+1}{2L},\\qquad K=4L\\cdot\\log\\frac{(1+L^{2})d+\\|\\nabla f_{*}(\\mathbf{0})\\|^{2}}{\\epsilon^{2}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(\\hat{p}_{(k+1)\\eta|k\\eta}(\\cdot|\\hat{x}),p_{(k+1)\\eta|k\\eta}^{\\leftarrow}(\\cdot|\\hat{x})\\right)\\le\\frac{\\epsilon}{K}=\\frac{\\epsilon}{4L}\\cdot\\left[\\log\\frac{(1+L^{2})d+\\|\\nabla f_{*}(\\mathbf{0})\\|^{2}}{\\epsilon^{2}}\\right]^{-1},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "we have the total variation distance between the underlying distribution of Alg $I$ outputandthedata distribution $p_{*}$ will satisfyTV $(\\hat{p}_{K\\eta},p_{*})\\le2\\epsilon$ ", "page_idx": 19}, {"type": "text", "text": "Proof. According to Lemma B.2, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}\\left(\\hat{p}_{K\\eta},p_{*}\\right)\\leq\\sqrt{\\left(1+L^{2}\\right)d+\\left\\Vert\\nabla f_{*}(\\mathbf{0})\\right\\Vert^{2}}\\cdot\\exp(-K\\eta)}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\displaystyle\\sum_{k=0}^{K-1}\\mathbb{E}_{\\hat{\\mathbf{x}}\\sim\\hat{p}_{k\\eta}}\\left[\\mathrm{TV}\\left(\\hat{p}_{(k+1)\\eta\\vert k\\eta}(\\cdot\\vert\\hat{\\mathbf{x}}),p_{(k+1)\\eta\\vert k\\eta}^{\\leftarrow}(\\cdot\\vert\\hat{\\mathbf{x}})\\right)\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for any $K\\in\\mathbb{N}_{+}$ and $\\eta\\in\\mathbb{R}_{+}$ . To achieve the upper bound 7 $\\mathrm{~\\nabla~}\\mathrm{~N~}(p_{\\infty},p_{K\\eta})\\leq\\epsilon$ we only require ", "page_idx": 19}, {"type": "equation", "text": "$$\nT=K\\eta\\geq\\frac{1}{2}\\log\\frac{(1+L^{2})d+\\left\\|\\nabla f_{*}(\\mathbf{0})\\right\\|^{2}}{\\epsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For Term 2. For any $\\pmb{x}\\in\\mathbb{R}^{d}$ , the formulation of $p_{k+1|k}^{\\leftarrow}(\\cdot|\\hat{\\mathbf{x}})$ .is ", "text_level": 1, "page_idx": 19}, {"type": "equation", "text": "$$\np_{(k+1)\\eta|k\\eta}^{\\leftarrow}(\\pmb{x}|\\hat{\\pmb{x}})=p_{(K-k-1)\\eta|(K-1)\\eta}(\\pmb{x}|\\hat{\\pmb{x}})\\propto\\exp\\left(-f_{(K-k-1)\\eta}(\\pmb{x})-\\frac{\\|\\hat{\\pmb{x}}-\\pmb{x}\\cdot e^{-\\eta}\\|^{2}}{2(1-e^{-2\\eta})}\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "whose negative log Hessian satisfies ", "page_idx": 19}, {"type": "equation", "text": "$$\n-\\nabla_{x}^{2}\\log p_{(k+1)\\eta|k\\eta}^{\\leftarrow}(x|\\hat{x})=\\nabla^{2}f_{(K-k-1)\\eta}(x)+\\frac{e^{-2\\eta}}{1-e^{-2\\eta}}\\cdot I\\succeq\\left(\\frac{e^{-2\\eta}}{1-e^{-2\\eta}}-L\\right)\\cdot I.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that the last inequality follows from [A1]. In this condition, if we require ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left(\\frac{e^{-2\\eta}}{1-e^{-2\\eta}}-L\\right)\\geq L\\quad\\Leftrightarrow\\quad\\eta\\leq\\frac{1}{2}\\log{\\frac{2L+1}{2L}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "then we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{e^{-2\\eta}}{2(1-e^{-2\\eta})}\\cdot I\\preceq-\\nabla_{x}^{2}\\log p_{(k+1)\\eta|k\\eta}^{\\leftarrow}(x|\\hat{x})\\preceq\\frac{3e^{-2\\eta}}{2(1-e^{-2\\eta})}\\cdot I.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "To simplify the following analysis, we choose $\\eta$ to its upper bound, and we know for all $k\\ \\in$ $\\{0,1,\\bar{\\dots},\\bar{K}-1\\}$ , the conditional density $p_{k+1|k}^{\\leftarrow}({\\pmb x}|\\hat{\\pmb x})$ is strongly-log concave, and its score is $3L$ -Lipschitz. Besides, combining Eq. 12 and the choice of $\\eta$ ,we require ", "page_idx": 20}, {"type": "equation", "text": "$$\nK=T/\\eta\\geq\\log\\frac{(1+L^{2})d+\\left\\|\\nabla f_{*}(\\mathbf{0})\\right\\|^{2}}{\\epsilon^{2}}\\Big/\\log\\frac{2L+1}{2L}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which can be achieved by ", "page_idx": 20}, {"type": "equation", "text": "$$\nK:=4L\\cdot\\log\\frac{(1+L^{2})d+\\left\\|\\nabla f_{*}(\\mathbf{0})\\right\\|^{2}}{\\epsilon^{2}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "whenwesuppose $L\\geq1$ without loss of generality. In this condition, if there is a uniform upper bound for all conditional probability approximation, i.e., ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(\\hat{p}_{(k+1)\\eta|k\\eta}(\\cdot|\\hat{x}),p_{(k+1)\\eta|k\\eta}^{\\leftarrow}(\\cdot|\\hat{x})\\right)\\le\\frac{\\epsilon}{K}=\\frac{\\epsilon}{4L}\\cdot\\left[\\log\\frac{(1+L^{2})d+\\|\\nabla f_{*}(\\mathbf{0})\\|^{2}}{\\epsilon^{2}}\\right]^{-1},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "then we can find Term 2 in Eq. 11 will be upper bounded by $\\epsilon$ . Hence, the proof is completed. ", "page_idx": 20}, {"type": "text", "text": "Lemma B.4 (Chain rule of KL). Consider four random variables, $\\mathbf{x},\\mathbf{z},\\tilde{\\mathbf{x}},\\tilde{\\mathbf{z}},$ whose underlying distributions are denoted as $p_{x},p_{z},q_{x},q_{z}$ .Suppose $p_{x,z}$ and $q_{x,z}$ denotes the densities of joint distributions of $(\\mathbf{x},\\mathbf{z})$ and $(\\Tilde{{\\mathbf x}},\\Tilde{{\\mathbf z}})$ ,which wewrite in terms of the conditionals and marginals as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{x,z}(\\pmb{x},z)=p_{x|z}(\\pmb{x}|z)\\cdot p_{z}(z)=p_{z|x}(z|\\pmb{x})\\cdot p_{x}(\\pmb{x})}\\\\ &{q_{x,z}(\\pmb{x},z)=q_{x|z}(\\pmb{x}|z)\\cdot q_{z}(z)=q_{z|x}(z|\\pmb{x})\\cdot q_{x}(\\pmb{x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "thenwe have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}\\left(p_{x,z}\\middle\\|q_{x,z}\\right)=\\mathrm{KL}\\left(p_{z}\\middle\\|q_{z}\\right)+\\mathbb{E}_{\\mathbf{z}\\sim p_{z}}\\left[\\mathrm{KL}\\left(p_{x|z}(\\cdot|\\mathbf{z})\\middle|\\middle|q_{x|z}(\\cdot|\\mathbf{z})\\right)\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\mathrm{KL}\\left(p_{x}\\middle\\|q_{x}\\right)+\\mathbb{E}_{\\mathbf{x}\\sim p_{x}}\\left[\\mathrm{KL}\\left(p_{z|x}(\\cdot|\\mathbf{x})\\middle|\\middle|q_{z|x}(\\cdot|\\mathbf{x})\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the latter equation implies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{KL}\\left(p_{x}\\Vert q_{x}\\right)\\leq\\mathrm{KL}\\left(p_{x,z}\\Vert q_{x,z}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. According to the formulation of KL divergence, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}\\left(p_{x,z}\\middle\\vert\\middle\\vert q_{x,z}\\right)=\\int p_{x,z}(\\pmb{x},z)\\log\\frac{p_{x,z}(\\pmb{x},z)}{q_{x,z}(\\pmb{x},z)}\\mathrm{d}(\\pmb{x},z)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\int p_{x,z}(\\pmb{x},z)\\left(\\log\\frac{p_{x}(\\pmb{x})}{q_{x}(\\pmb{x})}+\\log\\frac{p_{z}|x(\\pmb{z}|\\pmb{x})}{q_{z|x}(\\pmb{z}|\\pmb{x})}\\right)\\mathrm{d}(\\pmb{x},z)}\\\\ &{\\qquad\\qquad\\qquad=\\int p_{x,z}(\\pmb{x},z)\\log\\frac{p_{x}(\\pmb{x})}{q_{x}(\\pmb{x})}\\mathrm{d}(\\pmb{x},z)+\\int p_{x}(\\pmb{x})\\int p_{z|x}(z|\\pmb{x})\\log\\frac{p_{z|x}(z|\\pmb{x})}{q_{z|x}(z|\\pmb{x})}\\mathrm{d}z\\mathrm{d}x}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathrm{KL}\\left(p_{x}\\middle\\vert\\middle\\vert q_{x}\\right)+\\mathbb{E}_{\\times\\sim p_{x}}\\left[\\mathrm{KL}\\left(p_{z|x}(\\cdot|\\pmb{x})\\middle\\vert\\vert q_{z|x}(\\cdot|\\pmb{x})\\right)\\right]\\geq\\mathrm{KL}\\left(p_{x}\\middle\\vert\\middle\\vert q_{x}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last inequality follows from the fact ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{KL}\\left(p_{z|x}(\\cdot|x)\\middle|\\middle|\\tilde{p}_{z|x}(\\cdot|x)\\right)\\geq0\\quad\\forall\\:x.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "With a similar technique, it can be obtained that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}\\left(p_{x,z}\\middle|\\middle|q_{x,z}\\right)=\\int p_{x,z}(\\pmb{x},z)\\log\\frac{p_{x,z}(\\pmb{x},z)}{q_{x,z}(\\pmb{x},z)}\\mathrm{d}(\\pmb{x},z)}\\\\ &{\\qquad\\qquad\\qquad=\\int p_{x,z}(\\pmb{x},z)\\left(\\log\\frac{p_{z}(z)}{q_{z}(z)}+\\log\\frac{p_{x|z}(\\pmb{x}|z)}{q_{x|z}(\\pmb{x}|z)}\\right)\\mathrm{d}(\\pmb{x},z)}\\\\ &{\\qquad\\qquad=\\int p_{x,z}(\\pmb{x},z)\\log\\frac{p_{z}(z)}{q_{z}(z)}\\mathrm{d}(\\pmb{x},z)+\\int p_{z}(z)\\int p_{x|z}(\\pmb{x}|z)\\log\\frac{p_{x|z}(\\pmb{x}|z)}{q_{x|z}(\\pmb{x}|z)}\\mathrm{d}z\\mathrm{d}x}\\\\ &{\\qquad\\qquad=\\mathrm{KL}\\left(p_{z}\\middle|q_{z}\\right)+\\mathbb{E}_{\\mathbf{z}\\sim p_{z}}\\left[\\mathrm{KL}\\left(p_{x|z}(\\cdot|\\pmb{z})\\middle|\\middle|\\tilde{p}_{x|z}(\\cdot|\\pmb{z})\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, the proof is completed ", "page_idx": 20}, {"type": "text", "text": "Proof of Lemma 3.2. This Lemma uses nearly the same techniques as those in Lemma B.2, while it may have a better smoothness dependency in convergence since the chain rule of KL divergence. Hence, we will omit several steps overlapped in Lemma B.2. ", "page_idx": 21}, {"type": "text", "text": "For any k E {0, 1, ., K - 1), let p(k+1)m,kn and p(k+1)m,kn denote the joint distribution of $\\big(\\hat{\\mathbf{x}}_{(k+1)\\eta},\\hat{\\mathbf{x}}_{k\\eta}\\big)$ and $(\\mathbf{x}_{(k+1)\\eta}^{\\leftarrow},\\mathbf{x}_{k\\eta}^{\\leftarrow})$ , which we write in term of the conditionals and marginals as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{p}_{(k+1)\\eta,k\\eta}(\\pmb{x}^{\\prime},\\pmb{x})=\\hat{p}_{(k+1)\\eta|k\\eta}(\\pmb{x}^{\\prime}|\\pmb{x})\\cdot\\hat{p}_{k\\eta}(\\pmb{x})=\\hat{p}_{k\\eta|(k+1)\\eta}(\\pmb{x}|\\pmb{x}^{\\prime})\\cdot\\hat{p}_{(k+1)\\eta}(\\pmb{x}^{\\prime})}\\\\ &{p_{(k+1)\\eta,k\\eta}^{\\leftarrow}(\\pmb{x}^{\\prime},\\pmb{x})=p_{(k+1)\\eta|k\\eta}^{\\leftarrow}(\\pmb{x}^{\\prime}|\\pmb{x})\\cdot p_{k\\eta}^{\\leftarrow}(\\pmb{x})=p_{k\\eta|(k+1)\\eta}^{\\leftarrow}(\\pmb{x}|\\pmb{x}^{\\prime})\\cdot p_{(k+1)\\eta}^{\\leftarrow}(\\pmb{x}^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Besides, we consider a reference Markov process $\\{\\tilde{\\mathbf{x}}_{k}\\}$ whose initial marginal distribution and transition kernels satisfy ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{x}}_{0}\\sim\\tilde{p}_{0}=\\hat{p}_{0}\\quad\\mathrm{and}\\quad\\tilde{p}_{(k+1)\\eta|k\\eta}(\\pmb{x}^{\\prime}|\\pmb{x})=p_{(k+1)\\eta|k\\eta}^{\\leftarrow}(\\pmb{x}^{\\prime}|\\pmb{x}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Under these conditions, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{TV}\\left(\\hat{p}_{K\\eta},p_{*}\\right)\\leq\\mathrm{TV}\\left(\\hat{p}_{K\\eta},\\tilde{p}_{K\\eta}\\right)+\\mathrm{TV}\\left(\\tilde{p}_{K\\eta},p_{*}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $\\{\\tilde{\\mathbf{x}}_{k}\\}$ and $\\{\\mathbf{x}_{k}^{\\leftarrow}\\}$ share the same transition kernel, then we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}\\left(\\tilde{p}_{K\\eta},p_{*}\\right)\\leq\\mathrm{TV}\\left(\\tilde{p}_{0},p_{0}^{\\leftarrow}\\right)\\leq\\sqrt{\\frac{1}{2}\\mathrm{KL}\\left(p_{0}^{\\leftarrow}\\|\\tilde{p}_{0}^{\\leftarrow}\\right)}}\\\\ &{=\\sqrt{\\frac{1}{2}\\mathrm{KL}\\left(p_{K\\eta}\\|p_{\\infty}\\right)}\\leq\\sqrt{(1+L^{2})d+\\|\\nabla f_{*}(\\mathbf{0})\\|^{2}}\\cdot\\exp(-K\\eta),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the first inequality follows from the chain rule of TV distance, the second inequality follows from Pinsker's inequality, and the last inequality follows from Lemma E.2. Besides, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}\\left(\\hat{p}_{K\\eta},\\tilde{p}_{K\\eta}\\right)\\le\\sqrt{\\frac{1}{2}\\mathrm{KL}\\left(\\hat{p}_{K\\eta}\\|\\tilde{p}_{K\\eta}\\right)}\\le\\sqrt{\\frac{1}{2}\\mathrm{KL}\\left(\\hat{p}_{K\\eta,(K-1)\\eta}\\|\\tilde{p}_{K\\eta,(K-1)\\eta}\\right)}}\\\\ &{\\le\\sqrt{\\frac{1}{2}\\mathrm{KL}\\left(\\hat{p}_{(K-1)\\eta}\\|\\tilde{p}_{(K-1)\\eta}\\right)+\\frac{1}{2}\\mathbb{E}_{\\hat{\\mathbf{x}}\\sim\\hat{p}_{(K-1)\\eta}}\\left[\\mathrm{KL}\\left(\\hat{p}_{K\\eta|(K-1)\\eta}(\\cdot|\\hat{\\mathbf{x}})\\|p_{K\\eta|(K-1)\\eta}^{\\leftarrow}(\\cdot|\\hat{\\mathbf{x}})\\right)\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the first inequality follows from Pinsker's inequality, the second and the third inequalities follow from Lemma B.4. By using this inequality recursively, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}\\left(\\hat{p}_{K\\eta},\\tilde{p}_{K\\eta}\\right)\\le\\sqrt{\\frac{1}{2}\\mathrm{KL}\\left(\\hat{p}_{0}\\left\\lVert\\tilde{p}_{0}\\right\\rVert\\right)+\\frac{1}{2}\\sum_{k=0}^{K-1}\\mathbb{E}_{\\hat{\\mathbf{x}}\\sim\\hat{p}_{k\\eta}}\\left[\\mathrm{KL}\\left(\\hat{p}_{(k+1)\\eta\\vert k\\eta}(\\cdot\\vert\\hat{\\mathbf{x}})\\right\\rVert p_{(k+1)\\eta\\vert k\\eta}^{\\leftarrow}(\\cdot\\vert\\hat{\\mathbf{x}})\\right)\\right]}}\\\\ &{\\quad\\quad\\quad\\quad=\\sqrt{\\frac{1}{2}\\sum_{k=0}^{K-1}\\mathbb{E}_{\\hat{\\mathbf{x}}\\sim\\hat{p}_{k\\eta}}\\left[\\mathrm{KL}\\left(\\hat{p}_{(k+1)\\eta\\vert k\\eta}(\\cdot\\vert\\hat{\\mathbf{x}})\\right\\Vert p_{(k+1)\\eta\\vert k\\eta}^{\\leftarrow}(\\cdot\\vert\\hat{\\mathbf{x}})\\right)\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the equation follows from the definition of process $\\{\\tilde{\\mathbf{x}}_{k}\\}$ . Therefore, combining Eq. 14 with Eq. 13, the proof is completed. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Corollary B.5. For Alg 1, if we set ", "text_level": 1, "page_idx": 21}, {"type": "equation", "text": "$$\n\\eta=\\frac{1}{2}\\cdot\\log\\frac{2L+1}{2L},\\qquad K=4L\\cdot\\log\\frac{(1+L^{2})d+\\|\\nabla f_{*}(\\mathbf{0})\\|^{2}}{\\epsilon^{2}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{KL}\\left(\\hat{p}_{(k+1)\\eta\\vert k\\eta}(\\cdot\\vert\\hat{x})\\Big\\Vert p_{(K-k-1)\\eta\\vert(K-k)\\eta}(\\cdot\\vert\\hat{x})\\right)\\le\\frac{\\epsilon^{2}}{4L}\\cdot\\left[\\log\\frac{(1+L^{2})d+\\Vert\\nabla f_{*}(\\mathbf{0})\\Vert^{2}}{\\epsilon^{2}}\\right]^{-1},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "we have the totalvariation distance between the underlying distribution of Alg 1 output and the data distribution $p_{*}$ will satisfyTV $(\\hat{p}_{K\\eta},p_{*})\\le2\\epsilon$ ", "page_idx": 21}, {"type": "text", "text": "Proof. According to Lemma 3.2, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{TV}\\left(\\hat{p}_{K\\eta},p_{*}\\right)\\le\\underbrace{\\sqrt{(1+L^{2})d+\\|\\nabla f_{*}(\\mathbf{0})\\|^{2}}\\cdot\\exp(-K\\eta)}_{\\mathrm{Term~1}}}\\\\ {+\\underbrace{\\sqrt{\\frac{1}{2}\\displaystyle\\sum_{k=0}^{K-1}\\mathbb{E}_{\\hat{\\mathbf{x}}\\sim\\hat{p}_{k\\eta}}\\left[\\mathrm{KL}\\left(\\hat{p}_{(k+1)\\eta\\vert k\\eta}(\\cdot\\vert\\hat{\\mathbf{x}})\\vert\\vert p_{(k+1)\\eta\\vert k\\eta}^{\\leftarrow}(\\cdot\\vert\\hat{\\mathbf{x}})\\right)\\right]}_{\\mathrm{Term~2}}}_{\\mathrm{Term~2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To achieve the upper bound Term $1\\leq\\epsilon$ , we only require ", "page_idx": 22}, {"type": "equation", "text": "$$\nT=K\\eta\\geq\\frac{1}{2}\\log\\frac{(1+L^{2})d+\\left\\|\\nabla f_{*}(\\mathbf{0})\\right\\|^{2}}{\\epsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For Term 2, by choosing ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\eta=\\frac{1}{2}\\log\\frac{2L+1}{2L},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "we know for all $k\\in\\{0,1,\\ldots,K-1\\}$ , the conditional density $p_{k+1|k}^{\\leftarrow}(x|\\hat{\\mathbf{x}})$ is strongly-log concave, and its score is $3L$ -Lipschitz. In this condition, we require ", "page_idx": 22}, {"type": "equation", "text": "$$\nK:=4L\\cdot\\log\\frac{(1+L^{2})d+\\left\\|\\nabla f_{*}(\\mathbf{0})\\right\\|^{2}}{\\epsilon^{2}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "whenwesuppose $L\\geq1$ without loss of generality. Then, to achieve Term $2\\leq\\epsilon$ the sufficient condition is to require a uniform upper bound for all conditional probability approximation, i.e., ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{KL}\\left(\\hat{p}_{(k+1)\\eta\\vert k\\eta}(\\cdot\\vert\\hat{x})\\big\\Vert p_{k+1\\vert k}^{\\leftarrow}(\\cdot\\vert\\hat{x})\\right)\\leq\\frac{\\epsilon^{2}}{K}=\\frac{\\epsilon^{2}}{4L}\\cdot\\left[\\log\\frac{(1+L^{2})d+\\|\\nabla f_{*}(\\mathbf{0})\\|^{2}}{\\epsilon^{2}}\\right]^{-1}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, the proof is completed. ", "page_idx": 22}, {"type": "text", "text": "Remark 1. To achieve the TV error tolerance shown in Corollary B.3, .i.e., ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(\\hat{p}_{(k+1)\\eta\\vert k\\eta}(\\cdot\\vert\\hat{x}),p_{(k+1)\\eta\\vert k\\eta}^{\\leftarrow}(\\cdot\\vert\\hat{x})\\right)\\le\\frac{\\epsilon}{4L}\\cdot\\left[\\log\\frac{(1+L^{2})d+\\Vert\\nabla f_{*}(\\mathbf{0})\\Vert^{2}}{\\epsilon^{2}}\\right]^{-1},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "it requires the KL divergence error to satisfy ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{TV}\\left(\\hat{p}_{(k+1)\\eta\\vert k\\eta}(\\cdot\\vert\\hat{x}),p_{(k+1)\\eta\\vert k\\eta}^{\\leftarrow}(\\cdot\\vert\\hat{x})\\right)\\leq\\sqrt{\\frac{1}{2}}\\mathrm{KL}\\left(\\hat{p}_{(k+1)\\eta\\vert k\\eta}(\\cdot\\vert\\hat{x})\\big\\Vert p_{(k+1)\\eta\\vert k\\eta}^{\\leftarrow}(\\cdot\\vert\\hat{x})\\right)}\\\\ {\\leq\\frac{\\epsilon^{2}}{16L^{2}}\\cdot\\left[\\log\\frac{\\left(1+L^{2}\\right)d+\\left\\Vert\\nabla f_{*}\\left(\\mathbf{0}\\right)\\right\\Vert^{2}}{\\epsilon^{2}}\\right]^{-2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Compared with the results shown in Corollary B.5, this result requires a higher accuracy with an $\\mathcal{O}(L)$ factor, which is not acceptable sometimes. ", "page_idx": 22}, {"type": "text", "text": "Lemma B.6. Suppose Assumption [A1]-[A2] hold, the choice of $\\eta$ keeps the same as that in Corollary B.5, and the second moment of the underlying distribution of $\\hat{\\mathbf{x}}_{k\\eta}$ is $M_{k}$ ,thenwehave ", "page_idx": 22}, {"type": "equation", "text": "$$\nM_{k+1}\\leq{\\frac{2\\delta_{k}}{L}}+16(d+m_{2}^{2})+24M_{k}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Considering the second moment of $\\hat{\\mathbf{x}}_{(k+1)\\eta}$ ,wehave ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}_{\\hat{p}_{(k+1)\\eta}}\\left[\\left\\|\\hat{\\mathbf{x}}_{(k+1)\\eta}\\right\\|^{2}\\right]=\\displaystyle\\int\\hat{p}_{(k+1)\\eta}(\\pmb{x})\\cdot\\|\\pmb{x}\\|^{2}\\mathrm{d}\\pmb{x}}\\\\ {\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\int\\left(\\displaystyle\\int\\hat{p}_{k\\eta}(\\pmb{y})\\cdot\\hat{p}_{(k+1)\\eta|k\\eta}(\\pmb{x}|\\pmb{y})\\mathrm{d}\\pmb{y}\\right)\\cdot\\|\\pmb{x}\\|^{2}\\mathrm{d}\\pmb{x}}\\\\ {\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\int\\hat{p}_{k\\eta}(\\pmb{y})\\cdot\\int\\hat{p}_{(k+1)\\eta|k\\eta}(\\pmb{x}|\\pmb{y})\\cdot\\|\\pmb{x}\\|^{2}\\mathrm{d}\\pmb{x}\\mathrm{d}\\pmb{y}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then, we focus on the innermost integration, suppose $\\hat{\\gamma}_{y}(\\cdot,\\cdot)$ as the optimal coupling between $\\hat{p}_{(k+1)\\eta|k\\eta}(\\cdot|\\pmb{y})$ and $p_{(k+1)\\eta|k\\eta}^{\\leftarrow}(\\cdot|\\pmb{y})$ .Then, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int\\hat{p}_{(k+1)\\eta\\vert k\\eta}(\\pmb{x}\\vert\\pmb{y})\\left\\vert\\pmb{x}\\right\\vert\\right\\vert^{2}\\mathrm{d}\\pmb{x}-2\\int p_{(k+1)\\eta\\vert k\\eta}^{\\leftarrow}(\\pmb{x}\\vert\\pmb{y})\\left\\vert\\pmb{x}\\right\\vert^{2}\\mathrm{d}\\pmb{x}}\\\\ &{\\le\\displaystyle\\int\\hat{\\gamma}_{\\pmb{y}}(\\hat{\\pmb{x}},\\pmb{x})\\left(\\left\\vert\\hat{\\pmb{x}}\\right\\vert^{2}-2\\left\\vert\\left\\vert\\pmb{x}\\right\\vert\\right\\vert^{2}\\right)\\mathrm{d}(\\hat{\\pmb{x}},\\pmb{x})\\le\\int\\hat{\\gamma}_{\\pmb{y}}(\\hat{\\pmb{x}},\\pmb{x})\\left\\vert\\hat{\\pmb{x}}-\\pmb{x}\\right\\vert^{2}\\mathrm{d}(\\hat{\\pmb{x}},\\pmb{x})}\\\\ &{=W_{2}^{2}\\left(\\hat{p}_{(k+1)\\eta\\vert k\\eta},p_{(k+1)\\eta\\vert k\\eta}^{\\leftarrow}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since $p_{(k+1)\\eta|k\\eta}^{\\leftarrow}$ is strongly log-concave, i.e. ", "page_idx": 23}, {"type": "equation", "text": "$$\n-\\nabla_{x}^{2}\\log p_{(k+1)\\eta|k\\eta}^{\\leftarrow}(x|\\hat{x})=\\nabla^{2}f_{(K-k-1)\\eta}(x)+\\frac{e^{-2\\eta}}{1-e^{-2\\eta}}\\cdot I\\succeq L I,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "the distribution p(k+1)lkn also satisfies $1/L$ log-Sobolev inequality due to Lemma E.9. By Talagrand's inequality, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nW_{2}^{2}\\left(\\hat{p}_{(k+1)\\eta\\vert k\\eta},p_{(k+1)\\eta\\vert k\\eta}^{\\leftarrow}\\right)\\leq\\frac{2}{L}\\cdot\\mathrm{KL}\\left(\\hat{p}_{k+1\\vert k+\\frac{1}{2},b}\\vert\\vert p_{k+1\\vert k+\\frac{1}{2},b}\\right):=\\frac{2\\delta_{k}}{L}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Plugging Eq 18 and Eq 19 into Eq 17, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\hat{\\mathbf{x}}_{(k+1)\\eta}\\right\\Vert^{2}\\right]\\leq\\int\\hat{p}_{k\\eta}(y)\\cdot\\left(\\frac{2\\delta_{k}}{L}+2\\int p_{(k+1)\\eta|k\\eta}(x|y)\\left\\Vert x\\right\\Vert^{2}\\mathrm{d}x\\right)\\mathrm{d}y.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "To upperboud the inemost intgration, we uppose the optmal coupling between $p(K\\!-\\!k\\!-\\!1)\\eta$ and $p_{(k+1)\\eta|k\\eta}^{\\leftarrow}(\\cdot|\\pmb{y})$ $\\gamma_{y}(\\cdot,\\cdot)$ . Then it has ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int p_{(k+1)\\eta\\mid k\\eta}^{<}(\\pmb{x}|\\pmb{y})\\left\\|\\pmb{x}\\right\\|^{2}\\mathrm{d}\\pmb{x}-2\\int p_{(K-k-1)\\eta}(\\pmb{x})\\left\\|\\pmb{x}\\right\\|^{2}\\mathrm{d}\\pmb{x}}\\\\ &{\\le\\displaystyle\\int\\gamma_{\\pmb{y}}(\\pmb{x}^{\\prime},\\pmb{x})\\left(\\left\\|\\pmb{x}^{\\prime}\\right\\|^{2}-2\\left\\|\\pmb{x}\\right\\|^{2}\\right)\\mathrm{d}(\\pmb{x}^{\\prime},\\pmb{x})\\le\\displaystyle\\int\\gamma_{\\pmb{y}}(\\pmb{x}^{\\prime},\\pmb{x})\\left\\|\\pmb{x}^{\\prime}-\\pmb{x}\\right\\|^{2}\\mathrm{d}(\\pmb{x}^{\\prime},\\pmb{x})}\\\\ &{=W_{2}^{2}(p_{(K-k-1)\\eta},p_{(k+1)\\eta\\mid k\\eta}^{\\leftarrow})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since $p_{(k+1)\\eta|k\\eta}^{\\leftarrow}$ satisfies LSI with constant $1/L$ . By Talagrand's inequality and LSI, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{2}^{2}(p_{(K-k-1)\\eta},p_{(k+1)\\eta|k\\eta}^{\\leftarrow})\\le\\frac{2}{L}\\cdot\\mathrm{KL}\\left(p_{(K-k-1)\\eta}\\big\\|p_{(k+1)\\eta|k\\eta}\\right)}\\\\ &{\\le\\frac{4}{L^{2}}\\cdot\\int p_{(K-k-1)\\eta}(\\mathbf x)\\cdot\\left\\|\\nabla\\log\\frac{p_{(K-k-1)\\eta}(\\mathbf x)}{p_{(k+1)\\eta|k\\eta}^{\\leftarrow}(\\mathbf x)}\\right\\|^{2}\\mathrm{d}\\mathbf x}\\\\ &{=\\frac{4}{L^{2}}\\cdot\\int p_{(K-k-1)\\eta}(\\mathbf x)\\cdot\\left\\|\\frac{e^{-\\eta}y}{1-e^{-2\\eta}}\\right\\|^{2}\\mathrm{d}\\mathbf x}\\\\ &{\\le12\\left\\|y\\right\\|^{2}+8\\int p_{(K-k-1)\\eta}(\\mathbf x)\\|x\\|^{2}\\mathrm{d}\\mathbf x}\\\\ &{\\le12\\|y\\|^{2}+8(d+m_{2}^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last inequality follows from the choice of $\\eta\\;=\\;1/2\\cdot\\log(2L+1)/2L$ and the fact $E_{p_{(K-k-1)\\eta}}[||\\mathbf{x}||^{2}]\\stackrel{.}{\\leq}(d\\stackrel{.}{+}m_{2}^{2})$ obtained by Lemma E.7. Plugging this results into Eq. 20, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\hat{\\mathbf{x}}_{(k+1)\\eta}\\right\\Vert^{2}\\right]\\leq\\frac{2\\delta_{k}}{L}+16(d+m_{2}^{2})+24\\cdot\\mathbb{E}\\left[\\left\\Vert\\hat{\\mathbf{x}}_{k\\eta}\\right\\Vert^{2}\\right].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "C  Implement RTK inference with MALA ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we consider introducing aMAA ariant to samle fm $p_{k+1|k}^{\\leftarrow}(z|\\pmb{x}_{0})$ To simplify the notation, we set ", "page_idx": 23}, {"type": "equation", "text": "$$\ng(z):=f_{(K-k-1)\\eta}(z)+\\frac{\\|x_{0}-z\\cdot e^{-\\eta}\\|^{2}}{2(1-e^{-2\\eta})}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and consider $k$ and $\\pmb{x}_{0}$ to be fixed. Besides, we set ", "page_idx": 24}, {"type": "equation", "text": "$$\np^{\\leftarrow}(z|x_{0}):=p_{k+1|k}^{\\leftarrow}(z|x_{0})\\propto\\exp(-g(z))\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "According to Corollary B.5 and Corollary B.3, when we choose ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\eta=\\frac{1}{2}\\log\\frac{2L+1}{2L},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "the log density $g$ will be $L$ -strongly log-concave and $3L$ -smooth. With the following two approximations, ", "page_idx": 24}, {"type": "equation", "text": "$$\ns_{\\theta}(z)\\approx\\nabla g(z)\\quad\\mathrm{and}\\quad r_{\\theta^{\\prime}}(z,z^{\\prime})\\approx g(z)-g(z^{\\prime}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We left the approximation level here and determined when we needed the detailed analysis. we can use the following Algorithm to replace Line 3 of Alg. 1. ", "page_idx": 24}, {"type": "text", "text": "In this section, we introduce several notations about three transition kernels presenting the standard, the projected, and the ideally projected implementation of Alg. 2. ", "page_idx": 24}, {"type": "text", "text": "Standard implementation of Alg. 2. According to Step 4, the transition distribution satisfies ", "page_idx": 24}, {"type": "equation", "text": "$$\nQ_{z_{s}}=\\mathcal{N}\\left(z_{s}-\\tau\\cdot s_{\\theta}(z_{s}),2\\tau\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with a density function ", "page_idx": 24}, {"type": "equation", "text": "$$\nq(\\tilde{z}_{s}|z_{s})=\\varphi_{2\\tau}\\left(\\tilde{z}_{s}-\\big(z_{s}-\\tau\\cdot s_{\\theta}(z_{s})\\big)\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Considering a $1/2$ -lazy version of the update, we set ", "page_idx": 24}, {"type": "equation", "text": "$$\nT_{z_{s}}^{\\prime}(\\mathrm{d}z^{\\prime})=\\frac{1}{2}\\cdot\\delta_{z_{s}}(\\mathrm{d}z^{\\prime})+\\frac{1}{2}\\cdot Q_{z_{s}}(\\mathrm{d}z^{\\prime}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, with the following Metropolis-Hastings filter, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\iota_{z_{s}}(z^{\\prime})=\\operatorname*{min}\\left\\{1,\\frac{q(z_{s}|z^{\\prime})}{q(z^{\\prime}|z_{s})}\\cdot\\exp\\left(-r_{\\theta}(z^{\\prime},z_{s})\\right)\\right\\}\\quad\\mathrm{where}\\quad a_{z_{s}}(z^{\\prime})=a(z^{\\prime}-(z_{s}-\\tau\\cdot s_{\\theta}(z_{s})),z_{s}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "the transition kernel for the standard implementation of Alg, 2 will be ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{T}_{z_{s}}(\\mathrm{d}z_{s+1})=\\mathcal{T}_{z_{s}}^{\\prime}(\\mathrm{d}z_{s+1})\\cdot a_{z_{s}}(z_{s+1})+\\bigg(1-\\int a_{z_{s}}(z^{\\prime})\\mathcal{T}_{z_{s}}^{\\prime}(\\mathrm{d}z^{\\prime})\\bigg)\\cdot\\delta_{z_{s}}(\\mathrm{d}z_{s+1}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Projected implementation of Alg. 2. According to Step 4, the transition distribution satisfies ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{Q}_{z_{s}}=\\mathcal{N}\\left(z_{s}-\\tau\\cdot s_{\\theta}(z_{s}),2\\tau\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with a density function ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{q}(\\tilde{z}_{s}|z_{s})=\\varphi_{2\\tau}\\left(\\tilde{z}_{s}-\\left(z_{s}-\\tau\\cdot\\nabla s_{\\theta}(z_{s})\\right)\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Considering the projection operation, i.e., Step 5 in Alg 1, if we suppose the feasible set ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Omega=\\mathcal{B}(\\mathbf{0},R)\\quad\\mathrm{and}\\quad\\Omega_{z}=\\mathcal{B}(z,r)\\cap\\mathcal{B}(\\mathbf{0},R)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "the transition distribution becomes ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{Q}_{z_{s}}^{\\prime}(A)=\\int_{A\\cap\\Omega_{z_{s}}}\\tilde{Q}_{z_{s}}(\\mathrm{d}z^{\\prime})+\\int_{A-\\Omega_{z_{s}}}\\tilde{Q}_{z_{s}}(\\mathrm{d}z^{\\prime})\\cdot\\delta_{z_{s}}(A).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Hence, a $1/2$ -lazy version of the transition distribution becomes ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{T}}_{z_{s}}^{\\prime}(\\mathrm{d}z^{\\prime})=\\frac{1}{2}\\cdot\\delta_{z_{s}}(\\mathrm{d}z^{\\prime})+\\frac{1}{2}\\cdot\\tilde{Q}_{z_{s}}^{\\prime}(\\mathrm{d}z^{\\prime}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, with the following Metropolis-Hastings filter, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{i}_{z_{s}}(z^{\\prime})=\\operatorname*{min}\\bigg\\{1,\\frac{\\tilde{q}(z_{s}|z^{\\prime})}{\\tilde{q}(z^{\\prime}|z_{s})}\\cdot\\exp\\left(-r_{\\theta}(z^{\\prime},z_{s})\\right)\\bigg\\}\\quad\\mathrm{where}\\quad\\tilde{a}_{z_{s}}(z^{\\prime})=a(z^{\\prime}-(z_{s}-\\tau\\cdot s_{\\theta}(z_{s})),z_{s}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "the transition kernel for the projected implementation of Alg, 2 will be ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{T}}_{z_{s}}(\\mathrm{d}z_{s+1})=\\tilde{\\mathcal{T}}_{z_{s}}^{\\prime}(\\mathrm{d}z_{s+1})\\boldsymbol{\\cdot}\\tilde{\\boldsymbol{a}}_{z_{s}}(z_{s+1})+\\left(1-\\int_{\\Omega}\\tilde{\\boldsymbol{a}}_{z_{s}}(z^{\\prime})\\tilde{\\mathcal{T}}_{z_{s}}^{\\prime}(\\mathrm{d}z^{\\prime})\\right)\\boldsymbol{\\cdot}\\delta_{z_{s}}(\\mathrm{d}z_{s+1}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Ideally projected implementation of Alg. 2. In this condition, we know the accurate $g(z)-g(z^{\\prime})$ and $\\nabla g(z)$ . In this condition, the ULA step will provide ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tilde{Q}_{\\ast,z_{s}}=\\mathscr{N}\\big(z_{s}-\\tau\\cdot\\nabla g(z_{s}),2\\tau\\big)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "with a density function ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tilde{q}_{*}(\\tilde{z}_{s}|z_{s})=\\varphi_{2\\tau}\\left(\\tilde{z}_{s}-(\\tau\\cdot\\nabla g(z_{s}))\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Considering the projection operation, i.e., Step 5 in Alg 1, the transition distribution becomes ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tilde{Q}_{*,z_{s}}^{\\prime}(\\mathcal{A})=\\int_{\\mathcal{A}\\cap\\Omega_{z_{s}}}\\tilde{Q}_{*,z_{s}}(\\mathrm{d}z^{\\prime})+\\int_{\\mathcal{A}-\\Omega_{z_{s}}}\\tilde{Q}_{*,z_{s}}(\\mathrm{d}z^{\\prime})\\cdot\\delta_{z_{s}}(\\mathcal{A}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Hence, a $1/2$ -lazy version of the transition distribution becomes ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{T}}_{*,z_{s}}^{\\prime}(\\mathrm{d}z^{\\prime})=\\frac{1}{2}\\cdot\\delta_{z_{s}}(\\mathrm{d}z^{\\prime})+\\frac{1}{2}\\cdot\\tilde{Q}_{*,z_{s}}^{\\prime}(\\mathrm{d}z^{\\prime}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then, with the following Metropolis-Hastings filter, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tilde{a}_{*,z_{s}}(z^{\\prime})=\\operatorname*{min}\\left\\{1,\\frac{\\tilde{q}_{*}(z_{s}|z^{\\prime})}{\\tilde{q}_{*}(z^{\\prime}|z_{s})}\\cdot\\exp\\left(-\\left(g(z^{\\prime})-g(z_{s})\\right)\\right)\\right\\},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "the transition kernel for the accurate projected update will be ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{T}}_{*,z_{s}}(\\mathrm{d}z_{s+1})=\\tilde{\\mathcal{T}}_{*,z_{s}}^{\\prime}(\\mathrm{d}z_{s+1})\\cdot\\tilde{a}_{*,z_{s}}(z_{s+1})+\\left(1-\\int_{\\Omega}\\tilde{a}_{*,z_{s}}(z^{\\prime})\\tilde{\\mathcal{T}}_{*,z_{s}}^{\\prime}(\\mathrm{d}z^{\\prime})\\right)\\cdot\\delta_{z_{s}}(\\mathrm{d}z_{s+1}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma C.1. Suppose we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\eta=\\frac{1}{2}\\log\\frac{2L+1}{2L},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "then the target distribution of the Inner MALA, i.e., $p^{\\leftarrow}(z|x_{0})$ willbe $L$ -strongly log-concave and $3L$ -smooth for any given $\\scriptstyle x_{0}$ ", "page_idx": 25}, {"type": "text", "text": "Proof. Consider the energy function $g(z)$ of $p^{\\leftarrow}(z|x_{0})$ ,we have ", "page_idx": 25}, {"type": "equation", "text": "$$\ng(z)=f_{(K-k-1)\\eta}(z)+\\frac{\\|x_{0}-z\\cdot e^{-\\eta}\\|^{2}}{2(1-e^{-2\\eta})}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "whose Hessian matrix satisfies ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\langle{\\frac{e^{-2\\eta}}{(1-e^{-2\\eta})}}+L\\rangle\\cdot I\\succeq\\nabla^{2}g(z)=\\nabla^{2}f_{(K-k-1)}(z)+{\\frac{e^{-2\\eta}}{(1-e^{-2\\eta})}}\\cdot I\\succeq\\left({\\frac{e^{-2\\eta}}{(1-e^{-2\\eta})}}-L\\right)\\cdot I.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Under these conditions, if we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\eta\\leq\\frac{1}{2}\\log\\frac{2L+1}{2L}\\quad\\Leftrightarrow\\quad\\frac{e^{-2\\eta}}{1-e^{-2\\eta}}\\geq2L,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which means ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{3e^{-2\\eta}}{2(1-e^{-2\\eta})}\\succeq\\nabla^{2}g(z)\\succeq\\frac{e^{-2\\eta}}{2(1-e^{-2\\eta})}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For the analysis convenience, we set ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\eta=\\frac{1}{2}\\log\\frac{2L+1}{2L},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "that is to say $g(z)$ is $L$ -strongly convex and $3L$ -smooth. ", "page_idx": 25}, {"type": "text", "text": "C.1 Control the error from the projected transition kernel ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Here, we consider the marginal distribution of $\\{\\mathbf{z}_{s}\\}$ and $\\{\\widetilde{\\mathbf{z}}_{s}\\}$ to be the random process when Alg. 2 is implemented by the standard and projected version, respectively. The underlying distributions of these two processes are denoted as $\\mathbf{z}_{s}\\,\\sim\\,\\mu_{s}$ and $\\tilde{\\bf z}_{s}\\sim\\tilde{\\mu}_{s}$ , and we would like to upper bound $\\mathrm{TV}\\left(\\mu_{S},\\tilde{\\mu}_{S}\\right)$ for any given $\\scriptstyle x_{0}$ ", "page_idx": 26}, {"type": "text", "text": "Rewrite the formulation of $\\mathbf{z}_{S}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{z}_{S}=\\tilde{\\mathbf{z}}_{S}\\cdot\\mathbf{1}\\left(\\mathbf{z}_{S}=\\tilde{\\mathbf{z}}_{S}\\right)+\\mathbf{z}_{S}\\cdot\\mathbf{1}\\left(\\mathbf{z}_{S}\\neq\\tilde{\\mathbf{z}}_{S}\\right)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\mathbf{1}(\\cdot)$ is the indicator function. In this condition, for any set $\\boldsymbol{\\mathcal{A}}$ ,wehave ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{1}\\left(\\mathbf{z}_{S}\\in\\mathcal{A}\\right)=\\mathbf{1}\\left(\\tilde{\\mathbf{z}}_{S}\\in\\mathcal{A}\\right)\\cdot\\mathbf{1}\\left(\\mathbf{z}_{S}=\\tilde{\\mathbf{z}}_{S}\\right)+\\mathbf{1}\\left(\\mathbf{z}_{S}\\in\\mathcal{A}\\right)\\cdot\\mathbf{1}\\left(\\mathbf{z}_{S}\\neq\\tilde{\\mathbf{z}}_{S}\\right)}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbf{1}\\left(\\tilde{\\mathbf{z}}_{S}\\in\\mathcal{A}\\right)-\\mathbf{1}\\left(\\tilde{\\mathbf{z}}_{S}\\in\\mathcal{A}\\right)\\cdot\\mathbf{1}\\left(\\mathbf{z}_{S}\\neq\\tilde{\\mathbf{z}}_{S}\\right)+\\mathbf{1}\\left(\\mathbf{z}_{S}\\in\\mathcal{A}\\right)\\cdot\\mathbf{1}\\left(\\mathbf{z}_{S}\\neq\\tilde{\\mathbf{z}}_{S}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which means ", "page_idx": 26}, {"type": "equation", "text": "$$\n-\\mathbf{1}\\left(\\tilde{\\mathbf{z}}_{S}\\in A\\right)\\cdot\\mathbf{1}\\left(\\mathbf{z}_{S}\\neq\\tilde{\\mathbf{z}}_{S}\\right)\\leq\\mathbf{1}\\left(\\mathbf{z}_{S}\\in A\\right)-\\mathbf{1}\\left(\\tilde{\\mathbf{z}}_{S}\\in A\\right)\\leq\\mathbf{1}\\left(\\mathbf{z}_{S}\\in A\\right)\\cdot\\mathbf{1}\\left(\\mathbf{z}_{S}\\neq\\tilde{\\mathbf{z}}_{S}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, the total variation distance between $\\mu_{S}$ and $\\hat{\\mu}_{S}$ can be upper bounded with ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(\\mu_{S},\\tilde{\\mu}_{S}\\right)\\leq\\operatorname*{sup}_{A\\subseteq\\mathbb{R}^{d}}\\left|\\mu_{S}(A)-\\tilde{\\mu}_{S}(A)\\right|\\leq{\\bf1}\\left({\\bf z}_{S}\\neq\\tilde{\\bf z}_{S}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Hence, to require $\\mathrm{TV}\\left(\\mu_{S},\\tilde{\\mu}_{S}\\right)\\;\\leq\\;\\epsilon/4$ a sufficient condition is to consider $\\operatorname*{Pr}[\\mathbf{z}_{S}\\;\\neq\\;\\tilde{\\mathbf{z}}_{S}]$ . The next step is to show that, in Alg. 2, the projected version generates the same outputs as that of the standard version with probability at least $1-\\epsilon/4$ . It suffices to show that with probability at least $1-\\epsilon/4$ , projected MALA will accept all $S$ iterates. In this condition, let $\\{z_{1},z_{2},\\dots,z_{S}\\}$ be the iterates generated by the standard MALA (without the projection step), our goal is to prove that with probability at least $1-\\epsilon/4$ all $\\mathbf{z}_{s}$ stay inside the region $B(\\mathbf{0},R)$ and $\\|z_{s}-z_{s-1}\\|\\leq r$ for all $s\\leq S$ That means we need to prove the following two facts ", "page_idx": 26}, {"type": "text", "text": "I. With probability at least $1-\\epsilon/8$ , all iterates stay inside the region $B(\\mathbf{0},R)$ ", "page_idx": 26}, {"type": "text", "text": "Lemma C.2. Let $\\mu_{S}$ and $\\tilde{\\mu}_{S}$ be distributions of the outputs of standard and projected implementation of Alg. 2. For any $\\epsilon\\in(0,1)$ weset ", "page_idx": 26}, {"type": "equation", "text": "$$\nR\\geq\\operatorname*{max}\\left\\{8\\cdot\\sqrt{\\frac{\\|\\nabla g(\\mathbf{0})\\|^{2}}{L^{2}}+\\frac{d}{L}},63\\cdot\\sqrt{\\frac{d}{L}\\log\\frac{16S}{\\epsilon}}\\right\\},\\quad r\\geq(\\sqrt{2}+1)\\cdot\\sqrt{\\tau d}+2\\sqrt{\\tau\\log\\frac{8S}{\\epsilon}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Where $z_{\\ast}$ is denoted as the global optimum of the energy function, i.e., $g_{:}$ definedinEq.22.Suppose $\\mathrm{P}(\\|\\mathbf{z}_{0}\\|\\geq R/2)\\leq\\epsilon/4$ and set ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\tau\\leq\\operatorname*{min}\\bigg\\{\\frac{d}{(3L R+\\|\\nabla g(\\mathbf{0})\\|+\\epsilon_{\\mathrm{score}})^{2}},\\frac{16d}{L^{2}R^{2}}\\bigg\\}=\\frac{d}{(3L R+\\|\\nabla g(\\mathbf{0})\\|+\\epsilon_{\\mathrm{score}})^{2}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "then wehave ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(\\mu_{S},\\tilde{\\mu}_{S}\\right)\\leq\\frac{\\epsilon}{4}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. We borrow the proof techniques provided in Lemma 6.1 of [40] to control the TVD gap between the standard and the projected implementation of Alg. 2. ", "page_idx": 26}, {"type": "text", "text": "Particles stay inside $B(\\mathbf{0},R)$ .We first consider the expectation of $\\left\\Vert\\mathbf{z}_{s+1}\\right\\Vert^{2}$ when $\\mathbf{z}_{s}$ is given, and have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\Vert\\mathbf{z}_{s+1}\\Vert^{2}\\,\\middle|z_{s}\\right]=\\int\\Vert z^{\\prime}\\Vert^{2}\\,\\mathcal{T}_{z_{s}}(\\mathrm{d}z^{\\prime})}\\\\ &{=\\int\\Vert z^{\\prime}\\Vert^{2}\\cdot\\left[T_{z_{s}}^{\\prime}(\\mathrm{d}z^{\\prime})\\cdot a_{z_{s}}(z^{\\prime})+\\left(1-\\int a_{z_{s}}(\\tilde{z})T_{z_{s}}^{\\prime}(\\mathrm{d}\\tilde{z})\\right)\\delta_{z_{s}}(\\mathrm{d}z^{\\prime})\\right]}\\\\ &{=\\Vert z_{s}\\Vert^{2}+\\int\\left(\\Vert z^{\\prime}\\Vert^{2}-\\Vert z_{s}\\Vert^{2}\\right)\\cdot a_{z_{s}}(z^{\\prime})T_{z_{s}}^{\\prime}(\\mathrm{d}z^{\\prime})\\cdot}\\\\ &{=\\Vert z_{s}\\Vert^{2}+\\int\\left(\\Vert z^{\\prime}\\Vert^{2}-\\Vert z_{s}\\Vert^{2}\\right)\\cdot a_{z_{s}}(z^{\\prime})\\cdot\\left(\\frac{1}{2}\\cdot\\delta_{z_{s}}(\\mathrm{d}z^{\\prime})+\\frac{1}{2}\\cdot Q_{z_{s}}(\\mathrm{d}z^{\\prime})\\right)}\\\\ &{=\\Vert z_{s}\\Vert^{2}+\\frac{1}{2}\\int\\left(\\Vert z^{\\prime}\\Vert^{2}-\\Vert z_{s}\\Vert^{2}\\right)\\cdot\\operatorname*{min}\\left\\{q(z^{\\prime}\\vert z_{s}),q(z_{s}\\vert z^{\\prime})\\cdot\\exp\\left(-r_{\\theta}(z^{\\prime},z_{s})\\right)\\right\\}\\mathrm{d}z^{\\prime}}\\\\ &{\\leq\\frac{1}{2}\\Vert z_{s}\\Vert^{2}+\\frac{1}{2}\\int\\Vert z^{\\prime}\\Vert^{2}\\cdot q(z^{\\prime}\\vert z_{s})\\mathrm{d}z^{\\prime},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the second equation follows from Eq. 28, the forth equation follows from Eq. 26 and the fifth equation follows from Eq. 27 and Eq. 25. Note that $q(\\pmb{z}^{\\prime}|\\pmb{z}_{s}^{\\bar{}})$ is a Gaussian-type distribution whose mean and variance are $\\ensuremath{\\boldsymbol{z}}_{s}-\\tau\\cdot\\ensuremath{\\boldsymbol{s}}_{\\theta}(\\ensuremath{\\boldsymbol{z}}_{s})$ and $2\\tau$ respectively. It means ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\int\\left\\|\\boldsymbol{z}^{\\prime}\\right\\|^{2}\\cdot q(\\boldsymbol{z}^{\\prime}|\\boldsymbol{z}_{s})\\mathrm{d}\\boldsymbol{z}^{\\prime}=\\left\\|\\boldsymbol{z}_{s}-\\boldsymbol{\\tau}\\cdot\\boldsymbol{s}_{\\theta}(\\boldsymbol{z}_{s})\\right\\|^{2}+2\\tau d.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Suppose $z_{\\ast}$ is the global optimum of the function $g$ due to Lemma C.1, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|z_{s}-\\tau\\cdot s_{\\theta}(z_{s})\\|^{2}=\\|z_{s}\\|^{2}-2\\tau\\cdot z_{s}^{\\top}s_{\\theta}(z_{s})+\\tau^{2}\\cdot\\|s_{\\theta}(z_{s})\\|^{2}}\\\\ &{=\\|z_{s}\\|^{2}-2\\tau\\cdot z_{s}^{\\top}\\nabla g(z_{s})+2\\tau\\cdot z_{s}^{\\top}\\left(s_{\\theta}(z_{s})-\\nabla g(z_{s})\\right)+\\tau^{2}\\cdot\\|s_{\\theta}(z_{s})-\\nabla g(z_{s})+\\nabla g(z_{s})\\|^{2}}\\\\ &{\\le\\|z_{s}\\|^{2}-2\\tau\\cdot\\left(\\frac{L\\left\\|z_{s}\\right\\|^{2}}{2}-\\frac{\\left\\|\\nabla g(0)\\right\\|^{2}}{2L}\\right)+\\tau^{2}\\cdot\\|z_{s}\\|^{2}+\\|s_{\\theta}(z_{s})-\\nabla g(z_{s})\\|^{2}}\\\\ &{\\quad+\\,2\\tau^{2}\\cdot\\|\\nabla g(z_{s})\\|^{2}+2\\tau^{2}\\cdot\\|s_{\\theta}(z_{s})-\\nabla g(z_{s})\\|^{2}}\\\\ &{=\\left(1-L\\tau+\\tau^{2}\\right)\\cdot\\|z_{s}\\|^{2}+\\tau\\cdot\\|\\nabla g(0)\\|^{2}/L+\\left(1+2\\tau^{2}\\right)\\epsilon_{\\mathrm{score}}^{2}+2\\tau^{2}\\cdot\\|\\nabla g(z_{s})\\|^{2}}\\\\ &{\\le\\left(1-L\\tau+(1+36L^{2})\\cdot\\tau^{2}\\right)\\cdot\\|z_{s}\\|^{2}+\\tau\\cdot\\|\\nabla g(0)\\|^{2}/L+4\\tau^{2}\\cdot\\|\\nabla g(0)\\|^{2}+\\left(1+2\\tau^{2}\\right)\\epsilon_{\\mathrm{score}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the first inequality follows from the combination of $L$ -strong convexity of $g$ and Lemma E.3 , the second inequality follows from the $3L$ -smoothness of $g$ The strong convexity and the smoothness of $g$ follow from Lemma C.1. ", "page_idx": 27}, {"type": "text", "text": "Combining Eq. 34, Eq. 35 and Eq. 36, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert\\mathbf{z}_{s+1}\\right\\Vert^{2}\\middle|z_{s}\\right]\\leq\\left(1-\\frac{L\\tau}{2}+\\frac{1+36L^{2}}{2}\\cdot\\tau^{2}\\right)\\cdot\\left\\Vert z_{s}\\right\\Vert^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\left(\\frac{\\tau}{2L}+2\\tau^{2}\\right)\\cdot\\left\\Vert\\nabla g(\\mathbf{0})\\right\\Vert^{2}+\\frac{\\left(1+2\\tau^{2}\\right)\\epsilon_{\\mathrm{score}}^{2}}{2}+\\tau d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By requiring $\\epsilon_{\\mathrm{score}}\\le\\tau\\le L/(2+72L^{2})<1$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\mathbf{z}_{s+1}\\right\\Vert^{2}\\bigg|z_{s}\\right]\\le\\left(1-\\frac{L\\tau}{4}\\right)\\cdot\\left\\Vert z_{s}\\right\\Vert^{2}+\\frac{\\tau}{L}\\cdot\\left\\Vert\\nabla g(\\mathbf{0})\\right\\Vert^{2}+(2+d)\\tau.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Suppose a radio $R$ satisfies ", "page_idx": 27}, {"type": "equation", "text": "$$\nR\\geq8\\cdot\\sqrt{\\frac{\\|\\nabla g(\\mathbf{0})\\|^{2}}{L^{2}}+\\frac{d}{L}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then, if $\\|z_{s}\\|\\geq R/2\\geq4\\sqrt{\\|\\nabla g(\\mathbf{0})\\|^{2}/L^{2}+d/L}$ , it has ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|z_{s}\\right\\|^{2}\\geq16\\cdot\\left(\\frac{\\|\\nabla g(\\mathbf{0})\\|^{2}}{L^{2}}+\\frac{d}{L}\\right)\\geq\\frac{8\\|\\nabla g(\\mathbf{0})\\|^{2}}{L^{2}}+\\frac{8\\cdot(2+d)}{L}}\\\\ {\\Leftrightarrow}&{\\,\\frac{L\\tau\\,\\left\\|z_{s}\\right\\|^{2}}{8}\\geq\\frac{\\tau}{L}\\cdot\\left\\|\\nabla g(\\mathbf{0})\\right\\|^{2}+(2+d)\\tau}\\\\ {\\Leftrightarrow}&{\\,\\mathbb{E}\\left[\\left\\|\\mathbf{z}_{s+1}\\right\\|^{2}\\left|\\mathbf{z}_{s}\\right]\\leq\\left(1-\\frac{L\\tau}{8}\\right)\\cdot\\left\\|z_{s}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "To prove $\\|z_{s}\\|\\quad\\le\\quad R$ for all $s\\ \\ \\leq\\ \\ S$ , we only need to consider $\\mathit{\\Pi}_{z_{s}}$ satisfying $\\begin{array}{r l}{\\|z_{s}\\|}&{{}\\ge}\\end{array}$ $4\\sqrt{\\|\\nabla g(\\mathbf{0})\\|^{2}/L^{2}+d/L}$ , otherwise $\\|z_{s}\\|\\,\\le\\,R/2\\,\\le\\,R$ naturally holds. Then, by the concavity of the function $\\log(\\cdot)$ , for any $\\|z_{s}\\|\\geq^{\\cdot}R/2$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\log(\\|\\mathbf{z}_{s+1}\\|^{2})|\\mathbf{z}_{s}\\right]\\leq\\log\\mathbb{E}\\left[\\|\\mathbf{z}_{s+1}\\|^{2}|\\mathbf{z}_{s}\\right]\\leq\\log(1-\\frac{L\\tau}{4})+\\log(\\|z_{s}\\|^{2})\\leq\\log(\\|z_{s}\\|^{2})-\\frac{L\\tau}{4}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Consider the random variable ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{z}}_{s}:=z_{s}-\\tau\\cdot s_{\\theta}(z_{s})+\\sqrt{2\\tau}\\cdot\\xi\\quad\\mathrm{where}\\quad\\xi\\sim\\mathcal{N}(\\mathbf{0},I)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "obtained by the transition kernel Eq. 24, Note that $\\|\\xi\\|$ is the square root of a $\\chi(d)$ random variable, which is subgaussian and satisfies ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\lVert\\xi\\rVert\\geq\\sqrt{d}+\\sqrt{2}t\\right]\\leq e^{-t^{2}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for any $t\\geq0$ . Under these conditions, requiring ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\tau\\leq(3L R+G+\\epsilon_{\\mathrm{score}})^{-2}\\cdot d\\quad\\mathrm{where}\\quad G:=\\|\\nabla g(\\mathbf{0})\\|\\,,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[\\|\\mathbf{z}_{s+1}\\|-\\|z_{s}\\|\\ge3\\sqrt{\\tau d}+2\\sqrt{\\tau}t\\right]\\le\\mathbb{P}\\left[\\|\\tilde{\\mathbf{z}}_{s}\\|-\\|z_{s}\\|\\ge3\\sqrt{\\tau d}+2\\sqrt{\\tau}t\\right]}\\\\ &{\\le\\mathbb{P}\\left[\\tau\\left\\|s_{\\theta}(z_{s})\\right\\|+\\sqrt{2\\tau}\\left\\|\\xi\\right\\|\\ge3\\sqrt{\\tau d}+2\\sqrt{\\tau}t\\right]\\le\\mathbb{P}\\left[\\sqrt{2\\tau}\\|\\xi\\|\\ge\\sqrt{2\\tau d}+2\\sqrt{\\tau}t\\right]\\le e^{-t^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In Eq. 40, the first inequality follows from the definition of transition kernel $\\mathcal{T}_{z_{s}}$ shown in Eq. 28 and the second inequality follows from ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\tilde{\\mathbf{z}}_{s}\\|-\\|z_{s}\\|\\leq\\tau\\,\\|s_{\\theta}(z_{s})\\|+\\sqrt{2\\tau}\\,\\|\\xi\\|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "According to the fact ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tau\\left\\|s_{\\theta}(z_{s})\\right\\|\\le\\tau\\left\\|\\nabla g(z_{s})\\right\\|+\\tau\\epsilon_{\\mathrm{score}}\\le\\tau\\cdot(\\|\\nabla g(z_{s})-\\nabla g(\\mathbf{0})\\|+\\|\\nabla g(\\mathbf{0})\\|+\\epsilon_{\\mathrm{score}})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\le\\tau\\cdot(3L\\cdot\\|z_{s}\\|+\\|\\nabla g(\\mathbf{0})\\|+\\epsilon_{\\mathrm{score}})\\le\\sqrt{\\tau d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the second inequality follows from the smoothness of $g$ , and the last inequality follows from Eq. 39 and $\\|z_{s}\\|\\le R$ wehave ", "page_idx": 28}, {"type": "equation", "text": "$$\n3\\sqrt{\\tau d}+2\\sqrt{\\tau}t-\\tau\\|s_{\\theta}(z_{s})\\|\\geq\\sqrt{2\\tau d}+2\\sqrt{\\tau}t,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which implies the last inequality of Eq. 40 for all $t\\geq0$ . Furthermore, suppose $\\|z_{s}\\|\\geq R/2$ ,it follows that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\log(\\|z_{s+1}\\|^{2})-\\log(\\|z_{s}\\|^{2})=2\\log(\\|z_{s+1}\\|/\\|z_{s}\\|)\\le\\|z_{s+1}\\|/\\|z_{s}\\|-1\\le\\frac{2\\|z_{s+1}\\|-2\\|z_{s}\\|}{R}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, we have $\\log(\\|z_{s+1}\\|^{2})-\\log(\\|z_{s}\\|^{2})$ is also a sub-Gaussian random variable and satisfies ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left[\\log(\\|\\mathbf{z}_{s+1}\\|^{2})-\\log(\\|z_{s}\\|^{2})\\geq6R^{-1}\\sqrt{\\tau d}+4R^{-1}t\\sqrt{\\tau}\\right]\\leq\\exp(-t^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We consider any subsequence among $\\{\\mathbf{z}_{k}\\}_{k=1}^{S}$ , with all iterates, except the first one, staying outside the region $B(\\mathbf{0},R/2)$ . Denote such subsequence by $\\{\\mathbf{y}_{s}\\}_{s=0}^{S^{\\prime}}$ where $\\|\\mathbf{y}_{0}\\|\\le R/2$ and $S^{\\prime}\\le S$ Then, we know ${\\bf y}_{s}$ and $\\mathbf{y}_{s+1}$ satisfy Eq. 38 and Eq. 42 for all $s\\geq1$ . Under these conditions, by requiring ", "page_idx": 28}, {"type": "text", "text": "$\\|{\\bf z}_{0}\\|\\le R/2$ with a probability at least $1-\\epsilon/16$ we only need to prove all points in $\\{\\mathbf{y}_{s}\\}_{s=0}^{S^{\\prime}}$ will stay inside the region $B(\\mathbf{0},R)$ with probability at least $1-\\epsilon/16$ ", "page_idx": 29}, {"type": "text", "text": "Then, set $\\mathcal{E}_{s}$ to be the event that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{s}=\\left\\{\\|\\mathbf{y}_{s^{\\prime}}\\|\\leq R,\\forall s^{\\prime}\\leq s\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which satisfies $\\mathcal{E}_{s-1}\\subseteq\\mathcal{E}_{s}$ . Besides, suppose the filtration $\\mathcal{F}_{s}=\\{\\mathbf{y}_{0},\\mathbf{y}_{1},\\ldots,\\mathbf{y}_{s}\\}$ , the sequence ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\{\\mathbf{1}(\\mathcal{E}_{s-1})\\cdot\\left(\\log(\\|\\mathbf{y}_{s}\\|^{2}+L s\\tau/4)\\right)\\right\\}_{s=1,2,\\ldots,S}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "is a super-martingale, and the martingale difference has a subgaussian tail, i.e., for any $t\\geq0$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{P}\\left[\\log(\\|\\mathbf{y}_{s+1}\\|^{2})+\\displaystyle\\frac{L(s+1)\\tau}{4}-\\log(\\|\\mathbf{y}_{s}\\|^{2})-\\displaystyle\\frac{L s\\tau}{4}\\geq7R^{-1}\\sqrt{\\tau d}+4R^{-1}t\\sqrt{\\tau d}\\right]}\\\\ {\\leq\\mathbb{P}\\left[\\log(\\|\\mathbf{y}_{s+1}\\|^{2})+\\displaystyle\\frac{L(s+1)\\tau}{4}-\\log(\\|\\mathbf{y}_{s}\\|^{2})-\\displaystyle\\frac{L s\\tau}{4}\\geq6R^{-1}\\sqrt{\\tau d}+4R^{-1}t\\sqrt{\\tau}+\\displaystyle\\frac{L\\tau}{4}\\right]}\\\\ {=\\mathbb{P}\\left[\\log(\\|\\mathbf{z}_{s+1}\\|^{2})-\\log(\\|z_{s}\\|^{2})\\geq6R^{-1}\\sqrt{\\tau d}+4R^{-1}t\\sqrt{\\tau}\\right]\\leq\\exp(-t^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the first inequality is established when ", "page_idx": 29}, {"type": "equation", "text": "$$\n{\\frac{L\\tau}{4}}\\leq{\\frac{\\sqrt{\\tau d}}{R}}\\quad\\Leftrightarrow\\quad\\tau\\leq{\\frac{16d}{L^{2}R^{2}}}\\quad{\\mathrm{and}}\\quad d\\geq1.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Under these conditions, suppose ", "page_idx": 29}, {"type": "equation", "text": "$$\nu=\\frac{6\\sqrt{\\tau d}}{R}+\\frac{4t\\sqrt{\\tau d}}{R}\\quad\\Leftrightarrow\\quad t=\\frac{u R}{4\\sqrt{\\tau d}}-\\frac{3}{2},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "it implies ", "page_idx": 29}, {"type": "equation", "text": "$$\nt^{2}\\geq\\frac{R^{2}u^{2}}{64\\tau d}-1\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which follows from the fact $(a-b)^{2}\\geq a^{2}/4-b^{2}/3$ for all $a,b\\in\\mathbb{R}$ . Then, for any $u\\geq0$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n>\\left[\\log(\\|\\mathbf{y}_{s+1}\\|^{2})+{\\frac{L(s+1)\\tau}{4}}-\\log(\\|\\mathbf{y}_{s}\\|^{2})-{\\frac{L s\\tau}{4}}\\geq u\\right]\\leq\\exp\\left(-{\\frac{R^{2}u^{2}}{64\\tau d}}+1\\right)\\leq3\\exp\\left(-{\\frac{R^{2}u^{2}}{64\\tau d}}\\right)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which implies that the martingale difference is subgaussian. Then by Theorem 2 in [29], for any $s$ wehave ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\log(\\|\\mathbf{y}_{s}\\|^{2})+{\\frac{L s\\tau}{4}}\\leq\\log(\\|\\mathbf{y}_{0}\\|^{2})+{\\frac{74}{R}}\\cdot{\\sqrt{s\\tau d\\log(1/\\epsilon^{\\prime})}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "with the probability at least $1\\mathrm{~-~}\\epsilon^{\\prime}$ conditioned on $\\mathcal{E}_{s-1}$ . Taking the union bound over all $s=$ $1,2,\\ldots,S^{\\prime}$ $(S^{\\prime}\\ \\le\\ S)$ and set $\\epsilon\\;=\\;16\\epsilon^{\\prime}S^{\\prime}$ , we have with probability at least $1\\mathrm{~-~}\\epsilon/16$ , for all $s=1,2,\\ldots,S^{\\prime}$ , it holds ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log(\\|\\mathbf{y}_{s}\\|^{2})\\leq2\\log(R/2)+\\displaystyle\\frac{74}{R}\\cdot\\sqrt{s\\tau d\\log(16S/\\epsilon)}-\\frac{L s\\tau}{4}}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\log(R/2)+\\frac{74^{2}\\cdot d\\log(16S/\\epsilon)}{R^{2}L}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By requiring ", "page_idx": 29}, {"type": "equation", "text": "$$\nR\\geq63\\cdot\\sqrt{\\frac{d}{L}\\log\\frac{16S}{\\epsilon}}\\quad\\Rightarrow\\quad\\frac{74^{2}\\cdot d\\log(16S/\\epsilon)}{R^{2}L}\\leq2\\log2,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "we have $\\log(\\|\\mathbf{y}_{s}\\|^{2})\\leq\\log(R^{2})$ , which is equivalent to $\\|\\mathbf{y}_{s}\\|\\leq R$ Combining with the fact that with probability at least $1-\\epsilon/16$ the initial point $\\mathbf{y}_{0}$ stays inside $B(\\mathbf{0},R/2)$ , we can conclude that with probability at least $1-\\epsilon/8$ alliterates stay inside the region $B(\\mathbf{0},R)$ ", "page_idx": 29}, {"type": "text", "text": "The difference between $\\mathbf{z}_{s+1}$ and $\\mathbf{z}_{s}$ is smaller than $r$ . In this paragraph, we aim to prove $\\|\\mathbf{z}_{s+1}-\\mathbf{z}_{s}\\|\\leq r$ for all $s\\leq S$ . Similar to the previous techniques, we consider ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{z}}_{s}:=\\mathbf{z}_{s}-\\boldsymbol{\\tau}\\cdot s_{\\theta}(\\mathbf{z}_{s})+\\sqrt{2\\boldsymbol{\\tau}}\\cdot\\boldsymbol{\\xi}\\quad\\mathrm{where}\\quad\\boldsymbol{\\xi}\\sim\\mathcal{N}(\\mathbf{0},\\boldsymbol{I}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "According to the transition kernel Eq. 28, it has ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left[\\|\\mathbf{z}_{s+1}-\\mathbf{z}_{s}\\|\\ge r\\right]\\le\\mathbb{P}\\left[\\|\\tilde{\\mathbf{z}}_{s}-\\mathbf{z}_{s}\\|\\ge r\\right]\\le\\mathbb{P}\\left[\\tau\\|s_{\\theta}(\\mathbf{z}_{s})\\|+\\sqrt{2\\tau}\\|\\xi\\|\\ge r\\right]}\\\\ {=\\!\\mathbb{P}\\left[\\|\\xi\\|\\ge\\frac{r-\\tau\\|s_{\\theta}(\\mathbf{z}_{s})\\|}{\\sqrt{2\\tau}}\\right]\\le\\mathbb{P}\\left[\\|\\xi\\|\\ge\\frac{r-\\sqrt{\\tau d}\\|}{\\sqrt{2\\tau}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the second inequality follows from the triangle inequality, and the last inequality follows from Eq. 41 when the choice of $\\tau$ satisfies Eq. 39. Under these conditions, by choosing ", "page_idx": 30}, {"type": "equation", "text": "$$\nr\\geq(\\sqrt{2}+1)\\cdot\\sqrt{r d}+2\\sqrt{\\tau\\log(8S/\\epsilon)}\\quad\\Leftrightarrow\\quad\\frac{r-\\sqrt{\\tau d}}{\\sqrt{2\\tau}}\\geq\\sqrt{d}+\\sqrt{2}\\cdot\\sqrt{\\log(8S/\\epsilon)},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Eq. 45 becomes ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left\\Vert\\mathbf{z}_{s+1}-\\mathbf{z}_{s}\\right\\Vert\\geq r\\right]\\leq\\mathbb{P}\\left[\\left\\Vert\\xi\\right\\Vert\\geq\\frac{r-\\sqrt{\\tau d}\\Vert}{\\sqrt{2\\tau}}\\right]\\leq\\mathbb{P}\\left[\\left\\Vert\\xi\\right\\Vert\\geq\\sqrt{d}+\\sqrt{2}\\cdot\\sqrt{\\log(8S/\\epsilon)}\\right]\\leq\\frac{\\epsilon}{8S},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which means ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left\\lVert\\mathbf{z}_{s+1}-\\mathbf{z}_{s}\\right\\rVert\\leq r\\right]\\geq1-\\frac{\\epsilon}{8S}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Taingdlwwaatisyi $\\|\\mathbf{z}_{s+1}-$ $\\mathbf{z}_{s}\\mathinner{\\|{\\mathbf{\\psi}\\leq\\boldsymbol{r}}\\end{array}$ with the probability at least $1-\\epsilon/8$ . Hence, the proof is completed. ", "page_idx": 30}, {"type": "text", "text": "C.2 Control the error from the approximation of score and energy ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Lemma C.3. Under Assumption [A1]-[A2], we set ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\eta=\\frac{1}{2}\\log\\frac{2L+1}{2L}\\quad\\mathrm{and}\\quad G:=\\|\\nabla g(\\mathbf{0})\\|\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For any $\\epsilon\\in(0,1)$ , we set ", "page_idx": 30}, {"type": "equation", "text": "$$\nR\\geq\\operatorname*{max}\\left\\{8\\cdot\\sqrt{\\frac{\\|\\nabla g(\\mathbf{0})\\|^{2}}{L^{2}}+\\frac{d}{L}},63\\cdot\\sqrt{\\frac{d}{L}\\log\\frac{16S}{\\epsilon}}\\right\\},\\quad r=3\\cdot\\sqrt{\\tau d\\log\\frac{8S}{\\epsilon}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Suppose it has ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{\\delta}{16}:=\\frac{3\\epsilon_{\\mathrm{score}}}{2}\\cdot\\sqrt{\\tau d\\log\\frac{8S}{\\epsilon}}+\\frac{\\tau\\epsilon_{\\mathrm{score}}^{2}}{4}+\\frac{\\tau(3L R+G)\\epsilon_{\\mathrm{score}}}{2}\\leq\\frac{1}{32}\\quad\\mathrm{and}\\quad\\epsilon_{\\mathrm{energy}}\\leq\\frac{1}{10},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1-\\delta-5\\epsilon_{\\mathrm{energy}})\\cdot\\tilde{\\mathcal{T}}_{*,z}(\\Omega_{z}^{\\prime})\\leq\\tilde{\\mathcal{T}}_{z}(\\Omega_{z}^{\\prime})\\leq(1+\\delta+5\\epsilon_{\\mathrm{energy}})\\cdot\\tilde{\\mathcal{T}}_{*,z}(\\Omega_{z}^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for any set $A\\subseteq B(0,R)$ and point $z\\in B(0,R)$ ", "page_idx": 30}, {"type": "text", "text": "Proof. Note that the Markov process defined by $\\tilde{\\mathcal{T}}_{z}(\\cdot)$ and $\\tilde{\\mathcal{T}}_{\\ast,z}(\\cdot)$ are $1/2$ -lazy. We prove the lemma by considering two cases: $z\\not\\in A$ and $z\\in{\\mathcal{A}}$ ", "page_idx": 30}, {"type": "text", "text": "When $z\\not\\in A$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\tilde{\\cal T}_{z}(A)=\\int_{A}\\tilde{a}_{z}(z^{\\prime})\\tilde{\\cal T}_{z}^{\\prime}(\\mathrm{d}z^{\\prime})=\\frac{1}{2}\\int_{A}\\tilde{a}_{z}(z^{\\prime})\\tilde{Q}_{z}^{\\prime}(\\mathrm{d}z^{\\prime})\\ ~}}\\\\ {{\\displaystyle~~~~~~~=\\frac{1}{2}\\int_{A\\cap\\Omega_{z}}\\tilde{a}_{z}(z^{\\prime})\\tilde{Q}_{z}(\\mathrm{d}z^{\\prime})=\\frac{1}{2}\\int_{A\\cap\\Omega_{z}}\\tilde{a}_{z}(z^{\\prime})\\tilde{q}(z^{\\prime}|z)\\mathrm{d}z^{\\prime}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Similarly, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{T}}_{*,z}(A)=\\frac{1}{2}\\int_{\\mathcal{A}\\cap\\Omega_{z}}\\tilde{a}_{*,z}(z^{\\prime})\\tilde{q}_{*}(z^{\\prime}|z)\\mathrm{d}z^{\\prime}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "In this condition, we consider ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{2\\tilde{\\mathcal{T}}_{z}(A)-2\\tilde{\\mathcal{T}}_{*,z}(A)=-\\displaystyle\\int_{A\\cap\\Omega_{z}}\\tilde{a}_{*,z}(z^{\\prime})\\tilde{q}_{*}(z^{\\prime}|z)\\mathrm{d}z^{\\prime}+\\displaystyle\\int_{A\\cap\\Omega_{z}}\\tilde{a}_{*,z}(z^{\\prime})\\tilde{q}(z^{\\prime}|z)\\mathrm{d}z^{\\prime}}\\\\ &{}&{\\qquad\\qquad\\qquad-\\displaystyle\\int_{A\\cap\\Omega_{z}}\\tilde{a}_{*,z}(z^{\\prime})\\tilde{q}(z^{\\prime}|z)\\mathrm{d}z^{\\prime}+\\displaystyle\\int_{A\\cap\\Omega_{z}}\\tilde{a}_{z}(z^{\\prime})\\tilde{q}(z^{\\prime}|z)\\mathrm{d}z^{\\prime},\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which means ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\tilde{\\mathcal{I}}_{z}(A)-\\tilde{\\mathcal{I}}_{*,z}(A)}{\\tilde{\\mathcal{I}}_{*,z}(A)}=\\underbrace{\\frac{\\int_{A\\cap\\Omega_{z}}\\tilde{a}_{*,z}(z^{\\prime})\\cdot\\left(\\tilde{q}(z^{\\prime}|z)-\\tilde{q}_{*}(z^{\\prime}|z)\\right)\\mathrm{d}z^{\\prime}}{\\int_{A\\cap\\Omega_{z}}\\tilde{a}_{*,z}(z^{\\prime})\\tilde{q}_{*}(z^{\\prime}|z)\\mathrm{d}z^{\\prime}}}_{\\mathrm{Term~1}}}\\\\ {+\\underbrace{\\frac{\\int_{A\\cap\\Omega_{z}}\\left(\\tilde{a}_{z}(z^{\\prime})-\\tilde{a}_{*,z}(z^{\\prime})\\right)\\tilde{q}(z^{\\prime}|z)\\mathrm{d}z^{\\prime}}{\\int_{A\\cap\\Omega_{z}}\\tilde{a}_{*,z}(z^{\\prime})\\tilde{q}_{*}(z^{\\prime}|z)\\mathrm{d}z^{\\prime}}}_{\\mathrm{Term~2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "First, we try to control Term 1, which can be achieved by investigating $\\tilde{q}(z^{\\prime}|z)/\\tilde{q}_{*}(z^{\\prime}|z)$ as follows. ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{\\tilde{q}(z^{\\prime}|z)}{\\tilde{q}_{*}(z^{\\prime}|z)}=\\exp\\left(-\\frac{\\left\\|z^{\\prime}-(z-\\tau\\cdot s_{\\theta}(z))\\right\\|^{2}}{4\\tau}+\\frac{\\left\\|z^{\\prime}-(z-\\tau\\cdot\\nabla g(z))\\right\\|^{2}}{4\\tau}\\right),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "In this condition, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\tilde{q}\\left(z^{\\prime}|z\\right)}{\\tilde{q}_{*}\\left(z^{\\prime}|z\\right)}=\\exp\\left((4\\tau)^{-1}\\cdot\\left(-\\left\\Vert z^{\\prime}-z\\right\\Vert^{2}-2\\tau\\cdot\\left(z^{\\prime}-z\\right)^{\\top}s_{\\theta}(z)-\\tau^{2}\\cdot\\left\\Vert s_{\\theta}(z)\\right\\Vert^{2}\\right.\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.+\\left\\Vert z^{\\prime}-z\\right\\Vert^{2}+2\\tau\\cdot\\left(z^{\\prime}-z\\right)^{\\top}\\nabla g(z)+\\tau^{2}\\cdot\\left\\Vert\\nabla g(z)\\right\\Vert^{2}\\right)\\right)}\\\\ &{\\qquad\\qquad\\left.=\\exp\\left(\\frac{1}{2}(z^{\\prime}-z)^{\\top}\\left(-s_{\\theta}(z)+\\nabla g(z)\\right)+\\frac{\\tau}{4}\\left(-\\left\\Vert s_{\\theta}(z)\\right\\Vert^{2}+\\left\\Vert\\nabla g(z)\\right\\Vert^{2}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "It means ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\ln\\frac{\\tilde{q}(z^{\\prime}|z)}{\\tilde{q}_{*}(z^{\\prime}|z)}\\bigg|=\\bigg|\\frac{1}{2}(z^{\\prime}-z)^{\\top}\\left(-s_{\\theta}(z)+\\nabla g(z)\\right)+\\frac{\\tau}{4}\\left(-\\left\\|s_{\\theta}(z)\\right\\|^{2}+\\left\\|\\nabla g(z)\\right\\|^{2}\\right)\\bigg|}}\\\\ &{\\leq\\frac{1}{2}\\left\\|z^{\\prime}-z\\right\\|\\cdot\\left\\|s_{\\theta}(z)-\\nabla g(z)\\right\\|+\\frac{\\tau}{4}\\cdot\\left[\\left\\|s_{\\theta}(z)+\\nabla g(z)\\right\\|\\cdot\\left\\|s_{\\theta}(z)-\\nabla g(z)\\right\\|\\right]}\\\\ &{\\leq\\frac{1}{2}\\left\\|z^{\\prime}-z\\right\\|\\cdot\\left\\|s_{\\theta}(z)-\\nabla g(z)\\right\\|+\\frac{\\tau}{4}\\cdot\\left\\|s_{\\theta}(z)-\\nabla g(z)\\right\\|^{2}+\\frac{\\tau}{2}\\cdot\\left\\|\\nabla g(z)\\right\\|\\cdot\\left\\|s_{\\theta}(z)-\\nabla g(z)\\right\\|^{2}}\\\\ &{\\leq\\frac{r\\epsilon_{\\mathrm{score}}}{2}+\\frac{\\tau\\epsilon_{\\mathrm{scroe}}^{2}}{4}+\\frac{\\tau(3L R+G)\\epsilon_{\\mathrm{score}}}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the last inequality follows from the fact $z^{\\prime}\\in\\mathcal{B}(z,r)\\cap\\mathcal{B}(\\mathbf{0},R)/\\{z\\}$ ${\\boldsymbol{z}}\\in B(\\mathbf{0},R)$ and ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla g(z)\\|=\\|\\nabla g(z)-\\nabla g(\\mathbf{0})+\\nabla g(\\mathbf{0})\\|\\leq3L\\cdot\\|z\\|+G\\leq3L R+G.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "According to the definition of $R$ and r shown in Lemma C.2, we choose ", "page_idx": 31}, {"type": "equation", "text": "$$\nr:=3\\cdot\\sqrt{\\tau}\\cdot\\sqrt{d\\log\\frac{8S}{\\epsilon}}\\geq(\\sqrt{2}+1)\\cdot\\sqrt{\\tau d}+2\\sqrt{\\tau\\log\\frac{8S}{\\epsilon}}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Under this condition, we require ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{\\delta}{16}:=\\frac{3\\epsilon_{\\mathrm{score}}}{2}\\cdot\\sqrt{\\tau d\\log\\frac{8S}{\\epsilon}}+\\frac{\\tau\\epsilon_{\\mathrm{score}}^{2}}{4}+\\frac{\\tau(3L R+G)\\epsilon_{\\mathrm{score}}}{2}\\leq\\frac{1}{32},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "then we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\ln\\left(1-\\frac{\\delta}{8}\\right)\\leq\\ln\\frac{\\tilde{q}(z^{\\prime}|z)}{\\tilde{q}_{*}(z^{\\prime}|z)}\\leq\\ln\\left(1+\\frac{\\delta}{8}\\right)\\quad\\Leftrightarrow\\quad1-\\frac{\\delta}{8}<\\frac{\\tilde{q}(z^{\\prime}|z)}{\\tilde{q}_{*}(z^{\\prime}|z)}\\leq1+\\frac{\\delta}{8},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and ", "page_idx": 32}, {"type": "equation", "text": "$$\n-\\frac{\\delta}{8}\\leq\\operatorname*{min}_{z^{\\prime}\\in A\\cap\\Omega_{z}}\\frac{\\tilde{q}(z^{\\prime}|z)}{\\tilde{q}_{*}(z^{\\prime}|z)}-1\\leq\\mathrm{Term}\\,1\\leq\\operatorname*{max}_{z^{\\prime}\\in A\\cap\\Omega_{z}}\\frac{\\tilde{q}(z^{\\prime}|z)}{\\tilde{q}_{*}(z^{\\prime}|z)}-1\\leq\\frac{\\delta}{8}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "with the definition of Term 1 shown in Eq. 46. ", "page_idx": 32}, {"type": "text", "text": "Then, we try to control Term 2 of Eq. 46 and have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{\\int_{A\\cap\\Omega_{\\varepsilon}}\\left(\\tilde{a}_{z}(z^{\\prime})-\\tilde{a}_{*,z}(z^{\\prime})\\right)\\tilde{q}(z^{\\prime}|z)\\mathrm{d}z^{\\prime}}{\\int_{A\\cap\\Omega_{\\varepsilon}}\\tilde{a}_{*,z}(z^{\\prime})\\tilde{q}_{*}(z^{\\prime}|z)\\mathrm{d}z^{\\prime}}=\\frac{\\int_{A\\cap\\Omega_{\\varepsilon}}\\left(\\tilde{a}_{z}(z^{\\prime})-\\tilde{a}_{*,z}(z^{\\prime})\\right)\\tilde{q}(z^{\\prime}|z)\\mathrm{d}z^{\\prime}}{\\int_{A\\cap\\Omega_{\\varepsilon}}\\tilde{a}_{*,z}(z^{\\prime})\\tilde{q}(z^{\\prime}|z)\\mathrm{d}z^{\\prime}}\\cdot\\frac{\\int_{A\\cap\\Omega_{\\varepsilon}}\\tilde{a}_{*,z}(z^{\\prime})\\tilde{q}(z^{\\prime})\\mathrm{d}z}{\\int_{A\\cap\\Omega_{\\varepsilon}}\\tilde{a}_{*,z}(z^{\\prime})\\tilde{q}_{*}(z^{\\prime})\\mathrm{d}z^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "According to Eq. 49, it has ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{1-\\displaystyle\\frac{\\delta}{8}\\leq\\operatorname*{min}_{z^{\\prime}\\in A\\cap\\Omega_{z}}\\frac{\\tilde{q}(z^{\\prime}|z)}{\\tilde{q}_{*}(z^{\\prime}|z)}\\leq\\frac{\\int_{A\\cap\\Omega_{z}}\\tilde{a}_{*,z}(z^{\\prime})\\tilde{q}(z^{\\prime}|z)\\mathrm{d}z^{\\prime}}{\\int_{A\\cap\\Omega_{z}}\\tilde{a}_{*,z}(z^{\\prime})\\tilde{q}_{*}(z^{\\prime}|z)\\mathrm{d}z^{\\prime}}\\leq\\operatorname*{max}_{z^{\\prime}\\in A\\cap\\Omega_{z}}\\frac{\\tilde{q}(z^{\\prime}|z)}{\\tilde{q}_{*}(z^{\\prime}|z)}\\leq1+\\displaystyle\\frac{\\delta}{8},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "then we can upper and lower bounding Term 2 by investigating $\\tilde{a}_{z}(z^{\\prime})/\\tilde{a}_{*,z}(z^{\\prime})$ asfollows ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{a}_{\\ast,z}(z^{\\prime})=\\operatorname*{min}\\left\\{1,\\exp\\left(-(g(z^{\\prime})-g(z))\\right)\\cdot\\frac{\\tilde{q}_{\\ast}\\left(z\\mid z^{\\prime}\\right)}{\\tilde{q}_{\\ast}\\left(z^{\\prime}\\mid z\\right)}\\right\\},}\\\\ &{\\tilde{a}_{z}(z^{\\prime})=\\operatorname*{min}\\left\\{1,\\exp\\left(-r_{\\theta}(z^{\\prime},z)\\right)\\cdot\\frac{\\tilde{q}\\left(z\\mid z^{\\prime}\\right)}{\\tilde{q}\\left(z^{\\prime}\\mid z\\right)}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "In this condition, for any $0<\\delta\\leq1$ , we first consider two cases. When ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{\\iota}_{*,z}(z^{\\prime})=1\\leq\\exp\\left(-(g(z^{\\prime})-g(z))\\right)\\cdot\\frac{\\tilde{q}_{*}(z|z^{\\prime})}{\\tilde{q}_{*}(z^{\\prime}|z)}\\quad\\mathrm{and}\\quad\\tilde{a}_{z}(z^{\\prime})=\\exp\\left(-r_{\\theta}(z^{\\prime},z)\\right)\\cdot\\frac{\\tilde{q}(z|z^{\\prime})}{\\tilde{q}(z^{\\prime}|z)}\\leq1,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\underbrace{\\exp\\left(-r_{\\theta}(z^{\\prime},z)\\right)\\cdot\\frac{\\tilde{q}(z|z^{\\prime})}{\\tilde{q}(z^{\\prime}|z)}}_{\\mathrm{exp}\\,(-(g(z^{\\prime})-g(z)))\\cdot\\,\\frac{\\tilde{q}_{*}(z|z^{\\prime})}{\\tilde{q}_{*}(z|z)}}\\leq\\frac{\\tilde{a}_{z}(z^{\\prime})}{\\tilde{a}_{*,z}(z^{\\prime})}=\\exp\\left(-r_{\\theta}(z^{\\prime},z)\\right)\\cdot\\frac{\\tilde{q}(z|z^{\\prime})}{\\tilde{q}(z^{\\prime}|z)}\\leq1.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Besides, when ", "text_level": 1, "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{\\iota}_{*,z}(z^{\\prime})=\\exp\\left(-(g(z^{\\prime})-g(z))\\right)\\cdot\\frac{\\tilde{q}_{*}(z|z^{\\prime})}{\\tilde{q}_{*}(z^{\\prime}|z)}\\leq1\\quad\\mathrm{and}\\quad\\tilde{a}_{z}(z^{\\prime})=1\\leq\\exp\\left(-r_{\\theta}(z^{\\prime},z)\\right)\\cdot\\frac{\\tilde{q}(z|z^{\\prime})}{\\tilde{q}(z^{\\prime}|z)},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n1\\leq\\frac{\\tilde{a}_{z}(z^{\\prime})}{\\tilde{a}_{*,z}(z^{\\prime})}=\\frac{1}{\\exp\\left(-(g(z^{\\prime})-g(z))\\right)\\cdot\\frac{\\tilde{q}_{*}(z|z^{\\prime})}{\\tilde{q}_{*}(z^{\\prime}|z)}}\\leq\\frac{\\exp\\left(-r_{\\theta}(z^{\\prime},z)\\right)\\cdot\\frac{\\tilde{q}(z|z^{\\prime})}{\\tilde{q}(z^{\\prime}|z)}}{\\exp\\left(-(g(z^{\\prime})-g(z))\\right)\\cdot\\frac{\\tilde{q}_{*}(z|z^{\\prime})}{\\tilde{q}_{*}(z^{\\prime}|z)}}\\,.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then, we start to consider finding the range of $\\mathrm{ln(Term\\;2.1)}$ as follows ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\ln\\left(\\mathrm{Term~2.1}\\right)\\right|=\\left|(-r_{\\theta}(z^{\\prime},z)+(g(z^{\\prime})-g(z)))+\\ln\\frac{\\tilde{q}_{*}(z^{\\prime}|z)}{\\tilde{q}(z^{\\prime}|z)}+\\ln\\frac{\\tilde{q}(z|z^{\\prime})}{\\tilde{q}_{*}(z|z^{\\prime})}\\right|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\epsilon_{\\mathrm{energy}}+\\left|\\ln\\frac{\\tilde{q}_{*}(z^{\\prime}|z)}{\\tilde{q}(z^{\\prime}|z)}\\right|+\\left|\\ln\\frac{\\tilde{q}(z|z^{\\prime})}{\\tilde{q}_{*}(z|z^{\\prime})}\\right|\\leq\\epsilon_{\\mathrm{energy}}+\\frac{\\delta}{16}+\\left|\\ln\\frac{\\tilde{q}(z|z^{\\prime})}{\\tilde{q}_{*}(z|z^{\\prime})}\\right|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the last inequality follows from Eq. 48. Besides, similar to Eq. 47, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\tilde{q}(z|z^{\\prime})}{\\tilde{q}_{*}(z|z^{\\prime})}=\\exp\\left((4\\tau)^{-1}\\cdot\\left(-\\left\\|z-z^{\\prime}\\right\\|^{2}-2\\tau\\cdot(z-z^{\\prime})^{\\top}s_{\\theta}(z^{\\prime})-\\tau^{2}\\cdot\\left\\|s_{\\theta}(z^{\\prime})\\right\\|^{2}\\right.\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.\\left.+\\left\\|z-z^{\\prime}\\right\\|^{2}+2\\tau\\cdot(z-z^{\\prime})^{\\top}\\nabla g(z^{\\prime})+\\tau^{2}\\cdot\\left\\|\\nabla g(z^{\\prime})\\right\\|^{2}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "which means ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ln\\frac{\\tilde{q}\\left(z\\left|z^{\\prime}\\right>\\right)}{\\tilde{q}_{*}\\left(z\\left|z^{\\prime}\\right>\\right)}\\bigg|=\\bigg|\\frac{1}{2}(z-z^{\\prime})^{\\top}(-s_{\\theta}(z^{\\prime})+\\nabla g(z^{\\prime}))+\\frac{\\tau}{4}\\left(-\\left\\|s_{\\theta}(z^{\\prime})\\right\\|^{2}+\\left\\|\\nabla g(z^{\\prime})\\right\\|^{2}\\right)\\bigg|}\\\\ &{\\qquad\\qquad\\leq\\frac{1}{2}\\left\\|z-z^{\\prime}\\right\\|\\cdot\\left\\|s_{\\theta}(z^{\\prime})-\\nabla g(z^{\\prime})\\right\\|+\\frac{\\tau}{4}\\cdot\\left\\|s_{\\theta}(z^{\\prime})+\\nabla g(z^{\\prime})\\right\\|\\cdot\\left\\|s_{\\theta}(z^{\\prime})-\\nabla g(z^{\\prime})\\right\\|}\\\\ &{\\qquad\\qquad\\leq\\frac{1}{2}\\left\\|z-z^{\\prime}\\right\\|\\cdot\\left\\|s_{\\theta}(z^{\\prime})-\\nabla g(z^{\\prime})\\right\\|+\\frac{\\tau}{4}\\cdot\\left\\|s_{\\theta}(z^{\\prime})-\\nabla g(z^{\\prime})\\right\\|^{2}+\\frac{\\tau}{2}\\left\\|\\nabla g(z^{\\prime})\\right\\|\\cdot\\left\\|s_{\\theta}(z^{\\prime})\\right\\|}\\\\ &{\\qquad\\qquad\\leq\\frac{r\\epsilon_{\\mathrm{score}}}{2}+\\frac{\\tau\\epsilon_{\\mathrm{score}}^{2}}{4}+\\frac{\\tau(3L R+G)\\epsilon_{\\mathrm{score}}}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the last inequality follows from the fact $z^{\\prime}\\in\\mathcal{B}(z,r)\\cap\\mathcal{B}(\\mathbf{0},R)/\\{z\\}$ and ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla g(z^{\\prime})\\|=\\|\\nabla g(z^{\\prime})-\\nabla g(\\mathbf{0})+\\nabla g(\\mathbf{0})\\|\\leq3L\\cdot\\|z^{\\prime}\\|+G\\leq3L R+G.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Combining this result with Eq. 48, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left|\\ln\\frac{\\tilde{q}(z|z^{\\prime})}{\\tilde{q}_{*}(z|z^{\\prime})}\\right|\\le\\frac{\\delta}{16}\\quad\\Leftrightarrow\\quad\\frac{\\delta}{16}=\\frac{3\\epsilon_{\\mathrm{score}}}{2}\\cdot\\sqrt{\\tau d\\log\\frac{8S}{\\epsilon}}+\\frac{\\tau\\epsilon_{\\mathrm{score}}^{2}}{4}+\\frac{\\tau(3L R+G)\\epsilon_{\\mathrm{score}}}{2}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Plugging this result into Eq. 54, it has ", "page_idx": 33}, {"type": "equation", "text": "$$\n|\\ln{\\left(\\mathrm{Term}\\,2.1\\right)}|\\leq\\frac{\\delta}{8}+\\epsilon_{\\mathrm{energy}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "By requiring $\\epsilon_{\\mathrm{energy}}\\leq0.1\\$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ln{\\left(1-\\frac{\\delta}{4}-2\\epsilon_{\\mathrm{energy}}\\right)}\\leq\\ln{\\left(\\mathrm{Term~2.1}\\right)}\\leq\\ln{\\left(1+\\frac{\\delta}{4}+2\\epsilon_{\\mathrm{energy}}\\right)}}\\\\ &{\\Leftrightarrow\\quad1-\\frac{\\delta}{4}-2\\epsilon_{\\mathrm{energy}}\\leq\\mathrm{Term~2.1}\\leq1+\\frac{\\delta}{4}+2\\epsilon_{\\mathrm{energy}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Combining this result with Eq. 52 and Eq. 53, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n1-\\frac{\\delta}{4}-2\\epsilon_{\\mathrm{energy}}\\leq\\frac{a_{z}(z^{\\prime})}{a_{*,z}(z^{\\prime})}\\leq1+\\frac{\\delta}{4}+2\\epsilon_{\\mathrm{energy}},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which implies ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\int_{A\\cap\\Omega_{z}}\\left(\\tilde{a}_{z}\\left(z^{\\prime}\\right)-\\tilde{a}_{*,z}\\left(z^{\\prime}\\right)\\right)\\tilde{q}(z^{\\prime}|z)\\mathrm{d}z^{\\prime}}{\\int_{A\\cap\\Omega_{z}}\\tilde{a}_{*,z}\\left(z^{\\prime}\\right)\\tilde{q}(z^{\\prime}|z)\\mathrm{d}z^{\\prime}}\\geq\\underset{z^{\\prime}\\in A}{\\operatorname*{min}}\\frac{\\tilde{a}_{z}\\left(z^{\\prime}\\right)}{\\tilde{a}_{*,z}\\left(z^{\\prime}\\right)}-1\\geq-\\frac{\\delta}{4}-2\\epsilon_{\\mathrm{energy}}}\\\\ &{\\frac{\\int_{A\\cap\\Omega_{z}}\\left(\\tilde{a}_{z}\\left(z^{\\prime}\\right)-\\tilde{a}_{*,z}\\left(z^{\\prime}\\right)\\right)\\tilde{q}(z^{\\prime}|z)\\mathrm{d}z^{\\prime}}{\\int_{A\\cap\\Omega_{z}}\\tilde{a}_{*,z}\\left(z^{\\prime}\\right)\\tilde{q}(z^{\\prime}|z)\\mathrm{d}z^{\\prime}}\\leq\\underset{z^{\\prime}\\in A}{\\operatorname*{max}}\\frac{\\tilde{a}_{z}\\left(z^{\\prime}\\right)}{\\tilde{a}_{*,z}\\left(z^{\\prime}\\right)}-1\\leq\\frac{\\delta}{4}+2\\epsilon_{\\mathrm{energy}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Plugging Eq. 55 and Eq. 51 into Eq. 50, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{\\delta}{3}-\\frac{5\\epsilon_{\\mathrm{energy}}}{2}\\leq\\bigg(-\\frac{\\delta}{4}-2\\epsilon_{\\mathrm{energy}}\\bigg)\\cdot\\bigg(1+\\frac{\\delta}{8}\\bigg)\\leq\\mathrm{Term~}2\\leq\\bigg(\\frac{\\delta}{4}+2\\epsilon_{\\mathrm{energy}}\\bigg)\\cdot\\bigg(1+\\frac{\\delta}{8}\\bigg)\\leq\\frac{\\delta}{3}+\\frac{5\\epsilon_{\\mathrm{energy}}}{2}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "In this condition, combining Eq. 56, Eq. 49 with Eq. 46, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\;\\frac{\\delta+5\\epsilon_{\\mathrm{energy}}}{2}\\leq\\frac{\\tilde{\\mathcal{T}}_{z}(A)-\\tilde{\\mathcal{T}}_{*,z}(A)}{\\tilde{\\mathcal{T}}_{*,z}(A)}\\leq\\frac{\\delta+5\\epsilon_{\\mathrm{energy}}}{2}}\\\\ &{\\Leftrightarrow\\quad\\bigg(1-\\frac{\\delta+5\\epsilon_{\\mathrm{energy}}}{2}\\bigg)\\cdot\\tilde{\\mathcal{T}}_{*,z}(A)\\leq\\tilde{\\mathcal{T}}_{z}(A)\\leq\\bigg(1+\\frac{\\delta+5\\epsilon_{\\mathrm{energy}}}{2}\\bigg)\\cdot\\tilde{\\mathcal{T}}_{*,z}(A).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Hence, we complete the proof for $z\\not\\in A$ ", "page_idx": 33}, {"type": "text", "text": "When $z\\in{\\mathcal{A}}$ , suppose there exist some $r^{\\prime}$ satisfying ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\Omega_{z}^{\\prime}:=B(z,r^{\\prime})\\subseteq A.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We can split $\\boldsymbol{\\mathcal{A}}$ into $A-\\Omega_{z}^{\\prime}$ and $\\Omega_{z}^{\\prime}$ . Note that by our results in the first case, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left(1-\\frac{\\delta+5\\epsilon_{\\mathrm{energy}}}{2}\\right)\\cdot\\tilde{\\mathcal{T}}_{*,z}(A-\\Omega_{z}^{\\prime})\\leq\\tilde{\\mathcal{T}}_{z}(A-\\Omega_{z}^{\\prime})\\leq\\left(1+\\frac{\\delta+5\\epsilon_{\\mathrm{energy}}}{2}\\right)\\cdot\\tilde{\\mathcal{T}}_{*,z}(A-\\Omega_{z}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then for the set $\\Omega_{z}^{\\prime}$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\frac{\\tilde{\\mathcal{T}}_{z}(\\Omega_{z}^{\\prime})-\\tilde{\\mathcal{T}}_{*,z}(\\Omega_{z}^{\\prime})}{\\tilde{\\mathcal{T}}_{*,z}(\\Omega_{z}^{\\prime})}\\right|=\\left|\\frac{\\left(1-\\tilde{\\mathcal{T}}_{z}(\\Omega-\\Omega_{z}^{\\prime})\\right)-\\left(1-\\tilde{\\mathcal{T}}_{*,z}(\\Omega-\\Omega_{z}^{\\prime})\\right)}{\\tilde{\\mathcal{T}}_{*,z}(\\Omega_{z}^{\\prime})}\\right|}\\\\ &{=\\left|\\frac{\\tilde{\\mathcal{T}}_{*,z}(\\Omega-\\Omega_{z}^{\\prime})-\\tilde{\\mathcal{T}}_{z}(\\Omega-\\Omega_{z}^{\\prime})}{\\tilde{\\mathcal{T}}_{*,z}(\\Omega_{z}^{\\prime})}\\right|\\le\\left|\\frac{\\tilde{\\mathcal{T}}_{*,z}(\\Omega-\\Omega_{z}^{\\prime})-\\tilde{\\mathcal{T}}_{z}(\\Omega-\\Omega_{z}^{\\prime})}{\\tilde{\\mathcal{T}}_{*,z}(\\Omega-\\Omega_{z}^{\\prime})}\\right|\\cdot\\left|\\frac{\\tilde{\\mathcal{T}}_{*,z}(\\Omega-\\Omega_{z}^{\\prime})}{\\tilde{\\mathcal{T}}_{*,z}(\\Omega_{z}^{\\prime})}\\right|}\\\\ &{\\le\\frac{\\delta+5\\epsilon_{\\mathrm{energy}}}{2}\\cdot2=\\delta+5\\epsilon_{\\mathrm{energy}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the last inequality follows from Eq. 57 and the property of $1/2$ lazy,i.e., ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{T}}_{*,z}(\\Omega-\\Omega_{z}^{\\prime})\\leq1\\quad\\mathrm{and}\\quad\\tilde{\\mathcal{T}}_{*,z}(\\Omega_{z}^{\\prime})\\geq\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "In this condition, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1-\\delta-5\\epsilon_{\\mathrm{energy}})\\cdot\\tilde{\\mathcal{T}}_{*,z}(\\Omega_{z}^{\\prime})\\leq\\tilde{\\mathcal{T}}_{z}(\\Omega_{z}^{\\prime})\\leq(1+\\delta+5\\epsilon_{\\mathrm{energy}})\\cdot\\tilde{\\mathcal{T}}_{*,z}(\\Omega_{z}^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Hence, we complete the proof for $z\\in{\\mathcal{A}}$ ", "page_idx": 34}, {"type": "text", "text": "Corollary C.4. Under the same conditions as shown in Lemma C.3, if we require ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\epsilon_{\\mathrm{energy}}\\le\\delta/5,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "then we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n(1-2\\delta)\\cdot\\tilde{\\mathcal{T}}_{*,z}(A)\\leq\\tilde{\\mathcal{T}}_{z}(A)\\leq(1+2\\delta)\\cdot\\tilde{\\mathcal{T}}_{*,z}(A),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "for any set $A\\subseteq B(0,R)$ and point $z\\in B(0,R)$ ", "page_idx": 34}, {"type": "text", "text": "C.3 Control the error from Inner MALA to its stationary ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this section, we denote the ideally projected implementation of Alg. 2 whose Markov process, transition kernel, and particles' underlying distributions are denoted as $\\{\\widetilde{\\mathbf{z}}_{\\ast,s}\\}_{s=0}^{S}$ ,Eq. 33,and $\\tilde{\\mu}_{\\ast,s}$ respectively. According to [40], we know the stationary distribution of the time-reversible process $\\{\\tilde{\\mathbf{z}}_{\\ast,s}^{}\\}_{s=0}^{S}$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\tilde{\\mu}_{*}(\\mathrm{d}z)=\\left\\{\\frac{e^{-g(z)}}{\\int_{\\Omega}e^{-g(z^{\\prime})}\\mathrm{d}z^{\\prime}}\\mathrm{d}z\\right.\\ \\ \\ \\ \\ \\left.x\\in\\Omega;\\right.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Here, we denote ${\\boldsymbol\\Omega}=B(\\mathbf{0},R)$ and ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\Omega_{z}=\\mathcal{B}(\\mathbf{0},R)\\cap\\mathcal{B}(z,r).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "In the following analysis, we default ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\eta=\\frac{1}{2}\\log\\frac{2L+1}{2L}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Under this condition, the smoothness of $g$ is $3L$ and the strong convexity constant is $L$ we aim to build the connection between the underlying distribution of the output particles obtained by projected $\\mathrm{Alg~}2$ i.e., $\\tilde{\\mu}_{S}$ and thestationary distribution $\\tilde{\\mu}_{*}$ though the process $\\{\\widetilde{\\mathbf{z}}_{\\ast,s}\\}_{s=0}^{S}$ Since the ideally projected implementation of Alg. 2 is similar to standard MALA except for the projection, we prove its convergence through its conductance properties, which can be deduced by the Cheeger isoperimetric inequality of $\\tilde{\\mu}_{*}$ ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "Under these conditions, we organize this subsection in the following three steps: ", "page_idx": 34}, {"type": "text", "text": "1. Find the Cheeger isoperimetric inequality of $\\tilde{\\mu}_{*}$   \n2. Find the conductance properties of $\\tilde{\\mathcal{T}}_{*}$   \n3. Build te connectionbetween $\\tilde{\\mu}_{S}$ and $\\tilde{\\mu}_{*}$ through the process $\\{\\widetilde{\\mathbf{z}}_{\\ast,s}\\}_{s=0}^{S}$ ", "page_idx": 34}, {"type": "text", "text": "C.3.1 The Cheeger isoperimetric inequality of $\\tilde{\\mu}_{*}$ ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Definition 1 (Definition 2.5.9 in [12]). A probability measure $\\mu$ definedonaPolishspace $(\\boldsymbol{\\mathcal{X}},\\mathrm{dis})$ satisfies a Cheeger isoperimetric inequality with constant $\\rho>0$ iffor allBorelset $A\\subseteq\\mathcal{X}$ ,ithas ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\epsilon\\to0}\\operatorname*{inf}_{\\epsilon}\\frac{\\mu(A^{\\epsilon})-\\mu(A)}{\\epsilon}\\geq\\frac{1}{\\rho}\\mu(A)\\mu(A^{c}).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Lemma C.5 (Theorem 2.5.14 in [12]). Let $\\mu\\in\\mathcal P_{1}(\\mathcal{X})$ and let $\\mathrm{Ch}>0$ . The following are equivalent. ", "page_idx": 35}, {"type": "text", "text": "1.p satisfies a Cheeger isoperimetric inequality with constant Ch. ", "page_idx": 35}, {"type": "text", "text": "2. For all Lipschitz $f\\colon\\mathcal{X}\\rightarrow\\mathbb{R},$ it holds that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mu}\\left|f-\\mathbb{E}_{\\mu}f\\right|\\leq2\\rho\\cdot\\mathbb{E}_{\\mu}\\left\\|\\nabla f\\right\\|\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Remark 2. For a general non-log-concave distribution, a tight bound on the Cheeger constant can hardly be provided. However, considering the Cheeger isoperimetric inequality is stronger than the Poincar\u00e9 inequality, [6] lower bound the Cheeger constant $\\rho$ with $\\Omega(\\mathring{d}^{1/2}c_{P}\\big)$ where $c_{P}$ is the Poincar\u00e9 constant of $\\tilde{\\mu}_{*}$ .The lower bound of $c_{P}$ can be generally obtained by the Bakry-Emery criterion and achieve $\\exp(-\\tilde{O}(d))$ .While for target distributions with better properties, $\\rho$ can usually be much better When the target distribution is a mixture of strongly log-concave distributions, the lower bound of $\\rho$ can achieve $1/\\mathrm{poly}(d)$ by [19]. For log-concave distributions, $[22]$ proved that $\\rho=\\Omega(1/(\\mathrm{Tr}(\\Sigma^{2}))^{1/4})$ , where $\\Sigma$ is the covariance matrix of the distribution $\\tilde{\\mu}_{*}$ . When the target distribution is $m$ -strongly log-concave, based on [15], $\\rho$ can even achieve $\\Omega(\\sqrt{L})$ . In the following, we will prove that the Cheeger constant can be independent of $x_{0}$ ", "page_idx": 35}, {"type": "text", "text": "Lemma C.6. Suppose $\\mu_{*}$ and $\\tilde{\\mu_{*}}$ are defined as Eq. 22 and Eq. 58, respectively, where $R$ in $\\tilde{\\mu_{*}}$ is chosen as that in Lemma C.2. For any $\\epsilon\\in(0,1)$ wehave ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\leq\\frac{\\int_{\\Omega}\\tilde{\\mu}_{*}(\\mathrm{d}z)}{\\int_{\\mathbb{R}^{d}}\\mu_{*}(\\mathrm{d}z)}\\leq1.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof.Suppose $\\mu_{*}\\propto\\exp(-g)$ and $\\tilde{\\mu}_{*}$ are the original and truncated target distributions of the inner loops. Following from Lemma C.13, it has ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(\\mu_{*},\\tilde{\\mu}_{*}\\right)\\leq\\frac{\\epsilon}{4}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "when $\\tilde{\\mu}_{*}$ is deduced by the $R$ shown in Lemma C.2. Under these conditions, supposing ${\\boldsymbol\\Omega}=B(\\mathbf{0},R)$ then we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{{FV}}\\left(\\tilde{\\mu}_{*},\\mu\\right)=\\displaystyle\\int_{\\mathbb{R}^{d}}\\left|\\mu_{*}(\\mathrm{d}z)-\\tilde{\\mu}_{*}(\\mathrm{d}z)\\right|=\\int_{\\Omega}\\left|\\mu_{*}(\\mathrm{d}z)-\\tilde{\\mu}_{*}(\\mathrm{d}z)\\right|+\\displaystyle\\int_{\\mathbb{R}^{d}-\\Omega}\\mu_{*}(\\mathrm{d}z)}\\\\ &{\\qquad\\qquad=\\displaystyle\\int_{\\Omega}\\left|\\frac{\\exp\\left(-g(z)\\right)}{\\int_{\\mathbb{R}^{d}}\\exp\\left(-g(z^{\\prime})\\right)\\mathrm{d}z^{\\prime}}-\\frac{\\exp\\left(-g(z)\\right)}{\\int_{\\Omega}\\exp\\left(-g(z^{\\prime})\\right)\\mathrm{d}z^{\\prime}}\\right|\\mathrm{d}z+\\displaystyle\\int_{\\mathbb{R}^{d}-\\Omega}\\frac{\\exp\\left(-g(z)\\right)}{\\int_{\\mathbb{R}^{d}}\\exp\\left(-g(z^{\\prime})\\right)\\mathrm{d}z^{\\prime}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Suppose ", "page_idx": 35}, {"type": "equation", "text": "$$\nZ=\\int_{\\mathbb{R}^{d}}\\exp\\left(-g(z)\\right)\\mathrm{d}z\\quad\\mathrm{and}\\quad Z_{\\Omega}=\\int_{\\Omega}\\exp\\left(-g(z)\\right)\\mathrm{d}z,\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "then the first term of RHS of Eq. 60 satisfies ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{\\Omega}\\left|\\frac{\\exp\\left(-g(z)\\right)}{\\int_{\\mathbb{R}^{d}}\\exp\\left(-g(z^{\\prime})\\right)\\,\\mathrm{d}z^{\\prime}}-\\frac{\\exp\\left(-g(z^{\\prime})\\right)}{\\int_{\\Omega}\\exp\\left(-g(z^{\\prime})\\right)\\,\\mathrm{d}z^{\\prime}}\\right|\\mathrm{d}z}\\\\ &{=\\left(\\frac{1}{\\int_{\\Omega}\\exp\\left(-g(z^{\\prime})\\right)\\,\\mathrm{d}z^{\\prime}}-\\frac{1}{\\int_{\\mathbb{R}^{d}}\\exp\\left(-g(z^{\\prime})\\right)\\,\\mathrm{d}z^{\\prime}}\\right)\\cdot\\int_{\\Omega}\\exp\\left(-g(z^{\\prime})\\right)\\mathrm{d}z^{\\prime}=1-\\frac{Z_{\\Omega}}{Z}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and the second term satisfies ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}^{d}-\\Omega}\\frac{\\exp\\left(-g(z)\\right)}{\\int_{\\mathbb{R}^{d}}\\exp\\left(-g(z^{\\prime})\\right)\\mathrm{d}z^{\\prime}}\\mathrm{d}z=\\frac{\\int_{\\mathbb{R}^{d}}\\exp\\left(-g(z^{\\prime})\\right)\\mathrm{d}z^{\\prime}-\\int_{\\Omega}\\exp\\left(-g(z^{\\prime})\\right)\\mathrm{d}z^{\\prime}}{\\int_{\\mathbb{R}^{d}}\\exp\\left(-g(z^{\\prime})\\right)\\mathrm{d}z^{\\prime}}=1-\\frac{Z_{\\Omega}}{Z}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Combining all these things, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n2\\cdot\\left(1-\\frac{Z_{\\Omega}}{Z}\\right)\\leq\\frac{\\epsilon}{4}\\quad\\Rightarrow\\quad\\frac{1}{2}\\leq\\frac{Z_{\\Omega}}{Z}\\leq1\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where we suppose $\\epsilon\\leq1$ without loss of generality. Hence, the proof is completed. ", "page_idx": 35}, {"type": "text", "text": "Lemma C.7. Suppose $\\mu_{*}$ \uff0c $\\tilde{\\mu}_{*}$ and $\\epsilon$ are under the same settings as those in Lemma C.6, the variance of $\\tilde{\\mu}_{*}$ can be upper bounded by $2d/L$ ", "page_idx": 36}, {"type": "text", "text": "Proof. According to the fact that $\\mu_{*}$ is a $L$ -strongly log-concave distribution defined on $\\mathbb{R}^{d}$ with the mean $\\pmb{v_{m}}$ , which satisfies ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\int_{\\mathbb R^{d}}\\mu(\\boldsymbol z)\\left\\|\\boldsymbol z-\\boldsymbol v_{m}\\right\\|^{2}\\mathrm{d}\\boldsymbol z\\leq\\frac{d}{L}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "following from Lemma E.8. Suppose ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\Omega=\\mathcal{B}(\\mathbf{0},R),\\quad Z=\\int_{\\mathbb{R}^{d}}\\exp(-g(z))\\mathrm{d}z,\\quad Z_{\\Omega}=\\int_{\\Omega}\\exp(-g(z))\\mathrm{d}z\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $R$ shown in Lemma C.2, then the variance bound can be reformulated as ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\int_{\\Omega}\\frac{\\exp(-g(z))}{Z}\\left\\|z-v_{m}\\right\\|^{2}\\mathrm{d}z+\\int_{\\mathbb{R}^{d}-\\Omega}\\frac{\\exp(-g(z))}{Z}\\left\\|z-v_{m}\\right\\|^{2}\\mathrm{d}z\\leq\\frac{d}{L},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "which implies ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\int_{\\Omega}\\frac{\\exp(-g(z))}{Z_{\\Omega}}\\left\\|z-v_{m}\\right\\|^{2}\\mathrm{d}z\\leq\\frac{Z}{Z_{\\Omega}}\\cdot\\frac{d}{L}\\leq\\frac{2d}{L}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Note that the last inequality follows from Lemma C.6. Besides, suppose the mean of $\\tilde{\\mu}_{*}$ is $\\pmb{v}_{\\tilde{m}}$ ,then wehave ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int_{\\Omega}\\frac{\\exp(-g(z))}{Z_{\\Omega}}\\cdot\\|z-v_{m}\\|^{2}\\,\\mathrm{d}z=\\int_{\\Omega}\\frac{\\exp(-g(z))}{Z_{\\Omega}}\\cdot\\|z-v_{\\Tilde{m}}+v_{\\Tilde{m}}-v_{m}\\|^{2}\\,\\mathrm{d}z}\\\\ {\\displaystyle=\\int_{\\Omega}\\frac{\\exp(-g(z))}{Z_{\\Omega}}\\cdot\\|z-v_{\\Tilde{m}}\\|^{2}\\,\\mathrm{d}z+2\\cdot\\int_{\\Omega}\\frac{\\exp(-g(z))}{Z_{\\Omega}}\\cdot\\langle z-v_{\\Tilde{m}},v_{\\Tilde{m}}-v_{m}\\rangle\\,\\mathrm{d}z}\\\\ {\\displaystyle~~~+\\int_{\\Omega}\\frac{\\exp(-g(z))}{Z_{\\Omega}}\\cdot\\|v_{m}-v_{\\Tilde{m}}\\|^{2}\\,\\mathrm{d}z}\\\\ {\\displaystyle=\\int_{\\Omega}\\frac{\\exp(-g(z))}{Z_{\\Omega}}\\cdot\\|z-v_{\\Tilde{m}}\\|^{2}\\,\\mathrm{d}z+\\int_{\\Omega}\\frac{\\exp(-g(z))}{Z_{\\Omega}}\\cdot\\|v_{m}-v_{\\Tilde{m}}\\|^{2}\\,\\mathrm{d}z}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Combining Eq. 61 and Eq. 62, the variance of $\\tilde{\\mu}_{*}$ satisfies ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\int_{\\Omega}\\frac{\\exp(-g(z))}{Z_{\\Omega}}\\cdot\\left\\|z-v_{\\tilde{m}}\\right\\|^{2}\\mathrm{d}z\\leq\\frac{2d}{L}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Hence, the proof is completed. ", "page_idx": 36}, {"type": "text", "text": "Corollary C.8. For each truncated target distribution defined as Eq. 58, their Cheeger constant can be lowerboundedby $\\rho=\\Omega(\\sqrt{L/d})$ ", "page_idx": 36}, {"type": "text", "text": "Proof. It can be easily found that $\\tilde{\\mu}_{*}$ is log-concave distribution, which means their Cheeger constant can be upper bounded by $\\rho=\\Omega(1/(\\mathrm{{Tr}}(\\Sigma))^{1/2})$ ,where $\\Sigma$ is the covariance matrix of the distribution $\\tilde{\\mu}_{*}$ . Under these conditions, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathrm{Tr}\\left(\\Sigma\\right)=\\int_{\\Omega}\\frac{\\exp(-g(z))}{Z_{\\Omega}}\\cdot\\left\\|z-v_{\\tilde{m}}\\right\\|^{2}\\mathrm{d}z\\leq\\frac{2d}{L},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the last inequality follows from Lemma C.7. Hence, $\\rho=\\Omega(\\sqrt{L/d})$ and the proof is completed. ", "page_idx": 36}, {"type": "text", "text": "C.3.2  The conductance properties of $\\tilde{\\mathcal{T}}_{*}$ ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We prove the conductance properties of $\\tilde{\\mathcal{T}}_{\\ast,z}$ with the following lemma. ", "page_idx": 36}, {"type": "text", "text": "Lemma C.9 (Lemma 13 in [21]). Let $\\tilde{\\mathcal{T}}_{\\ast,z}$ be a be a time-reversible Markov chain on $\\Omega$ with stationarydistribution $\\tilde{\\mu}_{*}$ .Fix any $\\Delta\\,>\\,0$ \uff0csuppose for any $z,z^{\\prime}\\,\\in\\,\\Omega$ with $\\|z-z^{\\prime}\\|\\,\\leq\\,\\Delta$ we have TV $\\left(\\tilde{T}_{*,z},\\tilde{T}_{*,z^{\\prime}}\\right)\\leq0.99$ then the conductance of $\\tilde{\\mathcal{T}}_{\\ast,z}$ satisfies $\\phi\\geq C\\rho\\Delta$ for some absolute constant $C$ where $\\rho$ is the Cheeger constant of $\\tilde{\\mu}_{*}$ ", "page_idx": 36}, {"type": "text", "text": "In order to apply Lemma C.9, we have known the Cheeger constant of $\\tilde{\\mu}_{*}$ is $\\rho$ . We only need to verify the corresponding condition, i., proving that as long as $\\|z-z^{\\prime}\\|\\leq\\Delta$ wehave $\\mathrm{TV}\\left(\\tilde{\\mathcal{T}}_{\\ast,z},\\tilde{\\mathcal{T}}_{\\ast,z^{\\prime}}\\right)\\leq$ 0.99 for some $\\Delta$ . Recalling Eq. 33, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{r}_{*,z}(\\mathrm{d}\\hat{z})=\\tilde{\\mathcal{T}}_{*,z}^{\\prime}(\\mathrm{d}\\hat{z})\\cdot\\tilde{a}_{*,z}(\\hat{z})+\\biggl(1-\\int_{\\Omega}\\tilde{a}_{*,z}(\\hat{z})\\tilde{\\mathcal{T}}_{*,z}^{\\prime}(\\mathrm{d}\\hat{z})\\biggr)\\cdot\\delta_{z}(\\mathrm{d}\\hat{z})}\\\\ &{\\qquad\\qquad=\\biggl(\\frac{1}{2}\\delta_{z}(\\mathrm{d}\\hat{z})+\\frac{1}{2}\\cdot\\tilde{Q}_{*,z}^{\\prime}(\\mathrm{d}\\hat{z})\\biggr)\\cdot\\tilde{a}_{*,z}(\\hat{z})+\\biggl[1-\\int\\tilde{a}_{*,z}(\\hat{z})\\cdot\\biggl(\\frac{1}{2}\\delta_{z}(\\mathrm{d}\\hat{z})+\\frac{1}{2}\\tilde{Q}_{*,z}^{\\prime}(\\mathrm{d}\\hat{z})\\biggr)\\biggr]\\cdot}\\\\ &{\\qquad\\qquad=\\biggl(\\frac{1}{2}\\delta_{z}(\\mathrm{d}\\hat{z})+\\frac{1}{2}\\cdot\\tilde{Q}_{*,z}^{\\prime}(\\mathrm{d}\\hat{z})\\biggr)\\cdot\\tilde{a}_{*,z}(\\hat{z})+\\biggl(1-\\frac{1}{2}\\tilde{a}_{*,z}(z)-\\frac{1}{2}\\int\\tilde{a}_{*,z}(\\hat{z})\\cdot\\tilde{Q}_{*,z}^{\\prime}(\\mathrm{d}\\hat{z})\\biggr)\\cdot\\delta}\\\\ &{\\qquad\\qquad=\\biggl(1-\\frac{1}{2}\\int\\tilde{a}_{*,z}(\\hat{z})\\cdot\\tilde{Q}_{*,z}^{\\prime}(\\mathrm{d}\\hat{z})\\biggr)\\cdot\\delta_{z}(\\mathrm{d}\\hat{z})+\\frac{1}{2}\\cdot\\tilde{Q}_{*,z}^{\\prime}(\\mathrm{d}\\hat{z})\\cdot\\tilde{a}_{*,z}(\\hat{z})}\\\\ &{\\qquad\\qquad=\\biggl(1-\\frac{1}{2}\\int_{\\Omega_{*}}\\tilde{a}_{*,z}(\\hat{z})\\tilde{Q}_{*,z}(\\mathrm{d}\\hat{z})\\biggr)+\\frac{1}{2}\\cdot\\tilde{Q}_{*,z}(\\mathrm{d}\\hat{z})\\cdot\\tilde{a}_{*,z}(\\hat{z})\\cdot1\\left[\\hat{z}\\in\\Omega_{z}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the second inequality follows from Eq. 31 and the last inequality follows from Eq. 30. Then the rest will be proving the upper bound of $\\mathrm{TV}\\left(\\tilde{T}_{*,z},\\tilde{T}_{*,z^{\\prime}}\\right)$ , and we state another two useful lemmas as follows. ", "page_idx": 37}, {"type": "text", "text": "Lemma C.10 (Lemma B.6 in [40]). For any two points $z,z^{\\prime}\\in\\mathbb{R}^{d}$ it holds that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(\\tilde{Q}_{\\ast,z}(\\cdot),\\tilde{Q}_{\\ast,z^{\\prime}}(\\cdot)\\right)\\leq\\frac{(1+3L\\tau)\\,\\|z-z^{\\prime}\\|}{\\sqrt{2\\tau}}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. This lemma can be easily obtained by plugging the smoothness of $g$ ,i.e., $3L$ , into Lemma B.6 in [40]. \u53e3 ", "page_idx": 37}, {"type": "text", "text": "Corollary C.11 (Variant of Lemma 6.5 in [40]). Under Assumption [A1]-[A2], we set ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\eta=\\frac{1}{2}\\log\\frac{2L+1}{2L}\\quad\\mathrm{and}\\quad G:=\\|\\nabla g(\\mathbf{0})\\|\\,.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "If we set ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\tau\\leq\\frac{1}{16\\cdot(3L R+G+\\epsilon_{\\mathrm{score}})^{2}}\\quad\\mathrm{and}\\quad r=3\\cdot\\sqrt{\\tau d\\log\\frac{8S}{\\epsilon}}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "there exist absolute constants $c_{0}$ \uff0csuch that $\\phi\\,\\geq\\,c_{0}\\rho{\\sqrt{\\tau}}$ where $\\rho$ is the Cheeger constant of the distribution $\\tilde{\\mu}_{*}$ ", "page_idx": 37}, {"type": "text", "text": "Proof. By the definition of total variation distance, there exists a set ${\\mathcal{A}}\\subseteq\\Omega$ satisfying ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(\\tilde{T}_{*,z}(\\cdot),\\tilde{T}_{*,z^{\\prime}}(\\cdot)\\right)=\\left|\\tilde{T}_{*,z}(A)-\\tilde{T}_{*,z^{\\prime}}(A)\\right|.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Due to the closed form of $\\tilde{\\mathcal{T}}_{\\ast,z}$ shown in Eq. 63, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{T}}_{*,z}(A)=\\left(1-\\frac{1}{2}\\int_{\\tilde{z}\\in\\Omega_{z}}\\tilde{a}_{*,z}(\\tilde{z})\\tilde{Q}_{*,z}(\\mathrm{d}\\tilde{z})\\right)+\\frac{1}{2}\\int_{\\hat{z}\\in A}\\tilde{a}_{*,z}(\\hat{z})\\cdot\\mathbf{1}\\left[\\hat{z}\\in\\Omega_{z}\\right]\\tilde{Q}_{*,z}(\\mathrm{d}\\hat{z})\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Under this condition, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mathcal{T}}_{*,z}(A)-\\tilde{\\mathcal{T}}_{*,z^{\\prime}}(A)\\Big|\\leq\\underset{\\leq}{\\operatorname*{max}}\\left(1-\\frac{1}{2}\\int_{\\Omega_{\\delta}}\\tilde{a}_{*,\\xi}(\\tilde{z})\\tilde{Q}_{*,\\xi}(\\mathrm{d}\\tilde{z})\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{Term~1}}\\\\ &{\\qquad\\qquad\\qquad+\\left.\\frac{1}{2}\\left|\\int_{\\tilde{z}\\in A}\\tilde{a}_{*,z}(\\hat{z})\\cdot\\mathbf{1}\\left[\\hat{z}\\in\\Omega_{z}\\right]\\tilde{Q}_{*,z}(\\mathrm{d}\\hat{z})-\\tilde{a}_{*,z^{\\prime}}(\\hat{z})\\cdot\\mathbf{1}\\left[\\hat{z}\\in\\Omega_{z^{\\prime}}\\right]\\tilde{Q}_{*,z^{\\prime}}(\\mathrm{d}\\hat{z})\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Upper bound Term 1. We first consider to lower bound $\\tilde{a}_{\\ast,\\hat{z}}(\\tilde{z})$ in the following. According to Eq. 32, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\tilde{a}_{*,\\hat{z}}(\\tilde{z})\\geq\\exp\\left(-g(\\tilde{z})-\\frac{\\|\\hat{z}-\\tilde{z}+\\tau\\nabla g(\\tilde{z})\\|^{2}}{4\\tau}+g(\\hat{z})+\\frac{\\|\\tilde{z}-\\hat{z}+\\tau\\nabla g(\\hat{z})\\|^{2}}{4\\tau}\\right),\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which means ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{4\\ln\\tilde{a}_{*,\\hat{z}}(\\tilde{z})\\geq\\underbrace{\\tau\\cdot\\Big(\\|\\nabla g(\\hat{z})\\|^{2}-\\|\\nabla g(\\tilde{z})\\|^{2}\\Big)}_{\\mathrm{Term~1.1}}}\\\\ &{\\qquad\\qquad\\underbrace{-2\\cdot\\big(g(\\tilde{z})-g(\\hat{z})-\\langle\\nabla g(\\hat{z}),\\tilde{z}-\\hat{z}\\rangle\\big)}_{\\mathrm{Term~1.2}}}\\\\ &{\\qquad\\qquad\\underbrace{+2\\cdot\\big(g(\\hat{z})-g(\\tilde{z})-\\langle\\nabla g(\\tilde{z}),\\hat{z}-\\tilde{z}\\rangle\\big)}_{\\mathrm{Term~1.3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Since Term 1.2 and Term 1.3 are grouped to more easily apply the strong convexity and smoothness Oof $g$ (Lemma C.1), it has ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathrm{Term}\\,1.2\\geq-3L\\,\\|\\hat{z}-\\tilde{z}\\|^{2}\\quad\\mathrm{and}\\quad\\mathrm{Term}\\,1.3\\geq L\\,\\|\\hat{z}-\\tilde{z}\\|^{2}\\geq0.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Besides, by requiring $\\tau\\leq1/3L$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Term~1.1}=\\tau\\cdot\\langle\\nabla g(\\hat{z})-\\nabla g(\\hat{z}),\\nabla g(\\hat{z})+\\nabla g(\\hat{z})\\rangle}\\\\ &{\\qquad\\qquad\\geq-\\tau\\cdot\\Vert\\nabla g(\\hat{z})-\\nabla g(\\hat{z})\\Vert\\cdot\\Vert\\nabla g(\\hat{z})+\\nabla g(\\hat{z})\\Vert}\\\\ &{\\qquad\\qquad\\geq-3L\\tau\\left\\Vert\\hat{z}-\\tilde{z}\\right\\Vert\\cdot\\left(2\\left\\Vert\\nabla g(\\hat{z})\\right\\Vert+3L\\left\\Vert\\hat{z}-\\tilde{z}\\right\\Vert\\right)\\geq-3L\\tau^{2}\\left\\Vert\\nabla g(\\hat{z})\\right\\Vert^{2}-6L\\left\\Vert\\hat{z}-\\tilde{z}\\right\\Vert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Therefore, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\cdot\\ln\\tilde{a}_{*,\\hat{z}}(\\tilde{z})\\geq-\\,3L\\tau^{2}\\left\\|\\nabla g(\\hat{z})\\right\\|^{2}-9L\\left\\|\\hat{z}-\\tilde{z}\\right\\|^{2}=-3L\\tau^{2}\\left\\|\\nabla g(\\hat{z})\\right\\|^{2}-9L\\left\\|\\tau\\cdot\\nabla g(\\hat{z})+\\sqrt{2\\tau}\\cdot\\xi\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\geq-\\,21L\\tau^{2}\\left\\|\\nabla g(\\hat{z})\\right\\|^{2}-36L\\tau\\left\\|\\xi\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\ln\\tilde{a}_{*,\\hat{z}}(\\tilde{z})\\geq-6L\\tau^{2}\\left\\|\\nabla g(\\hat{z})\\right\\|^{2}-9L\\tau\\left\\|\\xi\\right\\|^{2}\\geq-6L\\tau^{2}\\cdot\\left(3L R+\\left\\|\\nabla g(\\mathbf{0})\\right\\|\\right)^{2}-9L\\tau\\|\\xi\\|^{2}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the last inequality follows from ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla g(\\hat{z})\\|\\leq\\|\\nabla g(\\hat{z})-\\nabla g(\\mathbf{0})\\|+\\|\\nabla g(\\mathbf{0})\\|\\leq3L R+\\|\\nabla g(\\mathbf{0})\\|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Under these conditions, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Term~1}\\leq1-\\displaystyle\\frac{1}{2}\\cdot\\exp\\left(-6L\\tau^{2}\\left(3L R+\\|\\nabla g(\\mathbf{0})\\|\\right)^{2}\\right)\\cdot\\operatorname*{min}\\int_{\\Omega_{\\hat{s}}}\\exp\\left(-9L\\tau\\|\\xi\\|^{2}\\right)\\cdot\\tilde{q}_{*,\\hat{z}}(\\tilde{z})\\mathrm{d}\\tilde{z}}\\\\ &{\\quad\\quad\\quad=1-\\displaystyle\\frac{1}{2}\\cdot\\exp\\left(-6L\\tau^{2}\\left(3L R+\\|\\nabla g(\\mathbf{0})\\|\\right)^{2}\\right)\\cdot\\mathbb{E}_{\\xi\\sim\\mathcal{N}(\\mathbf{0},I)}\\left[\\exp\\left(-9L\\tau\\|\\xi\\|^{2}\\right)\\right]}\\\\ &{\\quad\\quad\\quad\\leq1-0.4\\cdot\\exp\\left(-6L\\tau^{2}\\left(3L R+\\|\\nabla g(\\mathbf{0})\\|\\right)^{2}\\right)\\cdot\\exp\\left(-18L\\tau d\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the last inequality follows from the Markov inequality shown in the following ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{L}_{\\xi\\sim\\mathcal{N}(\\mathbf{0},I)}\\left[\\exp\\left(-9L\\tau\\|\\xi\\|^{2}\\right)\\right]\\ge\\exp\\left(-18L\\tau d\\right)\\cdot\\mathbb{P}_{\\xi\\sim\\mathcal{N}(\\mathbf{0},I)}\\left[\\exp\\left(-9L\\tau\\|\\xi\\|^{2}\\right)\\ge\\exp\\left(-18L\\tau d\\right)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\exp\\left(-18L\\tau d\\right)\\cdot\\mathbb{P}_{\\xi\\sim\\mathcal{N}(\\mathbf{0},I)}\\left[\\|\\xi\\|^{2}\\le2d\\right]\\ge\\exp\\left(-18L\\tau d\\right)\\cdot\\left(1-\\exp\\left(-2\\xi\\tau\\|\\xi\\|^{2}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Then, by choosing ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\tau\\leq\\frac{1}{16\\sqrt{L}\\cdot(3L R+\\|\\nabla g(\\mathbf{0})\\|)},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "it has $6L\\tau^{2}\\left(3L R+\\|\\nabla g(\\mathbf{0})\\|\\right)^{2}\\leq1/40$ Besides by choosing ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\tau\\leq\\frac{1}{L^{2}R^{2}}\\leq\\frac{1}{40\\cdot18L\\cdot\\left(\\sqrt{d}+\\sqrt{\\ln\\frac{16S}{\\epsilon}}\\right)^{2}}\\leq\\frac{1}{40\\cdot18L\\cdot d},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the last inequality follows from the range of $R$ shown in Lemma C.2, it has $18L d\\tau\\leq1/40$ Under these conditions, considering Eq. 64, we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathrm{Term}\\,1\\leq1-0.5\\cdot\\operatorname*{min}_{\\substack{\\hat{z}\\in\\Omega,\\hat{z}\\in\\Omega_{\\hat{z}}}}\\int_{\\Omega_{\\hat{z}}}\\tilde{a}_{*,\\hat{z}}(\\tilde{z})\\tilde{Q}_{*,\\hat{z}}(\\mathrm{d}\\tilde{z})\\leq1-0.4\\cdot e^{-1/20}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Then, combining the step size choices of Eq. 65, Eq. 66, and Lemma C.2, since the requirement ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\tau\\leq\\frac{1}{16\\sqrt{L}\\cdot(3L R+\\|\\nabla g(\\mathbf{0})\\|)},\\quad\\tau\\leq\\frac{1}{L^{2}R^{2}}\\quad\\mathrm{and}\\quad\\tau\\leq\\frac{d}{(3L R+\\|\\nabla g(\\mathbf{0})\\|+\\epsilon_{\\mathrm{score}})^{2}}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "can be achieved by ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\tau\\leq16^{-1}\\cdot(3L R+\\|\\nabla g(\\mathbf{0})\\|+\\epsilon_{\\mathrm{score}})^{-2},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "the range of $\\tau$ can be determined. ", "page_idx": 39}, {"type": "text", "text": "Upper bound Term 2.  In This part, we use similar techniques as those shown in Lemma 6.5 of [40]. According to the triangle inequality, we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{:\\mathrm{{Term}\\,}2\\leq\\int_{\\tilde{\\varepsilon}\\in A}\\left(1-\\tilde{a}_{*,z}(\\hat{z})\\right)\\tilde{q}(\\hat{z}|z){\\bf1}\\left[\\hat{z}\\in\\Omega_{z}\\right]\\mathrm{d}\\hat{z}+\\int_{\\tilde{\\varepsilon}\\in A}\\left(1-\\tilde{a}_{*,z^{\\prime}}(\\hat{z})\\right)\\tilde{q}(\\hat{z}|z^{\\prime}){\\bf1}\\left[\\hat{z}\\in\\Omega_{z^{\\prime}}\\right]\\mathrm{d}\\hat{z}}\\\\ &{\\qquad\\qquad+\\left|\\int_{\\tilde{\\varepsilon}\\in A}\\left(\\tilde{q}(\\hat{z}|z){\\bf1}\\left[\\hat{z}\\in\\Omega_{z}\\right]-\\tilde{q}(\\hat{z}|z^{\\prime}){\\bf1}\\left[\\hat{z}\\in\\Omega_{z^{\\prime}}\\right]\\right)\\mathrm{d}\\hat{z}\\right|}\\\\ &{\\qquad\\leq2\\cdot\\left(1-\\underset{\\varepsilon\\in A,\\varepsilon\\in\\Omega_{\\varepsilon}}{\\operatorname*{min}}\\int_{\\Omega_{\\hat{s}}}\\tilde{a}_{*,\\hat{z}}(\\hat{z})\\tilde{Q}_{*,\\hat{z}}(\\mathrm{d}\\tilde{z})\\right)}\\\\ &{\\qquad\\qquad+\\left|\\int_{\\tilde{\\varepsilon}\\in A}\\left(\\tilde{q}(\\hat{z}|z){\\bf1}\\left[\\hat{z}\\in\\Omega_{z}\\right]-\\tilde{q}(\\hat{z}|z^{\\prime}){\\bf1}\\left[\\hat{z}\\in\\Omega_{z^{\\prime}}\\right]\\right)\\mathrm{d}\\hat{z}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Then, we upper bound Term 2.1 as follows ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Ierm~2.1}\\leq\\left|\\displaystyle\\int_{\\hat{z}\\in A}{\\bf1}\\left[\\hat{z}\\in\\Omega_{z^{\\prime}}\\right]\\cdot\\left(\\tilde{q}_{\\{\\hat{z}\\}}|z)-\\tilde{q}_{(\\hat{z}}|z^{\\prime})\\right)\\right|+\\left|\\displaystyle\\int_{\\hat{z}\\in A}\\left({\\bf1}\\left[\\hat{z}\\in\\Omega_{z}\\right]-{\\bf1}\\left[\\hat{z}\\in\\Omega_{z^{\\prime}}\\right]\\right)\\cdot\\tilde{q}(\\hat{z}|z)\\right|}\\\\ &{\\qquad\\qquad\\leq\\mathrm{TV}\\left(\\tilde{Q}_{\\ast,z}(\\cdot),\\tilde{Q}_{\\ast,z^{\\prime}}(\\cdot)\\right)+\\operatorname*{max}\\left\\{\\displaystyle\\int_{\\hat{z}\\in\\Omega_{z^{\\prime}}-\\Omega_{z}}\\tilde{q}(\\hat{z}|z)\\mathrm{d}\\hat{z},\\displaystyle\\int_{\\hat{z}\\in\\Omega_{z}-\\Omega_{z^{\\prime}}}\\tilde{q}(\\hat{z}|z^{\\prime})\\mathrm{d}\\hat{z}\\right\\}}\\\\ &{\\qquad\\qquad\\leq\\mathrm{TV}\\left(\\tilde{Q}_{\\ast,z}(\\cdot),\\tilde{Q}_{\\ast,z^{\\prime}}(\\cdot)\\right)+\\operatorname*{max}\\left\\{\\displaystyle\\int_{\\hat{z}\\in\\mathbb{R}^{d}-\\Omega_{z}}\\tilde{q}(\\hat{z}|z)\\mathrm{d}\\hat{z},\\displaystyle\\int_{\\hat{z}\\in\\mathbb{R}^{d}-\\Omega_{z^{\\prime}}}\\tilde{q}(\\hat{z}|z^{\\prime})\\mathrm{d}\\hat{z}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "According to the definition, $\\tilde{q}_{*,z}(\\cdot)$ is Gaussian distribution with mean $z-\\tau\\nabla g(z)$ and covariance matrix $2\\tau I$ , thus we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int_{\\hat{z}\\in\\mathbb{R}^{d}-\\Omega_{z}}\\tilde{q}(\\hat{z}|z)\\mathrm{d}\\hat{z}\\leq\\mathbb{P}_{\\hat{z}\\sim\\chi_{d}^{2}}\\left[\\hat{\\mathbf{z}}\\geq\\frac{1}{2}\\left(r-\\tau\\left\\|\\nabla g(z)\\right\\|\\right)^{2}/\\tau\\right]}\\\\ &{\\displaystyle\\int_{\\hat{z}\\in\\mathbb{R}^{d}-\\Omega_{z^{\\prime}}}\\tilde{q}(\\hat{z}|z^{\\prime})\\mathrm{d}\\hat{z}\\leq\\mathbb{P}_{\\hat{z}\\sim\\chi_{d}^{2}}\\left[\\hat{\\mathbf{z}}\\geq\\frac{1}{2}\\left(r-\\tau\\left\\|\\nabla g(z)\\right\\|-\\left\\|z-z^{\\prime}\\right\\|\\right)^{2}/\\tau\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Then, we start to lower bound ", "page_idx": 39}, {"type": "equation", "text": "$$\nr-\\tau\\left\\|\\nabla g(z)\\right\\|-\\left\\|z-z^{\\prime}\\right\\|.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Then, we require ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\|z-z^{\\prime}\\|\\leq0.1r\\quad{\\mathrm{and}}\\quad\\tau\\leq{\\frac{d}{35\\cdot{\\left(3L R+G\\right)}^{2}}}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the latter condition can be easily covered by the choice in Eq. 68 when $d\\geq3$ without loss of generality. Under this condition, we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\tau\\leq(0.17)^{2}\\cdot\\frac{d}{(3L R+G)^{2}}\\quad\\Leftrightarrow\\quad\\sqrt{\\tau}\\leq\\frac{0.17\\sqrt{d}}{3L R+G}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Since we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla g(z)\\|=\\|\\nabla g(z)-\\nabla g(\\mathbf{0})+\\nabla g(\\mathbf{0})\\|\\leq3L\\cdot\\|z\\|+G\\leq3L R+G,}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "by the smoothness, it has ", "page_idx": 40}, {"type": "equation", "text": "$$\n{\\sqrt{\\tau}}\\leq{\\frac{0.17{\\sqrt{d}}}{\\|\\nabla g(z)\\|}}\\quad\\Leftrightarrow\\quad\\tau\\,\\|\\nabla g(z)\\|\\leq0.17{\\sqrt{\\tau d}}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Plugging Eq. 72 and Eq. 71 into Eq. 70, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\nr-\\tau\\left\\|\\nabla g(z)\\right\\|-\\left\\|z-z^{\\prime}\\right\\|\\geq0.9r-0.17{\\sqrt{\\tau d}}\\geq{\\sqrt{6.4\\tau d}}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where the last inequality follows from the choice of $r$ shown in Lemma C.3, i.e., ", "page_idx": 40}, {"type": "equation", "text": "$$\nr=3\\cdot{\\sqrt{\\tau d\\log{\\frac{8S}{\\epsilon}}}}\\geq3\\cdot{\\sqrt{\\tau d}}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Under these conditions, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{\\int_{\\hat{z}\\in\\mathbb R^{d}-\\Omega_{z}}\\tilde{q}(\\hat{z}|z)\\mathrm{d}\\hat{z},\\int_{\\hat{z}\\in\\mathbb R^{d}-\\Omega_{z^{\\prime}}}\\tilde{q}(\\hat{z}|z^{\\prime})\\mathrm{d}\\hat{z}\\right\\}\\leq\\mathbb{P}_{\\hat{z}\\sim\\chi_{d}^{2}}(\\|\\mathbf{z}\\|\\geq3.2d)\\leq0.1.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Then combine the above results and apply Lemma C.10, assume $\\tau\\leq1/(3L)$ ,wehave ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\textstyle\\operatorname{Term}\\,2.1\\leq0.1+\\operatorname{TV}\\left(\\tilde{Q}_{*,z}(\\cdot),\\tilde{Q}_{*,z^{\\prime}}(\\cdot)\\right)\\leq0.1+\\sqrt{2/\\tau}\\cdot\\|z-z^{\\prime}\\|\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Plugging the above into Eq. 69, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Term~2\\leq\\left(1-\\underset{\\hat{z}\\in\\Omega,\\hat{z}\\in\\Omega_{\\hat{z}}}{\\operatorname*{min}}\\int_{\\Omega_{\\hat{z}}}\\tilde{a}_{*,\\hat{z}}(\\tilde{z})\\tilde{Q}_{*,\\hat{z}}(\\mathrm{d}\\tilde{z})\\right)+\\frac{1}{2}\\cdot\\left(0.1+\\sqrt{\\frac{2}{\\tau}}\\cdot\\|z-z^{\\prime}\\|\\right)}\\\\ &{\\quad\\quad\\quad\\leq\\left(1-0.8\\cdot e^{-1/20}\\right)+0.05+(2\\tau)^{-1/2}\\cdot\\|z-z^{\\prime}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where the second inequality follows from Eq. 67. ", "page_idx": 40}, {"type": "text", "text": "After upper bounding Term 1 and Term 2, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}\\left(\\tilde{T}_{*,z}(\\cdot),\\tilde{T}_{*,z^{\\prime}}(\\cdot)\\right)\\leq1-0.4\\cdot e^{-1/20}+\\left(1-0.8\\cdot e^{-1/20}\\right)+0.05+(2\\tau)^{-1/2}\\cdot\\|z-z^{\\prime}\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq0.91+(2\\tau)^{-1/2}\\cdot\\|z-z^{\\prime}\\|\\leq0.99}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where the last inequality can be established by requiring $\\|z-z^{\\prime}\\|\\leq\\sqrt{2\\tau}$ . Combining Lemma C.9, the conductance of $\\tilde{\\mu}_{*}$ satisfies ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\phi\\geq c_{0}\\cdot\\rho{\\sqrt{2\\tau}}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Hence, the proof is completed. ", "page_idx": 40}, {"type": "text", "text": "The connection between $\\tilde{\\mu}_{S}$ and $\\tilde{\\mu}_{*}$ . With the conductance of truncated target distribution, we are able to find the convergence of the projected implementation of Alg. 2. Besides, the gap between the truncated target $\\tilde{\\mu}_{*}$ and the true target $\\mu_{*}$ can be upper bounded by controlling $R$ while such an $R$ will be dominated by the range of $R$ shown in Lemma C.2. In this section, we will omit several details since many of them have been proven in [40]. ", "page_idx": 40}, {"type": "text", "text": "Lemma C.12 (Lemma 6.4 in [40]). Let $\\tilde{\\mu}_{S}$ be distributions of the outputs of the projected implementation of Alg. 2. Under Assumption [A1]-[A2], if the transition kernel $\\tilde{T}_{z}(\\cdot)$ is $\\delta$ -close to $\\tilde{\\mathcal{T}}_{\\ast,z}$ with $\\delta\\le\\operatorname*{min}\\left\\{1-\\sqrt{2}/2,\\phi/16\\right\\}$ $\\dot{\\phi}$ denotes theconductance of $\\tilde{\\mu}_{*}$ ), then for any $\\lambda$ -warm start initial distributionwithrespect to $\\tilde{\\mu}_{*}$ itholdsthat ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(\\tilde{\\mu}_{S},\\tilde{\\mu}_{*}\\right)\\leq\\boldsymbol{\\lambda}\\cdot\\left(1-\\phi^{2}/8\\right)^{S}+16\\delta/\\phi.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Lemma C.13 (Lemma 6.6 in [40]). For any $\\epsilon\\in(0,1)$ Set $R$ to make it satisfy ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mu\\left(\\mathcal{B}(\\mathbf{0},R)\\right)\\geq1-\\frac{\\epsilon}{12},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and $\\tilde{\\mu}_{*}$ be the truncated target distribution of $\\mu_{*}$ .Then the total variation distance between $\\mu_{*}$ and $\\tilde{\\mu}_{*}$ can be upper bounded by TV $(\\tilde{\\mu}_{*},\\mu_{*})\\leq\\epsilon/4$ ", "page_idx": 40}, {"type": "text", "text": "C.4 Main Theorems of InnerMALA implementation ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Lemma C.14. Under Assumption [A1]-[A2], we can upper bound $G=\\|\\nabla g(\\mathbf{0})\\|$ as ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\|\\nabla g(\\mathbf{0})\\|\\leq L\\cdot{\\sqrt{2(d+m_{2}^{2})}}+3L\\cdot\\|\\mathbf{x}_{0}\\|\\,.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Furthermore, we can reformulate $R$ as ", "page_idx": 41}, {"type": "equation", "text": "$$\nR=63\\cdot\\sqrt{(d+m_{2}^{2}+\\|x_{0}\\|^{2})\\cdot\\log\\frac{16S}{\\epsilon}}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "to make it satisfy the requirement shown in Lemma C.3. Then, the range of inner step sizes, i.e., $\\tau$ will satisfy ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\tau\\leq C_{\\tau}\\cdot\\left(L^{2}\\left(d+m_{2}^{2}+\\|x_{0}\\|^{2}\\right)\\cdot\\log\\frac{16S}{\\epsilon}\\right)^{-1},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where the absolute constant $C_{\\tau}=2^{-4}\\cdot3^{-8}\\cdot7^{-2}$ ", "page_idx": 41}, {"type": "text", "text": "Proof. To make the bound more explicit, we control $R$ and $G$ in our previous analysis. For $G=$ $\\|\\nabla g(\\mathbf{0})\\|$ , according to Eq. 22, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\nabla g(z)=\\nabla f_{(K-k-1)\\eta}(z)+\\frac{e^{-2\\eta}z-e^{-\\eta}{\\pmb x}_{0}}{(1-e^{-2\\eta})},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "which means ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\lvert\\left(\\mathbf{0}\\right)\\right\\rvert\\right\\rvert\\leq\\left\\lVert\\nabla f_{(K-k-1)\\eta}(\\mathbf{0})\\right\\rVert+\\left\\lVert\\frac{e^{-\\eta}x_{0}}{1-e^{-2\\eta}}\\right\\rVert}\\\\ {\\displaystyle\\qquad\\leq\\left\\lVert\\nabla f_{(K-k-1)\\eta}(\\mathbf{0})\\right\\rVert+\\sqrt{\\frac{2L}{2L+1}}\\cdot(2L+1)\\cdot\\left\\lVert x_{0}\\right\\rVert\\leq\\left\\lVert\\nabla f_{(K-k-1)\\eta}(\\mathbf{0})\\right\\rVert+(2L+1)\\cdot\\left\\lVert x_{0}\\right\\rVert}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Besides, we should note $f_{(K-k-1)\\eta}$ is the smooth Assumption [A1) energy function of $p(K\\!-\\!k\\!-\\!1)\\eta$ denoting the underlying distribution of time $(K-k-1)\\eta$ in the forward OU process. Then, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\nabla f_{(K-k-1)\\eta}(\\mathbf{0})\\right|\\right|^{2}=\\!\\!\\mathbb{E}_{p_{(K-k-1)\\eta}}\\left[\\left\\|\\nabla f_{(K-k-1)\\eta}(\\mathbf{0})\\right\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\!\\!2\\mathbb{E}_{p_{(K-k-1)\\eta}}\\left[\\left\\|\\nabla f_{(K-k-1)\\eta}(\\mathbf{x})\\right\\|^{2}\\right]+2\\mathbb{E}_{p_{(K-k-1)\\eta}}\\left[\\left\\|\\nabla f_{(K-k-1)\\eta}(\\mathbf{x})-\\nabla f_{(K-k-1)\\eta}(\\mathbf{x})\\right\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\!2L d+2L^{2}\\mathbb{E}_{p_{(K-k-1)\\eta}}\\left[\\left\\|\\mathbf{x}\\right\\|^{2}\\right]\\leq2L d+2L^{2}\\operatorname*{max}\\left\\{d,m_{2}^{2}\\right\\}\\leq2L^{2}(d+m_{2}^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where the first inequality follows from Lemma E.6, and the third inequality follows from Lemma E.7. Under these conditions, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\|\\nabla g(\\mathbf{0})\\|\\leq L\\cdot{\\sqrt{2(d+m_{2}^{2})}}+3L\\cdot\\|\\mathbf{x}_{0}\\|\\,.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Then, for $R$ defined as ", "page_idx": 41}, {"type": "equation", "text": "$$\nR\\geq\\operatorname*{max}\\left\\{8\\cdot\\sqrt{\\frac{\\|\\nabla g(\\mathbf{0})\\|^{2}}{L^{2}}+\\frac{d}{L}},63\\cdot\\sqrt{\\frac{d}{L}\\log\\frac{16S}{\\epsilon}}\\right\\},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "we can choose $R$ to be the upper bound of RHS. Considering ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\vartheta\\cdot\\sqrt{\\frac{\\|\\nabla g(\\mathbf{0})\\|^{2}}{L^{2}}+\\frac{d}{L}}\\leq8\\cdot\\sqrt{\\frac{4L^{2}(d+m_{2}^{2})+18L^{2}\\|\\mathbf{x}_{0}\\|^{2}}{L^{2}}}+d\\leq63\\cdot\\sqrt{(d+m_{2}^{2}+\\|\\mathbf{x}_{0}\\|^{2})},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "then we choose ", "page_idx": 41}, {"type": "equation", "text": "$$\nR=63\\cdot\\sqrt{(d+m_{2}^{2}+\\|x_{0}\\|^{2})\\cdot\\log{\\frac{16S}{\\epsilon}}}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "After determining $R$ , the choice of $\\tau$ can be relaxed to ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\tau\\leq C_{\\tau}\\cdot\\left(L^{2}\\left(d+m_{2}^{2}+\\|x_{0}\\|^{2}\\right)\\cdot\\log\\frac{16S}{\\epsilon}\\right)^{-1},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where the absolute constant $C_{\\tau}=2^{-4}\\cdot3^{-8}\\cdot7^{-2}$ , since we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(3L R+G+\\epsilon_{\\mathrm{score}}\\right)^{2}\\leq9L^{2}R^{2}+4G^{2}}\\\\ &{\\leq9L^{2}\\cdot63^{2}\\cdot\\left(d+m_{2}^{2}+\\|x_{0}\\|^{2}\\right)\\cdot\\log\\displaystyle\\frac{16S}{\\epsilon}+4\\left(4L^{2}\\cdot\\left(d+m_{2}^{2}\\right)+18L^{2}\\|x_{0}\\|^{2}\\right)}\\\\ &{\\leq9\\cdot63^{2}\\cdot L^{2}\\left(d+m_{2}^{2}+\\|x_{0}\\|^{2}\\right)\\cdot\\log\\displaystyle\\frac{16S}{\\epsilon}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Hence, the proof is completed. ", "page_idx": 42}, {"type": "text", "text": "Theorem C.15. Under Assumption [A1]-[A2], for any $\\epsilon\\in(0,1)$ let $\\tilde{\\mu}_{*}(z)\\propto\\exp(-g(z))\\mathbf{1}[z\\in$ $B(\\mathbf{0},R)]$ be the truncated target distribution in $B(\\mathbf{0},R)$ with ", "page_idx": 42}, {"type": "equation", "text": "$$\nR=63\\cdot\\sqrt{(d+m_{2}^{2}+\\|x_{0}\\|^{2})\\cdot\\log\\frac{16S}{\\epsilon}}=\\tilde{\\mathcal{O}}\\left((d+m_{2}^{2}+\\|x_{0}\\|^{2})^{1/2}\\right),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "r in Alg. 2 satisfies ", "page_idx": 42}, {"type": "equation", "text": "$$\nr=3\\cdot\\sqrt{\\tau d\\log\\frac{8S}{\\epsilon}}=\\tilde{\\mathcal{O}}(\\tau^{1/2}d^{1/2})\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and $\\rho$ be the Cheeger constant of $\\tilde{\\mu}_{*}$ .Suppose $\\tilde{\\mu}_{0}(\\{\\|\\mathbf{x}\\|\\geq R/2\\})\\leq\\epsilon/16,$ the step size satisfy ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\tau\\leq C_{\\tau}\\cdot\\bigg(L^{2}\\left(d+m_{2}^{2}+\\|x_{0}\\|^{2}\\right)\\cdot\\log\\frac{16S}{\\epsilon}\\bigg)^{-1}=\\tilde{\\mathcal{O}}(L^{-2}\\cdot(d+m_{2}^{2}+\\|x_{0}\\|^{2})^{-1}),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "the score and energy estimation errors satisfy ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\epsilon_{\\mathrm{score}}\\leq\\frac{c_{0}\\rho}{32\\cdot36\\cdot\\sqrt{d\\log\\frac{8S}{\\epsilon}}}=\\mathcal{O}(\\rho d^{-1/2})\\quad\\mathrm{and}\\quad\\epsilon_{\\mathrm{energy}}\\leq\\frac{c_{0}\\rho\\sqrt{2\\tau}}{32\\cdot5}=\\mathcal{O}(\\rho\\tau^{1/2}),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "thenfor any $\\lambda$ -warm start with respect to $\\mu_{*}$ the output of both standard and projected implementation of Alg. 2 satisfies ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(\\mu_{S},\\mu_{*}\\right)=\\frac{\\epsilon}{2}+\\lambda\\left(1-\\frac{c_{0}^{2}\\rho^{2}}{4}\\cdot\\tau\\right)^{S}+\\tilde{\\mathcal{O}}(d^{1/2}\\rho^{-1}\\epsilon_{\\mathrm{score}})+\\mathcal{O}(\\rho^{-1}\\tau^{-1/2}\\epsilon_{\\mathrm{energy}})\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. We characterize the condition on the step size $\\tau$ . Combining Lemma C.2 and Corollary C.11, it requires the range of $\\tau$ to satisfy ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\tau\\leq16^{-1}\\cdot(3L R+\\|\\nabla g(\\mathbf{0})\\|+\\epsilon_{\\mathrm{score}})^{-2}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Under this condition, we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\tau\\leq\\frac{d\\log\\frac{8S}{\\epsilon}}{(3L R+G)^{2}},\\quad\\tau\\leq\\frac{d\\log\\frac{8S}{\\epsilon}}{\\epsilon_{\\mathrm{score}}^{2}},\\quad\\mathrm{and}\\quad\\tau\\leq\\left(72^{2}\\cdot\\epsilon_{\\mathrm{score}}^{2}\\cdot d\\log\\frac{8S}{\\epsilon}\\right)^{-1}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "which implies ", "page_idx": 42}, {"type": "equation", "text": "$$\n(3L R+G)\\epsilon_{\\mathrm{score}}\\cdot\\tau\\leq\\epsilon_{\\mathrm{score}}\\sqrt{\\tau}\\cdot\\sqrt{d\\log\\frac{8S}{\\epsilon}}\\quad\\mathrm{and}\\quad\\epsilon_{\\mathrm{score}}^{2}\\cdot\\tau\\leq\\epsilon_{\\mathrm{score}}\\sqrt{\\tau}\\cdot\\sqrt{d\\log\\frac{8S}{\\epsilon}}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Then, we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta=\\!16\\cdot\\left[\\frac{3\\epsilon_{\\mathrm{score}}}{2}\\cdot\\sqrt{\\tau d\\log\\frac{8S}{\\epsilon}}+\\frac{(3L R+G)\\epsilon_{\\mathrm{score}}\\cdot\\tau}{2}+\\frac{\\epsilon_{\\mathrm{score}}^{2}\\cdot\\tau}{4}\\right]}\\\\ &{~~\\leq\\!16\\cdot\\left[\\frac{3\\epsilon_{\\mathrm{score}}}{2}\\cdot\\sqrt{\\tau d\\log\\frac{8S}{\\epsilon}}+\\frac{\\epsilon_{\\mathrm{score}}}{2}\\cdot\\sqrt{\\tau d\\log\\frac{8S}{\\epsilon}}+\\frac{\\epsilon_{\\mathrm{score}}}{4}\\cdot\\sqrt{\\tau d\\log\\frac{8S}{\\epsilon}}\\right]}\\\\ &{~~=\\!36\\epsilon_{\\mathrm{score}}\\cdot\\sqrt{\\tau d\\log\\frac{8S}{\\epsilon}}\\leq\\frac{1}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "which matches the requirement of Lemma C.3. Under this condition, if we require ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\epsilon_{\\mathrm{score}}\\leq\\frac{c_{0}\\rho}{32\\cdot36\\cdot\\sqrt{d\\log\\frac{8S}{\\epsilon}}}=\\mathcal{O}(\\rho d^{-1/2})\\quad\\mathrm{and}\\quad\\epsilon_{\\mathrm{energy}}\\leq\\frac{c_{0}\\rho\\sqrt{2\\tau}}{32\\cdot5}=\\mathcal{O}(\\rho\\tau^{1/2}),\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "it makes ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\delta+5\\epsilon_{\\mathrm{energy}}\\leq36\\epsilon_{\\mathrm{score}}\\cdot\\sqrt{\\tau d\\log\\frac{8S}{\\epsilon}}+5\\epsilon_{\\mathrm{energy}}\\leq\\frac{c_{0}\\rho\\sqrt{2\\tau}}{16}\\leq\\frac{\\phi}{16}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "and satisfies the requirements shown in Lemma C.12. ", "page_idx": 43}, {"type": "text", "text": "Then, we are able to put the results of these lemmas together to establish the convergence of Alg. 2. Note that if $\\mu_{0}$ is a $\\lambda$ -warm start to $\\mu_{*}$ , it must be a $\\lambda$ warmstartto $\\tilde{\\mu}_{*}$ since $\\tilde{\\mu}_{*}(\\mathcal{A})\\stackrel{*}{\\geq}\\mu_{*}(\\mathcal{A})$ for all $A\\in\\Omega$ . Combining Lemma C.2, Lemma C.12 and Lemma C.13, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}\\left(\\mu_{S},\\mu_{*}\\right)\\le\\mathrm{TV}\\left(\\mu_{S},\\tilde{\\mu}_{S}\\right)+\\mathrm{TV}\\left(\\tilde{\\mu}_{S},\\tilde{\\mu}_{*}\\right)+\\mathrm{TV}\\left(\\tilde{\\mu}_{*},\\mu_{*}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\le\\frac{\\epsilon}{4}+\\left(\\lambda\\cdot\\left(1-\\frac{\\phi^{2}}{8}\\right)^{S}+\\frac{16\\left(\\delta+5\\epsilon_{\\mathrm{energy}}\\right)}{\\phi}\\right)+\\frac{\\epsilon}{4}}\\\\ &{\\quad\\quad\\quad\\le\\frac{\\epsilon}{2}+\\lambda\\left(1-\\frac{c_{0}^{2}\\rho^{2}}{4}\\cdot\\tau\\right)^{S}+408\\epsilon_{\\mathrm{score}}\\cdot\\frac{\\sqrt{d\\log\\frac{8S}{\\epsilon}}}{c_{0}\\rho}+\\frac{57\\epsilon_{\\mathrm{energy}}}{c_{0}\\rho\\sqrt{7}}}\\\\ &{\\quad\\quad\\quad\\quad=\\frac{\\epsilon}{2}+\\lambda\\left(1-\\frac{c_{0}^{2}\\rho^{2}}{4}\\cdot\\tau\\right)^{S}+\\tilde{\\mathcal{O}}(d^{1/2}\\rho^{-1}\\epsilon_{\\mathrm{score}})+\\mathcal{O}(\\rho^{-1}\\tau^{-1/2}\\epsilon_{\\mathrm{energy}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "After combining this result with the choice of parameters shown in Lemma C.14, the proof is completed. \u53e3 ", "page_idx": 43}, {"type": "text", "text": "Lemma C.16. Under the same assumptions and hyperparameter settings made in Theorem C.15, we useGaussian-typeinitialization ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\frac{\\mu_{0}(\\mathrm{d}z)}{\\mathrm{d}z}\\propto\\exp\\left(-L\\|z\\|^{2}-\\frac{\\left\\|x_{0}-e^{-\\eta}z\\right\\|^{2}}{2(1-e^{-2\\eta})}\\right).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Ifwesettheiterationnumberas ", "page_idx": 43}, {"type": "equation", "text": "$$\nS=\\tilde{\\mathcal{O}}\\left(L\\rho^{-2}\\cdot\\left(d+m_{2}^{2}\\right)\\tau^{-1}\\right),\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "the standard and projected implementation of Alg. 2 can achieve ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(\\mu_{S},\\mu_{*}\\right)\\leq\\frac{3\\epsilon}{4}+\\tilde{\\mathcal{O}}(d^{1/2}\\rho^{-1}\\epsilon_{\\mathrm{score}})+\\mathcal{O}(\\rho^{-1}\\tau^{-1/2}\\epsilon_{\\mathrm{energy}}).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Proof. We reformulate the target distribution $\\mu_{*}$ and the initial distribution $\\mu_{0}$ as follows ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\iota_{*}(\\mathrm{d}z)}{\\mathrm{d}z}\\propto\\exp\\left[-\\left(f_{(K-k-1)\\eta}(z)+\\frac{3L\\|z\\|^{2}}{2}\\right)-\\left(\\frac{\\|x_{0}-e^{-\\eta}z\\|^{2}}{2(1-e^{-2\\eta})}-\\frac{3L\\|z\\|^{2}}{2}\\right)\\right]:=\\exp\\left(-\\phi(z)\\cdot\\phi(z)\\right),}\\\\ &{\\frac{\\iota_{0}(\\mathrm{d}z)}{z}\\propto\\exp\\left[-L\\|z\\|^{2}-\\frac{3L\\|z\\|^{2}}{2}-\\left(\\frac{\\|x_{0}-e^{-\\eta}z\\|^{2}}{2(1-e^{-2\\eta})}-\\frac{3L\\|z\\|^{2}}{2}\\right)\\right]=\\exp\\left[-\\frac{5L\\|z\\|^{2}}{2}-\\psi(z)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Under this condition, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\frac{\\mu_{0}(\\mathrm{d}z)}{\\mu_{*}(\\mathrm{d}z)}\\leq\\frac{\\int_{\\mathbb{R}^{d}}\\exp\\left(-\\phi(z^{\\prime})-\\psi(z^{\\prime})\\right)\\mathrm{d}z^{\\prime}}{\\int_{\\mathbb{R}^{d}}\\exp\\left(-5L/2\\cdot\\|z^{\\prime}\\|^{2}-\\psi(z^{\\prime})\\right)\\mathrm{d}z^{\\prime}}\\cdot\\exp\\left(\\phi(z)-\\frac{5L\\|z\\|^{2}}{2}\\right)\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Due to Assumption [A1], we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n{\\frac{L I}{2}}\\preceq\\nabla^{2}f_{(K-k-1)\\eta}(z^{\\prime})+{\\frac{3L}{2}}=\\nabla^{2}\\phi(z^{\\prime})\\preceq{\\frac{5L I}{2}},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "which means ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\phi(z)\\leq\\phi(z_{*})+\\frac{5L}{4}\\cdot\\|z-z_{*}\\|^{2}\\leq\\phi(z_{*})+\\frac{5L\\|z\\|^{2}}{2}+\\frac{5L\\|z_{*}\\|^{2}}{2}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "and ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\exp\\left(\\phi(z)-\\frac{5L\\|z\\|^{2}}{2}\\right)\\leq\\exp\\left(\\phi(z_{*})+\\frac{5L\\|z_{*}\\|^{2}}{2}\\right).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Since the function $\\phi(z)$ is strongly log-concave, it satisfies ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\nabla\\phi(z)\\cdot z\\geq\\frac{L\\|z\\|^{2}}{4}-\\frac{\\|\\nabla\\phi(\\mathbf{0})\\|}{L}\\quad\\mathrm{and}\\quad\\phi(z)\\geq\\frac{L\\|z\\|^{2}}{16}+\\phi(z_{*})-\\frac{\\|\\nabla\\phi(\\mathbf{0})\\|^{2}}{2L}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "due to Lemma E.3 and Lemma E.4. Under these conditions, we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int\\exp\\left[-\\phi(z^{\\prime})-\\psi(z^{\\prime})\\right]\\mathrm{d}z^{\\prime}\\le\\exp\\left(-\\phi(z_{*})+\\frac{\\|\\nabla\\phi(\\mathbf{0})\\|^{2}}{2L}\\right)\\cdot\\int\\exp\\left[-\\frac{L\\|z^{\\prime}\\|^{2}}{16}-\\psi(z^{\\prime})\\right]\\mathrm{d}z^{\\prime}}\\\\ &{=\\exp\\left(-\\phi(z_{*})+\\frac{\\|\\nabla\\phi(\\mathbf{0})\\|^{2}}{2L}\\right)\\cdot\\int\\exp\\left[-\\frac{23L\\|z^{\\prime}\\|^{2}}{16}-\\frac{\\|x_{0}-e^{-\\eta}z^{\\prime}\\|^{2}}{2(1-e^{-2\\eta})}\\right]\\mathrm{d}z^{\\prime}}\\\\ &{\\displaystyle=\\exp\\left(-\\phi(z_{*})+\\frac{\\|\\nabla\\phi(\\mathbf{0})\\|^{2}}{2L}\\right)\\cdot\\int\\exp\\left[-\\frac{23L\\|z^{\\prime}\\|^{2}}{16}-\\frac{\\|x_{0}-e^{-\\eta}z^{\\prime}\\|^{2}}{2(1-e^{-2\\eta})}\\right]\\mathrm{d}z^{\\prime}}\\\\ &{\\displaystyle=\\exp\\left(-\\phi(z_{*})+\\frac{\\|\\nabla\\phi(\\mathbf{0})\\|^{2}}{2L}\\right)\\cdot\\int\\exp\\left[-\\frac{23L\\|z^{\\prime}\\|^{2}}{16}-\\frac{\\|x_{0}-e^{-\\eta}z^{\\prime}\\|^{2}}{2(1-e^{-2\\eta})}\\right]\\mathrm{d}z^{\\prime}}\\\\ &{\\displaystyle=\\exp\\left(-\\phi(z_{*})+\\frac{\\|\\nabla\\phi(\\mathbf{0})\\|^{2}}{2L}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Besides, we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\int\\exp\\left[-\\frac{5L\\|z^{\\prime}\\|^{2}}{2}-\\psi(z^{\\prime})\\right]\\mathrm{d}z^{\\prime}=\\int\\exp\\left[-L\\|z^{\\prime}\\|^{2}-\\frac{\\|x_{0}-e^{-\\eta}z^{\\prime}\\|^{2}}{2(1-e^{-2\\eta})}\\right]\\mathrm{d}z^{\\prime},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "which implies ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int\\exp\\left[-\\frac{5L\\|z^{\\prime}\\|^{2}}{2}-\\psi(z^{\\prime})\\right]\\mathrm{d}z^{\\prime}\\cdot\\int\\exp\\left[-\\frac{7L\\|z^{\\prime}\\|^{2}}{16}\\right]\\mathrm{d}z^{\\prime}}\\\\ {\\displaystyle\\geq\\int\\exp\\left[-\\frac{23L\\|z^{\\prime}\\|^{2}}{16}-\\frac{\\|x_{0}-e^{-\\eta}z^{\\prime}\\|^{2}}{2(1-e^{-2\\eta})}\\right]\\mathrm{d}z^{\\prime}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Plugging Eq. 76, Eq. 77 and Eq. 78 into Eq. 75, we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{\\mu_{0}(\\mathrm{d}z)}{\\tilde{\\mu}_{*}(\\mathrm{d}z)}\\leq\\exp\\left(\\frac{5L\\|z_{*}\\|^{2}}{2}+\\frac{\\|\\nabla\\phi(\\mathbf{0})\\|^{2}}{2L}\\right)\\cdot\\int\\exp\\left[-\\frac{7L\\|z^{\\prime}\\|^{2}}{16}\\right]\\mathrm{d}z^{\\prime}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Due to the strong convexity of $\\phi$ , it has ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\|z_{*}\\|^{2}\\leq\\frac{4\\|\\nabla\\phi(\\mathbf{0})-\\nabla\\phi(z_{*})\\|^{2}}{L^{2}}=\\frac{4\\|\\nabla\\phi(\\mathbf{0})\\|^{2}}{L^{2}}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla\\phi(\\mathbf{0})\\|^{2}=\\left\\|\\nabla f_{(K-k-1)\\eta}(\\mathbf{0})\\right\\|^{2}\\leq2L^{2}(d+m_{2}^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the inequality follows from Eq. 73. Combining with the fact ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\int\\exp\\left[-\\frac{7L\\|z^{\\prime}\\|^{2}}{16}\\right]\\mathrm{d}z^{\\prime}=\\left(\\frac{16\\pi}{7L}\\right)^{d/2},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Eq. 79 can be relaxed to ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\lambda\\leq\\operatorname*{max}_{z}\\frac{\\mu_{0}(\\mathrm{d}z)}{\\tilde{\\mu}_{*}(\\mathrm{d}z)}\\leq\\exp\\left(22L\\cdot(d+m_{2}^{2})\\right)\\cdot\\left(\\frac{16\\pi}{L}\\right)^{d/2}=\\exp\\left(\\mathcal{O}(L(d+m_{2}^{2}))\\right)\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "which is independent on $\\|\\pmb{x}_{0}\\|$ . Then, In order to ensure the convergence of the total variation distance is smaller than $\\epsilon$ ,itsufficesto choose $\\tau$ and $S$ suchthat ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\lambda\\left(1-\\frac{c_{0}^{2}\\rho^{2}}4\\cdot\\tau\\right)^{S}\\leq\\frac\\epsilon4\\quad\\Leftrightarrow\\quad S=\\mathcal{O}\\left(\\frac{\\log(\\lambda/\\epsilon)}{\\rho^{2}\\tau}\\right)=\\tilde{\\mathcal{O}}\\left(L\\rho^{-2}\\cdot\\left(d+m_{2}^{2}\\right)\\tau^{-1}\\right),\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the last two inequalities follow from Theorem C.15. Hence, the proof is completed. ", "page_idx": 44}, {"type": "text", "text": "Theorem C.17. Under Assumption $[A I]$ [A2], for Alg. 1, we choose ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\eta=\\frac{1}{2}\\log\\frac{2L+1}{2L}\\quad\\mathrm{and}\\quad K=4L\\cdot\\log\\frac{(1+L^{2})d+\\left\\Vert\\nabla f_{*}(\\mathbf{0})\\right\\Vert^{2}}{\\epsilon^{2}}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and implement Step 3 of Alg. 1 with projected Alg. 2. For the $k$ -th run of Alg. 2, we use Gaussian-type initialization ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\frac{\\mu_{0}(\\mathrm{d}z)}{\\mathrm{d}z}\\propto\\exp\\left(-L\\|z\\|^{2}-\\frac{\\left\\|\\hat{\\mathbf{x}}_{k}-e^{-\\eta}z\\right\\|^{2}}{2(1-e^{-2\\eta})}\\right).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "If we set the hyperparameters as shown in Lemma C.16, it can achieve ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(\\hat{p}_{K\\eta},p_{*}\\right)\\le\\epsilon+\\tilde{\\mathcal{O}}(L d^{1/2}\\rho^{-1}\\epsilon_{\\mathrm{score}})+\\mathcal{O}(\\hat{\\tau}^{-1/2}\\cdot L^{2}(d^{1/2}+m_{2}+Z)\\rho^{-1}\\epsilon_{\\mathrm{energy}})\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "with a gradient complexity as follows ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{O}}\\left(L^{4}\\rho^{-2}\\hat{\\tau}^{-1}\\cdot\\left(d+m_{2}^{2}\\right)^{2}Z^{2}\\right)\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "for any $\\hat{\\tau}\\,\\in\\,(0,1)$ where $Z$ denotes the maximal $l_{2}$ norm of particles appearing in outer loops $(A l g.\\;I)$ ", "page_idx": 45}, {"type": "text", "text": "Proof. According to Lemma B.3, we know that under the choice ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\eta=\\frac{1}{2}\\ln{\\frac{2L+1}{2L}},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "it requires to run Alg. 2 for $K$ times where ", "page_idx": 45}, {"type": "equation", "text": "$$\nK=4L\\cdot\\log\\frac{(1+L^{2})d+\\left\\|\\nabla f_{*}(\\mathbf{0})\\right\\|^{2}}{\\epsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "For each run of Alg. 2, we require the total variation error to achieve ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}\\left(\\hat{p}_{(k+1)\\eta\\mid k\\eta}(\\cdot\\vert\\hat{x}),\\bar{p}_{(k+1)\\eta\\mid k\\eta}^{\\leftarrow}(\\cdot\\vert\\hat{x})\\right)\\le\\!\\frac{\\epsilon}{4L}\\cdot\\left[\\log\\frac{(1+L^{2})d+\\|\\nabla f_{*}(\\mathbf{0})\\|^{2}}{\\epsilon^{2}}\\right]^{-1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\tilde{\\mathcal{O}}(d^{1/2}\\rho^{-1}\\epsilon_{\\mathrm{score}})+\\mathcal{O}(\\rho^{-1}\\tau_{k}^{-1/2}\\epsilon_{\\mathrm{energy}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Combining with Lemma C.16, we consider a step size ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tau_{k}=\\!C_{\\tau}\\cdot\\left(L^{2}\\left(d+m_{2}^{2}+\\|\\hat{\\mathbf{x}}_{k}\\|^{2}\\right)\\cdot\\log\\frac{48L S\\log\\frac{(1+L^{2})d+\\|\\nabla f_{*}(\\mathbf{0})\\|^{2}}{\\epsilon^{2}}}{\\epsilon}\\right)^{-1}\\cdot\\hat{\\tau}}\\\\ &{\\quad=\\!\\tilde{O}(L^{-2}\\cdot(d+m_{2}^{2}+\\|\\hat{\\mathbf{x}}_{k}\\|^{2})^{-1}\\cdot\\hat{\\tau})}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where $\\tau^{\\prime}\\in(0,1)$ ,to solvethe $k$ -th inner sampling subproblem. Then, the maximum iteration number willbe ", "page_idx": 45}, {"type": "equation", "text": "$$\nS=\\tilde{\\mathcal{O}}\\left(L^{3}\\rho^{-2}\\hat{\\tau}^{-1}\\cdot\\left(d+m_{2}^{2}\\right)^{2}\\cdot\\Vert\\hat{\\mathbf{{x}}}_{k}\\Vert^{2}\\right).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "This means that with the total gradient complexity ", "page_idx": 45}, {"type": "equation", "text": "$$\nK\\cdot S=\\tilde{\\mathcal{O}}\\left(L^{4}\\rho^{-2}\\hat{\\tau}^{-1}\\cdot\\left(d+m_{2}^{2}\\right)^{2}Z^{2}\\right)\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where $Z$ denotes the maximal $l_{2}$ norm of particles appearing in outer loops (Alg. 1), we can obtain ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{TV}\\left(\\hat{p}_{K\\eta},p_{*}\\right)\\leq\\epsilon+\\tilde{\\mathcal{O}}(K d^{1/2}\\rho^{-1}\\epsilon_{\\mathrm{score}})+\\mathcal{O}(K L(d+m_{2}^{2}+Z^{2})^{1/2}\\hat{\\tau}^{-1/2}\\rho^{-1}\\epsilon_{\\mathrm{energy}})}\\\\ {=\\epsilon+\\tilde{\\mathcal{O}}(L d^{1/2}\\rho^{-1}\\epsilon_{\\mathrm{score}})+\\mathcal{O}(\\hat{\\tau}^{-1/2}\\cdot L^{2}(d^{1/2}+m_{2}+Z)\\rho^{-1}\\epsilon_{\\mathrm{energy}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Hence, the proof is completed ", "page_idx": 45}, {"type": "text", "text": "Lemma C.18. Suppose we implement Alg. 2 with its projected version, we have ", "page_idx": 45}, {"type": "equation", "text": "$$\nZ^{2}\\leq\\tilde{\\mathcal{O}}\\left(L^{3}(d+m_{2}^{2})^{2}\\rho^{-2}\\right).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where $Z$ denotesthemaximal $l_{2}$ norm of particles appearing in outer loops (Alg. 1) ", "page_idx": 45}, {"type": "text", "text": "Proof. Suppose we implement Alg. 2 with its projected version, where each update will be projected to a ball with a ratio $r$ shown in Lemma C.2. Under these conditions, we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\left\\|{\\hat{x}}_{K}\\right\\|^{2}=\\left\\|{\\hat{x}}_{0}+\\sum_{i=1}^{K}({\\hat{x}}_{i}-{\\hat{x}}_{i-1})\\right\\|^{2}\\leq{(K+1)}\\left\\|{\\hat{x}}_{0}\\right\\|^{2}+{(K+1)}\\cdot\\sum_{i=1}^{K}\\left\\|{\\hat{x}}_{i}-{\\hat{x}}_{i-1}\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "For each $i\\in\\{1,2,\\ldots K\\}$ , we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\left\\|\\hat{x}_{i}-\\hat{x}_{i-1}\\right\\|^{2}=\\left\\|z_{S}-z_{0}\\right\\|^{2}\\leq(S+1)\\cdot\\sum_{j=1}^{S}\\left\\|z_{j}-z_{j-1}\\right\\|^{2}\\leq2S\\cdot r^{2}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Follows from Lemma C.16, it has ", "page_idx": 46}, {"type": "equation", "text": "$$\nS\\cdot r^{2}=\\mathcal{O}\\left(\\frac{\\log(\\lambda/\\epsilon)}{\\rho^{2}\\tau}\\right)\\cdot\\tilde{\\mathcal{O}}\\left(\\tau d\\right)=\\tilde{\\mathcal{O}}\\left(\\frac{d\\log(\\lambda/\\epsilon)}{\\rho^{2}}\\right)=\\tilde{\\mathcal{O}}\\left(L(d+m_{2}^{2})^{2}\\rho^{-2}\\right).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Then, we have ", "page_idx": 46}, {"type": "equation", "text": "$$\nZ^{2}\\leq\\mathcal{O}(K^{2})\\cdot\\tilde{\\mathcal{O}}\\left(L(d+m_{2}^{2})^{2}\\rho^{-2}\\right)=\\tilde{\\mathcal{O}}\\left(L^{3}(d+m_{2}^{2})^{2}\\rho^{-2}\\right),\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Hence, the proof is completed. ", "page_idx": 46}, {"type": "text", "text": "C.5 Control the error from Energy Estimation ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Corollary C.19. Suppose the diffusion model $\\scriptstyle{s_{\\theta}}$ satisfies ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\|\\hat{\\pmb{s}}_{\\pmb{\\theta}}(\\pmb{x},t)+\\nabla\\log p_{t}(\\pmb{x})\\|_{\\infty}\\leq\\frac{\\rho\\epsilon}{L d^{1/2}},\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "and another parameterized model $\\hat{l}_{\\hat{\\theta}}(\\mathbf x,t)$ is used to estimate the log-likelihood of $p_{t}(\\pmb{x})$ satisfying ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\left\\|\\hat{l}_{\\theta^{\\prime}}(\\mathbf{x},t)+\\log p_{t}(\\mathbf{x})\\right\\|_{\\infty}\\leq\\frac{\\rho\\epsilon}{L^{2}\\cdot(d^{1/2}+m_{2}+Z)}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "If we implement Alg. 1 with the projected version of Alg. 2, it has ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(\\hat{p}_{K\\eta},p_{*}\\right)\\le\\tilde{\\mathcal{O}}(\\epsilon)\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "with the following gradient complexity ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{O}}\\left(L^{4}\\rho^{-2}\\cdot\\left(d+m_{2}^{2}\\right)^{2}Z^{2}\\right).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Proof. Since we have highly accurate scores and energy estimation, we can construct $\\scriptstyle{s_{\\theta}}$ and $r_{\\theta^{\\prime}}$ (shown in Eq. 23) for the $k$ -th inner loop as follows ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{s_{\\theta}(z)=\\hat{s}_{\\theta}(z,(K-k-1)\\eta)+\\frac{e^{-2\\eta}z-e^{-\\eta}\\hat{x}_{k}}{1-e^{-2\\eta}}}\\\\ &{r_{\\theta^{\\prime}}(z,z^{\\prime})=\\hat{l}_{\\theta^{\\prime}}(z,(K-k-1)\\eta)+\\frac{\\left\\|\\hat{x}_{k}-e^{-\\eta}\\cdot z\\right\\|^{2}}{2\\left(1-e^{-2\\eta}\\right)}}\\\\ &{\\qquad\\qquad\\quad-\\left(\\hat{l}_{\\theta^{\\prime}}(z^{\\prime},(K-k-1)\\eta)+\\frac{\\left\\|\\hat{x}_{k}-e^{-\\eta}\\cdot z^{\\prime}\\right\\|^{2}}{2\\left(1-e^{-2\\eta}\\right)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Under these conditions, we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\epsilon_{\\mathrm{energy}}\\leq\\frac{\\rho\\epsilon}{L^{2}\\cdot(d^{1/2}+m_{2}+Z)}\\quad\\mathrm{and}\\quad\\epsilon_{\\mathrm{score}}\\leq\\frac{\\rho\\epsilon}{L d^{1/2}}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Plugging these results into Theorem C.17 and setting $\\hat{\\tau}=1/2$ ,wehave ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(\\hat{p}_{K\\eta},p_{*}\\right)\\le\\tilde{\\mathcal{O}}(\\epsilon)\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "with the following gradient complexity ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{O}}\\left(L^{4}\\rho^{-2}\\cdot\\left(d+m_{2}^{2}\\right)^{2}Z^{2}\\right).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Corollary C.20. Suppose the score estimation is extremely small, i.e., ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\lVert\\hat{\\pmb{s}}_{\\pmb{\\theta}}(\\pmb{x},t)+\\nabla\\log p_{t}(\\pmb{x})\\rVert_{\\infty}\\ll\\frac{\\rho\\epsilon}{L d^{1/2}},\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "and the log-likelihood function of $p_{t}$ has a bounded 3-order derivative, e.g., ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\left\\|\\nabla^{(3)}f(z)\\right\\|\\leq L,\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "we have a non-parametric estimation for log-likelihood to make we have T $\\mathrm{V}\\left(\\hat{p}_{K\\eta},p_{*}\\right)\\le\\tilde{\\mathcal{O}}(\\epsilon)$ with ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{O}}\\left(L^{4}\\rho^{-3}\\cdot\\left(d+m_{2}^{2}\\right)^{2}Z^{3}\\cdot\\epsilon\\right).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "gradient calls. ", "page_idx": 47}, {"type": "text", "text": "Proof. Combining the Alg. 2 and the definition of $\\epsilon_{\\mathrm{{energy}}}$ shown in Lemma C.4, we actually require to control ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\epsilon_{\\mathrm{energy}}:=(g(\\tilde{z}_{s})-g(z_{s}))-r_{\\theta}(\\tilde{z}_{s},z_{s})\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "for any $s\\in[0,S-1]$ . Then, we start to construct $r_{\\theta}(\\tilde{z}_{s},z_{s})$ . Since we have ", "page_idx": 47}, {"type": "equation", "text": "$$\ng(\\tilde{z}_{s})-g(z_{s})=f_{(K-k-1)\\eta}(\\tilde{z}_{s})+\\frac{\\|x_{0}-\\tilde{z}_{s}\\cdot e^{-\\eta}\\|^{2}}{2(1-e^{-2\\eta})}-f_{(K-k-1)\\eta}(z_{s})-\\frac{\\|x_{0}-z_{s}\\cdot e^{-\\eta}\\|^{2}}{2(1-e^{-2\\eta})},\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "we should only estimate the difference of the energy function $f_{(K-k-1)\\eta}$ which will be presented as $f$ for abbreviation. Besides, we define the following function ", "page_idx": 47}, {"type": "equation", "text": "$$\nh(t)=f\\left((\\tilde{z}_{s}-z_{s})\\cdot t+z_{s}\\right),\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "which means ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle h^{(1)}(t):=\\frac{\\mathrm{d}h(t)}{\\mathrm{d}t}=\\nabla f\\left((\\tilde{z}_{s}-z_{s})\\cdot t+z_{s}\\right)\\cdot\\left(\\tilde{z}_{s}-z_{s}\\right)}\\ ~}\\\\ {{\\displaystyle h^{(2)}(t):=\\frac{\\mathrm{d}^{2}h(t)}{(\\mathrm{d}t)^{2}}=\\left(\\tilde{z}_{s}-z_{s}\\right)^{\\top}\\nabla^{2}f\\left(\\left(\\tilde{z}_{s}-z_{s}\\right)\\cdot t+z_{s}\\right)\\left(\\tilde{z}_{s}-z_{s}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Under the high-order smoothness condition, i.e., ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\left\\|\\nabla^{3}f(z)\\right\\|\\leq L\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where $\\|\\cdot\\|$ denotes the nuclear norm, then we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n|h(1)-h(0)|\\leq\\sum_{i=1}^{2}\\frac{h^{(i)}(0)}{i!}+\\frac{L\\cdot\\|\\tilde{z}_{s}-z_{s}\\|^{3}}{3!}\\leq\\sum_{i=1}^{2}\\frac{h^{(i)}(0)}{i!}+\\frac{L r^{3}}{3!}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "It means we need to approximate $h^{(i)}$ with high accuracy. ", "page_idx": 47}, {"type": "text", "text": "For $i=1$ , the ground truth $h^{(1)}(0)$ is ", "page_idx": 47}, {"type": "equation", "text": "$$\nh^{(1)}(0)=\\frac{\\,\\mathrm{d}h(t)}{\\,\\mathrm{d}t}=\\nabla f\\left(z_{s}\\right)\\cdot\\left(\\tilde{z}_{s}-z_{s}\\right)\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "we can approximate it numerically as ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\tilde{h}^{(1)}(0):=s_{\\theta}(z_{s})\\cdot(\\tilde{z}_{s}-z_{s})\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "since we have score approximation. Then it has ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\delta^{(1)}(0)=h^{(1)}(0)-\\tilde{h}^{(1)}(0)\\leq\\|\\nabla f(z_{s})-s_{\\theta}(z_{s})\\|\\cdot\\|\\tilde{z}_{s}-z_{s}\\|\\leq\\epsilon_{\\mathrm{score}}\\cdot r.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Then, for $i=2$ , we obtain the ground truth $h^{(2)}(0)$ by ", "page_idx": 47}, {"type": "equation", "text": "$$\nh^{(1)}(t)-h^{(1)}(0)=\\int_{0}^{t}h^{(2)}(\\tau)\\mathrm{d}\\tau=t h^{(2)}(0)+\\int_{0}^{t}h^{(2)}(\\tau)-h^{(2)}(0)\\mathrm{d}\\tau,\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "which means ", "page_idx": 47}, {"type": "equation", "text": "$$\nh^{(2)}(0)=\\frac{h^{(1)}(t)-h^{(1)}(0)}{t}+\\frac{1}{t}\\cdot\\int_{0}^{t}h^{(2)}(\\tau)-h^{(2)}(0)\\mathrm{d}\\tau.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "If we use the differential to approximate $h^{(2)}(0)$ , i.e., ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\tilde{h}^{(2)}(0):=\\frac{\\tilde{h}^{(1)}(t)-\\tilde{h}^{(1)}(0)}{t},\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "we find the error term will be ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\delta^{(2)}(0)=\\left|h^{(2)}(0)-\\tilde{h}^{(2)}(0)\\right|=\\left|\\frac{2\\delta^{(1)}}{t}+\\frac{1}{t}\\cdot\\int_{0}^{t}h^{(2)}(\\tau)-h^{(2)}(0)\\mathrm{d}\\tau\\right|.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "If we use smoothness to relax the integration term, we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|h^{(2)}(\\tau)-h^{(2)}(0)\\right|\\le\\left\\|\\nabla^{2}f\\left((\\tilde{z}_{s}-z_{s})\\cdot\\tau+z_{s}\\right)-\\nabla^{2}f(z_{s})\\right\\|\\cdot\\left\\|\\tilde{z}_{s}-z_{s}\\right\\|^{2}\\le L\\tau\\cdot\\left\\|\\tilde{z}_{s}-z_{s}\\right\\|^{3},}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "which means ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\frac{1}{t}\\cdot\\int_{0}^{t}h^{(2)}(\\tau)-h^{(2)}(0)\\mathrm{d}\\tau\\leq\\frac{L\\left\\|\\tilde{z}_{s}-z_{s}\\right\\|^{3}}{t}\\cdot\\int_{0}^{t}\\tau\\mathrm{d}\\tau\\leq\\frac{t L r^{3}}{2}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Combining Eq. 80, Eq. 81 and Eq. 82, we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\delta^{(2)}(0)\\le\\frac{2\\epsilon_{\\mathrm{score}}r}{t}+\\frac{L r^{3}t}{2},\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "which means the final energy estimation error will be ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|h(1)-h(0)-\\left(\\tilde{h}^{(1)}(0)+\\frac{\\tilde{h}^{(1)}(t)-\\tilde{h}^{(1)}(0)}{2t}\\right)\\right|}\\\\ &{\\leq\\frac{\\delta^{(1)}(0)}{1}+\\frac{\\delta^{(2)}(0)}{2}+\\frac{L r^{3}}{3!}=\\underbrace{\\epsilon_{\\mathrm{score}}\\cdot r}_{\\mathrm{Term~1}}+\\underbrace{\\frac{1}{2}\\cdot\\left(\\frac{2\\epsilon_{\\mathrm{score}}r}{t}+\\frac{L r^{3}t}{2}\\right)}_{\\mathrm{Term~2}}+\\frac{L r^{3}}{6}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Considering $\\epsilon_{\\mathrm{score}}$ is extremely small (compared with the output performance error tolerance $\\epsilon$ ),we can choose $t$ depending on $\\epsilon_{\\mathrm{score}}$ ,e.g., $t=\\sqrt{\\epsilon_{\\mathrm{score}}}$ , to make Term 1 and Term 2 in Eq. 83 diminish. Under this condition, the term $L r^{3}/6$ will dominate RHS of Eq. 83. Besides, we have ", "page_idx": 48}, {"type": "equation", "text": "$$\nr=3\\cdot\\sqrt{\\tau d\\log\\frac{8S}{\\epsilon}}=\\tilde{O}(\\tau^{1/2}d^{1/2}),\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "then we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\epsilon_{\\mathrm{energy}}=\\mathcal{O}(L r^{3})=\\mathcal{O}(L d^{3/2}\\tau^{3/2})=\\tilde{\\mathcal{O}}\\left(L^{-2}d^{3/2}\\left(d+m_{2}^{2}+\\|\\hat{\\pmb{x}}_{k}\\|^{2}\\right)^{-3/2}\\hat{\\tau}^{3/2}\\right)\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where the last equation follows from the choice of $\\tau$ shown in Theorem C.15. Then, plugging this result into Theorem C.17 and considering $\\epsilon_{\\mathrm{score}}\\ll\\epsilon$ wehave ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}\\left(\\hat{p}_{K\\eta},p_{*}\\right)\\le\\epsilon+\\tilde{\\mathcal{O}}(L d^{1/2}\\rho^{-1}\\epsilon_{\\mathrm{score}})+\\mathcal{O}(\\hat{\\tau}^{-1/2}\\cdot L^{2}(d^{1/2}+m_{2}+Z)\\rho^{-1}\\epsilon_{\\mathrm{energy}})}\\\\ &{\\qquad\\qquad\\qquad\\le\\tilde{\\mathcal{O}}(\\epsilon)+\\tilde{\\mathcal{O}}\\left(\\hat{\\tau}(d^{1/2}+m_{2}+Z)\\rho^{-1}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "with a gradient complexity as follows ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{O}}\\left(L^{4}\\rho^{-2}\\hat{\\tau}^{-1}\\cdot\\left(d+m_{2}^{2}\\right)^{2}Z^{2}\\right).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Then, by choosing ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\tau=\\frac{\\epsilon\\rho}{d^{1/2}+m_{2}+Z},\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "we have $\\mathrm{TV}\\left(\\hat{p}_{K\\eta},p_{*}\\right)\\le\\tilde{\\mathcal{O}}(\\epsilon)$ with ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{O}}\\left(L^{4}\\rho^{-3}\\cdot\\left(d+m_{2}^{2}\\right)^{2}Z^{3}\\cdot\\epsilon\\right).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Hence, the proof is completed. ", "page_idx": 48}, {"type": "text", "text": "Remark 3. If we consider more high-order smooth, i.e., ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\left\\|\\nabla^{(u)}f(z)\\right\\|\\leq L,\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "with similar techniques shown in Corollary C.20, we can have the following bound, i.e., ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\epsilon_{\\mathrm{energy}}={\\mathcal O}(L r^{u})\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "when Escore $i s$ extremely small. Under this condition, since it has ", "page_idx": 49}, {"type": "equation", "text": "$$\nr=3\\cdot\\sqrt{\\tau d\\log\\frac{8S}{\\epsilon}}=\\tilde{O}(\\tau^{1/2}d^{1/2}),\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathrm{energy}=\\mathcal{O}(L r^{u})=\\mathcal{O}(L d^{u/2}\\tau^{u/2})=\\tilde{\\mathcal{O}}\\left(L^{-u+1}d^{u/2}\\left(d+m_{2}^{2}+\\|\\hat{x}_{k}\\|^{2}\\right)^{-u/2}\\hat{\\tau}^{u/2}\\right)=\\tilde{\\mathcal{O}}(L^{-u+1}\\hat{\\tau}^{u/2}).\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Then, plugging this result into Theorem C.17 and considering Escore $\\ll\\epsilon$ wehave ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}\\left(\\hat{p}_{K\\eta},p_{*}\\right)\\le\\epsilon+\\tilde{\\mathcal{O}}(L d^{1/2}\\rho^{-1}\\epsilon_{\\mathrm{score}})+\\mathcal{O}(\\hat{\\tau}^{-1/2}\\cdot L^{2}(d^{1/2}+m_{2}+Z)\\rho^{-1}\\epsilon_{\\mathrm{energy}})}\\\\ &{\\qquad\\qquad\\qquad=\\tilde{\\mathcal{O}}(\\epsilon)+\\tilde{\\mathcal{O}}\\left(\\hat{\\tau}^{(u-1)/2}L^{-u+3}(d^{1/2}+m_{2}+Z)\\rho^{-1}\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\tilde{\\mathcal{O}}(\\epsilon)+\\tilde{\\mathcal{O}}\\left(\\hat{\\tau}^{(u-1)/2}(d^{1/2}+m_{2}+Z)\\rho^{-1}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "wherewesuppose $L\\geq1$ in the last equation without loss of generality. Then, by supposing ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\hat{\\tau}=\\frac{\\epsilon^{2/(u-1)}\\rho}{d^{1/2}+m_{2}+Z}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "we have TV $\\dot{\\left(p_{K\\eta},p_{*}\\right)}\\le\\tilde{\\mathcal{O}}(\\epsilon)$ with ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{O}}\\left(L^{4}\\rho^{-3}\\cdot\\left(d+m_{2}^{2}\\right)^{2}Z^{3}\\cdot\\epsilon^{-2/(u-1)}\\cdot2^{u}\\right)\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "wherethelast $2^{u}$ appears since the estimation of high-order derivatives requires an exponentially increasingcall of score estimations. ", "page_idx": 49}, {"type": "text", "text": "D  Implement RTK inference with ULD ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "In this section, we consider introducing a ULD to sample frm $p_{k+1|k}^{\\leftarrow}(z|x_{0})$ . To simplify the notation, we set ", "page_idx": 49}, {"type": "equation", "text": "$$\ng(z):=f_{(K-k-1)\\eta}(z)+\\frac{\\|x_{0}-z\\cdot e^{-\\eta}\\|^{2}}{2(1-e^{-2\\eta})}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "and consider $k$ and $\\pmb{x}_{0}$ to be fixed. Besides, we set ", "page_idx": 49}, {"type": "equation", "text": "$$\np^{\\leftarrow}(z|x_{0}):=p_{k+1|k}^{\\leftarrow}(z|x_{0})\\propto\\exp(-g(z))\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "According to Corollary B.5 and Corollary B.3, when we choose ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\eta=\\frac{1}{2}\\log\\frac{2L+1}{2L},\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "the log density $g$ will be $L$ -strongly log-concave and $3L$ -smooth. ", "page_idx": 49}, {"type": "text", "text": "For the underdamped Langevin dynamics, we utilize a form similar to that shown in [39], i.e., ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}\\hat{\\mathbf{z}}_{t}=\\hat{\\mathbf{v}}_{t}\\mathrm{d}t}\\\\ &{\\mathrm{d}\\hat{\\mathbf{v}}_{t}=-\\gamma\\hat{\\mathbf{v}}_{t}\\mathrm{d}t-s_{\\theta}\\big(\\hat{\\mathbf{z}}_{s\\tau}\\big)\\mathrm{d}t+\\sqrt{2\\gamma}\\mathrm{d}B_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "with a little abuse of notation for $t\\in[s\\tau,(s+1)\\tau)$ . We denote the underlying distribution of $\\left(\\hat{\\mathbf{z}}_{t},\\hat{\\mathbf{v}}_{t}\\right)$ as $\\hat{\\pi}_{t}$ , and the exact continuous SDE ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}\\mathbf{z}_{t}=\\mathbf{v}_{t}\\mathrm{d}t}\\\\ &{\\mathrm{d}\\mathbf{v}_{t}=-\\gamma\\mathbf{v}_{t}\\mathrm{d}t-\\nabla g(\\mathbf{z}_{t})\\mathrm{d}t+\\sqrt{2\\gamma}\\mathrm{d}B_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "has the underlying distribution $(\\mathbf{z}_{t},\\mathbf{v}_{t})\\sim\\pi_{t}$ . The stationary distribution of the continuous version is defined as ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\pi^{\\leftarrow}(z,\\pmb{v}|\\pmb{x}_{0})\\propto\\exp\\left(-g(z)-\\frac{\\|\\pmb{v}\\|^{2}}{2}\\right)\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where the $_{z}$ -marginal of $\\pi^{\\leftarrow}\\big(\\cdot|\\mathbf{x}_{0}\\big)$ is $p^{\\leftarrow}(\\cdot|{\\pmb x}_{0})$ which is the desired target distribution of inner loops. Therefore, by taking a small step size for the discretization and a large number of iterations, ULD will yield an approximate sample from $p^{\\leftarrow}(\\cdot|x_{0})$ . Besides, in the analysis of ULD, we usually consider an alternate system of coordinates ", "page_idx": 50}, {"type": "equation", "text": "$$\n(\\phi,\\psi):=\\mathcal{M}(z,\\boldsymbol{v}):=(z,z+\\frac{2}{\\gamma}\\boldsymbol{v}),\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "their distributions of the continuous time iterates $\\pi_{t}^{\\mathcal{M}}$ and the target in these alternate coordinates $\\pi^{\\mathcal{M}}$ , respectively. Besides, we need to define log-Sobolev inequality as follows ", "page_idx": 50}, {"type": "text", "text": "Definition 2 (Log-Sobolev Inequality). The target distribution $p_{*}$ satisfies the following inequality ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\mathbb{E}_{p_{*}}\\left[g^{2}\\log g^{2}\\right]-\\mathbb{E}_{p_{*}}[g^{2}]\\log\\mathbb{E}_{p_{*}}[g^{2}]\\le2C_{\\mathrm{LSI}}\\mathbb{E}_{p_{*}}\\left\\|\\nabla g\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "withaconstant $C_{\\mathrm{LSI}}$ forallsmoothfunction $g\\colon\\ensuremath{\\mathbb{R}}^{d}\\to\\ensuremath{\\mathbb{R}}$ satisfying $\\mathbb{E}_{p_{*}}[g^{2}]<\\infty$ ", "page_idx": 50}, {"type": "text", "text": "Remark 4. Log-Sobolev inequality is a milder condition than strong log-concavity. Suppose $p$ satisfies $m$ -strongly log-concavity,it satisfies $1/m\\,L S I,$ which is proved in Lemma E.9. ", "page_idx": 50}, {"type": "text", "text": "Definition 3 (Poincar\u00e9 Inequality). The target distribution $p$ satisfies thefollowinginequality ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathbf{x}\\sim p}\\left[\\left\\|g(\\mathbf{x})-\\mathbb{E}_{\\mathbf{x}\\sim p}[g(\\mathbf{x})]\\right\\|^{2}\\right]\\le C_{\\mathrm{PI}}\\mathbb{E}_{p}\\left\\|\\nabla g\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "witha constant $C_{\\mathrm{PI}}$ for all smoothfunction $g\\colon\\ensuremath{\\mathbb{R}}^{d}\\to\\ensuremath{\\mathbb{R}}$ satisfying $\\mathbb{E}_{p_{*}}[g^{2}]<\\infty$ ", "page_idx": 50}, {"type": "text", "text": "In the following, we mainly follow the idea of proof shown in [39], which provides the convergence of KL divergence for ULD, to control the error from the sampling subproblems. ", "page_idx": 50}, {"type": "text", "text": "Lemma D.1 (Proposition 14 in [39]). Let $\\pi_{t}^{\\mathcal{M}}$ denotethelawof thecontinuous-timeunderdamped Langevin diffusion with $\\gamma\\,=\\,c{\\sqrt{3L}}$ for $c\\ge\\sqrt{2}$ in the $(\\phi,\\psi)$ coordinates.Supposetheinitial distribution $\\pi_{0}$ has a log-Sobolev $(L S I)$ constant(inthealteredcoordinates) $C_{\\mathrm{LSI}}(\\bar{\\pi}_{0}^{\\bar{M}})$ then $\\{\\pi_{t}^{\\mathcal{M}}\\}$ satisfiesLSI witha constant that canbeuniformly upper boundedby ", "page_idx": 50}, {"type": "equation", "text": "$$\nC_{\\mathrm{LSI}}(\\pi_{t}^{\\mathcal{M}})\\leq\\exp\\left(-\\sqrt{\\frac{2L}{3}}\\cdot t\\right)\\cdot C_{\\mathrm{LSI}}(\\pi_{0}^{\\mathcal{M}})+\\frac{2}{L}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Lemma D.2 (Adapted from Proposition 1 of [25]). Consider the following Lyapunov functional ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\mathcal{F}(\\pi^{\\prime},\\pi^{\\leftarrow}):=\\mathrm{KL}\\left(\\pi^{\\prime}\\|\\pi^{\\leftarrow}\\right)+\\mathbb{E}_{\\pi^{\\prime}}\\left[\\left\\|\\mathfrak{M}^{1/2}\\nabla\\log\\frac{\\pi^{\\prime}}{\\pi^{+}}\\right\\|^{2}\\right],\\quad\\mathrm{where}\\quad\\mathfrak{M}=\\left[\\frac{\\frac{1}{12L}}{\\frac{1}{\\sqrt{6L}}}\\quad\\frac{\\frac{1}{\\sqrt{6L}}}{4}\\right]\\otimes I_{d}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "For targets $\\pi^{\\leftarrow}\\propto\\exp(-g)$ which are $3L$ -smooth and satisfy $L S I$ with constant $1/L$ let $\\gamma=2\\sqrt{6L}$ Then the law $\\pi_{t}$ of ULD satisfies ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\partial_{t}\\mathcal{F}(\\pi_{t},\\pi^{\\leftarrow})\\leq-\\frac{\\sqrt{L}}{10\\sqrt{6}}\\cdot\\mathcal{F}(\\pi_{t},\\pi^{\\leftarrow}).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Lemma D.3 (Variant of Lemma 4.8 in [1]). Let $\\hat{\\pi}_{t}$ denote the law of SDE. 85 and $\\pi_{t}$ denote the law of the continuous time underdamped Langevin diffusion with the same initialization, i.e., $\\hat{\\pi}_{0}=\\pi_{0}$ $I\\!f$ $\\gamma\\asymp\\sqrt{L}$ and the step size $\\tau$ satisfies ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\tau=\\tilde{\\mathcal{O}}\\left(L^{-3/2}d^{-1/2}T^{-1/2}\\right)\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "then we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\chi^{2}(\\hat{\\pi}_{T}\\|\\pi_{T})\\lesssim L^{3/2}d\\tau^{2}T+\\epsilon_{\\mathrm{score}}^{2}L^{-1/2}T\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Proof. The main difference of this discretization analysis is whether the score $\\nabla\\log{p_{t}}$ can be exactly obtained or only be approximated by $\\scriptstyle{s_{\\theta}}$ . Therefore, in this proof, we will omit various steps the same as those shown in [1]. ", "page_idx": 51}, {"type": "text", "text": "We consider the following difference ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{l}{G_{T}:=\\cfrac{1}{\\sqrt{2\\gamma}}\\displaystyle\\sum_{s=0}^{S-1}\\int_{s\\tau}^{(s+1)\\tau}\\langle\\nabla g(z_{t})-s_{\\theta}(z_{s\\tau}),\\mathrm{d}B_{t}\\rangle}\\\\ {\\displaystyle\\qquad-\\,\\frac{1}{4\\gamma}\\sum_{s=0}^{S-1}\\int_{s\\tau}^{(s+1)\\tau}\\|\\nabla g(z_{t})-s_{\\theta}(z_{s\\tau})\\|^{2}\\,\\mathrm{d}t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "From Girsanov's theorem, we obtain immediately using Ito's formula ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathfrak{I}_{\\pi_{T}}\\left[\\left(\\frac{\\mathrm{d}\\hat{\\pi}_{T}}{\\mathrm{d}\\pi_{T}}\\right)^{2}\\right]-1=\\mathbb{E}\\left[\\exp\\left(2G_{T}\\right)\\right]-1=\\frac{1}{2\\gamma}\\mathbb{E}_{\\pi_{T}}\\underset{s=0}{\\overset{s-1}{\\sum}}\\left[\\int_{s\\tau}^{(s+1)\\tau}\\exp(2G_{t})\\left\\Vert\\nabla g(z_{t})-s_{\\theta}(z_{s\\tau})\\right\\Vert^{2}\\right.}\\\\ &{\\left.\\overset{\\le}\\frac{1}{\\gamma}\\cdot\\underset{s=0}{s\\overset{\\ldots}{\\sum}}\\int_{s\\tau}^{(s+1)\\tau}\\sqrt{\\mathbb{E}\\left[\\exp(4G_{t})\\right]\\cdot\\mathbb{E}\\left[\\|\\nabla g(z_{t})-s_{\\theta}(z_{s\\tau})\\|^{4}\\right]}\\mathrm{d}t}\\\\ &{\\le\\frac{4}{\\gamma}\\sum_{s=0}^{s-1}\\int_{s\\tau}^{(s+1)\\tau}\\sqrt{\\mathbb{E}\\left[\\exp(4G_{t})\\right]\\cdot\\mathbb{E}\\left[\\|\\nabla g(z_{t})-\\nabla g(z_{s\\tau})\\|^{4}\\right]}\\mathrm{d}t}\\\\ &{\\qquad+\\left.\\frac{4\\epsilon_{\\mathrm{sore}}^{2}}{\\gamma}\\sum_{s=0}^{s-1}\\int_{s\\tau}^{(s+1)\\tau}\\sqrt{\\mathbb{E}\\left[\\exp(4G_{t})\\right]}\\mathrm{d}t}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "According to Corollary 20 of [39], we have ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\exp(4G_{t})\\right]\\leq\\sqrt{\\mathbb{E}\\left[\\exp\\left(\\frac{16}{\\gamma}\\sum_{s=0}^{\\delta-1}\\int_{s\\tau}^{(s+1)\\tau\\wedge t}\\lVert\\nabla g(z_{r})-s_{\\theta}(z_{s\\tau})\\rVert^{2}\\,\\mathrm{d}r\\right)\\right]}}\\\\ &{\\qquad\\qquad\\leq\\sqrt{\\mathbb{E}\\exp\\left[\\frac{32}{\\gamma}\\cdot\\sum_{s=0}^{\\delta-1}\\left(\\int_{s\\tau}^{(s+1)\\tau\\wedge t}\\lVert\\nabla g(z_{r})-\\nabla g(z_{s\\tau})\\rVert^{2}\\,\\mathrm{d}r+\\int_{s\\tau}^{(s+1)\\tau\\wedge t}\\epsilon_{s\\mathrm{s}\\mathrm{s}\\tau}^{2}\\,\\mathrm{d}r\\right)\\right]}}\\\\ &{\\qquad\\qquad=\\exp\\left(\\frac{16t\\epsilon_{s\\mathrm{over}}^{2}}{\\gamma}\\right)\\cdot\\sqrt{\\mathbb{E}\\exp\\left[\\frac{32}{\\gamma}\\cdot\\sum_{s=0}^{\\delta-1}\\int_{s\\tau}^{(s+1)\\tau\\wedge t}\\lVert\\nabla g(z_{r})-\\nabla g(z_{s\\tau})\\rVert^{2}\\,\\mathrm{d}r\\right]}}\\\\ &{\\qquad\\qquad\\leq3\\cdot\\sqrt{\\mathbb{E}\\exp\\left[\\frac{32}{\\gamma}\\cdot\\sum_{s=0}^{\\delta-1}\\int_{s\\tau}^{(s+1)\\tau\\wedge t}\\lVert\\nabla g(z_{r})-\\nabla g(z_{s\\tau})\\rVert^{2}\\,\\mathrm{d}r\\right]},}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where the last inequality can be established by requiring ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\epsilon_{\\mathrm{score}}=\\mathcal{O}\\left(\\gamma^{1/2}T^{-1/2}\\right)\\quad\\Rightarrow\\quad\\frac{16t\\epsilon_{\\mathrm{score}}^{2}}{\\gamma}\\leq1\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "since $\\exp(u)\\leq1+2u$ for any $u\\in[0,1]$ ", "page_idx": 51}, {"type": "text", "text": "With similar techniques utilized in Lemma 4.8 of [1], we know that if ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\gamma\\asymp\\sqrt{3L},\\quad\\tau\\lesssim\\frac{\\gamma^{1/2}}{6L\\cdot d^{1/3}T^{1/2}(\\log S)^{1/2}},\\quad\\mathrm{and}\\quad T\\gtrsim\\frac{\\sqrt{3L}}{L}=\\sqrt{\\frac{3}{L}},\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "it holds that ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbb{E}\\exp\\left[\\frac{32}{\\gamma}\\cdot\\sum_{s=0}^{S-1}\\int_{s\\tau}^{(s+1)\\tau\\wedge t}\\|\\nabla g(z_{r})-\\nabla g(z_{s\\tau})\\|^{2}\\,\\mathrm{d}r\\right]\\leq\\exp\\left(\\mathcal{O}\\left(L^{3/2}d\\tau^{2}T\\log S\\right)\\right).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Furthermore, for ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\tau\\lesssim L^{-3/2}d^{-1/2}T^{-1/2}(\\log S)^{-1/2},\n$$", "text_format": "latex", "page_idx": 51}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\in[0,T]}\\,\\mathbb{E}\\left[\\exp(4G_{t})\\right]\\lesssim1.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Then, still with similar techniques utilized in Lemma 4.8 of [1], we have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sqrt{\\mathbb{E}\\left[\\left\\|\\nabla g(z_{t})-\\nabla g(z_{s\\tau})\\right\\|^{4}\\right]}\\leq(3L)^{2}\\sqrt{\\mathbb{E}\\left[\\left\\|z_{t}-z_{s\\tau}\\right\\|^{4}\\right]}\\lesssim L^{2}d\\tau^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "In summary, we have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi_{T}}\\left[\\left(\\frac{\\mathrm{d}\\hat{\\pi}_{T}}{\\mathrm{d}\\pi_{T}}\\right)^{2}\\right]-1\\lesssim\\frac{L^{2}d\\tau^{2}T}{\\gamma}+\\frac{\\epsilon_{\\mathrm{score}}^{2}T}{\\gamma},\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "and the proof is completed. ", "page_idx": 52}, {"type": "text", "text": "Corollary D.4. Under the same assumptions and hyperparameter setings made in Lemma D.3. If thestepsize $\\tau$ and the score estimation error Escore satisfies ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\tau=\\tilde{\\Theta}\\left(\\frac{\\epsilon}{L^{3/4}d^{1/2}T^{1/2}}\\right)\\quad\\mathrm{and}\\quad\\epsilon_{\\mathrm{score}}=\\mathcal{O}\\left(T^{-1/2}\\epsilon\\right)\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Then we have $\\chi^{2}(\\hat{\\pi}_{T}\\|\\pi_{T})\\lesssim\\epsilon^{2}$ ", "page_idx": 52}, {"type": "text", "text": "Proof. We can easily obtain this result by plugging the choice of $\\tau$ and $\\epsilon$ into Lemma D.3. Noted that we suppose $L\\geq1$ without loss of generality. \u53e3 ", "page_idx": 52}, {"type": "text", "text": "Theorem D.5 (Variant of Theorem 6 in [39]). Under Assumption [A1]-[A2], for any $\\epsilon\\in(0,1)$ ,we require Gaussian-type initialization and high-accurate score estimation, i.e., ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\hat{\\pi}_{0}=\\mathcal{N}(\\mathbf{0},e^{2\\eta}-1)\\otimes\\mathcal{N}(\\mathbf{0},I)\\quad\\mathrm{and}\\quad\\epsilon_{\\mathrm{score}}=\\tilde{\\mathcal{O}}(\\epsilon).\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "If weset thestepsize and the iteration number as ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\tau=\\tilde{\\Theta}\\left(\\epsilon d^{-1/2}L^{-1/2}\\cdot\\left(\\log\\left[\\frac{L(d+m_{2}^{2}+\\|\\pmb{x}_{0}\\|^{2})}{\\epsilon^{2}}\\right]\\right)^{-1/2}\\right)}\\\\ {\\displaystyle S=\\tilde{\\Theta}\\left(\\epsilon^{-1}d^{1/2}\\cdot\\left(\\log\\left[\\frac{L(d+m_{2}^{2}+\\|\\pmb{x}_{0}\\|^{2})}{\\epsilon^{2}}\\right]\\right)^{1/2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "the marginal distribution of output particles $\\hat{p}_{T}$ will satisfy $\\mathrm{KL}\\left(\\hat{p}_{T}\\lVert p^{\\leftarrow}(\\cdot|\\pmb{x}_{0})\\right)\\leq\\mathcal{O}(\\epsilon^{2}).$ ", "page_idx": 52}, {"type": "text", "text": "Proof. Consider the underlying distribution of the twisted coordinates $(\\phi,\\psi)$ for SDE. 85, the decomposition of the KL using Cauchy-Schwarz: ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}\\left(\\hat{\\pi}_{T}^{M}\\lVert\\pi^{M}\\right)=\\int\\log\\frac{\\hat{\\pi}_{T}^{M}}{\\pi^{M}}\\mathrm{d}\\hat{\\pi}_{T}^{M}=\\mathrm{KL}\\left(\\hat{\\pi}_{T}^{M}\\lVert\\pi_{T}^{M}\\right)+\\int\\log\\frac{\\pi_{T}^{M}}{\\pi^{M}}\\mathrm{d}\\hat{\\pi}_{T}^{M}}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\mathrm{KL}\\left(\\hat{\\pi}_{T}^{M}\\lVert\\pi_{T}^{M}\\right)+\\mathrm{KL}\\left(\\pi_{T}^{M}\\lVert\\pi^{M}\\right)+\\int\\log\\frac{\\pi_{T}^{M}}{\\pi^{M}}\\mathrm{d}(\\hat{\\pi}_{T}^{M}-\\pi_{T}^{M})}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\mathrm{KL}\\left(\\hat{\\pi}_{T}^{M}\\lVert\\pi_{T}^{M}\\right)+\\mathrm{KL}\\left(\\pi_{T}^{M}\\lVert\\pi^{M}\\right)+\\sqrt{\\chi^{2}\\left(\\hat{\\pi}_{T}^{M}\\lVert\\pi_{T}^{M}\\right)\\times\\mathrm{var}_{\\pi_{T}^{M}}\\left(\\log\\frac{\\pi_{T}^{M}}{\\pi^{M}}\\right)}_{\\mathrm{.}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Using LSI of the iterations via Lemma D.1, we have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\operatorname{var}_{\\pi_{T}^{M}}\\left(\\log\\frac{\\pi_{T}^{M}}{\\pi^{M}}\\right)\\leq C_{\\mathrm{LSI}}(\\pi_{T}^{M})\\cdot\\mathbb{E}_{\\pi_{T}^{M}}\\left[\\left\\|\\nabla\\log\\frac{\\pi_{T}^{M}}{\\pi^{M}}\\right\\|^{2}\\right]\\lesssim\\frac{1}{L}\\cdot\\mathbb{E}_{\\pi_{T}^{M}}\\left[\\left\\|\\nabla\\log\\frac{\\pi_{T}^{M}}{\\pi^{M}}\\right\\|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Then, we start to upper bound the relative Fisher information. Since $\\pi^{\\mathcal M}=\\mathcal M_{\\#}\\pi^{\\leftarrow}(\\cdot|\\pmb{x}_{0})$ , then ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\pi^{\\mathcal M}(\\phi,\\psi)\\propto\\pi^{\\leftarrow}(\\mathcal M^{-1}(\\phi,\\psi)|\\pmb{x}_{0}).\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Therefore, we have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\nabla\\log\\pi^{\\mathcal{M}}=(\\mathcal{M}^{-1})^{\\top}\\nabla\\log\\pi^{\\leftarrow}(\\cdot|\\pmb{x}_{0})\\circ\\mathcal{M}^{-1},\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "and similarly for $\\nabla\\log\\pi_{T}^{\\mathcal{M}}$ . This yields the expression ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi_{T}^{M}}\\left[\\left\\lVert\\nabla\\log\\frac{\\pi_{T}^{\\mathcal{M}}}{\\pi^{\\mathcal{M}}}\\right\\rVert^{2}\\right]=\\mathbb{E}_{\\pi_{T}}\\left[\\left\\lVert\\left(\\mathcal{M}^{-1}\\right)^{\\top}\\nabla\\log\\frac{\\pi_{T}}{\\pi^{\\leftarrow}}\\right\\rVert^{2}\\right].\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "According to the definition of $\\mathcal{M}$ , we have ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathcal{M}^{-1}(\\mathcal{M}^{-1})^{\\top}=\\left[\\!\\!\\begin{array}{c c}{1}&{-\\gamma/2}\\\\ {-\\gamma/2}&{\\gamma^{2}/2}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "For any $c_{0}>0$ and ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathfrak{M}:=\\left[\\!\\!\\begin{array}{c c}{\\frac{1}{12L}}&{\\frac{1}{\\sqrt{6L}}}\\\\ {\\frac{1}{\\sqrt{6L}}}&{4}\\end{array}\\!\\!\\right]\\otimes I_{d},\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "we have ", "page_idx": 53}, {"type": "equation", "text": "$$\nL9\\!\\!\\!\\!/-c_{0}.M^{-1}(\\mathcal{M}^{-1})^{\\top}=\\left[\\sqrt{3L}(1/\\sqrt{2}+c_{0}\\sqrt{2})\\!\\!\\!\\!/\\right.\\left.\\begin{array}{r}{{\\sqrt{3L}(1/\\sqrt{2}+c_{0}\\sqrt{2})}}\\\\ {{3L(4-c_{0})}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "The determinant is ", "page_idx": 53}, {"type": "equation", "text": "$$\n3L\\cdot\\left[\\left(\\frac{1}{4}-c_{0}\\right)\\cdot(4-c_{0})-\\left(\\frac{1}{\\sqrt{2}}+c_{0}\\sqrt{2}\\right)^{2}\\right]>0\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "for $c_{0}>0$ sufficiently small, which means that ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{M}^{-1}(\\mathcal{M}^{-1})^{\\top}\\preceq c_{0}^{-1}L\\mathfrak{M}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Therefore, Eq. 88 becomes ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi_{T}^{\\mathcal{M}}}\\left[\\left\\lVert\\nabla\\log\\frac{\\pi_{T}^{\\mathcal{M}}}{\\pi^{\\mathcal{M}}}\\right\\rVert^{2}\\right]\\lesssim3L\\cdot\\mathbb{E}_{\\pi_{T}}\\left[\\left\\lVert\\mathfrak{M}^{1/2}\\nabla\\log\\frac{\\pi_{T}}{\\pi^{\\leftarrow}}\\right\\rVert^{2}\\right].\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "According to Lemma D.2, the decay of the Fisher information requires us to set ", "page_idx": 53}, {"type": "equation", "text": "$$\nT\\gtrsim L^{-1/2}\\cdot\\log\\left[\\epsilon^{-2}\\cdot\\left(\\mathrm{KL}\\left(\\pi_{0}\\big|\\big|\\pi^{\\leftarrow}\\right)+\\mathbb{E}_{\\pi_{0}}\\left(\\left|\\left|\\mathfrak{M}^{1/2}\\nabla\\log\\frac{\\pi_{0}}{\\pi^{\\leftarrow}}\\right|\\right|^{2}\\right)\\right)\\right],\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "which yields $\\mathrm{KL}\\left(\\pi_{T}^{\\mathcal{M}}\\left\\|\\pi^{\\mathcal{M}}\\right)\\leq\\epsilon^{2}$ . Besides, we can easily have ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi_{0}}\\left(\\left\\lVert\\mathfrak{M}^{1/2}\\nabla\\log\\frac{\\pi_{0}}{\\pi^{\\leftarrow}}\\right\\rVert^{2}\\right)\\lesssim\\frac{1}{3L}\\cdot\\mathrm{FI}\\left(\\pi_{0}\\right\\lVert\\pi^{\\leftarrow}\\right)=\\frac{1}{3L}\\cdot\\mathbb{E}_{\\pi_{0}}\\left(\\left\\lVert\\nabla\\log\\frac{\\pi_{0}}{\\pi^{\\leftarrow}}\\right\\rVert^{2}\\right).\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "According to the definition of LSI, we also have ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathrm{KL}\\left(\\pi_{0}\\big\\|\\pi^{\\leftarrow}\\right)\\leq\\frac{C_{\\mathrm{LSI}}}{2}\\cdot\\mathrm{FI}\\left(\\pi_{0}\\big\\|\\pi^{\\leftarrow}\\right)=\\frac{1}{2L}\\cdot\\mathbb{E}_{\\pi_{0}}\\left(\\left\\|\\nabla\\log\\frac{\\pi_{0}}{\\pi^{\\leftarrow}}\\right\\|^{2}\\right).\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Recall as well that this requires $\\gamma\\,\\asymp\\,\\sqrt{3L}$ in SDE. 85. For the remaining $\\mathrm{KL}\\left(\\hat{\\pi}_{T}^{M}\\|\\pi_{T}^{M}\\right)$ and $\\chi^{2}\\left(\\hat{\\pi}_{T}^{M}||\\pi_{T}^{M}\\right)$ in Eq. 87, we invoke Lemma D.3 with the value $T\\,=\\,S\\tau$ specified and desired accuracy $\\epsilon$ , which consequently yields ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\tau=\\tilde{\\Theta}\\left(\\frac{\\epsilon}{L^{3/4}d^{1/2}T^{1/2}}\\right)\\quad\\mathrm{and}\\quad S=\\tilde{\\Theta}\\left(\\frac{T^{3/2}L^{3/4}d^{1/2}}{\\epsilon}\\right).\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Under this condition, we start to consider the initialization error. Suppose we have $\\pi_{0}=\\mathcal{N}(\\mathbf{0},e^{2\\eta}-$ $1)\\otimes\\mathcal{N}(\\mathbf{0},I)$ , which implies ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{FI}\\left(\\pi_{0}\\middle\\|\\pi^{\\leftarrow}\\right)\\lesssim\\!\\mathbb{E}_{\\pi_{0}}\\left[\\left\\Vert\\nabla f_{(K-k-1)\\eta}(\\mathbf{z})-\\nabla f_{(K-k-1)\\eta}(\\mathbf{0})+\\nabla f_{(K-k-1)\\eta}(\\mathbf{0})-\\frac{e^{-\\eta}x_{0}}{1-e^{-2\\eta}}\\right\\Vert^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq\\!3L^{2}\\mathbb{E}_{\\pi_{0}}[\\Vert\\mathbf{z}\\Vert^{2}]+3\\left\\Vert\\nabla f_{(K-k-1)\\eta}(\\mathbf{0})\\right\\Vert^{2}+\\frac{3e^{-2\\eta}}{(1-e^{-2\\eta})^{2}}\\cdot\\Vert x_{0}\\Vert^{2}}\\\\ &{\\qquad\\qquad=\\!3L^{2}\\cdot\\left(e^{2\\eta}-1\\right)+3\\left\\Vert\\nabla f_{(K-k-1)\\eta}(\\mathbf{0})\\right\\Vert^{2}+\\frac{3e^{-2\\eta}}{(1-e^{-2\\eta})^{2}}\\cdot\\Vert x_{0}\\Vert^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Following the $\\eta$ setting, i.e., ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\eta=\\frac{1}{2}\\log\\frac{2L+1}{2L}\\quad\\Leftrightarrow\\quad e^{2\\eta}=\\frac{2L+1}{2L},\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "which yields ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\mathrm{FI}\\left(\\pi_{0}\\|\\pi^{\\leftarrow}\\right)\\lesssim\\!L+\\left\\|\\nabla f_{(K-k-1)\\eta}(\\mathbf{0})\\right\\|^{2}+L^{2}\\|\\pmb{x}_{0}\\|^{2}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "equation", "text": "$$\n\\lesssim\\!L+L^{2}(d+m_{2}^{2})+L^{2}\\|\\pmb{x}_{0}\\|^{2}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where the inequality follows from Eq. 73. Therefore, combining Eq. 91, Eq. 90 and Eq. 89, we have ", "page_idx": 54}, {"type": "equation", "text": "$$\nT^{1/2}\\gtrsim L^{-1/4}\\cdot\\left(\\log\\left[\\frac{L(d+m_{2}^{2}+\\|x_{0}\\|^{2})}{\\epsilon^{2}}\\right]\\right)^{1/2}\\gtrsim L^{1/4}\\cdot\\left(\\log\\left[\\frac{\\mathbb{E}_{\\pi_{0}}\\left(\\left\\|\\nabla\\log\\frac{\\pi_{0}}{\\pi^{\\epsilon}}\\right\\|^{2}\\right)}{L\\epsilon^{2}}\\right]\\right)^{1/2},\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "which implies ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\tau=\\tilde{\\Theta}\\left(\\epsilon d^{-1/2}L^{-1/2}\\cdot\\left(\\log\\left[\\frac{L(d+m_{2}^{2}+\\|{\\pmb x}_{0}\\|^{2})}{\\epsilon^{2}}\\right]\\right)^{-1/2}\\right)}\\\\ {\\displaystyle S=\\tilde{\\Theta}\\left(\\epsilon^{-1}d^{1/2}\\cdot\\left(\\log\\left[\\frac{L(d+m_{2}^{2}+\\|{\\pmb x}_{0}\\|^{2})}{\\epsilon^{2}}\\right]\\right)^{1/2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "In this condition, the score estimation error is required to be ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\epsilon_{\\mathrm{score}}=\\mathcal{O}\\left(\\gamma^{1/2}T^{-1/2}\\cdot\\epsilon\\right)=\\tilde{\\mathcal{O}}\\left(\\epsilon/\\sqrt{L}\\right).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Hence, the proof is completed. ", "page_idx": 54}, {"type": "text", "text": "Theorem D.6. Under Assumption [A1]-[A2], for Alg. 1, we choose ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\eta=\\frac{1}{2}\\log\\frac{2L+1}{2L}\\quad\\mathrm{and}\\quad K=4L\\cdot\\log\\frac{(1+L^{2})d+\\left\\Vert\\nabla f_{*}(\\mathbf{0})\\right\\Vert^{2}}{\\epsilon^{2}}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "andimplementStep $^3$ of Alg. 1 with projected Alg. 3. For the $k$ -th run of Alg. 3, we require Gaussian-type initialization and high-accurate score estimation, i.e., ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\hat{\\pi}_{0}=\\mathcal{N}(\\mathbf{0},e^{2\\eta}-1)\\otimes\\mathcal{N}(\\mathbf{0},I)\\quad\\mathrm{and}\\quad\\epsilon_{\\mathrm{score}}=\\tilde{\\mathcal{O}}(\\epsilon).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "If we set the hyperparameters as shown in Lemma $D.5_{i}$ it can achieve $\\mathrm{TV}\\left(\\hat{p}_{K\\eta},p_{*}\\right)\\lesssim\\epsilon$ with an $\\tilde{\\mathcal{O}}\\left(L^{2}d^{1/2}\\epsilon^{-1}\\right)$ gradient complexity. ", "page_idx": 54}, {"type": "text", "text": "Proof. According to Corollary B.5, we know that under the choice ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\eta=\\frac{1}{2}\\ln\\frac{2L+1}{2L},\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "it requires to run Alg. 3 for $K$ times where ", "page_idx": 54}, {"type": "equation", "text": "$$\nK=4L\\cdot\\log\\frac{(1+L^{2})d+\\left\\|\\nabla f_{*}(\\mathbf{0})\\right\\|^{2}}{\\epsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "For each run of Alg. 3, we require the KL divergence error to achieve ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\mathrm{KL}\\left(\\hat{p}_{(k+1)\\eta\\vert k\\eta}(\\cdot\\vert\\hat{x})\\vert\\vert p_{(k+1)\\eta\\vert k\\eta}^{\\leftarrow}(\\cdot\\vert\\hat{x})\\right)\\le\\frac{\\epsilon^{2}}{4L}\\cdot\\left[\\log\\frac{(1+L^{2})d+\\Vert\\nabla f_{\\ast}(\\mathbf{0})\\Vert^{2}}{\\epsilon^{2}}\\right]^{-1}.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Combining with Theorem D.5, we consider a step size ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\tau_{k}=\\tilde{\\mathcal{O}}\\left(L^{-1}d^{-1/2}\\boldsymbol{\\epsilon}\\cdot(\\log\\left[L^{2}\\cdot(d+m_{2}^{2}+\\|\\hat{\\mathbf{x}}_{k}\\|^{2})\\right])^{-1/2}\\right)\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "then the iteration number will be ", "page_idx": 54}, {"type": "equation", "text": "$$\nS_{k}=\\tilde{\\mathcal{O}}\\left(L^{1/2}d^{1/2}\\epsilon^{-1}\\cdot(\\log\\left[L^{2}\\cdot(d+m_{2}^{2}+\\|\\hat{{\\pmb x}}_{k}\\|^{2})\\right])^{1/2}\\right).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "For an expectation perspective, we have ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\hat{p}_{k\\eta}}\\left[\\log(L^{2}\\|\\hat{\\mathbf{x}}_{k}\\|^{2})\\right]\\leq\\log\\left[\\mathbb{E}_{\\hat{p}_{k\\eta}}(\\|\\hat{\\mathbf{x}}_{k}\\|^{2})\\right]=\\tilde{\\mathcal{O}}(L)\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where the last inequality follows from Lemma B.6. This means that with the total gradient complexity ", "page_idx": 54}, {"type": "equation", "text": "$$\nK\\cdot S=\\tilde{\\mathcal{O}}\\left(L^{2}d^{1/2}\\epsilon^{-1}\\right)\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Hence, the proof is completed. ", "page_idx": 54}, {"type": "text", "text": "E Auxiliary Lemmas ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Lemma E.1 (Theorem 4 in [34]). Suppose $p\\propto\\exp(-f)$ defined on $\\mathbb{R}^{d}$ satisfies LSI with constant $\\mu>0$ . Along the Langevin dynamics, i.e., ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}_{t}=-\\nabla f(\\mathbf{x})\\mathrm{d}t+\\sqrt{2}\\mathrm{d}B_{t},\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where $\\mathbf{x}_{t}\\sim p_{t}$ ,then it has ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{KL}\\left(p_{t}\\|p\\right)\\leq\\exp\\left(-2\\mu t\\right)\\cdot\\mathrm{KL}\\left(p_{0}\\|p\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Lemma E.2. Suppose $p\\propto\\exp(-f)$ defined on $\\mathbb{R}^{d}$ satisfies $L S I$ with constant $\\mu>0$ where $f$ is $L$ -smooth, i.e., ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\|\\nabla f(\\pmb{x}^{\\prime})-\\nabla f(\\pmb{x})\\|\\leq L\\,\\|\\pmb{x}^{\\prime}-\\pmb{x}\\|\\,.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "f $p_{0}$ is the standard Gaussian distribution defined on $\\mathbb{R}^{d}$ ,thenwe have ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\mathrm{KL}\\left(p_{0}\\|p\\right)\\leq\\frac{(1+2L^{2})d+2\\left\\|\\nabla f(\\mathbf{0})\\right\\|^{2}}{\\mu}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Proof. According to the definition of LSI, we have ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathrm{KL}\\left(p_{0}\\|p\\right)\\leq\\frac{1}{2\\mu}\\int p_{0}(\\mathbf{x})\\left\\|\\nabla\\log\\frac{p_{0}(\\mathbf{x})}{p(\\mathbf{x})}\\right\\|^{2}\\mathrm{d}\\mathbf{x}=\\frac{1}{2\\mu}\\int p_{0}(\\mathbf{x})\\left\\|-\\mathbf{x}+\\nabla f(\\mathbf{x})\\right\\|^{2}\\mathrm{d}\\mathbf{x}}\\\\ {\\displaystyle\\leq\\mu^{-1}\\cdot\\left[\\int p_{0}(\\mathbf{x})\\|\\mathbf{x}\\|^{2}\\mathrm{d}\\mathbf{x}+\\int p_{0}(\\mathbf{x})\\|\\nabla f(\\mathbf{x})-\\nabla f(\\mathbf{0})+\\nabla f(\\mathbf{0})\\|^{2}\\mathrm{d}\\mathbf{x}\\right]}\\\\ {\\displaystyle\\leq\\mu^{-1}\\cdot\\left[(1+2L^{2})\\int p_{0}(\\mathbf{x})\\|\\mathbf{x}\\|^{2}\\mathrm{d}\\mathbf{x}+2\\left\\|\\nabla f(\\mathbf{0})\\right\\|^{2}\\right]}\\\\ {\\displaystyle=\\frac{(1+2L^{2})d+2\\left\\|\\nabla f(\\mathbf{0})\\right\\|^{2}}{\\mu}}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where the third inequality follows from the $L$ -smoothness of $f_{*}$ and the last equation establishes since $\\mathbb{E}_{p_{0}}[||\\pmb{x}||^{2}]=\\bar{d}$ is for the standard Gaussian distribution $p_{0}$ in $\\mathbb{R}^{d}$ \u53e3 ", "page_idx": 55}, {"type": "text", "text": "Lemma E.3 (Variant of Lemma B.1 in [40]). Suppose $f\\colon\\ensuremath{\\mathbb{R}}^{d}\\to\\ensuremath{\\mathbb{R}}$ isa $m$ -strongly convex function andsatisfies $L$ -smooth.Then,wehave ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\nabla f(\\mathbf{x})\\cdot\\mathbf{x}\\geq\\frac{m\\left\\Vert x\\right\\Vert^{2}}{2}-\\frac{\\left\\Vert\\nabla f(\\mathbf{0})\\right\\Vert^{2}}{2m}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where $\\pmb{x}_{*}$ is the global optimum of the function $f$ ", "page_idx": 55}, {"type": "text", "text": "Proof. According to the definition of strongly convex, the function $f$ satisfies ", "page_idx": 55}, {"type": "equation", "text": "$$\nf(\\mathbf{0})-f(\\mathbf{\\boldsymbol{x}})\\geq\\nabla f(\\mathbf{\\boldsymbol{x}})\\cdot(\\mathbf{0}-\\mathbf{\\boldsymbol{x}})+\\frac{m}{2}\\cdot\\left\\|\\mathbf{\\boldsymbol{x}}\\right\\|^{2}\\;\\Leftrightarrow\\;\\nabla f(\\mathbf{\\boldsymbol{x}})\\cdot\\mathbf{\\boldsymbol{x}}\\geq f(\\mathbf{\\boldsymbol{x}})-f(\\mathbf{0})+\\frac{m}{2}\\cdot\\left\\|\\mathbf{\\boldsymbol{x}}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Besides, we have ", "page_idx": 55}, {"type": "equation", "text": "$$\nf(\\mathbf{x})-f(\\mathbf{0})\\geq\\nabla f(\\mathbf{0})\\cdot\\mathbf{x}+{\\frac{m}{2}}\\cdot\\|\\mathbf{x}\\|^{2}\\geq{\\frac{m}{2}}\\cdot\\|\\mathbf{x}\\|^{2}-{\\frac{m}{2}}\\cdot\\|\\mathbf{x}\\|^{2}-{\\frac{\\|\\nabla f(\\mathbf{0})\\|^{2}}{2m}}=-{\\frac{\\|\\nabla f(\\mathbf{0})\\|^{2}}{2m}}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Combining the above two inequalities, the proof is completed. ", "page_idx": 55}, {"type": "text", "text": "Lemma E.4 (Lemma A.1 in [40]). Suppose a function $f$ satisfy ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\nabla f(\\mathbf{x})\\cdot\\mathbf{x}\\geq\\frac{m\\|\\mathbf{x}\\|^{2}}{2}-\\frac{\\|\\nabla f(\\mathbf{0})\\|}{2m},\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "then we have ", "page_idx": 55}, {"type": "equation", "text": "$$\nf(\\ensuremath{\\mathbf{{x}}})\\geq\\frac{m}{8}\\|\\ensuremath{\\mathbf{{|}}}\\ensuremath{\\mathbf{{|}}}\\|^{2}+f(\\ensuremath{\\mathbf{{x}}}_{\\ast})-\\frac{\\|\\nabla f(\\ensuremath{\\mathbf{{0}}})\\|^{2}}{4m}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Lemma E.5 (Lemma 1 in [17]). Consider the Ornstein-Uhlenbeck forward process ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{x}_{t}=-\\mathbf{x}_{t}\\mathrm{d}t+\\sqrt{2}\\mathrm{d}B_{t},\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "and denote the underlying distribution of the particle $\\mathbf{x}_{t}$ as $p_{t}$ .Then, the score function can be rewrittenas ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\nabla_{x}\\ln{p_{t}(x)}=\\mathbb{E}_{{\\pmb{x}_{0}}\\sim q_{t}(\\cdot|x)}\\frac{e^{-t}{\\pmb{x}_{0}}-{\\pmb{x}}}{\\left(1-e^{-2t}\\right)}\\mathrm{,}}}\\\\ {{q_{t}({\\pmb{x}_{0}}|x)\\propto\\exp\\left(-f_{*}({\\pmb{x}_{0}})-\\frac{\\left\\|{\\pmb{x}}-e^{-t}{\\pmb{x}_{0}}\\right\\|^{2}}{2\\left(1-e^{-2t}\\right)}\\right)\\mathrm{.}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Lemma E.6 (Lemma 11 in [34]). Assume $p\\propto\\exp(-f)$ and the energy function $f$ is $L$ -smooth. Then ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{x}\\sim p}\\left[\\left\\|\\nabla f(\\mathbf{x})\\right\\|^{2}\\right]\\leq L d\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Lemma E.7 (Lemma 10 in [8]). Suppose that Assumption [A1]-[A2] hold. Let $\\{\\mathbf{x}_{t}\\}_{t\\in[0,T]}$ denote the forward process, i.e., Eq. 1, for all $t\\geq0$ ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|\\mathbf{x}\\right\\|^{2}\\right]\\leq\\operatorname*{max}\\left\\{d,m_{2}^{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Lemma E.8. Suppose $q$ is a distribution which satisfies $L S I$ with constant $\\mu,$ then its variance satisfies ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\int q(\\pmb{x})\\left\\|\\pmb{x}-\\mathbb{E}_{\\tilde{q}}\\left[\\mathbf{x}\\right]\\right\\|^{2}\\mathrm{d}\\pmb{x}\\leq\\frac{d}{\\mu}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Proof. It is known that LSI implies Poincare inequality with the same constant, i.e., $\\mu$ ,which means if for all smooth function $g\\colon\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\operatorname{var}_{q}\\left(g(\\mathbf{x})\\right)\\leq\\frac{1}{\\mu}\\mathbb{E}_{q}\\left[\\|\\nabla g(\\mathbf{x})\\|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "In this condition, we suppose $\\pmb{b}=\\mathbb{E}_{q}[\\mathbf{x}]$ , and have the following equation ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int q(\\boldsymbol{x})\\left\\|\\boldsymbol{x}-\\mathbb{E}_{q}\\left[\\mathbf{x}\\right]\\right\\|^{2}\\mathrm{d}\\boldsymbol{x}=\\displaystyle\\int q(\\boldsymbol{x})\\left\\|\\boldsymbol{x}-\\boldsymbol{b}\\right\\|^{2}\\mathrm{d}\\boldsymbol{x}}\\\\ {\\displaystyle=\\int\\sum_{i=1}^{d}q(\\boldsymbol{x})\\left(\\boldsymbol{x}_{i}-\\boldsymbol{b}_{i}\\right)^{2}\\mathrm{d}\\boldsymbol{x}=\\sum_{i=1}^{d}\\int q(\\boldsymbol{x})\\left(\\left\\langle\\boldsymbol{x},\\boldsymbol{e}_{i}\\right\\rangle-\\left\\langle\\boldsymbol{b},\\boldsymbol{e}_{i}\\right\\rangle\\right)^{2}\\mathrm{d}\\boldsymbol{x}}\\\\ {\\displaystyle=\\sum_{i=1}^{d}\\int q(\\boldsymbol{x})\\left(\\left\\langle\\boldsymbol{x},\\boldsymbol{e}_{i}\\right\\rangle-\\mathbb{E}_{q}\\left[\\left\\langle\\mathbf{x},\\boldsymbol{e}_{i}\\right\\rangle\\right]\\right)^{2}\\mathrm{d}\\boldsymbol{x}=\\sum_{i=1}^{d}\\mathrm{var}_{q}\\left(g_{i}(\\mathbf{x})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where $g_{i}(x)$ is defined as $g_{i}(\\pmb{x}):=\\langle\\pmb{x},\\pmb{e}_{i}\\rangle$ and $e_{i}$ is a one-hot vector ( the $i$ -th element of $e_{i}$ is 1 others are O). Combining this equation and Poincare inequality, for each $i$ , we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\operatorname{var}_{q}\\left(g_{i}(\\mathbf{x})\\right)\\leq\\frac{1}{\\mu}\\mathbb{E}_{q}\\left[\\left\\|e_{i}\\right\\|^{2}\\right]=\\frac{1}{\\mu}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Hence, the proof is completed. ", "page_idx": 56}, {"type": "text", "text": "Lemma E.9 (Variant of Lemma 10 in [11]). Suppose $-\\log{p_{*}}$ is $m$ -strongly convex function, for any distribution with density function $p$ wehave ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\mathrm{KL}\\left(p\\Vert p_{*}\\right)\\leq\\frac{1}{2m}\\int p(\\pmb{x})\\left\\Vert\\nabla\\log\\frac{p(\\pmb{x})}{p_{*}(\\pmb{x})}\\right\\Vert^{2}\\mathrm{d}\\pmb{x}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "By choosing $p({\\pmb x})=g^{2}({\\pmb x})p_{*}({\\pmb x})/\\mathbb{E}_{p_{*}}\\left[g^{2}({\\bf x})\\right]$ for the test function $g\\colon\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ and $\\mathbb{E}_{p_{*}}\\left[g^{2}(\\mathbf{x})\\right]<$ $\\infty$ ,we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\mathbb{E}_{p_{*}}\\left[g^{2}\\log g^{2}\\right]-\\mathbb{E}_{p_{*}}\\left[g^{2}\\right]\\log\\mathbb{E}_{p_{*}}\\left[g^{2}\\right]\\le\\frac{2}{m}\\mathbb{E}_{p_{*}}\\left[\\left\\|\\nabla g\\right\\|^{2}\\right],\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "which implies $p_{*}$ satisfies $1/m$ -log-Sobolev inequality. ", "page_idx": 56}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 57}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 57}, {"type": "text", "text": "\u00b7 You should answer [Yes] , [No] , or [NA] .   \n\u00b7 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u00b7 Please provide a short (1-2 sentence) justification right after your answer (even for NA). ", "page_idx": 57}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 57}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 57}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 57}, {"type": "text", "text": "\u00b7 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u00b7 Keep the checklist subsection headings, questions/answers and guidelines below. \u00b7 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 57}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Justification: We have clear claim that we improve the diffusion inference by RTK framework. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 57}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 57}, {"type": "text", "text": "Justification: We discussed the limitation in section 5. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 58}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Justification: See Section 4 A1, A2, E1, E2, E3 for Assumptions. Proof are provided in the Appendix. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 58}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: See our Appendix. Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 59}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 59}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Justification: We have detailed information in the Appendix to reproduce the emprical results. ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https : //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 59}, {"type": "text", "text": "\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 60}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: See the Appendix for more information. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 60}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 60}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 60}, {"type": "text", "text": "Justification: We just have illustrative empirical result about the trend and generated samples.   \nNot claims for accuracy or error from the empiral side. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 60}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: See appendix. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: \u00b7 The answer NA means that the paper does not include experiments. ", "page_idx": 60}, {"type": "text", "text": "\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 61}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] Justification: Conducted ", "page_idx": 61}, {"type": "text", "text": "", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 61}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 61}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 61}, {"type": "text", "text": "Justification: It is a theoretical paper. No societal impact is visible in a short term. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 61}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 61}, {"type": "text", "text": "Answer: [NA] Justification: No real data in this paper. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 62}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 62}, {"type": "text", "text": "Justification: No real data are used. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 62}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 62}, {"type": "text", "text": "Justification: Only synthetic data are used with detailed instructions. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 62}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 63}, {"type": "text", "text": "Justification: NA. ", "page_idx": 63}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 63}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 63}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 63}, {"type": "text", "text": "Justification: NA. Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 63}]