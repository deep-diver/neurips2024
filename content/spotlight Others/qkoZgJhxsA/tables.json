[{"figure_path": "qkoZgJhxsA/tables/tables_7_1.jpg", "caption": "Table 1: Teaching performances. For all metrics, the higher value denotes the better performance. The best methods are highlighted in bold. The runner-up baselines are represented by underline.", "description": "This table presents a comparison of various LLMs' performance on five pedagogical dimensions: Overall Quality, Incorrect Answer Recognition Accuracy (IARA), Correct Answer Recognition Accuracy (CARA), Successful Explanation Rate (SER), and Successful Rejection Rate (SRR).  It also includes BLEU and ROUGE scores, which are standard metrics for evaluating language generation quality.  The performance of SocraticLM (the authors' model) is compared to several baselines (ChatGPT, GPT4, Vicuna-7b, Llama2-7b, Llama2-13b, Llama3-8b, ChatGLM3-6b, and EduChat-32b). Ablation studies are also included in the table, showing the effect of removing different components of the SocraticLM training data on its overall performance.", "section": "6 Main Results"}, {"figure_path": "qkoZgJhxsA/tables/tables_8_1.jpg", "caption": "Table 2: Performance without problem-solving data and three ability-balancing training strategies in Section 4. ACCG, ACCM represent the accuracy on GSM8K and MAWPS, respectively.", "description": "This table presents the results of an ablation study on the SocraticLM model.  It shows the impact of removing the problem-solving data and each of the three training strategies (Separate Training, Instruction Tuning, Mixed Prompt Setting) on the overall teaching performance (Overall) and problem-solving accuracy on two benchmark datasets (GSM8K and MAWPS).  The results demonstrate the contribution of each component to the overall performance of the model.", "section": "6.2 Ablation Study"}, {"figure_path": "qkoZgJhxsA/tables/tables_17_1.jpg", "caption": "Table 1: Teaching performances. For all metrics, the higher value denotes the better performance. The best methods are highlighted in bold. The runner-up baselines are represented by underline.", "description": "This table presents the performance comparison of several language models (LLMs) on five pedagogical dimensions: Overall Quality, Incorrect Answer Recognition Accuracy (IARA), Correct Answer Recognition Accuracy (CARA), Successful Explanation Rate (SER), and Successful Rejection Rate (SRR).  The models are evaluated on their ability to engage in Socratic-style teaching.  Higher scores indicate better performance across the pedagogical dimensions. SocraticLM, the model proposed in the paper, achieves significantly better results than other models including GPT4 on most metrics.", "section": "6.1 Main Results"}]