[{"type": "text", "text": "Peri-midFormer: Periodic Pyramid Transformer for Time Series Analysis ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qiang Wu Gechang Yao Zhixi Feng\u2020 Shuyuan Yang ", "page_idx": 0}, {"type": "text", "text": "Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, China {wu_qiang, yao_gechang}@stu.xidian.edu.cn, {zxfeng, syyang}@xidian.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Time series analysis finds wide applications in fields such as weather forecasting, anomaly detection, and behavior recognition. Previous methods attempted to model temporal variations directly using 1D time series. However, this has been quite challenging due to the discrete nature of data points in time series and the complexity of periodic variation. In terms of periodicity, taking weather and traffic data as an example, there are multi-periodic variations such as yearly, monthly, weekly, and daily, etc. In order to break through the limitations of the previous methods, we decouple the implied complex periodic variations into inclusion and overlap relationships among different level periodic components based on the observation of the multi-periodicity therein and its inclusion relationships. This explicitly represents the naturally occurring pyramid-like properties in time series, where the top level is the original time series and lower levels consist of periodic components with gradually shorter periods, which we call the periodic pyramid. To further extract complex temporal variations, we introduce self-attention mechanism into the periodic pyramid, capturing complex periodic relationships by computing attention between periodic components based on their inclusion, overlap, and adjacency relationships. Our proposed Peri-midFormer demonstrates outstanding performance in five mainstream time series analysis tasks, including short- and long-term forecasting, imputation, classification, and anomaly detection. The code is available at https://github.com/WuQiangXDU/Peri-midFormer. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Time series analysis stands as a foundational challenge pivotal across diverse real-world scenarios [1], such as weather forecasting [2], imputation of missing data within offshore wind speed time series [3], anomaly detection for industrial maintenance [4], and classification [5]. Due to its substantial practical utility, time series analysis has garnered considerable interest, leading to the development of a large number of deep learning-based methods for it. ", "page_idx": 0}, {"type": "text", "text": "Different from other forms of sequential data, like language or video, time series data is continuously recorded, capturing scalar values at each time point. Furthermore, real-world time series variations often entail complex temporal patterns, where multiple fluctuations (e.g., ascents, descents, fluctuations, etc.) intermingle and intertwine, particularly salient is the presence of various overlapping periodic components in it, rendering the modeling of temporal variations exceptionally challenging. ", "page_idx": 0}, {"type": "text", "text": "Deep learning models, known for their powerful non-linear capabilities, capture intricate temporal variations in real-world time series. Recurrent neural networks (RNNs) leverage sequential data, allowing past information to influence future predictions [6, 7]. However, they face challenges with long-term dependencies and computational inefficiency due to their sequential nature. Temporal convolutional neural networks (TCNs) [8, 9] extract variation information but struggle with capturing long-term dependencies. Transformers with attention mechanisms have gained popularity for sequential modeling [10, 11], capturing pairwise temporal dependencies among time points. Yet, discerning reliable dependencies directly from scattered time points remains challenging [12]. Timesnet [13] innovatively transforms 1D time series into 2D tensors, unifying intra- and inter-period variations in 2D space. However, it overlooks inclusion relationships between periods of different scales and is constrained by limited feature extraction capability of CNNs, hindering its ability to explore complex relationships within time series. ", "page_idx": 0}, {"type": "image", "img_path": "5iUxMVJVEV/tmp/48d2c3b01b8dd7a98827830b3e92a960d75bd293e274f0ecfe8af4e6501ace7c.jpg", "img_caption": ["Figure 1: Multi-periodicity, inclusion of periodic components, and Periodic Pyramid. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We analyze time series by examining the inclusion and overlap (hereinafter collectively referred to as inclusion) relationships between various periodic components to address complex temporal variations. Real-world time series often show multiple periodicities, like yearly and daily weather variations or weekly and daily traffic fluctuations. And these periods exhibit clear inclusion relationships, for instance, yearly weather variations encompass multiple daily weather variations. Besides variations between different period levels, it also occur within periods of the same level. For example, daily weather variations differ based on conditions like sunny or cloudy. Due to these inclusion and adjacency relationships, different periods show similarities, with short and long periods being consistent in overlapping portions, and periods of the same level being similar. Additionally, a long period can be decomposed into multiple short ones, forming a hierarchical pyramid structure. In our study, time series without explicit periodicity are treated as having infinitely long periods. ", "page_idx": 1}, {"type": "text", "text": "Based on the above analysis, we decompose the time series into multiple periodic components, forming a pyramid structure where longer components encompass shorter ones, termed the Periodic Pyramid as shown in Figure 1, which illustrates the intricate periodic inclusion relationships within the time series. Each level consists of components with the same period, exhibiting the same phase, while different levels contain components with inclusion relationships. This transformation converts the original 1D time series into a 2D representation, explicitly showing the implicit multi-period relationships. Within the Periodic Pyramid, a shorter period may belong to two longer periods simultaneously, reflecting the complexity of the time series. There is a clear similarity between components within the same level and those in adjacent levels where inclusion relationships exist. Thus inspired by Pyraformer [10], we propose the Periodic Pyramid Transformer (Peri-midFormer), which computes self-attention among periodic components to capture complex temporal variations in time series. Furthermore, we consider each branch in the Periodic Pyramid as a Periodic Feature Flow, and aggregating features from multiple flows to provide rich periodic information for downstream tasks. In experiments, Peri-midFormer achieves state-of-the-art performance in various analytic tasks, including forecasting, imputation, anomaly detection, and classification. ", "page_idx": 1}, {"type": "text", "text": "1. Based on the inclusion relationships of multiple periods in time series, this paper proposes a top-down constructed Periodic Pyramid structure, which expands 1D time series variations into 2D, explicitly representing the implicit multi-period relationships within the time series.   \n2. We propose Peri-midFormer, which uses the Periodic Pyramid Attention Mechanism to automatically capture dependencies between different and same-level periodic components, extracting diverse temporal variations in time series. Additionally, to further harness the potential of Peri-midFormer, we introduce Periodic Feature Flows to provide rich periodic information for downstream tasks.   \n3. We conduct extensive experiments on five mainstream time series analysis tasks, and PerimidFormer achieves state-of-the-art across all of them, demonstrating its superior capability in time series analysis. ", "page_idx": 1}, {"type": "text", "text": "The remainder of this paper is structured as follows. Section 2 briefly summarizes the related work. Section 3 details the proposed model structure. Section 4 extensively evaluates our method\u2019s performance across five main time series analysis tasks. Section 5 presents ablations analysis, Section 6 presents complexity analysis, and Section 7 discusses our results and future directions. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Temporal variation modeling, a crucial aspect of time series analysis, has been extensively investigated. In recent years, numerous deep models have emerged for this purpose, including MLP [14, 15], TCN [8], and RNN [6, 7]-based architectures. Furthermore, Transformers have shown remarkable performance in time series forecasting [16, 12, 17, 18]. They utilize attention mechanisms to uncover temporal dependencies among time points. For instance, Wu et al. [12] introduce Autoformer with an Auto-Correlation mechanism, adept at capturing series-wise temporal dependencies derived from learned periods. Moreover, to address complex temporal patterns, they adopted a deep decomposition architecture to extract seasonal and trend parts from input series. Subsequently, FEDformer [17] enhances seasonal-trend decomposition through a mixture-of-expert design and introduces sparse attention within the frequency domain. Pyraformer [10] constructs a down-top pyramid structure through multiple convolution operations on time series to address the issue of long information propagation paths in Transformers, significantly reducing both time and space complexity. PatchTST [19] partitions individual data points into patches and uses them as tokens for the Transformer, thereby enhancing its understanding of local information in time series. Additionally, PatchTST innovatively processes each channel separately, making it particularly suitable for forecasting tasks. ", "page_idx": 2}, {"type": "text", "text": "Additionally, there are some recent innovative works. Timesnet [13] unravels intricate temporal patterns by exploring the multi-periodicity of time series and captures temporal 2D-variations using computer vision CNN backbones. GPT4TS [20] ingeniously utilizes the large language model GPT2 as a pretrained model, fine-tuning some of its structures with time series, achieving state-of-the-art results. FITS [21] proposes a time series analysis model based on frequency domain operations, requiring very low parameter count and memory consumption. And recent works have considered multi-scale information in time series. PDF [22] captures both short-term and long-term variations by transforming 1D time series into 2D tensors using a multi-periodic decoupling block. It achieves superior forecasting performance by modeling these decoupled variations and integrating them for accurate predictions. SCNN [23] decomposes multivariate time series into long-term, seasonal, shortterm, co-evolving, and residual components, enhancing interpretability, adaptability to distribution shifts, and scalability by modeling each component separately. TimeMixer [24] uses a novel multiscale mixing approach, decomposing time series into fine and coarse scales to capture both detailed and macroscopic variations. It employs Past-Decomposable-Mixing to extract historical information and Future-Multipredictor-Mixing to leverage multiscale forecasting capabilities, achieving great performance in forecasting task. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Model Structure ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The overall flowchart of the proposed approach is shown in Figure 2, it begins with time embedding of the original time series at the top. Then, we use the FFT to decompose it into multiple periodic components of varying lengths across different levels, with lines indicating the inclusion relationships between them. Moving down, padding and projection are then applied to ensure uniform dimensions, forming the Periodic Pyramid. Each component is treated as an independent token and receives positional embedding. Next, the Periodic Pyramid is fed into Peri-midFormer, which consists of multiple layers for computing Periodic Pyramid Attention. Finally, depending on the task, two strategies are employed: for classification, components are directly concatenated and projected into the category space; for other reconstruction tasks (since forecasting, imputation, and anomaly detection all necessitate the model to reconstruct the channel dimensions or input lengths, we collectively refer to such tasks as reconstruction tasks), features from different pyramid branches are integrated through Periodic Feature Flows Aggregation to generate the final output. Please note that we referred to [11] for de-normalization and [12] for time series decomposition to maximize the effectiveness of our method, but we omitted these details from the figure to maintain simplicity. See Appendix A for a complete flowchart. Further details are provided below. ", "page_idx": 2}, {"type": "text", "text": "3.2 Periodic Pyramid Construction ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Multiple periods in the time series exhibit clear inclusion relationships, however, the 1D structure limits the representation of variations between them. Hence, it\u2019s crucial to separate periodic components with inclusion relationships to explicitly represent implicit periodic relationships. Firstly, as Peri-midFormer is designed to focus on periodic components, we first normalize the original time series $\\mathbf{X}\\in\\mathbb{R}^{L\\times C}$ that with length $L$ and $C$ channels, then decompose it to obtain the seasonal part $\\mathbf{X}_{s}\\,\\in\\,\\mathbb{R}^{L\\times C}$ , thus removing the interference of the trend part. For a detailed description of normalization and decomposition, please refer to the Appendix A. Then we partition $\\mathbf{X}_{s}$ into periodic components, following the approach used in Timesnet [13]. It\u2019s important to note that, inspired by PatchTST [19], we retain the channel dimension $C$ , as it is advantageous for Peri-midFormer in capturing periodic features within each channel (note that we adopt a channel independent strategy and the Figure 2 shows the processing of only one of the channels). The periodic components are extracted in the frequency domain, accomplished specifically through FFT: ", "page_idx": 2}, {"type": "image", "img_path": "5iUxMVJVEV/tmp/161fb4548ad1e5c1e12c43621553fc01138c265a7b904f02b48fb48592f13b56.jpg", "img_caption": ["Figure 2: Model architecture. PPAM denotes Periodic Pyramid Attention Mechanism. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{A}=A v g\\left(A m p\\left(F F T\\left(\\mathbf{X}_{s}\\right)\\right)\\right),\\left\\{f_{1},\\cdots,f_{k}\\right\\}=\\operatornamewithlimits{a r g\\,T o p k}_{f_{\\ast}\\in\\left\\{1,\\cdots,\\left\\lceil\\frac{L}{2}\\right\\rceil\\right\\}}\\left(\\mathbf{A}\\right),p_{i}=\\left\\lceil\\frac{L}{f_{i}}\\right\\rceil,i\\in\\left\\{1,\\cdots,k\\right\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $F F T(\\cdot)$ and $A m p(\\cdot)$ denote Fourier Transform and amplitude calculation, respectively. $\\mathbf A\\in$ $\\mathbb{R}^{L}$ represents the amplitude of each frequency, averaged across $C$ channels using Avg(\u00b7). Note that the $j$ -th value $\\mathbf{A}_{j}$ denotes the intensity of the $j$ -th frequency periodic basis function, associated with period length $\\left\\lceil{\\frac{L}{j}}\\right\\rceil$ . To handle frequency domain sparsity and minimize noise from irrelevant high frequencies [17], the top- $k$ amplitude values $\\{\\mathbf{A}_{f_{1}},\\cdot\\cdot\\cdot,\\mathbf{A}_{f_{k}}\\}$ corresponding to the most significant frequencies $\\{f_{1},\\cdot\\cdot\\cdot,f_{k}\\}$ are selected, where $k$ is a hyper-parameter, beginning from 2 to ensure the fundamental pyramid structure. Additionally, to ensure the top level of the pyramid corresponds to the original time series, we define $f_{1}=1$ , with other frequencies arranged in ascending order. These selected frequencies correspond to $k$ period lengths $\\{\\bar{p}_{1},\\cdot\\cdot\\cdot\\,,p_{k}\\}$ , arranged in descending order. Due to the frequency domain\u2019s conjugacy, only frequencies within $\\left\\{1,\\cdot\\cdot\\cdot,\\left\\lceil{\\frac{L}{2}}\\right\\rceil\\right\\}$ are considered. Based on the selected frequencies $\\{f_{1},\\cdot\\cdot\\cdot,f_{k}\\}$ and their associated period lengths $\\{p_{1},\\cdot\\cdot\\cdot,p_{k}\\}$ , we partition the original time series into periodic components for each pyramid level, denoted as $\\mathbf{C}_{\\ell}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{C}_{\\ell}=\\{\\mathbf{C}_{\\ell}^{1},\\mathbf{C}_{\\ell}^{2},\\cdot\\cdot\\cdot,\\mathbf{C}_{\\ell}^{n}\\},\\ell\\in\\{1,\\cdot\\cdot\\cdot,k\\},n\\in\\{1,\\cdot\\cdot\\cdot,f_{k}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ${\\bf C}_{\\ell}^{n}$ denotes the $n$ -th periodic component in the $\\ell$ -th pyramid level. Here, $\\ell$ is the pyramid level index, starting from the top and increasing, with a maximum value of $k$ , indicating the number of levels determined by the selected periods. Similarly, $n$ represents the component index within a level, increasing from left to right, with a maximum value of $f_{k}$ , indicating the number of components per level is determined by the frequency corresponding to that period in the original series. The Periodic Pyramid can thus be represented as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\bf P}=S t a c k\\left({\\bf C}_{\\ell}\\right),\\ell\\in\\{1,\\cdots\\,,k\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "5iUxMVJVEV/tmp/d3712461e6f804f27e849a9e50ce93ca5135e2972f3a77c17a598def7ff0b34b.jpg", "img_caption": ["Figure 3: Inclusion relationships of periodic components (left) and Periodic Pyramid Attention Mechanism (right). "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "where $S t a c k(\\cdot)$ denotes a stacking operation. The constructed Periodic Pyramid, depicted in the upper right of Figure 2, displays evident inclusion relationship among different levels, shown by connections between levels. Let $R$ denote the relationship between pairs of periodic components from the upper and lower levels, determined by the presence or absence of overlap as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{\\ell-1,\\ell}^{n_{\\ell-1},n_{\\ell}}=\\left\\{\\begin{array}{l l}{1,\\ I n d e x(\\mathbf{C}_{\\ell-1}^{n_{\\ell-1}})\\bigcap I n d e x(\\mathbf{C}_{\\ell}^{n_{\\ell}})>0}\\\\ {0,\\ e l s e}\\end{array}\\right.,\\ \\ell\\in\\{2,\\cdots,k\\},\\ n_{\\ell-1},n_{\\ell}\\in\\{1,\\cdots,f_{k}\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "when $R\\,=\\,1$ , it signifies an inclusion relationship, while $R\\,=\\,0$ indicates no overlap. This is illustrated in the left half of Figure 3. $n_{\\ell}$ denotes the index of the $n$ -th component in the $\\ell_{}$ -th level. $I n d e x(\\cdot)$ denotes the positional index of each data point within the periodic component at that level. Indices for points in the first level are contained in $\\{0,\\ldots,L-\\bar{1\\}$ . For subsequent levels, most indices match those of the first level. However, due to varying component lengths, there may be slight differences in indices for the last portion. Nonetheless, this doesn\u2019t impact relationship determination between components across levels. In practice, the relationship between the components is realized by masking the corresponding elements in the attention matrix. ", "page_idx": 4}, {"type": "text", "text": "Thanks to the inclusion relationships between periodic components across different levels in the Periodic Pyramid, complex periodic relationships inherent in 1D time series are explicitly represented. Next, due to the varying lengths of the components, it\u2019s necessary to map $\\mathbf{C}^{\\ell}$ to the same scale for subsequent Periodic Pyramid Attention Mechanism, with the equation provided as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{P}^{\\prime}=P r o j e c t i o n\\left(P a d d i n g\\left(\\mathbf{C}_{\\ell}^{n}\\right)\\right),\\ell\\in\\{1,\\cdots\\,,k\\},n\\in\\{1,\\cdots\\,,f_{k}\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $P a d d i n g(\\cdot)$ denotes zero-padding the periodic components across the time dimension to match the length of the original data, while Projection ${\\big.}(\\cdot)$ represents a single linear mapping layer. ", "page_idx": 4}, {"type": "text", "text": "3.3 Periodic Pyramid Transformer (Peri-midFormer) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Once we have the Periodic Pyramid, it can be inputted into the Peri-midFormer, as depicted in Figure 2. The Peri-midFormer introduces a specialized attention mechanism tailored for the Periodic Pyramid, called Periodic Pyramid Attention Mechanism (PPAM), shown in the right half of Figure 3. Here, original connections are replaced with bidirectional arrows, and also added within the same level. These bidirectional arrows signify attention between periodic components. In PPAM, inter-level attention focuses on period dependencies across levels, while intra-level attention focuses on dependencies within the same level. Note that attention occurs among all components within the same level, not just between adjacent ones. However, for clarity, not all attention connections within the same level are depicted. ", "page_idx": 4}, {"type": "text", "text": "In Periodic Pyramid, a periodic component ${\\bf C}_{\\ell}^{n}$ generally has three types of interconnected relationships (denoted as $\\mathbb{I}_{.}$ ) with other components: the parent node in the level above (denoted as $\\mathbb{P}$ ), all nodes within the same level including itself (denoted as A), and the child nodes in its next level (denoted as $\\mathbb{C}$ ). Therefore, the relationships of ${\\bf C}_{\\ell}^{n}$ can be expressed by the following equation: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\mathbb{I}_{\\ell}^{(n)}=\\mathbb{P}_{\\ell}^{(n)}\\bigcup\\mathbb{A}_{\\ell}^{(n)}\\bigcup\\mathbb{C}_{\\ell}^{(n)}}\\\\ {\\mathbb{P}_{\\ell}^{(n)}=\\left\\{\\mathbf{C}_{\\ell-1}^{j}:j=\\{n_{\\ell-1}:R_{\\ell-1,\\ell}^{n_{\\ell-1},n_{\\ell}}=1\\}\\right\\},\\quad\\mathrm{if~}\\ell\\geq2\\thinspace\\mathrm{else}\\otimes\\thinspace}\\\\ {\\mathbb{A}_{\\ell}^{(n)}=\\left\\{\\mathbf{C}_{\\ell}^{j}:1\\leq j\\leq f_{k}\\right\\}}\\\\ {\\mathbb{C}_{\\ell}^{(n)}=\\left\\{\\mathbf{C}_{\\ell+1}^{j}:j=\\{n_{\\ell+1}:R_{\\ell,\\ell+1}^{n_{\\ell},n_{\\ell+1}}=1\\}\\right\\},\\quad\\mathrm{if~}\\ell\\leq k-1\\thinspace\\mathrm{else}\\otimes\\thinspace}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "5iUxMVJVEV/tmp/e7ed53ddf57204639b3bdc95590c229c683629123305552b09f589d0e32d1c20.jpg", "img_caption": ["Figure 4: Periodic Feature Flows Aggregation. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "The equation shows that a component at the topmost level lacks a parent node, while one at the bottommost level lacks a child node. Based on the interconnected relationships $\\mathbb{I},$ , the attention of the component ${\\bf C}_{\\ell}^{n}$ can be expressed as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{a}_{i}=\\sum_{m\\in\\mathbb{I}_{\\ell}^{\\left(n\\right)}}\\frac{\\exp\\left(\\mathbf{q}_{i}\\mathbf{k}_{m}^{\\top}/\\sqrt{d_{K}}\\right)\\mathbf{v}_{m}}{\\sum_{m\\in\\mathbb{I}_{\\ell}^{\\left(n\\right)}}\\exp\\left(\\mathbf{q}_{i}\\mathbf{k}_{m}^{\\top}/\\sqrt{d_{K}}\\right)},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{q},\\mathbf{k}$ , and $\\mathbf{v}$ denote query, key, and value vectors, respectively, as in the classical self-attention mechanism. $m$ used for selecting components that have interconnected relationships with ${\\bf C}_{\\ell}^{n}$ . $\\mathbf{k}_{m}^{\\top}$ represents the transpose of row $m$ in $K$ . $d_{K}$ refers to the dimension of key vectors, ensuring stable attention scores through scaling. ", "page_idx": 5}, {"type": "text", "text": "We apply this attention mechanism to each component across all levels of the Periodic Pyramid, enabling the automatic detection of dependencies among all components in the Periodic Pyramid and capturing the intricate temporal variations in the time series. For a detailed theoretical proof of the PPAM see the Appendix F. ", "page_idx": 5}, {"type": "text", "text": "3.4 Periodic Feature Flows Aggregation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Here we explain the Periodic Feature Flows Aggregation used for reconstruction tasks. The output of the Peri-midFormer retains the original pyramid structure. To leverage the diverse periodic components across different levels, we treat a single branch from the top to the bottom of the pyramid as a periodic feature flow, highlighted by the red line in Figure 4. Since a periodic feature flow passes through periodic components at different levels, it contains periodic features of different scales from the time series. Additionally, due to variations among periodic components within each level, each feature flow carries distinct information. Therefore, we aggregate multiple feature flows through Periodic Feature Flow Aggregation. This involves linearly mapping each feature flow to match the length of the target time series and then averaging it across multiple feature flows to obtain the aggregated result $\\mathbf{Y}_{s}$ , as expressed in the following equation: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{Y}_{s}=M e a n P o l l i n g\\left(P r o j e c t i o n\\left(\\left\\{\\hat{\\mathbf{C}}_{1}^{n_{1}},\\hat{\\mathbf{C}}_{2}^{n_{2}},\\cdots,\\hat{\\mathbf{C}}_{\\ell}^{n_{k}}\\right\\}\\right)\\right),\\ell\\in\\{2,\\cdots,k\\},n_{k}\\in\\{1,\\cdots,f_{k}\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\hat{\\mathbf{C}}_{\\ell}^{n_{k}}$ represents a specific periodic component in the Peri-midFormer\u2019s output, and $\\{\\hat{\\mathbf{C}}_{1}^{n_{1}},\\hat{\\mathbf{C}}_{2}^{n_{2}},\\cdot\\cdot\\cdot\\,,\\hat{\\mathbf{C}}_{\\ell}^{n_{k}}\\}$ e tf oorumtsp uat  fleeantugrteh . floMwe, aasn pshooolwinn gi iagvuerrea 4g.e sP trhoej feecattiuorn $(\\cdot)$ l omwas.p cihn dfiecatauteres flow to match the targ $(\\cdot)$ $\\mathbf{Y}_{s}$ that this is the output from the seasonal part. Since we retained the channel dimension of the original time series, the result obtained after aggregating the periodic feature flows here becomes the shape of the final output. Finally, adding the trend part and de-normalization to obtain the ultimate output. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We extensively test Peri-midFormer on five mainstream analysis tasks: short- and long-term forecasting, imputation, classification, and anomaly detection. We adopted the same benchmarks as Timesnet [13], see Appendix C for details. Due to space limits, we provide only a summary of the results here, more details about the datasets, experiment implementation, model configuration, and full results can be found in Appendix. ", "page_idx": 5}, {"type": "text", "text": "Baselines The baselines include CNN-based models: TimesNet [13]; MLP-based models: LightTS [15], DLinear [14] and FITS [21]; Transformer-based models: GPT4TS [20], TimeLLM [25], iTransformer [26], TSLANet [27] , Reformer [28], Pyraformer [10], Informer [16], Autoformer [12], FEDformer [17], Non-stationary Transformer [11], ETSformer [29], PatchTST [19]. Besides, N-HiTS [30] and N-BEATS [31] are used for short-term forecasting. Anomaly Transformer [32] is used for anomaly detection. Rocket [33], LSTNet [34], TCN [8] and Flowformer [35] are used for classification. ", "page_idx": 6}, {"type": "text", "text": "4.1 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Figure 5 displays the comprehensive comparison results between PerimidFormer and other methods, it consistently excels across all five tasks. ", "page_idx": 6}, {"type": "text", "text": "4.2 Long-term Forecasting ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Setups Referring to [13], we adopt eight real-world benchmark datasets for long-term forecasting, including Weather [36], Traffic [37], Electricity [38], Exchange [34], and four ETT [16] datasets (ETTh1, ETTh2, ETTm1, ETTm2). Forecast lengths are set to 96, 192, 336, and 720. For the fairness of the comparison, we set the look-back window for all the methods to 512 (64 on Exchange), the results for other look-back windows can be found in the Appendix H.3 ", "page_idx": 6}, {"type": "image", "img_path": "5iUxMVJVEV/tmp/35dbb4682e7e34c5b680748f3ce547bd999ea2591c782f6c3cdeaa1618de12b7.jpg", "img_caption": ["Figure 5: Model performance comparison. "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "5iUxMVJVEV/tmp/f11d137726bab11ac9083bcd654edb1690fd297047a6e1e8c769dcdff2dd9fca.jpg", "table_caption": ["Table 1: Long-term forecasting task. The results are averaged from four different series length {96, 192, 336, 720}. See Table 13 and 14 for full results. Red: best, Blue: second best. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Results From Table 1, it is evident that Peri-midFormer performs exceptionally well, even completely outperforms GPT4TS and closely approaching the state-of-the-art method Time-LLM. While TimeLLM demonstrates remarkable capabilities in long-term forecasting, our Peri-midFormer shows clear advantages on the ETTh2, Electricity, and Exchange datasets. Although Time-LLM achieves the best results, it relies on a very large model, leading to significant computational overhead that is unavoidable. The same issue exists for GPT4TS. In contrast, our Peri-midFormer achieves performance close to that of Time-LLM without requiring excessive computational resources, making it more suitable for practical applications. Further analysis of model complexity is provided in Section 6. In addition, our Peri-midFormer exhibits better performance with longer look-back window, as further detailed in the Appendix E.6. ", "page_idx": 6}, {"type": "text", "text": "Table 2: Short-term forecasting task on M4. The prediction lengths are in $\\{6,48\\}$ and results are weighted averaged from several datasets under different sample intervals. ( $^*$ means former, Station means the Non-stationary Transformer.) See Table 12 for full results. Red: best, Blue: second best. ", "page_idx": 7}, {"type": "table", "img_path": "5iUxMVJVEV/tmp/b0a87ab92ce204ddb2af81d881deb26edd0b605ef199ac5ea2bce8b2853a5d98.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Short-term Forecasting ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Setups For short-term analysis, we adopt the M4 [39], which contains the yearly, quarterly and monthly collected univariate marketing data. We measure forecast performance using the symmetric mean absolute percentage error (SMAPE), mean absolute scaled error (MASE), and overall weighted average (OWA), which are calculated as detailed in the Appendix D.1. ", "page_idx": 7}, {"type": "text", "text": "Results Table 2 shows that Peri-midFormer outperforms Time-LLM, GPT4TS, TimesNet, and NBEATS, highlighting its exceptional performance in short-term forecasting. In the M4 dataset, some data lacks clear periodicity, such as the Yearly data, which mainly exhibits a strong trend. A similar situation is observed in the Exchange dataset for long-term forecasting task. Peri-midFormer performs well on these datasets due to its time series decomposition strategy. For a detailed analysis, please refer to the Appendix E.7. ", "page_idx": 7}, {"type": "text", "text": "4.4 Time Series Classification ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Setups We assessed Peri-midFormer\u2019s capacity for high-level representation learning via classification task. Mimicking settings akin to TimesNet [13], we tested it on 10 multivariate UEA classification datasets from [44], covering tasks like gesture recognition, action recognition, audio recognition, medical diagnoses, and other real-world applications. ", "page_idx": 7}, {"type": "text", "text": "Results As shown in Figure 6, PerimidFormer achieves an average accuracy of $76.6\\%$ , surpassing all baselines including TSLANet $(76.0\\%)$ , GPT4TS $(74.0\\%)$ , TimesNet $(73.6\\%)$ , and all other Transformer-based methods. This suggests that Peri-midFormer has excellent time series representation capabilities. See Appendix H.1 for full results. ", "page_idx": 7}, {"type": "image", "img_path": "5iUxMVJVEV/tmp/576e72ae8b5bb51670ef2cd3d008617b570e371eee63147ac2cb93d03e4e5359.jpg", "img_caption": ["Figure 6: Model comparison in classification. The results are averaged from 10 subsets of UEA. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.5 Imputation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Setups To validate Peri-midFormer\u2019s imputation capabilities, we conduct experiment on six realworld datasets, including four ETT datasets [16] (ETTh1, ETTh2, ETTm1, ETTm2), Electricity [38], and Weather [36]. We evaluate different random mask ratios ( $12.5\\%$ , $25\\%$ , $37.5\\%$ , $50\\%$ ) for varying levels of missing data. Notably, due to the large number of missing values, the time series do not reflect their original periodicity. Therefore, before imputation, we simply interpolate the original missing data through a linear interpolation strategy in order to use Peri-midFormer efficiently, which we call pre-interpolation. For a description of pre-interpolation and its impact on other methods, please refer to the Appendix E.5. ", "page_idx": 7}, {"type": "text", "text": "Results Table 3 demonstrates Peri-midFormer\u2019s outstanding performance on specific datasets (Electricity and Weather), surpassing other methods significantly and securing the highest average scores. However, its performance on other datasets was ordinary, possibly due to the lack of obvious periodic characteristics in them. ", "page_idx": 7}, {"type": "text", "text": "Table 3: Imputation task. We randomly mask $\\{12.5\\%$ , $25\\%$ , $37.5\\%$ , $50\\%\\}$ time points of length-96 time series. The results are averaged from 4 different mask ratios. ( $^*$ means former, Station means the Non-stationary Transformer.) See Table 15 for full results. Red: best, Blue: second best. ", "page_idx": 8}, {"type": "table", "img_path": "5iUxMVJVEV/tmp/183f4cdddf206947ca4840e8c0d3871dacf5a68e9c28acee129d6f1f9d565064.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.6 Time Series Anomaly Detection ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Setups For anomaly detection, we assess models on five standard datasets: SMD [45], MSL [46], SMAP [46], SWaT [47] and PSM [48]. To ensure fairness, we exclusively use classical reconstruction error for all baseline models, aligning with the approach in TimesNet [13]. Specifically, normal data is used for training, and a simple reconstruction loss is employed to help the model learn the distribution of normal data. In the testing phase, parts of the reconstructed output that exceed a certain threshold are considered anomalies. We use a point adjustment technique combined with a manually set threshold for this purpose. ", "page_idx": 8}, {"type": "text", "text": "Table 4: Anomaly detection task. We calculate the F1-score (as $\\%$ ) for each dataset. $^*$ means former, Station means the Non-stationary Transformer.) A higher value of F1-score indicates a better performance. See Table 16 for full results. Red: best, Blue: second best. ", "page_idx": 8}, {"type": "table", "img_path": "5iUxMVJVEV/tmp/ab2c6e0db88841c249fbab8c2c0a2fffa607bda609855f0c6b8546c11ea7f7fa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Results The results in Table 4 illustrate that Peri-midFormer\u2019s anomaly detection capability is second only to GPT4TS, but a large gap does exist. This is due to the binary event data nature in the anomaly detection dataset [21], which makes it difficult for Peri-midFormer to capture useful periodic characteristics, leading to general performance. ", "page_idx": 8}, {"type": "text", "text": "5 Ablations ", "text_level": 1, "page_idx": 8}, {"type": "table", "img_path": "5iUxMVJVEV/tmp/b6ba58a328d0cc21a74007a6e3bded5ae49af0440b28d910362b568dcca41fde.jpg", "table_caption": ["Table 5: Ablation experiments on long-term forecasting task to verify the effect of each component. Red: best, Blue: second best. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Setups We performed ablation experiments on key modules of Peri-midFormer for the long-term forecasting task, with results presented in Table 5. The table outlines the progression of module additions, from top to bottom. \"Pyraformer\" refers to the Pyraformer [10], building the pyramid downtop with convolutions and employing a simple two-fold relationship for attention distribution. \"w/o periodic components\" constructs a pyramid top-down by dividing the time series into patches without considering periodic components. \"w/o PPAM\" divides the time series into periodic components but without Period Pyramid Attention Mechanism, using periodic full attention instead, wherein attention is computed among all periodic components. \"w/o Feature Flows Aggregation\" employs PPAM but without Periodic Feature Flows Aggregation. \"Peri-midFormer\" indicates our final approach. ", "page_idx": 8}, {"type": "image", "img_path": "5iUxMVJVEV/tmp/cdf09811e83df12d674bd18808558c0063e5c3d6585bc59f749437390a8499a9.jpg", "img_caption": ["Classification on the UEA Heartbeat ", "Figure 7: Number of training parameters and FLOPs for Peri-midFormer versus baseline in terms of classification accuracy for the UEA Heartbeat dataset (left) and long-term forecasting MSE for the ETTh2 dataset (right). In the left graph, the closer to the top left, the better, while in the right graph, the closer to the bottom left, the better. Note that in the long-term forecasting, we did not fully depict the corresponding sizes due to the oversized FLOPs of Time-LLM, but instead illustrated it with text. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Results Table 5 illustrates the incremental performance enhancement achieved with the integration of each additional module, validating the effectiveness of Peri-midFormer. Notably, good results are achieved even without PPAM. This can be attributed to the model\u2019s ability to extract periodic characteristics inherent in the original time series data by delineating the periodic components. However, without highlighting inclusion relationships through PPAM, periodic full attention\u2019s ability to capture temporal changes is limited, emphasizing the significance of PPAM. ", "page_idx": 9}, {"type": "text", "text": "6 Complexity Analysis ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We conducted experiments on the model complexity of Peri-midFormer using the Heartbeat dataset for the classification task and the ETTh2 dataset for the long-term forecasting task. We considered the number of training parameters, FLOPs, and accuracy (or MSE). The results are depicted in Figure 7. In the classification task, Peri-midFormer not only achieves a significant advantage in accuracy but also requires relatively fewer training parameters and FLOPs, much less than many methods such as TimesNet, GPT4TS, Crossformer, and PatchTST. In the long-term forecasting task, Peri-midFormer achieves the lowest MSE without requiring the enormous FLOPs that Time-LLM does. This shows that although Time-LLM has strong long-term forecasting capabilities on most datasets, its computational demands are unacceptable. See Appendix E.4 for more analysis. ", "page_idx": 9}, {"type": "text", "text": "Further Model Analysis is provided in the Appendix E. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduced a method for general time series analysis called Peri-midFormer. It leverages the multi-periodicity of time series and the inclusion relationships between different periods. By segmenting the original time series into different levels of periodic components, PerimidFormer constructs a Periodic Pyramid along with its corresponding attention mechanism. Through extensive experiments covering forecasting, classification, imputation, and anomaly detection tasks, we validated the capabilities of Peri-midFormer in time series analysis, achieving outstanding results across all tasks. However, Peri-midFormer exhibits limitations, particularly in scenarios where the periodic characteristics are less apparent. We aim to address this limitation in future research to broaden its applicability. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by the National Natural Science Foundation of China under Grant Nos.   \n62276205, U22B2018, and Graduate Student Innovation Fund under Grant Nos. YJSJ24012. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Qingsong Wen, Linxiao Yang, Tian Zhou, and Liang Sun. Robust time series analysis and applications: An industrial perspective. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 4836\u20134837, 2022.   \n[2] Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. Accurate medium-range global weather forecasting with 3d neural networks. Nature, 619(7970):533\u2013538, 2023.   \n[3] Yaoran Chen, Candong Cai, Leilei Cao, Dan Zhang, Limin Kuang, Yan Peng, Huayan Pu, Chuhan Wu, Dai Zhou, and Yong Cao. Windfix: Harnessing the power of self-supervised learning for versatile imputation of offshore wind speed time series. Energy, 287:128995, 2024.   \n[4] Haotian Si, Changhua Pei, Hang Cui, Jingwen Yang, Yongqian Sun, Shenglin Zhang, Jingjing Li, Haiming Zhang, Jing Han, Dan Pei, et al. Timeseriesbench: An industrial-grade benchmark for time series anomaly detection models. arXiv preprint arXiv:2402.10802, 2024.   \n[5] Zhen Liu, Wenbin Pei, Disen Lan, and Qianli Ma. Diffusion language-shapelets for semi-supervised time-series classification. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 14079\u201314087, 2024.   \n[6] Hongming Li, Shujian Yu, and Jose Principe. Causal recurrent variational autoencoder for medical time series generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 8562\u20138570, 2023.   \n[7] Minrong Lu and Xuerong Xu. Trnn: An efficient time-series recurrent neural network for stock price prediction. Information Sciences, 657:119951, 2024.   \n[8] Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi. Unsupervised scalable representation learning for multivariate time series. Advances in Neural Information Processing Systems, 32, 2019.   \n[9] Yangdong He and Jiabao Zhao. Temporal convolutional networks for anomaly detection in time series. J. Phys. Conf. Ser, 2019.   \n[10] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In International Conference on Learning Representations, 2021.   \n[11] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers: Exploring the stationarity in time series forecasting. In Advances in Neural Information Processing Systems, 2022.   \n[12] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. In Advances in Neural Information Processing Systems, pages 101\u2013112, 2021.   \n[13] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. In The Eleventh International Conference on Learning Representations, 2023.   \n[14] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 11121\u201311128, 2023.   \n[15] Tianping Zhang, Yizhuo Zhang, Wei Cao, Jiang Bian, Xiaohan Yi, Shun Zheng, and Jian Li. Less is more: Fast multivariate time series forecasting with light sampling-oriented mlp structures. arXiv preprint arXiv:2207.01186, 2022.   \n[16] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In 35th AAAI Conference on Artificial Intelligence, pages 11106\u201311115, 2021.   \n[17] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting. In Proc. 39th International Conference on Machine Learning, 2022.   \n[18] Zelin Ni, Hang Yu, Shizhan Liu, Jianguo Li, and Weiyao Lin. Basisformer: Attention-based time series forecasting with learnable and interpretable basis. Advances in Neural Information Processing Systems, 36, 2024.   \n[19] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. In The Eleventh International Conference on Learning Representations, 2022.   \n[20] Tian Zhou, Peisong Niu, Liang Sun, Rong Jin, et al. One fits all: Power general time series analysis by pretrained lm. Advances in Neural Information Processing Systems, 36, 2024.   \n[21] Zhijian Xu, Ailing Zeng, and Qiang Xu. Fits: Modeling time series with $10k$ parameters. In The Twelfth International Conference on Learning Representations, 2023.   \n[22] Tao Dai, Beiliang Wu, Peiyuan Liu, Naiqi Li, Jigang Bao, Yong Jiang, and Shu-Tao Xia. Periodicity decoupling framework for long-term series forecasting. In The Twelfth International Conference on Learning Representations, 2024.   \n[23] Jinliang Deng, Xiusi Chen, Renhe Jiang, Du Yin, Yi Yang, Xuan Song, and Ivor W Tsang. Disentangling structured components: Towards adaptive, interpretable and scalable time series forecasting. IEEE Transactions on Knowledge and Data Engineering, 2024.   \n[24] Shiyu Wang, Haixu Wu, Xiaoming Shi, Tengge Hu, Huakun Luo, Lintao Ma, James Y Zhang, and JUN ZHOU. Timemixer: Decomposable multiscale mixing for time series forecasting. In The Twelfth International Conference on Learning Representations, 2024.   \n[25] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-LLM: Time series forecasting by reprogramming large language models. In The Twelfth International Conference on Learning Representations, 2024.   \n[26] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. itransformer: Inverted transformers are effective for time series forecasting. In The Twelfth International Conference on Learning Representations, 2023.   \n[27] Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, and Xiaoli Li. Tslanet: Rethinking transformers for time series representation learning. arXiv preprint arXiv:2404.08472, 2024.   \n[28] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020.   \n[29] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. Etsformer: Exponential smoothing transformers for time-series forecasting. arXiv preprint arXiv:2202.01381, 2022.   \n[30] Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza Ramirez, Max Mergenthaler Canseco, and Artur Dubrawski. N-hits: Neural hierarchical interpolation for time series forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 6989\u20136997, 2023.   \n[31] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural basis expansion analysis for interpretable time series forecasting. In International Conference on Learning Representations, 2019.   \n[32] Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Anomaly transformer: Time series anomaly detection with association discrepancy. In International Conference on Learning Representations, 2021.   \n[33] Angus Dempster, Franccois Petitjean, and Geoffrey I Webb. ROCKET: Exceptionally fast and accurate time series classification using random convolutional kernels. Data Mining and Knowledge Discovery, 34(5):1454\u20131495, 2020.   \n[34] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In The 41st international ACM SIGIR conference on research & development in information retrieval, pages 95\u2013104, 2018.   \n[35] Haixu Wu, Jialong Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Flowformer: Linearizing transformers with conservation flows. In International Conference on Machine Learning, pages 24226\u2013 24242. PMLR, 2022.   \n[36] Wetterstation. Weather. https://www.bgc-jena.mpg.de/wetter/.   \n[38] UCI. Electricity. https://archive.ics.uci.edu/ml/datasets/ ElectricityLoadDiagrams20112014.   \n[39] Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The m4 competition: Results, findings, conclusion and way forward. International Journal of Forecasting, 34(4):802\u2013808, 2018.   \n[40] Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza, Max Mergenthaler, and Artur Dubrawski. N-hits: Neural hierarchical interpolation for time series forecasting. arXiv preprint arXiv:2201.12886, 2022.   \n[41] T. Zhang, Yizhuo Zhang, Wei Cao, J. Bian, Xiaohan Yi, Shun Zheng, and Jian Li. Less is more: Fast multivariate time series forecasting with light sampling-oriented mlp structures. arXiv preprint arXiv:2207.01186, 2022.   \n[42] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? 2023.   \n[43] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers: Exploring the stationarity in time series forecasting. volume 35, pages 9881\u20139893, 2022.   \n[44] Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom, Paul Southam, and Eamonn Keogh. The uea multivariate time series classification archive, 2018. arXiv preprint arXiv:1811.00075, 2018.   \n[45] Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. Robust anomaly detection for multivariate time series through stochastic recurrent neural network. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 2828\u20132837, 2019.   \n[46] Kyle Hundman, Valentino Constantinou, Christopher Laporte, Ian Colwell, and Tom S\u00f6derstr\u00f6m. Detecting spacecraft anomalies using lstms and nonparametric dynamic thresholding. Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 387\u2013395, 2018.   \n[47] Aditya P Mathur and Nils Ole Tippenhauer. Swat: A water treatment testbed for research and training on ics security. In 2016 international workshop on cyber-physical systems for smart water networks (CySWater), pages 31\u201336. IEEE, 2016.   \n[48] Ahmed Abdulaal, Zhuanghua Liu, and Tomer Lancewicki. Practical approach to asynchronous multivariate time series anomaly detection and localization. In Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining, pages 2485\u20132494, 2021.   \n[49] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. In NeurIPS, 2019.   \n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017.   \n[51] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Method Details ", "text_level": 1, "page_idx": 13}, {"type": "image", "img_path": "5iUxMVJVEV/tmp/7590356038f2ee559f80afb780ddcc93b480d73f2cb6aa1a73a1a190f984d417.jpg", "img_caption": ["Figure 8: Full flowchart. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Here, we provide further elaboration on the details of Peri-midFormer. Its full flowchart is depicted in Figure 8, depicts two strategies for input. The strategy indicated by the red line is for classification task, where de-normalization and time series decomposition are not used. This is because, in classification task, there is no need to reconstruct the original data; therefore, the trend part does not need to be extracted and added back. Additionally, it is important to note that the trend part is a significant discriminative feature for classification data, so it cannot be separated from the original data before feature extraction. In the strategy indicated by the blue line, employed for reconstruction tasks, PerimidFormer needs to focus on the periodicity in the time series. Therefore, we utilize de-normalization and time series decomposition to eliminate other influencing factors. To achieve this, we first refer to [11] to address instability factors. We normalize the input $\\mathbf{X}=[x_{1},x_{2},...,x_{L}]\\in\\mathbb{R}^{L\\times C}$ to obtain $\\mathbf{\\bar{X}}_{n o r m}^{\\ \\setminus}=[x_{1}^{\\prime},x_{2}^{\\prime},...,x_{L}^{\\prime}]\\in\\mathbb{R}^{L\\times C}$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mu_{\\mathbf{x}}=\\frac{1}{L}\\sum_{i=1}^{L}{x_{i}},\\;\\sigma_{\\mathbf{x}}^{2}=\\frac{1}{L}\\sum_{i=1}^{L}({x_{i}}-\\mu_{\\mathbf{x}})^{2},\\;{x_{i}^{\\prime}}=\\frac{1}{\\sigma_{\\mathbf{x}}}\\odot({x_{i}}-\\mu_{\\mathbf{x}}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mu_{\\mathbf{x}},\\sigma_{\\mathbf{x}}\\in\\mathbb{R}^{C\\times1}$ are the mean and variance, respectively, $\\frac{1}{\\sigma_{\\mathbf{x}}}$ means the element-wise division and $\\odot$ is the element-wise product. Normalization reduces the disparity in distribution among individual input time series, thereby stabilizing the model input distribution. ", "page_idx": 13}, {"type": "text", "text": "Then, to remove the trend part from the time series and only preserve the seasonal part for PerimidFormer, we refer to [12] for time series decomposition, as shown in the following equation: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\bf{X}}_{t}=A v g P o o l(P a d d i n g({\\bf{X}}_{n o r m})),}\\\\ &{{\\bf{X}}_{s}={\\bf{X}}_{n o r m}-{\\bf{X}}_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathbf{X}_{s},\\mathbf{X}_{t}\\in\\mathbb{R}^{L\\times C}$ denote the seasonal and the trend part respectively. We adopt the $A v g P o o l(\\cdot)$ for moving average with the padding operation to keep the series length unchanged. ", "page_idx": 13}, {"type": "text", "text": "The seasonal part $\\mathbf{X}_{s}$ obtained after decomposition can be directly input into Peri-midFormer. After the output from Peri-midFormer, we add the trend part back, then de-normalize it to obtain the final output $\\hat{\\mathbf{Y}}=[\\hat{y_{1}},\\hat{y_{2}},...,\\hat{y_{T}}]$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{Y}_{s}=\\mathcal{H}(\\mathbf{X}_{s}),\\mathbf{Y}=\\mathbf{Y}_{s}+P r o j e c t i o n(\\mathbf{X}_{t}),\\hat{y}_{i}=\\sigma_{\\mathbf{x}}\\odot y_{i}+\\mu_{\\mathbf{x}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathcal{H}$ represents the Peri-midFormer model, ${\\bf Y}_{s}$ represents the output of Peri-midFormer, $P r o j e c t i o n(\\cdot)$ represents mapping the trend part to the target output length, and $\\hat{y}_{i}=\\sigma_{\\mathbf{x}}\\odot y_{i}+\\mu_{\\mathbf{x}}$ denotes de-normalization. ", "page_idx": 13}, {"type": "text", "text": "B Visualization ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To provide a clearer demonstration of Peri-midFormer\u2019s representational capabilities, Figure 9, 10 and 11 visualize some of the results for imputation, long-term forecasting and short-term forecasting, respectively. It illustrates that Peri-midFormer outperforms other methods in capturing periodic variations in time series. ", "page_idx": 13}, {"type": "image", "img_path": "5iUxMVJVEV/tmp/25422359cdb439d6d8c0ded04add4329688104a2d2aa98d1c0f455757b4ef78f.jpg", "img_caption": ["(1) Inputation on Weather dataset under $50\\%$ mask ratio ", "(2) Inputation on Electricity dataset under $50\\%$ mask ratio "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "5iUxMVJVEV/tmp/6e5bdf251b7e474d5057b7601ada2bf36525fca74abaa4c4c1968088152afdd0.jpg", "img_caption": ["Figure 9: Visualization of imputation. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "5iUxMVJVEV/tmp/a945f0493be9d1b720ab14bfba1fe35d921781296c69c0868f5765467003f672.jpg", "img_caption": ["(1) Long-term forecasting with 96 prediction length on ETTh2 ", "(2) Long-term forecasting with 96 prediction length on Electricity "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "5iUxMVJVEV/tmp/378741fe3bc7822b737f200c26be268abf2c17bac60fc80606fe30eb43ae6f91.jpg", "img_caption": ["Figure 10: Visualization of long-term forecasting. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "5iUxMVJVEV/tmp/814aa32f1169fda951827272f2bb1d25aeca9a4e466ec49d6fdb357b6f51bbbd.jpg", "img_caption": ["(1) Short-term forecasting on M4 Weekly ", ""], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "(2) Short-term forecasting on M4 Monthly ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "5iUxMVJVEV/tmp/45b9e02ffe400c6c1ece875e8c7c46b46e67b86f2773136b99bc96908ae4fccf.jpg", "img_caption": ["Figure 11: Visualization of short-term forecasting. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "C Dataset Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A detailed description of the dataset is given in Table 6. ", "page_idx": 17}, {"type": "table", "img_path": "5iUxMVJVEV/tmp/83480742db5660a1361ac2119b1c54740f96a0ea7a3595ef8cf69ebb42fd4815.jpg", "table_caption": ["Table 6: Dataset descriptions. The dataset size is organized in (Train, Validation, Test). "], "table_footnote": ["Since our datasets setup is the same as Timesnet [13], an excerpt from it describes the datasets. "], "page_idx": 17}, {"type": "text", "text": "D Experimental Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "All the deep learning networks are implemented in PyTorch and trained on NVIDIA 4090 24GB GPU. We repeated each experiment three times to eliminate randomness. The detailed experiment configuration is shown in Table 7. ", "page_idx": 17}, {"type": "text", "text": "D.1 Metrics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We utilize various metrics to evaluate different tasks. For long-term forecasting and imputations, we employ the mean square error (MSE) and mean absolute error (MAE). In anomaly detection, we utilize the F1-score, which combines precision and recall. For short-term forecasting, we utilize the symmetric mean absolute percentage error (SMAPE), mean absolute scaled error (MASE), and overall weighted average (OWA), with OWA being a unique metric used in the M4 competition. ", "page_idx": 17}, {"type": "table", "img_path": "5iUxMVJVEV/tmp/b889965a54d659066a321421ba6525530b02c749d7bac876315571ce7fb8fe4b.jpg", "table_caption": ["Table 7: Experiment configuration of Peri-midFormer. All the experiments use the ADAM [51] optimizer with the default hyperparameter configuration for $(\\beta_{1},\\beta_{2}\\bar{)}$ as (0.9, 0.999). "], "table_footnote": ["$^*$ LR means the initial learning rate. "], "page_idx": 18}, {"type": "text", "text": "These metrics are computed as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{SMAPE}=\\displaystyle\\frac{200}{T}\\displaystyle\\sum_{i=1}^{T}\\frac{|\\mathbf{X}_{i}-\\hat{\\mathbf{Y}}_{i}|}{|\\mathbf{X}_{i}|+|\\hat{\\mathbf{Y}}_{i}|},}\\\\ &{\\mathrm{MAPE}=\\displaystyle\\frac{100}{T}\\sum_{i=1}^{T}\\Big|\\frac{\\mathbf{X}_{i}-\\hat{\\mathbf{Y}}_{i}|}{|\\mathbf{X}_{i}|},}\\\\ &{\\mathrm{MASE}=\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{i=1}^{T}\\frac{|\\mathbf{X}_{i}-\\hat{\\mathbf{Y}}_{i}|}{\\frac{1}{T-q}\\sum_{j=q+1}^{T}|\\mathbf{X}_{j}-\\mathbf{X}_{j-q}|},}\\\\ &{\\mathrm{OWA}=\\displaystyle\\frac{1}{2}\\left[\\frac{\\mathrm{SMAPE}}{\\mathrm{SMAPE}_{\\mathrm{Nave2}}}+\\frac{\\mathrm{MASE}}{\\mathrm{MASE}_{\\mathrm{ainve2}}}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $q$ is the periodicity of the data. X, $\\mathbf{\\Psi},\\widehat{\\mathbf{Y}}\\in\\mathbb{R}^{T\\times C}$ are the ground truth and prediction result of the future with $T$ time pints and $C$ dimensio ns. $\\mathbf{X}_{i}$ means the $i$ -th future time point. ", "page_idx": 18}, {"type": "text", "text": "E Model Analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "E.1 Hyper Parameter Analysis and Model Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Equation (1), we introduced a hyperparameter $k$ to select the most important frequency, which also determines the number of levels in the Periodic Pyramid. We conducted sensitivity analysis on it, as shown in Figure 12. It\u2019s evident that our proposed Peri-midFormer exhibits relatively stable performance across different choices of $k$ for all four tasks. However, there are still some fluctuation in results among different $k$ values depending on the task and dataset, which are determined by the periodic characteristics in the dataset. To illustrate this, we visualize individual data for long-term forecasting and classification tasks, as shown in Figure 13. It can be observed that in the long-term forecasting task, the Etth1 dataset exhibits clear periodicity. Therefore, with larger $k$ , Peri-midFormer can capture more periodic information, resulting in better performance. In contrast, the Etth2 dataset has less obvious periodicity, so it achieves better results with smaller $k$ , as larger $k$ introduce unnecessary noise, affecting model performance. In the classification task, the EthanolConcentration dataset is difficult to classify, whereas a larger $k$ allows for the construction of Periodic Pyramid with more levels, thus extracting more representative features and achieving higher accuracy. In addition, the SelfRegulationSCP1 dataset contains a lot of high-frequency noise, so larger $k$ would focus on irrelevant information, leading to decreased accuracy. Therefore, we adjusted $k$ values differently for different tasks and datasets, as shown in the range in Table 7. ", "page_idx": 18}, {"type": "text", "text": "The above analysis reveals that the limitation of Peri-midFormer lies in its inability to fully leverage its advantages on datasets with poor periodicity characteristics. We plan to address this issue in our future work. ", "page_idx": 18}, {"type": "image", "img_path": "5iUxMVJVEV/tmp/57bb22dde975fe2722fd655cd6ce535a32451f6ba7d208058ad0c1a27ec3355e.jpg", "img_caption": ["Figure 12: Sensitivity analysis of hyper-parameters $k$ in each task. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "5iUxMVJVEV/tmp/41185728aa12949fd898d1b19a218d2a1d8a4b95ad907656b2b20d4bc1d8bfb2.jpg", "img_caption": ["Figure 13: Visualization of the Etth1 and Etth2 datasets for the Long-term forecasting task (1), and the EthanolConcentration and SelfRegulationSCP1 datasets for the classification task (2). "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "E.2 Periodic Pyramid Attention Mechanism Analysis ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To illustrate the PPAM more clearly, we visualize the original time series and attention scores within the periodic pyramid for the SelfRegulationSCP2 dataset in the classification task, as shown in Figure 14. It is evident that the attention scores among components are distributed based on inclusion and adjacency relationships, meaning that components in different levels with inclusion relationships or those in the same level have higher attention scores. This demonstrates the rationality of the Periodic Pyramid structure. Additionally, when the hyperparameter $k$ in Equation (1) is set to 2, the PPAM degenerates into periodic full attention, wherein attention is computed among all periodic ", "page_idx": 19}, {"type": "image", "img_path": "5iUxMVJVEV/tmp/e458eca69c2674f22d7d44b55cd7dee7ba457705728f0f708bf47c509362310c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 14: Visualization of the original time series and attention scores within the periodic pyramid for the SelfRegulationSCP2 dataset in the classification task. The left side shows the original data, the middle displays the corresponding pyramid structure, and the right side depicts the attention scores within the pyramid. In this example, $k=3$ , $f_{1}=1,f_{2}=2,f_{3}\\stackrel{\\circ}{=}4$ . ", "page_idx": 20}, {"type": "image", "img_path": "5iUxMVJVEV/tmp/2f3531a86b16b0fa0ba088f5d1c741af1d23314a41269c5f99a2c92122b1f4be.jpg", "img_caption": ["Figure 15: Original time series of Electricity dataset (left) and Periodic Ptramid Attention score (right). "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "components. To illustrate this, we visualize the Electricity dataset in the long-term forecasting task and its corresponding attention distribution, as shown in Figure 15. The figure shows that the Periodic Attention Mechanism can capture the dependencies among the periodic components and identify which components belong to a longer component (note that $k=2$ corresponds to 21 periodic components with larger amplitudes). This is attributed to the separation of the periodic components, which explicitly expresses the hidden periodic inclusion relationships in the time series. The above analysis demonstrates the effectiveness of the PPAM. ", "page_idx": 20}, {"type": "text", "text": "E.3 Periodic Feature Flows Analysis ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To intuitively understand the Periodic Feature Flows, we visualize it as shown in Figure 16. On the left side is the pyramid representation of the Periodic Feature Flows, with the horizontal axis representing the number of feature flows and the vertical axis representing the length of each feature flow. The feature flows are divided into multiple levels, each containing multiple periodic components. The red box encloses one feature flow, with its position from top to bottom corresponding to the pyramid from top to bottom. It can be observed that some adjacent feature flows are the same at the corresponding positions, this is because they pass through the same periodic component. On the right side, the waveform of each feature flow is displayed in different colors, with the position from left to right corresponding to the top to the bottom of the pyramid. It can be observed that each feature flow differs, which is why it is necessary to aggregate different feature flows. The aim is to fully utilize the information from each periodic component to better reconstruct the target sample. ", "page_idx": 20}, {"type": "text", "text": "E.4 Training/Inferencing Cost ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To further validate the computational complexity and scalability of the proposed method, we conducted detailed experiments on Electricity and ETTh1 datasets for the long-term forecasting task. These experiments focused on the complexity and actual time of training and inference, as well as memory usage, with results presented in Tables 8 and 9. It can be seen that our proposed PerimidFormer demonstrates a significant advantage in computational complexity on the Electricity dataset and achieves the lowest MSE. Similarly, on the ETTh1 dataset, Peri-midFormer\u2019s computational cost and inference time are not disadvantages. Instead, it achieves a lower MSE, second only to Time-LLM, while having much lower computational cost and inference time compared to it. These results highlight the advantages of our method in terms of computational complexity and scalability. ", "page_idx": 20}, {"type": "image", "img_path": "5iUxMVJVEV/tmp/3eb07674b742100587b6acd52f9aac8ccf47dbbc75bd8e632a526fc7b2d67182.jpg", "img_caption": ["Figure 16: Visualization of the pyramid form of Periodic Feature Flows (left) and the waveform of Periodic Feature Flows (right). "], "img_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "5iUxMVJVEV/tmp/463f4b10e2bd0a065e304ac89b9553d79a8c11c12cfff986f463bf0f4d6fbd86.jpg", "table_caption": ["Table 8: Complexity and scalability experiments in the long-term forecasting of length 720 on Electricity "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "5iUxMVJVEV/tmp/8ca7e054876e46035337617c98bef7198598504b1bd2347eaa4a4e4e374f12a9.jpg", "table_caption": ["Table 9: Complexity and scalability experiments in the long-term forecasting of length 720 on ETTh1 "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "E.5 Pre-interpolation ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Since Peri-midFormer is designed to focus on the periodic components of time series, directly handling data with missing values in imputation tasks may prevent it from correctly capturing the periodic characteristics of the original data without missing values, thus affecting its imputation effectiveness. Therefore, to adapt Peri-midFormer for imputation tasks, we first apply a linear interpolation strategy (Equation (13)) to the data with missing values to partially restore the periodic characteristics of the original data before inputting it into Peri-midFormer for further imputation. The visualization of original data, data with $50\\%$ missing values, and pre-interpolated data of Electricity dataset are illustrated in Figure 17. It is evident that missing values significantly disrupt the periodic characteristics of the original data, while pre-interpolation partially restores them. It is worth noting that this is not a speculative action but an effort to further explore the potential of deep learning models in imputation tasks. Our goal is for deep models to capture subtle variations in time series to achieve imputation in the details rather than wasting effort on goals achievable through simple linear interpolation. Additionally, since we can retain the indices of missing values in practical applications, there is no concern about the reliability of evaluating the imputation results. To validate the effectiveness of the pre-interpolation strategy, we applied it to other methods, as shown in Table 10. We conducted experiments on four mask ratios $\\left\\{0.125,0.25,0.375,0.5\\right\\}$ , where the first row uses only the pre-interpolation strategy. From the results, it can be seen that pre-interpolation significantly improves the performance of each method across all four mask ratios, demonstrating its importance for deep learning-based methods in interpolation tasks. ", "page_idx": 21}, {"type": "image", "img_path": "5iUxMVJVEV/tmp/0ffc2422494320086b34553c3e79244e02095dc907321b0f382d3f8bd775d716.jpg", "img_caption": ["Figure 17: Visualization of original data, data with $50\\%$ missing values, and pre-interpolated data of Electricity dataset. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "The simple linear interpolation strategy we adopted can be represented by the equation: ", "page_idx": 22}, {"type": "equation", "text": "$$\nx_{i n t e r}=\\left\\{\\begin{array}{l l}{\\frac{x_{b e f o r e}+x_{a f t e r}}{2},}&{i f\\ (x_{b e f o r e}\\neq\\mathrm{None})\\ a n d\\ (x_{a f t e r}\\neq\\mathrm{None})}\\\\ {x_{a f t e r},}&{i f\\ (x_{b e f o r e}=\\mathrm{None})\\ a n d\\ (x_{a f t e r}\\neq\\mathrm{None})}\\\\ {x_{b e f o r e},}&{i f\\ (x_{b e f o r e}\\neq\\mathrm{None})\\ a n d\\ (x_{a f t e r}=\\mathrm{None})}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "table", "img_path": "5iUxMVJVEV/tmp/b2a6a0c08d579d4e21f012e2f5e4588b5e1df9f8fedf3f65a5d306abf31d46ef.jpg", "table_caption": ["Table 10: Ablation Experiments of pre-interpolation in inputation task on ECL dataset. "], "table_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "5iUxMVJVEV/tmp/e2e03eb1f1d0693537b8671c52aac22772fc4efa9126447bc5964def19d68083.jpg", "img_caption": ["Figure 18: Performance of different length look-back window on the long-term forecasting task in Electricity dataset. Prediction lengths are $\\{96,192,336,720,1000\\}$ . "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "where $x_{i n t e r}$ represents the value used to replace the 0 value, $x_{b e f o r e}$ represents the nearest non-zero value before the 0 value, and $x_{a f t e r}$ represents the nearest non-zero value after the 0 value. ", "page_idx": 23}, {"type": "text", "text": "E.6 Look-back window length ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Here, we investigate the performance of Peri-midFormer under different look-back window lengths in long-term forecasting task on the Electricity dataset. As shown in Figure 18, we experimented not only with prediction lengths of $\\{96,192,336,720\\}$ , but also with longer lengths such as 1000. It can be observed that there is a clear improvement in performance across all five prediction lengths as the look-back window length increases. This improvement is particularly notable for longer prediction lengths such as 336, 720, and 1000. It indicates that Peri-midFormer can effectively utilize more historical information, as longer look-back windows contain more stable periodic characteristics, which is precisely what Peri-midFormer requires. ", "page_idx": 23}, {"type": "text", "text": "E.7 Time Series Decomposition Analysis ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Peri-midFormer performs well on the Exchange dataset for long-term forecasting task and the Yearly dataset for short-term forecasting. Although these two datasets lack obvious periodicity, they exhibit strong trends, as shown in Figure (19)). Peri-midFormer uses a time series decomposition strategy, where the trend part is first separated from the original data before dividing the periodic components. After the output of Peri-midFormer, the predicted trend part is added back, as illustrated in Figure (8). The separated trend part is easier to predict, especially when the trend is strong. This is why Peri-midFormer achieves strong performance on the Exchange and Yearly datasets. ", "page_idx": 23}, {"type": "text", "text": "Exchange ", "text_level": 1, "page_idx": 24}, {"type": "image", "img_path": "5iUxMVJVEV/tmp/9ba786253015caa9467c5ff5195476792dcfbdc64000f0a6996c513e2f511eee.jpg", "img_caption": ["Figure 19: Visualization of Exchange and Yearly dataset. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "F Proof ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "To demonstrate the essence of attention computation among multi-level periodic components, we need to analyze how the interactions between periodic components at different levels affect the final feature extraction. In time series analysis, different periodic components correspond to different time scales. This means that through decomposition, we can capture components of various frequencies within the time series. The essence of the periodic pyramid is to capture these different frequency components through its hierarchical structure. ", "page_idx": 24}, {"type": "text", "text": "Using single-channel data as an example, and given that we adopt an independent channel strategy, this can be easily extended to all channels. Assume the time series $x(t)$ can be decomposed into multiple periodic components $x_{n}(t)$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\nx(t)=\\sum_{n=1}^{N}x_{n}(t).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Taking two different periodic components as examples: ", "page_idx": 24}, {"type": "equation", "text": "$$\nx_{i}(t)=A_{i}\\sin\\left(\\frac{2\\pi t}{T_{i}}+\\phi_{i}\\right),x_{j}(t)=A_{j}\\cos\\left(\\frac{2\\pi t}{T_{j}}+\\phi_{j}\\right),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $A$ is amplitude, $T$ is period, and $\\phi$ is phase. Due to the overlap and inclusion relationships between different periodic components, we employ an attention mechanism in the periodic pyramid to capture the similarities between different periodic components, focusing on important periodic features. When applying the attention mechanism, we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\nQ_{i}=W_{Q}x_{i}(t),\\quad K_{j}=W_{K}x_{j}(t),\\quad V_{j}=W_{V}x_{j}(t),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $W_{Q},W_{K}$ and $W_{V}$ are learnable weight matrices. From Equations (15) and (16): ", "page_idx": 24}, {"type": "equation", "text": "$$\nQ_{i}=W_{Q}A_{i}\\sin\\left(\\frac{2\\pi t}{T_{i}}+\\phi_{i}\\right),~~~K_{j}=W_{K}A_{j}\\cos\\left(\\frac{2\\pi t}{T_{j}}+\\phi_{j}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Further, the dot-product attention can be expressed as: ", "page_idx": 24}, {"type": "equation", "text": "$$\nQ_{i}K_{j}^{T}=A_{i}A_{j}\\left(W_{Q}\\sin\\left(\\frac{2\\pi t}{T_{i}}+\\phi_{i}\\right)\\right)\\left(W_{K}\\cos\\left(\\frac{2\\pi t}{T_{j}}+\\phi_{j}\\right)\\right)^{T}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Using the trigonometric identity $\\begin{array}{r}{\\sin(a)\\cos(b)=\\frac{1}{2}[\\sin(a+b)+\\sin(a-b)]}\\end{array}$ , the dot-product $Q_{i}K_{j}^{T}$ can be further expressed as: ", "page_idx": 24}, {"type": "equation", "text": "$$\n2_{i}K_{j}^{T}=\\frac{1}{2}A_{i}A_{j}\\left\\{W_{Q}\\left[\\sin\\left(\\frac{2\\pi t}{T_{i}}+\\phi_{i}+\\frac{2\\pi t}{T_{j}}+\\phi_{j}\\right)+\\sin\\left(\\frac{2\\pi t}{T_{i}}+\\phi_{i}-\\frac{2\\pi t}{T_{j}}-\\phi_{j}\\right)\\right]\\right\\}({W_{K}})^{T}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Based on this, considering the periodicity and symmetry of $\\sin(a+b)$ and $\\sin(a-b)$ , when the periods of two time series components are close $/$ same (intra-layer attention in the pyramid, see the right side of Figure 3 in the original paper) or have overlapping / inclusive parts (inter-layer attention in the pyramid, see the right side of Figure 3 in the original paper), the values of these two sine functions will be highly correlated, resulting in a large $\\bar{Q_{i}}\\bar{K_{j}^{T}}$ value. This indicates that the periodic pyramid model can effectively capture similar periodic patterns across different time scales. ", "page_idx": 25}, {"type": "text", "text": "Next, incorporating this into the calculation of the attention score: ", "page_idx": 25}, {"type": "equation", "text": "$$\ns_{i j}=\\frac{\\exp\\left(\\frac{Q_{i}K_{j}^{T}}{\\sqrt{d_{k}}}\\right)}{\\sum_{j^{\\prime}}\\exp\\left(\\frac{Q_{i}K_{j^{\\prime}}^{T}}{\\sqrt{d_{k}}}\\right)}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It can be seen that the attention scores between highly correlated periodic components will be higher, which we have already validated in Figures 13 and 14 of the original paper. ", "page_idx": 25}, {"type": "text", "text": "Further, the attention vector ${\\bf a}_{i}$ of $x_{i}(t)$ can be obtained as: ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\bf a}_{i}=\\sum_{j}s_{i j}V_{j}\\mathrm{~,~}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "From the above derivation, it can be seen that the attention mechanism can measure the similarity between different periodic components. This similarity reflects the alignment between different periodic components in the time series, allowing the model to capture important periodic patterns. By capturing these periodic patterns, the periodic pyramid can extract key features of the time series, resulting in a comprehensive and accurate time series representation. This representation not only includes information across different time scales but also enhances the representation of important periodic patterns. ", "page_idx": 25}, {"type": "text", "text": "G Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Our research has implications for time-series-based analyses such as weather forecasting and anomaly detection for industrial maintenance, as discussed in the Introduction 1. Our work carries no negative social impact. ", "page_idx": 25}, {"type": "text", "text": "H Full Results ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "H.1 Full Results of Classification (Table 11) H.2 Full Results of Short-term Forecasting (Table 12) H.3 Full Results of Long-term Forecasting (Table 13 and 14) H.4 Full Results of Imputation (Table 15) H.5 Full Results of Anomaly Detection (Table 16) ", "page_idx": 25}, {"type": "table", "img_path": "5iUxMVJVEV/tmp/f609426cb83a0ae66eb76c362ce8620cecdfecb997967ea307181ec253fdb8ba.jpg", "table_caption": ["Table 11: Full results for the classification task. $^*$ means former, T-LLM means Time-LLM, GPT means GPT4TS, T-Net means TimesNet, Patch means PatchTST, Light means LightTS, Station means the Non-stationary Transformer.) We report the classification accuracy $(\\%)$ as the result. The standard deviation is within $1\\%$ . We reproduced the results of PatchTST by https://github.com/thuml/Time-Series-Library, reproduced TSLANet by https:// github.com/emadeldeen24/TSLANet, and copied the others from GPT4TS [20]. Red: best, Blue: second best. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "5iUxMVJVEV/tmp/6441c07c76fe1c1baad64592ccca06d410ac0e8039a6d16d552689ca8caec87d.jpg", "table_caption": ["Table 12: Full results for the short-term forecasting task in the M4 dataset. ( $^*$ means former, T-LLM means Time-LLM, GPT means GPT4TS, T-Net means TimesNet, Patch means PatchTST, HiTS means N-HiTS, BEATS means N-BEATS, Light means LightTS, Station means the Non-stationary Transformer.) The standard deviation is within $0.5\\%$ . We copied the results of Time-LLM from Time-LLM [25], Pyraformer from TimesNet [13], and the remaining results from GPT4TS [20]. Red: best, Blue: second best. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "5iUxMVJVEV/tmp/028a675a7ab755eb3a0a92a5bc01b2cdb8e4e426f6a5be4535bb027f0a55e1ed.jpg", "table_caption": ["Table 13: Full results of 512 look-back window length in long-term forecasting task (since FITS defaults to a look-back window length of 720, its results at 720 length are attached at the end). The standard deviation is within $0.5\\%$ . We copied the results of GPT4TS from GPT4TS [20], Time-LLM from TSLANet [27], reproduced TSLANet by https://github.com/emadeldeen24/TSLANet, and reproduced the others by https://github.com/thuml/Time-Series-Library. Red: best, Blue: second best. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "5iUxMVJVEV/tmp/babba2f7f29d87aa9630949a7b33e511f137adb39163bca86ec646b2cb181ba7.jpg", "table_caption": ["Table 14: Full results of 96 look-back window length in long-term forecasting task. ( $^*$ means former, Station means the Non-stationary Transformer.) The standard deviation is within $0.5\\%$ . We copied the results of iTransformer from TSLANet [27], Pyraformer from TimesNet [13], reproduced FITS by https://github.com/VEWOXIC/FITS, and copied the others from GPT4TS [20]. Red: best, Blue: second best. "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "5iUxMVJVEV/tmp/83b5c4c1be714aba98261424ff94cda8081b8116ebe3f95e352fea4be24145ac.jpg", "table_caption": ["Table 15: Full results for the imputation task. We randomly mask $12.5\\%$ , $25\\%$ , $37.5\\%$ and $50\\%$ time points to compare the model performance under different missing degrees. $^*$ means former, Station means the Non-stationary Transformer.) The standard deviation is within $0.5\\%$ . We reproduced the results of Pyraformer by https://github.com/thuml/Time-Series-Library, and copied the others from GPT4TS [20]. Red: best, Blue: second best. "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "5iUxMVJVEV/tmp/846d54e7a42e7394131009b200156bf937fb4d6e0d33af2fba52bc204a0a3740.jpg", "table_caption": ["Table 16: Full results for the anomaly detection task. The P, R and F1 represent the precision, recall and F1-score $(\\%)$ respectively. F1-score is the harmonic mean of precision and recall. A higher value of P, R and F1 indicates a better performance. (Station means the Non-stationary Transformer.) The standard deviation is within $1\\%$ . We copied the results from GPT4TS [20]. Red: best, Blue: second best. "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 31}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 31}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] ", "page_idx": 31}, {"type": "text", "text": "\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 31}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 31}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 31}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 31}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We made our main claims in the abstract and introduction. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. ", "page_idx": 31}, {"type": "text", "text": "\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We illustrate the limitations of the model in conjunction with the hyperparametric analysis in the Appendix E.1. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: We provide a full theoretical description of the proposed methodology in the main paper and appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results. \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems. \u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We describe the proposed methodology in detail in Section 3 and provide further explanation in Appendix A. Experimental details are outlined in Appendix D. Additionally, all the code related to the proposed method is included in the supplementary material. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: The code is available at https://github.com/WuQiangXDU/Peri-midFormer. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We offer a detailed description of the dataset in Appendix C and outline the experimental setup in Appendix D. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: In Appendix D, we demonstrate that the experiments were repeated multiple times to eliminate randomness. However, due to the large number of experiments, detailed standard deviations are not shown. Instead, we uniformly present them in the headings of each table in Appendix H. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: We describe the compute workers required for the experiments in Appendix D, and the amount of computation in Section 6 and Appendix E.4. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: We have read the NeurIPS Code of Ethics and conducted it in the paper conform in every respect. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: We illustrate broader impacts of our research in Appendix G. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We cite all the datasets and models involved in the paper. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: The code is available at https://github.com/WuQiangXDU/Peri-midFormer. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]