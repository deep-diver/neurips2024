[{"figure_path": "qvdc0oCX2n/figures/figures_3_1.jpg", "caption": "Figure 1: Illustration of negCLIPLoss. CLIPScore may underestimate (bottom left, where the data quality is high but CLIPScore is low) or overestimate (bottom right, where the data quality is low but CLIPScore is high) the quality of image-text pairs. However, this issue can be mitigated by simply subtracting a normalization term R. negCLIPLoss employs the teacher model to calculate the negative CLIP loss on training data and serves as a more accurate metric. Here, \"Top X%\" denotes that the score represents the top X% high values within the entire dataset (i.e., the (100-X)% percentile among all the values). For example, \"R: Top 100%\" means this data has almost the smallest R among the whole dataset, which represents that it contains highly specific elements in both images and texts.", "description": "This figure illustrates how the proposed negCLIPLoss metric addresses the limitations of the traditional CLIPScore metric in evaluating the quality of image-text pairs for multimodal contrastive learning.  CLIPScore can either underestimate or overestimate quality due to systematic biases.  negCLIPLoss incorporates a normalization term derived from the CLIP training loss to mitigate these biases, providing a more accurate and robust measure of data quality.", "section": "3.1 negCLIPLoss: A Better Metric than CLIPScore"}, {"figure_path": "qvdc0oCX2n/figures/figures_3_2.jpg", "caption": "Figure 2: Comparison of negCLIPLoss and CLIPScore across different downsampling ratios on DataComp-medium.", "description": "This figure compares the performance of negCLIPLoss and CLIPScore on the DataComp-medium benchmark across various downsampling ratios.  It shows the ImageNet-1k accuracy and the average performance across 38 downstream datasets. The results demonstrate that negCLIPLoss consistently outperforms CLIPScore across different downsampling ratios, suggesting its effectiveness as a superior metric for data selection in multimodal contrastive learning.", "section": "3 Data Filtering Strategy"}, {"figure_path": "qvdc0oCX2n/figures/figures_4_1.jpg", "caption": "Figure 3: Illustration of NormSim on DataComp. Xtarget is the target prior data. \u201cTop X%\u201d denotes that the score represents the top X% high values within the entire dataset. (a) Visualization of data with different NormSim and negCLIPLoss. Here we use NormSim2(ImageNet-1k) as an example. Although both Type 2 and Type 4 data have high negCLIPLoss and thus high quality, data with low NormSim2 (Type 4) are more irrelevant to downstream tasks like ImageNet, VTAB, and MSCOCO. For example, they contain many images dominated by OCR content and make little contribution to improving downstream performance. (b) Illustration of a rough comparison of sampling data for different filtering methods. Using \u201cnegCLIPLoss \u2229 NormSim\u201d filtering can balance the quality and relevance to downstream tasks, thus increasing the proportion of Type 2 data. (Refer to Appendix E for more visualization.)", "description": "This figure illustrates the NormSim metric's effectiveness in data selection within the DataComp benchmark.  It shows how NormSim, combined with negCLIPLoss, balances data quality (measured by negCLIPLoss) with relevance to target downstream tasks (measured by NormSim).  Panel (a) visualizes four data types based on their NormSim and negCLIPLoss scores, highlighting that high-quality data (high negCLIPLoss) may not always be relevant to the downstream tasks (low NormSim). Panel (b) compares the distribution of these data types across various data selection methods, demonstrating that the combination of negCLIPLoss and NormSim effectively selects a higher proportion of target-related, high-quality data.", "section": "3 Data Filtering Strategy"}, {"figure_path": "qvdc0oCX2n/figures/figures_16_1.jpg", "caption": "Figure 4: Illustration of different directions for data selection methods for multimodal contrastive learning. Here we use four colors to denote the four main resources we can obtain: CLIP teacher model, downstream target data (which is much smaller than the external multimodal dataset or pretraining dataset), the external image-text dataset, and the external non-CLIP model. Direction 1 denotes the methods that only use the original OAI CLIP teacher model and the downstream target data. Direction 2 represents the methods that use external datasets to train a new CLIP teacher model for improving filtering, like DFN [2]. Direction 3 denotes the methods that use external non-CLIP model to select the data that may be heuristically helpful for downstream tasks, like image without too much text or be more special. In general, D1 method using only CLIP embedding, like negCLIPLoss, is orthogonal to D2. And both D1 and D2 can be combined with D3 to explore better filtering results. In the experiments part of the main paper (Sec. 4), we further show that our proposed D1 methods: NormSim and negCLIPLoss, can outperform all the D3 baselines except the best method \"HYPE U DFN\". And we can achieve the new state-of-the-art by combining our methods with that method.", "description": "This figure illustrates the three main directions for data selection methods in multimodal contrastive learning.  It highlights the resources used in each approach: the CLIP teacher model, downstream target data, external image-text datasets, and external non-CLIP models.  Direction 1 uses only the original CLIP model and target data; Direction 2 trains a new CLIP model using external data; and Direction 3 leverages external non-CLIP models for data selection. The figure emphasizes that the proposed methods (negCLIPLoss and NormSim) are orthogonal to existing techniques and can be combined to achieve state-of-the-art results.", "section": "4 Experimental Results"}, {"figure_path": "qvdc0oCX2n/figures/figures_21_1.jpg", "caption": "Figure 4: Illustration of different directions for data selection methods for multimodal contrastive learning. Here we use four colors to denote the four main resources we can obtain: CLIP teacher model, downstream target data (which is much smaller than the external multimodal dataset or pretraining dataset), the external image-text dataset, and the external non-CLIP model. Direction 1 denotes the methods that only use the original OAI CLIP teacher model and the downstream target data. Direction 2 represents the methods that use external datasets to train a new CLIP teacher model for improving filtering, like DFN [2]. Direction 3 denotes the methods that use external non-CLIP model to select the data that may be heuristically helpful for downstream tasks, like image without too much text or be more special. In general, D1 method using only CLIP embedding, like negCLIPLoss, is orthogonal to D2. And both D1 and D2 can be combined with D3 to explore better filtering results. In the experiments part of the main paper (Sec. 4), we further show that our proposed D1 methods: NormSim and negCLIPLoss, can outperform all the D3 baselines except the best method \"HYPE U DFN\". And we can achieve the new state-of-the-art by combining our methods with that method.", "description": "This figure illustrates the three main approaches to data selection for multimodal contrastive learning.  The three directions are differentiated by the resources they leverage: Direction 1 uses only the original CLIP model and target downstream data. Direction 2 trains a new CLIP-style model using external data for improved selection. Direction 3 employs external non-CLIP models to aid in the selection process.  The figure shows how the authors' proposed methods (NormSim and negCLIPLoss) fit within this framework, ultimately achieving state-of-the-art results by combining with existing methods.", "section": "4 Experimental Results"}, {"figure_path": "qvdc0oCX2n/figures/figures_27_1.jpg", "caption": "Figure 5: Results of negCLIPLoss with a different number of batch samples (denoted as K) on DataComp-medium. Solid lines denote negCLIPLoss, while dashed lines denote CLIPScore. Here, we use OAI CLIP-L/14 as the pretrained model. We can see that once K \u2265 5, negCLIPLoss consistently outperforms CLIPScore across all subtask metrics. In the main paper, we set K = 10.", "description": "This figure shows the performance of negCLIPLoss and CLIPScore on the DataComp-medium benchmark across different downstream tasks (ImageNet-1k, IN Dist, VTAB, Retrieval) with varying numbers of randomly selected batches (K) used in the negCLIPLoss calculation. The results indicate that using at least 5 batches (K\u22655) leads to consistent improvement of negCLIPLoss over CLIPScore across all metrics.", "section": "D.1 Stability Analysis of Batch Sampling Numbers in negCLIPLoss"}, {"figure_path": "qvdc0oCX2n/figures/figures_29_1.jpg", "caption": "Figure 1: Illustration of negCLIPLoss. CLIPScore may underestimate (bottom left, where the data quality is high but CLIPScore is low) or overestimate (bottom right, where the data quality is low but CLIPScore is high) the quality of image-text pairs. However, this issue can be mitigated by simply subtracting a normalization term R. negCLIPLoss employs the teacher model to calculate the negative CLIP loss on training data and serves as a more accurate metric. Here, \"Top X%\" denotes that the score represents the top X% high values within the entire dataset (i.e., the (100-X)% percentile among all the values). For example, \"R: Top 100%\" means this data has almost the smallest R among the whole dataset, which represents that it contains highly specific elements in both images and texts.", "description": "This figure illustrates how the proposed negCLIPLoss method addresses the limitations of the traditional CLIPScore metric in evaluating the quality of image-text pairs.  CLIPScore can misjudge data quality, either underestimating or overestimating it.  negCLIPLoss improves accuracy by incorporating a normalization term derived from CLIP training loss, effectively mitigating biases and leading to a more precise quality assessment. The examples show how CLIPScore's limitations result in inaccurate rankings, while negCLIPLoss offers a more reliable evaluation.", "section": "3 Data Filtering Strategy"}, {"figure_path": "qvdc0oCX2n/figures/figures_30_1.jpg", "caption": "Figure 1: Illustration of negCLIPLoss. CLIPScore may underestimate (bottom left, where the data quality is high but CLIPScore is low) or overestimate (bottom right, where the data quality is low but CLIPScore is high) the quality of image-text pairs. However, this issue can be mitigated by simply subtracting a normalization term R. negCLIPLoss employs the teacher model to calculate the negative CLIP loss on training data and serves as a more accurate metric. Here, \"Top X%\" denotes that the score represents the top X% high values within the entire dataset (i.e., the (100-X)% percentile among all the values). For example, \"R: Top 100%\" means this data has almost the smallest R among the whole dataset, which represents that it contains highly specific elements in both images and texts.", "description": "This figure illustrates how the proposed negCLIPLoss metric addresses the limitations of the traditional CLIPScore metric in evaluating the quality of image-text pairs for multimodal contrastive learning.  CLIPScore, which measures cosine similarity between image and text embeddings, can underestimate or overestimate quality due to systematic biases. NegCLIPLoss incorporates a normalization term derived from CLIP training loss, effectively mitigating these biases and providing a more accurate assessment of data quality.  The examples in the figure demonstrate scenarios where CLIPScore misjudges quality, while negCLIPLoss provides a more nuanced and accurate evaluation.", "section": "3.1 negCLIPLoss: A Better Metric than CLIPScore"}, {"figure_path": "qvdc0oCX2n/figures/figures_31_1.jpg", "caption": "Figure 1: Illustration of negCLIPLoss. CLIPScore may underestimate (bottom left, where the data quality is high but CLIPScore is low) or overestimate (bottom right, where the data quality is low but CLIPScore is high) the quality of image-text pairs. However, this issue can be mitigated by simply subtracting a normalization term R. negCLIPLoss employs the teacher model to calculate the negative CLIP loss on training data and serves as a more accurate metric. Here, \"Top X%\" denotes that the score represents the top X% high values within the entire dataset (i.e., the (100-X)% percentile among all the values). For example, \"R: Top 100%\" means this data has almost the smallest R among the whole dataset, which represents that it contains highly specific elements in both images and texts.", "description": "This figure illustrates the concept of negCLIPLoss and how it addresses the limitations of CLIPScore in assessing the quality of image-text pairs.  CLIPScore, a commonly used metric, can either underestimate or overestimate the quality. negCLIPLoss improves upon this by incorporating a normalization term derived from the negative CLIP loss calculated on training data. This normalization helps to correct for systematic biases that might lead to inaccurate CLIPScore values. The figure uses examples with visual and textual descriptions to demonstrate how negCLIPLoss provides a more accurate and reliable metric for evaluating data quality.", "section": "3.1 negCLIPLoss: A Better Metric than CLIPScore"}, {"figure_path": "qvdc0oCX2n/figures/figures_32_1.jpg", "caption": "Figure 3: Illustration of NormSim on DataComp. Xtarget is the target prior data. \u201cTop X%\u201d denotes that the score represents the top X% high values within the entire dataset. (a) Visualization of data with different NormSim and negCLIPLoss. Here we use NormSim2(ImageNet-1k) as an example. Although both Type 2 and Type 4 data have high negCLIPLoss and thus high quality, data with low NormSim2 (Type 4) are more irrelevant to downstream tasks like ImageNet, VTAB, and MSCOCO. For example, they contain many images dominated by OCR content and make little contribution to improving downstream performance. (b) Illustration of a rough comparison of sampling data for different filtering methods. Using \u201cnegCLIPLoss NormSim\u201d filtering can balance the quality and relevance to downstream tasks, thus increasing the proportion of Type 2 data. (Refer to Appendix E for more visualization.)", "description": "This figure illustrates the NormSim metric's effectiveness in data selection for multimodal contrastive learning.  Panel (a) shows how NormSim and negCLIPLoss interact; high negCLIPLoss indicates high-quality data, but NormSim further filters out data irrelevant to downstream tasks, as shown using ImageNet-1k as an example. Panel (b) demonstrates that combining NormSim and negCLIPLoss improves data quality and relevance to the target task, thus boosting performance. ", "section": "3 Data Filtering Strategy"}, {"figure_path": "qvdc0oCX2n/figures/figures_33_1.jpg", "caption": "Figure 10: Visualization of the images from a small subset whose NormSim\u221e(Target) rank top 50% high in DataComp-medium.", "description": "This figure visualizes a subset of images selected using the NormSim\u221e(Target) metric.  NormSim\u221e(Target) is a method introduced in the paper to select high-quality training data by measuring the similarity between the visual features of the training data and a target dataset (in this case, the ImageNet-1k dataset). The figure shows examples of images from the top 50% of the ranked data, illustrating the type of images considered to be high quality and relevant to the downstream tasks according to this metric. The visualization is intended to give the reader an idea of the kinds of images the model considers high-quality, and potentially offers insights into the characteristics that make them valuable for multimodal contrastive learning.", "section": "4 Experimental Results"}, {"figure_path": "qvdc0oCX2n/figures/figures_34_1.jpg", "caption": "Figure 1: Illustration of negCLIPLoss. CLIPScore may underestimate (bottom left, where the data quality is high but CLIPScore is low) or overestimate (bottom right, where the data quality is low but CLIPScore is high) the quality of image-text pairs. However, this issue can be mitigated by simply subtracting a normalization term R. negCLIPLoss employs the teacher model to calculate the negative CLIP loss on training data and serves as a more accurate metric. Here, \"Top X%\" denotes that the score represents the top X% high values within the entire dataset (i.e., the (100-X)% percentile among all the values). For example, \"R: Top 100%\" means this data has almost the smallest R among the whole dataset, which represents that it contains highly specific elements in both images and texts.", "description": "This figure illustrates the concept of negCLIPLoss and how it addresses the limitations of CLIPScore in accurately assessing the quality of image-text pairs. CLIPScore, by only considering the cosine similarity between image and text embeddings of a single sample, can underestimate or overestimate the quality, especially in cases with systematic biases.  negCLIPLoss improves upon this by introducing a normalization term based on the alignment between a sample and its contrastive pairs, leading to more robust and accurate quality measurements.", "section": "3.1 negCLIPLoss: A Better Metric than CLIPScore"}]