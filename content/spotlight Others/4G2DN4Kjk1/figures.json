[{"figure_path": "4G2DN4Kjk1/figures/figures_8_1.jpg", "caption": "Figure 1: Same input dist., k = 16, large min. distance between regression vectors.", "description": "The figure compares the performance of the proposed algorithm with that of the algorithm in [KSS+20] under the same setting as in [KSS+20], i.e., with more restrictive assumptions. The results show that the proposed algorithm significantly outperforms [KSS+20] in terms of MSE, especially for smaller medium batch sizes.", "section": "4 Empirical Results"}, {"figure_path": "4G2DN4Kjk1/figures/figures_8_2.jpg", "caption": "Figure 1: Same input dist., k = 16, large min. distance between regression vectors.", "description": "The figure shows the mean squared error (MSE) for different medium batch sizes when comparing the proposed algorithm with the KSS+20 algorithm. The input distributions are the same for all sub-populations, the number of sub-populations is 16, and there is a large minimum distance between the regression vectors. The plot shows the MSE for new batches of size 4 and 8, averaged over 10 runs.  The error bars represent standard errors.", "section": "Empirical Results"}, {"figure_path": "4G2DN4Kjk1/figures/figures_35_1.jpg", "caption": "Figure 1: Same input dist., k = 16, large min. distance between regression vectors.", "description": "This figure compares the performance of the proposed algorithm and the KSSKO algorithm on a linear regression task with 16 sub-populations.  The input distributions are identical across all sub-populations, and the regression vectors are well-separated (large minimum distance). The plot shows the mean squared error (MSE) achieved by each algorithm for different medium batch sizes and new batch sizes of 4 and 8.  The shaded regions represent the standard error over multiple runs.", "section": "4 Empirical Results"}, {"figure_path": "4G2DN4Kjk1/figures/figures_35_2.jpg", "caption": "Figure 1: Same input dist., k = 16, large min. distance between regression vectors.", "description": "This figure compares the performance of the proposed algorithm with the algorithm from [KSS+20] under more restrictive assumptions.  Specifically, it shows the Mean Squared Error (MSE) for both algorithms across different medium batch sizes (4 and 8 samples per batch).  The results demonstrate that the proposed algorithm significantly outperforms the baseline, even under these restrictive conditions where all input distributions are identical, and there's a large separation between regression vectors.", "section": "4 Empirical Results"}]