[{"figure_path": "cqfE9eYMdP/tables/tables_7_1.jpg", "caption": "Table 1: Comparison of our NeurKItt and GMRES computation time and iterations across datasets, preconditioning, and tolerances. The first column lists datasets with matrix size, and the next details tolerances. Results are displayed as \"time speedup / iteration speedup\".", "description": "This table compares the performance of NeurKItt and GMRES in solving linear systems. It shows the time and iteration speedup of NeurKItt over GMRES for various datasets (Darcy, Heat, Helmholtz), preconditioning methods (None, Jacobi, BJacobi, SOR, ASM, ICC, ILU), and tolerance levels (1e-2 to 1e-12).  Each cell presents the speedup in computation time and the speedup in the number of iterations.  The results demonstrate NeurKItt's efficiency improvements across diverse settings.", "section": "6.1 Experiment Settings"}, {"figure_path": "cqfE9eYMdP/tables/tables_8_1.jpg", "caption": "Table 2: Comparison of different settings on principal Angle (rad) for the Darcy flow problem, with a matrix size of 32,400. We replace the FNO with MLP for \"w/o FNO\" and replace the projection loss with MSE loss for \"w/o Projection Loss\"", "description": "This table presents the ablation study results for the Darcy flow problem with a matrix size of 32,400. It compares the principal angle (a measure of subspace similarity) obtained using the full NeurKItt model against variations where either the Fourier Neural Operator (FNO), QR decomposition, or projection loss are removed.  The principal angle is a measure of the difference between the predicted subspace and the actual invariant subspace; smaller angles indicate better predictions.  The results show that FNO and the projection loss are crucial for accurate subspace prediction.", "section": "6.4 Ablation Study"}, {"figure_path": "cqfE9eYMdP/tables/tables_19_1.jpg", "caption": "Table 1: Comparison of our NeurKItt and GMRES computation time and iterations across datasets, preconditioning, and tolerances. The first column lists datasets with matrix size, and the next details tolerances. Results are displayed as \"time speedup / iteration speedup\"", "description": "This table compares the performance of NeurKItt and GMRES in solving linear systems.  It shows the speedup (both in computation time and number of iterations) achieved by NeurKItt compared to GMRES across various datasets (Darcy, Heat, Helmholtz), preconditioning techniques (None, Jacobi, BJacobi, SOR, ASM, ICC, ILU), and tolerance levels (1e-2, 1e-4, 1e-7, 1e-10, 1e-12).  A speedup greater than 1 indicates that NeurKItt outperforms GMRES.", "section": "6.1 Experiment Settings"}, {"figure_path": "cqfE9eYMdP/tables/tables_19_2.jpg", "caption": "Table 8: Performance of NeurKItt with changes in layers, modes, and widths. We use Darcy Flow problem with matrix size of 32400 in these experiments, where the tolerance is fixed to le-5 and the preconditioning is None. Subspace dimension is fixed to 20.", "description": "This table presents the results of experiments conducted to evaluate the impact of changing the number of layers, modes, and width in the Fourier Neural Operator (FNO) component of NeurKItt.  The Darcy Flow problem was used with a fixed matrix size of 32400, a tolerance of 1e-5, and no preconditioning. The subspace dimension was held constant at 20. The table shows the principal angle (a measure of how well the predicted subspace aligns with the true invariant subspace), the time speedup (the ratio of GMRES solving time to NeurKItt solving time), and the iteration speedup (the ratio of GMRES iterations to NeurKItt iterations) for different layer, mode, and width configurations.", "section": "G.1 Hyperparameters of Fourier Neural Operator"}, {"figure_path": "cqfE9eYMdP/tables/tables_20_1.jpg", "caption": "Table 9: The inference time spent of subspace prediction module", "description": "This table shows the inference time in milliseconds for the subspace prediction module.  The inference time is quite low across all three datasets (Helmholtz, Heat, and Darcy), ranging from approximately 6 to 8 milliseconds. This demonstrates the efficiency of the subspace prediction module, as this computation time is negligible compared to the overall solving time of the linear system.", "section": "H.1 Time Efficiency Analysis for Subspace Prediction Module"}, {"figure_path": "cqfE9eYMdP/tables/tables_20_2.jpg", "caption": "Table 10: The 120 epochs training cost for subspace prediction module.", "description": "This table presents the training time in hours for the subspace prediction module of the NeurKItt algorithm.  The training was conducted for 120 epochs on three different datasets: Helmholtz (62500), Heat (90000), and Darcy (160000). The table shows that the training time varies significantly across datasets, likely due to differences in dataset size and complexity. The Helmholtz dataset required significantly less training time (0.51 hours) compared to the Heat (6.48 hours) and Darcy (10.47 hours) datasets.", "section": "H.2 Training Time Analysis"}, {"figure_path": "cqfE9eYMdP/tables/tables_21_1.jpg", "caption": "Table 1: Comparison of our NeurKItt and GMRES computation time and iterations across datasets, preconditioning, and tolerances. The first column lists datasets with matrix size, and the next details tolerances. Results are displayed as \"time speedup / iteration speedup\".", "description": "This table compares the performance of NeurKItt and GMRES across various datasets, preconditioning techniques, and tolerance levels.  For each combination, it shows the speedup achieved by NeurKItt in terms of both computation time and the number of iterations required, relative to GMRES.  The speedup values represent the ratio of GMRES time/iterations to NeurKItt time/iterations. A value greater than 1 indicates that NeurKItt is faster or uses fewer iterations.", "section": "6.1 Experiment Settings"}, {"figure_path": "cqfE9eYMdP/tables/tables_22_1.jpg", "caption": "Table 1: Comparison of our NeurKItt and GMRES computation time and iterations across datasets, preconditioning, and tolerances. The first column lists datasets with matrix size, and the next details tolerances. Results are displayed as \"time speedup / iteration speedup\".", "description": "This table presents a comparison of the performance of NeurKItt and GMRES across various datasets, preconditioning techniques, and tolerance levels.  The \"time speedup\" represents the ratio of GMRES solving time to NeurKItt solving time, indicating how much faster NeurKItt is. Similarly, \"iteration speedup\" shows the ratio of the number of iterations used by GMRES to the number used by NeurKItt.  The data demonstrates NeurKItt's acceleration capabilities under different conditions.", "section": "6.1 Experiment Settings"}, {"figure_path": "cqfE9eYMdP/tables/tables_23_1.jpg", "caption": "Table 1: Comparison of our NeurKItt and GMRES computation time and iterations across datasets, preconditioning, and tolerances. The first column lists datasets with matrix size, and the next details tolerances. Results are displayed as \"time speedup / iteration speedup\".", "description": "This table presents a comparison of the performance of NeurKItt and GMRES, two algorithms used for solving linear systems.  The comparison is done across various datasets (Darcy, Heat, Helmholtz), preconditioning techniques (None, Jacobi, BJacobi, SOR, ASM, ICC, ILU), and tolerances (1e-2, 1e-4, 1e-7, 1e-10, 1e-12).  The results are expressed as the speedup factor achieved by NeurKItt relative to GMRES in both computation time and number of iterations.", "section": "6.1 Experiment Settings"}, {"figure_path": "cqfE9eYMdP/tables/tables_24_1.jpg", "caption": "Table 1: Comparison of our NeurKItt and GMRES computation time and iterations across datasets, preconditioning, and tolerances. The first column lists datasets with matrix size, and the next details tolerances. Results are displayed as \"time speedup / iteration speedup\".", "description": "This table compares the performance of NeurKItt and GMRES in solving linear systems.  It shows the time speedup and iteration speedup achieved by NeurKItt relative to GMRES across various datasets (Darcy, Heat, Helmholtz), preconditioning techniques (None, Jacobi, BJacobi, SOR, ASM, ICC, ILU), and tolerance levels (1e-2, 1e-4, 1e-7, 1e-10, 1e-12).  Higher values indicate better performance for NeurKItt.", "section": "6.1 Experiment Settings"}]