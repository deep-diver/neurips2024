[{"figure_path": "D4yRz3s7UL/figures/figures_1_1.jpg", "caption": "Figure 1: Token depth distribution in terms of transformer blocks for a clean (top) and adversarial (bottom) image for three TS mechanisms (b)-(d). The colors indicate the maximum depth each token reaches before being discarded. The adversarial image is crafted using the single-image attack variant (Section 4.1), which results in worst-case performance.", "description": "This figure visualizes the token depth distribution across transformer blocks for both clean and adversarial images using three different token sparsification mechanisms: ATS, AdaViT, and A-ViT.  The color intensity represents the maximum depth a token reaches before being discarded by the sparsification mechanism. The adversarial image was generated using the single-image attack variant described in section 4.1, designed to trigger worst-case performance by exhausting the system resources. Comparing the clean and adversarial examples shows how the attack impacts token sparsification, which ultimately affects performance.", "section": "1 Introduction"}, {"figure_path": "D4yRz3s7UL/figures/figures_7_1.jpg", "caption": "Figure 3: Distribution of activated tokens in each ATS block on clean and adversarial images.", "description": "This figure shows the distribution of activated tokens for each of the 9 transformer blocks (block 4 through block 12) in the ATS mechanism.  The blue line represents the distribution of tokens used in the clean images, and the orange line represents the distribution of tokens used in the adversarial images created by the DeSparsify attack. The figure shows that the DeSparsify attack increases the number of tokens used in nearly all blocks, especially in the later blocks (block 10 through 12). This illustrates how the attack works to exhaust resources by preventing token sparsification.", "section": "5.2 Results"}, {"figure_path": "D4yRz3s7UL/figures/figures_8_1.jpg", "caption": "Figure 4: GFLOPS transferability results for the single-image variant attack. Ensemble refers to perturbations that were trained on all modules simultaneously.", "description": "This heatmap visualizes the transferability of adversarial examples generated by the DeSparsify attack across different token sparsification mechanisms (ATS, AdaViT, A-ViT). Each cell shows the GFLOPS (giga floating-point operations per second) achieved when a perturbation trained on the mechanism specified by the row is applied to a model using the mechanism specified by the column.  The \"Ensemble\" row shows results when the perturbation is trained on all three mechanisms at once.  \"Clean\" and \"Clean w/o\" represent performance on clean images with and without sparsification respectively.  The darker the color, the more GFLOPS were achieved.", "section": "5.2 Results"}, {"figure_path": "D4yRz3s7UL/figures/figures_13_1.jpg", "caption": "Figure 5: Ablation study examining the effect of the \u03bb value on the GFLOPS and accuracy for the ATS.", "description": "This ablation study shows the impact of the scaling hyperparameter \u03bb (Accuracy Loss Component Weight) on both the GFLOPS (Giga Floating-Point Operations per Second, a measure of computational performance) and the accuracy of the model.  The plot shows an optimal \u03bb value exists, balancing model accuracy and increased computational cost induced by the attack. Using too small a \u03bb leads to high computational cost, but low model accuracy, while too large a \u03bb sacrifices some increase in computational cost for good accuracy. The best \u03bb value provides a nearly optimal combination, maximizing computational cost while maintaining good accuracy.", "section": "5 Evaluation"}, {"figure_path": "D4yRz3s7UL/figures/figures_17_1.jpg", "caption": "Figure 6: Adversarial examples from the baselines and our DeSparsify attacks. The leftmost column shows the clean images. In the next three columns, we show adversarial examples from random, standard PGD and sponge examples, respectively. The last four columns include adversarial examples from the different DeSparsify variants.", "description": "This figure visualizes the adversarial examples generated by different attack methods.  The first column shows the original, clean images. Subsequent columns show examples generated by random noise, standard Projected Gradient Descent (PGD), Shumailov's sponge attack, and four variants of the DeSparsify attack (single-image, ensemble, universal, and universal patch). This allows comparison of the visual differences between adversarial examples generated by various attacks.", "section": "D Attack Visualizations"}]