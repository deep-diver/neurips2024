[{"figure_path": "F8aSOovlEP/tables/tables_7_1.jpg", "caption": "Table 1: Main results. Experiments validate the effectiveness of our VGCM framework in providing causal relations in multi-event videos, outperforming GPT-4o and VideoLLaVA by 5.7% and 4.1%, respectively. * indicates the fine-tuned result, and + indicates without causal inference.", "description": "This table presents the main results of the experiments conducted to evaluate the performance of the proposed Video Granger Causality Model (VGCM).  It compares VGCM's accuracy and structural Hamming distance (SHD) against several baseline models, including various LLMs (Large Language Models) and VLLMs (Vision-Language Models) both with and without fine-tuning,  and a random guess baseline. The results show that VGCM significantly outperforms these baselines in the multi-event video causal reasoning task. The table also highlights the impact of using causal inference techniques within the VGCM.", "section": "5.1 Main results"}, {"figure_path": "F8aSOovlEP/tables/tables_7_2.jpg", "caption": "Table 1: Main results. Experiments validate the effectiveness of our VGCM framework in providing causal relations in multi-event videos, outperforming GPT-4o and VideoLLaVA by 5.7% and 4.1%, respectively.  indicates the fine-tuned result, and + indicates without causal inference.", "description": "This table presents the main results of the experiments conducted to evaluate the performance of the proposed Video Granger Causality Model (VGCM) for multi-event video causal discovery.  It compares the accuracy of VGCM against several baseline models, including traditional multi-modal models, large language models (LLMs), and video-LLMs.  The results show that VGCM outperforms the state-of-the-art models, demonstrating its effectiveness in discovering causal relations within multi-event videos. The table also includes results with and without causal inference methods integrated into the VGCM, highlighting their impact on performance.", "section": "5.1 Main results"}, {"figure_path": "F8aSOovlEP/tables/tables_9_1.jpg", "caption": "Table 1: Main results. Experiments validate the effectiveness of our VGCM framework in providing causal relations in multi-event videos, outperforming GPT-4o and VideoLLaVA by 5.7% and 4.1%, respectively. * indicates the fine-tuned result, and + indicates without causal inference.", "description": "This table presents the main results of the experiments conducted to evaluate the performance of the proposed Video Granger Causality Model (VGCM). It compares the accuracy of VGCM against several baseline models, including various LLMs and VLLMs, in the task of multi-event causal discovery.  The table highlights that VGCM outperforms other models, particularly GPT-4 and VideoLLaVA, demonstrating its effectiveness in identifying causal relationships within long videos containing multiple events.  It also shows results with and without causal inference applied and indicates which models were fine-tuned.", "section": "5.1 Main results"}, {"figure_path": "F8aSOovlEP/tables/tables_18_1.jpg", "caption": "Table 8: VGCM performance with masked premise event caption input. * indicates 30 frames masked at the same time.", "description": "This table presents the results of an ablation study on the Video Granger Causality Model (VGCM).  The experiment explores the impact of masking different numbers of words from the caption of each premise event on the model's accuracy in predicting causal relationships. The accuracy is evaluated under different masking scenarios (2, 5, 8, and 11 words per event).  A separate row shows the accuracy when 30 frames are masked at the same time.", "section": "5.2 Ablation Study"}, {"figure_path": "F8aSOovlEP/tables/tables_18_2.jpg", "caption": "Table 9: VGCM performance with masked premise event visual input. * indicates 10 words masked at the same time.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of masking visual input (video frames) on the performance of the Video Granger Causality Model (VGCM).  The model's accuracy in predicting causal relations is measured under different levels of visual masking (5, 15, 20, and 40 frames masked per event). A comparison is made with a non-masked condition for baseline accuracy. The impact of simultaneous masking of both visual and textual data is also examined by comparing the accuracy of masking 20 frames and 10 words compared to only masking 20 frames.", "section": "5.2 Ablation Study"}]