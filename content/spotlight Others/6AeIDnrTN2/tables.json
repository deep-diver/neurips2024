[{"figure_path": "6AeIDnrTN2/tables/tables_5_1.jpg", "caption": "Table 1: Quantitative Comparisons in Real-world Large-scale Scenes: Voxel-based methods [12, 14] exhibit insufficient capacity for representing large-scale scenes and are unable to achieve real-time performance. Mip-NeRF360 [7] produces the highest visual quality, but requires over 16 seconds to render a single image. Our method strikes a balance among FPS, model size, and rendering quality, achieving the best balance among efficient representations. For fair metric and visual comparison, we re-trained 3D-GS and Compressed 3D-GS [34] using their original code and configurations on our platform (NVIDIA A6000 GPU), with results marked by *.", "description": "This table provides a quantitative comparison of LightGaussian against several state-of-the-art methods for large-scale scene representation on real-world datasets (Mip-NeRF 360 and Tanks & Temples).  The comparison includes metrics like FPS, model size (Size), PSNR, SSIM, and LPIPS, highlighting LightGaussian's superior balance of speed, size, and visual quality.", "section": "4 Experimental Results"}, {"figure_path": "6AeIDnrTN2/tables/tables_6_1.jpg", "caption": "Table 2: Ablation studies on the Gaussian Pruning & Recovery, SH Compactness, and the Vector Quantization. Scene: Room. Zero-shot Gaussian pruning leads degraded rendering quality (#2), but Co-adaptation can recover most of the scene details (#3). Directly eliminating high-order SH negatively affects the quality (#4), while distillation with pseudo-view helps to mitigate the gap (#5, #6). Codebook quantization further reduces the required model size and bandwidth (#7, #8).", "description": "This table presents the ablation study results for LightGaussian. It shows the impact of each component (Gaussian Pruning & Recovery, SH Compactness, and Vector Quantization) on the overall performance. The results are presented for a single scene ('Room') and demonstrate how each component contributes to the final model's size, speed, and quality.", "section": "4 Experiments"}, {"figure_path": "6AeIDnrTN2/tables/tables_6_2.jpg", "caption": "Table 3: Ablation study of the Gaussian Pruning & Recovery, by using different Gaussian attributes for computing its global significance score. By considering only the hit count of each Gaussian from training rays, the zero-shot pruning leads to inferior performance. Incorporating the opacity and volume drives us to a better criterion. The subsequent Gaussian Co-adaptation is used to recover most of the information loss from the pruning of redundant Gaussians.", "description": "This table presents an ablation study evaluating different criteria for Gaussian pruning in the LightGaussian model.  It compares using only hit counts, opacity, volume, and combinations thereof, with and without co-adaptation. The results demonstrate that incorporating opacity and volume into the significance calculation significantly improves performance, while co-adaptation helps mitigate losses from pruning.", "section": "4.3 Ablation Studies"}, {"figure_path": "6AeIDnrTN2/tables/tables_6_3.jpg", "caption": "Table 4: Ablation Study: Gaussian Attribute Vector Quantization (VQ). Quantizing all attributes to FP16 achieves a smaller model, but applying VQ to all attributes degrades modeling accuracy. Using Global Significance (GS) to target less crucial Gaussians mitigates this loss. Some attributes (e.g., scale) are sensitive to VQ, so we limit VQ to the SH coefficients. By combining VQ with significance scores, LightGaussian achieves an effective balance between model size and quality.", "description": "This table presents an ablation study on the impact of vector quantization (VQ) on model size and performance.  Different VQ strategies are compared, showing that applying VQ to all attributes significantly reduces accuracy, while selectively applying VQ to spherical harmonics (SH) coefficients based on their global significance produces a good balance between model size reduction and visual quality.  The final row shows the results after fine-tuning the model using the combined VQ and significance approach.", "section": "4 Experiments"}, {"figure_path": "6AeIDnrTN2/tables/tables_7_1.jpg", "caption": "Table 5: LightGaussian removes redundant neural Gaussians in Scaffold-GS, reducing model size and improving rendering speed. All experiments were rerun on our platform for fair comparison, with results averaged on the MipNeRF-360 dataset.", "description": "This table presents a comparison of the performance of Scaffold-GS alone and Scaffold-GS enhanced with LightGaussian. It demonstrates the effectiveness of LightGaussian in reducing model size and improving frame rates (FPS) while maintaining comparable visual quality (PSNR, SSIM, LPIPS).  The results are averages across the MipNeRF-360 dataset.", "section": "4.2 Experimental Results"}, {"figure_path": "6AeIDnrTN2/tables/tables_8_1.jpg", "caption": "Table 6: Ablation with varying SH compactness: With a Gaussian pruning ratio of 66% and SH reduced to 2 degrees, LightGaussian nearly maintains rendering quality (SSIM drops slightly from 0.927 to 0.926). Reducing SH further to 1 degree lowers SSIM to 0.923.", "description": "This table presents ablation study results focusing on the impact of reducing the degree of spherical harmonics (SH) coefficients in the LightGaussian model.  It shows the effect of reducing SH coefficients to 2 degrees and 1 degree while maintaining a 66% Gaussian pruning ratio.  The metrics evaluated are model size, frames per second (FPS), Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS). The results demonstrate the trade-off between model size, rendering speed, and visual quality when decreasing the SH degree.", "section": "4.3 Ablation Studies"}, {"figure_path": "6AeIDnrTN2/tables/tables_14_1.jpg", "caption": "Table 1: Quantitative Comparisons in Real-world Large-scale Scenes: Voxel-based methods [12, 14] exhibit insufficient capacity for representing large-scale scenes and are unable to achieve real-time performance. Mip-NeRF360 [7] produces the highest visual quality, but requires over 16 seconds to render a single image. Our method strikes a balance among FPS, model size, and rendering quality, achieving the best balance among efficient representations. For fair metric and visual comparison, we re-trained 3D-GS and Compressed 3D-GS [34] using their original code and configurations on our platform (NVIDIA A6000 GPU), with results marked by *.", "description": "This table quantitatively compares LightGaussian with several state-of-the-art methods for large-scale scene rendering.  It focuses on metrics like model size, rendering speed (FPS), PSNR, SSIM, and LPIPS, showcasing LightGaussian's superior balance between speed, size, and visual quality compared to alternatives like Plenoxels, INGP, Mip-NeRF 360, and 3D-GS.", "section": "4 Experimental Results"}, {"figure_path": "6AeIDnrTN2/tables/tables_16_1.jpg", "caption": "Table 1: Quantitative Comparisons in Real-world Large-scale Scenes: Voxel-based methods [12, 14] exhibit insufficient capacity for representing large-scale scenes and are unable to achieve real-time performance. Mip-NeRF360 [7] produces the highest visual quality, but requires over 16 seconds to render a single image. Our method strikes a balance among FPS, model size, and rendering quality, achieving the best balance among efficient representations. For fair metric and visual comparison, we re-trained 3D-GS and Compressed 3D-GS [34] using their original code and configurations on our platform (NVIDIA A6000 GPU), with results marked by *.", "description": "This table provides a quantitative comparison of LightGaussian against several state-of-the-art methods for large-scale scene representation on the Mip-NeRF 360 and Tanks & Temples datasets.  The comparison includes metrics such as model size, rendering speed (FPS), PSNR, SSIM, and LPIPS.  It highlights LightGaussian's superior balance between speed, model size, and visual quality, outperforming other methods in terms of overall efficiency.", "section": "4 Experimental Results"}, {"figure_path": "6AeIDnrTN2/tables/tables_16_2.jpg", "caption": "Table 1: Quantitative Comparisons in Real-world Large-scale Scenes: Voxel-based methods [12, 14] exhibit insufficient capacity for representing large-scale scenes and are unable to achieve real-time performance. Mip-NeRF360 [7] produces the highest visual quality, but requires over 16 seconds to render a single image. Our method strikes a balance among FPS, model size, and rendering quality, achieving the best balance among efficient representations. For fair metric and visual comparison, we re-trained 3D-GS and Compressed 3D-GS [34] using their original code and configurations on our platform (NVIDIA A6000 GPU), with results marked by *.", "description": "This table presents a quantitative comparison of LightGaussian against several state-of-the-art methods for large-scale scene rendering.  It compares model size, rendering speed (FPS), PSNR, SSIM, and LPIPS scores across different methods on the Mip-NeRF 360 and Tanks & Temples datasets.  The results highlight LightGaussian's balance between speed, model size, and visual quality.", "section": "4 Experimental Results"}, {"figure_path": "6AeIDnrTN2/tables/tables_16_3.jpg", "caption": "Table 1: Quantitative Comparisons in Real-world Large-scale Scenes: Voxel-based methods [12, 14] exhibit insufficient capacity for representing large-scale scenes and are unable to achieve real-time performance. Mip-NeRF360 [7] produces the highest visual quality, but requires over 16 seconds to render a single image. Our method strikes a balance among FPS, model size, and rendering quality, achieving the best balance among efficient representations. For fair metric and visual comparison, we re-trained 3D-GS and Compressed 3D-GS [34] using their original code and configurations on our platform (NVIDIA A6000 GPU), with results marked by *.", "description": "This table compares the performance of LightGaussian against other state-of-the-art methods for large-scale scene rendering.  It shows a comparison of model size, rendering speed (FPS), PSNR, SSIM, and LPIPS scores on standard benchmark datasets. LightGaussian achieves a good balance between model size, speed, and visual quality compared to other methods.", "section": "4 Experimental Results"}, {"figure_path": "6AeIDnrTN2/tables/tables_18_1.jpg", "caption": "Table 1: Quantitative Comparisons in Real-world Large-scale Scenes: Voxel-based methods [12, 14] exhibit insufficient capacity for representing large-scale scenes and are unable to achieve real-time performance. Mip-NeRF360 [7] produces the highest visual quality, but requires over 16 seconds to render a single image. Our method strikes a balance among FPS, model size, and rendering quality, achieving the best balance among efficient representations. For fair metric and visual comparison, we re-trained 3D-GS and Compressed 3D-GS [34] using their original code and configurations on our platform (NVIDIA A6000 GPU), with results marked by *.", "description": "This table compares the performance of LightGaussian against other state-of-the-art methods for large-scale scene rendering.  Metrics include model size, frames per second (FPS), Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS).  It highlights LightGaussian's superior balance between speed, size, and visual quality.", "section": "4 Experimental Results"}]