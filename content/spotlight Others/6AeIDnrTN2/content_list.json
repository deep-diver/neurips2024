[{"type": "text", "text": "LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and ${\\bf200+}$ FPS ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhiwen Fan1\u2020,\u2217 Kevin Wang1\u2217, Kairun Wen2, Zehao ${\\mathbf Z}{\\mathbf h}{\\mathbf u}^{1}$ , Dejia $\\mathbf{X}\\mathbf{u}^{1}$ , Zhangyang Wang1 ", "page_idx": 0}, {"type": "text", "text": "1The University of Texas at Austin 2XMU {zhiwenfan,kevinwang.1839,atlaswang}@utexas.edu ", "page_idx": 0}, {"type": "text", "text": "Project Website: https://lightgaussian.github.io ", "page_idx": 0}, {"type": "image", "img_path": "6AeIDnrTN2/tmp/5e87efe8869c75be0a9a3e635480ef13b1bde3f2d0d3e6c8694306549e4b7bb3.jpg", "img_caption": ["Figure 1: Compressibility and Speed: LightGaussian compacts 3D Gaussians, reducing storage from 782MB to 45MB and boosting FPS from 144 to 237, while maintaining visual quality. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advances in real-time neural rendering using point-based techniques have enabled broader adoption of 3D representations. However, foundational approaches like 3D Gaussian Splatting impose substantial storage overhead, as Structure-fromMotion (SfM) points can grow to millions, often requiring gigabyte-level disk space for a single unbounded scene. This growth presents scalability challenges and hinders splatting efficiency. To address this, we introduce LightGaussian, a method for transforming 3D Gaussians into a more compact format. Inspired by Network Pruning, LightGaussian identifies Gaussians with minimal global significance on scene reconstruction, and applies a pruning and recovery process to reduce redundancy while preserving visual quality. Knowledge distillation and pseudo-view augmentation then transfer spherical harmonic coefficients to a lower degree, yielding compact representations. Gaussian Vector Quantization, based on each Gaussian\u2019s global significance, further lowers bitwidth with minimal accuracy loss. LightGaussian achieves an average $\\mathbf{15}\\times$ compression rate while boosting FPS from 144 to 237 within the 3D-GS framework, enabling efficient complex scene representation on the Mip-NeRF 360 and Tank & Temple datasets. The proposed Gaussian pruning approach is also adaptable to other 3D representations (e.g., Scaffold-GS), demonstrating strong generalization capabilities. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Novel view synthesis (NVS) aims to generate photo-realistic images of a 3D scene from unobserved viewpoints, given a set of calibrated multi-view images. This capability has widespread applications in virtual reality [1], augmented reality [2], digital twins [3], and autonomous driving [4]. Neural Radiance Fields (NeRFs) [5\u20137] have shown promise in 3D modeling from multi-view images by mapping 3D locations and view directions to view-dependent color and volumetric density, with pixel intensity rendered through volume rendering [8]. However, NeRF and its variants face limitations in rendering speed, limiting their deployment in real-world scenarios. To address this, voxel-based representations [9\u201313], hash grids [14], and neural light fields [15] have been developed. Despite improvements, these methods often compromise between quality and speed. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Recent progress in point-based 3D Gaussian Splatting (3D-GS) [16] has enabled real-time rendering with photo-realistic quality for complex scenes. By representing the scene with explicit 3D Gaussians and using a splatting technique [17], 3D-GS balances speed and quality, making it suitable for large-scale scenarios like digital twins and autonomous driving. However, point-based methods incur high storage costs, as each point and its attributes require independent storage. Additionally, heuristic densification of sparse SfM points into dense Gaussians often results in overparameterization, leading to excessive storage and slower rendering speeds. For instance, a typical unbounded 360-degree scene in 3D-GS [7] may require over 1GB of storage (e.g., 1.4GB for the Bicycle scene). ", "page_idx": 1}, {"type": "text", "text": "In this paper, we address storage and rendering speed issues by developing a compact representation that retains the original rendering quality. The heuristic densification process in 3D-GS results in significant redundancy. Our method, LightGaussian, reduces redundancy by targeting both Gaussian count (N) and feature dimension (F) through a comprehensive pipeline: ", "page_idx": 1}, {"type": "text", "text": "\u2022 To reduce Gaussian count (N), we propose a Gaussian Pruning and Recovery step, identifying and removing Gaussians with minimal impact on visual quality, followed by recovery to ensure smooth adaptation. \u2022 For compressing features (F), we introduce an SH Distillation process to compact higherdegree spherical harmonic (SH) coefficients, supported by pseudo-view augmentation. Additionally, we employ Vector Quantization (VQ) to adaptively select a codebook of Gaussian attributes (e.g, positions, scales, and rotations), reducing precision for less significant features and applying quantization-aware fine-tuning to maintain quality. ", "page_idx": 1}, {"type": "text", "text": "In summary, LightGaussian achieves substantial compression (e.g., from 782MB to 45MB) while maintaining visual fidelity (SSIM decrease of only 0.007 on Mip-NeRF 360). Rendering speed also improves, reaching over 200 FPS on complex scenes. Our Gaussian Pruning and Recovery method generalizes well, enhancing performance across different 3D Gaussian formats, such as Scaffold-GS. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Efficient 3D Scene Representations. Neural radiance fields (NeRF) [5] use multi-layer perceptrons (MLPs) to represent scenes, setting new standards for view synthesis quality. However, NeRF models face challenges with slow inference speeds, limiting practical use. Efforts to address this have explored ray re-parameterizations [6, 7], explicit spatial data structures [18\u201321, 9, 14, 13], caching and distillation techniques [15, 22\u201324], and ray-based representations [25, 26]. Nevertheless, achieving real-time rendering remains challenging for NeRF methods, especially for large-scale scenes where multiple queries per pixel hinder performance. ", "page_idx": 1}, {"type": "text", "text": "Point-based representations, like Gaussians, have been explored in various applications, such as shape reconstruction [27], molecular modeling [28], and point-cloud replacement [29], as well as in shadow [30] and cloud rendering [31]. Pulsar [32] demonstrated efficient sphere rasterization, while 3D Gaussian Splatting (3D-GS) [16] applies anisotropic Gaussians [33] with tile-based sorting to achieve real-time speed and quality comparable to MLP-based methods like Mip-NeRF 360 [7]. ", "page_idx": 1}, {"type": "text", "text": "Despite its strengths, 3D-GS has high storage demands due to the extensive attributes stored with each Gaussian, often requiring gigabytes per scene and hindering rendering efficiency. Recent concurrent works address these issues by using region-based vector quantization [34], K-means codebooks [35], view-direction exclusion [36], or learned binary masks and grid-based neural networks instead of spherical harmonics (SHs) [37] to reduce model size. ", "page_idx": 1}, {"type": "text", "text": "Pruning and Quantization. Model pruning reduces neural network complexity by removing nonsignificant parameters, balancing performance and resource use. Unstructured [38] and structured pruning [39, 40] eliminate parameters at the weight level and neuron/channel levels, resulting in a smaller, more efficient architecture. Iterative magnitude pruning (IMP), where low-magnitude weights are progressively removed, has proven effective in methods like lottery ticket rewinding [41, 42]. ", "page_idx": 1}, {"type": "image", "img_path": "6AeIDnrTN2/tmp/f18dbd4cc02c81974e1dcf027aac448bae0e8436b8831e08e76d20b54df9b466.jpg", "img_caption": ["Figure 2: Pipeline of LightGaussian. 3D Gaussians are optimized from multi-view images and SfM points. LightGaussian calculates each Gaussian\u2019s global significance on training data, pruning those with the least significance. Next, SH coefficients are distilled into a compact format using synthesized pseudo-views. Finally, vector quantization, including codebook initialization and assignment, reduces model bandwidth. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Vector quantization (VQ) [43] compresses data by representing it with discrete codebook entries (tokens), using mean square error (MSE) to select the closest match in the codebook for each data vector. Prior studies [44\u201346] have shown that learning discrete, compact representations enhances visual understanding and model robustness. VQ has thus been widely applied in image synthesis [47], text-to-image generation [48], and novel view synthesis [18, 49, 50]. ", "page_idx": 2}, {"type": "text", "text": "Knowledge Distillation. Knowledge Distillation (KD) [51\u201354, 15] trains a smaller student model by transferring knowledge from a larger teacher model [55]. In 3D vision, KD has been applied to neural scene representations, leveraging view renderings to incorporate 2D priors. For example, DreamFusion [56] and NeuralLift-360 [57] use pre-trained diffusion models for 3D generation, while models like DFF [58], NeRF-SOS [59], INS [60], and SA3D [61] distill 2D image feature extractors to 3D tasks. KD has also been central to compressing scene representation models [15, 24]. ", "page_idx": 2}, {"type": "text", "text": "3 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Overview. The LightGaussian framework is illustrated in Fig. 2. The 3D-GS model is trained on multi-view images, initialized from SfM point clouds, and expanded to millions of Gaussians to represent the scene comprehensively. Our pipeline then processes the 3D-GS model into a compact format using Gaussian Prune and Recovery to reduce the number of Gaussians, SH Distillation to eliminate redundant SHs while retaining key lighting information, and Vector Quantization to store Gaussians with lower bit-width. ", "page_idx": 2}, {"type": "text", "text": "3.1 Background: 3D Gaussian Splatting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3D Gaussian Splatting (3D-GS) [16] is an explicit point-based 3D scene representation, utilizing Gaussians with various attributes to model the scene. When representing a complex real-world scene, 3D-GS is initialized from a sparse point cloud generated by SfM, and Gaussian Densification is applied to increase the Gaussian counts that are used for handling small-scale geometry insufficiently covered and over reconstruction. Formally, each Gaussian is characterized by a covariance matrix $\\Sigma$ and a center point $\\mathbf{\\deltaX}$ , which is referred to as the mean value of the Gaussian: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname{G}(X)=e^{-{\\frac{1}{2}}X^{T}\\Sigma^{-1}X},\\Sigma=\\operatorname{RSS}^{T}\\!\\mathbf{R}^{T},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\Sigma$ can be decomposed into a scaling matrix $\\mathbf{S}$ and a rotation matrix $\\mathbf{R}$ . ", "page_idx": 2}, {"type": "text", "text": "The complex directional appearance is modeled by an additional property, Spherical Harmonics (SH), with $n$ coefficients, $\\{c_{i}\\in\\mathbb{R}^{3}|i=1,2,\\dots,n\\}$ where $n=D^{2}$ represents the number of coefficients of SH with degree $\\dot{D}$ . A higher degree $D$ equips 3D-GS with a better capacity to model the viewdependent effect but causes a significantly heavier attribute load. ", "page_idx": 2}, {"type": "text", "text": "When rendering 2D images from the 3D Gaussians, the technique of splatting [62, 17] is employed for the Gaussians within the camera planes. With a viewing transform denoted as $W$ and the Jacobian of the affine approximation of the projective transformation represented by $_{J}$ , the covariance matrix $\\Sigma^{\\prime}$ in camera coordinates can be computed as $\\begin{array}{r}{\\Sigma^{\\prime}=J W\\Sigma W^{T}J^{T}}\\end{array}$ . Specifically, for each pixel, the color and opacity of all the Gaussians are computed using the Gaussian\u2019s representation Eq. 1. The blending of $N$ ordered points that overlap the pixel is given by the formula: ", "page_idx": 2}, {"type": "image", "img_path": "6AeIDnrTN2/tmp/f3c37aad290ffb11a58d9918b7b1084ab545491298aa7191dd01e08ade190aba.jpg", "img_caption": ["Figure 3: Zero-shot Opacity-based Pruning. A significant number of Gaussians exhibit small opacity values (top). Simply utilizing Gaussian opacity as an indicator for pruning the least important Gaussians results in the rendered image losing intricate details (bottom), with the PSNR dropping from 27.2 to 25.3. This has inspired us to find better criteria to measure global significance in terms of rendering quality. The accumulated Probability Density Function(PDF) is equal to 1. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nC=\\sum_{i\\in N}c_{i}\\alpha_{i}\\prod_{j=1}^{i-1}(1-\\alpha_{i}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $c_{i},\\alpha_{i}$ represents the view-dependent color and opacity, calculated from a 2D Gaussian with covariance $\\Sigma$ multiplied by an optimizable per-3D Gaussian\u2019s opacity. In summary, each Gaussian point is characterized by attributes including: position $\\pmb{X}\\in\\mathbb{R}^{3}$ , color defined by spherical harmonics coefficients $\\mathrm{SH}\\in\\mathbb{R}^{(k+1)^{2}}\\times3$ (where $k$ represents the degrees of freedom), opacity $\\alpha\\in\\mathbb{R}$ , rotation factor $\\mathbf{R}\\in\\mathbb{R}^{4}$ , and scaling factor $\\mathbf{S}\\in\\mathbb{R}^{3}$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Gaussian Pruning & Recovery ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Gaussian densification [16], which clones and refines the initial SfM point cloud, enhances small-scale geometry and detailed scene appearance by improving coverage. While this approach significantly boosts reconstruction quality, it increases the number of Gaussians from thousands to millions after optimization, resulting in substantial storage demands.Inspired by neural network pruning techniques [63] that remove less impactful neurons while preserving overall performance, we propose a tailored pruning strategy for 3D Gaussian Splatting to reduce over-parameterized points while maintaining accuracy. Identifying redundant yet recoverable Gaussians is essential to our approach. However, pruning Gaussians based on simple criteria (e.g., point opacity) risks degrading modeling performance, as it may eliminate essential scene details, as shown in Fig. 3. ", "page_idx": 3}, {"type": "text", "text": "Global Significance Calculation. Inspired by Eq. 2, we go beyond using Gaussian opacity alone to assess significance by evaluating 3D Gaussians within the view frustum, projecting them onto the camera viewpoint for rendering. The initial significance score of each Gaussian (denoted as ${\\bf G}(X_{j}))$ ) can then be quantified based on its contribution to each pixel (ray, $r_{i}$ ) across all training views using the criteria $\\bar{\\mathbb{1}}({\\bf G}({\\bf X}_{j}),r_{i})$ whether they intersect or not. Consequently, we iterate over all training pixels to calculate the hit count of each Gaussian. The score is further refined by the adjusted 3D Gaussian\u2019s volume $\\gamma(\\pmb{\\Sigma}_{j})$ and 3D Gaussian\u2019s opacity $\\sigma_{j}$ , as they all contribute for the rendering formula. The volume calculation equation of the $j$ -th 3D Guassian is $\\begin{array}{r}{\\mathrm{V}(\\Sigma_{j})=\\frac{4}{3}\\pi a b c.}\\end{array}$ , where abc are the 3 dimensions of Scale (S). We also consider the transmittance $\\mathbf{T}$ is calculated with one subtract all the 3D Gaussian\u2019s opacity the ray hit before the $j$ -th 3D Gaussian, $\\begin{array}{r}{\\mathbf{T}=\\prod_{i=1}^{j-1}\\left(1-\\sigma_{i}\\right)}\\end{array}$ . Finally, the global significance score can be summarized as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{GS}_{\\mathrm{j}}=\\sum_{i=1}^{M H W}\\mathbb{1}(\\mathrm{G}(X_{j}),r_{i})\\cdot\\sigma_{j}\\cdot\\mathbf{T}\\cdot\\gamma(\\Sigma_{j}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $j$ is the Gaussian index, $i$ denotes a pixel, and $M,H$ , and $W$ represent the number of training views, image height, and width, respectively. $\\mathbb{1}$ is an indicator function that determines whether a Gaussian intersects a given ray. However, using Gaussian volume alone tends to overemphasize background Gaussians, leading to excessive pruning of Gaussians modeling fine geometry. Thus, we propose a more adaptive approach to measuring volume dimensions: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\gamma(\\Sigma)=\\left(\\mathrm{V}_{\\mathrm{norm}}\\right)^{\\beta},}\\\\ {\\mathrm{V}_{\\mathrm{norm}}=\\operatorname*{min}\\left(\\frac{\\mathrm{V}\\left(\\Sigma\\right)}{\\mathrm{V}_{\\mathrm{max90}}},1\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, the calculated Gaussian volume is firstly normalized by the $90\\%$ largest of all sorted Gaussians, clipping the range between 0 and 1, to avoid excessive floating Gaussians derived from vanilla 3D-GS. The $\\beta$ is introduced to provide additional flexibility. ", "page_idx": 4}, {"type": "text", "text": "Gaussian Co-adaptation. We rank all Gaussians by their global significance scores to quantitatively guide pruning of the lower-ranked Gaussians. The remaining Gaussians are then jointly adapted by fine-tuning their attributes\u2014without additional densification\u2014to offset the minor loss from pruning. This adaptation is performed using photometric loss on the original training views, aligned with 3D-GS training for 5,000 iterations. ", "page_idx": 4}, {"type": "text", "text": "3.3 Distilling into Compact SHs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the uncompressed Gaussian Splat representation, a substantial 81.3 percent of the feature dimension is occupied by Spherical Harmonics (SH) coefficients, requiring $(45\\substack{+3})$ floating-point values per splat. Directly reducing the SH degree can save disk space but also diminishes surface \u201cshininess\u201d and affects specular reflections. ", "page_idx": 4}, {"type": "text", "text": "To balance model size with scene quality, we introduce a knowledge distillation approach. Knowledge is distilled from a teacher model with full-degree SHs to a student model with truncated, lower-degree SHs. Supervision is based on the difference in predicted pixel intensities between the two models, with images synthesized at camera positions by sampling around each training view according to a Gaussian distribution: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{L}_{\\mathrm{distill}}=\\frac{1}{H W}\\sum_{i=1}^{H W}\\left\\|C_{\\mathrm{teacher}}(\\boldsymbol{r}_{i};[{\\bf R}|{\\bf t}])-C_{\\mathrm{sudent}}(\\boldsymbol{r}_{i};[{\\bf R}|{\\bf t}])\\right\\|_{2}^{2}.}\\\\ {\\displaystyle\\mathbf{t}_{\\mathrm{pseudo}}=\\mathbf{t}_{\\mathrm{train}}+\\mathcal{N}(0,\\sigma^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{R}$ and $\\mathbf{t}$ denote rendering camera rotation and position, $\\mathbf{t}_{\\mathrm{pseudo}}$ and $\\mathbf{t}_{\\mathrm{train}}$ represent the newly synthesized and training camera positions, respectively. $\\mathcal{N}$ denotes a Gaussian distribution with mean 0 and variance $\\sigma^{2}$ , which is added to the original position to generate the new position. ", "page_idx": 4}, {"type": "text", "text": "3.4 Gaussian Attributes Vector Quantization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Vector quantization (VQ) clusters voxels into compact codebooks, enabling high compression rates. However, quantizing Gaussian attributes in a point-based, inherently non-Euclidean representation poses notable challenges. Our empirical findings indicate that applying quantization across all elements\u2014especially for attributes like position, rotation, and scale\u2014results in significant accuracy losses and a marked reduction in precision when represented discretely. ", "page_idx": 4}, {"type": "text", "text": "We propose applying VQ to the Spherical Harmonics (SH) coefficients, based on the assumption that a subgroup of 3d Gaussians typically exhibits a similar appearance. Fundamentally, VQ segments the Gaussians $\\mathcal{G}\\,=\\,\\{{\\bf g}_{1},{\\bf g}_{2},\\ldots,{\\bf g}_{N}\\}$ fgfgfg2N(here we apply it on SH) to the $\\mathbf{K}$ codes in the codebook $\\mathcal{C}=\\{\\mathbf{c}_{1},\\mathbf{c}_{2},\\ldots,\\mathbf{c}_{K}\\}$ \\{ \\mathb {c}_1, \\mathb {c}_, \\ldots , \\mathb {c}_K \\} ,fff2 where each $\\mathbf{g}_{j}$ $\\mathbf{c}_{k}\\in\\mathbb{R}^{d}$ ijdkand $\\ K\\ll\\Nu$ . $d$ means SH dimension. We aim to strike a balance between rendering quality loss and compression rate by leveraging the pre-computed significance score from Eq. 3. Based on this, we apply VQ selectively on the least significant elements in the Spherical Harmonics (SHs). Specifically, we initialize $\\mathcal{C}$ via K-means, iteratively sample a batch of $\\mathcal{G}$ , associates them to the closest codes by euclidean distance, and update each $\\mathbf{c}_{k}$ via moving average rule: $\\begin{array}{r}{\\mathbf{c}_{k}=\\lambda_{d}\\cdot\\mathbf{c}_{k}+(1-\\lambda_{d})\\cdot1/\\mathbf{T}_{k}\\cdot\\overleftarrow{\\sum_{\\mathbf{g}_{j}\\in\\mathcal{R}(\\mathbf{c}_{k})}\\mathbf{G}\\mathbf{S}_{j}}\\cdot\\mathbf{g}_{j}}\\end{array}$ , where $\\begin{array}{r}{\\mathbf{T}_{k}=\\sum_{\\mathbf g{j}\\in\\mathcal{R}(\\mathbf{c}_{k})}\\mathbf G\\mathbf S_{j}}\\end{array}$ is the significance score (Eq. 3) which is assigned to the code vector $\\mathbf{c}_{k}$ , $\\mathcal{R}(\\mathbf{c}_{k})$ is the set of Gaussians associated to the $k$ -th code. $\\lambda_{d}=0.8$ represents the decay value, which is utilized to update the ", "page_idx": 4}, {"type": "table", "img_path": "6AeIDnrTN2/tmp/06ea57943b43e84611ed562968514ab0a36dd895dfbe6dcfe4b89ad65e257661.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Table 1: Quantitative Comparisons in Real-world Large-scale Scenes: Voxel-based methods [12, 14] exhibit insufficient capacity for representing large-scale scenes and are unable to achieve real-time performance. Mip-NeRF360 [7] produces the highest visual quality, but requires over 16 seconds to render a single image. Our method strikes a balance among FPS, model size, and rendering quality, achieving the best balance among efficient representations. For fair metric and visual comparison, we re-trained 3D-GS and Compressed 3D-GS [34] using their original code and configurations on our platform (NVIDIA A6000 GPU), with results marked by \u2217. ", "page_idx": 5}, {"type": "image", "img_path": "6AeIDnrTN2/tmp/294cff9e3b6c9295b65ca9778cf88997a3ce5cfe6579e063452aecbe180d0c71.jpg", "img_caption": ["Figure 4: Visual Comparisons. We compare LightGaussian with the vanilla 3D-GS [16], displaying a residual map between predictions and ground truth scaled from 0 to 127 to emphasize differences. LightGaussian retains most specular reflections (yellow boxes) in its compact format, with a slight change in lightness visible in the bottom white box. The residual maps illustrate discrepancies between rendered images and ground-truth RGB values, where darker areas indicate closer alignment. For a full dynamic viewpoint comparison, please see our supplementary video. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "code vector using a moving average. We fine-tune the codebook for 5,000 iterations, while fixing the gaussian-to-codebook mapping. We disable additional clone/split operations and leverage photometric loss on the training views. To preserve essential attributes, including Spherical Harmonics with higher global significance scores, along with Gaussian position, shape, rotation, and opacity, we skip VQ for these elements and store them directly in float16 format. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Settings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets and Metrics. We conduct comparisons using the scene-scale view synthesis dataset provided by Mip-NeRF360 [64], which comprises nine real-world large-scale scenes, including five unbounded outdoor and four indoor settings with complex backgrounds. In addition, we utilize the Tanks and Temples dataset [65], a comprehensive unbounded dataset, and select the same scenes as used in [16]. Performance metrics on synthetic object-level datasets will be detailed in the supplementary materials. We report metrics including the peak signal-to-noise ratio (PSNR), structural similarity (SSIM), and perceptual similarity as measured by LPIPS [66]. ", "page_idx": 5}, {"type": "text", "text": "Compared Baselines. We compare our approach with methods suited for large-scale scene modeling, including Plenoxel [12], Mip-NeRF360 [7], and 3D-GS [16]. Additionally, we evaluate against efficient techniques like Instant-NGP [14], which uses a hash grid for storage efficiency, and VQ-DVGO [49], which extends DVGO [9] with voxel pruning and VQ for optimized representation. ", "page_idx": 5}, {"type": "table", "img_path": "6AeIDnrTN2/tmp/7c6e235e094081f5555fef80648fc0369d38c704d03c40b7560d1418951f051e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "6AeIDnrTN2/tmp/a4676ecae7c335889a6d9dd475371a62d7742f4b6c02bc23130aed3257fea777.jpg", "table_caption": ["Table 2: Ablation studies on the Gaussian Pruning & Recovery, SH Compactness, and the Vector Quantization. Scene: Room. Zero-shot Gaussian pruning leads degraded rendering quality (#2), but Co-adaptation can recover most of the scene details (#3). Directly eliminating high-order SH negatively affects the quality $(\\#4)$ , while distillation with pseudo-view helps to mitigate the gap (#5, #6). Codebook quantization further reduces the required model size and bandwidth (#7, #8). "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "6AeIDnrTN2/tmp/f6ff8f58ca8b172bfd68d7dc12bf6be2ee94fc328a3022760eb7aa2dfd0c639a.jpg", "table_caption": ["Table 3: Ablation study of the Gaussian Pruning & Recovery, by using different Gaussian attributes for computing its global significance score. By considering only the hit count of each Gaussian from training rays, the zero-shot pruning leads to inferior performance. Incorporating the opacity and volume drives us to a better criterion. The subsequent Gaussian Co-adaptation is used to recover most of the information loss from the pruning of redundant Gaussians. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Implementation Details. Our framework is implemented in PyTorch and integrates the differentiable Gaussian rasterization technique from 3D-GS [16]. All performance evaluations are conducted on an A6000 GPU. In the Global Significance Calculation phase, we assign a power value of 0.1 in Eq. 4 and proceed to fine-tune the model for 5,000 steps during the Gaussian Co-adaptation process. For SH distillation, we downscale the 3-degree SHs to 2-degree, thereby reducing 21 elements for each Gaussian. This is further optimized by setting $\\sigma$ to 0.1 in the pseudo view synthesis stage. In the Gaussian VQ step, the codebook size is configured to 8192, selecting SHs with the least $60\\%$ significance score for the vector quantization (VQ ratio), to balance the trade-off between compression efficiency and fidelity. ", "page_idx": 6}, {"type": "table", "img_path": "6AeIDnrTN2/tmp/09be7079127e7edcb90fcf7f41c0479b1bfcfeac51179b3675640266ec89722f.jpg", "table_caption": [], "table_footnote": ["Table 5: LightGaussian removes redundant neural Gaussians in Scaffold-GS, reducing model size and improving rendering speed. All experiments were rerun on our platform for fair comparison, with results averaged on the MipNeRF-360 dataset. "], "page_idx": 7}, {"type": "text", "text": "4.2 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Quantitative Results. We assess the performance of various methods for novel view synthesis, with quantitative metrics summarized in Tab. 1. This includes efficient voxel-based NeRFs like Plenoxel [12] and Instant-NGP [14], the compact MLP-based Mip-NeRF360 [7], vector-quantized NeRF [49], and 3D Gaussian Splatting [16]. ", "page_idx": 7}, {"type": "text", "text": "On the Mip-NeRF360 dataset, MLP-based NeRF methods achieve competitive accuracy with compact representations but suffer from slow inference speeds (0.06 FPS), limiting their practicality. Voxelbased NeRFs improve rendering efficiency but still fall short of real-time performance; for example, Plenoxel requires 2.1GB for a single large-scale scene. In contrast, 3D-GS offers a good balance between quality and speed but demands substantial storage per scene. ", "page_idx": 7}, {"type": "text", "text": "Our method, LightGaussian, exceeds existing techniques with rendering speeds over 200 FPS, enabled by efficient rasterization that prunes insignificant Gaussians. This approach reduces 3D Gaussian model redundancy, cutting storage from 782MB to 45MB on Mip-NeRF360\u2014a $15\\times$ reduction. LightGaussian also outperforms 3D-GS on the Tank & Temple datasets, nearly doubling rendering speed and reducing storage from 380MB to 22MB. ", "page_idx": 7}, {"type": "text", "text": "Qualitative Results. We conducted a comparative analysis of rendering results between 3D-GS and LightGaussian, focusing on intricate details and background regions, as shown in Fig. 4. Both 3D-GS and LightGaussian exhibit comparable visual quality, even in challenging scenes with thin structures, showing that LightGaussian effectively removes redundancy while preserving reconstruction fidelity. ", "page_idx": 7}, {"type": "text", "text": "Generalization to Other Point-based Representations We further apply our Gaussian Pruning approach to Scaffold-GS [67], an advanced neural Gaussian-based 3D representation. In this method, we prune Gaussians that contribute minimally to scene reconstruction, reducing redundancy while preserving visual fidelity. Experiments on the MipNeRF360 dataset demonstrate that pruning $80\\%$ of neural Gaussians increases the rendering speed from 152 to 178 FPS, as shown in Tab. 5. These results underscore LightGaussian\u2019s potential as an effective optimization tool for other Gaussian-based representations. ", "page_idx": 7}, {"type": "text", "text": "4.3 Ablation Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We performed ablation studies on each component of our method to evaluate their individual impacts. Results show that Gaussian Pruning and Recovery effectively removes redundant Gaussians, retaining only those crucial for scene representation. SH Distillation reduces the Spherical Harmonics degree, simplifying the lighting model with minimal quality loss. Furthermore, Vector Quantization efficiently compresses the feature space, creating a more compact representation. Combined, these modules substantially improve the overall efficiency and performance of our framework. ", "page_idx": 7}, {"type": "text", "text": "Significance Criteria To identify the optimal criteria for measuring each Gaussian\u2019s global significance, we evaluated key characteristics: Gaussian-ray interaction count, Gaussian opacity, and functional volume. Tab. 3 illustrates that integrating these three factors \u2014 by weighting the hit count with opacity and Gaussian volume \u2014 yields the highest rendering quality post zero-shot pruning. A quick Gaussian Co-adaptation (the last row) which optimizes the remaining Gaussians, can effectively recover the rendering accuracy to its pre-pruning level. The visual comparison of rasterized images, before and after pruning, alongside the depiction of pruned Gaussians, is presented in Fig. 5. ", "page_idx": 7}, {"type": "text", "text": "SH Distillation & Vector Quantization. Directly removing high-degree components in Spherical Harmonics (SHs) causes significant performance degradation compared to the full model (Exp $\\#3\\rightarrow$ ", "page_idx": 7}, {"type": "image", "img_path": "6AeIDnrTN2/tmp/b1e4a141a696f6717667ab173e4f807b8bd80a467af0335a8f6eb7c2fa0d4f3a.jpg", "img_caption": ["Figure 5: Visualization of Pruned Gaussians. We show the pruned Gaussians (middle) obtained by applying the proposed Gaussian Prune and Recovery. The residual is visualized by rasterizing the pruned Gaussians. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4), particularly with the loss of specular reflections across varying viewpoints. However, introducing knowledge distillation from the full model $\\operatorname{Exp}\\#4\\to5$ ) allows for size reduction while preserving essential viewing effects. Additionally, incorporating pseudo-views during training $(\\mathrm{Exp}\\#5\\to6)$ demonstrates the effectiveness of simulated views in enhancing the learning of specular reflections. While applying VQ to all attributes yields poorer results (see Tab. 4), this impact is mitigated by leveraging Gaussian Global Significance. ", "page_idx": 8}, {"type": "table", "img_path": "6AeIDnrTN2/tmp/07596a734450d7c5085e14def41ad51f3d242257f7b12c5a974bf249373e06ec.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "6AeIDnrTN2/tmp/ded50478fae267a9c5a56f33aa2b9e92b601c74f14a459699f40232df1873507.jpg", "img_caption": ["Table 6: Ablation with varying SH compactness: With a Gaussian pruning ratio of $\\overline{{6}}6\\%$ and SH reduced to 2 degrees, LightGaussian nearly maintains rendering quality (SSIM drops slightly from 0.927 to 0.926). Reducing SH further to 1 degree lowers SSIM to 0.923. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 6: Rendering performance comparison across different pruning levels and VQ ratios. Note that in subfigure (b), we use the VQ ratio to indicate the proportion of SH parameters quantized to an 8192-codebook, while the (overall) quantization ratio reflects combined compression from VQ and FP32-to-FP16 bitwise quantization. ", "page_idx": 8}, {"type": "text", "text": "Degradation & Speed vs. Compression Ratio We investigate the interplay between rendering quality and speed across various model compression ratios. Specifically, we adjust parameters such as Gaussian Pruning & Recovery, SH Distilling, and $V Q$ . As depicted in Fig 6, we note a marked decline in rendering quality when the pruning ratio reaches $70\\%$ , with a more rapid deterioration as the VQ ratio approaches $65\\%$ . In Tab 6, we observe that 2-degree SH compactness reduces the SSIM from 0.927 to 0.926, while 1-degree SH compactness further reduces it to 0.923. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Conclusion, Limitations, and Broad Impact ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We present LightGaussian, a novel framework that transforms heavy point-based representations into a compact format for efficient novel view synthesis. Designed for practical use, LightGaussian leverages 3D Gaussians to model large-scale scenes, effectively identifying and pruning the least significant Gaussians generated through densification. It achieves over $15\\times$ data reduction, boosts FPS to over 200, and minimally impacts rendering quality. Exploring zero-shot compression across various 3D-GS-based frameworks remains a promising direction for future research. ", "page_idx": 9}, {"type": "text", "text": "Broadly, LightGaussian\u2019s compact representation has the potential to democratize high-quality 3D content for applications in VR, AR, and autonomous driving by reducing resource demands, enabling more accessible and scalable deployment across industries. While we do not foresee any significant risk from this specific technique, 3D reconstruction may infringe on personal privacy when applied in public spaces or with drone footage, contradicting our ethical intentions. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Jianmei Dai, Zhilong Zhang, Shiwen Mao, and Danpu Liu. A view synthesis-based $360^{\\circ}$ vr caching system over mec-enabled c-ran. IEEE Transactions on Circuits and Systems for Video Technology, 30(10):3843\u20133855, 2019.   \n[2] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018.   \n[3] Luca De Luigi, Damiano Bolognini, Federico Domeniconi, Daniele De Gregorio, Matteo Poggi, and Luigi Di Stefano. Scannerf: a scalable benchmark for neural radiance fields. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 816\u2013825, 2023.   \n[4] Zirui Wu, Tianyu Liu, Liyi Luo, Zhide Zhong, Jianteng Chen, Hongmin Xiao, Chao Hou, Haozhe Lou, Yuantao Chen, Runyi Yang, et al. Mars: An instance-aware, modular and realistic simulator for autonomous driving. arXiv preprint arXiv:2307.15058, 2023.   \n[5] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing Scenes As Neural Radiance Fields for View Synthesis. Communications of the ACM, 65(1):99\u2013106, 2021.   \n[6] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan. Mip-Nerf: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5855\u20135864, 2021.   \n[7] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5460\u20135469, 2022.   \n[8] Robert A Drebin, Loren Carpenter, and Pat Hanrahan. Volume rendering. ACM Siggraph Computer Graphics, 22(4):65\u201374, 1988.   \n[9] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct Voxel Grid Optimization: Super-Fast Convergence for Radiance Fields Reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5459\u20135469, 2022.   \n[10] Liao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao, Yanshun Zhang, Yingliang Zhang, Minye Wu, Jingyi Yu, and Lan Xu. Fourier PlenOctrees for Dynamic Radiance Field Rendering in Real-Time. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13524\u201313534, 2022.   \n[11] Katja Schwarz, Axel Sauer, Michael Niemeyer, Yiyi Liao, and Andreas Geiger. Voxgraf: Fast 3d-Aware Image Synthesis With Sparse Voxel Grids. ArXiv Preprint ArXiv:2206.07695, 2022.   \n[12] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance Fields Without Neural Networks. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5491\u20135500, 2022.   \n[13] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In European Conference on Computer Vision, pages 333\u2013350. Springer, 2022.   \n[14] Thomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. Instant Neural Graphics Primitives With a Multiresolution Hash Encoding. ACM Transactions on Graphics (TOG), 41:1 \u2013 15, 2022.   \n[15] Huan Wang, Jian Ren, Zeng Huang, Kyle Olszewski, Menglei Chai, Yun Fu, and Sergey Tulyakov. R2l: Distilling neural radiance field to neural light field for efficient novel view synthesis. In European Conference on Computer Vision, pages 612\u2013629. Springer, 2022.   \n[16] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00fchler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics (ToG), 42(4):1\u2013 14, 2023.   \n[17] Georgios Kopanas, Julien Philip, Thomas Leimk\u00fchler, and George Drettakis. Point-based neural rendering with per-view optimization. In Computer Graphics Forum, volume 40, pages 29\u201343. Wiley Online Library, 2021.   \n[18] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for real-time rendering of neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5752\u20135761, 2021.   \n[19] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance Fields Without Neural Networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5501\u20135510, 2022.   \n[20] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural Sparse Voxel Fields. Advances in Neural Information Processing Systems, 33:15651\u201315663, 2020.   \n[21] Tao Hu, Shu Liu, Yilun Chen, Tiancheng Shen, and Jiaya Jia. EfficientNeRF Efficient Neural Radiance Fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12902\u201312911, 2022.   \n[22] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. Fastnerf: High-fidelity neural rendering at 200fps. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14346\u201314355, 2021.   \n[23] Peter Hedman, Pratul P Srinivasan, Ben Mildenhall, Jonathan T Barron, and Paul Debevec. Baking neural radiance fields for real-time view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5875\u20135884, 2021.   \n[24] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14335\u201314345, 2021.   \n[25] Benjamin Attal, Jia-Bin Huang, Michael Zollhoefer, Johannes Kopf, and Changil Kim. Learning Neural Light Fields With Ray-Space Embedding Networks. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19787\u201319797, 2022.   \n[26] Vincent Sitzmann, Semon Rezchikov, William T. Freeman, Joshua B. Tenenbaum, and Fr\u00e9do Durand. Light Field Networks: Neural Scene Representations With Single-Evaluation Rendering. Advances in Neural Information Processing Systems, 34, 2021.   \n[27] Leonid Keselman and Martial Hebert. Flexible techniques for differentiable rendering with 3d gaussians. arXiv preprint arXiv:2308.14737, 2023.   \n[28] James F Blinn. A generalization of algebraic surface drawing. ACM transactions on graphics (TOG), 1(3):235\u2013256, 1982.   \n[29] Benjamin Eckart, Kihwan Kim, Alejandro Troccoli, Alonzo Kelly, and Jan Kautz. Accelerated generative models for 3d point cloud data. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5497\u20135505, 2016.   \n[30] Manjushree Nulkar and Klaus Mueller. Splatting with shadows. In Volume Graphics 2001: Proceedings of the Joint IEEE TCVG and Eurographics Workshop in Stony Brook, New York, USA, June 21\u201322, 2001, pages 35\u201349. Springer, 2001.   \n[31] Petr Man. Generating and real-time rendering of clouds. In Central European seminar on computer graphics, volume 1. Citeseer Cast\u00e1-Papiernicka, Slovakia, 2006.   \n[32] Christoph Lassner and Michael Zollhofer. Pulsar: Efficient sphere-based neural rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1440\u20131449, 2021.   \n[33] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. Ewa volume splatting. In Proceedings Visualization, 2001. VIS\u201901., pages 29\u2013538. IEEE, 2001.   \n[34] Simon Niedermayr, Josef Stumpfegger, and R\u00fcdiger Westermann. Compressed 3d gaussian splatting for accelerated novel view synthesis. arXiv preprint arXiv:2401.02436, 2023.   \n[35] KL Navaneet, Kossar Pourahmadi Meibodi, Soroush Abbasi Koohpayegani, and Hamed Pirsiavash. Compact3d: Compressing gaussian splat radiance field models with vector quantization. arXiv preprint arXiv:2311.18159, 2023.   \n[36] Yuanhao Cai, Yixun Liang, Jiahao Wang, Angtian Wang, Yulun Zhang, Xiaokang Yang, Zongwei Zhou, and Alan Yuille. Radiative gaussian splatting for efficient x-ray novel view synthesis. In European Conference on Computer Vision, pages 283\u2013299. Springer, 2025.   \n[37] Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, and Eunbyung Park. Compact 3d gaussian representation for radiance field. arXiv preprint arXiv:2311.13681, 2023.   \n[38] Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural information processing systems, 2, 1989.   \n[39] Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft filter pruning for accelerating deep convolutional neural networks. arXiv preprint arXiv:1808.06866, 2018.   \n[40] Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung. Structured pruning of deep convolutional neural networks. ACM Journal on Emerging Technologies in Computing Systems (JETC), 13(3):1\u201318, 2017.   \n[41] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018.   \n[42] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In International Conference on Machine Learning, pages 3259\u20133269. PMLR, 2020.   \n[43] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017.   \n[44] Gabriella Csurka, Christopher Dance, Lixin Fan, Jutta Willamowski, and C\u00e9dric Bray. Visual categorization with bags of keypoints. In Workshop on statistical learning in computer vision, ECCV, volume 1, pages 1\u20132. Prague, 2004.   \n[45] Robert Gray. Vector quantization. IEEE Assp Magazine, 1(2):4\u201329, 1984.   \n[46] Chengzhi Mao, Lu Jiang, Mostafa Dehghani, Carl Vondrick, Rahul Sukthankar, and Irfan Essa. Discrete representations strengthen vision transformer robustness. arXiv preprint arXiv:2111.10493, 2021.   \n[47] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873\u201312883, 2021.   \n[48] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10696\u201310706, 2022.   \n[49] Lingzhi Li, Zhen Shen, Zhongshu Wang, Li Shen, and Liefeng Bo. Compressing volumetric radiance fields to 1 mb. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4222\u20134231, 2023.   \n[50] Haoyu Guo, Sida Peng, Yunzhi Yan, Linzhan Mou, Yujun Shen, Hujun Bao, and Xiaowei Zhou. Compact neural volumetric video representations with dynamic codebooks. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[51] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.   \n[52] Zhiqiang Shen, Zechun Liu, Dejia Xu, Zitian Chen, Kwang-Ting Cheng, and Marios Savvides. Is label smoothing truly incompatible with knowledge distillation: An empirical study. arXiv preprint arXiv:2104.00676, 2021.   \n[53] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.   \n[54] Lin Wang and Kuk-Jin Yoon. Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks. IEEE transactions on pattern analysis and machine intelligence, 44(6):3048\u20133068, 2021.   \n[55] Cristian Bucilu\u02c7a, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535\u2013541, 2006.   \n[56] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022.   \n[57] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang, and Zhangyang Wang. Neurallift360: Lifting an in-the-wild 2d photo to a 3d object with $360\\;\\mathrm{\\{\\backslash\\mathrm{deg}\\}}$ views. arXiv preprint arXiv:2211.16431, 2022.   \n[58] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann. Decomposing nerf for editing via feature field distillation. Advances in Neural Information Processing Systems, 35:23311\u201323330, 2022.   \n[59] Zhiwen Fan, Peihao Wang, Yifan Jiang, Xinyu Gong, Dejia Xu, and Zhangyang Wang. Nerf-sos: Any-view self-supervised object segmentation on complex scenes. arXiv preprint arXiv:2209.08776, 2022.   \n[60] Zhiwen Fan, Yifan Jiang, Peihao Wang, Xinyu Gong, Dejia Xu, and Zhangyang Wang. Unified implicit neural stylization. In European Conference on Computer Vision, pages 636\u2013654. Springer, 2022.   \n[61] Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian. Segment anything in 3d with nerfs. arXiv preprint arXiv:2304.12308, 2023.   \n[62] Wang Yifan, Felice Serena, Shihao Wu, Cengiz \u00d6ztireli, and Olga Sorkine-Hornung. Differentiable surface splatting for point-based geometry processing. ACM Transactions on Graphics (TOG), 38(6):1\u201314, 2019.   \n[63] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015.   \n[64] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Mipnerf 360: Unbounded anti-aliased neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5470\u20135479, 2022.   \n[65] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (ToG), 36(4):1\u201313, 2017.   \n[66] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586\u2013595, 2018.   \n[67] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffoldgs: Structured 3d gaussians for view-adaptive rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20654\u201320664, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Algorithm Explanation. We detail the procedures of LightGaussian in Algorithm 1. The trained 3D-GS [16] features a Gaussian location with a dimension of 3, Spherical Harmonics coefficients with a dimension of 48, and opacity, rotation, and scale, whose dimensions are 1, 4, and 3, respectively. ", "page_idx": 14}, {"type": "text", "text": "Algorithm 1 The overall pipeline of LightGaussian   \nInitialize: Training view images $\\mathcal{Z}\\;=\\;\\{I_{i}\\;\\in\\;\\mathbb{R}^{M}\\}_{i=1}^{N}$ and their associated camera poses $\\mathcal{P}~=~\\{\\phi_{i}~\\in$   \n$\\mathbb{R}^{3\\times4}\\}_{i=1}^{N}$ .   \n1: # Pre-Training 3D-GS [16].   \n2: # $G_{i}$ with attributes XYZ, SH-3deg, Opacity, Rotation, Scale.   \n3: $\\mathcal{G}=\\{G_{i}\\in\\mathbb{R}^{(3+48+1+4+3)})\\}_{i=1}^{N}\\overset{\\cdot}{\\leftarrow}\\3\\mathrm{D}{\\cdot}\\bar{\\mathrm{GS}}(\\mathcal{Z},\\mathcal{P})$ \\tingeight lrraInitial 3D-GS: $\\mathcal{G}$   \n4: #Gaussian Pruning and Recovery: $\\mathcal{G}\\mapsto\\mathcal{G}^{\\prime}$ .   \n5: $\\mathcal{G}S\\gets\\mathrm{CALGS}(\\mathcal{G})$ $\\triangleright$ lrraCalculate Global Significance Score $\\mathit{\\Pi}_{9S}$   \n6: ${\\mathcal{G}}^{\\prime}\\gets\\operatorname{PRUNE}({\\mathcal{G}},{\\mathcal{G}}{\\mathcal{S}})$ \\tingeight lrraPrune Least Significant Ones of $\\mathcal{G}$ , based on $\\mathit{\\Pi}_{9S}$   \n7: $\\mathcal{G}^{\\prime}\\leftarrow\\mathrm{RECOVERY}(\\mathcal{G}^{\\prime},\\mathcal{P})$ \\tingeight lrraGaussian Recovery (Finetune $\\mathcal{G}^{\\prime}$ on $\\mathcal{P}$ )   \n8: #Distilling into Compact SHs: Reduce from 3-degree to 2-degree   \n9: $\\mathcal{G}^{\\prime\\prime}\\gets\\mathrm{REDUCESH}(\\mathcal{G}^{\\prime})$ \\tingeight lrraReduce the SH degree   \n10: while Few Steps do \\tingeight lrraSH Distillation   \n11: $\\hat{\\mathcal{P}}=\\mathrm{SampleView}(\\mathcal{P})$ \\tingeight lrraSynthesize Pseudo Views   \n12: It \u2190TEACHER( P\u02c6, G\u2032) $\\triangleright$ lrraTeacher render   \n13: Is \u2190STUDENT( P\u02c6, G\u2032\u2032) $\\triangleright$ lrraStudent render   \n14: $\\nabla L\\gets\\mathrm{Loss}(I_{s},I_{t})$   \n15: $\\mathcal{G}^{\\prime\\prime}\\gets\\mathrm{ADAM}(\\nabla L)$ \\tingeight lrraBackprop & Step   \n16: end while   \n17: #Vector Quantization.   \n18: $\\mathcal{G}^{\\prime\\prime\\prime}\\leftarrow\\mathrm{VQ}(\\mathcal{G}^{\\prime\\prime},\\mathcal{G}S)$ \\tingeight lrraVQ based on Significance Score   \n19: $\\mathcal{G}^{\\prime\\prime\\prime}\\leftarrow\\mathrm{VQFINETUNE}(\\mathcal{G}^{\\prime\\prime\\prime})$ \\tingeight lrraVQ Finetuning   \n20: #Save Model.   \n21: Save optimized model $\\mathcal{G}^{\\prime\\prime\\prime}$ to disk. ", "page_idx": 14}, {"type": "text", "text": "Specifically, we detail how we calculate each Gaussian\u2019s Global Significance Score in Algorithm 2. ", "page_idx": 14}, {"type": "table", "img_path": "6AeIDnrTN2/tmp/9d33d64d518400d29ab57cab8aadfa4ec915108a932ac6cda92c4f1e9569fa69.jpg", "table_caption": ["Algorithm 2 Global significance calculation for gaussians. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Post-Finetuning of Vector Quantization. Applying vector quantization (VQ) based on the significance score facilitates a reduction in the required codebook size compared to the full model. Nonetheless, this method results in a decrease in accuracy as it groups the attributes into several clusters (codes). We observe that the SSIM decreases from 0.923 (#6) to 0.915 (#7), as illustrated in Table 2 of the main draft. This observation motivates us to joint optimize all other attributes and the codes post the VQ. Specifically, by assigning a unique code to each Gaussian, we proceed to implement differentiable finetuning using photometric loss, which is similar to the methodology employed in 3D-GS, and optimize all the attributes in conjunction with VQ. This approach permits precise adjustments to the codebook and the rest attributes, improving its alignment with the training images (#8, SSIM improves from 0.915 to 0.923). ", "page_idx": 14}, {"type": "image", "img_path": "6AeIDnrTN2/tmp/187c117476b25bd9d323186c5d33e0a0584fcc3dce9ab9225db03341c64810fe.jpg", "img_caption": ["Figure 7: Visual Comparisons for Ablation Study. We visualize the rendered RGB images and the residual map between the ground-truth image, aligned with the experiment ID as shown in Tab. The final model (Exp #9) demonstrates close results to 3D-GS (Exp #1), while the Gaussian Co-adaptation, along with SH distillation, almost completely mitigates the information loss. To highlight the difference, we visualize the SSIM between the rendered images and the GT, where a whiter output in the residual signifies closer alignment with the GT, denoting better quality, and the area with greater color intensity indicates lower quality. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "6 More Experiment Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In addition to realistic indoor and outdoor scenes in the Mip-NeRF360 [7] and Tank and Temple datasets [65], we further evaluate our method on the synthetic Blender dataset [5], and provide a scene-wise evaluation on all datasets, accompanied by detailed visualizations. ", "page_idx": 15}, {"type": "text", "text": "Results on NeRF-Synthetic $360^{\\circ}$ (Blender) Dataset. The synthetic Blender dataset [5] includes eight photo-realistic synthetic objects with ground-truth controlled camera poses and rendered viewpoints (100 for training and 200 for testing). Similar to 3D-GS [16], we start training the model using random initialization. Consequently, we calculate the Global Significance of each Gaussian, work to reduce the SH redundancy, and apply the Vector Quantization (codebook size set at 8192) to the learned representation. Overall comparisons with previous methods are listed in Table 7, where we observe our LightGaussian markedly reduces the average storage size from 52.38MB to 7.89MB, while improving the FPS from 310 to 411 with only a slight rendering quality decrease. ", "page_idx": 15}, {"type": "text", "text": "Additional Qualitative Results on Mip-NeRF360. We provide extra visualizations for 3D-GS [16], LightGaussian (ours), and VQ-DVGO [49], accompanied by the corresponding residual maps from the ground truth. As evidenced in Figure 8 and Figure 9, LightGaussian outperforms VQ-DVGO, which utilizes NeRF as a basic representation. Furthermore, LightGaussian achieves a comparable rendering quality to 3D-GS [16], demonstrating the effectiveness of our proposed compact representation. ", "page_idx": 15}, {"type": "table", "img_path": "6AeIDnrTN2/tmp/788e732f06232c98f85ea7a2eafabab2785822081043795f674b0faa73d18eeb.jpg", "table_caption": ["Table 7: Per-scene results on Synthetic-NeRF. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "6AeIDnrTN2/tmp/89ef5e63019b2827b944ef2fca8f49ba05abd58ed9cd960589988d2bfbe306ca.jpg", "table_caption": ["Table 8: Quantitative Comparison (PSNR) on Mip-NeRF360 and Tank & Temple Scenes. We re-train 3D-GS, Compressed 3D-GS, Compact 3D-GS using their original code and 3D-GS configurations on our platform, to perform a fair comparison. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Additional Quantitative Results on Mip-NeRF360. Tables 8, 9, and 10 present the comprehensive error metrics compiled for our evaluation across all real-world scenes (Mip-NeRF360 and Tank and Temple datasets). Our method not only compresses the average model size from 782MB to 45MB, but also consistently demonstrates comparable metrics with 3D-GS on all scenes. LightGaussian additionally shows better rendering quality than Plenoxel, INGP, mip-NeRF360, and VQ-DVGO. ", "page_idx": 16}, {"type": "text", "text": "Implementation Details of VQ-DVGO. In the implementation of VQ-DVGO [49], we initially obtain a non-compressed grid model following the default training configuration of DVGO [9]. The pruning quantile $\\beta_{p}$ is set to 0.001, the keeping quantile $\\beta_{k}$ is set to 0.9999, and the codebook size is configured to 4096. We save the volume density and the non-VQ voxels in the fp16 format without additional quantization. For the joint finetuning process, we have increased the iteration count to 25,000, surpassing the default setting of 10,000 iterations, to maximize the model\u2019s capabilities. All other parameters are aligned with those specified in the original VQ-DVGO paper [49], ensuring a faithful replication of established methodologies. ", "page_idx": 16}, {"type": "table", "img_path": "6AeIDnrTN2/tmp/6e489a18dd41e1365197eb809f44e9570d4bbdfdc761c645dc83125a2c7f764d.jpg", "table_caption": ["Table 9: Quantitative Comparison (SSIM) on Mip-NeRF360 and Tank & Temple Scenes. We re-train 3D-GS, Compressed 3D-GS, Compact 3D-GS using their original code and 3D-GS configurations on our platform, to perform a fair comparison. "], "table_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "6AeIDnrTN2/tmp/81b5e978d7c5a7d26dc599a44a0f58579cf9448773eff0c196032a05b82ac422.jpg", "img_caption": ["Figure 8: Additional Visual Comparisons on the Mip-NeRF360 Datasets. We present the rendering results from 3D-GS [16], LightGaussian, and VQ-DVGO [49]. The residual maps highlight the differences between the rendered images and ground truth (GT) images. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Overall Analysis. We report the performance of the investigation on the proposed modules in Tab. 2 and Fig. 7. We verify that our design of Gaussian Pruning & Recovery is effective in removing redundant Gaussians $(\\mathrm{Exp}\\oplus1\\!\\rightarrow\\!3)$ with negligible quality degradation while preserving rendering accuracy. This proves that the proposed global significance, based on principle of splatting, accurately represents the critical aspects of each Gaussian. By removing the high-degree SHs and transferring the knowledge to a compact representation $\\operatorname{Exp}\\#3\\to5)$ ), our method successfully demonstrates the beneftis from using soft targets and extra data from view augmentation, results in negligible changes in specular reflection. In practical post-processing, the vector quantization on the least important Gaussians (Exp #7), showcases the advantage of adopting VQ to further reduce model size. ", "page_idx": 17}, {"type": "image", "img_path": "6AeIDnrTN2/tmp/9ff8329e21601a108d343da4a76434ddcd8f7b3bbf15ce67479e0d2a0b93f765.jpg", "img_caption": ["Figure 9: Additional Visual Comparisons on the Mip-NeRF360 Datasets. We present the rendering results from 3D-GS [16], LightGaussian, and VQ-DVGO [49]. The residual maps highlight the differences between the rendered images and ground truth (GT) images. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "table", "img_path": "6AeIDnrTN2/tmp/21eb94151ac1c20acb5716918f3752894b66255176857a925ba172b8d96d74d4.jpg", "table_caption": ["Table 10: Quantitative Comparison (LPIPS) on Mip-NeRF360 and Tank & Temple Scenes. We re-train 3D-GS, Compressed 3D-GS, Compact 3D-GS using their original code and 3D-GS configurations on our platform, to perform a fair comparison. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We have clearly stated the contributions and scope in the Abstract and Introduction. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We describe the limitations in the Section 5. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We have presented the information in Section 4.1. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: We will release our code after our paper gets accepted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The training and test details are specified in Section 4.1 Implementation Details. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper does not involved statistical test. ", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We report the type of resources used in the experiments in Section 4.1. ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] Justification: The research conducted in the paper conform the NeurIPS Code of Ethics. ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: We have discusses the broader impact in Section 5. ", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: The released model does not have a high risk for misuse. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We cite corresponding papers for the asserts we use in Section 2. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: This work does not involve crowdsourcing nor research with human subjects. ", "page_idx": 20}]