[{"figure_path": "SQVns9hWJT/tables/tables_6_1.jpg", "caption": "Table 1: Text style fidelity assessment within text image level and full-size image level, highlighted with best and second best results. For full-size image evaluation, we replace the unedited region with the origin image while values in \u201c()\u201d denote the direct output of inpainting-based methods.", "description": "This table presents a quantitative comparison of different scene text editing methods in terms of style fidelity.  It assesses two levels: text image level (cropped region) and full image level (entire image with the edited text region replacing the original). Metrics used include SSIM (structural similarity), PSNR (peak signal-to-noise ratio), MSE (mean squared error), and FID (Fr\u00e9chet Inception Distance). The best and second-best results for each metric and level are highlighted. For the full-size image evaluation, methods using inpainting were evaluated in two ways: using only the inpainted region and by replacing the inpainted region with the original to evaluate the impact of the background restoration.", "section": "4.2 Performance Comparison"}, {"figure_path": "SQVns9hWJT/tables/tables_6_2.jpg", "caption": "Table 2: Text rendering accuracy evaluation with different methods, highlighted with best and second best results. \"Random\" denotes that we replace the paired target text in SCENEPAIR with randomly chosen text to verify the model robustness. Note that we are not able to evaluate inpainting-based STE methods [20, 21, 22] on TamperScene [14] since it does not contain full-size images.", "description": "This table presents a comparison of different scene text editing (STE) methods on two datasets: ScenePair and TamperScene.  The metrics used are text rendering accuracy (ACC) and normalized edit distance (NED).  ScenePair is a new dataset of real-world image pairs, while TamperScene is an existing dataset.  The \"Random\" column in ScenePair shows results when the target text is replaced with random text to test model robustness.  The table highlights the best and second-best performing methods for each metric and dataset.", "section": "4 Experiments"}, {"figure_path": "SQVns9hWJT/tables/tables_7_1.jpg", "caption": "Table 3: Ablation experiment on glyph structure representation pre-training.", "description": "This table presents the results of ablation experiments conducted to evaluate the impact of glyph structure representation pre-training on the performance of the TextCtrl model.  Specifically, it compares the text rendering accuracy (ACC) and Normalized Edit Distance (NED) achieved by using different text encoders (CLIP, T without font-variance augmentation, and T with font-variance augmentation) on the ScenePair dataset and a randomized version of the ScenePair dataset.", "section": "4.3 Ablation Study"}, {"figure_path": "SQVns9hWJT/tables/tables_8_1.jpg", "caption": "Table 4: Ablation experiment on style disentanglement.", "description": "This table presents the ablation study on the style disentanglement pre-training. It compares the performance of three different methods on the SSIM, MSE, and FID metrics: using ControlNet [31], using the proposed style encoder (S) without pre-training, and using the style encoder (S) with pre-training. The results demonstrate that using the proposed style encoder (S) with pre-training achieves the best performance across all metrics.", "section": "4.3 Ablation Study"}, {"figure_path": "SQVns9hWJT/tables/tables_9_1.jpg", "caption": "Table 5: Ablation experiment on inference enhancement.", "description": "This table presents the ablation study on the inference enhancement method, Glyph-adaptive Mutual Self-Attention (GaMuSa). It compares the performance of three different inference methods: without any enhancement, with MasaCtrl [36], and with GaMuSa. The evaluation metrics used are SSIM, MSE, and FID. The results show that GaMuSa significantly improves the style fidelity and reduces visual inconsistencies during inference.", "section": "4.3 Ablation Study"}, {"figure_path": "SQVns9hWJT/tables/tables_14_1.jpg", "caption": "Table 6: The parameter sizes of each module in TEXTCTRL", "description": "This table shows the number of parameters for each module in the TEXTCTRL model. The modules are the diffusion generator (G), the encoder-decoder VAE (\u03b5), the text glyph structure encoder (T), the text style encoder (S), and the vision encoder (R).  The total number of parameters in the model is 1216M.", "section": "B Implementation Setting"}]