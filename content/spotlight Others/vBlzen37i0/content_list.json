[{"type": "text", "text": "Optimal deep learning of holomorphic operators between Banach spaces ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ben Adcock Department of Mathematics Simon Fraser University Canada ", "page_idx": 0}, {"type": "text", "text": "Nick Dexter Department of Scientific Computing Florida State University USA ", "page_idx": 0}, {"type": "text", "text": "Sebastian Moraga   \nDepartment of Mathematics   \nSimon Fraser University Canada ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Operator learning problems arise in many key areas of scientific computing where Partial Differential Equations (PDEs) are used to model physical systems. In such scenarios, the operators map between Banach or Hilbert spaces. In this work, we tackle the problem of learning operators between Banach spaces, in contrast to the vast majority of past works considering only Hilbert spaces. We focus on learning holomorphic operators \u2013 an important class of problems with many applications. We combine arbitrary approximate encoders and decoders with standard feedforward Deep Neural Network (DNN) architectures \u2013 specifically, those with constant width exceeding the depth \u2013 under standard $\\ell^{2}$ -loss minimization. We first identify a family of DNNs such that the resulting Deep Learning (DL) procedure achieves optimal generalization bounds for such operators. For standard fully-connected architectures, we then show that there are uncountably many minimizers of the training problem that yield equivalent optimal performance. The DNN architectures we consider are \u2018problem agnostic\u2019, with width and depth only depending on the amount of training data $m$ and not on regularity assumptions of the target operator. Next, we show that DL is optimal for this problem: no recovery procedure can surpass these generalization bounds up to log terms. Finally, we present numerical results demonstrating the practical performance on challenging problems including the parametric diffusion, Navier-Stokes-Brinkman and Boussinesq PDEs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Operator learning is increasingly being investigated for problems arising in computational science and engineering. These problems are often posed in terms of Partial Differential Equations (PDEs), which can be viewed as operators mapping function spaces to function spaces. Depending on the requirements for well-posedness of the PDE, both the input and solution spaces are often Hilbert, or, more generally, Banach spaces. The aim of operator learning is to efficiently capture the dynamic behavior of these operators using surrogate models, typically based on Deep Neural Networks (DNNs). Specifically, we want to learn ", "page_idx": 0}, {"type": "equation", "text": "$$\nF:\\mathcal{X}\\to\\mathcal{Y},\\qquad X\\in\\mathcal{X}\\mapsto F(X)\\in\\mathcal{Y},\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $\\boldsymbol{\\wp}$ is the PDE solution space, $X$ represents the data supplied to the PDE, i.e., possibly multiple functions describing initial and boundary conditions or forcing terms or, equivalently, a vector of parameters defining such functions. ", "page_idx": 0}, {"type": "text", "text": "Let $\\mu$ be a probability measure on $\\mathcal{X}$ . Given noisy training data ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\{(X_{i},F(X_{i})+E_{i})\\}_{i=1}^{m}\\,,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $X_{1},\\ldots,X_{m}\\sim_{\\mathrm{i.i.d.}}$ $\\mu$ and $E_{i}$ is noise, a typical operator learning methodology consists of three objects: an approximate encoder $\\mathcal{E}_{\\mathcal{X}}:\\mathcal{X}\\rightarrow\\mathbf{\\dot{R}}^{d_{\\mathcal{X}}}$ , an approximate decoder $\\mathcal{D}_{\\mathcal{Y}}:\\breve{\\mathbb{R}^{d_{\\mathcal{Y}}}}\\rightarrow\\mathcal{Y}$ and a DNN $\\widehat{N}:\\mathbb{R}^{d_{\\mathcal{X}}}\\rightarrow\\mathbb{R}^{d_{\\mathcal{Y}}}$ . It then approximates $F$ as ", "page_idx": 1}, {"type": "equation", "text": "$$\nF\\approx\\widehat{F}:=\\mathcal{D}_{\\mathcal{V}}\\circ\\widehat{N}\\circ\\mathcal{E}_{\\mathcal{X}}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The encoder and decoder are either specified by the problem, learned separately from data, or learned concurrently with $\\widehat{N}$ . The goal, as in all supervised learning problems, is to ensure good generalization via the learned operator $\\widehat F$ from as little training data $m$ as possible. ", "page_idx": 1}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "As noted in, e.g., [16, 66], the theory of deep operator learning is still in its infancy. We contribute to this growth in the following ways. We consider learning classes of holomorphic operators (Assumption 2.2), with arbitrary approximate encoders $\\mathcal{E}_{\\mathcal{X}}$ and decoders $\\mathcal{D}_{\\mathcal{Y}}$ . As we explain in $\\S2.3$ (see also [52, $\\S5.2]$ , [53, $\\S3.4]$ and [41]) these operators are relevant in many applications, notably those involving parametric PDEs. The main contributions of this work are as follows. ", "page_idx": 1}, {"type": "text", "text": "1. We consider operators taking values in general Banach spaces. As noted, the vast majority of existing work (with the notable exception of [16]) considers Hilbert spaces.   \n2. We consider standard feedforward DNN architectures (constant width, width exceeds depth) and training procedures ( $\\ell^{2}$ -loss minimization).   \n3. (Theorem 3.1) We construct a family of DNNs such that any approximate minimizer of the corresponding training problem satisfies a generalization bound that is explicit in the various error sources: namely, an approximation error, which decays algebraically in the amount of training data $m$ ; encoding-decoding errors, which depend on the accuracy of the learned encoders and decoders; an optimization error, and; a sampling error, which depends on the noise $E_{i}$ in (1.2).   \n4. These DNN architectures are problem agnostic; they depend on $m$ only. In particular, the architectures are completely independent on the regularity assumptions of target operator.   \n5. (Theorem 3.2) We show that training problems based on any family of fully-connected DNNs possess uncountably many minimizers that achieve the same generalization bounds.   \n6. (Theorems 3.1-3.2) We provide bounds in both the $L_{\\mu}^{2}$ - and $L_{\\mu}^{\\infty}$ -norms that hold in high probability, rather than just expectation.   \n7. (Theorems 4.1-4.2) We show that the generalization bound is optimal with respect to $m$ : no learning procedure (not necessarily DL-based) can achieve better rates in $m$ up to log terms.   \n8. Finally, we present a series of experiments demonstrating the efficacy of DL on challenging problems such as the parametric diffusion, Navier-Stokes-Brinkman and Boussinesq PDEs, the latter two of which involve operators whose codomains are Banach, as opposed to Hilbert, spaces. ", "page_idx": 1}, {"type": "text", "text": "1.2 Relation to previous work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Approximating an operator between function spaces with training data obtained through numerical PDEs solves presents a formidable challenge. Nevertheless, in recent years, significant advances have been made through the development of DL techniques, leading to the field of operator learning [15, 40, 51, 53, 56, 58, 60, 62, 69, 83, 93, 103]. These approaches often leverage intricate DNN architectures to approximate the complex mappings inherent in physical modelling scenarios. Many works have also focused on the practical aspects of operator learning in real-world applications [13, 24, 35\u201337, 42, 45, 48, 49, 59, 61, 64, 65, 70, 73, 77, 80, 81, 84, 96, 98\u2013101, 104, 105]. ", "page_idx": 1}, {"type": "text", "text": "On the theoretical side, universal approximation theorems for operator learning have been developed in [50, 55, 68, 69] and elsewhere. Such bounds are typically not quantitative in the size of the DNN needed to achieve a certain error. For this, one typically either restricts to specific operators (e.g., certain PDEs) or imposes regularity conditions. One such assumption is Lipschitz regularity \u2013 see [10, 16, 55, 66, 87] and references therein. However, learning Lipschitz operators suffers from a curse of parametric complexity [54], meaning that algebraic rates may not be achievable. Another common assumption is holomorphy. While stronger, it is, as noted, very relevant to operator learning problems involving parametric PDEs. Quantitative approximation results for holomorphic operators have been shown in [26, 31, 41, 55, 71] and elsewhere. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "However, these works do not consider the generalization error, i.e., the error incurred when learning the approximation (1.3) from the finite training data (1.2). This is particularly important in applications of operator learning where data is obtained through expensive numerical PDE solves, since such problems are highly data-starved. Several works have tackled this question from the perspective of statistical learning theory and nonparametric estimation [16, 55, 66], but only for Lipschitz operators. As observed in [6, $\\S9.5]$ , this approach generally leads to a best $\\mathcal{O}(m^{-1/2})$ decay of the $L_{\\mu}^{2}$ -norm error with respect to $m$ . Theorem 4.1 shows that such a rate is strictly suboptimal for learning the classes of holomorphic operators we consider. Our generalization bounds in Theorems 3.1-3.2 do not use such techniques, and yield near-optimal rates in both the $L_{\\mu}^{2}$ - and $L_{\\mu}^{\\infty}$ -norms. See also [30] for some related work in this direction for reduced-order modelling with convolutional autoencoders. ", "page_idx": 2}, {"type": "text", "text": "Our work is inspired by recent research on learning holomorphic, Banach-valued functions [2, 5, 6]. We extend both these works, in particular, [5], to learning holomorphic operators. We also significantly improve the error decay rates in [5] with respect to $m$ and show they can be achieved using substantially smaller DNNs with standard training (i.e., $\\ell^{2}$ -loss minimization). See Remarks C.1-C.2. Our theoretical guarantees fall into the category of encoder-decoder-nets [52], which includes the well-known PCA-Net [10] and DeepONet [68] frameworks. As in other recent works [16, 30, 55, 66], in Theorems 3.1-3.2 we assume the encoder-decoder pair $(\\mathcal{E}_{\\mathcal{X}},\\mathcal{D}_{\\mathcal{Y}})$ in (1.3) have been learned, and focus on the generalization error when training the DNN $\\widehat{N}$ . ", "page_idx": 2}, {"type": "text", "text": "2 Notation, assumptions, setup and examples ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Notation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $(\\mathcal{X},\\|\\cdot\\|_{\\mathcal{X}})$ and $(\\mathcal{D},\\|\\cdot\\|_{\\mathcal{D}})$ be Banach spaces and $\\mu$ be a probability measure on $\\mathcal{X}$ . Let $(\\mathfrak{y}^{\\ast},\\|\\cdot\\|_{\\mathfrak{y}^{\\ast}})$ be the dual of $\\bar{y}$ and $B(\\mathfrak{y}^{\\ast})$ be its unit ball. The Bochner and Pettis $L^{p}$ -norms of a (strongly and weakly, respectively) measurable operator $F:\\mathcal X\\to\\mathcal Y$ are defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|F\\|_{L_{\\mu}^{p}(\\mathcal{X};\\mathcal{Y})}=\\left(\\displaystyle\\int_{\\mathcal{X}}\\|F(X)\\|_{\\mathcal{Y}}^{p}\\,\\mathrm{d}\\mu(X)\\right)^{1/p}}\\\\ {\\displaystyle\\|F\\|_{L_{\\mu}^{p}(\\mathcal{X};\\mathcal{Y})}=\\operatorname*{sup}_{y^{*}\\in B(\\mathcal{Y}^{*})}\\left(\\displaystyle\\int_{\\mathcal{X}}|y^{*}(F)|^{p}\\,\\mathrm{d}\\mu(X)\\right)^{1/p},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "respectively, for $1\\ \\leq\\ p\\ <\\ \\infty$ , and analogously for $\\textit{p}=\\infty$ (see, e.g., [8, 44]). Notice that $\\|F\\|_{L_{\\mu}^{p}(\\mathcal{X};\\mathcal{Y})}\\leq\\|F\\|_{L_{\\mu}^{p}(\\mathcal{X};\\mathcal{Y})}$ for $1\\leq p<\\infty$ , while $\\|F\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}=\\|F\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}$ . ", "page_idx": 2}, {"type": "text", "text": "Throughout this work, $\\ell^{p}(\\mathbb{N})$ , $0<p\\leq\\infty$ denotes the standard $\\,\\ell^{p}$ space with (quasi-)norm $\\|\\cdot\\|_{p}$ . We also define the monotone $\\,\\ell^{p}$ space $\\ell_{\\mathsf{M}}^{p}(\\mathbb{N})$ as the space of all sequences $z=(z_{i})_{i=1}^{\\infty}\\in\\mathbb{R}^{\\mathbb{N}}$ whose minimal monotone majorant $\\tilde{z}\\in\\ell^{p}(\\mathbb{N})$ . Here $z=(\\bar{z}_{i})_{i=1}^{\\infty}$ is defined as $\\tilde{z}_{i}=\\operatorname*{sup}_{j\\geq i}|z_{j}|$ . ", "page_idx": 2}, {"type": "text", "text": "Given a (componentwise) activation function $\\sigma$ , we consider feedforward DNNs of the form ", "page_idx": 2}, {"type": "equation", "text": "$$\nN:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{k},\\;z\\mapsto N(z)=A_{L+1}(\\sigma(A_{L}(\\sigma(\\cdot\\cdot\\cdot\\sigma(A_{0}(z))\\cdot\\cdot\\cdot)))),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{A}_{l}:\\mathbb{R}^{N_{l}}\\rightarrow\\mathbb{R}^{N_{l+1}}$ are affine maps, and $N_{0}=n$ and $N_{L+2}=k$ . We define width $\\mathbf{(}N)=$ $\\operatorname*{max}\\{N_{1},\\ldots,N_{L+1}\\}$ and $\\mathrm{depth}(N)\\,=\\,L$ . We denote a class of DNNs of the form (2.1) with a fixed architecture (i.e., fixed activation function, depth and widths) as $\\mathcal{N}$ , and write width $\\mathbf{\\Psi}(\\mathcal{N})=$ $\\operatorname*{max}\\{N_{1},\\ldots,N_{L+1}\\}$ and $\\mathrm{depth}(\\mathcal{N})=L$ . ", "page_idx": 2}, {"type": "text", "text": "2.2 Assumptions and setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $F:\\mathcal{X}\\rightarrow\\mathcal{Y}$ be the unknown operator we seek to learn and ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widetilde{\\varepsilon}_{x}:\\mathcal{X}\\to\\mathbb{R}^{d_{x}},\\ \\widetilde{\\mathcal{D}}_{x}:\\mathbb{R}^{d_{x}}\\to\\mathcal{X},\\qquad\\mathcal{E}_{y}:\\mathcal{Y}\\to\\mathbb{R}^{d_{y}},\\ \\mathcal{D}_{y}:\\mathbb{R}^{d_{y}}\\to\\mathcal{Y}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "be approximate encoders and decoders for $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ , respectively. As mentioned, we assume that these maps have already been learned, and focus on the training of the DNNN in (1.3). Our main results allow for arbitrary encoders and decoders (subject to the assumptions detailed below), and provide generalization bounds that are explicit in these terms: specifically, they depend on how well each encoder-decoder pair approximates the respective identity map on $\\mathcal{X}$ or $\\boldsymbol{\\wp}$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "In order to formulate the precise notion holomorphy for the operator $F$ , we require the following. Let $\\boldsymbol{D}=[-1,1]^{\\mathbb{N}}$ and $\\varrho$ be the uniform probability measure on $D$ . Given $\\rho>1$ , we define the Bernstein ellipse $\\mathcal{E}(\\rho)=\\left\\{(x+x^{-1})/2:x\\in\\mathbb{C}\\right.$ , $1\\leq|x|\\leq\\rho\\}\\subset\\mathbb{C}$ , and, for convenience, we let $\\mathcal{E}(1)\\,=\\,[-1,1]$ . Next, for $\\pmb{\\rho}=(\\rho_{i})_{i\\in\\mathbb{N}}\\ge\\mathbf{1}$ , we define the Bernstein polyellipse as the product $\\mathcal{E}(\\pmb{\\rho})=\\mathcal{E}(\\rho_{1})\\times\\mathcal{E}(\\rho_{2})\\times\\cdots\\subset\\mathbb{C}^{\\mathbb{N}}$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 2.1 (Holomorphic map). Let $\\varepsilon>0$ , $b\\in\\ell^{1}(\\mathbb{N})$ with $\\mathbf{\\Delta}b\\geq\\mathbf{0}$ . A Banach-valued function $f:D\\rightarrow\\mathcal{Y}$ is $(b,\\varepsilon)$ -holomorphic if it is holomorphic in the region ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{R}(b,\\varepsilon)=\\bigcup\\left\\{\\mathcal{E}(\\rho):\\rho\\geq\\mathbf{1},\\,\\sum_{j=1}^{\\infty}\\left((\\rho_{j}+\\rho_{j}^{-1})/2-1\\right)b_{j}\\leq\\varepsilon\\right\\}\\subset\\mathbb{C}^{\\mathbb{N}},\\quad b=(b_{j})_{j\\in\\mathbb{N}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "See, e.g., [17, 85]. As noted in [7] we can, by rescaling $^{b}$ , assume that $\\varepsilon=1$ . For convenience, we define the following unit ball, consisting of all such functions of norm at most one over $\\mathcal{R}(b,1)$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{H}(\\pmb{b})=\\{f:D\\rightarrow\\mathcal{V}\\left(b,1\\right)\\mathrm{-holomorphic:~}\\|f(\\pmb{x})\\|_{\\mathcal{V}}\\leq1,\\;\\forall\\pmb{x}\\in\\mathcal{R}(b,1)\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Assumption 2.2. Let $\\boldsymbol{D}=[-1,1]^{\\mathbb{N}}$ and $\\varrho$ be the uniform probability measure on $D$ . ", "page_idx": 3}, {"type": "text", "text": "(A.I) There is a measurable mapping $\\iota:\\mathcal{X}\\to\\mathbb{R}^{\\mathbb{N}}$ such that pushforward $\\varsigma:=\\iota\\sharp\\mu$ is a quasi-uniform measure supported on $D$ and $\\iota|_{\\mathrm{supp}(\\mu)}:\\mathcal{X}\\to\\ell^{\\infty}(\\mathbb{N})$ is Lipschitz with constant $L_{\\iota}\\geq0$ . (A.II) The operator $F$ has the form $F=f\\circ\\iota$ , where $f\\in{\\mathcal{H}}(b)$ for some $b\\in\\ell_{\\mathsf{M}}^{p}(\\mathbb{N})$ and $0<p<1$ . (A.III) The map $\\mathcal{E}_{\\mathcal{X}}:=\\iota_{d_{\\mathcal{X}}}\\circ\\tilde{\\mathcal{D}}_{\\mathcal{X}}\\circ\\tilde{\\mathcal{E}}_{\\mathcal{X}}$ is measurable (here $\\iota_{d_{\\mathcal{X}}}:\\mathcal{X}\\to\\mathbb{R}^{d_{\\mathcal{X}}}$ is the restriction of $\\iota$ , i.e., $\\iota_{d x}(X)=(\\iota(X)_{i})_{i=1}^{d_{X}})$ and the pushforward $\\tilde{\\varsigma}:=\\mathcal{E}_{\\mathcal{X}}\\sharp\\mu$ is absolutely continuous with respect to $\\varrho$ . $(A.I V)$ The maps $\\mathcal{D}_{\\mathcal{Y}}$ and $\\mathcal{E}_{\\mathcal{Y}}$ are linear and bounded. ", "page_idx": 3}, {"type": "text", "text": "Now let $X_{1},\\ldots,X_{m}\\sim_{\\mathrm{i.i.d.}}\\mu$ $\\mu$ and consider the training data ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\{(X_{i},Y_{i})\\}_{i=1}^{m}\\subset(\\mathcal{X}\\times\\mathcal{Y})^{m},\\quad\\mathrm{where~}Y_{i}=F(X_{i})+E_{i}\\in\\mathcal{Y}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and $E_{i}\\in\\mathcal{V}$ represents noise. Let $\\mathcal{N}$ be a class of DNNs $N:\\mathbb{R}^{d_{\\mathcal{X}}}\\rightarrow\\mathbb{R}^{d_{\\mathcal{Y}}}$ , and define ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\boldsymbol{F}\\approx\\widehat{\\boldsymbol{F}}:=\\mathcal{D}_{\\mathcal{Y}}\\circ\\widehat{\\boldsymbol{N}}\\circ\\mathcal{E}_{\\mathcal{X}},\\quad\\mathrm{where~}\\widehat{N}\\in\\underset{N\\in\\mathcal{N}}{\\mathrm{argmin}}\\frac{1}{m}\\sum_{i=1}^{m}\\|Y_{i}-\\mathcal{D}_{\\mathcal{Y}}\\circ N\\circ\\mathcal{E}_{\\mathcal{X}}(X_{i})\\|_{\\mathcal{Y}}^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "2.3 Discussion of assumptions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now discuss (A.I)-(A.IV). In $\\S6$ we describe future work on relaxing these assumptions. ", "page_idx": 3}, {"type": "text", "text": "(A.I) is a weak assumption. It asserts that there is a Lipschitz map $\\iota$ under which the pushforward of $\\mu$ is a quasi-uniform measure supported in $D$ . As we discuss in Example 2.3, this is notably the case when $\\mu$ is the law of some random field with an affine parametrization involving bounded random variables \u2013 a situation that occurs frequently in parametric and stochastic PDE problems. (A.II) describes the specific holomorphy of the operator $F-\\mathrm{see}$ Remark 2.4 for details. Note that we require $b\\in\\ell_{\\mathsf{M}}^{p}(\\mathbb{N})$ , not just $\\pmb{b}\\in\\ell^{p}(\\mathbb{N})$ . It is known [7] that one cannot learn holomorphic functions (and hence operators) from finite data if $\\pmb{b}\\in\\ell^{p}(\\mathbb{N})$ only. (A.III) is a relatively weak assumption. In view of (A.I), we expect it to hold as long as theD X \u25e6E X \u2248IX sufficiently well. Finally, (A.IV) is a standard assumption, which holds for instance in the case of PCA-Net and DeepONet. The former also enforces the learned encoder $\\widetilde{\\mathcal{E}}_{\\mathcal{X}}$ to be linear, which is not needed in our setup. Moreover, both approaches usually only deal with  the case where both $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ are Hilbert spaces. ", "page_idx": 3}, {"type": "text", "text": "Example 2.3 (Parametric PDEs) A common operator learning problem involves learning the map ", "page_idx": 3}, {"type": "equation", "text": "$$\nF:a\\in\\mathcal{X}\\mapsto u(a)\\in\\mathcal{Y},\\quad\\mathrm{where~}u(a)\\ \\mathrm{satisfies}\\ \\mathcal{F}_{a}u=0\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and $\\mathcal{F}_{a}$ specifies a certain PDE depending on a parameter or function $a$ . A standard example is the elliptic diffusion equation over a domain $\\Omega\\subset\\mathbb{R}^{n}$ . Here $a=a(\\pmb{x})\\in\\mathrm{L}^{\\infty}(\\Omega)=:\\mathcal{X}$ is the diffusion coefficient and $u=u(\\cdot;a)$ is the solution of the PDE ", "page_idx": 3}, {"type": "equation", "text": "$$\n-\\nabla\\cdot\\left(a\\nabla u(z;a)\\right)=g,\\;z\\in\\Omega,\\quad u(z;a)=0,\\;z\\in\\partial\\Omega.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Problems such as (2.6) are ubiquitous in scientific computing, with many applications in engineering, biology, physics, finance and beyond. In many such applications, it is common to assume that the measure $\\mu$ on $\\mathcal{X}$ is the law of a random field ", "page_idx": 4}, {"type": "equation", "text": "$$\na({\\pmb x})=a(\\cdot;{\\pmb x})=a_{0}(\\cdot)+\\sum_{i=1}^{\\infty}c_{i}x_{i}\\phi_{i}(\\cdot),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for functions $a_{0},\\phi_{i}\\in\\mathcal{X}$ , where the $x_{i}$ are random variables and $c_{i}\\geq0$ are scalars that ensure that $a\\in\\mathrm{L}^{\\infty}(\\Omega)$ . Under some mild assumptions, (2.8) is then the Karhunen\u2013Lo\u00e8ve (KL) expansion of the measure $\\mu$ . See, e.g., [91] (see also [55, $\\S3.5.1]$ ). The $x_{i}$ are typically independent. While in some settings, they may have infinite support, it is also common in practice to assume they range between finite maxima and minima. After rescaling, one may therefore assume that $\\pmb{x}\\in D\\dot{=}[-\\bar{1},1]^{\\mathbb{N}}$ . ", "page_idx": 4}, {"type": "text", "text": "Problems of this type fits into our framework. Suppose that $\\pmb{x}=(x_{1},x_{2},\\ldots)\\sim\\varrho$ . The measure $\\mu$ is then given as the pushforward $\\mu\\,=\\,a\\sharp\\varrho$ and $f\\,:\\,D\\,\\rightarrow\\,y$ is the parametric solution map $f:x\\in D\\mapsto u(\\cdot;a({\\pmb x}))\\in\\mathcal{Y}$ . If needed, the map $\\iota$ can be defined in a number of different ways. Suppose, for instance, that $\\mathcal{X}$ is a Hilbert space, e.g., $\\mathscr{X}=\\mathrm{L}^{2}(\\Omega)$ , and $\\{\\phi_{i}\\}_{i=1}^{\\infty}$ is a Riesz system (this holds, for instance, in the case of a KL expansion, in which case $\\{\\phi_{i}\\}_{i=1}^{\\infty}$ is an orthonormal basis). Then $\\{\\phi_{i}\\}_{i=1}^{\\infty}$ has a unique biorthogonal dual Riesz system $\\{\\psi_{i}\\}_{i=1}^{\\infty}$ . We may therefore define $\\iota:a\\mapsto\\left(\\langle a-a_{0},\\psi_{i}\\rangle_{\\mathrm{L}^{2}(\\Omega)}/c_{i}\\right)_{i=1}^{\\infty}$ . Notice that $\\iota$ is a bounded linear map and $F(X)=f\\circ\\iota(X)=$ $f({\\boldsymbol{x}})$ for $X=a(\\pmb{x})\\sim\\mu$ . However, evaluating $\\iota$ is often not required for computations (see $\\S\\mathrm{A.l}_{.}$ ). ", "page_idx": 4}, {"type": "text", "text": "This example considers an affine parametrization (2.8) inducing the measure $\\mu$ . Note that other parametrizations can be considered. Common examples include the quadratic $a(z;{\\pmb x})=a_{0}(z)+$ $\\textstyle{(\\sum_{i=1}^{\\infty}c_{i}x_{i}\\phi_{i}(z))^{2}}$ and log-transformed $\\begin{array}{r}{a(z,\\mathbf{\\boldsymbol{x}})=\\exp\\left(\\sum_{i=1}^{\\infty}c_{i}x_{i}\\phi_{i}(z)\\right)}\\end{array}$ parametrizations [17]. ", "page_idx": 4}, {"type": "text", "text": "Remark 2.4 (Holomorphy assumption) In the previous example, the operator $F$ stems from the solution map $f:D\\rightarrow\\mathcal{Y}$ of a parametric PDE. The regularity of solution maps of parametric PDEs has been intensively studied, and it is known that many such maps are $(b,\\varepsilon)$ -holomorphic (hence the resulting operator satisfies (A.II)). Consider, for instance, the affine diffusion problem (2.7)-(2.8). Under a mild uniform ellipticity condition, the solution map of the standard weak form of the PDE $f:x\\in D\\mapsto u\\bar{(}a(\\cdot;\\pmb{x}))\\stackrel{\\cdot}{\\in}\\mathrm{H}_{0}^{\\mathrm{i}}(\\Omega)$ is $(b,\\varepsilon)$ -holomorphic with $\\pmb{b}=(b_{i})_{i=1}^{\\infty}$ and $b_{i}=c_{i}\\lVert\\phi_{i}\\rVert_{\\mathrm{L}^{\\infty}(\\Omega)}$ See, e.g., [3, Prop. 4.9], as well as $\\S B.3$ . Similar results are known for other parametric PDEs. This includes parabolic PDEs, various types of nonlinear, elliptic PDEs, PDEs over parametrized domains, parametric hyperbolic problems and parametric control problems. See [19] or [3, Chpt. 4] for reviews. ", "page_idx": 4}, {"type": "text", "text": "3 Main results I: upper bounds ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now present our first two main results. In these results, given an optimization problem $\\operatorname*{min}_{t}\\boldsymbol{f}(t)$ , we say that $\\hat{t}$ is a $\\tau$ -approximate minimizer for some $\\tau\\geq0$ if $f(\\hat{t})\\leq\\operatorname*{min}_{t}f(t)+\\tau^{2}$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1 (Existence of good DNN architectures). Let $m~\\geq~3,~\\delta~>~0,~0~<~\\epsilon~<~1$ and $L\\,=\\,L(m,\\epsilon)\\,=\\,\\log^{4}(m)+\\log(1/\\epsilon)$ . Then there exists a class $\\mathcal{N}$ of hyperbolic tangent (tanh) DNNs $N:\\mathbb{R}^{d_{\\boldsymbol{x}}}\\rightarrow\\mathbb{R}^{d_{\\boldsymbol{y}}}$ depending on m and \u03f5 only with ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{width}(N)\\lesssim(m/L)^{1+\\delta},\\quad\\mathrm{depth}(N)\\lesssim\\log(m/L),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "such that following holds. Suppose that Assumption 2.2 holds and ", "page_idx": 4}, {"type": "equation", "text": "$$\nd\\boldsymbol{x}\\geq\\lceil m/L\\rceil,\\qquad L_{\\iota}\\cdot\\|\\mathbb{Z}_{\\mathcal{X}}-\\widetilde{\\mathcal{D}}_{\\mathcal{X}}\\circ\\widetilde{\\mathcal{E}}_{\\mathcal{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{X})}\\leq c\\cdot(m/L)^{-1/2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{Z}_{\\mathcal{X}}:\\mathcal{X}\\rightarrow\\mathcal{X}$ is the identity map and $c>0$ is a universal constant. Let $X_{1},\\ldots,X_{m}$ \u223ci.i.d. $\\mu$ and consider the noisy training data (2.4) with arbitrary noise $E_{i}\\in\\mathcal{V}$ . Then, with probability at least $1-\\epsilon,$ , every $\\tau$ -minimizer $\\widehat{N}$ of (2.5), where $\\tau\\geq0$ is arbitrary, yields an approximation $\\hat{F}$ that satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\|F-\\widehat{F}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\lesssim E_{\\mathsf{a p p},2}+E_{\\mathcal{X},2}+E_{\\mathcal{Y},2}+E_{\\mathsf{o p t},2}+E_{\\mathsf{s a m p},2},}\\\\ &{\\|F-\\widehat{F}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}\\lesssim E_{\\mathsf{a p p},\\infty}+E_{\\mathcal{X},\\infty}+E_{\\mathcal{Y},\\infty}+E_{\\mathsf{o p t},\\infty}+E_{\\mathsf{s a m p},\\infty},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and, $i f\\mathcal{Y}$ is a Hilbert space, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|F-\\widehat{F}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\lesssim E_{\\mathsf{a p p},2}+E_{\\mathcal{X},2}+E_{\\mathcal{Y},2}+E_{\\mathsf{o p t},2}+E_{\\mathsf{s a m p},2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, the approximation error terms $E_{\\mathsf{a p p},q},$ , $q=2,\\infty$ , are given by ", "page_idx": 5}, {"type": "equation", "text": "$$\nE_{\\mathsf{a p p},q}=a_{\\mathcal{Y}}\\cdot C(b,p,\\xi)\\cdot(m/L)^{\\theta+1-1/q-1/p},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $a y=\\|\\mathcal{D}_{\\mathcal{Y}}\\circ\\mathcal{E}_{\\mathcal{Y}}\\|_{\\mathcal{Y}\\to\\mathcal{Y}}.$ , $C(b,p,\\xi)>0$ depends on $b,\\,p$ and $\\xi$ only and $\\theta=0$ if Y is a Hilbert space (as in (3.5)) or $\\theta=1/2$ otherwise (as in (3.3)-(3.4)). The other terms are given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{\\mathcal{X},2}=a y\\cdot L_{t}\\cdot\\sqrt{m/(L\\epsilon)}\\cdot\\|\\mathcal{Z}_{\\mathcal{X}}-\\widetilde{\\mathcal{D}}_{\\mathcal{X}}\\circ\\widetilde{\\mathcal{E}}_{\\mathcal{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{X})}}\\\\ &{E_{\\mathcal{X},\\infty}=a y\\cdot L_{t}\\cdot\\sqrt{m/L}\\cdot\\Big(\\sqrt{m/L}\\cdot\\|\\mathcal{Z}_{\\mathcal{X}}-\\widetilde{\\mathcal{D}}_{\\mathcal{X}}\\circ\\widetilde{\\mathcal{E}}_{\\mathcal{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{X})}+\\|\\mathcal{Z}_{\\mathcal{X}}-\\widetilde{\\mathcal{D}}_{\\mathcal{X}}\\circ\\widetilde{\\mathcal{E}}_{\\mathcal{X}}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{X})}\\Big)}\\\\ &{\\quad E_{\\mathcal{Y},2}=\\|\\mathcal{Z}_{\\mathcal{Y}}-\\mathcal{D}_{\\mathcal{Y}}\\circ\\mathcal{E}_{\\mathcal{Y}}\\|_{L_{F\\sharp\\mu}^{2}(\\mathcal{Y};\\mathcal{Y})}/\\sqrt{\\epsilon}}\\\\ &{E_{\\mathcal{Y},\\infty}=\\|\\mathcal{Z}_{\\mathcal{Y}}-\\mathcal{D}_{\\mathcal{Y}}\\circ\\mathcal{E}_{\\mathcal{Y}}\\|_{L_{F\\sharp\\mu}^{\\infty}(\\mathcal{Y};\\mathcal{Y})}+\\sqrt{m/L}\\cdot\\|\\mathcal{Z}_{\\mathcal{Y}}-\\mathcal{D}_{\\mathcal{Y}}\\circ\\mathcal{E}_{\\mathcal{Y}}\\|_{L_{F\\sharp\\mu}^{2}(\\mathcal{Y};\\mathcal{Y})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{T}_{\\mathcal{Y}}:\\mathcal{Y}\\rightarrow\\mathcal{Y}$ is the identity map and, i $\\begin{array}{r}{^{c}\\left\\|\\pmb{E}\\right\\|_{2;\\ y}^{2}=\\sum_{i=1}^{m}\\left\\|\\pmb{E}_{i}\\right\\|_{\\ y}^{2}}\\end{array}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\nE_{\\mathrm{opt}}=\\Bigg\\{\\tau+2^{-m}\\quad\\;q=2\\atop\\sqrt{m/L}\\tau+2^{-m}\\quad q=\\infty\\,^{\\prime}\\quad E_{\\mathrm{samp},q}=\\Bigg\\{\\|E\\|_{2;y}/\\sqrt{m}\\quad q=2\\atop\\|E\\|_{2;y}/\\sqrt{L}\\quad q=\\infty\\,^{\\prime}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "(Proofs of this and all other theorems are in $\\mathrm{\\bf\\nabla\\mit{C-G}}$ of the supplemental material.) This theorem shows that there is a family of tanh DNNs that yield provable bounds for learning holomorphic operators. The error (3.3)-(3.5) decomposes into an approximation error (3.6), which decays algebraically in the amount of training data $m$ . Later, in Theorems 4.1-4.2, we show that these rates are optimal when $\\boldsymbol{\\wp}$ is a Hilbert space, up to log factors. Next, are the encoding-decoding errors (3.7), which depend on how well the approximate encoder-decoder pairs $(\\tilde{\\mathcal{E}}_{\\mathcal{X}},\\tilde{\\mathcal{D}}_{\\mathcal{X}})$ and $(\\mathcal{E}_{\\mathcal{V}},\\mathcal{D}_{\\mathcal{V}})$ approximate the identity maps on $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ , respectively. Observe that these terms are increasing in $m$ for fixed encoders and decoders. Therefore, as one expects, the accuracy of the encoder-decoder approximations $\\widetilde{\\mathcal{D}}_{\\mathcal{X}}\\circ\\widetilde{\\mathcal{E}}_{\\mathcal{X}}\\approx\\mathcal{T}_{\\mathcal{X}}$ and $\\mathcal D_{\\mathcal{V}}\\circ\\mathcal E_{\\mathcal Y}\\approx\\mathcal Z_{\\mathcal Y}$ should increase with increasing $m$ to ensure decay to zero of the generalization error as $m\\rightarrow\\infty$ . The specific terms in (3.7) (for $q=2$ ) are quite standard in operator learning. See, e.g., [53, 55]. When the encoders and decoders are computed via PCA, as in PCA-Net, standard bounds can be derived for these terms [53]. For similar analysis in the case of DeepONets, see [55]. Finally, the error (3.3)-(3.5) involves an optimization error $E_{\\mathrm{opt}}$ , which primarily depends on how accurately the optimization problem (2.5) is solved (i.e., the term $\\tau$ ), and a sampling error $E_{\\mathsf{s a m p}}$ , which depends on the error in the training data (2.4). ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1 allows $\\boldsymbol{\\wp}$ to be a Banach or a Hilbert space. Overall, when $\\boldsymbol{\\wp}$ is only a Banach space, we obtain a weaker $L_{\\mu}^{2}$ -norm bound involving the Pettis norm (3.3) and, moreover, the approximation error $E_{\\mathsf{a p p},q}$ is worse by a factor of $1/2$ than when $\\boldsymbol{\\wp}$ is a Hilbert space. (Note that one can establish a bound for the Bochner $L_{\\mu}^{2}$ -norm error when $\\boldsymbol{\\wp}$ is a Banach space via (3.4) and the inequality $\\|\\cdot\\|_{L_{\\mu}^{2}(X;\\mathcal{Y})}\\,\\le\\,\\|\\cdot\\|_{L_{\\mu}^{\\infty}(X;\\mathcal{Y})}$ . However, we do not believe the resulting bound is sharp). As we discuss in Remark D.18, the discrepancies between the two cases stem from the lack of an inner product structure and, in particular, the absence of Parseval\u2019s identity when $\\boldsymbol{\\wp}$ is a Banach space. ", "page_idx": 5}, {"type": "text", "text": "Observe that the DNN architecture in Theorem 3.1 is independent of the smoothness of the operator being learned. We term such an architecture problem agnostic. This theorem considers tanh activations only. However, as we discuss in Remark D.11, other activations can be readily used instead. Other key facets of Theorem 3.1 are the width and depth bounds (3.1). Qualitatively, these agree with empirical practice: namely, better performing DNNs tend to be wider than they are deep, and relatively shallow DNNs perform well in practice (see [24, 25] and references therein). We also see this later in $\\S5$ . ", "page_idx": 5}, {"type": "text", "text": "On the other hand, the family $\\mathcal{N}$ is not fully connected. As we describe in $\\S C.2.1$ , while the weights on the final layer can be arbitrary real numbers, the weights and biases in the hidden layers come from a finite (but large) set: they are handcrafted to approximately emulate certain multivariate orthogonal polynomials. Since fully-connected DNNs are typically used in practice, Theorem 3.1 is essentially a theoretical contribution. In our next result, we consider the more practical scenario of fully-connected DNNs. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2 (Fully-connected DNN architectures are good). There are universal constants $c_{1},c_{2},c_{3},c_{4}\\geq1$ such that the following holds. Let m, $\\delta$ , \u03f5 and $L$ be as in Theorem 3.1, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d_{\\mathcal{X}}\\geq c_{1}(m+\\log(1/\\epsilon)),\\quad L_{\\iota}\\cdot\\|\\mathcal{Z}_{\\mathcal{X}}-\\widetilde{\\mathcal{D}}_{\\mathcal{X}}\\circ\\widetilde{\\mathcal{E}}_{\\mathcal{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{X})}\\leq c(\\delta)\\cdot(m+\\log(1/\\epsilon))^{-1/2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $c(\\delta)>0$ depends on $\\delta$ only, consider any class $\\mathcal{N}$ of fully-connected DNNs satisfying ", "page_idx": 6}, {"type": "equation", "text": "$$\nn_{0},n_{L+2})=(d_{X},d_{\\mathcal{Y}}),\\quad N_{1},\\ldots,N_{L+1}\\geq c_{2}\\cdot(m+\\log(1/\\epsilon))\\cdot{(m/L)}^{\\delta},\\quad L\\geq c_{3}\\cdot\\log(m/L).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Suppose that Assumption 2.2 holds and that the pushforward $\\varsigma$ in (A.I) is the tensor-product of $a$ univariate probability distribution with mean zero and variance $\\omega\\gtrsim1$ . Let $X_{1},\\ldots,X_{m}$ \u223ci.i.d. $\\mu$ and consider (2.4) with arbitrary $E_{i}\\in\\mathcal{V}$ . Then the following hold with probability at least $1-\\epsilon$ . ", "page_idx": 6}, {"type": "text", "text": "(A) Uncountably many \u2018good\u2019 minimizers. The problem (2.5) has uncountably many minimizers that satisfy (3.3) with $\\tau=0$ or (3.5) with $\\tau=0\\;i f y$ is a Hilbert sp\u221aace. They also satisf\u221ay (3.4) with $\\tau=0$ and the modified right-hand side $\\sqrt{L}E_{\\mathsf{a p p},\\infty}\\!+\\!L E_{\\chi,\\infty}\\!+\\!\\bar{\\sqrt{L}}E_{\\mathcal{Y},\\infty}\\!+\\!E_{\\mathsf{o p t},\\infty}\\!+\\!\\bar{\\sqrt{L}}E_{\\mathsf{s a m p},\\infty}$ . ", "page_idx": 6}, {"type": "text", "text": "(B) Good minimizers are stable. Suppose that $\\mathcal{E}_{\\mathcal{X}}\\in L_{\\mu_{\\cdot}}^{\\infty}(\\mathcal{X};\\mathbb{R}^{d_{\\mathcal{X}}})$ and let $\\tau_{o}>0$ be arbitrary. Then there is a neighbourhood of DNN parameters around the parameters of each minimizer in (A) for which the approximation corresponding to any parameters in this neigbourhood also satisfies the same bounds as in $(A)$ with $\\tau=\\tau_{o}$ . ", "page_idx": 6}, {"type": "text", "text": "$(C)$ Good minimizers can be far apart in parameter space. For sufficiently large $m$ , there are at least $(m/(c_{4}L))^{2\\delta m}$ minimizers satisfying the bounds in $(A)$ such that, for any two such minimizers, their parameters satisfy $\\lVert\\pmb{\\theta}^{\\prime}\\rVert=\\lVert\\pmb{\\theta}\\rVert$ and $\\|\\pmb{\\theta}^{\\prime}-\\pmb{\\theta}\\|\\gtrsim1$ . ", "page_idx": 6}, {"type": "text", "text": "This theorem states that DL with fully-connected DNN architectures of sufficient width and depth (3.10) can succeed, since there are minimizers that yield the optimal bounds of Theorem 3.1. Such minimizers are uncountably many in number (A), stable to perturbations (B) and many of them (exponentially in $m$ ) have sufficiently distinct and nonvanishing/nonexploding parameters (C). This theorem does not imply that all minimizers are \u2018good\u2019 \u2013 an issue we discuss further in $\\S6$ \u2013 but our numerical results in $\\S5$ suggest that (approximate) minimizers obtained through training do, at least for the experiments considered, achieve the rates specified in Theorem 3.1. ", "page_idx": 6}, {"type": "text", "text": "4 Main results II: lower bounds ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now show that the various approximation errors are nearly optimal. For this, we ignore the encoding-decoding, optimization and sampling errors and proceed as follows. Let $C(\\mathcal{X};\\mathcal{Y})$ be the Banach space of continuous operators. We term an (adaptive) sampling map as any map ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}:C(\\mathcal{X},\\mathcal{Y})\\to\\mathcal{Y}^{m},\\quad F\\mapsto\\mathcal{L}(F)=\\left(F(X_{i})\\right)_{i=1}^{m},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $X_{1}\\,\\in\\,{\\mathcal{X}}$ , $X_{2}\\,=\\,X_{2}(F(X_{1}))\\,\\in\\,{\\mathcal{X}}$ potentially depends on the previous evaluation $F(X_{1})$ , $X_{3}\\,=\\,X_{3}(F(X_{1}),F(X_{2}))\\,\\in\\,\\mathcal{X}$ , and so forth. Next, we term a reconstruction map as any map $\\mathcal{R}:\\mathcal{V}^{m}\\rightarrow L_{\\mu}^{2}(\\mathcal{X};\\mathcal{V})$ . Given this, we let $\\mathcal{H}(\\pmb{b},\\iota)=\\{F=f\\circ\\iota:f\\in\\mathcal{H}(\\pmb{b})\\}$ and define ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\theta_{m}(b)=\\operatorname*{inf}_{\\mathcal{L},\\mathcal{R}}\\operatorname*{sup}_{F\\in\\mathcal{H}(b,\\iota)}\\|F-\\mathcal{R}\\circ\\mathcal{L}(F)\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the infimum is taken over all such $\\mathcal{L}$ and $\\mathcal{R}$ . In other words, $\\theta_{m}(\\pmb{b})$ measures how well one can learn holomorphic operators using arbitrary training data and an arbitrary reconstruction procedure. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.1 (Optimal $L^{2}$ error rates). Suppose that (A.I) holds. Then, for any $0<p<1$ there is $a$ constant $c(p)>0$ such that the following hold. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\colon b\\in\\ell_{\\mathsf{M}}^{p}(\\mathbb{N}),\\,b\\geq\\mathbf{0},\\,\\|b\\|_{p,\\mathsf{M}}=1\\,s u c h\\,t h a t\\,\\theta_{m}(b)\\geq c(p)\\cdot\\frac{m^{1/2-1/p}}{\\log^{2/p}(2m)},\\forall m\\in\\mathbb{N}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This theorem shows that the error $E_{\\mathsf{a p p},2}$ in Theorems 3.1-3.2 is optimal, up to log terms, whenever $\\boldsymbol{\\wp}$ is a Hilbert space: there does not exist a reconstruction map surpasses the rate $m^{1/2-1/p}$ for learning holomorphic operators. Note that this result applies not only to DL-based procedures, but any procedure that learns such operators from $m$ samples. Another consequence of this result is that adaptive sampling, i.e., active learning, is of no benefti. As shown by Theorems 3.1-3.2, the optimal rate $m^{1/2-1/\\bar{p}}$ can, up to log terms, be achieved through inactive learning, i.e., i.i.d. sampling from $\\mu$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.1 considers $L^{2}$ -norm. For the $L^{\\infty}$ -norm, we present a somewhat weaker result. Let ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widetilde{\\theta}_{m}(b)=\\operatorname*{inf}_{\\mathcal{R}}\\{\\mathbb{E}_{X_{1},\\ldots,X_{m}\\sim\\mu}\\operatorname*{sup}_{F\\in\\mathcal{H}(b,\\iota)}\\|F-\\mathcal{R}(\\{X_{i},F(X_{i})\\})\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}\\},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the infimum is taken over all reconstruction maps $\\mathcal{R}:(\\mathcal{X}\\times\\mathcal{Y})^{m}\\to L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})$ only. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.2 (Optimal $L^{\\infty}$ error rates). Suppose that (A.I) holds and that the pushforward $\\varsigma$ is the tensor-product of a univariate probability distribution with mean zero and variance $\\omega\\gtrsim1$ . Then, for any $0<p<1$ there is a constant $c(p)>0$ such that the following hold. ", "page_idx": 7}, {"type": "text", "text": "As with the previous theorem, this result asserts that the rate $m^{1-1/p}$ is optimal in the $L^{\\infty}$ -norm when $\\boldsymbol{\\wp}$ is a Hilbert space. However, it is strictly weaker than Theorem 4.1 as it only considers i.i.d. random sampling from $\\mu$ , as opposed to arbitrary (adaptive) samples. Note that Theorem 4.1 is an extension of [7, Thm. 4.4]. Theorem 4.2 is new, and is of independent interest since it partially addresses an open problem of [7] about deriving lower bounds in the $L_{\\mu}^{\\infty}$ -norm, as opposed to just the $L_{\\mu}^{2}$ -norm. See $\\S C.2.3\u2013C.2.4$ for more discussion. ", "page_idx": 7}, {"type": "text", "text": "5 Numerical experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now present numerical results for DL applied to various different parametric PDE problems, as in Example 2.3. For a full description of our experimental setup, see $\\S\\mathrm{A}{-}\\mathrm{B}$ . ", "page_idx": 7}, {"type": "text", "text": "Since the main objective of this work is to examine the approximation error, we follow a standard setup and fix the encoder and decoders for each experiment, so that $\\mathcal{E}_{\\mathcal{X}}$ and $\\mathcal{D}_{\\mathcal{Y}}$ in (1.3) do not change for different choices of $\\widehat{N}$ . We also set up our experiments so that encoding-decoding (3.7) and sampling (3.8) errors are  zero. We do this in a standard way. To ensure that $E_{\\mathcal{X},q}=0$ , we truncate the parametric expansions (2.8) after $d$ terms (henceforth termed the parametric dimension) and define the encoder $\\mathcal{E}_{\\mathcal{X}}$ accordingly. This means we effectively consider a parametric PDE depending on finitely-many parameters. We use Finite Element Methods (FEMs) to both solve the PDE (for generating training and testing data) and define the decoder $\\mathcal{D}_{\\mathcal{Y}}$ (see (A.2)). To ensure that $E_{\\mathcal{Y},q}=0$ , we compute errors with respect to the Bochner $L_{\\mu}^{2}(\\mathcal{X};\\tilde{\\mathcal{Y}})$ -norm, where $\\widetilde{y}=\\mathcal{D}_{\\mathcal{Y}}(\\mathbb{R}^{d_{\\mathcal{Y}}})$ is the FEM discretization of $\\boldsymbol{\\wp}$ . In other words, we use the same FE M code to genera te test data and compute the errors as we do to construct the operator approximationF. See $\\S\\mathrm{A.l}$ for further details. ", "page_idx": 7}, {"type": "text", "text": "The DNNs in our experiments are fully-connected and of the form (2.1). We denote by $\\sigma~L\\times N$ DNN a DNN $\\widehat{N}$ with activation function $\\sigma$ , width $N$ and depth $L$ . To solve (2.5) we use Adam [47] with early stopping and an exponentially decaying learning rate. We train our DNN architectures for 60,000 epochs and results are averaged over a number of trials. See $\\S\\mathrm{A}.2$ for further details. ", "page_idx": 7}, {"type": "text", "text": "Parametric elliptic diffusion equation. Our first example is the parametric elliptic diffusion equation (2.7). This PDE arises in many scientific computing applications, such as groundwater flow modelling, see, e.g., [95]. We describe the full PDE and its FE discretization in $\\S B.3$ . In our experiments, we consider both affine (B.1) and log-transformed (B.2) diffusion coefficients. The latter is particularly useful in the groundwater flow problem as the permeability of various layers of sediment can vary on logarithmic scales. Differing from most prior work, we consider a novel mixed variational formulation [32] of (2.7), which has a number of key practical benefits (see $\\S B.3.1\\$ ). In this case, $\\mathrm{\\Delta\\y=L^{2}(\\Omega)}$ is a Hilbert space. Fig. 1 compares the error versus the amount of training data $m$ for various DNN architectures for learning the solution map of this PDE in $d=4$ and $d=8$ parametric dimensions with these two diffusion coefficients. We observe that architectures with the Exponential Linear Unit (ELU) or hyperbolic tangent (tanh) activation generally outperform similar architectures with the Rectified Linear Unit (ReLU) activation (as we discuss in Remark D.11, this difference is in agreement with our theoretical analysis). Overall, the best performing DNNs appear to roughly match the plotted rate $m^{-1}$ . As we explain further in $\\S B.3.2$ , this rate is precisely that predicted by our theory. In particular, the parametric solution map (recall Remark 2.4) is $(b,\\varepsilon)$ -holomorphic with $\\pmb{b}\\in\\ell_{\\mathsf{M}}^{p}(\\mathbb{N})$ for any $p<2/3$ , giving an effective convergent rate $m^{1/2-1/p}$ that is arbitrarily close to $m^{-1}$ . Another important fact that we observe is that despite the parametric dimension doubling from 4 to 8, there is little change in the error behaviour. ", "page_idx": 7}, {"type": "text", "text": "Parametric Navier-Stokes-Brinkman equations. We next consider the parametric Navier-StokesBrinkman (NSB) equations. See $\\S B.4$ and (B.14) for the full definition. Here the solution is a pair $({\\pmb u},p)$ , where $\\textbf{\\em u}$ is the velocity field and $p$ is the pressure. These equations describe the dynamics of a viscous fluid flowing through porous media with random viscosity. See, e.g., [28, 43, 46, 94]. We use a mixed variational formulation [34] to discretize the PDE. This formulation is more sophisticated that standard variational formulations, but conveys various practical advantages. Unlike the previous example, it leads to $\\boldsymbol{\\wp}$ being either $\\boldsymbol{\\mathcal{V}}=\\mathbf{L}^{4}(\\Omega)$ for $\\textbf{\\em u}$ or $\\dot{\\boldsymbol{\\mathcal{V}}}=\\mathrm{L}^{2}(\\boldsymbol{\\Omega})$ for $p$ . See $\\S B.4.1$ for details. Fig. 2 compares a variety of DNN architectures for approximating the velocity field component in $d\\,=\\,4$ and $d\\,=\\,8$ parametric dimensions. Here again we observe the ELU and tanh DNN architectures outperform similar sized ReLU architectures. We also observe a rate close to $m^{-1}$ . Note that it is currently unknown whether this or the next example possess the same $(b,\\varepsilon)$ -holomorphy guarantee as that of the previous example. Yet we observe the same rate, and therefore conjecture that such a property does indeed hold in these cases. Similar to the previous example, there is also no deterioration of the rate when moving from $d=4$ to $d=8$ . ", "page_idx": 7}, {"type": "image", "img_path": "vBlzen37i0/tmp/b45e90afb7ac5be92eb682b80ae7fac3d0046fa9319909b39b5933126848e042.jpg", "img_caption": ["Figure 1: Elliptic diffusion equation. Average relative $L_{\\mu}^{2}(\\mathcal{X};\\widetilde{\\mathcal{Y}})$ -norm error versus $m$ for different DNNs approximating the solution operator for the elliptic diffusion equ ation (B.9). The first two plots use the affine coefficient $^{a_{1,d}}$ (B.1) with $d=4,8$ , respectively. The rest use the log-transformed coefficient $^{a_{2,d}}$ (B.2). "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "vBlzen37i0/tmp/294d171e5a641704c9e2c5598ac8bd0dfb4d82381d2f03e57b4df9a017984ea9.jpg", "img_caption": ["Figure 2: NSB equations. Average relative $L_{\\mu}^{2}(\\mathcal{X};\\tilde{\\mathcal{Y}})$ -norm error versus $m$ for different DNNs approximating the velocity field $\\pmb{u}$ of the NSB problem in (B.14) . See Fig. 7 for results for the pressure component $p$ . The diffusion coefficients $a_{1,d},a_{2,d}$ and $d=4,8$ are as in Fig. 1. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Parametric stationary Boussinesq equation. Our final example is a parametric stationary Boussinesq PDE. See $\\S B.5$ and (B.16) for the full definition. Here the solution is a triplet $\\left(\\pmb{u},\\varphi,p\\right)$ , where $\\textbf{\\em u}$ is the velocity field, $\\varphi$ is the temperature and $p$ is the pressure of the solution. The Boussinesq model arises in a variety of engineering, fluid dynamics and natural convection problems where changes in temperature affect the velocity of a fluid [14, 22, 39]. Similar to the previous example, we consider a fully mixed variational formulation (see $\\S B.5.1)$ , which leads to $\\dot{\\boldsymbol{\\mathcal{V}}}=\\mathbf{L}^{4}(\\Omega)$ (for $\\textbf{\\em u}$ ), $\\mathcal{V}=\\mathrm{L}^{4}(\\Omega)$ (for $\\varphi$ ) or $\\mathcal{V}=\\mathrm{L}_{0}^{2}(\\Omega)$ (for $p$ ). Fig. 3 provides numerical results. Our observations are in line with the previous two examples, with the ELU and the smaller tanh networks being most often the best performers in this problem. Once more, the errors roughly correspond to the rate $m^{-1}$ and there is no deterioration with increasing $d$ . ", "page_idx": 8}, {"type": "text", "text": "6 Conclusions and limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The purpose of this work was to derive near-optimal generalization bounds for learning certain classes of holomorphic operators that arise frequently in operator learning tasks involving PDEs. Complementing and extending previous works [26, 31, 41, 55, 71] on the approximation of such operators via DNNs, we showing sharp algebraic rates of convergence in $m$ , thus confirming that such operators can be learned efficiently and without the curse of dimensionality. It is notable that the sizes of the various DNNs in Theorems 3.1-3.2 also do not succumb to the so-called curse of parametric complexity [54], since the width and depth bounds are at most algebraic in $m$ . ", "page_idx": 8}, {"type": "image", "img_path": "vBlzen37i0/tmp/af266a50feaf866f559beb4501d3e2e38cb5ac788318f0c36912cb3c5776fd8d.jpg", "img_caption": ["Figure 3: Boussinesq equation. Average relative $L_{\\mu}^{2}(\\mathcal{X};\\widetilde{\\mathcal{Y}})$ -norm error versus $m$ for different DNNs approximating the temperature $\\varphi$ of the Boussinesq problem in (B.16) (see Fig. 9 for $\\mathbf{\\delta}_{\\mathbf{\\lambda}}$ and $p$ ). The diffusion coefficients $a_{1,d},a_{2,d}$ and $d=4,8$ are as in Fig. 1. In this example, we also consider an additional parametric dependence in the tensor $\\mathbb{K}=\\mathbb{K}_{d}$ describing the thermal conductivity of the fluid. See $\\S B.5$ and (B.17). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "We end by discussing a number of limitations. First, assumption (A.I) may not hold in some applications. The domain $D$ can easily be replaced by bounded hyperrectangle through rescaling and the condition that $\\varsigma$ be quasi-uniform relaxed to quasi-ultraspherical (by considering ultraspherical polynomials). However, it is currently an open problem whether our results can be extended to the case where $\\mu$ is Gaussian, in which case $\\varsigma$ would typically be a tensor-product Gaussian measure on $\\mathbb{R}^{\\mathbb{N}}$ and the relevant polynomials would be the Hermite polynomials. Second, the reader may have noticed that the encoder $\\mathcal{E}_{\\mathcal{X}}$ defined in (A.III) and used to construct the approximation (2.5) involves the pair $(\\widetilde{\\mathcal{E}}_{\\mathcal{X}},\\widetilde{\\mathcal{D}}_{\\mathcal{X}})$ and the map $\\iota_{d_{\\mathcal{X}}}$ . This is a technical requirement \u2013 also found in other theoretical works on op erator learning \u2013 needed to obtain encoding-decoding errors of the form $E_{X,q}$ $q=2,\\infty$ . It is unknown whether it can be relaxed. It is also unknown whether the assumption on $\\tilde{\\varsigma}$ in (A.III) can be relaxed. We believe this can be done, at least if the $L_{\\mu}^{2}$ -norm in (3.2) is replaced by the $L_{\\mu}^{\\infty}$ -norm. Whether this is possible without modifying (3.2) is currently unknown. ", "page_idx": 9}, {"type": "text", "text": "Third, a limitation of Theorem 3.2 is that it only asserts that some minimizers are \u2018good\u2019, not all. Techniques from statistical learning theory can provide stronger bounds that hold for all minimizers. Yet, as noted in $\\S1.2$ , these tools typically produce slower rates of decay in $m$ . Overcoming this limitation \u2013 e.g., by refining these tools for the holomorphic setting or showing that the \u2018good\u2019 minimizers can indeed be obtained via standard training \u2013 is a topic of future work. ", "page_idx": 9}, {"type": "text", "text": "Finally, as noted, our theorems provided worse generalization bounds when $\\boldsymbol{\\wp}$ is a Banach space than when $\\boldsymbol{\\wp}$ is a Hilbert space. Our numerical results in Figs. 2-3 suggest that this factor is an artefact of the proofs. Whether it can be removed is an interesting open problem. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "BA acknowledges the support of the Natural Sciences and Engineering Research Council of Canada of Canada (NSERC) through grant RGPIN-2021-611675. ND acknowledges the support of Florida State University through the CRC 2022-2023 FYAP grant program. The authors would like to thank Gregor Maier for helpful comments and feedback. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] B. Adcock and N. Dexter. The gap between theory and practice in function approximation with deep neural networks. SIAM J. Math. Data Sci., 3(2):624\u2013655, 2021.   \n[2] B. Adcock, S. Brugiapaglia, N. Dexter, and S. Moraga. Deep neural networks are effective at learning high-dimensional Hilbert-valued functions from limited data. In J. Bruna, J. S. Hesthaven, and L. Zdeborov\u00e1, editors, Proceedings of The Second Annual Conference on Mathematical and Scientific Machine Learning, volume 145 of Proc. Mach. Learn. Res. (PMLR), pages 1\u201336. PMLR, 2021.   \n[3] B. Adcock, S. Brugiapaglia, and C. G. Webster. Sparse Polynomial Approximation of HighDimensional Functions. Comput. Sci. Eng. Society for Industrial and Applied Mathematics, Philadelphia, PA, 2022.   \n[4] B. Adcock, N. Dexter, and S. Moraga. Optimal approximation of infinite-dimensional holomorphic functions II: recovery from i.i.d. pointwise samples. arXiv:2310.16940, 2023.   \n[5] B. Adcock, S. Brugiapaglia, N. Dexter, and S. Moraga. Near-optimal learning of Banachvalued, high-dimensional functions via deep neural networks. Neural Networks (in press), 2024.   \n[6] B. Adcock, S. Brugiapaglia, N. Dexter, and S. Moraga. Learning smooth functions in high dimensions: from sparse polynomials to deep neural networks. In S. Mishra and A. Townsend, editors, Numerical Analysis Meets Machine Learning, volume 25 of Handbook of Numerical Analysis, pages 1\u201352. Elsevier, 2024.   \n[7] B. Adcock, N. Dexter, and S. Moraga. Optimal approximation of infinite-dimensional holomorphic functions. Calcolo, 61(1):12, 2024. [8] C. D. Aliprantis and K. C. Border. Infinite Dimensional Analysis: A Hitchhiker\u2019s Guide. Springer\u2013Verlag, Berlin, Heidelberg, 3rd edition, 2006.   \n[9] S. Aln\u00e6s, J. Blechta, J. Hake, A. Johansson, B. Kehlet, A. Logg, C. Richardson, J. Ring, M. E. Rognes, and G. N. Wells. The FEniCS Project Version 1.5. Archive of Numerical Software, 3 (100), 2015.   \n[10] K. Bhattacharya, B. Hosseini, N. B. Kovachki, and A. M. Stuart. Model reduction and neural networks for parametric PDEs. SMAI J. Comput. Math., 7:121\u2013157, 2021.   \n[11] D. Boff,i F. Brezzi, and M. Fortin. Mixed Finite Element Methods and Applications. Springer, Berlin, Heidelberg, 1st edition, 2013.   \n[12] S. Brugiapaglia, S. Dirksen, H. C. Jung, and H. Rauhut. Sparse recovery in bounded Riesz systems with applications to numerical methods for PDEs. Appl. Comput. Harmon. Anal., 53: 231\u2013269, 2021.   \n[13] S. Cai, Z. Wang, L. Lu, T. A. Zaki, and G. E. Karniadakis. DeepM&Mnet: Inferring the electroconvection multiphysics fields based on operator approximation by neural networks. J. Comput. Phys., 436:110296, 2021.   \n[14] C. Cao and J. Wu. Global regularity for the two-dimensional anisotropic Boussinesq equations with vertical dissipation. Arch. Rational Mech. Anal., 208:985\u20131004, 2013.   \n[15] Q. Cao, S. Goswami, and G. E. Karniadakis. LNO: Laplace neural operator for solving differential equations. arXiv:2303.10528, 2023.   \n[16] K. Chen, C. Wang, and H. Yang. Deep operator learning lessens the curse of dimensionality for pdes. arXiv:2301.12227, 2023.   \n[17] A. Chkifa, A. Cohen, and C. Schwab. Breaking the curse of dimensionality in sparse polynomial approximation of parametric PDEs. J. Math. Pures Appl., 103(2):400\u2013428, 2015.   \n[18] P. G. Ciarlet. The Finite Element Method for Elliptic Problems. Society for Industrial and Applied Mathematics, Philadelphia, PA, 2002.   \n[19] A. Cohen and R. A. DeVore. Approximation of high-dimensional parametric PDEs. Acta Numer., 24:1\u2013159, 2015.   \n[20] E. Colmenares, G. N. Gatica, and S. Moraga. A Banach spaces-based analysis of a new fully-mixed finite element method for the Boussinesq problem. ESAIM Math. Model. Numer. Anal., 54(5):1525\u20131568, 2020.   \n[21] D. D\u02dcung, V. K. Nguyen, and D. T. Pham. Deep ReLU neural network approximation in Bochner spaces and applications to parametric PDEs. J. Complexity, 79:101779, 2023.   \n[22] I. Danaila, R. Moglan, F. Hecht, and S. Le Masson. A newton method with adaptive finite elements for solving phase-change problems with natural convection. J. Comput. Phys., 274: 826\u2013840, 2014.   \n[23] J. Daws and C. Webster. Analysis of deep neural networks with quasi-optimal polynomial approximation rates. arXiv:1912.02302, 2019.   \n[24] M. De Hoop, D. Z. Huang, E. Qian, and A. Stuart. The cost-accuracy trade-off in operator learning with neural networks. J. Mach. Learn., 1:299\u2013341, 2022.   \n[25] T. De Ryck, S. Lanthaler, and S. Mishra. On the approximation of functions by tanh neural networks. Neural Networks, 143:732\u2013750, 2021.   \n[26] B. Deng, Y. Shin, L. Lu, Z. Zhang, and G. E. Karniadakis. Approximation rates of DeepONets for learning operators arising from advection\u2013diffusion equations. Neural Networks, 153: 411\u2013426, 2022.   \n[27] N. Dexter, H. Tran, and C. Webster. A mixed $\\ell_{1}$ regularization approach for sparse simultaneous approximation of parameterized PDEs. ESAIM Math. Model. Numer. Anal., 53:2025\u20132045, 2019.   \n[28] Y. Dutil, D. R. Rousse, N. B. Salah, S. Lassue, and L. Zalewski. A review on phase-change materials: Mathematical modeling and simulations. Renew. Sustain. Energy Rev., 15(1): 112\u2013130, 2011.   \n[29] S. Foucart and H. Rauhut. A Mathematical Introduction to Compressive Sensing. Appl. Numer. Harmon. Anal. Birkh\u00e4user, New York, NY, 2013.   \n[30] N. R. Franco and S. Brugiapaglia. A practical existence theorem for reduced order models based on convolutional autoencoders. arXiv:2402.00435, 2024.   \n[31] N. R. Franco, S. Fresca, A. Manzoni, and P. Zunino. Approximation bounds for convolutional neural networks in operator learning. Neural Networks, 161:129\u2013141, 2023.   \n[32] G. N. Gatica. A Simple Introduction to the Mixed Finite Element Method. SpringerBriefs Math. Springer, Cham, Switzerland, 2014.   \n[33] G. N. Gatica, R. Oyarz\u00faa, R. Ruiz-Baier, and Y. D. Sobral. Banach spaces-based analysis of a fully-mixed finite element method for the steady-state model of fluidized beds. Comput. Math. Appl., 84:244\u2013276, 2021.   \n[34] G. N. Gatica, N. Nu\u00f1ez, and R. Ruiz-Baier. New non-augmented mixed finite element methods for the Navier\u2013Stokes\u2013Brinkman equations using Banach spaces. J. Numer. Math., 31(4): 343\u2013373, 2022.   \n[35] S. Goswami, K. Kontolati, M. D. Shields, and G. E. Karniadakis. Deep transfer operator learning for partial differential equations under conditional shift. Nat. Mach. Intell., 4(12): 1155\u20131164, 2022.   \n[36] S. Goswami, D. S. Li, B. V. Rego, M. Latorre, J. D. Humphrey, and G. E. Karniadakis. Neural operator learning of heterogeneous mechanobiological insults contributing to aortic aneurysms. J. R. Soc. Interface, 19(193):20220410, 2022.   \n[37] S. Goswami, A. D. Jagtap, H. Babaee, B. T. Susi, and G. E. Karniadakis. Learning stiff chemical kinetics using extended deep neural operators. arXiv:2302.12645, 2023.   \n[38] I. G\u00fchring and M. Raslan. Approximation rates for neural networks with encodable weights in smoothness spaces. Neural Networks, 134:107\u2013130, 2021.   \n[39] Z. Guo, B. Shi, and C. Zheng. A coupled lattice BGK model for the Boussinesq equations. Int. J. Numer. Meth. Fluids, 39:325\u2013342, 2002.   \n[40] Z. Hao, Z. Wang, H. Su, C. Ying, Y. Dong, S. Liu, Z. Cheng, J. Song, and J. Zhu. GNOT: A General Neural Operator Transformer for Operator Learning. In International Conference on Machine Learning, pages 12556\u201312569. PMLR, 2023.   \n[41] L. Herrmann, C. Schwab, and J. Zech. Neural and spectral operator surrogates: unified construction and expression rate bounds. Adv. Comput. Math., 50:72, 2024.   \n[42] A. A. Howard, M. Perego, G. E. Karniadakis, and P. Stinis. Multifidelity deep operator networks for data-driven and physics-informed problems. J. Comput. Phys., 493:112462, 2024.   \n[43] W. R. Hwang and S. G. Advani. Numerical simulations of Stokes\u2013Brinkman equations for permeability prediction of dual scale fibrous porous media. Phys. Fluids., 22:113101, 2010.   \n[44] T. Hyt\u00f6nen, J. van Neerven, M. Veraar, and L. Weis. Analysis in Banach Spaces, Volume I: Martingales and Littlewood\u2013Paley Theory. Ergebnisse der Mathematik und ihrer Grenzgebiete. 3. Folge. Springer, Cham, 2016.   \n[45] G. E. Karniadakis, I. G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang, and L. Yang. Physicsinformed machine learning. Nature Reviews Physics, 3(6):422\u2013440, 2021.   \n[46] K. Khadra, P. Angot, S. Parneix, and J. P. Caltagirone. Fictitious domain approach for numerical modelling of Navier-Stokes equations. Int. J. Numer. Meth. Fluids, 34:651\u2013684, 2000.   \n[47] D. P. Kingma and J. Ba. Adam: a method for stochastic optimization. arXiv:1412.6980, 2017.   \n[48] K. Kontolati, S. Goswami, M. D. Shields, and G. E. Karniadakis. On the influence of overparameterization in manifold based surrogates and deep neural operators. J. Comput. Phys., 479:112008, 2023.   \n[49] S. Koric, A. Viswantah, D. W. Abueidda, N. A. Sobh, and K. Khan. Deep learning operator network for plastic deformation with variable loads and material properties. Eng. Comput., pages 1\u201313, 2023.   \n[50] N. Kovachki, S. Lanthaler, and S. Mishra. On universal approximation and error bounds for Fourier neural operators. J. Mach. Learn. Res., 22(1):13237\u201313312, 2021.   \n[51] N. B. Kovachki, Z. Li, B. Liu, K. Azizzadenesheli, K. Bhattacharya, A. M. Stuart, and A. Anandkumar. Neural operator: learning maps between function spaces with applications to PDEs. J. Mach. Learn. Res., 24(89):1\u201397, 2023.   \n[52] N. B. Kovachki, S. Lanthaler, and A. M. Stuart. Operator learning: algorithms and analysis. In S. Mishra and A. Townsend, editors, Numerical Analysis Meets Machine Learning, volume 25 of Handbook of Numerical Analysis, pages 419\u2013467. Elsevier, 2024.   \n[53] S. Lanthaler. Operator learning with PCA-Net: upper and lower complexity bounds. arXiv:2303.16317, 2023.   \n[54] S. Lanthaler and A. M. Stuart. The parametric complexity of operator learning. arXiv:2306.15924, 2024.   \n[55] S. Lanthaler, S. Mishra, and G. E. Karniadakis. Error estimates for DeepONets: A deep learning framework in infinite dimensions. Trans. Math. Appl., 6(1):tnac001, 2022.   \n[56] S. Lanthaler, Z. Li, and A. M. Stuart. The nonlocal neural operator: universal approximation. arXiv:2304.13221, 2023.   \n[57] B. Li, S. Tang, and H. Yu. Better approximations of high dimensional smooth functions by deep neural networks with rectified power units. Commun. Comput. Phys., 27:379\u2013411, 2020.   \n[58] Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anandkumar. Neural operator: graph kernel network for partial differential equations. In ICLR, 2020.   \n[59] Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, A. Stuart, K. Bhattacharya, and A. Anandkumar. Multipole graph neural operator for parametric partial differential equations. Advances in Neural Information Processing Systems, 33:6755\u20136766, 2020.   \n[60] Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anandkumar. Fourier neural operator for parametric partial differential equations. In ICLR, 2021.   \n[61] Z. Li, M. Liu-Schiaffini, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anandkumar. Learning chaotic dynamics in dissipative systems. Advances in Neural Information Processing Systems, 35:16768\u201316781, 2022.   \n[62] Z. Li, N. B. Kovachki, C. Choy, B. Li, J. Kossaif,i S. P. Otta, M. A. Nabian, M. Stadler, C. Hundt, K. Azizzadenesheli, and A. Anandkumar. Geometry-informed neural operator for large-scale 3D PDEs. arXiv:2309.00583, 2023.   \n[63] S. Liang and R. Srikant. Why deep neural networks for function approximation? In ICLR, 2017.   \n[64] C. Lin, Z. Li, L. Lu, S. Cai, M. Maxey, and G. E. Karniadakis. Operator learning for predicting multiscale bubble growth dynamics. J. Chem. Phys., 154(10), 2021.   \n[65] C. Lin, M. Maxey, Z. Li, and G. E. Karniadakis. A seamless multiscale operator neural network for inferring bubble dynamics. J. Fluid Mech., 929:A18, 2021.   \n[66] H. Liu, H. Yang, M. Chen, T. Zhao, and W. Liao. Deep nonparametric estimation of operators between infinite dimensional spaces. J. Mach. Learn. Res., 25:1\u201367, 2024.   \n[67] J. Lu, Z. Shen, H. Yang, and S. Zhang. Deep network approximation for smooth functions. SIAM J. Math. Anal., 53(5):5465\u20135506, 2021.   \n[68] L. Lu, P. Jin, and G. E. Karniadakis. DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators. arXiv:1910.03193, 2019.   \n[69] L. Lu, P. Jin, G. Pang, Z. Zhang, and G. E. Karniadakis. Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators. Nat. Mach. Intell., 3 (3):218\u2013229, 2021.   \n[70] L. Lu, X. Meng, S. Cai, Z. Mao, S. Goswami, Z. Zhang, and G. E. Karniadakis. A comprehensive and fair comparison of two neural operators (with practical extensions) based on fair data. Comput. Methods Appl. Mech. Engrg., 393:114778, 2022.   \n[71] C. Marcati and C. Schwab. Exponential convergence of deep operator networks for elliptic partial differential equations. SIAM J. Numer. Anal., 61(3):1513\u20131545, 2023.   \n[72] H. Mhaskar. Neural networks for optimal approximation of smooth and analytic functions. Neural Comput., 8(1):164\u2013177, 1996.   \n[73] K. Micha\u0142owska, S. Goswami, G. E. Karniadakis, and S. Riemer-S\u00f8rensen. Neural operator learning for long-time integration in dynamical systems with recurrent neural networks. arXiv:2303.02243, 2023.   \n[74] S. Moraga. Optimal and efficient algorithms for learning high-dimensional, Banach-valued functions from limited samples. PhD thesis, Simon Fraser University, 2024.   \n[75] H. H. Nguyen and V. H. Vu. Normal vector of a random hyperplane. Int. Math. Res. Not. IMRN, 2018(6):1754\u20131778, 2018.   \n[76] F. Nobile, R. Tempone, and C. G. Webster. A sparse grid stochastic collocation method for partial differential equations with random input data. SIAM J. Numer. Anal., 46(5):2309\u20132345, 2008.   \n[77] V. Oommen, K. Shukla, S. Goswami, R. Dingreville, and G. E. Karniadakis. Learning twophase microstructure evolution using neural operators and autoencoder architectures. npj Comput. Mater., 8(1):190, 2022.   \n[78] J. A. A. Opschoor and C. Schwab. Deep ReLU networks and high-order finite element methods II: Chebyshev emulation. arXiv:2310.07261, 2023.   \n[79] J. A. A. Opschoor, C. Schwab, and J. Zech. Exponential ReLU DNN expression of holomorphic maps in high dimension. Constr. Approx., 55(1):537\u2013582, 2022.   \n[80] J. D. Osorio, Z. Wang, G. Karniadakis, S. Cai, C. Chryssostomidis, M. Panwar, and R. Hovsapian. Forecasting solar-thermal systems performance under transient operation using a data-driven machine learning approach based on the deep operator network architecture. Energy Conversion and Management, 252:115063, 2022.   \n[81] W. Peng, Z. Yuan, Z. Li, and J. Wang. Linear attention coupled fourier neural operator for simulation of three-dimensional turbulence. Physics of Fluids, 35(1), 2023.   \n[82] M. Pinkus. N-widths in Approximation Theory. Springer\u2013Verlag, Berlin, 1968.   \n[83] B. Raoni\u00b4c, R. Molinaro, T. Rohner, S. Mishra, and E. de Bezenac. Convolutional neural operators. In ICLR, 2023.   \n[84] P. I. Renn, C. Wang, S. Lale, Z. Li, A. Anandkumar, and M. Gharib. Forecasting subcritical cylinder wakes with Fourier neural operators. arXiv:2301.08290, 2023.   \n[85] C. Schwab and J. Zech. Deep learning in high dimension: neural network expression rates for generalized polynomial chaos expansions in UQ. Anal. Appl. (Singap.), 17(1):19\u201355, 2019.   \n[86] C. Schwab and J. Zech. Deep learning in high dimension: neural network expression rates for analytic functions in $L^{2}(\\mathbb{R}^{d},\\gamma_{d})$ . SIAM/ASA J. Uncertain. Quantif., 11(1):199\u2013234, 2023.   \n[87] C. Schwab, A. Stein, and J. Zech. Deep operator network approximation rates for Lipschitz operators. arXiv:2307.09835, 2023.   \n[88] M. Stoyanov. User manual: Tasmanian sparse grids. Technical Report ORNL/TM-2015/596, Oak Ridge National Laboratory, One Bethel Valley Road, Oak Ridge, TN, 2015.   \n[89] M. Stoyanov, D. Lebrun-Grandie, J. Burkardt, and D. Munster. Tasmanian, 9 2013. URL https://github.com/ORNL/Tasmanian.   \n[90] M. K. Stoyanov and C. G. Webster. A dynamically adaptive sparse grid method for quasioptimal interpolation of multidimensional functions. Comput. Math. Appl., 71(11):2449\u20132465, 2016.   \n[91] A. M. Stuart. Inverse problems: a Bayesian perspective. Acta Numer., 19:451\u2013559, 2010.   \n[92] S. Tang, B. Li, and Y. Haijun. ChebNet: efficient and stable constructions of deep neural networks with rectified power units via Chebyshev approximation. Commun. Math. Stat. (In press), 2024.   \n[93] T. Tripura and S. Chakraborty. Wavelet neural operator for solving parametric partial differential equations in computational mechanics problems. Comput. Methods Appl. Mech. Engrg., 404:115783, 2023.   \n[94] S. Wang, A. Faghri, and T. L. Bergman. A comprehensive numerical model for melting with natural convection. Int. J. Heat Mass Transfer., 53(9-10):1986\u2013200, 2010.   \n[95] C. L. Winter and D. M. Tartakovsky. Groundwater flow in heterogeneous composite aquifers. Water Resour. Res., 38(8), 2002.   \n[96] K. Wu, T. O\u2019Leary-Roseberry, P. Chen, and O. Ghattas. Large-scale Bayesian optimal experimental design with derivative-informed projected neural network. J. Sci. Comput., 95 (1):30, 2023.   \n[97] D. Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Networks, 94:103\u2013114, 2017.   \n[98] E. Yeung, S. Kundu, and N. Hodas. Learning deep neural network representations for Koopman operators of nonlinear dynamical systems. In 2019 American Control Conference (ACC), pages 4832\u20134839. IEEE, 2019.   \n[99] M. Yin, E. Ban, B. V. Rego, E. Zhang, C. Cavinato, J. D. Humphrey, and G. Em Karniadakis. Simulating progressive intramural damage leading to aortic dissection using DeepONet: an operator\u2013regression neural network. Journal of the Royal Society Interface, 19(187):20210670, 2022.   \n[100] M. Yin, E. Zhang, Y. Yu, and G. E. Karniadakis. Interfacing finite elements with deep neural operators for fast multiscale modeling of mechanics problems. Comput. Methods Appl. Mech. Engrg., 402:115027, 2022.   \n[101] E. Zappala, A. H. d. O. Fonseca, A. H. Moberly, M. J. Higley, C. Abdallah, J. A. Cardin, and D. van Dijk. Neural integro-differential equations. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 11104\u201311112, 2023.   \n[102] J. Zech. Sparse-grid approximation of high-dimensional parametric PDEs. PhD thesis, ETH Zurich, 2018.   \n[103] Z. Zhang, L. Wing Tat, and H. Schaeffer. Belnet: basis enhanced learning, a mesh-free neural operator. Proc. R. Soc. A, 479(2276):20230043, 2023.   \n[104] M. Zhu, H. Zhang, A. Jiao, G. E. Karniadakis, and L. Lu. Reliable extrapolation of deep neural operators informed by physics or sparse observations. Comput. Methods Appl. Mech. Engrg., 412:116064, 2023.   \n[105] Z. Zou, X. Meng, A. F. Psaros, and G. E. Karniadakis. NeuralUQ: A comprehensive library for uncertainty quantification in neural differential equations and operators. SIAM Rev., 66(1): 161\u2013190, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Experimental setup ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we describe our experimental setup. ", "page_idx": 16}, {"type": "text", "text": "A.1 Formulation of the learning problems ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We first explain how all our experiments are formulated as operator learning problems. We do this in a standard way, by first truncating the random field which generates the measure $\\mu$ , and then using an FEM to discretize the output space. ", "page_idx": 16}, {"type": "text", "text": "All our examples follow Example (2.3) and involve operators of the form (2.6), where $\\mathcal{F}_{a}$ represents a different type of PDE in each case (specifically, either an elliptic diffusion, Navier-Stokes-Brinkman or Boussinesq PDE). We consider both affine (2.8) and log-transformed parametrizations of the random field $a(x)$ . Thus, in general we write ", "page_idx": 16}, {"type": "equation", "text": "$$\na({\\pmb x})=a(\\cdot;{\\pmb x})=g\\left(a_{0}(\\cdot)+\\sum_{i=1}^{\\infty}c_{i}x_{i}\\phi_{i}(\\cdot)\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $g:\\mathbb{R}\\rightarrow\\mathbb{R}$ is a (measurable) map. In the affine case $g(t)=1$ . In the log-transformed case $g(t)=\\exp(t)$ . ", "page_idx": 16}, {"type": "text", "text": "As discussed, since the main objective of this work is to examine the approximation error, we set up our experiments so that encoding-decoding errors are zero. We do this as follows. First, we fix a parametric dimension $d$ and truncate the expansion in (A.1) after $d$ terms, giving a map $a_{d}:[-1,\\^{\\underline{{\\cdot}}}1]^{d}\\to\\mathcal{X}$ and measure $\\mu=a_{d}\\sharp\\varrho_{d}$ , where $\\varrho_{d}$ is the uniform probability measure on $[-1,1]^{\\bar{d}}$ . We then define the operator $F$ as $F(a_{d}({\\pmb x}))=f({\\pmb x})=u(\\cdot;a_{d}({\\pmb x}))$ , where $u(\\cdot;a)$ is the solution of the PDE $\\mathcal{F}_{a}u=0$ . ", "page_idx": 16}, {"type": "text", "text": "In alignment with our theorems, we focus on the in-distribution performance of the learned approximation $\\widehat F$ . This means we define the encoder only on $\\operatorname{supp}(\\mu)$ , as ${\\mathcal{E}}_{{\\mathcal{X}}}(X)=x$ when $X=a_{d}(\\mathbf{x})$ with $\\pmb{x}\\in[-1,1]^{d}$ . As a result, the encoding-decoding error $E_{X,q}$ in (3.7) satisfies $E_{\\mathcal{X},q}=0$ . ", "page_idx": 16}, {"type": "text", "text": "To perform our simulations, we use FEMs to solve the PDE and discretize the output space $\\boldsymbol{\\wp}$ . Let $\\{\\bar{\\varphi_{i}}\\}_{i=1}^{K}\\subset\\mathcal{V}$ be a FEM basis and set $d y=K$ . Then we define the decoder as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{D}_{\\mathcal{Y}}:\\mathbb{R}^{K}\\rightarrow\\mathcal{Y},\\quad\\mathcal{D}_{\\mathcal{Y}}(\\pmb{c})=\\sum_{i=1}^{K}c_{i}\\varphi_{i}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and set $\\widetilde{\\mathcal{V}}=\\mathcal{D}_{\\mathcal{V}}(\\mathbb{R}^{K})$ as the discretization of $\\boldsymbol{\\wp}$ . ", "page_idx": 16}, {"type": "text", "text": "With this in hand, we now describe the simulation of training data in general terms. First, we draw $\\pmb{x}_{1},\\pmb{\\ldots},\\pmb{x}_{m}\\sim_{\\mathrm{i.i.d.}}$ $\\varrho_{d}$ . Then, for each training sample $\\pmb{x}_{i}$ , we compute $Y_{i}$ by using the FEM to solve the PDE with parameter $X_{i}=a_{d}(\\pmb{x}_{i})$ . Notice that $Y_{i}\\in\\widetilde{\\mathcal{Y}}$ in this setup. ", "page_idx": 16}, {"type": "text", "text": "We consider a DNN architecture with input dimension $n_{0}=d$ and output dimension $n_{L+2}=K$ . After training, we evaluate the learned approximation $\\widehat{F}=\\mathcal{D}_{\\mathcal{Y}}\\circ\\widehat{N}\\circ\\mathcal{E}_{\\mathcal{X}}$ over $\\operatorname{supp}(\\mu)$ as ${\\widehat{F}}(X)=$ $\\mathcal{D}_{\\mathcal{Y}}\\circ\\widehat{N}\\circ\\mathcal{E}_{\\dot{\\mathcal{X}}}(X)=\\mathcal{D}_{\\mathcal{Y}}\\circ\\widehat{N}(\\pmb{x})$ for $X=a_{d}(\\pmb{x})$ with $\\pmb{x}\\in[-1,1]^{d}$ . Finally, as noted, we us the same FEM discretization to generate testing data, which allows us to measure the error with respect to the $L_{\\mu}^{2}(\\mathcal{X};\\widetilde{\\mathcal{Y}})$ -norm. This effectively means that the encoding-decoding error $E_{\\mathcal{Y},q}$ in (3.7) satisfies $E_{\\mathcal{y},q}=0$ as well. ", "page_idx": 16}, {"type": "text", "text": "A.2 Computational setup for the numerical experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this work, we investigate the trade-off between the accuracy of the learned operator and the number of samples $m$ used in training. Our methodology is summarized as follows. ", "page_idx": 16}, {"type": "text", "text": "(i) Implementation. We use the open-source finite element library FEniCS, specifically version 2019.1.0 [9], and Google\u2019s TensorFlow version 2.12.0. More information about TensorFlow can be found at https://www.tensorflow.org/.   \n(ii) Hardware. We train the DNN models in single precision on the Digital Research Alliance of ", "page_idx": 16}, {"type": "text", "text": "Canada\u2019s Cedar compute cluster (see https://docs.alliancecan.ca/wiki/Cedar), using ", "page_idx": 16}, {"type": "text", "text": "Intel Xenon Processor E5-2683 v4 CPUs with either 125GB or 250GB per node. The setup for each of our PDEs is as follows. For each experiment we consider training with 14 sets of points of size $m\\in\\{10,20,30,40,50,60,70,80,90,100,200,300,400,500\\}$ and for 6 different architectures ( $^{4}\\mathrm{~x~}40$ and $10\\mathrm{~x~}100$ with ReLU, ELU, and tanh activations) over two parametric dimensions ( $d=4$ and $d=8$ ) and two coefficients $(a_{1,d}$ from (B.1) and $^{a_{2,d}}$ from (B.2)), giving 336 DNNs to be trained for each trial. For the Poisson PDE and Navier-Stokes-Brinkman PDEs we run 12 trials. For the Boussinesq PDE we run 8 trials due to the larger problem size. For the Poisson PDE, we allocate 336 nodes with $1\\times32$ core CPUs running 4 threads (totalling 1344 threads, 6.8 GB RAM per node) each running 3 trials, taking approximately 4 hours and 15 minutes to complete. For the Navier-Stokes-Brinkman PDE, we use the same setup allocating 9.88 GB RAM per node and the runs take approximately 9 hours and 13 minutes for each of the two components of the solution to complete. For the Boussinesq PDE, we allocate 336 nodes with $1\\times32$ core CPUs running 4 threads per node (totaling 1344 threads, 10 GB RAM per node) each running 2 trials, taking approximately 12 hours and 32 minutes for each of the 3 components to complete. Given this, the total time required to reproduce the results in parallel with the above setup is approximately 60 hours or 2.5 days. Results were stored locally on the cluster and the estimated total space used to store the data for testing and training and results from computation is approximately 50 GB. Trained models were not retained due to space limitations on the cluster. ", "page_idx": 17}, {"type": "text", "text": "(iii) Choice of architectures and initialization. Based on the strategies in [1], we fix the number of nodes per layer $N$ and depth $L$ such that the ratio $\\beta:=L/N$ is $\\beta=0.5$ . In addition, we initialize the weights and biases using the HeUniform initializer from keras setting the seed to the trial number. We consider the Rectified Linear Unit (ReLU) ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sigma_{1}(z):=\\operatorname*{max}\\{0,z\\},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "hyperbolic tangent (tanh) ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sigma_{2}(z):=\\frac{\\mathrm{e}^{z}-\\mathrm{e}^{-z}}{\\mathrm{e}^{z}+\\mathrm{e}^{-z}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "or Exponential Linear Unit (ELU) ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sigma_{3}(z)=\\binom{z}{\\mathrm{e}^{z}-1}\\quad z\\geq0,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "activation functions in our experiments. ", "page_idx": 17}, {"type": "text", "text": "(iv) Optimizers for training and parametrization. To train the DNNs, we use the Adam optimizer [47], incorporating an exponentially-decaying learning rate. We train our models for 60,000 epochs or until converging to a tolerance level of $\\epsilon_{\\mathrm{tol}}\\,=\\,5\\cdot10^{-7}$ in single precision. In light of the nonmonotonic convergence behavior observed during the minimization of the nonconvex loss (see, e.g., [1, 2]), we implement early stopping. More precisely, we save the weights and biases of the partially trained network once the ratio between the current loss and the last checkpoint loss is reduced below $1/8$ , or if the current weights and biases produce the best loss value observed in training. We then restore these weights after training only if the loss value of the current weights is larger than that of the saved checkpoint. ", "page_idx": 17}, {"type": "text", "text": "(v) Training data and design of experiments. First, we define a \u2018trial\u2019 as a complete training run for a DNN approximating a specific function, initialized as mentioned above. ", "page_idx": 17}, {"type": "text", "text": "Following the setup of $\\S\\mathrm{A.l}$ , we run several trials solving the problem: ", "page_idx": 17}, {"type": "equation", "text": "$\\begin{array}{r}{\\{(X_{i},Y_{i})\\}_{i=1}^{m}\\subset(\\mathcal{X}\\times\\mathcal{Y})^{m},\\;\\;X_{i}\\sim_{\\mathrm{i.i.d.}}\\mu,\\;\\;Y_{i}=F(X_{i})+E_{i}\\in\\mathcal{Y},}\\end{array}$ ", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We generate the measurements $Y_{i}$ using mixed variational formulations of the parametric elliptic, Navier-Stokes-Brinkman and Boussinesq PDEs discretized using FEniCS with input data $X_{i}$ . The noise $E_{i}\\in\\mathcal{V}$ encompasses the discretization errors from numerical solution. Further details of the discretization can be found in $\\S B$ . Each of our architectures is trained across a range of datasets with increasing sizes. This involves using a set of training data consisting of values $\\{(X_{i},Y_{i}))\\}_{i=1}^{m}$ , where $m$ denotes the size of the training data and belongs to the set $\\{10,20,30,40,50,60,7\\hat{0},80,90,100,200,300,400,500\\}$ . After training we calculate the testing error for each trial and run statistics across all trials for each dataset. ", "page_idx": 17}, {"type": "text", "text": "(vi) Testing data and error metric. The testing data is generated similarly to the training data, obtaining solutions at different points $X_{i}\\,\\in\\,{\\mathcal{X}}$ for $i\\,=\\,1,\\ldots,m_{\\mathrm{test}}$ . However, the testing data $\\{(X_{i},Y_{i}=F(X_{i})\\!+\\!E_{i})\\}_{i=1}^{m_{\\mathrm{test}}}$ is generated using a deterministic high-order sparse grid collocation method [76]. In particular, we use sparse grid quadrature rules to compute approximations to the Bochner norms ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|F\\|_{L_{\\mu}^{2}(\\mathcal{X};\\widetilde{\\mathcal{Y}})}=\\left(\\int_{\\mathcal{X}}\\|F(\\boldsymbol{X})\\|_{\\widetilde{\\mathcal{Y}}}^{2}\\,\\mathrm{d}\\mu(\\boldsymbol{X}))\\right)^{1/2}\\approx\\left(\\sum_{i=1}^{m_{\\mathrm{test}}}\\|F(\\boldsymbol{X}_{i})\\|_{\\widetilde{\\mathcal{Y}}}^{2}w_{i}\\right)^{1/2},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mu=a_{d}\\sharp\\varrho_{d}$ is the pushforward measure defined in (A.1) and $w_{i}$ , $i=1,\\dots,m_{\\mathrm{test}}$ are the quadrature weights. We use these approximations to compute the relative $L_{\\mu}^{2}(\\mathcal{X};\\widetilde{\\mathcal{Y}})$ error ", "page_idx": 18}, {"type": "equation", "text": "$$\ne_{F}^{\\mathrm{test}}=\\frac{\\left(\\sum_{i=1}^{m_{\\mathrm{test}}}\\|F(X_{i})-\\widehat{F}(X_{i})\\|_{\\widetilde{\\mathcal{V}}}^{2}w_{i}\\right)^{1/2}}{\\left(\\sum_{i=1}^{m_{\\mathrm{test}}}\\|F(X_{i})\\|_{\\widetilde{\\mathcal{V}}}^{2}w_{i}\\right)^{1/2}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We use a high order isotropic Clenshaw Curtis sparse grid quadrature rule to evaluate $e_{F}^{\\mathrm{test}}$ , as described in [2]. This method shows superior convergence over Monte Carlo integration to evaluate the global Bochner error. The sparse grid rule gives $m_{\\mathrm{test}}$ points at a level $\\ell$ for $d$ dimensions. We rely on the TASMANIAN sparse grid toolkit [88\u201390] for the generation of the isotropic rule to study the generalization performance of the DNN. ", "page_idx": 18}, {"type": "text", "text": "(vii) Visualization. The graphs in Figs. 1-3 show the geometric mean (the main curve) and plus/minus one (geometric) standard deviation (the shaded region). We use the geometric mean because our errors are plotted in logarithmic scale on the $y$ -axis. See [3, Sec. A.1] for further discussion about this choice. ", "page_idx": 18}, {"type": "text", "text": "B Description of the parametric PDEs used in the numerical experiments and their discretization ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we provide full details of the parametric PDEs considered in our numerical experiments.   \nWe also describe their variational formulations and numerical solution using FEM. ", "page_idx": 18}, {"type": "text", "text": "B.1 Parametric coefficients ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We consider two parametric coefficients of the form (A.1) in our numerical experiments. Our first coefficient is ", "page_idx": 18}, {"type": "equation", "text": "$$\na_{1}(\\pmb{z},\\pmb{x})=2.62+\\sum_{j=1}^{\\infty}x_{j}\\frac{\\sin(\\pi z_{1}j)}{j^{3/2}},\\quad\\forall\\pmb{z}\\in\\Omega,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $x_{j}\\,\\in\\,[-1,1],\\,\\forall j$ . Our second example involves a log-transformed coefficient, which is a rescaling of an example from [76] of a diffusion coefficient with one-dimensional (layered) spatial dependence given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{2}(z,x)=\\exp\\left(1+x_{1}\\left(\\frac{\\sqrt{\\pi}\\beta}{2}\\right)^{1/2}+\\displaystyle\\sum_{j=2}^{\\infty}\\zeta_{j}\\vartheta_{j}(z)x_{j}\\right),\\quad\\forall z\\in\\Omega,}\\\\ &{\\qquad\\zeta_{j}:=(\\sqrt{\\pi}\\beta)^{1/2}\\exp\\left(\\frac{-\\left(\\lfloor j/2\\rfloor\\pi\\beta\\right)^{2}}{8}\\right),}\\\\ &{\\qquad\\vartheta_{j}(z):=\\left\\{\\begin{array}{l l}{\\sin\\left(\\lfloor j/2\\rfloor\\pi z_{1}/\\beta_{p}\\right)}&{\\mathrm{~if~}j\\mathrm{~is~even}}\\\\ {\\cos\\left(\\lfloor j/2\\rfloor\\pi z_{1}/\\beta_{p}\\right)}&{\\mathrm{~if~}j\\mathrm{~is~odd}}\\end{array},\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $x_{j}\\in[-1,1],\\forall j$ . Here we let $\\beta_{c}=1/8$ , and $\\beta_{p}=\\operatorname*{max}\\{1,2\\beta_{c}\\},\\beta=\\beta_{c}/\\beta_{p}.$ ", "page_idx": 18}, {"type": "text", "text": "In both cases we consider truncation of the expansion after $d$ terms, giving the map $a_{j,d}:[-1,1]^{d}\\rightarrow$ $\\mathcal{X}$ , with $j\\,=\\,1$ corresponding to (B.1) and $j\\,=\\,2$ corresponding to (B.2). Our input samples are then $\\{X_{i}=a_{j,d}(\\pmb{x}_{i})\\}_{i=1}^{m}\\subset\\mathcal{X}$ with $\\pmb{x}_{i}\\in[-1,1]^{d}$ drawn identically and independently from $\\varrho_{d}$ and $j\\in\\{1,2\\}$ . Note for the Boussinesq problem we also consider an additional parametric dependence in the tensor $\\mathbb{K}$ describing the thermal conductivity of the fluid. See $\\S B.5$ and (B.17). ", "page_idx": 18}, {"type": "text", "text": "B.2 Relevant spaces ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We require several function space definitions for the development of the mixed variation formulations of the Poisson, Navier-Stokes-Brinkman and Boussinesq PDEs. Let $\\Omega\\subset\\mathbb{R}^{n}$ , $n\\in\\{2,3\\}$ , be the physical domain of a PDE. We write $\\mathrm{L}^{p}(\\Omega)$ , $1\\leq p\\leq\\infty$ , for the $L^{p}$ -space of scalar-valued functions with respect to the Lebesgue measure (to avoid confusion with the Bochner space $L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y}))$ . We denote the standard Sobolev spaces as $\\mathrm{W}^{s,p}(\\Omega)$ for $s~\\in~\\mathbb{R}$ and $p\\,>\\,1$ , and write $\\mathrm{H}^{k}(\\Omega)$ when $p=2$ and $s=k$ . Additionally, we consider the space of traces of functions in $\\mathrm{H}^{1}(\\Omega)$ , denoted by $\\mathrm{H}^{1/2}(\\partial\\Omega)$ , and its dual, $\\mathrm{H}^{-1/2}(\\partial\\Omega)$ (see, e.g., [11, Sec. 1.2] for further details). We also define the following closed subspace of $\\mathrm{H}^{1}(\\Omega)$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{H}_{0}^{1}(\\Omega):=\\overline{{C_{0}^{\\infty}(\\Omega)}}^{\\lVert\\cdot\\rVert_{1,\\Omega}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here $\\overline{{C_{0}^{\\infty}(\\Omega)}}^{\\lVert\\cdot\\rVert_{1,\\Omega}}$ denotes the closure of $C_{0}^{\\infty}(\\Omega)$ (i.e., the space of $C^{\\infty}(\\Omega)$ functions with compact support) with respect to the norm $\\|\\cdot\\|_{1,\\Omega}$ , which, for any $\\bar{v}\\bar{\\in}\\,\\mathrm{H}^{1}(\\Omega)$ , is given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|v\\|_{1,\\Omega}:=\\left\\{|v|_{1,\\Omega}^{2}+\\|v\\|_{\\mathrm{L}^{2}(\\Omega)}^{2}\\right\\}^{1/2},\\quad\\mathrm{where~}|v|_{1,\\Omega}:=\\|\\nabla v\\|_{\\mathrm{L}^{2}(\\Omega)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For scalar functions $u$ and vector fields $\\pmb{v}$ , we use $\\nabla u$ and $\\mathrm{div}(\\pmb{v})$ to denote their gradient and divergence, respectively. For tensor fields $\\pmb{\\sigma}$ and $\\tau$ , represented by $(\\sigma_{i,j})_{i,j=1}^{n}$ and $(\\tau_{i,j})_{i,j=1}^{n}$ , respectively, we define $\\scriptstyle\\mathbf{div}({\\boldsymbol\\sigma})$ as the divergence operator div acting along the rows of $\\pmb{\\sigma}$ , and we define the trace and the tensor inner-product as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname{tr}(\\pmb{\\sigma})=\\sum_{i=1}^{n}\\sigma_{i,i},{\\mathrm{~and~}}\\pmb{\\tau}:\\pmb{\\sigma}=\\sum_{i,j=1}^{n}\\tau_{i,j}\\sigma_{i,j},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "respectively. Furthermore, we introduce the notation $\\mathbf{L}^{p}(\\Omega)$ and $\\mathbb{L}^{p}(\\Omega)$ to represent the vectorial and tensorial counterparts of $\\mathrm{L}^{p}(\\Omega)$ , respectively, and $\\mathbf{H}^{1}(\\Omega)$ and $\\mathbb{H}^{1}(\\Omega)$ for the vectorial and tensorial counterparts of $\\bar{\\mathrm{H}}^{1}(\\Omega)$ , respectively. Keeping this in mind, we introduce the Banach spaces ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{H}(\\mathrm{div}_{q};\\Omega):=\\Big\\{\\pmb{v}\\in\\mathbf{L}^{2}(\\Omega):\\mathrm{div}(\\pmb{v})\\in\\mathrm{L}^{q}(\\Omega)\\Big\\},}\\\\ &{\\mathbb{H}(\\mathbf{div}_{q};\\Omega):=\\Big\\{\\pmb{\\tau}\\in\\mathbb{L}^{2}(\\Omega):\\mathbf{div}(\\pmb{\\tau})\\in\\mathbf{L}^{q}(\\Omega)\\Big\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with norms ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|v\\|_{\\mathbf{H}(\\mathrm{div}_{q};\\Omega)}\\,:=\\,\\|v\\|_{\\mathbf{L}^{2}(\\Omega)}\\,+\\,\\|\\mathrm{div}(v)\\|_{\\mathbf{L}^{q}(\\Omega)},}\\\\ &{\\|\\tau\\|_{\\mathbb{H}(\\mathbf{div}_{q};\\Omega)}\\,:=\\,\\|\\tau\\|_{\\mathbb{L}^{2}(\\Omega)}\\,+\\,\\|\\mathbf{div}(\\tau)\\|_{\\mathbf{L}^{q}(\\Omega)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The cases of $q=4/3$ and $q=2$ appear in the mixed variational formulations of the considered PDEs, and for the latter we simply write $\\mathbf{H}(\\mathrm{div};\\Omega)$ . ", "page_idx": 19}, {"type": "text", "text": "Often, under certain conditions, such as incompressibility conditions [33, eq.(2.4)], it is convenient to define variants of these spaces. For example, we define ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{L}_{\\mathrm{tr}}^{2}(\\Omega)\\,:=\\,\\Big\\{\\pmb{\\tau}\\in\\mathbb{L}^{2}(\\Omega):\\mathrm{tr}(\\pmb{\\tau})\\,=\\,0\\Big\\}\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which represents the space of integrable functions with zero trace over $\\Omega$ . Furthermore, given the decomposition (see, e.g., [32]) ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{H}(\\mathbf{div}_{4/3};\\Omega)\\,=\\,\\mathbb{H}_{0}(\\mathbf{div}_{4/3};\\Omega)\\,\\oplus\\,\\mathbb{R}\\,\\mathbb{I}\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "we may also consider ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{H}_{0}(\\mathbf{div}_{4/3};\\Omega)\\;:=\\;\\Bigl\\{\\pmb{\\tau}\\in\\mathbb{H}(\\mathbf{div}_{4/3};\\Omega):\\int_{\\Omega}\\mathrm{tr}(\\pmb{\\tau})=0\\Bigr\\}\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "as the space of elements in $\\mathbb{H}(\\mathbf{div}_{4/3};\\Omega)$ with zero mean trace. Finally, we define ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{L}_{\\mathrm{skew}}^{2}(\\Omega)=\\{\\pmb{\\eta}\\in\\mathbb{L}^{2}(\\Omega):\\,\\pmb{\\eta}+\\pmb{\\eta}^{t}=0\\},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and the space of $\\mathrm{L^{2}(\\Omega)}$ functions with zero integral over $\\Omega$ as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{L}_{0}^{2}(\\Omega)=\\left\\{\\nu\\in\\mathrm{L}^{2}(\\Omega):\\ \\int_{\\Omega}\\nu=0\\right\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.3 The parametric diffusion equation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We now describe our first example, which is the parametric elliptic diffusion equation. Let $\\Omega\\subset\\mathbb{R}^{2}$ be a bounded Lipschitz domain, $\\partial\\Omega$ be the boundary of $\\Omega$ , $f\\in\\mathrm{L}^{2}(\\Omega)$ and $g\\in\\mathrm{H}^{1/2}(\\partial\\Omega)$ . Given $\\pmb{x}\\in[-1,1]^{d}$ , we consider the PDE ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{-\\mathrm{div}(a(\\mathbf{x})\\nabla u(\\mathbf{x}))=f,}&{~\\mathrm{in}\\;\\Omega}\\\\ {u(\\mathbf{x})=g,}&{~\\mathrm{on}\\;\\partial\\Omega.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Here $a(\\pmb{x})=a(\\cdot,\\pmb{x})\\in\\mathrm{L}^{\\infty}(\\Omega)=:\\mathcal{X}$ is the parametric diffusion coefficient. The terms $f$ and $g$ are not parametric. ", "page_idx": 20}, {"type": "text", "text": "B.3.1 Mixed variational formulation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Our first step in precisely defining the problem is to identify sufficient conditions for the solution map $\\pmb{x}\\mapsto\\boldsymbol{u}(\\pmb{x})$ to be well-defined. To do this, we diverge from the standard variational formulation involving the space $\\mathcal{V}=\\mathrm{H}_{0}^{1}(\\Omega)$ (see Remark B.3) and instead consider a mixed variational formulation of (B.9). Using a mixed formulation to study the solution of PDEs offers several benefits over the standard variational formulation. One key advantage is that it allows us to introduce additional variables that can be of physical interest. Additionally, mixed formulations can naturally accommodate different types of boundary conditions and introduce Dirichlet boundary conditions directly into the formulation rather than imposing them on the search space. For further details on mixed formulations, we refer to [32] and references within. ", "page_idx": 20}, {"type": "text", "text": "Assume that there exists $r,M>0$ such that, for all $\\pmb{x}\\in[-1,1]^{d}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{0<r\\leq\\mathrm{essinf}_{z\\in\\Omega}a(z,\\mathbf{x})=:a_{\\operatorname*{min}}(x)\\mathrm{~and~}a_{\\operatorname*{max}}(x):=\\mathrm{esssup}_{z\\in\\Omega}a(z,\\mathbf{x})\\leq M.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then the problem can be recast as a first-order system: given $\\pmb{x}\\in\\ [-1,1]^{d}$ , find $({\\pmb\\sigma},u)({\\pmb x})~\\in$ $\\mathbf{H}(\\mathrm{div};\\Omega)\\^{\\bullet}\\times\\mathrm{L}^{2}(\\Omega)$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{d_{a(\\boldsymbol{x})}(\\sigma(\\boldsymbol{x}),\\tau)+b(\\tau,u(\\boldsymbol{x}))=G(\\tau),\\quad\\forall\\tau\\in\\mathbf{H}(\\mathrm{div};\\Omega),}\\\\ &{}&{b(\\sigma,v)=F(v),\\quad\\forall v\\in\\mathrm{L}^{2}(\\Omega).\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Here $d$ and $b$ are the bilinear forms defined by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{d_{a({\\pmb x})}}({\\pmb\\sigma},{\\pmb\\tau})=\\int_{\\Omega}\\frac{{\\pmb\\sigma}\\cdot{\\pmb\\tau}}{a({\\pmb x})},\\quad\\forall({\\pmb\\tau},{\\pmb\\sigma})\\in{\\bf H}(\\mathrm{div};\\Omega)\\times{\\bf H}(\\mathrm{div};\\Omega),}\\\\ {\\displaystyle b({\\pmb\\tau},{\\pmb v})=\\int_{\\Omega}\\mathrm{div}({\\pmb\\tau})v,\\quad\\forall({\\pmb\\tau},{\\pmb v})\\in{\\bf H}(\\mathrm{div};\\Omega)\\times{\\bf L}^{2}(\\Omega)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and the functionals $G\\in(\\mathbf{H}(\\mathrm{div};\\Omega))^{\\prime}$ and $J\\in(\\mathrm{L}^{2}(\\Omega))^{\\prime}$ are defined by ", "page_idx": 20}, {"type": "equation", "text": "$$\nJ(v)=-\\int_{\\Omega}f v,\\quad\\forall v\\in\\mathrm{L}^{2}(\\Omega)\\mathrm{~and~}G(\\tau)=\\langle\\gamma(\\tau)\\cdot n,g\\rangle_{1/2,\\partial\\Omega},\\quad\\forall\\tau\\in\\mathbf{H}(\\mathrm{div};\\Omega).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the experiments in this work, we consider $\\Omega=(0,1)^{2}$ and $f=10$ . For the boundary condition, we consider a constant value $u(z,x)=0.5$ on the bottom of the boundary $(0,1)\\times\\{0\\}$ , and zero boundary conditions on the remainder of the boundary. ", "page_idx": 20}, {"type": "text", "text": "B.3.2 Holomorphy assumption ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Consider the affine parametrization (B.1). Setting $M=2.7$ and observing that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\sum_{j\\in\\mathbb{N}}x_{j}{\\frac{\\sin(\\pi z_{1}j)}{j^{3/5}}}\\right|\\leq\\sum_{j\\in\\mathbb{N}}{\\frac{1}{j^{3/5}}}\\approx2.61238=2.62-r,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for some $\\textit{r}<\\ 0.00762$ , we deduce that (B.10) holds, which makes the mixed variational formulation (B.11) well defined, i.e., for each $\\textbf{\\em x}\\in~[-1,1]^{\\infty}$ , there exists a unique solution $({\\pmb\\sigma},u)({\\pmb x})~\\in~\\mathbf{H}(\\mathrm{div};\\Omega)\\,\\times\\,\\mathrm{L}^{2}(\\Omega)$ . Moreover, one can show that the parametric solution map $\\pmb{x}\\mapsto(\\sigma,u)(\\pmb{x})$ is $(b,\\varepsilon)$ -holomorphic for $0\\,<\\,\\varepsilon\\,<\\,0.00762$ and where $\\pmb{b}\\,=\\,(b_{i})_{i=1}^{\\infty}$ is given by $b_{j}=\\|\\sin(\\pi j\\cdot)/j^{3/2}\\|_{\\mathrm{L}^{\\infty}(\\Omega)}=j^{-3/2}$ . See [74, Prop. A.3.2]. ", "page_idx": 20}, {"type": "image", "img_path": "vBlzen37i0/tmp/646f74990a277086a4356d9e3d2af05c5f57e1f985a8bcaca44004c3b2f5937d.jpg", "img_caption": ["Figure 4: The domain $\\Omega$ and FE mesh for the parametric diffusion equation. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "vBlzen37i0/tmp/b69534461cfe18d442e1729dff440b260e98e3676c2e7bd8dd8d545a45e6200b.jpg", "img_caption": ["Figure 5: The solution ${\\pmb u}({\\pmb x})$ of the parametric Poisson problem in (B.9) for a given parameter $\\pmb{x}=(1,0,0,0)^{\\top}$ with affine coefficient $^{a_{1,d}}$ and $d=4$ , using a total of $\\bar{K}=2622$ DoF. The left plot shows the solution given by the FEM solver. The right plot show the ELU $4\\times40$ DNN approximation after 60, 000 epochs of training with $m=500$ sample points for training. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "In view of this property, this example falls within our theory. Note that $b\\in\\ell_{\\mathsf{M}}^{p}(\\mathbb{N})$ for every $p<2/3$ . Thus, we expect a theoretical rate of convergence with respect to the amount of training data that is arbitrarily close to $m^{1/2-3/2}=m^{-1}$ . This holomorphy result applies to the affine diffusion (B.1), not the log-transformed diffusion (B.2). However, we expect that it is possible to extend [74, Prop. A.3.2] to the latter case. ", "page_idx": 21}, {"type": "text", "text": "B.3.3 Finite element discretization ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We use so-called conforming Finite Element (FE) discretizations [18, Chp. 3]. Given a number of Degrees of Freedom (DoF) $K$ , this results in finite-dimensional spaces $H_{K}\\subseteq\\mathbf{H}(\\mathrm{div};\\Omega)$ and $Q_{K}\\subseteq\\mathrm{L}^{2}(\\Omega)$ . Specifically, we consider a regular triangulation $\\tau_{\\kappa}$ of $\\overline{\\Omega}$ made up of triangles of minimum diameter $h_{\\operatorname*{min}}=0.0844$ and maximum diameter $h_{\\mathrm{max}}=0.1146$ . This corresponds to a total number of DoF $K=2622$ . See Fig. 4 for an illustration of the FE mesh. ", "page_idx": 21}, {"type": "text", "text": "Fig. 5 shows a comparison between a reference solution computed by the FEniCS FEM solver and the approximation obtained by an ELU $4\\times40$ DNN. ", "page_idx": 21}, {"type": "text", "text": "B.4 Parametric Navier-Stokes-Brinkman equations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We next consider a parametric model describing the dynamics of a viscous fluid through porous media. Consider a bounded and Lipschitz physical domain $\\Omega\\,\\subseteq\\,\\mathbb{R}^{2}$ . Given $\\pmb{x}\\in[-\\Bar{1},\\Bar{1}]^{d}$ , we consider the incompressible nonlinear stationary Navier-Stokes-Brinkman (NSB) equations: find ", "page_idx": 21}, {"type": "text", "text": "$\\pmb{u}:[-1,1]^{d}\\times\\Omega\\rightarrow\\mathbb{R}^{2}$ and $p:[-1,1]^{d}\\times\\Omega\\rightarrow\\mathbb{R}$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\eta\\boldsymbol{u}-\\lambda\\mathbf{div}(a(\\boldsymbol{x})e(\\boldsymbol{u}(\\boldsymbol{x})))+(\\boldsymbol{u}(\\boldsymbol{x})\\cdot\\nabla)\\boldsymbol{u}(\\boldsymbol{x})+\\nabla p(\\boldsymbol{x})=\\boldsymbol{f}}&{\\mathrm{~in~}\\Omega}\\\\ {\\operatorname{div}(\\boldsymbol{u}(\\boldsymbol{x}))=0}&{\\mathrm{~in~}\\Omega}\\\\ {\\boldsymbol{u}=\\left\\{\\boldsymbol{u}_{D}\\quad\\mathrm{on~}\\partial\\Omega_{\\mathrm{in}}\\right.}\\\\ {0}&{\\mathrm{~on~}\\partial\\Omega_{\\mathrm{wall}}}\\\\ {\\left.(a(\\boldsymbol{x})\\nabla e(\\boldsymbol{u})-p\\mathbb{I})\\nu=0\\quad\\mathrm{on~}\\partial\\Omega_{\\mathrm{out}}}\\\\ {\\int_{\\Omega}p=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\lambda\\:=\\:\\mathrm{Re}^{-1}$ and $\\mathrm{Re}$ is the Reynolds number, $a(\\pmb{x})\\ =\\ a(\\cdot;\\pmb{x})\\ \\in\\ \\mathcal{X}\\ :=\\ \\mathrm{L}^{\\infty}(\\Omega)$ and $a:$ $[-1,1]^{d}\\times\\Omega\\rightarrow\\mathbb{R}^{+}$ is the random viscosity of the fluid, $\\eta\\in\\mathbb{R}^{+}$ is the scaled inverse permeability of the porous media, $\\textbf{\\em u}$ is the velocity of the fluid, $\\begin{array}{r}{{e}(\\pmb{u})\\stackrel{!}{=}\\frac{1}{2}(\\nabla\\pmb{u}+(\\nabla\\pmb{u})_{\\tiny-}^{t})}\\end{array}$ is the symmetric part of the gradient, $p$ is the pressure of the fluid and $f:\\Omega\\to\\mathbb{R}$ is an external force independent of the parameters. Here, the fourth condition imposes a zero normal Cauchy stress ", "page_idx": 22}, {"type": "equation", "text": "$$\n(a(\\mathbf{\\boldsymbol{x}})\\nabla e(\\boldsymbol{u})-p\\mathbb{I})\\nu=0\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for the output boundary $\\partial\\Omega_{\\mathrm{out}}$ .In addition, the incompressibility of the fluid imposes the following compatibility condition on uD: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\int_{\\Gamma}\\pmb{u}_{D}\\cdot\\pmb{n}=0\\quad\\mathrm{~on~}\\partial\\Omega_{\\mathrm{in}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The third condition also imposes a no-slip condition on the walls $\\Omega_{\\mathrm{wall}}$ [34, eq.(2.3)]. ", "page_idx": 22}, {"type": "text", "text": "B.4.1 Mixed variational formulation ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The analysis of the detailed mixed formulation used for this problem in the nonparametric case can be found in [34]. Over the last decade, many works have used a mixed formulation employing a Banach space framework, allowing one to solve different PDEs in continuum mechanics in suitable Banach spaces. The advantage of this formulation is that no augmentation is required, the spaces are simpler and closer to the original model, and it allows one to obtain more direct approximations of the variables of physical interest [34, Sec. 1]. ", "page_idx": 22}, {"type": "text", "text": "Based on the analysis in [34], the mixed variational formulation of the parametric NSB equations in (B.14) becomes: given $\\bar{\\mathbf{\\boldsymbol{x}}}\\in[-1,1]^{d}$ , find $(u,t,\\sigma,\\gamma)(x)\\in\\mathbf{L}^{4}(\\Omega)\\times\\mathring\\mathbb{L}_{\\mathrm{tr}}^{2}(\\Omega)\\times\\mathbb{H}_{0}(\\mathbf{div}_{4/3};\\Omega)\\times$ $\\mathbb{L}_{\\mathrm{skew}}^{2}(\\Omega)$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{\\lambda\\displaystyle\\int_{\\Omega}a_{i}({\\pmb x})t({\\pmb x}):s\\,-\\,\\int_{\\Omega}s:\\sigma({\\pmb x})\\,-\\,\\int_{\\Omega}({\\pmb u}\\otimes{\\pmb u})({\\pmb x}):s\\,-}}&{{0,}}\\\\ {{\\displaystyle\\int_{\\Omega}t({\\pmb x}):\\pmb\\tau+\\,\\int_{\\Omega}\\gamma({\\pmb x}):\\pmb\\tau+\\,\\int_{\\Omega}u({\\pmb x})\\cdot\\mathbf{div}(\\pmb\\tau)}}&{{=}}&{{\\langle\\pmb\\tau n,u_{D}\\rangle_{\\partial\\Omega_{i n}},}}\\\\ {{\\displaystyle\\int_{\\Omega}\\delta:\\sigma({\\pmb x})\\,+\\,\\int_{\\Omega}v\\cdot\\mathbf{div}(\\sigma({\\pmb x}))\\,-\\,\\int_{\\Omega}\\eta u({\\pmb x})\\cdot\\pmb v}}&{{=}}&{{\\displaystyle\\int_{\\Omega}f\\cdot\\pmb v,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for all $(v,s,\\tau,\\delta)\\in\\mathbf{L}^{4}(\\Omega)\\times\\mathbb{L}_{\\mathrm{tr}}^{2}(\\Omega)\\times\\mathbb{H}_{0}(\\mathbf{div}_{4/3};\\Omega)\\times\\mathbb{L}_{\\mathrm{skew}}^{2}(\\Omega)$ . Numerically, the skew-symmetry of $\\gamma$ is imposed by searching for $\\gamma\\in\\mathrm{L}^{2}(\\Omega)$ and setting ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\gamma=\\left[\\!\\!\\begin{array}{l l}{0}&{\\gamma}\\\\ {-\\gamma}&{0}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Moreover, we impose the Neumann boundary condition via a Nietsche method as in [34, Sec. 5.2]. Specifically, we add ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\kappa\\langle(\\pmb{\\sigma}+\\pmb{u}\\otimes\\pmb{u})\\pmb{n}),\\pmb{\\tau}\\pmb{n}\\rangle_{\\partial\\Omega_{\\mathrm{out}}}=0\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "to the second equation where $\\kappa\\gg1$ is a large constant (e.g., $\\kappa=10^{4}$ ). As usual in this formulation, the pressure $p\\in\\mathrm{L}^{2}(\\Omega)$ can be computed according to the post-processing formula ", "page_idx": 22}, {"type": "equation", "text": "$$\np=-\\frac{1}{2}\\mathrm{tr}(\\pmb{\\sigma}+(\\pmb{u}\\otimes\\pmb{u})).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "image", "img_path": "vBlzen37i0/tmp/17d8714afa0a7bdd4b73edb17675436a4f15c95cd663de50d04309c9556587ea.jpg", "img_caption": ["Figure 6: The solution $({\\pmb u},p)({\\pmb x})$ of the parametric NSB problem (B.14) for a given parameter $\\pmb{x}=(1,0,0,0)^{\\top}$ with affine coefficient $^{a_{1,d}}$ and $d=4$ , using a total of 1464 DoF for $\\pmb{u}$ and $244\\,\\mathrm{DoF}$ for $p$ . The top row shows the solution given by the FEM solver, and the bottom row shows the ELU $4\\times40$ DNN approximation after 60, 000 epochs of training with $m=500$ sample points. The left plots show the vector field $\\mathbf{\\delta}_{\\mathbf{\\lambda}}$ . The right plots show the points of highest pressure $p$ . "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Note that above we omitted the term $\\textbf{\\em x}$ for simplicity. ", "page_idx": 23}, {"type": "text", "text": "In our experiments, we consider approximating solutions to the parametric NSB problem with $\\lambda=0.1$ , a scaled inverse permeability of $\\eta=\\bar{10}+z_{1}^{2}+z_{2}^{2}$ , an external force $\\pmb{f}=\\dot{(0,-1)}^{\\top}$ and random viscosity $a_{j,d}$ as in (B.1)\u2013(B.2) with $j\\in\\{1,2\\}$ . ", "page_idx": 23}, {"type": "text", "text": "We consider the unit square $\\Omega=(0,1)^{2}$ as the domain, an inlet boundary $\\partial\\Omega_{\\mathrm{in}}=(0,1)\\times\\{1\\}$ , an outlet boundary $\\partial\\Omega_{\\mathrm{out}}=\\{1\\}\\times(0,1)$ and walls $\\partial\\Omega_{\\mathrm{wall}}=\\{0\\}\\times(0,1)\\cup(0,1)\\times\\{0\\}$ . For simplicity, we use the same mesh as that of the previous example. See Fig. 4. On the Neumann boundary $\\partial\\Omega_{\\mathrm{out}}$ we consider a zero normal Cauchy stress, a Dirichlet condition $u_{D}=(0.0625)^{-1}((z_{2}-\\dot{0.5})(1-$ $z_{2}),0)$ on $\\partial\\Omega_{\\mathrm{in}}$ and a no-slip velocity on $\\partial\\Omega_{\\mathrm{wall}}$ . ", "page_idx": 23}, {"type": "text", "text": "Fig. 6 provides a comparison between a reference solution of the vector field $\\textbf{\\em u}$ and pressure $p$ computed by the FEniCS FEM solver and the approximation generated by a ELU $4\\times40$ DNN. ", "page_idx": 23}, {"type": "text", "text": "Remark B.1 (Other auxiliary variables) We report the performance of the DNNs approximating $({\\pmb u},p)({\\pmb x})\\in{\\bf L}^{4}(\\Omega)\\times\\mathrm{L}^{2}(\\Omega)$ . Note that any solver based on the above formulation outputs several other variables, e.g., $(t,\\sigma,\\gamma)(\\pmb{x})\\in\\mathbb{L}_{\\mathrm{tr}}^{2}(\\Omega)\\stackrel{cdot}{\\times}\\mathbb{H}_{0}(\\mathbf{div}_{4/3};\\Omega)\\times\\mathbb{L}_{\\mathrm{skew}}^{2}(\\Omega)$ . One could also approximate these auxiliary variables using DNNs. However, we restrict our experiments to $\\left(\\pmb{u},p\\right)$ as these are the primary variables of interest in the problem. ", "page_idx": 23}, {"type": "text", "text": "To conclude this discussion, in Fig. 7 we plot the numerical results for the approximation of the pressure $p$ in the above problem. This complements Fig. 2, which showed results for the velocity field $\\textbf{\\em u}$ . We once more observe similar results: ELU and tanh DNNs outperform ReLU DNNs, the rate of convergence appears to be close to $O(m^{-1})$ and there is no degradation with increasing parametric dimension $d$ . ", "page_idx": 23}, {"type": "text", "text": "B.5 Parametric stationary Boussinesq equation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To recap, in our first example, we considered a mixed formulation of a parametric diffusion equation that provably satisfies the $(b,\\varepsilon)$ -holomorphy assumption. Using this formulation, we considered problems with nonzero Dirichlet boundary conditions, whereas previous works [2, 19, 27] study the more restrictive case of homogeneous Dirichlet boundary conditions, where $u\\,\\in\\mathrm{H}_{0}^{1}(\\Omega)$ . In our next example, we studied a more complicated parametric PDE, namely, the parametric NSB equations. While this example currently lacks a holomorphy guarantee, we observe a convergence rate that aligns with what we expect. We conjecture that the $\\bar{m}^{-1}$ rate holds both for this and for even more complicated problems. To illustrate this claim with an example, we now consider a parametric coupled partial differential equation in three dimensions $\\Omega\\subset\\Bar{\\mathbb{R}}^{3}$ ) with two random coefficients affecting different parts of the coupled problem. The nonparametric version of this problem is based on [20]. ", "page_idx": 23}, {"type": "image", "img_path": "vBlzen37i0/tmp/b55565317f83efc5d859076741b210dc1ecab5cd019a4b34587ba01b40a32e59.jpg", "img_caption": ["Figure 7: The same as Fig. 2, except showing results for the pressure $p$ . "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "Specifically, we consider the Boussinesq formulation in [20] that combines a parametric incompressible Navier\u2013Stokes equation with a parametric heat equation. The parametric dependence affects both equations. The Navier\u2013Stokes equation is affected by a parametric variable multiplying the temperature-dependent viscosity, and the equation for heat flow is affected directly by the thermal conductivity of the fluid. To be more precise, given $\\pmb{x}\\in[-1,1]^{d}$ , our goal is to find the velocity $\\pmb{u}:[-1,1]^{d}\\times\\Omega\\rightarrow\\mathbb{R}^{2}$ , pressure $p:[-1,1]^{d}\\times\\Omega\\rightarrow\\mathbb{R}$ and temperature $\\varphi:[-1,1]^{d}\\times\\Omega\\rightarrow\\mathbb{R}$ of a fluid such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{-\\mathbf{div}(2a(\\boldsymbol{x})\\varpi(\\varphi(\\boldsymbol{x}))e(\\boldsymbol{u}(\\boldsymbol{x})))+(\\boldsymbol{u}(\\boldsymbol{x})\\cdot\\nabla)\\boldsymbol{u}(\\boldsymbol{x})+\\nabla p(\\boldsymbol{x})=\\varphi(\\boldsymbol{x})g\\quad\\mathrm{~in~}\\Omega,}&{}\\\\ {\\mathrm{div}(\\boldsymbol{u}(\\boldsymbol{x}))=0\\quad\\mathrm{~in~}\\Omega,}&{}\\\\ {-\\mathrm{div}(\\mathbb{K}(\\boldsymbol{x})\\nabla\\varphi(\\boldsymbol{x}))+\\boldsymbol{u}(\\boldsymbol{x})\\cdot\\nabla\\varphi(\\boldsymbol{x})=0\\quad\\mathrm{~in~}\\Omega,}&{}\\\\ {\\boldsymbol{u}=\\boldsymbol{u}_{D}\\quad\\mathrm{~on~}\\partial\\Omega,}&{}\\\\ {\\varphi=\\varphi_{D}\\quad\\mathrm{~on~}\\partial\\Omega,}&{}\\\\ {\\displaystyle\\int_{\\Omega}p(\\boldsymbol{x})=0.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Here $\\pmb{g}=(0,0,-1)^{\\top}$ is a gravitational force and $\\mathbb{K}(\\pmb{x})=\\mathbb{K}(\\cdot;\\pmb{x})\\in\\mathbb{L}^{\\infty}(\\Omega)$ , where $\\mathbb{K}:[-1,1]^{d}\\times$ $\\Omega\\rightarrow\\mathbf{\\bar{R}}^{3\\times3}$ is a parametric uniformly-positive tensor describing the thermal conductivity of the fluid. It is given explicitly by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{K}(z,\\pmb{x})=\\left(1.89+\\sum_{j\\in\\mathbb{N}}x_{j}\\frac{\\sin(\\pi z_{3}j)}{j^{9/5}}\\right)\\left[\\!\\!\\begin{array}{c c c}{\\exp(-z_{1})}&{0}&{0}\\\\ {0}&{\\exp(-z_{2})}&{0}\\\\ {0}&{0}&{\\exp(-z_{3})}\\end{array}\\!\\!\\right],\\;\\forall z\\in\\Omega,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for $\\pmb{x}\\in[-1,1]^{d}$ . The term $\\varpi:\\mathbb{R}\\to\\mathbb{R}^{+}$ is a temperature-dependent viscosity given by $\\varpi(\\varphi)=$ $0.1+\\exp(-\\varphi)$ and the term $a(\\pmb{x})=a(\\cdot;\\pmb{x})\\in\\mathrm{L}^{\\infty}(\\Omega)$ , where $a:[-1,1]^{d}\\times\\Omega\\rightarrow\\mathbb{R}$ is a parametric variable affecting the viscosity of the fluid. As in the previous example $e(u)$ is the symmetric part of $\\nabla\\pmb{u}$ . Note that in this case, we have $(a(\\pmb{x}),\\mathbb{K}(\\pmb{x}))\\in\\mathcal{X}:=\\mathrm{L}^{\\infty}(\\Omega)\\times\\mathbb{L}^{\\infty}(\\Omega)$ . ", "page_idx": 24}, {"type": "text", "text": "B.5.1 Fully mixed variational formulation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The complete derivation of a fully-mixed variational formulation for the non-parametric Boussinesq equation in Banach spaces can be found in [20, Sec. 3.1]. To make the presentation simpler we rewrite it for the parametric case. Given $\\pmb{x}\\in[-1,1]^{d}$ , find $(\\mathbf{\\dot{u}},t,\\pmb{\\sigma},\\varphi,\\tilde{t},\\tilde{\\pmb{\\sigma}})(\\mathbf{\\dot{x}})\\in\\mathbf{L}^{4}(\\Omega)\\times\\mathbb{L}_{\\mathrm{tr}}^{\\dot{2}}(\\Omega)\\times$ ", "page_idx": 24}, {"type": "text", "text": "$\\mathbb{H}_{0}(\\mathbf{div}_{4/3};\\Omega)\\times\\mathrm{L}^{4}(\\Omega)\\times\\mathrm{L}^{2}(\\Omega)\\times\\mathbf{H}(\\mathrm{div}_{4/3};\\Omega)$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{-\\displaystyle\\int_{\\Omega}\\boldsymbol{v}\\cdot\\mathrm{d}\\mathbf{v}(\\boldsymbol{\\sigma}(\\boldsymbol{x}))+\\frac{1}{2}\\int_{\\Omega}\\boldsymbol{t}(\\alpha)\\boldsymbol{u}(\\boldsymbol{x})\\cdot\\boldsymbol{v}-\\displaystyle\\int_{\\Omega}\\boldsymbol{\\varphi}(\\boldsymbol{x})\\boldsymbol{g}\\cdot\\boldsymbol{v}}&{=}\\\\ {\\displaystyle\\int_{\\Omega}2\\alpha(\\boldsymbol{x})\\boldsymbol{\\varpi}(\\boldsymbol{\\varphi}(\\boldsymbol{x}))\\boldsymbol{t}_{\\mathbf{s}\\mathbf{v}}(\\boldsymbol{x}):s-\\frac{1}{2}\\int_{\\Omega}(u\\otimes\\boldsymbol{u})(\\boldsymbol{x}):s}&{=}&{\\displaystyle\\int_{\\Omega}\\boldsymbol{\\sigma}(\\boldsymbol{x})\\cdot\\boldsymbol{s}}\\\\ {\\displaystyle\\int_{\\Omega}\\boldsymbol{\\tau}:\\boldsymbol{t}(\\boldsymbol{x})+\\int_{\\Omega}u\\boldsymbol{u}(\\boldsymbol{x})\\cdot\\mathrm{d}\\mathbf{v}(\\boldsymbol{\\tau})}&{=}&{\\displaystyle\\langle\\boldsymbol{r}\\,\\boldsymbol{w},\\boldsymbol{u}\\rangle_{\\Gamma}}\\\\ {\\displaystyle-\\int_{\\Omega}\\boldsymbol{\\it{t}}\\,\\mathrm{d}\\mathbf{v}(\\boldsymbol{\\vartheta}(\\boldsymbol{x}))+\\frac{1}{2}\\int_{\\Omega}\\boldsymbol{\\ell}(\\boldsymbol{x})\\boldsymbol{u}(\\boldsymbol{x})\\cdot\\boldsymbol{\\dot{t}}}&{=}&{0}\\\\ {\\displaystyle\\int_{\\Omega}\\mathbb{E}(\\boldsymbol{x})\\boldsymbol{\\dot{u}}(\\boldsymbol{x})\\cdot\\boldsymbol{\\hat{s}}-\\frac{1}{2}\\int_{\\Omega}\\boldsymbol{\\varphi}(\\boldsymbol{x})\\boldsymbol{u}(\\boldsymbol{x})\\cdot\\boldsymbol{\\hat{s}}}&{=}&{\\displaystyle\\int_{\\Omega}\\boldsymbol{\\bar{\\sigma}}(\\boldsymbol{x})\\cdot\\boldsymbol{\\hat{s}}}\\\\ {\\displaystyle\\int_{\\Omega}\\boldsymbol{\\tau}\\cdot\\boldsymbol{\\hat{t}}(\\boldsymbol{x})+\\int_{\\Omega}\\boldsymbol{\\varphi}(\\boldsymbol{x})\\,\\mathrm{d}\\mathbf{v}(\\boldsymbol{\\bar{\\tau}})}&{=}&{\\displaystyle\\langle\\boldsymbol{\\bar{\\tau}}\\cdot\\boldsymbol{\\nu},\\boldsymbol{\\varphi}_{D}\\rangle_{\\Gamma}}\\\\ {\\displaystyle\\int_{\\Omega}\\boldsymbol{\\Psi}(2\\boldsymbol{\\sigma}+u\\otimes\\boldsymbol{u})(\\boldsymbol{x})}&{=}&{0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for all $\\begin{array}{r}{(v,s,\\tau,\\psi,\\tilde{\\sigma},\\tilde{\\tau})\\in\\mathbf{L}^{4}(\\Omega)\\times\\mathbb{L}_{\\mathrm{tr}}^{2}(\\Omega)\\times\\mathbb{H}_{0}(\\mathbf{div}_{4/3};\\Omega)\\times\\mathbf{L}^{4}(\\Omega)\\times\\mathbf{L}^{2}(\\Omega)\\times\\mathbf{H}(\\mathrm{div}_{4/3};\\Omega).}\\end{array}$ Here $p\\in\\mathrm{L}_{0}^{2}(\\Omega)$ can be recovered as ", "page_idx": 25}, {"type": "equation", "text": "$$\np=-\\frac{1}{6}\\mathrm{tr}(2\\sigma+2c\\mathbb{I}+{\\pmb u}\\otimes{\\pmb u}),\\quad\\mathrm{where~}c=-\\frac{1}{6|\\Omega|}\\int_{\\Omega}\\mathrm{tr}({\\pmb u}\\otimes{\\pmb u}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "As in the previous example, we omitted the term $\\textbf{\\em x}$ from this equation for simplicity. For further details on this formulation we refer to [20] and references within. ", "page_idx": 25}, {"type": "text", "text": "Given $\\pmb{x}\\in[-1,1]^{d}$ , we approximate the solution $(\\pmb{u},p,\\varphi)(\\pmb{x})\\,\\in\\,(\\mathbf{L}^{4}(\\Omega)\\times\\mathrm{L}_{0}^{2}(\\Omega)\\times\\mathrm{L}^{4}(\\Omega))$ of (B.18) by using DNNs and study the approximation capabilities as we increase the number of training samples $m$ . As in the previous example (see Remark B.1), we do not aim to approximate the other variables $\\begin{array}{r}{(t,\\sigma,\\tilde{t},\\tilde{\\sigma})(\\dot{\\mathbf{x}})\\in\\mathbb{L}_{\\mathrm{tr}}^{2}(\\Omega)\\times\\mathbf{\\dot{\\mathbb{H}}}_{0}(\\mathbf{div}_{4/3};\\Omega)\\times\\mathbf{L}^{2}(\\Omega)\\times\\mathbf{H}(\\mathrm{div}_{4/3};\\Omega).}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "In our experiments, we consider the unit cube $\\Omega=(0,1)^{3}$ as the domain in $\\mathbb{R}^{3}$ . We consider a nonzero boundary condition $u_{D}=(1,1,0)$ on the bottom face of the cube $\\partial\\Omega_{\\mathrm{bottom}}=(0,1)\\times(0,1)\\times\\{0\\}$ , and zero on the other faces. We set $\\varphi_{D}=\\exp(4(-(z_{1}-0.5)^{2}-(z_{2}-0.5)^{2}))$ on $\\partial\\Omega_{\\mathrm{bottom}}$ and zero otherwise. For simplicity, we consider the same parametric coefficients $^{a_{1,d}}$ and $^{a_{2,d}}$ given by (B.1) and (B.2), respectively. See Fig. 8 for an example of the solution $({\\pmb u},p,\\varphi)({\\pmb x})$ for a given $\\pmb{x}\\in[-1,1]^{4}$ . ", "page_idx": 25}, {"type": "text", "text": "To conclude this section, we provide a comparison of the performance of the DNN architectures in approximating the velocity field $\\textbf{\\em u}$ and pressure $p$ for the Boussinesq PDE. This complements Fig. 3, which showed results for the temperature $\\varphi$ . As with $\\varphi$ , the convergence rate for $p$ agrees roughly with the rate $m^{-1}$ , and does not appear to deteriorate with the parametric dimension $d$ . On the other hand, the convergence rate for the velocity field $\\textbf{\\em u}$ is somewhat slower. ", "page_idx": 25}, {"type": "text", "text": "C Overview of the proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we first introduce additional notation that is needed for the proofs of the main results.   \nWe then give a brief overview of the proofs. ", "page_idx": 25}, {"type": "text", "text": "C.1 Additional notation ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "C.1.1 Lipschitz constants ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Let $(\\mathcal{X},\\|\\cdot\\|_{\\mathcal{X}})$ and $(\\mathcal{D},\\|\\cdot\\|_{\\mathcal{D}})$ be Banach spaces, $G:\\mathcal{X}\\rightarrow\\mathcal{Y}$ and $B\\subseteq\\mathcal{X}$ . We define the Lipschitz constant as $L=\\mathrm{Lip}(G;B,\\dot{\\mathcal{y}})$ as the smallest constant $L\\geq0$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|G(X^{\\prime})-G(X)\\|_{\\mathcal{Y}}\\leq L\\|X^{\\prime}-X\\|_{\\mathcal{X}},\\quad\\forall X,X^{\\prime}\\in\\mathcal{B}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "image", "img_path": "vBlzen37i0/tmp/bb730b6f91214d2c5f44e13e5c4b3a3fe4493c247b6c2e93b82c921e13d54099.jpg", "img_caption": ["Figure 8: The solution $({\\pmb u},\\varphi,p)({\\pmb x})$ to the parametric Boussinesq problem in (B.18) for a given parameter $\\pmb{x}=(1,0,0,0)^{\\top}$ with affine coefficient $^{a_{1,d}}$ and $d=4$ , using a total of 18, $480\\;\\mathrm{DoF}$ for $\\pmb{u}$ and $528\\;\\mathrm{DoF}$ for both $\\varphi$ and $p$ . The top row shows the solution given by the FEM solver and the bottom row shows the $4\\times40$ ELU\u2013DNN approximation after 60, 000 epochs of training with $m=500$ sample points. The left plots show streamlines of the vector field $\\mathbf{\\delta}_{\\mathbf{\\lambda}}$ and their directions indicated with coloured arrows. The middle plots visualize the temperature distribution inside the cube using coloured spheres, with the hottest region at the centre of the cube. The right plots illustrate the points of highest pressure $p$ . "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "vBlzen37i0/tmp/ef0c52084d1133d4e01abccf5793063f4fa369c59c7ed72d011a2fb8cf04d24e.jpg", "img_caption": ["Figure 9: The same as Fig. 3, except showing results for the velocity field $\\pmb{u}$ , where $\\mathcal{V}=\\mathrm{L}^{4}(\\Omega)$ , and pressure $p$ , where $\\mathcal{V}=\\mathrm{L}_{0}^{2}(\\Omega)$ . "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "C.1.2 Sequence spaces ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We require some notation for sequences. Let $(\\mathcal{Z},\\|\\cdot\\|_{\\mathcal{Z}})$ be a Banach space, $d=\\mathbb{N}\\cup\\{\\infty\\}$ and write $\\pmb{\\nu}=(\\nu_{k})_{k=1}^{d}$ for an arbitrary multi-index in $\\mathbb{N}_{0}^{d}$ . If $\\Lambda\\subseteq\\mathbb{N}_{0}^{d}$ is a finite or countable set of multi-indices and $0<p\\leq\\infty$ we define the space $\\ell^{p}(\\Lambda;\\mathcal{Z})$ as the set of all $\\mathcal{Z}$ -valued sequences $\\pmb{c}=(c_{\\nu})_{\\nu\\in\\Lambda}$ for which $\\|c\\|_{p;\\mathcal{Z}}<\\infty$ , where ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|c\\right\\|_{p;\\mathcal{Z}}=\\left\\{\\left(\\sum_{\\nu\\in\\Lambda}\\left\\|c_{\\nu}\\right\\|_{\\mathcal{Z}}^{p}\\right)^{1/p}\\begin{array}{l}{0<p<\\infty,}\\\\ {\\operatorname*{sup}_{\\nu\\in\\Lambda}\\left\\|c_{\\nu}\\right\\|_{\\mathcal{Z}}}\\end{array}\\right.}\\\\ {\\left.\\|c\\|_{p;\\mathcal{Z}}\\right.=\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "When $(\\mathcal{Z},\\|\\cdot\\|_{\\mathcal{Z}})=(\\mathbb{R},|\\cdot|)$ , we just write $\\ell^{p}(\\Lambda)$ and ${\\|\\cdot\\|}_{p}$ . We also define the following: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left\\|c\\right\\|_{p;{\\mathcal{Z}}}=\\operatorname*{sup}_{z^{*}\\in B({\\mathcal{Z}}^{*})}\\left\\|z^{*}(c)\\right\\|_{p},\\quad\\forall c\\in{\\ell}^{p}(\\Lambda;{\\mathcal{Z}}),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $z^{*}(\\pmb{c})=(z^{*}(c_{\\pmb{\\nu}}))_{\\pmb{\\nu}\\in\\Lambda}\\in\\mathbb{R}^{|\\Lambda|}$ for $\\pmb{c}=(c_{\\nu})_{\\nu\\in\\Lambda}$ . Notice that $\\|\\pmb{c}\\|_{p;\\mathcal{Z}}\\leq\\|\\pmb{c}\\|_{p;\\mathcal{Z}}$ . ", "page_idx": 27}, {"type": "text", "text": "Given a sequence $\\pmb{c}=(c_{\\nu})_{\\nu\\in\\Lambda}$ , we write ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{supp}(c)=\\{\\pmb{\\nu}\\in\\Lambda:c_{\\pmb{\\nu}}\\neq0\\}\\subseteq\\Lambda.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For $d\\in\\mathbb{N}\\cup\\{\\infty\\}$ , we write $e_{j}$ , $j=1,\\ldots,d$ , for the canonical basis vectors in $\\mathbb{R}^{d}$ . We also write 0 for the multi-index of zeros and 1 for the multi-index of ones. Finally, for multi-indices $\\pmb{\\nu}=(\\nu_{k})_{k=1}^{d}$ and $\\pmb{\\mu}=(\\mu_{k})_{k=1}^{d}$ , we write $\\nu\\geq\\mu$ to mean $\\nu_{k}\\geq\\mu_{k}$ , $\\forall k$ and likewise for $\\nu>\\mu$ . ", "page_idx": 27}, {"type": "text", "text": "C.1.3 Weights and weighted sequence spaces ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Let $d\\in\\mathbb{N}\\cup\\{\\infty\\}$ , $\\Lambda\\subseteq\\mathbb{N}_{0}^{d}$ and $\\pmb{w}=(w_{\\pmb{\\nu}})_{\\pmb{\\nu}\\in\\Lambda}\\ge\\mathbf{0}$ be a sequence of nonnegative weights. We define the weighted cardinality of a set $S\\subseteq\\Lambda$ as ", "page_idx": 27}, {"type": "equation", "text": "$$\n|S|_{\\pmb w}=\\sum_{\\pmb\\nu\\in S}w_{\\pmb\\nu}^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Given a sequence $\\pmb{c}=(c_{\\nu})_{\\nu\\in\\Lambda}$ we write ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|c\\|_{0,w}=|\\mathrm{supp}(c)|_{w}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Next, for a Banach space $(\\mathcal{Z},\\|\\cdot\\|_{\\mathcal{Z}})$ and $0<p\\le2$ , we define the weighted $\\ell_{w}^{p}(\\Lambda;\\mathcal{Z})$ space as the space of $\\mathcal{Z}$ -valued sequences $\\pmb{c}=\\overline{{(c_{\\nu})_{\\nu\\in\\Lambda}}}$ for which $\\|c\\|_{p,w;\\mathcal{Z}}<\\infty$ , where ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\pmb{c}\\|_{p,\\mathbf{u};\\mathcal{Z}}=\\left(\\sum_{\\nu\\in\\Lambda}w_{\\nu}^{2-p}\\|c_{\\nu}\\|_{\\mathcal{Z}}^{p}\\right)^{1/p}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Notice that $\\|\\cdot\\|_{2,u;\\mathcal{Z}}$ coincides with the unweighted norm $\\|\\cdot\\|_{2;\\mathcal{Z}}$ . ", "page_idx": 27}, {"type": "text", "text": "C.1.4 Legendre polynomials ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We write $\\{P_{n}\\}_{n\\in\\mathbb{N}_{0}}$ for the classical Legendre polynomials on $[-1,1]$ with normalization $P_{n}(1)=1$ . Since $\\begin{array}{r}{\\int_{-1}^{1}|P_{n}(x)|^{2}\\,\\mathrm{d}x\\,=\\,(n+1/2)^{-1}}\\end{array}$ , we define the orthonormal (with respect to the uniform probability measure) Legendre polynomials as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\psi_{n}(x)=\\sqrt{2n+1}P_{n}(x),\\quad x\\in[-1,1],\\;n\\in\\mathbb{N}_{0}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Let $\\boldsymbol{D}=[-1,1]^{\\mathbb{N}}$ as before, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{F}=\\{\\pmb{\\nu}=(\\nu_{i})_{i=1}^{\\infty}\\in\\mathbb{N}_{0}^{\\mathbb{N}}:|\\mathrm{supp}(\\pmb{\\nu})|<\\infty\\}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "be the set of multi-indices with finitely-many nonzero terms and define the multivariate Legendre polynomials as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Psi_{\\pmb{\\nu}}(\\pmb{x})=\\prod_{i\\in\\mathbb{N}}\\psi_{\\nu_{i}}(y_{i})\\equiv\\prod_{i\\in\\mathrm{supp}(\\pmb{\\nu})}\\psi_{\\nu_{i}}(x_{i}),\\quad\\forall\\pmb{x}\\in D,\\,\\pmb{\\nu}=(\\nu_{i})_{i=1}^{\\infty}\\in\\mathcal{F}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Here, the second equality follows from the fact that $\\psi_{0}=1$ . Then it is known that the set ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\{\\Psi_{\\nu}\\}_{\\nu\\in\\mathcal{F}}\\subset L_{\\varrho}^{2}(D)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "constitutes an orthonormal basis for $L_{\\varrho}^{2}(D)$ [19, \u00a73]. For later use, we also define the sequences ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\boldsymbol{u}=(u_{\\nu})_{\\nu\\in\\mathcal{F}},\\quad\\mathrm{where~}u_{\\nu}=\\|\\Psi_{\\nu}\\|_{L_{\\varrho}^{\\infty}(D)}=\\prod_{k\\in\\mathbb{N}}\\sqrt{2\\nu_{k}+1},\\,\\forall\\nu\\in\\mathcal{F},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{v}=(v_{\\pmb{\\nu}})_{\\pmb{\\nu}\\in\\mathcal{F}},\\quad\\mathrm{where~}v_{\\pmb{\\nu}}=u_{\\pmb{\\nu}}^{5+\\xi},\\;\\forall\\pmb{\\nu}\\in\\mathcal{F}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Here $\\xi\\geq0$ will be chosen later in the proof. ", "page_idx": 27}, {"type": "text", "text": "C.1.5 Miscellaneous ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Given an optimization problem $\\operatorname*{min}_{t}\\,f(t)$ , we say that $\\hat{t}$ is a $(\\sigma,\\tau)$ -approximate minimizer for some $\\sigma\\geq1$ and $\\tau\\geq0$ if $f(\\hat{t})\\leq\\sigma^{2}\\operatorname*{min}_{t}f(t)+\\tau^{2}$ . (In the main paper, we consider only $\\sigma=1$ , but for the proofs it is useful to allow $\\sigma>1$ ). ", "page_idx": 28}, {"type": "text", "text": "Finally, for convenience, given $X_{1},\\allowbreak\\cdot\\cdot\\cdot,X_{m}\\in\\mathcal{X}$ , we define the semi-norm ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|G\\|_{{\\mathsf{d i s c}},\\mu}={\\sqrt{{\\frac{1}{m}}\\sum_{i=1}^{m}\\|G(X_{i})\\|_{y}^{2}}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "of an operator $G:\\mathcal{X}\\rightarrow\\mathcal{Y}$ and ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|g\\|_{{\\sf d i s c},\\tilde{\\varsigma}}=\\|g\\circ\\mathcal{E}_{\\boldsymbol{X}}\\|_{{\\sf d i s c},\\mu}=\\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}\\|g\\circ\\mathcal{E}_{\\boldsymbol{X}}(\\boldsymbol{X}_{i})\\|_{\\mathcal{Y}}^{2}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "of a function $g:\\mathbb{R}^{\\mathbb{N}}\\to\\mathcal{V}$ . Here, as in (A.III), $\\tilde{\\zeta}$ denotes the pushforward measure $\\mathcal{E}_{\\mathcal{X}}\\#\\mu$ . ", "page_idx": 28}, {"type": "text", "text": "C.2 Overview of the proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "C.2.1 Theorem 3.1 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Theorems 3.1-3.2 are based on polynomials, and specifically, procedures for learning efficient Legendre polynomial approximations to holomorphic operators. As observed, these results are based on recent work on learning holomorphic, Banach-valued functions [2, 5, 6]. See also Remark C.1 below. ", "page_idx": 28}, {"type": "text", "text": "The proof of Theorem 3.1 involves three mains steps. ", "page_idx": 28}, {"type": "text", "text": "(a) Formulation (\u00a7D.1) and analysis (\u00a7D.2-D.4) of a suitable polynomial learning procedure.   \n(b) Construction of a family of DNNs that approximately emulates the Legendre polynomials $(\\S\\mathrm{D}.5)$ .   \n(c) Analysis of the corresponding training problem (2.5) (\u00a7D.6\u2013D.8). ", "page_idx": 28}, {"type": "text", "text": "Since our goal in this work is \u2018agnostic\u2019 DNN architectures (i.e., independent of the smoothness of the underlying operator), in step (a) we first define a nonlinear set (D.2) spanned by Legendre polynomials with nonzero indices in certain sets of bounded weighted cardinality. This is effectively a form of sparse polynomial approximation, and the analysis of the resulting learning procedure (D.3) relies heavily on techniques from compressed sensing. In order to bound the encoding-decoding error, we also require several results on Lipschitz continuity (Lemma D.2) and norm equivalences (Lemma D.4) for multivariate polynomials. ", "page_idx": 28}, {"type": "text", "text": "Step (b) relies on what have now become fairly standard results in DNN approximation theory: namely, the approximate emulation of orthogonal polynomials via DNNs of given width and depth. We present such a result in Lemma D.9, then use this to define the DNN family $\\mathcal{N}$ in (D.19). ", "page_idx": 28}, {"type": "text", "text": "We then analyze the DNN training problem (2.5) in step (c). Using the emulation result of step (b), we first show that any approximate minimizer $\\widehat{N}$ of (2.5) yields a polynomial that is also an approximate minimizer of the polynomial training problem (D.3) (Lemma D.12). We may then apply the results shown in Step (a) to prove a generalization bound (Theorem D.13). Up to this point, we have not used the holomorphy assumption. We now use this assumption to bound the various best polynomial approximation errors that arise in the previously-derived generalization bound $(\\S D.7)$ . Finally, in $\\S D.8$ we put all these estimates together to complete the proof. ", "page_idx": 28}, {"type": "text", "text": "Remark C.1 Theorem 3.1 is a generalization and improvement of [5, Thms. 4.1 & 4.2], which deals with the case of learning Banach-valued functions rather than operators. Specifically, the setting of [5] can be considered a special case of this paper where $\\mathcal{X}=\\ell^{\\infty}(\\mathbb{N})$ and the encoding error $E_{\\mathcal{X},q}=0$ . Moreover, Theorem 3.1 improves the main results of [5] in three key ways. First, the the DNN architectures bounds are much narrower: $\\mathrm{width}(\\mathcal{N})\\lesssim(m/L)^{1+\\delta}$ versus $\\mathrm{width}(N)\\lesssim m^{3+\\log_{2}(m)}$ in the latter. Second, Theorem 3.1 considers standard training, i.e., $\\ell^{2}$ -loss minimization, whereas [5, Thms. 4.1 & 4.2] requires regularization. Third, for Banach spaces, the error decay rate with respect to $m$ is roughly doubled: $E_{\\mathsf{a p p},2}=\\mathcal{O}(m^{1-1/p})$ in Theorem 3.1 versus $\\mathcal{O}(m^{1/2(1-1/p)})$ in [5, Thm. 4.1]. Finally, Theorem 3.1 also provides bounds in the $L_{\\mu}^{\\infty}$ -norm, whereas the results in [5] only consider the $L_{\\mu}^{2}$ -norm. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "C.2.2 Theorem 3.2 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The proof of Theorem 3.2 relies of three key steps. ", "page_idx": 29}, {"type": "text", "text": "(a) Using a minimizer of the polynomial training problem (D.3) to construct a family of minimizers of the DNN training problem (2.5).   \n(b) Analysis of the corresponding minimizers using the previously-derived bound for polynomial minimizers.   \n(c) Using the Lipschitz continuity of DNNs to show stability of the DNN minimizer and a permutation argument to show the existence of many parameters that lead to equally \u2018good\u2019 minimizers. ", "page_idx": 29}, {"type": "text", "text": "Given any minimizer of the polynomial training problem (D.3), it is straightforward to define a DNN with the desired generalization bound. Unfortunately, this will generally not be a minimizer of (2.5). To achieve the aims of step (a) we proceed as follows. First, we note that a DNN will be a minimizer if the corresponding approximation satisfies $\\widehat{F}(X_{i})=\\widetilde{Y}_{i}$ , where ${\\widetilde{Y}}_{i}$ are the closest points to the $Y_{i}$ from $\\widetilde{\\mathcal{V}}=\\mathcal{D}_{\\mathcal{V}}(\\mathbb{R}^{d_{\\mathcal{V}}})$ . To achieve this, we take the existing DNN then add on a suitable number of additional terms corresponding to the first $r>m$ order-one Legendre polynomials $(\\S\\mathrm{E.}1)$ . We show that by doing this, we can construct a DNN for which $\\widehat{F}(X_{i})=\\widetilde{Y}_{i}$ (Lemma E.1). ", "page_idx": 29}, {"type": "text", "text": "In Step (b), we first bound this DNN minimizer in terms of the polynomial minimizer plus the contributions of these additional terms (Lemma E.2). The latter involves the minimal singular value of a certain $m\\times r$ matrix $_B$ , which is the matrix of the linear system that enforces the condition $\\widehat F(X_{i})=\\widetilde Y_{i}$ . We bound this minimal singular value in $\\S E.3$ . ", "page_idx": 29}, {"type": "text", "text": "We then use this to complete the proof of part (A) of Theorem 3.2 in $\\S E.4$ . In this section, we also complete step (c) to establish parts (B) and (C). ", "page_idx": 29}, {"type": "text", "text": "Remark C.2 Like Theorem 3.1, Theorem 3.2 also relies on ideas from [5]. However, [5] does not address fully-connected DNN architectures. To address this challenge, the proof of Theorem 3.2 involves the technical construction described above. ", "page_idx": 29}, {"type": "text", "text": "C.2.3 Theorem 4.1 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Theorems 4.1 is based on [7, Thm. 4.4]. The basic idea is to consider a family of affine, holomorphic operators (F.4). This allows us to lower bound the quantity $\\theta_{m}(\\pmb{b})$ by the so-called Gelfand width (F.1) of a certain weighted unit ball in a finite-dimensional space. Bounds for such Gelfand widths are known, and this allows us to derive the corresponding result. The main difference between this and [7, Thm. 4.4] is the setup leading to the construction in (F.4). ", "page_idx": 29}, {"type": "text", "text": "C.2.4 Theorem 4.2 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Theorems 4.2 employs similar ideas, but in a more technical manner. We consider a family of linear, holomorphic operators (G.2), which involves a sum over $r$ groups of $m+1$ coefficients. We restrict to coefficients that lie in the null space of the corresponding sampling operator. Then, through a series of inequalities, we can lower bound $\\tilde{\\theta}_{m}(\\boldsymbol{b})$ by a sum over $r$ terms, each involving the $\\ell^{1}$ -norms of certain vectors in the null space of the matrix of the corresponding sampling operator (G.3). This matrix is a subgaussian random matrix. We now use a technical estimate from [75] for vectors in the null space of subgaussian random matrices, which shows that they cannot be \u2018spiky\u2019. Applying this and a series of further inequalities yields the result. ", "page_idx": 29}, {"type": "text", "text": "D Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "D.1 Formulation of an approximate polynomial training problem ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Let $\\Lambda\\subset{\\mathcal{F}}$ with ${\\mathrm{supp}}(\\pmb{\\nu})\\subseteq\\{1,\\ldots,d_{\\mathcal{X}}\\}$ , $\\forall\\pmb{\\nu}\\in\\Lambda$ , and write $N=|\\Lambda|$ . Let $k>0$ and define the set $S=S_{\\Lambda,k}=\\{S\\subseteq\\Lambda:|S|_{v}\\leq k\\}.$ (D.1) ", "page_idx": 29}, {"type": "text", "text": "Both $\\Lambda$ and $k$ will be chosen later in the proof. For any Banach space $(\\mathcal{Z},\\|\\cdot\\|_{\\mathcal{Z}})$ define the space ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{P}_{S;\\mathcal{Z}}=\\left\\{\\sum_{\\nu\\in S}c_{\\nu}\\Psi_{\\nu}:c_{\\nu}\\in\\mathcal{Z},S\\in\\mathcal{S}\\right\\}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then, given the training data (2.4), consider the problem ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{p\\in\\mathcal{P}_{s;\\tilde{y}}}\\frac{1}{m}\\sum_{i=1}^{m}\\|Y_{i}-p\\circ\\mathcal{E}_{\\mathcal{X}}(X_{i})\\|_{\\mathcal{Y}}^{2},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\widetilde{\\mathcal{V}}=\\mathcal{D}_{\\mathcal{V}}(\\mathbb{R}^{d_{\\mathcal{V}}})$ . Here and throughout the proofs, we slightly abuse notation: the polynomial $p:\\mathbb{R}^{\\mathbb{N}}\\to\\mathbb{R}$ , whereas $\\mathcal{E}_{\\mathcal{X}}$ has codomain $\\mathbb{R}^{d_{\\boldsymbol{X}}}$ . However, by construction, $p$ is independent of all but the first $d_{\\mathcal{X}}$ variables. Hence we may consider $p$ as a function $\\mathbb{R}^{d_{\\mathcal{X}}}\\rightarrow\\dot{\\mathbb{R}}$ . This aside, if $\\hat{p}$ is an approximate minimizer of (D.3), then we define the approximation to $F$ as ", "page_idx": 30}, {"type": "equation", "text": "$$\nF\\approx\\widehat{F}=\\widehat{p}\\circ\\mathcal{E}_{\\mathcal{X}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Notice that $p\\circ\\mathcal{E}_{\\mathcal{X}}=\\mathcal{D}_{\\mathcal{Y}}\\circ P\\circ\\mathcal{E}_{\\mathcal{X}}$ , where $P:\\mathbb{R}^{d_{\\mathcal{X}}}\\rightarrow\\mathbb{R}^{d_{\\mathcal{Y}}}$ is a vector-valued polynomial, since, by (A.IV), the map $\\mathcal{D}_{\\mathcal{Y}}$ is linear. The idea exploited in this proof is to construct a class of DNNs $\\mathcal{N}$ such that (i) all such polynomials $P$ are approximated by members of $\\mathcal{N}$ and (ii) the polynomial training problem (D.3) is approximated by the DNN training problem (2.5). The first step in this analysis is therefore to analyze the polynomial training problem (D.3). ", "page_idx": 30}, {"type": "text", "text": "D.2 Supporting lemmas ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We require several lemmas. The first relates the $L_{\\varrho}^{\\infty}$ -norm of a polynomial to its Pettis $L_{\\varrho}^{2}$ -norm. ", "page_idx": 30}, {"type": "text", "text": "Lemma D.1 (Nikolskii inequality for polynomials). Let $(\\mathcal{Z},\\|\\cdot\\|_{\\mathcal{Z}})$ be any Banach space and $\\begin{array}{r}{p=\\sum_{\\nu\\in S}c_{\\nu}\\Psi_{\\nu}}\\end{array}$ for some finite set $S\\subset{\\mathcal{F}}$ , where $c_{\\nu}\\in\\mathcal{Z}$ . Then ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|p\\|_{L_{\\varrho}^{\\infty}(D;\\mathcal{Z})}\\leq\\sqrt{|S|_{u}}\\||p\\||_{L_{\\varrho}^{2}(D;\\mathcal{Z})}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. By definition ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|p\\|_{L_{\\varrho}^{\\infty}(D;\\mathcal{Z})}=\\|p\\|_{L_{\\varrho}^{\\infty}(D;\\mathcal{Z})}=\\operatorname*{sup}_{z^{*}\\in B(\\mathcal{Z}^{*})}\\|z^{*}(p)\\|_{L_{\\varrho}^{\\infty}(D)}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Fix $z^{*}\\in B(\\mathcal{Z}^{*})$ and write $\\begin{array}{r}{z^{*}(p)=\\sum_{\\nu\\in S}z^{*}(c_{\\nu})\\Psi_{\\nu}}\\end{array}$ . By the triangle inequality and the definition of the weights (C.3), we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|z^{*}(p)\\|_{L_{\\varrho}^{\\infty}(D)}\\leq\\sum_{\\nu\\in S}|z^{*}(c_{\\nu})|\\|\\Psi_{\\nu}\\|_{L_{\\varrho}^{\\infty}(D)}=\\sum_{\\nu\\in S}u_{\\nu}|z^{*}(c_{\\nu})|.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We now apply the Cauchy\u2013Schwarz inequality, (C.1) and Parseval\u2019s identity to get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|z^{*}(p)\\|_{L_{\\varrho}^{\\infty}(D)}\\leq\\sqrt{|S|_{u}}\\sqrt{\\sum_{\\nu\\in S}|z^{*}(c_{\\nu})|^{2}}=\\sqrt{|S|_{u}}\\|z^{*}(p)\\|_{L_{\\varrho}^{2}(D)}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Since $z^{*}$ was arbitrary, we deduce that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|p\\|_{L_{\\varrho}^{\\infty}(D;\\mathcal{Z})}\\leq\\sqrt{|S|_{u}}\\operatorname*{sup}_{z^{*}\\in B(\\mathcal{Z}^{*})}\\|z^{*}(p)\\|_{L_{\\varrho}^{2}(D)}=\\sqrt{|S|_{u}}\\|p\\|_{L_{\\varrho}^{2}(D;\\mathcal{Z})},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "as required. ", "page_idx": 30}, {"type": "text", "text": "Next, we require the following bound on the Lipschitz constant of a multivariate polynomial. ", "page_idx": 30}, {"type": "text", "text": "Lemma D.2 (Lipschitz continuity for polynomials). Let $(\\mathcal{Z},\\|\\cdot\\|_{\\mathcal{Z}})$ be any Banach space and suppose that $\\begin{array}{r}{p=\\sum_{\\nu\\in S}c_{\\nu}\\Psi_{\\nu}}\\end{array}$ for some finite $S\\subset{\\mathcal{F}}$ , where $c_{\\nu}\\in\\mathcal{Z}$ . Then satisfies ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathrm{Lip}(p;B^{\\infty}(\\mathbb{N}),\\mathcal{Z})\\leq\\frac{1}{2}\\sqrt{|S|_{v}}\\cdot\\||p|\\|_{L_{\\varrho}^{2}(D;\\mathcal{Z})},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $B^{\\infty}(\\mathbb{N})=\\{\\pmb{x}\\in\\mathbb{R}^{\\mathbb{N}}:\\|\\pmb{x}\\|_{\\infty}\\leq1\\}$ is the unit ball of $\\ell^{\\infty}(\\mathbb{N})$ . ", "page_idx": 30}, {"type": "text", "text": "Proof. Fix $z^{*}\\in B(\\mathcal{Z}^{*})$ and let $\\begin{array}{r}{\\tilde{p}=z^{*}(p)=\\sum_{\\nu\\in S}z^{*}(c_{\\nu})\\Psi_{\\nu}=:\\sum_{\\nu\\in S}\\tilde{c}_{\\nu}\\Psi_{\\nu}}\\end{array}$ be the corresponding scalar-valued polynomial. Let $\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{x}}^{\\prime}\\in D$ . Then the mean value theorem gives that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\tilde{p}(\\pmb{x}^{\\prime})-\\tilde{p}(\\pmb{x})=\\sum_{i=1}^{\\infty}(x_{i}^{\\prime}-x_{i})\\frac{\\partial\\tilde{p}}{\\partial x_{i}}(t\\pmb{x}+(1-t)\\pmb{x}^{\\prime})\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for some $0\\le t\\le1$ . Since $B^{\\infty}(\\mathbb{N})\\equiv D$ , this and the fact that $D$ is convex give that ", "page_idx": 31}, {"type": "equation", "text": "$$\n|\\tilde{p}(\\pmb{x}^{\\prime})-\\tilde{p}(\\pmb{x})|\\leq\\|\\pmb{x}^{\\prime}-\\pmb{x}\\|_{\\infty}\\sum_{i=1}^{\\infty}\\left\\|\\frac{\\partial\\tilde{p}}{\\partial x_{i}}\\right\\|_{L_{\\varrho}^{\\infty}(D)}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We now consider the terms in the sum separately. Using the definition of the Legendre polynomials (see $\\S C.1.4)$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{\\partial\\tilde{p}}{\\partial x_{i}}=\\sum_{\\nu\\in S}\\tilde{c}_{\\nu}u_{\\nu}P_{\\nu_{i}}^{\\prime}(x_{i})\\prod_{j\\neq i}P_{\\nu_{j}}(x_{j}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The unnormalized Legendre polynomials satisy $1=P_{n}(1)=\\|P_{n}\\|_{L^{\\infty}([-1,1])}$ and $n(n+1)/2=$ $\\begin{array}{r}{P_{n}^{\\prime}(1)=\\|P_{n}^{\\prime}\\|_{L^{\\infty}([-1,1])}.}\\end{array}$ . Hence ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left\\lVert\\frac{\\partial\\tilde{p}}{\\partial x_{i}}\\right\\rVert_{L_{\\varrho}^{\\infty}(D)}\\leq\\sum_{\\nu\\in S}|\\tilde{c}_{\\nu}|u_{\\nu}\\nu_{i}(\\nu_{i}+1)/2.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We deduce that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{\\infty}\\left\\|\\frac{\\partial\\tilde{p}}{\\partial x_{i}}\\right\\|_{L_{\\varrho}^{\\infty}(D)}\\leq\\sum_{\\nu\\in S}|\\tilde{c}_{\\nu}|u_{\\nu}\\sum_{i=1}^{\\infty}\\nu_{i}(\\nu_{i}+1)/2.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Now, by definition of the weights $u_{\\nu}$ and $v_{\\nu}$ (see (C.3) and (C.4)), we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n(\\nu_{i}+1)\\leq\\prod_{j\\in\\mathbb{N}}(2\\nu_{j}+1)=u_{\\nu}^{2}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{\\infty}\\nu_{i}\\leq\\prod_{j\\in\\mathbb{N}}(2\\nu_{j}+1)=u_{\\nu}^{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Hence ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{\\infty}u_{\\nu}\\nu_{i}(\\nu_{i}+1)\\leq u_{\\nu}^{5}\\leq v_{\\nu}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Here we also used the fact that $\\textbf{\\em u}\\geq\\textbf{1}$ . We now apply this, the Cauchy-Schwarz inequality and Parseval\u2019s identity to get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{\\infty}\\left\\lVert\\frac{\\partial\\tilde{p}}{\\partial x_{i}}\\right\\rVert_{L_{\\varrho}^{\\infty}(D)}\\leq\\frac{1}{2}\\lVert\\tilde{p}\\rVert_{L_{\\varrho}^{2}(D)}\\sqrt{|S|_{v}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Substituting this into (D.5) now gives ", "page_idx": 31}, {"type": "equation", "text": "$$\n|\\tilde{p}(\\pmb{x}^{\\prime})-\\tilde{p}(\\pmb{x})|\\leq\\frac{1}{2}\\|\\tilde{p}\\|_{L_{\\varrho}^{2}(D)}\\sqrt{|S|_{v}}\\|\\pmb{x}^{\\prime}-\\pmb{x}\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We now recall that $\\tilde{p}=z^{*}(p)$ and $z^{*}\\in B(\\mathcal{Z}^{*})$ was arbitrary to get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|p(\\pmb{x}^{\\prime})-p(\\pmb{x})\\|_{\\mathcal{Z}}=\\underset{z^{*}\\in B(\\mathcal{Z}^{*})}{\\operatorname*{sup}}|z^{*}(p(\\pmb{x}^{\\prime})-p(\\pmb{x}))|}\\\\ &{\\phantom{\\leq\\frac{1}{2}}\\leq\\frac{1}{2}\\underset{z^{*}\\in B(\\mathcal{Z}^{*})}{\\operatorname*{sup}}\\,\\|z^{*}(p)\\|_{L_{\\varrho}^{2}(D)}\\sqrt{|S|_{v}}\\|\\pmb{x}^{\\prime}-\\pmb{x}\\|_{\\infty}}\\\\ &{\\phantom{\\leq\\frac{1}{2}}=\\frac{1}{2}\\|p\\|_{L_{\\varrho}^{2}(D;\\mathcal{Z})}\\sqrt{|S|_{v}}\\|\\pmb{x}^{\\prime}-\\pmb{x}\\|_{\\infty},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "as required. ", "page_idx": 31}, {"type": "text", "text": "We now show that this result can be used to imply a norm equivalence for polynomials. For this we first require the following lemma. Note that in this and subsequent results, we abuse notation and write $\\tilde{\\zeta}$ for both the measure on $[-1,1]^{d_{\\mathcal{X}}}$ defined in (A.III) and the measure on $\\boldsymbol{D}=[-1,1]^{\\mathbb{N}}$ defined by tensoring this measure (corresponding to the first $d_{\\mathcal{X}}$ variables $x_{1},\\ldots,x_{d_{\\mathcal{X}}})$ ) with the uniform measure on $D\\backslash[-1,1]^{d_{\\mathcal{X}}}$ (corresponding to the remaining variables $x_{d_{x}+1},x_{d_{x}+2},...)$ . ", "page_idx": 32}, {"type": "text", "text": "Lemma D.3 (Closeness of $L^{2}$ norms). Let $(\\mathcal{Z},\\|\\cdot\\|_{\\mathcal{Z}})$ be any Banach space, $\\varsigma$ be the measure defined in (A.I) and $\\tilde{\\zeta}$ be the measure defined in (A.III). Suppose that $f\\in L_{\\varsigma}^{2}(D;\\mathcal{Z})$ is Lipschitz continuous with constant $L=\\mathrm{Lip}(f;B^{\\infty}(\\mathbb{N}),\\mathcal{Z})<\\infty$ and that $f$ depends only on its first $d_{\\mathcal{X}}$ variables. Then $f\\in L_{\\widetilde{\\zeta}}^{2}(D;\\mathcal{Z})$ and ", "page_idx": 32}, {"type": "text", "text": "where ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|f\\|_{L_{\\varsigma}^{2}(D;\\mathcal{Z})}-\\delta\\leq\\|f\\|_{L_{\\xi}^{2}(D;\\mathcal{Z})}\\leq\\|f\\|_{L_{\\varsigma}^{2}(D;\\mathcal{Z})}+\\delta,}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{\\qquad\\delta=L\\cdot\\|\\iota_{d_{\\mathcal{X}}}-\\iota_{d_{\\mathcal{X}}}\\circ\\widetilde{\\mathcal{D}}_{\\mathcal{X}}\\circ\\widetilde{\\mathcal{E}}_{\\mathcal{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\ell^{\\infty}(\\mathbb{N}))}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. Fix $z^{*}\\in B(\\mathcal{Z}^{*})$ , let $g=z^{*}(f)\\in L_{\\varsigma}^{2}(D)$ and set ", "page_idx": 32}, {"type": "equation", "text": "$$\na=\\left\\lVert g\\right\\rVert_{L_{\\xi}^{2}(D)},\\qquad b=\\left\\lVert g\\right\\rVert_{L_{\\xi}^{2}(D)}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Notice that ", "page_idx": 32}, {"type": "equation", "text": "$$\n|g(\\pmb{x})-g(\\pmb{x}^{\\prime})|\\leq\\|\\pmb{z}^{*}\\|_{\\mathcal{Z}^{*}}\\|f(\\pmb{x})-f(\\pmb{x}^{\\prime})\\|_{\\mathcal{Z}}\\leq L\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|_{\\infty},\\quad\\forall\\pmb{x},\\pmb{x}^{\\prime}\\in B^{\\infty}(\\mathbb{N}).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Now, with slight abuse of notation, $g(\\iota(X))=g(\\iota_{d_{\\mathcal{X}}}(X))$ due to the assumption on $f$ . Therefore, using this and the Cauchy\u2013Schwarz inequality, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\imath^{2}-b^{2}=\\int_{D}\\big(g(\\iota_{d_{x}}\\circ\\widetilde{\\mathcal{D}}_{\\mathcal{X}}\\circ\\widetilde{\\mathcal{E}}_{\\mathcal{X}}(X))\\big)^{2}-\\big(g(\\iota_{d_{x}}(X))\\big)^{2}\\,\\mathrm{d}\\mu(X)}}\\\\ &{}&{\\leq L\\displaystyle\\int_{D}\\|\\iota_{d_{x}}(X)-\\iota_{d_{x}}\\circ\\widetilde{\\mathcal{D}}_{\\mathcal{X}}\\circ\\widetilde{\\mathcal{E}}_{\\mathcal{X}}(X)\\|_{\\infty}\\,\\Big(\\lvert g(\\iota_{d_{x}}(X))\\rvert+\\lvert g(\\iota_{d_{x}}\\circ\\widetilde{\\mathcal{D}}_{\\mathcal{X}}\\circ\\widetilde{\\mathcal{E}}_{\\mathcal{X}})\\rvert\\Big)\\,\\,\\mathrm{d}\\mu(X)}\\\\ &{}&{\\leq L\\|\\iota_{d_{x}}-\\iota_{d_{x}}\\circ\\widetilde{\\mathcal{D}}_{\\mathcal{X}}\\circ\\widetilde{\\mathcal{E}}_{\\mathcal{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\ell^{\\infty}(\\mathbb{N}))}\\,(a+b)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We deduce that $a-b\\leq\\delta$ . Since $z^{*}$ was arbitrary, we get ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Vert f\\Vert_{L_{\\xi}^{2}(D;\\mathcal{Z})}=\\underset{z^{*}\\in B(\\mathcal{Z}^{*})}{\\operatorname*{sup}}\\Vert z^{*}(f)\\Vert_{L_{\\xi}^{2}(D)}\\le\\underset{z^{*}\\in B(\\mathcal{Z}^{*})}{\\operatorname*{sup}}\\Vert z^{*}(f)\\Vert_{L_{\\xi}^{2}(D)}+\\delta=\\Vert f\\Vert_{L_{\\xi}^{2}(D;\\mathcal{Z})}+\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "which gives the upper bound. The same argument applied to $b^{2}-a^{2}$ also gives the lower bound. ", "page_idx": 32}, {"type": "text", "text": "Lemma D.4 (Norm equivalences for polynomials). Let $(\\mathcal{Z},\\|\\cdot\\|_{\\mathcal{Z}})$ be any Banach space, $\\varsigma$ be the measure defined in (A.I), $\\tilde{\\zeta}$ be the measure defined in (A.III) and $S\\subset{\\mathcal{F}}$ with ${\\mathrm{supp}}(\\pmb{\\nu})\\in\\left\\{1,\\ldots,d_{\\mathcal{X}}\\right\\}$ , $\\forall\\pmb{\\nu}\\in S$ . Suppose that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sqrt{|S|_{\\pmb{v}}}\\cdot\\|\\iota_{d_{\\mathcal{X}}}-\\iota_{d_{\\mathcal{X}}}\\circ\\widetilde{\\mathcal{D}}_{\\mathcal{X}}\\circ\\widetilde{\\mathcal{E}}_{\\mathcal{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\ell^{\\infty}(\\mathbb{N}))}\\leq c,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for some sufficiently small universal constant $c>0$ . Then the norm equivalence ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|p\\|_{L_{\\varrho}^{2}(D;\\mathcal{Z})}\\lesssim\\|p\\|_{L_{\\xi}^{2}(D;\\mathcal{Z})}\\lesssim\\|p\\|_{L_{\\varrho}^{2}(D;\\mathcal{Z})}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "holds for all $\\begin{array}{r}{p=\\sum_{\\nu\\in S}c_{\\nu}\\Psi_{\\nu}}\\end{array}$ , where $c_{\\nu}\\in\\mathcal{Z}$ . ", "page_idx": 32}, {"type": "text", "text": "Proof. Combining Lemmas D.2 and D.3, we see that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|p\\|_{L_{\\varsigma}^{2}(D;\\mathcal{Z})}-\\delta\\leq\\|p\\|_{L_{\\xi}^{2}(D;\\mathcal{Z})}\\leq\\|p\\|_{L_{\\varsigma}^{2}(D;\\mathcal{Z})}+\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\begin{array}{r}{\\delta=\\frac{1}{2}\\||p|\\|_{L_{\\varrho}^{2}(D;\\mathbb{Z})}\\sqrt{|S|_{v}}\\|\\iota_{d_{\\mathcal{X}}}-\\iota_{d_{\\mathcal{X}}}\\circ\\widetilde{\\mathcal{D}}_{\\mathcal{X}}\\circ\\widetilde{\\mathcal{E}}_{\\mathcal{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\ell^{\\infty}(\\mathbb{N}))}\\leq c\\||p\\|\\|_{L_{\\varrho}^{2}(D;\\mathcal{Z})}/2.}\\end{array}$ . By (A.I), there are constants $c_{1}\\geq c_{2}>0$ such that ", "page_idx": 32}, {"type": "equation", "text": "$$\nc_{1}\\|p\\|_{L_{\\varrho}^{2}(D;\\mathcal{Z})}\\leq\\|p\\|_{L_{\\varsigma}^{2}(D;\\mathcal{Z})}\\leq c_{2}\\|p\\|_{L_{\\varrho}^{2}(D;\\mathcal{Z})}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We now take $c=c_{1}/2$ . ", "page_idx": 32}, {"type": "text", "text": "D.3 Analysis of (D.3) ", "page_idx": 33}, {"type": "text", "text": "We now analyze (D.3). Our analysis relies on the following result, which shows an error bound subject to a certain discrete metric inequality (D.7). ", "page_idx": 33}, {"type": "text", "text": "Lemma D.5 (Discrete metric inequality implies error bounds). Let $\\mathcal{Q}:\\mathcal{Y}\\to\\widetilde{\\mathcal{Y}}=\\mathcal{D}_{\\mathcal{Y}}(\\mathbb{R}^{d_{\\mathcal{Y}}})$ be $a$ bounded linear operator and $\\pi_{\\mathcal{Q}}=||\\mathcal{Q}||_{\\mathcal{V}\\to\\mathcal{V}}$ . Suppose that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Vert p-q\\Vert_{\\mathrm{disc},\\tilde{\\varsigma}}\\geq\\alpha\\operatorname*{max}\\{\\Vert p-q\\Vert_{L_{\\varrho}^{2}(D;y)},\\Vert p-q\\Vert_{L_{\\tilde{\\varsigma}}^{2}(D;y)}\\},\\quad\\forall p,q\\in\\mathcal{P}_{S;\\tilde{\\mathcal{Y}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "for some $\\alpha>0$ . Then, for any $F\\in L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y}),\\,p\\in\\mathcal{P}_{\\mathcal{S};\\widetilde{\\mathcal{Y}}}$ and $q\\in\\mathcal{P}_{S;\\mathcal{D}}$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|F-p\\circ\\mathcal{E}_{\\boldsymbol{X}}\\|_{L_{\\mu}^{2}(\\boldsymbol{X};\\mathcal{Y})}\\leq\\|\\vert F-\\mathcal{Q}\\circ F\\vert\\|_{L_{\\mu}^{2}(\\boldsymbol{X};\\mathcal{Y})}+\\alpha^{-1}\\|p-\\mathcal{Q}\\circ q\\|_{\\mathrm{disc},\\tilde{\\mathcal{G}}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\;\\pi_{\\mathcal{Q}}\\|F-q\\circ\\mathcal{E}_{\\boldsymbol{X}}\\|_{L_{\\mu}^{2}(\\boldsymbol{X};\\mathcal{Y})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\||F-p\\circ\\mathcal{E}_{\\mathcal{X}}|\\right\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}\\leq\\left\\||F-\\mathcal{Q}\\circ F|\\right\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\sqrt{2k}/\\alpha\\left\\|p-\\mathcal{Q}\\circ q\\right\\|_{{\\sf d i s c},\\tilde{\\mathcal{G}}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left.\\pi_{\\mathcal{Q}}\\right\\|F-q\\circ\\mathcal{E}_{\\mathcal{X}}\\right\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. By the triangle inequality and properties of $\\mathcal{Q}$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|F-p\\circ\\mathcal{E}_{X}\\right\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}}\\\\ &{\\leq\\|F-Q\\circ F\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}+\\left\\|p\\circ\\mathcal{E}_{X}-Q\\circ q\\circ\\mathcal{E}_{X}\\right\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}+\\left\\|Q\\circ F-Q\\circ q\\circ\\mathcal{E}_{X}\\right\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}}\\\\ &{\\leq\\left\\|F-Q\\circ F\\right\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}+\\left\\|p\\circ\\mathcal{E}_{X}-Q\\circ q\\circ\\mathcal{E}_{X}\\right\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}+\\pi_{Q}\\left\\|F-q\\circ\\mathcal{E}_{X}\\right\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Now, since $p,Q\\circ q\\in\\mathcal{P}_{S;\\widetilde{\\mathcal{D}}}$ , the second term can be bounded by ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|p\\circ\\mathcal{E}_{\\mathcal{X}}-\\mathcal{Q}\\circ q\\circ\\mathcal{E}_{\\mathcal{X}}\\right\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}=\\left\\|p-\\mathcal{Q}\\circ q\\right\\|_{L_{\\tilde{\\varepsilon}}^{2}(D;\\mathcal{Y})}\\leq\\alpha^{-1}\\|p-\\mathcal{Q}\\circ q\\|_{{\\sf d i s c},\\tilde{\\mathcal{G}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This yields the first result. ", "page_idx": 33}, {"type": "text", "text": "For the second result, we once more write ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\||F-p\\circ\\mathcal{E}_{\\mathcal{X}}\\right\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}\\leq\\left\\||F-\\mathcal{Q}\\circ F\\right\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\left\\||p\\circ\\mathcal{E}_{\\mathcal{X}}-\\mathcal{Q}\\circ q\\circ\\mathcal{E}_{\\mathcal{X}}\\right\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left.\\pi_{\\mathcal{Q}}\\right\\||F-q\\circ\\mathcal{E}_{\\mathcal{X}}\\right\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For the second term, we use (A.III) to write ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|p\\circ\\mathcal{E}_{\\mathcal{X}}-\\mathcal{Q}\\circ q\\circ\\mathcal{E}_{\\mathcal{X}}\\right\\|_{L_{\\varepsilon}^{\\infty}(\\mathcal{X};\\mathcal{Y})}=\\left\\|p-\\mathcal{Q}\\circ q\\right\\|_{L_{\\varepsilon}^{\\infty}(D;\\mathcal{Y})}\\leq\\left\\|p-\\mathcal{Q}\\circ q\\right\\|_{L_{\\varepsilon}^{\\infty}(D;\\mathcal{Y})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Notice that $p-\\mathcal{Q}\\circ q$ is a polynomial supported in a set $S$ with $|S|_{u}\\le2k$ . We now apply Lemma D.1 and (D.7) to obtain ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|p\\circ\\mathcal{E}_{\\mathcal{X}}-\\mathcal{Q}\\circ q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}\\leq\\sqrt{2k}\\|p-\\mathcal{Q}\\circ q\\|_{L_{\\varrho}^{2}(D;\\mathcal{Y})}\\leq\\sqrt{2k}/\\alpha\\|p-\\mathcal{Q}\\circ q\\|_{{\\sf d i s c},\\tilde{\\mathcal{G}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This gives the result. ", "page_idx": 33}, {"type": "text", "text": "Theorem D.6 (Error bound for polynomial minimizers). Let $\\mathcal{Q}:\\mathcal{Y}\\to\\widetilde{\\mathcal{Y}}=\\mathcal{D}_{\\mathcal{Y}}(\\mathbb{R}^{d_{\\mathcal{Y}}})$ be a bounded linear operator, $\\pi_{\\mathcal{Q}}\\,=\\,\\|\\mathcal{Q}\\|_{\\mathcal{V}\\to\\mathcal{V}}$ and suppose that (D.7) holds. Let $\\widehat F$ be as in (D.4) for some $(\\sigma,\\tau)$ -approximate minimizer $\\hat{p}$ of (D.3). Then ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}&{\\displaystyle\\||F-\\widehat{F}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\leq\\||F-\\mathcal{Q}\\circ F\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}+\\frac{\\sigma+1}{\\alpha}\\||F-\\mathcal{Q}\\circ F\\|_{\\mathrm{disc},\\mu}}\\\\ &{\\quad+\\,\\pi_{\\mathcal{Q}}\\left(\\|F-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}+\\frac{\\sigma+1}{\\alpha}\\|F-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{\\mathrm{disc},\\mu}\\right)}\\\\ &{\\quad+\\,\\frac{\\tau}{\\alpha}+\\frac{\\sigma+1}{\\alpha\\sqrt{m}}\\|E\\|_{2,\\mathcal{Y}}}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\big\\|F-\\widehat{F}\\big\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}\\leq\\big\\|F-\\mathcal{Q}\\circ F\\big\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\frac{\\sqrt{2k}(\\sigma+1)}{\\alpha}\\big\\|\\big\\|F-\\mathcal{Q}\\circ F\\big\\|_{\\mathrm{disc},\\mu}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad+\\left.\\pi_{\\mathcal{Q}}\\left(\\big\\|F-q\\circ\\mathcal{E}_{\\mathcal{X}}\\big\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\frac{\\sqrt{2k}(\\sigma+1)}{\\alpha}\\big\\|\\big\\|F-q\\circ\\mathcal{E}_{\\mathcal{X}}\\big\\|_{\\mathrm{disc},\\mu}\\right)\\right.}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad+\\left.\\frac{\\sqrt{2k}\\tau}{\\alpha}+\\frac{\\sqrt{2k}(\\sigma+1)}{\\alpha\\sqrt{m}}\\|E\\|_{2;\\mathcal{Y}}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "for all $q\\in\\mathcal{P}_{S;\\mathcal{D}}$ , where $\\pmb{E}=(E_{i})_{i=1}^{m}\\in\\mathcal{V}^{m}$ is the (Banach-valued) vector of noise terms. ", "page_idx": 34}, {"type": "text", "text": "Proof. We apply the previous lemma with $p=\\hat{p}$ . This gives ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|F-\\widehat{F}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\leq\\|F-\\mathcal{Q}\\circ F\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}+\\alpha^{-1}\\|\\widehat{p}-\\mathcal{Q}\\circ q\\|_{{\\sf d i s c},\\tilde{\\mathcal{G}}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\pi_{\\mathcal{Q}}\\|F-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}}\\\\ &{\\|F-\\widehat{F}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}\\leq\\|F-\\mathcal{Q}\\circ F\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\sqrt{2k}/\\alpha\\|\\widehat{p}-\\mathcal{Q}\\circ q\\|_{{\\sf d i s c},\\tilde{\\mathcal{G}}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\pi_{\\mathcal{Q}}\\|F-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Consider the second term. We have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{p}-\\mathcal{Q}\\circ q\\left\\|_{\\mathsf{d i s c},\\bar{\\epsilon}}=\\left\\|\\hat{p}\\circ\\mathcal{E}_{\\mathcal{X}}-\\mathcal{Q}\\circ q\\circ\\mathcal{E}_{\\mathcal{X}}\\right\\|_{\\mathsf{d i s c},\\mu}\\leq\\left\\|F-\\mathcal{Q}\\circ q\\circ\\mathcal{E}_{\\mathcal{X}}\\right\\|_{\\mathsf{d i s c},\\mu}+\\left\\|F-\\hat{p}\\circ\\mathcal{E}_{\\mathcal{X}}\\right\\|_{\\mathsf{d i s c},\\mu}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Consider the second term of this expression. By the triangle inequality and the facts that $\\hat{p}$ is a $(\\sigma,\\tau)$ -minimizer and $\\mathcal Q\\circ q$ is feasible, we obtain ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|F-\\hat{p}\\circ\\mathcal{E}_{\\boldsymbol{X}}\\|_{\\mathrm{disc},\\mu}\\leq\\sqrt{\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\|Y_{i}-\\hat{p}\\circ\\mathcal{E}_{\\boldsymbol{X}}\\|_{y}^{2}}+\\frac{1}{\\sqrt{m}}\\|\\boldsymbol{E}\\|_{2;y}}}\\\\ &{\\leq\\sigma\\sqrt{\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\|Y_{i}-\\mathcal{Q}\\circ q\\circ\\mathcal{E}_{\\boldsymbol{X}}\\|_{y}^{2}}+\\tau+\\frac{1}{\\sqrt{m}}\\|\\boldsymbol{E}\\|_{2;y}}\\\\ &{\\leq\\sigma\\|\\boldsymbol{F}-\\mathcal{Q}\\circ q\\circ\\mathcal{E}_{\\boldsymbol{X}}\\|_{\\mathrm{disc},\\mu}+\\tau+\\frac{\\sigma+1}{\\sqrt{m}}\\|\\boldsymbol{E}\\|_{2;y}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore, we get ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\Vert\\hat{p}-\\mathcal{Q}\\circ q\\Vert_{{\\bf d i s c},\\tilde{\\varsigma}}\\leq(\\sigma+1)\\Vert|F-\\mathcal{Q}\\circ q\\circ\\mathcal{E}_{\\boldsymbol{X}}\\Vert_{{\\bf d i s c},\\mu}+\\tau+\\frac{\\sigma+1}{\\sqrt{m}}\\Vert E\\Vert_{2;\\mathcal{Y}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We now estimate the first term in this expression as follows: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Vert F-\\mathcal{Q}\\circ q\\circ\\mathcal{E}_{\\mathcal{X}}\\Vert_{\\mathrm{disc},\\mu}\\leq\\Vert F-\\mathcal{Q}\\circ F\\Vert_{\\mathrm{disc},\\mu}+\\Vert\\mathcal{Q}\\circ F-\\mathcal{Q}\\circ q\\circ\\mathcal{E}_{\\mathcal{X}}\\Vert_{\\mathrm{disc},\\mu}}&{}\\\\ {\\leq\\Vert F-\\mathcal{Q}\\circ F\\Vert_{\\mathrm{disc},\\mu}+\\pi_{\\mathcal{Q}}\\Vert F-q\\circ\\mathcal{E}_{\\mathcal{X}}\\Vert_{\\mathrm{disc},\\mu}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore, we conclude that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\hat{p}-\\mathcal{Q}\\circ q\\|_{\\mathrm{disc},\\tilde{\\varsigma}}\\leq(\\sigma+1)\\|F-\\mathcal{Q}\\circ F\\|_{\\mathrm{disc},\\mu}+(\\sigma+1)\\pi_{\\mathcal{Q}}\\||F-q\\circ\\mathcal{E}_{\\boldsymbol{X}}\\|_{\\mathrm{disc},\\mu}+\\tau+\\frac{\\sigma+1}{\\sqrt{m}}\\|E\\|_{2,\\mathcal{Y}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Combining this with (D.10) now gives the result. ", "page_idx": 34}, {"type": "text", "text": "D.4 Ensuring (D.7) holds with high probability ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "For the proof of the next lemma and subsequent steps of the proof, we let $\\Lambda=\\{\\nu_{1},\\dots,\\nu_{N}\\}$ and define the matrix ", "page_idx": 34}, {"type": "equation", "text": "$$\nA=\\left(\\frac{\\Psi_{\\pmb{\\nu}_{j}}(\\mathcal{E}_{\\mathcal{X}}(X_{i}))}{\\sqrt{m}}\\right)_{i,j=1}^{m,N}\\in\\mathbb{R}^{m\\times N}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Lemma D.7. Let $0<\\epsilon<1$ , $0<\\delta<1$ , $k>0$ and suppose that ", "page_idx": 35}, {"type": "equation", "text": "$$\nm\\ge c_{0}\\cdot\\delta^{-2}\\cdot k\\cdot(\\log(\\mathrm{e}N)\\cdot\\log^{2}(k/\\delta)+\\log(2/\\epsilon))\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "for some universal constant $c_{0}>0$ . Let $T=\\{c\\in\\mathbb{R}^{N}:\\|c\\|_{2}=1,\\ \\|c\\|_{0,v}\\leq k\\}$ and ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\theta_{+}=\\operatorname*{sup}\\left\\{\\mathbb{E}\\|A\\pmb{c}\\|_{2}^{2}:\\b{c}\\in T\\right\\},\\quad\\theta_{-}=\\operatorname*{inf}\\left\\{\\mathbb{E}\\|A\\pmb{c}\\|_{2}^{2}:\\b{c}\\in T\\right\\}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\pmb{A c}\\|_{2}^{2}\\geq\\left(\\theta_{-}-(1+\\theta_{+})c_{1}\\delta\\right)\\|\\pmb{c}\\|_{2}^{2},\\quad\\forall\\pmb{c}\\in\\mathbb{R}^{N},\\;\\|\\pmb{c}\\|_{0,v}\\leq k.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "with probability at least $1-\\epsilon$ , where $c_{1}>0$ is a universal constant. ", "page_idx": 35}, {"type": "text", "text": "Proof. The proof is based on [12, Thm. 2.13]. Since $\\|c\\|_{1,v}\\leq\\sqrt{\\|c\\|_{0,v}}\\|c\\|_{2}\\leq\\sqrt{k}$ , we see that ", "page_idx": 35}, {"type": "equation", "text": "$$\nT\\subseteq\\left\\{c\\in\\mathbb{R}^{N}:\\|c\\|_{1,v}\\leq{\\sqrt{k}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Define the random vector $\\boldsymbol{X}\\in\\mathbb{R}^{N}$ by $\\pmb{X}=(\\Psi_{\\pmb{\\nu}_{i}}(\\mathcal{E}_{\\mathcal{X}}(X)))_{i=1}^{N}$ for $X\\sim\\mu$ . Observe that ", "page_idx": 35}, {"type": "equation", "text": "$$\n|\\langle X,e_{j}\\rangle|=|\\Psi_{\\pmb{\\nu}_{i}}(\\mathcal{E}_{X}(X))|\\leq u_{\\pmb{\\nu}_{i}}\\leq v_{\\pmb{\\nu}_{i}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "almost surely, by (A.III) and the definition of the weights. Therefore, by [12, Thm. 2.13], if ", "page_idx": 35}, {"type": "equation", "text": "$$\nm\\geq c_{0}\\cdot\\delta^{-2}\\cdot k\\cdot\\log(\\mathrm{e}N)\\cdot\\log^{2}(k/(c_{1}\\delta)),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "it holds that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{c\\in T}\\left|\\frac{1}{m}\\sum_{i=1}^{m}|\\langle c,X_{i}\\rangle|^{2}-\\mathbb{E}|\\langle c,X\\rangle|^{2}\\right|\\leq c_{1}\\delta\\left(1+\\operatorname*{sup}_{c\\in T}\\mathbb{E}|\\langle c,X\\rangle|^{2}\\right)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "with probability at least $1-2\\exp(-c_{2}\\delta^{2}m/k)$ . Here $c_{0},c_{1},c_{2}>0$ are universal constants. Now observe that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{1}{m}\\sum_{i=1}^{m}|\\langle c,X_{i}\\rangle|^{2}=\\|A c\\|_{2}^{2},\\qquad\\mathbb{E}|\\langle c,X\\rangle|^{2}=\\mathbb{E}(\\|A c\\|_{2}^{2}).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Therefore, we have shown that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{A}\\mathbf{c}\\right\\|_{2}^{2}\\geq\\theta_{-}-(1+\\theta_{+})c_{1}\\delta,\\quad\\forall\\mathbf{c}\\in T,\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "with probability at least $1-2\\exp(-c_{2}\\delta^{2}m/k)$ , provided $m$ satisfies (D.12). To conclude the result, we observe that (D.11) implies (D.12), up to a possible change in the universal constant $c_{0}$ . Moreover, it also implies that ", "page_idx": 35}, {"type": "equation", "text": "$$\n2\\exp(-c_{2}\\delta^{2}m/k)\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Hence we obtain the result. ", "page_idx": 35}, {"type": "text", "text": "Lemma D.8. There exist universal constants $c_{0},c_{1},c_{2}>0$ such that the following holds. Suppose that ", "page_idx": 35}, {"type": "equation", "text": "$$\nm\\ge c_{0}\\cdot k\\cdot(\\log(\\mathrm{e}N)\\cdot\\log^{2}(k)+\\log(2/\\epsilon))\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sqrt{k}\\cdot\\|\\iota_{d_{\\mathcal{X}}}-\\iota_{d_{\\mathcal{X}}}\\circ\\widetilde{{\\mathcal{D}}}_{\\mathcal{X}}\\circ\\widetilde{{\\mathcal{E}}}_{\\mathcal{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\ell^{\\infty}(\\mathbb{N}))}\\leq c_{1}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then (D.7) holds with probability at least $1-\\epsilon$ and constant $\\alpha\\geq c_{2}$ . ", "page_idx": 35}, {"type": "text", "text": "Proof. We shall apply the previous lemma with $k$ replaced by $2k$ . First, we estimate $\\theta_{+}$ and $\\theta_{-}$ . Let $c\\in T$ , with $T$ as defined therein with $2k$ in place of $k$ . Write $\\begin{array}{r}{p=\\sum_{\\nu\\in\\Lambda}c_{\\nu}\\Psi_{\\nu}}\\end{array}$ for the corresponding (scalar-valued) polynomial. Then ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{A}\\mathbf{c}\\right\\|_{2}^{2}=\\frac{1}{m}\\sum_{i=1}^{m}|p\\circ\\mathcal{E}_{\\mathcal{X}}(X_{i})|^{2},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and therefore ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big\\lVert A\\pmb{c}\\big\\rVert_{2}^{2}=\\big\\lVert p\\big\\rVert_{L_{\\hat{\\varsigma}}^{2}(D)}^{2}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Since $\\|c\\|_{0,v}\\leq2k$ , (D.14) implies (D.6). We now apply Lemma D.4 and the fact that $\\left\\|p\\right\\|_{L_{\\varrho}^{2}(D)}=$ $\\|c\\|_{2}=1$ to get ", "page_idx": 36}, {"type": "equation", "text": "$$\nc_{3}\\leq\\mathbb{E}\\|A c\\|_{2}^{2}\\leq c_{4}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "for universal constants $c_{4}\\geq c_{3}>0$ . Since $c\\in T$ was arbitrary to deduce that ", "page_idx": 36}, {"type": "equation", "text": "$$\nc_{3}\\leq\\theta_{-}\\leq\\theta_{+}\\leq c_{4}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We now show that (D.7) holds with the desired probability. Let $p,q\\in\\mathcal{P}_{\\mathcal{S};\\tilde{\\mathcal{D}}}$ be arbitrary. Then their difference $h=p-q$ can be expressed as ", "page_idx": 36}, {"type": "equation", "text": "$$\nh=\\sum_{\\nu\\in\\Lambda}d_{\\nu}\\Psi_{\\nu}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\pmb{d}=(d_{\\pmb{\\nu}})_{\\pmb{\\nu}\\in\\Lambda}\\in\\widetilde{\\mathcal{V}}^{N}$ satisfies ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\|d\\|_{0,u}\\leq\\|d\\|_{0,v}\\leq2k.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Now, this, (D.14) and Lemma D.4 imply that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|h\\|_{L_{\\xi}^{2}(D;\\mathcal{V})}\\le c_{4}\\|h\\|_{L_{\\varrho}^{2}(D;\\mathcal{V})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Therefore, in order to prove (D.7), it suffices to show that $\\|h\\|_{L_{\\varrho}^{2}(D;\\mathcal{V})}\\lesssim\\|h\\|_{{\\sf d i s c},\\tilde{\\varsigma}}.$ . ", "page_idx": 36}, {"type": "text", "text": "Observe that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\|h\\|_{\\mathrm{disc},\\tilde{\\varsigma}}=\\|\\pmb{A}\\pmb{d}\\|_{2;\\mathcal{V}}\\geq\\|\\pmb{A}\\pmb{d}\\|_{2;\\mathcal{V}}=\\operatorname*{sup}_{y^{\\ast}\\in B(\\mathcal{V}^{\\ast})}\\|y^{\\ast}(\\pmb{A}\\pmb{d})\\|_{2},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where we recall that for a vector $z\\,=\\,(z_{i})_{i=1}^{N}\\,\\in\\,\\mathcal{V}^{N}$ , we write $y^{*}(z)\\,=\\,(y^{*}(z_{i}))_{i=1}^{N}\\,\\in\\,\\mathbb{R}^{N}$ . By linearity, we have $y^{*}(A d)=A y^{*}(d)$ . Therefore, by Lemma D.7, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|h\\|_{\\mathrm{dic},\\tilde{\\zeta}}^{2}=\\|A d\\|_{2;y}^{2}}\\\\ &{\\qquad\\qquad\\geq\\underset{y^{*}\\in B(y^{*})}{\\operatorname*{sup}}\\,\\|A y^{*}(d)\\|_{2}^{2}}\\\\ &{\\qquad\\geq\\underset{y^{*}\\in B(y^{*})}{\\operatorname*{sup}}\\,(\\theta_{-}-(1+\\theta_{+})c_{5}\\delta)\\,\\|y^{*}(d)\\|_{2}^{2}}\\\\ &{\\qquad\\geq(c_{3}-(1+c_{4})c_{5}\\delta)\\underset{y^{*}\\in B(y^{*})}{\\operatorname*{sup}}\\,\\|y^{*}(h)\\|_{L_{\\varrho}^{2}(D)}^{2}}\\\\ &{\\qquad=(c_{3}-(1+c_{4})c_{5}\\delta)\\,\\|h\\|_{L_{\\varrho}^{2}(D;y)}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "for some universal constant $c_{5}>0$ , provided $m$ satisfies (D.11). We now set $\\delta=c_{3}/(2(1+c_{4})c_{5})$ to get ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\|h\\|_{\\mathsf{d i s c},\\widetilde{\\mathsf{S}}}^{2}\\geq c_{3}/2\\|h\\|_{L_{\\varrho}^{2}(D;\\mathcal{Y})}^{2},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "provided $m$ satisfies (D.11) with this value of $\\delta$ . However, this is implied by the condition (D.13). We deduce the result. \u53e3 ", "page_idx": 36}, {"type": "text", "text": "D.5 Construction of the DNN family $\\mathcal{N}$ ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Recall that $\\Lambda\\subset{\\mathcal{F}}$ , $|\\Lambda|=N$ is an arbitrary set and $\\boldsymbol{S}$ is defined by (D.1). In this and what follows, we slightly abuse notation and consider a DNN $N:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ as a function $\\mathbb{R}^{\\mathbb{N}}\\to\\mathbb{R}$ which depends on only the first $d$ variables. ", "page_idx": 36}, {"type": "text", "text": "Lemma D.9 (Approximating Legendre polynomials with tanh DNNs). Let $\\Gamma\\subset\\Lambda$ with $\\operatorname{supp}(\\pmb{\\nu})\\subseteq$ $\\{1,\\ldots,d\\}$ , $\\forall\\nu\\in\\Gamma$ , and $m(\\Gamma)=\\operatorname*{max}_{\\pmb{\\nu}\\in\\Gamma}\\|\\pmb{\\nu}\\|_{1}<\\infty$ . There exists a fully-connected family $\\mathcal{N}_{o}$ of tanh DNNs $N:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ with ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathrm{width}({\\mathcal N}_{o})\\lesssim m(\\Gamma),\\quad\\mathrm{depth}({\\mathcal N}_{o})\\lesssim\\log(m(\\Gamma)),\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "such that, for any $0<\\delta<1$ and $\\pmb{\\nu}\\in\\Gamma$ , there is a DNN $N_{\\nu}\\in\\mathcal{N}_{o}$ with ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\lVert N_{\\pmb{\\nu}}-\\Psi_{\\boldsymbol{\\nu}}\\rVert_{L_{\\varrho}^{\\infty}(D)}\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Moreover, the zero network $0:x\\mapsto0$ also belongs to $\\mathcal{N}_{o}$ (trivially, since $\\operatorname{tanh}(0)=0.$ ). ", "page_idx": 36}, {"type": "text", "text": "Proof. We follow the argument of [5, Thm. 7.4], which is based on [79, Prop. 2.6]. Let $\\pmb{\\nu}\\in\\Gamma$ . Then via the fundamental theorem of algebra we can write $\\Psi_{\\nu}(x)$ as a product of $\\|\\pmb{\\nu}\\|_{1}$ numbers as ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\Psi_{\\pmb{\\nu}}(\\pmb{x})=\\prod_{i\\in\\mathrm{supp}(\\pmb{\\nu})}\\prod_{j=1}^{\\nu_{i}}d_{i}(x_{i}-r_{i j}).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Here $\\{r_{i j}\\}_{j=1}^{\\nu_{i}}$ are the roots of the univariate Legendre polynomial $P_{\\nu_{i}}$ . We append $m(\\Gamma)-\\left\\|\\pmb{\\nu}\\right\\|_{1}$ ones and write $\\Psi_{\\nu}(x)$ as a product of exactly $m(\\Gamma)$ numbers. Now define the affine map ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathcal{A}_{\\nu}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{m(\\Gamma)},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "so that ${\\mathcal{A}}_{\\nu}(x)$ is the vector consisting of the values $d_{i}(x_{i}-r_{i j})$ for $j=1,\\dots,\\nu_{i}$ and $i\\in\\mathrm{supp}(\\pmb{\\nu})$ and 1 otherwise. To complete the proof, we need to construct a tanh DNN mapping $\\mathbb{R}^{m(\\Gamma)}\\rightarrow\\mathbb{R}$ that approximately multiplies these numbers. To do this, we argue as in the proof of [5, Thm. 7.4] to see that there is a tanh DNN $N_{\\nu}$ with ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathrm{width}(N_{\\nu})\\lesssim m(\\Gamma),\\quad\\mathrm{depth}(N_{\\nu})\\lesssim\\log(m(\\Gamma))\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "that satisfies the desired bound (D.17). Since these width and depth bounds are independent of $\\pmb{\\nu}$ , we deduce the result. \u53e3 ", "page_idx": 37}, {"type": "text", "text": "Fix $\\delta\\,>\\,0$ , let $\\Gamma=\\cup_{S\\in S}S$ , $d\\,=\\,d_{\\mathcal{X}}$ and consider the corresponding family $\\mathcal{N}_{o}$ and DNNs $N_{\\nu}$ , $\\pmb{\\nu}\\in\\Gamma$ , asserted by this lemma. For any $\\pmb{\\nu}\\in\\Gamma$ , we have $\\pmb{\\nu}\\in S$ for some $S\\in S$ , and any such $S$ satisfies $|S|_{v}\\leq k$ . Therefore u2\u03bd(5+\u03be)= v\u03bd2 \u2264k for any \u03bd \u2208S and we deduce that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\|\\pmb{\\nu}\\|_{1}\\leq\\prod_{j=1}^{d}(2\\nu_{j}+1)=u_{\\nu}^{2}\\leq k^{1/(5+\\xi)}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "This implies that $m(\\Gamma)\\leq k^{1/(5+\\xi)}$ and therefore ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathrm{width}({\\mathcal N}_{o})\\lesssim k^{1/(5+\\xi)},\\quad\\mathrm{depth}({\\mathcal N}_{o})\\lesssim\\log(k).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "With this in hand, we now define the family $\\mathcal{N}$ of DNNs $N:\\mathbb{R}^{d_{\\boldsymbol{x}}}\\rightarrow\\mathbb{R}^{d_{\\boldsymbol{y}}}$ by ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{N}=\\left\\{N=C\\left[\\begin{array}{c}{N_{\\nu_{1}}}\\\\ {\\vdots}\\\\ {N_{\\nu_{|S|}}}\\\\ {0}\\\\ {\\vdots}\\\\ {0}\\end{array}\\right]:S=\\{\\nu_{1},\\dots,\\nu_{|S|}\\}\\in S,\\,C\\in\\mathbb{R}^{d_{\\mathcal{V}}\\times\\lfloor k\\rfloor}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Here we also use the fact that $|S|\\leq|S|_{v}\\leq k$ . Notice that this family satisfies ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathrm{width}({\\mathcal N})\\leq k^{1+1/(5+\\xi)},\\quad\\mathrm{depth}(N)\\lesssim\\log(k),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "due to the bounds for $\\mathcal{N}_{o}$ . ", "page_idx": 37}, {"type": "text", "text": "Now let $N\\in\\mathcal N$ and write $C=[c_{1}|\\cdot\\cdot\\cdot|c_{\\lfloor k\\rfloor}]$ , where $c_{i}\\in\\mathbb{R}^{d_{\\mathcal{D}}}$ . Then ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathcal{D}_{\\mathcal{Y}}\\circ N=\\sum_{i=1}^{|S|}\\mathcal{D}_{\\mathcal{Y}}(c_{i})N_{\\nu_{i}}=\\sum_{i=1}^{|S|}c_{i}N_{\\nu_{i}},\\quad\\mathrm{where~}c_{i}\\in\\widetilde{\\mathcal{Y}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Therefore, we can associate $\\mathcal{N}$ with the space of functions $\\Tilde{P}_{s;\\tilde{\\mathcal{D}}}$ , where, for any arbitrary Banach space $(\\mathcal{Z},\\|\\cdot\\|_{\\mathcal{Z}})$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{P}}_{S;\\mathcal{Z}}=\\left\\{\\sum_{\\nu\\in S}c_{\\nu}N_{\\nu}:c_{\\nu}\\in\\mathcal{Z},\\ S\\in\\mathcal{S}\\right\\}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We now require the following lemma, which relates the distance between a function in $\\widetilde{\\mathcal{P}}_{S;\\mathcal{Z}}$ and the corresponding polynomial in $\\mathcal{P}_{S;\\mathcal{Z}}$ . ", "page_idx": 37}, {"type": "text", "text": "Lemma D.10 (Discrete norms of polynomials and their approximating DNNs). Let $S\\ \\subset\\ {\\mathcal{F}}$ , $(\\mathcal{Z},\\|\\cdot\\|_{\\mathcal{Z}})$ be any Banach space and suppose that $\\begin{array}{r}{p=\\sum_{\\nu\\in S}c_{\\nu}\\Psi_{\\nu},}\\end{array}$ , where $c_{\\nu}\\in\\mathcal{Z}$ . Define ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\tilde{p}=\\sum_{\\nu\\in S}c_{\\nu}N_{\\nu}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Then ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|p-\\tilde{p}\\|_{\\mathrm{disc},\\tilde{\\varsigma}}\\leq\\|p-\\tilde{p}\\|_{L_{\\varrho}^{\\infty}(D;\\mathcal{Z})}\\leq\\delta\\sqrt{|\\cal S|}\\|p\\|_{L_{\\varrho}^{2}(D;\\mathcal{Z})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Moreover, if $\\mathcal{Z}=\\widetilde{\\mathcal{V}}$ , $S\\in S$ and (D.7) holds with $\\alpha$ satisfying $\\delta\\sqrt{|S|_{\\pmb{u}}}/\\alpha<1$ then ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\|p\\|_{L_{\\varrho}^{2}(D;\\mathcal{Y})}\\leq\\frac{1}{\\alpha-\\delta\\sqrt{|\\mathcal{S}|}}\\|\\tilde{p}\\|_{\\mathrm{disc},\\tilde{\\varsigma}}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. By (A.III) and the definition of the $N_{\\nu}$ we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|p-\\tilde{p}\\|_{\\mathrm{disc},\\tilde{\\varsigma}}\\leq\\|p-\\tilde{p}\\|_{L_{\\varrho}^{\\infty}(D;\\mathcal{L})}}\\\\ &{\\qquad\\qquad\\qquad=\\underset{z^{*}\\in B(\\mathcal{Z}^{*})}{\\operatorname*{sup}}\\,\\|z^{*}(p-\\tilde{p})\\|_{L_{\\varrho}^{\\infty}(D)}}\\\\ &{\\qquad\\qquad\\leq\\underset{z^{*}\\in B(\\mathcal{Z}^{*})}{\\operatorname*{sup}}\\sum_{\\nu\\in S}|z^{*}(c_{\\nu})|\\|\\Psi_{\\nu}-N_{\\nu}\\|_{L_{\\varrho}^{\\infty}(D)}}\\\\ &{\\leq\\delta\\|\\mathfrak{c}\\|_{1;\\mathcal{L}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We now apply the Cauchy\u2013Schwarz inequality and Parseval\u2019s identity to obtain ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|p-\\tilde{p}\\right\\|_{\\mathrm{disc},\\tilde{\\varsigma}}\\leq\\delta\\sqrt{|\\boldsymbol{S}|}\\|\\boldsymbol{c}\\|_{2;\\mathcal{Z}}=\\delta\\sqrt{|\\boldsymbol{S}|_{u}}\\|p\\|_{L_{\\varrho}^{2}(D;\\mathcal{Z})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which gives the first result. ", "page_idx": 38}, {"type": "text", "text": "For the second result, we apply (D.7) with $q=0$ to get ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|p\\|_{L_{\\varrho}^{2}(D;y)}\\leq\\alpha^{-1}\\|p\\|_{\\mathrm{disc},\\tilde{\\varsigma}}\\leq\\alpha^{-1}\\left(\\|p-\\tilde{p}\\|_{\\mathrm{disc},\\tilde{\\varsigma}}+\\|\\tilde{p}\\|_{\\mathrm{disc},\\tilde{\\varsigma}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Using (D.21) and the fact that $\\delta\\sqrt{|S|}/\\alpha<1$ now gives the result. ", "page_idx": 38}, {"type": "text", "text": "Remark D.11 (Other activation functions) As seen in this section, a key step in our proofs is emulating the polynomials via DNNs of quantifiable width and depth. There is an extensive literature on this topic. See, e.g., [5, 21, 23, 25, 38, 57, 63, 67, 72, 78, 79, 85, 86, 92, 97] and references therein. The proof of Lemma D.9 reduces this to the task of emulating the multiplication operation $(x_{1},\\ldots,x_{d})\\ {\\stackrel{\\cdot}{\\in}}\\ \\mathbb{R}^{d}\\mapsto x_{1}\\cdot\\cdot\\cdot x_{d}\\in\\mathbb{R}$ via a DNN. As shown in the proof of [5, Thm. 7.4] (which is based on [79, Prop. 2.6]), this can in turn be achieved using a binary tree of $\\lceil\\log_{2}(d)\\rceil$ DNNs that approximately compute the multiplication of two numbers $({\\bar{x}},y)\\in\\mathbb{R}^{2}\\mapsto x y\\in\\mathbb{R}$ . Further, this task can be achieved via the identity $\\bar{x y}=((x+y)^{2}-(x-y)^{2})/4$ by using a DNN that approximately computes the squaring function $x\\in\\mathbb{R}\\mapsto x^{2}\\in\\mathbb{R}$ . To summarize, provide a DNN of quantifiable width and depth can approximately compute the squaring function, it can also approximately emulate the multivariate Legendre polynomials. ", "page_idx": 38}, {"type": "text", "text": "In view of this, we can adapt our main theorems to various other activation functions without change. This includes Rectified Polynomial Units (RePUs), where the emulation is, in fact, exact (see, e.g., [57, Lem. 2.1]). It also includes the Exponential Linear Unit (ELU) used in our numerical experiments and many others. See, for instance, Proposition 4.7 of [38] and the ensuing discussion. Rectified Linear Units (ReLUs) are slightly different, as in this case the depth of the DNN that performs the approximation multiplication depends on the desired accuracy (see, e.g., [79, Prop. 2.6]). One could modify Theorem 3.1 to consider ReLU DNNs, with the result being a worse depth bound than that presented for tanh DNNs. ", "page_idx": 38}, {"type": "text", "text": "D.6 Analysis of (2.5) ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Lemma D.12 (Approximate minimizers of (2.5) yield approximate minimizers of (D.3)). Suppose that (D.7) holds, let N be the family of DNNs defined in $\\S D.5$ and $\\widehat{N}$ be any $(\\sigma,\\tau)$ -approximate minimizer of (2.5). Let ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathcal{D}_{\\mathcal{Y}}\\circ\\widehat{N}=\\sum_{\\nu\\in\\mathcal{S}}\\hat{c}_{\\nu}N_{\\nu}\\in\\widetilde{\\mathcal{P}}_{\\mathcal{S};\\widetilde{\\mathcal{Y}}},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $S\\in S$ and $\\hat{c}_{\\nu}\\in\\widetilde{\\mathcal{N}}$ , and define ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\hat{p}=\\sum_{\\nu\\in S}\\hat{c}_{\\nu}\\Psi_{\\nu}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Then $\\hat{p}$ is a $(\\sigma^{\\prime},\\tau^{\\prime})$ -approximate minimizer of (D.3), where ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sigma^{\\prime}\\leq\\sigma(1+\\delta\\sqrt{k}/\\alpha)}\\\\ {\\displaystyle\\tau^{\\prime}\\leq\\tau+\\sigma\\delta\\sqrt{k}/\\alpha\\left(\\|F\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\frac{1}{\\sqrt{m}}\\|\\boldsymbol{E}\\|_{2;\\mathcal{Y}}\\right)+\\delta\\sqrt{k}\\|\\hat{p}\\|_{L_{\\varrho}^{2}(D;\\mathcal{Y})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Proof. Let $\\begin{array}{r}{p=\\sum_{\\nu\\in S}c_{\\nu}\\Psi_{\\nu}\\in P_{S;\\tilde{\\mathcal{V}}}}\\end{array}$ be arbitrary and $N\\in\\mathcal N$ be the corresponding DNN so that $\\begin{array}{r}{\\mathcal{D}_{\\mathcal{V}}\\circ N=\\sum_{\\nu\\in S}c_{\\nu}N_{\\nu}}\\end{array}$ . Then by the triangle inequality and the fact that $\\widehat{N}$ is an approximate minimizer,  we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{\\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\|Y_{i}-\\hat{p}\\circ\\mathcal{E}_{X}(X_{i})\\|_{y}^{2}}}\\\\ &{\\leq\\sqrt{\\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\|Y_{i}-\\mathcal{D}_{y}\\circ\\hat{K}_{}(X_{i})\\|_{y}^{2}}+\\|\\hat{p}-\\mathcal{D}_{y}\\circ\\hat{N}\\|_{\\mathrm{duc},\\hat{\\mathcal{X}}}}\\\\ &{\\leq\\sigma\\sqrt{\\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\|Y_{i}-\\mathcal{D}_{y}\\circ N\\circ\\mathcal{E}_{X}(X_{i})\\|_{y}^{2}}+\\tau+\\|\\hat{p}-\\mathcal{D}_{y}\\circ\\hat{N}\\|_{\\mathrm{duc},\\hat{\\mathcal{X}}}}\\\\ &{\\leq\\sigma\\sqrt{\\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\|Y_{i}-p\\circ\\mathcal{E}_{X}(X_{i})\\|_{y}^{2}}+\\tau+\\sigma\\|p-\\mathcal{D}_{y}\\circ N\\|_{\\mathrm{duc},\\hat{\\mathcal{X}}}+\\|\\hat{p}-\\mathcal{D}_{y}\\circ\\hat{N}\\|_{\\mathrm{duc},\\hat{\\mathcal{X}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We now apply Lemma D.10 to the last two terms, noting that $|S|\\leq|S|_{v}\\leq k$ since $S\\in S$ , to obtain ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sqrt{\\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\|Y_{i}-\\hat{p}\\circ\\mathcal E_{\\mathcal X}(X_{i})\\|_{\\mathcal Y}^{2}}\\leq\\sigma\\sqrt{\\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\|Y_{i}-p\\circ\\mathcal E_{\\mathcal X}(X_{i})\\|_{\\mathcal Y}^{2}}}\\\\ {+\\,\\tau+\\sigma\\delta\\sqrt{k}\\|p\\|_{L_{\\varrho}^{2}(D;\\mathcal V)}+\\delta\\sqrt{k}\\|\\hat{p}\\|_{L_{\\varrho}^{2}(D;\\mathcal V)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Consider the third term. We first apply (D.7) with $q=0$ to get ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|p\\|_{L_{\\varrho}^{2}(D;y)}\\leq\\alpha^{-1}\\|p\\|_{{\\sf d i s c},\\tilde{\\varsigma}}=\\alpha^{-1}\\|p\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{{\\sf d i s c},\\mu}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We then use the triangle inequality to get ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\||p\\circ\\mathcal{E}_{\\mathcal{X}}\\right\\|_{\\mathrm{disc},\\mu}\\leq\\|\\boldsymbol{F}-p\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{\\mathrm{disc},\\mu}+\\|\\boldsymbol{F}\\|_{\\mathrm{disc},\\mu}\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ {\\leq\\sqrt{\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\|\\boldsymbol{Y}_{i}-p\\circ\\mathcal{E}_{\\mathcal{X}}(\\boldsymbol{X}_{i})\\|_{\\mathcal{Y}}^{2}+\\|\\boldsymbol{F}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\displaystyle\\frac{1}{\\sqrt{m}}\\|\\boldsymbol{E}\\|_{2;\\mathcal{Y}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Therefore, we obtain ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sqrt{\\displaystyle\\cfrac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\|Y_{i}-\\hat{p}\\circ\\mathcal E_{X}(X_{i})\\|_{\\mathcal P}^{2}}\\leq\\sigma(1+\\delta\\sqrt{k}/\\alpha)\\sqrt{\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\|Y_{i}-p\\circ\\mathcal E_{X}(X_{i})\\|_{\\mathcal P}^{2}}+\\tau}\\\\ {+\\sigma\\delta\\sqrt{k}/\\alpha\\left(\\|F\\|_{L_{\\mu}^{\\infty}(X;\\mathcal P)}+\\displaystyle\\frac{1}{\\sqrt{m}}\\|E\\|_{2;\\mathcal P}\\right)+\\delta\\sqrt{k}\\|\\hat{p}\\|_{L_{\\varrho}^{2}(D;\\mathcal P)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Since $p\\in P_{\\mathcal{S};\\widetilde{\\mathcal{D}}}$ was arbitrary we get the result. ", "page_idx": 39}, {"type": "text", "text": "Theorem D.13 (Error bound for DNN minimizers). Let $\\mathcal{Q}:\\mathcal{V}\\to\\widetilde{\\mathcal{V}}=\\mathcal{D}_{\\mathcal{V}}(\\mathbb{R}^{d_{\\mathcal{V}}})$ be\u221a a bounded linear operator, $\\pi_{\\mathcal{Q}}=||\\mathcal{Q}||_{\\mathcal{V}\\to\\mathcal{V}}$ and suppose that (D.7) holds with $\\alpha\\geq c_{0}$ and $\\alpha-\\delta\\sqrt{k}\\geq c_{1}$ for suitable universal constants $c_{0},c_{1}>0.$ . Let $\\mathcal{N}$ be the family of DNNs defined in $\\S D.5$ and $\\widehat{N}$ be any $(\\sigma,\\tau)$ -approximate minimizer of (2.5). Then the approximation $\\widehat{F}=\\mathcal{D}_{\\mathcal{Y}}\\circ\\widehat{N}\\circ\\mathcal{E}_{\\mathcal{X}}$ satisfies ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\||F-\\widehat{F}|\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\lesssim\\||F-\\mathcal{Q}\\circ F|\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}+\\sigma\\||F-\\mathcal{Q}\\circ F\\|_{\\mathrm{disc},\\mu}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\pi_{\\mathcal{Q}}\\left(\\|F-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}+\\sigma\\|F-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{\\mathrm{disc},\\mu}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\tau+\\sigma\\delta\\sqrt{k}\\|F\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\frac{\\sigma}{\\sqrt{m}}\\|E\\|_{2;\\mathcal{Y}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|F-\\widehat{F}\\|_{L_{\\mu}^{\\infty}(\\varLambda;y)}\\lesssim\\|F-Q\\circ F\\|_{L_{\\mu}^{\\infty}(\\varLambda;y)}+\\sqrt{k}\\sigma\\|F-Q\\circ F\\|_{\\mathrm{disc},\\mu}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left.\\pi_{Q}\\left(\\|F-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{L_{\\mu}^{\\infty}(\\varLambda;y)}+\\sqrt{k}\\sigma\\|F-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{\\mathrm{disc},\\mu}\\right)\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.+\\left.\\sqrt{k}\\tau+\\sigma\\delta k\\|F\\right\\|_{L_{\\mu}^{\\infty}(\\varLambda;y)}+\\frac{\\sqrt{k}\\sigma}{\\sqrt{m}}\\|E\\|_{2;y}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "for all $q\\in\\mathcal{P}_{S;\\mathcal{D}}$ . ", "page_idx": 40}, {"type": "text", "text": "Proof. Let $\\hat{p}$ be as in Lemma D.12. Then ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\||F-\\widehat{F}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\leq\\||F-\\widehat{p}\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}+\\||\\widehat{p}\\circ\\mathcal{E}_{\\mathcal{X}}-\\mathcal{D}_{\\mathcal{Y}}\\circ\\widehat{N}\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}}\\\\ &{\\|F-\\widehat{F}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}\\leq\\|F-\\widehat{p}\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\|\\widehat{p}\\circ\\mathcal{E}_{\\mathcal{X}}-\\mathcal{D}_{\\mathcal{Y}}\\circ\\widehat{N}\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "For the second term, we have, by (A.III) and Lemma D.10, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\hat{p}\\circ\\mathcal{E}_{\\mathcal{X}}-\\mathcal{D}_{\\mathcal{Y}}\\circ\\hat{N}\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\leq\\|\\hat{p}\\circ\\mathcal{E}_{\\mathcal{X}}-\\mathcal{D}_{\\mathcal{Y}}\\circ\\hat{N}\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\|\\hat{p}-\\mathcal{D}_{\\mathcal{Y}}\\circ\\hat{N}\\|_{L_{\\hat{\\mathcal{E}}}^{\\infty}(D;\\mathcal{Y})}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\|\\hat{p}-\\mathcal{D}_{\\mathcal{Y}}\\circ\\hat{N}\\|_{L_{\\varrho}^{\\infty}(D;\\mathcal{Y})}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\delta\\sqrt{k}\\|\\hat{p}\\|_{L_{\\varrho}^{2}(D;\\mathcal{Y})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We now apply this, Lemma D.12, Theorem D.6 and the facts that $\\alpha^{-1}\\lesssim1$ and $\\sigma^{\\prime}\\geq1$ to obtain ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\boldsymbol{F}-\\widehat{\\boldsymbol{F}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\lesssim\\|\\boldsymbol{F}-\\mathcal{Q}\\circ\\boldsymbol{F}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}+\\sigma^{\\prime}\\|\\boldsymbol{F}-\\mathcal{Q}\\circ\\boldsymbol{F}\\|_{\\mathrm{disc},\\mu}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\pi_{Q}\\left(\\|\\boldsymbol{F}-\\boldsymbol{q}\\circ\\mathcal{E}_{\\boldsymbol{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}+\\sigma^{\\prime}\\|\\boldsymbol{F}-\\boldsymbol{q}\\circ\\mathcal{E}_{\\boldsymbol{X}}\\|_{\\mathrm{disc},\\mu}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\tau^{\\prime}+\\frac{\\sigma^{\\prime}}{\\sqrt{m}}\\|\\boldsymbol{E}\\|_{2;\\mathcal{Y}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|F-\\widehat{F}\\|_{L_{\\mu}^{\\infty}(\\varLambda;y)}\\lesssim\\|F-Q\\circ F\\|_{L_{\\mu}^{\\infty}(\\varLambda;y)}+\\sqrt{k}\\sigma^{\\prime}\\|F-Q\\circ F\\|_{\\mathrm{disc},\\mu}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left.\\pi_{Q}\\left(\\|F-q\\circ\\mathcal{E}_{\\varLambda}\\|_{L_{\\mu}^{\\infty}(\\varLambda;y)}+\\sqrt{k}\\sigma^{\\prime}\\|F-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{\\mathrm{disc},\\mu}\\right)\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.+\\left.\\sqrt{k}\\tau^{\\prime}+\\frac{\\sqrt{k}\\sigma^{\\prime}}{\\sqrt{m}}\\|E\\right\\|_{2;y}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "for any $q\\in\\mathcal{P}_{\\mathcal{S};\\mathcal{X}}$ , where $\\sigma^{\\prime}$ and $\\tau^{\\prime}$ are as in (D.22). Due to the various assumptions, these satisfy ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sigma^{\\prime}\\lesssim\\sigma,\\quad\\tau^{\\prime}\\lesssim\\tau+\\sigma\\delta\\sqrt{k}\\|F\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\frac{\\sigma}{\\sqrt{m}}\\|E\\|_{2;\\mathcal{Y}}+\\delta\\sqrt{k}\\|\\widehat{p}\\|_{L_{\\varrho}^{2}(D;\\mathcal{Y})}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We deduce that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\|F-\\widehat{F}\\|_{L_{\\mu}^{2}(x;y)}\\lesssim\\|F-{Q}\\circ F\\|_{L_{\\mu}^{2}(x;y)}+\\sigma\\|F-{Q}\\circ F\\|_{{\\mathrm{disc}},\\mu}}&{}\\\\ {\\displaystyle\\quad+\\,\\pi{Q}\\left(\\|F-q\\circ\\mathcal{E}\\chi\\|_{L_{\\mu}^{2}(\\mathcal{X};y)}+\\sigma\\|F-q\\circ\\mathcal{E}\\chi\\|_{{\\mathrm{disc}},\\mu}\\right)}&{}\\\\ {\\displaystyle\\quad+\\,\\tau+\\sigma\\delta\\sqrt{k}\\|F\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};y)}+\\frac{\\sigma}{\\sqrt{m}}\\|E\\|_{2;y}+\\delta\\sqrt{k}\\||\\widehat{p}\\|_{L_{\\varrho}^{2}(D;\\mathcal{Y})}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|F-\\widehat{F}\\|_{L_{\\mu}^{\\infty}(X;\\mathcal{Y})}\\lesssim\\|F-\\mathcal{Q}\\circ F\\|_{L_{\\mu}^{\\infty}(X;\\mathcal{Y})}+\\sqrt{k}\\sigma\\|F-\\mathcal{Q}\\circ F\\|_{\\mathrm{disc},\\mu}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\pi_{\\mathcal{Q}}\\left(\\|F-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\sqrt{k}\\sigma\\|F-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{\\mathrm{disc},\\mu}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\sqrt{k}\\tau+\\sigma\\delta k\\|F\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\frac{\\sqrt{k}\\sigma}{\\sqrt{m}}\\|E\\|_{2;\\mathcal{Y}}+\\delta k\\|\\widehat{p}\\|_{L_{\\varrho}^{2}(D;\\mathcal{Y})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We no\u221aw bound the final term. Using Lemma D.10 once more, in combination with the fact that $\\alpha-\\delta\\sqrt{k}\\gtrsim1$ , we see that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\|\\hat{p}\\|_{L_{\\sigma}^{2}(D;y)}\\leq\\frac{1}{\\alpha-\\delta\\sqrt{k}}\\|\\mathcal{D}_{y}\\circ\\hat{N}\\|_{\\mathrm{dis},\\xi}\\lesssim\\|\\mathcal{D}_{y}\\circ\\hat{N}\\|_{\\mathrm{dis},\\xi}.}&{}\\\\ {\\displaystyle}&{\\leq\\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}\\|Y_{i}-\\mathcal{D}_{y}\\circ\\hat{N}\\circ\\mathcal{E}_{x}(X_{i})\\|_{y}^{2}}+\\frac{1}{\\sqrt{m}}\\|Y\\|_{2,y}}\\\\ {\\displaystyle}&{\\leq\\sigma\\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}\\|Y_{i}-0\\|_{y}^{2}}+\\tau+\\frac{1}{\\sqrt{m}}\\|Y\\|_{2,y}}\\\\ {\\displaystyle}&{\\leq(1+\\sigma)\\left(\\|F\\|_{L_{\\sigma}^{\\infty}(X;y)}+\\frac{1}{\\sqrt{m}}\\|E\\|_{2,y}\\right)+\\tau.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Here, we also used the fact that $\\widehat{N}$ is an approximate minimizer in the fourth step, as well as the facts that the zero network $0\\in\\mathcal{N}$ an d that $\\mathcal{D}_{\\mathcal{Y}}$ is a linear map. Plugging this into the previous expressions now gives the result. \u53e3 ", "page_idx": 41}, {"type": "text", "text": "D.7 Bounding the best polynomial approximation error terms ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "When $\\sigma=1$ (as will be the case when we come to prove Theorem 3.1), the error bounds in Theorem D.13 involve best polynomial approximation error terms of the form ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|F-q\\circ\\mathcal{E}_{\\boldsymbol{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}+\\|F-q\\circ\\mathcal{E}_{\\boldsymbol{X}}\\|_{\\mathrm{disc},\\mu},\\quad\\|F-q\\circ\\mathcal{E}_{\\boldsymbol{X}}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\sqrt{k}\\|F-q\\circ\\mathcal{E}_{\\boldsymbol{X}}\\|_{\\mathrm{disc},\\mu}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "for arbitrary $q\\in\\mathcal{P}_{S;\\mathcal{D}}$ . By the triangle inequality, these are bounded by ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{2}(F,q):=\\|F-q\\circ\\iota\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}+\\|F-q\\circ\\iota\\|_{{\\bf d i s c},\\mu}}\\\\ &{\\qquad\\qquad\\qquad+\\,\\|q\\circ\\iota-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}+\\|q\\circ\\iota-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{{\\bf d i s c},\\mu},}\\\\ &{E_{\\infty}(F,q):=\\|F-q\\circ\\iota\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\sqrt{k}\\|F-q\\circ\\iota\\|_{{\\bf d i s c},\\mu}}\\\\ &{\\qquad\\qquad\\qquad+\\,\\|q\\circ\\iota-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\sqrt{k}\\|q\\circ\\iota-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{{\\bf d i s c},\\mu}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "In this section, we construct a suitable polynomial $q$ in the case where (A.II) holds and thereby derive a bound for these term. We first require the following lemma. ", "page_idx": 41}, {"type": "text", "text": "Lemma D.14. Let $G\\in L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})$ and $0<\\epsilon<1$ . Then the following hold. ", "page_idx": 41}, {"type": "text", "text": "(a) With probability at least $1-\\epsilon$ on the draw of the $X_{i}$ , we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\|G\\|_{\\mathsf{d i s c},\\mu}\\leq\\|G\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}/\\sqrt{\\epsilon}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "(b) Suppose that $m\\geq2r\\log(2/\\epsilon)$ for some $r>0$ . Then, with probability at least $1-\\epsilon$ on the draw of the $X_{i}$ , we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Vert|G|\\Vert_{\\mathrm{disc},\\mu}\\leq\\sqrt{2}\\left(\\Vert G\\Vert_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}/\\sqrt{r}+\\Vert G\\Vert_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof. Observe that the random variable $\\|G\\|_{\\mathsf{d i s c},\\mu}^{2}$ satisfies ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Vert G\\Vert_{{\\mathsf{d i s c}},\\mu}^{2}=\\Vert G\\Vert_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}^{2}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "For (a), we use Markov\u2019s inequality to get ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left\\|\\boldsymbol{G}\\right\\|_{\\mathsf{d i s c},\\mu}\\geq\\left\\|\\boldsymbol{G}\\right\\|_{L_{\\mu}^{2}(\\boldsymbol{\\mathcal{X}};\\mathcal{Y})}/\\sqrt{\\epsilon}\\right)\\leq\\frac{\\mathbb{E}\\left\\|\\boldsymbol{G}\\right\\|_{\\mathsf{d i s c},\\mu}^{2}}{\\left\\|\\boldsymbol{G}\\right\\|_{L_{\\mu}^{2}(\\boldsymbol{\\mathcal{X}};\\mathcal{Y})}^{2}/\\epsilon}=\\epsilon,\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "as required. ", "page_idx": 42}, {"type": "text", "text": "The proof of (b) is based on [3, Lem. 7.11]. We repeat it here for convenience. Define the random variable $Z_{i}=\\|G(X_{i})\\|_{\\mathcal{V}}^{2}$ and observe that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}(Z_{i})=\\mathbb{E}_{X\\sim\\mu}\\|G(X)\\|_{\\mathcal{Y}}^{2}=\\|G\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}^{2}=:a.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Let $X_{i}=Z_{i}-\\mathbb{E}(Z_{i})$ so that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\|G\\|_{{\\mathsf{d i s c}},\\mu}^{2}={\\frac{1}{m}}\\sum_{i=1}^{m}Z_{i}={\\frac{1}{m}}\\sum_{i=1}^{m}X_{i}+a.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Now let $b=\\left\\|G\\right\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}^{2}$ and observe that ", "page_idx": 42}, {"type": "equation", "text": "$$\nX_{i}\\leq Z_{i}\\leq b,\\quad-X_{i}\\leq\\mathbb{E}(Z_{i})\\leq b,\\quad{\\mathrm{a.s.}}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "We also have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{m}\\mathbb{E}(X_{i}^{2})\\leq\\sum_{i=1}^{m}\\mathbb{E}(Z_{i}^{2})\\leq b\\sum_{i=1}^{m}\\mathbb{E}(Z_{i})=a b m.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Therefore, Bernstein\u2019s inequality for bounded random variables (see, e.g., [29, Cor. 7.31]) implies that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\frac{1}{m}\\sum_{i=1}^{m}X_{i}\\right|\\geq t\\right)\\leq2\\exp\\left(-\\frac{t^{2}m/2}{a b+b t/3}\\right)\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "$t>0$ $t=a+b/r$ $\\begin{array}{r}{\\frac{t^{2}m/2}{a b+b t/3}\\geq\\frac{3m}{5r}\\geq\\log(2/\\epsilon)}\\end{array}$ ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left|{\\frac{1}{m}}\\sum_{i=1}^{m}X_{i}\\right|<a+b/r\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "with probability at least $1-\\epsilon$ . It follows that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|G\\|_{\\mathrm{disc},\\mu}\\leq\\sqrt{2a+b/r}\\leq\\sqrt{2}\\left(\\sqrt{a}+\\sqrt{b/r}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "with the same probability. Substituting the values for $a$ and $b$ now gives the result. ", "page_idx": 42}, {"type": "text", "text": "Lemma D.15. Let $F\\in L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y}),$ , $q\\in\\mathcal{P}_{S;\\mathcal{D}}$ be arbitrary and $m\\geq2r\\log(6/\\epsilon)$ for some $r>0$ and $0<\\epsilon<1$ . Then ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{2}(F,q)\\lesssim\\|F-q\\circ\\iota\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}/\\sqrt{r}+\\|F-q\\circ\\iota\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}}\\\\ &{\\qquad\\qquad\\qquad+\\,\\sqrt{k/\\epsilon}\\|q\\|_{L_{\\sigma}^{2}(D;\\mathcal{Y})}\\|\\iota_{d_{\\mathcal{X}}}-\\iota_{d_{\\mathcal{X}}}\\circ\\widetilde{\\mathcal{D}}_{\\mathcal{X}}\\circ\\widetilde{\\mathcal{E}}_{\\mathcal{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{\\ell}^{\\infty}(\\mathbb{R}^{d_{\\mathcal{X}}}))}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{\\infty}(F,q)\\lesssim\\!(1+\\sqrt{k/r})\\|F-q\\circ\\iota\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\sqrt{k}\\|F-q\\circ\\iota\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}}\\\\ &{\\qquad\\qquad\\qquad+\\,k\\|q\\|_{L_{\\varrho}^{2}(D;\\mathcal{Y})}\\|\\iota_{d_{\\mathcal{X}}}-\\iota_{d_{\\mathcal{X}}}\\circ\\widetilde{\\mathcal{D}}_{\\mathcal{X}}\\circ\\widetilde{\\mathcal{E}}_{\\mathcal{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{\\ell}^{\\infty}(\\mathbb{R}^{d}\\mathcal{X}))}}\\\\ &{\\qquad\\qquad+\\,\\sqrt{k}(1+\\sqrt{k/r})\\|q\\|_{L_{\\varrho}^{2}(D;\\mathcal{Y})}\\|\\iota_{d_{\\mathcal{X}}}-\\iota_{d_{\\mathcal{X}}}\\circ\\widetilde{\\mathcal{D}}_{\\mathcal{X}}\\circ\\widetilde{\\mathcal{E}}_{\\mathcal{X}}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{\\ell}^{\\infty}(\\mathbb{R}^{d}\\mathcal{X}))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "with probability at least $1-\\epsilon$ . ", "page_idx": 42}, {"type": "text", "text": "Proof. We apply part (b) of Lemma D.14 with $\\epsilon$ replaced by $\\epsilon/3$ to the term $\\|\\boldsymbol{F}-\\boldsymbol{q}\\circ\\iota\\|_{\\mathsf{d i s c},\\mu}$ and parts (a) and (b) of Lemma D.14 with $\\epsilon$ replaced by $\\epsilon/3$ to the term $\\|q\\circ\\iota-q\\circ\\mathcal{E}_{\\mathcal{X}}\\||_{\\mathsf{d i s c},\\mu}$ . Using the union bound, this gives that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\|{\\cal F}-q\\circ\\iota\\|_{\\mathrm{disc},\\mu}\\lesssim\\|{\\cal F}-q\\circ\\iota\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}+\\|{\\cal F}-q\\circ\\iota\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}/\\sqrt{r}}&\\\\ {\\displaystyle\\|q\\circ\\iota-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{\\mathrm{disc},\\mu}\\lesssim\\|q\\circ\\iota-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}/\\sqrt{\\epsilon}}&\\\\ {\\displaystyle\\|q\\circ\\iota-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{\\mathrm{disc},\\mu}\\lesssim\\|q\\circ\\iota-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}/\\sqrt{r}+\\|q\\circ\\iota-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}}&\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "with probability at least $1-\\epsilon$ . This yields ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{2}(F,q)\\lesssim\\|F-q\\circ\\iota\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}+\\|F-q\\circ\\iota\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}/\\sqrt{r}}\\\\ &{\\qquad\\qquad\\qquad+\\,\\|q\\circ\\iota-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}/\\sqrt{\\epsilon}}\\\\ &{E_{\\infty}(F,q)\\lesssim(1+\\sqrt{k/r})\\|F-q\\circ\\iota\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\sqrt{k}\\|F-q\\circ\\iota\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}}\\\\ &{\\qquad\\qquad\\qquad+\\,(1+\\sqrt{k/r})\\|q\\circ\\iota-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\sqrt{k}\\|q\\circ\\iota-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "with the same probability. It remains to bound the terms involving $q\\circ\\iota-q\\circ\\mathcal{E}_{\\mathcal{X}}$ . Using (A.III), Lemma D.2 and the fact that $q$ depends on its first $d_{\\mathcal{X}}$ variables only, we see that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\|q\\circ\\iota(X)-q\\circ\\mathcal{E}_{X}(X)\\|_{\\mathcal{Y}}\\leq\\frac{1}{2}\\sqrt{k}\\|q\\|_{L_{\\varrho}^{2}(D;\\mathcal{Y})}\\|\\iota_{d_{\\mathcal{X}}}(X)-\\mathcal{E}_{\\mathcal{X}}(X)\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Therefore ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\big\\|q\\circ\\iota-q\\circ\\mathcal{E}_{\\mathcal{X}}\\big\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\lesssim\\sqrt{k}\\big\\|q\\big\\|_{L_{\\varrho}^{2}(D;\\mathcal{Y})}\\big\\|\\iota_{d_{\\mathcal{X}}}-\\mathcal{E}_{\\mathcal{X}}\\big\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{C}^{\\infty}(\\mathbb{N}))}}\\\\ {\\big\\|q\\circ\\iota-q\\circ\\mathcal{E}_{\\mathcal{X}}\\big\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}\\lesssim\\sqrt{k}\\big\\|q\\big\\|_{L_{\\varrho}^{2}(D;\\mathcal{Y})}\\big\\|\\iota_{d_{\\mathcal{X}}}-\\mathcal{E}_{\\mathcal{X}}\\big\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{C}^{\\infty}(\\mathbb{N}))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Substituting this into the previous bounds and using the definition of $\\mathcal{E}_{\\mathcal{X}}$ from (A.III) now gives the result. \u53e3 ", "page_idx": 43}, {"type": "text", "text": "We are now ready to choose the polynomial $q$ . First, we require the following technical lemma, which shows the existence of multi-index sets of weighted cardinality $k$ which achieve the desired algebraic rates of convergence. ", "page_idx": 43}, {"type": "text", "text": "Lemma D.16. Let $\\begin{array}{r}{f=\\sum_{\\nu\\in\\mathcal{F}}c_{\\nu}\\Psi_{\\nu}}\\end{array}$ satisfy $f\\in{\\mathcal{H}}(b)$ for some $\\pmb{b}\\in\\ell^{p}(\\mathbb{N})$ with $\\mathbf{\\delta}b\\geq\\mathbf{0}$ . Then there are index sets $S_{1},S_{2}\\subset{\\mathcal{F}}$ with $|S_{1}|_{\\pmb{v}},|S_{2}|_{\\pmb{v}}\\leq k$ such that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\|c-c_{S_{1}}\\|_{2;y}\\leq C(b,p,\\xi)\\cdot k^{1/2-1/p},\\quad\\|c-c_{S_{2}}\\|_{1,v;y}\\leq C(b,p,\\xi)\\cdot k^{1-1/p},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $C(b,p,\\xi)\\geq0$ depends on $b,\\,p$ and $\\xi$ only. ", "page_idx": 43}, {"type": "text", "text": "Proof. We first show that $\\pmb{c}\\in\\ell_{\\pmb{v}}^{p}(\\mathcal{F};\\mathcal{V})$ . By definition of $\\mathcal{H}(\\pmb{b})$ (see (2.3)), $f$ is holomorphic in every Bernstein polyellipse $\\mathcal{E}(\\rho)$ for which $\\rho$ satisfies ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\rho\\ge1,\\quad\\sum_{j=1}^{\\infty}\\left(\\frac{\\rho_{j}+\\rho_{j}^{-1}}{2}-1\\right)b_{j}\\le1.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Using [4, Lem. 5.3] (which is based on [102, Cor. B.2.7]) with $\\alpha=\\beta={\\bf0}$ , we get that $\\|c_{\\mathbf{0}}\\|_{\\mathcal{Y}}\\leq1$ and ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\|c_{\\nu}\\|_{\\mathcal{Y}}\\leq\\prod_{k\\in I(\\nu,\\rho)}\\frac{\\rho_{k}^{-\\nu_{k}+1}}{(\\rho_{k}-1)^{2}}(\\nu_{k}+1),\\quad\\nu\\in\\mathcal{F}\\backslash\\{\\mathbf{0}\\}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "for all such $\\rho$ , where $I(\\pmb{\\nu},\\pmb{\\rho})=\\operatorname*{supp}(\\pmb{\\nu})\\cap\\{k:\\rho_{k}>1\\}$ . Define the sequence $d_{0}=1$ and ", "page_idx": 43}, {"type": "equation", "text": "$$\nd_{\\nu}=v_{\\nu}^{2/p-1}\\cdot\\operatorname*{inf}\\left\\{\\prod_{k\\in I(\\nu,\\rho)}\\frac{\\rho_{k}^{-\\nu_{k}+1}}{(\\rho_{k}-1)^{2}}(\\nu_{k}+1):\\rho\\ \\mathrm{satisfies}\\ (\\mathrm{D.25})\\right\\},\\quad\\nu\\in\\mathcal{F}\\backslash\\{\\mathbf{0}\\}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Using (C.3)-(C.4), we see that ", "page_idx": 44}, {"type": "equation", "text": "$$\nv_{\\pmb{\\nu}}=\\prod_{k\\in\\mathrm{supp}({\\pmb\\nu})}(2\\nu_{k}+1)^{(5+\\xi)/2}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and therefore ", "page_idx": 44}, {"type": "equation", "text": "$$\nd\\pmb{\\nu}\\leq\\prod_{k\\in I(\\pmb{\\nu},\\pmb{\\rho})}\\frac{\\rho_{k}^{-\\nu_{k}+1}}{(\\rho_{k}-1)^{2}}(2\\nu_{k}+1)^{\\gamma}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "for all $\\rho$ satisfying (D.25) and some $\\gamma=\\gamma(p,\\xi)\\ge0$ . We now use [4, Lem. 5.4] to deduce that $\\pmb{d}=(d_{\\pmb{\\nu}})_{\\pmb{\\nu}\\in\\mathcal{F}}\\in\\ell^{p}(\\mathbb{N})$ with $\\left\\|\\pmb{d}\\right\\|_{p}\\leq C(\\pmb{b},p,\\xi)$ for some $C(b,p,\\xi)\\ge0$ depending on $b,\\,p$ and $\\xi$ only. Returning to $^c$ , this gives ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\|c\\|_{p,v;\\mathcal{V}}^{p}=\\sum_{\\nu\\in\\mathcal{F}}v_{\\nu}^{2-p}\\|c_{\\nu}\\|_{\\mathcal{V}}^{p}\\leq\\sum_{\\nu\\in\\mathcal{F}}|d_{\\nu}|^{p}\\leq C(b,p,\\xi)^{p}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Hence $\\pmb{c}\\in\\ell_{\\pmb{v}}^{p}(\\mathcal{F};\\mathfrak{V})$ with $\\|\\pmb{c}\\|_{p,\\pmb{v};\\mathcal{Y}}\\leq C(\\pmb{b},p,\\xi)$ , as required. ", "page_idx": 44}, {"type": "text", "text": "The second step involves the application of the weighted Stechkin\u2019s inequality (see [3, Lem. 3.12]). This gives that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\left\\{\\|c-c_{S}\\|_{q,v;\\mathcal{Y}}:S\\subset\\mathcal{F},\\,|S|_{v}\\leq k\\right\\}=:\\sigma_{k}(c)_{q,v;\\mathcal{Y}}\\leq\\|c\\|_{p,v;\\mathcal{Y}}k^{1/q-1/p},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "for any $q\\in\\left(p,2\\right]$ and $k>0$ . Applying this result with $q=2$ implies the existence of the set $S_{1}$ (recall that $\\|\\cdot\\|_{2,v;\\mathcal{V}}=\\|\\cdot\\|_{2;\\mathcal{V}})$ and applying it with $q=1$ implies the existence of the set $S_{2}$ . ", "page_idx": 44}, {"type": "text", "text": "We now define the set ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\Lambda_{n}^{\\sf H C l}=\\left\\{\\nu=(\\nu_{k})_{k=1}^{\\infty}\\in\\mathcal{F}:\\prod_{k:\\nu_{k}\\neq0}(\\nu_{k}+1)\\leq n,\\;\\nu_{k}=0,\\;k>n\\right\\}\\subset\\mathcal{F}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Notice that $\\Lambda_{n}^{\\mathsf{H C I}}$ is isomorphic to an index set in $\\mathbb{N}_{0}^{n}$ by the natural restriction map. ", "page_idx": 44}, {"type": "text", "text": "Lemma D.17. Let $k>0$ and suppose that $\\Lambda\\supseteq\\Lambda_{n}^{\\mathsf{H C I}}$ for some $n\\in\\mathbb{N}$ . Let $\\begin{array}{r}{f=\\sum_{\\nu\\in\\mathcal{F}}c_{\\nu}\\Psi_{\\nu}}\\end{array}$ satisfy $f\\in\\mathcal{H}(b)$ for some $\\pmb{b}\\in\\ell_{\\mathsf{M}}^{p}(\\mathbb{N})$ with $\\mathbf{\\delta}b\\geq\\mathbf{0}$ . Then there exists an index set $S\\in S$ such that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|f-f_{S}\\|_{L_{\\varrho}^{\\infty}(D;\\mathcal{Y})}\\leq C(b,p,\\xi)\\cdot\\left(k^{1-1/p}+n^{1-1/p}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where $\\begin{array}{r}{f_{S}=\\sum_{\\nu\\in S}c_{\\nu}\\Psi_{\\nu}}\\end{array}$ . Moreover, $i f\\mathcal{Y}$ is a Hilbert space, then we also have that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|f-f_{S}\\|_{L_{\\varrho}^{2}(D;y)}\\leq C(b,p,\\xi)\\cdot\\left(k^{1/2-1/p}+n^{1/2-1/p}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Here $C(b,p,\\xi)>0$ is a constant depending on $^{b}$ , $p$ and $\\xi$ only. ", "page_idx": 44}, {"type": "text", "text": "Proof of Lemma $D.I7.$ . The previous lemma implies that there exist index sets $S_{1},S_{2}\\subset\\mathcal{F}$ with $|S_{1}|_{v},|S_{2}|_{v}\\le k/2$ such that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\|c-c_{S_{1}}\\|_{2;y}\\leq C(b,p,\\xi)\\cdot k^{1/2-1/p},\\quad\\|c-c_{S_{2}}\\|_{1,v;y}\\leq C(b,p,\\xi)\\cdot k^{1-1/p}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Now define $S=S_{1}\\cup S_{2}\\cap\\Lambda$ and notice that $S\\subseteq\\Lambda$ and $|S|_{v}\\leq|S_{1}|_{v}+|S_{2}|_{v}\\leq k$ . Hence $S\\in S$ Since $v_{\\pmb{\\nu}}\\geq u_{\\pmb{\\nu}}=\\|\\Psi_{\\pmb{\\nu}}\\|_{L_{\\varrho}^{\\infty}(D)}$ , we have (using [5, Lem. 5.1]) ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|f-f_{S}\\|_{L_{\\varrho}^{\\infty}(D;\\mathcal{Y})}\\leq\\|c-c_{S}\\|_{1,u;\\mathcal{Y}}\\leq\\|c-c_{S_{2}}\\|_{1,v;\\mathcal{Y}}+\\|c-c_{\\Lambda}\\|_{1,u;\\mathcal{Y}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Hence, to complete the proof of the first result, we need only show that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\|c-c_{\\Lambda}\\|_{1,{\\pmb u};\\mathcal{V}}\\leq C({\\pmb b},p)\\cdot n^{1-1/p}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "for some constant $C(b,p)\\geq0$ . First, by construction, $\\Lambda\\supseteq\\Lambda_{n}^{\\mathsf{H C I}}$ contains every anchored set of size at most $n$ . See, e.g.. Therefore $\\lVert c-c_{\\Lambda}\\rVert_{1,u;\\mathcal{V}}\\leq\\left\\lVert c-c_{S}\\right\\rVert_{1,u;\\mathcal{V}}$ for any such set $S$ . The result now follows from [5, Cor. 8.2]. ", "page_idx": 44}, {"type": "text", "text": "Now suppose that $\\boldsymbol{\\wp}$ is a Hilbert space. Then Parseval\u2019s identity gives that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|f-f_{S}\\|_{L_{\\varrho}^{2}(D;y)}=\\|c-c_{S}\\|_{2;\\mathcal{Y}}\\leq\\|c-c_{S_{1}}\\|_{2;\\mathcal{Y}}+\\|c-c_{\\Lambda}\\|_{2;\\mathcal{Y}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "As before, it suffices to show that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|c-c_{\\Lambda}\\|_{2;y}\\leq C(b,p)\\cdot n^{1/2-1/p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "This follows from the same approach and [5, Cor. 8.2] once more. ", "page_idx": 44}, {"type": "text", "text": "D.8 Final arguments ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "We are now, finally, ready to prove Theorem 3.1. We first consider the case where $\\boldsymbol{\\wp}$ is a Banach space, then treat the case where $\\boldsymbol{\\wp}$ is a Hilbert space afterwards. ", "page_idx": 45}, {"type": "text", "text": "Proof of Theorem 3.1 when $\\boldsymbol{\\wp}$ is a Banach space. We divide the proof into a series of steps. ", "page_idx": 45}, {"type": "text", "text": "Step 1: Setup and DNN width/depth bounds. Let $m$ , $\\delta,\\epsilon$ and $L$ be as in the theorem statement. We may without loss of generality assume that $\\delta\\leq1/5$ . Now let ", "page_idx": 45}, {"type": "equation", "text": "$$\nn=\\left\\lceil{\\frac{m}{L}}\\right\\rceil\\leq d\\ x\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "and ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\Lambda=\\Lambda_{n}^{\\mathsf{H C I}},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where $\\Lambda_{n}^{\\mathsf{H C I}}$ is as in (D.26). We also set ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\xi=1/\\delta-5\\geq0\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "and ", "page_idx": 45}, {"type": "equation", "text": "$$\nk={\\frac{m}{c L}},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where $c\\geq1$ is a universal constant that will be chosen in the next step. Finally, let $\\delta=2^{-m}/m$ and $\\mathcal{N}$ by the tanh DNN family (D.19), where the $N_{\\nu}$ are as in Lemma D.9 for this $\\Lambda$ and value of $\\delta$ . ", "page_idx": 45}, {"type": "text", "text": "By (D.20) and the definition of $\\xi$ and $k$ , we immediately see that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathrm{width}({\\cal N})\\lesssim(m/L)^{1+\\delta},\\qquad\\mathrm{depth}({\\cal N})\\lesssim\\log(m/L).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "This yields the width and depth bounds (3.1). The rest of the proof is therefore devoted to showing the error bounds (3.3)-(3.4). ", "page_idx": 45}, {"type": "text", "text": "Step 2: Ensuring (D.7) holds with probability at least $1-\\epsilon/2$ . A standard bound (see, e.g., the proof of Lemma 6.4 in [5]) gives that $N=|\\Lambda|$ satisfies ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\log(\\mathrm{e}N)\\leq4\\log^{2}(\\mathrm{e}n)\\lesssim\\log^{2}(m).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Here, in the final step we used the fact that $m\\geq3$ and $L(m,\\epsilon)\\geq1$ . This and the fact that $c\\geq1$ also implies that $k\\leq m$ . Therefore, the right-hand side of (D.13) with $\\epsilon$ replaced by $\\epsilon/2$ satisfies ", "page_idx": 45}, {"type": "equation", "text": "$$\nc_{0}\\cdot k\\cdot(\\log(\\mathrm{e}N)\\cdot\\log^{2}(k)+\\log(4/\\epsilon))\\lesssim k\\cdot L(m,\\epsilon).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Hence, for sufficiently large $c\\geq1$ , we deduce that (D.13) holds with $\\epsilon/2$ . Using (A.I), we see that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\iota_{d_{\\mathcal{X}}}-\\iota_{d_{\\mathcal{X}}}\\circ\\widetilde{\\mathcal{D}}_{\\mathcal{X}}\\circ\\widetilde{\\mathcal{E}}_{\\mathcal{X}}\\|_{L_{\\mu}^{q}(\\mathcal{X};\\ell^{\\infty}(\\mathbb{N}))}\\leq L_{\\iota}\\|\\mathbb{Z}_{\\mathcal{X}}-\\widetilde{\\mathcal{D}}_{\\mathcal{X}}\\circ\\widetilde{\\mathcal{E}}_{\\mathcal{X}}\\|_{L_{\\mu}^{q}(\\mathcal{X};\\mathcal{X})},\\quad q=2,\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Hence (D.14) is implied by (3.2). We conclude from Lemma D.8 that (D.7) holds with probability at least $1-\\epsilon/2$ and $\\alpha\\gtrsim1$ . ", "page_idx": 45}, {"type": "text", "text": "Step 3: Error analysis. Let $f\\in{\\mathcal{H}}(b)$ be the function asserted by (A.II) and define $q=f_{S}$ as the polynomial asserted by Lemma D.17 with $n$ as in (D.29). We also observe that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\|F\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}=\\|f\\|_{L_{\\varsigma}^{\\infty}(D;\\mathcal{Y})}\\lesssim\\|f\\|_{L_{\\varrho}^{\\infty}(D;\\mathcal{Y})}\\lesssim1,\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "since $f\\in{\\mathcal{H}}(b)$ . We now apply Theorem D.13 with $\\sigma=1$ and use (D.31), the definition of $\\delta$ and (3.8) to see that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\||F-\\widehat{F}|\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\lesssim\\||F-\\mathcal{Q}\\circ F|\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}+\\||F-\\mathcal{Q}\\circ F|\\|_{\\mathrm{disc},\\mu}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\pi_{\\mathcal{Q}}E_{2}(F,q)+E_{\\sf o p t}{,2}+E_{\\sf s a m p},}\\\\ &{\\|F-\\widehat{F}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}\\lesssim\\|F-\\mathcal{Q}\\circ F\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\sqrt{k}\\|F-\\mathcal{Q}\\circ F\\|_{\\mathrm{disc},\\mu}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\pi_{\\mathcal{Q}}E_{\\infty}(F,q)+E_{\\sf o p t},\\infty+E_{\\sf s a m p},\\infty}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "with probability at least $1-\\epsilon/2$ , where $E_{2}(F,q)$ and $E_{\\infty}(F,q)$ are as in (D.24), $\\mathcal{Q}:\\mathcal{Y}\\rightarrow\\widetilde{\\mathcal{Y}}=$ DY(RdY) is any bounded linear operator and \u03c0Q = \u2225Q\u2225Y\u2192Y. ", "page_idx": 45}, {"type": "text", "text": "Notice that Parseval\u2019s identity and (D.35) imply that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|q\\|_{L_{\\varrho}^{2}(D;\\mathcal{Y})}\\leq\\|\\boldsymbol{f}\\|_{L_{\\varrho}^{2}(D;\\mathcal{Y})}\\leq\\|\\boldsymbol{f}\\|_{L_{\\varrho}^{\\infty}(D;\\mathcal{Y})}\\lesssim1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Hence, this, the previous bounds, Lemma D.15 with $\\epsilon$ replaced by $\\epsilon/4$ and the union bound yield ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\boldsymbol{F}-\\widehat{\\boldsymbol{F}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\lesssim\\|\\boldsymbol{F}-\\mathcal{Q}\\circ\\boldsymbol{F}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}+\\|\\boldsymbol{F}-\\mathcal{Q}\\circ\\boldsymbol{F}\\|_{\\mathrm{disc},\\mu}}&{}\\\\ {+\\,\\pi_{Q}\\left(\\|\\boldsymbol{F}-\\boldsymbol{q}\\circ\\boldsymbol{\\iota}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}/\\sqrt{r}+\\|\\boldsymbol{F}-\\boldsymbol{q}\\circ\\boldsymbol{\\iota}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\right)}&{}\\\\ {+\\,\\pi_{Q}\\sqrt{k/\\epsilon}\\|\\iota_{d_{\\mathcal{X}}}-\\iota_{d_{\\mathcal{X}}}\\circ\\widetilde{\\mathcal{D}}_{\\mathcal{X}}\\circ\\widetilde{\\mathcal{E}}_{\\mathcal{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\ell^{\\infty}(\\mathbb{R}^{d_{\\mathcal{X}}}))}}&{}\\\\ {+\\,E_{\\mathrm{opt},2}+E_{\\mathrm{samp},2}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "and ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|F-\\widehat{F}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}\\lesssim\\|F-\\mathcal{Q}\\circ F\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\sqrt{k}\\|F-\\mathcal{Q}\\circ F\\|_{\\mathrm{disc},\\mu}}&{}\\\\ {\\quad+\\,\\pi_{\\mathcal{Q}}\\left((1+\\sqrt{k/r})\\|F-q\\circ t\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\sqrt{k}\\|F-q\\circ t\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\right)}&{}\\\\ {\\quad+\\,\\pi_{\\mathcal{Q}}k\\|\\iota_{d_{\\mathcal{X}}}-\\iota_{d_{\\mathcal{X}}}\\circ\\widetilde{\\mathcal{D}}_{\\mathcal{X}}\\circ\\widetilde{\\mathcal{E}}_{\\mathcal{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{\\ell}^{\\infty}(\\mathbb{R}^{d}\\mathcal{X}))}}&{}\\\\ {\\quad+\\,\\pi_{\\mathcal{Q}}\\sqrt{k}(1+\\sqrt{k/r})\\|\\iota_{d_{\\mathcal{X}}}-\\iota_{d_{\\mathcal{X}}}\\circ\\widetilde{\\mathcal{D}}_{\\mathcal{X}}\\circ\\widetilde{\\mathcal{E}}_{\\mathcal{X}}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{\\ell}^{\\infty}(\\mathbb{R}^{d}\\mathcal{X}))}}&{}\\\\ {\\quad+\\,E_{\\mathrm{opt},\\infty}+E_{\\mathrm{samp},\\infty}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "cwhitoho sper itdyu ea tt hlee adste $1-3\\epsilon/4$ f, o(r Da.n3y1 ) $r$ sWuec hn etxhta tb $m\\,\\geq\\,2r\\log(24/\\epsilon)$ .r rIonr $r=k$ $k$ $\\|F-\\mathcal Q\\circ F\\|_{\\mathsf{d i s c},\\mu}$ Applying Lemma D.14 with $\\epsilon$ replaced by $\\epsilon/8$ and the union bound, we see that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{F-{\\mathcal{Q}}\\circ F}\\right\\|_{\\mathrm{disc},\\mu}\\lesssim\\left\\|{F-{\\mathcal{Q}}\\circ F}\\right\\|_{L_{\\mu}^{2}({\\mathcal{X}};{\\mathcal{Y}})}/\\sqrt\\epsilon}\\\\ &{\\left\\|{F-{\\mathcal{Q}}\\circ F}\\right\\|_{\\mathrm{disc},\\mu}\\lesssim\\left\\|{F-{\\mathcal{Q}}\\circ F}\\right\\|_{L_{\\mu}^{\\infty}({\\mathcal{X}};{\\mathcal{Y}})}/\\sqrt r+\\left\\|{F-{\\mathcal{Q}}\\circ F}\\right\\|_{L_{\\mu}^{2}({\\mathcal{X}};{\\mathcal{Y}})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "with probability at least $1-\\epsilon/4$ , provided $m\\geq2r\\log(16/\\epsilon)$ . In particular, we may take $r=k$ once more. Substituting this into the previous expressions, setting $r\\,=\\,k$ throughout, using the union bound once more and recalling (3.7), (D.31) and (D.34), we deduce that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\||F-\\widehat{F}|\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\lesssim\\|F-\\mathcal{Q}\\circ F\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}/\\sqrt{\\epsilon}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\pi_{\\mathcal{Q}}\\left(\\|F-q\\circ\\iota\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}/\\sqrt{m/L}+\\|F-q\\circ\\iota\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\big(\\pi_{\\mathcal{Q}}/a y\\big)\\cdot E_{\\mathcal{X},2}+E_{\\mathrm{opt},2}+E_{\\mathrm{samp},2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "and ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\big\\|F-\\widehat{F}\\big\\|_{L_{\\mu}^{\\infty}(\\varLambda;\\mathcal{Y})}\\lesssim\\big\\|F-\\mathcal{Q}\\circ F\\big\\|_{L_{\\mu}^{\\infty}(\\varLambda;\\mathcal{Y})}+\\sqrt{m/L}\\big\\|F-\\mathcal{Q}\\circ F\\big\\|_{L_{\\mu}^{2}(\\varLambda;\\mathcal{Y})}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\,\\pi_{\\mathcal{Q}}\\left(\\big\\|F-q\\circ\\iota\\big\\|_{L_{\\mu}^{\\infty}(\\varLambda;\\mathcal{Y})}+\\sqrt{m/L}\\big\\|F-q\\circ\\iota\\big\\|_{L_{\\mu}^{2}(\\varLambda;\\mathcal{Y})}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\,\\big(\\pi_{\\mathcal{Q}}/a y\\big)\\cdot E_{\\mathcal{X},\\infty}+E_{\\mathrm{opt},\\infty}+E_{\\mathrm{samp},\\infty}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "with probability at least $1-\\epsilon$ . This holds for any bounded linear operator $\\mathcal{Q}:\\mathcal{Y}\\to\\widetilde{\\mathcal{Y}}=D_{\\mathcal{Y}}(\\mathbb{R}^{d_{\\mathcal{Y}}})$ . We now set $\\mathcal{Q}=\\mathcal{D}_{\\mathcal{Y}}\\circ\\mathcal{E}_{\\mathcal{Y}}$ , which is linear and bounded by (A.IV) with $\\pi_{\\mathcal{Q}}=\\|\\mathcal{D}_{\\mathcal{V}}\\circ\\mathcal{E}_{\\mathcal{V}}\\|_{\\mathcal{V}\\to\\mathcal{V}}=a_{\\mathcal{V}}$ by definition. Using this, we observe that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\|F-\\mathscr{Q}\\circ F\\|_{L_{\\mu}^{q}(\\mathcal{X};\\mathcal{Y})}=\\|\\mathbb{Z}_{\\mathcal{Y}}-\\mathscr{D}_{\\mathcal{Y}}\\circ\\mathcal{E}_{\\mathcal{Y}}\\|_{L_{F\\sharp\\mu}^{q}(\\mathcal{Y};\\mathcal{Y})},\\quad q=2,\\infty.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Substituting this into the previous expressions and recalling (3.7) now gives ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\|F-\\widehat{F}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\lesssim a_{\\mathcal{Y}}\\left(\\left\\|F-q\\circ\\iota\\right\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}/\\sqrt{m/L}+\\left\\|F-q\\circ\\iota\\right\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,E_{\\mathcal{X},2}+E_{\\mathcal{Y},2}+E_{\\mathrm{opt},2}+E_{\\mathrm{samp},2}}\\\\ &{\\left\\|F-\\widehat{F}\\right\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}\\lesssim a_{\\mathcal{Y}}\\left(\\left\\|F-q\\circ\\iota\\right\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\sqrt{m/L}\\right\\|F-q\\circ\\iota\\right\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,E_{\\mathcal{X},\\infty}+E_{\\mathcal{Y},\\infty}+E_{\\mathrm{opt},\\infty}+E_{\\mathrm{samp},\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Step 4: Bounding the polynomial error terms. It remains to bound the error terms $F-q\\circ\\iota$ in (D.37). Using (A.I), (A.II) and Lemma D.17, we now notice that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\left\\|F-q\\circ\\iota\\right\\|_{L_{\\varphi}^{\\infty}(x;y)}=\\left\\|f-q\\right\\|_{L_{\\varphi}^{\\infty}(D;y)}\\lesssim\\left\\|f-q\\right\\|_{L_{\\varphi}^{\\infty}(D;y)}\\lesssim C(b,p,\\xi)\\cdot(k^{1-1/p}+n^{1-1/p}).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Recall that $n\\geq k$ . Therefore, using this, (D.31) and (3.6), we see that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a y\\left(\\left\\|F-q\\circ\\iota\\right\\|_{L_{\\mu}^{\\infty}(X;y)}/\\sqrt{m/L}+\\left\\|F-q\\circ\\iota\\right\\|_{L_{\\mu}^{2}(X;y)}\\right)\\lesssim a y\\left\\|F-q\\circ\\iota\\right\\|_{L_{\\mu}^{\\infty}(X;y)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\lesssim a y\\cdot C(b,p,\\xi)\\cdot\\left(m/L\\right)^{1-1/p}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=E_{\\mathrm{app},2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "and likewise for the $L_{\\mu}^{\\infty}$ -norm bound. Combining this with (D.37) now completes the proof. ", "page_idx": 47}, {"type": "text", "text": "Proof of Theorem 3.1 when $\\boldsymbol{\\wp}$ is a Hilbert space. The two differences in theorem statement when $\\boldsymbol{\\wp}$ is a Hilbert space are: (i) the $L^{2}$ -norm error is with respect to the stronger Bochner norm, and (ii) the approximation error terms $E_{\\mathsf{a p p},q}$ , $q=2,\\infty$ , are smaller by a factor of $1/2$ in the exponent (recall (3.6)). We treat both issues separately. ", "page_idx": 47}, {"type": "text", "text": "For the (i), we commence with the supporting results in $\\S D.2$ . First, we note that Lemmas D.1 and D.2 also hold in the Bochner $L^{2}$ -norm, since these results already give upper bounds involving the Pettis $L^{2}$ -norm. Next, we observe that the proof of Lemma D.3 is readily adapted to yield an equivalent result in the Bochner $L^{2}$ -norm with the same constant $\\delta$ . The same therefore applies to Lemma D.4. ", "page_idx": 47}, {"type": "text", "text": "We next consider the analysis of (D.3) in $\\S\\mathrm{D}.3$ . If we replace (D.7) by the condition ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\lVert\\boldsymbol{p}-\\boldsymbol{q}\\right\\rVert_{\\mathrm{disc},\\tilde{\\boldsymbol{\\varsigma}}}\\geq\\alpha\\operatorname*{max}\\{\\left\\lVert\\boldsymbol{p}-\\boldsymbol{q}\\right\\rVert_{L_{\\varrho}^{2}(D;\\mathcal{Y})},\\left\\lVert\\boldsymbol{p}-\\boldsymbol{q}\\right\\rVert_{L_{\\xi}^{2}(D;\\mathcal{Y})}\\},\\quad\\forall\\boldsymbol{p},\\boldsymbol{q}\\in\\mathcal{P}_{\\mathcal{S};\\tilde{\\mathcal{Y}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "then the proof of Lemma D.5 yields the same error bounds, except with the Pettis $L^{2}$ -norm replaced by the Bochner $L^{2}$ -norm. Theorem D.6 is likewise modified to provide a bound in the Bochner $L^{2}$ -norm. ", "page_idx": 47}, {"type": "text", "text": "Up to this point, we have not used the fact that $\\boldsymbol{\\wp}$ is a Hilbert space. We now need this property. As in $\\S D.4$ , the next step is to establish that (D.38) holds with high probability. Lemma D.7 is unchanged, therefore our focus is on Lemma D.8. We now describe the steps needed to modify the proof of this lemma to assert (D.38) subject to the same conditions (D.13)-(D.14). First, let $\\{\\varphi_{i}\\}_{i=1}^{d y}$ be an orthonormal basis of $\\widetilde{\\boldsymbol{y}}$ and write each coefficient $c_{\\nu_{i}}$ of the function $h$ in (D.16) as $\\begin{array}{r}{c_{\\pmb{\\nu}_{i}}=\\sum_{j=1}^{d_{\\mathcal{V}}}b_{i j}\\varphi_{j}}\\end{array}$ for scalars $b_{i j}$ . Then it is a short exercise to write ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\left\\|h\\right\\|_{{\\sf d i s c},\\tilde{\\varsigma}}^{2}=\\left\\|A\\pmb{c}\\right\\|_{2;\\mathcal{Y}}^{2}=\\sum_{j=1}^{d_{\\mathcal{Y}}}\\|\\pmb{A}\\pmb{b}_{j}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where $\\pmb{b}_{j}=(b_{i j})_{i=1}^{N}$ . Since $b_{i j}=0,\\forall j$ , whenever $c_{\\nu_{i}}=0$ , this vector also satisfies $\\|b_{j}\\|_{0,v}\\leq2k$ . Hence, by Lemma D.7 and Parseval\u2019s identity twice, ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left\\lVert h\\right\\rVert_{\\mathrm{disc},\\tilde{\\varsigma}}^{2}\\ge\\displaystyle\\sum_{j=1}^{d_{\\mathcal{Y}}}(\\theta_{-}-(1+\\theta_{+}c_{5}\\delta)\\|b_{j}\\|_{2}^{2}}\\\\ &{\\displaystyle\\qquad\\qquad=(\\theta_{-}-(1+\\theta_{+}c_{5}\\delta)\\displaystyle\\sum_{i=1}^{N}\\|c_{\\nu_{i}}\\|_{2}^{2}}\\\\ &{\\displaystyle\\qquad=(\\theta_{-}-(1+\\theta_{+}c_{5}\\delta)\\|h\\|_{L_{\\rho}^{2}(D;\\mathcal{Y})}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "We now use the bounds (D.15) (which are unchanged) and set $\\delta=c_{3}/(2(1+c_{4})c_{5})$ once more to get ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|h\\|_{\\mathsf{d i s c},\\widetilde{\\mathsf{c}}}^{2}\\geq c_{3}/2\\|h\\|_{L_{\\varrho}^{2}(D;\\mathcal{Y})}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "This gives the desired result. ", "page_idx": 47}, {"type": "text", "text": "This completes the changes needed in order to analyze the polynomial training problem (D.3). We next consider the DNN training problem (2.5). $\\S D.5$ remains unchanged. After reviewing their proofs, we see that Lemma D.12 and Theorem D.13 both hold with Pettis norms replaced by Bochner norms whenever (D.38) holds instead of (D.7). Finally, we also observe that Lemma D.15 also holds with Pettis norms replaced by Bochner norms, both in the bound and in the definition (D.24) of $E_{2}(F,q)$ and $E_{\\infty}(F,q)$ . ", "page_idx": 47}, {"type": "text", "text": "", "page_idx": 48}, {"type": "text", "text": "Having completed the changes needed in all the preparatory results, we now follow the same steps as above in the Banach space case. Steps 1 and 2 are unchanged. For Step 3, we go through and replace Pettis norms by Bochner norms throughout. Using this, we obtain (D.37), except with the Bochner norm on the left-hand side in the first inequality, i.e., ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|F-\\widehat{F}\\right\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\lesssim a_{\\mathcal{Y}}\\left(\\left\\|F-q\\circ\\iota\\right\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}/\\sqrt{m/L}+\\left\\|F-q\\circ\\iota\\right\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,E_{\\mathcal{X},2}+E_{\\mathcal{Y},2}+E_{\\mathrm{opt},2}+E_{\\mathrm{samp},2}}\\\\ &{\\left\\|F-\\widehat{F}\\right\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}\\lesssim a_{\\mathcal{Y}}\\left(\\left\\|F-q\\circ\\iota\\right\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\sqrt{m/L}\\right\\|F-q\\circ\\iota\\right\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,E_{\\mathcal{X},\\infty}+E_{\\mathcal{Y},\\infty}+E_{\\mathrm{opt},\\infty}+E_{\\mathrm{samp},\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "This concludes the changes needed to address (i). To address (ii), we bound the terms $F\\mathrm{~-~}q\\mathrm{~o~}\\iota$ . Using (A.I), (A.II), Lemma D.17, the fact that $\\boldsymbol{\\wp}$ is a Hilbert space and the definitions (D.31) and (D.29) of $k$ and $n$ , we see that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\left\\|F-q\\circ\\iota\\right\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}=\\left\\|f-q\\right\\|_{L_{\\epsilon}^{\\infty}(D;\\mathcal{Y})}\\lesssim\\left\\|f-q\\right\\|_{L_{\\sigma}^{\\infty}(D;\\mathcal{Y})}\\lesssim C(b,p,\\xi)\\cdot(m/L)^{1-1/p}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "and ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\|F-q\\circ\\iota\\|_{L_{\\mu}^{2}(X;y)}=\\|f-q\\|_{L_{\\zeta}^{2}(D;y)}\\lesssim\\|f-q\\|_{L_{\\varrho}^{2}(D;y)}\\lesssim C(b,p,\\xi)\\cdot(m/L)^{1/2-1/p}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Substituting this into (D.39) and recalling (3.6) now completes the proof. ", "page_idx": 48}, {"type": "text", "text": "Remark D.18 (Differences between the Banach and Hilbert space case) Having seen the proof, we now summarize these differences as follows. First, the matter of whether the Pettis versus Bochner norm can be used reduces to the choice of such norm in the discrete metric inequality (D.7). When $\\boldsymbol{\\wp}$ is a Banach space, we are able to establish this in terms of the Pettis norm subject to a log-linear scaling between $m$ and $k$ (see Lemma D.8 and (D.13)). However, when $\\boldsymbol{\\wp}$ is also a Hilbert space, we can establish the stronger version (D.38) of this inequality by exploiting the additional structure. This, in short, is what leads to the stronger norm bound in this case. ", "page_idx": 48}, {"type": "text", "text": "Second, in the Hilbert space case, we get an improved approximation error. This stems from (D.17) and, specifically, the fact that when $\\boldsymbol{\\wp}$ is a Hilbert space we may use Parseval\u2019s identity in the Bochner space $\\bar{L}_{\\varrho}^{2}(D;\\bar{\\mathcal{V}})$ to bound the $L_{\\varrho}^{2}$ -norm error term via (D.28). This is not possible when $\\boldsymbol{\\wp}$ is a Banach space, so we settle for bounding this term via (D.27) instead. ", "page_idx": 48}, {"type": "text", "text": "E Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "E.1 Setup ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "As in $\\S D.1$ , let $\\Lambda\\subset{\\mathcal{F}}$ with ${\\mathrm{supp}}(\\pmb{\\nu})\\subseteq\\{1,\\ldots,d_{\\mathcal{X}}\\}$ , $\\forall\\pmb{\\nu}\\in\\Lambda$ , and write $N=|\\Lambda|$ . Let $\\boldsymbol{S}$ be as in (D.1), $r\\in\\mathbb{N}$ , $r>\\operatorname*{max}\\{m,k\\}$ (its precise value will be chosen later in the proof) and define the set ", "page_idx": 48}, {"type": "equation", "text": "$$\n{\\Gamma}={\\Gamma}_{o}\\cup\\bigcup_{S\\in\\mathcal{S}}S\\subset\\mathcal{F},\\quad\\mathrm{where~}{\\Gamma}_{o}=\\{e_{i}:i=1,\\ldots,r\\}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Finally, let $0<\\delta<1$ and consider the family $\\mathcal{N}_{o}$ and the tanh DNNs $\\{N_{\\nu}\\}_{\\nu\\in\\Gamma}$ whose existence is implied by Lemma D.9. We will specify $\\Lambda,k,r$ and $\\delta$ later in the proof. ", "page_idx": 48}, {"type": "text", "text": "Next, let ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\hat{p}=\\sum_{\\nu\\in S}\\hat{c}_{\\nu}\\Psi_{\\nu}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "be any minimizer of (D.3), where $|S|_{v}\\leq k$ and define ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\tilde{p}=\\sum_{\\nu\\in S}\\hat{c}_{\\nu}N_{\\nu}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Let ", "page_idx": 49}, {"type": "equation", "text": "$$\nB=\\frac{1}{\\sqrt{r}}\\left(N_{e_{j}}\\circ\\mathcal{E}_{\\boldsymbol{X}}(\\boldsymbol{X}_{i})\\right)_{i,j=1}^{m,r}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Now, $\\widetilde{\\boldsymbol{y}}:=\\mathcal{D}_{\\boldsymbol{y}}(\\mathbb{R}^{d_{\\boldsymbol{y}}})$ is a finite-dimensional subspace of the Banach space $\\boldsymbol{\\wp}$ . Hence, for any $Y\\in\\mathcal{V}$ there exists a closest point $\\widetilde{Y}\\in\\widetilde{\\mathcal{Y}}$ , i.e., a point satisfying ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\|Y-\\widetilde{Y}\\|_{\\mathcal{Y}}=\\operatorname*{inf}\\left\\{\\|Y-Z\\|_{\\mathcal{Y}}:Z\\in\\widetilde{\\mathcal{Y}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Given $Y_{1},\\ldots,Y_{m}$ , let $\\widetilde{Y}_{1},\\ldots,\\widetilde{Y}_{m}\\in\\widetilde{\\mathcal{Y}}$ be the corresponding closest points. Now define ", "page_idx": 49}, {"type": "equation", "text": "$$\ne=\\frac{1}{\\sqrt{r}}\\left(\\widetilde{Y}_{i}-\\tilde{p}\\circ\\mathcal{E}_{X}(X_{i})\\right)_{i=1}^{m}\\in\\widetilde{\\mathcal{V}}^{m}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "and ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\bar{p}=\\tilde{p}+\\sum_{i=1}^{r}(B^{\\dagger}e+y z)_{i}N_{e_{i}},\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $z\\in N(B)\\backslash\\{0\\}$ and $y\\in\\widetilde{\\mathcal{P}},\\|y\\|_{\\mathcal{V}}=1$ , are arbitrary. Note that such a $_{\\textit{z}}$ exists, since $r>m$ by assumption. Notice that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\bar{p}=\\sum_{\\nu\\in S\\cup\\Gamma_{o}}\\bar{c}_{\\nu}N_{\\nu}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "for coefficients $\\bar{c}_{\\nu}\\in\\widetilde{\\mathcal{D}}$ . Therefore, we can write ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{p}=\\mathcal{D}_{\\mathcal{Y}}\\circ\\widehat{N},}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\widehat{N}=C\\left[\\begin{array}{c}{N_{\\nu_{1}}}\\\\ {\\vdots}\\\\ {N_{\\nu_{|S\\cup\\Gamma_{o}|}}}\\\\ {0}\\\\ {\\vdots}\\\\ {0}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "and $\\begin{array}{r}{C\\in\\mathbb{R}^{d_{\\mathcal{V}}\\times(\\lfloor k\\rfloor+r)}}\\end{array}$ . Finally, we define the approximation ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\widehat{F}=\\mathcal{D}_{\\mathcal{Y}}\\circ\\widehat{N}\\circ\\mathcal{E}_{\\mathcal{X}}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "and, for convenience, ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\check{F}=\\hat{p}\\circ\\mathcal{E}_{\\mathcal{X}}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "E.2 Estimation of the DNN minimizer ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Lemma E.1 ( $\\widehat{N}$ is a minimizer). If $_B$ is full rank, then ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\widehat{F}(X_{i})=\\widetilde{Y}_{i},\\quad\\forall i=1,\\ldots,m.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Therefore, $\\widehat{N}$ is a minimizer of (2.5). ", "page_idx": 49}, {"type": "text", "text": "Proof. Observe that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{F}(X_{i})=\\overline{{p}}\\circ\\mathcal E_{X}(X_{i})}\\\\ &{\\qquad=\\tilde{p}\\circ\\mathcal E_{X}(X_{i})+\\sqrt{r}\\displaystyle\\sum_{i=1}^{r}(B)_{i j}(B^{\\dagger}e+y z)_{j}}\\\\ &{\\qquad=\\tilde{p}\\circ\\mathcal E_{X}(X_{i})+\\sqrt{r}(B(B^{\\dagger}e+y z))_{i}}\\\\ &{\\qquad=\\tilde{p}\\circ\\mathcal E_{X}(X_{i})+\\sqrt{r}(e)_{i}}\\\\ &{\\qquad=\\widetilde{Y}_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Here, in the penultimate step we use the facts that $B y z=y B z=\\mathbf{0}$ since $z\\in N(B)$ and $B B^{\\dagger}=I$ since $r\\geq m$ and $_B$ is full rank by assumption. This gives the first result. ", "page_idx": 50}, {"type": "text", "text": "For the second result, we recall that ${\\widetilde{Y}}_{i}$ is a closest point to $Y_{i}$ from $\\widetilde{\\mathcal{V}}=\\mathcal{D}_{\\mathcal{V}}(\\mathbb{R}^{d_{\\mathcal{V}}})$ . Therefore, for any DNN $N$ , ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\frac{1}{m}\\sum_{i=1}^{m}\\|Y_{i}-\\mathcal{D}_{y}\\circ N\\circ\\mathcal{E}_{X}(X_{i})\\|_{y}^{2}\\geq\\frac{1}{m}\\sum_{i=1}^{m}\\left\\|Y_{i}-\\widetilde{Y}_{i}\\right\\|_{y}^{2}=\\frac{1}{m}\\sum_{i=1}^{m}\\left\\|Y_{i}-\\mathcal{D}_{y}\\circ\\widehat{N}\\circ\\mathcal{E}_{X}(X_{i})\\right\\|_{y}^{2}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "as required. ", "page_idx": 50}, {"type": "text", "text": "Lemma E.2 (Bounding $\\widehat F$ in terms of $\\protect\\widecheck{F}$ ). Suppose that (D.7) holds with $\\alpha\\geq c_{0}$ and $\\alpha-\\delta\\sqrt{k}\\geq c_{1}$ , and also that ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\sqrt{r}\\|\\iota_{d_{\\mathcal{X}}}-\\iota_{d_{\\mathcal{X}}}\\circ\\widetilde{{\\mathcal{D}}}_{\\mathcal{X}}\\circ\\widetilde{{\\mathcal{E}}}_{\\mathcal{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\ell^{\\infty}(\\mathbb{N}))}\\leq c_{2},\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where $c_{0},c_{1},c_{2}>0$ are suitable universal constants. Then the approximation $\\widehat F$ satisfies ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|F-\\widehat{F}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\lesssim\\|F-\\check{F}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}}\\\\ &{\\phantom{=}\\;+(1+\\delta\\sqrt{r})\\delta\\sqrt{k}\\left(1+\\frac{\\sqrt{m}}{\\sqrt{r}\\sigma_{\\operatorname*{min}}(B)}\\right)\\left(\\|F\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\frac{1}{\\sqrt{m}}\\|E\\|_{2;\\mathcal{Y}}\\right)}\\\\ &{\\phantom{=}\\;+\\frac{\\sqrt{m}}{\\sqrt{r}\\sigma_{\\operatorname*{min}}(B)}\\left(\\sqrt{\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\|\\widetilde{Y}_{i}-\\check{F}(X_{i})\\|_{\\mathcal{Y}}^{2}}+\\frac{1}{\\sqrt{m}}\\|E\\|_{2;\\mathcal{Y}}\\right)}\\\\ &{\\phantom{=}\\;+(1+\\delta\\sqrt{r})\\|z\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "and ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|F-\\widehat{F}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}\\lesssim\\|F-\\check{F}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}}\\\\ &{\\qquad\\qquad\\qquad+\\left(1+\\delta\\right)\\!\\sqrt{r}\\delta\\sqrt{k}\\left(1+\\frac{\\sqrt{m}}{\\sqrt{r}\\sigma_{\\operatorname*{min}}(B)}\\right)\\left(\\left\\|F\\right\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\frac{1}{\\sqrt{m}}\\|E\\|_{2;\\mathcal{Y}}\\right)}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle\\frac{\\sqrt{m}(1+\\delta)}{\\sigma_{\\operatorname*{min}}(B)}\\left(\\sqrt{\\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\|\\widetilde{Y}_{i}-\\check{F}(X_{i})\\|_{y}^{2}}+\\frac{1}{\\sqrt{m}}\\|E\\|_{2;y}\\right)}\\\\ &{\\qquad\\qquad\\qquad+\\left(1+\\delta\\right)\\!\\sqrt{r}\\left\\|z\\right\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Proof. By the triangle inequality, ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\||F-\\widehat{F}|\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\leq\\|F-\\widecheck{F}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}+\\|\\widecheck{F}-\\widehat{F}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}}\\\\ &{\\|F-\\widehat{F}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}\\leq\\|F-\\widecheck{F}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\|\\widecheck{F}-\\widehat{F}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Consider t he second term. We have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r}{\\|\\widecheck{F}-\\widehat{F}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}=\\|\\widehat{p}-\\mathcal{D}_{\\mathcal{Y}}\\circ\\widehat{N}\\|_{L_{\\xi}^{2}(D;\\mathcal{Y})}\\leq\\|\\widehat{p}-\\widehat{p}\\|_{L_{\\xi}^{2}(D;\\mathcal{Y})}+\\|q\\|_{L_{\\xi}^{2}(D;\\mathcal{Y})},}\\\\ {\\|\\widecheck{F}-\\widehat{F}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}=\\|\\widehat{p}-\\mathcal{D}_{\\mathcal{Y}}\\circ\\widehat{N}\\|_{L_{\\xi}^{\\infty}(D;\\mathcal{Y})}\\leq\\|\\widehat{p}-\\widetilde{p}\\|_{L_{\\xi}^{\\infty}(D;\\mathcal{Y})}+\\|q\\|_{L_{\\xi}^{\\infty}(D;\\mathcal{Y})}}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where $\\begin{array}{r}{q=\\bar{p}-\\tilde{p}=\\sum_{i=1}^{r}(B^{\\dagger}e+y z)_{i}N_{e i}}\\end{array}$ . Lemma D.10 and (A.III) give that ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\||\\hat{p}-\\tilde{p}|\\|_{L_{\\tilde{\\zeta}}^{2}(D;\\mathcal{Y})}=\\|\\hat{p}-\\tilde{p}\\|_{L_{\\tilde{\\zeta}}^{\\infty}(D;\\mathcal{Y})}\\leq\\delta\\sqrt{k}\\|\\hat{p}\\|_{L_{\\varrho}^{2}(D;\\mathcal{Y})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Now consider the other term in (E.7). Define $\\begin{array}{r}{\\tilde{q}=\\sum_{i=1}^{r}(B^{\\dagger}e+y z)_{i}\\Psi_{e_{i}}}\\end{array}$ . Then Lemma D.10 and (A.III) once more give that ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r}{\\|q\\|_{L_{\\xi}^{2}(D;y)}\\le\\|\\tilde{q}\\|_{L_{\\xi}^{2}(D;y)}+\\|q-\\tilde{q}\\|_{L_{\\xi}^{2}(D;y)}\\le\\|\\tilde{q}\\|_{L_{\\xi}^{2}(D;y)}+\\delta\\sqrt{r}\\|\\tilde{q}\\|_{L_{\\varrho}^{2}(D;y)}\\,.}\\\\ {\\|q\\|_{L_{\\xi}^{\\infty}(D;y)}\\le\\|\\tilde{q}\\|_{L_{\\xi}^{\\infty}(D;y)}+\\|q-\\tilde{q}\\|_{L_{\\xi}^{\\infty}(D;y)}\\le\\|\\tilde{q}\\|_{L_{\\xi}^{\\infty}(D;y)}+\\delta\\sqrt{r}\\|\\tilde{q}\\|_{L_{\\varrho}^{2}(D;y)}.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Here, we also used the fact that $|\\Gamma_{o}|=r$ . We now apply Lemmas D.1 and D.4 and (A.III) once more to get ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|q\\|_{L_{\\xi}^{2}(D;y)}\\lesssim(1+\\delta\\sqrt{r})\\|\\tilde{q}\\|_{L_{\\varrho}^{2}(D;y)},\\qquad\\|q\\|_{L_{\\xi}^{\\infty}(D;y)}\\lesssim(1+\\delta)\\sqrt{r}\\|\\tilde{q}\\|_{L_{\\varrho}^{2}(D;y)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "We next analyze the term $\\|\\tilde{q}\\|_{L_{\\varrho}^{2}(D;\\mathcal{D})}$ . Let $y^{\\ast}\\in\\mathcal{V}^{\\ast}$ . Since $y^{*}((B^{\\dagger}e\\!+\\!y z)_{i})=(B^{\\dagger}y^{*}(e)\\!+\\!y^{*}(y)z)_{i}$ , we have ", "page_idx": 51}, {"type": "equation", "text": "$$\ny^{*}(\\tilde{q})=\\sum_{i=1}^{r}(B^{\\dagger}y^{*}(e)+y^{*}(y)z)_{i}\\Psi_{e_{i}}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Hence, by Parseval\u2019s identity, ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\tilde{q}\\|_{L_{\\varrho}^{2}(D;y)}=\\underset{y^{*}\\in B(y^{*})}{\\operatorname*{sup}}\\|y^{*}(\\tilde{q})\\|_{L^{2}(D)}}\\\\ &{\\qquad\\qquad=\\underset{y^{*}\\in B(y^{*})}{\\operatorname*{sup}}\\,\\|B^{\\dagger}y^{*}(e)+y^{*}(y)z\\|_{2}}\\\\ &{\\qquad\\qquad\\leq\\frac{1}{\\sigma_{\\operatorname*{min}}(B)}\\underset{y^{*}\\in B(y^{*})}{\\operatorname*{sup}}\\,\\|y^{*}(e)\\|_{2}+\\underset{y^{*}\\in B(y^{*})}{\\operatorname*{sup}}\\,|y^{*}(y)|\\|z\\|_{2}}\\\\ &{\\qquad=\\frac{1}{\\sigma_{\\operatorname*{min}}(B)}\\|e\\|_{2;y}+\\|z\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "We now use the definition of $^e$ and the inequality $\\|e\\|_{2;\\mathcal{V}}\\leq\\|e\\|_{2;\\mathcal{V}}$ to obtain ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\|\\tilde{q}\\|_{L_{\\varrho}^{2}(D;y)}\\leq\\frac{\\sqrt{m}}{\\sqrt{r}\\sigma_{\\operatorname*{min}}(B)}\\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}\\|\\widetilde{Y_{i}}-\\widetilde{p}\\circ\\mathcal{E}_{X}(X_{i})\\|_{y}^{2}}+\\|z\\|_{2}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "We next apply the triangle inequality and Lemma D.10 once more to get ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\|\\tilde{q}\\|_{L_{\\varrho}^{2}(D;y)}\\leq\\frac{\\sqrt{m}}{\\sqrt{r}\\sigma_{\\operatorname*{min}}(B)}\\left(\\sqrt{\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\|\\tilde{Y}_{i}-\\hat{p}\\circ\\mathcal{E}_{X}(X_{i})\\|_{y}^{2}}+\\|\\hat{p}-\\tilde{p}\\|_{\\mathrm{disc},\\tilde{\\varsigma}}\\right)+\\|z\\|_{2}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{\\sqrt{m}}{\\sqrt{r}\\sigma_{\\operatorname*{min}}(B)}\\left(\\sqrt{\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\|\\tilde{Y}_{i}-\\hat{p}\\circ\\mathcal{E}_{X}(X_{i})\\|_{y}^{2}}+\\delta\\sqrt{k}\\|\\hat{p}\\|_{L_{\\varrho}^{2}(D;y)}\\right)+\\|z\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Combining this with (E.7) and (E.9), we deduce that ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\|\\widecheck{F}-\\widehat{F}\\|_{L_{\\rho}^{2}(\\mathcal{X};\\mathcal{Y})}\\lesssim(1+\\delta\\sqrt{r})\\left[\\delta\\sqrt{k}\\left(1+\\frac{\\sqrt{m}}{\\sqrt{r}\\sigma_{\\operatorname*{min}}(B)}\\right)\\|\\widehat{p}\\|_{L_{\\varrho}^{2}(D;\\mathcal{Y})}+\\|z\\|_{2}\\right.}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\left.+\\left.\\frac{\\sqrt{m}}{\\sqrt{r}\\sigma_{\\operatorname*{min}}(B)}\\left(\\sqrt{\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\|\\widecheck{Y}_{i}-\\widehat{p}\\circ\\mathcal{E}_{\\mathcal{X}}(X_{i})\\|_{\\mathcal{Y}}^{2}}+\\frac{1}{\\sqrt{m}}\\|E\\|_{2;\\mathcal{Y}}\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "and ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\|\\widecheck{F}-\\widehat{F}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}\\lesssim(1+\\delta)\\sqrt{r}\\bigg[\\delta\\sqrt{k}\\left(1+\\frac{\\sqrt{m}}{\\sqrt{r}\\sigma_{\\operatorname*{min}}(B)}\\right)\\|\\widehat{p}\\|_{L_{\\varrho}^{2}(D;\\mathcal{Y})}+\\|z\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\frac{\\sqrt{m}}{\\sqrt{r}\\sigma_{\\operatorname*{min}}(B)}\\left(\\sqrt{\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\|\\widetilde{Y}_{i}-\\widehat{p}\\circ\\mathcal{E}_{\\mathcal{X}}(X_{i})\\|_{\\mathcal{Y}}^{2}}+\\frac{1}{\\sqrt{m}}\\|{\\cal E}\\|_{2;\\mathcal{Y}}\\right)\\bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "It remains to bound the term $\\|\\hat{p}\\|_{L_{\\varrho}^{2}(D;\\mathcal{D})}.$ Using (D.7) and the fact that $\\hat{p}$ is a minimizer of (D.3) and that the zero polynomial is feasible for (D.3), we get ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\|\\hat{p}\\|_{L_{\\varrho}^{2}(D;y)}\\lesssim\\|\\hat{p}\\|_{\\L{\\mathrm{disc}},\\tilde{\\mathcal{G}}}}&{}\\\\ {\\displaystyle}&{\\leq\\sqrt{\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\|Y_{i}-\\hat{p}\\circ\\mathcal{E}_{X}(X_{i})\\|_{\\mathcal{Y}}^{2}}+\\displaystyle\\frac{1}{\\sqrt{m}}\\|\\boldsymbol{Y}\\|_{2;y}}\\\\ {\\displaystyle}&{\\leq\\displaystyle\\frac{2}{\\sqrt{m}}\\|\\boldsymbol{Y}\\|_{2;y}}\\\\ {\\displaystyle}&{\\leq2\\|\\boldsymbol{F}\\|_{L_{\\varrho}^{\\infty}(\\mathcal{X};y)}+\\displaystyle\\frac{2}{\\sqrt{m}}\\|\\boldsymbol{E}\\|_{2;y}}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Combining this with the previous bound and (E.6) now completes the proof. ", "page_idx": 52}, {"type": "text", "text": "E.3 Estimation of $\\sigma_{\\mathrm{min}}({\\pmb B})$ ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Recall that a matrix $A\\in\\mathbb{R}^{m\\times n}$ is a subgaussian random matrix if its entries are i.i.d. subgaussian random variables with mean zero and variance one (see, e.g., [29, Def. 9.1]). The following result can be found in, e.g., [29, Ex. 9.3]. ", "page_idx": 52}, {"type": "text", "text": "Lemma E.3 (Smallest singular value of a subgaussian random matrix). Let $A\\,\\in\\,\\mathbb{R}^{m\\times n}$ be $a$ subgaussian random matrix and $\\sigma_{\\mathrm{min}}$ be the smallest singular value of $\\textstyle{\\frac{1}{\\sqrt{m}}}A$ . Then, for all $0<t<1$ ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\sigma_{\\operatorname*{min}}\\leq1-c_{1}\\sqrt{n/m}-t\\right)\\leq2\\exp(-c_{2}m t^{2}),\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where $c_{1},c_{2}>0$ are universal constants. ", "page_idx": 52}, {"type": "text", "text": "Lemma E.4 (Bounding $\\sigma_{\\mathrm{min}}(B))$ ). Suppose that ${\\sqrt{m}}\\delta\\leq{\\sqrt{\\omega}}/8,$ , ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\frac{3^{(5+\\xi)/2}\\sqrt{r}}{2}\\|\\iota_{d_{\\mathcal{X}}}-\\iota_{d_{\\mathcal{X}}}\\circ\\widetilde{\\mathcal{D}}_{\\mathcal{X}}\\circ\\widetilde{\\mathcal{E}}_{\\mathcal{X}}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X},\\ell^{\\infty}(\\mathbb{N}))}\\leq\\frac{\\sqrt{\\omega}}{8},\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where $\\omega$ is the variance of the univariate probability measure as specified in Theorem 3.2 and ", "page_idx": 52}, {"type": "equation", "text": "$$\nd_{\\mathcal{X}}\\geq r\\geq c\\,(m+\\log(2/\\epsilon))\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "for some universal constant $c\\geq1$ . Then, with probability at least $1-\\epsilon$ , the matrix $_B$ is full rank and ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\sigma_{\\mathrm{min}}(B)\\geq\\sqrt{\\omega}/4.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Proof. Define the matrices ", "page_idx": 52}, {"type": "equation", "text": "$$\nB^{\\prime}=\\frac{1}{\\sqrt{r}}\\left(\\Psi_{e_{j}}\\circ\\mathcal{E}_{X}(X_{i})\\right)_{i,j=1}^{m,r},\\quad B^{\\prime\\prime}=\\frac{1}{\\sqrt{r}}\\left(\\Psi_{e_{j}}\\circ\\iota_{d_{X}}(X_{i})\\right)_{i,j=1}^{m,r}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Then, since $r\\geq m$ , ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\sigma_{\\operatorname*{min}}(B)=\\operatorname*{inf}\\left\\{\\|B^{\\top}d\\|_{2}:d\\in\\mathbb{C}^{m},\\ \\|d\\|_{2}=1\\right\\}}&{}\\\\ {\\geq\\sigma_{\\operatorname*{min}}(B^{\\prime})-\\|(B-B^{\\prime})^{\\top}\\|_{2}}&{}\\\\ {=\\sigma_{\\operatorname*{min}}(B^{\\prime})-\\|B-B^{\\prime}\\|_{2}}&{}\\\\ {\\geq\\sigma_{\\operatorname*{min}}(B^{\\prime\\prime})-\\|B-B^{\\prime}\\|_{2}-\\|B^{\\prime}-B^{\\prime\\prime}\\|_{2}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Now, for any $\\pmb{c}\\in\\mathbb{C}^{r}$ , ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|(B-B^{\\prime})c\\right\\|_{2}^{2}=\\displaystyle\\frac1{r}\\sum_{i=1}^{m}\\left(\\displaystyle\\sum_{j=1}^{r}\\left(N_{e_{j}}(x_{i})-\\Psi_{e_{j}}(x_{i})\\right)c_{j}\\right)^{\\frac{1}{2}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac1{r}\\sum_{i=1}^{m}\\delta^{2}\\|c\\|_{1}^{2}}\\\\ &{\\qquad\\qquad\\leq m\\delta^{2}\\|c\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "We deduce that $\\|B-B^{\\prime}\\|_{2}\\leq\\sqrt{m}\\delta\\leq\\sqrt{\\omega}/8$ . Hence ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\sigma_{\\mathrm{min}}({\\pmb B})\\geq\\sigma_{\\mathrm{min}}({\\pmb B}^{\\prime\\prime})-\\sqrt{\\omega}/8-\\|{\\pmb B}^{\\prime}-{\\pmb B}^{\\prime\\prime}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Now let $\\pmb{c}\\in\\mathbb{C}^{r}$ and $\\begin{array}{r}{p=\\sum_{i=1}^{r}c_{i}\\Psi_{e_{i}}}\\end{array}$ be the corresponding polynomial. Then, by (A.I), (A.III) and Lemma D.2, ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|(B^{\\prime}-B^{\\prime\\prime})c\\|_{2}=\\sqrt{\\displaystyle\\frac{1}{r}\\sum_{i=1}^{m}\\left|p\\circ\\iota_{d_{\\mathcal{X}}}(X_{i})-p\\circ\\iota_{d_{\\mathcal{X}}}\\circ\\widetilde{{\\mathcal{D}}}_{\\mathcal{X}}\\circ\\widetilde{{\\mathcal{E}}}_{\\mathcal{X}}(X_{i})\\right|^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathrm{Lip}(p,B^{\\infty}(\\mathbb{N}),\\mathbb{R})\\|\\iota_{d_{\\mathcal{X}}}-\\iota_{d_{\\mathcal{X}}}\\circ\\widetilde{{\\mathcal{D}}}_{\\mathcal{X}}\\circ\\widetilde{{\\mathcal{E}}}_{\\mathcal{X}}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X},\\ell^{\\infty}(\\mathbb{N}))}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{3^{(5+\\xi)/2}\\sqrt{r}}{2}\\|p\\|_{L_{\\varrho}^{2}(D)}\\|\\iota_{d_{\\mathcal{X}}}-\\iota_{d_{\\mathcal{X}}}\\circ\\widetilde{{\\mathcal{D}}}_{\\mathcal{X}}\\circ\\widetilde{{\\mathcal{E}}}_{\\mathcal{X}}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X},\\ell^{\\infty}(\\mathbb{N}))}}\\\\ &{\\qquad\\qquad\\leq\\frac{\\sqrt{\\omega}}{8}\\|c\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Here, in the third\u221a step we used the fact that $|\\Gamma_{o}|_{v}\\,=\\,3^{5+\\xi_{r}}$ , since $u_{e_{i}}~=~\\sqrt{3}$ . We deduce that $\\|B^{\\prime}-B^{\\prime\\prime}\\|_{2}\\leq\\sqrt{\\omega}/\\dot{8}$ and therefore ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\sigma_{\\mathrm{min}}(B)\\geq\\sigma_{\\mathrm{min}}(B^{\\prime\\prime})-\\sqrt{\\omega}/4.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "It remains to show that $\\sigma_{\\mathrm{min}}(B^{\\prime\\prime})\\geq\\sqrt{\\omega}/2$ with high probability. By construction, $\\Psi_{e_{j}}({\\pmb x})=\\sqrt{3}x_{j}$ . Now recall that the pushforward $\\varsigma$ is a tensor-product of a univariate probability measure supported in $[-1,1]$ with mean zero and variance $\\omega>0$ . Therefore ", "page_idx": 53}, {"type": "equation", "text": "$$\n(B^{\\prime\\prime})^{\\top}=\\sqrt{3}\\frac{\\sqrt{\\omega}}{\\sqrt{r}}A,\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where $\\pmb{A}=(\\iota(X_{j})_{i}/\\sqrt\\omega)_{i,j=1}^{r,m}\\in\\mathbb{R}^{r\\times m}$ . By construction, the entries of $\\pmb{A}$ are i.i.d. subgaussian random variables with mean zero and variance one. Hence $\\pmb{A}$ is a subgaussian random matrix. We now apply Lemma E.3. Let $t=1/4$ and observe that ", "page_idx": 53}, {"type": "equation", "text": "$$\nr\\ge4c_{1}^{2}m,\\quad r\\ge\\frac{16}{c_{2}}\\log(2/\\epsilon),\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "by assumption. Therefore, ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\sigma_{\\operatorname*{min}}(B^{\\prime\\prime})\\leq\\sqrt{\\omega}/2)\\leq\\mathbb{P}(\\sigma_{\\operatorname*{min}}(A/\\sqrt{r})\\leq1/(2\\sqrt{3}))\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "This gives the result. ", "page_idx": 53}, {"type": "text", "text": "E.4 Final arguments ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "We are now ready to complete the proof of Theorem 3.2. ", "page_idx": 53}, {"type": "text", "text": "Proof of Theorem 3.2, Statement $(A)$ . We divide the proof into a series of steps. ", "page_idx": 53}, {"type": "text", "text": "Step 1: Setup. Let $m,\\delta,\\epsilon$ and $L$ be as in the theorem statement. We once more assume without loss of generality that $\\delta\\le1/5$ . Let $n$ be as in (D.29) and $\\Lambda=\\Lambda_{n}^{\\mathsf{H C I}}$ , let $\\xi$ be as in (D.30) and ", "page_idx": 53}, {"type": "equation", "text": "$$\nk={\\frac{m}{c_{1}L}},\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where $c_{1}\\geq1$ is a constant that will be chosen in the next step. Let ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\delta=\\operatorname*{min}\\left\\{2^{-m}/r^{2},\\sqrt{\\omega}/(8\\sqrt{m})\\right\\},\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where $\\omega$ is the variance of the univariate probability measure and ", "page_idx": 53}, {"type": "equation", "text": "$$\nr=\\lceil c_{2}(m+\\log(1/\\epsilon))\\rceil,\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where $c_{2}\\geq2$ will also be chosen in the next step. Note that $d_{\\mathcal{X}}\\geq r>m$ by assumption. Finally, let $\\boldsymbol{S}$ be as in (D.1), $\\Gamma$ be as in (E.1) and $\\{N_{\\nu}\\}_{\\nu\\in\\Gamma}$ be the corresponding family of tanh DNNs ensured by Lemma D.9. Finally, let $\\widehat F$ be given by (E.5), with DNN $\\widehat{N}$ as in (E.4). ", "page_idx": 53}, {"type": "text", "text": "Step 2: $\\widehat{N}$ is a minimizer. By construction, ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\mathrm{width}(\\widehat{N})\\lesssim(k+r)\\cdot m(\\Gamma),\\quad\\mathrm{depth}(\\widehat{N})\\lesssim\\log(k)\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "If $\\nu=e_{i}$ , then $\\|\\pmb{\\nu}\\|_{1}=1$ . Hence $m(\\Gamma)\\leq1+\\operatorname*{max}_{S\\in\\mathcal{S}}m(S)\\leq1+k^{1/(5+\\xi)}=1+k^{\\delta}$ (see $\\S D.5\\rangle$ . Using the values of $k$ and $r$ , we see that ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\mathrm{width}(\\widehat{N})\\lesssim(m+\\log(1/\\epsilon))(m/L)^{\\delta},\\quad\\mathrm{depth}(\\widehat{N})\\lesssim\\log(m/L).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Therefore, $\\widehat{N}\\in\\mathcal{N}$ is feasible for (2.5). Lemma E.1 now implies that $\\widehat{N}$ is a minimizer, provided $_B$ is full rank . We will show that this holds in the next step. ", "page_idx": 54}, {"type": "text", "text": "Step 3: Ensuring that $\\sigma_{\\mathrm{min}}(B)\\geq\\sqrt{\\omega}/4$ with pro\u221abability at least $1-\\epsilon/4$ . We seek to use Lemma E.4. By definition of $\\delta$ , we have that ${\\sqrt{m}}\\delta\\,\\leq\\,{\\sqrt{\\omega}}/8$ . Now (D.34), (E.13) and (3.9) imply that (E.10)-(E.11) hold, the latter with $\\epsilon$ replaced by $\\epsilon/4$ . Hence Lemma E.4 implies the result. ", "page_idx": 54}, {"type": "text", "text": "Step 4: Ensuring (D.7) holds with probability at least $1-\\epsilon/4$ . This step is very similar to Step 2 of the proof of Theorem 3.1. The only difference comes in the estimation of $N$ in (D.32), since now $N\\,\\dot{=}\\,|\\Gamma|\\leq|\\Lambda|+r$ . Since $m\\geq3$ , we have ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\log(\\mathrm{e}N)\\leq\\log(\\mathrm{e}|\\Lambda|(r+1))\\lesssim\\log^{2}(m)+\\log(m+\\log(1/\\epsilon)).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "If $m\\le\\log(1/\\epsilon)$ then ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log(\\mathrm{e}N)\\lesssim\\log^{2}(m)+\\log(\\log(1/\\epsilon))\\leq\\log^{2}(m)+\\sqrt{\\log(1/\\epsilon)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where in the second step we use the fact that $\\log(t)\\leq\\sqrt{t}$ for $t>0$ . Conversely, if $m\\geq\\log(1/\\epsilon)$ , then ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\log(\\mathrm{e}N)\\lesssim\\log^{2}(m)\\leq\\log^{2}(m)+\\sqrt{\\log(1/\\epsilon)}.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Therefore, (D.33) with $\\epsilon/4$ reads ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{c_{0}\\cdot k\\cdot(\\log(\\exp)\\cdot\\log^{2}(k)+\\log(6/\\epsilon))\\lesssim k\\cdot\\left(\\log^{2}(m)\\left(\\log^{2}(m)+\\sqrt{\\log(1/\\epsilon)}\\right)+\\log(1/\\epsilon)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\lesssim k\\cdot\\left(\\log^{4}(m)+\\log(1/\\epsilon)\\right).}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=k\\cdot L(m,\\epsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Hence, due to the definition of $k$ , we get that (D.13) holds once more with probability at least $1-\\epsilon/4$ .   \nThe remainder of this step is identical to Step 2 of the proof of Theorem 3.1. ", "page_idx": 54}, {"type": "text", "text": "Step 5: E\u221arror analysis. We now apply Lemma E.2 to the approximation $\\widehat F$ . Since $\\sigma_{\\mathrm{min}}(B)\\gtrsim\\sqrt{\\omega}\\gtrsim$ 1, $\\bar{\\delta}\\leq\\delta\\sqrt{r}\\leq1$ and $r\\geq m$ , we deduc e that ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}&{\\displaystyle\\||F-\\widehat{F}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\lesssim\\||F-\\widecheck{F}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}+\\delta\\sqrt{k}\\left(\\|F\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\frac{1}{\\sqrt{m}}\\|E\\|_{2;\\mathcal{Y}}\\right)}\\\\ &{\\displaystyle\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ +\\left\\|z\\right\\|_{2}+\\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}\\|\\widetilde{Y}_{i}-\\widecheck{F}(X_{i})\\|_{\\mathcal{Y}}^{2}}+\\frac{1}{\\sqrt{m}}\\|E\\|_{2;\\mathcal{Y}}}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "and ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\left\\|{F-\\widehat{F}}\\right\\|_{L_{\\mu}^{\\infty}(X;y)}\\lesssim\\left\\|{F-\\widecheck{F}}\\right\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};y)}+\\sqrt{r}\\delta\\sqrt{k}\\left(\\left\\|{F}\\right\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};y)}+\\frac{1}{\\sqrt{m}}{\\left\\|{E}\\right\\|_{2;\\mathcal{Y}}}\\right)}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad+\\left.\\sqrt{r}\\right\\|z\\right\\|_{2}+\\sqrt{m}\\left(\\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}{\\left\\|\\widetilde{Y}_{i}-\\widecheck{F}(X_{i})\\right\\|_{y}^{2}}}+\\frac{1}{\\sqrt{m}}{\\left\\|{E}\\right\\|_{2;\\mathcal{Y}}}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "with probability at least $1-\\epsilon/2$ , where $\\check{F}=\\hat{p}\\circ\\mathcal{E}_{\\mathcal{X}}$ and $\\hat{p}$ is the corresponding m\u221ainimizer of (D.3). We now appeal to Theorem D.6 with $\\sigma=1$ and $\\tau=0$ , recalling that $\\alpha\\gtrsim1$ and $\\delta\\sqrt{k}\\lesssim1$ , to get that ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|F-\\widehat{F}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\lesssim\\|F-Q\\circ F\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}+\\|F-Q\\circ F\\|_{\\mathrm{disc},\\mu}}&{}\\\\ {\\quad+\\,\\pi_{Q}\\left(\\|F-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}+\\|F-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{\\mathrm{disc},\\mu}\\right)}&{}\\\\ {\\quad+\\,\\frac{1}{\\sqrt{m}}\\|E\\|_{2,\\mathcal{Y}}+\\delta\\sqrt{k}\\|F\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}}&{}\\\\ {\\quad+\\,\\|z\\|_{2}+\\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}\\|\\widetilde{Y}_{i}-\\widecheck{F}(X_{i})\\|_{\\mathcal{Y}}^{2}}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "and, since $\\delta\\sqrt{r}\\lesssim1$ , ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{l}{\\|F-\\widehat{F}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}\\lesssim\\|F-Q\\circ F\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\sqrt{k}\\|F-Q\\circ F\\|_{{\\mathrm{disc}},\\mu}}\\\\ {\\quad\\qquad\\qquad\\qquad+\\,\\pi_{Q}\\,\\Big(\\|F-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\sqrt{k}\\|F-q\\circ\\mathcal{E}_{\\mathcal{X}}\\|_{{\\mathrm{disc}},\\mu}\\Big)}\\\\ {\\quad+\\,\\|E\\|_{2;\\mathcal{Y}}+\\sqrt{r}\\delta\\sqrt{k}\\|F\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}}\\\\ {\\quad+\\,\\sqrt{r}\\|z\\|_{2}+\\sqrt{m}\\sqrt{\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\|\\widetilde{Y}_{i}-\\widecheck{F}(X_{i})\\|_{\\mathcal{Y}}^{2}}}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "for any $q\\in\\mathcal{P}_{\\mathcal{S};\\mathcal{X}}$ and linear operator $\\mathcal{Q}:\\mathcal{Y}\\to\\widetilde{\\mathcal{Y}}=\\mathcal{D}_{\\mathcal{Y}}(\\mathbb{R}^{d_{\\mathcal{Y}}})$ with $\\pi_{\\mathcal{Q}}=\\|\\mathcal{Q}\\|_{\\mathcal{V}\\to\\mathcal{V}}$ . Consider the final term. Since $\\widetilde{Y}_{i}\\in\\widetilde{\\mathcal{Y}}$ is the closest point to $Y_{i}=F(X_{i})+E_{i}$ we have ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\widetilde{Y}_{i}-Y_{i}\\|_{\\mathcal{Y}}\\leq\\|F({X}_{i})-\\mathcal{Q}\\circ F({X}_{i})\\|_{\\mathcal{Y}}+\\|E_{i}\\|_{\\mathcal{Y}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "We now use this, the fact that $\\hat{p}$ is a minimizer and $\\mathcal Q\\circ q$ is feasible in combination with triangle inequality to get ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\left\\{\\frac{1}{m}\\sum_{i=1}^{m}\\|\\widetilde{Y}_{i}-\\check{F}(X_{i})\\|_{y}^{2}\\leq\\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}\\|Y_{i}-Q\\circ q\\circ\\mathcal{E}_{X}(X_{i})\\|_{y}^{2}}+\\|F-Q\\circ F\\|_{\\mathrm{disc},\\mu}+\\frac{1}{\\sqrt{m}}\\|E\\|_{2;y}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Now let $f\\,\\in\\,{\\mathcal{H}}(b)$ be the function asserted by (A.II) and $q\\,=\\,f_{S}$ be the polynomial asserted by Lemma D.17. Substituting this into the previous expression, recalling (D.35) and using the definition of $\\delta$ , we deduce that ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\||F-\\widehat{F}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\lesssim\\||F-\\mathcal{Q}\\circ F|\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}+\\||F-\\mathcal{Q}\\circ F\\|_{\\mathsf{d i s c},\\mu}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\pi_{\\mathcal{Q}}E_{2}(F,q)+\\|z\\|_{2}+2^{-m}+E_{\\mathrm{samp},2}}\\\\ &{\\|F-\\widehat{F}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}\\lesssim\\|F-\\mathcal{Q}\\circ F\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\sqrt{m}\\|F-\\mathcal{Q}\\circ F\\|_{\\mathsf{d i s c},\\mu}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\pi_{\\mathcal{Q}}\\widetilde{E}_{\\infty}(F,q)+\\sqrt{r}\\|z\\|_{2}+2^{-m}+E_{\\mathrm{samp},\\infty}^{\\prime}}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "with probability at least $1-\\epsilon/2$ . Here $E_{2}(F,q)$ is as in (D.24), $\\widetilde{E}_{\\infty}(F,q)$ is as in (D.24) with $k$ replaced by $m$ and $E_{\\mathsf{s a m p,c o}}^{\\prime}=\\|E\\|_{2;\\mathcal{Y}}=\\sqrt{L}E_{\\mathsf{s a m p,c o}}$ . ", "page_idx": 55}, {"type": "text", "text": "Now observe that the first bound is identical to the corresponding bound in (D.36), except with $E_{\\mathrm{opt,2}}$ replaced by $\\lVert z\\rVert_{2}+2^{-m}$ . Following the same arguments as in Step 3 of the proof of Theorem 3.1, this gives the corresponding bound in (D.37), which is ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}{\\||F-\\widehat{F}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\lesssim a_{\\mathcal{Y}}\\left(\\|F-q\\circ\\iota\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}/\\sqrt{m/L}+\\|F-q\\circ\\iota\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\right)}&{}\\\\ {+\\,E_{\\mathcal{X},2}+E_{\\mathcal{Y},2}+\\|z\\|_{2}+2^{-m}+E_{\\mathrm{samp},2}.}&{}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "The second bound above is identical to the corresponding bound in (D.36), except with $E_{\\mathsf{o p t,\\infty}}$ replaced b\u221ay $\\sqrt{r}\\|z\\|_{2}\\!+\\!2^{-m},E_{\\mathsf{s a m p,c o}}$ and $E_{\\infty}(F,q)$ replaced by $E_{\\mathsf{s a m p,\\infty}}^{\\prime}$ and $\\widetilde{E}_{\\infty}(F,q)$ , respectively, and with $\\sqrt{k}$ replaced by $\\sqrt{m}$ . We once more follow the same arguments as in Step 3, with these changes. This yields the corresponding version of (D.37), which is ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}{\\|F-\\widehat{F}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}\\lesssim a_{\\mathcal{Y}}\\left(\\sqrt{L}\\|F-q\\circ\\iota\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}+\\sqrt{m}\\|F-q\\circ\\iota\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\right)}&{}\\\\ {+\\,E_{\\mathcal{X},\\infty}^{\\prime}+E_{\\mathcal{Y},\\infty}^{\\prime}+\\sqrt{r}\\|z\\|_{2}+2^{-m}+E_{\\mathsf{s a m p},\\infty}^{\\prime}.}&{}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Here $E_{\\mathcal{X,\\infty}}^{\\prime}=L E_{\\mathcal{X},\\infty}$ and $E_{\\mathcal{Y},\\infty}^{\\prime}=\\sqrt{L}E_{\\mathcal{Y},\\infty}$ . ", "page_idx": 55}, {"type": "text", "text": "Having done this, we then use the bounds from Step 4 of the proof of Theorem 3.1, to get ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\||F-\\widehat{F}|\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\lesssim E_{\\mathsf{a p p},2}+E_{\\mathcal{X},2}+E_{\\mathcal{Y},2}+\\|z\\|_{2}+2^{-m}+E_{\\mathsf{s a m p},2}}\\\\ &{\\|F-\\widehat{F}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}\\lesssim E_{\\mathsf{a p p},\\infty}^{\\prime}+E_{\\mathcal{X},\\infty}^{\\prime}+E_{\\mathcal{Y},\\infty}^{\\prime}+\\sqrt{r}\\|z\\|_{2}+2^{-m}+E_{\\mathsf{s a m p},\\infty}^{\\prime},}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where E\u2032 $E_{\\mathsf{a p p,\\infty}}^{\\prime}=\\sqrt{L}E_{\\mathsf{a p p,\\infty}}$ . ", "page_idx": 56}, {"type": "text", "text": "Step 6: Existence of uncountably many minimizers. Let $z=(z_{i})_{i=1}^{r}\\in N(B)\\backslash\\{\\mathbf{0}\\}$ be any vector and consider $z_{1}=\\theta_{1}z$ and $z_{2}=\\theta_{2}z$ for $\\theta_{1},\\theta_{2}\\in[-1,1]$ with $\\theta_{1}\\neq\\theta_{2}$ . Then these vectors define functions $\\bar{p}_{1}$ and $\\bar{p}_{2}$ as in (E.2) and DNNs $\\widehat{N}_{1}$ and $\\widehat{N}_{2}$ as in (E.3). Suppose that $\\widehat{N}_{1}=\\widehat{N}_{2}$ . Then, since $\\mathcal{D}_{\\mathcal{Y}}$ is linear, we have that $\\bar{p}_{1}=\\bar{p}_{2}$ . But then, by definition and the fact that $y\\in\\widetilde{\\mathcal{Y}}\\backslash\\{0\\}$ , we must have ", "page_idx": 56}, {"type": "equation", "text": "$$\n0=\\sum_{i=1}^{r}(z_{1}-z_{2})_{i}N_{e_{i}}=(\\theta_{1}-\\theta_{2})\\sum_{i=1}^{r}z_{i}N_{e_{i}}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Suppose th\u221aat $\\theta_{1}>\\theta_{2}$ without loss of generality and let $\\pmb{x}\\in D$ be the vector $(\\mathrm{sign}(z_{i}))_{i=1}^{\\infty}$ . Then, $\\Psi_{e_{i}}(\\pmb{x})=\\sqrt{3}\\mathrm{sign}(z_{i})$ and therefore ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r}{0\\ge(\\theta_{1}-\\theta_{2})\\left(\\sqrt{3}\\|z\\|_{1}-\\delta r\\right)\\ge(\\theta_{1}-\\theta_{2})\\left(\\sqrt{3}\\|z\\|_{2}-\\delta r\\right)=(\\theta_{1}-\\theta_{2})\\left(\\sqrt{3}\\|z\\|_{2}-2^{-m}/r\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "since $\\|N_{e_{i}}-\\Psi_{e_{i}}\\|_{L_{\\varrho}^{\\infty}(D)}\\,\\leq\\,\\delta$ and $\\delta\\,\\leq\\,2^{-m}/r^{2}$ by definition. We now choose $_{\\textit{z}}$ with $\\|z\\|_{2}=$ $2^{-m}/r\\leq2^{-m}$ . Note that this choice of $_{z}$ does not change the error bound, except for a constant. It also yields ", "page_idx": 56}, {"type": "equation", "text": "$$\n0\\geq(\\theta_{1}-\\theta_{2})(\\sqrt{3}2^{-m}-2^{-m})>0\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "which is a contradiction. Hence $\\widehat{N}_{1}\\neq\\widehat{N}_{2}$ . Thus, we have shown that any $\\theta\\in[-1,1]$ leads to a distinct DNN minimizer that satisfies the desired bounds. We get the result. ", "page_idx": 56}, {"type": "text", "text": "Step 7: Modifications when $\\boldsymbol{\\wp}$ is a Hilbert space. The modifications required when $\\boldsymbol{\\wp}$ is a Hilbert space are identical to those needed in the proof of Theorem 3.1 (see $\\S D.8]$ ). We omit the details. ", "page_idx": 56}, {"type": "text", "text": "Proof of Theorem 3.2, Statement $(B)$ . Let $N=N_{\\pmb\\theta}:\\mathbb{R}^{d_{\\mathcal{X}}}\\rightarrow\\mathbb{R}^{d_{\\mathcal{Y}}}$ be a tanh DNN, where $\\pmb\\theta\\in\\mathbb{R}^{D}$ are the network parameters (weights and biases). Let $\\|\\cdot\\|_{(d_{\\mathcal{X}})},\\|\\cdot\\|_{(d_{\\mathcal{Y}})}$ and $\\|\\cdot\\|_{(D)}$ be arbitrary norms on $\\mathbb{R}^{d_{\\boldsymbol{x}}},\\mathbb{R}^{d_{\\boldsymbol{y}}}$ and $\\mathbb{R}^{D}$ , respectively. Then, since the activation function is a Lipschitz function, we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\|N_{\\pmb{\\theta}^{\\prime}}(\\pmb{x})-N_{\\pmb{\\theta}}(\\pmb{x})\\|_{(d_{\\mathcal{X}})}\\leq c_{\\pmb{\\theta}}\\|\\pmb{\\theta}^{\\prime}-\\pmb{\\theta}\\|_{(D)}(\\|\\pmb{x}\\|_{(d_{\\mathcal{X}})}+1),\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where $c_{\\theta}>0$ is a constant depending on $\\pmb{\\theta}$ . ", "page_idx": 56}, {"type": "text", "text": "Now let $\\widehat{N}=\\widehat{N}_{\\widehat{\\pmb{\\theta}}}$ and $\\widehat{F}=\\mathcal{D}_{\\mathcal{Y}}\\circ\\widehat{N}\\circ\\mathcal{E}_{\\mathcal{X}}$ and consider $F=D y\\circ N\\circ\\mathcal{E}_{\\mathcal{X}}$ for $N\\,=\\,N_{\\theta}$ . Since $\\mathcal{D}_{\\mathcal{Y}}:\\mathbb{R}^{d_{\\mathcal{Y}}}\\rightarrow\\mathcal{Y}$ is linear and therefore bounded, we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\|\\widehat{F}(X)-F(X)\\|_{\\mathcal{Y}}\\leq c_{\\hat{\\theta}}\\|\\mathcal{D}_{\\mathcal{Y}}\\|_{(\\mathbb{R}^{d_{\\mathcal{Y}}},\\|\\cdot\\|_{(d_{\\mathcal{Y}})})\\rightarrow\\mathcal{Y}}\\|\\hat{\\theta}-\\theta\\|_{(D)}\\left(\\|\\mathcal{E}_{X}(X)\\|_{(d_{\\mathcal{X}})}+1\\right).\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "We deduce that ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widehat{F}-F\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\leq\\|\\widehat{F}-F\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}}\\\\ &{\\qquad\\qquad\\qquad\\leq c_{\\hat{\\theta}}\\|\\mathcal{D}_{\\mathcal{Y}}\\|_{(\\mathbb{R}^{d_{\\mathcal{Y}}},\\|\\cdot\\|_{(d_{\\mathcal{Y}})})\\to\\mathcal{Y}}\\|\\hat{\\theta}-\\theta\\|_{(D)}\\left(\\|\\mathcal{E}_{\\mathcal{X}}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};(\\mathbb{R}^{d_{\\mathcal{X}}},\\|\\cdot\\|_{(d_{\\mathcal{X}})})}+1\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Therefore, there exists a neighbourhood around $\\hat{\\pmb{\\theta}}$ for which ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\||\\widehat{F}-F|\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\leq\\|\\widehat{F}-F\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}\\leq\\tau_{\\sigma}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "for all parameters $\\pmb{\\theta}$ in the neighbourhood. The result now follows. ", "page_idx": 56}, {"type": "text", "text": "Proof of Theorem 3.2, Statement $(C)$ . By construction, the DNN $\\widehat{N}$ defined in (E.4) contains subnetworks that compute the DNNs $N_{e_{i}}$ , $i=1,\\hdots,r$ , which themselves are approximations to the Legendre polynomials $\\Psi_{e_{i}}$ . The construction of these subnetworks was described in the proof of ", "page_idx": 56}, {"type": "text", "text": "Lemma D.9 as the composition of an affine map defined by the fundamental theorem of algebra and a tanh DNN that approximately multiplies $m(\\Gamma)$ numbers. In this specific case, we have ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\Psi_{e_{i}}({\\pmb x})=x_{i}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Therefore, the corresponding affine map (D.18) $\\boldsymbol{A}_{e_{i}}:\\mathbb{R}^{d_{\\boldsymbol{x}}}\\,\\rightarrow\\,\\mathbb{R}^{m(\\Gamma)}$ has a bias vector that is all ones, except for a single entry that has value zero. We deduce that the bias vector $^{b}$ of the full DNN (E.4) contains a subblock of size $r m(\\Gamma)$ that is all ones, except for $r$ zeroes. There are $\\binom{r m(\\Gamma)}{r}$ rearrangements of this subblock, each of which leading to a bias vector $b^{\\prime}$ with $\\|\\pmb{b}-\\pmb{b}^{\\prime}\\|\\gtrsim1$ Moreover, $b^{\\prime}$ leads to the same DNN, after permuting the various weight matrices in the corresponding way. Indeed, if $_{P}$ is a permutation matrix, then $\\sigma(\\overbar{W}x+b)=P^{-1}\\sigma(P W x+P b)$ , since $\\sigma$ acts componentwise. ", "page_idx": 57}, {"type": "text", "text": "It remains to bound $\\binom{r m(\\Gamma)}{r}$ from below. We have ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\binom{r m(\\Gamma)}{r}\\geq\\frac{(r m(\\Gamma))^{r}}{r^{r}}=m(\\Gamma)^{r}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "By (E.13), we have that $r\\geq2m$ . Now, by the definition (E.1) of $\\Gamma$ , ", "page_idx": 57}, {"type": "equation", "text": "$$\nm(\\Gamma)\\geq\\operatorname*{max}_{S\\in\\mathcal{S}}m(S),\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where $\\boldsymbol{S}$ is as in (D.1) for $k$ as in (E.12) and $\\Lambda=\\Lambda_{n}^{\\mathsf{H C I}}$ as in (D.26) with $n$ as in (D.29). Now consider the set $S=\\{l e_{1}\\}$ for some $l\\in\\mathbb N$ . Then ", "page_idx": 57}, {"type": "equation", "text": "$$\n|S|_{v}=v_{\\nu}^{2}=(2l+1)^{5+\\xi}=(2l+1)^{1/\\delta}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Therefore, $S\\in S$ provided $(2l+1)^{1/\\delta}\\leq k$ and $l\\leq n$ . Set $l=\\lfloor(k^{\\delta}-1)/2\\rfloor$ and observe that $l\\leq n$ since $k\\leq n$ and $\\delta\\leq1/5$ by assumption. Therefore $S\\in S$ . Using the definition of $k$ , we get that ", "page_idx": 57}, {"type": "equation", "text": "$$\nm(\\Gamma)\\geq m(S)=l\\geq(m/(c_{4}L))^{\\delta}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "for all sufficiently large $m$ . The result now follows. ", "page_idx": 57}, {"type": "text", "text": "F Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "The proof of Theorem 4.1 will follow as a consequence of the following result. For this, we require the following notation. Given $0<p\\leq\\infty$ , $s\\in\\mathbb{N}$ and a sequence $\\pmb{c}=\\bar{(c_{i})}_{i=1}^{\\infty}$ , we let ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\sigma_{s}({\\pmb c})_{p}=\\operatorname*{min}\\{\\left\\|{\\pmb c}-{\\pmb z}\\right\\|_{p}:{\\pmb z}\\in\\ell^{2},|\\mathrm{supp}({\\pmb z})|\\leq s\\},\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where $\\operatorname{supp}(z)=\\{i\\in\\mathbb{N}:z_{i}\\neq0\\}$ for $z=(z_{i})_{i\\in\\mathbb{N}}\\in\\mathbb{R}^{\\mathbb{N}}$ . ", "page_idx": 57}, {"type": "text", "text": "Theorem F.1. For any $0<p<1$ then term $\\theta_{m}(\\pmb{b})$ defined in (4.2) satisfies ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\theta_{m}(\\pmb{b})\\gtrsim\\sigma_{m}(\\pmb{b})_{2},\\quad\\forall\\pmb{b}\\in\\ell^{1}(\\mathbb{N}),\\,\\pmb{b}\\geq\\mathbf{0},\\|\\pmb{b}\\|_{1}\\leq1.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "As noted, the proof of this theorem is based on [7, Thm. 4.4]. We recap the details as they will also be needed in the proof of the next result. First, we recall some basic definitions. See [82] or [29, Ch. 10] for more details. Let $\\kappa$ be a subset of a normed space $(\\mathcal{X},\\|\\cdot\\|_{\\mathcal{X}})$ . Then its Gelfand $m$ -width is ", "page_idx": 57}, {"type": "equation", "text": "$$\nd^{m}(K,\\mathcal{X})=\\operatorname*{inf}\\left\\{\\operatorname*{sup}_{x\\in K\\cap L^{m}}\\|x\\|_{\\mathcal{X}},\\ L^{m}\\mathrm{~a~subspace~of~}\\mathcal{X}\\mathrm{~with~codim}(L^{m})\\leq m\\right\\}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "An equivalent representation is ", "page_idx": 57}, {"type": "equation", "text": "$$\nd^{m}(K,\\mathcal{X})=\\operatorname*{inf}\\left\\{\\operatorname*{sup}_{x\\in K\\cap K\\mathrm{er}(A)}\\|x\\|_{\\mathcal{X}},\\ A:\\mathcal{X}\\to\\mathbb{R}^{m}\\,\\operatorname*{linear}\\right\\}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "The Gelfand width is related to the following quantity: ", "page_idx": 57}, {"type": "equation", "text": "$$\nE_{\\mathrm{ada}}^{m}(K,\\mathcal{X})=\\operatorname*{inf}\\left\\{\\operatorname*{sup}_{x\\in K}\\|x-\\Delta(\\Gamma(x))\\|_{\\mathcal{X}},\\,\\Gamma:\\mathcal{X}\\to\\mathbb{R}^{m}\\,\\mathrm{adaptive},\\,\\Delta:\\mathbb{R}^{m}\\to\\mathcal{X}\\right\\},\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where $\\Delta$ is an arbitrary (potentially nonlinear) reconstruction map and $\\Gamma$ is an adaptive sampling map. By this, we mean that ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\Gamma(x)=\\left[\\begin{array}{c}{\\Gamma_{1}(x)}\\\\ {\\Gamma_{2}(x,\\Gamma_{1}(x))}\\\\ {\\vdots}\\\\ {\\Gamma_{m}(x,\\Gamma_{1}(x),\\ldots,\\Gamma_{m-1}(x))}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where $\\Gamma_{1}:\\mathcal{X}\\rightarrow\\mathbb{R}$ is linear and, for $i\\,=\\,2,\\dots,m$ , $\\Gamma_{i}\\,:\\,\\mathcal{X}\\,\\times\\,\\mathbb{R}^{i-1}\\,\\rightarrow\\,\\mathbb{R}$ is linear in its first component. ", "page_idx": 58}, {"type": "text", "text": "Proof of Theorem F.1. We proceed in a series of steps. ", "page_idx": 58}, {"type": "text", "text": "Step 1: Setup. Define the functions ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\phi_{i}(\\pmb{x})=\\sqrt{3}x_{i},\\quad\\pmb{x}\\in D,\\qquad i=1,2,\\ldots.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Notice that these functions form an orthonormal system in $L_{\\varrho}^{2}(D)$ . (A.I) implies that these functions form a Riesz system in $L_{\\varsigma}^{2}(D)$ , with Riesz constants that are $\\asymp1$ . Hence they have a (unique) biorthogonal Riesz system $\\{\\psi_{i}\\}_{i=1}^{\\infty}\\,\\subset\\,L_{\\varsigma}^{2}(D)$ . Now define $\\Phi_{i}\\,=\\,\\phi_{i}\\mathrm{~o~}\\iota$ and $\\Psi_{i}\\,=\\,\\psi_{i}\\mathrm{~o~}\\iota$ . Let $G\\in L_{\\mu}^{2}(\\mathcal{X};\\mathbb{R})$ be arbitrary. Then ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\|G\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathbb{R})}\\geq\\frac{\\langle G,\\sum_{i=1}^{\\infty}\\langle G,\\Psi_{i}\\rangle_{L_{\\mu}^{2}(\\mathcal{X};\\mathbb{R})}\\Psi_{i}\\rangle_{L_{\\mu}^{2}(\\mathcal{X};\\mathbb{R})}}{\\|\\sum_{i=1}^{\\infty}\\langle G,\\Psi_{i}\\rangle_{L^{2}(\\mathcal{X};\\mathbb{R})}\\Psi_{i}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathbb{R})}}=\\frac{\\sum_{i=1}^{\\infty}|\\langle G,\\Psi_{i}\\rangle_{L^{2}(\\mathcal{X};\\mathbb{R})}|^{2}}{\\|\\sum_{i=1}^{\\infty}\\langle G,\\Psi_{i}\\rangle_{L^{2}(\\mathcal{X};\\mathbb{R})}\\Psi_{i}\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathbb{R})}}.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Consider the denominator. Using (A.II) and fact that the $\\psi_{i}$ form a Riesz system, we see that ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\left\\lVert\\sum_{i=1}^{\\infty}\\langle G,\\Psi_{i}\\rangle_{L^{2}(\\mathcal{X};\\mathbb{R})}\\Psi_{i}\\right\\rVert_{L_{\\mu}^{2}(\\mathcal{X};\\mathbb{R})}=\\left\\lVert\\sum_{i=1}^{\\infty}\\langle G,\\Psi_{i}\\rangle_{L^{2}(\\mathcal{X};\\mathbb{R})}\\psi_{i}\\right\\rVert_{L_{\\mathfrak{c}}^{2}(D)}\\lesssim\\sqrt{\\sum_{i=1}^{\\infty}|\\langle G,\\Psi_{i}\\rangle_{L^{2}(\\mathcal{X};\\mathbb{R})}|^{2}}.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "We deduce that ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\|G\\|_{L^{2}(X;\\mathbb{R})}\\gtrsim\\sqrt{\\sum_{i=1}^{\\infty}|\\langle G,\\Psi_{i}\\rangle_{L^{2}(X;\\mathbb{R})}|^{2}},\\quad\\forall G\\in L_{\\mu}^{2}(\\mathcal{X};\\mathbb{R}).\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Now let $\\mathbf{\\Delta}b\\geq\\mathbf{0}$ with $b\\in\\ell^{1}(\\mathbb{N})$ and $I\\subset\\mathbb{N}$ with $|I|=N$ . Using [7, Lem. 5.2] we see that the function ", "page_idx": 58}, {"type": "equation", "text": "$$\nf=c\\sum_{i\\in I}c_{i}y\\phi_{i}\\in\\mathcal{H}(\\pmb{b}),\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "for any $y\\in\\mathcal{V}$ , $\\|y\\|_{\\mathcal{Y}}=1$ and $\\pmb{c}=(c_{i})_{i\\in\\mathbb{N}}\\subset\\mathbb{R}^{\\mathbb{N}}$ with $|c|\\leq b$ (i.e., $\\left|{c_{i}}\\right|\\leq b_{i},\\forall i)$ , where $c>0$ is a universal constant. ", "page_idx": 58}, {"type": "text", "text": "Step 2: Reduction to a discrete problem. Let $\\mathcal{L}$ and $\\mathcal{R}$ be arbitrary sampling and reconstruction maps as in (4.2). Following [7, Lem. 5.3], let $F=f\\circ\\iota$ and observe that ", "page_idx": 58}, {"type": "equation", "text": "$$\nF(X)=c y\\sum_{i\\in I}c_{i}\\Phi_{i}(X)\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "and therefore ", "page_idx": 58}, {"type": "equation", "text": "$$\n{\\mathcal{L}}(F)=y\\Gamma(c),\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where $\\Gamma:\\mathbb{R}^{|I|}\\rightarrow\\mathbb{R}^{m}$ is given by ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\Gamma(z)=\\left[\\begin{array}{c}{{c\\sum_{i\\in I}z_{i}\\Phi_{i}(X_{1})}}\\\\ {{\\vdots}}\\\\ {{c\\sum_{i\\in I}z_{i}\\Phi_{i}(X_{m})}}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "due to (4.1). Notice that $\\Gamma$ is an adaptive sampling map of the form defined above. Now let $y^{*}\\in B(\\mathcal{Y}^{*})$ be such that $\\left|y^{*}(y)\\right|=\\|y\\|_{\\mathcal{Y}}$ . Then, by (F.3), ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\big\\||F-\\mathcal{R}\\circ\\mathcal{L}(F)|\\big\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}^{2}\\geq\\big\\|y^{*}(F-\\mathcal{R}\\circ\\mathcal{L}(F))\\big\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathbb{R})}^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\gtrsim\\displaystyle\\sum_{i\\in I}|\\langle y^{*}(F-\\mathcal{R}\\circ\\mathcal{L}(F)),\\Psi_{i}\\rangle_{L_{\\mu}^{2}(\\mathcal{X};\\mathbb{R})}|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "By biorthogonality, $\\langle y^{*}(F),\\Psi_{i}\\rangle_{L_{\\mu}^{2}(\\mathcal{X};\\mathbb{R})}=c\\|y\\|_{\\mathcal{Y}}c_{i}=c\\cdot c_{i}$ , which implies that ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\||F-\\mathcal{R}\\circ\\mathcal{L}(F)\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\gtrsim\\|c-\\Delta\\circ\\Gamma(c)\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "where ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\Delta:\\mathbb{R}^{m}\\rightarrow\\mathbb{R}^{N},\\quad y\\mapsto\\Delta(y)=\\left(\\langle y^{*}(\\mathcal{R}(y\\cdot y)),\\Psi_{i}\\rangle_{L_{\\mu}^{2}(\\mathcal{X};\\mathbb{R})}/c\\right)_{i\\in I}.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Therefore, ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{F\\in\\mathcal{H}(b,\\iota)}\\|\\vert F-\\mathcal{R}\\circ\\mathcal{L}(F)\\|_{L_{\\mu}^{2}(\\mathcal{X};\\mathcal{Y})}\\gtrsim\\operatorname*{inf}_{\\Gamma,\\Delta\\atop\\operatorname*{supp}(c)\\subseteq I}\\|c-\\Delta\\circ\\Gamma(c)\\|_{2},\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "where the infimum is taken over all adaptive sampling maps $\\Gamma$ and reconstruction maps $\\Delta$ . Since $\\mathcal{L}$ and $\\mathcal{R}$ were arbitrary, we get ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\theta_{m}(\\pmb{b})\\gtrsim E_{m}^{\\mathbf{ada}}(B(\\pmb{b},I),\\ell_{N}^{2}),\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "where $B(\\pmb{b},I)=\\{\\pmb{z}\\in\\mathbb{R}^{|I|}:|z_{i}|\\leq b_{i},\\;i\\in I\\}$ and $\\ell_{N}^{2}=(\\mathbb{R}^{N},\\|\\cdot\\|_{2})$ . ", "page_idx": 59}, {"type": "text", "text": "Step 3: Derivation of the lower bounds. The next step is identical to the proof of Theorem 4.4 in [7]. This gives (F.1). \u53e3 ", "page_idx": 59}, {"type": "text", "text": "Proof of Theorem 4.1. We use Theorem F.1. For (i), we let $\\pmb{b}=(b_{i})_{i=1}^{\\infty}$ by defined by ", "page_idx": 59}, {"type": "equation", "text": "$$\nb_{i}=(2m)^{-1/p},\\;i=1,\\ldots,2m,\\qquad b_{i}=0,\\;i>2m.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "This sequence $b\\in\\ell_{\\mathsf{M}}^{p}(\\mathbb{N})$ with $\\left\\|\\pmb{b}\\right\\|_{p,\\mathsf{M}}=\\left\\|\\pmb{b}\\right\\|_{p}=1$ . Moreover, we have ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\sigma_{m}({\\pmb b})_{2}=2^{-1/p}m^{1/2-1/p}.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "For (ii) we let $\\begin{array}{r l r}{b}&{{}=}&{(b_{i})_{i=1}^{\\infty}}\\end{array}$ be defined by $\\begin{array}{r l r}{b_{i}}&{{}=}&{c_{p}(i\\log^{2}(i))^{-1/p}}\\end{array}$ , where $\\begin{array}{r l}{c_{p}}&{{}=}\\end{array}$ $\\begin{array}{r}{\\left(\\sum_{i=1}^{\\infty}1/(i\\log^{2}(i))\\right)^{-1/p}}\\end{array}$ . This sequence $b\\in\\ell_{\\mathsf{M}}^{p}(\\mathbb{N})$ with $\\|\\pmb{b}\\|_{p,\\mathsf{M}}=\\|\\pmb{b}\\|_{p}=1$ . Moreover, ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\sigma_{m}(b)_{2}^{2}\\geq c_{p}^{2}\\sum_{i=m+1}^{2m}(i\\log^{2}(i))^{-2/p}\\geq c_{p}^{2}\\cdot m\\cdot(2m\\log^{2}(2m))^{-2/p},\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "as required. ", "page_idx": 59}, {"type": "text", "text": "G Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Much as in the previous section, the proof of Theorem 4.2 is a consequence of the following result. ", "page_idx": 59}, {"type": "text", "text": "Theorem G.1. Suppose that the pushforward $\\varsigma$ in (A.I) is a tensor-product of a univariate probability measure. Then the term $\\tilde{\\theta}_{m}(\\boldsymbol{b})$ defined in (4.3) satisfies ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\tilde{\\theta}_{m}(b)\\gtrsim\\sigma_{m}(b)_{1}/\\log(m),\\quad\\forall b\\in\\ell^{1}(\\mathbb{N}),\\,\\,b\\geq\\mathbf{0},\\|b\\|_{1}\\leq1.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Proof of Theorem G.1. We proceed in a similar series of steps to those of the last proof. ", "page_idx": 59}, {"type": "text", "text": "Step 1: Setup. Let $\\pi:\\mathbb{N}\\rightarrow\\mathbb{N}$ be a bijection that gives a nonincreasing rearrangement of $^{b}$ , i.e., $b_{\\pi(1)}\\geq b_{\\pi(2)}\\geq\\cdot\\cdot\\cdot$ . Now, let $r\\in\\mathbb N$ be arbitrary and consider the index set ", "page_idx": 59}, {"type": "equation", "text": "$$\nI=I_{1}\\cup\\cdot\\cdot\\cdot\\cup I_{r},\\quad I_{l}=\\{\\pi((l-1)(m+1)+1),\\ldots,\\pi(l(m+1))\\}.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Notice that $|I|=r(m+1)$ . Define the matrix ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\mathbf{\\cal{A}}=\\left((\\iota(X_{i}))_{\\pi(j)}\\right)_{i,j=1}^{m,r(m+1)}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "and notice that we may write ", "page_idx": 59}, {"type": "equation", "text": "$$\nA=[A_{1}\\quad\\cdot\\cdot\\cdot\\quad A_{r}]\\,,\\quad\\mathrm{where}\\;A_{l}=\\left((\\iota(X_{i}))_{\\pi((l-1)(m+1)+j)}\\right)_{i,j=1}^{m,m+1}.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Let $\\sigma$ be the one-dimensional probability measure associated with $\\varsigma$ . Then notice that each $\\pmb{A}_{l}$ is a random matrix whose entries are drawn i.i.d. from $\\sigma$ . Since $\\sigma$ is supported in $[-1,1]$ , we deduce that ", "page_idx": 59}, {"type": "text", "text": "the $\\pmb{A}_{l}$ are independent subgaussian random matrices with the same distribution. Write $\\gamma$ for this distribution. Let $t_{1},\\ldots,t_{r}>0$ and $E_{l,t_{l}}$ be the event ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{E}_{l,t_{l}}=\\left\\{\\exists\\pmb{u}\\in N(\\pmb{A}_{l}):\\|\\pmb{u}\\|_{2}=1,\\;\\|\\pmb{u}\\|_{\\infty}<\\sqrt{t_{l}/(m+1)}\\right\\},\\quad l=1,\\ldots,r.}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "We will make a suitable choice of $t_{1},\\ldots,t_{r}$ later. ", "page_idx": 60}, {"type": "text", "text": "Step 2: Reduction to a discrete problem. Let ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{C}=\\left\\{c\\in\\mathbb{R}^{|I|}:|c_{i}|\\leq b_{i},\\;\\forall i\\in I,\\;c=\\left[\\!\\!\\begin{array}{c}{c_{1}}\\\\ {\\vdots}\\\\ {c_{r}}\\end{array}\\!\\!\\right],c_{l}\\in N(\\mathbf{A}_{l}),\\:l=1,\\ldots,r\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Notice that any $c\\in\\mathcal{C}$ also satisfies $\\pmb{c}\\,\\in\\,N(\\pmb{A})$ . Now let $\\textit{f}=\\textit{f}_{c}$ be as in (F.4) (we make the dependence on $^c$ explicit now for convenience). Let $\\pmb{x}\\in D$ . Then ", "page_idx": 60}, {"type": "equation", "text": "$$\nf_{c}({\\pmb x})=\\sqrt{3}c y\\sum_{i\\in I}c_{i}x_{i}.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "We deduce that ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\left(F_{c}(X_{i})\\right)_{i=1}^{m}=\\sqrt{3}c y A c=\\mathbf{0},\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "where $F_{c}=f_{c}\\circ\\iota$ . This implies that ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\big\\|F-{\\mathcal{R}}\\circ{\\mathcal{L}}(F)\\big\\|_{L_{\\mu}^{\\infty}({\\mathcal{X}};{\\mathcal{Y}})}=\\big\\|F-{\\mathcal{R}}\\big(\\{X_{i},0\\}_{i=1}^{m}\\big)\\big\\|_{L_{\\mu}^{\\infty}({\\mathcal{X}};{\\mathcal{Y}})},\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "where, for convenience, we let $\\mathcal{L}:F\\mapsto\\{X_{i},F(X_{i})\\}_{i=1}^{m}$ . Therefore, ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{F\\in\\mathcal{H}(b,\\iota)}\\|F-\\mathcal{R}\\circ\\mathcal{L}(F)\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}\\geq\\operatorname*{sup}_{c\\in\\mathcal{C}}\\|F_{c}-\\mathcal{R}\\big(\\{X_{i},0\\}_{i=1}^{m}\\big)\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Now observe that $F_{0}=0$ and $\\mathbf{0}\\in{\\mathcal{C}}$ . Hence ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{F\\in\\mathcal{H}(b,t)}{\\operatorname*{sup}}\\,\\|F-\\mathcal{R}\\circ\\mathcal{L}(F)\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}}\\\\ &{\\qquad\\qquad\\geq\\operatorname*{max}\\left\\{\\|\\mathcal{R}(\\{X_{i},0\\}_{i=1}^{m})\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})},\\underset{c\\in\\mathcal{C}}{\\operatorname*{sup}}\\,\\|F_{c}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}-\\|\\mathcal{R}(\\{X_{i},0\\}_{i=1}^{m})\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "For $a>0$ , the function $x\\mapsto\\operatorname*{max}\\{x,a-x\\}$ is minimized at $x=a/2$ and takes value $a/2$ there. We deduce that ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{F\\in\\mathcal{H}(b,\\iota)}\\|F-\\mathcal{R}\\circ\\mathcal{L}(F)\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}\\geq\\frac{1}{2}\\operatorname*{sup}_{c\\in\\mathcal{C}}\\|F_{c}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Now, by (A.I), ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\|F_{c}\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}=\\|f_{c}\\|_{L_{\\varsigma}^{\\infty}(D;\\mathcal{Y})}\\gtrsim\\|f_{c}\\|_{L_{\\sigma}^{\\infty}(D;\\mathcal{Y})}=\\sqrt{3}c\\operatorname*{sup}_{\\|x\\|_{\\infty}\\leq1}\\left|\\sum_{i\\in I}c_{i}x_{i}\\right|=\\sqrt{3}c\\|c\\|_{1}.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "With this in hand, we conclude that ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X_{1},\\ldots,X_{m}\\sim\\mu}\\operatorname*{sup}_{F\\in\\mathcal{H}(b,\\iota)}\\|F-\\mathcal{R}(\\{X_{i},F(X_{i})\\}_{i=1}^{m})\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}\\gtrsim\\mathbb{E}_{A_{1},\\ldots,A_{r}\\sim\\gamma}\\operatorname*{sup}_{c\\in\\mathcal{C}}\\|c\\|_{1}.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "We now use the definition of $\\mathcal{C}$ to write ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}_{X_{1},\\ldots,X_{m}\\sim\\mu}\\operatorname*{sup}_{F\\in\\mathcal{H}(b,\\iota)}\\|F-\\mathcal{R}(\\{X_{i},F(X_{i})\\}_{i=1}^{m})\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}}}\\\\ &{\\gtrsim\\displaystyle\\sum_{l=1}^{r}\\mathbb{E}_{A_{l}\\sim\\gamma}\\operatorname*{sup}_{\\stackrel{c_{l}\\in N(A_{l})}{|(c_{l})_{i}|\\leq b_{i},\\forall i\\in I_{l}}}\\!\\!\\!\\|c_{l}\\|_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Step 3: Bounding the expected error. Fix $l\\,=\\,1,\\,.\\,.\\,,r$ and suppose the event $E_{l,t_{l}}$ defined (G.1) occurs. Let $\\pmb{u}_{l}$ be the corresponding vector and define ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\pmb{c}_{l}=\\frac{b_{\\pi(l(m+1))}}{\\left\\|\\pmb{u}_{l}\\right\\|_{\\infty}}\\pmb{u}_{l},\\;l=1,\\ldots,r.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "By construction, we have that $c_{l}\\in N(A_{l})$ and $|(c_{l})_{i}|\\leq b_{i},\\forall i\\in I_{l}$ . We deduce that ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\pmb{c}_{l}\\in N(\\pmb{A}_{l})}\\|\\pmb{c}_{l}\\|_{1}\\geq\\frac{b_{\\pi(l(m+1))}\\|\\pmb{u}_{l}\\|_{1}}{\\|\\pmb{u}_{l}\\|_{\\infty}}.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Now observe that ", "page_idx": 61}, {"type": "equation", "text": "$$\n1=\\|\\pmb{u}_{l}\\|_{2}^{2}\\leq\\|\\pmb{u}_{l}\\|_{1}\\|\\pmb{u}_{l}\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Therefore, we get that ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbf{\\Phi}_{c_{l}\\in N(A_{l})}}\\quad\\|c_{l}\\|_{1}\\geq\\frac{b_{\\pi(l(m+1))}}{\\|\\mathbf{u}_{l}\\|_{\\infty}^{2}}\\geq\\frac{b_{\\pi(l(m+1))}(m+1)}{t_{l}}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "whenever the event $E_{l,t_{l}}$ occurs. Using the law of total expectation, we deduce that ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\mathbb{E}_{A_{l}\\sim\\gamma}\\operatorname*{sup}_{\\substack{c_{l}\\in N(A_{l})}}\\|c_{l}\\|_{1}\\geq\\frac{b_{\\pi(l(m+1))}(m+1)}{t_{l}}\\mathbb{P}(E_{l,t_{l}})\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "for any fixed $t_{l}>0$ . We now appeal to [75, Thm. 1.4]. This shows that ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(E_{l,t_{l}}^{c})\\leq c_{2}m^{2}\\exp(-t_{l}/c_{2}),\\quad\\forall t_{l}\\geq c_{1}\\log(m+1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "where $c_{1},c_{2}>0$ are universal constants. We may without loss of generality assume that $c_{2}\\geq c_{1}\\geq1$ . Now set $t_{l}=c_{2}\\log(2c_{2}m^{2})\\geq c_{1}\\log(m+1)$ . Hence ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\mathbb{P}(E_{l,t_{l}}^{c})\\le1/2.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "We deduce that $\\mathbb{P}(E_{t,t_{l}})>1/2$ and therefore ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\mathbb{E}_{A_{l}\\sim\\gamma}\\operatorname*{sup}_{\\substack{c_{l}\\in N(A_{l})}}\\|c_{l}\\|_{1}\\geq\\frac{b_{\\pi(l(m+1))}\\left(m+1\\right)}{2c_{2}\\log\\left(2c_{2}m^{2}\\right)}.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Now observe that ", "page_idx": 61}, {"type": "equation", "text": "$$\nb_{\\pi(l(m+1))}(m+1)\\geq b_{\\pi(l(m+1))}+\\cdot\\cdot\\cdot+b_{\\pi(l(m+1)+m)}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Substituting this into (G.3), we deduce that ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X_{1},\\ldots,X_{m}\\sim\\mu}\\operatorname*{sup}_{F\\in\\mathcal{H}(b,\\iota)}\\|F-\\mathcal{R}(\\{X_{i},F(X_{i})\\}_{i=1}^{m})\\|_{L_{\\mu}^{\\infty}(\\mathcal{X};\\mathcal{Y})}\\gtrsim\\frac{1}{\\log(2m)}\\sum_{i=m+1}^{r(m+1)+m}b_{\\pi(i)}.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Since $r$ was arbitrary, we may take the limit $r\\rightarrow\\infty$ . We now use the fact that ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\sigma_{m}(b)_{1}=b_{\\pi(m+1)}+b_{\\pi(m+2)}+\\cdot\\cdot\\cdot\\;.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "to obtain the result. ", "page_idx": 61}, {"type": "text", "text": "Proof of Theorem 4.2. Using Theorem G.1, statements (i) and (ii) are derived in exactly the same way as in the proof of Theorem 4.1 \u53e3 ", "page_idx": 61}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 62}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 62}, {"type": "text", "text": "Justification: We thoroughly discuss the main claims made in the abstract and introduction and the necessary assumptions to show them. Our main theoretical contributions directly address these claims. We also have several further remarks after these results to provide further context for our work. We also provide detailed numerical experiments showing that DNN architectures compatible with the setup for the theoretical results actually achieve the presented rates of approximation for challenging operator learning problems posed in Banach spaces in terms of the number of samples needed to achieve a given tolerance. ", "page_idx": 62}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 62}, {"type": "text", "text": "Justification: We discuss our main assumptions and their relative strengths in detail in a separate section, $\\S2.3$ . We also end the paper with a section discussing limitations. See $\\S6$ . ", "page_idx": 62}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 62}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 62}, {"type": "text", "text": "Justification: We have a detailed discussion of the assumptions needed to show our theoretical results in $\\S2.2\u20132.3$ . We provide further discussion of the results themselves in $\\S3{-}4$ to place the theoretical advancements in this work in the broader context of the operator learning literature. We provide full proofs of our results in the supplemental material. ", "page_idx": 62}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 62}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 62}, {"type": "text", "text": "Justification: We provide all of the necessary source code in the supplemental material. Furthermore, we provide a detailed discussion of the setup for the numerical experiments in $\\S\\mathrm{A}{-}\\mathrm{B}$ of the supplemental material. Given the code and description of the experiments, reproducing the experiments is straightforward. ", "page_idx": 62}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 62}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Justification: We provide all the necessary software to reproduce our experiments along with instructions for running the code to generate the results. ", "page_idx": 62}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 62}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Justification: We specify all of the necessary hyperparameters to obtain our experimental results as well as the optimizers used for training and the software used to generate our training and testing data. All of these are open source, no proprietary data was used in this work. ", "page_idx": 62}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 62}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 62}, {"type": "text", "text": "Justification: We plot not just the average error over all of the trials but also the (corrected) sample standard deviation of the transformed sample with shaded plots to provide an estimate of the variability in the runs. ", "page_idx": 62}, {"type": "text", "text": "8. Experiments Compute Resources ", "page_idx": 63}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: The experimental details are provided in the Appendix $\\S\\mathrm{A}.2$ . All computational resources are reported, including the type of workers, memory requirements, storage requirements, and time of execution. ", "page_idx": 63}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Justification: We have complied with the NeurIPS Code of Ethics in the preparation of this manuscript. No human subject data was used to generate the results for our numerical experiments and data-related concerns are not relevant to this work. ", "page_idx": 63}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 63}, {"type": "text", "text": "Justification: This work is primarily foundational, and the examples considered do not directly impact society. ", "page_idx": 63}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 63}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Justification: This work does not release any data or models that have a high risk for misuse. All code is open source and no proprietary data was used in this work. ", "page_idx": 63}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: The code submitted in the supplemental material to both generate the data for training and testing our models and generate the experimental results was written by the authors. No other code or datasets were used in the production of this work. ", "page_idx": 63}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: New code and data is included as a zip file as supplemental material. ", "page_idx": 63}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 63}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 63}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 63}, {"type": "text", "text": "Justification: We do not use crowdsourcing for our experiments and no research was conducted with human subjects. ", "page_idx": 63}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 63}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 63}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Justification: We do not use crowdsourcing for our experiments and no research was conducted with human subjects. ", "page_idx": 64}]