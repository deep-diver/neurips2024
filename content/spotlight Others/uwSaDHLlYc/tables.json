[{"figure_path": "uwSaDHLlYc/tables/tables_5_1.jpg", "caption": "Table 1: Comparison with SOTA dataset distillation baselines on CIFAR-10/100. Unless otherwise specified, we use the same network architecture for distillation and validation. Following the settings in their original papers, DC [54], DM [53], CAFE [42], MTT [1], and TESLA [2] use ConvNet-128 (small model). For SRe2L [46], ResNet-18 (large model) is used for synthesis and validation.", "description": "This table presents a comparison of the proposed DWA method against several state-of-the-art (SOTA) dataset distillation methods on the CIFAR-10 and CIFAR-100 datasets.  The table shows the top-1 accuracy achieved by models trained on the synthetic datasets generated by each method, using different numbers of images per class (ipc).  The results highlight DWA's superior performance compared to existing methods, particularly when using the larger ResNet-18 model.", "section": "4.1 Results & Discussions"}, {"figure_path": "uwSaDHLlYc/tables/tables_5_2.jpg", "caption": "Table 2: Comparison with SOTA dataset distillation baselines on Tiny-ImageNet and ImageNet-1K. Unless otherwise specified, we use the same network architecture for distillation and validation. Following the settings in their original papers, MTT [1], and TESLA [2] use ConvNet-128 (small model). For SRe2L [46], ResNet-18 (large model) is used for synthesis, and the distilled dataset is evaluated on ResNet-18, 50, and 101. \u2020 indicates MTT is performed on a 10-class subset of the full ImageNet-1K dataset.", "description": "This table compares the performance of the proposed Diversity-Driven Weight Adjustment (DWA) method against other state-of-the-art (SOTA) dataset distillation methods on the Tiny-ImageNet and ImageNet-1K datasets.  It shows the top-1 accuracy achieved by models trained on the synthetic datasets generated by each method, using different network architectures (ConvNet, ResNet-18, ResNet-50, ResNet-101) and varying numbers of images per class (ipc). The results highlight the superior performance of DWA, particularly on larger datasets and with more complex network architectures.", "section": "4.1 Results & Discussions"}, {"figure_path": "uwSaDHLlYc/tables/tables_7_1.jpg", "caption": "Table 3: An ablation study of DWA was conducted using various network architectures. The synthetic dataset was distilled by ResNet-18 from the CIFAR-100 dataset. We use X to denote the distilled dataset without weight adjustment, to denote the distilled dataset with random weight adjustment, and to represent Directed Weight Adjustment (DWA).", "description": "This ablation study evaluates the impact of different weight adjustment methods on the performance of dataset distillation using various network architectures.  It compares the results of using no weight adjustment (X), random weight adjustment (\u25cb), and the proposed directed weight adjustment (DWA)(\u2713) method on the CIFAR-100 dataset with ResNet-18 as the backbone.  The results are shown for different image-per-class (ipc) settings: 10 and 50.", "section": "4.2 Ablation Study"}, {"figure_path": "uwSaDHLlYc/tables/tables_7_2.jpg", "caption": "Table 2: Comparison with SOTA dataset distillation baselines on Tiny-ImageNet and ImageNet-1K. Unless otherwise specified, we use the same network architecture for distillation and validation. Following the settings in their original papers, MTT [1], and TESLA [2] use ConvNet-128 (small model). For SRe2L [46], ResNet-18 (large model) is used for synthesis, and the distilled dataset is evaluated on ResNet-18, 50, and 101. \u2020 indicates MTT is performed on a 10-class subset of the full ImageNet-1K dataset.", "description": "This table compares the performance of the proposed DWA method against other state-of-the-art (SOTA) dataset distillation methods on Tiny-ImageNet and ImageNet-1K datasets.  It shows the top-1 classification accuracy achieved using models trained on the synthesized datasets.  The table highlights the superior performance of DWA, especially on the larger ImageNet-1K dataset, across different model sizes (ResNet-18, 50, and 101).  The choice of backbone network for different methods is also specified.", "section": "4.1 Results & Discussions"}, {"figure_path": "uwSaDHLlYc/tables/tables_8_1.jpg", "caption": "Table 2: Comparison with SOTA dataset distillation baselines on Tiny-ImageNet and ImageNet-1K. Unless otherwise specified, we use the same network architecture for distillation and validation. Following the settings in their original papers, MTT [1], and TESLA [2] use ConvNet-128 (small model). For SRe2L [46], ResNet-18 (large model) is used for synthesis, and the distilled dataset is evaluated on ResNet-18, 50, and 101. \u2020 indicates MTT is performed on a 10-class subset of the full ImageNet-1K dataset.", "description": "This table compares the performance of the proposed DWA method against state-of-the-art (SOTA) dataset distillation methods on Tiny-ImageNet and ImageNet-1K datasets.  It shows the Top-1 classification accuracy achieved by models trained on the distilled datasets generated by each method.  Different network architectures (ConvNet-128 and ResNet-18, 50, 101) are used for evaluation, highlighting the generalizability of the methods. Note that MTT results are based on a 10-class subset of ImageNet-1K.", "section": "4.1 Results & Discussions"}, {"figure_path": "uwSaDHLlYc/tables/tables_14_1.jpg", "caption": "Table 6: Hyper-parameter settings for CIFAR-10/100.", "description": "This table details the hyperparameters used for both the distillation and validation phases of the CIFAR-10/100 dataset experiments.  For distillation, it shows the number of iterations, batch size, optimizer (Adam), learning rate (using cosine decay), and augmentation strategy.  The validation settings similarly list the number of epochs, batch size, optimizer (AdamW with weight decay), learning rate (cosine decay), augmentation techniques (RandomCrop and RandomHorizontalFlip), and temperature.  The table also specifies the values used for \u03bbvar (variance regularization strength), and the parameters \u03c1 and K related to the directed weight adjustment method.", "section": "4 Experiments"}, {"figure_path": "uwSaDHLlYc/tables/tables_14_2.jpg", "caption": "Table 7: Hyper-parameter settings for Tiny-ImageNet.", "description": "This table presents the hyperparameter settings used for both the distillation and validation phases of the Tiny-ImageNet experiments.  It shows settings for the number of iterations/epochs, batch size, optimizer, learning rate, augmentation techniques, the decoupled variance coefficient (\u03bb_var), and the parameters \u03c1 and K used in the directed weight adjustment (DWA) method. These hyperparameters were crucial in fine-tuning the model's performance for this specific dataset.", "section": "4 Experiments"}, {"figure_path": "uwSaDHLlYc/tables/tables_14_3.jpg", "caption": "Table 7: Hyper-parameter settings for Tiny-ImageNet.", "description": "This table details the hyperparameters used during both the distillation and validation phases for the Tiny-ImageNet dataset.  It specifies settings for the number of iterations, batch size, optimizer (Adam with beta1 and beta2 parameters), learning rate (using cosine decay), augmentations (random resized crop and random horizontal flip), the decoupled variance coefficient (\u03bb_var), and the parameters \u03c1, k, and K used in the weight adjustment.  The validation settings differ slightly, employing AdamW with weight decay and a different learning rate, while other settings like augmentation are kept the same.", "section": "4 Experiments"}, {"figure_path": "uwSaDHLlYc/tables/tables_15_1.jpg", "caption": "Table 9: Generalization to a vision transformer-based model DeiT-Tiny.", "description": "This table presents the results of generalizing the proposed DWA method to a vision transformer-based model, DeiT-Tiny.  It compares the performance of DWA and the baseline method, SRe2L, when using different backbone networks (ResNet-18, ResNet-50, and ResNet-101) for the ImageNet-1K dataset.  The table demonstrates the superior performance of DWA across different architectures.", "section": "4.2 Ablation Study"}, {"figure_path": "uwSaDHLlYc/tables/tables_15_2.jpg", "caption": "Table 10: Application to continual learning task.", "description": "This table presents the results of applying the proposed DWA method and the baseline SRe2L method to a continual learning task using the CIFAR-100 dataset. The dataset is divided into five tasks, each with 20 images per class. The table shows the accuracy achieved at each stage of the continual learning process, demonstrating the superior performance of the DWA method in retaining knowledge across tasks.", "section": "4.2 Ablation Study"}, {"figure_path": "uwSaDHLlYc/tables/tables_15_3.jpg", "caption": "Table 1: Comparison with SOTA dataset distillation baselines on CIFAR-10/100. Unless otherwise specified, we use the same network architecture for distillation and validation. Following the settings in their original papers, DC [54], DM [53], CAFE [42], MTT [1], and TESLA [2] use ConvNet-128 (small model). For SRe2L [46], ResNet-18 (large model) is used for synthesis and validation.", "description": "This table compares the performance of the proposed DWA method against other state-of-the-art (SOTA) dataset distillation methods on CIFAR-10 and CIFAR-100 datasets.  It shows the top-1 accuracy achieved by models trained on the synthetic datasets generated by each method.  Different image-per-class (ipc) settings are used for a comprehensive evaluation, and the table notes the network architecture used for each method.", "section": "4.1 Results & Discussions"}]