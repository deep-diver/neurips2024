[{"type": "text", "text": "Improving the Worst-Case Bidirectional Communication Complexity for Nonconvex Distributed Optimization under Function Similarity ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kaja Gruntkowska Alexander Tyurin Peter Richtarik KAUST\\* KAUST\\* AIRI, Skoltech+ KAUST\\* ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Effective communication between the server and workers plays a key role in distributed optimization. In this paper, we focus on optimizing communication, uncovering inefficiencies in prevalent downlink compression approaches. Considering first the pure setup where the uplink communication costs are negligible, we introduce MARINA-P, a novel method for downlink compression, employing a collection of correlated compressors. Theoretical analysis demonstrates that MARINA-P with permutation compressors can achieve a server-to-worker communication complexity improving with the number of workers, thus being provably superior to existing algorithms. We further show that MARINA-P can serve as a starting point for extensions such as methods supporting bidirectional compression: we introduce M3, a method combining MARINA-P with uplink compression and a momentum step, achieving bidirectional compression with provable improvements in total communication complexity as the number of workers increases. Theoretical findings align closely with empirical experiments, underscoring the efficiency of the proposed algorithms. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In federated learning (McMahan et al., 2017; Koneeny et al., 2016) and large-scale machine learning (Ramesh et al., 2021; OpenAI, 2023), a typical environment consists of multiple devices working together to train a model. Facilitating this collaborative process requires the transmission of substantial information (e.g., gradients, current model) between these devices. In the centralized framework, communication takes place via a server. As a result, practical challenges arise due to the large size of machine learning models and network speed limitations, potentially creating a communication bottleneck (Kairouz et al., 2021; Wang et al., 2023a). One possible strategy to reduce this communication burden is to use lossy compression (Seide et al., 2014; Alistarh et al., 2017). Our paper focuses on this research direction. ", "page_idx": 0}, {"type": "text", "text": "We consider the following nonconvex distributed optimization task: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{x\\in\\mathbb{R}^{d}}{\\operatorname*{min}}\\left\\{f(x):=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}(x)\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ is the vector of parameters of the model, $n$ is the number of workers and $f_{i}\\;:\\;\\mathbb{R}^{d}\\rightarrow$ $\\mathbb{R},\\,i\\,\\in\\,[n]\\,:=\\,\\{1,\\dots,n\\}$ are smooth nonconvex functions. We investigate the scenario where the functions $f_{i}$ are stored on $n$ distinct workers, each directly connected to the server via some ", "page_idx": 0}, {"type": "text", "text": "communication port (Kairouz et al., 2021). At present, we operate under the following generic assumptions: ", "page_idx": 1}, {"type": "text", "text": "Assumption 1.1. The function $f$ is $L$ -smooth, i.e., $\\|\\nabla f(x)-\\nabla f(y)\\|\\leq L\\,\\|x-y\\|\\,\\forall x,y\\in\\mathbb{R}^{d}.$ Assumption 1.2. There exists $f^{*}\\in\\mathbb{R}$ such that $f(x)\\geq f^{*}\\,\\forall x\\in\\mathbb{R}^{d}$ ", "page_idx": 1}, {"type": "text", "text": "In the nonconvex world, our goal is to find a (possibly) random point $\\bar{x}$ such that $\\mathbb{E}[\\|\\nabla f(\\bar{x})\\|^{2}]\\leq\\varepsilon$ We refer to such a point an $\\varepsilon,$ -stationary point. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Before we discuss more advanced optimization methods, let us consider the simplest baseline: the gradient descent (GD) (Lan, 2020), which iteratively performs updates $x^{t+1}=^{\\star}x^{t}-\\gamma\\nabla f(x^{t})=$ $\\begin{array}{r}{\\bar{x}^{t}-\\gamma/{n}\\sum_{i=1}^{n}\\nabla f_{i}(x^{t})}\\end{array}$ In the distributed setting, the method can be implemented as follows: each worker calculates $\\nabla f_{i}(x^{t})$ and sends it to the server where the gradients are aggregated, after which the server takes the step and broadcasts $x^{t+1}$ back to the workers. With step size $\\gamma={^1\\!\\!\\big/L}$ , GD finds an $\\varepsilon,$ -stationary point after $\\mathcal{O}\\left(\\delta^{0}L/\\varepsilon\\right)$ steps, where $\\delta^{0}:=f(x^{0})-f^{*}$ for a starting point $x^{0}$ . Since at each step the workers and the server send $\\Theta(d)$ coordinates/bits, the worker-to-server (w2s, uplink) and server-to-worker (s2w, downlink) communication costs are ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{O}\\left(\\frac{d\\delta^{0}L}{\\varepsilon}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Definition 1.3. The worker-to-server $(w2s)$ and server-to-worker $(s2w)$ communication complexities of a method are the expected number of coordinates/floats that a worker sends to the server and that the server sends to a worker, respectively, to find an $\\varepsilon,$ -solution.Thetotal communicationcomplexity is the sum of these complexities. ", "page_idx": 1}, {"type": "text", "text": "Unbiased compressors. In this work, to perform lossy compression, we employ mappings from the following family: ", "page_idx": 1}, {"type": "text", "text": "Definition 1.4. A stochastic mapping $\\mathcal{C}\\,:\\,\\mathbb{R}^{d}\\to\\mathbb{R}^{d}$ is an unbiased compressor if there exists $\\omega\\ge0$ such that ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\mathscr{C}(x)\\right]=x,\\,\\mathbb{E}\\left[\\left\\|\\mathscr{C}(x)-x\\right\\|^{2}\\right]\\leq\\omega\\left\\|x\\right\\|^{2}\\,\\forall x\\in\\mathbb{R}^{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "We denote the family of such mappings by $\\mathbb{U}(\\omega)$ . A canonical example is the $\\mathrm{Rand}K\\in\\mathbb{U}(d/K-1)$ sparsifier, which preserves $K$ random coordinates of a vector scaled by $d/{\\cal K}$ (Beznosikov et al., 2020). More examples can be found in Wangni et al. (2018); Beznosikov et al. (2020); Szlendak et al. (2021); Horvath et al. (2022). A larger family of compressors, called biased compressors, also exists (see Section B). In this paper, we implicitly assume that compressors are mutually independent across iterations of algorithms. ", "page_idx": 1}, {"type": "text", "text": "Worker-to-server compression scales with $n$ . Many previous works ignore the s2w communication costs and focus solely on w2s compression, assuming that broadcasting is free. For nonconvex objective functions, the current state-of-the-art w2s communication complexities are achieved by the MARINA and DASHA methods (Gorbunov et al., 2021; Szlendak et al., 2021; Tyurin and Richtarik, 2023b). Here, two additional assumptions are needed: ", "page_idx": 1}, {"type": "text", "text": "Assumption 1.5. The function $f_{i}$ is $L_{i}$ -smooth. We define $\\begin{array}{r}{\\widehat{L}^{2}:=\\;\\frac{1}{n}\\sum_{i=1}^{n}L_{i}^{2}}\\end{array}$ and $L_{\\operatorname*{max}}~:=$ $\\operatorname*{max}_{i\\in[n]}L_{i}$ ", "page_idx": 1}, {"type": "text", "text": "Assumption 1.6. For all $\\mathcal{C}\\in\\mathbb{U}(\\omega)$ , all calls of $\\mathcal{C}$ are mutually independent.4 ", "page_idx": 1}, {"type": "text", "text": "Under Assumptions 1.1, 1.2, 1.5, 1.6, and considering the Rand $K$ compressor with $K\\leq{d\\!\\!\\left/{\\sqrt{n}}\\right.\\!\\!}$ as an example, the w2s communication complexity of both methods is ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underbrace{K}_{\\mathrm{\\#\\;of\\;sent\\;coord.}}\\times\\underbrace{\\mathcal{O}\\left(\\frac{\\delta^{0}}{\\varepsilon}(L+\\frac{\\omega}{\\sqrt{n}}\\widehat{L})\\right)}_{\\mathrm{\\#\\;of\\;iterations}}=\\mathcal{O}\\left(\\frac{d\\delta^{0}\\widehat{L}}{\\sqrt{n}\\varepsilon}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where we use the facts that $L\\leq\\widehat{L}$ and $\\omega\\,=\\,d\\big/\\kappa\\,-\\,1$ for Rand $K$ .The key observation is that when comparing (2) and (4), one sees that (4) can be $\\sqrt{n}$ times smaller if $\\overrightharpoon{L}\\approx L$ . Consequently, the communication complexity of MARINA/DASHA scales with the number of workers $n$ , and can provably improve the worker-to-server communication complexity $\\mathcal{O}\\left(d\\delta^{0}L/\\varepsilon\\right)$ achieved by GD. ", "page_idx": 2}, {"type": "text", "text": "Server-to-worker compression does not scale with $n$ . In certain applications, the significance of s2w communication cannot be ignored. In 4G LTE and 5G networks, w2s and s2w communication speeds can be almost the same (Huang et al., 2012) or differ by at most a factor of 10 (Narayanan et al., 2021). Although important, this issue is often overlooked and that is why it is the s2w communication that this work places a central emphasis on. ", "page_idx": 2}, {"type": "text", "text": "There exist many papers using communication compression techniques to reduce the s2w communication (Zheng et al., 2019; Liu et al., 2020; Philippenko and Dieuleveut, 2021; Fatkhullin et al., 2021; Gruntkowska et al., 2023; Tyurin and Richtarik, 2023a). However, to the best of our knowledge, under Assumptions 1.1, 1.2, 1.5, and 1.6, in the worst case, all previous theoretical s2w communication guarantees are greater or equal to (2). As an example, let us consider the result from Gruntkowska et al. (2023)[Theorem E.3]. If the server employs operators from $\\mathbb{U}(\\omega)$ and we ignore w2s compression, the method from Gruntkowska et al. (2023) converges in $\\mathcal{O}\\left((\\omega{+}1)\\delta^{0}L/\\varepsilon\\right)$ iterations. Thus, with RandK, the $\\mathrm{s}2\\mathrm{w}$ communication complexity is $\\mathcal{O}\\left(K\\times(\\omega\\!+\\!1)\\delta^{0}L\\middle/\\varepsilon\\right)=\\mathcal{O}\\left(d\\delta^{0}L\\middle/\\varepsilon\\right)$ . Another method, called CORE, proposed by Yue et al. (2023), achieves s2w and w2s communication complexities equal to $\\mathcal{O}\\left(r_{1}\\overset{.}{(}f)\\delta^{\\frac{\\mathbf{1}}{0}}L_{\\left/\\varepsilon\\right.}\\right)$ , where $r_{1}(f)$ is a uniform upper bound of the trace of the Hessian. When $r_{1}(f)\\leq d L$ , CORE can improve on GD. However, this complexity does not scale with $n$ and requires an additional assumption about the Hessian of $f$ ", "page_idx": 2}, {"type": "text", "text": "2 Contributions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In our work, we aim to investigate whether the server-to-worker and total communication complexities (2) of the vanilla GD method can be improved. We make the following contributions: ", "page_idx": 2}, {"type": "text", "text": "1. We start by proving the impossibility of devising a method where the server communicates with the workers using unbiased compressors $\\mathbb{U}(\\omega)$ (or biased compressors from Section B) and achieves an iteration rate faster than $\\Omega\\left((\\omega\\!+\\!1)L\\delta^{0}\\!/\\varepsilon\\right)$ (Theorem 3.1) under Assumptions 1.1, 1.2 and 1.6. This result provides a lower bound for any method that applies such compressors to vectors sent from the server to the workers in every iteration. Moreover, we prove a more general iteration lower bound of $\\Omega\\left({{\\left(\\omega+1\\right)}L\\delta^{0}}/{\\varepsilon}\\right)$ for all methods where the server zeroes out a coordinate with probability $^1\\!/\\!(\\omega\\!+\\!1)$ (see Remark 3.2). ", "page_idx": 2}, {"type": "text", "text": "2. In view of this result, it is clear that an extra assumption is needed to break the lower bound $\\Omega\\left((\\omega\\!+\\!1)L\\delta^{0}\\!/\\varepsilon\\right)$ . In response, we introduce a novel assumption termed \u201cFunctional $\\left(L_{A},L_{B}\\right)$ Inequality\u201d (see Assumption 4.2). We prove that this assumption is relatively weak and holds, for instance, under the local smoothness of the functions $f_{i}$ (see Assumption 1.5). ", "page_idx": 2}, {"type": "text", "text": "3.We develop a new method for downlink compression, MARINA-P, and show that under our new assumption, along with Assumptions 1.1 1.2, and 1.6, it can achieve the iteration rate of ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{O}\\left(\\frac{\\delta^{0}L}{\\varepsilon}+\\frac{\\delta^{0}L_{A}(\\omega+1)}{\\varepsilon}+\\frac{\\delta^{0}L_{B}(\\omega+1)}{\\sqrt{n}\\varepsilon}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "(see Theorem D.1 with $p=\\,^{1}\\!/(\\omega\\!+\\!1)\\;+$ Lemma A.5). Notably, when $L_{A}$ is small and $n\\gg1$ , this complexity is provably superior to $\\Theta\\big((\\delta^{0}L(\\omega\\!+\\!1))\\big/\\varepsilon\\big)$ and the complexities of the previous compressed methods. In this context, $L_{A}$ serves as a measure of the similarity between the functions $f_{i}$ , and can be bounded by the \u201cvariance\u201d of the Hessians of the functions $f_{i}$ (see Theorem 4.8). Thus, MARINA-P is the first method whose iteration complexity can provably improve with the number of workers $n$ ", "page_idx": 2}, {"type": "text", "text": "4. Moreover, MARINA-P can achieve the s2w communication complexity of ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{O}\\left(\\frac{d\\delta^{0}L}{n\\varepsilon}+\\frac{d\\delta^{0}L_{A}}{\\varepsilon}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "When $L_{A}$ is small and $n\\gg1$ , this communication complexity is provably superior to (2) and the communication complexities of the previous compressed methods. ", "page_idx": 2}, {"type": "text", "text": "5. Our theoretical improvements can be combined with techniques enhancing the $w2s$ communication complexities. In particular, by combining MARINA-P with MARINA (Gorbunov et al. 2021) and adding the crucial momentum step, we develop a new method, M3, that guarantees a total communicationcomplexity $(\\mathrm{s}2\\mathrm{w}+\\mathrm{w}2\\mathrm{s})$ of ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{O}\\left(\\frac{d\\delta^{0}L_{\\mathrm{max}}}{n^{1/3}\\varepsilon}+\\frac{d\\delta^{0}L_{A}}{\\varepsilon}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "When $n\\gg1$ and in the close-to-homogeneous regime, i.e., when $L_{A}$ is small, this complexity is better than (2) and the complexities of the previous bidirectionally compressed methods. ", "page_idx": 3}, {"type": "text", "text": "6. Our theoretical results are supported by numerical experiments (see Section F) ", "page_idx": 3}, {"type": "text", "text": "3  Lower Bound under Smoothness ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Let us first investigate the possibility of improving the iteration complexity $\\mathcal{O}\\left((\\omega\\!+\\!1)\\delta^{0}L\\middle/\\varepsilon\\right)$ underAssumptions 1.1,1.2 and 1.6. In Section G, we consider a family of methods that include those proposed in Zheng et al. (2019); Liu et al. (2020); Philippenko and Dieuleveut (2021); Fatkhullin et al. (2021); Gruntkowska et al. (2023), where the server communicates with workers using unbiased/biased compressors, and establish that ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1 (Slightly Less Formal Reformulation of Theorem G.5). Under Assumptions 1.1, 1.2 and 1.6, all methods in which the server communicates with clients using different and independent unbiasedcompressorsfrom $\\mathbb{U}\\left(\\omega\\right)$ and sends one compressed vector to each worker cannot converge before $\\Omega\\left((\\omega\\!+\\!1)L\\delta^{0}\\!/\\varepsilon\\right)$ iterations. ", "page_idx": 3}, {"type": "text", "text": "Remark 3.2. The theorem remains applicable to biased compressors $\\mathbb{B}\\left(\\alpha\\right)$ (see Section B) with a lower bound of $\\Theta\\left(L\\delta^{0}/\\alpha\\varepsilon\\right)$ . This is because if $\\mathcal{C}\\in\\mathbb{U}(\\omega)$ , then $(\\omega+1)^{-1}\\mathcal{C}\\in\\mathbb{B}\\left((\\omega+1)^{-1}\\right)$ . We also establish a more general result (Theorem G.4): \u201call methods in which the server zeroes out a coordinate with probability $\\leq p$ independently across iterations cannot converge before $\\Omega\\left(L\\delta^{0}/p\\varepsilon\\right)$ iterations.\" ", "page_idx": 3}, {"type": "text", "text": "This lower bound is tight up to a constant factor. For instance, under exactly the same assumptions, the EF21-P mechanism from Gruntkowska et al. (2023) converges after $\\Theta\\left((\\omega\\!+\\!1)L\\delta^{0}\\!/\\varepsilon\\right)$ iterations. Unlike (4), this convergence rate does not scale with $n$ , and Theorem 3.1 leaves no room for improvement. Consequently, breaking the lower bound requires an additional assumption about the structure of the problem. Before presenting our candidate assumption, we first introduce the ingredients needed to leverage it to the fullest extent: our novel downlink compression method and the type of compressors we shall employ. ", "page_idx": 3}, {"type": "text", "text": "4 The MARINA-P Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Let us first recall the MARINA method (Gorbunov et al., 2021; Szlendak et al., 2021): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{x^{t+1}=x^{t}-\\gamma g^{t},\\quad\\quad c^{t}\\sim\\mathrm{Bernoulli}(p)}\\\\ {g_{i}^{t+1}=\\left\\{\\nabla f_{i}(x^{t+1})\\right.}&{\\mathrm{if~}c^{t}=1,}\\\\ {g^{t}+\\mathcal{C}_{i}^{t}(\\nabla f_{i}(x^{t+1})-\\nabla f_{i}(x^{t}))}&{\\mathrm{if~}c^{t}=0}\\\\ {g^{t+1}=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}g_{i}^{t+1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $g^{0}=\\nabla f(x^{0})$ . Motivated by MARINA, we design its primal counterpart, MARINA-P (Algorithm 1), operating in the primal space of the model parameters, as outlined in (6). ", "page_idx": 3}, {"type": "text", "text": "At each iteration of MARINA-P, the workers calculate $\\nabla f_{i}(w_{i}^{t})$ and transmit it to the server. The server then averages the gradients and updates the global model $x^{t}$ . Subsequently, with some (typically small) probability $p$ , the master sends the non-compressed vector $x^{t+1}$ to all workers. Otherwise, the $i^{\\mathrm{th}}$ worker receives a compressed vector $\\mathcal{C}_{i}^{t}(x^{t+1}-x^{t})$ . Each worker then uses the received message to compute $w_{i}^{t+1}$ locally. Importantly, $\\mathcal{C}_{1}^{t}(x^{t+1}-x^{t}),\\ldots,\\mathcal{C}_{n}^{t}(x^{t+1}-x^{t})$ can differ, and this distinction will form the basis of our forthcoming advancements. ", "page_idx": 3}, {"type": "image", "img_path": "gkJ5nBIOU4/tmp/81ef43ee8f8cdad2c79e4fb760baeedcf409078d1ec1a84ffcd4a346b3e22172.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "We denote $\\begin{array}{r}{w^{t}:=1/{n}\\sum_{i=1}^{n}w_{i}^{t}}\\end{array}$ See the implementation in Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "Comparing (5) and (6), MARINA-P and MARINA are dual methods: both learn control variables $(w_{i}^{t}$ and $\\bar{g}_{i}^{t}$ ), compressthe differences $(x^{t+1}-x^{t}$ and $\\nabla f_{i}(x^{t+1})-\\nabla f_{i}(x^{t}))$ and with some probability $p$ send non-compressed vectors $\\textstyle x^{t+1}$ and $\\nabla f_{i}(x^{t+1}))$ However,unlikeMARINA,whichcompresss vectors sent from workers to server and operates in the dual space of gradients, MARINA-P compresses messages sent from server to workers and operates in the primal space of arguments. ", "page_idx": 4}, {"type": "text", "text": "Let us take $\\mathrm{Rand}K\\,\\in\\,\\mathbb{U}(d/\\kappa\\,-\\,1)$ as an example. If we set $p\\,=\\,(\\omega+1)^{-1}\\,=\\,K/d$ to balance heavy communications of $\\boldsymbol{x}^{t+1}$ and light communications of $\\mathcal{C}_{i}^{t}$ in (6), MARINA-P averages sending $p d+(1-p)K\\leq2K$ coordinates per iteration. Then, the lower bound from Theorem G.4 implies that at least $\\Omega\\left((\\omega\\!+\\!1)\\delta^{0}L\\middle/\\varepsilon\\right)$ iterations of the algorithm are needed. ", "page_idx": 4}, {"type": "text", "text": "At first glance, it seems that MARINA-P does not offer any extra benefits compared to previous methods, and that is true - we could not expect to break the lower bound. However, as we shall soon see, under an extra assumption, MARINA-P can achieve a communication complexity that improves With $n$ ", "page_idx": 4}, {"type": "text", "text": "4.1   Three ways to compress ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Existing algorithms performing s2w compression share a common characteristic: at each iteration, the server broadcasts the same message to all workers (Zheng et al., 2019; Liu et al., 2020; Fatkhullin et al., 2021; Gruntkowska et al., 2023; Tyurin and Richtarik, 2023a).5 In contrast, in w2s compression methods, each worker sends to the server a different message, specific to the data stored on that particular device. An analogous approach can be taken in the s2w communication: intuitively, sending $n$ distinct messages would convey more information, potentially leading to theoretical improvements. This indeed proves to be the case. While the usual approach of the server broadcasting the same vector to all clients does not lead to an improvement over (2), allowing these vectors to differ enables a well-crafted method to achieve communication complexity that improves with $n$ (see CorollaryD.4). ", "page_idx": 4}, {"type": "text", "text": "In Appendix A we provide a detailed discussion of the topic and compare the theoretical complexities of MARINA-P when the server employs three different compression techniques: a) uses one compressor and sends the same vector to all clients, b) uses a collection of independent compressors, or c) uses a collection of correlated compressors. We now turn to presenting the technique that gives the best theoretical s2w communication complexity out of these, namely the use of a set of correlated compressors. ", "page_idx": 4}, {"type": "text", "text": "4.2  Recap: permutation compressors Perm $K$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Szlendak et al. (2021) propose compressors that will play a key role in our new theory. For clarity of presentation, we shall assume that $d\\geq n$ and $n|d$ ", "page_idx": 5}, {"type": "text", "text": "Definition 4.1 (Perm K (for $d\\geq n$ and $n|d)$ ).Assume that $d\\geq n$ and $d=q n$ , where $q\\in\\mathbb{N}_{>0}$ . Let ${\\boldsymbol\\pi}=(\\pi_{1},\\ldots,\\pi_{d})$ be a random permutation of $\\{1,\\ldots,d\\}$ . For all $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ and each $i\\in\\{1,2,\\dots,n\\}$ wedefine ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal C_{i}(x):=n\\times\\sum_{j=q(i-1)+1}^{q i}x_{\\pi_{j}}e_{\\pi_{j}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Unpacking this definition: when the server compresses a vector using a $\\mathrm{Perm}K$ compressor, it randomly partitions its coordinates across the workers, so that each client receives a sparse vector containing a random subset of entries of the input vector. Like Rand $K$ , Perm $K$ is also a sparsifier. However, unlike Rand $K$ , it does not allow flexibility in choosing $K$ , as it is fixed to $d/n$ . Furthermore, it can be shown (Lemma A.6) that $\\mathcal{C}_{i}\\in\\mathbb{U}(n-1)$ for all $i\\in[n]$ \uff1a ", "page_idx": 5}, {"type": "text", "text": "An appealing property of Perm $K$ is the fact that ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\frac{1}{n}}\\sum_{i=1}^{n}{\\mathcal{C}}_{i}(x)=x\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for all $x\\in\\mathbb{R}^{d}$ deterministically. Here, it is important to note that by design, compressors $\\mathcal{C}_{i}$ from Definition 4.1 are correlated, and do not satisfy Assumption 1.6. This correlation proves advantageous - Szlendak et al. (2021) show that MARINA with Perm $K$ compressors performs provably better than with i.i.d. Rand $K$ compressors. ", "page_idx": 5}, {"type": "text", "text": "4.3  Warmup: homogeneous quadratics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We are finally ready to present our first result showing that the $\\scriptstyle\\mathrm{s}2\\mathbf{w}$ communication complexity can scale with the number of workers $n$ . To explain the intuition behind our approach, let us consider the simplest (and somewhat impractical) choice of functions $f_{i}$ - the homogeneous quadratics: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{i}(\\boldsymbol{x})=\\frac{1}{2}\\boldsymbol{x}^{\\top}\\mathbf{A}\\boldsymbol{x}+\\boldsymbol{b}^{\\top}\\boldsymbol{x}+\\boldsymbol{c},\\quad\\boldsymbol{i}\\in[n],}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{A}\\in\\mathbb{R}^{d\\times d}$ is a symmetric but not necesarily positive semidefnite matrix, $b\\in\\ensuremath{\\mathbb{R}}^{d}$ and $c\\in\\mathbb{R}$ We now investigate the operation of MARINA-P with Perm $K$ compressors. With probability $p$ ,we have $\\boldsymbol{w}^{t+1}=\\boldsymbol{x}^{t+1}$ . Otherwise $\\begin{array}{r}{w^{t+1}=w^{t}+\\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{C}_{i}^{t}(x^{t+1}-x^{t})\\overset{(7)}{=}x^{t+1}+(w^{t}-x^{t}).}\\end{array}$ Hence,if we initialize $w_{i}^{0}=x^{0}$ for all $i\\in[n]$ , an inductive argument shows that $w^{t}=x^{t}$ deterministically for all $t\\geq0$ . Then, substituting the gradients of $f_{i}$ to (6), one gets ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{g^{t}=\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}(\\mathbf{A}w_{i}^{t}+b)=\\mathbf{A}w^{t}+b=\\mathbf{A}x^{t}+b=\\nabla f(x^{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for all $t\\geq0$ . Therefore, MARINA-P with $\\mathrm{Perm}K$ compressor in this setting is essentially a smart implementation of vanilla GD! Indeed, for $p\\leq{^{1}\\!/n}$ , MARINA-P with Perm $K$ sends on average $\\leq\\left.2d\\right/\\!n$ coordinates to each worker, so the s2w communication complexity is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\mathrm{\\partial}\\mathrm{\\partial}\\mathbf{\\boldsymbol{d}}}{\\mathrm{\\partial}n}\\times\\underbrace{\\boldsymbol{\\mathcal{O}}\\left(\\frac{\\delta^{0}L}{\\varepsilon}\\right)}_{\\sf{G D}\\,\\mathrm{{rate}}}=\\mathcal{O}\\left(\\frac{d\\delta^{0}L}{n\\varepsilon}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which is $n$ times smaller than in (2)! ", "page_idx": 5}, {"type": "text", "text": "4.4  Functional $\\left(L_{A},L_{B}\\right)$ Inequality ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "From the discussion in Section 3, we know that to improve (2), an extra assumption about the structure of the problem is needed. Building on the example from Section 4.3, we introduce the Functional $\\left(L_{A},L_{B}\\right)$ Inequality. ", "page_idx": 5}, {"type": "table", "img_path": "gkJ5nBIOU4/tmp/8b94cb8c62d8a12a2a76b90fa88d1b1876d9d4b3141b614bd162fc6103dd343a.jpg", "table_caption": ["Table 1: The worst case communication complexities to find an $\\varepsilon\\cdot$ -stationary point. For simplicity, we compare the complexities with non-homogeneous quadratics: $\\begin{array}{r}{f_{i}(x)=\\frac{\\mathbf{\\bar{\\alpha}}}{2}\\mathbf{\\bar{\\alpha}}^{\\top}\\mathbf{A}_{i}x+b_{i}^{\\top}\\bar{x^{\\big}}+c_{i}^{\\top}}\\end{array}$ where $\\mathbf{A}_{i}\\in\\mathbb{R}^{d\\times d}$ is symmetric but not necessarily positive semidefinite, $b_{i}\\in\\mathbb{R}^{d}$ and $c_{i}\\in\\mathbb{R}$ for $i\\in[n]$ . We denote $\\begin{array}{r}{{\\bf A}\\doteq\\frac{1}{n}\\sum_{i=1}^{n}{\\bf A}_{i}}\\end{array}$ "], "table_footnote": ["The complexitie of MARINA-P and M3 with Perm K are beter when $n>1$ and in close-to-homogeneous rgimes,.e., when max[n] $\\|\\mathbf{A}_{i}-\\mathbf{A}\\|$ ismall. et al, 2021). (b) This table only showeases theresuts for Rand $K$ and $\\mathrm{Perm}\\,K$ Amore ralfal es priddet "], "page_idx": 6}, {"type": "text", "text": "Assumption 4.2 (Functional $\\left(L_{A},L_{B}\\right)$ Inequality). There exist constants $L_{A},L_{B}\\ge0$ such that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}(\\nabla f_{i}(x+u_{i})-\\nabla f_{i}(x))\\right\\|^{2}\\leq L_{A}^{2}\\left(\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\|u_{i}\\|^{2}\\right)+L_{B}^{2}\\left\\|\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}u_{i}\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for all $x,u_{1},\\dots,u_{n}\\in\\mathbb{R}^{d}$ ", "page_idx": 6}, {"type": "text", "text": "Remark 4.3. A similar assumption, termed \u201cHeterogeneity-driven Lipschitz Condition on Averaged Gradients\", is proposed in Wang et al. (2023b). Our assumption aligns with theirs when $L_{B}=0$ However, our formulation proves to be more powerful. The possibility that $L_{B}~>~0$ becomes instrumental in driving the enhancements we introduce. ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.2 is defined for all functions together, and intuitively, it tries to capture the similarities between the functions $f_{i}$ .For $n=1$ , inequality (9) reduces to ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\nabla f(x)-\\nabla f(y)\\right\\|^{2}\\leq\\left(L_{A}^{2}+L_{B}^{2}\\right)\\left\\|x-y\\right\\|^{2}\\forall x,y\\in\\mathbb{R}^{d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "equivalent to standard $L$ -smoothness (Assumption 1.1) with $L^{2}\\,=\\,L_{A}^{2}+L_{B}^{2}$ . The Functional $\\left(L_{A},L_{B}\\right)$ Inequality is reasonably weak also for $n>1$ , as the next theorem shows. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.4. For all $i\\in[n]$ , assume that the functions $f_{i}$ are $L_{i}$ -smooth (Assumption 1.5). Then, Assumption 4.2 holds with $L_{A}=L_{\\mathrm{max}}$ and $L_{B}=0$ ", "page_idx": 6}, {"type": "text", "text": "Therefore, Assumption 4.2 holds whenever the functions $f_{i}$ are smooth, which is a standard assumption in the literature. Now, returning to the example from Section 4.3, ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.5. For all $i\\in[n]$ , assume that the functions $f_{i}$ are homogeneous quadratics defined in (8). Then, Assumption 4.2 holds with $L_{A}=0$ and $L_{B}=\\|\\mathbf{A}\\|$ ", "page_idx": 6}, {"type": "text", "text": "Under Assumption 1.5, no information about the similarity of the functions $f_{i}$ is available, yielding $L_{B}=0$ and $L_{A}>0$ in Theorem 4.4. However, once we have some information limiting heterogeneity, $L_{A}$ can decrease. Notably, $L_{A}=0$ for homogeneous quadratics. As we shall see in Section 4.5, the values $L_{A}$ and $L_{B}$ significantly influence the $\\mathrm{s}2\\mathrm{w}$ communication complexity of MARINA-P, with lower $L_{A}$ values leading to greatly improved performance. ", "page_idx": 6}, {"type": "text", "text": "4.5  The Convergence Theory of MARINA-P with PermK ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We are ready to present our main convergence result, focusing on the PermK compressor from Section 4.2. This choice simplifies the presentation, but our approach generalizes to a much larger class of compression operators. The full theoretical framework, covering all unbiased compressors, is detailed in AppendixD. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.6. Let Assumptions 1.1, 1.2 and 4.2 be satisfied. Set $w_{i}^{0}=x^{0}$ for all $i\\in[n]$ . Take PermK as $\\mathcal{C}_{i}^{t}$ and $\\gamma=\\left(L+L_{A}\\sqrt{\\omega_{P}(^{1}/p-1)}\\right)^{-1}$ , where $\\omega_{P}=n-1$ (Lemma A.6). Then, MARINA-P finds an $\\varepsilon,$ -stationarypointafter ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{O}\\left(\\frac{\\delta^{0}}{\\varepsilon}\\left(L+L_{A}\\sqrt{\\omega_{P}/p}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "iterations. ", "page_idx": 7}, {"type": "text", "text": "Corollary 4.7. Let $p=K/d\\equiv1/n$ . Then, in the view of Theorem 4.6, the average $s2w$ communication complexity of MARINA-P with PermK compressor is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{O}\\left(\\frac{d\\delta^{0}L}{n\\varepsilon}+\\frac{d\\delta^{0}L_{A}}{\\varepsilon}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The key observation is that (10) is independent of $L_{B}$ , and only depends on $L_{A}$ . This particular property is specific to correlated compressors with parameter $\\theta=0$ (defined in Appendix A), such as $\\mathrm{Perm}K$ . A similar result holds for independent Rand $K$ compressors (see Corollary D.4), but the convergence rate is worse and depends on $L_{B}$ . Nevertheless, this dependence improves with $n$ ", "page_idx": 7}, {"type": "text", "text": "When $L_{A}=0$ , which is the case for homogeneous quadratics, the step size bound from Theorem 4.6 simplifies to $\\gamma\\leq1/L$ , the standard GD stepsize (recall that in this case our method reduces to GD). Most importantly, (10) scales with the number of workers $n$ ! Even when $L_{A}>0$ , for sufficiently big $n$ , (10) can improve (2) to $\\mathcal{O}\\left(d\\delta^{0}L_{A}\\big/\\varepsilon\\right)$ ", "page_idx": 7}, {"type": "text", "text": "Let us now investigate how the constants $L_{A}$ and $L_{B}$ change in the general case. ", "page_idx": 7}, {"type": "text", "text": "4.6Estimating $L_{A}$ and $L_{B}$ in the General Case ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "It is clear from Corollary 4.7 that MARINA-P with $\\mathrm{Perm}K$ shines when $L_{A}$ is small. To gain further insights into what values $L_{A}$ may take, we now provide an analysis based on the Hessians of the functions $f_{i}$ ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.8. Assume that the functions $f_{i}$ are twice continuously differentiable, $L_{i}$ -smooth(Assumption 1.5), and that there exist $D_{i}\\geq0$ suchthat ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{sup}_{z_{1},\\dotsc,z_{n}\\in\\mathbb{R}^{d}}\\left\\|\\nabla^{2}f_{i}(z_{i})-\\frac{1}{n}\\sum_{j=1}^{n}\\nabla^{2}f_{j}(z_{j})\\right\\|\\leq D_{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for all $i\\,\\in\\,[n]$ Then, Assumption 4.2 holds with $L_{A}={\\sqrt{2}}\\operatorname*{max}_{i\\in[n]}D_{i}\\,\\leq\\,2{\\sqrt{2}}\\operatorname*{max}_{i\\in[n]}L_{i}$ and $\\begin{array}{r}{L_{B}=\\sqrt{2}\\left(\\frac{1}{n}\\sum_{i=1}^{n}L_{i}\\right)}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "Intuitively, (11) measures the similarity between the functions $f_{i}$ . The above theorem yields a more refined result than Theorem 4.4: it is always true that $\\begin{array}{r}{\\operatorname*{max}_{i\\in[n]}D_{i}\\,\\leq\\,2\\operatorname*{max}_{i\\in[n]}L_{i}}\\end{array}$ , and, in fact, $\\operatorname*{max}_{i\\in[n]}D_{i}$ can be much smaller, as the next result shows. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.9. Assume that $\\begin{array}{r}{f_{i}(x)=\\frac{1}{2}{x^{\\top}}{\\mathbf{A}}_{i}x+b_{i}^{\\top}x+c_{i}}\\end{array}$ ,where $\\mathbf{A}_{i}\\in\\mathbb{R}^{d\\times d}$ is symmetric but not $b_{i}\\in\\mathbb{R}^{d}$ and $c_{i}\\in\\mathbb{R}$ $i\\in[n]$ Define $\\textstyle\\mathbf{A}={\\frac{1}{n}}\\sum_{i=1}^{n}\\mathbf{A}_{i}$ Then, Assumption 4.2 holds with $L_{A}=\\sqrt{2}\\,\\mathrm{max}_{i\\in[n]}\\,\\|\\mathbf{A}_{i}-\\mathbf{A}\\|$ and $\\begin{array}{r}{L_{B}=\\sqrt{2}\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\|\\mathbf{A}_{i}\\|\\right)}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "Thus, $L_{A}$ is less than or equal to $\\sqrt{2}\\operatorname*{max}_{i\\in[n]}\\left\\|\\mathbf{A}_{i}-\\mathbf{A}\\right\\|$ : which serves as a measure of similarity between the matrices. The smaller the values of $\\|\\mathbf{A}_{i}-\\mathbf{A}\\|$ (indicating greater similarity among the functions $f_{i})$ , the smaller the $L_{A}$ value. ", "page_idx": 7}, {"type": "text", "text": "In the view of this theorem, the s2w communication complexity of MARINA-P with $\\mathrm{Perm}K$ on non-homogeneous quadratics is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{O}\\left(\\frac{d\\delta^{0}\\|\\mathbf{A}\\|}{n\\varepsilon}+\\frac{d\\delta^{0}\\operatorname*{max}_{i\\in[n]}\\|\\mathbf{A}_{i}-\\mathbf{A}\\|}{\\varepsilon}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Since the corresponding complexity of GD is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{O}\\left(\\frac{d\\delta^{0}\\|\\mathbf{A}\\|}{\\varepsilon}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "in the close-to-homogeneous regimes (i.e., when $\\operatorname*{max}_{i\\in[n]}\\|\\mathbf{A}_{i}-\\mathbf{A}\\|$ is small),the complexity (12) can be provably much smaller than (13). The same reasoning applies to the general case when the ", "page_idx": 7}, {"type": "text", "text": "The M3 Method ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "(M3 = MARINA-P $^+$ Momentum $^+$ MARINA): ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Initialize vectors $x_{0},w_{i}^{0},g_{i}^{0},z_{i}^{0}\\;\\in\\;\\mathbb{R}^{d}$ for all $i\\;\\in\\;[n]$ , step size $\\gamma\\,>\\,0$ probabilities $0~<$ $p_{P},p_{D}\\leq1$ and compressors $\\overset{\\mathcal{C}_{1}^{t}}{\\mathcal{C}_{1}^{\\prime}},\\ldots,\\mathcal{C}_{n}^{t}\\,\\in\\,\\mathbb{U}(\\omega_{P})\\,\\overset{.}{\\cap}\\,\\mathbb{P}(\\theta)^{a}$ \uff0c $\\mathcal{Q}_{1}^{t},\\ldots,\\mathcal{Q}_{n}^{t}\\in\\mathbb{U}(\\omega_{D})$ for all $t\\geq0$ . The method iterates ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x^{t+1}=x^{t}-\\gamma g^{t},}\\\\ &{w_{i}^{t+1}=\\left\\{\\begin{array}{l l}{x^{t+1}}&{\\mathrm{with~probability~}p_{P},}\\\\ {w_{i}^{t}+\\mathcal{C}_{i}^{t}(x^{t+1}-x^{t})}&{\\mathrm{with~probability~}1-p_{P},}\\end{array}\\right.}\\\\ &{z_{i}^{t+1}=\\beta w_{i}^{t+1}+(1-\\beta)z_{i}^{t}\\qquad\\mathrm{(Momentum)}}\\\\ &{g_{i}^{t+1}=\\left\\{\\begin{array}{l l}{\\nabla f_{i}(z_{i}^{t+1})}&{\\mathrm{with~probability~}p_{D},}\\\\ {g_{i}^{t}+\\mathcal{Q}_{i}^{t}(\\nabla f_{i}(z_{i}^{t+1})-\\nabla f_{i}(z_{i}^{t}))}&{\\mathrm{with~probability~}1-p_{D}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where the probabilistic decisions are the same for all $i\\,\\in\\,[n]$ , i.e., one coin is tossed for all workers (as in (5) and (6)), and the coins for the first and second probabilistic decisions with $p_{P}$ and $p_{D}$ are independent. We denote $\\begin{array}{r}{w^{t}\\ :=\\ {1}/{n}\\sum_{i=1}^{n}w_{i}^{\\bar{t}}}\\end{array}$ \uff0c $\\begin{array}{r}{g^{t}\\;:=\\;^{1}\\!/\\!n\\sum_{i=1}^{n}g_{i}^{t}}\\end{array}$ $z^{t}:=1/{n}\\sum_{i=1}^{n}z_{i}^{t}$ See the implementation in Algorithm 2. ", "page_idx": 8}, {"type": "text", "text": "dBy $\\mathbb{P}(\\theta)$ we denote a family of correlated compressors (defined in Appendix A). It includes, among Others,Perm $K$ compressors. ", "page_idx": 8}, {"type": "text", "text": "functions $f_{i}$ are not quadratics: MARINA-P improves with the number of workers $n$ in the regimes when $D_{i}$ are small (see Theorem 4.8). ", "page_idx": 8}, {"type": "text", "text": "Let us note that there is another method, CORE, by Yue et al. (2023), that can also provably outperform GD, achieving the s2w communication complexity of $\\Omega\\left({\\delta^{0}}\\mathrm{t}\\mathbf{A}/_{\\varepsilon}\\right)$ on non-homogeneous quadratics. Neither their method nor ours universally provides the best possible communication guarantees. Our method excels in the close-to-homogeneous regimes: for example, if we take ${\\bf A}_{i}=L_{i}{\\bf I}$ for all $i\\in[n]$ , and define $\\begin{array}{r}{L=1/n\\sum_{i=1}^{n}L_{i}}\\end{array}$ , then the complexity of CORE is $\\Omega\\left(d\\delta^{0}L/\\varepsilon\\right)$ \uff0c Wwileours is (+ dmxiel-) . Hence, our guarantes are superior in regimes where $\\mathrm{max}_{i\\in[n]}|L_{i}-L|\\ll L$ . One interesting research direction is to develop a universally better method combining the benefits of both approaches. ", "page_idx": 8}, {"type": "text", "text": "5  M3: A New Bidirectional Method ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In the previous sections, we introduce a new method that provably improves the server-to-worker communication, but ignores the worker-to-server communication overhead. Our aim now is to treat MARINA-P as a starting point for developing methods applicable to more practical scenarios, by combining it with techniques that compress in the opposite direction. Since the theoretical state-ofthe-art w2s communication complexity is obtained by MARINA (see Section 1.1), our next research step was to combine the two and analyze \u201cMARIN $\\mathsf{A}+\\mathsf{M}\\mathsf{A}$ RINA-P'\"', but this naive approach did not yield communication complexity guarantees surpassing (2) in any regime. It became apparent that some \u201cbuffer\u201d step between these two techniques is needed, and this step turned out to be the momentum. Our new method, M3 (Algorithm 2), is described in (14). ", "page_idx": 8}, {"type": "text", "text": "M3 combines (5), (6), and the momentum step $z_{i}^{t+1}\\,=\\,\\beta w_{i}^{t+1}+(1-\\beta)z_{i}^{t}$ which is the key to our improvements. A similar technique is used to reduce the variance in Fatkhullin et al. (2023). Let us explain how M3 works in practice. First, the server calculates $x^{t+1}$ . Depending on the first pobablisdsont s $x^{t+1}$ $\\mathcal{C}_{i}^{t}(x^{t+1}-x^{t})$ totheworkers, w then callate $w_{i}^{t+1}$ locallytre $z_{i}^{t+1}$ andepending o thesecondprobaistiedecision,they send either $\\nabla f_{i}(z_{i}^{t+1})$ or $\\mathcal{Q}_{i}^{t}(\\nabla f_{i}(z_{i}^{t+1})-\\nabla f_{i}(z_{i}^{t}))$ back to the server. The server aggregates the received vectors and calculates $g^{t+1}$ . As in MARINA, $p_{P}$ and $p_{D}$ are chosen in such a way that the non-compressed communication does not negatively affect the communication complexity. Therefore, the method predominantly transmits compressed information, with only a marginal probability of sending uncompressed vectors. ", "page_idx": 8}, {"type": "image", "img_path": "gkJ5nBIOU4/tmp/ac32ce367f085625af857af7e6d09bef74c56ad80a4b93be772bb1d01257f810.jpg", "img_caption": ["Figure 1: Experiments on the quadratic optimization problem from Section 6. We plot the norm of the gradient w.r.t. # of coordinates sent from the server to the workers. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5.1  The Convergence Theory of M3 ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "For simplicity, we consider PermK in the role of $\\mathcal{C}_{i}^{t}$ and Rand $K$ in the role of $\\mathcal{Q}_{i}^{t}$ . The general theory for all unbiased compressors is presented in Section E. ", "page_idx": 9}, {"type": "text", "text": "Theorem 5.1. Let Assumptions 1.1, 1.2, 1.5 and 4.2 be satisfied. Take $\\begin{array}{r l r}{\\gamma}&{{}=}&{\\left(L\\,+\\right.}\\end{array}$ $34\\left(n L_{A}+n^{2/3}L_{B}+n^{2/3}L_{\\operatorname*{max}}\\right)\\big)^{-1}$ \uff0c $p_{D}~=~p_{P}~=~1/n,$ $\\beta\\ =\\ n^{-2/3}$ \uff0c $w_{i}^{0}~=~z_{i}^{0}~=~x^{0}$ and $g_{i}^{0}=\\nabla f_{i}(x^{0})$ for all $i\\in[n]$ . Then MARINA-P with $\\mathcal{C}_{i}^{t}=\\!P e r m K$ and $\\mathcal{Q}_{i}^{t}=R a n d K$ with $K=d/n$ finds an $\\varepsilon\\cdot$ -stationary point after $\\begin{array}{r}{\\mathcal{O}\\big(\\frac{\\delta^{0}}{\\varepsilon}\\left(n^{2/3}L_{\\mathrm{max}}+n L_{A}\\right)}\\end{array}$ )iterations.The total communication complexity is ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{O}\\left(\\frac{d\\delta^{0}L_{\\mathrm{max}}}{n^{1/3}\\varepsilon}+\\frac{d\\delta^{0}L_{A}}{\\varepsilon}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Once again, we observe improvement with the number of workers $n$ , and the obtained complexity (15) can be provably smaller than (2). Indeed, in scenarios like federated learning, where the number of workers (e.g., mobile phones) is typically large (Kairouz et al., 2021; Chowdhery et al., 2023), the first term can be significantly smaller than $d\\bar{\\delta}^{0}L\\big/\\varepsilon$ . The second term can also be small in close-tohomogeneous regimes (see Section 4). ", "page_idx": 9}, {"type": "text", "text": "6  Experimental Highlights ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This section presents insights from the experiments, with further details and additional results in Appendix F. The experiment aims to empirically test the theoretical results from Section 4. We consider a quadratic optimization problem, where the functions $f_{i}$ are as defined in Theorem 4.9 and $\\mathbf{A}_{i}\\in\\mathbb{R}^{300\\dot{\\times}300}$ WecomareGDMRIN-Psending th samemessagecompressed using asingle Rand $K$ compressor to all workers (\\*SameRand $K^{\\bullet}$ from Appendix A), MARINA-P with independent Rand $K$ compressors, MARINA-P with PermK compressors, and EF21-P with $\\mathrm{Top}K$ compressor. We consider $n\\in\\{10,100,1000\\}$ and fine-tune the step size for each algorithm. The results, presented in Figure 1, align closely with the theory, with MARINA-P using $\\mathrm{Perm}K$ compressors consistently performing best. Moreover, the convergence rate of MARINA-P with $\\mathrm{Perm}K$ and independent Rand $K$ compressors improves with $n$ . Since this is not the case for EF21-P, even though it outperforms MARINA-P with independent Rand $K$ compressors for $n=10$ , it falls behind for $\\bar{n}\\in\\{10\\bar{0},1000\\}$ ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The research reported in this publication was supported by funding from King Abdullah University of Science and Technology (KAUST): i) KAUST Baseline Research Scheme, ii) Center of Excellence for Generative AI, under award number 5940, i) SDAIA-KAUST Center of Excellence in Artificial Intelligence and Data Science. The work of A.T. was partially supported by the Analytical center under the RF Government (subsidy agreement 000000D73032iP5Q0002, Grant No. 70-2021-00145 02.11.2021). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Alistarh, D., Grubic, D., Li, J., Tomioka, R., and Vojnovic, M. (2017). QSGD: Communicationefficient SGD via gradient quantization and encoding. In Advances in Neural Information Processing Systems (NIPS), pages 1709-1720.   \nArjevani, Y., Carmon, Y., Duchi, J. C., Foster, D. J., Srebro, N., and Woodworth, B. (2022). Lower bounds for non-convex stochastic optimization. Mathematical Programming, pages 1-50.   \nBeznosikov, A., Horvath, S., Richtarik, P., and Safaryan, M. (2020). On biased compression for distributed learning. arXiv preprint arXiv:2002.12410.   \nCarmon, Y, Duchi, J. C., Hinder, O., and Sidford, A. (2020). Lower bounds for finding stationary points I. Mathematical Programming, 184(1):71-120.   \nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. (2023). PaLM: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1-113.   \nFang, C., Li, C. J., Lin, Z., and Zhang, T. (2018). SPIDER: Near-optimal non-convex optimization via stochastic path integrated differential estimator. In NeurIPS Information Processing Systems.   \nFatkhullin, I., Sokolov, I., Gorbunov, E., Li, Z., and Richtarik, P. (2021). EF21 with bells & whistles: Practical algorithmic extensions of modern error feedback. arXiv preprint arXiv:2110.03294.   \nFatkhullin, I., Tyurin, A., and Richtarik, P. (2023). Momentum provably improves error feedback! Advances in Neural Information Processing Systems.   \nGorbunov, E., Burlachenko, K., Li, Z., and Richtarik, P. (2021). MARINA: Faster non-convex distributed learning with compression. In 38th International Conference on Machine Learning.   \nGruntkowska, K., Tyurin, A., and Richtarik, P. (2023). EF21-P and friends: Improved theoretical communication complexity for distributed optimization with bidirectional compression. In International Conference on Machine Learning, pages 11761-11807. PMLR.   \nHorvath, S., Ho, C.-Y, Horvath, L., Sahu, A. N., Canini, M., and Richtarik, P. (2022). Natural compression for distributed deep learning. In Mathematical and Scientific Machine Learning, pages 129-141. PMLR.   \nHuang, J., Qian, F., Gerber, A., Mao, Z. M., Sen, S., and Spatscheck, O. (2012). A close examination of performance and power characteristics of 4G LTE networks. In Proceedings of the 10th international conference on Mobile systems, applications, and services, pages 225-238.   \nHuang, X., Chen, Y., Yin, W., and Yuan, K. (2022). Lower bounds and nearly optimal algorithms in distributed learning with communication compression. arXiv preprint arXiv:2206.03665.   \nKairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A. N., Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et al. (2021). Advances and open problems in federated learning. Foundations and Trends? in Machine Learning, 14(1-2):1-210.   \nKonecny, J., McMahan, H. B., Yu, F. X., Richtarik, P., Suresh, A. T., and Bacon, D. (2016). Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492.   \nLan, G. (2020). First-order and stochastic optimization methods for machine learning. Springer.   \nLeCun, Y, Cortes, C., and Burges, C. (2010). MNIST handwritten digit database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2.   \nLi, Z., Bao, H., Zhang, X., and Richtarik, P (2021). PAGE: A simple and optimal probabilistic gradient estimator for nonconvex optimization. In International Conference on Machine Learning, pages 6286-6295. PMLR.   \nLiu, X., Li, Y., Tang, J., and Yan, M. (2020). A double residual compression algorithm for efficient distributediearing In International Conference on Artifcial Inteligence and Statistics, pages 133-143. PMLR.   \nLu, Y. and De Sa, C. (2021). Optimal complexity in decentralized training. In International Conference on Machine Learning, pages 7111-7123. PMLR.   \nMcMahan, B., Moore, E., Ramage, D., Hampson, S., and y Arcas, B. A. (2017). Communicationeffcient learningof deep networks from decentralized data. In Artifcial inteligence and statistic, pages 1273-1282. PMLR.   \nNarayanan, A., Zhang, X., Zhu, R., Hassan, A., Jin, S., Zhu, X., Zhang, X., Rybkin, D., Yang, Z.. Mao, Z. M., et al. (2021). A variegated look at 5G in the wild: performance, power, and QoE implications. In Proceedings of the 2021 ACM SIGCOMM 2021 Conference, pages 610-625.   \nOpenAI (2023). GPT-4 technical report. ArXiv, abs/2303.08774.   \nPhilippenko, C. and Dieuleveut, A. (2021). Preserved central model for faster bidirectional compression in distributed settings. Advances in Neural Information Processing Systems, 34:2387-2399.   \nRamesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. (2021). Zero-sttext-mage generation. n Itnational Conference onMachnLeaing,pas 8821-8831. PMLR.   \nRichtarik, P., Sokolov, I., and Fatkhullin, I. (2021). EF21: A new, simpler, theoretically better, and practically faster error feedback. In Neural Information Processing Systems, 2021.   \nSeide, F., Fu, H., Droppo, J., Li, G., and Yu, D. (2014). 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs. In Fiftenth Annual Conference of the International Speech Communication Association.   \nSzlendak, R, Tyurin, A., and Richtarik, P. (2021). Permutation compressors for provably faster distributed nonconvex optimization. In International Conference on Learning Representations.   \nTyurin, A. and Richtarik, P. (2023a). 2Direction: Theoretically faster distributed training with bidirectional communication compression. Advances in Neural Information Processing Systems.   \nTyurin, A. and Richtarik, P. (2023b). DASHA: Distributed nonconvex optimization with communication compression, optimal oracle complexity, and no client synchronization. 1lth International Conference on Learning Representations (ICLR).   \nTyurin, A. and Richtarik, P. (2023c). Optimal time complexities of parallel stochastic optimization methods under a fixed computation model. Advances in Neural Information Processing Systems.   \nWang, J., Lu, Y, Yuan, B., Chen, B., Liang,P, De Sa, C., Re, C., and Zhang, C.(2023a). CocktailSGD: Fine-tuning foundation models over 50OMbps networks. In International Conference on Machine Learning, pages 36058-36076. PMLR.   \nWang, J, Wang, S., Chen, R-R., and Ji, M. (2023b). A new theoretical perspective on data heterogeneity in federated optimization. In Federated Learning and Analytics in Practice: Algorithms, Systems, Applications, and Opportunities.   \nWangni, J, Wang, J, Liu, J., and Zhang, T. (2018). Gradient sparsifcation for communicationefficient distributed optimization. Advances in Neural Information Processing Systems, 31.   \nYue, P., Zhao, H., Fang, C., He, D., Wang, L., Lin, Z., and Zhu, S.-c. (2023). CORE: Common random reconstruction for distributed optimization with provable low communication complexity. arXiv preprint arXiv:2309.13307.   \nZheng, S., Huang, Z., and Kwok, J. (2019). Communication-efficient distributed blockwise momentum SGD with error-feedback. Advances in Neural Information Processing Systems, 32. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Introduction .1Related Work 2 ", "page_idx": 12}, {"type": "text", "text": "2Contributions 3 ", "page_idx": 12}, {"type": "text", "text": "3  Lower Bound under Smoothness ", "page_idx": 12}, {"type": "text", "text": "4The MARINA-P Method ", "page_idx": 12}, {"type": "text", "text": "4.1 Three ways to compress 5   \n4.2 Recap: permutation compressors PermK 6   \n4.3 Warmup: homogeneous quadratics 6   \n4.4 Functional $\\left(L_{A},L_{B}\\right)$ Inequality 6   \n4.5 The Convergence Theory of MARINA-P with PermK 7   \n4.6 Estimating $L_{A}$ and $L_{B}$ in the General Case 8   \nS M3: A New Bidirectional Method 9   \n5.1 The Convergence Theory of M3 10 ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "6   Experimental Highlights 10 ", "page_idx": 12}, {"type": "text", "text": "A   Three unbiased ways to compress 15 ", "page_idx": 12}, {"type": "text", "text": "B Biased Compressors 17 ", "page_idx": 12}, {"type": "text", "text": "C  Properties of $L_{A}$ and $L_{B}$ 18 ", "page_idx": 12}, {"type": "text", "text": "D Convergence of MARINA-P in the General Case 21 ", "page_idx": 12}, {"type": "text", "text": "D.1 Main Results 21   \nD.2 Proofs 22   \nD.3 Polyak-Lojasiewicz condition . 27   \nD.3.1Main Results 27   \nD.3.2Proofs 28   \nEConvergence of M3 in the General Case 30   \nE.1 Main Results 30   \nE.2Proofs 31   \nE.3 Polyak-Lojasiewicz condition . 42   \nE.3.1 Main Results 42   \nE.3.2 Proofs 43   \nF  Experiments 51   \nF.1 Experiments with M3 on quadratic optimization tasks . 51   \nF.2 Experiments with an autoencoder and MNIST 51   \nF.3 Extra experiments with quadratic optimization tasks . . 52 ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "G Proof of the Lower Bounds 55 ", "page_idx": 13}, {"type": "text", "text": "G.1 The \u201cdifficult\u2019 function from the nonconvex world 55   \nG.2Theorems 55   \nG.3  Compressed communication with independent compressors 58 ", "page_idx": 13}, {"type": "text", "text": "H Useful Identities and Inequalities 60 ", "page_idx": 13}, {"type": "text", "text": "Notation 61 ", "page_idx": 13}, {"type": "text", "text": "A  Three unbiased ways to compress ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The main focus of this paper is handling the server-to-worker communication costs. To explain better where the improvements outlined in the main part of this paper come from, let us first consider the scenario where uplink communication cost is negligible but downilnk communication cost is not. While we do no necessarily say that this is a realistic setup, examining it first enables us to understand how downlink compression should be performed, capturing all the intricacies. ", "page_idx": 14}, {"type": "text", "text": "Existing algorithms with lossy s2w and w2s communication have a certain common feature. The compression mechanism employed on the clients is very different from the one used on the server: while each client transmits to the server a different message, specific to the data stored on each device, the server broadcasts the same update to all clients. We want to question this algorithmic step and suggest to the reader that if compression is applied multiple times and each worker receives its individual update, then intuitively more information can be transmitted. A well-designed algorithm should be able to take advantage of this. ", "page_idx": 14}, {"type": "text", "text": "One can depart from the usual approach of sending the same update to all workers in two ways: a) compress the update $n$ times independently, or b) produce $n$ such updates in a correlated way. Either way, the server broadcasts $n$ different compressed messages rather than one, and sends a different update to each worker. The key discovery here is that both a) and b) are mathematically provably better than the prevalent approach of sending the same update to all clients. ", "page_idx": 14}, {"type": "text", "text": "This is a crucial improvement in a system where the above setup is a good approximation of reality. And even if it is not, and the current model is not perfectly capturing the reality, we can accept it for now, as it allows us to focus on the novel aspects of the approach. With that said, these considerations can serve as a starting point for thinking about bidirectional compression: having focused on the simplified setup and equipped with knowledge on how the compression on the master should be performed, we employ this mechanism in more complex scenarios (see Section 5). ", "page_idx": 14}, {"type": "text", "text": "Let us now describe the three possible ways to perform compression on the server. ", "page_idx": 14}, {"type": "text", "text": "\"Same compressors.  The prevalent approach in downlink compression is to transmit the same update to all workers. To illustrate this, let us call a collection $\\mathcal{C}_{1},\\ldots,\\mathcal{C}_{n}$ of compressors \u201cSameRand $K^{\\bullet}$ if for all $i\\in[n]$ we have $\\mathcal{C}_{i}=\\mathcal{C}$ for some Rand $K$ compressor $\\mathcal{C}$ . Now, consider one iteration $t$ of MARINA-P with SameRand $K$ compressor. The server calculates $\\mathcal{C}_{i}^{t}(x^{t+1}-x^{t})$ for $i\\in[n]$ , but in this case, $\\mathcal{C}_{1}^{t}(x^{t+1}-x^{t})=\\ldots...=\\dot{\\mathcal{C}}_{n}^{t}(x^{t+1}-x^{t})=\\mathcal{C}^{t}(x^{t+1}-x^{t})$ husaplng SameRand $K$ compressors to some vector $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ is equivalent to using a single Rand $K$ compression operator and transmitting the same message ${\\mathcal{C}}(x)$ to all workers. ", "page_idx": 14}, {"type": "text", "text": "Independent Compressors. Rather than setting $\\mathcal{C}_{i}(x)=\\mathcal{C}(x)$ for all $i\\in[n]$ , one can break the dependency between the messages and allow the compressors to differ. For illustrational purposes, suppose that $\\mathcal{C}_{i},i\\in[n]$ are independent Rand $K$ compressors (Assumption 1.6). Then, applying such a collection of mappings to the vector of interest $x\\in\\mathbb{R}^{d}$ , one obtains $n$ distinct and independent sparse vectors $\\mathcal{C}_{1}(x),\\ldots,\\mathcal{C}_{n}(x)$ ", "page_idx": 14}, {"type": "text", "text": "Remark A.1. We are aware of only one method that uses $n$ distinct compressors in downlink compression, Rand-MCM by Philippenko and Dieuleveut (2021). Given the absence of results in the non-convex case, let us compare the communication complexities of Rand-MCM and M3 under the Polyak-Lojasiewicz condition (Assumption D.9), which holds under strong convexity. In the strongly convex case, the proved iteration complexity of Rand-MCM is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Omega\\left(\\frac{L_{\\mathrm{max}}}{\\mu}\\left(\\omega_{P}^{3/2}+\\frac{\\omega_{P}\\omega_{D}^{1/2}}{\\sqrt{n}}+\\frac{\\omega_{D}}{n}\\right)\\log\\frac{\\delta^{0}}{\\varepsilon}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Assuming for simplicity that the server and the workers use Rand $K$ compressors with $K=d/n$ , this gives the total communication complexity of ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Omega\\left(\\frac{d}{n}\\times\\frac{L_{\\mathrm{max}}}{\\mu}\\left(\\omega_{P}^{3/2}+\\frac{\\omega_{P}\\omega_{D}^{1/2}}{\\sqrt{n}}+\\frac{\\omega_{D}}{n}\\right)\\log\\frac{\\delta^{0}}{\\varepsilon}\\right)=\\Omega\\left(\\frac{d\\sqrt{n}L_{\\mathrm{max}}}{\\mu}\\log\\frac{\\delta^{0}}{\\varepsilon}\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is getting worse as the number of workers $n$ increases. Meanwhile, by Corollary E.10, the total communication complexity of M3 (where $\\mathcal{C}_{i}^{t}$ are the Perm $K$ compressors and $\\mathcal{Q}_{i}^{t}$ are independent ", "page_idx": 14}, {"type": "text", "text": "Rand $K$ compressors, both with $K=d/n$ ) under the Polyak-Lojasiewicz condition is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\left(\\frac{d L_{\\mathrm{max}}}{n^{1/3}\\mu}+\\frac{d L_{A}}{\\mu}+d\\right)\\log\\frac{\\delta^{0}}{\\varepsilon}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $n$ is typically large, the total communication complexity of M3 can be much better than that of Rand-MCM. ", "page_idx": 15}, {"type": "text", "text": "Correlated Compressors. In their work, Szlendak et al. (2021) introduce an alternative class of compressors, which satisfy the following condition: ", "page_idx": 15}, {"type": "text", "text": "Definition A.2 (AB-inequality (Szlendak et al., 2021)). There exist constants $A,B\\ge0$ such that the random operators $\\mathcal{C}_{1},...\\,\\mathcal{C}_{n}$ satisfy ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathbb{E}\\left[\\mathcal{C}_{i}(x)\\right]=x,}\\\\ {\\displaystyle\\mathbb{E}\\left[\\bigg\\|\\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{C}_{i}(x_{i})-\\frac{1}{n}\\sum_{i=1}^{n}x_{i}\\bigg\\|^{2}\\right]\\leq A\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|x_{i}\\right\\|^{2}-B\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}x_{i}\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for all $x,x_{1},\\ldots,x_{n}\\in\\mathbb{R}^{d}$ If these conditions hold, we write $\\{{\\mathcal{C}}_{i}\\}_{i=1}^{n}\\in\\mathbb{U}(A,B)$ ", "page_idx": 15}, {"type": "text", "text": "Following on this idea, we introduce the concept of a collection of correlated compressors. ", "page_idx": 15}, {"type": "text", "text": "Definition A.3 (Collection of Correlated Compressors). There exists a constant $\\theta\\ge0$ such that the random operators $\\mathcal{C}_{1},...\\,\\mathcal{C}_{n}$ satisfy: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{C}_{i}(x)-x\\right\\Vert^{2}\\right]\\leq\\theta\\left\\Vert x\\right\\Vert^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for all $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ . If these conditions hold, we write $\\left\\{{\\mathcal{C}}_{i}\\right\\}_{i=1}^{n}\\in\\mathbb{P}(\\theta)$ ", "page_idx": 15}, {"type": "text", "text": "Definition A.3 will play a key role in our upcoming advancements. But what makes this assumption reasonable? ", "page_idx": 15}, {"type": "text", "text": "First, it is easy to note that condition (17) is weaker than (16). Indeed, if $\\{{\\mathcal{C}}_{i}\\}_{i=1}^{n}\\in\\mathbb{U}(A,B)$ , then inequality (17) holds with $\\theta:=A-B$ . It turns out that it is in fact strictly weaker, as the following example shows. ", "page_idx": 15}, {"type": "text", "text": "Example A.4. Let $n=2$ \uff0c $d=1$ Let $\\{\\zeta_{x}:x\\in\\mathbb{R}\\}$ be a collection of independent Cauchy variables indexed by real numbers. Define $\\mathcal{C}_{1}(u)=u+\\zeta_{u}$ and $\\mathcal{C}_{2}(u)=u-\\zeta_{u}$ .Then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\left(\\mathcal{C}_{1}(u)+\\mathcal{C}_{2}(u)\\right)=u,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$\\mathcal{C}_{1}(u)$ and $\\mathcal{C}_{2}(u)$ satisfy Definition A.3 with $\\theta=0$ . However, for $u_{1}\\neq u_{2}$ , by the properties of Cauchy distribution we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left(\\frac{1}{2}\\left(\\mathcal{C}_{1}(u)+\\mathcal{C}_{2}(u)\\right)-\\frac{1}{2}\\left(u_{1}+u_{2}\\right)\\right)^{2}\\right]=\\mathbb{E}\\left[\\left(\\frac{1}{2}\\left(\\zeta_{1}+\\zeta_{2}\\right)\\right)^{2}\\right]}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~=\\frac{1}{4}\\mathbb{E}\\left[\\zeta_{1}^{2}+\\zeta_{2}^{2}+2\\zeta_{1}\\zeta_{2}\\right]=\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, $\\mathcal{C}_{1}(u)$ and $\\mathcal{C}_{2}(u)$ do not satisfy Definition A.2 ", "page_idx": 15}, {"type": "text", "text": "In fact, the condition specified in Definition A.3 does not impose any restrictions on the compressor class when working with unbiased compressors. This is because, for any set of compressors $\\mathcal{C}_{1},\\ldots,\\mathcal{C}_{n}\\in\\mathbb{U}(\\omega)$ thereexists $\\theta\\ge0$ such that $\\left\\{{\\mathcal{C}}_{i}\\right\\}_{i=1}^{n}\\in\\mathbb{P}(\\theta)$ , as shown in the following lemma. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.5. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "2. Let us further assume that $\\mathcal{C}_{1},\\ldots,\\mathcal{C}_{n}$ are independent (Assumption 1.6). Then $\\{{\\mathcal{C}}_{i}\\}_{i=1}^{n}\\in$ $\\mathbb{P}(\\omega/n)$ ", "page_idx": 16}, {"type": "text", "text": "Proof. 1. Jensen's inequality gives ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{C}_{i}(u)-u\\right\\Vert^{2}\\right]\\overset{(50)}{\\leq}\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\left\\Vert\\mathcal{C}_{i}(u)-u\\right\\Vert^{2}\\right]\\overset{\\mathrm{Def.1.4}}{\\leq}\\frac{1}{n}\\sum_{i=1}^{n}\\omega\\left\\Vert u\\right\\Vert^{2}=\\omega\\left\\Vert u\\right\\Vert^{2},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$\\theta=\\omega$ ", "page_idx": 16}, {"type": "text", "text": "2. Using independence of compressors, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{C}_{i}(u)-u\\right\\Vert^{2}\\right]=\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\left\\Vert\\mathcal{C}_{i}(u)-u\\right\\Vert^{2}\\right]\\overset{\\mathrm{Def.l.}4}{\\leq}\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\omega\\left\\Vert u\\right\\Vert^{2}=\\frac{\\omega}{n}\\left\\Vert u\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus $\\theta=\\omega/n$ ", "page_idx": 16}, {"type": "text", "text": "However, the true advantages of employing correlated compressors become apparent when the definition holds with $\\theta=0$ , as in the case of Perm $K$ compressors. ", "page_idx": 16}, {"type": "text", "text": "Lemma A.6. Let $\\mathcal{C}_{1},\\ldots,\\mathcal{C}_{n}$ be a collection of a) SameRandK, $^b$ ) independent RandK, c) PermK compressors. Then ", "page_idx": 16}, {"type": "text", "text": "Proof. Unbiasedness follows easily from definitions of compressors (the proof for PermK compressors can be found in Szlendak et al. (2021)). That Rand $\\bar{K}\\in\\mathbb{U}(d/K-\\bar{1})$ (and hence trivially SameRand $K\\;\\in\\;\\mathbb{U}(d/\\kappa\\mathrm{~-~}1))$ is a well-known fact. Next, the fact that a) $\\left\\{C_{i}\\right\\}_{i=1}^{n}\\;\\in\\;\\mathbb{P}(\\omega)$ for SameRand $K$ compressors and b) $\\left\\{{\\mathcal{C}}_{i}\\right\\}_{i=1}^{n}\\in\\mathbb{P}(\\omega/n)$ for independent Rand $K$ compressors follows directly from Lemma A.5. ", "page_idx": 16}, {"type": "text", "text": "To compute $\\omega$ for $\\mathrm{Perm}K$ compressor, first assume that $d\\,\\geq\\,n$ . Then $\\mathbb{E}\\left[\\left\\|\\boldsymbol{\\mathcal{C}}_{i}(x)\\right\\|^{2}\\right]\\,=\\,n\\left\\|x\\right\\|^{2}$ (Szlendak et al., 2021), so ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\Vert C_{i}(x)-x\\right\\Vert^{2}\\right]\\overset{(48)}{=}\\mathbb{E}\\left[\\left\\Vert C_{i}(x)\\right\\Vert^{2}\\right]-\\left\\Vert x\\right\\Vert^{2}=(n-1)\\left\\Vert x\\right\\Vert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Similarly, suppose that $d\\leq n$ , and write $n$ as $n=q d+r$ ,where $q\\in\\mathbb{N}_{>0}$ and $0\\leq r<d$ . Then $\\mathbb{E}\\left[\\left\\|\\boldsymbol{C}_{i}(x)\\right\\|^{2}\\right]=n/q\\left\\|x\\right\\|^{2}$ (Szlendak et al., 2021), and hence ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert\\mathcal{C}_{i}(x)-x\\Vert^{2}\\right]\\overset{(48)}{=}\\mathbb{E}\\left[\\Vert\\mathcal{C}_{i}(x)\\Vert^{2}\\right]-\\Vert x\\Vert^{2}=\\left(\\frac{n}{q}-1\\right)\\left\\Vert x\\right\\Vert^{2}\\leq\\left(n-1\\right)\\left\\Vert x\\right\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In both cases $\\omega=n-1$ ", "page_idx": 16}, {"type": "text", "text": "Finally, by construction of $\\mathrm{Perm}K$ ,we have $\\textstyle{\\frac{1}{n}}\\sum_{i=1}^{n}C_{i}(x)=x$ implying $\\theta=0$ ", "page_idx": 16}, {"type": "text", "text": "In what follows, when considering the Perm $K$ compressor, we shall assume for simplicity that $d\\geq n$ The results for $d<n$ are analogous. ", "page_idx": 16}, {"type": "text", "text": "B Biased Compressors ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In addition to unbiased compressors (Definition 1.4), the literature of compressed methods distinguishes another class of mappings: ", "page_idx": 16}, {"type": "text", "text": "1: Input: initial model $\\boldsymbol{x}_{0}\\in\\mathbb{R}^{d}$ (stored on the server),initial model shifts $w_{1}^{0}=\\ldots=w_{n}^{0}=x^{0}$   \n(stored on the workers), step size $\\gamma>0$ ,probability $0<p\\le1$ ,compressors $\\mathcal{\\hat{C}}_{1}^{t},\\ldots,\\mathcal{C}_{n}^{t}\\in\\overset{\\cdot}{\\mathbb{U}}(\\omega_{P})$   \n2: for $t=0,\\dots,T$ do   \n3: for $i=1,\\hdots,n$ in parallel do   \n4: Calculate $\\nabla f_{i}(w_{i}^{t})$ and send it to the server Workers evaluate the gradients at the current model estimate   \n5: end for   \nOn the server:   \n6: $\\begin{array}{r}{g^{t}=\\frac{1}{n}\\sum_{i=1}^{n}\\nabla f_{i}(w_{i}^{t})}\\end{array}$ Server averages the messages received from the workers   \n7: $x^{t+1}=x^{t}-\\gamma g^{t}$ Server takes a gradient-type step to update the global model   \n8: Sample ct \\~ Bernoulli(p)   \n9: if $\\boldsymbol{c}^{t}=0$ then   \n10: Send $\\mathcal{C}_{i}^{t}(x^{t+1}-x^{t})$ to worker $i$ for $i\\in[n]$ Server sends compressed messages to all workers w.p. $1-p$   \n11: else   \n12: Send $x^{t+1}$ to worker $i$ for $i\\in[n]$ Server sends the same uncompressed message t ll workers w.p. $p$   \n13: end if   \nOn the workers:   \n14: for $i=1,\\hdots,n$ in parallel do   \n15: $\\begin{array}{r}{w_{i}^{t+1}=\\left\\{\\!\\!\\begin{array}{l l}{x^{t+1}}&{\\mathrm{if~}c^{t}=1,\\ }\\\\ {w_{i}^{t}+\\mathcal{C}_{i}^{t}(x^{t+1}-x^{t})}&{\\mathrm{if~}c^{t}=0}\\end{array}\\right.}\\end{array}$ Worker i updates its local model shift   \n16: end for   \n17 ", "page_idx": 17}, {"type": "text", "text": "Definition B.1. A stochastic mapping $\\mathcal{C}\\,:\\,\\mathbb{R}^{d}\\to\\mathbb{R}^{d}$ is a biased compressor if there exists $\\alpha\\in(0,1]$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|C(x)-x\\right\\|^{2}\\right]\\leq\\left(1-\\alpha\\right)\\left\\|x\\right\\|^{2}\\quad\\forall x\\in\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The family of such compressors is denoted by $\\mathbb{B}(\\alpha)$ . It is well-known that if $\\mathcal{C}\\,\\in\\,\\mathbb{U}(\\omega)$ ,then $(\\omega+1)^{-1}\\overset{\\cdot}{\\mathcal{C}}\\in\\mathbb{B}\\left((\\omega+1)^{-1}\\right)$ , meaning that the family of biased compressors is broader. A canonical example is the $\\mathrm{Top}K\\in\\mathbb{B}(K/d)$ compressor, which preserves the $K$ largest in magnitude coordinates of the input vector (Beznosikov et al., 2020). ", "page_idx": 17}, {"type": "text", "text": "C Properties of $L_{A}$ and $L_{B}$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We first prove the results from Section 4, starting with calculating the constants $L_{A}$ and $L_{B}$ from Assumption 4.2 in some special cases. ", "page_idx": 17}, {"type": "text", "text": "Theorem 4.4. For all $i\\in[n]$ , assume that the functions $f_{i}$ are $L_{i}$ -smooth (Assumption 1.5). Then, Assumption 4.2 holds with $L_{A}=L_{\\mathrm{max}}$ and $L_{B}=0$ ", "page_idx": 17}, {"type": "text", "text": "Proof. From Assumption 1.5 it follows that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\left\\lvert\\bigg\\lvert\\frac{1}{n}\\sum_{i=1}^{n}(\\nabla f_{i}(x+u_{i})-\\nabla f_{i}(x))\\bigg\\rvert\\right\\lvert^{2}}&{\\leq\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left\\lvert\\lvert\\nabla f_{i}(x+u_{i})-\\nabla f_{i}(x)\\rvert\\right\\rvert^{2}}\\\\ &{\\stackrel{\\mathrm{Ass.1.5}}{\\leq}\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}L_{i}^{2}\\left\\lvert|u_{i}\\rvert\\right\\rvert^{2}}\\\\ {\\leq}&{L_{\\mathrm{max}}^{2}\\left(\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left\\lvert|u_{i}\\rvert\\right\\rvert^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "so Assumption 4.2 holds with $L_{A}=L_{\\mathrm{max}}$ and $L_{B}=0$ ", "page_idx": 17}, {"type": "text", "text": "Theorem 4.5. For all $i\\in[n]$ , assume that the functions $f_{i}$ are homogeneous quadratics defined in (8). Then, Assumption 4.2 holds with $L_{A}=0$ and $L_{B}=\\|\\mathbf{A}\\|$ ", "page_idx": 17}, {"type": "text", "text": "Proof. It is easy to verify that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Bigg\\|\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}(\\nabla f_{i}(x+u_{i})-\\nabla f_{i}(x))\\Bigg\\|^{2}=\\Bigg\\|\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\big(\\mathbf{A}(x+u_{i})+b-(\\mathbf{A}x+b)\\big)\\Bigg\\|^{2}}\\\\ {=\\Bigg\\|\\mathbf{A}\\left(\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}u_{i}\\right)\\Bigg\\|^{2}}\\\\ {\\leq\\|\\mathbf{A}\\|^{2}\\left\\|\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}u_{i}\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "meaning that Assumption 4.2 holds with $L_{A}=0$ and $L_{B}=\\|\\mathbf{A}\\|$ ", "page_idx": 18}, {"type": "text", "text": "Lemma C.1. Let Assumption 1.5 hold. Then, there exist constants $L_{A},L_{B}\\ge0$ suchthatAssumption4.2holds and $L_{A}^{2}+\\mathrm{\\dot{L}_{\\cal B}}^{2}\\leq L_{\\mathrm{max}}^{2}$ ", "page_idx": 18}, {"type": "text", "text": "Proof. Assumption 1.5 gives ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}(\\nabla f_{i}(x+u_{i})-\\nabla f_{i}(x))\\right\\|^{2}}&{\\leq\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\|\\nabla f_{i}(x+u_{i})-\\nabla f_{i}(x)\\|^{2}}\\\\ &{{\\overset{\\mathrm{Ass.1.5}}{\\leq}}\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}L_{i}^{2}\\left\\|u_{i}\\right\\|^{2}}\\\\ {\\leq}&{L_{\\operatorname*{max}}^{2}\\left(\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\|u_{i}\\|^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "andhence Asumtionholds wih $L_{A}^{2}=L_{\\operatorname*{max}}^{2}$ and $L_{B}^{2}=0$ ", "page_idx": 18}, {"type": "text", "text": "Remark C.2. Under Assumption 4.2 we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}(\\nabla f_{i}(x+u_{i})-\\nabla f_{i}(x))\\right\\|_{\\mathrm{~\\tiny~\\textnormal~{\\leq~}~}}^{2}L_{A}^{2}\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\|u_{i}\\|^{2}\\right)+L_{B}^{2}\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}u_{i}\\right\\|^{2}}}\\\\ &{\\stackrel{\\mathrm{Ass.1.5}}{\\leq}\\left(L_{A}^{2}+L_{B}^{2}\\right)\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\|u_{i}\\|^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "so, in principle, one could always set $L_{B}^{2}=0$ . However, the bound could be tightened by decreasing $L_{A}$ and increasing $L_{B}$ . The smaller $L_{A}$ , the better the performance of our algorithms (see Corollaries D.4 and E.3). ", "page_idx": 18}, {"type": "text", "text": "Now, we proceed to prove the result that relates the values of $L_{A}$ and $L_{B}$ to the Hessians of the functions $f_{i}$ ", "page_idx": 18}, {"type": "text", "text": "Theorem 4.8. Assume that the functions $f_{i}$ are twice continuously differentiable, $L_{i}$ -smooth(Assumption 1.5), and that there exist $D_{i}\\geq0$ suchthat ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{sup}_{z_{1},\\dotsc,z_{n}\\in\\mathbb{R}^{d}}\\left\\|\\nabla^{2}f_{i}(z_{i})-\\frac{1}{n}\\sum_{j=1}^{n}\\nabla^{2}f_{j}(z_{j})\\right\\|\\leq D_{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for all $i\\,\\in\\,[n]$ . Then, Assumption 4.2 holds with $L_{A}={\\sqrt{2}}\\operatorname*{max}_{i\\in[n]}D_{i}\\,\\leq\\,2{\\sqrt{2}}\\operatorname*{max}_{i\\in[n]}L_{i}$ and $\\begin{array}{r}{L_{B}=\\sqrt{2}\\left(\\frac{1}{n}\\sum_{i=1}^{n}L_{i}\\right)}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "Proof. By the fundamental theorem of calculus, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla f_{i}(x+u_{i})-\\nabla f_{i}(x)=\\int_{0}^{1}\\nabla^{2}f_{i}(x+t u_{i})u_{i}d t=\\left(\\int_{0}^{1}\\nabla^{2}f_{i}(x+t u_{i})d t\\right)u_{i}=\\mathbf{Q}_{i}u_{i},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathbf{Q}_{i}=\\int_{0}^{1}\\nabla^{2}f_{i}(x+t u_{i})d t}\\end{array}$ . Letting $\\begin{array}{r}{\\mathbf{Q}=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{Q}_{i}}\\end{array}$ , we can write ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bigg\\|\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}(\\nabla f_{i}(x+u_{i})-\\nabla f_{i}(x))\\bigg\\|^{2}=}&{\\bigg\\|\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\mathbf{Q}_{i}u_{i}\\bigg\\|^{2}}\\\\ &{=\\bigg\\|\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}(\\mathbf{Q}_{i}-\\mathbf{Q})\\,u_{i}+\\mathbf{Q}\\left(\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}u_{i}\\right)\\bigg\\|^{2}}\\\\ &{\\overset{(4)}{\\leq}2\\left\\|\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}(\\mathbf{Q}_{i}-\\mathbf{Q})\\,u_{i}\\right\\|^{2}+2\\left\\|\\mathbf{Q}\\left(\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}u_{i}\\right)\\right\\|^{2}}\\\\ &{\\overset{(5)}{\\leq}2\\displaystyle\\frac{1}{n-1}\\displaystyle\\sum_{i=1}^{n}|(\\mathbf{Q}_{i}-\\mathbf{Q})\\,u_{i}|^{2}+2\\left\\|\\mathbf{Q}\\right\\|^{2}\\left\\|\\displaystyle\\frac{1}{n+1}u_{i}\\right\\|^{2}}\\\\ &{\\leq2\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}|\\mathbf{Q}_{i}-\\mathbf{Q}|^{2}\\left\\|u_{i}\\right\\|^{2}+2\\left\\|\\mathbf{Q}\\right\\|^{2}\\left\\|\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}u_{i}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Further, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\mathbf{Q}_{i}-\\mathbf{Q}\\|=\\displaystyle\\left\\|\\int_{0}^{1}\\nabla^{2}f_{i}(x+t u_{i})d t-\\frac{1}{n}\\sum_{j=1}^{n}\\int_{0}^{1}\\nabla^{2}f_{j}(x+t u_{j})d t\\right\\|}\\\\ {=}&{\\left\\|\\int_{0}^{1}\\nabla^{2}f_{i}(x+t u_{i})d t-\\int_{0}^{1}\\frac{1}{n}\\sum_{j=1}^{n}\\nabla^{2}f_{j}(x+t u_{j})d t\\right\\|}\\\\ &{=\\left\\|\\int_{0}^{1}\\frac{1}{n}\\sum_{j=1}^{n}(\\nabla^{2}f_{i}(x+t u_{i})-\\nabla^{2}f_{j}(x+t u_{j}))\\ d t\\right\\|}\\\\ {\\leq\\displaystyle\\int_{0}^{1}\\left\\|\\nabla^{2}f_{i}(x+t u_{i})-\\frac{1}{n}\\sum_{j=1}^{n}\\nabla^{2}f_{j}(x+t u_{j})\\right\\|d t}\\\\ &{\\leq\\displaystyle\\int_{0}^{1}D_{i}d t=D_{i},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\|\\mathbf{Q}\\right\\|=\\left\\|\\frac{1}{n}\\sum_{j=1}^{n}\\int_{0}^{1}\\nabla^{2}f_{j}(x+t u_{j})d t\\right\\|=\\left\\|\\int_{0}^{1}\\frac{1}{n}\\sum_{j=1}^{n}\\nabla^{2}f_{j}(x+t u_{j})d t\\right\\|}\\\\ {\\displaystyle\\qquad\\leq\\int_{0}^{1}\\left\\|\\frac{1}{n}\\sum_{j=1}^{n}\\nabla^{2}f_{j}(x+t u_{j})\\right\\|d t\\leq\\int_{0}^{1}\\frac{1}{n}\\sum_{j=1}^{n}\\left\\|\\nabla^{2}f_{j}(x+t u_{j})\\right\\|d t}\\\\ {\\displaystyle\\qquad\\leq\\int_{0}^{1}\\frac{1}{n}\\sum_{j=1}^{n}L_{j}d t=\\frac{1}{n}\\sum_{j=1}^{n}L_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By combining the above, we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\nabla f_{i}(x+u_{i})-\\nabla f_{i}(x)\\right)\\right\\|^{2}\\leq2\\frac{1}{n}\\sum_{i=1}^{n}D_{i}^{2}\\left\\|u_{i}\\right\\|^{2}+2\\left(\\frac{1}{n}\\sum_{j=1}^{n}L_{j}\\right)^{2}\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}u_{i}\\right\\|^{2}}}\\\\ &{}&{\\leq2\\left(\\underset{i}{\\operatorname*{max}}D_{i}^{2}\\right)\\left\\|u_{i}\\right\\|^{2}+2\\left(\\frac{1}{n}\\sum_{j=1}^{n}L_{j}\\right)^{2}\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}u_{i}\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which means that Assumption 4.2 holds with $L_{A}^{2}=2(\\operatorname*{max}_{i}D_{i}^{2})$ and $\\begin{array}{r}{L_{B}^{2}=2\\left(\\frac{1}{n}\\sum_{i=1}^{n}L_{i}\\right)^{2}}\\end{array}$ \uff1a\u53e3 ", "page_idx": 19}, {"type": "text", "text": "Remark C.3. Clearly, if Assumption 1.5 holds, i.e., if there exists $\\begin{array}{r l r}{L_{i}}&{{}\\ge}&{0}\\end{array}$ such that $\\begin{array}{r l r}{\\operatorname*{sup}_{z_{i}\\in\\mathbb{R}^{d}}\\left\\|\\nabla^{2}f_{i}\\bar{(z_{i})}\\right\\|}&{{}\\leq}&{\\dot{L_{i}}}\\end{array}$ for all $\\begin{array}{r l r}{i}&{{}\\in}&{[n]}\\end{array}$ , then there exists\" $D_{i}$ such that $\\begin{array}{r}{\\operatorname*{sup}_{z_{1},\\dotsc,z_{n}\\in\\mathbb{R}^{d}}\\left\\|\\nabla^{2}f_{i}(z_{i})-\\frac{1}{n}\\sum_{j=1}^{n}\\nabla^{2}f_{j}(z_{j})\\right\\|\\ \\leq\\ D_{i}}\\end{array}$ which means that this later condition is not restrictive. Indeed, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bigg\\|\\nabla^{2}f_{i}(z_{i})-\\displaystyle\\frac{1}{n}\\sum_{j=1}^{n}\\nabla^{2}f_{j}(z_{j})\\bigg\\|\\leq\\big\\|\\nabla^{2}f_{i}(z_{i})\\big\\|+\\Bigg\\|\\displaystyle\\frac{1}{n}\\sum_{j=1}^{n}\\nabla^{2}f_{j}(z_{j})\\Bigg\\|}&{}\\\\ {\\leq\\big\\|\\nabla^{2}f_{i}(z_{i})\\big\\|+\\displaystyle\\frac{1}{n}\\sum_{j=1}^{n}\\big\\|\\nabla^{2}f_{j}(z_{j})\\big\\|}&{}\\\\ {\\leq L_{i}+\\displaystyle\\frac{1}{n}\\sum_{j=1}^{n}L_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "However, $D_{i}$ can be small even if the constants $\\{L_{i}\\}$ are large, as the next theorem shows. ", "page_idx": 20}, {"type": "text", "text": "Theorem 4.9. Assume that $\\begin{array}{r}{f_{i}(x)=\\frac{1}{2}{x^{\\top}}{\\mathbf{A}}_{i}x+b_{i}^{\\top}x+c_{i}}\\end{array}$ ,where $\\mathbf{A}_{i}\\in\\mathbb{R}^{d\\times d}$ is symmetric but not necessarily postive semidefnite, $b_{i}\\in\\mathbb{R}^{d}$ and $c_{i}\\in\\mathbb{R}$ for $i\\in[n]$ .Define $\\textstyle\\mathbf{A}={\\frac{1}{n}}\\sum_{i=1}^{n}\\mathbf{A}_{i}$ . Then, Assumption 4.2 holds with $L_{A}=\\sqrt{2}\\,\\mathrm{max}_{i\\in[n]}\\,\\|\\mathbf{A}_{i}-\\mathbf{A}\\|$ and $\\begin{array}{r}{L_{B}=\\sqrt{2}\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\|\\mathbf{A}_{i}\\|\\right)}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "Proof. In this case $\\nabla^{2}f_{i}(z_{i})\\equiv\\mathbf{A}_{i}$ , and the result easily follows from Theorem 4.8. ", "page_idx": 20}, {"type": "text", "text": "D  Convergence of MARINA-P in the General Case ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "D.1 Main Results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "As promised, we now present a result generalizing Theorem 4.6 to all unbiased compressors. ", "page_idx": 20}, {"type": "text", "text": "Theorem D.1. Let Assumptions 1.1, 1.2 and 4.2 be satisfied and suppose that $\\left\\{{\\mathcal{C}}_{i}^{t}\\right\\}_{i=1}^{n}\\,\\in\\,\\mathbb{P}(\\boldsymbol{\\theta})$ (Def.A.3) and $\\mathcal{C}_{i}^{t}\\in\\mathbb{U}(\\omega_{P})$ (Def. 1.4) for all $i\\in[n]$ .Let ", "page_idx": 20}, {"type": "equation", "text": "$$\n0<\\gamma\\leq\\frac{1}{L+\\sqrt{\\left(L_{A}^{2}\\omega_{P}+L_{B}^{2}\\theta\\right)\\left(\\frac{1}{p}-1\\right)}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Letting ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Psi^{t}=f(x^{t})-f^{*}+\\frac{\\gamma L_{A}^{2}}{2p}\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|w_{i}^{t}-x^{t}\\right\\|^{2}+\\frac{\\gamma L_{B}^{2}}{2p}\\left\\|w^{t}-x^{t}\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for each $T\\geq1$ we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{T-1}\\frac{1}{T}\\mathbb{E}\\left[\\left\\Vert\\nabla f(x^{t})\\right\\Vert^{2}\\right]\\leq\\frac{2\\Psi^{0}}{\\gamma T}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let us provide some important examples: ", "page_idx": 20}, {"type": "text", "text": "Theorem D.2. Let Assumptions 1.1, 1.2 and 4.2 be satisfied. Choose ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\gamma=\\left\\{\\begin{array}{l l}{\\left(L+\\sqrt{(L_{A}^{2}+L_{B}^{2})\\,\\omega_{P}/p}\\right)^{-1}}&{f o r\\;\\mathsf{S a m e R a n d K}\\,c o m p r e s s o r s}\\\\ {\\left(L+\\sqrt{\\left(L_{A}^{2}+L_{B}^{2}/n\\right)\\,\\omega_{P}/p}\\right)^{-1}}&{f o r\\;i n d e p e n d e n t\\,\\mathsf{R a n d K}\\,c o m p r e s s o r s}\\\\ {\\left(L+L_{A}\\sqrt{\\omega_{P}/p}\\right)^{-1}}&{f o r\\;\\mathsf{P e r m K}\\,c o m p r e s s o r s}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and set $w_{i}^{0}=x^{0}$ for all $i\\in[n]$ . Then MARINA-P finds an $\\varepsilon.$ stationary point after ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{T}=\\left\\{\\mathcal{O}\\left(\\frac{\\delta^{0}\\left(L+\\sqrt{\\left(L_{A}^{2}+L_{B}^{2}\\right)\\omega_{P}/p}\\right)}{\\varepsilon}\\right)\\right.}\\\\ {\\mathcal{O}\\left(\\frac{\\delta^{0}\\left(L+\\sqrt{\\left(L_{A}^{2}+L_{B}^{2}/n\\right)\\omega_{P}/p}\\right)}{\\varepsilon}\\right)}\\\\ {\\mathcal{O}\\left(\\frac{\\delta^{0}\\left(L+L_{A}\\sqrt{\\omega_{P}/p}\\right)}{\\varepsilon}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for SameRandK compressors forindependent RandK compressors for PermK compressors ", "page_idx": 21}, {"type": "text", "text": "iterations. ", "page_idx": 21}, {"type": "text", "text": "Remark D.3. ", "page_idx": 21}, {"type": "text", "text": "\u00b7 The result for $\\mathrm{Perm}K$ compressors proves Theorem 4.6. ", "page_idx": 21}, {"type": "text", "text": "\u00b7 The above theorem demonstrates the complexities for a) SameRand $K$ , b) independent Rand $K$ and c) Perm $K$ compressors. However, the result applies to any families of compressors such that for ali $t~\\geq~0$ we have a) $\\mathring{C_{1}^{t}}\\,=\\,.\\,.\\,.\\,=\\,\\mathring{C_{n}^{t}}\\,=\\,\\mathcal{C}^{t}\\,\\in\\,\\mathbb{U}(\\omega_{P})$ , b) $\\mathcal{C}_{1}^{t},...\\,,\\mathcal{C}_{n}^{t}\\in\\mathbb{U}(\\omega_{P})$ are independent, and c) $\\mathcal{C}_{1}^{t},\\ldots,\\mathcal{C}_{n}^{t}\\in\\mathbb{U}(\\omega_{P})\\cap\\mathbb{P}(\\theta)$ , respectively. ", "page_idx": 21}, {"type": "text", "text": "We now derive the communication complexities: ", "page_idx": 21}, {"type": "text", "text": "Corollary D.4. Let us take $p=1/n$ andset $K=d/n$ (corresponding to the sparsification level of a PermK compressor). Then, in the view of Theorem D.2, the average $s2w$ communicationcomplexity ofMARINA-Pis ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\mathcal{O}\\left(\\frac{d\\delta^{0}L}{n\\varepsilon}+\\frac{d\\delta^{0}}{\\varepsilon}\\sqrt{L_{A}^{2}+L_{B}^{2}}\\right)}&{f o r\\;\\mathsf{S a m e R a n d K}\\;c o m p r e s s o r s}\\\\ {\\mathcal{O}\\left(\\frac{d\\delta^{0}L}{n\\varepsilon}+\\frac{d\\delta^{0}}{\\varepsilon}\\sqrt{L_{A}^{2}+\\frac{L_{B}^{2}}{n}}\\right)}&{f o r\\;i n d e p e n d e n t\\;\\mathsf{R a n d K}\\;c o m p r e s s o r s}\\\\ {\\mathcal{O}\\left(\\frac{d\\delta^{0}L}{n\\varepsilon}+\\frac{d\\delta^{0}}{\\varepsilon}L_{A}\\right)}&{f o r\\;\\mathsf{P e r m K}\\;c o m p r e s s o r s}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Remark D.5. ", "page_idx": 21}, {"type": "text", "text": "\u00b7 The result for $\\mathrm{Perm}K$ compressors proves Corollary 4.7. ", "page_idx": 21}, {"type": "text", "text": "\u00b7 The key observation from (19) is the dependence on $L_{A}$ and $L_{B}$ . In particular, if $L_{A}\\approx0$ (which is the case, e.g., for homogeneous quadratics), the above communication complexities are $\\left\\{\\begin{array}{l l}{\\mathcal{O}\\left(\\frac{\\delta^{0}}{\\varepsilon}d\\left(\\frac{L}{n}+L_{B}\\right)\\right)}&{\\mathrm{for~\\mathsf{SameRandK~compressors},}}\\\\ {\\mathcal{O}\\left(\\frac{\\delta^{0}}{\\varepsilon}d\\left(\\frac{L}{n}+\\frac{L_{B}}{\\sqrt{n}}\\right)\\right)}&{\\mathrm{for~independent~\\mathsf{RandK~compressors},}}\\\\ {\\mathcal{O}\\left(\\frac{\\delta^{0}}{\\varepsilon}d\\frac{L}{n}\\right)}&{\\mathrm{for~\\mathsf{PermK~compressors}.}}\\end{array}\\right.$ ", "page_idx": 21}, {"type": "text", "text": "Hence, only by sending different messages to different clients, one obtains complexities improving with $n$ . In particular, for $\\mathrm{Perm}K$ , the complexity scales linearly with the number ofworkers. ", "page_idx": 21}, {"type": "text", "text": "D.2Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To prove the results from the previous section, we first establish several identities and inequalities satisfied by the sequences $\\{\\dot{w}_{1}^{t},\\dots,w_{n}^{t}\\}_{t\\ge0}$ We start by studying the evolution of the quantity $\\left\\|w_{i}^{t}-x^{t}\\right\\|^{2}$ In what follows, $\\mathbb{E}_{t}\\left[\\cdot\\right]$ denotes the expectation conditioned on the first $t$ iterations. ", "page_idx": 21}, {"type": "text", "text": "Lemma D.6. Let $\\mathcal{C}_{i}^{t}\\in\\mathbb{U}(\\omega_{P})$ for all $i\\in[n]$ .Then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\left\\|w_{i}^{t+1}-x^{t+1}\\right\\|^{2}\\right]\\leq(1-p)\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\left\\|w_{i}^{t}-x^{t}\\right\\|^{2}\\right]+(1-p)\\omega_{P}\\mathbb{E}\\left[\\left\\|x^{t+1}-x^{t}\\right\\|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. In the view of definition of $w^{t+1}$ , we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}\\left[\\left\\Vert w_{i}^{t+1}-x^{t+1}\\right\\Vert^{2}\\right]}\\\\ &{\\quad=\\quad(1-p)\\mathbb{E}_{t}\\left[\\left\\Vert w_{i}^{t}+\\mathcal{C}_{i}^{t}(x^{t+1}-x^{t})-x^{t+1}\\right\\Vert^{2}\\right]}\\\\ &{\\quad\\overset{(48)}{=}\\quad(1-p)\\mathbb{E}_{t}\\left[\\left\\Vert\\mathcal{C}_{i}^{t}(x^{t+1}-x^{t})-(x^{t+1}-x^{t})\\right\\Vert^{2}\\right]+(1-p)\\mathbb{E}_{t}\\left[\\left\\Vert w_{i}^{t}-x^{t}\\right\\Vert^{2}\\right]}\\\\ &{\\quad\\overset{\\mathrm{Def.l.}4}{\\leq}(1-p)\\omega_{P}\\left\\Vert x^{t+1}-x^{t}\\right\\Vert^{2}+(1-p)\\left\\Vert w_{i}^{t}-x^{t}\\right\\Vert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Averaging, taking expectation and using the tower property, we get the result ", "page_idx": 22}, {"type": "text", "text": "This lemma is less powerful: it is not an identity, and hence some information is lost. Moreover, it focuses on a single client $i$ , and is therefore not able to take advantage of the correlation among the compressors. On the other hand, it can be used in the convergence analysis without any need to restrict the function class. ", "page_idx": 22}, {"type": "text", "text": "Next, we study the evolution of the quantity $\\lVert\\boldsymbol{w}^{t}-\\boldsymbol{x}^{t}\\rVert^{2}$ ", "page_idx": 22}, {"type": "text", "text": "Lemma D.7. Let $\\left\\{{\\mathcal{C}}_{i}^{t}\\right\\}_{i=1}^{n}\\in\\mathbb{P}(\\theta)$ Then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|w^{t+1}-x^{t+1}\\right\\|^{2}\\right]\\leq(1-p)\\mathbb{E}\\left[\\left\\|w^{t}-x^{t}\\right\\|^{2}\\right]+(1-p)\\theta\\mathbb{E}\\left[\\left\\|x^{t+1}-x^{t}\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. In the view of definition of $w^{t+1}$ , we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}\\left[\\Big\\|w^{t+1}-x^{t+1}\\Big\\|^{2}\\right]}\\\\ {=}&{\\mathbb{E}_{t}\\left[\\left\\|\\frac{1}{n}\\sum_{i=1}^{m}u_{i}^{t+1}-x^{t+1}\\right\\|^{2}\\right]}\\\\ {=}&{(1-p)\\mathbb{E}_{t}\\left[\\left\\|\\frac{1}{n-1}\\sum_{i=1}^{m}(w_{i}^{t}+C_{i}^{\\dagger}(x^{t+1}-x^{t}))-x^{t+1}\\right\\|^{2}\\right]}\\\\ {=}&{(1-p)\\mathbb{E}_{t}\\left[\\left\\|\\frac{1}{n}\\sum_{i=1}^{m}(x^{t+1}-x^{t})-(x^{t+1}-x^{t})-x^{t}+w^{t}\\right\\|^{2}\\right]}\\\\ {\\overset{(a\\leq)}{\\leq}}&{(1-p)\\mathbb{E}_{t}\\left[\\left\\|\\frac{1}{n}\\sum_{i=1}^{m}(x^{t+1}-x^{t})-(x^{t+1}-x^{t})\\right\\|^{2}\\right]+(1-p)\\mathbb{E}_{t}\\left[\\left\\|w^{t}-x^{t}\\right\\|^{2}\\right]}\\\\ {\\overset{(b\\leq)}{\\leq}}&{(1-p)\\mathbb{E}_{t}\\left[\\Bigg\\|x^{t}+x^{t}\\right\\|^{2}+(1-p)\\left\\|w^{t}-x^{t}\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Taking expectation and using the tower property, we get the result ", "page_idx": 22}, {"type": "text", "text": "This lemma is more powerful since it is able to take advantage of the correlation among the compressors. Indeed, if $\\theta=0$ (as in the case of Perm $K$ compressors), then it becomes an identity: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|w^{t+1}-x^{t+1}\\right\\|^{2}\\right]=(1-p)\\mathbb{E}\\left[\\left\\|w^{t}-x^{t}\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We now prove convergence of MARINA-P in the general case. ", "page_idx": 22}, {"type": "text", "text": "Theorem D.1. Let Assumptions 1.1, 1.2 and 4.2 be satisfied and suppose that $\\left\\{{\\mathcal{C}}_{i}^{t}\\right\\}_{i=1}^{n}\\,\\in\\,\\mathbb{P}(\\boldsymbol{\\theta})$ (Def. A.3) and $\\mathcal{C}_{i}^{t}\\in\\mathbb{U}(\\omega_{P})$ (Def. 1.4) for all $i\\in[n]$ .Let ", "page_idx": 22}, {"type": "equation", "text": "$$\n0<\\gamma\\leq\\frac{1}{L+\\sqrt{\\left(L_{A}^{2}\\omega_{P}+L_{B}^{2}\\theta\\right)\\left(\\frac{1}{p}-1\\right)}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Letting ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Psi^{t}=f(x^{t})-f^{*}+\\frac{\\gamma L_{A}^{2}}{2p}\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|w_{i}^{t}-x^{t}\\right\\|^{2}+\\frac{\\gamma L_{B}^{2}}{2p}\\left\\|w^{t}-x^{t}\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for each $T\\geq1$ we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{T-1}\\frac{1}{T}\\mathbb{E}\\left[\\left\\Vert\\nabla f(x^{t})\\right\\Vert^{2}\\right]\\leq\\frac{2\\Psi^{0}}{\\gamma T}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. First, combining the inequalities in Lemmas D.6 and D.7, we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\gamma L_{\\beta}^{2}}{2p}\\frac{1}{n}\\displaystyle\\frac{n}{\\sum_{i=1}^{n}}\\mathbb{E}\\left[\\left\\|w_{i}^{t+1}-x^{t+1}\\right\\|^{2}\\right]+\\frac{\\gamma L_{\\beta}^{2}}{2p}\\mathbb{E}\\left[\\left\\|w^{t+1}-x^{t+1}\\right\\|^{2}\\right]}\\\\ &{\\leq\\frac{\\gamma L_{\\alpha}^{2}}{2p}(1-p)\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\left\\|w_{i}^{t}-x^{t}\\right\\|^{2}\\right]+\\frac{\\gamma L_{\\alpha}^{2}}{2p}(1-p)\\omega p\\mathbb{E}\\left[\\left\\|x^{t+1}-x^{t}\\right\\|^{2}\\right]}\\\\ &{\\quad+\\frac{\\gamma L_{\\beta}^{2}}{2p}(1-p)\\mathbb{E}\\left[\\left\\|w^{t}-x^{t}\\right\\|^{2}\\right]+\\frac{\\gamma L_{\\beta}^{2}}{2p}(1-p)\\theta\\mathbb{E}\\left[\\left\\|x^{t+1}-x^{t}\\right\\|^{2}\\right]}\\\\ &{=\\frac{\\gamma L_{\\alpha}^{2}}{2p}(1-p)\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\left\\|w_{i}^{t}-x^{t}\\right\\|^{2}\\right]+\\frac{\\gamma L_{\\beta}^{2}}{2p}(1-p)\\mathbb{E}\\left[\\left\\|w^{t}-x^{t}\\right\\|^{2}\\right]}\\\\ &{\\quad+\\frac{\\gamma}{2p}\\left(L_{\\alpha}^{2}\\omega_{P}+L_{\\beta}^{2}\\theta\\right)(1-p)\\mathbb{E}\\left[\\left\\|x^{t+1}-x^{t}\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Next, using Assumption 4.2, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbb{E}\\left[\\left\\Vert g^{t}-\\nabla f(x^{t})\\right\\Vert^{2}\\right]}&{=}&{\\mathbb{E}\\left[\\left\\Vert\\displaystyle\\frac1n\\sum_{i=1}^{n}\\left(\\nabla f_{i}(w_{i}^{t})-\\nabla f_{i}(x^{t})\\right)\\right\\Vert^{2}\\right]}\\\\ &{\\overset{\\mathrm{Ass.4.}2}{\\le}L_{A}^{2}\\displaystyle\\frac1n\\sum_{i=1}^{n}\\mathbb{E}\\left[\\left\\Vert w_{i}^{t}-x^{t}\\right\\Vert^{2}\\right]+L_{B}^{2}\\mathbb{E}\\left[\\left\\Vert\\displaystyle\\frac1n\\sum_{i=1}^{n}w_{i}^{t}-x^{t}\\right\\Vert^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining the above inequality with Lemma H.1 gives ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\Xi\\left[\\delta^{t+1}\\right]\\leq\\mathbb{E}\\left[\\delta^{t}\\right]-\\frac{\\gamma}{2}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{t})\\right\\|^{2}\\right]-\\left(\\frac{1}{2\\gamma}-\\frac{L}{2}\\right)\\mathbb{E}\\left[\\left\\|x^{t+1}-x^{t}\\right\\|^{2}\\right]+\\frac{\\gamma}{2}\\mathbb{E}\\left[\\left\\|g^{t}-\\nabla f(x^{t})\\right\\|^{2}\\right]}}\\\\ &{}&{\\leq\\mathbb{E}\\left[\\delta^{t}\\right]-\\frac{\\gamma}{2}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{t})\\right\\|^{2}\\right]-\\left(\\frac{1}{2\\gamma}-\\frac{L}{2}\\right)\\mathbb{E}\\left[\\left\\|x^{t+1}-x^{t}\\right\\|^{2}\\right]}\\\\ &{}&{\\quad+\\frac{\\gamma}{2}\\left(L_{A}^{2}\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left[\\left\\|w_{i}^{t}-x^{t}\\right\\|^{2}\\right]+L_{B}^{2}\\mathbb{E}\\left[\\left\\|w^{t}-x^{t}\\right\\|^{2}\\right]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By adding inequalities (20) and (21), we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\Psi^{t+1}\\right]=\\mathbb{E}\\left[\\delta^{t+1}\\right]+\\frac{\\gamma L_{\\mathbf{A}}^{2}}{2p}\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\left\\Vert w_{i}^{t+1}-x^{t+1}\\right\\Vert^{2}\\right]+\\frac{\\gamma L_{\\mathbf{B}}^{2}}{2p}\\mathbb{E}\\left[\\left\\Vert w^{t+1}-x^{t+1}\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\delta^{t}\\right]-\\frac{\\gamma}{2}\\mathbb{E}\\left[\\left\\Vert\\nabla f(x^{t})\\right\\Vert^{2}\\right]-\\left(\\frac{1}{2\\gamma}-\\frac{L}{2}\\right)\\mathbb{E}\\left[\\left\\Vert x^{t+1}-x^{t}\\right\\Vert^{2}\\right]}\\\\ &{\\quad+\\frac{\\gamma}{2}\\left(L_{\\mathbf{A}}^{2}\\frac{1}{n}\\underset{i=1}{\\overset{n}{\\sum}}\\mathbb{E}\\left[\\left\\Vert w_{i}^{t}-x^{t}\\right\\Vert^{2}\\right]+L_{B}^{2}\\mathbb{E}\\left[\\left\\Vert w^{t}-x^{t}\\right\\Vert^{2}\\right]\\right)}\\\\ &{\\quad+\\frac{\\gamma L_{\\mathbf{A}}^{2}}{2p}(1-p)\\frac{1}{n}\\underset{i=1}{\\overset{n}{\\sum}}\\mathbb{E}\\left[\\left\\Vert w_{i}^{t}-x^{t}\\right\\Vert^{2}\\right]+\\frac{\\gamma L_{B}^{2}}{2p}(1-p)\\mathbb{E}\\left[\\left\\Vert w^{t}-x^{t}\\right\\Vert^{2}\\right]}\\\\ &{\\quad+\\frac{\\gamma}{2p}\\left(L_{A}^{2}\\omega_{P}+L_{B}^{2}\\theta\\right)(1-p)\\mathbb{E}\\left[\\left\\Vert x^{t+1}-x^{t}\\right\\Vert^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathbb{E}\\left[\\Psi^{t}\\right]-\\frac{\\gamma}{2}\\mathbb{E}\\left[\\left\\Vert\\nabla f(x^{t})\\right\\Vert^{2}\\right]}\\\\ &{\\quad-\\left(\\frac{1}{2\\gamma}-\\frac{L}{2}-\\frac{\\gamma}{2p}\\left(L_{A}^{2}\\omega_{P}+L_{B}^{2}\\theta\\right)(1-p)\\right)\\mathbb{E}\\left[\\left\\Vert x^{t+1}-x^{t}\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\Psi^{t}\\right]-\\frac{\\gamma}{2}\\mathbb{E}\\left[\\left\\Vert\\nabla f(x^{t})\\right\\Vert^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where in the last line we use the assumption on the step size and Lemma H.2. Summing up the above inequality for $t=0,1,\\ldots,T-1$ and rearranging the terms, we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{t})\\right\\|^{2}\\right]\\leq\\frac{2}{\\gamma T}\\left(\\mathbb{E}\\left[\\Psi^{0}\\right]-\\mathbb{E}\\left[\\Psi^{T}\\right]\\right)\\leq\\frac{2\\Psi^{0}}{\\gamma T}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "With the above result, we can establish the iteration and communication complexities of MARINA-P for three different compression schemes described in Appendix A. First, let us prove a result when independent compressors are used. ", "page_idx": 24}, {"type": "text", "text": "Theorem D.8. Let Assumptions 1.1, 1.2 and 4.2 be satisfied and suppose that $\\mathcal{C}_{1}^{t},\\ldots,\\mathcal{C}_{n}^{t}$ is $a$ collection of independent compressors (Assumption 1.6) such that $\\mathcal{C}_{i}^{t}\\in\\mathbb{U}(\\omega)$ for all $i\\in[n]$ $t\\in\\mathbb{N}$ Choose ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\gamma=\\bigg(L+\\sqrt{\\big(L_{A}^{2}+L_{B}^{2}\\big/n\\big)\\;\\omega_{P}\\big/p}\\bigg)^{-1}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and set $w_{i}^{0}=x^{0}$ for all $i\\in[n]$ .Then MARINA-P finds an $\\varepsilon$ -stationary point after ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\bar{T}=\\mathcal{O}\\left(\\frac{\\delta^{0}\\left(L+\\sqrt{\\left(L_{A}^{2}+L_{B}^{2}/n\\right)\\omega_{P}/p}\\right)}{\\varepsilon}\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "iterations. ", "page_idx": 24}, {"type": "text", "text": "Proof. In view of Theorem D.1, the step size satisfies the inequality ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\gamma\\leq\\frac{1}{L+\\sqrt{\\left(L_{A}^{2}\\omega_{P}+L_{B}^{2}\\theta\\right)\\left(\\frac{1}{p}-1\\right)}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since by Lemma A.5, when the compressors are independent we have $\\theta\\,=\\,\\omega_{P}/{n}$ , the algorithm converges in ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{T}=\\frac{\\Psi^{0}}{\\varepsilon}\\left(L+\\sqrt{\\left(L_{A}^{2}\\omega_{P}+L_{B}^{2}\\frac{\\omega_{P}}{n}\\right)\\left(\\frac{1}{p}-1\\right)}\\right)}\\\\ &{\\quad=\\mathcal{O}\\left(\\frac{\\Psi^{0}}{\\varepsilon}\\left(L+\\sqrt{\\frac{\\omega_{P}}{p}\\left(L_{A}^{2}+\\frac{L_{B}^{2}}{n}\\right)}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "iterations. ", "page_idx": 24}, {"type": "text", "text": "Theorem D.2. Let Assumptions 1.1, 1.2 and 4.2 be satisfed. Choose ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\gamma=\\left\\{\\begin{array}{l l}{\\left(L+\\sqrt{(L_{A}^{2}+L_{B}^{2})\\,\\omega_{P}/p}\\right)^{-1}}&{f o r\\;\\mathsf{S a m e R a n d K}\\,c o m p r e s s o r s}\\\\ {\\left(L+\\sqrt{\\left(L_{A}^{2}+L_{B}^{2}/n\\right)\\,\\omega_{P}/p}\\right)^{-1}}&{f o r\\;i n d e p e n d e n t\\,\\mathsf{R a n d K}\\,c o m p r e s s o r s}\\\\ {\\left(L+L_{A}\\sqrt{\\omega_{P}/p}\\right)^{-1}}&{f o r\\;\\mathsf{P e r m K}\\,c o m p r e s s o r s}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and set $w_{i}^{0}=x^{0}$ for all $i\\in[n]$ . Then MARINA-P finds an $\\varepsilon.$ stationary point after ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{T}=\\left\\{\\mathcal{O}\\left(\\frac{\\delta^{0}\\left(L+\\sqrt{\\left(L_{A}^{2}+L_{B}^{2}\\right)^{\\omega_{P}/p}}\\right)}{\\varepsilon}\\right)\\right.}\\\\ {\\mathcal{O}\\left(\\frac{\\delta^{0}\\left(L+\\sqrt{\\left(L_{A}^{2}+L_{B}^{2}/n\\right)^{\\omega_{P}/p}}\\right)}{\\varepsilon}\\right)}\\\\ {\\mathcal{O}\\left(\\frac{\\delta^{0}\\left(L+L_{A}\\sqrt{\\omega_{P}/p}\\right)}{\\varepsilon}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for SameRandK compressors for independent RandK compressors for PermK compressors ", "page_idx": 25}, {"type": "text", "text": "iterations. ", "page_idx": 25}, {"type": "text", "text": "Proof. In view of Theorem D.1, the step size is such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\gamma\\leq\\frac{1}{L+\\sqrt{\\left(L_{A}^{2}\\omega_{P}+L_{B}^{2}\\theta\\right)\\left(\\frac{1}{p}-1\\right)}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We now use Lemma A.6 and substitute the vales of $\\theta$ specific to each compression type. For SameRand $K$ wehave $\\theta=\\omega_{P}$ , so the algorithm converges after ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\bar{T}=\\frac{\\Psi^{0}}{\\varepsilon}\\left(L+\\sqrt{(L_{A}^{2}\\omega_{P}+L_{B}^{2}\\omega_{P})\\left(\\frac{1}{p}-1\\right)}\\right)=\\mathcal{O}\\left(\\frac{\\Psi^{0}}{\\varepsilon}\\left(L+\\sqrt{\\frac{\\omega_{P}}{p}\\left(L_{A}^{2}+L_{B}^{2}\\right)}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "iterations. Following the same reasoning as in the proof of Theorem D.8, for Rand $K$ wehave ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\bar{T}=\\frac{\\Psi^{0}}{\\varepsilon}\\left(L+\\sqrt{\\left(L_{A}^{2}\\omega_{P}+L_{B}^{2}\\frac{\\omega_{P}}{n}\\right)\\left(\\frac{1}{p}-1\\right)}\\right)=\\mathcal{O}\\left(\\frac{\\Psi^{0}}{\\varepsilon}\\left(L+\\sqrt{\\frac{\\omega_{P}}{p}\\left(L_{A}^{2}+\\frac{L_{B}^{2}}{n}\\right)}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Finally, for PermK we have $\\theta=0$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\bar{T}=\\frac{\\Psi^{0}}{\\varepsilon}\\left(L+\\sqrt{L_{A}^{2}\\omega_{P}\\left(\\frac{1}{p}-1\\right)}\\right)=\\mathcal{O}\\left(\\frac{\\Psi^{0}}{\\varepsilon}\\left(L+L_{A}\\sqrt{\\frac{\\omega_{P}}{p}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The result follows from the fact that $w_{i}^{0}=x^{0}$ for all $i\\in[n]$ ", "page_idx": 25}, {"type": "text", "text": "Corollary D.4. Let us take $p=1/n$ andset $K=d/n$ (corresponding to the sparsification level of a PermK compressor). Then, in the view of Theorem $D.2$ the average $s2w$ communication complexity ofMARINA-P is ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\mathcal{O}\\left(\\frac{d\\delta^{0}L}{n\\varepsilon}+\\frac{d\\delta^{0}}{\\varepsilon}\\sqrt{L_{A}^{2}+L_{B}^{2}}\\right)}&{f o r\\;\\mathsf{S a m e R a n d K}\\;c o m p r e s s o r s}\\\\ {\\mathcal{O}\\left(\\frac{d\\delta^{0}L}{n\\varepsilon}+\\frac{d\\delta^{0}}{\\varepsilon}\\sqrt{L_{A}^{2}+\\frac{L_{B}^{2}}{n}}\\right)}&{f o r\\;i n d e p e n d e n t\\;\\mathsf{R a n d K}\\;c o m p r e s s o r s}\\\\ {\\mathcal{O}\\left(\\frac{d\\delta^{0}L}{n\\varepsilon}+\\frac{d\\delta^{0}}{\\varepsilon}L_{A}\\right)}&{f o r\\;\\mathsf{P e r m K}\\;c o m p r e s s o r s}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. The expected number of floats a server is relaying to each client at each iteration of MARINA-P is ", "page_idx": 25}, {"type": "equation", "text": "$$\np d+(1-p)k=\\frac{d}{n}+\\frac{n-1}{n}k\\leq\\frac{2d}{n}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Next, using the results from Lemma A.6, our choice of compressors and parameters gives $\\omega_{P}=$ $d/K-1=n-1$ in each of the three cases. Hence, substituting $p={^{1}\\!/}n$ in (23), (24) and (25), we obtain the following server-to-worker communication complexities: ", "page_idx": 25}, {"type": "text", "text": "1. for SameRand $K$ compressors: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\frac{d}{n}\\times\\frac{\\delta^{0}}{\\varepsilon}\\left(L+\\sqrt{\\omega_{P}\\left(L_{A}^{2}+L_{B}^{2}\\right)\\left(\\frac{1}{p}-1\\right)}\\right)=\\frac{\\delta^{0}}{\\varepsilon}\\left(\\frac{d}{n}L+\\frac{d}{n}\\sqrt{\\left(L_{A}^{2}+L_{B}^{2}\\right)\\left(n-1\\right)^{2}}\\right)}\\\\ &{}&{=\\mathcal{O}\\left(\\frac{\\delta^{0}}{\\varepsilon}\\left(\\frac{d}{n}L+d\\sqrt{L_{A}^{2}+L_{B}^{2}}\\right)\\right),\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "2. for RandK compressors: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{d}{n}\\times\\frac{\\delta^{0}}{\\varepsilon}\\left(L+\\sqrt{\\omega_{P}\\left(L_{A}^{2}+\\frac{L_{B}^{2}}{n}\\right)\\left(\\frac{1}{p}-1\\right)}\\right)=\\frac{\\delta^{0}}{\\varepsilon}\\left(\\frac{d}{n}L+\\frac{d}{n}\\sqrt{\\left(L_{A}^{2}+\\frac{L_{B}^{2}}{n}\\right)(n-1)^{2}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathcal{O}\\left(\\frac{\\delta^{0}}{\\varepsilon}\\left(\\frac{d}{n}L+d\\sqrt{L_{A}^{2}+\\frac{L_{B}^{2}}{n}}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "3. for $\\mathrm{Perm}K$ compressors: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\frac{d}{n}\\times\\frac{\\delta^{0}}{\\varepsilon}\\left(L+\\sqrt{L_{A}^{2}\\omega_{P}\\left(\\frac{1}{p}-1\\right)}\\right)=\\frac{\\delta^{0}}{\\varepsilon}\\left(\\frac{d}{n}L+\\frac{d}{n}L_{A}\\sqrt{(n-1)^{2}}\\right)}\\\\ &{}&{=\\mathcal{O}\\left(\\frac{\\delta^{0}}{\\varepsilon}\\left(\\frac{d}{n}L+d L_{A}\\right)\\right).\\quad\\,\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "D.3 Polyak-Lojasiewicz condition ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "D.3.1 Main Results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "To complete the theory, we now establish a convergence result for MARINA-P under the PolyakLojasiewicz assumption. ", "page_idx": 26}, {"type": "text", "text": "Assumption D.9 (Polyak-Lojasiewicz condition). The function $f$ satisfies Polyak-Lojasiewicz $(\\mathrm{PE})$ condition with parameter $\\mu$ i.e, for all $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ there exists $x^{*}\\in\\arg\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}f(x)$ such that ", "page_idx": 26}, {"type": "equation", "text": "$$\n2\\mu\\left(f(x)-f(x^{*})\\right)\\leq\\left\\|\\nabla f(x)\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Theorem D.10. Let Assumptions 1.1, 1.2, 4.2 and $D.9$ be satisfied and suppose that $\\left\\{{\\mathcal{C}}_{i}^{t}\\right\\}_{i=1}^{n}\\in\\mathbb{P}(\\theta)$ and $\\mathcal{C}_{i}^{t}\\in\\mathbb{U}(\\omega_{P})$ for all $i\\in[n]$ Take ", "page_idx": 26}, {"type": "equation", "text": "$$\n0<\\gamma\\leq\\operatorname*{min}\\left\\{{\\frac{1}{L+{\\sqrt{2\\left(L_{A}^{2}\\omega_{P}+L_{B}^{2}\\theta\\right)\\left({\\frac{1}{p}}-1\\right)}}}},{\\frac{p}{2\\mu}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Letting ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\Psi^{t}=f(x^{t})-f^{*}+\\frac{\\gamma L_{A}^{2}}{p}\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\left\\Vert w_{i}^{t}-x^{t}\\right\\Vert^{2}\\right]+\\frac{\\gamma L_{B}^{2}}{p}\\mathbb{E}\\left[\\left\\Vert\\frac{1}{n}\\sum_{i=1}^{n}w_{i}^{t}-x^{t}\\right\\Vert^{2}\\right],\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for each $T\\geq1$ we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Psi^{T}\\right]\\leq\\left(1-\\gamma\\mu\\right)^{T}\\Psi^{0}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Corollary D.11. Let $\\mathcal{C}_{i}^{t}\\in\\mathbb{P}(0)$ for all $i\\in[n]$ (e.g. PermK), choose $p=1/(\\omega_{P}+1)$ . Then, in the view of Theorem D.10, Algorithm $^{\\,l}$ ensures that E $:\\left[f(x^{T})-f^{*}\\right]\\leq\\varepsilon$ after ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\operatorname*{max}\\left\\{\\frac{L+L_{A}\\omega_{P}}{\\mu},\\omega_{P}+1\\right\\}\\log\\frac{\\Psi^{0}}{\\varepsilon}\\right)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "iterations. ", "page_idx": 26}, {"type": "text", "text": "Corollary D.12. Let $\\mathcal{C}_{i}^{t}$ be thePermK compressors $'K=d/n_{\\ast}$ ). Then, in the view of Corollary D.11, the $s2w$ communication complexity of MARINA-P with PermK is ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\left(\\frac{d L}{n\\mu}+\\frac{d L_{A}}{\\mu}+d\\right)\\log\\frac{\\Psi^{0}}{\\varepsilon}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "D.3.2Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Theorem D.10. Let Assumptions 1.1 1.2, 4.2 and $D.9$ be satisfied and suppose that $\\left\\{{\\mathcal{C}}_{i}^{t}\\right\\}_{i=1}^{n}\\in\\mathbb{P}(\\theta)$ and $\\mathcal{C}_{i}^{t}\\in\\mathbb{U}(\\omega_{P})$ forall $i\\in[n]$ Take ", "page_idx": 27}, {"type": "equation", "text": "$$\n0<\\gamma\\leq\\operatorname*{min}\\left\\{{\\frac{1}{L+{\\sqrt{2\\left(L_{A}^{2}\\omega_{P}+L_{B}^{2}\\theta\\right)\\left({\\frac{1}{p}}-1\\right)}}}},{\\frac{p}{2\\mu}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Letting ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Psi^{t}=f(x^{t})-f^{*}+\\frac{\\gamma L_{A}^{2}}{p}\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\left\\Vert w_{i}^{t}-x^{t}\\right\\Vert^{2}\\right]+\\frac{\\gamma L_{B}^{2}}{p}\\mathbb{E}\\left[\\left\\Vert\\frac{1}{n}\\sum_{i=1}^{n}w_{i}^{t}-x^{t}\\right\\Vert^{2}\\right],\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for each $T\\geq1$ we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Psi^{T}\\right]\\leq\\left(1-\\gamma\\mu\\right)^{T}\\Psi^{0}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. We proceed similarly as in the proof of Theorem D.1. Combining the inequalities in Lemmas D.6 and D.7 gives ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\gamma L_{\\Delta}^{2}}{p}\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}\\left[\\left\\|w_{i}^{t+1}-x^{t+1}\\right\\|^{2}\\right]+\\frac{\\gamma L_{\\Delta}^{2}}{p}\\mathbb{E}\\left[\\left\\|w^{t+1}-x^{t+1}\\right\\|^{2}\\right]}\\\\ &{\\leq\\frac{\\gamma L_{\\Delta}^{2}}{p}(1-p)\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\left\\|w_{i}^{t}-x^{t}\\right\\|^{2}\\right]+\\frac{\\gamma L_{\\Delta}^{2}}{p}(1-p)\\omega p\\mathbb{E}\\left[\\left\\|x^{t+1}-x^{t}\\right\\|^{2}\\right]}\\\\ &{\\quad+\\frac{\\gamma L_{\\Delta}^{2}}{p}(1-p)\\mathbb{E}\\left[\\left\\|w^{t}-x^{t}\\right\\|^{2}\\right]+\\frac{\\gamma L_{\\Delta}^{2}}{p}(1-p)\\theta\\mathbb{E}\\left[\\left\\|x^{t+1}-x^{t}\\right\\|^{2}\\right]}\\\\ &{=\\frac{\\gamma L_{\\Delta}^{2}}{p}(1-p)\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\left\\|w_{i}^{t}-x^{t}\\right\\|^{2}\\right]+\\frac{\\gamma L_{\\Delta}^{2}}{p}(1-p)\\mathbb{E}\\left[\\left\\|w^{t}-x^{t}\\right\\|^{2}\\right]}\\\\ &{\\quad+\\frac{\\gamma}{p}\\left(L_{\\Delta}^{2}\\omega p+L_{\\Delta}^{2}\\theta\\right)(1-p)\\mathbb{E}\\left[\\left\\|x^{t+1}-x^{t}\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By adding inequalities (21) and (29), we get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}{\\mathbb{E}\\left[\\Psi^{t+1}\\right]}&{}&&{=\\;\\;\\;\\mathbb{E}\\left[\\delta^{t+1}\\right]+\\frac{\\gamma L_{0}^{2}}{p}\\frac{1}{n}\\frac{\\gamma}{\\ln{\\varepsilon}}\\mathbb{E}\\left[\\left\\Vert w_{i}^{t+1}-x^{t+1}\\right\\Vert^{2}\\right]+\\frac{\\gamma L_{0}^{2}}{p}\\mathbb{E}\\left[\\left\\Vert w^{t+1}-x^{t+1}\\right\\Vert^{2}\\right]}\\\\ &{}&&{\\leq\\;\\;\\;\\mathbb{E}\\left[\\delta^{t}\\right]-\\frac{\\gamma}{2}\\mathbb{E}\\left[\\left\\Vert\\nabla f(x^{t})\\right\\Vert^{2}\\right]-\\left(\\frac{1}{2\\gamma}-\\frac{L}{2}\\right)\\mathbb{E}\\left[\\left\\Vert x^{t+1}-x^{t}\\right\\Vert^{2}\\right]}\\\\ &{}&&{+\\frac{\\gamma}{2}\\left(L_{A}^{2}\\frac{1}{n}\\frac{\\gamma}{\\ln{\\varepsilon}+1}\\mathbb{E}\\left[\\left\\Vert w_{i}^{t}-x^{t}\\right\\Vert^{2}\\right]+L_{B}^{2}\\mathbb{E}\\left[\\left\\Vert w^{t}-x^{t}\\right\\Vert^{2}\\right]\\right)}\\\\ &{}&&{+\\frac{\\gamma L_{0}^{2}}{p}(1-p)\\frac{1}{n}\\sum_{i=1}^{N}\\mathbb{E}\\left[\\left\\Vert w_{i}^{t}-x^{t}\\right\\Vert^{2}\\right]+\\frac{\\gamma L_{B}^{2}}{p}(1-p)\\mathbb{E}\\left[\\left\\Vert w^{t}-x^{t}\\right\\Vert^{2}\\right]}\\\\ &{}&&{+\\frac{\\gamma}{p}(L_{A}^{2}\\omega\\gamma+L_{B}^{2}\\theta)\\left(1-p)\\mathbb{E}\\left[\\left\\Vert x^{t+1}-x^{t}\\right\\Vert^{2}\\right]}\\\\ &{}&&{\\mathbb{E}\\left[\\delta^{t}\\right]-\\frac{\\gamma}{2}\\mathbb{E}\\left[\\left\\Vert\\nabla f(x^{t})\\right\\Vert^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle-\\left(\\frac{1}{2\\gamma}-\\frac{L}{2}-\\frac{\\gamma}{p}\\left(L_{A}^{2}\\omega_{P}+L_{B}^{2}\\theta\\right)\\left(1-p\\right)\\right)\\mathbb{E}\\left[\\left\\Vert x^{t+1}-x^{t}\\right\\Vert^{2}\\right]}\\\\ &{\\displaystyle+\\gamma L_{A}^{2}\\left(\\frac{1}{p}-\\frac{1}{2}\\right)\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\left\\Vert w_{i}^{t}-x^{t}\\right\\Vert^{2}\\right]+\\gamma L_{B}^{2}\\left(\\frac{1}{p}-\\frac{1}{2}\\right)\\mathbb{E}\\left[\\left\\Vert w^{t}-x^{t}\\right\\Vert^{2}\\right]}\\\\ &{\\displaystyle\\overset{\\mathrm{As.}\\,D,\\,0.27}{\\leq}\\left(1-\\gamma\\mu\\right)\\mathbb{E}\\left[\\delta^{t}\\right]+\\frac{\\gamma L_{A}^{2}}{p}\\left(1-\\gamma\\mu\\right)\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\left\\Vert w_{i}^{t}-x^{t}\\right\\Vert^{2}\\right]}\\\\ &{\\displaystyle\\ +\\frac{\\gamma L_{B}^{2}}{p}\\left(1-\\gamma\\mu\\right)\\mathbb{E}\\left[\\left\\Vert w^{t}-x^{t}\\right\\Vert^{2}\\right]}\\\\ {=}&{\\left(1-\\gamma\\mu\\right)\\mathbb{E}\\left[\\Psi^{t}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last inequality follows from the Polyak-Lojasiewicz condition, Lemma H.2 and our choice Oof $\\gamma$ . Applying the above inequality iteratively, we finish the proof. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Corollary D.11. Let $\\mathcal{C}_{i}^{t}\\in\\mathbb{P}(0)$ for all $i\\in[n]$ (e.g. PermK), choose $p=1/(\\omega_{P}+1)$ . Then, in the view of Theorem D.10, Algorithm $^{\\,l}$ ensures that E $\\left[f(x^{T})-f^{*}\\right]\\leq\\varepsilon$ after ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\operatorname*{max}\\left\\{\\frac{L+L_{A}\\omega_{P}}{\\mu},\\omega_{P}+1\\right\\}\\log\\frac{\\Psi^{0}}{\\varepsilon}\\right)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "iterations. ", "page_idx": 28}, {"type": "text", "text": "Proof. In view of Theorem D.10, the step size satisfies ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\gamma\\leq\\operatorname*{min}\\left\\{{\\frac{1}{L+{\\sqrt{2\\left(L_{A}^{2}\\omega_{P}+L_{B}^{2}\\theta\\right)\\left({\\frac{1}{p}}-1\\right)}}}},{\\frac{p}{2\\mu}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, since $\\theta=0$ and $p=1/(\\omega_{P}+1)$ , the algorithm converges after ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{T}=\\operatorname*{max}\\left\\{\\frac{L+\\sqrt{2\\left(L_{A}^{2}\\omega_{P}+L_{B}^{2}\\theta\\right)\\left(\\frac{1}{p}-1\\right)}}{\\mu},\\frac{2}{p}\\right\\}\\log\\frac{\\Psi^{0}}{\\varepsilon}}\\\\ &{\\quad=\\mathcal{O}\\left(\\operatorname*{max}\\left\\{\\frac{L+L_{A}\\omega_{P}}{\\mu},\\omega_{P}+1\\right\\}\\log\\frac{\\Psi^{0}}{\\varepsilon}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "iterations. ", "page_idx": 28}, {"type": "text", "text": "Corollary D.12. Let $\\mathcal{C}_{i}^{t}$ be thePermK compressors $'K=d/n_{\\ast}$ ). Then, in the view of Corollary D.11, the $s2w$ communication complexity of MARINA-P with PermK is ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\left(\\frac{d L}{n\\mu}+\\frac{d L_{A}}{\\mu}+d\\right)\\log\\frac{\\Psi^{0}}{\\varepsilon}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof.For $\\mathrm{Perm}K$ $\\omega_{P}=n-1$ . Therefore, the iteration complexity is ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\operatorname*{max}\\left\\{\\frac{L+L_{A}n}{\\mu},n\\right\\}\\log\\frac{\\Psi^{0}}{\\varepsilon}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Since the expected number of floats the server is relaying to each client is ", "page_idx": 28}, {"type": "equation", "text": "$$\np d+(1-p)k=\\frac{d}{n}+\\frac{n-1}{n}k\\leq\\frac{2d}{n},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "the server-to-worker communication complexity is ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\operatorname*{max}\\left\\{\\frac{\\frac{d}{n}L+d L_{A}}{\\mu},d\\right\\}\\log\\frac{\\Psi^{0}}{\\varepsilon}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "E  Convergence of M3 in the General Case ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We now move on to the bidirectionally compressed method. Below is a generalization of Theorem 5.1 to all unbiased compressors. ", "page_idx": 29}, {"type": "text", "text": "E.1 Main Results ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Theorem E.1. Let Assumptions 1.1, 1.2, 1.5 and 4.2 hold and suppose that the compressors $\\mathcal{Q}_{i}^{t}\\in$ $\\mathbb{U}(\\omega_{D})$ satisfy Assumption 1.6, $\\left\\{{\\mathcal{C}}_{i}^{t}\\right\\}_{i=1}^{n}\\in\\mathbb{P}(\\theta)$ and $\\mathcal{C}_{i}^{t}\\in\\mathbb{U}(\\omega_{P})$ for all $i\\in[n]$ .Let $\\gamma>0$ be such that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\prime\\leq\\left(L+\\sqrt{288\\left(\\left(\\frac{\\theta}{p_{P}}+\\frac{1+\\theta p_{P}}{\\beta^{2}}\\right)L_{B}^{2}+\\left(\\frac{\\omega p}{p_{P}}+\\frac{1+\\omega_{P}p_{P}}{\\beta^{2}}\\right)L_{A}^{2}+\\left(\\frac{\\omega_{D}\\omega_{P}\\beta}{n p_{D}}+\\frac{\\omega_{D}\\left(1+\\omega_{P}p_{P}\\right)}{n p_{D}}\\right)L_{\\operatorname*{max}}^{2}\\right)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Letting ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Psi^{t}=\\delta^{t}+\\kappa\\left\\|g^{t}-\\frac1n\\sum_{i=1}^{n}\\nabla f_{i}(z_{i}^{t})\\right\\|^{2}+\\eta\\left\\|z^{t}-w^{t}\\right\\|^{2}+\\nu\\frac1n\\sum_{i=1}^{n}\\left\\|z_{i}^{t}-w_{i}^{t}\\right\\|^{2}}\\\\ {\\displaystyle\\quad+\\,\\rho\\left\\|w^{t}-x^{t}\\right\\|^{2}+\\mu\\frac1n\\sum_{i=1}^{n}\\left\\|w_{i}^{t}-x^{t}\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\begin{array}{r l}{\\kappa}&{{}=\\,\\,\\frac{\\gamma}{p_{D}}}\\end{array}$ \uff0c $\\begin{array}{r c l}{\\eta}&{=}&{\\frac{4\\gamma L_{B}^{2}}{\\beta}}\\end{array}$ $\\begin{array}{r}{\\nu\\ =\\ \\frac{4\\gamma L_{A}^{2}}{\\beta}\\,+\\,\\frac{6\\gamma\\omega_{D}\\beta L_{\\mathrm{max}}^{2}}{n p_{D}}}\\end{array}$ \uff0c $\\begin{array}{r}{\\rho\\ =\\ 32\\gamma L_{B}^{2}\\left(\\frac{1}{p_{P}}+\\frac{p_{P}}{\\beta^{2}}\\right)}\\end{array}$ and $\\mu=$ $\\begin{array}{r l r}&{}&{32\\gamma L_{A}^{2}\\left(\\frac{1}{p_{P}}+\\frac{p_{P}}{\\beta^{2}}\\right)+\\frac{48\\gamma\\omega_{D}L_{\\mathrm{max}}^{2}}{n p_{D}}\\left(\\beta+p_{P}\\right)\\!,}\\end{array}$ 48uwD2a (\u03b2 + pP),M3 ensures that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{t})\\right\\|^{2}\\right]=\\mathcal{O}\\left(\\frac{\\Psi^{0}}{\\gamma T}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We now simplify the above result by considering $\\theta=0$ ", "page_idx": 29}, {"type": "text", "text": "Corollary E.2. Let $\\mathcal{C}_{i}^{t}\\,\\in\\,\\mathbb{P}(0)$ for all $i\\,\\in\\,[n]$ (e.g. PermK), choose $p_{P}\\,=\\,1/(\\omega_{P}+1)$ $p_{D}=$ $1/(\\omega_{D}+1)$ and ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\beta=\\operatorname*{min}\\left\\{\\left(\\frac{n}{\\omega_{D}\\omega_{P}(\\omega_{D}+1)}\\right)^{1/3},1\\right\\}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then, in the view of Theorem E.1, the iteration complexity is ", "page_idx": 29}, {"type": "equation", "text": "$$\n>\\left(\\frac{\\Psi^{0}}{\\varepsilon}\\left(L_{\\operatorname*{max}}+\\left(\\frac{\\omega_{D}\\omega_{P}(\\omega_{D}+1)}{n}\\right)^{1/3}L_{\\operatorname*{max}}+\\sqrt{\\frac{\\omega_{D}(\\omega_{D}+1)}{n}}L_{\\operatorname*{max}}+\\sqrt{\\omega_{P}(\\omega_{P}+1)}L_{A}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We now give the bound for the total communication complexity of M3. ", "page_idx": 29}, {"type": "text", "text": "Corollary E.3. Let $\\mathcal{C}_{i}^{t}$ be thePermK compressors and $\\mathcal{Q}_{i}^{t}$ be the independent (Assumption 1.6) RandK compressors, both with $K=d/_{n}$ .Then, in the view of Corollary E.2, the iteration complexity s ", "page_idx": 29}, {"type": "equation", "text": "$$\n{\\mathcal O}\\left(\\frac{\\Psi^{0}}{\\varepsilon}\\left(n^{2/3}L_{\\mathrm{max}}+n L_{A}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and the total communication complexity is ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\frac{\\Psi^{0}}{\\varepsilon}\\left(\\frac{d L_{\\operatorname*{max}}}{n^{1/3}}+d L_{A}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Remark E.4. The above result proves the complexities from Theorem 5.1. ", "page_idx": 29}, {"type": "text", "text": "1: Input: initial model $x_{0}\\ \\in\\ \\mathbb{R}^{d}$ (stored on the server), initial model shifts $w_{i}^{0}\\;=\\;z_{i}^{0}\\;=\\;x^{0}$ \uff0c   \n$i\\in[n]$ (stored on the workers), initial gradient estimators $g^{0}=\\nabla f(x^{0})$ (stored on the sever),   \nstep size $\\gamma\\,>\\,0$ probabilities $0\\,<\\,p_{P},p_{D}\\,\\leq\\,1$ , compressors $\\mathcal{C}_{1}^{t},\\ldots,\\mathcal{C}_{n}^{t}\\,\\in\\,\\mathbb{U}(\\omega_{P})\\cap\\mathbb{P}(\\theta)$   \n$\\mathcal{Q}_{1}^{t},\\ldots,\\mathcal{Q}_{n}^{t}\\in\\mathbb{U}(\\omega_{D})$ for all $t\\geq0$   \n2: for $t=0,\\dots,T$ do   \n3: $x^{t+1}=x^{t}-\\gamma g^{t}$ Server takes a gradient-type step to update the global model   \n4: Sample $c_{P}^{t}\\sim\\mathrm{Bernoulli}(p_{P})$ \uff0c $c_{D}^{t}\\sim$ Bernoull(p)   \n5: For $i\\in[n]$ , send $\\mathcal{C}_{i}^{t}(x^{t+1}-x^{t}\\bar{)}$ to worker $i$ $c_{P}^{t}=0$ and $x^{t+1}$ otherwise   \n6: for $i=1,\\hdots,n$ in parallel do   \n7: $\\begin{array}{r}{w_{i}^{t+1}=\\left\\{\\!\\!\\begin{array}{l l}{x^{t+1}}&{\\mathrm{if}\\;c_{P}^{t}=1,}\\\\ {w_{i}^{t}+\\mathcal{C}_{i}^{t}(x^{t+1}-x^{t})}&{\\mathrm{if}\\;c_{P}^{t}=0,}\\end{array}\\!\\right.}\\end{array}$ Worker $i$ updates its local model shift   \n8: $z_{i}^{t+1}=\\beta w_{i}^{t+1}+(1-\\beta)z_{i}^{t}$ Worker $i$ takes the momentum step   \n9: Send $\\mathcal{Q}_{i}^{t}(\\nabla f_{i}(z_{i}^{t+1})-\\nabla f_{i}(z_{i}^{t}))$ to the server if $c_{D}^{t}=0$ and $\\nabla f_{i}(z_{i}^{t+1})$ otherwise   \n10: end for   \n11: ifcD $c_{D}^{t}=1$ then   \n12: $\\begin{array}{r}{\\overbar{g^{t+1}}=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}(z_{i}^{t+1})}\\end{array}$   \n13: else   \n14: $\\begin{array}{r}{g_{\\ldots}^{t+1}=g^{t}+\\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{Q}_{i}^{t}(\\nabla f_{i}(z_{i}^{t+1})-\\nabla f_{i}(z_{i}^{t}))}\\end{array}$   \n15: end if ", "page_idx": 30}, {"type": "text", "text": "16: end for ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "(maintaining only the sequence $g^{t}$ in the implementation is sufficient; the sequences $g_{i}^{t}$ from (14) are virtual) ", "page_idx": 30}, {"type": "text", "text": "E.2Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Similar to our approach from the previous section, we start by establishing several inequalities satisfied by the sequences $\\{w_{1}^{t},\\dots,\\stackrel{\\cdot}{{},{}}w_{n}^{t}\\}_{t\\geq0},\\,\\{z_{1}^{t},\\dots,z_{n}^{t}\\}_{t\\geq0}$ and $\\{g_{1}^{t},\\dots,g_{n}^{t}\\}_{t\\geq0}$ ", "page_idx": 30}, {"type": "text", "text": "Lemma E.5. Let $\\mathcal{C}_{i}^{t}\\in\\mathbb{U}(\\omega_{P})$ for all $i\\in[n]$ and $\\left\\{{\\mathcal{C}}_{i}^{t}\\right\\}_{i=1}^{n}\\in\\mathbb{P}(\\theta)$ .Then ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}\\left[\\left\\|w_{i}^{t+1}-z_{i}^{t}\\right\\|^{2}\\right]\\leq\\left\\|(x^{t+1}-x^{t})-p_{P}(w_{i}^{t}-x^{t})+(w_{i}^{t}-z_{i}^{t})\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left.p_{P}\\left\\|w_{i}^{t}-x^{t}\\right\\|^{2}+\\omega_{P}\\left\\|x^{t+1}-x^{t}\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for all $i\\in[n]$ , and ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}\\left[\\left\\|w^{t+1}-z^{t}\\right\\|^{2}\\right]\\leq\\left\\|(x^{t+1}-x^{t})-p_{P}(w^{t}-x^{t})+(w^{t}-z^{t})\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left.p_{P}\\left\\|w^{t}-x^{t}\\right\\|^{2}+\\theta\\left\\|x^{t+1}-x^{t}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. Using the definition of $w_{i}^{t+1}$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t}\\left[w_{i}^{t+1}\\right]=x^{t+1}+(1-p_{P})(w_{i}^{t}-x^{t})\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and hence ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t}\\left[\\left\\|w_{i}^{t+1}-z_{i}^{t}\\right\\|^{2}\\right]\\stackrel{(48)}{=}\\left\\|x^{t+1}+(1-p_{P})(w_{i}^{t}-x^{t})-z_{i}^{t}\\right\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\mathbb{E}_{t}\\left[\\left\\|w_{i}^{t+1}-(x^{t+1}+(1-p_{P})(w_{i}^{t}-x^{t}))\\right\\|^{2}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\left\\|(x^{t+1}-x^{t})-p_{P}(w_{i}^{t}-x^{t})+(w_{i}^{t}-z_{i}^{t})\\right\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\mathbb{E}_{t}\\left[\\left\\|w_{i}^{t+1}-(x^{t+1}+(1-p_{P})(w_{i}^{t}-x^{t}))\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Using the definition of $w_{i}^{t+1}$ again, we get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t}\\left[\\left\\Vert w_{i}^{t+1}-z_{i}^{t}\\right\\Vert^{2}\\right]\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{=}&{\\left\\|\\left(x^{t+1}-x^{t}\\right)-p p\\left(w_{i}^{t}-x^{t}\\right)+\\left(w_{i}^{t}-z_{i}^{t}\\right)\\right\\|^{2}}\\\\ &{+p p\\left\\|x^{t+1}-\\left(x^{t+1}+\\left(1-p p\\right)(w_{i}^{t}-x^{t})\\right)\\right\\|^{2}}\\\\ &{+(1-p p)\\mathbb{E}_{t}\\left[\\left\\|w_{i}^{t}+c_{i}^{t}(x^{t+1}-x^{t})-(x^{t+1}+(1-p r)(w_{i}^{t}-x^{t}))\\right\\|^{2}\\right]}\\\\ {=}&{\\left\\|\\left(x^{t+1}-x^{t}\\right)-p p\\left(w_{i}^{t}-x^{t}\\right)+\\left(w_{i}^{t}-z_{i}^{t}\\right)\\right\\|^{2}+p p_{P}(1-p r)^{2}\\left\\|w_{i}^{t}-x^{t}\\right\\|^{2}}\\\\ &{+(1-p p)\\mathbb{E}_{t}\\left[\\left\\|c_{i}^{t}(x^{t+1}-x^{t})-(x^{t+1}-x^{t})+p p(w_{i}^{t}-x^{t})\\right\\|^{2}\\right]}\\\\ {\\overset{(4)}{=}}&{\\left\\|\\left(x^{t+1}-x^{t}\\right)-p p\\left(w_{i}^{t}-x^{t}\\right)+\\left(w_{i}^{t}-z_{i}^{t}\\right)\\right\\|^{2}+p p_{P}(1-p r)^{2}\\left\\|w_{i}^{t}-x^{t}\\right\\|^{2}}\\\\ &{+(1-p p)p_{P}^{2}\\left\\|w_{i}^{t}-x^{t}\\right\\|^{2}+(1-p p)\\mathbb{E}_{t}\\left[\\left\\|c_{i}^{t}(x^{t+1}-x^{t})-(x^{t+1}-x^{t})\\right\\|^{2}\\right]}\\\\ {\\overset{\\mathrm{Ref.LA}}{\\leq}\\left\\|\\left(x^{t+1}-x^{t}\\right)-p p\\left(w_{i}^{t}-x^{t}\\right)+\\left(w_{i}^{t}-z_{i}^{t}\\right)\\right\\|^{2}+p p\\left\\|w_{i}^{t}-x^{t}\\right\\|^{2}+\\omega p\\left\\|x^{t+1}-x^{t}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Using the same reasoning, we now prove the second inequality: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\left\\Vert\\mathbf{a}^{t+1}-\\mathbf{z}^{t}\\right\\Vert^{2}\\right]}&{}\\\\ {\\psi^{*1}}&{=\\left\\Vert\\mathbf{\\Phi}\\left[\\alpha^{*1}+(1-p)(w^{*}-z^{t})\\right]+\\mathbf{\\Phi}\\left[\\alpha^{*1}\\right]\\left[\\alpha^{*1}+(\\alpha^{*1+}+(1-p)(w^{*}-z^{t}))\\right\\Vert^{2}\\right]}\\\\ {=}&{\\left\\Vert\\alpha^{*1}+\\mathbf{z}^{t}-p\\cdot p(w^{*}-z^{t})+(w^{*}-z^{t})\\right\\Vert^{2}}\\\\ &{\\quad+\\mathbb{E}\\left[\\left\\Vert\\mathbf{a}^{t+1}-\\mathbf{z}^{t+1}+(1-p)(w^{*}-z^{t})\\right\\Vert^{2}\\right]}\\\\ {=}&{\\left\\Vert\\alpha^{*1}+\\mathbf{z}^{t}-p\\cdot p(w^{*}-z^{t})+(w^{*}-z^{t})\\right\\Vert^{2}+p(1-p)^{2}\\left\\Vert\\alpha^{*1}-x^{t}\\right\\Vert^{2}}\\\\ &{\\quad+(1-p)\\mathbb{E}\\left[\\alpha^{*1}-\\frac{1}{\\alpha}\\sum_{x}C_{x}^{t}(z^{t+1}-x^{t})-(\\alpha^{*1+}+(1-p)(w^{*}-z^{t}))\\right\\Vert^{2}\\right]}\\\\ {=}&{\\left\\Vert\\alpha^{*1}+\\mathbf{z}^{t}-p\\cdot p(w^{*}-z^{t})+(w^{*}-z^{t})\\right\\Vert^{2}+p(1-p)^{2}\\left\\Vert\\alpha^{*1}-x^{t}\\right\\Vert^{2}}\\\\ &{\\quad+(1-p)\\mathbb{E}\\left[\\frac{1}{\\alpha}\\sum_{x}C_{x}^{t}(z^{t+1}-x^{t})-(\\alpha^{*1+}-z^{t})+p(w^{*}-z^{t})\\right\\Vert^{2}\\right]}\\\\ &{\\quad+(1-p)\\mathbb{E}\\left[\\frac{1}{\\alpha}\\sum_{x}C_{x}^{t}(z^{t+1}-x^{t})-(\\alpha^{*1+}-z^{t})+p(w^{*}-z^{t})\\right]}\\\\ {\\Leftrightarrow}&{\\left\\Vert\\alpha^{*1}+\\mathbf{z}^{t}-p\\cdot p(w^{*}-z^{t})+(w^{*}-z^{t})\\\n$$De(a+-a)-pp(ut-a)+(vt-)\u00b2+pPlt-a\u00b2+0l+1-x2. ", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Lemma E.6.Let Assumption 1.5 hold. Furthermore, suppose that the compressors $\\mathcal{Q}_{i}^{t}\\in\\mathbb{U}(\\omega_{D})$ satisfyAssumption1.6andthat $\\mathcal{C}_{i}^{t}\\in\\mathbb{U}(\\omega_{P})$ for $i\\in[n]$ .Then ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\left[\\left\\Vert g^{t+1}-\\frac{1}{n}\\sum_{i=1}^{n}\\nabla f_{i}(z_{i}^{t+1})\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\frac{\\omega_{D}L_{\\operatorname*{max}}^{2}}{n}\\left(4p_{P}\\beta^{2}\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\left\\Vert w_{i}^{t}-x^{t}\\right\\Vert^{2}\\right]+3\\beta^{2}\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\left\\Vert z_{i}^{t}-w_{i}^{t}\\right\\Vert^{2}\\right]+3(\\omega_{P}+1)\\beta^{2}\\mathbb{E}\\left[\\left\\Vert x^{t+1}-x^{t}\\right\\Vert^{2}\\right]\\right.}\\\\ &{\\quad+\\left.(1-p_{D})\\mathbb{E}\\left[\\left\\Vert g^{t}-\\frac{1}{n}\\sum_{i=1}^{n}\\nabla f_{i}(z_{i}^{t})\\right\\Vert^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. First, from the defnition of $g_{i}^{t+1}$ , we get ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|g^{t+1}-\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\nabla f_{i}(z_{i}^{t+1})\\right\\|^{2}\\right]}\\\\ &{\\quad=(1-p_{D})\\mathbb{E}\\left[\\left\\|\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\left(g_{i}^{t}+\\mathcal{Q}_{i}^{t}(\\nabla f_{i}(z_{i}^{t+1})-\\nabla f_{i}(z_{i}^{t})\\right)-\\nabla f_{i}(z_{i}^{t+1})\\right)\\right\\|^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and hence ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\left\\|g^{t+1}-\\frac{1}{n}\\sum_{i=1}^{n}\\nabla f_{i}(z_{i}^{t+1})\\right\\|^{2}\\right]}\\\\ {\\overset{(\\mathrm{sta})}{=}}&{(1-p_{D})\\mathbb{E}\\left[\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}Q_{i}^{t}\\big(\\nabla f_{i}(z_{i}^{t+1})-\\nabla f_{i}(z_{i}^{t})\\big)-\\frac{1}{n}\\sum_{i=1}^{n}\\big(\\nabla f_{i}(z_{i}^{t+1})-\\nabla f_{i}(z_{i}^{t})\\big)\\right\\|^{2}\\right]}\\\\ &{\\qquad+(1-p_{D})\\mathbb{E}\\left[\\left\\|g^{t}-\\frac{1}{n}\\sum_{i=1}^{n}\\nabla f_{i}(z_{i}^{t})\\right\\|^{2}\\right]}\\\\ {\\overset{(d+1,d)}{\\leq}}&{\\frac{\\omega_{D}}{n}\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\|\\nabla f_{i}(z_{i}^{t+1})-\\nabla f_{i}(z_{i}^{t})\\|^{2}\\right]+(1-p_{D})\\mathbb{E}\\left[\\left\\|g^{t}-\\frac{1}{n}\\sum_{i=1}^{n}\\nabla f_{i}(z_{i}^{t})\\right\\|^{2}\\right]}\\\\ {\\overset{(\\mathrm{subs})\\leq}}&{\\frac{\\omega_{D}L_{\\operatorname*{max}}^{2}}{\\leq}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\big\\|z_{i}^{t+1}-z_{i}^{t}\\big\\|^{2}\\right]+(1-p_{D})\\mathbb{E}\\left[\\bigg\\|g^{t}-\\frac{1}{n}\\sum_{i=1}^{n}\\nabla f_{i}(z_{i}^{t})\\bigg\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Let us consider the first term separately: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|z_{i}^{t+1}-z_{i}^{t}\\right\\|^{2}\\right]=\\mathbb{E}\\left[\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|\\beta w_{i}^{t+1}+(1-\\beta)z_{i}^{t}-z_{i}^{t}\\right\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\beta^{2}\\mathbb{E}\\left[\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|w_{i}^{t+1}-z_{i}^{t}\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Using the result from Lemma E.5, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\|z_{i}^{t+1}-z_{i}^{t}\\|^{2}\\right]}\\\\ &{\\quad\\le\\beta^{2}\\mathbb{E}\\left[\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\|(x^{t+1}-x^{t})-p_{P}(w_{i}^{t}-x^{t})+(w_{i}^{t}-z_{i}^{t})\\|^{2}\\right]}\\\\ &{\\quad\\quad+\\beta^{2}\\mathbb{E}\\left[p_{P}\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\|w_{i}^{t}-x^{t}\\|^{2}+\\omega_{P}\\left\\|x^{t+1}-x^{t}\\right\\|^{2}\\right]}\\\\ &{\\quad\\stackrel{(i\\leq)}{\\le}\\beta^{2}\\mathbb{E}\\left[\\displaystyle\\frac{3}{n}\\displaystyle\\sum_{i=1}^{n}\\|w_{i}^{t}-z_{i}^{t}\\|^{2}+4p_{P}\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\|w_{i}^{t}-x^{t}\\|^{2}+(\\omega_{P}+3)\\left\\|x^{t+1}-x^{t}\\right\\|^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where in the last line we use the fact that $p_{P}\\,\\leq\\,1$ . It remains to substitute the above inequality in (30). \u53e3 ", "page_idx": 32}, {"type": "text", "text": "Lemma E.7. Let $\\mathcal{C}_{i}^{t}\\in\\mathbb{U}(\\omega_{P})$ for all $i\\in[n]$ and $\\left\\{{\\mathcal{C}}_{i}^{t}\\right\\}_{i=1}^{n}\\in\\mathbb{P}(\\theta)$ .Then ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\left\\Vert z_{i}^{t+1}-w_{i}^{t+1}\\right\\Vert^{2}\\right]\\leq\\left(1-\\frac{\\beta}{2}\\right)\\mathbb{E}\\left[\\left\\Vert z_{i}^{t}-w_{i}^{t}\\right\\Vert^{2}\\right]+4\\left(\\frac{1}{\\beta}+\\omega_{P}\\right)\\mathbb{E}\\left[\\left\\Vert x^{t+1}-x^{t}\\right\\Vert^{2}\\right]}\\\\ {+\\,4p_{P}\\left(1+\\frac{p_{P}}{\\beta}\\right)\\mathbb{E}\\left[\\left\\Vert w_{i}^{t}-x^{t}\\right\\Vert^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for all $i\\in[n]$ , and ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert z^{t+1}-w^{t+1}\\right\\Vert^{2}\\right]\\leq\\left(1-\\displaystyle\\frac{\\beta}{2}\\right)\\mathbb{E}\\left[\\left\\Vert z^{t}-w^{t}\\right\\Vert^{2}\\right]+4\\left(\\displaystyle\\frac{1}{\\beta}+\\theta\\right)\\mathbb{E}\\left[\\left\\Vert x^{t+1}-x^{t}\\right\\Vert^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,4p_{P}\\left(1+\\displaystyle\\frac{p_{P}}{\\beta}\\right)\\mathbb{E}\\left[\\left\\Vert w^{t}-x^{t}\\right\\Vert^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. From the definition of $z_{i}^{t+1}$ , we get ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|z_{i}^{t+1}-w_{i}^{t+1}\\right\\|^{2}\\right]=\\mathbb{E}\\left[\\left\\|\\beta w_{i}^{t+1}+(1-\\beta)z_{i}^{t}-w_{i}^{t+1}\\right\\|^{2}\\right]=(1-\\beta)^{2}\\mathbb{E}\\left[\\left\\|w_{i}^{t+1}-z_{i}^{t}\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then, Lemma E.5 gives ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\left\\|\\frac{\\hat{\\eta}^{k+1}}{\\hat{\\eta}}(\\mathbf{r}^{k}-\\mathbf{r}^{k+1})\\right\\|^{2}\\right]}&{}\\\\ {\\leq}&{(1-\\beta)^{2}\\mathbb{E}\\left[\\left\\|\\frac{\\hat{\\eta}^{k+1}}{\\hat{\\eta}}\\!-\\hat{\\eta}^{k}\\right\\|^{2}\\right]}\\\\ &{\\qquad+(1-\\beta)^{2}\\mathbb{E}\\left[\\eta^{k}\\left\\|w_{t}^{\\star}-x^{\\star}\\right\\|^{2}+\\omega_{P}\\left\\|x^{\\star}\\right\\|+(w_{t}^{\\star}-z_{t}^{\\star})\\right\\|^{2}\\right]}\\\\ {\\overset{(a)(c)}{\\leq}}&{\\left(1-\\frac{\\beta)}{2}\\mathbb{E}\\left[\\left\\|w_{t}^{\\star}-z_{t}^{\\star}\\right\\|^{2}\\right]+\\frac{2}{\\beta}\\mathbb{E}\\left[\\left\\|x^{\\star}-x^{\\star}\\right\\|-p r(w_{t}^{\\star}-x^{\\star})\\right\\|^{2}\\right]}\\\\ &{\\qquad+\\mathbb{E}\\left[p\\left\\|w_{t}^{\\star}-z_{t}^{\\star}\\right\\|^{2}+\\omega_{P}\\left\\|x^{\\star}\\right\\|^{2}\\right]}\\\\ {\\overset{(b)(c)}{\\leq}}&{\\left(1-\\frac{\\beta}{\\hat{\\eta}}\\right)\\mathbb{E}\\left[\\left\\|w_{t}^{\\star}-z_{t}^{\\star}\\right\\|^{2}+\\omega\\left\\|t\\right\\|+\\frac{\\lambda}{\\beta}\\mathbb{E}\\left[\\left\\|x^{\\star+1}-x^{\\star}\\right\\|^{2}\\right]+\\frac{4\\beta}{\\beta}\\mathbb{E}\\left[\\left\\|w_{t}^{\\star}-x^{\\star}\\right\\|^{2}\\right]\\right]}\\\\ &{\\qquad+\\mathbb{E}\\left[p\\left\\|w_{t}^{\\star}-z_{t}^{\\star}\\right\\|^{2}+\\omega\\left\\|r\\right\\|\\right]+4\\frac{1}{\\beta}\\mathbb{E}\\left[\\left\\|x^{\\star}-x^{\\star}\\right\\|^{2}\\right]}\\\\ {\\leq}&{\\left(1-\\frac{\\beta}{\\hat{\\eta}}\\right)\\mathbb{E}\\left[\\left\\|w_{t}^{\\star}-z_{t}^{\\star}\\right\\|^{2}\\right]+4\\left(\\frac{1}{\\beta}+\\omega_{P}\\right)\\mathbb{E}\\left[\\left\\|x^{\\star+1}-x^{\\star}\\\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The second inequality is proved almost in the same way. First, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|z^{t+1}-w^{t+1}\\right\\|^{2}\\right]=(1-\\beta)^{2}\\mathbb{E}\\left[\\left\\|w^{t+1}-z^{t}\\right\\|^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and using Lemma E.5, we obtain ", "text_level": 1, "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|z^{t+1}-w^{t+1}\\right\\|^{2}\\right]}\\\\ &{\\quad\\leq(1-\\beta)^{2}\\mathbb{E}\\left[\\left\\|(v^{t+1}-x^{t})-p r(w^{t}-x^{t})+(w^{t}-z^{t})\\right\\|^{2}+p p\\left\\|w^{t}-x^{t}\\right\\|^{2}+\\theta\\left\\|z^{t+1}-x^{t}\\right\\|^{2}\\right]}\\\\ &{\\quad\\overset{(a)}{\\leq}\\left(1-\\frac{\\beta}{2}\\right)\\mathbb{E}\\left[\\left\\|w^{t}-z^{t}\\right\\|^{2}\\right]+\\frac{2}{\\beta}\\mathbb{E}\\left[\\left\\|(v^{t+1}-x^{t})-p r(w^{t}-z^{t})\\right\\|^{2}\\right]}\\\\ &{\\quad\\quad+\\mathbb{E}\\left[p p\\left\\|w^{t}-z^{t}\\right\\|^{2}+\\theta\\left\\|x^{t+1}-x^{t}\\right\\|^{2}\\right]}\\\\ &{\\quad\\overset{(b)}{\\leq}\\left(1-\\frac{\\beta}{2}\\right)\\mathbb{E}\\left[\\left\\|w^{t}-z^{t}\\right\\|^{2}\\right]+4\\left(\\frac{1}{\\beta}+\\theta\\right)\\mathbb{E}\\left[\\left\\|x^{t+1}-x^{t}\\right\\|^{2}\\right]}\\\\ &{\\quad\\quad+\\frac{4\\eta^{2}}{\\beta}\\mathbb{E}\\left[\\left\\|w^{t}-x^{t}\\right\\|^{2}\\right]+\\mathbb{E}\\left[p p\\left\\|w^{t}-z^{t}\\right\\|^{2}\\right]}\\\\ &{\\quad\\leq\\left(1-\\frac{\\beta}{2}\\right)\\mathbb{E}\\left[\\left\\|w^{t}-z^{t}\\right\\|^{2}\\right]+4\\left(\\frac{1}{\\beta}+\\theta\\right)\\mathbb{E}\\left[\\left\\|x^{t+1}-x^{t}\\right\\|^{2}\\right]}\\\\ &{\\quad\\quad+4p p\\left(N+\\frac{\\beta}{2}\\right)\\mathbb{E}\\left[\\left\\|w^{t}-z^{t}\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Theorem E.1. Let Assumptions 1.1, 1.2, 1.5 and 4.2 hold and suppose that the compressors $\\mathcal{Q}_{i}^{t}\\in$ $\\mathbb{U}(\\omega_{D})$ satisfy Assumption 1.6, $\\left\\{{\\mathcal{C}}_{i}^{t}\\right\\}_{i=1}^{n}\\in\\mathbb{P}(\\theta)$ and $\\mathcal{C}_{i}^{t}\\in\\mathbb{U}(\\omega_{P})$ for all $i\\in[n]$ Let $\\gamma>0$ be such that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\prime\\leq\\left(L+\\sqrt{288\\left(\\left(\\frac{\\theta}{p_{P}}+\\frac{1+\\theta p_{P}}{\\beta^{2}}\\right)L_{B}^{2}+\\left(\\frac{\\omega p}{p_{P}}+\\frac{1+\\omega_{P}p_{P}}{\\beta^{2}}\\right)L_{A}^{2}+\\left(\\frac{\\omega_{D}\\omega_{P}\\beta}{n p_{D}}+\\frac{\\omega_{D}\\left(1+\\omega_{P}p_{P}\\right)}{n p_{D}}\\right)L_{\\operatorname*{max}}^{2}\\right)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Letting ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Psi^{t}=\\delta^{t}+\\kappa\\left\\|g^{t}-\\frac1n\\sum_{i=1}^{n}\\nabla f_{i}(z_{i}^{t})\\right\\|^{2}+\\eta\\left\\|z^{t}-w^{t}\\right\\|^{2}+\\nu\\frac1n\\sum_{i=1}^{n}\\left\\|z_{i}^{t}-w_{i}^{t}\\right\\|^{2}}\\\\ {\\displaystyle\\quad+\\,\\rho\\left\\|w^{t}-x^{t}\\right\\|^{2}+\\mu\\frac1n\\sum_{i=1}^{n}\\left\\|w_{i}^{t}-x^{t}\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\begin{array}{r l}{\\kappa}&{{}=\\,\\,\\frac{\\gamma}{p_{D}}}\\end{array}$ $\\begin{array}{r c l}{\\eta}&{=}&{\\frac{4\\gamma L_{B}^{2}}{\\beta}}\\end{array}$ $\\begin{array}{r}{\\nu\\ =\\ \\frac{4\\gamma L_{A}^{2}}{\\beta}\\,+\\,\\frac{6\\gamma\\omega_{D}\\beta L_{\\mathrm{max}}^{2}}{n p_{D}}}\\end{array}$ \uff0c $\\begin{array}{r}{\\rho\\ =\\ 32\\gamma L_{B}^{2}\\left(\\frac{1}{p_{P}}+\\frac{p_{P}}{\\beta^{2}}\\right)}\\end{array}$ and $\\mu=$ $\\begin{array}{r l r}&{}&{32\\gamma L_{A}^{2}\\left(\\frac{1}{p_{P}}+\\frac{p_{P}}{\\beta^{2}}\\right)+\\frac{48\\gamma\\omega_{D}L_{\\mathrm{max}}^{2}}{n p_{D}}\\left(\\beta+p_{P}\\right)\\!,}\\end{array}$ 48wDLaux (\u03b2 + Pp), M3 ensures that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{t})\\right\\|^{2}\\right]=\\mathcal{O}\\left(\\frac{\\Psi^{0}}{\\gamma T}\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. Lemma H.1 gives ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\mathbb{E}\\left\\{R^{1}-\\frac{2}{\\delta}\\mathbb{E}\\left[\\left\\|\\nabla f^{\\alpha}\\right\\|^{2}\\right]-\\left(\\frac{1}{2\\gamma}-\\frac{L}{2}\\right)\\mathbb{E}\\left[\\left\\|x^{\\alpha}-x^{\\beta}\\right\\|^{2}\\right]+\\frac{2}{2}\\mathbb{E}\\left[\\left\\|y^{\\alpha}-\\nabla f^{\\alpha}(x^{\\beta})\\right\\|^{2}\\right]\\right.}\\\\ &{\\overset{(a)}{\\leq}\\mathbb{E}\\left\\{R^{1}-\\frac{2}{\\gamma}\\mathbb{E}\\left[\\nabla f^{\\alpha}(x^{\\beta})\\right]^{2}-\\left(\\frac{1}{2\\gamma}-\\frac{L}{2}\\right)\\mathbb{E}\\left[\\left\\|x^{\\alpha+1}-x^{\\beta}\\right\\|^{2}\\right]}\\\\ &{\\qquad+\\mathbb{E}\\left[\\Bigg\\|R^{2}-\\frac{1}{\\gamma}\\frac{\\nabla}{\\mu_{0}\\gamma}\\nabla f_{\\alpha}(x^{\\beta})\\Bigg\\|^{2}\\right]+\\mathbb{E}\\left[\\left\\|\\frac{1}{\\mu_{0}\\gamma}\\frac{\\nabla}{\\mu_{0}\\gamma}(\\nabla f_{\\alpha}(x^{\\beta})-\\nabla f_{\\alpha}(x^{\\beta}))\\right\\|^{2}\\right]}\\\\ &{\\overset{(b)}{\\leq}\\mathbb{E}\\left[\\sigma\\right]-\\frac{2}{\\delta}\\mathbb{E}\\left[\\nabla f^{\\alpha}(x^{\\beta})\\right]^{2}-\\left(\\frac{1}{2\\gamma}-\\frac{L}{2}\\right)\\mathbb{E}\\left[\\left\\|x^{\\alpha+1}-x^{\\beta}\\right\\|^{2}\\right]}\\\\ &{\\quad+\\mathbb{E}\\left[\\left\\|\\phi-\\frac{1}{n}\\frac{\\nabla}{\\mu_{0}\\gamma}\\nabla f_{\\alpha}(x^{\\beta})\\right\\|^{2}\\right]+\\gamma\\lambda\\frac{2}{\\lambda}\\frac{1}{n}\\frac{\\nabla}{\\mu_{0}\\gamma}\\mathbb{E}\\left[\\left\\|x^{\\alpha}-x^{\\beta}\\right\\|^{2}\\right]+\\gamma L_{\\frac{2}{\\beta}}^{2}\\mathbb{E}\\left[\\left\\|x^{\\alpha}-x^{\\beta}\\right\\|^{2}\\right]}\\\\ &{\\overset{(c)}{\\leq}\\mathbb{E}\\left[\\sigma\\right]-\\frac{2}{\\delta}\\mathbb{E}\\left[\\nabla f^{\\alpha}(x^{\\beta})\\right]^{2}-\\left(\\frac{1}{2\\gamma}-\\frac{L}{2}\\right)\\mathbb{E}\\left[\\left\\|x\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Let $\\kappa,\\eta,\\nu,\\rho,\\mu\\geq0$ be some non-negative numbers that we define later. Using Lemmas D.6, D.7, E.6 and E.7, we get ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\delta^{t+1}\\right]+\\kappa\\mathbb{E}\\left[\\left\\|g^{t+1}-\\frac{1}{n}\\sum_{i=1}^{n}\\nabla f_{i}(z_{i}^{t+1})\\right\\|^{2}\\right]+\\eta\\mathbb{E}\\left[\\left\\|z^{t+1}-w^{t+1}\\right\\|^{2}\\right]}\\\\ {\\quad+\\,\\nu\\mathbb{E}\\left[\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|z_{i}^{t+1}-w_{i}^{t+1}\\right\\|^{2}\\right]+\\rho\\mathbb{E}\\left[\\left\\|w^{t+1}-x^{t+1}\\right\\|^{2}\\right]+\\mu\\mathbb{E}\\left[\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|w_{i}^{t+1}-x^{t+1}\\right\\|^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "\u2264B[]-[f(x)]-(-)E[+-]+ ()   \n$\\begin{array}{r l}&{\\quad-\\nu-\\frac{2}{r}\\nu}\\\\ &{\\qquad=\\nu-\\frac{1}{r}\\sum_{k=1}^{r}\\left(g_{r,k}^{r}-u_{j}^{k}\\right)^{-1}\\mathcal{F}(u,v^{k})\\quad\\nu-\\nu\\quad-\\frac{\\nu-1}{r}\\quad[1]\\quad\\textrm{,}}\\\\ &{\\qquad=\\nu\\int(\\nu-\\frac{\\nu-1}{r})^{\\frac{1}{r}}\\left(g_{r,k}^{r}-u_{j}^{k}\\right)^{-1}\\mathcal{F}(u,v^{k})\\quad\\nu-\\nu\\quad[1]\\quad\\textrm{,}}\\\\ &{\\qquad=\\nu\\int\\nu\\left(\\frac{1}{r}\\left[g_{r,k}^{r}-u_{j}^{k}\\right]^{\\frac{1}{r}}+\\frac{1}{r}\\left[u^{\\nu-\\frac{\\nu-1}{r}}\\right]^{\\frac{1}{r}}\\right)\\quad\\textrm{d}\\nu\\quad[\\nu-\\nu\\quad t_{r}^{\\frac{1}{r}}]\\quad\\textrm{d}\\nu}\\\\ &{\\qquad=\\nu\\left(\\nu\\frac{\\nu-1}{r}\\left(g_{r,k}^{r}-u_{j}^{k}\\right)^{\\frac{1}{r}}\\left[u^{\\nu-1}-v_{j}^{k}\\right]^{\\frac{1}{r}}\\right)\\quad\\textrm{d}\\nu\\quad[\\nu\\quad\\textrm{d}\\nu]\\quad\\textrm{d}\\nu}\\\\ &{\\qquad\\qquad\\qquad\\quad+\\ N\\alpha+1\\nu\\Re\\left[1-u^{\\nu-1}v_{j}^{k}\\left[u^{\\nu-1}-v_{j}^{k}\\right]^{\\frac{1}{r}}\\right]\\quad\\right)}\\\\ &{\\qquad=\\nu\\int\\nu\\quad[-\\frac{1}{r}\\sum_{k=1}^{r}\\nu\\nabla_{k}u(v)\\left[\\nu\\quad\\textrm{d}\\nu\\quad(1-u^{\\nu-1}v_{j}^{k}\\right)^{\\frac{1}{r}}\\right.}\\\\ &{\\qquad\\qquad\\quad\\left.+\\nu\\int_{r}\\nu\\nabla_{k}u^{k}\\left[u^{\\nu-1}-v_{j}^{k}\\right]^{\\frac{1}{r}}\\right)\\quad\\textrm{d}\\nu\\quad[\\nu\\quad\\textrm{d}\\nu\\quad-\\nu\\quad t_{r}^{\\frac{1}{r}}]}\\\\ &{\\qquad\\quad+\\nu\\mathcal{F}_{r}\\quad\\left(1+\\frac{1}{r}\\partial_{r}\\nu^{\\frac{1}{r}}\\right)^{\\frac{1}{r}}\\left[u^{\\nu-1}-v_{j}$ =1   \n+\u03bc(1-pp) IE n ", "page_idx": 35}, {"type": "text", "text": "Taking $\\begin{array}{r}{\\kappa=\\frac{\\gamma}{p_{D}}}\\end{array}$ and  = $\\begin{array}{r}{\\eta=\\frac{4\\gamma L_{B}^{2}}{\\beta}}\\end{array}$ , we get+k(1-pD) =kand 2L2+n(1-\u03b2B/2) =n, which gives ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mu^{k+1}+k x^{k}\\left\\lVert\\left\\lVert\\tilde{\\mathbf{g}}^{t+1}-\\frac{1}{n}\\frac{\\hat{\\mathbf{b}}}{\\hat{\\mathbf{b}}_{n}^{t}}\\nabla f_{i}(\\varepsilon^{t+1})\\right\\rVert_{s}^{2}\\right]+\\eta\\mathbb{E}\\left[\\left\\lVert\\tilde{\\mathbf{g}}^{t+1}-w^{t+1}\\right\\rVert_{s}^{2}\\right]}\\\\ &{\\quad+\\nu\\mathbb{E}\\left[\\frac{1}{n}\\frac{\\hat{\\mathbf{b}}}{f_{i}}\\right\\rVert_{s}^{2}\\left\\lVert\\tilde{\\mathbf{g}}_{i}^{t+1}-w_{i}^{t+1}\\right\\rVert_{s}^{2}\\right]+\\beta\\mathbb{E}\\left[\\left\\lVert w^{t+1}-x^{t+1}\\right\\rVert_{s}^{2}\\right]+\\mu\\mathbb{E}\\left[\\frac{1}{n}\\frac{\\hat{\\mathbf{b}}}{f_{i}}\\right\\rVert_{s}^{4}\\left\\lVert u_{i}^{t+1}-x^{t+1}\\right\\rVert_{s}^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\nu^{t}\\right]-\\frac{\\gamma}{n}\\mathbb{E}\\left[\\left\\lVert\\nabla f(\\varepsilon^{t})\\right\\rVert_{s}^{2}\\right]-\\left(\\frac{1}{2\\gamma}-\\frac{L}{2}\\right)\\mathbb{E}\\left[\\left\\lVert u^{t+1}-x^{t}\\right\\rVert_{s}^{2}\\right]+\\kappa\\mathbb{E}\\left[\\left\\lVert\\rho^{t}-\\frac{1}{n}\\frac{\\hat{\\mathbf{b}}}{c_{n}^{t}}\\nabla f_{i}(\\varepsilon^{t})\\right\\rVert_{s}^{2}\\right]}\\\\ &{\\quad+\\eta\\mathbb{E}\\left[\\left\\lVert\\tilde{\\mathbf{g}}^{t}-w^{t}\\right\\rVert_{s}^{2}\\right]+2\\gamma L\\frac{\\gamma}{\\lambda}\\frac{\\hat{\\mathbf{b}}}{\\prod_{s}}\\frac{\\hat{\\mathbf{b}}}{\\gamma}\\left(\\mathbb{E}\\left[\\left\\lVert\\tilde{\\mathbf{g}}_{i}^{t}-w_{i}^{t}\\right\\rVert_{s}^{2}\\right]+\\mathbb{E}\\left[\\left\\lVert u_{i}^{t}-x^{t}\\right\\rVert_{s}^{2}\\right]+2\\gamma L\\frac{\\gamma}{\\lambda}\\mathbb{E}\\left[\\left\\lVert u^{t}-x^{t}\\right\\rVert_{s}^{2}\\right]\\right)}\\\\ &{\\quad+\\frac{\\gamma}{n{\\overline{{\\eta}}}\\rho}\\left( \n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle+\\left\\nu\\left(\\left(1-\\frac{\\beta}{2}\\right)\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|z_{i}^{t}-w_{i}^{t}\\right\\|^{2}\\right]+4\\left(\\frac{1}{\\beta}+\\omega_{P}\\right)\\mathbb{E}\\left[\\left\\|x^{t+1}-x^{t}\\right\\|^{2}\\right]\\right.}\\\\ &{\\displaystyle\\qquad\\left.+\\,4p_{P}\\left(1+\\frac{p_{P}}{\\beta}\\right)\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|w_{i}^{t}-x^{t}\\right\\|^{2}\\right]\\right)}\\\\ &{\\displaystyle+\\left.\\rho\\left(\\left(1-p_{P}\\right)\\mathbb{E}\\left[\\left\\|w^{t}-x^{t}\\right\\|^{2}\\right]+\\theta\\mathbb{E}\\left[\\left\\|x^{t+1}-x^{t}\\right\\|^{2}\\right]\\right)}\\\\ &{\\displaystyle+\\left.\\mu\\left((1-p_{P})\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|w_{i}^{t}-x^{t}\\right\\|^{2}\\right]+\\omega_{P}\\mathbb{E}\\left[\\left\\|x^{t+1}-x^{t}\\right\\|^{2}\\right]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We rearrange the terms to obtain ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mu^{k+1}\\right]+\\kappa^{2}\\left[\\Bigg\\|\\mathcal{H}^{+1}-\\frac{1}{n}\\frac{\\nabla}{\\nabla}\\nabla f_{i}(\\varepsilon^{k+1})\\Bigg\\|^{2}\\Bigg]+\\eta\\Bigg[\\left\\{\\frac{1}{n}\\nabla\\bigg[\\mu^{k+1}-\\nu^{\\varepsilon}\\bigg]\\right\\}^{2}\\Bigg]}\\\\ &{\\quad+\\eta\\Bigg[\\frac{1}{n}\\frac{\\nabla}{\\sqrt{n}}\\bigg[\\mu^{k+1}-\\eta^{\\varepsilon+1}\\nabla^{2}\\bigg]+\\beta\\Bigg[\\left\\{\\mu^{k+1}-\\frac{\\eta^{\\varepsilon+1}}{n}\\right\\}^{2}\\bigg]+\\eta\\Bigg[\\bigg\\|\\frac{1}{n}\\frac{\\nabla}{\\sqrt{n}}\\bigg[\\mu^{k+1}-\\eta^{\\varepsilon+1}\\bigg]\\Bigg]}\\\\ &{\\leq\\mathbb{E}\\left[\\nu\\bigg]-\\frac{2\\eta}{n}\\mathbb{E}\\left[\\nabla f_{i}(\\varepsilon^{k})\\right]^{2}-\\left(\\frac{1}{2\\eta}-\\frac{1}{n}\\frac{\\nabla}{\\sqrt{n}}\\right)\\mathbb{E}\\left[\\mu^{k+1}-\\eta^{\\varepsilon}\\right]+\\kappa^{2}\\mathbb{E}\\Bigg[\\Bigg\\|\\nabla-\\frac{1}{n}\\frac{\\nabla}{\\sqrt{n}}\\nabla f_{i}(\\varepsilon^{k})\\Bigg\\|^{2}\\right]}\\\\ &{\\quad+\\eta\\Bigg[\\bigg\\|\\mu^{k}-\\eta^{\\varepsilon}\\bigg]\\Bigg[+\\bigg(\\nu^{\\varepsilon}-\\frac{3}{2}\\bigg)+2\\eta\\lambda_{n}^{2}+3\\frac{2\\eta\\rho\\nabla^{2}\\lambda_{n}^{2}}{n\\rho\\rho\\rho}\\bigg]\\mathbb{E}\\left[\\frac{1}{n}\\frac{\\nabla}{\\sqrt{n}}\\bigg[\\mu_{n}^{k}-\\eta^{\\varepsilon}\\bigg]\\right]}\\\\ &{\\quad+\\left(\\frac{3\\eta\\rho\\nabla\\bigg(\\mu^{k}-\\frac{1}{n}\\bigg)\\eta^{2}\\lambda_{n}^{2}}{n\\rho\\rho}+\\eta+\\frac{16\\eta\\rho\\nabla^{2}\\lambda_{n}^{2}}{n\\rho}\\left(\\frac{3}{n}+\\eta\\right)\\right)}\\\\ &{\\quad\\quad+\\nu\\bigg(\\frac{3}{n}+\\nu^{\\varepsilon}\\bigg)+\\mu\\nu\\bigg[\\beta\\bigg]\\Bigg[\\mu^{k+1}-\\eta^{\\varepsilon}\\bigg]}\\\\ &{\\quad+\\left(\\nu^{\\varepsilon}(1-\\rho)+2\\eta\\lambda_{n}^{ \n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We now consider the coeffcient of the term $\\mathbb{E}\\left[\\left\\|w^{t}-x^{t}\\right\\|^{2}\\right]$ Using the inequality $\\begin{array}{r}{x y\\le\\frac{x^{2}+y^{2}}{2}}\\end{array}$ for all $x,y\\geq0$ we get ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\rho(1-p_{P})+2\\gamma L_{B}^{2}+\\frac{16\\gamma L_{B}^{2}p_{P}}{\\beta}\\left(1+\\frac{p_{P}}{\\beta}\\right)\\leq\\rho(1-p_{P})+16\\gamma L_{B}^{2}\\left(1+\\frac{p_{P}}{\\beta}+\\frac{p_{P}^{2}}{\\beta^{2}}\\right)}&{}\\\\ {\\leq\\rho(1-p_{P})+32\\gamma L_{B}^{2}\\left(1+\\frac{p_{P}^{2}}{\\beta^{2}}\\right)}&{}\\\\ {=\\rho}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "for $\\begin{array}{r}{\\rho=32\\gamma L_{B}^{2}\\left(\\frac{1}{p_{P}}+\\frac{p_{P}}{\\beta^{2}}\\right)}\\end{array}$ With this choice of $\\rho$ , we obtain ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Xi\\left[\\delta^{t+1}\\right]+\\kappa\\mathbb{E}\\left[\\left\\|g^{t+1}-\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\nabla f_{i}(z_{i}^{t+1})\\right\\|^{2}\\right]+\\eta\\mathbb{E}\\left[\\left\\|z^{t+1}-w^{t+1}\\right\\|^{2}\\right]}\\\\ &{\\quad+\\nu\\mathbb{E}\\left[\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|z_{i}^{t+1}-w_{i}^{t+1}\\right\\|^{2}\\right]+\\rho\\mathbb{E}\\left[\\left\\|w^{t+1}-x^{t+1}\\right\\|^{2}\\right]+\\mu\\mathbb{E}\\left[\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|w_{i}^{t+1}-x^{t+1}\\right\\|^{2}\\right]}\\\\ &{\\le\\mathbb{E}\\left[\\delta^{t}\\right]-\\displaystyle\\frac{\\gamma}{2}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{t})\\right\\|^{2}\\right]-\\left(\\displaystyle\\frac{1}{2\\gamma}-\\displaystyle\\frac{L}{2}\\right)\\mathbb{E}\\left[\\left\\|x^{t+1}-x^{t}\\right\\|^{2}\\right]+\\kappa\\mathbb{E}\\left[\\left\\|g^{t}-\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\nabla f_{i}(z_{i}^{t})\\right\\|^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle+\\left.\\eta\\mathbb{E}\\left[\\left\\|z^{t}-w^{t}\\right\\|^{2}\\right]+\\rho\\mathbb{E}\\left[\\left\\|w^{t}-x^{t}\\right\\|^{2}\\right]\\right.}\\\\ &{\\displaystyle+\\left.\\left(\\nu\\left(1-\\frac{\\beta}{2}\\right)+2\\gamma L_{A}^{2}+\\frac{3\\gamma\\omega_{D}\\beta^{2}L_{\\operatorname*{max}}^{2}}{n p D}\\right)\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|z_{i}^{t}-w_{i}^{t}\\right\\|^{2}\\right]\\right.}\\\\ &{\\displaystyle+\\left.\\left(\\frac{3\\gamma\\omega_{D}(\\omega_{P}+1)\\beta^{2}L_{\\operatorname*{max}}^{2}}{n p D}+32\\gamma L_{B}^{2}\\left(\\frac{1}{p_{P}}+\\frac{p_{P}}{\\beta^{2}}\\right)\\theta+\\frac{16\\gamma L_{B}^{2}}{\\beta}\\left(\\frac{1}{\\beta}+\\theta\\right)\\right.\\right.}\\\\ &{\\displaystyle\\left.\\quad\\quad+4\\nu\\left(\\frac{1}{\\beta}+\\omega_{P}\\right)+\\mu\\omega_{P}\\right)\\mathbb{E}\\left[\\left\\|x^{t+1}-x^{t}\\right\\|^{2}\\right]}\\\\ &{\\displaystyle\\left.\\quad\\quad+\\left(\\mu(1-p_{P})+2\\gamma L_{A}^{2}+4\\nu p_{P}\\left(1+\\frac{p_{P}}{\\beta}\\right)+\\frac{4\\gamma\\omega_{D}p\\beta^{2}L_{\\operatorname*{max}}^{2}}{n p_{D}}\\right)\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|w_{i}^{t}-x^{t}\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Next, taking $\\begin{array}{r}{\\nu=\\frac{4\\gamma L_{A}^{2}}{\\beta}+\\frac{6\\gamma\\omega_{D}\\beta L_{\\mathrm{max}}^{2}}{n p_{D}}}\\end{array}$ gives ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mu^{k+1}\\right]+\\kappa\\mathbb{E}\\left[\\left\\|\\mathcal{F}^{k+1}-\\frac{1}{n}\\frac{\\sqrt{\\alpha}}{n}\\nabla f_{k}(\\varepsilon^{k+1})\\right\\|^{2}\\right]+\\mathbb{E}\\left[\\left\\|\\mathcal{F}^{k+1}-u^{k+1}-u^{k+1}\\right\\|^{2}\\right]}\\\\ &{\\quad+\\varepsilon\\mathbb{E}\\left[\\frac{1}{n}\\frac{\\sqrt{\\alpha}}{n}\\|\\mathcal{F}^{k+1}-u^{k+1}\\|^{2}\\right]+\\beta\\mathbb{E}\\left[\\left\\|u^{k+1}-u^{k-1}\\right\\|^{2}\\right]+n\\mathbb{E}\\left[\\frac{1}{n}\\frac{\\sqrt{\\alpha}}{n}\\|u^{k+1}-x^{k+1}\\|^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\|\\mathcal{F}^{k}-\\frac{1}{n}\\mathcal{E}\\left[\\nabla f_{k}(\\varepsilon^{k})\\right]^{2}-\\left(\\frac{1}{n},\\frac{\\sqrt{\\alpha}}{n}\\right)\\mathbb{E}\\left[\\big\\|u^{k+1}-x^{k}\\big\\|^{2}\\right]+\\beta\\mathbb{E}\\left[\\bigg\\|u^{k}-\\frac{1}{n}\\frac{\\sqrt{\\alpha}}{n}\\nabla f_{k}(\\varepsilon^{k})\\right]^{2}}\\\\ &{\\quad+\\beta\\mathbb{E}\\left[\\big\\|\\mathcal{F}^{k}-u^{k}\\big\\|^{2}+\\beta\\mathbb{E}\\left[\\big\\|u^{k}-x^{k}\\big\\|^{2}\\right]+n\\mathbb{E}\\left[\\frac{1}{n}\\frac{\\sqrt{\\alpha}}{n}\\|u^{k}-\\varepsilon^{k}\\|^{2}\\right]}\\\\ &{\\quad+\\left(\\frac{3\\log(n\\cdot1)+19\\beta^{2}L_{\\operatorname*{max}}^{2}}{n}+\\frac{3\\varepsilon^{2}L_{\\operatorname*{max}}^{2}}{32}\\left(\\frac{1}{n},\\frac{\\sqrt{\\alpha}}{n},\\frac{\\sqrt{\\alpha}}{n}\\right)\\right)\\theta+\\frac{16\\sqrt{L_{\\operatorname*{max}}^{2}}}{\\beta}\\left(\\frac{1}{n}+\\theta\\right)}\\\\ &{\\qquad+\\left(\\frac{4\\sqrt{L_{\\operatorname*{max}}^{2}}}{\\beta}+\\frac{6\\log(n\\cdot1)^{2}\\alpha}{n}\\right)\\left(\\frac{1}{n}+\\varepsilon\\right)+m\\varepsilon\\right)\\bigg)\\mathbb{E}\\left[\\left\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Let us consider the last bracket: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu(1-p_{P})+2\\gamma L_{A}^{2}+4\\left(\\frac{4\\gamma L_{A}^{2}}{\\beta}+\\frac{6\\gamma\\omega\\beta L_{m a x}^{2}}{n p_{D}}\\right)p_{P}\\left(1+\\frac{p_{P}}{\\beta}\\right)+\\frac{4\\gamma\\omega_{D P}p\\beta^{2}L_{m a x}^{2}}{n p_{D}}}\\\\ &{=\\mu(1-p_{P})+2\\gamma L_{A}^{2}+\\frac{16\\gamma p_{P}L_{A}^{2}}{\\beta}+\\frac{24\\gamma\\omega_{D P}p\\beta L_{m a x}^{2}}{n p_{D}}+\\frac{16\\gamma p_{P}^{2}L_{A}^{2}}{\\beta^{2}}}\\\\ &{\\quad+\\frac{24\\gamma\\omega_{D P}p_{P}^{2}L_{m a x}^{2}}{n p_{D}}+\\frac{4\\gamma\\omega_{D P}p\\beta^{2}L_{m a x}^{2}}{n p_{D}}}\\\\ &{\\leq\\mu(1-p_{P})+16\\gamma L_{A}^{2}\\left(1+\\frac{p_{P}}{\\beta}+\\frac{p_{P}^{2}}{\\beta^{2}}\\right)+\\frac{24\\gamma\\omega_{D P}p L_{m a x}^{2}}{n p_{D}}\\left(\\beta+p_{P}+\\beta^{2}\\right)}\\\\ &{\\leq\\mu(1-p_{P})+32\\gamma L_{A}^{2}\\left(1+\\frac{p_{P}^{2}}{\\beta^{2}}\\right)+\\frac{48\\gamma\\omega_{D P}p L_{m a x}^{2}}{n p_{D}}\\left(\\beta+p_{P}\\right)}\\\\ &{=\\mu}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "for $\\begin{array}{r}{\\mu=32\\gamma L_{A}^{2}\\left(\\frac{1}{p_{P}}+\\frac{p_{P}}{\\beta^{2}}\\right)+\\frac{48\\gamma\\omega_{D}L_{\\mathrm{max}}^{2}}{n p_{D}}\\left(\\beta+p_{P}\\right).}\\end{array}$ For this choice, we get ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\beta^{k+1}\\right]+x\\mathbb{E}\\left[\\Bigg\\|\\int_{0}^{t+1}-\\frac{1}{n}\\frac{\\Delta}{\\Delta x_{1}}\\nabla f_{i}(z_{t}^{k+1})\\Bigg\\|_{t}^{2}\\Bigg]+\\eta\\mathbb{E}\\Bigg[\\bigg\\|z^{k+1}-w^{k+1}\\bigg\\|_{t}^{2}\\Bigg]}\\\\ &{\\quad+x\\mathbb{E}\\Bigg[\\frac{1}{n}\\sum_{i=1}^{n}[f_{i}^{k+1}-w_{i+1}^{k+1}]^{2}\\Bigg]+\\beta z\\Bigg[\\ln\\sigma^{4}-x^{k+1}\\bigg\\|_{t}^{2}\\Bigg]+i\\beta\\mathbb{E}\\Bigg[\\frac{1}{n}\\sum_{i=1}^{n}[w_{i}^{k+1}-x^{k+1}]^{2}\\Bigg]}\\\\ &{\\leq\\mathbb{E}\\left[\\sigma^{2}-\\frac{1}{n}\\mathbb{E}\\left[\\nabla f(x^{k})\\right]^{2}\\Bigg]-\\Bigg(\\frac{1}{2}\\gamma-\\frac{L}{2}\\Bigg)\\mathbb{E}\\left[\\Vert z^{k+1}-x^{k}\\Vert^{2}\\right]+i\\beta z\\Bigg[\\bigg\\|\\sigma^{2}-\\frac{1}{n}\\frac{\\Delta}{\\Delta x_{1}}\\nabla f_{i}(z_{t}^{k})\\bigg\\|_{t}^{2}\\Bigg]}\\\\ &{\\quad+\\eta\\mathbb{E}\\Bigg[\\bigg\\|\\sigma^{2}-w^{k}\\bigg\\|_{t}^{2}+\\eta\\mathbb{E}\\Bigg[\\frac{1}{n}\\sum_{i=1}^{n}[i\\zeta_{i}^{k}-w_{i}^{k}]^{2}\\Bigg]+i\\beta z\\mathbb{E}\\Bigg[\\bigg\\|\\sigma^{4}-x^{k}\\bigg\\|_{t}^{2}+i\\beta z\\bigg\\|_{t}^{2}}\\\\ &{\\quad+\\left(\\frac{3\\eta\\rho(x/n+1)}{2}\\eta\\frac{\\partial(x_{1}/n+1)\\beta^{2}}{\\partial(y_{1})}-\\frac{1}{3}\\gamma\\frac{L}{\\rho}\\right)\\left(\\frac{1}{\\rho}w_{1}^{k}+\\frac{p_{1}/n}{\\beta^{2}}\\right)\\theta+\\frac{16\\eta\\hat{L}_{2}^{2}\\Delta}{\\beta}\\left(\\frac{1}{\\beta}+\\theta\\right)}\\\\ &{\\qquad+\\left(\\frac{4\\eta\\hat{L}_{2}^{2}\\Delta}{\\beta}+\\\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Let us simplify the last bracket. ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{:=\\frac{3\\gamma\\omega_{D}(\\omega_{P}+1)\\beta^{2}L_{\\operatorname*{max}}^{2}}{n p_{D}}+32\\gamma L_{B}^{2}\\left(\\frac{1}{p_{P}}+\\frac{p_{P}}{\\beta^{2}}\\right)\\theta+\\frac{16\\gamma L_{B}^{2}}{\\beta}\\left(\\frac{1}{\\beta}+\\theta\\right)}}\\\\ &{+\\,4\\left(\\frac{4\\gamma L_{A}^{2}}{\\beta}+\\frac{6\\gamma\\omega_{D}\\beta L_{\\operatorname*{max}}^{2}}{n p_{D}}\\right)\\left(\\frac{1}{\\beta}+\\omega_{P}\\right)}\\\\ &{+\\,32\\gamma\\omega_{P}L_{A}^{2}\\left(\\frac{1}{p_{P}}+\\frac{p_{P}}{\\beta^{2}}\\right)+\\frac{48\\gamma\\omega_{D}\\omega_{P}L_{\\operatorname*{max}}^{2}}{n p_{D}}\\left(\\beta+p_{P}\\right)}\\\\ &{\\le\\left(\\frac{3\\gamma\\omega_{D}\\left(\\omega_{P}+1\\right)\\beta^{2}}{n p_{D}}+\\frac{24\\gamma\\omega_{D}}{n p_{D}}+\\frac{24\\gamma\\omega_{D}\\omega_{P}\\beta}{n p_{D}}+\\frac{48\\gamma\\omega_{D}\\omega_{P}\\beta}{n p_{D}}+\\frac{48\\gamma\\omega_{D}\\omega_{P}p_{P}}{n p_{D}}\\right)L_{\\operatorname*{max}}^{2}}\\\\ &{\\quad+\\left(\\frac{16\\gamma}{\\beta^{2}}+\\frac{16\\gamma\\omega_{P}}{\\beta}+\\frac{32\\gamma\\omega_{P}}{p_{P}}+\\frac{32\\gamma\\omega_{P}p_{P}}{\\beta^{2}}\\right)L_{A}^{2}+\\left(\\frac{32\\gamma\\theta}{p_{P}}+\\frac{32\\gamma p\\rho}{\\beta^{2}}+\\frac{16\\gamma}{\\beta^{2}}+\\frac{16\\gamma\\theta}{\\beta}\\right)L_{\\operatorname*{max}}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We next consider the coeffcients of $L_{B}^{2},L_{A}^{2}$ and $L_{\\mathrm{max}}^{2}$ . First, for $L_{B}^{2}$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{32\\gamma\\theta}{p{\\cal P}}+\\frac{32\\gamma p_{\\cal P}\\theta}{\\beta^{2}}+\\frac{16\\gamma}{\\beta^{2}}+\\frac{16\\gamma\\theta}{\\beta}\\le32\\gamma\\left(\\frac{\\theta}{p_{\\cal P}}\\left(1+\\frac{p_{\\cal P}}{\\beta}+\\frac{p_{\\cal P}^{2}}{\\beta^{2}}\\right)+\\frac{1}{\\beta^{2}}\\right)}}\\\\ &{}&{\\le64\\gamma\\left(\\frac{\\theta}{p_{\\cal P}}+\\frac{\\theta p_{\\cal P}}{\\beta^{2}}+\\frac{1}{\\beta^{2}}\\right)}\\\\ &{}&{=64\\gamma\\left(\\frac{\\theta}{p_{\\cal P}}+\\frac{1+\\theta p_{\\cal P}}{\\beta^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Next, the coefficient of $L_{A}^{2}$ can be bounded as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\frac{16\\gamma}{\\beta^{2}}+\\frac{16\\gamma\\omega_{P}}{\\beta}+\\frac{32\\gamma\\omega_{P}}{p_{P}}+\\frac{32\\gamma\\omega_{P}p_{P}}{\\beta^{2}}\\leq32\\gamma\\left(\\frac{1}{\\beta^{2}}+\\frac{\\omega_{P}}{p_{P}}\\left(1+\\frac{p_{P}}{\\beta}+\\frac{p_{P}^{2}}{\\beta^{2}}\\right)\\right)}&{}&\\\\ {\\leq64\\gamma\\left(\\frac{\\omega_{P}}{p_{P}}+\\frac{\\omega_{P}p_{P}}{\\beta^{2}}+\\frac{1}{\\beta^{2}}\\right)}&{}&\\\\ {\\leq64\\gamma\\left(\\frac{\\omega_{P}}{p_{P}}+\\frac{1+\\omega_{P}p_{P}}{\\beta^{2}}\\right),}&\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and for L2 $L_{\\mathrm{max}}^{2}$ we obtain ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{3\\gamma\\omega_{D}\\left(\\omega_{P}+1\\right)\\beta^{2}}{n p_{D}}+\\frac{24\\gamma\\omega_{D}}{n p_{D}}+\\frac{24\\gamma\\omega_{D}\\omega_{P}\\beta}{n p_{D}}+\\frac{48\\gamma\\omega_{D}\\omega_{P}\\beta}{n p_{D}}+\\frac{48\\gamma\\omega_{D}\\omega_{P}p_{P}}{n p_{D}}}\\\\ &{\\leq72\\gamma\\omega_{D}\\left(\\frac{\\left(\\omega_{P}+1\\right)\\beta^{2}}{n p_{D}}+\\frac{1}{n p_{D}}+\\frac{\\omega_{P}\\beta}{n p_{D}}+\\frac{\\omega_{P}p_{P}}{n p_{D}}\\right)}\\\\ &{\\leq144\\gamma\\omega_{D}\\left(\\frac{1}{n p_{D}}+\\frac{\\omega_{P}\\beta}{n p_{D}}+\\frac{\\omega_{P}p_{P}}{n p_{D}}\\right)}\\\\ &{=144\\gamma\\left(\\frac{\\omega_{D}\\omega_{P}\\beta}{n p_{D}}+\\frac{\\omega_{D}\\left(1+\\omega_{P}p_{P}\\right)}{n p_{D}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "since (wp+1)\u03b22 $\\begin{array}{r}{\\frac{(\\omega_{P}+1)\\beta^{2}}{n p_{D}}\\leq\\frac{1}{n p_{D}}+\\frac{\\omega_{P}\\beta}{n p_{D}}}\\end{array}$ Substituting these inequalities to (31) and (32), we get ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\beta^{+1}\\right]+\\mathbb{E}\\mathbb{E}\\left[\\left\\|g^{t+1}-\\frac{1}{n}\\frac{\\ln}{\\ln\\xi_{n}}\\right\\|_{L^{2}}^{2}\\right]+\\eta\\mathbb{E}\\left[\\left\\|g^{t+1}-w^{t+1}\\right\\|_{1}^{2}\\right]}\\\\ &{\\quad+\\,\\nu\\mathbb{E}\\left[\\frac{1}{n}\\frac{\\ln}{\\ln\\xi_{n}}\\right\\|_{L^{\\frac{t+1}{\\alpha}}-w^{t+1}}^{2}\\|^{2}\\right]+\\beta\\mathbb{E}\\left[\\left\\|w^{t+1}-x^{t+1}\\right\\|_{1}^{2}\\right]+\\mu\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\|w_{i}^{t+1}-x^{t+1}\\|_{1}^{2}\\right]}\\\\ &{\\lesssim\\mathbb{E}\\left[\\delta^{2}\\right]-\\frac{2}{n}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{t})\\right\\|_{1}^{2}\\right]-\\left(\\frac{1}{2\\gamma}-\\frac{L}{2}\\right)\\mathbb{E}\\left[\\left\\|x^{t+1}-x^{t}\\right\\|_{1}^{2}\\right]+\\kappa\\mathbb{E}\\left[\\left\\|g^{t}-\\frac{1}{n}\\frac{\\ln}{\\ln\\xi_{n}}\\nabla f_{i}(z_{t}^{t})\\right\\|_{1}^{2}\\right]}\\\\ &{\\quad+\\,\\eta\\mathbb{E}\\left[\\left\\|z^{t}-w^{t}\\right\\|_{1}^{2}\\right]+\\nu\\mathbb{E}\\left[\\frac{1}{n}\\frac{\\ln}{\\ln\\xi_{n}}\\left\\|z_{i}^{t}-w^{t}\\right\\|_{1}^{2}\\right]+\\beta\\mathbb{E}\\left[\\left\\|w^{t}-z^{t}\\right\\|_{1}^{2}+\\mu\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\|w_{i}^{t}-z^{t}\\|_{1}^{2}\\right]\\right]}\\\\ &{\\quad+\\,144\\gamma\\left(\\left(\\frac{\\theta}{p p}+\\frac{1+\\theta p p}{\\beta^{2}}\\right)L_{n}^{2}+\\left(\\frac{\\log}{p p}+\\frac{1+\\omega p p\\nu}{\\beta^{2}}\\right)L_{n}^{2}\\right.}\\\\ &{\\qquad\\qquad\\left.+\\left(\\frac{\\omega p\\,\\partial p\\,\\partial p}{\\eta p}+\\frac{\\omega\\mu(1+\\omega p p\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "By collecting all the terms w.r.t. $\\mathbb{E}\\left[\\left\\Vert x^{t+1}-x^{t}\\right\\Vert^{2}\\right]$ , using the step size $\\gamma$ from the theorem and Lemma H.2, we obtain ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\Psi^{t+1}\\right]=\\mathbb{E}\\left[\\delta^{t+1}\\right]+\\kappa\\mathbb{E}\\left[\\left\\|g^{t+1}-\\frac{1}{n}\\frac{\\ln{n}}{\\log{n}}\\nabla f_{t}(z_{t}^{t+1})\\right\\|^{2}\\right]+\\eta\\mathbb{E}\\left[\\left\\|g^{t+1}-w^{t+1}\\right\\|^{2}\\right]}\\\\ &{\\qquad+\\nu\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\|z_{i}^{t+1}-w_{i}^{t+1}\\|^{2}\\right]+\\rho\\mathbb{E}\\left[\\left\\|w^{t+1}-x^{t+1}\\right\\|^{2}\\right]}\\\\ &{\\qquad+\\mu\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\|w_{i}^{t+1}-x^{t+1}\\|^{2}\\right]}\\\\ &{\\qquad\\leq\\mathbb{E}\\left[\\delta^{t}\\right]-\\frac{\\gamma}{2}\\mathbb{E}\\left[\\left\\|\\nabla f^{t}(x^{t})\\right\\|^{2}\\right]+\\kappa\\mathbb{E}\\left[\\left\\|g^{t}-\\frac{1}{n}\\sum_{i=1}^{n}\\nabla f_{i}(z_{t}^{t})\\right\\|^{2}\\right]+\\eta\\mathbb{E}\\left[\\left\\|z^{t}-w^{t}\\right\\|^{2}\\right]}\\\\ &{\\qquad+\\nu\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\|z_{i}^{t}-w_{i}^{t}\\|^{2}\\right]+\\rho\\mathbb{E}\\left[\\left\\|w^{t}-x^{t}\\right\\|^{2}\\right]+\\mu\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\|w_{i}^{t}-x^{t}\\|^{2}\\right]}\\\\ &{\\qquad=\\mathbb{E}\\left[\\nu\\right\\|^{2}-\\frac{\\gamma}{2}\\mathbb{E}\\left[\\left\\|\\nabla f^{t}(x^{t})\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "It remains to rearrange and sum the last inequality for $t=0,\\dots,T-1$ ", "page_idx": 39}, {"type": "text", "text": "Corollary E.2. Let $\\mathcal{C}_{i}^{t}\\,\\in\\,\\mathbb{P}(0)$ for all $i\\,\\in\\,[n]$ (e.g. PermK), choose $p_{P}\\,=\\,1/(\\omega_{P}+1)$ $p_{D}=$ $1/(\\omega_{D}+1)$ and ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\beta=\\operatorname*{min}\\left\\{\\left(\\frac{n}{\\omega_{D}\\omega_{P}(\\omega_{D}+1)}\\right)^{1/3},1\\right\\}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Then, in the view of Theorem E.1, the iteration complexity is ", "page_idx": 40}, {"type": "equation", "text": "$$\n>\\left(\\frac{\\Psi^{0}}{\\varepsilon}\\left(L_{\\operatorname*{max}}+\\left(\\frac{\\omega_{D}\\omega_{P}(\\omega_{D}+1)}{n}\\right)^{1/3}L_{\\operatorname*{max}}+\\sqrt{\\frac{\\omega_{D}(\\omega_{D}+1)}{n}}L_{\\operatorname*{max}}+\\sqrt{\\omega_{P}(\\omega_{P}+1)}L_{A}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof. By Theorem E.1, up to a constant factor, the algorithm converges after ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{v}:=\\frac{\\Psi^{0}}{\\varepsilon}\\left(L+\\sqrt{\\left(\\frac{\\theta}{p{p_{P}}}+\\frac{1+\\theta p_{P}}{\\beta^{2}}\\right)L_{B}^{2}+\\left(\\frac{\\omega p}{p{P_{P}}}+\\frac{1+\\omega p{p_{P}}}{\\beta^{2}}\\right)L_{A}^{2}+\\left(\\frac{\\omega p\\omega\\gamma\\beta}{n p_{D}}+\\frac{\\omega p\\left(1+\\omega p_{P}\\right)}{n p_{D}}\\right.\\right.}\\\\ &{=\\frac{\\Psi^{0}}{\\varepsilon}\\left(L+\\sqrt{\\frac{1}{\\beta^{2}}L_{B}^{2}+\\left(\\frac{\\omega p}{p{p_{P}}}+\\frac{1+\\omega p{p_{P}}}{\\beta^{2}}\\right)L_{A}^{2}+\\left(\\frac{\\omega p\\omega\\gamma\\beta}{n p_{D}}+\\frac{\\omega p\\left(1+\\omega p{p_{P}}\\right)}{n p_{D}}\\right)L_{\\operatorname*{max}}^{2}\\right)}\\\\ &{\\dot{\\leq}\\left(L+\\sqrt{\\frac{L_{B}^{2}}{\\beta^{2}}+\\left(\\frac{\\omega p}{p{p_{P}}}+\\frac{2}{\\beta^{2}}\\right)L_{A}^{2}+\\left(\\frac{\\omega p\\omega\\gamma\\beta}{n p_{D}}+\\frac{2\\omega p}{n p_{D}}\\right)L_{\\operatorname*{max}}^{2}\\right)}\\\\ &{\\dot{\\leq}\\left(L+\\sqrt{\\frac{L_{B}^{2}}{\\beta^{2}}+\\left(\\omega p\\left(\\omega p+1\\right)+\\frac{2}{\\beta^{2}}\\right)L_{A}^{2}+\\left(\\frac{\\omega p\\omega\\gamma\\left(\\omega p+1\\right)\\beta}{n}+\\frac{2\\omega p\\left(\\omega p+1\\right)}{n}\\right)L_{\\operatorname*{max}}^{2}\\right)}\\\\ &{\\dot{\\leq}\\left(L+\\sqrt{\\frac{L_{A}^{2}+L_{B}^{2}}{\\beta^{2}}+\\frac{\\omega p\\omega\\gamma\\left(\\omega p+1\\right)\\beta}{n}}L_{\\operatorname*{max}}^{2}+\\frac{\\omega p\\left(\\omega p+1\\right)}{n}L_{\\operatorname*{max}}^{2}+\\omega p\\left(\\omega p+1\\right)L_{A}^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "$p_{P}$ $p_{D}$ $L_{A}^{2}+L_{B}^{2}\\leq L_{\\operatorname*{max}}^{2}$ ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\tau}\\leq\\frac{2\\Psi^{0}}{\\varepsilon}\\left(L+\\sqrt{\\left(\\frac{1}{\\beta^{2}}+\\frac{\\omega_{D}\\omega_{P}\\left(\\omega_{D}+1\\right)\\beta}{n}\\right)L_{\\operatorname*{max}}^{2}+\\frac{\\omega_{D}\\left(\\omega_{D}+1\\right)}{n}L_{\\operatorname*{max}}^{2}+\\omega_{P}\\left(\\omega_{P}+1\\right)L_{A}^{2}}\\right)}\\\\ &{\\ \\leq\\frac{4\\Psi^{0}}{\\varepsilon}\\left(L+\\sqrt{\\left(1+\\left(\\frac{\\omega_{D}\\omega_{P}\\left(\\omega_{D}+1\\right)}{n}\\right)^{2/3}\\right)L_{\\operatorname*{max}}^{2}+\\frac{\\omega_{D}\\left(\\omega_{D}+1\\right)}{n}L_{\\operatorname*{max}}^{2}+\\omega_{P}\\left(\\omega_{P}+1\\right)L_{A}^{2}}\\right)}\\\\ &{\\ \\leq\\frac{8\\Psi^{0}}{\\varepsilon}\\left(L_{\\operatorname*{max}}+\\left(\\frac{\\omega_{D}\\omega_{P}\\left(\\omega_{D}+1\\right)}{n}\\right)^{1/3}L_{\\operatorname*{max}}+\\sqrt{\\frac{\\omega_{D}\\left(\\omega_{D}+1\\right)}{n}}L_{\\operatorname*{max}}+\\sqrt{\\omega_{P}\\left(\\omega_{P}+1\\right)}L_{A}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where we substitute our choice of $\\beta$ ", "page_idx": 40}, {"type": "text", "text": "Corollary E.3. Let $\\mathcal{C}_{i}^{t}$ be the PermK compressors and $\\mathcal{Q}_{i}^{t}$ be the independent (Assumption 1.6) RandK compressors, both with $K=d/_{n}$ Then, in the view of Corollary $E.2$ the iteration complexity is ", "page_idx": 40}, {"type": "equation", "text": "$$\n{\\mathcal O}\\left(\\frac{\\Psi^{0}}{\\varepsilon}\\left(n^{2/3}L_{\\mathrm{max}}+n L_{A}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and the total communication complexity is ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\frac{\\Psi^{0}}{\\varepsilon}\\left(\\frac{d L_{\\operatorname*{max}}}{n^{1/3}}+d L_{A}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof. The choice of compressors and parameters ensures that $\\omega_{P}=\\omega_{D}=n-1$ (Lemma A.6). Thus, the iteration complexity is ", "page_idx": 40}, {"type": "equation", "text": "$$\n?\\left(\\frac{\\Psi^{0}}{\\varepsilon}\\left(L_{\\operatorname*{max}}+\\left(\\frac{\\omega_{D}\\omega_{P}(\\omega_{D}+1)}{n}\\right)^{1/3}L_{\\operatorname*{max}}+\\sqrt{\\frac{\\omega_{D}(\\omega_{D}+1)}{n}}L_{\\operatorname*{max}}+\\sqrt{\\omega_{P}(\\omega_{P}+1)}L_{A}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 40}, {"type": "equation", "text": "$$\n=\\mathcal{O}\\left(\\frac{\\Psi^{0}}{\\varepsilon}\\left(L_{\\operatorname*{max}}+n^{2/3}L_{\\operatorname*{max}}+\\sqrt{n}L_{\\operatorname*{max}}+n L_{A}\\right)\\right)=\\mathcal{O}\\left(\\frac{\\Psi^{0}}{\\varepsilon}\\left(n^{2/3}L_{\\operatorname*{max}}+n L_{A}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Since $p_{P}=p_{D}=1/n$ and $K=d/n$ , on average, the algorithm sends $\\leq~{\\frac{2d}{n}}$ coordinates in both directions. Therefore, the total communication complexity is ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\frac{d}{n}\\times\\frac{\\Psi^{0}}{\\varepsilon}\\left(n^{2/3}L_{\\operatorname*{max}}+n L_{A}\\right)\\right)=\\mathcal{O}\\left(\\frac{\\Psi^{0}}{\\varepsilon}\\left(\\frac{d}{n^{1/3}}L_{\\operatorname*{max}}+d L_{A}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "E.3Polyak-Lojasiewicz condition ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "E.3.1 Main Results ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "As with MARINA-P, we provide the analysis of M3 under the Polyak-Lojasiewicz condition. ", "page_idx": 41}, {"type": "text", "text": "Theorem E.8. Let Assumptions 1.1, 1.2, 1.5, 4.2 and $D.9$ be satisfied and suppose that the compressors $\\mathcal{Q}_{i}^{t}\\in\\mathbb{U}(\\omega_{D})$ satisfy Assumption 1.6, $\\left\\{{\\mathcal{C}}_{i}^{t}\\right\\}_{i=1}^{n}\\in\\mathbb{P}(\\theta)$ and $\\mathcal{C}_{i}^{t}\\in\\mathbb{U}(\\omega_{P})$ for all $i\\in[n]$ .Let $\\gamma>0$ be such that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\prime=\\operatorname*{min}\\Bigg\\{\\Bigg(L+\\sqrt{1536\\Bigg(\\left(\\frac{\\theta}{p_{P}}+\\frac{1+\\theta p_{P}}{\\beta^{2}}\\right)L_{B}^{2}+\\left(\\frac{\\omega_{P}}{p_{P}}+\\frac{1+\\omega_{P}p_{P}}{\\beta^{2}}\\right)L_{A}^{2}+\\left(\\frac{\\omega_{D}\\omega_{P}\\beta}{n p_{D}}+\\frac{\\omega_{D}\\left(1+\\omega_{P}p_{P}\\right)}{n p_{D}}\\right)}\\Bigg)}\\\\ &{\\qquad}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ &{\\qquad}&{\\qquad\\qquad\\frac{p_{P}}{2\\mu},\\frac{p_{D}}{2\\mu},\\frac{\\beta}{4\\mu}\\Bigg\\}.\\Bigg.}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(33)}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Letting ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Psi^{t}=\\delta^{t}+\\kappa\\left\\|g^{t}-\\frac1n\\sum_{i=1}^{n}\\nabla f_{i}(z_{i}^{t})\\right\\|^{2}+\\eta\\left\\|z^{t}-w^{t}\\right\\|^{2}+\\nu\\frac1n\\sum_{i=1}^{n}\\left\\|z_{i}^{t}-w_{i}^{t}\\right\\|^{2}}\\\\ {\\displaystyle\\quad+\\,\\rho\\left\\|w^{t}-x^{t}\\right\\|^{2}+\\tau\\frac1n\\sum_{i=1}^{n}\\left\\|w_{i}^{t}-x^{t}\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $\\begin{array}{r}{\\kappa\\ =\\ \\frac{2\\gamma}{p_{D}}}\\end{array}$ \uff0c $\\begin{array}{r}{\\eta~=~\\frac{8\\gamma L_{B}^{2}}{\\beta}}\\end{array}$ $\\begin{array}{r}{\\nu\\ =\\ \\frac{8\\gamma L_{A}^{2}}{\\beta}\\,+\\,\\frac{24\\gamma\\omega_{D}\\beta L_{\\mathrm{max}}^{2}}{n p_{D}}}\\end{array}$ \uff0c $\\begin{array}{r}{\\rho\\ =\\ 128\\gamma L_{B}^{2}\\left(\\frac{1}{p_{P}}+\\frac{p_{P}}{\\beta^{2}}\\right)}\\end{array}$ and $\\tau=$ $\\begin{array}{r}{128\\gamma L_{A}^{2}\\left(\\frac{1}{p_{P}}+\\frac{p_{P}}{\\beta^{2}}\\right)+\\frac{384\\gamma\\omega_{D}L_{\\mathrm{max}}^{2}}{n p_{D}}\\left(\\beta+p_{P}\\right)\\;}\\end{array}$ 34pD \u03b2 + p), M3 enswresthat for each T \u2265 1 ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Psi^{T}\\right]\\leq\\left(1-\\gamma\\mu\\right)^{T}\\Psi^{0}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Corollary E.9. Let $\\mathcal{C}_{i}^{t}\\,\\in\\,\\mathbb{P}(0)$ for all $i\\,\\in\\,[n]$ (e.g. PermK), choose $p_{P}\\,=\\,1/(\\omega_{P}+1),\\,p_{D}\\,=$ $1/(\\omega_{D}+1)$ and ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\beta=\\operatorname*{min}\\left\\{\\left(\\frac{n}{\\omega_{D}\\omega_{P}(\\omega_{D}+1)}\\right)^{1/3},1\\right\\}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{I}\\left(\\operatorname*{max}\\left\\{\\frac{\\left(1+\\left(\\frac{\\omega_{D}\\omega_{P}(\\omega_{D}+1)}{n}\\right)^{1/3}+\\sqrt{\\frac{\\omega_{D}(\\omega_{D}+1)}{n}}\\right)L_{\\operatorname*{max}}+\\sqrt{\\omega_{P}(\\omega_{P}+1)}L_{A}}{\\mu},\\omega_{P}+1,\\omega_{D}+1,\\left(\\frac{\\omega_{D}\\omega_{P}(\\omega_{D}+1)}{n}\\right)L_{A}\\right\\}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "iterations. ", "page_idx": 41}, {"type": "text", "text": "Corollary E.10. Let $\\mathcal{C}_{i}^{t}$ be thePerm $K$ compressors and $\\mathcal{Q}_{i}^{t}$ be the independent (Assumption 1.6) RandK compressors, both with $K=d/_{n}$ . Then, in the view of Corollary $E.9$ the total communication complexityis ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\left(\\frac{d L_{\\mathrm{max}}}{n^{1/3}\\mu}+\\frac{d L_{A}}{\\mu}+d\\right)\\log\\frac{\\Psi^{0}}{\\varepsilon}\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "E.3.2Proofs ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Theorem E.8. Let Assumptions 1.1, 1.2, 1.5, 4.2 and $D.9$ besatisfied andsupposethat thecompressors $\\mathcal{Q}_{i}^{t}\\in\\mathbb{U}(\\omega_{D})$ satisfy Assumption 1.6, $\\left\\{{\\mathcal{C}}_{i}^{t}\\right\\}_{i=1}^{n}\\in\\mathbb{P}(\\theta)$ and $\\mathcal{C}_{i}^{t}\\in\\mathbb{U}(\\omega_{P})$ for all $i\\in[n]$ .Let $\\gamma>0$ be such that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\prime=\\operatorname*{min}\\Bigg\\{\\Bigg(L+\\sqrt{1536\\Bigg(\\left(\\frac{\\theta}{p_{P}}+\\frac{1+\\theta p_{P}}{\\beta^{2}}\\right)L_{B}^{2}+\\left(\\frac{\\omega_{P}}{p_{P}}+\\frac{1+\\omega_{P}p_{P}}{\\beta^{2}}\\right)L_{A}^{2}+\\left(\\frac{\\omega_{D}\\omega_{P}\\beta}{n p_{D}}+\\frac{\\omega_{D}\\left(1+\\omega_{P}p_{P}\\right)}{n p_{D}}\\right)}\\Bigg)}\\\\ &{\\Bigg.}&{\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\frac{p_{P}}{2\\mu},\\frac{p_{D}}{2\\mu},\\frac{\\beta}{4\\mu}\\Bigg\\}.\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(33)}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Letting ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Psi^{t}=\\delta^{t}+\\kappa\\left\\|g^{t}-\\frac1n\\sum_{i=1}^{n}\\nabla f_{i}(z_{i}^{t})\\right\\|^{2}+\\eta\\left\\|z^{t}-w^{t}\\right\\|^{2}+\\nu\\frac1n\\sum_{i=1}^{n}\\left\\|z_{i}^{t}-w_{i}^{t}\\right\\|^{2}}\\\\ {\\displaystyle\\quad+\\,\\rho\\left\\|w^{t}-x^{t}\\right\\|^{2}+\\tau\\frac1n\\sum_{i=1}^{n}\\left\\|w_{i}^{t}-x^{t}\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $\\begin{array}{r}{\\kappa\\ =\\ \\frac{2\\gamma}{p_{D}}}\\end{array}$ \uff0c $\\begin{array}{r}{\\eta~=~\\frac{8\\gamma L_{B}^{2}}{\\beta}}\\end{array}$ $\\begin{array}{r}{\\nu\\ =\\ \\frac{8\\gamma L_{A}^{2}}{\\beta}\\,+\\,\\frac{24\\gamma\\omega_{D}\\beta L_{\\mathrm{max}}^{2}}{n p_{D}}}\\end{array}$ \uff0c $\\begin{array}{r}{\\rho\\ =\\ 128\\gamma L_{B}^{2}\\left(\\frac{1}{p_{P}}+\\frac{p_{P}}{\\beta^{2}}\\right)}\\end{array}$ and $\\tau=$ $\\begin{array}{r}{128\\gamma L_{A}^{2}\\left(\\frac{1}{p_{P}}+\\frac{p_{P}}{\\beta^{2}}\\right)+\\frac{384\\gamma\\omega_{D}L_{\\mathrm{max}}^{2}}{n p_{D}}\\left(\\beta+p_{P}\\right)\\!,}\\end{array}$ 384wDa \u03b2 + pP), M3 ensuresthat for ach T \u2265 1 ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Psi^{T}\\right]\\leq\\left(1-\\gamma\\mu\\right)^{T}\\Psi^{0}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. Starting as in the proof of Theorem E.1, we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mu^{k+1}\\right]+\\mathbb{E}\\Bigg[\\Bigg\\|\\int_{t}^{t+1}-\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\left[f_{i}(\\hat{\\varepsilon}_{i}^{t+1})\\Bigg\\|^{2}\\Bigg]+\\mathbb{E}\\Bigg[\\left[\\mu^{k}\\right]^{2}\\Bigg\\|\\exp\\left[\\imath\\left(\\frac{\\hbar}{\\hbar}\\sum_{i=1}^{n}\\nu_{i}^{t+1}\\right)\\right]^{2}\\Bigg]}\\\\ &{\\quad+\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\Bigg\\|\\exp\\left[\\imath\\left(\\frac{\\hbar}{\\sqrt{n}}\\right)\\Bigg]^{2}+\\mathbb{E}\\Bigg[\\left[\\mu^{k}\\right]^{2}-\\mu^{k+1}\\Bigg]\\right]+\\mathbb{E}\\Bigg[\\Bigg\\|\\frac{1}{n}\\sum_{i=1}^{n}\\Bigg\\|\\exp\\left[\\imath\\left(\\frac{\\hbar}{\\sqrt{n}}\\right)-\\frac{\\hbar}{\\sqrt{n}}\\right]^{2}\\Bigg]}\\\\ &{\\leq\\mathbb{E}\\Bigg[\\Bigg\\|\\int_{t}^{t}-\\frac{1}{n}\\mathbb{E}\\Bigg[\\big[\\gamma_{t}(\\hat{\\varepsilon}_{i}^{t})\\Bigg]^{2}\\Bigg]-\\Bigg(\\frac{1}{2}-\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\Bigg[\\Bigg\\|\\exp\\left[\\imath\\left(-\\frac{\\hbar}{\\sqrt{n}}\\right)\\right]^{2}\\Bigg]+\\mathbb{E}\\Bigg[\\Bigg\\|\\tilde{\\sigma}_{i}^{t}-\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{\\gamma_{i}(\\hat{\\varepsilon}_{i}^{t})}\\Bigg]^{2}\\Bigg]}\\\\ &{\\quad+2\\mathbb{E}\\Bigg[\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\Bigg[\\Bigg\\|\\alpha\\Bigg(\\frac{\\hbar}{\\sqrt{n}}\\Bigg\\|-\\gamma_{t}^{t}\\Bigg)^{2}\\Bigg\\|+\\mathbb{E}\\Bigg[\\Bigg\\|\\gamma_{t}(\\hat{\\varepsilon}_{i}^{t})\\Bigg\\|^{2}\\Bigg]}\\\\ &{\\quad+\\mathbb{E}\\Bigg[\\delta\\Bigg(\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\Bigg[\\Bigg\\|\\alpha\\Bigg(\\frac{\\hbar}{\\sqrt{n}}\\Bigg)\\Bigg\\|\\Bigg)+\\mathbb{E}\\Bigg[\\Bigg\\|\\alpha^{k}-\\gamma_{t}^{t}\\Bigg\\|^{2}\\Bigg]\\Bigg)}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad+\\,4p_{P}\\left(1+\\frac{p_{P}}{\\beta}\\right)\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|w_{i}^{t}-x^{t}\\right\\|^{2}\\right]\\right)}\\\\ &{+\\,\\rho(1-p_{P})\\left(\\mathbb{E}\\left[\\left\\|w^{t}-x^{t}\\right\\|^{2}\\right]+\\theta\\mathbb{E}\\left[\\left\\|x^{t+1}-x^{t}\\right\\|^{2}\\right]\\right)}\\\\ &{+\\,\\tau(1-p_{P})\\left(\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|w_{i}^{t}-x^{t}\\right\\|^{2}\\right]+\\omega_{P}\\mathbb{E}\\left[\\left\\|x^{t+1}-x^{t}\\right\\|^{2}\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "for some $\\kappa,\\eta,\\nu,\\rho,\\tau\\geq0$ . This time, we let $\\begin{array}{r}{\\kappa=\\frac{2\\gamma}{p_{D}}}\\end{array}$ and $\\begin{array}{r}{\\eta=\\frac{8\\gamma L_{B}^{2}}{\\beta}}\\end{array}$ L, which gives /+(1 - pD) = $\\kappa\\left(1-\\textstyle{\\frac{p_{D}}{2}}\\right)$ and $\\begin{array}{r}{2\\gamma L_{B}^{2}+\\eta(1-\\beta/2)=\\eta\\left(1-\\frac{\\beta}{4}\\right)}\\end{array}$ . Hence ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\{\\mu^{\\alpha+1}\\}+\\lambda\\Bigg[\\Bigg|\\hat{\\mu}^{\\alpha+1}-\\frac{1}{n_{x}^{\\alpha}}\\sum_{i=1}^{n}\\mathbb{E}\\{f_{i}(\\hat{u}^{\\alpha})^{\\alpha}\\}\\Bigg|\\Bigg]+\\eta\\Bigg[\\Big|f_{i}(\\hat{u}^{\\alpha}-\\alpha^{\\alpha,\\beta})\\Big|^{2}\\Bigg]}\\\\ &{\\quad+\\eta\\Bigg[\\frac{1}{n_{x}^{\\alpha}}\\sum_{i=1}^{n}\\mathbb{E}\\{\\mu^{\\alpha},\\eta^{\\alpha+1}\\}^{2}\\Bigg]+\\eta\\Bigg[\\mathbb{E}\\{\\mu^{\\alpha+1}-\\alpha^{\\alpha,\\beta}\\}\\Bigg|^{2}+\\eta\\Bigg[\\frac{1}{n_{x}^{\\alpha}}\\sum_{i=1}^{n}\\mathbb{E}\\Bigg|\\frac{1}{n_{x}^{\\alpha}}(\\hat{u}^{\\alpha}-\\alpha^{\\alpha,\\beta})\\Bigg]}\\\\ &{\\leq\\mathbb{E}\\{\\mu^{\\alpha},\\eta^{\\alpha}\\}-\\frac{2}{n_{x}^{\\alpha}}\\mathbb{E}\\Bigg[\\eta\\{\\nu^{\\alpha}\\}\\Bigg|^{2}-\\Bigg(\\frac{1}{n_{x}^{\\alpha}}-\\frac{1}{n_{x}^{\\alpha}}\\sum_{i=1}^{n}\\mathbb{E}\\Big|\\eta^{\\alpha+1}-\\eta^{\\alpha}\\}\\\\ &{\\qquad+\\nu\\{\\frac{99}{2n_{x}^{\\alpha}}\\}\\Bigg)\\mathbb{E}\\Bigg[\\Bigg|\\nu^{\\alpha}-\\frac{1}{n_{x}^{\\alpha}}\\sum_{i=1}^{n}\\mathbb{E}\\{\\mu^{\\alpha}\\}\\Bigg|^{2}\\Bigg]+\\eta\\Bigg(-\\frac{\\alpha}{4}\\frac{\\alpha}{2}\\Bigg)\\mathbb{E}\\{\\mu^{\\alpha}-\\nu\\}\\Bigg|^{2}}\\\\ &{\\quad+2\\eta\\lambda_{x}^{\\alpha}\\sum_{i=1}^{n}\\mathbb{E}\\Bigg[\\eta\\Bigg|\\frac{1}{n_{x}^{\\alpha}}\\mathbb{E}\\Bigg[\\eta\\Bigg|\\Bigg|^{2}+\\eta\\Bigg[\\alpha^{\\alpha}-\\nu^{\\alpha}\\}\\end{array}\\Bigg|\\eta^{\\alpha}\\Bigg]+\\eta^{\\alpha}\\Bigg[\\mathbb{E}\\{\\mu^{\\alpha}-\\nu^{\\alpha}\\}\\Bigg|^{2}\\Bigg]}\\\\ &{\\quad+\\frac{2}{\\eta\\alpha}\\Bigg(\\frac{\\alpha\\eta^{\\alpha}}{4}\\mathbb{E}\n$$((1-\uff09-\u00b2+4(+p\uff09[+-] ", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Rearranging the terms ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\delta^{t+1}\\right]+\\kappa\\mathbb{E}\\left[\\left\\|g^{t+1}-\\frac{1}{n}\\sum_{i=1}^{n}\\nabla f_{i}(z_{i}^{t+1})\\right\\|^{2}\\right]+\\eta\\mathbb{E}\\left[\\left\\|z^{t+1}-w^{t+1}\\right\\|^{2}\\right]}\\\\ {\\quad+\\,\\nu\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|z_{i}^{t+1}-w_{i}^{t+1}\\right\\|^{2}\\right]+\\rho\\mathbb{E}\\left[\\left\\|w^{t+1}-x^{t+1}\\right\\|^{2}\\right]+\\tau\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|w_{i}^{t+1}-x^{t+1}\\right\\|^{2}\\right]}\\\\ {\\leq\\mathbb{E}\\left[\\delta^{t}\\right]-\\frac{\\gamma}{2}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{t})\\right\\|^{2}\\right]-\\left(\\frac{1}{2\\gamma}-\\frac{L}{2}\\right)\\mathbb{E}\\left[\\left\\|x^{t+1}-x^{t}\\right\\|^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left.+\\kappa\\left(1-\\displaystyle\\frac{p_{D}}{2}\\right)\\mathbb{E}\\left[\\left\\|g^{t}-\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\nabla f_{i}(z_{i}^{t})\\right\\|^{2}\\right]+\\eta\\left(1-\\displaystyle\\frac{\\beta}{4}\\right)\\mathbb{E}\\left[\\left\\|z^{t}-w^{t}\\right\\|^{2}\\right]\\right.}\\\\ &{+\\left(\\nu\\left(1-\\displaystyle\\frac{\\beta}{2}\\right)+2\\gamma L_{A}^{2}+\\frac{6\\gamma\\alpha_{D}\\beta^{2}L_{\\operatorname*{max}}^{2}}{n p_{D}}\\right)\\mathbb{E}\\left[\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\|z_{i}^{t}-w_{i}^{t}\\|^{2}\\right]}\\\\ &{+\\left(\\frac{6\\gamma\\omega_{D}\\left(\\omega p+1\\right)\\beta^{2}L_{\\operatorname*{max}}^{2}}{n p_{D}}+\\rho\\theta+\\frac{32\\gamma L_{B}^{2}}{\\beta}\\left(\\displaystyle\\frac{1}{\\beta}+\\theta\\right)+4\\nu\\left(\\displaystyle\\frac{1}{\\beta}+\\omega p\\right)+\\tau\\omega p\\right)\\mathbb{E}\\left[\\left\\|x^{t+1}-x^{t}\\right\\|^{2}\\right]}\\\\ &{+\\left(\\rho(1-p_{P})+2\\gamma L_{B}^{2}+\\frac{32\\gamma L_{B}^{2}p_{P}}{\\beta}\\left(1+\\frac{p_{P}}{\\beta}\\right)\\right)\\mathbb{E}\\left[\\left\\|w^{t}-x^{t}\\right\\|^{2}\\right]}\\\\ &{+\\left(\\tau(1-p_{P})+2\\gamma L_{A}^{2}+4\\nu p_{P}\\left(1+\\frac{p_{P}}{\\beta}\\right)+\\frac{8\\gamma\\omega_{D}p p\\beta^{2}L_{\\operatorname*{max}}^{2}}{n p_{D}}\\right)\\mathbb{E}\\left[\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\left\\|w_{i}^{t}-x^{t}\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Considering the coeficient of $\\mathbb{E}\\left[\\left\\|w^{t}-x^{t}\\right\\|^{2}\\right]$ and using the inequality $\\begin{array}{r}{x y\\leq\\frac{x^{2}+y^{2}}{2}}\\end{array}$ for all $x,y\\geq0$ we get ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\rho(1-p_{P})+2\\gamma L_{B}^{2}+\\frac{32\\gamma L_{B}^{2}p_{P}}{\\beta}\\left(1+\\frac{p_{P}}{\\beta}\\right)\\le\\rho(1-p_{P})+32\\gamma L_{B}^{2}\\left(1+\\frac{p_{P}}{\\beta}+\\frac{p_{P}^{2}}{\\beta^{2}}\\right)}}\\\\ &{}&{\\le\\rho(1-p_{P})+64\\gamma L_{B}^{2}\\left(1+\\frac{p_{P}^{2}}{\\beta^{2}}\\right)}\\\\ &{}&{=\\rho\\left(1-\\frac{p_{P}}{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where we define $\\begin{array}{r}{\\rho=128\\gamma L_{B}^{2}\\left(\\frac{1}{p_{P}}+\\frac{p_{P}}{\\beta^{2}}\\right)}\\end{array}$ Substituting this choice of $\\rho$ , we obtain ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mu^{k+1}+\\Delta t\\right]\\Bigg\\Vert\\mu^{k+1}-\\frac{1}{n}\\frac{\\sqrt{1}}{m}\\nabla f(\\hat{t}_{i}^{(k+1)})\\Bigg\\Vert^{2}+\\eta\\nabla\\left[\\big[\\mu^{k+1}-\\nu^{\\mu+1}\\big]\\right]^{2}}\\\\ &{+\\mathbb{E}\\left[\\frac{1}{n}\\frac{\\sqrt{1}}{m}\\big[\\mu^{k}-\\nu_{t}^{(k+1)}t^{2}\\big]\\right]+\\eta\\left[\\mu^{k+1}-\\nu^{\\mu+1}\\right]^{2}\\Bigg\\Vert+\\frac{1}{n}\\frac{\\sqrt{1}}{m}\\left[\\mu_{t}^{k+1}-\\sigma^{\\mu+1}\\right]^{2}\\Bigg\\Vert}\\\\ &{\\leq\\mathbb{E}\\left[\\nu^{\\mu}\\right]-\\frac{2\\eta}{\\sqrt{1}}\\mathbb{E}\\left[\\nabla f(\\nu^{\\mu})\\right]^{2}-\\left(\\frac{1}{2},\\frac{1}{2}\\right)\\mathbb{E}\\left[\\nu^{\\mu+1}-\\nu^{\\mu}\\right]^{2}\\Bigg\\Vert}\\\\ &{+\\kappa\\left(1-\\frac{\\eta-2}{\\sqrt{1}}\\right)\\mathbb{E}\\Bigg[\\Bigg\\Vert\\mu^{k}-\\frac{1}{n}\\nabla\\nabla f(\\hat{t}_{i}^{(k)})\\Bigg]+\\eta\\left(1-\\frac{\\eta}{\\sqrt{1}}\\right)\\mathbb{E}\\left[\\nu^{\\mu}-\\nu^{\\mu}\\right]^{2}\\Bigg\\Vert}\\\\ &{+\\rho\\left(1-\\frac{\\eta}{\\sqrt{2}}\\right)\\mathbb{E}\\left[\\nu^{\\mu}-\\nu^{\\mu}\\right]^{2}+\\bigg(\\nu\\Big(1-\\frac{\\eta}{\\sqrt{2}}\\Big)+2\\gamma\\lambda_{2}^{2}+\\frac{6\\eta}{\\sqrt{2}}\\frac{\\sqrt{2}\\lambda_{2}^{2}}{m}\\bigg)\\mathbb{E}\\left[\\frac{1}{n}\\frac{\\sqrt{1}}{m}\\left[\\nu^{\\mu}-\\nu_{t}^{\\mu}\\right]^{2}}\\\\ &{+\\left(\\frac{9-9\\eta}{\\sqrt{1}}\\big(\\nu^{\\mu})\\frac{19\\lambda_{2}^{2}\\mu\\alpha}{m}+12\\nu_{2}\\nabla\\lambda_{3}^{2}\\left(\\frac{1-\\eta}{\\sqrt{1}}\\right)^{2}\\right)\\theta+\\frac{32\\eta}{\\sqrt{3}}\\frac{\\sqrt{3}}{m}\\left(\\\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Similarly, taking $\\begin{array}{r}{\\nu=\\frac{8\\gamma L_{A}^{2}}{\\beta}+\\frac{24\\gamma\\omega_{D}\\beta L_{\\operatorname*{max}}^{2}}{n p_{D}}}\\end{array}$ givs $\\begin{array}{r}{\\nu\\left(1-\\frac{\\beta}{2}\\right)+2\\gamma L_{A}^{2}+\\frac{6\\gamma\\omega_{D}\\beta^{2}L_{\\mathrm{max}}^{2}}{n p_{D}}=\\nu\\left(1-\\frac{\\beta}{4}\\right)\\!,}\\end{array}$ SO $\\mathbb{E}\\left[\\delta^{t+1}\\right]+\\kappa\\mathbb{E}\\left[\\left\\Vert g^{t+1}-\\frac{1}{n}\\sum_{i=1}^{n}\\nabla f_{i}(z_{i}^{t+1})\\right\\Vert^{2}\\right]+\\eta\\mathbb{E}\\left[\\left\\Vert z^{t+1}-w^{t+1}\\right\\Vert^{2}\\right]$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\nu\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\|x_{i}^{\\alpha_{i}-1}-x_{i}^{\\alpha_{i}+1}\\|^{2}\\right]+\\nu\\mathbb{E}\\left[\\left|w^{\\alpha_{i}-1}-x_{i}^{\\alpha_{i}+1}\\right|^{2}\\right]+\\nu\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\|x_{i}^{\\alpha_{i}-1}-x_{i}^{\\alpha_{i}}\\|^{2}\\right]}\\\\ &{\\leq\\nu\\mathbb{E}\\left[\\nu^{\\alpha_{i}}\\sum_{j=1}^{n}\\left\\|y^{\\alpha_{j}}\\right\\|^{2}\\right]-\\left(\\frac{1}{\\Delta y}-\\frac{1}{2}\\right)\\mathbb{E}\\left[\\left|x^{\\alpha_{i}-1}-x_{i}^{\\alpha_{j}}\\right|^{2}\\right]}\\\\ &{\\quad+\\nu\\left(1-\\frac{\\nu_{i}}{2}\\right)\\mathbb{E}\\left[\\left|y^{\\alpha_{i}-1}\\frac{1}{\\Delta z}\\nabla F(x_{i}^{\\alpha_{i}})\\right|^{2}\\right]+n\\left(1-\\frac{\\nu_{i}}{2}\\right)\\mathbb{E}\\left[\\left|z^{\\alpha_{i}-1}\\ e^{-1}\\right|^{2}\\right]}\\\\ &{\\quad+\\nu\\left(1-\\frac{\\nu_{i}}{2}\\right)\\mathbb{E}\\left[\\left|w^{\\alpha_{i}-1}z^{\\alpha_{i}}\\right|^{2}\\right]+\\nu\\left(1-\\frac{\\alpha_{i}}{2}\\right)\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\|c_{i}^{\\alpha_{i}}\\|^{2}\\right]}\\\\ &{\\quad+\\left(\\frac{\\nu_{i}\\times\\rho(\\alpha_{i}+1)}{n}+\\frac{1}{2}L_{n}^{2}\\right)+12\\nu\\mathbb{E}\\left[\\left|\\frac{1}{n}\\int_{n}^{+}+\\frac{\\nu_{i}}{2}\\right|^{2}\\right)\\theta+\\frac{32\\nu_{i}\\sum_{j=1}^{n}\\left(\\frac{1}{\\Delta}+\\theta\\right)}{\\beta}}\\\\ &{\\quad\\quad+\\frac{4\\nu\\left(1-\\frac{\\alpha_{i}}{2}\\right)\\Delta^{2}(1-\\frac{\\alpha_{j}\\Delta\\alpha_{i}\\cdot2}{\\beta})}{\\beta}\\frac{\\bigg(\\frac{1}{\\Delta}+ \n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Considering the last bracket, we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tau(1-p_{P})+2\\gamma L_{A}^{2}+4\\left(\\frac{8\\gamma L_{A}^{2}}{\\beta}+\\frac{24\\gamma\\omega_{D}\\beta L_{\\operatorname*{max}}^{2}}{n p_{D}}\\right)p_{P}\\left(1+\\frac{p_{P}}{\\beta}\\right)+\\frac{8\\gamma\\omega_{D}p\\gamma\\beta^{2}L_{\\operatorname*{max}}^{2}}{n p_{D}}}\\\\ &{=\\tau(1-p_{P})+2\\gamma L_{A}^{2}+\\frac{32\\gamma p_{P}L_{A}^{2}}{\\beta}+\\frac{32\\gamma p_{P}^{2}L_{A}^{2}}{\\beta^{2}}+\\frac{96\\gamma\\omega_{D}p p\\beta L_{\\operatorname*{max}}^{2}}{n p_{D}}+\\frac{96\\gamma\\omega_{D}p_{P}^{2}L_{\\operatorname*{max}}^{2}}{n p_{D}}}\\\\ &{\\quad+\\frac{8\\gamma\\omega_{D}p\\gamma\\beta^{2}L_{\\operatorname*{max}}^{2}}{n p_{D}}}\\\\ &{\\leq\\tau(1-p_{P})+32\\gamma L_{A}^{2}\\left(1+\\frac{p_{P}}{\\beta^{2}}+\\frac{p_{P}^{2}}{\\beta^{2}}\\right)+\\frac{96\\gamma\\omega_{D}p\\gamma L_{\\operatorname*{max}}^{2}}{n p_{D}}\\left(\\beta+p_{P}+\\beta^{2}\\right)}\\\\ &{\\leq\\tau(1-p_{P})+64\\gamma L_{A}^{2}\\left(1+\\frac{p_{P}^{2}}{\\beta^{2}}\\right)+\\frac{192\\gamma\\omega_{D}p\\gamma L_{\\operatorname*{max}}^{2}}{n p_{D}}\\left(\\beta+p_{P}\\right)}\\\\ &{=\\tau\\left(1-\\frac{p_{P}}{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "for $\\begin{array}{r}{\\tau=128\\gamma L_{A}^{2}\\left(\\frac{1}{p_{P}}+\\frac{p_{P}}{\\beta^{2}}\\right)+\\frac{384\\gamma\\omega_{D}L_{\\mathrm{max}}^{2}}{n p_{D}}\\left(\\beta+p_{P}\\right)}\\end{array}$ Then ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\delta^{t+1}\\right]+\\kappa\\mathbb{E}\\left[\\left\\|g^{t+1}-\\frac{1}{n}\\sum_{i=1}^{n}\\nabla f_{i}(z_{i}^{t+1})\\right\\|^{2}\\right]+\\eta\\mathbb{E}\\left[\\left\\|z^{t+1}-w^{t+1}\\right\\|^{2}\\right]}\\\\ &{\\quad+\\nu\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\|z_{i}^{t+1}-w_{i}^{t+1}\\|^{2}\\right]+\\rho\\mathbb{E}\\left[\\left\\|w^{t+1}-x^{t+1}\\right\\|^{2}\\right]+\\eta\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\|w_{i}^{t+1}-x^{t+1}\\|^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\delta^{t}\\right]-\\frac{\\gamma}{2}\\mathbb{E}\\left[\\left\\|\\nabla f(x^{t})\\|^{2}\\right]-\\left(\\frac{1}{2\\gamma}-\\frac{L}{2}\\right)\\mathbb{E}\\left[\\left\\|x^{t+1}-x^{t}\\right\\|^{2}\\right]}\\\\ &{\\quad+\\kappa\\left(1-\\frac{p_{D}}{2}\\right)\\mathbb{E}\\left[\\bigg\\|g^{t}-\\frac{1}{n}\\sum_{i=1}^{n}\\nabla f_{i}(z_{i}^{t})\\bigg\\|^{2}\\right]+\\eta\\left(1-\\frac{\\beta}{4}\\right)\\mathbb{E}\\left[\\|z^{t}-w^{t}\\|^{2}\\right]}\\\\ &{\\quad+\\rho\\left(1-\\frac{p_{D}}{2}\\right)\\mathbb{E}\\left[\\|w^{t}-x^{t}\\|^{2}\\right]+\\nu\\left(1-\\frac{\\beta}{4}\\right)\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\|z_{i}^{t}-w_{i}^{t}\\|^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\,+\\,\\tau\\left(1-\\frac{p_{P}}{2}\\right)\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|w_{i}^{t}-x^{t}\\right\\|^{2}\\right]}\\\\ &{\\,+\\,\\left(\\frac{6\\gamma\\omega_{D}\\left(\\omega_{P}+1\\right)\\beta^{2}L_{\\operatorname*{max}}^{2}}{n p_{D}}+128\\gamma L_{B}^{2}\\left(\\frac{1}{p_{P}}+\\frac{p_{P}}{\\beta^{2}}\\right)\\theta+\\frac{32\\gamma L_{B}^{2}}{\\beta}\\left(\\frac{1}{\\beta}+\\theta\\right)\\right.}\\\\ &{\\,\\,\\,\\quad\\left.\\,+\\,4\\left(\\frac{8\\gamma L_{A}^{2}}{\\beta}+\\frac{24\\gamma\\omega_{D}\\beta L_{\\operatorname*{max}}^{2}}{n p_{D}}\\right)\\left(\\frac{1}{\\beta}+\\omega_{P}\\right)\\right.}\\\\ &{\\,\\,\\,\\quad\\,+\\,128\\gamma\\omega_{P}L_{A}^{2}\\left(\\frac{1}{p_{P}}+\\frac{p_{P}}{\\beta^{2}}\\right)+\\frac{384\\gamma\\omega_{D}\\omega_{P}L_{\\operatorname*{max}}^{2}}{n p_{D}}\\left(\\beta+p_{P}\\right)\\right)\\mathbb{E}\\left[\\left\\|x^{t+1}-x^{t}\\right\\|^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where the last bracket can be bounded as ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{m}_{\\mathcal{C},\\alpha}^{\\mathrm{avary}}(\\pm\\eta,1)\\mathfrak{L}_{2,\\alpha}^{2}+12\\mathfrak{p}_{\\mathcal{C},\\alpha}^{\\mathrm{L}}]\\lambda_{\\alpha}^{2}\\left(\\frac{1}{p_{F_{2}}}+\\frac{p_{F_{2}}^{2}}{p_{F_{2}}^{2}}\\right)\\theta+\\frac{32\\sqrt{\\lambda}\\alpha}{\\beta}\\left(\\frac{1}{p_{F_{2}}}+\\theta\\right)}\\\\ &{\\quad+4\\left(\\frac{57\\sqrt{\\lambda}}{\\beta}+\\frac{24\\sqrt{\\kappa}}{\\alpha p_{F_{2}}}\\right)\\overline{{L_{2,\\alpha}^{2}}}\\left(\\frac{1}{p_{F_{2}}}+\\epsilon\\right)+12\\mathfrak{p}_{\\mathcal{C},\\alpha}^{\\mathrm{L}}\\overline{{L_{2,\\alpha}^{2}}}\\left(\\frac{1}{p_{F_{2}}}+\\frac{p_{F_{2}}^{2}}{p_{F_{2}}^{2}}\\right)}\\\\ &{\\quad+\\frac{32\\sqrt{\\kappa}\\alpha\\eta p_{F_{2}}\\alpha}{\\beta p_{F_{2}}}\\left(\\delta+p_{p}\\right)}\\\\ &{=\\left(\\frac{6\\sqrt{\\lambda}\\alpha\\eta\\left(\\delta^{2}+1\\right)\\beta^{2}}{\\alpha p_{F_{2}}}+\\frac{96\\sqrt{\\kappa}\\alpha\\beta}{\\eta p_{F_{2}}}\\left(\\frac{1}{p_{F_{2}}}+\\epsilon\\right)+\\frac{32\\sqrt{\\kappa}\\alpha\\beta\\alpha\\beta}{\\eta p_{F_{2}}}\\left(\\delta+p_{p}\\right)\\right)L_{\\alpha\\alpha}^{2}}\\\\ &{\\quad+\\left(\\frac{32\\sqrt{\\lambda}}{\\beta^{2}}\\left(\\frac{1}{p_{F_{2}}}+\\epsilon\\right)+12\\mathfrak{p}_{\\mathcal{C},\\alpha}\\right)\\overline{{L_{2,\\alpha}^{2}}}\\left(\\frac{9\\sqrt{\\lambda}}{p_{F_{2}}}+\\frac{p_{F_{2}}^{2}}{p_{F_{2}}^{2}}\\right)\\underline{{L_{2,\\alpha}^{2}}}}\\\\ &{\\quad+\\left(12\\mathfrak{L}_{\\alpha}^{2}\\left(\\frac{1}{p_{F_{2}}}+\\frac{p_{F_{2}}^{2}}{p_{F_{2}}^ \n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "We next consider the coefficients of $L_{B}^{2},L_{A}^{2}$ and $L_{\\mathrm{max}}^{2}$ . First, for $L_{B}^{2}$ , we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\frac{128\\gamma\\theta}{p_{P}}+\\frac{128\\gamma p_{P}\\theta}{\\beta^{2}}+\\frac{32\\gamma}{\\beta^{2}}+\\frac{32\\gamma\\theta}{\\beta}\\le128\\gamma\\left(\\frac{\\theta}{p_{P}}\\left(1+\\frac{p_{P}}{\\beta}+\\frac{p_{P}^{2}}{\\beta^{2}}\\right)+\\frac{1}{\\beta^{2}}\\right)}&{}&\\\\ {\\frac{\\d}{\\le256\\gamma}\\left(\\frac{\\theta}{p_{P}}+\\frac{\\theta p_{P}}{\\beta^{2}}+\\frac{1}{\\beta^{2}}\\right)}&{}&\\\\ {\\frac{\\d}{=256\\gamma}\\left(\\frac{\\theta}{p_{P}}+\\frac{1+\\theta p_{P}}{\\beta^{2}}\\right).}&{}&\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Next, the coefficient of $L_{A}^{2}$ can be bounded as ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\frac{32\\gamma}{\\beta^{2}}+\\frac{32\\gamma\\omega_{P}}{\\beta}+\\frac{128}{p_{P}}+\\frac{128\\gamma\\omega_{P}p_{P}}{\\beta^{2}}\\leq128\\gamma\\left(\\frac{1}{\\beta^{2}}+\\frac{\\omega_{P}}{p_{P}}\\left(1+\\frac{p_{P}}{\\beta}+\\frac{p_{P}^{2}}{\\beta^{2}}\\right)\\right)}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq256\\gamma\\left(\\frac{\\omega_{P}}{p_{P}}+\\frac{\\omega_{P}p_{P}}{\\beta^{2}}+\\frac{1}{\\beta^{2}}\\right)}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq256\\gamma\\left(\\frac{\\omega_{P}}{p_{P}}+\\frac{1+\\omega_{P}p_{P}}{\\beta^{2}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "and for $L_{\\mathrm{max}}^{2}$ we obtain ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\frac{6\\gamma\\omega_{D}(\\omega_{P}+1)\\beta^{2}}{n p_{D}}+\\frac{96\\gamma\\omega_{D}}{n p_{D}}+\\frac{96\\gamma\\omega_{D}\\omega_{P}\\beta}{n p_{D}}+\\frac{384\\gamma\\omega_{D}\\omega_{P}\\beta}{n p_{D}}+\\frac{384\\gamma\\omega_{D}\\omega_{P}p_{P}}{n p_{D}}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\leq384\\gamma\\omega_{D}\\left(\\frac{(\\omega_{P}+1)\\beta^{2}}{n p_{D}}+\\frac{1}{n p_{D}}+\\frac{\\omega_{P}\\beta}{n p_{D}}+\\frac{\\omega_{P}p_{P}}{n p_{D}}\\right)}\\\\ {\\displaystyle\\leq768\\gamma\\omega_{D}\\left(\\frac{1}{n p_{D}}+\\frac{\\omega_{P}\\beta}{n p_{D}}+\\frac{\\omega_{P}p_{P}}{n p_{D}}\\right)}\\\\ {\\displaystyle=768\\gamma\\left(\\frac{\\omega_{D}\\omega_{P}\\beta}{n p_{D}}+\\frac{\\omega_{D}\\left(1+\\omega_{P}p_{P}\\right)}{n p_{D}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "since (wp+1)32 $\\begin{array}{r}{\\frac{(\\omega_{P}+1)\\beta^{2}}{n p_{D}}\\leq\\frac{1}{n p_{D}}+\\frac{\\omega_{P}\\beta}{n p_{D}}}\\end{array}$ Substituting these inequalities to (34) and (35), we get ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\rho^{k+1}\\right]+\\kappa^{2}\\mathbb{E}\\left[\\left\\|\\rho^{k+1}-\\frac{1}{n}\\frac{\\sqrt{3}}{n}\\nabla f\\left(x_{t}^{(k+1)}\\right)\\right\\|^{2}+\\eta^{k}\\left[\\left\\|x^{+1}-u^{+1}\\right\\|^{2}\\right]\\right]}\\\\ &{\\quad+\\nu\\sum_{i=1}^{n}\\mathbb{E}\\left[\\frac{1}{n}\\sum_{j=1}^{k}\\|x_{j}^{+1}-u^{+1}\\|^{2}\\right]+\\beta^{k}\\left[\\left\\|u^{+1}-x^{+1}\\right\\|^{2}\\right]+\\mathbb{E}\\left[\\frac{1}{n}\\sum_{j=1}^{k}\\|u_{j}^{+1}-x^{+1}\\|^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\rho^{k}\\right]-\\frac{2\\nu}{2}\\mathbb{E}\\left[\\|\\nabla f(x^{(j)})\\right\\|^{2}-\\left(\\frac{1}{n}-\\frac{\\nu}{n}\\right)\\mathbb{E}\\left[\\left\\|x^{+1}-x^{+1}\\right\\|^{2}\\right]}\\\\ &{\\quad+\\kappa\\left(1-\\frac{\\nu}{n}\\right)\\mathbb{E}\\left[\\bigg\\|u^{+}-\\frac{1}{n}\\nabla f(x_{t}^{(k+1)})\\right\\|^{2}+\\eta^{k}\\left(1-\\frac{\\nu}{n}\\right)\\mathbb{E}\\left[\\left\\|t^{k}-u^{+1}\\right\\|^{2}\\right]}\\\\ &{\\quad+\\rho\\left(1-\\frac{\\nu}{n}\\right)\\mathbb{E}\\left[\\left\\|u^{+}-r^{2}\\right\\|^{2}+\\nu\\left(1-\\frac{\\nu}{n}\\right)\\mathbb{E}\\left[\\frac{1}{n}\\frac{\\sqrt{3}}{n}\\sum_{i}\\|t_{i}^{+}-u^{+1}\\|^{2}\\right]}\\\\ &{\\quad+\\tau\\left(1-\\frac{\\nu}{n}\\right)\\mathbb{E}\\left[\\frac{1}{n}\\sum_{j=1}^{k}\\|u_{j}^{+}-x^{+1}\\|^{2}\\right]}\\\\ &{\\quad+\\eta^{k}\\alpha\\left(\\left(\\frac{\\nu}{n p}+\\frac{1}{n}+\\phi_{j}^{k}\\right)\\right)L_{k}+\\left(\\frac{\\nu}{n p}+\\frac{1}{n}+\\omega p_{k}p_{k}\\right)L_{k}}\\\\ &{\\qquad\\quad+\\left(\\frac{\\nu\\sigma\\rho \n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "By collecting all the terms w.r.t. $\\mathbb{E}\\left[\\left\\Vert x^{t+1}-x^{t}\\right\\Vert^{2}\\right]$ , using the step size $\\gamma$ from the theorem and Lemma H.2, we obtain ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\Vert\\mathbf{g}^{t+1}\\right]=\\mathbb{E}\\left[\\Vert\\mathbf{\\mathcal{S}}^{t+1}\\right]+\\mathbb{E}\\mathbb{E}\\Bigg[\\Bigg\\Vert\\phi^{t+1}-\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\nabla f_{i}(\\mathbf{z}_{i}^{t+1})\\Bigg\\Vert^{2}\\Bigg]+\\eta\\mathbb{E}\\left[\\left\\Vert\\mathbf{z}^{t+1}-\\mathbf{w}^{t+1}\\right\\Vert^{2}\\right]}\\\\ &{\\qquad+\\nu\\mathbb{E}\\left[\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\left\\Vert\\mathbf{z}_{i}^{t+1}-\\mathbf{w}^{t+1}\\right\\Vert^{2}\\right]+\\rho\\mathbb{E}\\left[\\left\\Vert\\mathbf{w}^{t+1}-\\mathbf{z}^{t+1}\\right\\Vert^{2}\\right]}\\\\ &{\\qquad+\\tau\\mathbb{E}\\left[\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\left\\Vert\\mathbf{w}_{i}^{t+1}-\\mathbf{z}^{t+1}\\right\\Vert^{2}\\right]}\\\\ &{\\qquad\\leq\\mathbb{E}\\left[\\left\\Vert\\mathbf{z}^{t}-\\frac{2}{n}\\mathbb{E}\\left[\\left\\Vert\\mathbf{z}^{t}(\\mathbf{z}^{t})\\right\\Vert^{2}\\right]+\\kappa\\left(1-\\frac{p_{D}}{2}\\right)\\mathbb{E}\\left[\\left\\Vert\\mathbf{z}^{t}-\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\nabla f_{i}(\\mathbf{z}_{i}^{t})\\right\\Vert^{2}\\right]}\\\\ &{\\qquad+\\eta\\left(1-\\frac{\\delta}{4}\\right)\\mathbb{E}\\left[\\left\\Vert\\mathbf{z}^{t}-\\mathbf{w}^{t}\\right\\Vert^{2}\\right]+\\rho\\left(1-\\frac{p_{D}}{2}\\right)\\mathbb{E}\\left[\\left\\Vert\\mathbf{w}^{t}-\\mathbf{z}^{t}\\right\\Vert^{2}\\right]}\\\\ &{\\qquad+\\nu\\left(1-\\frac{\\delta}{4}\\right)\\mathbb{E}\\left[\\frac{1}{n+1}\\left\\Vert\\mathbf{z}_{i}^{t}-\\mathbf{w}^{t}\\right\\Vert^{2}\\right]+\\tau\\left(1-\\frac{p_{D}}{2}\\right)\\mathbb{E}\\left[\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\left\\Vert\\mathbf{w}_{i}^{t}-\\mathbf{z}^{t}\\right\\Vert^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Lastly, Assumption D.9 gives ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\Vert\\ell^{+1}\\right]}&{=\\phantom{-}\\phantom{\\sum}\\mathbb{E}\\left[\\ell^{+1}\\right]+\\kappa\\mathbb{E}\\left[\\left\\Vert\\ell^{+1}-\\frac{1}{n}\\frac{\\sqrt{n}}{\\ln{n}}\\nabla f_{\\ell}(\\ell_{n}^{+1})\\right\\Vert^{2}\\right]+\\eta\\mathbb{E}\\left[\\left\\Vert\\ell^{+1}-u^{\\ell+1}\\right\\Vert^{2}\\right]}\\\\ &{\\quad+\\nu\\mathbb{E}\\left[\\frac{1}{n}\\frac{\\sqrt{n}}{\\ln{n}}\\left\\Vert\\xi_{\\frac{n}{n}}^{+1}-u_{\\ell}^{+1}\\right\\Vert^{2}\\right]+\\beta\\mathbb{E}\\left[\\left\\Vert u^{\\ell+1}-x^{\\ell+1}\\right\\Vert^{2}\\right]}\\\\ &{\\quad+\\tau\\mathbb{E}\\left[\\frac{1}{n}\\frac{\\sqrt{n}}{\\ln{n}}\\left\\Vert u_{\\ell}^{+1}-x^{\\ell+1}\\right\\Vert^{2}\\right]}\\\\ {\\phantom{\\sum}_{k\\in\\mathbb{Z}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&{\\mathrm{in}\\rho_{\\mathbb{Z}}\\partial_{[0]}}\\\\ &{\\overset{\\mathrm{An},\\mathbb{D}_{k\\in\\mathbb{Z}}^{[0,n]}}{\\leq}(1-\\gamma\\mu)\\mathbb{E}\\left[\\ell^{+1}+\\kappa(1-\\gamma\\mu)\\mathbb{E}\\left[\\left\\Vert\\ell^{+}-\\frac{1}{n}\\frac{\\sqrt{n}}{\\ln{n}}\\nabla f_{\\ell}(\\ell_{n}^{+1})\\right\\Vert^{2}\\right]}\\\\ &{\\quad+\\eta(1-\\gamma\\mu)\\mathbb{E}\\left[\\left\\Vert\\ell^{+}-u^{\\ell}\\right\\Vert^{2}\\right]+\\nu(1-\\gamma\\mu)\\mathbb{E}\\left[\\frac{1}{n}\\frac{\\sqrt{n}}{\\ln{n}}\\left\\Vert\\xi_{\\frac{n}{n}}^{-}-u_{\\ell}^{\\ell}\\right\\Vert^{2}\\right]}\\\\ &{\\quad+\\rho(1-\\gamma\\mu)\\mathbb{E}\\left[\\left\\Vert u^{\\ell}-x^{\\ell}\\right\\Vert^{2}\\right]+\\tau(1-\\gamma\\mu)\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\left\\Vert u_{\\ell}^{+}-u^{\\ell}\\right\\Vert^{2}\\right]}\\\\ {\\phantom{\\sum}_{k\\in\\mathbb{Z}}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "It remains to apply the last inequality iteratively to finish the proof. ", "page_idx": 48}, {"type": "text", "text": "Corollary E.9. Let $\\mathcal{C}_{i}^{t}\\,\\in\\,\\mathbb{P}(0)$ for all $i\\,\\in\\,[n]$ (e.g. PermK), choose $p_{P}\\,=\\,1/(\\omega_{P}+1)$ \uff0c $p_{D}=$ $1/(\\omega_{D}+1)$ and ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\beta=\\operatorname*{min}\\left\\{\\left(\\frac{n}{\\omega_{D}\\omega_{P}(\\omega_{D}+1)}\\right)^{1/3},1\\right\\}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Then, in the view of Theorem E.8, Algorithm 2 ensures that $\\mathbb{E}\\left[f(x^{T})-f^{*}\\right]\\leq\\varepsilon$ after ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underbrace{^{\\prime}\\left(1+\\left(\\frac{\\omega_{D}\\omega_{P}(\\omega_{D}+1)}{n}\\right)^{1/3}+\\sqrt{\\frac{\\omega_{D}(\\omega_{D}+1)}{n}}\\right)L_{\\operatorname*{max}}+\\sqrt{\\omega_{P}(\\omega_{P}+1)}L_{A}}_{\\mu},\\omega_{P}+1,\\omega_{D}+1,\\left(\\frac{\\omega_{D}\\omega_{P}(\\omega_{D}+1)}{n}\\right)^{1/3}\\right\\}\\log(\\left(1+\\sqrt{\\frac{\\omega_{D}\\omega_{P}(\\omega_{D}+1)}{n}}\\right)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "iterations. ", "page_idx": 48}, {"type": "text", "text": "Proof. Note that $\\Psi^{T}\\,\\geq\\,f(x^{T})-f^{*}$ . In view of condition (33) from Theorem E.8, the step size satisfies ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\mathbf{\\Sigma}_{i}=\\Theta\\Bigg(\\operatorname*{min}\\Bigg\\{\\left(L+\\sqrt{\\left(\\frac{\\theta}{p_{P}}+\\frac{1+\\theta p_{P}}{\\beta^{2}}\\right)L_{B}^{2}+\\left(\\frac{\\omega_{P}}{p_{P}}+\\frac{1+\\omega_{P}p_{P}}{\\beta^{2}}\\right)L_{A}^{2}+\\left(\\frac{\\omega_{D}\\omega_{P}\\beta}{n p_{D}}+\\frac{\\omega_{D}(1+\\omega_{P}p_{P})}{n p_{D}}\\right)\\Bigg\\}L_{B}^{2}}+\\frac{\\omega_{P}-\\omega_{P}-2\\omega_{P}}{n p_{P}}\\Bigg)\\Bigg\\}\\;\\;\\mathrm{~a~n~d~}\\;\\;\\;\\mathrm{~f~o~r~}\\;\\;\\mathrm{~W~i~t~}\\;\\mathrm{~W~i~t~}\\;\\mathrm{~W~i~t~}\\;\\mathrm{~a~n~d~}\\;\\;\\mathrm{~f~o~r~}\\;\\mathrm{~W~i~t~}\\;\\mathrm{~W~i~t~}\\;\\mathrm{~W~i~t~}\\;\\mathrm{~W~i~t~}\\;\\mathrm{~W~i~t~}\\;\\mathrm{~W~i~t~}\\;\\mathrm{~W~i~t~}\\;\\mathrm{~W~i~t~},\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Therefore, since $\\theta=0$ , the algorithm converges after ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{I}=\\mathcal{O}\\left(\\operatorname*{max}\\left\\{\\frac{L+\\sqrt{\\frac{1}{\\beta^{2}}L_{B}^{2}+\\left(\\frac{\\omega_{P}}{p_{P}}+\\frac{1+\\omega_{P}p_{P}}{\\beta^{2}}\\right)L_{A}^{2}+\\left(\\frac{\\omega_{D}\\omega_{P}\\beta}{n p_{D}}+\\frac{\\omega_{D}(1+\\omega_{P}p_{P})}{n p_{D}}\\right)L_{\\operatorname*{max}}^{2}}{\\mu},\\frac{1}{p_{P}},\\frac{1}{p_{D}},\\frac{1}{\\beta}\\right\\}\\log\\frac{\\Psi^{0}}{\\varepsilon}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "iterations. Using the choice of $p_{P}$ and $p_{D}$ , we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\Gamma}=\\mathcal{O}\\left(\\operatorname*{max}\\left\\{\\frac{L+\\sqrt{\\frac{1}{\\beta^{2}}(L_{B}^{2}+L_{A}^{2})+\\omega_{P}(\\omega_{P}+1)L_{A}^{2}}+\\left(\\frac{\\omega_{D}(\\omega_{D}+1)\\omega_{P}\\beta}{n}+\\frac{\\omega_{D}(\\omega_{D}+1)}{n}\\right)L_{\\operatorname*{max}}^{2}}{\\mu},\\omega_{P}+1,\\omega_{D}+1,\\frac{1}{\\beta}\\right\\}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Due to Lemma C.1, we get ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\Gamma}=\\mathcal{O}\\left(\\operatorname*{max}\\left\\{\\frac{L+\\sqrt{\\omega_{P}(\\omega_{P}+1)L_{A}^{2}}+\\left(\\frac{1}{\\beta^{2}}+\\frac{\\omega_{D}(\\omega_{D}+1)\\omega_{P}\\beta}{n}+\\frac{\\omega_{D}(\\omega_{D}+1)}{n}\\right)L_{\\operatorname*{max}}^{2}}{\\mu},\\omega_{P}+1,\\omega_{D}+1,\\frac{1}{\\beta}\\right\\}\\log\\frac{\\Psi^{0}}{\\varepsilon}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Using the choice of $\\beta$ , we obtain the result of the theorem. ", "page_idx": 49}, {"type": "text", "text": "Corollary E.10. Let $\\mathcal{C}_{i}^{t}$ be the PermK compressors and $\\mathcal{Q}_{i}^{t}$ be the independent (Assumption 1.6) RandK compressors, both with $K=d/_{n}$ . Then, in the view of Corollary $E.\\mathcal{H}_{s}$ the total communication complexity is ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\left(\\frac{d L_{\\mathrm{max}}}{n^{1/3}\\mu}+\\frac{d L_{A}}{\\mu}+d\\right)\\log\\frac{\\Psi^{0}}{\\varepsilon}\\right).\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Proof. The choice of compressors and parameters ensures that $\\omega_{P}=\\omega_{D}=n-1$ (Lemma A.6). Thus, the iteration complexity is ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{O}\\left(\\operatorname*{max}\\left\\{\\frac{\\left(1+n^{2/3}+n^{1/2}\\right)\\,L_{\\operatorname*{max}}+n L_{A}}{\\mu},n,n,n^{2/3}\\right\\}\\log\\frac{\\Psi^{0}}{\\varepsilon}\\right)}\\\\ &{=\\mathcal{O}\\left(\\operatorname*{max}\\left\\{\\frac{n^{2/3}L_{\\operatorname*{max}}+n L_{A}}{\\mu},n\\right\\}\\log\\frac{\\Psi^{0}}{\\varepsilon}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Since $p_{P}=p_{D}=1/n$ and $K=\\left.d\\right/_{n}$ , on average, the algorithm sends $\\leq\\,^{2d}\\!/n$ coordinates in both directions. Therefore, the total communication complexity is ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\frac{d}{n}\\times\\operatorname*{max}\\left\\{\\frac{n^{2/3}L_{\\operatorname*{max}}+n L_{A}}{\\mu},n\\right\\}\\log\\frac{\\Psi^{0}}{\\varepsilon}\\right)=\\mathcal{O}\\left(\\operatorname*{max}\\left\\{\\frac{\\frac{d}{n^{1/3}}L_{\\operatorname*{max}}+d L_{A}}{\\mu},d\\right\\}\\log\\frac{\\Psi^{0}}{\\varepsilon}\\right).\n$$", "text_format": "latex", "page_idx": 49}, {"type": "image", "img_path": "gkJ5nBIOU4/tmp/55c9f31cf5fa9246e7fcb62b57eea9d697c8345b081747378861f3b75d4bb1ce.jpg", "img_caption": ["Figure 2: Experiments on the quadratic optimization problem from Section F.1. We plot the norm of the gradient w.r.t. # of coordinates sent from the server (s-to-w) and from the workers (w-to-s). "], "img_footnote": [], "page_idx": 50}, {"type": "image", "img_path": "gkJ5nBIOU4/tmp/ba9b6323377693afe49b58fd0dac3134fed472a3bc898405d3d0b2fa3fa23fda.jpg", "img_caption": ["Figure 3: Experiments on the autoencoder task from Section F.2. We plot the norm of the gradient w.r.t. # of coordinates sent from the server (s-to-w) and from the workers (w-to-s). "], "img_footnote": [], "page_idx": 50}, {"type": "text", "text": "F Experiments ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "The experiments were prepared in Python. The distributed environment was emulated on a machine with Intel(R)Xeon(R) Gold 6226R CPU $\\textcircled{a}\\ 2.90\\mathrm{GHz}$ and64cores. ", "page_idx": 50}, {"type": "text", "text": "F.1  Experiments with M3 on quadratic optimization tasks ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "We consider close-to-homogeneous quadratic optimization problem with $\\mathbf{A}_{i}=(1+\\xi_{i})\\mathbf{I}_{d}$ , where $\\xi_{i}\\,\\sim\\mathcal{N}(0,0.01)$ for all $i\\;\\in\\;[n]$ , and $d\\,=\\,1000$ . We run two algorithms from Table 1, M3 and CORE, and check whether theory matches practice. In M3, we use $\\mathrm{Perm}K$ followed by the natural compressor $\\mathcal{C}_{\\mathrm{nat}}$ (Horvath et al., 2022) (composition of two unbiased compressors) on the server's side, and Rand $K$ followed by $\\mathcal{C}_{\\mathrm{nat}}$ on the workers\u2019 side. We use $K=\\lfloor\\bar{d}/n\\rfloor\\,\\in\\,\\{1,10,100\\}$ for $n\\in\\{1000,100,10\\}$ . In CORE, the number of communicated coordinates is set to 10. We run each experiment 5 times with different seeds and plot the average to reduce the noise factor. Only the step size is fine-tuned for each algorithm ", "page_idx": 50}, {"type": "text", "text": "The results are presented in Figure 2. As expected, CORE does not change its behavior as the number of workers increases from 10 to 100; this is expected since CORE does not depend on $n$ .Atthesame time, M3 does improve with $n$ , which supports our findings from Theorem 5.1. ", "page_idx": 50}, {"type": "text", "text": "F.2  Experiments with an autoencoder and MNIST ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "We now compare MARINA-P, M3, CORE,EF21-P $^+$ DCGD, and GD on a non-convex autoencoder problem. We train it on the MNIST dataset (LeCun et al., 2010) with objective function $f(\\mathbf{D},\\mathbf{E}):=$ $\\begin{array}{r}{\\frac{1}{m}\\sum_{i=1}^{m}\\left\\|\\mathbf{D}\\mathbf{E}b_{i}-b_{i}\\right\\|^{2}+\\frac{\\lambda}{2}\\left\\|\\mathbf{D}\\mathbf{E}-\\mathbf{I}\\right\\|_{F}^{2}}\\end{array}$ where $\\mathbf{D}\\in\\mathbb{R}^{d_{1}\\times d_{2}}$ $\\mathbf{E}\\in\\mathbb{R}^{d_{2}\\times d_{1}}$ \uff0c $b_{i}\\in\\mathbb{R}^{d_{1}}$ aresamples, $d_{1}\\,=\\,784$ is the number of features, $d_{2}\\,=\\,16$ is the size of the encoding space, $\\lambda=0.001$ is a regularizer, and $m=60\\,000$ is the number of samples. The dimension of the problem is $d=25\\,088$ We randomly split the dataset among $n=100$ workers. For MARINA-P and M3, we take $\\mathrm{Perm}K$ followed by the natural compressor $\\mathcal{C}_{\\mathrm{nat}}$ on the server's side. On the workers\u2032 side, M3 uses RandK and $\\mathcal{C}_{\\mathrm{nat}}$ . For $\\mathsf{E F}21\\!-\\!\\mathsf{P}+\\mathsf{D C G D}$ , we take Rand $K$ with $\\mathcal{C}_{\\mathrm{nat}}$ on both the workers? and server's sides. In each case, $K=\\lfloor d/n\\rfloor=250$ . For CORE, we set the number of communicated coordinates to 100. As in previous experiments, we only fine-tune the step size, repeat each experiment 5 times, and plot the average results. ", "page_idx": 50}, {"type": "text", "text": "", "page_idx": 51}, {"type": "text", "text": "The results are presented in Figure 3. All methods with bidirectional compression: M3, CORE, and EF21 $\\cdot P+{\\sf D C G D}$ , converge much faster than GD. MARINA-P converges fastest only in the first plot. This is expected since it compresses only from the server to the workers. M3, CORE, and EF21- $\\cdot P+$ DCGD have similar convergence rates in both metrics, with M3 performing better in the low accuracy regime. ", "page_idx": 51}, {"type": "text", "text": "F.3  Extra experiments with quadratic optimization tasks ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "The aim of this set of experiments is to empirically test our results under Assumption 4.2. We consider the problem of quadratic minimization with varying level of heterogeneity between the $n$ functions stored on the workers. The goal is to minimize the squared norm of the gradient of $\\textstyle\\sum_{i=1}^{n}f_{i}$ ,where thefunctions $f_{i}$ areofform ", "page_idx": 51}, {"type": "equation", "text": "$$\nf_{i}(x)=\\frac{1}{2}x^{T}\\mathbf{A}_{i}x+b_{i}^{T}x.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Here, $\\mathbf{A}_{i}$ are $d\\times d$ matrices generated following the procedure in Algorithm 3, and $b_{i}$ denotes a standard normal vector in $\\bar{\\mathbb{R}}^{d}$ .The constants $L_{A}$ and $L_{B}$ from Assumption 4.2 (in this case, by Theorem $4.8~L_{A}=\\sqrt{2}\\operatorname*{max}_{i\\in[n]}\\|\\mathbf{A}_{i}-\\mathbf{A}\\|$ and $\\begin{array}{r}{L_{B}=\\sqrt{2}\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\|\\mathbf{A}_{i}\\|\\right)}\\end{array}$ are controlled by parameters $v_{i}$ and $\\sigma_{i}^{2}$ . In particular, for $\\sigma_{i}^{2}=0$ , all workers hold the same matrix $\\mathbf{A}_{i}$ , and hence in this case $L_{A}=0$ ", "page_idx": 51}, {"type": "text", "text": "We compare the following algorithms: ", "page_idx": 51}, {"type": "text", "text": "1. MARINA-P with Perm $K$ compressors,   \n2. MARINA-P with Rand $K$ compressors,   \n3. MARINA-P with SameRand $K$ compressor,   \n4. EF21-P with Top $K$ compressor,   \n5.GD. ", "page_idx": 51}, {"type": "text", "text": "In all compressed methods, we set $K=d/_{n}$ and use $p=k/d$ in MARINA-P. ", "page_idx": 51}, {"type": "text", "text": "The step sizes are tuned from $2^{i},i\\in\\mathbb{Z}$ multiples of the values predicted by the theory (indicated by $\\times1,\\times2,\\ldots$ in the plots). We fix $d=300$ and generate optimization tasks with $n\\in\\dot{\\{}10,}100,900\\dot{\\}$ The results are presented in Figures 4, 5, 6. ", "page_idx": 51}, {"type": "text", "text": "The empirical results align well with the theory. Among the algorithms tested, MARINA-P with $\\mathrm{Perm}K$ compressor exhibits the best performance, while MARINA-P with SameRand $K$ converges the slowest and comparable to GD. MARINA-P with RandK compressor and EF21-P achieve performance levels somewhere in between. Notably, the differences between the runs of MARINA-P with different compressors become more pronounced as the value of $n$ increases. As anticipated, the performance of MARINA-P with Rand $K$ and PermK compressors improves with an increase in the number of workers, while the performance of EF21-P does not follow the same behaviour. Specifically, for $n=10$ , EF21-P outperforms MARINA-P with Rand $K$ compressor, but this pattern reverses for both $n=100$ and $n=1000$ ", "page_idx": 51}, {"type": "text", "text": "Algorithm 3 Heterogeneous quadratic problem generation ", "text_level": 1, "page_idx": 52}, {"type": "image", "img_path": "gkJ5nBIOU4/tmp/320498766166561d775154a94b9daafbdc9f3bbb59f1edac17209c5e6cdc9d36.jpg", "img_caption": [], "img_footnote": [], "page_idx": 52}, {"type": "text", "text": "3: for $k=0,\\dotsc,4$ do   \n4: Generate $\\xi_{i}\\sim\\mathcal{N}(0,\\sigma_{k}^{2})\\cap[-v_{0},v_{0}]$ for $i\\in[n]$   \n5: for $l=0,\\dots,4$ do   \n6: Set $\\mathbf{A}_{i}^{k,l}=(v_{l}+\\xi_{i})\\mathbf{X}$ for $i\\in[n]$   \n7: Sample $b_{i}^{k,l}\\sim\\mathcal{N}(\\boldsymbol{0},\\mathbb{I}_{d})$ for $i\\in[n]$   \n8: end for   \n9: Qutput:matrices ${\\bf A}_{i}^{k,l}$ , vectors $b_{i}^{k,l}$ $,i\\in[n],k,l\\in[4]$   \n[O:end for ", "page_idx": 52}, {"type": "image", "img_path": "gkJ5nBIOU4/tmp/42575c843e9ea5b58d48cc165e0b93a7563469fd9672acdce97f94b0ff8b25ed.jpg", "img_caption": [], "img_footnote": [], "page_idx": 52}, {"type": "text", "text": "Figure 4: Experiments on the quadratic optimization problem from Section F.3 with $n=10$ for $L_{A}^{\\tilde{2}}\\in\\{0,1,\\dot{10},100\\}$ and $L_{B}^{2}\\in\\mathrm{\\bar{\\{100,1000,10000,100000\\}}}$ ", "page_idx": 52}, {"type": "image", "img_path": "gkJ5nBIOU4/tmp/0a8b9661e4b48aec7812584e27ecc47f3eb4196783e05a1a621ad471bc831a76.jpg", "img_caption": ["Figure 5: Experiments on the quadratic optimization problem from Section F.3 with $n=100$ for $L_{A}^{\\bar{2}}\\in\\{0,1,\\bar{10},100\\}$ and $L_{B}^{2}\\in\\bar{\\{100,1000,10000,100000\\}}$ "], "img_footnote": [], "page_idx": 53}, {"type": "image", "img_path": "gkJ5nBIOU4/tmp/9f08a6c3aa423ff825cacb389e13e86c9e3bc0f7dd81d325096f6a410ffd9e77.jpg", "img_caption": ["Figure 6: Experiments on the quadratic optimization problem from Section F.3 with $n=900$ for $L_{A}^{\\bar{2}}\\in\\{0,1,\\bar{10},100\\}$ and $L_{B}^{2}\\in\\bar{\\{100,1000,10000,100000\\}}$ "], "img_footnote": [], "page_idx": 53}, {"type": "text", "text": "G Proof of the Lower Bounds ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "G.1  The \u201cdifficult function from the nonconvex world ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "In our lower bound, we use the function from Carmon et al. (2020); Arjevani et al. (2022). For any $T\\in\\mathbb N$ ,let ", "page_idx": 54}, {"type": "equation", "text": "$$\nF_{T}(x):=-\\Psi(1)\\Phi([x]_{1})+\\sum_{i=2}^{T}\\left(\\Psi(-[x]_{i-1})\\Phi(-[x]_{i})-\\Psi([x]_{i-1})\\Phi([x]_{i})\\right),\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\Psi(x)=\\left\\{\\!\\!\\!\\begin{array}{l l}{0,}&{x\\leq1/2,}\\\\ {\\exp\\left(1-\\frac{1}{(2x-1)^{2}}\\right),}&{x\\geq1/2,}\\end{array}\\right.\\,\\,\\mathrm{and}\\quad\\Phi(x)=\\sqrt{e}\\int_{-\\infty}^{x}e^{-\\frac{1}{2}t^{2}}d t.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Carmon et al. (2020); Arjevani et al. (2022) also proved the following properties of the function: ", "page_idx": 54}, {"type": "text", "text": "Lemma G.1 (Carmon et al. (2020); Arjevani et al. (2022)). The function $F_{T}$ satisfies: ", "page_idx": 54}, {"type": "text", "text": "1. $\\begin{array}{r}{F_{T}(0)-\\operatorname*{inf}_{x\\in\\mathbb{R}^{T}}F_{T}(x)\\leq\\Delta^{0}T,w h e r e\\,\\Delta^{0}=12.}\\end{array}$   \n2.Thefunction $F_{T}$ is $l_{1}$ smooth,where $l_{1}=152$   \n3. For all $x\\in\\mathbb{R}^{T}$ \uff0c $\\|\\nabla F_{T}(x)\\|_{\\infty}\\leq\\gamma_{\\infty}$ , where $\\gamma_{\\infty}=23$   \n4. For all $x\\in\\mathbb{R}^{T}$ \uff0c $\\mathrm{prog}(\\nabla F_{T}(x))\\leq\\mathrm{prog}(x)+1.$   \n5. For all $x\\in\\mathbb{R}^{T}$ \uff0c $i f\\mathrm{prog}(x)<T$ then $\\|\\nabla F_{T}(x)\\|>1$ ", "page_idx": 54}, {"type": "text", "text": "where $\\mathrm{prog}(x):=\\operatorname*{max}\\{i\\geq0\\,|\\,x_{i}\\neq0\\}\\quad(x_{0}\\equiv1).$ ", "page_idx": 54}, {"type": "text", "text": "The function is a standard function that is used to establish lower bounds in the nonconvex world (Carmon et al., 2020; Arjevani et al., 2022; Lu and De Sa, 2021; Tyurin and Richtarik, 2023c). ", "page_idx": 54}, {"type": "text", "text": "G.2Theorems ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Our lower bound applies to the family of methods with the following structure: ", "page_idx": 54}, {"type": "text", "text": "Protocol 4 Protocol   \n1: Input: functions $f_{1},\\ldots,f_{n}\\in{\\mathcal{F}}$ , algorithm $A$ , probability $p$   \n2: for $k=0,\\ldots,\\infty$ do   \n3:  Server calculates a new point: $x^{k}=B_{1}^{k}(g_{1}^{1},\\ldots,g_{1}^{k},\\ldots,g_{n}^{1},\\ldots,g_{n}^{k})$   \n4:  Server aggregates all available information: $s_{i}^{k}=B_{2,i}^{k}(g_{1}^{1},\\ldots,g_{1}^{k},\\ldots,g_{n}^{1},\\ldots,g_{n}^{k})$   \n5:  Server sends sparsified vectors $\\bar{s}_{i}^{k}$ to the workers, where $[\\bar{s}_{i}^{k}]_{j}=[s_{i}^{k}]_{j}\\times\\eta_{i,j}^{k},$ and $\\eta_{i,j}^{k}$ is a random variable such that $\\mathbb{P}\\left(\\eta_{i,j}^{k}\\neq0\\right)\\leq p$ for all $j\\in[d^{\\prime}]$ and for all $i\\in[n]$ We define $d^{\\prime}:=\\dim(\\operatorname{dom}(f_{1}))$ , and $[\\cdot]_{j}$ means the $j^{\\mathrm{th}}$ coordinate. 6: Workers aggregate all available local information and calculate gradients: $\\begin{array}{r l}{g_{i}^{k+1}}&{{}=}\\end{array}$ $L_{i}^{k}(\\bar{s}_{i}^{0},\\ldots,\\bar{s}_{i}^{k})$ $L_{i}^{k}$ has access to the gradient oracle of $f_{i}$ and can callit as many times as it wants according to the rules (37) and (38))   \n7: Workers send g k+1 to the server   \n8: end for ", "page_idx": 54}, {"type": "text", "text": "We consider the following standard classes of functions and algorithms: ", "page_idx": 54}, {"type": "text", "text": "Definition G.2. Let the function $f\\quad:\\quad\\mathbb{R}^{d}\\quad\\rightarrow\\quad\\mathbb{R}$ be  differentiable, $L$ smooth (i.e., $\\|\\nabla f(x)-\\nabla f(y)\\|\\,\\leq\\,L\\,\\|x-y\\|$ for all $x,y\\,\\in\\,\\mathbb{R}^{d},$ ), and $f(0)\\mathrm{~-~}\\mathrm{inf~}_{x\\in\\mathbb{R}^{d}}\\;f(x)\\,\\leq\\,\\delta^{0}$ . We denote the familyof functions that satisfy these propertieby $\\mathcal{F}_{\\delta^{0},L}$ ", "page_idx": 54}, {"type": "text", "text": "Definition G.3. Consider Protocol 4.  A sequence of tuples of mappings ${\\cal A}\\quad=\\quad$ $\\{(B_{1}^{k},B_{2,1}^{k},\\ldots,B_{2,n}^{k},L_{1}^{k},\\ldots,L_{n}^{k})\\}_{k=0}^{\\infty}$ is azero-respecting algrithm,if, ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname{supp}\\left(x^{k}\\right)\\subseteq\\bigcup_{j=1}^{k}\\bigcup_{i=1}^{n}\\operatorname{supp}\\left(g_{i}^{j}\\right),\\operatorname{supp}\\left(s_{i}^{k}\\right)\\subseteq\\bigcup_{j=1}^{k}\\bigcup_{i=1}^{n}\\operatorname{supp}\\left(g_{i}^{j}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{supp}\\big(\\hat{g}_{i,1}^{k+1}\\big)\\subseteq\\bigcup_{j=0}^{k}\\operatorname*{supp}\\big(\\bar{s}_{i}^{j}\\big)\\,,}\\\\ &{\\operatorname*{supp}\\big(\\hat{g}_{i,2}^{k+1}\\big)\\subseteq\\bigcup_{j=0}^{k}\\operatorname*{supp}\\big(\\bar{s}_{i}^{j}\\big)\\bigcup\\operatorname*{supp}(\\nabla f_{i}(\\hat{g}_{i,1}^{k+1})),}\\\\ &{\\operatorname*{supp}\\big(\\hat{g}_{i,3}^{k+1}\\big)\\subseteq\\bigcup_{j=0}^{k}\\operatorname*{supp}\\big(\\bar{s}_{i}^{j}\\big)\\bigcup\\operatorname*{supp}(\\nabla f_{i}(\\hat{g}_{i,1}^{k+1}))\\bigcup\\operatorname*{supp}(\\nabla f_{i}(\\hat{g}_{i,2}^{k+1})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "we have ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\operatorname{supp}\\big(g_{i}^{k+1}\\big)\\subseteq\\bigcup_{j=1}^{\\infty}\\operatorname{supp}\\big(\\hat{g}_{i,j}^{k+1}\\big)\\,,\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "for all $k\\in\\ensuremath{\\mathbb{N}}_{0}$ and for all $i\\in[n]$ ,where $\\operatorname{supp}(x):=\\{i\\in[d]\\,|\\,x_{i}\\neq0\\}$ ", "page_idx": 55}, {"type": "text", "text": "We denote the set of all algorithms that satisfy these properties by $A_{\\mathrm{zr}}$ ", "page_idx": 55}, {"type": "text", "text": "The first three properties define the domains of the mapping. The last property is a standard assumption for a zero-respecting algorithm. Assumption (38) allows the mappings $L_{i}^{k}$ to calculate gradients. ", "page_idx": 55}, {"type": "text", "text": "Theorem G.4. Consider Protocol 4. Assume that the sets {n2,\u00a7jie[n],je[d], $\\{\\eta_{i,j}^{1}\\}_{i\\in[n],j\\in[d^{\\prime}]},\\ldots,\\{\\eta_{i,j}^{k}\\}_{i\\in[n],j\\in[d^{\\prime}]},\\ldots.$ are mutually independent (the variables within one set can be dependent). Let $p>0,L,\\delta^{0},\\varepsilon>0,n\\geq2$ be any numbers such that $\\bar{c}\\varepsilon<L\\delta^{0}$ . Then, for any algorithm $A\\in{\\mathcal{A}}_{\\mathrm{zr}}$ , there exists a function $f\\in\\mathcal{F}_{\\delta^{0},L}$ and functions $f_{1},\\ldots,f_{n}$ such that $\\textstyle f={\\frac{1}{n}}\\sum_{i=1}^{n}f_{i}$ and I $\\Xi\\left[\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right]>\\varepsilon$ for all ", "page_idx": 55}, {"type": "equation", "text": "$$\nk\\le\\hat{c}\\frac{L\\delta^{0}}{p\\varepsilon}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "The quantities c and $\\hat{c}$ are universal constants. ", "page_idx": 55}, {"type": "text", "text": "Proof. The proof is conceptually the same as in Arjevani et al. (2022); Lu and De Sa (2021); Huang et al. (2022); Fang et al. (2018); Carmon et al. (2020); Tyurin and Richtarik (2023c). We fix $\\lambda>0$ and consider the following function $f:\\mathbb{R}^{T}\\rightarrow\\mathbb{R}$ ", "page_idx": 55}, {"type": "equation", "text": "$$\nf(x):=\\frac{L\\lambda^{2}}{l_{1}}F_{T}\\left(\\frac{x}{\\lambda}\\right).\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "One can show (Arjevani et al., 2022)[Theorem 1] that $f\\in\\mathcal{F}_{\\delta^{0},L}$ if ", "page_idx": 55}, {"type": "equation", "text": "$$\nT=\\left\\lfloor\\frac{\\delta^{0}l_{1}}{L\\lambda^{2}\\Delta^{0}}\\right\\rfloor.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Next, we define ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\tau}_{i}(x):=\\left\\{-\\Psi(1)\\Phi([x]_{1})+\\sum_{2\\leq j\\leq T\\mathrm{~and~}(j-1)\\mathrm{~mod~}n=0}\\left(\\Psi(-[x]_{j-1})\\Phi(-[x]_{j})-\\Psi([x]_{j-1})\\Phi([x]_{j})\\right)\\right.}\\\\ {\\left.\\sum_{2\\leq j\\leq T\\mathrm{~and~}(j-1)\\mathrm{~mod~}n=i-1}^{T}\\left(\\Psi(-[x]_{j-1})\\Phi(-[x]_{j})-\\Psi([x]_{j-1})\\Phi([x]_{j})\\right),}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "and ", "page_idx": 56}, {"type": "equation", "text": "$$\nf_{i}(x):=\\frac{n L\\lambda^{2}}{l_{1}}F_{i}\\left(\\frac{x}{\\lambda}\\right).\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "The idea is that we take the first block from (36) to the first worker, the second block to the second worker,.. $\\cdot\\cdot,(n+1)^{\\mathrm{th}}$ block to the first worker, and so on. Then, one can show that ", "page_idx": 56}, {"type": "equation", "text": "$$\n{\\frac{1}{n}}\\sum_{i=1}^{n}f_{i}(x)=f(x).\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Using Lemma G.1, we obtain ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\|\\nabla f(x)\\|^{2}=\\frac{L^{2}\\lambda^{2}}{l_{1}^{2}}\\left\\|\\nabla F_{T}\\left(\\frac{x}{\\lambda}\\right)\\right\\|^{2}>\\frac{L^{2}\\lambda^{2}}{l_{1}^{2}}\\mathbb{1}[\\mathrm{prog}(x)<T].\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "The functions $f_{i}$ are zero-chain (Arjevani et al., 2022): for all $i\\in[n]$ ,if $\\mathrm{prog}(x)=j$ and $(j\\bmod n)+$ $1\\,=\\,i$ , then $\\mathrm{prog}(\\nabla f_{i}(x))\\,\\le\\,j+1$ , and for all $i\\,\\in\\,[n]$ ,if $\\mathrm{prog}(x)=j$ and $(j{\\mathrm{~mod~}}n)+1\\neq i$ then $\\mathrm{prog}(\\nabla f_{i}(x))\\le j$ . Using the zero-chain property and the fact that we consider the family of zero-respecting algorithms: ", "page_idx": 56}, {"type": "text", "text": "1. The first non-zero coordinate can be discovered only by the first worker. ", "page_idx": 56}, {"type": "text", "text": "2. Assume that $\\begin{array}{r}{\\operatorname*{max}_{j=1}^{k}\\operatorname*{max}_{i=1}^{n}\\operatorname{prog}\\left(g_{i}^{j}\\right)\\,=\\,j\\ \\geq\\,1}\\end{array}$ An algorithm can discover one new non-zero coordinate in the $(j+1)^{\\mathrm{th}}$ position only if the $(j\\bmod n+1)^{\\mathrm t h}$ worker gets a non-zero $j^{\\mathrm{th}}$ coordinate from the server. This is by the construction of the functions $f_{i}$ Note that for $n\\geq2$ , one worker cannot discover two consecutive coordinates. ", "page_idx": 56}, {"type": "text", "text": "Let us define ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "$\\xi^{j}=\\mathbb{I}[\\mathbf{In}$ the $j^{\\mathrm{th}}$ iteration, the coodinate with index $\\bar{p}\\equiv\\operatorname*{max}_{j=1}^{k}\\operatorname*{max}_{i=1}^{n}\\mathrm{prog}\\left(g_{i}^{j}\\right)$ is not zeroed out i Line 5 of Protocol 4 to the worker with index ( ${\\bar{p}}\\ {\\bmod{\\ n}}+1)$ AND $\\begin{array}{r}{T-1\\geq\\bar{p}\\geq1]\\quad(\\bar{p}=0\\,\\mathrm{if}\\,k=0).}\\end{array}$ Then, we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\mathrm{prog}(x^{k})\\geq T\\right)\\leq\\mathbb{P}\\left(\\sum_{j=0}^{k-1}\\xi^{j}\\geq T-1\\right).\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Assume that $\\mathcal{G}_{j}$ is the $\\sigma$ -algebra generated by all randomness up to the $j^{\\mathrm{th}}$ iteration (inclusive). Then, $\\xi^{j}$ .5 $\\mathscr{G}_{j}$ -measurable, and, by the construction of Line 5 of Protocol 4, $\\mathbb{P}\\left(\\xi^{j+1}=1\\middle|\\mathcal{G}_{j}\\right)\\leq p$ where we also use the assumption of the theorem that the sets of random variables are mutually independent. Using the standard approach with the Chernoff method (Arjevani et al., 2022; Lu and De Sa, 2021; Huang et al., 2022), one can show that ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\sum_{j=0}^{k-1}\\xi^{j}\\geq T-1\\right)\\leq\\rho\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "for all ", "page_idx": 56}, {"type": "equation", "text": "$$\nk\\leq\\frac{T-1-\\log\\frac{1}{\\rho}}{2p}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "and $\\rho\\in(0,1]$ . Therefore, we get ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\mathbf{prog}(x^{k})\\geq T\\right)\\leq\\rho\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "for all ", "page_idx": 57}, {"type": "equation", "text": "$$\nk\\leq\\frac{T-1-\\log\\frac{1}{\\rho}}{2p}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Using (40), we have ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}\\right]>2\\varepsilon\\mathbb{P}\\left(\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}>2\\varepsilon\\right)\\ge2\\varepsilon\\mathbb{P}\\left(\\frac{L^{2}\\lambda^{2}}{l_{1}^{2}}\\mathbb{1}[{\\mathrm{prog}}(x)<T]\\ge2\\varepsilon\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Let us take $\\begin{array}{r}{\\lambda=\\frac{\\sqrt{2\\varepsilon}l_{1}}{L}}\\end{array}$ \u221a2el1. Then ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}\\right]>2\\varepsilon\\mathbb{P}\\left(\\frac{L^{2}\\lambda^{2}}{l_{1}^{2}}\\mathbb{1}[\\mathrm{prog}(x)<T]\\ge2\\varepsilon\\right)=2\\varepsilon\\mathbb{P}\\left(\\mathrm{prog}(x)<T\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "From (41) with $\\rho=\\textstyle{\\frac{1}{2}}$ , we get ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\nabla f(x^{k})\\right\\Vert^{2}\\right]>2\\varepsilon\\mathbb{P}\\left(\\mathbf{prog}(x)<T\\right)\\geq\\varepsilon\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "for all ", "page_idx": 57}, {"type": "equation", "text": "$$\nk\\leq\\frac{T-1-\\log2}{2p}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "From (39), one can conclude that ", "page_idx": 57}, {"type": "equation", "text": "$$\nT=\\left\\lfloor\\frac{L\\delta^{0}l_{1}}{2\\varepsilon l_{1}^{2}\\Delta^{0}}\\right\\rfloor.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "By the theorem's assumption, $L\\delta^{0}\\geq\\bar{c}\\varepsilon$ . One can choose a universal constant $\\bar{c}$ such that (42) holds for ", "page_idx": 57}, {"type": "equation", "text": "$$\nk\\leq\\Theta\\left(\\frac{T}{p}\\right)=\\Theta\\left(\\frac{L\\delta^{0}}{\\varepsilon p}\\right),\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where $\\Theta$ hides only a universal constant. ", "page_idx": 57}, {"type": "text", "text": "G.3  Compressed communication with independent compressors ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Protocol 5 is exactly the same as Protocol 4 except for Line 5 and describes the family of methods that send compressed vectors from the server to the workers. ", "page_idx": 57}, {"type": "text", "text": "Protocol 5 Protocol with Compressors ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "1: Input: functions $f_{1},\\ldots,f_{n}\\in{\\mathcal{F}}$ , algorithm $A$ , compressors $\\mathcal{C}_{1},\\ldots,\\mathcal{C}_{n}$   \n2: for $k=0,\\ldots,\\infty$ do   \n3:  Server calculates a new point: $x^{k}=B_{1}^{k}(g_{1}^{1},\\ldots,g_{1}^{k},\\ldots,g_{n}^{1},\\ldots,g_{n}^{k})$   \n4: Server aggregates all available information: $s_{i}^{k}=B_{2,i}^{k}(g_{1}^{1},\\ldots,g_{1}^{k},\\ldots,g_{n}^{1},\\ldots,g_{n}^{k})$   \n5:  Server sends compressed vectors $\\bar{s}_{i}^{k}=\\mathcal{C}_{i}(s_{i}^{k})$ to the workers   \n6: Workers aggregate all available local information and calculate  gradients: $g_{i}^{k+1}\\;\\;=\\;\\;$ $L_{i}^{k}(\\bar{s}_{i}^{0},\\ldots,\\bar{s}_{i}^{k})$ $L_{i}^{k}$ has access to the gradient oracle of $f_{i}$ and can call it as many times as it wants according to the rules (37) and (38)   \n7:  Workers send k+1 to the server   \n8: end for ", "page_idx": 57}, {"type": "text", "text": "Theorem G.5. Consider Protocol 5. Let $\\omega\\,\\ge\\,0,L,\\delta^{0},\\varepsilon\\,>\\,0,n\\,\\ge\\,2$ be anynumbers such that $\\bar{c}\\varepsilon<L\\delta^{0}$ .Thenfor any algorithm $A\\in{\\mathcal{A}}_{\\mathrm{zr}}$ , there exists a function $f\\in\\mathcal{F}_{\\delta^{0},L}.$ functions $f_{1},\\ldots,f_{n}$ such that $\\textstyle f={\\frac{1}{n}}\\sum_{i=1}^{n}f_{i}$ , and i.d. compressors $\\mathcal{C}_{1},\\ldots,\\mathcal{C}_{n}\\in\\mathbb{U}(\\omega)$ such that] $\\mathbb{E}\\left[\\left\\|\\nabla f(x^{k})\\right\\|^{2}\\right]>\\varepsilon$ for all ", "page_idx": 57}, {"type": "equation", "text": "$$\nk\\leq\\hat{c}\\frac{(\\omega+1)L\\delta^{0}}{\\varepsilon}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "The quantities c and $\\hat{c}$ are universal constants. ", "page_idx": 57}, {"type": "text", "text": "Proof. We can use the result of Theorem G.4. It is suffcient to construct an appropriate compressor. Let us define $p:={^{1}\\!/}\\omega{+}1$ . We define the following compressor: ", "page_idx": 58}, {"type": "equation", "text": "$$\n[\\mathcal{C}(\\boldsymbol{x})]_{j}:=\\left\\{\\frac{1}{p}x_{j},\\quad j\\in S,\\right.\\quad\\forall j\\in[T],\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where $S$ is a random subset of $[T]$ and each element from $[T]$ appears with probability $p$ independently. Then, $\\mathcal{C}$ is unbiased: ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S}\\left[[\\mathcal{C}(x)]_{j}\\right]=x_{j}\\quad\\forall j\\in\\mathbb{R}^{T}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "and ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S}\\left[\\left\\|\\mathcal{C}(x)\\right\\|^{2}\\right]=\\mathbb{E}_{S}\\left[\\sum_{j=1}^{n T}\\mathbb{1}\\left[j\\in S\\right]\\frac{1}{p^{2}}x_{j}^{2}\\right]=\\sum_{j=1}^{n T}\\mathbb{P}\\left(j\\in S\\right)\\frac{1}{p^{2}}x_{j}^{2}=\\sum_{j=1}^{n T}\\frac{1}{p}x_{j}^{2}=\\left(\\omega+1\\right)\\left\\|x\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Therefore, we get $\\mathcal{C}\\in\\mathbb{U}(\\omega)$ . Let $\\mathcal{C}_{i}$ be i.i.d. instantiations of $\\mathcal{C}$ for all $i\\in[n]$ . Since $\\mathcal{C}$ is a sparsifier as in Line 5 of Protocol 4, we can use Theorem G.4 with $p={^1\\!/\\!\\omega\\!+\\!1}$ to finish the proof. \u53e3 ", "page_idx": 58}, {"type": "text", "text": "H   Useful Identities and Inequalities ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "For all $x,y,x_{1},\\ldots,x_{m}\\in\\mathbb{R}^{d},$ $s>0$ and $\\alpha\\in(0,1]$ ,wehave: ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\Vert x+y\\right\\Vert^{2}\\leq\\left(1+s\\right)\\left\\Vert x\\right\\Vert^{2}+\\left(1+s^{-1}\\right)\\left\\Vert y\\right\\Vert^{2},}\\\\ &{\\displaystyle\\left\\Vert\\sum_{i=1}^{m}x_{i}\\right\\Vert^{2}\\leq m\\left(\\sum_{i=1}^{m}\\left\\Vert x_{i}\\right\\Vert^{2}\\right),}\\\\ &{\\left(1-\\alpha\\right)\\left(1+\\frac{\\alpha}{2}\\right)\\leq1-\\frac{\\alpha}{2},}\\\\ &{\\left(1-\\alpha\\right)\\left(1+\\frac{2}{\\alpha}\\right)\\leq\\frac{2}{\\alpha}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Variance decomposition: For any random vector $X\\in\\mathbb{R}^{d}$ and any non-random vector $c\\in\\mathbb{R}^{d}$ we have ", "text_level": 1, "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|X-c\\right\\|^{2}\\right]=\\mathbb{E}\\left[\\left\\|X-\\mathbb{E}\\left[X\\right]\\right\\|^{2}\\right]+\\left\\|\\mathbb{E}\\left[X\\right]-c\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Tower property: For any random variables $X$ and $Y$ ,wehave ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathbb{E}\\left[X\\mid Y\\right]\\right]=\\mathbb{E}\\left[X\\right].\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Jensen's inequality: If $f$ is a convex function and $X$ is a random variable, then ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(X)\\right]\\geq f\\left(\\mathbb{E}\\left[X\\right]\\right).\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Lemma H.1 (Lemma 2 of Li et al. (2021). Suppose that function $f$ .s $L$ -smooth and let $x^{t+1}=$ $x^{t}-\\gamma g^{t}$ Then for any $\\boldsymbol{g}^{t}\\in\\mathbb{R}^{d}$ and $\\gamma>0$ we have ", "page_idx": 59}, {"type": "equation", "text": "$$\nf({x}^{t+1})\\leq f({x}^{t})-\\frac{\\gamma}{2}\\left\\|\\nabla f({x}^{t})\\right\\|^{2}-\\left(\\frac{1}{2\\gamma}-\\frac{L}{2}\\right)\\left\\|{x}^{t+1}-{x}^{t}\\right\\|^{2}+\\frac{\\gamma}{2}\\left\\|{g}^{t}-\\nabla f({x}^{t})\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Lemma H.2 (Lemma 5 of Richtarik et al. 2021). Let $a,b>0$ $\\begin{array}{r}{\\mathrm{~\\quad0~}\\leq\\gamma\\leq\\frac{1}{\\sqrt{a}+b}}\\end{array}$ then $a\\gamma^{2}\\!+\\!b\\gamma\\leq1$ Moreve hbois i thfctf $\\begin{array}{r}{\\frac{1}{\\sqrt{a}+b}\\leq\\operatorname*{min}\\left\\{\\frac{1}{\\sqrt{a}},\\frac{1}{b}\\right\\}\\leq\\frac{2}{\\sqrt{a}+b}}\\end{array}$ ", "page_idx": 59}, {"type": "text", "text": "I Notation ", "text_level": 1, "page_idx": 60}, {"type": "table", "img_path": "gkJ5nBIOU4/tmp/030fe5d587633907b2b23304acd691855aa7e2de64ffabe61072d444007cea35.jpg", "table_caption": [], "table_footnote": [], "page_idx": 60}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 61}, {"type": "text", "text": "Justification: Sections 3, 4, and 5 ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 61}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Justification: Sections 4.6 and 5.1 ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 61}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 61}, {"type": "text", "text": "Justification: The assumptions and the proofs are in Section 1.1 in the appendix. Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 62}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 62}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 62}, {"type": "text", "text": "Justification: Section F ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 62}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 62}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: In the supplementary materials. ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 63}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: Section F ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 63}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: We run experiments with different seeds and plot averages to reduce noise factors (see the description in Sections F.1 and F.2). ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 63}, {"type": "text", "text": "", "page_idx": 64}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 64}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 64}, {"type": "text", "text": "Justification: Section F ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 64}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 64}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 64}, {"type": "text", "text": "Justification: We have reviewed the code of ethics, and we are confident that our paper is in compliance with it. ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 64}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 64}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 64}, {"type": "text", "text": "Justification: Our work considers a mathematical problem from machine learning. ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 64}, {"type": "text", "text": "", "page_idx": 65}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 65}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 65}, {"type": "text", "text": "Justification: ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks. ", "page_idx": 65}, {"type": "text", "text": "\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 65}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 65}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 65}, {"type": "text", "text": "Justification: Section F ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u00b7 'I'he answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 65}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 66}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 66}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 66}, {"type": "text", "text": "Justification: In the supplementary materials. Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset isused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 66}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 66}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 66}, {"type": "text", "text": "Justification: ", "page_idx": 66}, {"type": "text", "text": "Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 66}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 66}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 66}, {"type": "text", "text": "Justification: ", "page_idx": 66}, {"type": "text", "text": "Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 66}]