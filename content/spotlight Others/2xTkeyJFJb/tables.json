[{"figure_path": "2xTkeyJFJb/tables/tables_6_1.jpg", "caption": "Table 1: Experimental results on datasets with multi-graded relevance. Results denoted with * are from [34, 55]. And *,\u2020, \u2021 and indicate statistically significant improvements over the best performing SR baseline QLM, the DR baseline PseudoQ, the GR baseline RIPOR, and all the baselines, respectively (p < 0.05).", "description": "This table presents the experimental results of various generative retrieval models and baselines on three datasets with multi-graded relevance. It compares the performance of these models using several evaluation metrics (nDCG@5, nDCG@20, P@20, ERR@5, ERR@20).  The results highlight the statistically significant improvements achieved by the proposed GR2 model over other baselines, particularly in terms of precision and NDCG.", "section": "5 Experiments"}, {"figure_path": "2xTkeyJFJb/tables/tables_7_1.jpg", "caption": "Table 1: Experimental results on datasets with multi-graded relevance. Results denoted with * are from [34, 55]. And *,\u2020, \u2021 and indicate statistically significant improvements over the best performing SR baseline QLM, the DR baseline PseudoQ, the GR baseline RIPOR, and all the baselines, respectively (p < 0.05).", "description": "This table presents the performance of various information retrieval methods on datasets with multi-graded relevance.  It compares the proposed GR2 method (GR2S and GR2P) against several baselines categorized as sparse retrieval (SR), dense retrieval (DR), and generative retrieval (GR) methods. The results are evaluated using nDCG@5, nDCG@20, ERR@20, and P@20 metrics, showing GR2's statistically significant improvements over existing state-of-the-art methods.", "section": "5 Experiments"}, {"figure_path": "2xTkeyJFJb/tables/tables_19_1.jpg", "caption": "Table 3: Data statistics. #Queries, #Documents and #Grades denote the number of labeled queries, documents and labeled relevance grades, respectively. #Avg denotes the average number of relevant documents for queries.", "description": "This table presents the statistics of five datasets used in the paper's experiments.  For each dataset, it shows the number of labeled queries, the number of documents, the number of relevance grades, and the average number of relevant documents per query. The datasets include multi-graded relevance datasets (Gov 500K, ClueWeb 500K, Robust04) and binary relevance datasets (MS 500K, NQ 320K).", "section": "5 Experiments"}, {"figure_path": "2xTkeyJFJb/tables/tables_20_1.jpg", "caption": "Table 4: Comparison between GR methods and the full-ranking baseline. * indicates statistically significant improvements over GR2P (p < 0.05).", "description": "This table compares the performance of different generative retrieval (GR) methods against a full-ranking baseline (BM25+monoBERT) on two datasets: Gov 500K and MS 500K.  The GR methods are RIPOR and GR2P. The asterisk (*) indicates statistically significant improvements (p < 0.05) of BM25+monoBERT compared to GR2P. The table shows nDCG@5 and MRR@20 scores for each method on each dataset. This comparison highlights the performance gap between the purely generative retrieval approaches and those that combine sparse retrieval with re-ranking.", "section": "5 Experiments"}, {"figure_path": "2xTkeyJFJb/tables/tables_21_1.jpg", "caption": "Table 5: Results on the MS 1M dataset.", "description": "This table presents the performance comparison between RIPOR and GR2P on the MS 1M dataset, a million-scale dataset.  The results show MRR@20 for both methods, indicating that GR2P achieves comparable results to RIPOR on a large-scale dataset, despite GR2P being designed with a multi-graded relevance approach.", "section": "5 Experiments"}]