{"importance": "This paper is crucial because it challenges the prevailing trend in long-term time series forecasting (LTSF) of increasing model complexity and demonstrates that decomposition methods, particularly the proposed SSCNN model, can achieve superior performance with significantly fewer parameters. **This addresses the critical issue of model scalability and computational cost**, a major obstacle in applying advanced LTSF models to real-world applications. Furthermore, **the paper provides a rigorous analysis comparing decomposition and patching techniques**, offering valuable insights for future research in LTSF model design.", "summary": "SSCNN, a novel decomposition-based model, achieves superior long-term time series forecasting accuracy using 99% fewer parameters than existing methods, proving that bigger isn't always better.", "takeaways": ["SSCNN, a new decomposition-based model for long-term time series forecasting, outperforms state-of-the-art methods.", "SSCNN uses significantly fewer parameters (99% less than most competitors) while maintaining accuracy, demonstrating the effectiveness of parsimonious models in LTSF.", "The study provides a comprehensive analysis of decomposition vs. patching techniques in LTSF, revealing valuable insights for model design and optimization."], "tldr": "Long-term time series forecasting (LTSF) models often suffer from excessive complexity and massive parameter scales, hindering their practical applicability.  Existing methods, such as those using data patching, struggle to maintain optimal effectiveness with fewer parameters because they often lose crucial temporal or spatial information during the patching process. This necessitates a high-dimensional latent space, exponentially inflating parameters.  The model also tends to overfit due to the increased size, especially when data is limited. \nTo overcome these limitations, the paper introduces SSCNN, a Selective Structured Components-based Neural Network. SSCNN employs a feature decomposition strategy with a selection mechanism, enabling it to maintain and harness temporal and spatial regularities effectively. This approach allows the model to selectively capture crucial fine-grained dependencies within the time series data, without the need for an excessively large latent space.  Empirically, SSCNN consistently outperforms state-of-the-art methods across various datasets while using significantly fewer parameters (over 99% reduction compared to most benchmarks).", "affiliation": "Hong Kong University of Science and Technology", "categories": {"main_category": "Machine Learning", "sub_category": "Deep Learning"}, "podcast_path": "wiEHZSV15I/podcast.wav"}