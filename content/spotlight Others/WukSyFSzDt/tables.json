[{"figure_path": "WukSyFSzDt/tables/tables_2_1.jpg", "caption": "Table 1: Summary of the worst-case convergence behaviors of the considered distributed optimization methods (in the BigO-notation) assuming each fi is L-smooth and \u00b5-convex with \u03bc < \u0398(\u03b4), where \u03b4, dmax, \u03b42 are defined in (2), Remark 2 and (3), and D := ||x\u00b0 \u2013 x*||. The '# local gradient queries' column represents the number of gradient oracle queries required between two communication rounds to achieve the corresponding complexity, assuming the most efficient local first-order algorithms are used. The column 'Guarantee' means whether the convergence guarantee holds in expectation or deterministically. The suboptimality \u03b4 is defined via ||xRx*||2 and f(x) - f* for strongly-convex and general convex functions where x is a certain output produced by the algorithm after R number of communications.", "description": "This table summarizes the convergence rates of various distributed optimization algorithms.  It compares their communication complexity (number of communication rounds and vectors communicated per round) and local computation complexity (number of local gradient queries). The algorithms are categorized by whether they handle general convex or strongly convex functions and whether their convergence guarantee is deterministic or in expectation. The table also defines key parameters like L-smoothness, \u00b5-convexity, and second-order dissimilarity (\u03b4).", "section": "Related Work"}, {"figure_path": "WukSyFSzDt/tables/tables_4_1.jpg", "caption": "Table 1: Summary of the worst-case convergence behaviors of the considered distributed optimization methods (in the BigO-notation) assuming each fi is L-smooth and \u00b5-convex with \u03bc < \u0398(\u03b4), where \u03b4, dmax, \u03b42 are defined in (2), Remark 2 and (3), and D := ||x\u00ba \u2013 x*||. The '# local gradient queries' column represents the number of gradient oracle queries required between two communication rounds to achieve the corresponding complexity, assuming the most efficient local first-order algorithms are used. The column 'Guarantee' means whether the convergence guarantee holds in expectation or deterministically. The suboptimality \u03b4 is defined via ||xR \u2212 x*||2 and f(xR) \u2212 f* for strongly-convex and general convex functions where xR is a certain output produced by the algorithm after R number of communications.", "description": "This table summarizes the convergence behavior of various distributed optimization methods.  It compares the number of communication rounds and local gradient queries needed to reach a desired accuracy, considering both strongly convex and general convex functions.  The table also indicates whether the convergence guarantee is deterministic or holds only in expectation.  Different methods are compared based on their communication and computational efficiency under various assumptions.", "section": "Related Work"}]