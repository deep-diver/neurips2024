[{"type": "text", "text": "Stabilized Proximal-Point Methods for Federated Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xiaowen Jiang Anton Rodomanov Sebastian U. Stich Saarland University and CISPA\u2217 CISPA\u2217 CISPA\u2217 xiaowen.jiang@cispa.de anton.rodomanov@cispa.de stich@cispa.de ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In developing efficient optimization algorithms, it is crucial to account for communication constraints\u2014a significant challenge in modern Federated Learning. The best-known communication complexity among non-accelerated algorithms is achieved by DANE, a distributed proximal-point algorithm that solves local subproblems at each iteration and that can exploit second-order similarity among individual functions. However, to achieve such communication efficiency, the algorithm requires solving local subproblems sufficiently accurately resulting in slightly sub-optimal local complexity. Inspired by the hybrid-projection proximalpoint method, in this work, we propose a novel distributed algorithm S-DANE. Compared to DANE, this method uses an auxiliary sequence of prox-centers while maintaining the same deterministic communication complexity. Moreover, the accuracy condition for solving the subproblem is milder, leading to enhanced local computation efficiency. Furthermore, S-DANE supports partial client participation and arbitrary stochastic local solvers, making it attractive in practice. We further accelerate S-DANE and show that the resulting algorithm achieves the best-known communication complexity among all existing methods for distributed convex optimization while still enjoying good local computation efficiency as S-DANE. Finally, we propose adaptive variants of both methods using line search, obtaining the first provably efficient adaptive algorithms that could exploit local second-order similarity without the prior knowledge of any parameters. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated learning is a rapidly emerging large-scale machine learning framework that allows training from decentralized data sources (e.g. mobile phones or hospitals) while preserving basic privacy and security [43, 24, 31]. Developing efficient federated optimization algorithms becomes one of the central focuses due to its direct impact on the effectiveness of global machine learning models. ", "page_idx": 0}, {"type": "text", "text": "One of the key challenges in modern federated optimization is to tackle communication bottlenecks [32]. The large-scale model parameters, coupled with relatively limited network capacity and unstable client participation, often make communication highly expensive. Therefore, the primary efficiency metric of a federated optimization algorithm is the total number of communication rounds required to reach a desired accuracy level. If two algorithms share equivalent communication complexity, their local computation efficiency becomes the second important metric. ", "page_idx": 0}, {"type": "text", "text": "The seminal algorithm DANE [57] is an outstanding distributed optimization method. It achieves the best-known deterministic communication complexity among existing non-accelerated algorithms (on the server side) [22]. This efficiency primarily hinges upon a mild precondition regarding the Secondorder Dissimilarity $\\delta$ . In numerous scenarios, like statistical learning for generalized model [19] and semi-supervised learning [7], $\\delta$ tends to be relatively small. However, to ensure such fast convergence, DANE requires each iteration subproblem to be solved with sufficiently high accuracy. This leads to sub-optimal local computation effort across the communication rounds, which is inefficient in practice. FEDRED [22] improves this weakness by using double regularization. However, this technique is only effective when using gradient descent as the local solver but cannot easily be combined with other optimization methods. For instance, applying local accelerated gradient or second-order methods cannot improve its local computation efficiency. Moreover, it is also unclear how to extend this method to the partial client participation setting relevant to federated learning. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "On the other hand, the communication complexities achieved by the current accelerated methods typically cannot be directly compared with those attained by DANE, as they either depend on sub-optimal constants or additional quantities such as the number of clients $n$ . The most relevant and state-of-the-art algorithm ACC-EXTRAGRADIENT [33] achieves a better complexity in terms of the accuracy $\\varepsilon$ but with dependency on the maximum Second-order Dissimilarity $\\delta_{\\mathrm{max}}$ which can in principle be much larger than $\\delta$ (see Remark 2). Unlike most federated learning algorithms, such as FEDAVG [43], this method requires communication with all the devices at each round to compute the full gradient and then assigns one device for local computation. In contrast, FEDAVG and similar algorithms perform local computations on parallel and utilize the standard averaging to compute the global model. The follow-up work AccSVRS [40] applies variance reduction to ACCEXTRAGRADIENT which results in less frequent full gradient updates. However, the communication complexity incurs a dependency on $n$ which is prohibitive for cross-device setting [24] where the number of clients can be potentially very large. Thus, there exists no accelerated federated algorithm that is uniformly better than DANE in terms of communication complexity. ", "page_idx": 1}, {"type": "text", "text": "Contributions. In this work, we aim to develop federated optimization algorithms that can achieve the best communication complexity while retaining efficient local computation. To this end, we first revisit the simple proximal-point method on a single machine. The accuracy requirement for solving the subproblem defined in this algorithm is slightly sub-optimal. Drawing inspiration from hybrid projection-proximal point method for finding zeroes of a maximal monotone operator [59], we observe that using a more stabilized prox-center improves the accuracy requirement. We make the following contributions: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We develop a novel federated optimization algorithm S-DANE that achieves the best-known communication complexity (for non-accelerated methods) while also enjoying improved local computation efficiency over DANE [57].   \n\u2022 We develop an accelerated version of S-DANE based on the Monteiro-Svaiter acceleration [46]. The resulting algorithm ACC-S-DANE achieves the best-known communication complexity among all existing methods for distributed convex optimization.   \n\u2022 Both algorithms support partial client participation and arbitrary stochastic local solvers, making them attractive in practice for federated optimization.   \n\u2022 We provide a simple analysis for both algorithms. We derive convergence estimates that are continuous in the strong convexity parameter $\\mu$ .   \n\u2022 We propose adaptive variants of both algorithms using line-search in the full client participation setting. The resulting methods achieve the same communication complexity (up to a logrithmic factor) as non-adaptive ones without requiring knowledge of the similarity constant.   \n\u2022 We illustrate strong practical performance of our proposed methods in experiments. ", "page_idx": 1}, {"type": "text", "text": "See also Table 1 for a summary of the main complexity results in the full-participation setting. ", "page_idx": 1}, {"type": "text", "text": "Related Work. Moreau first proposed the notion of the proximal approximation of a function [47]. Based on this operation, Martinet developed the first proximal-point method [42]. This method was first accelerated by G\u00fcller [17], drawing the inspiration from Nesterov\u2019s Fast gradient method [49]. Later, Lin et al. [41] introduced the celebrated CATALYST framework that builds upon G\u00fcller\u2019s acceleration. Using CATALYST acceleration, a large class of optimization algorithms can directly achieve faster convergence. In a similar spirit, Doikov and Nesterov [12] propose contracting proximal methods that can accelerate higher-order tensor methods. While G\u00fcller\u2019s acceleration has been successfully applied to many settings, its local computation is sub-optimal. Specifically, when minimizing smooth convex functions, a logarithmic dependence on the final accuracy is ", "page_idx": 1}, {"type": "table", "img_path": "WukSyFSzDt/tmp/910c8ff3a5cfbe53d59ae48f6606145f4b2686797bc94df3301a10696d48e77d.jpg", "table_caption": [], "table_footnote": ["aFor SCAFFNEW and FEDRED, the column \u2018# comm rounds\u2019 represents the expected number of total communications required to reach $\\varepsilon$ accuracy. The column \u2018# local gradient queries\u2019 is replaced with the expected number of local steps between two communications. bThe general convex result of SCAFFNEW is established in Theorem 11 in the RANDPROX paper [8]. We assume that $\\mathbf{h}_{i,0}=\\nabla f_{i}(\\mathbf{x}^{0})$ and estimate $\\begin{array}{r}{H_{0}^{2}:=\\frac{1}{n}\\sum_{i=1}^{n}\\|\\mathbf{h}_{i,0}-\\nabla f_{i}(\\mathbf{x}^{\\star})\\|\\leq L^{2}D^{2}}\\end{array}$ . Then the best p is of order 1. cSONATA, INEXACT ACC-SONATA, ACC-EXTRAGRADIENT and ACCSVRS only need to assume strong convexity of $f$ . dExact proximal local steps are used in SONATA eCATALYZED SVRP and ACCSVRS aim at minimizing a different measure which is the total amount of information transmitted between the server and the clients. Their iteration complexity is equivalent to the communication rounds in our notations. We refer to Remark 7 for details. fKhaled and Jin [28] assume exact evaluations of the proximal operator for the convenience of analysis. "], "page_idx": 2}, {"type": "text", "text": "Table 1: Summary of the worst-case convergence behaviors of the considered distributed optimization methods (in the BigO-notation) assuming each $f_{i}$ is $L$ -smooth and $\\mu$ -convex with $\\mu\\leq\\Theta(\\delta)$ , where $\\delta$ , $\\delta_{\\mathrm{max}}$ , $\\zeta^{2}$ are defined in (2), Remark 2 and (3), and $\\bar{D}:=\\|\\mathbf{x}^{0}-\\mathbf{x}^{\\star}\\|$ . The $\\#$ local gradient queries\u2019 column represents the number of gradient oracle queries required between two communication rounds to achieve the corresponding complexity, assuming the most efficient local first-order algorithms are used. The column \u2019Guarantee\u2019 means whether the convergence guarantee holds in expectation or deterministically. The suboptimality $\\varepsilon$ is defined via $\\|\\hat{\\mathbf{x}}^{R}-\\mathbf{x}^{\\star}\\|^{2}$ and $\\bar{f}(\\hat{\\mathbf{x}}^{R})=f^{\\star}$ for strongly-convex and general convex functions where $\\hat{\\mathbf{x}}^{R}$ is a certain output produced by the algorithm after $R$ number of communications. ", "page_idx": 2}, {"type": "image", "img_path": "WukSyFSzDt/tmp/7cf61e487c382f8f185e4234e5fcabb24bfabac558284ec1ef7d5456fcf18b3e.jpg", "img_caption": ["Figure 1: Comparison of S-DANE and ACC-S-DANE with DANE for solving a convex quadratic minimization problem. All three methods use GD as the local solver. S-DANE has improved local computation efficiency than DANE while ACC-S-DANE further improves the communication complexity. Finally, the adaptive variants can leverage local dissimilarities to achieve better performance. (The definitions of local smoothness and dissimilarity can be found in Section 6.) "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "incurred in its local computation complexity [12]. Solodov and Svaiter [59] proposed a HYBRID PROJECTION-PROXIMAL POINT method that allows significant relaxation of the accuracy condition for the proximal-point subproblems. More recent works such as ADAPTIVE CATALYST [21] and RECAPP [5] successfully get rid of the additional logarithmic factor for accelerated proximal-point methods as well. ", "page_idx": 2}, {"type": "text", "text": "Another type of acceleration based on the proximal extra-gradient method was introduced by Monteiro and Svaiter [46]. This method is more general in the sense that it allows arbitrary local solvers and the convergence rates depend on the these solvers. For instance, under convexity and Lipschitz secondorder derivative, the rate can be accelerated to $\\mathcal{O}(1/k^{3.5})$ by using Newton-type method. Moreover, when the gradient method is used, Monteiro-Svaiter Acceleration recovers the rate of G\u00fcller\u2019s acceleration and its accuracy requirement for the inexact solution is weaker. For minimizing smooth convex functions, one gradient step is enough for approximately solving the local subproblem [48]. This technique has been applied to centralized composite optimization, known as gradient sliding [35, 36, 33]. A comprehensive study of acceleration can be found in [14]. ", "page_idx": 2}, {"type": "text", "text": "2 Problem Setup and Background ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We consider the following distributed minimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{d}}\\Big\\{f(\\mathbf{x}):=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}(\\mathbf{x})\\Big\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where each function $f_{i}\\colon\\ensuremath{\\mathbb{R}}^{d}\\to\\ensuremath{\\mathbb{R}}$ is $\\mu$ -strongly convex2 for some $\\mu\\geq0$ . We focus on the standard federated setting where the functions $\\{f_{i}\\}$ are distributed among $n$ devices. The server coordinates the global optimization procedure among the devices. In each communication round, the server broadcasts certain information to the clients. The clients, in turn, perform local computation in parallel based on their own data and transmit the resulting local models back to the server to update the global model. ", "page_idx": 3}, {"type": "text", "text": "Main objective: Given the high cost of establishing connections between the server and the clients, our paramount objective is to minimize the number of required communication rounds to achieve the desired accuracy level. This represents a central metric in federated contexts, as outlined in references such as [43, 27]. Secondary objective: Efficiency in local computation represents another pivotal metric for optimization algorithms. For instance, if two algorithms share equivalent communication complexity, the algorithm with less local computational complexity is the more favorable choice. ", "page_idx": 3}, {"type": "text", "text": "Notation: We abbreviate $[n]:=\\{1,2,\\dots,n\\}$ . For a set $A$ and an integer $1\\leq s\\leq|A|$ , we use $\\binom{A}{s}$ to denote the power set comprised of all $s$ -element subsets of $A$ . Everywhere in this paper, $\\lVert\\cdot\\rVert$ denotes the standard Euclidean norm (or the corresponding spectral norm for matrices). We assume problem (1) has a solution which we denote by $\\mathbf{x}^{\\star}$ ; the corresponding optimal value is denoted by $f^{\\star}$ . For a set $S\\in\\left(\\mathbf{\\Sigma}_{s}^{[n]}\\right)$ , we use $\\begin{array}{r}{f_{S}:=\\frac{1}{s}\\sum_{i\\in S}f_{i}}\\end{array}$ to denote the average function over this set. We use $r$ to denote the index of the communication round and to denote the index of the local step. Finally, we use the superscript and subscript to denote the global and local models, respectively; for instance, $\\mathbf{x}^{r}$ represents the global model at round $r$ while ${\\bf x}_{i,r}$ is the local model computed by device $i$ at round $r$ . ", "page_idx": 3}, {"type": "text", "text": "2.1 Proximal-Point Methods on Single Machine ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we provide a brief background on proximal-point methods [47, 42, 54, 51], which are the foundation for many distributed optimization algorithms. ", "page_idx": 3}, {"type": "text", "text": "Proximal-Point Method. Given an iterate $\\mathbf{x}_{k}$ , the method defines $\\mathbf{x}_{k+1}$ to be an (approximate) minimizer of the proximal-point subproblem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{x}_{k+1}\\approx\\underset{\\mathbf{x}\\in\\mathbb{R}^{d}}{\\arg\\operatorname*{min}}\\Big\\{F_{k}(\\mathbf{x}):=f(\\mathbf{x})+\\frac{\\lambda}{2}\\|\\mathbf{x}-\\mathbf{x}_{k}\\|^{2}\\Big\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for an appropriately chosen parameter $\\lambda\\:\\geq\\:0$ . This parameter allows for a trade-off between the complexity of each iteration and the rate of convergence. If $\\lambda=0$ , the subproblem in each iteration is as difficult as solving the original problem because no regularization is applied. However, as $\\lambda$ increases, more regularization is added, simplifying the subproblem. For example, for a convex function $f$ , the proximal-point method guarantees $f\\!\\left({\\bar{\\mathbf{x}}}_{K}\\right)-f^{\\star}\\le\\mathcal{O}\\!\\left(\\frac{\\lambda}{K}\\|\\mathbf{x}_{0}-\\mathbf{x}^{\\star}\\|^{2}\\right)$ , where $\\begin{array}{r}{\\bar{\\mathbf{x}}_{K}:=\\frac{1}{K}\\sum_{k=1}^{K}\\mathbf{x}_{k}}\\end{array}$ [51, 5]. However, to achieve such a convergence rate, the subproblem (2) has to be solved to a fairly high accuracy [54, 5]. For instance, the accuracy condition should either depend on the target accuracy $\\varepsilon$ , or increase with $k$ : $\\begin{array}{r}{\\|\\nabla F_{k}({\\bf x}_{k+1})\\|\\,=\\,\\mathcal{O}(\\frac{\\lambda}{k}\\|{\\bf x}_{k+1}-{\\bf x}_{k}\\|)}\\end{array}$ [58]. Indeed, when $f$ is Lipschitz-smooth and the standard gradient descent is used as a local solver, the number of gradient steps required to solve the subproblem has a logarithmic dependence on the iteration counter $k$ . The same issue also arises when considering accelerated proximal-point methods [12, 17]. ", "page_idx": 3}, {"type": "text", "text": "Stabilized Proximal-Point Method. One of the key insights that we use in this work is the observation that using a different prox-center makes the accuracy condition of the subproblem weaker. ", "page_idx": 3}, {"type": "table", "img_path": "WukSyFSzDt/tmp/aab4c8bd5aebd06466036a57d74a2ee4dd46ff3a65d3d988c61af6d1848177fd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "The stabilized proximal-point method defines ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf x}_{k+1}\\approx\\arg\\operatorname*{min}_{{\\bf x}}\\Bigl\\{F_{k}({\\bf x}):=f({\\bf x})+\\frac{\\lambda}{2}\\|{\\bf x}-{\\bf v}_{k}\\|^{2}\\Bigr\\}},}\\\\ {{\\displaystyle{\\bf v}_{k+1}=\\arg\\operatorname*{min}_{{\\bf x}}\\Bigl\\{\\langle\\nabla f({\\bf x}_{k+1}),{\\bf x}\\rangle+\\frac{\\mu}{2}\\|{\\bf x}-{\\bf x}_{k+1}\\|^{2}+\\frac{\\lambda}{2}\\|{\\bf x}-{\\bf v}_{k}\\|^{2}\\Bigr\\}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda\\geq0$ is a parameter of the method and $\\mu\\geq0$ is the strong-convexity constant of $f$ . This algorithm updates the prox-center $\\mathbf{v}_{k}$ by performing an additional gradient step in each iteration. For instance, when $\\mu=0$ , the prox-center is updated as $\\begin{array}{r}{\\mathbf{\\check{v}}_{k+1}=\\mathbf{v}_{k}\\!-\\!\\frac{1}{\\lambda}\\nabla f(\\mathbf{x}_{k+1})}\\end{array}$ , which is often referred to as an extra-gradient update. The stabilized proximal-point method has the same convergence rate as the original method (2) but requires only that $\\|\\bar{\\nabla}\\bar{F}_{k}(\\mathbf{x}_{k+1})\\|\\leq\\mathcal{O}(\\lambda\\|\\mathbf{x}_{k+1}-\\mathbf{v}_{k}\\|)$ . As a result, there is no extra logarithmic factor of $k$ in the oracle complexity estimate when $f$ is $L$ -smooth. Specifically, by setting $\\bar{\\lambda}=\\Theta(L)$ , the previous condition can be satisfied by choosing $\\mathbf{x}_{k+1}$ as the result of one gradient step from $\\mathbf{v}_{k}$ [48]. This shows that the stabilized proximal-point method has a better overall oracle complexity than the standard proximal-point method (c.f. Theorem 1 for the special case $n\\,=\\,1$ ). It is worth noting that the former algorithm originates from the hybrid projection-proximal point algorithm [59] designed for solving the more general problem of finding zeroes of a monotone operator. In this work, we apply this algorithm in the distributed setting $(n\\geq2)$ ). ", "page_idx": 4}, {"type": "text", "text": "2.2 Distributed Proximal-Point Methods ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The proximal-point method can be adapted to solve the distributed optimization problem (1). This is the idea behind FEDPROX [39]. It replaces the global proximal step (2) by $n$ subproblems defined as $\\begin{array}{r}{\\mathbf{x}_{i,r+1}:=\\arg\\operatorname*{min}_{\\mathbf{x}}\\{f_{i}(\\mathbf{x})+\\frac{\\lambda}{2}\\|\\mathbf{x}-\\mathbf{x}^{r}\\|^{2}\\}}\\end{array}$ , which can be solved independently on each device, followed by the averaging step $\\begin{array}{r}{\\mathbf{x}^{r+1}=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{x}_{i,r+1}}\\end{array}$ . Here we switch the notation from $k$ to $r$ to highlight that one iteration of the proximal-point method corresponds to a communication round in this setting. To ensure convergence, FEDPROX has to use a large $\\lambda$ that depends on the target accuracy as well as the heterogeneity among $\\{f_{i}\\}$ , which slows down the communication efficiency [39]. DANE [57] improves this by incorporating a drift correction term into the subproblem: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{x}_{i,r+1}:=\\underset{\\mathbf{x}}{\\arg\\operatorname*{min}}\\Big\\{\\tilde{F}_{i,r}(\\mathbf{x}):=f_{i}(\\mathbf{x})+\\langle\\nabla f(\\mathbf{x}^{r})-\\nabla f_{i}(\\mathbf{x}^{r}),\\mathbf{x}\\rangle+\\frac{\\lambda}{2}\\|\\mathbf{x}-\\mathbf{x}^{r}\\|^{2}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Consequently, DANE allows to choose a much smaller $\\lambda$ in the algorithm. Moreover, it can exploit second-order similarity and achieve the best-known communication complexity among nonaccelerated methods [22]. However, as in the original proximal-point method, the subproblem needs to be solved sufficiently accurately leading to an extra logarithmic factor in the oracle complexity estimate. To overcome this problem, we propose new algorithms described in the following section. ", "page_idx": 4}, {"type": "text", "text": "3 Stabilized DANE ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now describe S-DANE (Alg. 1), our proposed federated proximal-point method that employs stabilized prox-centers in its subproblems. During each communication round $r$ , the server samples a subset of clients uniformly at random and sends $\\mathbf{v}_{}^{r}$ to these clients. Then the server collects $\\nabla f_{i}(\\mathbf{v}^{r})$ from these clients, computes $\\nabla f_{S_{r}}(\\mathbf{v}^{r})$ and sends $\\nabla f_{S_{r}}(\\mathbf{v}^{r})$ back to them. Each device in the set then calls an arbitrary local solver (which can be different on each device) to approximately solve its local subproblem. Finally, each device transmits $\\nabla f_{i}(\\mathbf{x}_{i,r+1})$ and ${\\bf x}_{i,r+1}$ back to the server which then aggregates these points and computes the new global model. ", "page_idx": 4}, {"type": "text", "text": "As DANE, S-DANE can also achieve communication speed-up if the functions among devices are similar to each other. This is formally captured by the following assumption. ", "page_idx": 5}, {"type": "text", "text": "Definition 1 (Second-order Dissimilarity). Let $f_{1},\\dots,f_{n}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ be functions, and let $s\\in[n]$ , $\\delta_{s}\\geq0$ . Then, $\\{f_{i}\\}_{i=1}^{n}$ are said to have $\\delta_{s}$ -SOD (of size $s$ ) if for any $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d}$ and any $S\\in\\left(\\O_{s}^{[n]}\\right)$ , $i t$ holds that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{s}\\sum_{i\\in S}\\lVert\\nabla h_{i}^{S}({\\bf x})-\\nabla h_{i}^{S}({\\bf y})\\rVert^{2}\\leq\\delta_{s}^{2}\\lVert{\\bf x}-{\\bf y}\\rVert^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $h_{i}^{S}:=f_{S}-f_{i}$ and $\\begin{array}{r}{f_{S}:=\\frac{1}{s}\\sum_{i\\in S}f_{i}}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Definition 1 quantifies the dissimilarity between any $s$ functions and their average, i.e., the \u201cinternal\u201d variation between any $s$ functions. Clearly, $\\delta_{1}=0$ , and, when $s=n$ , we recover the standard notion of second-order dissimilarity introduced in prior works: ", "page_idx": 5}, {"type": "text", "text": "Definition 2 ( $\\delta$ -SOD [28, 40, 22]). $\\delta{-}S O D:=\\delta_{n}$ -SOD of size $n$ . ", "page_idx": 5}, {"type": "text", "text": "When each function $f_{i}$ is twice continuously differentiable, a simple sufficient condition for (5) is that $\\begin{array}{r}{\\frac{1}{s}\\sum_{i\\in S}\\|\\nabla_{}^{2}h_{i}^{S}(\\mathbf{\\bar{x}})\\|^{2}\\leq\\delta_{s}^{2}}\\end{array}$ for any $\\mathbf{x}\\in\\dot{\\mathbb{R}}^{d}$ . However, this is not a necessary condition (see [22] for more details). ", "page_idx": 5}, {"type": "text", "text": "The quantity $V(\\mathbf{x},\\mathbf{y})$ in the left-hand side of (5) can be interpreted as the variance of the gradient difference estimator $\\nabla f_{\\hat{i}}(\\mathbf{x})-\\nabla f_{\\hat{i}}(\\mathbf{y})$ , where $\\hat{i}$ is chosen uniformly at random from $S$ . In particular, it can be rewritten as $\\begin{array}{r}{V(\\mathbf{x},\\mathbf{y})=\\frac{1}{s}\\sum_{i\\in S}\\lVert\\nabla f_{i}(\\mathbf{x})-\\nabla f_{i}(\\mathbf{y})\\rVert^{2}-\\lVert\\nabla f_{S}(\\mathbf{x})-\\nabla f_{S}(\\mathbf{y})\\rVert^{2}}\\end{array}$ . If each function $f_{i}$ is $L_{i}$ -smooth, then $\\begin{array}{r}{\\delta_{s}\\,\\le\\,(\\frac1s\\sum_{i\\in S}L_{i}^{2})^{1/2}}\\end{array}$ for any $s\\in[n]$ . However, in general, condition (5) is weaker than assuming that each $f_{i}$ is Lipschitz-smooth. ", "page_idx": 5}, {"type": "text", "text": "Full Client Participation. We first consider the cross-silo setting where all the clients are highly reliable ( $\\mathit{\\omega}_{s}=n_{\\mathit{\\omega}}$ ). This is typically the case with organizations and institutions having strong computing resources and stable network connection [24]. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Consider Algorithm $^{\\,l}$ with $s=n$ . Let $f_{i}\\colon\\ensuremath{\\mathbb{R}}^{d}\\to\\ensuremath{\\mathbb{R}}$ be $\\mu$ -convex with $\\mu\\geq0$ for any $i\\in[n]$ . Assume that $\\{f_{i}\\}_{i=1}^{n}$ have $\\delta$ -SOD. Let $\\lambda=2\\delta$ and suppose that, for any $r\\geq0$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\|\\nabla F_{i,r}(\\mathbf{x}_{i,r+1})\\|^{2}\\leq\\frac{\\lambda^{2}}{4}\\sum_{i=1}^{n}\\|\\mathbf{x}_{i,r+1}-\\mathbf{v}^{r}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then, for any $R\\geq1$ , it holds that3 ", "page_idx": 5}, {"type": "equation", "text": "$$\nf(\\bar{{\\bf x}}^{R})-f^{\\star}\\le\\frac{\\mu D^{2}}{2[(1+\\frac{\\mu}{2\\delta})^{R}-1]}\\le\\frac{\\delta D^{2}}{R},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where rR=11 pr rR=1 prxr for p := 1 + \u03bb\u00b5, and D := \u2225x0 \u2212x\u22c6\u2225. To obtain f(\u00afxR) \u2212f \u22c6\u2264\u03b5 for a given $\\varepsilon>0$ , it thus suffices to perform $\\begin{array}{r}{R=\\mathcal{O}\\big(\\frac{\\delta+\\mu}{\\mu}\\log(1+\\frac{\\mu D^{2}}{\\varepsilon})\\big)}\\end{array}$ communication rounds. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 provides the convergence guarantee for S-DANE in terms of the number of communication rounds. Note that the rate is continuous in $\\mu$ . ", "page_idx": 5}, {"type": "text", "text": "Remark 2. Some previous works express complexity estimates in terms of another constant, $\\delta_{\\mathrm{max}}$ , defined by the inequality $\\|\\nabla h_{i}({\\bf x})-\\bar{\\nabla}h_{i}({\\bf y})\\|\\bar{\\bf\\Phi}\\leq\\delta_{\\mathrm{max}}\\|{\\bf x}-{\\bf y}\\|$ holding for any x, $\\mathbf{y}\\in\\mathbb{R}^{d}$ and any $i\\in[n]$ , where $h_{i}=f-f_{i}$ . (See for instance the second line in Table 1). Note that our $\\delta$ is always not larger than $\\delta_{\\mathrm{max}},$ , and can in principle be much smaller (up to $\\sqrt{n}$ times). ", "page_idx": 5}, {"type": "text", "text": "The proven communication complexity is the same as that of DANE [22]. However, the accuracy condition is milder. Specifically, to achieve the same guarantee, DANE requires $\\begin{array}{r}{\\sum_{i=1}^{n}\\|\\nabla\\tilde{F}_{i,r}(\\mathbf{x}_{i,r+1})\\|^{2}\\;\\leq\\;\\mathcal{O}(\\frac{\\delta^{2}}{r^{2}}\\sum_{i=1}^{n}\\|\\mathbf{x}_{i,r+1}-\\mathbf{x}^{r}\\|^{2})}\\end{array}$ , where $\\tilde{F}_{i,r}$ is defined as in (4), which incurs an $r^{2}$ overhead in the denominator, as in the general discussion on proximal-point methods in Section 2.1. The next corollary shows that local computations in S-DANE could be computationally very efficient. ", "page_idx": 5}, {"type": "text", "text": "Corollary 3. Consider the same setting as in Theorem 1. Further, assume that each $f_{i}$ is $L$ -smooth. To ensure (6) with a certain first-order algorithm, each device i needs to perform at most $\\mathcal{O}(\\sqrt{\\frac{L}{\\delta}})$ computations of $\\nabla f_{i}$ at each round $r$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 4. Particular examples of algorithms that could be used to achieve the result from Corollary 3 are OGM-OG by Kim and Fessler [30] and the accumulative regularization method by Lan et al. [37], both designed for the fast minimization of the gradient norm. For the standard Gradient Method (GM), the required number of oracle calls is $\\begin{array}{r}{\\breve{\\mathscr{O}}(\\frac{L}{\\delta})}\\end{array}$ . The standard Fast Gradient Method (FGM) [49] can further decrease the complexity to $\\begin{array}{r}{\\mathcal{O}(\\sqrt{\\frac{L}{\\mu+\\delta}}\\log\\frac{L}{\\delta})}\\end{array}$ (see Remark 18 for details). Thus, each device can run a constant number of standard local $(F)G M$ steps to approximately solve their subproblems in S-DANE. ", "page_idx": 6}, {"type": "text", "text": "Partial Client Participation. Next, we turn our attention to the cross-device setting where a large number of clients (typically mobile phones) have either unstable network connection or weak computational power [24]. In such scenarios, the server typically cannot expect all the clients to be able to participate in the communication at each round. Furthermore, the clients may typically be asked to communicate only once during the whole training and are stateless [27]. Therefore, we now consider S-DANE with partial client participation and without storing any states on devices. ", "page_idx": 6}, {"type": "text", "text": "To prove convergence, it is necessary to assume a certain level of dissimilarity among clients. Here, we use the same assumption as in [27] to measure the gradient variance. ", "page_idx": 6}, {"type": "text", "text": "Definition 3 (Bounded Gradient Variance [27]). Let $f_{1},\\dots,f_{n}\\colon\\ensuremath{\\mathbb{R}}^{d}\\to\\ensuremath{\\mathbb{R}}$ be functions, and let $\\zeta\\geq0$ . We say that $\\{f_{i}\\}_{i=1}^{n}$ have $\\zeta$ -BGV if, for any $\\mathbf{x}\\in\\mathbb{R}^{d}$ and $\\textstyle f:={\\frac{1}{n}}\\sum_{i=1}^{n}f_{i},$ , it holds that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\lVert\\nabla f_{i}(\\mathbf{x})-\\nabla f(\\mathbf{x})\\rVert^{2}\\leq\\zeta^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Definition 3 is similar to the classical notion of uniformly bounded variance used in the context of classical stochastic gradient methods [3]. ", "page_idx": 6}, {"type": "text", "text": "We also need the following assumption which complements Definition 1. ", "page_idx": 6}, {"type": "text", "text": "Definition 4 (External Dissimilarity). Let $f_{1},\\dots,f_{n}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ be functions, and let $s\\in[n]$ , $\\Delta_{s}\\geq0$ . Then, $\\{f_{i}\\}_{i=1}^{n}$ are said to have $\\Delta_{s}$ -ED (of size $s$ ) $i f,$ for any x, $\\mathbf{y}\\in\\mathbb{R}^{d}$ and any $S\\in\\left(\\O_{s}^{[n]}\\right)$ , we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\|\\nabla m_{S}({\\bf x})-\\nabla m_{S}({\\bf y})\\|\\le\\Delta_{s}\\|{\\bf x-y}\\|,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $m_{S}:=f-f_{S}$ and $\\begin{array}{r}{f_{S}:=\\frac{1}{s}\\sum_{i\\in S}f_{i}}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "Compared to Definition 1, the new Definition 4 quantifies the \u201cexternal\u201d variation of any $s$ functions w.r.t. the original function $f$ . When each $f_{i}$ is twice continuously differentiable, (8) is equivalent to $\\|\\nabla^{2}m_{S}(\\mathbf{x})\\|\\leq\\Delta_{s}$ for any $\\mathbf{x}\\in\\mathbb{R}^{d}$ . If each $f_{i}$ is $L$ -smooth, then $\\Delta_{s}\\leq L$ for any $s\\in[n]$ . Therefore, using both Assumptions 1 and 4 is still weaker than assuming that each $f_{i}$ is $L$ -smooth. ", "page_idx": 6}, {"type": "text", "text": "In what follows, we work with a new second-order dissimilarity measure defined as the sum $\\delta_{s}+\\Delta_{s}$ .   \nNote that $\\delta_{1}+\\Delta_{1}=\\delta_{\\mathrm{max}}$ and $\\delta_{n}+\\Delta_{n}=\\delta$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 5. Consider Algorithm 1. Let $f_{i}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ be $\\mu$ -convex with $\\mu\\geq0$ for any $i\\in[n]$ and let $n\\geq2$ . Assume that $\\{f_{i}\\}_{i=1}^{n}$ have $\\delta_{s}$ -S $O D,\\,\\Delta_{s}.$ ED and $\\zeta$ -BGV. Let s4((nn\u2212\u22121s))\u03b6\u03b5 + 2(\u03b4s + \u2206s), and suppose that, for any $r\\geq0$ , we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{s}\\sum_{i\\in S_{r}}\\|\\nabla F_{i,r}(\\mathbf{x}_{i,r+1})\\|^{2}\\leq\\frac{\\lambda^{2}}{4}\\frac{1}{s}\\sum_{i\\in S_{r}}\\|\\mathbf{x}_{i,r+1}-\\mathbf{v}^{r}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then, to ensure that $\\mathbb{E}[f(\\bar{\\mathbf{x}}^{R})]-f(\\mathbf{x}^{\\star})\\le\\varepsilon$ for $a$ given $\\varepsilon>0$ , it suffices to perform at most the following number of communication rounds: ", "page_idx": 6}, {"type": "equation", "text": "$$\nR=\\Theta\\bigg(\\bigg[\\frac{\\delta_{s}+\\Delta_{s}+\\mu}{\\mu}+\\frac{n-s}{n-1}\\frac{\\zeta^{2}}{s\\varepsilon\\mu}\\bigg]\\log\\bigg(1+\\frac{\\mu D^{2}}{\\varepsilon}\\bigg)\\bigg)\\leq\\Theta\\bigg(\\frac{(\\delta_{s}+\\Delta_{s})D^{2}}{\\varepsilon}+\\frac{n-s}{n-1}\\frac{\\zeta^{2}D^{2}}{s\\varepsilon^{2}}\\bigg),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where R1 pr rR=1 prxr with p := 1 + \u03bb\u00b5, and D := \u2225x0 \u2212x\u22c6\u2225. ", "page_idx": 6}, {"type": "text", "text": "1: Input: $\\lambda>0$ , $\\mu\\geq0,\\mathbf{x}^{0}=\\mathbf{v}^{0}\\in\\mathbb{R}^{d},s\\in[n]$ .   \n2: Set $A_{0}=0$ , $B_{0}=1$ .   \n3: for $r=0,1,2,\\ldots$ do   \n4: Find $a_{r+1}>0$ from the equation $\\begin{array}{r}{\\lambda=\\frac{(A_{r}+a_{r+1})B_{r}}{a_{r+1}^{2}}}\\end{array}$ .   \n5: $\\begin{array}{r l}&{A_{r+1}=A_{r}+a_{r+1},B_{r+1}=B_{r}+\\mu a_{r+1}}\\\\ &{\\mathbf{y}^{r}=\\frac{A_{r}}{A_{r+1}}\\mathbf{x}^{r}+\\frac{a_{r+1}}{A_{r+1}}\\mathbf{v}^{r}.}\\end{array}$ .   \n6:   \n7: Sample $S_{r}\\,\\in\\,\\left(\\O_{s}^{[n]}\\right)$ uniformly at random without replacement.   \n8: for each device $i\\in S_{r}$ in parallel do   \n9: $\\begin{array}{r l}&{\\quad_{\\mathbf{X}_{i,r+1}}\\approx\\arg\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{d}}\\big\\{F_{i,r}(\\mathbf{x}):=f_{i}(\\mathbf{x})+\\langle\\nabla f_{S_{r}}(\\mathbf{y}^{r})-\\nabla f_{i}(\\mathbf{y}^{r}),\\mathbf{x}\\rangle+\\frac{\\lambda}{2}\\|\\mathbf{x}-\\mathbf{y}^{r}\\|^{2}\\big\\}.}\\\\ &{\\mathbf{x}^{r+1}=\\frac{1}{s}\\sum_{i\\in S_{r}}\\mathbf{x}_{i,r+1}.}\\\\ &{\\mathbf{v}^{r+1}=\\arg\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{d}}\\big\\{\\frac{a_{r+1}}{s}\\sum_{i\\in S_{r}}[\\langle\\nabla f_{i}(\\mathbf{x}_{i,r+1}),\\mathbf{x}\\rangle+\\frac{\\mu}{2}\\|\\mathbf{x}-\\mathbf{x}_{i,r+1}\\|^{2}]+\\frac{B_{r}}{2}\\|\\mathbf{x}-\\mathbf{v}^{r}\\|^{2}\\big\\}.}\\end{array}$   \n10:   \n11: ", "page_idx": 7}, {"type": "text", "text": "Theorem 5 provides the communication complexity of S-DANE with client sampling and arbitrary (deterministic) local solvers. The rate is again continuous in $\\mu$ . Compared with the previous case of $s=n$ , the efficiency now depends on the gradient variance $\\zeta$ . Note that this error term gets reduced when $s$ increases. Specifically, to achieve the $\\mathcal{O}(\\log\\frac{1}{\\varepsilon})$ and $\\mathcal{O}(\\frac{1}{\\varepsilon})$ rates, it suffices to ensure that $\\begin{array}{r}{s=\\Theta\\big(\\frac{n\\zeta^{2}}{\\zeta^{2}+n\\varepsilon(\\delta_{s}+\\Delta_{s})}\\big)}\\end{array}$ . Notably, the algorithm can reach any target accuracy even when $n\\to\\infty$ . Observe that the accuracy requirement (9) is the same as (6). Therefore, the discussions therein are valid in the partial-participation setting as well. In particular, if each $f_{i}$ is $L$ -smooth, then the number of oracle calls to $\\nabla f_{i}$ required at each round could be as small as $\\mathcal{O}(\\sqrt{\\frac{L}{\\lambda}})$ (see Corollary 3). At the same time, it is also possible to use a stochastic optimization algorithm as a local solver (for more details, see Section C.3.1). ", "page_idx": 7}, {"type": "text", "text": "4 Accelerated S-DANE ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we present the accelerated version of S-DANE, ACC-S-DANE (Alg. 2), that achieves a better communication complexity compared to the basic method. For simplicity, we only consider the full-participation setting and defer the partial participation to Appendix D.3. ", "page_idx": 7}, {"type": "text", "text": "Theorem 6. Consider Algorithm 2 with $s=n$ . Let $f_{i}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ be $\\mu$ -convex with $\\mu\\geq0$ for any $i\\in[n]$ . Assume that $\\{f_{i}\\}_{i=1}^{n}$ have $\\delta$ -SOD $(\\delta>0_{.}$ ). Let $\\lambda=2\\delta$ and suppose that, for any $r\\geq0$ , we have $\\begin{array}{r}{\\dot{\\sum_{i=1}^{n}}\\|\\nabla F_{i,r}(\\mathbf{x}_{i,r+1})\\|^{2}\\leq\\delta^{2}\\sum_{i=1}^{n}\\|\\mathbf{x}_{i,r+1}-\\mathbf{y}^{r}\\|^{2}.}\\end{array}$ If $\\mu\\leq8\\delta$ , then, for any $R\\geq1$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\nf(\\mathbf{x}^{R})-f^{\\star}\\le\\frac{2\\mu D^{2}}{\\big[\\big(1+\\sqrt{\\frac{\\mu}{8\\delta}}\\big)^{R}-\\big(1-\\sqrt{\\frac{\\mu}{8\\delta}}\\big)^{R}\\big]^{2}}\\le\\frac{4\\delta D^{2}}{R^{2}},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $D:=\\|\\mathbf{x}^{0}-\\mathbf{x}^{\\star}\\|$ . Otherwise, $\\begin{array}{r}{f(\\mathbf{x}^{R})-f^{\\star}\\le\\frac{4\\delta D^{2}}{(1+\\sqrt{\\frac{\\mu}{8\\delta}})^{2(R-1)}}}\\end{array}$ for any $R\\geq1$ . To ensure that $f(\\mathbf{x}^{R})-f^{\\star}\\leq\\varepsilon$ for a given $\\varepsilon>0$ , it thus suffices to perform $\\begin{array}{r}{R=\\mathcal{O}\\big(\\sqrt{\\frac{\\delta+\\mu}{\\mu}}\\log(1\\!+\\!\\sqrt{\\frac{\\operatorname*{min}\\{\\mu,\\delta\\}D^{2}}{\\varepsilon}})\\big)}\\end{array}$ communication rounds. ", "page_idx": 7}, {"type": "text", "text": "Let us consider the most interesting regime when $\\mu\\leq8\\delta$ . Comparing Theorems 1 and 6, we see that ACC-S-DANE essentially extracts the square root of the corresponding communication complexity of S-DANE by improving it from $\\tilde{\\mathcal{O}}(\\frac{\\delta}{\\mu})$ to $\\tilde{\\mathcal{O}}(\\sqrt{\\frac{\\delta}{\\mu}})$ when $\\mu>0$ , and from $\\mathcal{O}\\big(\\frac{\\delta D^{2}}{\\varepsilon}\\big)$ to $\\mathcal{O}(\\sqrt{\\frac{\\delta D^{2}}{\\varepsilon}})$ when $\\mu=0$ , while maintaining the same accuracy condition for solving the subproblem. Compared with ACC-EXTRAGRADIENT, the complexity depends on a better constant $\\delta$ instead of $\\delta_{\\mathrm{max}}$ . ", "page_idx": 7}, {"type": "text", "text": "Note that we can satisfy the accuracy condition in Theorem 6 in exactly the same way as in Corollary 3. In particular, if each $f_{i}$ is $L$ -smooth, each device $i$ needs at most $\\mathcal{O}(\\sqrt{\\frac{L}{\\delta}})$ computations of $\\nabla f_{i}$ at each round $r$ when using a fast algorithm for the gradient norm minimization. ", "page_idx": 7}, {"type": "text", "text": "Finally, let us highlight that Algorithm 2 gives a distributed framework for a generic acceleration scheme, that applies to a large class of local optimization methods\u2014in the same spirit as in the famous CATALYST [41] framework that applies to the case where $n=1$ . However, in contrast to CATALYST, this stabilized version removes the logarithmic overhead present in the original method. Specifically, when applying Theorem 6 with $n=1$ and $\\lambda=L$ for a smooth convex function $f$ , we recover the same rate as CATALYST. The accuracy condition $\\|\\nabla F_{r}(\\mathbf{x}^{r+1})\\|\\le L\\|\\mathbf{x}^{r+1}-\\mathbf{y}^{\\dot{r}}\\|$ , or equivalently $\\begin{array}{r}{\\langle\\nabla f({\\mathbf x}^{r+1}),{\\mathbf y}^{r}-{\\mathbf x}^{r+1}\\rangle\\ge\\frac{1}{2L}\\|\\nabla f(\\mathring{\\mathbf x}^{r+1})\\|^{2}}\\end{array}$ can be achieved with one gradient step $\\begin{array}{r}{\\mathbf{x}^{r+1}:=\\mathbf{y}^{r}-\\frac{1}{L}\\nabla f(\\mathbf{y}^{r})}\\end{array}$ (see Lemma 5 in [48]). ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5 Dynamic Estimation of Similarity Constant by Line Search ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "One drawback of Algorithms 1 and 2 is that they require the knowledge of the similarity constant $\\delta$ to choose an appropriate value for $\\lambda$ . This similarity constant is typically unknown in practice and might be difficult to estimate. One effective solution to this problem is to dynamically adjust the coefficient $\\lambda$ inside the algorithms by using the classical technique of line search. ", "page_idx": 8}, {"type": "text", "text": "The basic idea is as follows. The server first picks an arbitrary sufficiently small constant $\\tilde{\\lambda}$ as an initial approximation to the unknown \u201ccorrect\u201d value of $\\lambda=2\\delta$ . Then, at every round, the server sends the current estimate of $\\lambda$ to each client asking them to approximately solve their local subproblem. After receiving the corresponding local solutions, the server checks a certain inequality based on the obtained information. If this inequality is satisfied, the server accepts the resulting aggregated solution and goes to the next round while decreasing $\\lambda$ in two times (so as to be more optimistic in the future). Otherwise, it increases $\\lambda$ in two times, asks the clients to solve their subproblems with the new value of $\\lambda$ , and checks the inequality again. ", "page_idx": 8}, {"type": "text", "text": "The precise versions of Algorithms 1 and 2 with line search for the full-participation setting are presented in Algorithms 3 and 4. Importantly, our adaptive schemes are not just some heuristics but are probably efficient. Specifically, their complexity estimates (in terms of the total number of communication rounds) are exactly the same as those given by Theorems 1 and 6, respectively, up to an extra additive logarithmic term of $\\log{\\frac{2\\delta}{\\tilde{\\lambda}}}$ (see Theorems 27 and 28). ", "page_idx": 8}, {"type": "text", "text": "Another significant advantage of our adaptive algorithms is their ability to exploit local similarity, resulting in much stronger practical performance compared to the methods with fixed $\\lambda$ . We will demonstrate this in the next section. ", "page_idx": 8}, {"type": "text", "text": "6 Numerical Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we illustrate the performance of our methods in numerical experiments. The implementation can be found at https://github.com/mlolab/S-DANE. ", "page_idx": 8}, {"type": "text", "text": "Convex quadratic minimization. We first illustrate the properties of our algorithms as applied to minimizing a simple quadratic function: $\\begin{array}{r}{f(\\mathbf{x}):=\\frac{1}{n}\\sum_{i=1}^{n}\\dot{f}_{i}(\\mathbf{x})}\\end{array}$ where $\\begin{array}{r}{f_{i}(\\mathbf{x}):=\\frac{1}{m}\\sum_{j=1}^{m}\\frac{1}{2}\\langle\\mathbf{\\ddot{A}}_{i,j}(\\mathbf{x}-}\\end{array}$ $\\mathbf{b}_{i,j}),\\mathbf{x}-\\mathbf{b}_{i,j}\\rangle$ where $\\mathbf{b}_{i,j}\\ \\in\\ \\mathbb{R}^{d}$ and $\\mathbf{A}_{i,j}\\,\\in\\,\\mathbb{R}^{d\\times d}$ . The experimental details can be found in Appendix F.1. From Figure 1, we see that S-DANE converges as fast as DANE in terms of communication rounds, but with much fewer local gradient oracle calls. ACC-S-DANE achieves the best performance among the three methods. We also test S-DANE and DANE with the same fixed number of local steps. The result can be seen in Figure E.1 where S-DANE is again more efficient. Finally, we report the strong performances of two adaptive variants (Algorithms 3 and 4 with initial $\\lambda=10^{-3}$ ). We see from Figure 1 that the method can automatically change $\\lambda$ to adapt to the local second-order dissimilarity. (We use $\\frac{||\\nabla f(\\mathbf{v}^{r+1})-\\nabla f(\\mathbf{v}^{r})||}{||\\mathbf{v}^{r+1}-\\mathbf{v}^{r}||}$ and $\\begin{array}{r}{\\sqrt{\\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\|\\nabla h_{i}(\\mathbf{v}^{r+1})-\\nabla h_{i}(\\mathbf{v}^{r})\\|^{2}}{\\|\\mathbf{v}^{r+1}-\\mathbf{v}^{r}\\|^{2}}}}\\end{array}$ to approximate the local smoothness and dissimilarity.) ", "page_idx": 8}, {"type": "text", "text": "Strongly-convex polyhedron feasibility problem. We now consider the problem of finding a feasible point $\\mathbf{x}^{\\star}$ inside a polyhedron: $P=\\cap_{i=1}^{n}P_{i}$ , where $P_{i}\\,=\\,\\{{\\bf x}:\\langle{\\bf a}_{i,j},{\\bf x}\\rangle\\le{\\bf b}_{i,j},\\forall j\\,=\\,1,\\ldots,m_{i}\\}$ and $\\mathbf{a}_{i,j},\\mathbf{b}_{i,j}\\ \\in\\ \\mathbb{R}^{d}$ . Each individual function is defined as $\\begin{array}{r}{f_{i}\\;:=\\;\\frac{n}{m}\\sum_{j=1}^{m_{i}}[\\langle\\mathbf{a}_{i,j},\\mathbf{x}\\rangle-\\mathbf{b}_{i,j}]_{+}^{2}}\\end{array}$ where $\\textstyle\\sum_{i=1}^{n}m_{i}\\;=\\;m$ . We use $m\\:=\\:10^{5}$ and $d\\:=\\:10^{3}$ . We first generate $\\mathbf{x}^{\\star}$ randomly from a sphere with radius $10^{6}$ . We then follow [55] to generate $(\\mathbf{a}_{i,j},\\mathbf{b}_{i,j})$ such that $\\mathbf{x}^{\\star}$ is a feasible point of $P$ and the initial point of all optimizers is outside the polyhedron. We choose the best $\\lambda$ from $\\{10^{i}\\}_{i=-3}^{3}$ . We first consider the full client participation setting and use $n\\,=\\,s\\,=\\,10$ . We compare our proposed methods with GD, DANE with GD [57], SCAFFOLD with control variate of option I [26], SCAFFNEW [44], FEDPROX with GD [39] and ACC-EXTRAGRADIENT [33]. The result is shown in the first plot of Figure 2 where our proposed methods are consistently the best among all these algorithms. We next experiment with client sampling and use $n=100$ . We decrease the number of sampled clients from $s=80$ to $s=10$ . In addition to our methods, we also report the performances of SCAFFOLD and FEDPROX with client sampling. From the same figure, we see that the improvement of ACC-S-DANE over S-DANE gradually disappears as $s$ decreases. ", "page_idx": 8}, {"type": "image", "img_path": "WukSyFSzDt/tmp/8aecdda40c95d148b18fa00cd5c33a923073ac6009ecfe6a34ff56d26208098f.jpg", "img_caption": ["Figure 2: Comparisons of different algorithms for solving the polyhedron feasibility problem. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "WukSyFSzDt/tmp/20c0a1421d8e546c8eb4525ec70cd8f48024132b340fd8833fc02524ae59de7d.jpg", "img_caption": ["Figure 4: Illustration of the impact of adaptive $\\lambda$ used in (ACC-)S-DANE on the convergence of a regularized logistic regression problem on the ijcnn dataset [6]. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Adaptive choice of $\\lambda$ . We consider the standard regularized logistic regression: $\\begin{array}{r}{f(\\mathbf{x})=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}(\\mathbf{x})}\\end{array}$ with $\\begin{array}{r}{f_{i}(\\mathbf{x})\\;:=\\;\\frac{n}{M}\\sum_{j=1}^{m_{i}}\\log(1+\\exp(-y_{i,j}\\langle\\mathbf{a}_{i,j},\\mathbf{x}\\rangle))+\\frac{1}{2M}\\|\\mathbf{x}\\|^{2}}\\end{array}$ where $(\\mathbf{a}_{i,j},y_{i,j})\\ \\in\\ \\mathbb{R}^{d+1}$ are features and labels and $\\textstyle M:=\\sum_{i=1}^{n}m_{i}$ is the total number of data points in the training dataset. We use the ijcnn dataset from LIBSVM [6]. We split the dataset into 10 subsets according to the Dirichlet distribution with $\\alpha=2$ (i.i.d) and $\\alpha=0.2$ (non-i.i.d). From Figure 4, Adaptive (ACC-)S-DANE (Algorithm 3 and 4) converge much faster than the other best-tuned algorithms for both cases. (We set the initial $\\lambda=10^{-4}$ for non-i.i.d and $\\lambda=10^{-5}$ for i.i.d respectively.) ", "page_idx": 9}, {"type": "text", "text": "Deep learning task. Finally, we consider the multi-class classification tasks with CIFAR10 [34] using ResNet-18 [18]. The details can be found in Appendix F.2. From Figure 3, we see that S-DANE (DL) 5 reaches $9\\bar{0}\\%$ accuracy within 50 communication rounds while all the other methods are still below $90\\%$ after 80 epochs. The effectiveness of S-DANE on the training of other deep learning models such as Transformer requires further exploration. ", "page_idx": 9}, {"type": "image", "img_path": "WukSyFSzDt/tmp/cdb68b0e9d278a727b0a1008cc0c5fbafbc5e4103a428b75213946c18dcc1cb2.jpg", "img_caption": ["Figure 3: Comparison of S-DANE without control variate against other popular optimizers on multi-class classification tasks with CIFAR10 datasets using ResNet18. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have proposed new federated optimization methods (both basic and accelerated) that simultaneously achieve the best-known communication and local computation complexities. The new methods allow partial participation and arbitrary stochastic local solvers, making them attractive in practice. We further equip both algorithms with line search and the resulting schemes can adapt to the local dissimilarity without knowing the corresponding similarity constant. However, we assume that each function $f_{i}$ is $\\mu$ -strongly convex in all the theorems. This is stronger than assuming only $\\mu$ -strongly convexity of $f$ , which is used in some prior works. Possible directions for future research include consideration of weaker assumptions as well as empirical and theoretical analyses for non-convex problems. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors are grateful to Adrien Taylor and Thomas Pethick for the reference to [59]. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul Whatmough, and Venkatesh Saligrama. Federated learning based on dynamic regularization. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=B7v4QMR6Z9w.   \n[2] Aleksandr Beznosikov, Martin Tak\u00e1\u02c7c, and Alexander Gasnikov. Similarity, compression and local steps: Three pillars of efficient communications for distributed variational inequalities. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id= Rvk1wdwz1L.   \n[3] L\u00e9on Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM review, 60(2):223\u2013311, 2018. [4] S\u00e9bastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends\u00ae in Machine Learning, 8(3-4):231\u2013357, 2015. [5] Yair Carmon, Arun Jambulapati, Yujia Jin, and Aaron Sidford. Recapp: Crafting a more efficient catalyst for convex optimization. In International Conference on Machine Learning, pages 2658\u20132685. PMLR, 2022.   \n[6] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1\u201327:27, 2011. Software available at http: //www.csie.ntu.edu.tw/\\~cjlin/libsvm.   \n[7] El Mahdi Chayti and Sai Praneeth Karimireddy. Optimization with access to auxiliary information. arXiv preprint arXiv:2206.00395, 2022.   \n[8] Laurent Condat and Peter Richt\u00e1rik. Randprox: Primal-dual optimization algorithms with randomized proximal updates. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=cB4N3G5udUS.   \n[9] Laurent Condat, Grigory Malinovsky, and Peter Richt\u00e1rik. Tamuna: Accelerated federated learning with local training and partial participation. arXiv preprint arXiv:2302.09832, 2023.   \n[10] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. Advances in neural information processing systems, 27, 2014.   \n[11] Olivier Devolder. Stochastic first order methods in smooth convex optimization. CORE Discussion Papers, 2011/70, 2011.   \n[12] Nikita Doikov and Yurii Nesterov. Contracting proximal methods for smooth convex optimization. SIAM Journal on Optimization, 30(4):3146\u20133169, 2020.   \n[13] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(61):2121\u20132159, 2011. URL http: //jmlr.org/papers/v12/duchi11a.html.   \n[14] Alexandre d\u2019Aspremont, Damien Scieur, Adrien Taylor, et al. Acceleration methods. Foundations and Trends\u00ae in Optimization, 5(1-2):1\u2013245, 2021.   \n[15] Liang Gao, Huazhu Fu, Li Li, Yingwen Chen, Ming Xu, and Cheng-Zhong Xu. Feddc: Federated learning with non-iid data via local drift decoupling and correction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10112\u201310121, 2022.   \n[16] Micha\u0142 Grudzie\u00b4n, Grigory Malinovsky, and Peter Richt\u00e1rik. Can 5th generation local training methods support client sampling? yes! In International Conference on Artificial Intelligence and Statistics, pages 1055\u20131092. PMLR, 2023.   \n[17] Osman G\u00fcler. New proximal point algorithms for convex minimization. SIAM Journal on Optimization, 2 (4):649\u2013664, 1992. doi: 10.1137/0802032. URL https://doi.org/10.1137/0802032.   \n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770\u2013778, 2016. doi: 10.1109/CVPR.2016.90.   \n[19] Hadrien Hendrikx, Lin Xiao, Sebastien Bubeck, Francis Bach, and Laurent Massoulie. Statistically preconditioned accelerated gradient method for distributed optimization. In International conference on machine learning, pages 4203\u20134227. PMLR, 2020.   \n[20] Zhengmian Hu and Heng Huang. Tighter analysis for ProxSkip. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 13469\u201313496. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/ hu23a.html.   \n[21] Anastasiya Ivanova, Dmitry Pasechnyuk, Dmitry Grishchenko, Egor Shulgin, Alexander Gasnikov, and Vladislav Matyukhin. Adaptive catalyst for smooth convex optimization. In International Conference on Optimization and Applications, pages 20\u201337. Springer, 2021.   \n[22] Xiaowen Jiang, Anton Rodomanov, and Sebastian U Stich. Federated optimization with doubly regularized drift correction. arXiv preprint arXiv:2404.08447, 2024.   \n[23] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 1, NIPS\u201913, page 315\u2013323. Curran Associates Inc., 2013.   \n[24] Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D\u2019Oliveira, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adri\u00e0 Gasc\u00f3n, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Kone\u02c7cn\u00fd, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancr\u00e8de Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer \u00d6zg\u00fcr, Rasmus Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tram\u00e8r, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances and open problems in federated learning. Foundations and Trends\u00ae in Machine Learning, 14 (1\u20132):1\u2013210, 2021. URL https://arxiv.org/pdf/1912.04977.pdf.   \n[25] Avetik Karagulyan, Egor Shulgin, Abdurakhmon Sadiev, and Peter Richt\u00e1rik. Spam: Stochastic proximal point method with momentum variance reduction for non-convex cross-device federated learning. arXiv preprint arXiv:2405.20127, 2024.   \n[26] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian U. Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International conference on machine learning, pages 5132\u20135143. PMLR, 2020.   \n[27] Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian U. Stich, and Ananda Theertha Suresh. Breaking the centralized barrier for cross-device federated learning. In Advances in Neural Information Processing Systems, 2021.   \n[28] Ahmed Khaled and Chi Jin. Faster federated optimization under second-order similarity. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/ forum?id=ElC6LYO4MfD.   \n[29] Ahmed Khaled, Konstantin Mishchenko, and Peter Richt\u00e1rik. First analysis of local gd on heterogeneous data. arXiv preprint arXiv:1909.04715, 2019.   \n[30] Donghwan Kim and Jeffrey A Fessler. Generalizing the optimized gradient method for smooth convex minimization. SIAM Journal on Optimization, 28(2):1920\u20131950, 2018.   \n[31] Jakub Kone\u02c7cn\\`y, H Brendan McMahan, Daniel Ramage, and Peter Richt\u00e1rik. Federated optimization: Distributed machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527, 2016.   \n[32] Jakub Kone\u02c7cn\\`y, H Brendan McMahan, Felix X Yu, Peter Richt\u00e1rik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492, 2016.   \n[33] Dmitry Kovalev, Aleksandr Beznosikov, Ekaterina Borodich, Alexander Gasnikov, and Gesualdo Scutari. Optimal gradient sliding and its application to optimal distributed optimization under similarity. Advances in Neural Information Processing Systems, 35:33494\u201333507, 2022.   \n[34] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research). URL http://www.cs.toronto.edu/\\~kriz/cifar.html.   \n[35] Guanghui Lan. Gradient sliding for composite optimization. Mathematical Programming, 159:201\u2013235, 2016.   \n[36] Guanghui Lan and Yuyuan Ouyang. Accelerated gradient sliding for structured convex optimization. arXiv preprint arXiv:1609.04905, 2016.   \n[37] Guanghui Lan, Yuyuan Ouyang, and Zhe Zhang. Optimal and parameter-free gradient minimization methods for smooth optimization. arXiv preprint arXiv:2310.12139, 2023.   \n[38] Bo Li, Mikkel N Schmidt, Tommy S Alstr\u00f8m, and Sebastian U Stich. On the effectiveness of partial variance reduction in federated learning with heterogeneous data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3964\u20133973, 2023.   \n[39] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine learning and systems, 2:429\u2013450, 2020.   \n[40] Dachao Lin, Yuze Han, Haishan Ye, and Zhihua Zhang. Stochastic distributed optimization under average second-order similarity: Algorithms and analysis. Advances in Neural Information Processing Systems, 36, 2024.   \n[41] Hongzhou Lin, Julien Mairal, and Zaid Harchaoui. A universal catalyst for first-order optimization. Advances in neural information processing systems, 28, 2015.   \n[42] B. Martinet. Perturbation des m\u00e9thodes d\u2019optimisation. Applications. RAIRO. Analyse num\u00e9rique, 12(2): 153\u2013171, 1978. URL http://www.numdam.org/item/M2AN_1978__12_2_153_0/.   \n[43] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pages 1273\u20131282. PMLR, 2017.   \n[44] Konstantin Mishchenko, Grigory Malinovsky, Sebastian Stich, and Peter Richt\u00e1rik. Proxskip: Yes! local gradient steps provably lead to communication acceleration! finally! In International Conference on Machine Learning, pages 15750\u201315769. PMLR, 2022.   \n[45] Konstantin Mishchenko, Rui Li, Hongxiang Fan, and Stylianos Venieris. Federated learning under second-order data heterogeneity, 2024. URL https://openreview.net/forum?id=jkhVrIllKg.   \n[46] Renato D. C. Monteiro and B. F. Svaiter. An accelerated hybrid proximal extragradient method for convex optimization and its implications to second-order methods. SIAM Journal on Optimization, 23(2): 1092\u20131125, 2013. doi: 10.1137/110833786. URL https://doi.org/10.1137/110833786.   \n[47] J.J. Moreau. Proximit\u00e9 et dualit\u00e9 dans un espace hilbertien. Bulletin de la Soci\u00e9t\u00e9 Math\u00e9matique de France, 93:273\u2013299, 1965. doi: 10.24033/bsmf.1625. URL http://www.numdam.org/articles/10.24033/ bsmf.1625/.   \n[48] Yu. Nesterov. Gradient methods for minimizing composite functions. Mathematical Programming, 140 (1):125\u2013161, 2013. doi: 10.1007/s10107-012-0629-5. URL https://doi.org/10.1007/s10107-012- 0629-5.   \n[49] Yurii Nesterov. Lectures on Convex Optimization. Springer Publishing Company, Incorporated, 2nd edition, 2018. ISBN 3319915770.   \n[50] Yurii Nesterov and Sebastian U. Stich. Efficiency of the accelerated coordinate descent method on structured optimization problems. SIAM Journal on Optimization, 27(1):110\u2013123, 2017. doi: 10.1137/16M1060182. URL https://doi.org/10.1137/16M1060182.   \n[51] Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends\u00ae in Optimization, 1(3): 127\u2013239, 2014. ISSN 2167-3888. doi: 10.1561/2400000003. URL http://dx.doi.org/10.1561/ 2400000003.   \n[52] Kumar Kshitij Patel, Lingxiao Wang, Blake E Woodworth, Brian Bullins, and Nati Srebro. Towards optimal communication complexity in distributed non-convex optimization. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 13316\u201313328. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 56bd21259e28ebdc4d7e1503733bf421-Paper-Conference.pdf.   \n[53] Sashank J Reddi, Jakub Kone\u02c7cn\\`y, Peter Richt\u00e1rik, Barnab\u00e1s P\u00f3cz\u00f3s, and Alex Smola. Aide: Fast and communication efficient distributed optimization. arXiv preprint arXiv:1608.06879, 2016.   \n[54] R Tyrrell Rockafellar. Monotone operators and the proximal point algorithm. SIAM journal on control and optimization, 14(5):877\u2013898, 1976.   \n[55] Anton Rodomanov, Xiaowen Jiang, and Sebastian Stich. Universality of adagrad stepsizes for stochastic optimization: Inexact oracle, acceleration and variance reduction. arXiv preprint arXiv:2406.06398, 2024.   \n[56] Abdurakhmon Sadiev, Dmitry Kovalev, and Peter Richt\u00e1rik. Communication acceleration of local gradient methods via an accelerated primal-dual algorithm with an inexact prox. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=W72rB0wwLVu.   \n[57] Ohad Shamir, Nati Srebro, and Tong Zhang. Communication-efficient distributed optimization using an approximate newton-type method. In International conference on machine learning, pages 1000\u20131008. PMLR, 2014.   \n[58] M.V. Solodov and B. F. Svaiter. A unified framework for some inexact proximal point algorithms\\*. Numerical Functional Analysis and Optimization, 22(7-8):1013\u20131035, 2001. doi: 10.1081/NFA-100108320. URL https://doi.org/10.1081/NFA-100108320.   \n[59] M.V. Solodov and B.F. Svaiter. A hybrid projection-proximal point algorithm. Journal of Convex Analysis, 6(1):59\u201370, 1999. URL http://eudml.org/doc/120958.   \n[60] Ying Sun, Gesualdo Scutari, and Amir Daneshmand. Distributed optimization based on gradient tracking revisited: Enhancing convergence rate via surrogation. SIAM Journal on Optimization, 32(2):354\u2013385, 2022.   \n[61] Ye Tian, Gesualdo Scutari, Tianyu Cao, and Alexander Gasnikov. Acceleration in distributed optimization under similarity. In International Conference on Artificial Intelligence and Statistics, pages 5721\u20135756. PMLR, 2022.   \n[62] Farshid Varno, Marzie Saghayi, Laya Rafiee Sevyeri, Sharut Gupta, Stan Matwin, and Mohammad Havaei. Adabest: Minimizing client drift in federated learning via adaptive bias estimation. In European Conference on Computer Vision, pages 710\u2013726. Springer, 2022.   \n[63] Blake E Woodworth, Kumar Kshitij Patel, and Nati Srebro. Minibatch vs local sgd for heterogeneous distributed learning. Advances in Neural Information Processing Systems, 33:6281\u20136292, 2020.   \n[64] Honglin Yuan and Tengyu Ma. Federated accelerated stochastic gradient descent. Advances in Neural Information Processing Systems, 33:5332\u20135344, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1 Introduction 1 ", "page_idx": 14}, {"type": "text", "text": "2 Problem Setup and Background 4   \nProximal-Point Methods on Single Machine 4   \n2.2 Distributed Proximal-Point Methods 5   \n3 Stabilized DANE 5   \n4 Accelerated S-DANE 8   \n5 Dynamic Estimation of Similarity Constant by Line Search 9   \n6 Numerical Experiments 9   \n7 Conclusion 10   \nA More Related Work 16   \nB Technical Preliminaries 16   \nB.1 Basic Definitions 16   \nB.2 Useful Lemmas 17   \nC Proofs for S-DANE (Algorithm 1) 20   \nC.1 One-Step Recurrence 20   \nC.2 Full Client Participation (Proof of Theorem 1) 21   \nC.3 Partial Client Participation (Proof of Theorem 5) 22   \nC.3.1 Stochastic Local Solver . 23   \nD Proofs for Accelerated S-DANE (Algorithm 2) 25   \nD.1 One-Step Recurrence 25   \nD.2 Full Client Participation (Proof of Theorem 6) 26   \nD.3 Partial Client Participation . 27   \nD.3.1 Stochastic Local Solver 28   \nE Dynamic Estimation of Similarity Constant by Line Search 28   \nF Additional Details on Experiments 31   \nF.1 Convex Quadratics . 31   \nF.2 Deep Learning Tasks 31   \nF.3 Implementation 31   \nG Impact Statement 32 ", "page_idx": 14}, {"type": "text", "text": "A More Related Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the first several years of the development for federated learning algorithms, the convergence guarantees are focused on the smoothness parameter $L$ . The de facto standard algorithm for federated learning is FEDAVG. It reduces the communication frequency by doing multiple SGD steps on available clients before communication, which works well in practice [43]. However, in theory, if the heterogeneity among clients is large, then it suffers from the so-called client drift phenomenon [26] and might be worse than centralized mini-batch SGD [63, 29]. Numerous efforts have been made to mitigate this drift impact. FEDPROX adds an additional regularizer to the subproblem of each client based on the idea of centralized proximal point method to limit the drift of each client. However, the communication complexity still depends on the heterogeneity. The celebrated algorithm SCAFFOLD applies drift correction (similar to variance-reduction) to the update of FEDAVG and it successfully removes the impact of the heterogeneity. Afterwards, the idea of drift correction is employed in many other works [15, 62, 38, 1]. SCAFFNEW uses a special choice of control variate[44] and first illustrates the usefulness of taking standard local gradient steps under strongy-convexity, followed with more advanced methods with refined analysis and features such as client sampling and compression [20, 9, 8]. 5gCS [16] uses an approximate proximal-point step at each iteration and derives the convergence rate that is as good as SCAFFNEW, and it also supports client sampling. Later, Sadiev et al. [56] proposed APDA WITH INEXACT PROX that retains the same communication complexity as SCAFFNEW, but further provably reduces the local computation complexity. FedAC [64] applies nesterov\u2019s acceleration in the local steps and shows provably better convergence than FEDAVG under certain assumptions. ", "page_idx": 15}, {"type": "text", "text": "More recent works try to develop algorithms with guarantees that rely on a potentially smaller constant than $L$ . SCAFFOLD first illustrates the usefulness of taking local steps for quadratics under Bounded Hessian Dissimilarity $\\delta_{\\mathrm{max}}$ [26]. SONATA [60] and its accelerated version [61] prove explicit communication reduction in terms of $\\delta_{\\mathrm{max}}$ under strong convexity. MIME [27] and CE-LGD [52] work on non-convex settings and show the communication improvement on $\\delta_{\\mathrm{max}}$ and the latter achieves the min-max optimal rates. ACCELERATED EXTRAGRADIENT SLIDING [33] applies gradient sliding [33] technique and shows communication reduction in terms of $\\delta_{\\mathrm{max}}$ for strongly-convex and convex functions, the local computation of which is also efficient without logarithmic dependence on the target accuracy, DANE with inexact local solvers [57, 22, 53] has been shown recently to achieve the communication dependency on $\\delta$ under convexity and $\\delta_{\\mathrm{max}}$ under non-convexity. For solving convex problems, the local computation efficiency depends on the target accuracy $\\varepsilon$ . Otherwise, the accuracy condition for the subproblem should increase across the communication rounds. Hendrikx et al. [19] proposed SPAG, an accelerated method, and prove a better uniform concentration bound of the conditioning number when solving strongly-convex problems. SVRP and CATALYZED SVRP [28] transfer the idea of using the centralized proximal point method to the distributed setting and they achieve communication complexity (with a different notion) w.r.t $\\delta$ . Lin et al. [40] further improves these two methods either with a better rate or with weaker assumptions based on the ACCELERATED EXTRAGRADIENT SLIDING method. Beznosikov et al. [2] uses compression to reduce the bits required to communicate and the more general problem of Variational Inequalities is considered. Under the same settings but for non-convex optimization, SABER [45] achieves communication complexity reduction with better dependency on $\\delta_{\\mathrm{max}}$ and $_n$ . Karagulyan et al. [25] proposed SPAM that allows partial client particiption. ", "page_idx": 15}, {"type": "text", "text": "Remark 7. Khaled and Jin [28] and Lin et al. [40] consider the total amount of information transmitted between the server and clients as the main metric, which is similar to reducing the total stochastic oracle calls in centralized learning settings. This is a particularly meaningful setting if the server prefers to or has to receive/transmit vectors one by one and can set up communications very fast. The term \u2019client sampling\u2019 in these works refers to sampling one client to do the local computation. However, all the clients still need to participate in the communication from time to time to provide the full gradient information. This is orthogonal to the setup of this work since we assume each device can do the calculation in parallel. In the scenarios where the number of devices is too large such that receiving all the updates becomes problematic, we consider instead the standard partial participation setting. ", "page_idx": 15}, {"type": "text", "text": "B Technical Preliminaries ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Basic Definitions ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We use the following definitions throughout the paper. ", "page_idx": 15}, {"type": "text", "text": "Definition 5. A differentiable function $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is called $\\mu$ -convex for some $\\mu\\geq0$ if for all $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(\\mathbf{y})\\geq f(\\mathbf{x})+\\langle\\nabla f(\\mathbf{x}),\\mathbf{y}-\\mathbf{x}\\rangle+{\\frac{\\mu}{2}}\\|\\mathbf{x}-\\mathbf{y}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "If $f$ is $\\mu$ -convex, then, for any $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d}$ , we have (Nesterov [49], Theorem 2.1.10): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu\\|\\mathbf{x}-\\mathbf{y}\\|\\leq\\|\\nabla f(\\mathbf{x})-\\nabla f(\\mathbf{y})\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Definition 6. A differentiable function $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is called $L$ -smooth for some $L\\geq0$ if for all $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\nabla f(\\mathbf{x})-\\nabla f(\\mathbf{y})\\|\\leq L\\|\\mathbf{x}-\\mathbf{y}\\|.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "If $f$ is $L$ -smooth, then, for any $\\mathbf x,\\mathbf y\\in\\mathbb R^{d}$ , we have (Nesterov [49], Lemma 1.2.3) ", "page_idx": 16}, {"type": "equation", "text": "$$\nf(\\mathbf{y})\\leq f(\\mathbf{x})+\\langle\\nabla f(\\mathbf{x}),\\mathbf{y}-\\mathbf{x}\\rangle+\\frac{L}{2}\\|\\mathbf{y}-\\mathbf{x}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma 8 (Nesterov [49], Theorem 2.1.5). Let $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ be convex and $L$ -smooth. Then, for any $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{L}\\|\\nabla f(\\mathbf{x})-\\nabla f(\\mathbf{y})\\|^{2}\\leq\\langle\\nabla f(\\mathbf{x})-\\nabla f(\\mathbf{y}),\\mathbf{x}-\\mathbf{y}\\rangle.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.2 Useful Lemmas ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We frequently use the following helpful lemmas for the proofs. ", "page_idx": 16}, {"type": "text", "text": "Lemma 9. For any $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d}$ and any $\\gamma>0$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{|\\langle\\mathbf{x},\\mathbf{y}\\rangle|\\leq\\displaystyle\\frac{\\gamma}{2}\\|\\mathbf{x}\\|^{2}+\\displaystyle\\frac{1}{2\\gamma}\\|\\mathbf{y}\\|^{2},}\\\\ {\\|\\mathbf{x}+\\mathbf{y}\\|^{2}\\leq(1+\\gamma)\\|\\mathbf{x}\\|^{2}+\\Big(1+\\displaystyle\\frac{1}{\\gamma}\\Big)\\|\\mathbf{y}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma 10 (Jiang et al. [22], Lemma 14). Let $\\{x_{i}\\}_{i=1}^{n}$ be a set of vectors in $\\mathbb{R}^{d}$ and let $\\begin{array}{r}{\\bar{\\mathbf{x}}:=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{x}_{i}}\\end{array}$ . Let $\\mathbf{v}\\in\\mathbb{R}^{d}$ be an arbitrary vector. Then, ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\frac{1}{n}}\\sum_{i=1}^{n}\\lVert\\mathbf{x}_{i}-\\mathbf{v}\\rVert^{2}\\ =\\left\\lVert{\\bar{\\mathbf{x}}}-\\mathbf{v}\\right\\rVert^{2}+{\\frac{1}{n}}\\sum_{i=1}^{n}\\lVert\\mathbf{x}_{i}-{\\bar{\\mathbf{x}}}\\rVert^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma 11. Let $(F_{k})_{k=1}^{\\infty}$ and $(D_{k})_{k=0}^{\\infty}$ be two non-negative sequences such that, for any $k\\geq0$ , it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\nF_{k+1}+D_{k+1}\\leq q D_{k}+\\varepsilon,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $q\\in(0,1]$ and $\\varepsilon\\ge0$ are some constants. Then for all $K\\geq1$ and $\\begin{array}{r}{S_{K}:=\\sum_{k=1}^{K}\\frac{1}{q^{k}}}\\end{array}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{S_{K}}\\sum_{k=1}^{K}\\frac{F_{k}}{q^{k}}+\\frac{1-q}{1-q^{K}}D_{K}\\leq\\frac{1-q}{\\frac{1}{q^{K}}-1}D_{0}+\\varepsilon.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Indeed, for any $k\\geq0$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\frac{F_{k+1}}{q^{k+1}}}+{\\frac{D_{k+1}}{q^{k+1}}}\\leq{\\frac{D_{k}}{q^{k}}}+{\\frac{\\varepsilon}{q^{k+1}}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Summing up from $k=0$ to $k=K-1$ , we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\frac{{\\cal F}_{k}}{q^{k}}+\\frac{{\\cal D}_{K}}{q^{K}}\\le{\\cal D}_{0}+{\\cal S}_{K}\\varepsilon.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Dividing both sides by $S_{K}$ and substituting $\\begin{array}{r}{S_{K}=\\frac{1}{1-q}\\big(\\frac{1}{q^{K}}-1\\big)}\\end{array}$ , we get the claim. ", "page_idx": 16}, {"type": "text", "text": "Lemma 12 (c.f. Lemma 2.2.4 in [49]). Let $(A_{r})_{r=0}^{\\infty}$ be a non-negative non-decreasing sequence such that $A_{0}=0$ and, for any $r\\geq0$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\nA_{r+1}\\leq\\frac{c(A_{r+1}-A_{r})^{2}}{1+\\mu A_{r}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $c>0$ and $\\mu\\geq0$ are some constants. If $\\mu\\leq4c,$ , then for any $R\\geq0$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nA_{R}\\geq\\frac{1}{4\\mu}\\left[\\left(1+\\sqrt{\\frac{\\mu}{4c}}\\right)^{R}-\\left(1-\\sqrt{\\frac{\\mu}{4c}}\\right)^{R}\\right]^{2}\\geq\\frac{R^{2}}{4c}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Otherwise, for any $R\\geq1$ , it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\nA_{R}\\geq\\frac{1}{4c}\\bigg(1+\\sqrt{\\frac{\\mu}{4c}}\\bigg)^{2(R-1)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Denote $C_{r}=\\sqrt{\\mu A_{r}}$ . For any $r\\geq0$ , it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mu C_{r+1}^{2}(1+C_{r}^{2})\\leq c(C_{r+1}^{2}-C_{r}^{2})^{2}\\leq c\\bigl(2(C_{r+1}-C_{r})C_{r+1}\\bigr)^{2}=4c(C_{r+1}-C_{r})^{2}C_{r+1}^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, for any $r\\geq0$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\nC_{r+1}-C_{r}\\geq\\sqrt{\\frac{\\mu}{4c}}\\sqrt{1+C_{r}^{2}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "When $\\mu\\leq4c$ , by induction, one can show that, for any $R\\ge0$ (see the proof of Theorem 1 in [50] for details): ", "page_idx": 17}, {"type": "equation", "text": "$$\nC_{R}\\geq\\frac{1}{2}\\left[\\left(1+\\sqrt{\\frac{\\mu}{4c}}\\right)^{R}-\\left(1-\\sqrt{\\frac{\\mu}{4c}}\\right)^{R}\\right]\\geq\\sqrt{\\frac{\\mu}{4c}}R.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "When $\\mu>4c$ , we have $\\begin{array}{r}{C_{r+1}-C_{r}\\geq\\sqrt{\\frac{\\mu}{4c}}C_{r}}\\end{array}$ . It follows that, for any $R\\geq1$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\nC_{R}\\geq\\Big(1+\\sqrt{\\frac{\\mu}{4c}}\\Big)^{R-1}C_{1}\\geq\\Big(1+\\sqrt{\\frac{\\mu}{4c}}\\Big)^{R-1}\\sqrt{\\frac{\\mu}{4c}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Plugging in the definition of $C_{R}$ , we get the claims. ", "page_idx": 17}, {"type": "text", "text": "Lemma 13. Let $\\{{\\bf x}_{i}\\}_{i=1}^{n}$ be vectors in $\\mathbb{R}^{d}$ with $n\\geq2$ . Let $s\\in[n]$ and let $S\\in\\left({n\\atop s}\\right)$ be sampled uniformly at random without replacement. Let $\\begin{array}{r}{\\bar{\\mathbf{x}}:=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{x}_{i},\\,\\zeta^{2}:=\\frac{1}{n}\\sum_{i=1}^{n}\\|\\mathbf{x}_{i}-\\bar{\\mathbf{x}}\\|^{2},}\\end{array}$ , and $\\begin{array}{r}{\\dot{\\bf x}_{S}:=\\frac{1}{s}\\sum_{j\\in S}{\\bf x}_{j}}\\end{array}$ . Then, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\bar{\\mathbf{x}}s]=\\bar{\\mathbf{x}}\\qquad a n d\\qquad\\mathbb{E}[\\|\\bar{\\mathbf{x}}s-\\bar{\\mathbf{x}}\\|^{2}]=\\frac{n-s}{n-1}\\frac{\\zeta^{2}}{s}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Let mn $\\textstyle{\\binom{n}{m}}={\\frac{n!}{m!(n-m)!}}$ be the binomial coefficient for any $n\\geq m\\geq1$ . By the definition of $\\bar{\\bf x}_{S}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{x}}_{S}=\\frac{1}{s}\\sum_{j\\in S}\\mathbf{x}_{j}=\\frac{1}{s}\\sum_{i=1}^{n}\\mathbb{1}[i\\in S]\\mathbf{x}_{i},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathbb{1}[E]$ denotes the $\\{0,1\\}$ -indicator of the event $E$ . Taking the expectation on both sides, we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\bar{\\mathbf{x}}s]=\\frac{1}{s}\\sum_{i=1}^{n}\\operatorname*{Pr}\\left[i\\in S\\right]\\mathbf{x}_{i}=\\frac{1}{s}\\sum_{i=1}^{n}\\frac{\\binom{n-1}{s-1}}{\\binom{n}{s}}\\mathbf{x}_{i}=\\frac{1}{s}\\sum_{i=1}^{n}\\frac{s}{n}\\mathbf{x}_{i}=\\bar{\\mathbf{x}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Further, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\|\\mathbf{s}_{S}-\\mathbf{x}\\|^{2}\\Big]=\\mathbb{E}\\bigg[\\frac{1}{S}\\sum_{i\\in\\mathcal{S}}\\sum_{\\ell\\in\\mathcal{S}_{i}}(\\mathbf{x}_{i}-\\mathbf{\\bar{x}}_{S},\\mathbf{x}_{j}-\\mathbf{\\bar{x}})\\bigg]}\\\\ &{\\qquad=\\mathbb{E}\\bigg[\\frac{1}{S^{2}}\\sum_{i\\in\\mathcal{S}}\\|\\mathbf{x}_{i}-\\mathbf{x}\\|^{2}+\\frac{1}{S^{2}}\\sum_{i,j\\in\\mathcal{S}_{i}\\setminus\\mathcal{H}(\\mathcal{S}_{i}-\\mathcal{S}_{i},\\mathbf{x}_{j}-\\mathcal{S})}(\\mathbf{x}_{i}-\\mathbf{\\bar{x}},\\mathbf{x}_{j}-\\mathbf{\\bar{x}})\\bigg]}\\\\ &{\\qquad=\\mathbb{E}\\bigg[\\frac{1}{S}\\sum_{i=1}^{S}\\mathbf{1}\\{i\\in S\\}\\|\\mathbf{x}_{i}-\\bar{\\mathbf{x}}\\|^{2}+\\frac{1}{S^{2}}\\sum_{i,j\\in\\mathcal{S}_{i}\\cap\\mathcal{H}(\\mathcal{S}_{i},\\ell)}\\mathbf{1}\\{i,j\\in S\\}\\big\\vert\\mathbf{x}_{i}-\\bar{\\mathbf{x}},\\mathbf{x}_{j}-\\bar{\\mathbf{x}}\\big\\}\\bigg]}\\\\ &{\\qquad=\\frac{1}{S^{2}}\\sum_{i=1}^{S}\\mathbf{r}\\left[i\\in S\\right]\\|\\mathbf{x}_{i}-S\\|^{2}+\\frac{1}{S^{2}}\\sum_{i,j\\in\\mathcal{S}_{i}\\cap\\mathcal{H}(\\mathcal{S}_{i},\\ell)}P_{\\mathbb{r}}\\big[i,j\\in S\\big\\vert\\mathbf{x}_{i}-\\bar{\\mathbf{x}},\\mathbf{x}_{j}-\\bar{\\mathbf{x}}\\big]}\\\\ &{\\qquad=\\frac{1}{S^{2}}\\sum_{i=1}^{S}\\frac{\\binom{S-1}{S}}{\\binom{S-1}{S}}\\|\\mathbf{x}_{i}-\\bar{\\mathbf{x}}\\|^{2}+\\frac{1}{S^{2}}\\sum_{i,j\\in\\mathcal{S}_{i}\\cap\\mathcal{H}(\\mathcal{S}_{i},\\ell)}\\frac{\\binom{S-2}{S}}{\\binom{S-2}{S}}\\big(\\mathbf{x}_{i}-\\bar{\\mathbf{x}},\\mathbf{x}_{j}-\\bar{\\mathbf{x}}\\big)}\\\\ &{\\\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{\\substack{i,j\\in[n],\\,i\\neq j}}\\langle\\mathbf{x}_{i}-\\bar{\\mathbf{x}},\\mathbf{x}_{j}-\\bar{\\mathbf{x}}\\rangle=\\sum_{\\substack{i,j\\in[n]}}\\langle\\mathbf{x}_{i}-\\bar{\\mathbf{x}},\\mathbf{x}_{j}-\\bar{\\mathbf{x}}\\rangle-\\sum_{i=1}^{n}\\lVert\\mathbf{x}_{i}-\\mathbf{x}\\rVert^{2}=-n\\zeta^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\left\\|\\bar{\\mathbf{x}}s-\\bar{\\mathbf{x}}\\right\\|^{2}]=\\frac{\\zeta^{2}}{s}-\\frac{(s-1)\\zeta^{2}}{s(n-1)}=\\frac{n-s}{n-1}\\frac{\\zeta^{2}}{s}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma 14. Suppose $\\{f_{i}\\}_{i=1}^{n}$ satisfy $\\Delta_{s}\\!-\\!E D$ of size $s\\in[n]$ and $\\zeta$ -BGV with $n\\geq2$ . Let $\\textstyle f:={\\frac{1}{n}}\\sum_{i=1}^{n}f_{i}$ and $\\begin{array}{r}{f_{S}:=\\frac{1}{s}\\sum_{i\\in S}f_{i}}\\end{array}$ , where $S\\in\\left(\\mathbf{\\Sigma}_{s}^{[n]}\\right)$ is sampled uniformly at random without replacement. Further, let $\\mathbf{y}\\in\\mathbb{R}^{d}$ be a fixed point, and let $\\mathbf{x}_{S}\\in\\mathbb{R}^{d}$ be a random point defined by a deterministic function of $S$ . Then, for any $\\gamma>0,$ , it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S}[f({\\bf x}_{S})-f_{S}({\\bf x}_{S})]\\le\\frac{n-s}{n-1}\\frac{\\gamma\\zeta^{2}}{2s}+\\left(\\frac{1}{2\\gamma}+\\frac{\\Delta_{s}}{2}\\right)\\mathbb{E}_{S}[\\|{\\bf x}_{S}-{\\bf y}\\|^{2}].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Let $h_{S}:=f-f_{S}$ . Since $\\{f_{i}\\}$ satisfy $\\Delta_{s}$ -ED (Definition 4), we have, in view of inequality (B.4), ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle h_{S}({\\bf x}_{S})\\leq h_{S}({\\bf y})+\\langle\\nabla h_{S}({\\bf y}),{\\bf x}_{S}-{\\bf y}\\rangle+\\frac{\\Delta_{s}}{2}\\|{\\bf x}_{S}-{\\bf y}\\|^{2}}}\\\\ {\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\leq\\ h_{S}({\\bf y})+\\frac{\\gamma}{2}\\|\\nabla h_{S}({\\bf y})\\|^{2}+\\frac{1}{2\\gamma}\\|{\\bf x}_{S}-{\\bf y}\\|^{2}+\\frac{\\Delta_{s}}{2}\\|{\\bf x}_{S}-{\\bf y}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Rearranging and taking the expectation on both sides, we get, for any $\\gamma>0$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\boldsymbol{S}}[h_{\\boldsymbol{S}}(\\mathbf{x}_{\\boldsymbol{S}})-h_{\\boldsymbol{S}}(\\mathbf{y})]\\leq\\frac{\\gamma}{2}\\mathbb{E}_{\\boldsymbol{S}}[\\|\\nabla h_{\\boldsymbol{S}}(\\mathbf{y})\\|^{2}]+\\frac{1}{2\\gamma}\\mathbb{E}_{\\boldsymbol{S}}[\\|\\mathbf{x}_{\\boldsymbol{S}}-\\mathbf{y}\\|^{2}]+\\frac{\\Delta_{s}}{2}\\mathbb{E}_{\\boldsymbol{S}}[\\|\\mathbf{x}_{\\boldsymbol{S}}-\\mathbf{y}\\|^{2}]}\\\\ &{\\phantom{\\quad}\\leq\\frac{\\eta}{n-1}\\frac{\\gamma\\zeta^{2}}{2s}+\\left(\\frac{1}{2\\gamma}+\\frac{\\Delta_{s}}{2}\\right)\\mathbb{E}_{\\boldsymbol{S}}[\\|\\mathbf{x}_{\\boldsymbol{S}}-\\mathbf{y}\\|^{2}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last inequality is due to (B.11). Using the fact that $\\mathbb{E}_{S}[f(\\mathbf{y})-f_{S}(\\mathbf{y})]=0$ , we get the claim. ", "page_idx": 18}, {"type": "text", "text": "Lemma 15. Suppose $\\{f_{i}\\}_{i=1}^{n}$ satisfy $\\delta_{s}\u2013S O D$ of size $s\\ \\in[n]$ . Let $f_{S}\\;:=\\;{\\textstyle\\frac{1}{s}}\\sum_{i\\in S}f_{i}$ where $s~\\in~[n]$ and $S\\in\\left(\\mathbf{\\Sigma}_{s}^{[n]}\\right)$ . Let $\\mathbf{v}\\in\\mathbb{R}^{d}$ be a fixed point, $\\lambda>\\delta_{s}$ , and let ", "page_idx": 18}, {"type": "equation", "text": "$$\nF_{i}(\\mathbf{x}):=f_{i}(\\mathbf{x})+\\langle\\nabla h_{i}^{S}(\\mathbf{v}),\\mathbf{x}\\rangle+\\frac{\\lambda}{2}\\|\\mathbf{x}-\\mathbf{v}\\|^{2},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $h_{i}^{S}:=f_{S}-f_{i}$ . Let $\\{\\mathbf{x}_{i}\\}_{i\\in S}$ be a set of points in $\\mathbb{R}^{d}$ (such that $\\begin{array}{r}{\\mathbf{x}_{i}\\approx\\arg\\operatorname*{min}_{\\mathbf{x}}F_{i}(\\mathbf{x})}\\end{array}$ in the sense that $\\|\\nabla F_{i}(\\mathbf{x}_{i})\\|$ is sufficiently small), and let $\\begin{array}{r}{\\bar{\\mathbf{x}}_{S}=\\frac{1}{s}\\sum_{i\\in S}\\mathbf{x}_{i}}\\end{array}$ . Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{s}\\displaystyle\\sum_{i\\in S}\\langle\\nabla f_{i}(\\mathbf{x}_{i})+\\nabla h_{i}^{S}(\\bar{\\mathbf{x}}_{S}),\\mathbf{v}-\\mathbf{x}_{i}\\rangle-\\displaystyle\\frac{1}{2\\lambda}\\Big\\|\\displaystyle\\frac{1}{s}\\displaystyle\\sum_{i\\in S}\\nabla f_{i}(\\mathbf{x}_{i})\\Big\\|^{2}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\geq\\displaystyle\\frac{\\lambda-\\delta_{s}}{2}\\displaystyle\\frac{1}{s}\\displaystyle\\sum_{i\\in S}\\|\\mathbf{v}-\\mathbf{x}_{i}\\|^{2}-\\displaystyle\\frac{1}{\\lambda}\\displaystyle\\frac{1}{s}\\displaystyle\\sum_{i\\in S}\\|\\nabla F_{i}(\\mathbf{x}_{i})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Using the definition of $F_{i}$ , we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla F_{i}(\\mathbf{x}_{i})=\\nabla f_{i}(\\mathbf{x}_{i})+\\nabla h_{i}^{S}(\\mathbf{v})+\\lambda(\\mathbf{x}_{i}-\\mathbf{v}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\langle\\nabla f_{i}(\\mathbf{x}_{i})+\\nabla h_{i}^{S}(\\bar{\\mathbf{x}}_{S}),\\mathbf{v}-\\mathbf{x}_{i}\\rangle=\\lambda\\|\\mathbf{v}-\\mathbf{x}_{i}\\|^{2}+\\langle\\nabla h_{i}^{S}(\\bar{\\mathbf{x}}_{S})-\\nabla h_{i}^{S}(\\mathbf{v}),\\mathbf{v}-\\mathbf{x}_{i}\\rangle+\\langle\\nabla F_{i}(\\mathbf{x}_{i}),\\mathbf{v}-\\mathbf{x}_{i}\\rangle.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Taking the average over $i$ on both sides of the first display, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{s}\\sum_{i\\in S}\\nabla f_{i}(\\mathbf{x}_{i})=\\lambda(\\mathbf{v}-\\bar{\\mathbf{x}}_{S})+\\frac{1}{s}\\sum_{i\\in S}\\nabla F_{i}(\\mathbf{x}_{i}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{2\\lambda}\\Big\\|\\frac{1}{s}\\displaystyle\\sum_{i\\in S}\\nabla f_{i}(\\mathbf{x}_{i})\\Big\\|^{2}=\\displaystyle\\frac{1}{2\\lambda}\\Big\\|\\lambda(\\mathbf{v}-\\bar{\\mathbf{x}}_{S})+\\frac{1}{s}\\displaystyle\\sum_{i\\in S}\\nabla F_{i}(\\mathbf{x}_{i})\\Big\\|^{2}}\\\\ {\\displaystyle=\\frac{\\lambda}{2}\\|\\mathbf{v}-\\bar{\\mathbf{x}}_{S}\\|^{2}+\\frac{1}{s}\\displaystyle\\sum_{i\\in S}\\langle\\nabla F_{i}(\\mathbf{x}_{i}),\\mathbf{v}-\\bar{\\mathbf{x}}_{S}\\rangle+\\frac{1}{2\\lambda}\\Big\\|\\frac{1}{s}\\displaystyle\\sum_{i\\in S}\\nabla F_{i}(\\mathbf{x}_{i})\\Big\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It follows that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{s}\\sum_{i\\in S}\\langle\\nabla f_{i}(\\mathbf{x}_{i})+\\nabla h_{i}^{S}(\\bar{\\mathbf{x}}_{S}),\\mathbf{v}-\\mathbf{x}_{i}\\rangle-\\displaystyle\\frac{1}{2\\lambda}\\Big\\|\\displaystyle\\frac{1}{s}\\sum_{i\\in S}\\nabla f_{i}(\\mathbf{x}_{i})\\Big\\|^{2}}\\\\ {\\displaystyle\\qquad=\\lambda\\displaystyle\\frac{1}{s}\\sum_{i\\in S}\\lVert\\mathbf{v}-\\mathbf{x}_{i}\\rVert^{2}-\\displaystyle\\frac{\\lambda}{2}\\lVert\\mathbf{v}-\\bar{\\mathbf{x}}_{S}\\rVert^{2}+\\displaystyle\\frac{1}{s}\\sum_{i\\in S}\\langle\\nabla h_{i}^{S}(\\bar{\\mathbf{x}}_{S})-\\nabla h_{i}^{S}(\\mathbf{v}),\\mathbf{v}-\\mathbf{x}_{i}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\frac{1}{3}\\displaystyle\\sum_{i\\neq\\delta}\\langle\\nabla F_{i}(\\mathbf{x}_{i}),\\bar{\\mathbf{x}}_{\\delta}-\\mathbf{x}_{i}\\rangle-\\frac{1}{2\\lambda}\\displaystyle\\left\\|\\frac{1}{3}\\displaystyle\\sum_{i\\neq\\delta}\\nabla F_{i}(\\mathbf{x}_{i})\\right\\|^{2}}\\\\ &{\\overset{\\mathrm{as.}}{=}\\frac{1}{2}\\|\\mathbf{v}-\\mathbf{x}_{i}\\|^{2}+\\lambda\\displaystyle\\frac{1}{3}\\displaystyle\\sum_{i\\neq\\delta}\\|\\mathbf{x}_{i}-\\mathbf{x}_{\\delta}\\|^{2}+\\frac{1}{3}\\displaystyle\\sum_{i\\neq\\delta}\\langle\\nabla\\mathbf{x}_{i}^{\\delta}(\\mathbf{x}_{\\delta})-\\nabla\\mathbf{x}_{i}^{\\delta}(\\mathbf{v}),\\bar{\\mathbf{x}}_{\\delta}-\\mathbf{x}_{i}\\rangle}\\\\ &{\\qquad+\\frac{1}{3}\\displaystyle\\sum_{i\\neq\\delta}\\langle\\nabla F_{i}(\\mathbf{x}_{i}),\\mathbf{x}_{\\delta}-\\mathbf{x}_{i}\\rangle-\\frac{1}{2\\lambda}\\displaystyle\\prod_{s=\\delta}^{1}\\nabla F_{i}(\\mathbf{x}_{i})\\|^{2}}\\\\ &{\\overset{\\mathrm{as.}}{=}\\frac{\\lambda}{2}\\|\\mathbf{v}-\\mathbf{x}_{\\delta}\\|^{2}+\\lambda\\displaystyle\\frac{1}{3}\\displaystyle\\sum_{i\\neq\\delta}\\|\\mathbf{x}_{i}-\\mathbf{x}_{\\delta}\\|^{2}-\\frac{1}{2\\delta}\\displaystyle\\sum_{i\\neq\\delta}\\frac{1}{s}\\|\\nabla F_{i}(\\mathbf{x}_{i})-\\nabla\\mathbf{x}_{i}^{\\delta}(\\mathbf{v})\\|^{2}-\\frac{\\delta_{\\delta}}{2}\\displaystyle\\sum_{s=\\delta}^{1}\\frac{1}{s}\\displaystyle\\sum_{i\\in\\delta}\\|\\mathbf{x}_{i}-\\bar{\\mathbf{x}}_{\\delta}\\|^{2}}\\\\ &{\\qquad-\\frac{\\lambda}{2}\\displaystyle\\sum_{s=\\delta}^{1}\\left\\|\\mathbf{x}_{i}-\\mathbf{x}_{\\delta}\\right\\|^{2}-\\frac{1}{2\\lambda}\\displaystyle\\sum_{i\\neq\\delta}\\left\\|\\nabla F_{i}(\\mathbf{x}_{i})\\right\\|^{2}-\\frac{1}{2}\\displaystyle\\sum_{i\\neq\\delta}\\left\\|\\frac{1}{s}\\displaystyle\\sum_{i\\neq\\delta}\\nabla F_{i}( \n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where in the second equality, we use the fact that $\\begin{array}{r}{\\frac{1}{s}\\sum_{i\\in S}[\\nabla h_{i}^{S}(\\bar{\\bf x}_{S})-\\nabla h_{i}^{S}({\\bf v})]=0}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "C Proofs for S-DANE (Algorithm 1) ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "C.1 One-Step Recurrence ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma 16. Consider Algorithm 1. Let $f_{i}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ be $\\mu$ -convex with $\\mu\\geq0$ for any $i\\in[n]$ . Assume that $\\{f_{i}\\}_{i=1}^{n}$ have $\\delta_{s}$ -SOD. Then, for any $r\\geq0$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac1\\lambda[f_{S_{r}}(\\mathbf{x}^{r+1})-f_{S_{r}}(\\mathbf{x}^{\\star})]+\\frac{1+\\mu/\\lambda}{2}\\|\\mathbf{v}^{r+1}-\\mathbf{x}^{\\star}\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac{1}{2}\\|\\mathbf{v}^{r}-\\mathbf{x}^{\\star}\\|^{2}-\\frac{1-\\delta_{s}/\\lambda}{2}\\frac{1}{s}\\sum_{i\\in S_{r}}\\|\\mathbf{v}^{r}-\\mathbf{x}_{i,r+1}\\|^{2}+\\frac{1}{\\lambda^{2}}\\displaystyle\\frac{1}{s}\\sum_{i\\in S_{r}}\\|\\nabla F_{i,r}(\\mathbf{x}_{i,r+1})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. By $\\mu$ -convexity of $f_{i}$ , for any $r\\geq0$ , it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{\\lambda}f_{S r}(\\mathbf{x}^{\\star})+\\frac{1}{2}\\|\\mathbf{v}^{r}-\\mathbf{x}^{\\star}\\|^{2}=\\frac{1}{\\lambda}\\frac{1}{s}\\sum_{i\\in S_{r}}f_{i}(\\mathbf{x}^{\\star})+\\frac{1}{2}\\|\\mathbf{v}^{r}-\\mathbf{x}^{\\star}\\|^{2}}\\\\ &{\\quad\\quad\\overset{(\\mathrm{B},1)}{\\geq}\\displaystyle\\frac{1}{\\lambda}\\frac{1}{s}\\sum_{i\\in S_{r}}\\Bigl[f_{i}(\\mathbf{x}_{i,r+1})+\\langle\\nabla f_{i}(\\mathbf{x}_{i,r+1}),\\mathbf{x}^{\\star}-\\mathbf{x}_{i,r+1}\\rangle+\\frac{\\mu}{2}\\|\\mathbf{x}_{i,r+1}-\\mathbf{x}^{\\star}\\|^{2}\\Bigr]+\\frac{1}{2}\\|\\mathbf{v}^{r}-\\mathbf{x}^{\\star}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Recall that $\\mathbf{v}^{r+1}$ is the minimizer of the final expression in $\\mathbf{x}^{\\star}$ . This expression is a $(1+\\mu/\\lambda)$ -convex function in $\\mathbf{x}^{\\star}$ . We can then estimate it by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\lambda}f_{S_{r}}(\\mathbf x^{\\star})+\\displaystyle\\frac12\\|\\mathbf v^{r}-\\mathbf x^{\\star}\\|^{2}}\\\\ &{\\qquad\\overset{\\mathrm{(B.1)}}{\\geq}\\displaystyle\\frac{1}{\\lambda}\\frac{1}{s}\\sum_{i\\in S_{r}}\\Big[f_{i}\\big(\\mathbf x_{i,r+1}\\big)+\\langle\\nabla f_{i}\\big(\\mathbf x_{i,r+1}\\big),\\mathbf v^{r+1}-\\mathbf x_{i,r+1}\\rangle+\\frac{\\mu}{2}\\|\\mathbf x_{i,r+1}-\\mathbf v^{r+1}\\|^{2}\\Big]}\\\\ &{\\qquad\\quad+\\displaystyle\\frac12\\|\\mathbf v^{r}-\\mathbf v^{r+1}\\|^{2}+\\frac{1+\\mu/\\lambda}{2}\\|\\mathbf v^{r+1}-\\mathbf x^{\\star}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using the convexity of $f_{i}$ and dropping the non-negative $\\begin{array}{r}{\\frac{\\mu}{2\\lambda}\\frac{1}{s}\\sum_{i\\in S_{r}}\\|\\mathbf{x}_{i,r+1}-\\mathbf{v}^{r+1}\\|^{2}}\\end{array}$ , we further get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\lambda}f_{S_{r}}(\\mathbf{x}^{\\star})+\\displaystyle\\frac{1}{2}\\|\\mathbf{v}^{r}-\\mathbf{x}^{\\star}\\|^{2}}\\\\ &{\\qquad\\overset{\\mathrm{(B.1)}}{\\geq}\\displaystyle\\frac{1}{\\lambda}\\frac{1}{s}\\sum_{i\\in S_{r}}[f_{i}(\\mathbf{x}^{r+1})+\\langle\\nabla f_{i}(\\mathbf{x}^{r+1}),\\mathbf{x}_{i,r+1}-\\mathbf{x}^{r+1}\\rangle]+\\displaystyle\\frac{1+\\mu/\\lambda}{2}\\|\\mathbf{v}^{r+1}-\\mathbf{x}^{\\star}\\|^{2}}\\\\ &{\\qquad\\quad+\\displaystyle\\frac{1}{\\lambda}\\frac{1}{s}\\sum_{i\\in S_{r}}\\langle\\nabla f_{i}(\\mathbf{x}_{i,r+1}),\\mathbf{v}^{r}-\\mathbf{x}_{i,r+1}\\rangle+\\displaystyle\\frac{1}{\\lambda}\\Big\\langle\\frac{1}{s}\\sum_{i\\in S_{r}}\\nabla f_{i}(\\mathbf{x}_{i,r+1}),\\mathbf{v}^{r+1}-\\mathbf{v}^{r}\\Big\\rangle}\\\\ &{\\qquad\\quad+\\displaystyle\\frac{1}{2}\\|\\mathbf{v}^{r+1}-\\mathbf{v}^{r}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\~\\overset{(8,6)}{\\geq}\\frac{1}{\\lambda}f_{S_{r}}(\\mathbf{x}^{r+1})+\\frac{1+\\mu/\\lambda}{2}\\|\\mathbf{v}^{r+1}-\\mathbf{x}^{\\star}\\|^{2}+\\frac{1}{\\lambda}\\displaystyle\\frac{1}{s}\\sum_{i\\in S_{r}}\\langle\\nabla f_{i}(\\mathbf{x}^{r+1}),\\mathbf{x}_{i,r+1}-\\mathbf{x}^{r+1}\\rangle}\\\\ &{\\qquad+\\displaystyle\\frac{1}{\\lambda}\\displaystyle\\frac{1}{s}\\sum_{i\\in S_{r}}\\langle\\nabla f_{i}(\\mathbf{x}_{i,r+1}),\\mathbf{v}^{r}-\\mathbf{x}_{i,r+1}\\rangle-\\frac{1}{2\\lambda^{2}}\\Big\\|\\displaystyle\\frac{1}{s}\\sum_{i\\in S_{r}}\\nabla f_{i}(\\mathbf{x}_{i,r+1})\\Big\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Denote $h_{i}^{r}:=f_{S_{r}}-f_{i}$ . Note that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{i\\in S_{r}}\\langle\\nabla f_{i}(\\mathbf{x}^{r+1}),\\mathbf{x}_{i,r+1}-\\mathbf{x}^{r+1}\\rangle=\\sum_{i\\in S_{r}}\\langle-\\nabla h_{i}^{r}(\\mathbf{x}^{r+1}),\\mathbf{x}_{i,r+1}-\\mathbf{v}^{r}\\rangle,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we have used: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{x}^{r+1}=\\frac{1}{s}\\sum_{i\\in S_{r}}\\mathbf{x}_{i,r+1}\\qquad\\mathrm{and}\\qquad\\sum_{i\\in S^{r}}\\nabla h_{i}^{r}(\\mathbf{x}^{r+1})=0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "It follows that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\lambda}f_{S_{r}}(\\mathbf{x}^{\\star})+\\displaystyle\\frac{1}{2}\\|\\mathbf{v}^{r}-\\mathbf{x}^{\\star}\\|^{2}\\geq\\displaystyle\\frac{1}{\\lambda}f_{S_{r}}(\\mathbf{x}^{r+1})+\\frac{1+\\mu/\\lambda}{2}\\|\\mathbf{v}^{r+1}-\\mathbf{x}^{\\star}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle\\frac{1}{\\lambda}\\frac{1}{s}\\displaystyle\\sum_{i\\in S_{r}}\\langle\\nabla f_{i}(\\mathbf{x}_{i,r+1})+\\nabla h_{i}^{r}(\\mathbf{x}^{r+1}),\\mathbf{v}^{r}-\\mathbf{x}_{i,r+1}\\rangle-\\displaystyle\\frac{1}{2\\lambda^{2}}\\Big\\|\\frac{1}{s}\\displaystyle\\sum_{i\\in S_{r}}\\nabla f_{i}(\\mathbf{x}_{i,r+1})\\Big\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We now apply Lemma 15 (with $\\mathbf{x}_{i}=\\mathbf{x}_{i,r+1},\\mathbf{v}=\\mathbf{v}^{r},S=S_{r}$ and $\\mathbf{x}=\\mathbf{x}^{r+1}$ ) to get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{s}\\displaystyle\\sum_{i\\in S_{r}}\\left\\langle\\nabla f_{i}(\\mathbf{x}_{i,r+1})+\\nabla h_{i}^{r}(\\mathbf{x}^{r+1}),\\mathbf{v}^{r}-\\mathbf{x}_{i,r+1}\\right\\rangle-\\frac{1}{2\\lambda}\\Big\\|\\frac{1}{s}\\displaystyle\\sum_{i\\in S_{r}}\\nabla f_{i}(\\mathbf{x}_{i,r+1})\\Big\\|^{2}}\\\\ &{\\phantom{\\frac{1}{s}\\displaystyle\\sum_{i\\in S_{r}}}\\geq\\frac{\\lambda-\\delta_{s}}{2}\\displaystyle\\frac{1}{s}\\displaystyle\\sum_{i\\in S_{r}}\\|\\mathbf{v}^{r}-\\mathbf{x}_{i,r+1}\\|^{2}-\\frac{1}{\\lambda}\\displaystyle\\frac{1}{s}\\displaystyle\\sum_{i\\in S_{r}}\\|\\nabla F_{i,r}(\\mathbf{x}_{i,r+1})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Substituting this lower bound into the previous display, we get the claim. ", "page_idx": 20}, {"type": "text", "text": "C.2 Full Client Participation (Proof of Theorem 1) ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. Applying Lemma 16 and using $\\begin{array}{r}{\\sum_{i=1}^{n}\\|\\nabla F_{i,r}({\\bf x}_{i,r+1})\\|^{2}\\leq\\delta^{2}\\sum_{i=1}^{n}\\|{\\bf x}_{i,r+1}-{\\bf v}^{r}\\|^{2}}\\end{array}$ and $\\lambda=2\\delta$ , for any $r\\geq0$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{\\lambda}[f({\\mathbf x}^{r+1})-f^{\\star}]+\\frac{1+\\mu/\\lambda}{2}\\|{\\mathbf v}^{r+1}-{\\mathbf x}^{\\star}\\|^{2}}\\\\ &{\\qquad\\leq\\displaystyle\\frac{1}{2}\\|{\\mathbf v}^{r}-{\\mathbf x}^{\\star}\\|^{2}-\\frac{1-\\delta/\\lambda}{2}\\frac{1}{n}\\sum_{i=1}^{n}\\|{\\mathbf v}^{r}-{\\mathbf x}_{i,r+1}\\|^{2}+\\frac{1}{\\lambda^{2}}\\frac{1}{n}\\sum_{i=1}^{n}\\|\\nabla F_{i,r}({\\mathbf x}_{i,r+1})\\|^{2}}\\\\ &{\\qquad\\leq\\displaystyle\\frac{1}{2}\\|{\\mathbf v}^{r}-{\\mathbf x}^{\\star}\\|^{2}-\\Big(\\frac{1-1/2}{2}-\\frac{1}{4}\\Big)\\|{\\mathbf v}^{r}-{\\mathbf x}_{i,r+1}\\|^{2}=\\displaystyle\\frac{1}{2}\\|{\\mathbf v}^{r}-{\\mathbf x}^{\\star}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Rearranging, we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{2q}{\\lambda}[f(\\mathbf{x}^{r+1})-f^{\\star}]\\leq q\\|\\mathbf{v}^{r}-\\mathbf{x}^{\\star}\\|^{2}-\\|\\mathbf{v}^{r+1}-\\mathbf{x}^{\\star}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\begin{array}{r}{q:=\\frac{1}{1+\\mu/\\lambda}}\\end{array}$ . Applying Lemma 11 with $\\varepsilon=0$ and using convexity of $f$ , we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{2q}{\\lambda}[f(\\bar{\\mathbf{x}}^{R})-f^{\\star}]+\\frac{1-q}{1-q^{R}}\\|\\mathbf{v}^{R}-\\mathbf{x}^{\\star}\\|^{2}\\leq\\frac{1-q}{(1/q)^{R}-1}\\|\\mathbf{v}^{0}-\\mathbf{x}^{\\star}\\|^{2}=\\frac{1-q}{(1/q)^{R}-1}D^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Dropping the non-negative term $\\begin{array}{r}{\\frac{1-q}{1-q^{R}}\\|\\mathbf{v}^{R}-\\mathbf{x}^{\\star}\\|^{2}}\\end{array}$ and rearranging, we get ", "page_idx": 20}, {"type": "equation", "text": "$$\nf(\\bar{\\bf x}^{R})-f^{\\star}\\le\\frac{(1-q)\\lambda}{2q\\big[(1/q)^{R}-1\\big]}D^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Plugging in the choice of $\\lambda$ and the definition of $q$ , we get the claim. ", "page_idx": 20}, {"type": "text", "text": "Corollary 17. Under the same setting as in Theorem $^{\\,l}$ , to achieve $f(\\bar{\\mathbf{x}}^{R})\\mathrm{~-~}f^{\\star}\\,\\leq\\,\\varepsilon$ , we need at most the following number of communication rounds: ", "page_idx": 20}, {"type": "equation", "text": "$$\nR=\\mathcal{O}\\Big(\\frac{\\mu+\\delta}{\\mu}\\log\\Big(1+\\frac{\\mu D^{2}}{\\varepsilon}\\Big)\\Big).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Using the fact that $\\begin{array}{r}{(1+q)^{k}\\geq\\exp(\\frac{q}{1+q}k)}\\end{array}$ for any $q\\geq0$ and $k>0$ , we get ", "page_idx": 20}, {"type": "equation", "text": "$$\nf(\\bar{\\mathbf{x}}^{R})-f^{\\star}\\le\\frac{\\mu D^{2}}{2[(1+\\frac{\\mu}{2\\delta})^{R}-1]}\\le\\frac{\\mu D^{2}}{2[\\exp(\\frac{\\mu}{\\mu+2\\delta}R)-1]}\\le\\varepsilon.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Rearranging, we get the claim. ", "page_idx": 20}, {"type": "text", "text": "Proof of Corollary 3. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof. To achieve $\\begin{array}{r}{\\sum_{i=1}^{n}\\|\\nabla F_{i,r}({\\bf x}_{i,r+1})\\|^{2}\\,\\le\\,\\delta^{2}\\sum_{i=1}^{n}\\|{\\bf x}_{i,r+1}-{\\bf y}^{r}\\|^{2}}\\end{array}$ , for each $i\\,\\in\\,[n]$ , it is sufficient to ensure that $\\|\\nabla F_{i,r}(\\mathbf{x}_{i,r+1})\\|\\leq\\delta\\|\\mathbf{x}_{i,r+1}-\\mathbf{v}^{r}\\|$ . Let $\\begin{array}{r}{\\mathbf{\\check{x}}_{i,r}^{\\star}:=\\arg\\operatorname*{min}_{\\mathbf{x}}F_{i,r}(\\mathbf{x})}\\end{array}$ . Since ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}_{i,r+1}-\\mathbf{v}^{r}\\|\\geq\\|\\mathbf{v}^{r}-\\mathbf{x}_{i,r}^{\\star}\\|-\\|\\mathbf{x}_{i,r+1}-\\mathbf{x}_{i,r}^{\\star}\\|\\overset{(\\mathtt{B}_{2})}{\\geq}\\|\\mathbf{v}^{r}-\\mathbf{x}_{i,r}^{\\star}\\|-\\frac{1}{\\lambda}\\|\\nabla F_{i,r}(\\mathbf{x}_{i,r+1})\\|\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and $\\lambda=2\\delta$ , it suffices to ensure that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\nabla F_{i,r}(\\mathbf{x}_{i,r+1})\\|\\leq\\frac{2\\delta}{3}\\|\\mathbf{v}^{r}-\\mathbf{x}_{i,r}^{\\star}\\|\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for any $i\\in[n]$ . According to Theorem 2 from [33] (or Theorem 3.2 from [37]), there exists a certain algorithm such that when started from the point $\\mathbf{v}^{r}$ , after $K$ queries to $\\nabla F_{i,r}$ , it generates the point $\\mathbf{v}_{i,r+1}$ such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\nabla F_{i,r}(\\mathbf{x}_{i,r+1})\\|\\leq\\mathcal{O}\\Big(\\frac{(L+\\lambda)\\|\\mathbf{v}^{r}-\\mathbf{x}_{i,r}^{\\star}\\|}{K^{2}}\\Big)=\\mathcal{O}\\Big(\\frac{L\\|\\mathbf{v}^{r}-\\mathbf{x}_{i,r}^{\\star}\\|}{K^{2}}\\Big)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "(recall that $\\delta\\leq L$ ). Setting $\\begin{array}{r}{K=\\Theta(\\sqrt{\\frac{L}{\\delta}})}\\end{array}$ concludes the proof. ", "page_idx": 21}, {"type": "text", "text": "Remark 18. Recall that $F_{i,r}$ is $(L+\\lambda)$ -smooth and $(\\mu+\\lambda)$ -convex, $\\lambda=\\Theta(\\delta)$ and $\\delta\\leq L$ . Suppose worker $i$ uses the standard GD to approximately solve the local subproblem at round $r$ starting at $\\mathbf{v}^{r}$ for $K$ steps and return the last point, then by Lemma $I9$ , we have that $\\begin{array}{r}{\\|\\nabla F_{i,r}(\\mathbf{x}_{i,r+1})\\|^{2}\\leq\\mathcal{O}\\big(\\frac{(L+\\lambda)^{2}\\|\\mathbf{v}^{r}-\\mathbf{x}_{i,r}^{\\star}\\|^{2}}{K^{2}}\\big)}\\end{array}$ . To satisfy the accuracy condition (C.1), $i t$ is sufficient to make $\\begin{array}{r}{K=\\Theta(\\frac{L}{\\delta})}\\end{array}$ local steps. Suppose worker $i$ uses the fast gradient method, then by Theorem 3.18 from $\\it{[4]}_{:}$ , we have that $\\begin{array}{r}{\\|\\bar{\\nabla}F_{i,r}(\\mathbf{x}_{i,r+1})\\|^{2}\\leq2(L+\\lambda)\\big(F_{i,r}(\\mathbf{x}_{i,r+1})-}\\end{array}$ $\\begin{array}{r}{F_{i,r}(\\mathbf{x}_{i,r}^{\\star}))\\le\\mathcal{O}\\big((L+\\lambda)^{2}\\exp(-\\sqrt{\\frac{\\mu+\\lambda}{L+\\lambda}}K)\\|\\mathbf{v}^{r}-\\mathbf{x}_{i,r}^{\\star}\\|^{2}\\big)}\\end{array}$ . To satisfy the accuracy condition (C.1), it suffices to make $\\begin{array}{r}{K=\\Theta(\\sqrt{\\frac{L+\\delta}{\\mu+\\delta}}\\log(\\frac{L+\\delta}{\\delta}))=\\Theta(\\sqrt{\\frac{L}{\\mu+\\delta}}\\log(\\frac{L}{\\delta}))}\\end{array}$ gradient oracle calls. ", "page_idx": 21}, {"type": "text", "text": "Lemma 19 (Theorem 2.2.5 in [49]). Let $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ be a convex and $L$ -smooth function. Consider the gradient method with constant stepsize: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{x}_{k+1}=\\mathbf{x}_{k}-\\frac{1}{L}\\nabla f(\\mathbf{x}_{k}),\\qquad k\\geq0,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "started from some $\\mathbf{x}_{0}\\in\\mathbb{R}^{d}$ . Then, for any $K\\geq1$ , $i t$ holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\nabla f(\\mathbf{x}_{K})\\|\\le\\mathcal{O}\\Big(\\frac{L\\|\\mathbf{x}_{0}-\\mathbf{x}^{\\star}\\|}{K}\\Big).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. By Theorem 2.2.5 in [49], we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k\\in[K]}\\|\\nabla f(\\mathbf{x}_{k})\\|\\leq\\mathcal{O}\\bigg(\\frac{L\\|\\mathbf{x}_{0}-\\mathbf{x}^{\\star}\\|}{K}\\bigg).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It remains to note that the algorithm generates non-increasing $\\|\\nabla f(\\mathbf{x}_{k})\\|$ since ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla f(\\mathbf{x}_{k+1})\\|^{2}=\\|\\nabla f(\\mathbf{x}_{k+1})-\\nabla f(\\mathbf{x}_{k})+\\nabla f(\\mathbf{x}_{k})\\|^{2}}\\\\ &{\\qquad\\qquad=\\|\\nabla f(\\mathbf{x}_{k+1})-\\nabla f(\\mathbf{x}_{k})\\|^{2}+2\\langle\\nabla f(\\mathbf{x}_{k+1})-\\nabla f(\\mathbf{x}_{k}),\\nabla f(\\mathbf{x}_{k})\\rangle+\\|\\nabla f(\\mathbf{x}_{k})\\|^{2}}\\\\ &{\\qquad\\qquad=\\|\\nabla f(\\mathbf{x}_{k+1})-\\nabla f(\\mathbf{x}_{k})\\|^{2}-2L\\langle\\nabla f(\\mathbf{x}_{k})-\\nabla f(\\mathbf{x}_{k+1}),\\mathbf{x}_{k}-\\mathbf{x}_{k+1}\\rangle+\\|\\nabla f(\\mathbf{x}_{k})\\|^{2}}\\\\ &{\\qquad\\qquad\\overset{(\\mathrm{B.5})}{\\leq}\\|\\nabla f(\\mathbf{x}_{k})\\|^{2}-\\|\\nabla f(\\mathbf{x}_{k+1})-\\nabla f(\\mathbf{x}_{k})\\|^{2}\\leq\\|\\nabla f(\\mathbf{x}_{k})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "C.3 Partial Client Participation (Proof of Theorem 5) ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The following theorem is a slight extension of Theorem 5, which includes the use of stochastic local solvers. ", "page_idx": 21}, {"type": "text", "text": "Theorem 20. Consider Algorithm 1. Let $f_{i}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ be $\\mu$ -convex with $\\mu\\geq0$ for any $i\\in[n]$ and let $n\\geq2$ . Assume that $\\{f_{i}\\}_{i=1}^{n}$ have $\\delta_{s}$ -SOD, $\\Delta_{s}$ -ED and $\\zeta$ -BGV. Let $\\begin{array}{r}{\\lambda\\,=\\,\\frac{4(n-s)}{s(n-1)}\\frac{\\zeta^{2}}{\\varepsilon}+2(\\delta_{s}+\\Delta_{s})}\\end{array}$ . For any $r\\,\\geq\\,0$ , suppose we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{s}\\sum_{i\\in S_{r}}\\mathbb{E}_{\\xi_{i,r}}[\\|\\nabla F_{i,r}({\\mathbf x}_{i,r+1})\\|^{2}]\\leq\\frac{\\lambda^{2}}{4}\\frac{1}{s}\\sum_{i\\in S_{r}}\\mathbb{E}_{\\xi_{i,r}}[\\|{\\mathbf x}_{i,r+1}-{\\mathbf v}^{r}\\|^{2}]+\\frac{\\lambda\\varepsilon}{4},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for some $\\varepsilon>0$ , where $\\xi_{i,r}$ denotes the randomness coming from device $i$ when solving its subproblem at round $r$ . We assume that $\\{\\xi_{i,r}\\}$ are independent random variables. To reach $\\mathbb{E}[f(\\bar{\\mathbf{x}}^{R})-f^{\\star}]\\leq\\varepsilon$ , we need at most the following number of communication rounds: ", "page_idx": 21}, {"type": "equation", "text": "$$\nR=\\Theta\\bigg(\\bigg[\\frac{\\delta_{s}+\\Delta_{s}+\\mu}{\\mu}+\\frac{n-s}{n-1}\\frac{\\zeta^{2}}{s\\varepsilon\\mu}\\bigg]\\log\\Big(1+\\frac{\\mu D^{2}}{\\varepsilon}\\Big)\\big)\\leq\\Theta\\bigg(\\frac{(\\delta_{s}+\\Delta_{s})D^{2}}{\\varepsilon}+\\frac{n-s}{n-1}\\frac{\\zeta^{2}D^{2}}{s\\varepsilon^{2}}\\bigg),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{\\mathbf{x}}^{R}:=\\sum_{r=1}^{R}p^{r}\\mathbf{x}^{r}/\\sum_{r=1}^{R}p^{r},\\,p:=1+\\frac{\\mu}{\\lambda}}\\end{array}$ , and $D:=\\|\\mathbf{x}^{0}-\\mathbf{x}^{\\star}\\|$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. According to Lemma 16, we have for any $r\\geq0$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac1\\lambda[f_{S r}(\\mathbf{x}^{r+1})-f_{S r}(\\mathbf{x}^{\\star})]+\\frac{1+\\mu/\\lambda}{2}\\|\\mathbf{v}^{r+1}-\\mathbf{x}^{\\star}\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac{1}{2}\\|\\mathbf{v}^{r}-\\mathbf{x}^{\\star}\\|^{2}-\\frac{1-\\delta_{s}/\\lambda}{2}\\frac{1}{s}\\sum_{i\\in S_{r}}\\|\\mathbf{v}^{r}-\\mathbf{x}_{i,r+1}\\|^{2}+\\frac{1}{\\lambda^{2}}\\displaystyle\\frac{1}{s}\\displaystyle\\sum_{i\\in S_{r}}\\|\\nabla F_{i,r}(\\mathbf{x}_{i,r+1})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "According to Lemma 14 (with $S=S_{r}$ , $\\mathbf{x}=\\mathbf{x}^{r+1}$ and $\\mathbf{y}=\\mathbf{v}^{r}$ ), for any $\\gamma>0$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{S_{r}}[f(\\mathbf{x}^{r+1})-f_{S_{r}}(\\mathbf{x}^{r+1})]\\leq\\displaystyle\\frac{n-s}{n-1}\\frac{\\gamma\\zeta^{2}}{2s}+\\Big(\\displaystyle\\frac{1}{2\\gamma}+\\displaystyle\\frac{\\Delta_{s}}{2}\\Big)\\,\\mathbb{E}_{S_{r}}[\\|\\mathbf{x}^{r+1}-\\mathbf{v}^{r}\\|^{2}]\\,}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\overset{(\\mathrm{B.})}{\\leq}\\displaystyle\\frac{n-s}{n-1}\\frac{\\gamma\\zeta^{2}}{2s}+\\Big(\\displaystyle\\frac{1}{2\\gamma}+\\displaystyle\\frac{\\Delta_{s}}{2}\\Big)\\,\\mathbb{E}_{S_{r}}\\Big[\\displaystyle\\frac{1}{s}\\displaystyle\\sum_{i\\in S_{r}}\\|\\mathbf{x}_{i,r+1}-\\mathbf{v}^{r}\\|^{2}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Adding $\\begin{array}{r}{\\frac{1}{\\lambda}f(\\mathbf{x}^{r+1})}\\end{array}$ to both sides of the first display, taking the expectation over $S_{r}$ on both sides, substituting the previous upper bound and setting $\\begin{array}{r}{\\gamma=\\frac{s(n-1)\\varepsilon}{2\\zeta^{2}(n-s)}}\\end{array}$ 2\u03b62(n\u2212s), we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac1\\lambda\\mathbb{E}s_{r}[f({\\mathbf x}^{r+1})-f^{\\star}]+\\frac{1+\\mu/\\lambda}{2}\\,\\mathbb{E}s_{r}[\\|{\\mathbf v}^{r+1}-{\\mathbf x}^{\\star}\\|^{2}]}\\\\ &{\\qquad\\le\\frac12\\|{\\mathbf v}^{r}-{\\mathbf x}^{\\star}\\|^{2}-\\left(\\frac12-\\frac{\\delta_{s}+\\Delta_{s}}{2\\lambda}-\\frac{1}{2\\gamma\\lambda}\\right)\\mathbb{E}s_{r}\\left[\\frac1s\\sum_{i\\in S_{r}}\\|{\\mathbf x}_{i,r+1}-{\\mathbf v}^{r}\\|^{2}\\right]}\\\\ &{\\qquad\\quad+\\,\\frac{\\varepsilon}{4\\lambda}+\\frac{1}{\\lambda^{2}}\\,\\mathbb{E}s_{r}\\left[\\frac1s\\sum_{i\\in S_{r}}\\|\\nabla F_{i,r}({\\mathbf x}_{i,r+1})\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Denote all the randomness $\\{\\xi_{i,r}\\}_{i\\in S_{r}}$ by $\\xi_{r}$ . Since $\\xi_{i,r}$ is independent of the choice of $S_{r}$ for any $i\\,\\in\\,[n]$ , taking the expectation over $\\xi_{r}$ on both sides of the previous display and using our assumption (C.3), we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac1\\lambda\\mathbb E_{S_{r},\\xi_{r}}[f(\\mathbf{x}^{r+1})-f^{*}]+\\frac{1+\\mu/\\lambda}{2}\\mathbb E_{S_{r},\\xi_{r}}[\\|\\mathbf{v}^{r+1}-\\mathbf{x}^{\\star}\\|^{2}]}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\leq\\frac12\\|\\mathbf{v}^{r}-\\mathbf{x}^{\\star}\\|^{2}-\\Big(\\frac{1}{4}-\\frac{\\delta_{s}+\\Delta_{s}}{2\\lambda}-\\frac{1}{2\\gamma\\lambda}\\Big)\\mathbb E_{S_{r},\\xi_{r}}\\Big[\\frac{1}{s}\\sum_{i\\in S_{r}}\\|\\mathbf{x}_{i,r+1}-\\mathbf{v}^{r}\\|^{2}\\Big]+\\frac{\\varepsilon}{2\\lambda}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By our choice of $\\lambda$ , we have $\\begin{array}{r}{\\frac{\\lambda}{4}-\\frac{\\delta_{s}+\\Delta_{s}}{2}-\\frac{1}{2\\gamma}\\geq0}\\end{array}$ . Taking the full expectation on both sides, we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{1}{\\lambda}\\mathop{\\mathbb{E}}[f({\\mathbf x}^{r+1})-f^{\\star}]+\\frac{1+\\mu/\\lambda}{2}\\mathop{\\mathbb{E}}[\\|{\\mathbf v}^{r+1}-{\\mathbf x}^{\\star}\\|^{2}]\\leq\\frac{1}{2}\\mathop{\\mathbb{E}}[\\|{\\mathbf v}^{r}-{\\mathbf x}^{\\star}\\|^{2}]+\\frac{\\varepsilon}{2\\lambda}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "According to Lemma 11 and the fact that $\\|\\mathbf{v}^{0}-\\mathbf{x}^{\\star}\\|=D$ , we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{2}{\\mu+\\lambda}\\mathop{\\mathbb{E}}\\left[f(\\bar{{\\bf x}}^{R})-f^{\\star}\\right]+(1-q)\\mathop{\\mathbb{E}}\\left[\\left\\|{\\bf v}^{R}-{\\bf x}^{\\star}\\right\\|^{2}\\right]\\leq\\frac{1-q}{(1/q)^{R}-1}D^{2}+\\frac{1}{\\mu+\\lambda}\\varepsilon,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\begin{array}{r}{q:=\\frac{1}{1+\\mu/\\lambda}}\\end{array}$ . Rearranging and dropping the non-negative $\\mathbb{E}[\\|\\mathbf{v}^{R}-\\mathbf{x}^{\\star}\\|^{2}]$ , we get, for any $R\\geq1$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[f(\\bar{\\mathbf{x}}^{R})-f^{\\star}\\big]\\leq\\frac{\\mu D^{2}}{2[(\\frac{\\mu}{\\lambda}+1)^{R}-1]}+\\frac{\\varepsilon}{2}\\leq\\frac{\\mu D^{2}}{2[\\exp(\\frac{\\mu}{\\mu+\\lambda}R)-1]}+\\frac{\\varepsilon}{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To reach $\\varepsilon$ -accuracy, it suffices to le t2[exp(\u00b5\u00b5D R)\u22121] \u2264 2\u03b5. Rearranging gives the claim. ", "page_idx": 22}, {"type": "text", "text": "C.3.1 Stochastic Local Solver ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Note that there exist many stochastic optimization algorithms that can also achieve the accuracy condition (C.3) such as variance reduction methods [23, 10], adaptive SGD methods [13], etc. Here, we take the simplest algorithm: SGD with constant stepsize as an example. ", "page_idx": 22}, {"type": "text", "text": "Corollary 21. Consider Algorithm 1 under the same settings as in Theorem 20. Further assume that each $f_{i}$ is $L$ -smooth and each device has access to mini-batch stochastic gradient $g_{i}(\\mathbf{x},\\bar{\\xi}_{i})$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\bar{\\xi}_{i}}\\big[g_{i}(\\mathbf{x},\\bar{\\xi}_{i})\\big]=\\nabla f_{i}(\\mathbf{x}),\\qquad\\mathbb{E}_{\\bar{\\xi}_{i}}\\big[\\|g_{i}(\\mathbf{x},\\bar{\\xi}_{i})-\\nabla f_{i}(\\mathbf{x})\\|^{2}\\big]\\leq\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Suppose for any $r\\geq0$ , each device $i\\in S_{r}$ solves its subproblem approximately by using mini-batch $S G D$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{z}_{k+1}=\\mathbf{z}_{k}-\\frac{1}{H}\\big[g_{i}(\\mathbf{x},\\bar{\\xi}_{i,k}^{r})-\\nabla f_{i}(\\mathbf{v}^{r})+\\nabla f_{S_{r}}(\\mathbf{v}^{r})+\\lambda(\\mathbf{z}_{k}-\\mathbf{v}^{r})\\big],\\qquad0\\le k\\le K,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\mathbf{z}_{0}\\,=\\,\\mathbf{v}^{r}$ and $H\\,>\\,L\\,+\\,\\lambda$ is the stepsize coefficient. Let $\\xi_{i,r}$ denote $(\\bar{\\xi}_{i,k}^{r})_{k}$ . To achieve accuracy condition (C.3) for an appropriately chosen $H$ , each device $i$ requires at most the following number of stochastic mini-batch oracle calls: ", "page_idx": 23}, {"type": "equation", "text": "$$\nK=\\Theta\\bigg(\\bigg[\\frac{L+\\lambda}{\\mu+\\lambda}+\\frac{(L+\\lambda)\\sigma^{2}}{(\\mu+\\lambda)\\lambda\\varepsilon}\\bigg]\\log\\frac{L+\\lambda}{\\lambda}\\bigg).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. To get (C.3), it suffices to ensure that, for any $i\\in S_{r}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nE_{\\xi_{i,r}}[\\left\\|\\nabla F_{i,r}(\\mathbf{x}_{i,r+1})\\right\\|^{2}]\\leq\\frac{\\lambda^{2}}{4}\\operatorname{\\mathbb{E}}_{\\xi_{i,r}}[\\left\\|\\mathbf{x}_{i,r+1}-\\mathbf{v}^{r}\\right\\|^{2}]+\\frac{\\lambda\\varepsilon}{4}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For this, it suffices to ensure that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\xi_{i,r}}[\\|\\nabla F_{i,r}(\\mathbf{x}_{i,r+1})\\|^{2}]\\le\\frac{\\lambda^{2}}{10}\\|\\mathbf{v}^{r}-\\mathbf{x}_{i,r}^{\\star}\\|^{2}+\\frac{\\lambda\\varepsilon}{5}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathbf{x}_{i,r}^{\\star}:=\\arg\\operatorname*{min}_{\\mathbf{x}}F_{i,r}(\\mathbf{x})}\\end{array}$ . Indeed, suppose (C.4) holds, then we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}_{i,r+1}-\\mathbf{v}^{r}\\|\\geq\\|\\mathbf{v}^{r}-\\mathbf{x}_{i,r}^{\\star}\\|-\\|\\mathbf{x}_{i,r+1}-\\mathbf{x}_{i,r}^{\\star}\\|\\overset{(8,2)}{\\geq}\\|\\mathbf{v}^{r}-\\mathbf{x}_{i,r}^{\\star}\\|-\\frac{1}{\\lambda}\\|\\nabla F_{i,r}(\\mathbf{x}_{i,r+1})\\|.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\mathbf{v}^{r}-\\mathbf{x}_{i,r}^{\\star}\\|^{2}\\leq\\frac{2}{\\lambda^{2}}\\|\\nabla F_{i,r}(\\mathbf{x}_{i,r+1})\\|^{2}+2\\|\\mathbf{x}_{i,r+1}-\\mathbf{v}^{r}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Plugging in this inequality into (C.4) and taking expectation w.r.t $\\xi_{i,r}$ on both sides, we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\xi_{i,r}}[\\|\\nabla F_{i,r}(\\mathbf{x}_{i,r+1})\\|^{2}]\\le\\frac{1}{5}\\mathbb{E}_{\\xi_{i,r}}[\\|\\nabla F_{i,r}(\\mathbf{x}_{i,r+1})\\|^{2}]+\\frac{\\lambda^{2}}{5}\\mathbb{E}_{\\xi_{i,r}}[\\|\\mathbf{x}_{i,r+1}-\\mathbf{v}^{r}\\|^{2}]+\\frac{\\lambda}{5}\\varepsilon.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Rearranging gives the weaker condition. ", "page_idx": 23}, {"type": "text", "text": "We next consider the number of mini-batch stochastic gradient oracles required for SGD to achieve (C.4). Since $F_{i,r}$ is $(L+\\lambda)$ -smooth and $(\\mu+\\lambda)$ -convex, according to Lemma 22, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\xi_{i,r}}[\\|\\nabla F_{i,r}(\\bar{\\mathbf{z}}_{K})\\|^{2}]\\leq2(L+\\lambda)\\mathbb{E}_{\\xi_{i,r}}[F_{i,r}(\\bar{\\mathbf{z}}_{K})-F_{i,r}^{\\star}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ &{\\qquad\\qquad\\qquad\\leq2(L+\\lambda)\\bigg(\\displaystyle\\frac{(\\mu+\\lambda)\\|\\mathbf{v}^{r}-\\mathbf{x}_{i,r}^{\\star}\\|^{2}}{2[\\exp\\big((\\mu+\\lambda)K/H\\big)-1]}+\\displaystyle\\frac{\\sigma^{2}}{2(H-L-\\lambda)}\\bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{\\mathbf{z}}_{K}:=\\frac{1}{\\sum_{k=1}^{K}\\frac{1}{q^{k}}}\\sum_{k=1}^{K}\\frac{\\mathbf{z}_{k}}{q^{k}}}\\end{array}$ and $\\begin{array}{r}{q=\\frac{H-\\mu-\\lambda}{H}}\\end{array}$ . Choosing now $\\begin{array}{r}{H=(L+\\lambda)+\\frac{5(L+\\lambda)\\sigma^{2}}{\\lambda\\varepsilon}}\\end{array}$ L+\u03bb\u03b5\u03bb)\u03c3 , and letting the coefficient of the first part in the previous display be $\\leq\\textstyle{\\frac{\\lambda^{2}}{10}}$ , we get the claim. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Lemma 22. Let $f$ be a $\\mu$ -convex and $L$ -smooth function. Consider $S G D$ with constant stepsize $H>L$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{x}_{k+1}:=\\underset{\\mathbf{x}\\in\\mathbb{R}^{d}}{\\arg\\operatorname*{min}}\\Bigl\\{\\langle g_{k},\\mathbf{x}\\rangle+\\frac{H}{2}\\|\\mathbf{x}-\\mathbf{x}_{k}\\|^{2}\\Bigr\\},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $g_{k}:=g(\\mathbf{x}_{k},\\xi_{k})$ with $\\mathbb{E}_{\\xi}[g(\\mathbf{x},\\xi)]=\\nabla f(\\mathbf{x})$ and $\\begin{array}{r}{\\mathbb{E}_{\\xi}\\big[\\|g(\\mathbf{x},\\xi)-\\nabla f(\\mathbf{x})\\|^{2}\\big]\\leq\\sigma^{2}}\\end{array}$ for any $\\mathbf{x}\\in\\mathbb{R}^{d}$ . Then for any $K\\geq1$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(\\bar{\\mathbf{x}}_{K})]-f^{\\star}\\le\\frac{\\mu\\|\\mathbf{x}_{0}-\\mathbf{x}^{\\star}\\|^{2}}{2\\big[\\mathrm{exp}(\\mu K/H)-1\\big]}+\\frac{\\sigma^{2}}{2(H-L)}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{\\mathbf{x}}_{K}:=\\frac{1}{\\sum_{k=1}^{K}\\frac{1}{q^{k}}}\\sum_{k=1}^{K}\\frac{\\mathbf{x}_{k}}{q^{k}}\\,}\\end{array}$ and $\\begin{array}{r}{q=\\frac{H-\\mu}{H}}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "Proof. Indeed, for any $k\\geq0$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f({\\mathbf x}_{k})+\\langle g_{k},{\\mathbf x}^{\\star}-{\\mathbf x}_{k}\\rangle+\\displaystyle\\frac{H}{2}\\|{\\mathbf x}_{k}-{\\mathbf x}^{\\star}\\|^{2}}\\\\ {\\geq f({\\mathbf x}_{k})+\\langle g_{k},{\\mathbf x}_{k+1}-{\\mathbf x}_{k}\\rangle+\\displaystyle\\frac{H}{2}\\|{\\mathbf x}_{k+1}-{\\mathbf x}_{k}\\|^{2}+\\displaystyle\\frac{H}{2}\\|{\\mathbf x}_{k+1}-{\\mathbf x}^{\\star}\\|^{2}}\\\\ {{\\overset{{\\mathrm{(B.~3)}}}{\\geq}}f({\\mathbf x}_{k+1})+\\langle g_{k}-\\nabla f({\\mathbf x}_{k}),{\\mathbf x}_{k+1}-{\\mathbf x}_{k}\\rangle+\\displaystyle\\frac{H-L}{2}\\|{\\mathbf x}_{k+1}-{\\mathbf x}_{k}\\|^{2}+\\displaystyle\\frac{H}{2}\\|{\\mathbf x}_{k+1}-{\\mathbf x}^{\\star}\\|^{2}}\\\\ {{\\overset{{\\mathrm{(B.~6)}}}{\\geq}}f({\\mathbf x}_{k+1})-\\displaystyle\\frac{\\|g_{k}-\\nabla f({\\mathbf x}_{k})\\|^{2}}{2(H-L)}+\\displaystyle\\frac{H}{2}\\|{\\mathbf x}_{k+1}-{\\mathbf x}^{\\star}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Taking the expectation on both sides and using $\\mu$ -convexity of $f$ , we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(\\mathbf{x}_{k+1})-f^{\\star}]+\\frac{H}{2}\\,\\mathbb{E}[\\|\\mathbf{x}_{k+1}-\\mathbf{x}^{\\star}\\|^{2}]\\leq\\frac{H-\\mu}{2}\\,\\mathbb{E}[\\|\\mathbf{x}_{k}-\\mathbf{x}^{\\star}\\|^{2}+\\frac{\\sigma^{2}}{2(H-L)}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Applying Lemma 11, we have for any $K\\geq1$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(\\bar{\\mathbf{x}}_{K})-f^{\\star}]\\leq\\frac{\\mu\\|\\mathbf{x}_{0}-\\mathbf{x}^{\\star}\\|^{2}}{2\\big[(1/q)^{K}-1\\big]}+\\frac{\\sigma^{2}}{2(H-L)}\\leq\\frac{\\mu\\|\\mathbf{x}_{0}-\\mathbf{x}^{\\star}\\|^{2}}{2\\big[\\exp(\\mu K/H)-1\\big]}+\\frac{\\sigma^{2}}{2(H-L)}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "D Proofs for Accelerated S-DANE (Algorithm 2) ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "D.1 One-Step Recurrence ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Lemma 23. Consider Algorithm 2. Let $f_{i}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ be $\\mu$ -convex with $\\mu\\geq0$ for any $i\\in[n]$ . Assume that $\\{f_{i}\\}_{i=1}^{n}$ have $\\delta_{s}$ -SOD. For any $r\\geq0$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{r}f_{S_{r}}(\\mathbf{x}^{r})+a_{r+1}f_{S_{r}}(\\mathbf{x}^{\\star})+\\displaystyle\\frac{B_{r}}{2}\\|\\mathbf{v}^{r}-\\mathbf{x}^{\\star}\\|^{2}}\\\\ &{\\qquad\\geq A_{r+1}f_{S_{r}}(\\mathbf{x}^{r+1})+\\displaystyle\\frac{B_{r+1}}{2}\\|\\mathbf{v}^{r+1}-\\mathbf{x}^{\\star}\\|^{2}}\\\\ &{\\qquad\\qquad+\\ A_{r+1}\\bigg(\\displaystyle\\frac{\\lambda-\\delta_{s}}{2}\\displaystyle\\frac{1}{s}\\sum_{i\\in S_{r}}\\|\\mathbf{x}_{i,r+1}-\\mathbf{y}^{r}\\|^{2}-\\displaystyle\\frac{1}{\\lambda}\\displaystyle\\frac{1}{s}\\sum_{i\\in S_{r}}\\|\\nabla F_{i,r}(\\mathbf{x}_{i,r+1})\\|^{2}\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. By $\\mu$ -convexity of $f_{i}$ , for any $r\\geq0$ , it holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal A}_{r}f_{S_{r}}({\\bf x}^{r})+a_{r+1}f_{S_{r}}({\\bf x}^{\\star})+\\frac{B_{r}}{2}\\|{\\bf v}^{r}-{\\bf x}^{\\star}\\|^{2}}\\ ~}\\\\ {{\\displaystyle~~~~~={\\cal A}_{r}\\frac{1}{s}\\sum_{i\\in S_{r}}f_{i}({\\bf x}^{r})+a_{r+1}\\frac{1}{s}\\sum_{i\\in S_{r}}f_{i}({\\bf x}^{\\star})+\\frac{B_{r}}{2}\\|{\\bf v}^{r}-{\\bf x}^{\\star}\\|^{2}}\\ ~}\\\\ {{\\displaystyle~~~~~\\overset{{\\mathbb B}^{1}}{\\geq}{\\cal A}_{r}\\frac{1}{s}\\sum_{i\\in S_{r}}[f_{i}({\\bf x}_{i,r+1})+\\langle\\nabla f_{i}({\\bf x}_{i,r+1}),{\\bf x}^{r}-{\\bf x}_{i,r+1}\\rangle]+\\frac{B_{r}}{2}\\|{\\bf v}^{r}-{\\bf x}^{\\star}\\|^{2}}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~+a_{r+1}\\frac{1}{s}\\sum_{i\\in S_{r}}\\left[f_{i}\\big({\\bf x}_{i,r+1}\\big)+\\langle\\nabla f_{i}({\\bf x}_{i,r+1}),{\\bf x}^{\\star}-{\\bf x}_{i,r+1}\\rangle+\\frac{\\mu}{2}\\|{\\bf x}_{i,r+1}-{\\bf x}^{\\star}\\|^{2}\\right]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Recall that $\\mathbf{v}^{r+1}$ is the minimizer of the final expression in $\\mathbf{x}^{\\star}$ . This expression is a $(\\mu a_{r+1}+B_{r})$ -convex function in $\\mathbf{x}^{\\star}$ . By convexity and using the fact that $A_{r+1}=A_{r}+a_{r+1}$ and $B_{r+1}=\\mu a_{r+1}+B_{r}$ , we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle A_{r}f s_{r}({\\bf x}^{r})+a_{r+1}f s_{r}({\\bf x}^{\\star})+\\frac{B_{r}}{2}\\|{\\bf v}^{r}-{\\bf x}^{\\star}\\|^{2}}\\ ~}\\\\ {{\\displaystyle~~~~~\\geq A_{r+1}\\frac{1}{s}\\sum_{i\\in S_{r}}f_{i}({\\bf x}_{i,r+1})+\\frac{\\mu a_{r+1}}{2}\\frac{1}{s}\\sum_{i\\in S_{r}}\\|{\\bf x}_{i,r+1}-{\\bf v}^{r+1}\\|^{2}+\\frac{B_{r}}{2}\\|{\\bf v}^{r}-{\\bf v}^{r+1}\\|^{2}}\\ ~}\\\\ {{\\displaystyle~~~~~~~~+\\frac{1}{s}\\sum_{i\\in S_{r}}\\langle\\nabla f_{i}({\\bf x}_{i,r+1}),A_{r}{\\bf x}^{r}+a_{r+1}{\\bf v}^{r+1}-A_{r+1}{\\bf x}_{i,r+1}\\rangle+\\frac{B_{r+1}}{2}\\|{\\bf v}^{r+1}-{\\bf x}^{\\star}\\|^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Recall that $\\begin{array}{r}{\\mathbf{y}^{r}=\\frac{A_{r}}{A_{r+1}}\\mathbf{x}^{r}+\\frac{a_{r+1}}{A_{r+1}}\\mathbf{v}^{r}}\\end{array}$ ar+1vr. Therefore, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{B_{r}}{2}\\|\\mathbf{v}^{r}-\\mathbf{v}^{r+1}\\|^{2}+\\frac{1}{s}\\displaystyle\\sum_{i\\in S_{r}}\\langle\\nabla f_{i}(\\mathbf{x}_{i,r+1}),A_{r}\\mathbf{x}^{r}+a_{r+1}\\mathbf{v}^{r+1}-A_{r+1}\\mathbf{x}_{i,r+1}\\rangle}\\\\ &{\\quad\\quad=\\displaystyle\\frac{B_{r}}{2}\\|\\mathbf{v}^{r}-\\mathbf{v}^{r+1}\\|^{2}+a_{r+1}\\Big\\langle\\frac{1}{s}\\displaystyle\\sum_{i\\in S_{r}}\\nabla f_{i}(\\mathbf{x}_{i,r+1}),\\mathbf{v}^{r+1}-\\mathbf{v}^{r}\\Big\\rangle}\\\\ &{\\quad\\quad\\quad\\quad+A_{r+1}\\displaystyle\\frac{1}{s}\\displaystyle\\sum_{i\\in S_{r}}\\langle\\nabla f_{i}(\\mathbf{x}_{i,r+1}),\\mathbf{v}^{r}-\\mathbf{x}_{i,r+1}\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\stackrel{(\\mathrm{B},6)}{\\geq}-\\displaystyle\\frac{a_{r+1}^{2}}{2B_{r}}\\Big\\|\\frac{1}{s}\\displaystyle\\sum_{i\\in S_{r}}\\nabla f_{i}(\\mathbf{x}_{i,r+1})\\Big\\|^{2}+A_{r+1}\\displaystyle\\frac{1}{s}\\displaystyle\\sum_{i\\in S_{r}}\\langle\\nabla f_{i}(\\mathbf{x}_{i,r+1}),\\mathbf{y}^{r}-\\mathbf{x}_{i,r+1}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Substituting this lower bound, using convexity of $f_{i}$ and dropping the non-negative $\\begin{array}{r l}{\\frac{\\mu a_{r+1}}{2}\\frac{1}{s}\\sum_{i\\in S_{r}}\\lVert\\mathbf{x}_{i,r+1}-\\mathbf{\\xi}}&{{}}\\end{array}$ vr+1\u22252, we further get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{A_{r}f_{S_{r}}(\\mathbf{x}^{r})+a_{r+1}f_{S_{r}}(\\mathbf{x}^{\\star})+\\frac{B_{r}}{2}\\|\\mathbf{v}^{r}-\\mathbf{x}^{\\star}\\|^{2}}}\\\\ &{}&{\\stackrel{(\\mathrm{B.I.})}{\\geq}A_{r+1}\\frac{1}{s}\\displaystyle\\sum_{i\\in S_{r}}\\left[f_{i}(\\mathbf{x}^{r+1})+\\langle\\nabla f_{i}(\\mathbf{x}^{r+1}),\\mathbf{x}_{i,r+1}-\\mathbf{x}^{r+1}\\rangle\\right]+\\frac{B_{r+1}}{2}\\|\\mathbf{v}^{r+1}-\\mathbf{x}^{\\star}\\|^{2}}\\\\ &{}&{\\;\\;\\;\\;+\\,A_{r+1}\\frac{1}{s}\\displaystyle\\sum_{i\\in S_{r}}\\langle\\nabla f_{i}(\\mathbf{x}_{i,r+1}),\\mathbf{y}^{r}-\\mathbf{x}_{i,r+1}\\rangle-\\frac{a_{r+1}^{2}}{2B_{r}}\\Big\\|\\frac{1}{s}\\displaystyle\\sum_{i\\in S_{r}}\\nabla f_{i}(\\mathbf{x}_{i,r+1})\\Big\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Denote $h_{i}^{r}:=f_{S_{r}}-f_{i}$ . Substituting ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{i\\in S_{r}}\\langle\\nabla f_{i}(\\mathbf{x}^{r+1}),\\mathbf{x}_{i,r+1}-\\mathbf{x}^{r+1}\\rangle=\\sum_{i\\in S_{r}}\\langle-\\nabla h_{i}^{r}(\\mathbf{x}^{r+1}),\\mathbf{x}_{i,r+1}-\\mathbf{y}^{r}\\rangle\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "into the previous display, we get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle A_{r}f_{S_{r}}({\\bf x}^{r})+a_{r+1}f s_{r}({\\bf x}^{\\star})+\\frac{B_{r}}{2}\\|{\\bf v}^{r}-{\\bf x}^{\\star}\\|^{2}}\\ ~}\\\\ {{\\displaystyle~~~~\\geq A_{r+1}f s_{r}({\\bf x}^{r+1})+\\frac{B_{r+1}}{2}\\|{\\bf v}^{r+1}-{\\bf x}^{\\star}\\|^{2}}\\ ~}\\\\ {{\\displaystyle~~~~~~~+A_{r+1}\\frac{1}{s}\\sum_{i\\in S_{r}}\\langle\\nabla f_{i}({\\bf x}_{i,r+1})+\\nabla h_{i}^{r}({\\bf x}^{r+1}),{\\bf y}^{r}-{\\bf x}_{i,r+1}\\rangle-\\frac{a_{r+1}^{2}}{2B_{r}}\\Big\\|\\frac{1}{s}\\sum_{i\\in S_{r}}\\nabla f_{i}({\\bf x}_{i,r+1})\\Big\\|^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We now apply Lemma 15 (with $\\mathbf{x}_{i}=\\mathbf{x}_{i,r+1}$ , $\\mathbf{v}=\\mathbf{y}^{r}$ , $S=S_{r}$ and $\\mathbf{x}=\\mathbf{x}^{r+1}$ ) to get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{s}\\displaystyle\\sum_{i\\in S_{r}}\\left\\langle\\nabla f_{i}(\\mathbf{x}_{i,r+1})+\\nabla h_{i}^{r}(\\mathbf{x}^{r+1}),\\mathbf{y}^{r}-\\mathbf{x}_{i,r+1}\\right\\rangle-\\frac{1}{2\\lambda}\\Big\\|\\frac{1}{s}\\displaystyle\\sum_{i\\in S_{r}}\\nabla f_{i}(\\mathbf{x}_{i,r+1})\\Big\\|^{2}}\\\\ &{\\phantom{\\frac{1}{s}\\displaystyle\\sum_{i\\in S_{r}}}\\geq\\frac{\\lambda-\\delta_{s}}{2}\\displaystyle\\frac{1}{s}\\displaystyle\\sum_{i\\in S_{r}}\\|\\mathbf{y}^{r}-\\mathbf{x}_{i,r+1}\\|^{2}-\\frac{1}{\\lambda}\\displaystyle\\frac{1}{s}\\displaystyle\\sum_{i\\in S_{r}}\\|\\nabla F_{i,r}(\\mathbf{x}_{i,r+1})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Substituting this lower bound into the previous display and using $\\begin{array}{r}{A_{r+1}=\\frac{a_{r+1}^{2}\\lambda}{B_{r}}}\\end{array}$ arB+r1\u03bb, we get the claim. ", "page_idx": 25}, {"type": "text", "text": "D.2 Full Client Participation (Proof of Theorem 6) ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Proof. Applying Lemma 23 and using $\\begin{array}{r}{\\sum_{i=1}^{n}\\|\\nabla F_{i,r}({\\bf x}_{i,r+1})\\|^{2}\\leq\\delta^{2}\\sum_{i=1}^{n}\\|{\\bf x}_{i,r+1}-{\\bf y}^{r}\\|^{2}}\\end{array}$ and $\\lambda=2\\delta$ , for any $r\\geq0$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{A_{r}f(\\mathbf{x}^{r})+a_{r+1}f^{\\star}+\\displaystyle\\frac{B_{r}}{2}\\|\\mathbf{v}^{r}-\\mathbf{x}^{\\star}\\|^{2}}\\\\ {\\geq A_{r+1}f(\\mathbf{x}^{r+1})+\\displaystyle\\frac{B_{r+1}}{2}\\|\\mathbf{v}^{r+1}-\\mathbf{x}^{\\star}\\|^{2}+A_{r+1}\\Big(\\displaystyle\\frac{\\lambda-\\delta}{2}-\\displaystyle\\frac{\\delta^{2}}{\\lambda}\\Big)\\displaystyle\\frac{1}{s}\\sum_{i\\in S_{r}}\\|\\mathbf{x}_{i,r+1}-\\mathbf{y}^{r}\\|^{2}}\\\\ {=A_{r+1}f(\\mathbf{x}^{r+1})+\\displaystyle\\frac{B_{r+1}}{2}\\|\\mathbf{v}^{r+1}-\\mathbf{x}^{\\star}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Subtracting $A_{r+1}f^{\\star}$ on both sides, we get ", "page_idx": 25}, {"type": "equation", "text": "$$\nA_{r+1}[f({\\mathbf x}^{r+1})-f^{\\star}]+\\frac{B_{r+1}}{2}\\|{\\mathbf v}^{r+1}-{\\mathbf x}^{\\star}\\|^{2}\\leq A_{r}[f({\\mathbf x}^{r})-f^{\\star}]+\\frac{B_{r}}{2}\\|{\\mathbf v}^{r}-{\\mathbf x}^{\\star}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Recursively applying the previous display from $r=0$ to $r=R-1$ , we get ", "page_idx": 25}, {"type": "equation", "text": "$$\nA_{R}[f(\\mathbf{x}^{R})-f^{\\star}]+\\frac{B_{R}}{2}\\|\\mathbf{v}^{R}-\\mathbf{x}^{\\star}\\|^{2}\\leq A_{0}[f(\\mathbf{x}^{0})-f^{\\star}]+\\frac{1}{2}\\|\\mathbf{v}^{0}-\\mathbf{x}^{\\star}\\|^{2}=\\frac{1}{2}\\|\\mathbf{x}^{0}-\\mathbf{x}^{\\star}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It remains to apply Lemma 12 and plug in the estimation of the growth of $A_{R}$ . ", "page_idx": 25}, {"type": "text", "text": "Corollary 24. Under the same setting as in Theorem $^{6}$ , to achieve $f(\\mathbf{x}^{R})\\,-\\,f^{\\star}\\,\\leq\\,\\varepsilon,$ , we need at most the following number of communication rounds: ", "page_idx": 25}, {"type": "equation", "text": "$$\nR=\\mathcal{O}\\Bigg(\\sqrt{\\frac{\\delta+\\mu}{\\mu}}\\log\\Bigg(1+\\sqrt{\\frac{\\operatorname*{min}\\{\\mu,\\delta\\}D^{2}}{\\varepsilon}}\\Bigg)\\Bigg).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. When $\\mu\\leq8\\delta$ , by using ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Big(1+\\sqrt{\\frac{\\mu}{8\\delta}}\\Big)^{R}-\\Big(1-\\sqrt{\\frac{\\mu}{8\\delta}}\\Big)^{R}\\geq\\Big(1+\\sqrt{\\frac{\\mu}{8\\delta}}\\Big)^{R}-1\\geq\\exp\\!\\Big(\\frac{\\sqrt{\\mu}R}{\\sqrt{8\\delta}+\\sqrt{\\mu}}\\Big)-1,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "we get ", "page_idx": 25}, {"type": "equation", "text": "$$\nf(\\mathbf{x}^{R})-f^{\\star}\\leq\\frac{2\\mu D^{2}}{\\big[\\big(1+\\textstyle\\sqrt{\\frac{\\mu}{8\\delta}}\\big)^{R}-\\big(1-\\sqrt{\\frac{\\mu}{8\\delta}}\\big)^{R}\\big]^{2}}\\leq\\frac{2\\mu D^{2}}{\\big[\\exp(\\frac{\\sqrt{\\mu}R}{\\sqrt{8\\delta}+\\sqrt{\\mu}})-1\\big]^{2}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Making the right-hand side $\\leq\\varepsilon$ and rearranging, we get the claim. When $\\mu\\geq8\\delta$ , it suffices to ensure that 4\u03b4RD\u22122 \u2264\u03b5. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "D.3 Partial Client Participation ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "It is well known that accelerated stochastic gradient methods are not able to improve the complexity in the stochastic part compared with the basic methods [11]. A similar result is also shown for our accelerated distributed method. ", "page_idx": 26}, {"type": "text", "text": "Theorem 25. Consider Algorithm 2 under the same setting as in Theorem 20. Let ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\lambda=\\Theta\\bigg((\\delta_{s}+\\Delta_{s})+\\frac{(n-s)R}{s(n-1)}\\frac{\\zeta^{2}}{\\varepsilon}\\bigg)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and suppose that, for any $r\\geq0$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{1}{s}\\sum_{i\\in S_{r}}\\mathbb{E}_{\\xi_{i,r}}[\\|\\nabla F_{i,r}(\\mathbf{x}_{i,r+1})\\|^{2}]\\leq\\mathcal{O}\\bigg(\\frac{\\lambda^{2}}{4}\\frac{1}{s}\\sum_{i\\in S_{r}}\\mathbb{E}_{\\xi_{i,r}}[\\|\\mathbf{x}_{i,r+1}-\\mathbf{v}^{r}\\|^{2}]+\\frac{\\lambda\\varepsilon}{4R}\\bigg).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Denote $D:=\\|\\mathbf{x}^{0}-\\mathbf{x}^{\\star}\\|$ . Then, to ensure that $\\mathbb{E}[f(\\mathbf{x}^{R})]-f^{\\star}\\le\\varepsilon$ for some $\\varepsilon>0$ , we need to perform at most the following number of communication rounds: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{R}=\\Theta\\bigg(\\displaystyle\\frac{\\sqrt{\\delta_{s}+\\Delta_{s}}+\\sqrt{\\mu}}{\\sqrt{\\mu}}\\log\\bigg(1+\\sqrt{\\frac{\\operatorname*{min}\\{\\mu,\\lambda\\}D^{2}}{\\varepsilon}}\\bigg)+\\frac{n-s}{n-1}\\frac{\\zeta^{2}}{s\\varepsilon\\mu}\\log^{2}\\bigg(1+\\sqrt{\\frac{\\operatorname*{min}\\{\\mu,\\lambda\\}D^{2}}{\\varepsilon}}\\bigg)\\bigg)}\\\\ {\\leq\\Theta\\bigg(\\sqrt{\\frac{(\\delta_{s}+\\Delta_{s})D^{2}}{\\varepsilon}}+\\frac{n-s}{n-1}\\frac{\\zeta^{2}D^{2}}{s\\varepsilon^{2}}\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The error term that depends on $\\zeta^{2}$ and $\\varepsilon$ is at the same scale as S-DANE, i.e. $\\mathcal{O}\\big(\\frac{\\varsigma^{2}}{\\varepsilon}\\big)$ when $\\mu>0$ and $\\mathcal{O}\\big(\\frac{\\zeta^{2}}{\\varepsilon^{2}}\\big)$ when $\\mu=0$ . Nevertheless, when $s$ is large enough such that this second error becomes no larger than the first optimization term, then ACC-S-DANE can still be faster than S-DANE. ", "page_idx": 26}, {"type": "text", "text": "Proof of Theorem 25. ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proof. According to Lemma 23, we have, for any $r\\geq0$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{A_{r}f_{S_{r}}(\\mathbf x^{r})+a_{r+1}f_{S_{r}}(\\mathbf x^{\\star})+\\displaystyle\\frac{B_{r}}{2}\\|\\mathbf v^{r}-\\mathbf x^{\\star}\\|^{2}}\\\\ {\\geq A_{r+1}f_{S_{r}}(\\mathbf x^{r+1})+\\displaystyle\\frac{B_{r+1}}{2}\\|\\mathbf v^{r+1}-\\mathbf x^{\\star}\\|^{2}}\\\\ {\\qquad\\qquad+A_{r+1}\\displaystyle\\frac{\\lambda-\\delta_{s}}{2}\\displaystyle\\frac{1}{s}\\displaystyle\\sum_{i\\in S_{r}}\\|\\mathbf x_{i,r+1}-\\mathbf y^{r}\\|^{2}-A_{r+1}\\displaystyle\\frac1{\\lambda}\\displaystyle\\frac1{s}\\displaystyle\\sum_{i\\in S_{r}}\\|\\nabla F_{i,r}(\\mathbf x_{i,r+1})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "According to Lemma 14 (with $S=S_{r}$ , $\\mathbf{x}=\\mathbf{x}^{r+1}$ and $\\mathbf{y}=\\mathbf{y}^{r}$ ), for any $\\gamma>0$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{S_{r}}\\left[f({\\mathbf x^{r+1}})-f_{S_{r}}({\\mathbf x^{r+1}})\\right]\\le\\displaystyle\\frac{n-s}{n-1}\\frac{\\gamma\\zeta^{2}}{2s}+\\left(\\displaystyle\\frac{1}{2\\gamma}+\\displaystyle\\frac{\\Delta_{s}}{2}\\right)\\mathbb{E}_{S_{r}}[\\|{\\mathbf x^{r+1}}-{\\mathbf y^{r}}\\|^{2}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\overset{(\\mathrm B.8)}{\\le}\\displaystyle\\frac{n-s}{n-1}\\frac{\\gamma\\zeta^{2}}{2s}+\\left(\\displaystyle\\frac{1}{2\\gamma}+\\displaystyle\\frac{\\Delta_{s}}{2}\\right)\\mathbb{E}_{S_{r}}\\left[\\displaystyle\\frac{1}{s}\\sum_{i\\in S_{r}}\\|{\\mathbf x}_{i,r+1}-{\\mathbf y^{r}}\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Adding $A_{r+1}f(\\mathbf{x}^{r+1})$ to both sides of the first display, taking the expectation over $S_{r}$ on both sides, substituting the previous upper bound, and setting 2s\u03b6(2n(\u2212n1\u2212)\u03b5s) with \u03b5\u2032 > 0, we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{r}f(\\mathbf{x}^{r})+a_{r+1}f^{\\star}+\\frac{B_{r}}{2}\\|\\mathbf{v}^{r}-\\mathbf{x}^{\\star}\\|^{2}}\\\\ &{\\phantom{A_{r}f(\\mathbf{x}^{r})+\\mathbf{1}}\\mathbb{E}_{S_{r}}[f(\\mathbf{x}^{r+1})]+\\frac{B_{r+1}}{2}\\mathbb{E}_{S_{r}}[\\|\\mathbf{v}^{r+1}-\\mathbf{x}^{\\star}\\|^{2}]}\\\\ &{\\phantom{A_{r}f(\\mathbf{x}^{r})+\\mathbf{1}}+A_{r+1}\\Big(\\frac{\\lambda}{2}-\\frac{\\delta_{s}+\\Delta_{s}}{2}-\\frac{1}{2\\gamma}\\Big)\\mathbb{E}_{S_{r}}\\Big[\\frac{1}{s}\\sum_{i\\in S_{r}}^{}\\|\\mathbf{x}_{i,r+1}-\\mathbf{y}^{r}\\|^{2}\\Big]}\\\\ &{\\phantom{A_{r}f(\\mathbf{x}^{r})+\\mathbf{1}}-\\frac{A_{r+1}}{4}\\varepsilon^{\\prime}-\\frac{A_{r+1}}{\\lambda}\\mathbb{E}_{S_{r}}\\Big[\\frac{1}{s}\\sum_{i\\in S_{r}}^{}\\|\\nabla F_{i,r}(\\mathbf{x}_{i,r+1})\\|^{2}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Denote all the randomness $\\{\\xi_{i,r}\\}_{i\\in S_{r}}$ by $\\xi_{r}$ . Since $\\xi_{i,r}$ is independent of the choice of $S_{r}$ for any $i\\,\\in\\,[n]$ , taking the expectation over $\\xi_{r}$ on both sides of the previous display and using the assumption that we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{S_{r},\\xi_{r}}\\Big[\\frac{1}{s}\\sum_{i\\in S_{r}}\\!\\|\\nabla\\dot{F}_{i,r}({\\mathbf x}_{i,r+1})\\|^{2}\\Big]\\overset{\\cdot}{\\le}\\mathbb{E}_{S_{r},\\xi_{r}}\\Big[\\frac{\\lambda^{2}}{4}\\frac{1}{s}\\sum_{i\\in S_{r}}\\!\\|{\\mathbf x}_{i,r+1}-{\\mathbf y}^{\\dot{r}}\\|^{2}\\Big]+\\frac{\\lambda\\varepsilon^{\\prime}}{4}\\;,}\\\\ &{\\qquad\\qquad A_{r}f({\\mathbf x}^{r})+a_{r+1}f^{\\star}+\\frac{B_{r}}{2}\\|{\\mathbf v}^{r}-{\\mathbf x}^{\\star}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\geq A_{r+1}\\mathbb{E}_{S_{r},\\xi_{r}}[f({\\mathbf x}^{r+1})]+\\frac{B_{r+1}}{2}\\,\\mathbb{E}_{S_{r},\\xi_{r}}[\\|{\\mathbf v}^{r+1}-{\\mathbf x}^{\\star}\\|^{2}]}\\\\ {\\displaystyle\\qquad+\\,A_{r+1}\\Big(\\frac{\\lambda}{4}-\\frac{\\delta_{s}+\\Delta_{s}}{2}-\\frac{1}{2\\gamma}\\Big)\\,\\mathbb{E}_{S_{r},\\xi_{r}}\\Big[\\frac{1}{s}\\sum_{i\\in S_{r}}\\|{\\mathbf x}_{i,r+1}-{\\mathbf y}^{r}\\|^{2}\\Big]-\\frac{A_{r+1}}{2}\\varepsilon^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By choosing $\\begin{array}{r}{\\lambda=\\frac{4\\zeta^{2}(n-s)}{s(n-1)\\varepsilon^{\\prime}}+2(\\delta_{s}+\\Delta_{s})}\\end{array}$ , we have that $\\begin{array}{r}{\\frac{\\lambda}{4}-\\frac{\\left(\\delta_{s}+\\Delta_{s}\\right)}{2}-\\frac{1}{2\\gamma}\\geq0}\\end{array}$ . Taking the full expectation on both sides, we get ", "page_idx": 27}, {"type": "text", "text": "$A_{r}\\mathbb{E}[f({\\mathbf x}^{r})]+a_{r+1}f^{\\star}+\\frac{B_{r}}{2}\\,\\mathbb{E}[\\|{\\mathbf v}^{r}-{\\mathbf x}^{\\star}\\|^{2}]\\geq A_{r+1}\\,\\mathbb{E}[f({\\mathbf x}^{r+1})]+\\frac{B_{r+1}}{2}\\,\\mathbb{E}[\\|{\\mathbf v}^{r+1}-{\\mathbf x}^{\\star}\\|^{2}]-\\frac{A_{r+1}}{2}\\varepsilon^{\\prime}.$ Subtracting $A_{r+1}f(\\mathbf{x}^{\\star})$ on both sides , summing up from $r=0$ to $r=R-1$ and using the fact that $A_{0}=0$ , $\\mathbf{v}_{0}=\\mathbf{x}_{0}$ and $B_{0}=1$ , we get ", "page_idx": 27}, {"type": "equation", "text": "$$\nA_{R}\\,\\mathbb{E}[f({\\mathbf x}^{R})-f^{\\star}]+\\frac{B_{R}}{2}\\,\\mathbb{E}[\\|{\\mathbf v}^{R}-{\\mathbf x}^{\\star}\\|^{2}]\\leq\\frac{1}{2}\\|{\\mathbf x}^{0}-{\\mathbf x}^{\\star}\\|^{2}+\\frac{\\varepsilon^{\\prime}}{2}\\sum_{r=1}^{R}A_{r}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Dividing both sides by $A_{R}$ , setting $\\begin{array}{r}{\\varepsilon^{\\prime}=\\frac{\\varepsilon}{R}}\\end{array}$ and using the fact that the sequence $\\{A_{r}\\}$ is non-decreasing, we get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(\\mathbf{x}^{R})]-f^{\\star}\\le\\frac{1}{2A_{R}}\\|\\mathbf{x}^{0}-\\mathbf{x}^{\\star}\\|^{2}+\\frac{\\varepsilon}{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We now apply Lemma 12 with $c=\\lambda$ to get ", "page_idx": 27}, {"type": "equation", "text": "$$\nA_{R}\\geq\\frac{\\big[(1+\\sqrt{\\frac{\\mu}{4\\lambda}})^{R}-(1-\\sqrt{\\frac{\\mu}{4\\lambda}})^{R}\\big]^{2}}{4\\mu}\\geq\\frac{\\big[(1+\\sqrt{\\frac{\\mu}{4\\lambda}})^{R}-1\\big]^{2}}{4\\mu}\\geq\\frac{\\big[\\exp\\bigl(\\frac{\\sqrt{\\mu}R}{\\sqrt{4\\lambda}+\\sqrt{\\mu}}\\bigr)-1\\big]^{2}}{4\\mu}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "when $\\mu\\leq4\\lambda$ , and $\\begin{array}{r}{A_{R}\\,\\geq\\,\\frac{1}{4\\lambda}\\big(1+\\sqrt{\\frac{\\mu}{4\\lambda}}\\big)^{2(R-1)}}\\end{array}$ when $\\mu\\ge4\\lambda$ . Letting these lower bounds be larger than $\\frac{\\|\\mathbf{x}^{0}-\\mathbf{x}^{\\star}\\|^{2}}{\\varepsilon}$ , we get ", "page_idx": 27}, {"type": "equation", "text": "$$\nR=\\Omega\\left(\\frac{\\sqrt{\\mu}+\\sqrt{\\lambda}}{\\sqrt{\\mu}}\\log\\!\\left(1+\\sqrt{\\frac{\\operatorname*{min}\\{\\mu,\\lambda\\}\\|\\mathbf{x}_{0}-\\mathbf{x}^{\\star}\\|^{2}}{\\varepsilon}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Plugging $\\begin{array}{r}{\\lambda=\\Omega\\big(\\frac{\\zeta^{2}(n-s)R}{s n\\varepsilon}+(\\delta_{s}+\\Delta_{s})\\big)}\\end{array}$ into the last display and rearranging, we get the condition for $R$ . ", "page_idx": 27}, {"type": "text", "text": "D.3.1 Stochastic Local Solver ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Corollary 26. Consider Algorithm 2 under the same settings as in Theorem 25. Consider the same stochastic local solver used in Corollary 21. To achieve ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{1}{s}\\sum_{i\\in S_{r}}\\mathbb{E}_{\\xi_{i,r}}[\\|\\nabla F_{i,r}(\\mathbf{x}_{i,r+1})\\|^{2}]\\leq\\mathcal{O}\\bigg(\\frac{\\lambda^{2}}{4}\\frac{1}{s}\\sum_{i\\in S_{r}}\\mathbb{E}_{\\xi_{i,r}}[\\|\\mathbf{x}_{i,r+1}-\\mathbf{y}^{r}\\|^{2}]+\\frac{\\lambda\\varepsilon}{4R}\\bigg),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "each device $i$ requires at most the following number of stochastic mini-batch oracle calls: ", "page_idx": 27}, {"type": "equation", "text": "$$\nK=\\Theta\\bigg(\\bigg[\\frac{L+\\lambda}{\\mu+\\lambda}+\\frac{(L+\\lambda)\\sigma^{2}R}{(\\mu+\\lambda)\\lambda\\varepsilon}\\bigg]\\log\\frac{L+\\lambda}{\\lambda}\\bigg).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. The proof is the same as that for Corollary 21. ", "page_idx": 27}, {"type": "text", "text": "E Dynamic Estimation of Similarity Constant by Line Search ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Theorem 27. Consider Algorithm 3. Suppose that each function $f_{i}$ is $\\mu$ -convex for some $\\mu\\geq0$ , and $\\{f_{i}\\}_{i=1}^{n}$ have $\\delta$ -SOD for some $\\delta>0$ . Let $\\tilde{\\lambda}\\leq2\\delta$ . Then, for any $R\\geq1$ , it holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\nf(\\bar{{\\bf x}}^{R})-f^{\\star}\\le\\frac{\\mu D^{2}}{2[(1+\\frac{\\mu}{4\\delta})^{R}-1]}\\le\\frac{2\\delta D^{2}}{R},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{\\mathbf{x}}^{R}:=\\arg\\operatorname*{min}_{\\mathbf{x}\\in\\{\\mathbf{x}^{1},\\dots,\\mathbf{x}^{R}\\}}f(\\mathbf{x})}\\end{array}$ . To ensure that $f(\\bar{\\mathbf{x}}^{R})-f^{\\star}\\le\\varepsilon$ for any given $\\varepsilon>0,$ , it suffices to set ", "page_idx": 27}, {"type": "equation", "text": "$$\nR=\\Theta\\bigg(\\frac{\\delta+\\mu}{\\mu}\\log\\!\\Big(1+\\frac{\\mu D^{2}}{\\varepsilon}\\Big)\\bigg),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $D:=\\|\\mathbf{x}^{0}-\\mathbf{x}^{\\star}\\|$ . Furthermore, the total number of communication rounds spent inside the $r$ - and $k$ -loops since the start of the algorithm and up to the moment $\\bar{\\mathbf{x}}^{R}$ has been computed is ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{O}(1)\\sum_{k=0}^{R-1}(k_{r}+1)\\leq\\mathcal{O}\\bigg(R+\\log\\frac{2\\delta}{\\tilde{\\lambda}}\\bigg).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "1: Input: $\\tilde{\\lambda}>0$ , $\\mu\\geq0,\\mathbf{x}^{0}=\\mathbf{v}^{0}\\in\\mathbb{R}^{d}$ . Let $h_{i}:=f-f_{i}$ .   \n2: Set $\\lambda_{0,0}=\\tilde{\\lambda}$ .   \n3: for $r=0,1,2,\\ldots$ do   \n4: for $k=0,1,\\ldots$ do   \n5: for each device $i\\in[n]$ in parallel do   \n6: $\\begin{array}{r}{\\mathbf{x}_{i,r+1,k}\\approx\\arg\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{d}}\\big\\{F_{i,r,k}(\\mathbf{x}):=f_{i}(\\mathbf{x})+\\langle\\nabla h_{i}(\\mathbf{v}^{r}),\\mathbf{x}\\rangle+\\frac{\\lambda_{r,k}}{2}\\|\\mathbf{x}-\\mathbf{v}^{r}\\|^{2}\\big\\}.}\\end{array}$   \n7: (stop running the local solver once $\\begin{array}{r}{\\|\\nabla F_{i,r,k}(\\mathbf{x}_{i,r+1,k})\\|\\leq\\frac{\\lambda_{r,k}}{2}\\|\\mathbf{x}_{i,r+1}-\\mathbf{v}^{r}\\|)}\\end{array}$   \n8: Aggregate local models: $\\begin{array}{r}{{\\bf{x}}^{r+1,k}=\\frac{1}{n}\\sum_{i=1}^{n}{\\bf{x}}_{i,r+1,k}}\\end{array}$ .   \n9: $\\begin{array}{r}{\\begin{array}{r l}&{\\mathrm{if}\\;\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\langle\\nabla f_{i}(\\mathbf{x}_{i,r+1,k})+\\nabla h_{i}(\\mathbf{x}^{r+1,k}),\\mathbf{v}^{r}-\\mathbf{x}_{i,r+1,k}\\rangle\\ge\\frac{1}{2\\lambda_{r,k}}\\big\\|\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\nabla f_{i}\\big(\\mathbf{x}_{i,r+1,k}\\big)\\big\\|^{2}\\;\\mathrm{then}}\\\\ &{\\qquad\\qquad\\dots\\quad\\dots\\quad.}\\end{array}}\\end{array}$   \n10: $k_{r}=k$ and break the loop.   \n11: $\\lambda_{r,k+1}=2\\lambda_{r,k}.$ .   \n12: $\\begin{array}{r l}&{\\lambda_{r}=\\lambda_{r,k_{r}},\\ \\mathbf{x}_{i,r+1}=\\mathbf{x}_{i,r+1,k_{r}},\\ \\mathbf{x}^{r+1}=\\mathbf{x}^{r+1,k_{r}},\\ \\lambda_{r+1,0}=\\frac{1}{2}\\lambda_{r}.}\\\\ &{\\mathbf{v}^{r+1}=\\arg\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{d}}\\big\\{\\frac{1}{n}\\sum_{i=1}^{n}[\\langle\\nabla f_{i}(\\mathbf{x}_{i,r+1}),\\mathbf{x}\\rangle+\\frac{\\mu}{2}\\|\\mathbf{x}-\\mathbf{x}_{i,r+1}\\|^{2}]+\\frac{\\lambda_{r}}{2}\\|\\mathbf{x}-\\mathbf{v}^{r}\\|^{2}\\big\\}.}\\end{array}$   \n13: ", "page_idx": 28}, {"type": "text", "text": "Proof. According to Lemma 15 (with $\\mathbf{x}_{i}=\\mathbf{x}_{i,r+1}$ , $\\mathbf{v}=\\mathbf{v}^{r}$ , $S=[n]$ and $\\bar{\\mathbf{x}}_{S}=\\mathbf{x}^{r+1}$ ) and our requirement on $\\|\\nabla F_{i,r}({\\bf x}_{i,r+1})\\|$ , whenever $\\lambda_{r,k}\\ge2\\delta$ , we can estimate ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\langle\\nabla f_{i}(\\mathbf x_{i,r+1})+\\nabla h_{i}(\\mathbf x^{r+1}),\\mathbf v^{r}-\\mathbf x_{i,r+1}\\rangle-\\displaystyle\\frac1{2\\lambda_{r,k}}\\Big\\|\\displaystyle\\frac1n\\sum_{i=1}^{n}\\nabla f_{i}(\\mathbf x_{i,r+1})\\Big\\|^{2}}\\\\ &{\\qquad\\ge\\displaystyle\\frac{\\lambda_{r,k}-\\delta}{2}\\displaystyle\\frac1{n}\\sum_{i=1}^{n}\\|\\mathbf v^{r}-\\mathbf x_{i,r+1}\\|^{2}-\\displaystyle\\frac1{\\lambda_{r,k}}\\displaystyle\\frac1{n}\\sum_{i=1}^{n}\\|\\nabla F_{i,r}(\\mathbf x_{i,r+1})\\|^{2}}\\\\ &{\\qquad\\ge\\displaystyle\\frac{\\lambda_{r,k}-2\\delta}{4}\\displaystyle\\frac1{n}\\sum_{i=1}^{n}\\|\\mathbf v^{r}-\\mathbf x_{i,r+1}\\|^{2}\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Hence, at any iteration of the $r$ -loop, the corresponding $k$ -loop eventually terminates. Further, since $\\lambda_{0,0}\\leq2\\delta$ , we can easily prove by induction that the quantities $\\lambda_{r,k}$ stay reasonably bounded: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\lambda_{r,0}\\leq2\\delta,\\quad\\lambda_{r}\\equiv\\lambda_{r,k_{r}}\\leq4\\delta=:\\lambda_{\\operatorname*{max}},\\qquad\\forall r\\geq0.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proceeding exactly in the same way as in the proof of Lemma 16 and using the termination condition of the $k$ -loop, we conclude that, for any $r\\geq0$ , it holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{1}{\\lambda_{r}}[f({\\mathbf x}^{r+1})-f^{\\star}]+\\frac{1+\\mu/\\lambda_{r}}{2}\\|{\\mathbf v}^{r+1}-{\\mathbf x}^{\\star}\\|^{2}\\leq\\frac{1}{2}\\|{\\mathbf v}^{r}-{\\mathbf x}^{\\star}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In view of (E.1), this means that, for any $r\\geq0$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{1}{\\lambda_{\\operatorname*{max}}}[f({\\mathbf x}^{r+1})-f^{\\star}]+\\frac{1+\\mu/\\lambda_{\\operatorname*{max}}}{2}\\|{\\mathbf v}^{r+1}-{\\mathbf x}^{\\star}\\|^{2}\\le\\frac{1}{2}\\|{\\mathbf v}^{r}-{\\mathbf x}^{\\star}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Following the same proof as in Appendix C.2 but with $\\lambda$ replaced by $\\lambda_{\\mathrm{max}}$ , we obtain the first two claims. ", "page_idx": 28}, {"type": "text", "text": "It remains to estimate the total number of communication rounds required to construct the point $\\bar{\\mathbf{x}}^{R}$ . In order to carry out the $k$ -loop, the server needs to compute $\\nabla f(\\mathbf{v}^{r})$ and send this vector, as well as $\\mathbf{v}^{r}$ and $\\lambda_{r,0}$ , to each client, which requires $\\mathcal{O}(1)$ communication rounds. Every iteration of the $k$ -loop also requires $\\mathcal{O}(1)$ communication rounds, and the total number of such iterations is $k_{r}$ . Thus, every iteration of the $r$ -loop requires $\\mathcal{O}(k_{r}+1)$ communication rounds. Furthermore, during the corresponding rounds, the server may also additionally compute the function value $f(\\mathbf{x}^{r+1})$ needed for updating the output point $\\bar{\\bf x}^{r+1}$ ; this could be done, e.g., inside the $k$ -loop, alongside with the computation of the gradient $\\nabla f(\\mathbf{x}^{r+1,k})$ needed to evaluate the \u201cif\u201d condition. Thus, $\\bar{\\mathbf{x}}^{R}$ can be indeed computed after $\\begin{array}{r}{\\mathcal{O}(1)\\sum_{r=0}^{R-1}(k_{r}+1)}\\end{array}$ communication rounds. To estimate the latter sum, observe that, by construction, for any $r\\geq0$ , we have $\\lambda_{r+1,0}\\equiv\\textstyle{\\frac{1}{2}}\\lambda_{r,k_{r}}=2^{k_{r}-1}\\lambda_{r,0}$ . Taking logarithms, we see that kr = 1 + log2\u03bbr\u03bb+r,10,0 for any r \u22650. Thus, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{r=0}^{R-1}(k_{r}+1)=\\sum_{r=0}^{R-1}\\Bigl(2+\\log_{2}\\frac{\\lambda_{r+1,0}}{\\lambda_{r,0}}\\Bigr)=2R+\\log_{2}\\frac{\\lambda_{R,0}}{\\lambda_{0,0}}\\le2R+\\log_{2}\\frac{2\\delta}{\\tilde{\\lambda}},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the final inequality is due to (E.1) and our choice of $\\lambda_{0,0}$ . ", "page_idx": 28}, {"type": "text", "text": "1: Input: $\\tilde{\\lambda}>0$ , \u00b5 \u22650, $\\mathbf{x}^{0}=\\mathbf{v}^{0}\\in\\mathbb{R}^{d}$ . Let $h_{i}=f-f_{i}$ .   \n2: Set $A_{0}=0$ , $B_{0}=1$ , $\\lambda_{0,0}=\\tilde{\\lambda}$ .   \n3: for $r=0,1,2,\\ldots$ do   \n4: for $k=0,1,\\ldots$ do   \n5: Find $a_{r+1,k}>0$ from the equation $\\begin{array}{r}{\\lambda_{r,k}=\\frac{(A_{r}+a_{r+1,k})B_{r}}{a_{r+1,k}^{2}}}\\end{array}$ (Ar+a2r+1,k)Br. Set Ar+1,k = Ar + ar+1,k.   \n6: $\\begin{array}{r}{\\mathbf{y}^{r,k}=\\frac{A_{r}}{A_{r+1,k}}\\mathbf{x}^{r}+\\frac{a_{r+1,k}}{A_{r+1,k}}\\mathbf{v}^{r}}\\end{array}$   \n7: for each device $i\\in[n]$ in parallel do   \n8: $\\begin{array}{r}{{\\mathbf x}_{i,r+1,k}\\approx\\arg\\operatorname*{min}_{{\\mathbf x}\\in\\mathbb{R}^{d}}\\big\\{F_{i,r,k}({\\mathbf x}):=f_{i}({\\mathbf x})+\\langle\\nabla h_{i}(\\mathbf y^{r,k}),{\\mathbf x}\\rangle+\\frac{\\lambda_{r,k}}{2}\\|{\\mathbf x}-\\mathbf y^{r,k}\\|^{2}\\big\\}.}\\end{array}$   \n9: (stop running the local solver once $\\begin{array}{r}{\\|\\nabla F_{i,r,k}(\\mathbf{x}_{i,r+1,k})\\|\\le\\frac{\\lambda_{r,k}}{2}\\|\\mathbf{x}_{i,r+1,k}-\\mathbf{y}^{r,k}\\|)}\\end{array}$   \n10: Aggregate local models: $\\begin{array}{r}{{\\bf{x}}^{r+1,k}=\\frac{1}{n}\\sum_{i=1}^{n}{\\bf{x}}_{i,r+1,k}}\\end{array}$ .   \n11: $\\begin{array}{r}{\\mathrm{if}\\;\\frac{1}{n}\\sum_{i=1}^{n}\\langle\\nabla f_{i}(\\mathbf{x}_{i,r+1,k})+\\nabla h_{i}(\\mathbf{x}^{r+1,k}),\\mathbf{y}^{r,k}-\\mathbf{x}_{i,r+1,k}\\rangle\\ge\\frac{1}{2\\lambda_{r,k}}\\big\\|\\frac{1}{n}\\sum_{i=1}^{n}\\nabla f_{i}(\\mathbf{x}_{i,r+1,k})\\big\\|^{2}\\;\\mathrm{then}}\\end{array}$   \n12: $k_{r}=k$ and break the loop.   \n13: \u03bbr,k+1 = 2\u03bbr,k.   \n14: $\\begin{array}{r l}&{\\lambda_{r}=\\lambda_{r,k_{r}},\\ \\mathbf{x}_{i,r+1}=\\mathbf{x}_{i,r+1,k_{r}},\\ \\mathbf{x}^{r+1}=\\mathbf{x}^{r+1,k_{r}},\\ a_{r+1}=a_{r+1,k_{r}},\\ \\lambda_{r+1,0}=\\frac{1}{2}\\lambda_{r}.}\\\\ &{A_{r+1}=A_{r}+a_{r+1},\\ B_{r+1}=B_{r}+\\mu a_{r+1}.}\\\\ &{\\mathbf{v}^{r+1}=\\arg\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{d}}\\big\\{\\frac{\\mu_{r+1}}{n}\\sum_{i=1}^{n}[\\langle\\nabla f_{i}(\\mathbf{x}_{i,r+1}),\\mathbf{x}\\rangle+\\frac{\\mu}{2}\\|\\mathbf{x}-\\mathbf{x}_{i,r+1}\\|^{2}]+\\frac{B_{r}}{2}\\|\\mathbf{x}-\\mathbf{v}^{r}\\|^{2}\\big\\}.}\\end{array}$   \n15:   \n16: ", "page_idx": 29}, {"type": "text", "text": "Theorem 28. Consider Algorithm 4. Suppose that each function $f_{i}$ is $\\mu$ -convex for some $\\mu\\geq0$ , and $\\{f_{i}\\}_{i=1}^{n}$ have $\\delta$ -SOD for some $\\delta>0$ . Let $\\tilde{\\lambda}\\leq2\\delta.$ . If $\\mu\\leq16\\delta$ , then, for any $R\\geq1$ , it holds that ", "page_idx": 29}, {"type": "equation", "text": "$$\nf(\\mathbf{x}^{R})-f^{\\star}\\le\\frac{2\\mu D^{2}}{\\big[(1+\\sqrt{\\frac{\\mu}{16\\delta}})^{R}-(1-\\sqrt{\\frac{\\mu}{16\\delta}})^{R}\\big]^{2}}\\le\\frac{8\\delta D^{2}}{R^{2}},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $D\\;:=\\;\\|\\mathbf{x}^{0}\\:-\\:\\mathbf{x}^{\\star}\\|$ . Otherwise, $\\begin{array}{r l r}{f({\\bf x}^{R})\\!\\!\\!}&{{}-\\!\\!\\!}&{\\!\\!\\!f^{\\star}\\!\\!\\!}&{{}\\le}&{\\!\\!\\!\\frac{8\\delta D^{2}}{(1+\\sqrt{\\frac{\\mu}{16\\delta}})^{2(R-1)}}}\\end{array}$ (1+\u221a8\u03b4\u00b5D)22(R\u22121) for any R \u2265 1. To ensure that $f(\\mathbf{x}^{R})-f^{\\star}\\leq\\varepsilon$ for any given $\\varepsilon>0$ , it suffices to set ", "page_idx": 29}, {"type": "equation", "text": "$$\nR=\\Theta\\bigg(\\sqrt{\\frac{\\delta+\\mu}{\\mu}}\\log\\Big(1+\\sqrt{\\frac{\\operatorname*{min}\\{\\mu,\\delta\\}D^{2}}{\\varepsilon}}\\Big)\\bigg).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Furthermore, the total number of communication rounds spent inside the $r$ - and $k$ -loops since the start of the algorithm and up to the moment $\\mathbf{x}^{R}$ has been computed is ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathcal{O}(1)\\sum_{k=0}^{R-1}(k_{r}+1)\\leq\\mathcal{O}\\bigg(R+\\log\\frac{2\\delta}{\\tilde{\\lambda}}\\bigg).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. Using the same reasoning as in the proof of Theorem 27, we can justify that, at any iteration of the $r$ -loop, the corresponding $k$ -loop eventually terminates, and $\\lambda_{r,k}$ stays uniformly bounded as in (E.1). Next, we proceed in the same way as in the proof of Lemma 23 and use the termination condition of the $k$ -loop to obtain that, for any $r\\geq0$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\nA_{r+1}[f({\\mathbf x}^{r+1})-f^{\\star}]+\\frac{B_{r+1}}{2}\\|{\\mathbf v}^{r+1}-{\\mathbf x}^{\\star}\\|^{2}\\leq A_{r}[f({\\mathbf x}^{r})-f^{\\star}]+\\frac{B_{r}}{2}\\|{\\mathbf v}^{r}-{\\mathbf x}^{\\star}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This shows that, for any $R\\geq1$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\nf(\\mathbf{x}^{R})-f^{\\star}\\leq\\frac{D^{2}}{2A_{R}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "To estimate the rate of growth of $A_{R}$ , we use the equation for $\\boldsymbol{a}_{r+1}\\equiv\\boldsymbol{a}_{r+1,k_{r}}$ and the bound on $\\lambda_{r}$ from (E.1). This gives us, for any $r\\geq0$ , the following inequality: ", "page_idx": 29}, {"type": "equation", "text": "$$\n{\\frac{A_{r+1}B_{r}}{a_{r+1}^{2}}}=\\lambda_{r}\\leq\\lambda_{\\operatorname*{max}},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $B_{r}\\equiv1+\\mu A_{r}$ . Invoking Lemma 12, we get a lower bound on $A_{R}$ , and the first claim follows. ", "page_idx": 29}, {"type": "text", "text": "The bound on $R$ via $\\varepsilon$ can be justified by the same argument as in the proof of Corollary 24. ", "page_idx": 29}, {"type": "text", "text": "To estimate the total number of communication rounds, we can follow exactly the same argument as in the proof of Theorem 27. \u53e3 ", "page_idx": 29}, {"type": "image", "img_path": "WukSyFSzDt/tmp/6eb52b826c2bebf2cc409bda44f34574b61b48211f276117e11fc3885ab30afb.jpg", "img_caption": ["Figure E.1: Comparison of S-DANE against DANE for solving a convex quadratic minimization problem with the same number of local steps. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Algorithm 5 S-DANE (DL) ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1: Input: $\\lambda>0$ , $\\eta>0$ , \u03b3 \u2208[0, 1], x0 = v0 \u2208Rd, s \u2208[n]   \n2: for $r=0,1,2\\dots\\mathbf{do}$   \n3: Sample $S_{r}\\,\\in\\,\\left(\\O_{s}^{[n]}\\right)$ uniformly at random without replacement   \n4: for each device $i\\in S_{r}$ in parallel do   \n5: Set $\\begin{array}{r}{{\\bf x}_{i,r+1}\\approx\\arg\\operatorname*{min}_{{\\bf x}\\in\\mathbb{R}^{d}}^{}\\left\\{F_{i,r}({\\bf x})\\right\\}}\\end{array}$ , where   \n6: 7: $\\begin{array}{l}{F_{i,r}({\\bf x}):=f_{i}({\\bf x})-\\langle{\\bf x},\\nabla f_{i}({\\bf v}^{r})-\\nabla f_{S_{r}}({\\bf v}^{r})\\rangle+\\displaystyle\\frac{\\lambda}{2}\\|{\\bf x}-{\\bf v}^{r}\\|^{2}.\\quad\\mathrm{(option~1)}}\\\\ {F_{i,r}({\\bf x}):=f_{i}({\\bf x})+\\displaystyle\\frac{\\lambda}{2}\\|{\\bf x}-{\\bf v}^{r}\\|^{2}.\\qquad\\mathrm{(option~2)}}\\\\ {\\mathrm{Set}\\,{\\bf x}^{r+1}=\\frac{1}{s}\\sum_{i\\in S_{r}}{\\bf x}_{i,r+1}}\\\\ {\\mathrm{Set}\\,{\\bf v}^{r+1}=\\gamma{\\bf x}^{r+1}+(1-\\gamma){\\bf v}^{r}-\\eta\\frac{1}{s}\\sum_{i\\in S_{r}}\\nabla f_{i}({\\bf x}_{i,r+1})}\\end{array}$   \n8:   \n9: ", "page_idx": 30}, {"type": "text", "text": "F Additional Details on Experiments ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "F.1 Convex Quadratics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We generate random vectors $\\{b_{i,j}\\}$ and diagonal matrices $\\{A_{i,j}\\}$ in the same way as in [22] such that $\\operatorname*{max}_{i,j}\\left\\{\\|A_{i,j}\\|\\right\\}\\;=\\;100$ and $\\delta\\,\\approx\\,5$ . We use $n\\,=\\,10$ , $m\\ =\\ 5$ and $d\\:=\\:1000$ . We compare S-DANE and ACC-S-DANE with DANE. We use the standard gradient descent (GD) with constant stepsize $\\begin{array}{r}{{\\frac{1}{200}}\\leq{\\frac{1}{2L}}\\;}\\end{array}$ for all three methods as the local solver, where is the smoothness constant of each $f_{i}$ . We use $\\lambda=5$ for all three methods. We use the stopping criterion $\\begin{array}{r}{\\|\\nabla F_{i,r}({\\bf x}_{i,r+1})\\|\\,\\le\\,\\frac{\\lambda}{2}\\|{\\bf x}_{i,r+1}-{\\bf v}^{r}\\|}\\end{array}$ for our methods $(\\mathbf{v}^{r}$ becomes $\\mathbf{y}^{r}$ for the accelerated method). We use $\\begin{array}{r}{\\|\\nabla\\tilde{F}_{i,r}({\\bf x}_{i,r+1})\\|\\le\\frac{\\lambda}{r+1}\\|{\\bf x}_{i,r+1}-{\\bf x}^{r}\\|}\\end{array}$ for DANE. ", "page_idx": 30}, {"type": "text", "text": "F.2 Deep Learning Tasks ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We simulate the experiment on one NVIDIA DGX A100. We split the training dataset into $n\\,=\\,10$ parts according to the Dirichlet distribution with $\\alpha=0.5$ . We use SGD with a batch size of 512 as a local solver for each device. For all the methods considered in Figure 3, we choose the best number of local steps among $\\{10,20,\\dots80\\}$ (for SCAFFNEW, this becomes the inverse of the probability) and the best learning rate among $\\{0.02,0.05,0.1\\}$ . For this particular task, it is often observed that using control variate makes the training less efficient [38]. The possible issue comes from the fact that local smoothness is often much smaller than local dissimilarity for this task. We here remove the control variate term on line 6 in S-DANE which is defined as $\\langle\\mathbf{x},\\nabla f_{i}(\\mathbf{v}^{r})\\,-\\,\\nabla f_{S_{r}}(\\mathbf{v}^{r})\\rangle$ . Moreover, if we write the explicit formula for $\\mathbf{v}^{r+1}$ on line 8, it becomes $\\begin{array}{r}{\\mathbf{v}^{r+1}=\\gamma\\mathbf{\\dot{x}}^{r+1}+(1-\\dot{\\gamma})\\mathbf{v}_{\\ast}^{\\dot{r}}-\\eta\\frac{1}{s}\\sum_{i\\in S_{r}}\\nabla f_{i}\\underline{{(\\mathbf{x}_{i,r+1})}}_{\\vert\\mathbf{x}_{i,r+1}\\vert}}\\end{array}$ with $\\bar{\\gamma}\\in[0,1]$ and $\\eta>0$ . We set $\\gamma=0.99$ and $\\eta$ to be the local learning rate in our experiment. The method can be found in Algorithm 5. Note that the only difference between it and FEDPROX is the choice of the prox-center. The best number of local steps for the algorithms without using control variates is 70 while for the others is 10 (otherwise, the training loss explodes). ", "page_idx": 30}, {"type": "text", "text": "F.3 Implementation ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "To implement Algorithms 1 and 2 (Algorithm 5 with option 2 is the same as Algorithm 1) , each device has the freedom to employ any efficient optimization algorithm, depending on its computation power and the local data size. At each communication round $r$ , these local algorithms are called to approximately solve the sub-problems defined by $\\{F_{i,r}\\}$ , until the gradient norm satisfies a certain accuracy condition that is stated in the corresponding theorems. The server only needs to perform basic vector operations. Note that $G_{r}$ defined in those algorithms has a unique solution so that $\\mathbf{v}^{r+1}$ can be explicitly derived (in the same form as line 9 in Algorithm 5). ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "G Impact Statement ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "This paper presents work that aims to advance the field of distributed Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here ", "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: In the abstract and the introduction, we first stately clearly what are goals of this study, which is first to achieve the best-known communication complexity and then to maintain the overall computation efficiency. We then discussed the performance of the previous state-of-the-art algorithms. Finally, we compared our methods with them (for instance in Table 1) and showed what contribution and scope we made in this paper. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: In Section 7, we discuss the limitations of our work, in terms of the slightly stronger assumption on $\\mu$ -convexity and the lack of non-convex analysis. On top of that, we also discuss potential future works building on our results. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We make clear assumptions and provide complete and concise proofs in the Appendix for all the claims written in the main text. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We disclose all the necessary information to reproduce our experimental results in Section 6 and Appendix F. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We provide the github link to the code. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details. ", "page_idx": 33}, {"type": "text", "text": "\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The details of the experiments to reproduce our experimental results can be found in Section 6 and Appendix F. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [No] ", "page_idx": 34}, {"type": "text", "text": "Justification: Two sets of our experiments are defined in a totally deterministic setting (no randomness.) We ran the other two sets of experiments three times with different randomness seeds. The results are almost indistinguishable. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We provide the information on the computing resources at the start of Section F for the deep learning experiment. The other experiments are run on a MacBook Pro laptop. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We have read and understood Code of Ethics and we have conducted our research in accordance with it ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: We provide an impact statement in Appendix G. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our resesarch does not have any risks for misuse. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We properly cite and credit the original owners of the open-source datasets LIBSVM and CIFAR 10 in the paper. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: We do not release any new assets in the paper. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 36}, {"type": "text", "text": "\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]