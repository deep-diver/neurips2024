[{"type": "text", "text": "Reparameterization invariance in approximate Bayesian inference ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hrittik Roy\u2020, Marco Miani\u2020 Technical University of Denmark {hroy, mmia}@dtu.dk ", "page_idx": 0}, {"type": "text", "text": "Carl Henrik Ek   \nUniversity of Cambridge,   \nKarolinska Institutet   \nche29@cam.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Philipp Hennig, Marvin Pf\u00f6rtner, Lukas Tatzel ", "page_idx": 0}, {"type": "text", "text": "University of T\u00fcbingen, T\u00fcbingen AI Center {philipp.hennig, lukas.tatzel, marvin.pfoertner}@uni-tuebingen.de ", "page_idx": 0}, {"type": "text", "text": "S\u00f8ren Hauberg Technical University of Denmark sohau@dtu.dk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Current approximate posteriors in Bayesian neural networks (BNNs) exhibit a crucial limitation: they fail to maintain invariance under reparameterization, i.e. BNNs assign different posterior densities to different parametrizations of identical functions. This creates a fundamental flaw in the application of Bayesian principles as it breaks the correspondence between uncertainty over the parameters with uncertainty over the parametrized function. In this paper, we investigate this issue in the context of the increasingly popular linearized Laplace approximation. Specifically, it has been observed that linearized predictives alleviate the common underfitting problems of the Laplace approximation. We develop a new geometric view of reparametrizations from which we explain the success of linearization. Moreover, we demonstrate that these reparameterization invariance properties can be extended to the original neural network predictive using a Riemannian diffusion process giving a straightforward algorithm for approximate posterior sampling, which empirically improves posterior fit. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bayesian deep learning has not seen the same degree of success as deep learning in general. Theoretically, Bayesian posteriors should be superior to point estimates (Devroye et al., 1996), but the practical benefits of having the posterior are all too often not significant enough to justify their additional computational burden. This has raised the question if we even should attempt to estimate full posteriors of all network parameters (Sharma et al., 2023). ", "page_idx": 0}, {"type": "text", "text": "As an example, consider the Laplace approximation (MacKay, 1992), which places a Gaussian in weight space through a second-order Taylor expansion of the log-posterior. When applied to neural networks, this is known to significantly underfti and assign significant probability mass to functions that fail to fit the training data (Lawrence, 2001; Immer et al., 2021a). Fig. 2 (top-left) exemplifies this failure mode for a small regression problem. Interestingly, this behavior is rarely observed outside neural network models, and the failure appears linked to Bayesian deep learning. ", "page_idx": 0}, {"type": "text", "text": "Recently, the linearized Laplace approximation (LLA) has been shown to significantly improve on Laplace\u2019s approximation through an additional linearization of the neural network (Immer et al., 2021b; Khan et al., 2019). We are unaware of any theoretical justification for this rather counterintuitive result: why would an additional degree of approximation iiiiiiiiiiiiiiiiimmmmmmmmmmmmmmmmmppppppppppppppppprrrrrrrrrrrrrrrrrooooooooooooooooovvvvvvvvvvvvvvvvveeeeeeeeeeeeeeeee the posterior fit? ", "page_idx": 0}, {"type": "image", "img_path": "204YOrDHny/tmp/5a360da23444ee7744142f8a0e1af25f54c5d46c047a24e29604d7fc72b106c9.jpg", "img_caption": ["Figure 1: The weight space of a neural network (Eq. 1) overparametrizes the associated function space. This induces families (orange) of weights corresponding to the same functions. Model linearization (left) linearizes these families. In nonlinear models, Gaussian weight distributions (center) do not adapt to the families, while our geometric diffusion (right) captures the associated invariance with a metric (gray ellipses). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "We will show that the failures of Bayesian deep learning can partly be explained by insufficient handling of reparameterizations of network weights, while the LLA achieves infinitesimal invariance to reparameterizations. To motivate, consider the simple network (Fig. 1) ", "page_idx": 1}, {"type": "equation", "text": "$$\nf(x)=w_{1}\\mathrm{ReLU}\\left(w_{2}\\,x\\right);\\qquad f:\\mathbb{R}\\to\\mathbb{R}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "This can be reparametrized to form the same function realization from different weights as $f(x)=$ $w_{1}/\\alpha\\operatorname{ReLU}\\left(\\alpha\\,w_{2}\\,x\\right)$ for any $\\alpha>0$ . That is, the weight-pairs $(w_{1},w_{2})$ and $\\left(w_{1}/\\alpha,\\alpha w_{2}\\right)$ correspond to the same function even if the weights are different (Fig. 1, center). ", "page_idx": 1}, {"type": "text", "text": "Thus the approximate posterior cannot reflect the fundamental property of the true posterior, that it should assign a single unique density to a function regardless of its parametrization. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we analyze the reparameterization group driving deep learning and show that it is a pseudo-Riemannian manifold, with the generalized Gauss-Newton (GGN) matrix as its pseudo-metric. We prove that the commonly observed underfitting of the Laplace approximation (Fig. 2, top left) is caused by high in-distribution uncertainty in directions of reparametrizations (Fig. 2, top center). ", "page_idx": 1}, {"type": "text", "text": "We develop a reparametrization invariant diffusion posterior that proveably does not underfit despite using the neural network predictive (Fig. 2, center row). Figure 1 (right) visualizes how this posterior adapts to the geometry of ", "page_idx": 1}, {"type": "image", "img_path": "204YOrDHny/tmp/c2c60ca91461e040a79db3e29f4e50047ec058ab92bfbcc08ffa84ab6633885e.jpg", "img_caption": ["Figure 2: The function space is decomposed into directions of reparameterizations (kernel) and functional change (non-kernel). We improve the posterior fit by concentrating probability mass on directions of functional change. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "reparametrizations, thereby not underfitting. The diffusion can be simulated with a multi-step EulerMaruyama scheme from which the linearized Laplace approximation (LLA) is a single-step. This link implies that the LLA infinitesimally is invariant to reparameterizations, due to the otherwise counterintuitive linearization (Fig. 1, left). Experimentally, our diffusion consistently improves posterior fit, suggesting that reparameterizations should be given more attention in Bayesian deep learning. ", "page_idx": 1}, {"type": "text", "text": "2 Background: Laplace approximations ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Let $f_{\\mathbf{w}}:\\mathbb{R}^{I}\\rightarrow\\mathbb{R}^{O}$ denote a neural network with weights w, and define a likelihood $p(\\mathbf{y}|f_{\\mathbf{w}}(\\mathbf{x}))$ and a prior $p(\\mathbf{w})$ . Laplace\u2019s approximation (MacKay, 1992) performs a second-order Taylor expansion of the log-posterior around a mode w\u02c6. This results in a Gaussian approximate posterior $\\mathcal{N}(\\mathbf{w}|\\hat{\\mathbf{w}},-\\mathbf{H}_{\\hat{\\mathbf{w}}}^{-1})$ , where $\\mathbf{H}_{\\mathbf{w}}$ is the Hessian matrix. The linearized Laplace approximation (Immer et al., 2021b; Khan et al., 2019) further linearize $f_{\\mathbf{w}}$ at a chosen weight $\\hat{\\bf w}$ , i.e. $f_{\\mathbf{w}}(\\mathbf{x})\\;\\approx$ $f_{\\mathrm{lin}}^{\\hat{\\bf w}}({\\bf w},{\\bf x}))=f_{\\hat{\\bf w}}({\\bf x})+{\\bf J}_{\\hat{\\bf w}}({\\bf x})({\\bf w}-\\hat{\\bf w})$ , where $\\mathbf{J}_{\\hat{\\mathbf{w}}}(\\mathbf{x})=\\partial_{\\mathbf{w}}f_{\\mathbf{w}}(\\mathbf{x})|_{\\mathbf{w}=\\hat{\\mathbf{w}}}\\in\\mathbb{R}^{O\\times D}$ is the Jacobian of $f_{\\mathbf{w}}$ . Here $D=\\dim(\\mathbf{w})$ denotes the number of parameters in the network. Applying the usual Laplace approximation to the linearized model yields an approximate posterior (Immer et al., 2021b), ", "page_idx": 1}, {"type": "equation", "text": "$$\nq(\\mathbf{w}|\\mathcal{D})=\\mathcal{N}\\left(\\mathbf{w}\\mid\\hat{\\mathbf{w}},\\left(\\mathbf{G}\\mathbf{G}\\mathbf{N}_{\\hat{\\mathbf{w}}}+\\alpha\\mathbf{I}\\right)^{-1}\\right)\\qquad\\mathbf{G}\\mathbf{G}\\mathbf{N}_{\\hat{\\mathbf{w}}}=\\sum_{n=1}^{N}\\mathbf{J}_{\\hat{\\mathbf{w}}}(\\mathbf{x}_{n})^{\\top}\\mathbf{H}(\\mathbf{x}_{n})\\mathbf{J}_{\\hat{\\mathbf{w}}}(\\mathbf{x}_{n}),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathbf{H}(\\mathbf{x})=-\\partial_{f_{\\hat{\\mathbf{w}}}(\\mathbf{x})}^{2}\\log p(\\mathbf{y}|f_{\\hat{\\mathbf{w}}}(\\mathbf{x}))\\in\\mathbb{R}^{O\\times O}$ is the Hessian of the log-likelihood and we have assumed a weight prior $\\mathcal{N}(\\mathbf{0},\\alpha^{-1}\\mathbf{I})$ . Note that it is trivial to extend to other prior covariances. This particular covariance is known as the generalized Gauss-Newton (GGN) Hessian approximation, which is commonly used in Laplace approximations (Daxberger et al., 2021b). ", "page_idx": 2}, {"type": "text", "text": "To reduce the notational load we stack the per-datum Jacobians into ${\\mathbf{J}}_{\\hat{\\mathbf{w}}}=[{\\mathbf{J}}_{\\hat{\\mathbf{w}}}(\\mathbf{x}_{1});\\dots;{\\mathbf{J}}_{\\hat{\\mathbf{w}}}(\\mathbf{x}_{N})]\\in$ $\\mathbb{R}^{N O\\times D}$ and similarly for the Hessians, and write the GGN matrix as $\\mathrm{GGN}_{\\hat{\\mathbf{w}}}\\;=\\;\\mathbf{J}_{\\hat{\\mathbf{w}}}^{\\top}\\mathbf{H}\\mathbf{J}_{\\hat{\\mathbf{w}}}$ . For Gaussian likelihoods, the Hessian is an identity matrix and can be disregarded, and for other likelihoods simple expressions are generally available (Immer et al., 2021b). ", "page_idx": 2}, {"type": "text", "text": "Sampled and linearized Laplace. The Laplace approximation gives a Gaussian distribution $q(\\mathbf{w}|\\mathcal{D})$ over the weight space with mean w\u02c6 and covariance $\\Sigma$ . A predictive distribution is obtained by integrating the approximate posterior against the model likelihood, ", "page_idx": 2}, {"type": "equation", "text": "$$\np(\\mathbf{y}^{*}|\\mathbf{x}^{*},\\mathcal{D})=\\mathbb{E}_{\\mathbf{w}\\sim q}[p(\\mathbf{y}^{*}|f(\\mathbf{w},\\mathbf{x}^{*}))]\\approx\\frac{1}{S}{\\displaystyle\\sum_{i=1}^{S}p(\\mathbf{y}^{*}|f(\\mathbf{w}_{i},\\mathbf{x}^{*}))},\\quad\\mathbf{w}_{i}\\sim q.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We refer to this predictive method as sampled Laplace. Recent works have suggested linearizing the neural network in the likelihood model to obtain the predictive distribution (Immer et al., 2021b), ", "page_idx": 2}, {"type": "equation", "text": "$$\np(\\mathbf{y}^{*}|\\mathbf{x}^{*},\\mathcal{D})=\\mathbb{E}_{\\mathbf{w}\\sim q}[p(\\mathbf{y}^{*}|f_{\\mathrm{lin}}^{\\hat{\\mathbf{w}}}(\\mathbf{w},\\mathbf{x}^{*}))]\\approx\\frac{1}{S}\\!\\sum_{i=1}^{S}p(\\mathbf{y}^{*}|f_{\\mathrm{lin}}^{\\hat{\\mathbf{w}}}(\\mathbf{w}_{i},\\mathbf{x}^{*})),\\quad\\mathbf{w}_{i}\\sim q.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This is referred to as linearised Laplace. Immer et al. (2021b) argues that the common choice of approximating the posterior precision with the GGN implicitly linearizes the neural network and hence the predictive distribution should be modified for consistency. ", "page_idx": 2}, {"type": "text", "text": "Sampled Laplace is known to severely underfit, whereas the linearized Laplace approximation does not (Immer et al. 2021b; Fig. 2). It is an open problem why the crude linearization is beneficial (Papamarkou et al., 2024). This paper shows that the benefit is linked to the lack of reparameterization invariance. ", "page_idx": 2}, {"type": "text", "text": "The lack of reparameterization invariance leads to an additional problem for Laplace approximations. The precision of the approximate posterior is given either by the Hessian or the GGN. As shown by Dinh et al. (2017), the Hessian of the loss is not invariant to reparameterizations of the neural network, and the same holds for the GGN. Depending on which parametrization of the posterior mode is chosen by the optimizer, we, thus, get different covariances for the approximate posterior. Empirically, this can render Laplace\u2019s approximation unstable (Warburg et al., 2023). Figure 1 (center) illustrates the phenomena. ", "page_idx": 2}, {"type": "text", "text": "3 Reparameterizations of linear functions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Deep learning models excel when they are highly overparametrized, i.e. when they have significantly more parameters than observations $(D\\gg N O)$ ). This introduces many degrees of freedom to the model, which will be reflected in the Bayesian posterior. However, as we have argued, traditional approximate Bayesian inference does not correctly capture this and assigns different probability measures to identical functions. Next, we characterize these degrees of freedom to design suitable approximate posteriors. To develop the theory, we first consider the linear setting and then extend it to the general case. ", "page_idx": 2}, {"type": "text", "text": "The reparameterizations of linear functions can be characterized exactly. Consider $f(\\mathbf{w})~=~\\mathbf{A}\\mathbf{w}+\\mathbf{b}$ and a possible reparameterization, $g\\ :\\ \\mathbb{R}^{D}\\ \\rightarrow\\ \\mathbb{R}^{D}$ , of this function such that $f(g(\\mathbf{w}))~=~f(\\mathbf{w})$ . It is then evident that $\\mathbf{A}(g(\\mathbf{w})\\,-\\,\\mathbf{w})\\,=\\,\\mathbf{0}$ . This implies that for any reparameterization of a linear function, we have $g(\\mathbf{w})-\\mathbf{w}\\in\\ker(\\mathbf{A})$ , where $\\ker(\\mathbf{A})$ denotes the kernel (nullspace) of A. Hence, the linear function cannot be reparametrized if we restrict ourselves to the non-kernel subspace of the input space or if $\\mathbf{A}$ has a trivial kernel. ", "page_idx": 2}, {"type": "text", "text": "A linearized neural network $f_{\\mathrm{lin}}^{\\mathbf{w}^{\\prime}}:\\mathbf{w},\\mathbf{x}\\mapsto f_{\\mathbf{w}^{\\prime}}(\\mathbf{x})+\\mathbf{J}_{\\mathbf{w}^{\\prime}}(\\mathbf{x})(\\mathbf{w}-\\mathbf{w}^{\\prime})$ is a linear function in the parameters, where we have linearized around $\\mathbf{w}^{\\prime}$ . The above analysis then implies that the kernel of the stacked Jacobian $\\mathbf{J}_{\\mathbf{w}^{\\prime}}$ characterizes the reparameterizations of the linearized network. ", "page_idx": 2}, {"type": "text", "text": "We can also characterize the reparameterizations through the GGN and the corresponding neural tangent kernel (NTK; Jacot et al. 2018), ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{G}\\mathbf{G}\\mathbf{N}_{\\mathbf{w}}=\\mathbf{J}_{\\mathbf{w}}^{\\top}\\mathbf{J}_{\\mathbf{w}},\\qquad\\mathbf{N}\\mathrm{TK}_{\\mathbf{w}}=\\mathbf{J}_{\\mathbf{w}}\\mathbf{J}_{\\mathbf{w}}^{\\top}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "By construction, these have the same non-zero eigenvalues, and thereby also have identical ranks. We, thus, see that the kernel of the Jacobian coincides with that of the GGN, i.e. $\\mathrm{ker}(\\mathbf{J}_{\\mathbf{w}})=\\mathrm{ker}\\big(\\mathrm{GGN}_{\\mathbf{w}}\\big)$ . ", "page_idx": 3}, {"type": "text", "text": "Two orthogonal subspaces. For any selfadjoint operator (such as positive semi-definite matrices like the GGN), the image and the kernel orthogonally span the whole space, i.e. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname{im}(\\mathrm{GGN}_{\\mathbf{w}})\\oplus\\ker\\bigl(\\mathrm{GGN}_{\\mathbf{w}}\\bigr)=\\mathbb{R}^{D},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the kernel is the hyperplane of vectors that are mapped to zero and the image is the hyperplane of vectors spanned by the operator (Fig. 3). For a linearized neural network, $\\mathrm{im}\\big(\\mathrm{GGN}_{\\mathbf{w}}\\big)$ spans the effective parameters $\\mathcal{P}\\subset\\mathbb{R}^{D}$ , i.e. the maximal set of parameters that generate different linear functions RI \u2192 RO when evaluated on the training set. ", "page_idx": 3}, {"type": "image", "img_path": "204YOrDHny/tmp/8d1c921b8846a83e5ce72f80835568f9435672ab50644f91e89b510b7ad4dd01.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 3: The weight space can be decomposed into directions of reparameterizations and functional changes. For linear models (left) these are linear subspaces given by the kernel and the image, respectively. For nonlinear models, these are the nonlinear manifolds $\\mathcal{P}_{\\mathbf{w}_{i}}^{\\perp}$ and $\\mathcal{P}_{\\mathbf{w}_{i}}$ , respectively. ", "page_idx": 3}, {"type": "text", "text": "A Laplace covariance decomposes into the same subspaces. Recall that the posterior precision is $\\boldsymbol{\\Sigma}^{-1}\\stackrel{-}{=}\\mathrm{GGN}_{\\hat{\\mathbf{w}}}+\\alpha\\mathbf{I}$ . Let the eigendecomposition of $\\mathrm{GGN}_{\\hat{\\mathbf{w}}}$ be $\\mathbf{U}^{T}\\Lambda\\mathbf{U}$ , and assume that $\\mathbf{U_{1}}$ and $\\mathbf{U_{2}}$ are the eigenvectors corresponding to the non-zero eigenvalues $\\tilde{\\mathbf{A}}$ , and the zero eigenvalues respectively. These form a basis in the kernel and image subspace as discussed above. Then the covariance is, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma=\\left(\\left[\\frac{\\mathbf{U}_{1}}{\\mathbf{U}_{2}}\\right]^{T}\\left[\\frac{\\tilde{\\Lambda}}{\\mathbf{0}}\\left|\\begin{array}{l}{\\mathbf{0}}\\\\ {\\mathbf{0}}\\end{array}\\right.\\right]\\left[\\frac{\\mathbf{U}_{1}}{\\mathbf{U}_{2}}\\right]+\\alpha\\mathbf{I}\\right)^{-1}=\\mathbf{U}_{1}^{\\mathrm{T}}(\\tilde{\\Lambda}+\\alpha\\mathbf{I}_{k})^{-1}\\mathbf{U}_{1}+\\alpha^{-1}\\mathbf{U}_{2}^{\\mathrm{T}}\\mathbf{U}_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Consequently, we can decompose any sample from the Gaussian $\\mathcal{N}(\\hat{\\mathbf{w}},\\boldsymbol{\\Sigma})$ into a kernel and an image contribution, $\\mathbf{w}=\\hat{\\mathbf{w}}+\\mathbf{w}_{\\mathrm{ker}}+\\mathbf{w}_{\\mathrm{im}}$ , where $\\mathbf{w}_{\\mathrm{ker}}$ is the component of the sample that is in the kernel of $\\mathrm{GGN}_{\\hat{\\mathbf{w}}}$ and $\\mathbf{w}_{\\mathrm{im}}$ is in the image. Note that all probability mass in $\\mathrm{ker}\\big(\\mathrm{GGN}_{\\hat{\\mathbf{w}}}\\big)$ is due to the prior, i.e. we place prior probability on functional reparameterizations even if we can never observe data in support of such. ", "page_idx": 3}, {"type": "text", "text": "Underfitting in sampled Laplace can now be understood. For the linearized approximation, it holds for training data $\\mathbf{x}\\in\\mathcal{X}$ that, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{\\mathrm{lin}}^{\\hat{\\bf w}}(\\hat{\\bf w}\\!+\\!{\\bf w}_{\\mathrm{ker}}\\!+\\!{\\bf w}_{\\mathrm{im}},{\\bf x})=f_{\\mathrm{lin}}^{\\hat{\\bf w}}(\\hat{\\bf w}\\!+\\!{\\bf w}_{\\mathrm{im}},{\\bf x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Hence, the linearized predictive only samples in the image subspace consisting of unique functions. This is not true for the sampled Laplace approximation, which also samples in the kernel subspace. Since sampled Laplace does not linearize the neural network, the kernel does not correspond to reparameterizations. It hence adds \u201cincorrect\u201d degrees of freedom to the posterior as artifacts of the Gaussian approximation. ", "page_idx": 3}, {"type": "image", "img_path": "204YOrDHny/tmp/f318651d058db065469f8f0eec1b74f5b1079539b73452dbe58796d331f73c41.jpg", "img_caption": ["Figure 4: Underfitting of sampled Laplace is less pronounced when the rank of the GGN is higher for a fixed number of parameters. This is consistent with our hypothesis as a high GGN rank implies a lower dimensional kernel. For experimental details, see appendix E.2. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Empirically, sampled Laplace is only observed to underfit in overparametrized models. Fig. 4 illustrates this by increasing the amount of training data to decrease the kernel rank, i.e. reduce the reparametrization issue. We find that as the issue is lessened, sampled Laplace reduces its underfitting. ", "page_idx": 3}, {"type": "text", "text": "4 Reparameterizations of neural networks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We have seen that the parameters of linear models can be decomposed into two linear subspaces corresponding to reparameterizations and functional changes. We next analyze nonlinear models. ", "page_idx": 3}, {"type": "text", "text": "Intuitively, reparameterizations of a nonlinear neural network form continuous trajectories in the parameter space (c.f. Fig. 1). We define that all points along such a trajectory are identical, which changes the weight space geometry to be a manifold. Likewise, the parameter changes corresponding to actual function changes reside on a nonlinear manifold. This is sketched in Fig. 3. Interestingly, the GGN turns out to induce a natural (local) inner product on these nonlinear manifolds, which allows us to both understand and generalize the linearized Laplace approximation. ", "page_idx": 4}, {"type": "text", "text": "4.1 The effective-parameters quotient space ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For a nonlinear neural network $f:\\mathbb{R}^{D}\\times\\mathbb{R}^{I}\\to\\mathbb{R}^{O}$ , the surfaces in weight space along which the function does not change are generally not linear. Here, we formalize these reparameterization invariant surfaces and show that they are a partition of the weight space. ", "page_idx": 4}, {"type": "text", "text": "Definition 4.1. Given a datapoint $\\mathbf{x}\\in\\mathbb{R}^{I}$ , for any $\\mathbf{w}\\in\\mathbb{R}^{D}$ we define the $\\mathbf{x}$ -reparameterizations as the set $\\mathcal{R}_{\\mathbf{x}}^{f}(\\mathbf{w})=\\{\\mathbf{w}^{\\prime}$ such that $f(\\mathbf{w}^{\\prime},\\mathbf{x})=f(\\mathbf{w},\\mathbf{x})\\}$ . Consistently, given a collection of points $\\mathcal{X}\\subseteq\\mathbb{R}^{I}$ , we call the intersection $\\begin{array}{r}{\\mathcal{R}_{\\mathcal{X}}^{f}(\\mathbf{w})=\\bigcap_{\\mathbf{x}\\in\\mathcal{X}}\\mathcal{R}_{\\mathbf{x}}^{f}(\\mathbf{w})\\ \\chi}\\end{array}$ -reparameterizations. ", "page_idx": 4}, {"type": "text", "text": "Trivially, $\\mathbf{w}\\in\\mathcal{R}_{\\mathcal{X}}^{f}(\\mathbf{w})$ for any choice of $\\mathcal{X}$ . We next define the subset of $\\mathcal{X}$ -reparameterizations which can be obtained via a smooth deformation from w. ", "page_idx": 4}, {"type": "text", "text": "Definition 4.2. We say that a piecewise differentiable function $\\gamma:[0,1]\\rightarrow\\mathbb{R}^{D}$ is a homotopy of $(\\mathbf{w},\\hat{\\mathbf{w}})$ if $\\gamma(0)=\\mathbf{w}$ and $\\gamma(1)=\\mathbf{w}^{\\prime}$ . The set of $\\mathcal{X}$ -smooth-reparameterizations is defined as, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\mathcal{R}}_{\\mathcal{X}}^{f}(\\mathbf{w})=\\left\\{\\mathbf{w}^{\\prime}\\;\\mathrm{such\\;that}\\;\\stackrel{\\exists\\gamma\\;\\mathrm{a}\\;\\mathrm{homotopy\\;of}\\;(\\mathbf{w},\\mathbf{w}^{\\prime})}{\\gamma(t)\\in\\mathcal{R}_{\\mathcal{X}}^{f}(\\mathbf{w})\\;\\forall t\\in[0,1]}\\;\\;\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "A homotopy $\\gamma$ is, thus, a smooth path along which all neural networks have identical predictions on $\\mathcal{X}$ . We consider two networks, w and $\\mathbf{w}^{\\prime}$ , similar if they can be connected by such a homotopy. Formally, we define the relation $\\sim$ over $\\mathbb{R}^{D}$ as $\\mathbf{w}{\\sim}\\mathbf{w}^{\\prime}$ if $\\mathbf{w}^{\\prime}\\in\\bar{\\mathcal{R}}_{\\mathcal{X}}^{f}(\\mathbf{w})$ . ", "page_idx": 4}, {"type": "text", "text": "We next use this relation to form a new view on the weight space $\\mathbb{R}^{D}$ in which similar weights are seen as one point. This can be realized using quotient spaces (Lee, 2012). These are well-studied spaces that are constructed by considering a collection of points in one space as a single point in a new space. In our case, we have the following result. ", "page_idx": 4}, {"type": "text", "text": "Lemma 4.3. $\\sim$ is an equivalence relation, i.e. it is transitive, symmetric and reflexive. We can form the quotient space $\\mathcal{P}=\\mathbf{\\dot{R}}^{D}/\\sim$ of effective parameters. We denote $[\\mathbf{w}]\\in\\mathcal{P}$ the equivalence class of an element w $\\in\\mathbb{R}^{D}$ . ", "page_idx": 4}, {"type": "text", "text": "This quotient structure gives a rich mathematical foundation to construct reparameterization invariant neural networks. Within the quotient, two effective parameters $\\left[\\mathbf{w}_{1}\\right]$ $\\mathbf{\\bar{\\rho}}[\\mathbf{w}_{2}]\\bar{\\mathbf{\\rho}}\\in\\mathcal{P}$ are the same point if and only if $\\mathbf{w}_{1}\\sim\\mathbf{w}_{2}$ . This means that all parameters $\\mathbf{w}\\in[\\mathbf{w}_{1}]$ gives the same function over $\\mathcal{X}$ . ", "page_idx": 4}, {"type": "text", "text": "4.2 The effective-parameters manifold ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Geometry is the mathematical language of invariances. To this end would like to endow the weight space with a geometric structure such that two weights, $\\mathbf{w}_{1}$ and $\\mathbf{w}_{2}$ , corresponding to the same function, have a distance of zero, i.e. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname{dist}(\\mathbf{w}_{1},\\mathbf{w}_{2})=0\\quad\\Leftrightarrow\\quad\\mathbf{w}_{1}\\sim\\mathbf{w}_{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Since the weights generate the same function, we define a metric that measures differences in function values on the training data. Consider weights w and an infinitesimal displacement $\\epsilon$ , we then define, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{dist}^{2}(\\mathbf{w},\\mathbf{w}+\\pmb{\\epsilon})=\\sum_{n=1}^{N}\\|\\pmb{f}(\\mathbf{w},\\mathbf{x}_{n})-\\pmb{f}(\\mathbf{w}+\\pmb{\\epsilon},\\mathbf{x}_{n})\\|^{2}=\\pmb{\\epsilon}^{\\top}\\mathbf{G}\\mathbf{G}\\mathbf{N}_{\\mathbf{w}}\\pmb{\\epsilon}+\\mathcal{O}(\\epsilon^{3}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the last step follows from a first-order Taylor expansion of $f$ around w. This is a standard pullback metric $(f^{*}H)_{\\mathbf{w}}=\\operatorname{GGN}_{\\mathbf{w}}$ commonly used in Riemannian geometry. This implies that the GGN matrix infinitesimally defines an inner product, i.e. it is a Riemannian metric. By integrating over paths, the distance extend to any pair of points and satisfies Eq. 8 (Lee, 2012). ", "page_idx": 4}, {"type": "text", "text": "Watch out! It\u2019s a pseudo-metric. We have already seen that in overparametrized models, the GGN is rank-deficient, which implies that it is not positive definite. Consequently, it is not a Riemannian metric but rather a pseudo-Riemannian metric. A pseudo-metric can be a counterintuitive object: two points $\\mathbf{w}_{1}$ and $\\mathbf{w}_{2}$ at distance zero may have different pseudo-metrics $(f^{*}H)_{\\mathbf{w}_{1}}\\neq$ $(f^{\\bar{*}}H)_{\\mathbf{w}_{2}}$ . This is reflected in the Laplace approximation. The covariance prescribed by the Laplace approximation is $\\Sigma_{\\hat{\\mathbf{w}}}=(\\nabla_{\\mathbf{w}}^{2}\\mathcal{L}(\\hat{\\mathbf{w}})+\\alpha\\mathbf{I})^{-1}$ , where ${\\mathcal{L}}({\\bf w})$ is shorthand for the training log-likelihood. The Hessian is exactly the pullback pseudo-metric $\\nabla_{\\mathbf{w}}^{2}\\mathcal{L}(\\hat{\\mathbf{w}})=(f^{*}H)_{\\hat{\\mathbf{w}}}$ , which is not invariant to reparameterizations of the neural network. Specifically, for a reparameterization function $g$ that is also a diffeomorphism, the change of variable rules states that, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\underbrace{\\nabla_{\\mathbf{w}}^{2}\\mathcal{L}(g(\\hat{\\mathbf{w}}))}_{\\Sigma_{g(\\hat{\\mathbf{w}})}^{-1}-\\alpha\\mathbf{I}}=\\nabla_{\\mathbf{w}}g(\\hat{\\mathbf{w}})^{\\top}\\underbrace{\\nabla_{\\mathbf{w}}^{2}\\mathcal{L}(\\hat{\\mathbf{w}})}_{\\Sigma_{\\hat{\\mathbf{w}}}^{-1}-\\alpha\\mathbf{I}}\\nabla_{\\mathbf{w}}g(\\hat{\\mathbf{w}}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This means that, while each parameter w has its well-defined covariance $\\Sigma_{\\mathbf{w}}$ , each equivalence class does not have a unique one, since $\\hat{\\mathbf{w}}$ and $g(\\hat{\\mathbf{w}})$ belong to the same equivalence class and $\\Sigma_{\\hat{\\mathbf{w}}}\\neq\\Sigma_{g(\\hat{\\mathbf{w}})}$ . ", "page_idx": 5}, {"type": "text", "text": "Non-Gaussian likelihoods. The Euclidean distance measure in Eq. 9 corresponds to choosing a Gaussian likelihood. The distance definition readily extends to other likelihoods and the corresponding metric takes the form of the generalized Gauss-Newton matrix $\\mathbf{J}_{\\mathbf{w}}^{\\top}\\mathbf{H}\\mathbf{J_{w}}$ , where $\\mathbf{H}$ denotes the Hessian of the log-likelihood. For both Gaussian and Bernoulli likelihoods, this Hessian is positive definite, but e.g. the cross entropy has a rank-deficient Hessian and, thus, induces a pseudo-metric. ", "page_idx": 5}, {"type": "text", "text": "An impractical solution. The unfortunate behavior of approximate posteriors assigning different probabilities to the same function could be rectified by marginalizing over the set of reparameterizations of w, i.e. $\\begin{array}{r}{\\int_{\\mathbf{w}^{\\prime}\\in\\mathcal{R}(\\mathbf{w})}q(\\mathbf{w}^{\\prime}|\\mathcal{D})\\mathrm{d}\\mathbf{w}^{\\prime}}\\end{array}$ . While this construction solves the highlighted problem, its complexity makes it impractical and we are unaware of any works along these lines. ", "page_idx": 5}, {"type": "text", "text": "When restricted to a smaller class of reparameterization (the ones homotopic to the identity), the integral can be thought of as \u201ccollapsing\u201d each reparameterization equivalence class to a single point in $\\breve{\\mathcal{P}}=\\mathbb{R}^{D}/\\sim$ formalized in Lemma 4.3. Nontrivially, the pullback metric implicitly performs a similar operation, as shown later in Theorem 4.5. This connection motivates the dive into Riemannian geometry: we get a tractable approach to engaging with neural network reparameterizations. ", "page_idx": 5}, {"type": "text", "text": "4.3 Topological equivalence of the two views ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "So far we described two a priori very different objects: the quotient space $\\mathcal{P}=\\mathbb{R}^{D}/\\sim$ and the pseudo-Riemannian manifold $(\\mathbb{R}^{D},\\mathbb{G}\\dot{\\mathbf{G}}\\mathbf{N}_{\\mathbf{w}})$ . We referred to both of them as effective parameters and this is no coincidence as there is a natural relationship between the points at distance zero according to the pseudo-metric and the equivalence classes. ", "page_idx": 5}, {"type": "text", "text": "Proposition 4.4. For any $\\mathbf{w}_{0},\\mathbf{w}_{1}\\in\\mathbb{R}^{D}$ it holds ", "page_idx": 5}, {"type": "equation", "text": "$$\nd_{f^{*}H}(\\mathbf{w}_{0},\\mathbf{w}_{1})=0\\quad\\Longleftrightarrow\\quad[\\mathbf{w}_{0}]=[\\mathbf{w}_{1}]\\in\\mathcal{P}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Even better, these two spaces share the same topological structure. To state this we need a notion of distance on the quotient space and the most natural choice is to inherit the Euclidean distance $\\|\\cdot\\|$ from $\\mathbb{R}^{D}$ . This distance is defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\nd_{\\mathcal{P}}([\\mathbf{w}],[\\mathbf{w}^{\\prime}])=\\operatorname*{inf}\\left\\{\\|p_{1}-q_{1}\\|+\\ldots+\\|p_{n}-q_{n}\\|\\right\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the infimum is taken over all finite sequences $p_{1},\\ldots,p_{n}$ and $q_{1},\\ldots,q_{n}$ such that $\\left[{\\bf w}\\right]=\\left[p_{1}\\right]$ $[p_{i+1}]=[q_{i}]$ and $[q_{n}]=[\\mathbf{w}^{\\prime}]$ . ", "page_idx": 5}, {"type": "text", "text": "This distance $d_{\\mathcal{P}}$ induces a topology on the quotient space $\\mathcal{P}$ which is equivalent to the topology induced by the pullback distance $d_{f^{*}H}$ on the pseudo-Riemannian manifold. Formally ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.5. For any $\\mathbf{w}_{0},\\mathbf{w}_{1}\\in\\mathbb{R}^{D}$ , for any $\\epsilon>0$ there exists $\\delta>0$ such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{d_{\\mathcal{P}}([\\mathbf{w}_{0}],[\\mathbf{w}_{1}])<\\delta}&{\\implies\\;\\;d_{f^{*}H}(\\mathbf{w}_{0},\\mathbf{w}_{1})<\\epsilon}\\\\ {d_{f^{*}H}(\\mathbf{w}_{0},\\mathbf{w}_{1})<\\delta}&{\\implies\\;\\;d_{\\mathcal{P}}([\\mathbf{w}_{0}],[\\mathbf{w}_{1}])<\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This result connects an abstract quotient space $\\mathcal{P}$ with the pseudo-Riemannian metric $\\mathrm{GGN}_{\\mathbf{w}}$ . The quotient captures useful intuitions but is difficult to leverage computationally. In contrast, the pseudometric has some counterintuitive aspects but we can identify the underlying Riemannian structure which leads to tractable algorithms (Sec. 5). ", "page_idx": 5}, {"type": "text", "text": "A tale of two manifolds. For any given parameter $\\mathbf{w}\\in\\mathbb{R}^{D}$ and training set $\\mathcal{X}$ , we show that there exist two Riemannian manifolds $(\\mathcal{P}_{\\mathbf{w}},\\mathfrak{m})$ and $(\\mathcal{P}_{\\mathbf{w}}^{\\perp},\\mathfrak{m}^{\\perp})$ embedded in $\\mathbb{R}^{\\breve{D}}$ , illustrated in Fig. 3. They capture the functional change and reparameterization properties respectively, but, differently from the previously studied $(\\mathbb{R}^{D},\\mathbb{G}\\mathbb{G}\\mathbf{N}_{\\mathbf{w}})$ , they are Riemannian manifolds without degenerate directions in their metrics. Formally, ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.6. For any parameter w suppose the set of parameters that generate the same predictions is denoted by $\\mathcal{P}_{\\mathbf{w}}^{\\perp}=\\dot{\\{\\mathbf{w}^{\\prime}\\in\\mathbb{R}^{D}}}$ such that $f(\\mathbf{w}^{\\prime},x)\\,\\stackrel{\\cdot}{=}\\,f(\\mathbf{w},x)$ for all $x\\in\\mathcal{X}\\}$ . Then this set is $a$ smooth manifold embedded in $\\mathbb{R}^{D}$ . Furthermore, the set of parameters that locally generates unique predictions, $\\mathcal{P}_{\\mathbf{w}}$ is also a submanifold embedded in $\\mathbb{R}^{D}$ . ", "page_idx": 6}, {"type": "text", "text": "They are the direct generalization to the nonlinear case of the two spaces involved in Eq. 6, where $\\mathcal{P}_{\\mathbf{w}}$ plays the role of the image and $\\mathcal{P}_{\\mathbf{w}}^{\\perp}$ plays the role of the kernel. When $f$ is linear, they are identical. ", "page_idx": 6}, {"type": "text", "text": "In general, $\\mathcal{P}_{\\mathbf{w}}$ and $\\mathcal{P}_{\\mathbf{w}}^{\\perp}$ intersect only in $\\mathbf{w}$ , and the two respective tangent spaces in w span all directions. They can be thought of as two collections of parameters, and the associated functions have different properties: (1) $\\bar{\\mathcal{P}}_{\\mathbf{w}}^{\\perp}$ is entirely contained in the same equivalence class $\\mathcal{P}_{\\mathbf{w}}^{\\perp}\\subseteq[\\mathbf{w}]$ , thus all the parametrized functions are identical on the train set; in contrast, (2) $\\mathcal{P}_{\\mathbf{w}}$ never intersects the same equivalence class more than one time, at least locally, thus the parametrized functions always changes when moving in any direction. Thus $\\mathcal{P}_{\\mathbf{w}}$ resembles the effective parameter manifold $\\mathcal{P}$ , but with the difference of being an actual Riemannian manifold. These two manifolds exist under the assumption that Jacobian is full rank (see proof in appendix C). ", "page_idx": 6}, {"type": "text", "text": "The two metrics $\\mathfrak{m}$ and ${\\mathfrak{m}}^{\\perp}$ are not uniquely defined. A natural choice for m is to restrict $\\mathrm{GGN}_{\\mathbf{w}}$ to the tangent space of $\\mathcal{P}_{\\mathbf{w}}$ corresponding to the non-zero eigenvectors, i.e. $\\mathfrak{m}=\\mathbf{G}\\mathbf{G}\\mathbf{N}_{\\mathbf{w}}^{+}$ . While, for ${\\mathfrak{m}}^{\\perp}$ we can inherit the Euclidean metric, i.e. $\\mathfrak{m}^{\\perp}=\\alpha\\mathbf{I}$ for $\\alpha>0$ . ", "page_idx": 6}, {"type": "text", "text": "5 Exploring manifolds with random walks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "SDEs on manifolds. Given a Riemannian manifold $(\\mathrm{M},\\mathbf{G})$ , the simplest choice of distribution that respects the Riemannian metric $\\mathbf{G}$ is a Riemannian diffusion (or Brownian motion, c.f. Hsu (2002)) stopped at time $t$ . This follows the stochastic differential equation (Girolami & Calderhead, 2011), ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{w}=\\sqrt{2\\tau}\\mathbf{G}(\\mathbf{w})^{-\\frac{1}{2}}\\mathrm{d}W+\\tau\\Gamma\\mathrm{d}t\\qquad\\mathrm{where}\\quad\\Gamma_{i}(\\mathbf{w})=\\sum_{j=1}^{D}\\frac{\\partial}{\\partial\\mathbf{w}_{j}}(\\mathbf{G}(\\mathbf{w})^{-1})_{i j}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Practically speaking this simple process can be simulated using an Euler\u2013Maruyama (Maruyama, 1955) scheme. The Christoffel symbols, $\\Gamma_{i}(\\pmb\\theta)$ , are commonly disregarded as they have a high computational cost, and Li et al. (2015) showed that the resulting error is bounded. ", "page_idx": 6}, {"type": "text", "text": "Using the Euler\u2013Maruyama integrator with step size $h_{t}$ , setting $\\tau=1$ corresponding to standard Bayesian inference and disregarding the term involving the Christoffel symbols $\\Gamma$ , we obtain the simple update rule $\\mathbf{w}_{t+1}\\,=\\,\\mathbf{w}_{t}\\,+\\,\\sqrt{2h_{t}}\\mathbf{G}(\\mathbf{w}_{t})^{-\\frac{1}{2}}\\epsilon$ , where $\\mathbf{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ . This applies to any Riemannian manifold. However, the effective-parameter $(\\mathbb{R}^{D},\\mathbf{GGN}_{\\mathbf{w}})$ is only pseudo-Riemannian, we explore three Riemannian alternatives: $(\\mathbb{R}^{D},\\mathrm{GGN}_{\\mathbf{w}}+\\alpha\\mathbb{I})$ , $(\\mathcal{P}_{\\mathbf{w}}^{\\perp},\\alpha\\mathbf{I})$ and $(\\mathcal{P}_{\\mathbf{w}},\\mathtt{G G N}_{\\omega}^{+})$ ", "page_idx": 6}, {"type": "text", "text": "Diffusion on $(\\mathbb{R}^{D},\\mathbf{GGN}_{\\mathbf{w}}+\\alpha\\mathbf{I})$ . The Laplace approximation can also be written as a diffusion on a manifold. As we saw in Sec. 2, the Laplace approximation can be written as $\\mathbf{w}|D\\sim\\mathcal{N}(\\hat{\\mathbf{w}},\\boldsymbol{\\Sigma})$ with $\\boldsymbol{\\Sigma}^{-1}=\\operatorname{GGN}_{\\hat{\\mathbf{w}}}+\\alpha\\mathbf{I}$ . This can also be written as a sample at $t=1$ of a Riemannian diffusion on a manifold with a constant metric, $(\\mathbb{R}^{D},\\mathbf{G})$ , where $\\mathbf{G}=\\mathtt{G G N}_{\\hat{\\mathbf{w}}}+\\alpha\\mathbf{I}$ . The SDE $\\mathrm{d}\\mathbf{w}=\\mathbf{G}^{-\\frac{1}{2}}\\mathrm{d}W$ have a marginal distribution at $t=1$ that exactly match the standard Laplace approximation. Note that this formulation does not rely on the approximation of the SDE that disregards the term involving the Christoffel symbols $\\Gamma$ as these are zero for constant metrics. Hence, the above is exactly a Riemannian diffusion on the manifold with a constant metric given by the GGN at the MAP parameter. Note that this is only a valid diffusion for $\\alpha>0$ in which case it is not reparametrization invariant. ", "page_idx": 6}, {"type": "text", "text": "Kernel-manifold diffusion. The kernel-manifold $(\\mathcal{P}_{\\mathbf{w}}^{\\perp},\\alpha\\mathbf{I})$ consists of parameters that generate the same function over the training set. The effect of diffusion on this manifold and using the neural network predictive is similar to sampling from the kernel subspace while using the linearized predictive. On the training set the predictive variance is 0 because it only samples reparametrizations of the MAP predictions. On out-of-distribution data, the variance is greater than 0 if at least one of the reparameterizations on the training set is not a global reparameterization. This leads to a clear separation in the predictive variance of in-distribution and out-of-distribution data (Fig. 2) and further implies that this diffusion distribution never underfits. Stated formally, ", "page_idx": 6}, {"type": "table", "img_path": "204YOrDHny/tmp/5b62b6c9a44e35411e1c55ebe9ab2f9e18ed58a4f577e60bf84f9ec2f666d409.jpg", "table_caption": ["Table 1: In-distribution performance across methods trained on MNIST, FMNIST and CIFAR-10. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "204YOrDHny/tmp/d6976e105777e72ab46a92fb792662a00b4c7bf1b715bfdb82e162c0a7b599cb.jpg", "table_caption": ["Table 2: Out-of-distribution AUROC (\u2191) performance for MNIST, FMNIST and CIFAR-10. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Theorem 5.1. $\\mathrm{Var}_{\\mathbf{w}\\sim\\mathcal{P}_{\\hat{\\mathbf{w}}}^{\\perp}}\\left[f(\\mathbf{w},\\mathbf{x})\\right]=0$ for train data $\\mathbf{x}\\in\\mathcal{X}$ . For a test point $\\mathbf{x}_{t}\\notin\\boldsymbol{\\mathcal{X}}$ , if there exists a reparameterization $\\mathbf{w}^{\\prime}\\in\\mathcal{\\bar{R}}_{\\mathcal{X}}^{f}(\\hat{\\mathbf{w}})$ such that $\\mathbf{w}^{\\prime}\\notin\\bar{\\mathcal{R}}_{\\mathcal{X}\\cup\\{\\mathbf{x}_{t}\\}}^{f}(\\hat{\\mathbf{w}}).$ , then $\\mathrm{Var}_{\\mathbf{w}\\sim\\mathcal{P}_{\\hat{\\mathbf{w}}}^{\\perp}}\\left[f(\\mathbf{w},\\mathbf{x}_{t})\\right]>0$ . ", "page_idx": 7}, {"type": "text", "text": "Non-kernel-parameter manifold diffusion. The non-kernel-parameter manifold $(\\mathcal{P}_{\\mathbf{w}},\\mathtt{G G N}_{\\omega}^{+})$ consists of parameters that generate unique functions over the training set. Diffusion on this manifold samples functions that are necessarily different from the MAP predictions on the training set. However, the predictive variance in the training set is bounded such that the functional diversity in the predictive samples reflects the intrinsic variance of the training data (Fig. 2). ", "page_idx": 7}, {"type": "text", "text": "This is the only considered diffusion that acts on a Riemannian manifold while being reparametrization invariant, i.e. $\\bar{\\mathcal{R}}_{\\chi}^{f}(\\mathbf{w})=\\{\\mathbf{w}\\}$ . We call this Laplace diffusion and study it empirically in Sec. 7. ", "page_idx": 7}, {"type": "text", "text": "6 Related work ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Bayesian deep learning techniques are still in their infancy and generally involve poorly understood approximations. The arguably most popular tool for uncertainty quantification is ensembles (Lakshminarayanan et al., 2017; Hansen & Salamon, 1990). Several approaches make Gaussian approximations to the true posterior, including \u2018Bayes by backprop\u2019 (Blundell et al., 2015), stochastic weight averaging (SWAG) (Maddox et al., 2019) and the Laplace approximation (MacKay, 1992; Daxberger et al., 2022; Antor\u00e1n et al., 2023; Deng et al., 2022; Miani et al., 2022). ", "page_idx": 7}, {"type": "text", "text": "The high dimensionality of the weight space gives rise to significant computational challenges when constructing Bayesian approximations. This has motivated various low-rank approximations (review in Daxberger et al., 2021a), e.g. last layer approximations (Kristiadi et al., 2020), subnetwork inference (Daxberger et al., 2021c), subspace inference (Izmailov et al., 2020) or even PCA in weight space (Maddox et al., 2019). Such approaches lessen the computational load, while often improving predictive performance. Our analysis sheds light on why crude approximations perform favorably: smaller models are less affected by reparameterization issues. Our diffusion process, thus, provides an alternative, and less heuristic, path forward. ", "page_idx": 7}, {"type": "text", "text": "MacKay (1998) noted the importance of the choice of basis in Laplace approximations; our pseudoRiemannian view can be seen as having a continuously changing basis. Kristiadi et al. (2023) studied how a metric transforms under a bijective differentiable change of variables. They enforce geometric ", "page_idx": 7}, {"type": "image", "img_path": "204YOrDHny/tmp/7e9ff56b1af9d8ce49535bf7225926a90b1418a019cc81e8d8fbf83b1acb9f86.jpg", "img_caption": ["Figure 5: Benchmark results for Rotated MNIST (similar results for FMNIST and CIFAR are in appendix E.3.2). Sampled Laplace significantly underfits even for non-rotated data. Laplace diffusion consistently outperforms the other methods. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "consistency, highlighting, e.g., the non-invariance of the GGN to a change of variables. Petzka et al.   \n(2019); Jang et al. (2022) point to the same inconsistency with an emphasis on flatness measures. ", "page_idx": 8}, {"type": "text", "text": "Kim et al. (2022) and Antor\u00e1n et al. (2022) study global (rather than data-dependant) reparametrizations associated with specialized architectures. While analytic expressions can be obtained, the results do not apply to general networks. While not expressed in terms of reparametrizations, Izmailov et al. (2021) show that linearly dependent datasets give rise to a hyperplane in the kernel manifold. Kim et al. (2024) also study the kernel of the GGN in the context of influence functions. These works characterize subsets of the reparametrization group. We provide the first architecture-agnostic characterization of all continuous reparametrizations. ", "page_idx": 8}, {"type": "text", "text": "In a closely related work, Bergamin et al. (2024) introduced a Riemannian Laplace approximation (Hauberg, 2018) that improves posterior fit over a range of tasks. Furthering this line of research, Yu et al. (2023) explored the use of the Fisher information metric within this framework. While sharing the language of Riemannian geometry, our work focuses on analyzing the effectiveness of linearized Laplace within the context of neural network reparametrization, instead of primarily aiming to achieve better posterior approximations. This allows us to gain deeper insights into the underlying mechanisms that contribute to the success of this approximation technique. ", "page_idx": 8}, {"type": "text", "text": "7 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We benchmark Laplace diffusion with neural network predictive against linearized and sampled Laplace to validate the developed theory. Implementation details are in Appendix E.1. We will show that the diffusion posterior slightly outperforms linearized Laplace in terms of both in-distribution fit and out-of-distribution detection. For completeness, we include comparisons to other baselines such as SWAG, diagonal Laplace, and last-layer Laplace in Appendix E.3. Laplace diffusion is competitive with the best-performing Bayesian methods despite using the neural network predictive (i.e. no linearization). This contrasts sampled Laplace which severely underfits. This is evidence that the developed theory explains the key challenges of Bayesian deep learning. ", "page_idx": 8}, {"type": "text", "text": "Experimental details (appendix E.3). We train a 44,000-parameter LeNet(LeCun et al., 1989) on MNIST and FMNIST as well as a 270,000-parameter ResNet(He et al., 2016) on CIFAR-10(Krizhevsky et al., 2009). We sample from the Laplace approximation of the posterior and our Laplace diffusion. For the samples from the Laplace approximation, we consider both the linearized predictive and the neural network predictive, while for diffusion samples, we only consider the neural network predictive. These baselines were chosen to be as similar as possible to our approach to ease the comparison. We use the same prior precision for all methods to ensure a fair comparison. ", "page_idx": 8}, {"type": "text", "text": "In-distribution performance (Table 1). We measure the in-distribution performance of different posteriors on a held-out test set. We report means $\\pm$ standard deviations of several metrics: Confidence, Accuracy, Negative Log-Likelihood, Brier Score (Brier, 1950), Expected Calibration Error (Naeini et al., 2015) and Mean Calibration Error. We observe that Laplace diffusion has the best calibration and fit. We also confirm the underfitting of Sampled Laplace across cases. For CIFAR-10 we had to use a large prior precision to get meaningful samples from sampled Laplace, which explains the less severe underfitting. High prior precision is known to help with underfitting in sampled Laplace, but it also shrinks predictive uncertainty to almost zero. ", "page_idx": 8}, {"type": "text", "text": "Robustness to dataset shift (Fig. 5, appendix E.3.2). We use ROTATED-MNIST, ROTATEDFMNIST, and ROTATED-CIFAR to asses model-calibration and model fit under distribution shift. Fig. 5 plots negative log-likelihood (NLL) and expected calibration error (ECE) against the degrees of rotation. Laplace diffusion improves on other Laplace approximations. ", "page_idx": 8}, {"type": "text", "text": "Out-of-distribution detection (Table 2). On out-of-distribution data from other benchmarks, we see that Laplace diffusion outperforms the other Laplace approximations. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "While approximate Bayesian inference excels in many areas, it continues to face challenges in deep learning. Techniques that work well in shallow models struggle with deep ones even if they remain computationally tractable. This suggests that overparametrization plays a negative role in Bayesian models. Our theoretical analysis shows how overparametrization creates a growing reparameterization issue that conflicts with standard Euclidean approximate posteriors, such as the ever-present Gaussian. For small models this issue is negligible, but as models grow, so does the reparameterization issue. ", "page_idx": 9}, {"type": "text", "text": "Our geometric analysis also suggests a solution: we should consider approximate posteriors that respect the group structure of the reparameterizations. We observe that the generalized Gauss-Newton (GGN) matrix commonly used in Laplace approximations induces a pseudo-Riemannian structure on the parameter space that respects the topology of the reparameterization group. This implies that we can use pseudo-Riemannian probability distributions as approximate posteriors, and we experimented with the obvious choice of a geometric diffusion process. We also showed that the state-of-the-art linearized Laplace approximation can be viewed as a na\u00efve (or simple) numerical approximation to our proposed diffusion. This helps explain the success of the linearized approximation. ", "page_idx": 9}, {"type": "text", "text": "Our proposed approximate posterior does have issues. While sampling has the same complexity as standard Laplace approximations, it increases runtime by a constant factor. Common Laplace approximations do not sample according to the GGN but rather approximate this matrix with a diagonal or block-diagonal matrix. Mathematically, such approximations break the motivational reparameterization invariance, so it is unclear if such approaches should be applied in our framework. Our work, thus, raises the need for new computational pipelines for engaging with the GGN matrix. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by a research grant (42062) from VILLUM FONDEN. This project received funding from the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme (grant agreements 757360 and 101123955). The work was partly funded by the Novo Nordisk Foundation through the Center for Basic Machine Learning Research in Life Science (NNF20OC0062606). In addition to the ERC (above), PH, MP and LT thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for support, and gratefully acknowledge financial support by the DFG Cluster of Excellence \u201cMachine Learning - New Perspectives for Science\u201d, EXC 2064/1, project number 390727645; the German Federal Ministry of Education and Research (BMBF) through the T\u00fcbingen AI Center (FKZ: 01IS18039A); and funds from the Ministry of Science, Research and Arts of the State of Baden-W\u00fcrttemberg. The authors are also grateful to Magnus Waldemar Hoff Harder for alerting us to imprecisions in an early draft of this manuscript. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Allen-Zhu, Z., Li, Y., and Song, Z. A convergence theory for deep learning via over-parameterization. In International conference on machine learning, pp. 242\u2013252. PMLR, 2019. ", "page_idx": 9}, {"type": "text", "text": "Antor\u00e1n, J., Janz, D., Allingham, J. U., Daxberger, E., Barbano, R. R., Nalisnick, E., and Hern\u00e1ndezLobato, J. M. Adapting the linearised laplace model evidence for modern deep learning. In International Conference on Machine Learning, pp. 796\u2013821. PMLR, 2022. ", "page_idx": 9}, {"type": "text", "text": "Antor\u00e1n, J., Padhy, S., Barbano, R., Nalisnick, E., Janz, D., and Hern\u00e1ndez-Lobato, J. M. Samplingbased inference for large linear models, with application to linearised laplace, 2023. ", "page_idx": 9}, {"type": "text", "text": "Bergamin, F., Moreno-Mu\u00f1oz, P., Hauberg, S., and Arvanitidis, G. Riemannian laplace approximations for bayesian neural networks. Advances in Neural Information Processing Systems, 36, 2024. ", "page_idx": 9}, {"type": "text", "text": "Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D. Weight uncertainty in neural network. In International conference on machine learning, pp. 1613\u20131622. PMLR, 2015.   \nBombari, S., Amani, M. H., and Mondelli, M. Memorization and optimization in deep neural networks with minimum over-parameterization. In Advances in Neural Information Processing Systems (NeurIPS), 2022.   \nBrier, G. W. Verification of forecasts expressed in terms of probability. Monthly Weather Review, 78(1):1 \u2013 3, 1950. doi: 10.1175/1520-0493(1950)078<0001:VOFEIT $>\\!2.0$ . CO;2. URL https://journals.ametsoc.org/view/journals/mwre/78/1/1520-0493_ 1950_078_0001_vofeit_2_0_co_2.xml.   \nDaxberger, E., Kristiadi, A., Immer, A., Eschenhagen, R., Bauer, M., and Hennig, P. Laplace redux - effortless Bayesian deep learning. In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 20089\u201320103. Curran Associates, Inc., 2021a.   \nDaxberger, E., Kristiadi, A., Immer, A., Eschenhagen, R., Bauer, M., and Hennig, P. Laplace redux\u2013effortless Bayesian deep learning. In NeurIPS, 2021b.   \nDaxberger, E., Nalisnick, E., Allingham, J. U., Antor\u00e1n, J., and Hern\u00e1ndez-Lobato, J. M. Bayesian deep learning via subnetwork inference. In International Conference on Machine Learning, pp. 2510\u20132521. PMLR, 2021c.   \nDaxberger, E., Kristiadi, A., Immer, A., Eschenhagen, R., Bauer, M., and Hennig, P. Laplace redux \u2013 effortless bayesian deep learning, 2022.   \nDeng, Z., Zhou, F., and Zhu, J. Accelerated linearized laplace approximation for bayesian deep learning, 2022.   \nDevroye, L., Gy\u00f6rfi, L., and Lugosi, G. A probabilistic theory of pattern recognition. Springer Science & Business Media, 1996.   \nDinh, L., Pascanu, R., Bengio, S., and Bengio, Y. Sharp minima can generalize for deep nets. In International Conference on Machine Learning, pp. 1019\u20131028. PMLR, 2017.   \nDu, S., Lee, J., Li, H., Wang, L., and Zhai, X. Gradient descent finds global minima of deep neural networks. In International conference on machine learning, pp. 1675\u20131685. PMLR, 2019.   \nGeorge, T., Laurent, C., Bouthillier, X., Ballas, N., and Vincent, P. Fast approximate natural gradient descent in a kronecker factored eigenbasis. Advances in Neural Information Processing Systems, 31, 2018.   \nGirolami, M. and Calderhead, B. Riemann manifold langevin and hamiltonian monte carlo methods. Journal of the Royal Statistical Society Series B: Statistical Methodology, 73(2):123\u2013214, 2011.   \nHansen, L. K. and Salamon, P. Neural network ensembles. IEEE transactions on pattern analysis and machine intelligence, 12(10):993\u20131001, 1990.   \nHauberg, S. Directional statistics with the spherical normal distribution. In Proceedings of FUSION 2018, 2018.   \nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.   \nHsu, E. P. Stochastic analysis on manifolds. American Mathematical Soc., 2002.   \nImmer, A., Bauer, M., Fortuin, V., R\u00e4tsch, G., and Emtiyaz, K. M. Scalable marginal likelihood estimation for model selection in deep learning. In International Conference on Machine Learning (ICML), pp. 4563\u20134573, 2021a.   \nImmer, A., Korzepa, M., and Bauer, M. Improving predictions of Bayesian neural nets via local linearization. In International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 703\u2013711, 2021b.   \nIzmailov, P., Maddox, W. J., Kirichenko, P., Garipov, T., Vetrov, D., and Wilson, A. G. Subspace inference for bayesian deep learning. In Uncertainty in Artificial Intelligence, pp. 1169\u20131179. PMLR, 2020.   \nIzmailov, P., Nicholson, P., Lotfi, S., and Wilson, A. G. Dangers of bayesian model averaging under covariate shift. Advances in Neural Information Processing Systems, 34:3309\u20133322, 2021.   \nJacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in neural information processing systems, 2018.   \nJang, C., Lee, S., Park, F., and Noh, Y.-K. A reparametrization-invariant sharpness measure based on information geometry. Advances in neural information processing systems, 35:27893\u201327905, 2022.   \nKhan, M. E. E., Immer, A., Abedi, E., and Korzepa, M. Approximate inference turns deep networks into Gaussian processes. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019.   \nKim, S., Park, S., Kim, K.-S., and Yang, E. Scale-invariant bayesian neural networks with connectivity tangent kernel. In The Eleventh International Conference on Learning Representations, 2022.   \nKim, S., Kim, K., and Yang, E. Gex: A flexible method for approximating influence via geometric ensemble. Advances in Neural Information Processing Systems, 36, 2024.   \nKristiadi, A., Hein, M., and Hennig, P. Being Bayesian, even just a bit, fixes overconfidence in ReLU networks. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 5436\u20135446. PMLR, 13\u201318 Jul 2020.   \nKristiadi, A., Dangel, F., and Hennig, P. The geometry of neural nets\u2019 parameter spaces under reparametrization. arXiv preprint arXiv:2302.07384, 2023.   \nKrizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009.   \nLakshminarayanan, B., Pritzel, A., and Blundell, C. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems, 30, 2017.   \nLanczos, C. An iteration method for the solution of the eigenvalue problem of linear differential and integral operators. Journal of Research of the National Bureau of Standards, 45(4), October 1950. doi: 10.6028/jres.045.026. URL https://hal.science/hal-01712947.   \nLawrence, N. D. Variational inference in probabilistic models. PhD thesis, Citeseer, 2001.   \nLeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541\u2013551, 1989.   \nLee, J. M. Smooth manifolds. Springer, 2012.   \nLi, C., Chen, C., Carlson, D., and Carin, L. Preconditioned stochastic gradient langevin dynamics for deep neural networks, 2015.   \nLi, Z., Wang, T., and Arora, S. What happens after sgd reaches zero loss?\u2013a mathematical framework. arXiv preprint arXiv:2110.06914, 2021.   \nLippe, P. UvA Deep Learning Tutorials. https://uvadlc-notebooks.readthedocs.io/en/ latest/, 2022.   \nLiu, C., Zhu, L., and Belkin, M. On the linearity of large non-linear models: when and why the tangent kernel is constant. Advances in Neural Information Processing Systems, 33:15954\u201315964, 2020.   \nMacKay, D. J. Choice of basis for laplace approximation. Machine learning, 33:77\u201386, 1998.   \nMacKay, D. J. C. A practical Bayesian framework for backpropagation networks. Neural Computation, 4(3):448\u2013472, 1992.   \nMaddox, W. J., Izmailov, P., Garipov, T., Vetrov, D. P., and Wilson, A. G. A simple baseline for bayesian uncertainty in deep learning. Advances in neural information processing systems, 32, 2019.   \nMaruyama, G. Continuous markov processes and stochastic equations. Rendiconti del Circolo Matematico di Palermo, 4:48\u201390, 1955.   \nMiani, M., Warburg, F., Moreno-Mu\u00f1oz, P., Detlefsen, N. S., and Hauberg, S. Laplacian autoencoders for learning stochastic representations. In Advances in Neural Information Processing Systems (NeurIPS), 2022.   \nNaeini, M. P., Cooper, G., and Hauskrecht, M. Obtaining well calibrated probabilities using bayesian binning. In Proceedings of the AAAI conference on artificial intelligence, volume 29, 2015.   \nNguyen, Q., Mondelli, M., and Montufar, G. F. Tight bounds on the smallest eigenvalue of the neural tangent kernel for deep relu networks. In International Conference on Machine Learning, pp. 8119\u20138129. PMLR, 2021.   \nOymak, S. and Soltanolkotabi, M. Overparameterized nonlinear learning: Gradient descent takes the shortest path? In International Conference on Machine Learning, pp. 4951\u20134960. PMLR, 2019.   \nOymak, S. and Soltanolkotabi, M. Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks. IEEE Journal on Selected Areas in Information Theory, 1(1):84\u2013105, 2020.   \nPapamarkou, T., Skoularidou, M., Palla, K., Aitchison, L., Arbel, J., Dunson, D., Filippone, M., Fortuin, V., Hennig, P., Hubin, A., et al. Position paper: Bayesian deep learning in the age of large-scale ai. arXiv preprint arXiv:2402.00809, 2024.   \nPetzka, H., Adilova, L., Kamp, M., and Sminchisescu, C. A reparameterization-invariant flatness measure for deep neural networks. arXiv preprint arXiv:1912.00058, 2019.   \nRitter, H., Botev, A., and Barber, D. A scalable laplace approximation for neural networks. In 6th international conference on learning representations, ICLR 2018-conference track proceedings, volume 6. International Conference on Representation Learning, 2018.   \nSharma, M., Farquhar, S., Nalisnick, E., and Rainforth, T. Do Bayesian Neural Networks Need To Be Fully Stochastic? In International Conference on Artificial Intelligence and Statistics (AISTATS, notable paper award), 2023.   \nWarburg, F., Miani, M., Brack, S., and Hauberg, S. Bayesian metric learning for uncertainty quantification in image retrieval. In Neural Information Processing Systems (NeurIPS), 2023.   \nYu, H., Hartmann, M., Williams, B., Girolami, M., and Klami, A. Riemannian laplace approximation with the fisher metric, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Recap ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Notation. Consider a function $f:\\mathbb{R}^{D}\\times\\mathbb{R}^{I}\\to\\mathbb{R}^{O}$ with Jacobian $\\mathbf{J}_{\\mathbf{w}}(\\mathbf{x})=\\partial_{\\mathbf{w}}f_{\\mathbf{w}}(\\mathbf{x})|_{\\mathbf{w}=\\mathbf{w}}\\in$ $\\mathbb{R}^{O\\times D}$ with respect to w evaluated in $\\mathbf{x}$ and w. For a given log-likelihood we define the Hessian w.r.t. to the output $\\dot{\\mathbf{H_{w}}}(\\mathbf{x})=-\\partial_{f_{\\mathbf{w}}(\\mathbf{x})}^{2}\\log p(\\mathbf{y}|f_{w}(\\mathbf{x}))\\in\\mathring{\\mathbb{R}}^{O\\times O}$ and we assume it not to be dependent on $\\mathbf{y}$ (which is true, for example, for exponential families). ", "page_idx": 13}, {"type": "text", "text": "Consider being given a dataset of finite size $N$ , here we do not care about labels and we only refer to the collections of the datapoints $\\mathcal{X}=\\{\\mathbf{x}_{1},\\hdots,\\mathbf{x}_{N}\\}\\subset\\mathbb{R}^{I}$ . ", "page_idx": 13}, {"type": "text", "text": "Consider the stacking of the per-datum Jacobians ${\\bf J}_{\\mathbf{w}}=[{\\bf J}_{\\mathbf{w}}({\\bf x}_{1});\\ldots;{\\bf J}_{\\mathbf{w}}({\\bf x}_{N})]\\in\\mathbb{R}^{N O\\times D}$ , where we dropped the dependence on $\\mathcal{X}$ . Similarly consider $\\mathbf{H}_{\\mathbf{w}}\\;=\\;\\mathrm{diag}(\\mathbf{H}_{\\mathbf{w}}(\\mathbf{x}_{1});\\dots;\\mathbf{H}_{\\mathbf{w}}(\\mathbf{x}_{N}))\\;\\in$ RNO\u00d7NO the block diagonal stacking of the Hessians. ", "page_idx": 13}, {"type": "text", "text": "Consider the Generalized Gauss-Newton (GGN) and the Neural Tangent Kernel (NTK) matrices ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{G}\\mathbf{G}\\mathbf{N}_{\\mathbf{w}}=\\mathbf{J}_{\\mathbf{w}}^{\\top}\\mathbf{H}_{\\mathbf{w}}\\mathbf{J}_{\\mathbf{w}}\\in\\mathbb{R}^{D\\times D}\\qquad\\qquad{\\mathrm{NTK}}_{\\mathbf{w}}=\\mathbf{H}_{\\mathbf{w}}^{1/2}\\mathbf{J}_{\\mathbf{w}}\\mathbf{J}_{\\mathbf{w}}^{\\top}\\mathbf{H}_{\\mathbf{w}}^{1/2}\\in\\mathbb{R}^{N O\\times N O}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Recall also that the pullback pseudo-metric is $(f^{*}H)_{\\mathbf{w}}=\\operatorname{GGN}_{\\mathbf{w}}$ . ", "page_idx": 13}, {"type": "text", "text": "Assumptions. We assume uniform upper and lower bound on the eigenvalues of the NTK matrix, that is ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\exists l,L\\in\\mathbb{R}\\quad\\mathrm{~such~that~}\\qquad0<l\\leq\\frac{\\|\\mathbf{\\mu}\\mathbf{N}\\mathbf{T}\\mathbf{K_{w}}v\\|}{\\|v\\|}\\leq L\\quad\\forall v\\in\\mathbb{R}^{N O},\\forall\\mathbf{w}\\in\\mathbb{R}^{D},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where uniform means uniform over parameters, i.e. the bounds $l,L$ holds for every w. Moreover we assume that the Jacobian function $\\mathbf{w}\\mapsto J_{\\mathbf{w}}(\\mathbf{x})$ is Lipschitz for every $\\mathbf{x}\\in\\mathcal{X}$ . ", "page_idx": 13}, {"type": "text", "text": "How unreasonable are the assumptions? The assumption of an upper bound $L$ is equivalent to assuming that $\\mathbf{w}\\mapsto f(\\mathbf{w},\\mathbf{x})$ is Lipschitz for each datapoint $\\mathbf{x}\\in\\mathcal{X}$ . This is true with all standard activation functions if we restrict the parameter space to a ball of fixed radius. ", "page_idx": 13}, {"type": "text", "text": "The assumption of a lower bound $l$ is strongly supported by the literature on NTK, thanks to its direct implications on memorization capacity and generalization. The general trend is that the more overparametrized the network is, the stronger such lower bounds are. In the hardest setting of minimum overparametrization, Bombari et al. (2022) proved a bound that holds with high probability for fully connected MLPs at initialization. Similar results hold with bigger overparametrizations Nguyen et al. (2021); Allen-Zhu et al. (2019); Du et al. (2019). Building on top of that, other lines of work Liu et al. (2020); Oymak & Soltanolkotabi (2019, 2020) proved that the NTK does not change too much during training thanks to the PL inequality framework, and in particular the proximity of the neural network dynamics to the one described by NTK, is supported by spectral bounds on the Hessian of the landscape. ", "page_idx": 13}, {"type": "text", "text": "Lastly, the Lipschitzness assumption on the Jacobian is potentially the most unrealistic, although it would hold, for example, if the derivatives of activations are Lipschitz and the parameters are restricted to a finite radius ball. Nonetheless, we emphasize that this assumption is only used to control that the kernel of the GGN does not \u201crotate too fast\u201d, which is a much weaker assumption, but also much more cluttered to state formally. ", "page_idx": 13}, {"type": "text", "text": "B Proof for equivalence of the two settings ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This section contains the proof of Proposition 4.4 and Theorem 4.5 involving the pseudo-Riemannian manifold $(\\mathbb{R}^{D},f^{*}H)$ and the quotient group $(\\mathcal P,d_{\\mathcal P})$ with Euclidean-induced metric. ", "page_idx": 13}, {"type": "text", "text": "To ease the readability, we recall the two involved notions of distance and their respective definition: ", "page_idx": 13}, {"type": "text", "text": "\u2022 $d_{f^{*}H}(\\mathbf{w}_{0},\\mathbf{w}_{1})$ the geodesic distance for any two parameters $\\mathbf{w}_{0},\\mathbf{w}_{1}\\in\\mathbb{R}^{D}$ \u2022 $d_{\\mathcal{P}}([\\mathbf{w}_{0},\\mathbf{w}_{1}])$ the quotient Euclidean distance for any two equivalence classes $[\\mathbf{w}_{0}],[\\mathbf{w}_{1}]\\in$ $\\mathcal{P}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{f^{*}H}(\\mathbf{w}_{0},\\mathbf{w}_{1})=\\underset{\\gamma}{\\operatorname*{inf}}\\ \\mathrm{LEN}_{f^{*}H}(\\gamma)}\\\\ &{\\qquad\\qquad\\qquad=\\underset{\\gamma}{\\operatorname*{inf}}\\ \\int_{0}^{1}\\|\\gamma^{\\prime}(t)\\|_{f^{*}H_{\\gamma(t)}}\\mathrm{d}t}\\\\ &{\\qquad\\qquad\\qquad=\\underset{\\gamma}{\\operatorname*{inf}}\\ \\int_{0}^{1}\\sqrt{\\gamma^{\\prime}(t)^{\\top}\\cdot f^{*}H_{\\gamma(t)}\\cdot\\gamma^{\\prime}(t)}\\mathrm{d}t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the infimum is taken over smoothly differentiable curves $\\gamma:[0,1]\\rightarrow\\mathbb{R}^{D}$ such that ${\\boldsymbol\\gamma}(0)={\\bf w}_{0}$ and $\\gamma(1)=\\mathbf{w}_{1}$ . ", "page_idx": 14}, {"type": "equation", "text": "$$\nd\\mathcal P\\big([\\mathbf{w}_{0}],[\\mathbf{w}_{1}]\\big)=\\operatorname*{inf}\\left\\{\\sum_{i=1}^{n}\\|p_{i}-q_{i}\\|\\right\\},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the infimum is taken over all finite sequences $\\{p_{i}\\}_{i=1\\dots n},\\{q_{i}\\}_{i=1\\dots n}\\subset\\mathbb{R}^{D}$ such that $\\left[\\mathbf{w}_{0}\\right]=$ $[p_{1}],[p_{i+1}]=[q_{i}]$ and $\\left[q_{n}\\right]=\\left[\\mathbf{w}_{1}\\right]$ . ", "page_idx": 14}, {"type": "text", "text": "Let us state a theorem that encapsulate together both the 0-distance part in Proposition 4.4 and the $\\epsilon{-}\\delta$ part in Theorem 4.5 in a more unified way. ", "page_idx": 14}, {"type": "text", "text": "Theorem B.1. For any $\\mathbf{w}_{0},\\mathbf{w}_{1}\\in\\mathbb{R}^{D}$ it holds ", "page_idx": 14}, {"type": "equation", "text": "$$\nd_{f^{*}H}(\\mathbf{w}_{0},\\mathbf{w}_{1})=0\\quad\\Longleftrightarrow\\quad d_{\\mathcal{P}}([\\mathbf{w}_{0}],[\\mathbf{w}_{1}])=0,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and also that, for any $\\epsilon>0$ there exists $\\delta>0$ such that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{d_{\\mathcal{P}}([\\mathbf{w}_{0}],[\\mathbf{w}_{1}])<\\delta}&{\\implies\\;\\;d_{f^{*}H}(\\mathbf{w}_{0},\\mathbf{w}_{1})<\\epsilon}\\\\ {d_{f^{*}H}(\\mathbf{w}_{0},\\mathbf{w}_{1})<\\delta}&{\\implies\\;\\;d_{\\mathcal{P}}([\\mathbf{w}_{0}],[\\mathbf{w}_{1}])<\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We prove the 3 points separately, in Appendix B.1, Appendix B.2 and Appendix B.3 respectively. ", "page_idx": 14}, {"type": "text", "text": "B.1 Proof of Eq. 21 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The proof logic is ", "page_idx": 14}, {"type": "equation", "text": "$$\nd_{f^{*}H}(\\mathbf{w}_{0},\\mathbf{w}_{1})=0\\quad\\Longleftrightarrow\\quad[\\mathbf{w}_{0}]=[\\mathbf{w}_{1}]\\quad\\Longleftrightarrow\\quad d_{\\mathcal{P}}([\\mathbf{w}_{0}],[\\mathbf{w}_{1}])=0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and we prove the two steps in the two following Propositions, respectively. ", "page_idx": 14}, {"type": "text", "text": "Proposition B.2. For any $\\mathbf{w}_{0},\\mathbf{w}_{1}\\in\\mathbb{R}^{D}$ it holds ", "page_idx": 14}, {"type": "equation", "text": "$$\nd_{f^{*}H}(\\mathbf{w}_{0},\\mathbf{w}_{1})=0\\quad\\Longleftrightarrow\\quad[\\mathbf{w}_{0}]=[\\mathbf{w}_{1}].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. By definition $\\left[\\mathbf{w}_{0}\\right]=\\left[\\mathbf{w}_{1}\\right]$ if and only if there exists a piecewise differentiable $\\gamma:[0,1]\\rightarrow\\mathbb{R}^{D}$ such that ${\\boldsymbol\\gamma}(0)={\\bf w}_{0}$ , $\\gamma(1)=\\mathbf{w}_{1}$ and $f(\\mathbf{w}_{0},\\mathbf{x})=f(\\gamma(t),\\mathbf{x})$ for any $t\\in[0,1]$ and $\\mathbf{x}\\in\\mathcal{X}$ . Then $\\boxleftarrow$ Consider a $\\gamma$ from the definition of the equivalence relation $\\sim$ and define the points $\\mathbf{w}_{t}=\\gamma(t)$ for ease of notation. Then for any $t\\in[0,1]$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\gamma^{\\prime}(t)=\\operatorname*{lim}_{\\epsilon\\rightarrow0}\\frac{\\mathbf{w}_{t+\\epsilon}-\\mathbf{w}_{t}}{\\epsilon}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For any $\\mathbf{x}\\in\\mathcal{X}$ it holds that $f(\\mathbf{w}_{t},\\mathbf{x})=f(\\mathbf{w}_{t^{\\prime}},\\mathbf{x})\\forall t,t^{\\prime}\\in[0,1]$ , which implies that $f(\\mathbf{w}_{t+\\epsilon},\\mathbf{x})-$ $f(\\mathbf{w}_{t},\\mathbf{x})=0\\,\\forall t\\in[0,1]\\forall\\epsilon\\in[0,1-t]$ . Thus, ", "page_idx": 14}, {"type": "equation", "text": "$$\n0=\\operatorname*{lim}_{\\epsilon\\rightarrow0}\\frac{f(\\mathbf{w}_{t+\\epsilon},\\mathbf{x})-f(\\mathbf{w}_{t},\\mathbf{x})}{\\epsilon}=\\mathbf{J}_{\\mathbf{w}_{t}}(\\mathbf{x})\\cdot\\operatorname*{lim}_{\\epsilon\\rightarrow0}\\frac{\\mathbf{w}_{t+\\epsilon}-\\mathbf{w}_{t}}{\\epsilon}=\\mathbf{J}_{\\mathbf{w}_{t}}(\\mathbf{x})\\cdot\\gamma^{\\prime}(t).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This holds to any $\\mathbf{x}\\in\\mathcal{X}$ , so the same holds for the per-datum stacked jacobians $\\mathbf{J}_{\\mathbf{w}_{t}}\\cdot\\gamma^{\\prime}(t)=0$ Thus, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\gamma^{\\prime}(t)\\|_{f^{*}H_{\\gamma(t)}}^{2}=\\gamma^{\\prime}(t)^{\\top}\\cdot f^{*}H_{\\gamma(t)}\\cdot\\gamma^{\\prime}(t)=\\gamma^{\\prime}(t)^{\\top}\\cdot\\mathbf{J_{w_{t}}^{\\top}}\\mathbf{H_{w_{t}}}\\mathbf{J_{w_{t}}}\\cdot\\gamma^{\\prime}(t)=0\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and we can measure the length of $\\gamma$ in the pullback metric as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{LEN}_{f^{*}H}(\\gamma)=\\int_{0}^{1}\\|\\gamma^{\\prime}(t)\\|_{f^{*}H_{\\gamma(t)}}\\mathrm{d}t=\\int_{0}^{1}0\\,\\mathrm{d}t=0,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which gives an upper bound on the geodesic distance ", "page_idx": 15}, {"type": "equation", "text": "$$\nd_{f^{*}H}\\big(\\mathbf{w}_{0},\\mathbf{w}_{1}\\big)=\\operatorname*{inf}_{\\hat{\\gamma}}\\mathrm{LEN}_{f^{*}H}\\big(\\hat{\\gamma}\\big)\\leq\\mathrm{LEN}_{f^{*}H}\\big(\\gamma\\big)=0,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "thus, $d_{f^{*}H}(\\mathbf{w}_{0},\\mathbf{w}_{1})=0$ and this implication is proven. ", "page_idx": 15}, {"type": "text", "text": "$\\boxed{\\Longrightarrow}d_{f^{*}H}(\\mathbf{w}_{0},\\mathbf{w}_{1})=0$ implies that there exists a 0-length differentiable $\\gamma:[0,1]\\rightarrow\\mathbb{R}^{D}$ such th $\\overline{{{\\mathfrak{a t}\\,\\gamma}}}(0)=\\mathbf{w}_{0},\\,\\gamma(1)=\\mathbf{w}_{1}$ . Without loss of generality, we can assume $\\gamma$ to be non-stationary, i.e. $\\gamma^{\\prime}(t)\\neq0$ . Here 0-length means ", "page_idx": 15}, {"type": "equation", "text": "$$\n0=\\mathrm{LEN}_{f^{*}H}(\\gamma)=\\int_{0}^{1}\\|\\gamma^{\\prime}(t)\\|_{f^{*}H_{\\gamma(t)}}\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which implies that $\\|\\gamma^{\\prime}(t)\\|_{f^{*}H_{\\gamma(t)}}=0$ for any $t\\in[0,1]$ except a zero-measure set which we can neglect later. Then ", "page_idx": 15}, {"type": "equation", "text": "$$\n0=\\|\\gamma^{\\prime}(t)\\|_{f^{*}H_{\\gamma(t)}}^{2}=\\gamma^{\\prime}(t)^{\\top}\\cdot\\mathbf{J_{w_{t}}^{\\top}}\\mathbf{H}_{\\mathbf{w}_{t}}\\mathbf{J_{w}}_{t}\\cdot\\gamma^{\\prime}(t)\\quad\\Longrightarrow\\quad\\mathbf{J_{w_{t}}}\\cdot\\gamma^{\\prime}(t)=0,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "by positive definitess of $\\mathbf{H}_{\\mathbf{w}_{t}}$ , assumed as hypothesis. We highlight that the leftmost 0 in the previous equation is a scalar, while the rightmost 0 is a vector in $\\mathbb{R}^{N O}$ as obtained by the matrix-vector product of $\\mathbf{J}_{\\mathbf{w}_{t}}\\in\\mathbb{R}^{N O\\times D}$ with $\\gamma^{\\prime}(t)\\in\\overline{{\\mathbb{R}}}^{D}$ . Looking at the equation $\\mathbf{J}_{\\mathbf{w}_{t}}\\cdot\\gamma^{\\prime}(t)\\,\\dot{=}\\,0$ componentwise implies that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\langle\\nabla_{\\mathbf{w}}[f(\\mathbf{w}_{t},\\mathbf{x})]_{o},\\gamma^{\\prime}(t)\\rangle=0\\quad\\forall\\mathbf{x}\\in\\mathcal{X},\\forall o\\in\\{1,\\ldots,O\\},\\forall t\\in[0,1],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $[v]_{o}$ refers to the oth component of a vector $v$ . Thus, for $T\\in[0,1]$ , by Fundamental Theorem of Calculus we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n[f(\\mathbf{w}_{T},\\mathbf{x})]_{o}-[f(\\mathbf{w}_{0},\\mathbf{x})]_{o}=\\int_{0}^{T}\\langle\\nabla_{\\mathbf{w}}[f(\\mathbf{w}_{t},\\mathbf{x})]_{o},\\gamma^{\\prime}(t)\\rangle\\mathrm{d}t=0\\qquad\\forall\\mathbf{x}\\in\\mathcal{X},\\forall o\\in\\{1,\\ldots,O\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then $f(\\mathbf{w}_{T},\\mathbf{x})=f(\\mathbf{w}_{0},\\mathbf{x})\\,\\forall\\mathbf{x}\\in\\mathcal{X}$ and $\\forall T\\in[0,1]$ . So we proved that $\\gamma$ is an homotopy of $(\\mathbf{w},\\mathbf{w}^{\\prime})$ such that $\\gamma(t)\\in\\mathcal{R}_{\\chi}^{f}(\\mathbf{w})\\ \\forall t\\in[0,1]$ . Thus, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{w}^{\\prime}\\in\\bar{\\mathcal{R}}_{\\mathcal{X}}^{f}(\\mathbf{w})\\quad\\Longrightarrow\\quad\\mathbf{w}^{\\prime}\\sim\\mathbf{w}\\quad\\Longrightarrow\\quad[\\mathbf{w}^{\\prime}]=[\\mathbf{w}],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and this completes the proof. ", "page_idx": 15}, {"type": "text", "text": "Proposition B.3. For any $\\mathbf{w}_{0},\\mathbf{w}_{1}\\in\\mathbb{R}^{D}$ it holds ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{[\\mathbf{w}_{0}]=[\\mathbf{w}_{1}]\\quad\\Longleftrightarrow\\quad d_{\\mathcal{P}}([\\mathbf{w}_{0},\\mathbf{w}_{1}])=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. $\\boxed{\\implies}$ This arrow is trivially true by considering the two sequences in the definition of the quotient distance to be of length one and such that $p_{1}=\\mathbf{w}_{0}$ and $q_{1}=\\mathbf{w}_{1}$ . ", "page_idx": 15}, {"type": "text", "text": "$\\boxed{\\Longleftrightarrow}d\\mathcal{P}([\\mathbf{w}_{0},\\mathbf{w}_{1}])=0$ implies that, by definition of inf, there exists a sequence $\\epsilon_{m}\\to0$ such that \u2200m \u2208N there exists two finite sequences of points p(1 $p_{1}^{(m)},\\ldots,p_{n}^{(m)}\\in\\mathbb{R}^{D}$ and $q_{1}^{(m)},\\ldots,q_{n}^{(m)}\\in\\mathbb{R}^{D}$ such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\|p_{i}^{(m)}-q_{i}^{(m)}\\|=\\epsilon_{m},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $[\\mathbf{w}_{0}]\\,=\\,[p_{1}^{(m)}]$ , $[p_{i+1}^{(m)}]\\,=\\,[q_{i}^{(m)}]$ and $[q_{n}^{(m)}]\\,=\\,[\\mathbf{w}_{1}]$ . Note that the sequence length $n$ may depend on $m$ .   \nLipschitzness of $f$ in parameters, assumed by hypothesis, means that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|p-q\\|<\\epsilon\\quad\\Longrightarrow\\quad\\|f(p,\\mathbf{x})-f(q,\\mathbf{x})\\|<L\\epsilon\\qquad\\forall p,q\\in\\mathbb{R}^{D},\\forall\\mathbf{x}\\in\\mathcal{X},\\forall\\epsilon>0,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "thus, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\|p_{i}^{(m)}-q_{i}^{(m)}\\|=\\epsilon_{m}\\quad\\Longrightarrow\\quad\\sum_{i=1}^{n}\\|f(p_{i}^{(m)},\\mathbf{x})-f(q_{i}^{(m)},\\mathbf{x})\\|<2L\\epsilon_{m}\\qquad\\forall\\mathbf{x}\\in\\mathcal{X}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Also, by definition of the equivalence class, it holds $\\forall\\mathbf{x}\\in\\mathcal{X}$ that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{[\\mathbf{w}_{0}]=[p_{1}^{(m)}]\\quad\\Longrightarrow\\quad\\|f(\\mathbf{w}_{0},\\mathbf{x})-f(p_{1}^{(m)},\\mathbf{x})\\|=0}\\\\ {[p_{i+1}^{(m)}]=[q_{i}^{(m)}]\\quad\\Longrightarrow\\quad\\|f(p_{i+1}^{(m)},\\mathbf{x})-f(q_{i}^{(m)},\\mathbf{x})\\|=0}\\\\ {[q_{n}^{(m)}]=[\\mathbf{w}_{1}]\\quad\\Longrightarrow\\quad\\|f(q_{n}^{(m)},\\mathbf{x})-f(\\mathbf{w}_{1},\\mathbf{x})\\|=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, by the triangular inequality, the last four equations imply ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|f(\\mathbf{w}_{0},\\mathbf{x})-f(\\mathbf{w}_{1},\\mathbf{x})\\|\\leq\\|f(\\mathbf{w}_{0},\\mathbf{x})-f(p_{1}^{(m)},\\mathbf{x})\\|+\\|f(p_{1}^{(m)},\\mathbf{x})-f(q_{n}^{(m)},\\mathbf{x})\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\|f(q_{n}^{(m)},\\mathbf{x})-f(\\mathbf{w}_{1},\\mathbf{x})\\|}\\\\ &{\\qquad\\qquad\\qquad=\\|f(p_{1}^{(m)},\\mathbf{x})-f(q_{n}^{(m)},\\mathbf{x})\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{i=1}^{n}\\|f(p_{i}^{(m)},\\mathbf{x})-f(q_{i}^{(m)},\\mathbf{x})\\|+\\displaystyle\\sum_{i=1}^{n-1}\\|f(p_{i+1}^{(m)},\\mathbf{x})-f(q_{i}^{(m)},\\mathbf{x})\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad<2L\\epsilon_{m}+\\displaystyle\\sum_{i=1}^{n-1}0=2L\\epsilon_{m},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and this holds for any $m\\in\\mathbb{N}$ . Taking the limit $m\\rightarrow\\infty$ , $\\epsilon_{m}\\to0$ implies $\\|f(\\mathbf{w}_{0},\\mathbf{x})-f(\\mathbf{w}_{1},\\mathbf{x})\\|\\leq0$ . Thus, $f(\\mathbf{w}_{0},\\mathbf{x})=f(\\mathbf{w}_{1},\\mathbf{x})\\,\\forall\\mathbf{x}\\in\\mathcal{X}$ , and, thus, $\\left[\\mathbf{w}_{0}\\right]=\\left[\\mathbf{w}_{1}\\right]$ , which completes the proof. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "B.2 Proof of Eq. 22 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In order to prove Eq. 22, we first prove a weaker statement ", "page_idx": 16}, {"type": "text", "text": "Proposition B.4. For any $\\mathbf{w}_{0},\\mathbf{w}_{1}\\in\\mathbb{R}^{D}$ and for any $\\delta>0$ it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{w}_{0}-\\mathbf{w}_{1}\\|<\\delta\\quad\\Longrightarrow\\quad d_{f^{*}H}(\\mathbf{w}_{0},\\mathbf{w}_{1})<L\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Let $\\gamma:[0,1]\\rightarrow\\mathbb{R}^{D}$ be defined as $\\gamma(t)=(1-t)\\mathbf{w}_{0}+t\\mathbf{w}_{1}$ , then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{LEN}_{f^{*}H}(\\gamma)=\\displaystyle\\int_{0}^{1}\\|\\gamma^{\\prime}(t)\\|_{f^{*}H_{\\gamma(t)}}\\mathrm{d}t=\\displaystyle\\int_{0}^{1}\\|\\mathbf{w}_{1}-\\mathbf{w}_{0}\\|_{f^{*}H_{\\gamma(t)}}\\mathrm{d}t}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\int_{0}^{1}L\\|\\mathbf{w}_{1}-\\mathbf{w}_{0}\\|\\mathrm{d}t=L\\|\\mathbf{w}_{1}-\\mathbf{w}_{0}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and thus, ", "page_idx": 16}, {"type": "equation", "text": "$$\nd_{f^{*}H}(\\mathbf{w}_{0},\\mathbf{w}_{1})=\\operatorname*{inf}_{\\hat{\\gamma}}\\mathrm{LEN}_{f^{*}H}(\\hat{\\gamma})\\leq\\mathrm{LEN}_{f^{*}H}(\\gamma)=L\\|\\mathbf{w}_{1}-\\mathbf{w}_{0}\\|<L\\delta.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Definition B.5. Let $\\gamma_{1},\\,.\\,.\\,.\\,,\\gamma_{n}:[0,1]\\to\\mathbb{R}^{D}$ a sequence of piecewise differentiable paths such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\gamma_{i-1}(1)=\\gamma_{i}(0)\\quad\\forall i\\in\\{0,\\ldots,n\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Consider the piecewise differentiable path that is the concatenation of the paths one after the other, ${\\hat{\\gamma}}=\\operatorname{CAT}\\!\\left(\\gamma_{1},\\ldots,\\gamma_{n}\\right)$ defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{\\gamma}(t)=\\gamma_{i}\\left(n t-i\\right)\\quad\\mathrm{~if~}i\\leq t\\leq i+1.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "It is straightforward to see that $\\begin{array}{r}{\\mathrm{LEN}\\big(\\hat{\\gamma}\\big)=\\sum_{i=1}^{n}\\mathrm{LEN}\\big(\\gamma_{i}\\big)}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "The choice of $\\epsilon,\\delta$ that prove Eq. 22 trivially follows from the following ", "page_idx": 16}, {"type": "text", "text": "Proposition B.6. With the assumption of eigenvalue upper bound $L,$ for any $\\mathbf{v}_{0},\\mathbf{w}_{1}\\in\\mathbb{R}^{D}$ for any $\\delta>0$ it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\nd_{\\mathcal{P}}\\big(\\big[\\mathbf{w}_{0},\\mathbf{w}_{1}\\big]\\big)<\\delta\\quad\\Longrightarrow\\quad d_{f^{*}H}\\big(\\mathbf{w}_{0},\\mathbf{w}_{1}\\big)<3L\\delta.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. $d_{\\mathcal{P}}([\\mathbf{w}_{0},\\mathbf{w}_{1}])\\ <\\ \\delta$ implies that, by the definition of inf, there exists two sequences $\\{p_{i}\\}_{i=1\\dots n},\\{q_{i}\\}_{i=1\\dots n}\\subset\\mathbb{R}^{D}$ such that $[\\mathbf{w}_{0}]^{\\bullet}\\!\\!=[p_{1}],[p_{i+1}]=[q_{i}],[q_{n}]=[\\mathbf{w}_{1}]$ and such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\left\\|p_{i}-q_{i}\\right\\|<2\\delta.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now the idea is to define $n+1$ paths on the equivalence classes and $n$ paths connecting them, then the stacking of the $2n+1$ will give an upper bound on the geodesic distance. ", "page_idx": 16}, {"type": "text", "text": "Let us first define the paths $\\gamma_{2i}$ for $i=0,\\dots,n$ by making use of Proposition B.2 \u2022 $\\left[\\mathbf{w}_{0}\\right]=\\left[p_{1}\\right]$ imply that there exists $\\gamma_{0}:[0,1]\\rightarrow\\mathbb{R}^{D}$ be such that $\\gamma_{0}(0)=\\mathbf{w}_{0}$ , $\\gamma_{0}(1)=p_{1}$ and such that $\\bar{\\mathrm{LEN}}_{f^{\\ast}H}(\\gamma_{0})<L\\delta/{n+1}$ ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "\u2022 $\\left[q_{n}\\right]=\\left[\\mathbf{w}_{1}\\right]$ imply that there exists $\\gamma_{2n}:[0,1]\\rightarrow\\mathbb{R}^{D}$ be such that $\\gamma_{2n}(0)=q_{n}$ , $\\gamma_{2n}(1)=$ $\\mathbf{w}_{1}$ and such that $\\mathrm{LEN}_{f^{\\ast}H}(\\gamma_{2n})<L\\delta/_{n+1}$   \n\u2022 for all $i=1,\\ldots,n-1$ , $[p_{i+1}]=[q_{i}]$ imply that there exists $\\gamma_{2i}:[0,1]\\rightarrow\\mathbb{R}^{D}$ be such that $\\gamma_{2i}(0)=p_{i+1},\\,\\gamma_{2i}(1)=q_{i}$ and such that $\\mathrm{LEN}_{f^{\\ast}H}(\\gamma_{2i})<L\\delta/{n+1}$ ", "page_idx": 17}, {"type": "text", "text": "And then for all $i=1,\\hdots,n$ , Proposition B.4 and $\\textstyle\\sum_{i=1}^{n}\\left|\\left|p_{i}-q_{i}\\right|\\right|<2\\delta$ imply $\\gamma_{2i-1}:[0,1]\\to\\mathbb{R}^{D}$ be such that $\\gamma_{2i-1}(0)=p_{i}$ , $\\gamma_{2i-1}(1)=q_{i}$ and suc h that $\\begin{array}{r}{\\sum_{i=1}^{n}\\mathrm{LEN}_{f^{*}H}\\big(\\gamma_{2i-1}\\big)<2L\\delta}\\end{array}$ . ", "page_idx": 17}, {"type": "text", "text": "Then concatenating these $2n+1$ paths, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\r{d_{f^{*}H}(\\mathbf{w}_{0},\\mathbf{w}_{1})=\\operatorname*{inf}_{\\hat{\\gamma}}\\mathrm{LEN}_{f^{*}H}(\\hat{\\gamma})\\leq\\mathrm{LEN}_{f^{*}H}\\big(\\mathrm{CAT}(\\gamma_{0},\\ldots,\\gamma_{2n})\\big)}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad=\\sum_{i=0}^{2n}\\mathrm{LEN}_{f^{*}H}(\\gamma_{i})=\\sum_{i=1}^{n}\\mathrm{LEN}_{f^{*}H}(\\gamma_{2i-1})+\\sum_{i=0}^{n}\\mathrm{LEN}_{f^{*}H}(\\gamma_{2i})<2L\\delta+L\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B.3 Proof of Eq. 23 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proposition B.7. The pullback metric does not \u201crotate\u201d too much when moving from w to $\\mathbf{w}+\\epsilon$ , formally ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1-K\\|\\epsilon\\|)\\|v\\|_{f^{*}H_{\\mathbf{w}+\\epsilon}}\\leq\\|v\\|_{f^{*}H_{\\mathbf{w}}}\\leq(1+K\\|\\epsilon\\|)\\|v\\|_{f^{*}H_{\\mathbf{w}+\\epsilon}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Follows from the $K$ -Lischitz assumption on $\\mathbf{w}\\mapsto\\mathbf{J}_{\\mathbf{w}}$ ", "page_idx": 17}, {"type": "text", "text": "The choice of $\\epsilon,\\delta$ that prove Eq. 23 trivially follows from the following ", "page_idx": 17}, {"type": "text", "text": "Proposition B.8. With the assumption of eigenvalue upper bound $L_{i}$ , for any $\\mathbf{w}_{0},\\mathbf{w}_{1}\\in\\mathbb{R}^{D}$ for any $\\delta>0$ it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\nd_{f^{*}H}(\\mathbf{w}_{0},\\mathbf{w}_{1})<\\delta\\quad\\Longrightarrow\\quad d_{\\mathcal{P}}([\\mathbf{w}_{0},\\mathbf{w}_{1}])<\\frac{6\\delta}{l}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. $d_{f^{*}H}(\\mathbf{w}_{0},\\mathbf{w}_{1})<\\delta$ implies that there exist a path $\\gamma:[0,1]\\rightarrow\\mathbb{R}^{D}$ with ", "page_idx": 17}, {"type": "equation", "text": "$$\n2\\delta=\\mathtt{L E N}_{f^{*}H}(\\gamma)=\\int_{0}^{1}\\|\\gamma^{\\prime}(t)\\|_{f^{*}H_{\\gamma(t)}}\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Consider also the Euclidean length $\\mathrm{LEN}_{E}(\\gamma)$ which does not depend on $\\delta$ . Then, for any $\\delta$ there exist $n(\\delta)\\in\\mathbb{N}$ such that, considering the uniform partition of $[0,1]$ , $t_{i}=\\i/{n(\\delta)}$ for $i=0,\\ldots,n(\\delta)$ it holds the discrete approximation of the integral ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{3\\delta>\\displaystyle\\sum_{i=0}^{n(\\delta)-1}\\|\\gamma(t_{i+1})-\\gamma(t_{i})\\|_{f^{*}H_{\\gamma(t_{i})}}=\\displaystyle\\sum_{i=0}^{n(\\delta)-1}\\|\\gamma(t_{i+1})-p_{i}+p_{i}-\\gamma(t_{i})\\|_{f^{*}H_{\\gamma(t_{i})}}}&{\\quad(59\\delta)}\\\\ &{=\\displaystyle\\sum_{i=0}^{n(\\delta)-1}\\|\\gamma(t_{i+1})-p_{i}\\|_{f^{*}H_{\\gamma(t_{i})}}+\\underbrace{\\|p_{i}-\\gamma(t_{i})\\|_{f^{*}H_{\\gamma(t_{i})}}}_{=0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $p_{i}$ is defined for every $i\\in\\{0,\\ldots,n(\\delta)-1\\}$ as follow. Consider the projection $P_{i}^{K}$ on the kernel of $f^{*}H_{\\gamma(t_{i})}$ , and the projection $P_{i}^{I}$ on the image of $f^{*}H_{\\gamma(t_{i})}$ , such that the two projections are orthogonal and $P_{i}^{K}+P_{i}^{I}=\\mathbf{I}$ . Define the point $p_{i}\\in\\mathbb{R}^{D}$ as $p_{i}=P_{i}^{K}(\\gamma(t_{i+1})-\\gamma(t_{i}))+\\gamma(t_{i})$ , which implies that $|\\bar{|p_{i}}\\,-\\,\\bar{\\gamma}\\bar{(t_{i})}||_{f^{*}H_{\\gamma(t_{i})}}\\,=\\,0$ . By definition of the projections, it also hold that $p_{i}=\\gamma(t_{i+1})-P_{i}^{I}(\\gamma(t_{i+1})-\\gamma(t_{i}))$ , which implies that $\\gamma(t_{i+1})-p_{i}=P_{i}^{I}(\\gamma(t_{i+1})-\\gamma(t_{i}))$ is aligned with the non-zero eigenvalues of $f^{*}H_{\\gamma(t_{i})}$ , this means that we can resort to the lower bound $l$ on the non-zero eigenvalues and Proposition B.7 to see that ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Vert\\gamma(t_{i+1})-p_{i}\\Vert_{f^{*}H_{\\gamma(t_{i})}}\\ge(\\underbrace{1-K\\Vert\\gamma(t_{i+1})-\\gamma(t_{i})\\Vert}_{K})\\Vert\\gamma(t_{i+1})-p_{i}\\Vert_{f^{*}H_{\\gamma(t_{i+1})}}}&{}\\\\ {\\ge\\tilde{K}l\\Vert\\gamma(t_{i+1})-p_{i}\\Vert}&{}\\\\ {\\ge\\frac{l}{2}\\Vert\\gamma(t_{i+1})-p_{i}\\Vert}&{\\quad\\forall i\\in\\{0,\\ldots,n(\\delta)-1\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where, recalling that $\\|\\gamma(t_{i+1})-\\gamma(t_{i})\\|\\,=\\,1/{n(\\delta)}$ by construction, there always exist an $n(\\delta)$ big enough such that $\\tilde{K}\\geq{^1\\atop}/2$ , which we can assume without loss of generality. Rearranging the terms, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{6\\delta}{l}\\geq\\sum_{i=0}^{n(\\delta)-1}\\|\\gamma(t_{i+1})-p_{i}\\|.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, we can define the points $q_{i}=\\gamma(t_{i+1})$ and we have the two sequences $p_{0},\\ldots,p_{n(\\delta)-1}$ and $q_{0},\\ldots,q_{n(\\delta)-1}$ whose satisfies the costrains $\\left[\\mathbf{w}_{0}\\right]=\\left[p_{0}\\right]$ , $\\left[p_{i+1}\\right]=\\left[q_{i}\\right]$ and $\\left[q_{n(\\delta)-1}\\right]=\\left[\\mathbf{w}_{1}\\right]$ . We highlight that the dependence on $\\delta$ of the length of the sequence is not problematic, as it is sufficient that $n(\\delta)$ is finite for every fixed $\\delta$ , which it is. Thus, ", "page_idx": 18}, {"type": "equation", "text": "$$\nd_{\\mathcal{P}}([\\mathbf{w}_{0},\\mathbf{w}_{1}])\\leq\\sum_{i=0}^{n(\\delta)-1}\\|p_{i}-q_{i}\\|<\\frac{6\\delta}{l},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 18}, {"type": "text", "text": "C Proof of existence of the two Riemannian manifolds ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "This section contains the proof for the existence of the two Riemannian manifolds $(\\mathcal{P}_{\\bar{\\mathbf{w}}},\\mathfrak{m})$ and $(\\mathcal{P}_{\\bar{\\mathbf{w}}}^{\\perp},\\mathfrak{m}^{\\perp})$ embedded in $\\mathbb{R}^{D}$ . In the rest of the section, we consider a fixed $\\bar{\\bf w}\\in\\mathbb{R}^{D}$ . ", "page_idx": 18}, {"type": "text", "text": "Fix a training set $\\mathcal{X}=\\left\\{\\mathbf{x}_{1},\\boldsymbol{\\cdot}\\cdot\\boldsymbol{\\cdot},\\mathbf{x}_{N}\\right\\}\\subset\\mathbb{R}^{I}$ of size $N$ and consider the stacked partial evaluation of the network defined as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathfrak{F}:\\mathbb{R}^{D}\\longrightarrow\\mathbb{R}^{N O}}}\\\\ &{}&{\\mathbf{w}\\longmapsto(f(\\mathbf{w},\\mathbf{x}_{1}),\\dots,f(\\mathbf{w},\\mathbf{x}_{N}))\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "And define $\\bar{\\mathbf{y}}=\\mathfrak{F}(\\bar{\\mathbf{w}})\\in\\mathbb{R}^{N O}$ . The differential $\\nabla_{\\mathbf{w}}\\mathfrak{F}\\big|_{\\bar{\\mathbf{w}}}=\\mathbf{J}_{\\bar{\\mathbf{w}}}\\in\\mathbb{R}^{N O\\times D}$ equals the stacking of the per-datum Jacobians, and we assume it to be full rank thanks to the uniform lower bound on the eigenvalues of the NTK matrix. Full-rankness in the overparametrized setting $D>N O$ implies the surjectivity of the differential operator. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{J}_{\\bar{\\mathbf{w}}}=\\left(\\frac{\\partial\\mathfrak{F}_{i}}{\\partial\\mathbf{w}_{j}}\\bigg\\rvert_{\\bar{\\mathbf{w}}}\\right)_{i=1,\\dots,N O;\\,j=1,\\dots,D}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We reorder the $\\mathbf{w}_{i}$ so that the first $N O$ columns are independent. Then the $N O\\times N O$ matrix ", "page_idx": 18}, {"type": "equation", "text": "$$\nR=\\left(\\frac{\\partial\\mathfrak{F}_{i}}{\\partial\\mathbf{w}_{j}}\\bigg|_{\\bar{\\mathbf{w}}}\\right)_{i=1,...,N O;\\,j=1,...,N O}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "is non-singular. We consider the map ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\alpha(\\mathbf{w}_{1},\\hdots,\\mathbf{w}_{D})=(\\mathfrak{F}(\\mathbf{w})_{1},\\hdots,\\mathfrak{F}(\\mathbf{w})_{N O},\\mathbf{w}_{N O+1},\\hdots,\\mathbf{w}_{D})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{w}}\\alpha|_{\\bar{\\mathbf{w}}}=\\left(\\frac{\\partial\\alpha_{i}}{\\partial\\mathbf{w}_{j}}\\bigg|_{\\bar{\\mathbf{w}}}\\right)_{i=1,\\dots,D;\\,j=1,\\dots,D}=\\left(\\mathbf{0}\\quad\\ast\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and this is non-singular. By the inverse function theorem, $\\alpha$ is a local diffeomorphism. So there is an open $W\\subseteq\\mathbb{R}^{D}$ containing $\\bar{w}$ such that $\\alpha|_{W}:W\\to\\alpha(W)$ is smooth with smooth inverse. ", "page_idx": 18}, {"type": "text", "text": "Finally, define ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{P}_{\\bar{\\mathbf{w}}}^{\\perp}=\\left\\{\\alpha^{-1}(\\underbrace{\\bar{\\bf v}_{1},\\ldots,\\bar{\\bf y}_{N O}}_{N O},p_{1},\\ldots,p_{D-N O})\\quad\\mathrm{~for~}p\\in\\mathbb{R}^{D-N O}\\right\\}\\subseteq\\mathbb{R}^{D},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and similarly ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{P}_{\\bar{\\mathbf{w}}}=\\left\\{\\alpha^{-1}(p_{1},\\dots,p_{N O},\\underbrace{0,\\dots,0}_{D-N O})\\quad\\mathrm{~for~}p\\in\\mathbb{R}^{N O}\\right\\}\\subseteq\\mathbb{R}^{D}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We claim that the two restrictions of $\\alpha$ are slice charts of $\\mathcal{P}_{\\bar{\\mathbf{w}}}^{\\perp}$ and $\\mathcal{P}_{\\bar{\\mathbf{w}}}$ , respectively. Since it is a smooth diffeomorphism, it is certainly a chart. Moreover, by construction, the points in $\\mathcal{P}_{\\bar{\\mathbf{w}}}^{\\perp}$ are exaclty those whose image under $\\mathfrak{F}$ is $\\bar{y}$ , thus, $\\mathcal{P}_{\\bar{\\mathbf{w}}}^{\\perp}\\subseteq[\\bar{\\mathbf{w}}]$ . On the other hand, the points $\\mathbf{w}\\in\\mathcal{P}_{\\bar{\\mathbf{w}}}$ parametrize functions that take the values $\\mathfrak{F}(\\mathbf{w})=p$ in a local neighbourhood of $\\bar{y}$ . Thus, locally it never intersects the same equivalence class more than one time. ", "page_idx": 19}, {"type": "text", "text": "D Proof of Theorem 5.1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "By definition of the kernel manifold, we have that if $\\mathbf{w}\\in\\mathcal{P}_{\\mathbf{w}^{\\prime}}^{\\perp}$ then we have that $\\mathbf{w}\\in[\\mathbf{w}^{\\prime}]$ . Hence for all $\\mathbf{w}\\in\\mathcal{P}_{\\mathbf{w}^{\\prime}}^{\\perp}$ and for all $\\mathbf{x}\\in\\mathcal{X}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\nf(\\mathbf{w},\\mathbf{x})=f(\\mathbf{w}^{\\prime},\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "It follows that $\\mathrm{Var}_{\\mathbf{w}\\sim\\mathcal{P}_{\\mathbf{w}^{\\prime}}^{\\perp}}\\left[f(\\mathbf{w},\\mathbf{x})\\right]=0$ for any $\\mathbf{x}\\in\\mathcal{X}$ . ", "page_idx": 19}, {"type": "text", "text": "For the second statement notice that $\\mathrm{Var}_{\\mathbf{w}\\sim\\mathcal{P}_{\\mathbf{w^{\\prime}}}^{\\perp}}\\left[f(\\mathbf{w},\\mathbf{x}_{t e s t}\\right]=0$ if and only if $f(\\mathbf{w},\\mathbf{x}_{t e s t})=c$ for all $\\mathbf{w}\\sim\\mathcal{P}_{\\mathbf{w}^{\\prime}}^{\\perp}$ and for some constant $^c$ . ", "page_idx": 19}, {"type": "text", "text": "Suppose w\u02c6 $\\in\\mathcal{\\bar{R}}_{\\mathcal{X}}^{f}$ and $\\hat{\\textbf{w}}\\notin\\mathcal{\\bar{R}}_{\\mathcal{X}\\cup\\{\\mathbf{x}_{t e s t}\\}}^{f}$ , then $f(\\hat{\\mathbf{w}},\\mathbf{x}_{t e s t})\\;\\neq\\;f(\\mathbf{w}^{\\prime},\\mathbf{x}_{t e s t})$ , which means that $f(\\mathbf{w},\\mathbf{x}_{t e s t})$ is not constant. Hence we have that $\\mathrm{Var}_{\\mathbf{w}\\sim\\mathcal{P}_{\\mathbf{w^{\\prime}}}^{\\perp}}\\left[f(\\mathbf{w},\\mathbf{x}_{t e s t}\\right]>0$ ", "page_idx": 19}, {"type": "text", "text": "E Further results and experimental setup ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "E.1 Implementation details of the Laplace approximation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Sampling from Laplace\u2019s approximation requires computing the inverse square root of a matrix of size $D\\times D$ , where $D$ is the number of parameters. For most models this problem is intractable. The standard approach to this problem is to consider sparse approximations to the Hessian of the loss function such as KFAC, Last-Layer, and Diagonal approximations. However, these approximations introduce additional complexity making the task of validating our theoretical analysis much harder. In light of these considerations, we choose to sample from Laplace\u2019s approximation in a way that is closest to the theoretical ideal, at the cost of performing expensive computations. ", "page_idx": 19}, {"type": "text", "text": "In small experiments with the toy regression problem, we instantiate the exact GGN and compute vector products with its inverse-square root. For experiments with LeNet and ResNet, we rely on the empirical observation that the spectrum of the GGN is dominated by its leading eigenvalues (Figure 6) ", "page_idx": 19}, {"type": "text", "text": "This makes low-rank approximations of the GGN particularly attractive. We choose the Lanczos algorithm (Lanczos, 1950) with full reorthogonalization and run it for a very high number of iterations, to ensure numerical stability and very low reconstruction error to form our low-rank approximations of the GGN. Additionally, Lanczos only requires implicit access to the matrix so we avoid the memory cost and GGN vector products for neural networks can be performed efficiently using Jacobian-Vector products and Vector-Jacobian products. If we do ", "page_idx": 19}, {"type": "image", "img_path": "204YOrDHny/tmp/15e40da4a7af57247b769906bff9fd8a1591acf9b1338596392128585157e8e4.jpg", "img_caption": ["Figure 6: Eigenvalues of the GGN of a Convolutional Neural Network trained on MNIST. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "1: Input: Observed data $\\mathcal{D}$ , trained MAP point $\\mathbf{w}^{\\prime}$ , number of steps $\\textbf{\\emph{T}}$ , number of samples $\\boldsymbol{S}$ , rank $k$ . 23::  Ifonirt liinz e sd $\\mathbf{w}_{1}^{0}\\ldots\\mathbf{w}_{S}^{0}$ as the MAP estimate $\\mathbf{w}^{\\prime}$ $j$ $1,\\cdot\\cdot S$ 4: for $t$ in $\\scriptstyle1,\\,.\\,.\\,T$ do 5: Sample $\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I}_{k})$ 6: Compute the top- ${\\cdot k}$ eigenvalues $(\\Lambda_{j}^{t})$ and eigenvectors $(U_{j}^{t})$ of $\\mathrm{GGN}_{\\mathbf{w}_{j}^{t}}$ 7: $\\begin{array}{r}{\\mathbf{w}_{j}^{t}\\leftarrow\\mathbf{w}_{j}^{t}+\\frac{1}{\\sqrt{N}}U_{j}^{t}(\\Lambda_{j}^{t}+\\alpha\\mathbf{I})^{-\\frac{1}{2}}\\epsilon}\\end{array}$ 8: end for 9: end for 10: Return posterior samples $\\mathbf{w}_{1}^{T}\\ldots\\mathbf{w}_{S}^{T}$ . ", "page_idx": 20}, {"type": "text", "text": "a sufficiently large number of iterations we obtain the non-zero eigenvalues $\\tilde{\\mathbf{A}}$ , and corresponding eigenvectors $\\mathbf{U_{1}}$ . For obtaining samples from the diffusion we can use Algorithm 1 with the eigenvalues and eigenvectors computed using the Lanczos algorithm. ", "page_idx": 20}, {"type": "text", "text": "Given the non-zero eigenvalues $\\tilde{\\mathbf{A}}$ , and corresponding eigenvectors $\\mathbf{U_{1}}$ we can also form inverse vector products with the square root of $\\mathrm{GGN}+\\alpha I$ . It should be evident from the discussion in section 3 about decomposing the covariance that this vector product with a vector $v$ is given by: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\big(\\mathtt{G G N}+\\alpha I\\big)^{-\\frac{1}{2}}v=\\mathbf{U_{1}}(\\tilde{\\mathbf{A}}+\\alpha\\mathbf{I}_{k})^{-\\frac{1}{2}}v+\\frac{1}{\\sqrt{\\alpha}}\\mathbf{U_{2}}v}\\\\ &{\\phantom{\\big(}\\qquad\\qquad\\qquad=\\mathbf{U_{1}}(\\tilde{\\mathbf{A}}+\\alpha\\mathbf{I}_{k})^{-\\frac{1}{2}}v+\\frac{1}{\\sqrt{\\alpha}}(\\mathbf{I}-\\mathbf{U_{1}})v}\\\\ &{\\phantom{\\big(}\\qquad\\qquad\\qquad=\\mathbf{U_{1}}((\\tilde{\\mathbf{A}}+\\alpha\\mathbf{I}_{k})^{-\\frac{1}{2}}-\\frac{1}{\\sqrt{\\alpha}}\\mathbf{I}_{k})v+\\frac{1}{\\sqrt{\\alpha}}v}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This allows us to form inverse-square root vector products with the $\\mathrm{GGN}+\\alpha I$ given the non-zero eigenspectrum. We run the sampling algorithm on ${\\mathrm{H}}100$ GPUs to run the high-order Lanczos decomposition. This approach of sampling from Laplace\u2019s approximation has $O(p\\bar{k}^{2})$ time complexity, and $\\bar{O}(p k)$ memory cost, where $k$ is the number of Lanczos iterations and $p$ is the number of parameters. ", "page_idx": 20}, {"type": "text", "text": "E.2 Experimental details and further results for toy experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "E.2.1 Toy regression in Figure 2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this experiment, we fit a small MLP, with 2 hidden layers of width 10 on the sine curve. Due to the small size of the GGN it is possible to instantiate and do all the computations explicitly. We sample from the exact Laplace\u2019s approximation, the non-kernel and kernel subspace of the GGN, and use the neural network and the linearized predictive functions for the top row and the bottom row respectively. For the middle, we simulate a diffusion on the kernel manifold for the center plot, a diffusion in the non-kernel manifold for the right plot and we do alternating steps in the two manifolds for the left plot which gives us the full distribution. ", "page_idx": 20}, {"type": "text", "text": "We also do a similar experiment to show that the same phenomenon also holds for classification. We use a small convolutional neural network, with 2 convolutional layers with kernel of size 3, to classify a 2-class mixture of Gaussians and look at uncertainties of sampled Laplace, linearized Laplace, and Laplace\u2019s diffusion. We decompose these uncertainties into their kernel and non-kernel components respectively. We see the same effect for classification as we did in regression in Figure 7 ", "page_idx": 20}, {"type": "text", "text": "The main takeaway of these experiments is that sampled Laplace underfits in-distribution and this effect is related to the kernel component of the distribution. ", "page_idx": 20}, {"type": "text", "text": "E.2.2 Effect of kernel rank on in-distribution fit in figure 4 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For this experiment, we train a small convolutional neural network, with two convolutional layers and a kernel of size 3, on MNIST. In this case, the GGN can be instantiated explicitly. We recall that the ", "page_idx": 20}, {"type": "image", "img_path": "204YOrDHny/tmp/af24c61c3a0fb344a57427284ea1b3765c42e40bdbb67552348f9638497004d0.jpg", "img_caption": ["Figure 7: Decomposition of uncertainties of Laplace Approximation for the Gaussian mixture classification. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "GGN is a sum of $\\mathbf{J}_{\\mathbf{w}^{\\prime}}(\\mathbf{x}_{n})^{\\top}\\mathbf{H}(\\mathbf{x}_{n})\\mathbf{J}_{\\mathbf{w}^{\\prime}}(\\mathbf{x}_{n})$ over the dataset where ${\\bf x}_{n}$ are the individual data points. We recall that the rank of the GGN is bounded by $N O$ where $N$ is the number of data points and $O$ is the output dimensions. This suggests considering partial sums by subsampling the data points gives us a GGN with a lower rank. Equivalently, this is GGN with a higher dimensional kernel, and hence the usual covariance from Laplace\u2019s approximation $(\\mathrm{GGN}+\\alpha\\bar{I})^{-1}$ has a higher contribution from the kernel subspace. ", "page_idx": 21}, {"type": "text", "text": "We consider multiple such subsamples and plot the training accuracy for samples from Laplace\u2019s approximation against the kernel subspace dimension. Here we see a clear trend that the underfitting in sampled Laplace decreases as the rank of GGN increases, or the contribution from the kernel component decreases. This serves to further support our suggested hypothesis that the underfitting in sampled Laplace is caused by its kernel component and is hence deeply related to reparameterizations. ", "page_idx": 21}, {"type": "text", "text": "E.3 Additional benchmarks and results for image classification ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Training details: We use a standard LeNet for the MNIST and FashionMNIST experiments and a smaller version of ResNet (Lippe, 2022), with 272,378 parameters consisting of 3 times a group of 3 ResNet blocks. We use those instead of the standard ResNets due to constraints on the computational budget for the CIFAR-10 experiments. We train LeNet with Adam optimizer and a learning rate of $10^{-{\\overline{{3}}}}$ . For the ReNet we use SGD with a learning rate of 0.1 with momentum and weight decay. ", "page_idx": 21}, {"type": "text", "text": "Hyperparameters: We benchmark Laplace diffusion against SWAG, diagonal Laplace, last-layer Laplace, and MAP in addition to linearised Laplace and sampled Laplace. ", "page_idx": 21}, {"type": "text", "text": "For choosing the prior precision for diagonal Laplace and last-Layer Laplace for each benchmark we do a grid search over the set $\\{0.1,\\bar{1.0},5.0,1\\bar{0}.0,50.0,100.0\\}$ . For Laplace diffusion, sampled Laplace, and Linearised to ensure that the comparison can validate the theory it is preferable to have the same prior precision for all of these methods. So we only do the grid search to tune the prior precision for sampled Laplace and use this for all three methods. We keep the hyperparameters for these three methods as similar as possible to have the most informative comparisons. ", "page_idx": 21}, {"type": "text", "text": "For Laplace diffusion on MNIST and FMNIST, we simulate the diffusion with a step size of 0.05, with 2000 Lanczos iterations and we predict using $20\\,\\mathrm{MC}$ samples. For sampled Laplace and Linearised Laplace, we also use 2000 Lanczos iterations and we predict using $20\\,\\mathrm{MC}$ samples. For the CIFAR-10 experiments, we simulate the diffusion with a step size of 0.2, with 5000 Lanczos iterations and we predict using $20\\,\\mathrm{MC}$ samples. For sampled Laplace andlLinearised Laplace use the same number of Lanczos iterations and MC samples. ", "page_idx": 21}, {"type": "text", "text": "For SWAG we use a learning rate of $10^{-2}$ with momentum of 0.9 and weight decay of $3e^{-4}$ and the low-rank covariance structure in all experiments. For the MNIST and FMNIST experiments we collect 20 models and for the CIFAR-10 experiments we collect 3 models to sample from the posterior. ", "page_idx": 21}, {"type": "text", "text": "Last-layer Laplace is the recommended method by Daxberger et al. (2021c) so it should approximate the best performance one can get using various possible configurations. For the CIFAR-10 experiments, the last layer of ResNet is too large to instantiate the full GGN matrix. So we instead use the last 1000 parameters of the model to construct the covariance matrix of the posterior. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Diagonal Laplace requires high prior precision to ensure it does not severely underfit in-distribution (similar to sampled Laplace). It often becomes almost deterministic. So we exclude it from the CIFAR results. This has also been observed by Deng et al. (2022) and Ritter et al. (2018). ", "page_idx": 22}, {"type": "text", "text": "All additional information about the experimental setup can be found in the submitted code. ", "page_idx": 22}, {"type": "text", "text": "E.3.1 In-distribution fit and calibration ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We extend Table 1 to benchmark Laplace\u2019s diffusion against various other Bayesian methods. Here we see that despite using the neural network predictive it is competitive with the best-performing Bayesian methods whereas Sampled Laplace performs significantly worse. ", "page_idx": 22}, {"type": "table", "img_path": "204YOrDHny/tmp/c5ba2a02d34a54534f13318eea3cc4d1e6b376f4db80ee95b636fb7f0b565bf2.jpg", "table_caption": ["MNIST "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "204YOrDHny/tmp/f74f5fc19bcfb6bb41a7256f469a7e0346b0e17d1be06bc612197b013f00a08a.jpg", "table_caption": ["FMNIST "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "204YOrDHny/tmp/8de9bd8d5159bfd486f040aea5bb5da0712d6ae3bd08c52f986228b48ddab48c.jpg", "table_caption": ["CIFAR-10 "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "E.3.2 Robustness to dataset shift ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In these experiments (Fig. 8), to measure in-distribution fit and calibration, we report accuracy, negative log-likelihood (NLL), and expected calibration error (ECE)\u2014all evaluated on the standard test sets. We measure the robustness of dataset shift of various baselines by plotting the negative log-likelihood and the expected calibration error against shift intensity. The desired behavior is good in-distribution fit, as close as possible to MAP, and stable calibration errors and NLL under distribution shifts. We see that the Laplace\u2019s diffusion is competitive against other Bayesian methods. ", "page_idx": 22}, {"type": "text", "text": "E.3.3 Out-of-distribution detection ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We extend 2 to benchmark Laplace\u2019s diffusion against various other Bayesian methods for Out-ofDistribution Detection. Once again, we observe that, despite using the neural network predictive, ", "page_idx": 22}, {"type": "image", "img_path": "204YOrDHny/tmp/ca3a853a0444f5814ef556a75304939f1d4f385e232cb2c9c383318650797eb6.jpg", "img_caption": ["Figure 8: Model Fit and Calibration of various posterior sampling methods on in-distribution data(first column) and under distribution shift for MNIST(top row), Fashion MNIST(middle row) and CIFAR10(bottom row). We use rotated MNIST, rotated FMNIST, and rotated CIFAR in the second and third columns. Shift intensities denote angles of rotation. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "it is competitive with the best-performing Bayesian methods in terms of having a higher AUROC, whereas Sampled Laplace performs significantly worse. ", "page_idx": 23}, {"type": "table", "img_path": "204YOrDHny/tmp/10a13bc59389ab27c64ad24e90e58f74582a4b1c4694e16dc7e1e9ff45bb1781.jpg", "table_caption": ["MNIST "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "204YOrDHny/tmp/aa3089c77bd0f33798cb24a70d63eb0fb5e07e08689c82c64d1baad3e58685e5.jpg", "table_caption": ["FMNIST "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "204YOrDHny/tmp/bf8c16342c4b88f8aeacfa9f11207ae6091a6859a249585716689a6282596dff.jpg", "table_caption": ["CIFAR-10 "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "E.4 Short discussion on benchmarks ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Sparse Approximations of GGN. Our theoretical analysis is mainly concerned with the ideal versions of Laplace\u2019s approximations, where we consider the full GGN in the covariance without any approximations. However, it can also shed some light on other Bayesian methods. ", "page_idx": 24}, {"type": "text", "text": "It is common to use sparse approximations of the GGN when doing Laplace\u2019s approximations. Interestingly we observe that Laplace\u2019s approximations with sparse GGN such as diagonal Laplace, Last Layer, etc do not benefit from linearization to the same degree (Fig. 9). ", "page_idx": 24}, {"type": "image", "img_path": "204YOrDHny/tmp/21ef818c36d985a59051773e432ea6120034a1275d06d03ce84194d9c17ef6c2.jpg", "img_caption": ["Figure 9: Predictive uncertainty of Laplace\u2019s approximation with neural network and linearized predictive (top row) and diagonal Laplace with neural network and linearized predictive (bottom row). "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "This is perfectly consistent with our analysis as we show that the benefit of linearization primarily comes from the Jacobian term in the linearized predictive and the GGN sharing a kernel. Diagonal and other sparse approximations do not share the spectral properties with the Jacobian in the linearized predictive. George et al. (2018) note the potential advantages of having the spectrum of the approximate curvature more aligned with the true curvature. This suggests future directions to improve various approximations to the GGN by accounting for reparameterizations. ", "page_idx": 24}, {"type": "text", "text": "SWAG. Another baseline that can be explained using our method is the SWAG. It has been shown that in (Li et al., 2021) SGD steps close to the optimum can be decomposed into a normal space component and a tangent space component. In our terminology, this can be thought of as a diffusion step in the Kernel manifold and a diffusion step in the Non-kernel manifold. Hence it can be shown that SWAG roughly approximates a diffusion-based posterior. Hence our analysis can provide some theoretical grounding for heuristic methods like SWAG. ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The theoretical findings are states in abstract and introduction as clear as possible withouth first introducing all the notation of the paper. The experimental results match the claim and confirm the theoretical findings. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The conclusion raises several points of concern. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: all theoretical statements are given a detailed derivation in the appendix. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide a detailed appendix describing the experimental setup. We will further release all code to reproduce the paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We rely on established benchmark data. All code will be released under an open source software licence upon paper acceptance. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We specify the details about the experimental setup in the appendix. Furthermore, we also provide the code as supplemental material which contains the full details of the experiments. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide errors and/or standard deviations for presented results. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: these details are presented in the appendix. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: The work is predominantly theoretical and does not raise ethical concerns. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper is predominantly theoretical. However, uncertainty quantification is a potential remedy for several issues with currently deployed machine learning models, so there is a potential for positive societal impact. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 29}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: we do not deem this to be relevant. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: we only consider well-established benchmark data, which we cite appropriately. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: we will release open source code for reproducing experiments upon paper acceptance. This is well documented. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: this paper does not involve crowdsourcing or other forms of research involving human subjects. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: this paper does not involve crowdsourcing or other forms of research involving human subjects. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 31}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]