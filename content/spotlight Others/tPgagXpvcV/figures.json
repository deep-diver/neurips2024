[{"figure_path": "tPgagXpvcV/figures/figures_5_1.jpg", "caption": "Figure 1: Illustration of the architecture for a target graph of size 3 and M = 4.", "description": "This figure illustrates the Any2Graph architecture, which consists of three main modules: an encoder (input-data-dependent), a transformer (with encoder and decoder), and a graph decoder.  The encoder processes the input data and generates a set of features. These features are passed to the transformer, which converts them into node embeddings. The graph decoder then uses these embeddings to predict the properties of the output graph, including the node mask (\u0125), node features (\u00ca), and adjacency matrix (\u00c2). The PM-FGW loss function compares this prediction to the padded target graph (h, F, A), where 'h' represents the node mask, 'F' represents the node features, and 'A' represents the adjacency matrix of the target graph.", "section": "4 Any2Graph: a framework for end-to-end SGP"}, {"figure_path": "tPgagXpvcV/figures/figures_8_1.jpg", "caption": "Figure 1: Illustration of the architecture for a target graph of size 3 and M = 4.", "description": "The figure illustrates the Any2Graph architecture. It consists of three main modules: an encoder that extracts features from the input data, a transformer that converts these features into node embeddings, and a graph decoder that predicts the properties of the output graph. The output graph's properties, including node features, adjacency matrix, and node masks, are predicted using multi-layer perceptrons (MLPs). The Partially Masked Fused Gromov-Wasserstein (PMFGW) loss is used to compare the predicted graph with the target graph, considering node permutation invariance and handling graphs of varying sizes. The encoder is designed to be adaptable to different input modalities, making Any2Graph a versatile framework for end-to-end supervised graph prediction.", "section": "4 Any2Graph: a framework for end-to-end SGP"}, {"figure_path": "tPgagXpvcV/figures/figures_8_2.jpg", "caption": "Figure 2: Average number of solver iterations required for computing PMFGW loss.", "description": "This figure shows the average number of iterations required for the PMFGW solver to converge as a function of the maximum graph size (M).  It demonstrates that while the number of iterations increases with M, the increase is slower when feature diffusion (FD) is applied, suggesting a sub-linear relationship between iterations and M in that case.", "section": "5.3 Empirical study of Any2Graph properties"}, {"figure_path": "tPgagXpvcV/figures/figures_8_3.jpg", "caption": "Figure 3: Effect of M on test edit distance and number of active nodes for Coloring.", "description": "This figure shows the impact of the maximal graph size parameter M on the Any2Graph model's performance and efficiency for the Coloring dataset. The x-axis represents different values of M, while the left y-axis shows the number of active nodes (nodes with a predicted probability above 0.99), and the right y-axis displays the test edit distance.  As M increases, the number of active nodes initially rises sharply before plateauing; suggesting that Any2Graph efficiently utilizes more node embeddings when available. Notably, performance remains robust despite overparameterization, indicating that the model does not overfit.", "section": "5.3 Empirical study of Any2Graph properties"}, {"figure_path": "tPgagXpvcV/figures/figures_8_4.jpg", "caption": "Figure 4: Effect of \u03b1 on the test edit distance.", "description": "The figure shows the sensitivity analysis of the PMFGW loss to the triplet of weight hyperparameters \u03b1 = [\u03b1h, \u03b1f, \u03b1A]. The heatmap visualizes the test edit distance obtained for various combinations of \u03b1h, \u03b1f, and \u03b1A on the Coloring dataset. It shows that performance is optimal when \u03b1h = \u03b1f = \u03b1A = 1/3, showing relative robustness to the choice of \u03b1.", "section": "5 Numerical experiments"}, {"figure_path": "tPgagXpvcV/figures/figures_14_1.jpg", "caption": "Figure 1: Illustration of the architecture for a target graph of size 3 and M = 4.", "description": "This figure illustrates the Any2Graph architecture.  The architecture consists of three main modules: an encoder, a transformer, and a graph decoder. The encoder takes input data (which can vary depending on the task, such as images, text, or other features) and converts it into a set of features. This feature set is then passed to a transformer module.  The transformer processes these features, creating node embeddings which capture both feature and structural information. Finally, the graph decoder module uses these node embeddings to predict the properties of the output graph, including the node features, adjacency matrix, and node masking.  The predicted graph and a padded version of the target graph are then fed into the PM-FGW loss function, which measures the difference between them.", "section": "4 Any2Graph: a framework for end-to-end SGP"}, {"figure_path": "tPgagXpvcV/figures/figures_18_1.jpg", "caption": "Figure 1: Illustration of the architecture for a target graph of size 3 and M = 4.", "description": "This figure illustrates the Any2Graph architecture.  It shows the flow of data from the input through three main modules: the encoder, the transformer, and the graph decoder. The encoder processes various input types (images, text, graphs, vectors) to extract features. These features are fed into the transformer which converts them into a fixed number (M) of node embeddings, representing features and structure. The graph decoder uses the node embeddings to generate the predicted graph, including node features, edge weights (adjacency matrix), and the number of nodes (mask). Finally, the Partially-Masked Fused Gromov-Wasserstein (PMFGW) loss compares the predicted continuous graph to the padded target graph.", "section": "4 Any2Graph: a framework for end-to-end SGP"}, {"figure_path": "tPgagXpvcV/figures/figures_19_1.jpg", "caption": "Figure 1: Illustration of the architecture for a target graph of size 3 and M = 4.", "description": "This figure illustrates the Any2Graph architecture.  The input data is first processed by an encoder, specific to the input modality (images, text, etc.). This encoder output is fed into a transformer module which generates node embeddings. These embeddings are then input to a graph decoder, which predicts the structure and features of the output graph. The final output is then compared to the padded target graph using the PMFGW loss. The figure highlights the key components of the model, showing the flow of information from the input to the final output and the role of the PMFGW loss function in guiding the learning process.", "section": "4 Any2Graph: a framework for end-to-end SGP"}, {"figure_path": "tPgagXpvcV/figures/figures_22_1.jpg", "caption": "Figure 1: Illustration of the architecture for a target graph of size 3 and M = 4.", "description": "This figure shows the architecture of Any2Graph. The input data is processed by an encoder that produces a set of features. Then, a transformer converts these features into M node embeddings. Finally, a graph decoder predicts the properties of the output graph, i.e., (\u0125, F, \u00c2). The whole framework is optimized using the PMFGW loss. ", "section": "4 Any2Graph: a framework for end-to-end SGP"}, {"figure_path": "tPgagXpvcV/figures/figures_22_2.jpg", "caption": "Figure 1: Illustration of the architecture for a target graph of size 3 and M = 4.", "description": "This figure illustrates the Any2Graph architecture.  It shows the three main modules: an encoder (input-dependent), a transformer, and a graph decoder. The encoder processes the input data (which can vary depending on the task, such as images or text), and produces a set of features. These features are then processed by a transformer to generate node embeddings.  Finally, the graph decoder uses these embeddings to predict the properties of the output graph, including the node features, adjacency matrix, and node mask (which indicates whether a node is present in the graph). The Partially Masked Fused Gromov-Wasserstein (PMFGW) loss function compares the prediction to the padded target graph.  The architecture's flexibility is highlighted by its capacity to handle various input data types.", "section": "4 Any2Graph: a framework for end-to-end SGP"}, {"figure_path": "tPgagXpvcV/figures/figures_23_1.jpg", "caption": "Figure 11: First epochs of training for GDB13 with and without projection of the optimal transport plan to the set of permutations with Hungarian matching. Hungarian matching slightly decreases the performances and induces more oscillations of the loss, which could be explained by a less stable gradient.", "description": "The figure shows the training curves (test loss vs epochs) for the GDB13 dataset with and without using Hungarian matching during the training process.  It demonstrates that using Hungarian matching (projecting the optimal transport plan to the set of permutations) leads to slightly worse performance (higher loss) and more unstable training dynamics (more oscillations). The authors suggest that this is because a continuous transport plan offers a more stable gradient than a discrete permutation.", "section": "F.2 Effect of the OT relaxation on the performances"}, {"figure_path": "tPgagXpvcV/figures/figures_24_1.jpg", "caption": "Figure 1: Illustration of the architecture for a target graph of size 3 and M = 4.", "description": "This figure illustrates the Any2Graph architecture.  It consists of three main modules: an encoder that processes the input data (which can vary in type), a transformer that generates node embeddings, and a graph decoder that predicts the output graph structure and features using the PMFGW loss function. The encoder's design is adaptable based on the input modality. The transformer module processes the node embeddings to consider relationships between nodes.  The graph decoder then takes these embeddings to predict the graph's structure (adjacency matrix) and node features.  The overall output is compared to the ground truth using the Partially Masked Fused Gromov-Wasserstein (PMFGW) loss, which is designed to handle variable-sized graphs and is invariant to node permutations.", "section": "4 Any2Graph: a framework for end-to-end SGP"}, {"figure_path": "tPgagXpvcV/figures/figures_27_1.jpg", "caption": "Figure 1: Illustration of the architecture for a target graph of size 3 and M = 4.", "description": "This figure illustrates the Any2Graph architecture, showing the flow of information from input data through the encoder, transformer, and graph decoder to generate a predicted graph. The input is processed by an input-dependent encoder, followed by a transformer to process the feature vectors. The output of the transformer is then fed into a graph decoder to predict the node features (F), node existence (h), and adjacency matrix (A).  The PM-FGW loss function compares the predicted graph with a padded target graph to train the model. The architecture is designed to be adaptable to various input modalities by changing the encoder.", "section": "4 Any2Graph: a framework for end-to-end SGP"}, {"figure_path": "tPgagXpvcV/figures/figures_28_1.jpg", "caption": "Figure 1: Illustration of the architecture for a target graph of size 3 and M = 4.", "description": "This figure illustrates the Any2Graph architecture, which consists of three main modules: an encoder, a transformer, and a graph decoder. The encoder processes the input data (which can vary depending on the task, such as images or text), and outputs a set of features. These features are then processed by the transformer, which produces a fixed number (M) of node embeddings. Finally, these embeddings are fed to the graph decoder, which predicts the properties of the output graph, such as the node features, the adjacency matrix, and the node mask (indicating whether a node exists in the target graph).  The output is then compared against the target graph using the PMFGW loss function.", "section": "4 Any2Graph: a framework for end-to-end SGP"}, {"figure_path": "tPgagXpvcV/figures/figures_29_1.jpg", "caption": "Figure 1: Illustration of the architecture for a target graph of size 3 and M = 4.", "description": "This figure shows the architecture of Any2Graph, which consists of three main modules: an encoder, a transformer, and a graph decoder. The encoder takes as input different types of data and extracts features. The transformer then converts these features into node embeddings. Finally, the graph decoder predicts the properties of the output graph, including node features and the adjacency matrix.  The PM-FGW loss function is used to compare the predicted graph with the target graph. The figure highlights the flow of information through the model, from input data to the final prediction, emphasizing the use of transformers and a novel loss function designed for end-to-end supervised graph prediction.", "section": "4 Any2Graph: a framework for end-to-end SGP"}]