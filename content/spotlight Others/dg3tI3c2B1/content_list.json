[{"type": "text", "text": "Molecule Design by Latent Prompt Transformer ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deqian $\\mathbf{Kong^{1,\\star}}$ , Yuhao $\\mathbf{Huang}^{2,\\star}$ , Jianwen $\\mathbf{Xie^{3,4,\\star}}$ , Edouardo $\\mathbf{Honig^{1,\\star}}$ , Ming $\\mathbf{Xu^{5}}$ , Shuanghong $\\mathbf{Xue}^{5}$ , Pei $\\mathbf{Lin}^{6}$ , Sanping Zhou2, Sheng Zhong5,6, Nanning Zheng2, Ying Nian $\\mathbf{W}\\mathbf{u}^{1}$ ", "page_idx": 0}, {"type": "text", "text": "1Department of Statistics and Data Science, UCLA 2Institute of Artificial Intelligence and Robotics, Xi\u2019an Jiaotong University 3BioMap Research 4Akool Research 5Institute of Engineering in Medicine, UCSD 6Shu Chien-Gene Lay Department of Bioengineering, UCSD ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This work explores the challenging problem of molecule design by framing it as a conditional generative modeling task, where target biological properties or desired chemical constraints serve as conditioning variables. We propose the Latent Prompt Transformer (LPT), a novel generative model comprising three components: (1) a latent vector with a learnable prior distribution modeled by a neural transformation of Gaussian white noise; (2) a molecule generation model based on a causal Transformer, which uses the latent vector as a prompt; and (3) a property prediction model that predicts a molecule\u2019s target properties and/or constraint values using the latent prompt. LPT can be learned by maximum likelihood estimation on molecule-property pairs. During property optimization, the latent prompt is inferred from target properties and constraints through posterior sampling and then used to guide the autoregressive molecule generation. After initial training on existing molecules and their properties, we adopt an online learning algorithm to progressively shift the model distribution towards regions that support desired target properties. Experiments demonstrate that LPT not only effectively discovers useful molecules across single-objective, multi-objective, and structure-constrained optimization tasks, but also exhibits strong sample efficiency. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Molecule design plays a pivotal role in drug discovery, focusing on identifying or creating molecules with desired pharmacological or chemical properties. However, the vast and complex space of potential drug-like molecules presents significant challenges for systematically designing efficient machine learning models to navigate this space. Latent space optimization (LSO) (Maus et al., 2022; Tripp et al., 2020; G\u00f3mez-Bombarelli et al., 2018), a two-stage procedure, has emerged as a promising approach to address this challenge. The first stage involves training a deep generative model, typically a deep variational auto-encoder (VAE) (Kingma and Welling, 2014), to map low-dimensional continuous vectors to the data manifold in input space, creating a simplified and continuous analog of the original optimization problem. In the second stage, the objective function is optimized over this learned latent space using a surrogate model. While LSO has demonstrated potential in tackling high-dimensional and structured input spaces, existing LSO-based methods separate the training of the generative model from the property-conditioned optimization. This decoupling can lead to suboptimal performance, as the learned latent space may not be ideally suited to the specific optimization task. Furthermore, LSO depends on an additional inference model during training, increasing the number of model parameters and adding complexity to the model design process. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address the above limitations, we propose a novel generative model, the Latent Prompt Transformer (LPT), which unifies molecule generation and optimization within a single framework. The LPT can be directly trained on observed molecule-property pairs via maximum likelihood estimation (MLE) in an end-to-end fashion. We take advantage of the immediately-available Markov chain Monte Carlo (MCMC) inference engine derived from the posterior distribution, eliminating the need for an auxiliary network for variational inference. The LPT consists of three components: (1) a learnable prior model of the latent vector based on a neural transformation of Gaussian noise, (2) a molecule generation model that generates molecule sequences using a causal transformer with the latent vector serving as the prompt, and (3) a property predictor model that estimates the target property values given the latent vector. The latent prompts serve as shared representations for both molecules and properties. By incorporating the MLE of the LPT, we employ an online learning algorithm of the LPT for property optimization, formulated as a conditional generative modeling task. This algorithm progressively shifts the model distribution towards regions associated with desired properties. ", "page_idx": 1}, {"type": "text", "text": "To enhance the practical application of designed molecules and enable a comparative analysis of our generated designs versus those made by human experts, we introduce a new task, which is the conditional generation of molecules that bind to the NAD binding site of Phosphoglycerate dehydrogenase (PHGDH). In addition to single-objective optimization for this specific protein, we also introduce structure-constrained optimization to analyze the design pathways of learning-based models in comparison with human experts. This additional experiment aims to provide valuable insights for improving learning-based model design. We manually process the data to ensure compatibility with docking software, which is essential for facilitating precise score computation. Compared to other widely used evaluation metrics, the use of PHGDH offers greater practical significance and enables more accurate calculations in real-world drug discovery scenarios. ", "page_idx": 1}, {"type": "text", "text": "Our work makes the following key contributions: (1) We propose the LPT, a novel generative framework for jointly modeling molecule sequences and their target properties. This framework leverages a learnable informative prior distribution on a latent space, conceptualized as an intrinsic design representation. (2) We propose an MCMC-based MLE to train the LPT without needing an auxiliary network for variational inference. (3) We develop a novel online learning algorithm for LPT, allowing the model to gradually extrapolate to feasible regions associated with desired properties, improving computational and sample efficiency. (4) We present a new task for the conditional generation of molecules that bind to the NAD binding site of PHGDH, and introduce structure-constrained optimization to compare the design pathways of learning-based models with those of human experts. (5) Our model achieves state-of-the-art performance across a variety of molecule-based optimization tasks, including single-objective design, multi-objective design, and biological sequence design. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Let $x~=~(x^{(1)},...,x^{(t)},...,x^{(T)})~\\in~\\mathcal{X}$ be a sequence representation of molecules such as SMILES (Weininger, 1988) or SELFIES (Krenn et al., 2020; Cheng et al., 2023), where $x^{(t)}\\in\\mathcal{V}$ is the $t$ -th element in the vocabulary $\\nu$ , and $\\mathcal{X}$ is the space of molecules. In molecule design, the objective is to generate targeted molecules that optimize several properties of interest, represented by $\\pmb{y}~=~\\breve{\\{}y_{i}~=~o_{i}^{p}(\\breve{x)}~\\in~\\mathbb{R}\\}$ for $i=1,\\hdots,m$ , while satisfying specific constraints, $\\pmb{c}\\,=\\,\\{c_{j}\\,=\\,o_{j}^{c}(x)\\,\\,\\in\\,\\mathbb{Z}_{2}\\}$ for $j=1,\\dots,n$ . Here, $o^{p}(x)$ and $o^{c}(x)$ denote oracle functions determining property values and constraint satisfaction, respectively, and these values can be obtained either by querying existing software or by conducting wet lab experiments. ", "page_idx": 1}, {"type": "text", "text": "The multi-objective multi-constraint optimization (MMO) problem can be formulated as follows: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in\\mathcal{X}}\\quad F(x)=\\{o_{1}^{p}(x),o_{2}^{p}(x),\\ldots,o_{m}^{p}(x)\\}\\quad\\mathrm{s.t.}\\quad o_{j}^{c}(x)=1,\\quad j=1,\\ldots,n,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $o^{c}(x)=1$ denotes constraint satisfaction and $o^{c}(x)=0$ denotes constraint violation. From a probabilistic modeling perspective, the MMO problem can be viewed as a conditional sampling problem, $x^{*}\\sim p(x|y=\\bar{y}^{*},c=1)$ , where $\\boldsymbol{y}^{*}$ represents the desired values of the target properties, and $\\scriptstyle c\\,=\\,1$ indicates that all constraints are satisfied. ", "page_idx": 1}, {"type": "text", "text": "In generative molecule design, we assume access to a dataset of molecule sequences and their associated properties for offline pretraining of the generative model. During the design phase, the model queries existing software (oracle functions) to obtain single or multiple metrics for the generated molecules, with these oracle functions providing ground-truth property values to be optimized. It is important to acknowledge that developing proper software for accurate evaluation of molecule properties is of equal or even greater importance than designing molecules based on those software. However, it is beyond the scope of this paper to address the development of such software. Here, we treat the property values obtained from oracle functions as ground-truths for optimization. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 Latent Prompt Transformer ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Suppose $x=(x^{(1)},...,x^{(t)},...,x^{(T)})$ is a molecule sequence, (e.g. a string-based representation like SMILES (Weininger, 1988) or SELFIES (Krenn et al., 2020; Cheng et al., 2023)), where $x^{(t)}\\in\\mathcal{V}$ is the $t$ -th element of sequence in the vocabulary $\\mathcal{V}$ . The latent vector is $z\\in\\mathbb{R}^{d}$ and $y\\in\\mathbb R$ is the target property value, or $\\bar{y}\\in\\{0,1\\}$ indicates constraint satisfaction. The term property refers to both real-valued properties and binary-valued constraints in the following sections. The joint distribution of a molecule and its property is defined as $p(x,y)$ . ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathcal{Z}_{0}}&{}&{\\stackrel{!}{,}}&{\\stackrel{}{,}}\\\\ {\\mathcal{Z}_{0}}&{}&{\\stackrel{!}{,}}&{\\stackrel{!}{,}}\\\\ {z=U_{\\alpha}(z_{0})\\Big[}&{}&{\\stackrel{!}{,}}&{\\underbrace{!\\Bigg[\\begin{array}{c c c}{\\cdots---\\frac{\\kappa}{2}-}&{\\cdots--\\frac{\\kappa}{2}}&{}\\\\ {\\mathrm{Cross-atention}}&{}&{}\\end{array}\\Bigg]}_{\\mathrm{~i~}}}\\\\ {\\mathcal{Z}_{0}^{z}\\Bigg\\rangle_{\\mathrm{~R}_{\\alpha}}(z)}&{}&{\\stackrel{!}{,}}&{\\stackrel{!}{,}}\\\\ {\\mathcal{Z}_{\\lambda}^{z}\\Bigg\\rangle_{\\mathrm{~i~}}}&{}&{\\stackrel{!}{,}}&{\\Bigg\\{\\begin{array}{c c c}{\\sum_{\\alpha=1}^{\\infty}\\mathrm{Pransformer}}\\\\ {\\vdots}&{\\vdots}&{\\underbrace{\\mathrm{Pransfransformer}}}\\\\ {\\sum_{\\alpha=--\\infty-\\frac{\\kappa}{2}-}}&{\\cdots-\\cdots-\\cdots-\\cdots}\\\\ {\\sum_{\\alpha=\\infty-\\infty-\\infty-\\infty}^{\\infty}}&{\\vdots}&{\\Bigg\\}}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "", "img_caption": ["Figure 1: Left: Overview of Latent Prompt Transformer (LPT). The latent vector $z\\in\\mathbb{R}^{d}$ is a neural transformation of $z_{0}$ , i.e., $z=U_{\\alpha}(z_{0})$ , where $z_{0}\\sim\\mathcal{N}(0,I_{d})$ . Given $z,\\,x$ and $y$ are independent. $p_{\\beta}(x|z)$ is the molecule generation model and $p_{\\gamma}(y|z)$ predicts the property value or constraint based on $z$ . Right: Illustration of molecule generation model $p_{\\beta}(x|z)$ . The latent vector $z$ is used as a prompt in the $p_{\\beta}(x|z)$ via cross-attention. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "The latent variable $z$ , conceptualized as a design representation, decouples the molecule generation from property prediction. Specifically, given $z$ , we assume $x$ and $y$ are conditionally independent, making $z$ the information bottleneck. With this assumption, our Latent Prompt Transformer (LPT) is defined as, ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{\\theta}(x,y,z)=p_{\\alpha}(z)p_{\\beta}(x|z)p_{\\gamma}(y|z),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\theta=(\\alpha,\\beta,\\gamma)$ . $p_{\\alpha}(z)$ is a prior model with parameters $\\alpha$ . Here, $z$ serves as the latent prompt for the generation model $p_{\\beta}(x|z)$ parameterized by a causal Transformer with parameters $\\beta$ . $p_{\\gamma}(y|z)$ is the predictor model with parameters $\\gamma$ . As shown in Fig. 1, LPT defines the generation process as, ", "page_idx": 2}, {"type": "equation", "text": "$$\nz\\sim p_{\\alpha}(z),[x|z]\\sim p_{\\beta}(x|z),[y|z]\\sim p_{\\gamma}(y|z).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For the prior model, $p_{\\alpha}(z)$ is formulated as a learnable neural transformation from an uninformative distribution, such as an isotropic Gaussian, $z=U_{\\alpha}(z_{0})$ , where $z_{0}\\sim\\mathcal{N}(0,I_{d})$ . $U_{\\alpha}(\\cdot)$ is parameterized by an expressive neural network such as a Unet (Ronneberger et al., 2015) with parameters $\\alpha$ . In this way, $\\bar{p_{\\alpha}(z)}$ can be viewed as an implicit generator model. ", "page_idx": 2}, {"type": "text", "text": "The molecule generation model $p_{\\beta}(x|z)$ is a conditional autoregressive model, $\\begin{array}{r l}{p_{\\beta}(x|z)}&{{}=}\\end{array}$ $\\begin{array}{r}{\\prod_{t=1}^{T}p_{\\beta}(x^{(t)}|x^{(0)},...,x^{(t-1)},z)}\\end{array}$ , which is realized by a causal Transformer with parameters $\\beta$ . The latent vector $z$ , serving as the prompt, controls every step of the autoregressive molecule generation. We incorporate these latent prompts $z$ into the $p_{\\beta}(x|z)$ through cross-attention, as shown in Fig. 1. ", "page_idx": 2}, {"type": "text", "text": "The real-valued property predictor is a non-linear regression model $p_{\\gamma}(y|z)=\\mathcal{N}(s_{\\gamma}(z),\\sigma^{2})$ , where $s_{\\gamma}(z)$ is a small multi-layer perceptron (MLP) predicting $y$ based on the latent prompt $z$ . The variance $\\sigma^{2}$ is treated as a hyper-parameter that balances the exploitation-exploration trade-off. For binary-valued constraints, $\\bar{p_{\\gamma}(y|\\bar{z})}=s_{\\gamma}(z)^{y}(1-s_{\\gamma}(z))^{1-y}$ . For multi-objective tasks, we can use heuristic-based combinations of multiple objectives to form a special single-objective function. ", "page_idx": 2}, {"type": "text", "text": "3.2 Maximum Likelihood Learning of LPT ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Suppose we observe training examples from the dataset with molecule sequence and property pairs $\\bar{D}\\stackrel{...}{=}\\{(x_{i},y_{i}),i=1,...,n\\}$ . The log-likelihood function is $\\begin{array}{r}{L(\\theta)=1/_{n}\\sum_{i=1}^{\\hat{n}}\\log p_{\\theta}(\\dot{x_{i}},\\dot{y_{i}})}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Since $z=U_{\\alpha}(z_{0})$ , we can write the model as ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\theta}(x,y)=\\int p_{\\beta}(x|z=U_{\\alpha}(z_{0}))p_{\\gamma}(y|z=U_{\\alpha}(z_{0}))p_{0}(z_{0})d z_{0},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $p_{0}(z_{0})\\sim\\mathcal{N}(0,I_{d})$ . The learning gradient can be calculated as follows, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\log p_{\\theta}(x,y)=\\mathbb{E}_{p_{\\theta}(z_{0}\\vert x,y)}[\\nabla_{\\theta}\\log p_{\\beta}(x\\vert U_{\\alpha}(z_{0}))+\\nabla_{\\theta}\\log p_{\\gamma}(y\\vert U_{\\alpha}(z_{0}))].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For the prior model, the learning gradient is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\delta_{\\alpha}(x,y)=\\mathbb{E}_{p_{\\theta}(z_{0}|x,y)}[\\nabla_{\\alpha}\\log p_{\\beta}(x|U_{\\alpha}(z_{0}))+\\nabla_{\\alpha}\\log p_{\\gamma}(y|U_{\\alpha}(z_{0})))].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The learning gradient for the molecule generation model is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\delta_{\\beta}(x,y)=\\mathbb{E}_{p_{\\theta}(z_{0}|x,y)}[\\nabla_{\\beta}\\log p_{\\beta}(x|U_{\\alpha}(z_{0}))].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The learning gradient for the predictor model is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\delta_{\\gamma}(x,y)=\\mathbb{E}_{p_{\\theta}(z_{0}|x,y)}[\\nabla_{\\gamma}\\log p_{\\gamma}(y|U_{\\alpha}(z_{0}))].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Estimating these expectations requires MCMC sampling of the posterior distribution $p_{\\theta}(z_{0}|x,y)$ . We use Langevin dynamics (Neal, 2011). For a target distribution $\\pi(z)$ , the dynamics iterates as follows, ", "page_idx": 3}, {"type": "equation", "text": "$$\nz^{\\tau+1}=z^{\\tau}+s\\nabla_{z}\\log\\pi(z^{\\tau})+\\sqrt{2s}\\epsilon^{\\tau},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\tau$ indexes the time step, $s$ is step size, and $\\epsilon_{\\tau}\\,\\sim\\,{\\mathcal{N}}(0,I_{d})$ is the Gaussian white noise. Here, $\\pi(z)$ is instantiated by the posterior distribution $p_{\\theta}(z_{0}|x,y)$ . With $z\\,=\\,U_{\\alpha}(z_{0})$ , we have $p(z_{0}|x,\\dot{y})\\stackrel{}{\\propto}p_{0}(z_{0})p_{\\beta}(x|z)\\dot{p_{\\gamma}}(y|z)$ . The gradient is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla_{z_{0}}\\log p_{\\theta}(z_{0}|x,y)=\\nabla_{z_{0}}\\underbrace{\\log p_{0}(z_{0})}_{\\mathrm{prior}}+\\nabla_{z_{0}}\\underbrace{\\sum_{t=1}^{T}\\log p_{\\beta}(x^{(t)}|x^{(<t)},z)}_{\\mathrm{atoregresive\\;molecule\\;generation}}+\\nabla_{z_{0}}\\underbrace{\\log p_{\\gamma}(y|z)}_{\\mathrm{property\\;prediction}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We initialize $z_{0}^{\\tau=0}\\sim\\mathcal{N}(0,I_{d})$ , and employ $N$ steps of Langevin dynamics (e.g. $N=15$ ) for approximate sampling from the posterior distribution, rendering our learning algorithm as an approximate MLE. See Pang et al. (2020); Nijkamp et al. (2020); Xie et al. (2023) for a theoretical understanding of the learning algorithm based on the finite-step MCMC. ", "page_idx": 3}, {"type": "text", "text": "Pretrain LPT In practical applications involving multiple molecular generation tasks, each characterized by a different target property $y$ , each model $p_{\\theta}(x,y)$ may require separate training. To enhance efficiency, we adopt a pretaining strategy focusing solely on molecule sequences. In this case, we aim to maximize $\\begin{array}{r}{\\dot{\\sum}_{i=1}^{n^{\\mathrm{~\\.~}}}\\log p_{\\theta}(\\check{x}_{i})=\\check{\\sum}_{i=1}^{n}\\log\\check{\\int}p_{\\theta}(x_{i},z_{i})d z_{i}}\\end{array}$ . The learning gradient is $\\nabla_{\\theta}\\log p_{\\theta}(x)=\\mathbb{E}_{p_{\\theta}(z_{0}\\mid x)}\\left[\\overleftarrow{\\nabla_{\\beta}}\\log p_{\\theta}\\overleftarrow{(x\\vert z}=U_{\\alpha}(\\overline{{z_{0}}})\\right)\\right]$ . After pretraining LPT, we finetune the model with target properties using Eq. (5) for a small number of epochs. This two-stage approach is adaptable for semi-supervised scenarios where property values are limited. ", "page_idx": 3}, {"type": "text", "text": "3.3 Property Conditioned Generation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "After MLE learning from a collected (offline) dataset, LPT is capable of generating molecules with desired properties. Since the latent prompt is designed as the information bottleneck, we can generate the molecule given property value as $\\begin{array}{r}{p(\\dot{x}|y)=\\int\\displaylimits_{}^{}p(z_{0}|y)p(x|z_{0})d z_{0}}\\end{array}$ . We first infer the latent prompt via posterior sampling using Bayes\u2019 rule, ", "page_idx": 3}, {"type": "equation", "text": "$$\nz_{0}\\sim p_{\\theta}(z_{0}|y)\\propto p_{0}(z_{0})p_{\\gamma}(y|z=U_{\\alpha}(z_{0})).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This posterior sampling is performed using Langevin dynamics similar to the training process. Specifically, we replace the target distribution in Eq. (6) with $p_{\\theta}(z_{0}|y)$ and run MCMC for a fixed number of steps, i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{z_{0}^{\\tau+1}=z_{0}^{\\tau}+s\\nabla_{z_{0}}\\log p(z_{0}|y)+\\sqrt{2s}\\epsilon^{\\tau},}}\\\\ {{\\nabla_{z_{0}}\\log p(z_{0}|y)=\\nabla_{z_{0}}(\\log p(z_{0})+\\log p(y|z_{0}))=-z_{0}+\\displaystyle\\frac{1}{\\sigma^{2}}(y-s_{\\gamma}(z))\\nabla_{z_{0}}s_{\\gamma}(z).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This process of sampling $p(z_{0}|y)$ iteratively refines the latent prompts $z$ , increasing their likelihood given the desired property or constraints. This optimization of molecules is achieved in the latent space by gradient-based sampling as a form of test-time computation. Once we generate the latent prompt $z$ , the molecule generation model uses this latent prompt to sample the next element $x^{(t)}\\sim$ $\\bar{p}_{\\beta}(x^{\\bar{(t)}}|x^{(<t)},z=U_{\\alpha}(\\bar{z_{0}}))$ until termination. ", "page_idx": 4}, {"type": "text", "text": "MLE learning of the joint model is equivalent to minimizing the $D_{\\mathrm{KL}}(p_{\\mathrm{data}}(x,y)\\|p_{\\theta}(x,y))$ . Note that Eq. (7) is reliable when condition $y$ is supported by $p_{\\mathrm{data}}(y)$ . However, in real-world molecule design, desired property values often lie far from those in the collected offilne dataset. For such cases, LPT should be applied in an online learning setting, as discussed in the next section. ", "page_idx": 4}, {"type": "text", "text": "3.4 Optimization via Online Learning with LPT ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "When the desired property value $y$ is not supported within the learned distribution $p_{\\theta}(z,x,y)$ , we propose an online learning approach to gradually shift the model distribution towards regions supporting desired properties. We assume access to the oracle functions $o(x)$ via software such as RDKit (Landrum et al.), AutoDock-GPU (Santos-Martins et al., 2021). ", "page_idx": 4}, {"type": "text", "text": "As discussed in Sec. 3.3, generated molecules are reliable when the desired property value is supported by the learned model. To leverage this, we propose an iterative distribution shifting method, as shown in Alg. 2, as a form of online learning for LPT. In each iteration of distribution shifting, we ", "page_idx": 4}, {"type": "text", "text": "(a) Sample molecules from the learned LPT, using incremental extrapolation to generate a synthetic dataset with improved desired properties.   \n(b) Use hindsight relabeling for the sampled molecules\u2019 properties based on oracle functions.   \n(c) Apply MLE learning of LPT based on this synthetic dataset of molecule-property pairs as described in Sec. 3.2. ", "page_idx": 4}, {"type": "text", "text": "This process is repeated until the model converges or budget limits are reached. This online learning method maintains a latent space generative model (LPT) and a synthetic dataset and gradually shifts both towards regions of desired targets. ", "page_idx": 4}, {"type": "text", "text": "In step (a), to account for small extrapolation, we use property-conditioned generation as mentioned in Sec. 3.3 by setting the desired property as $y=y^{*}+\\delta_{y}$ , where $y^{\\ast}$ lies on the boundary of the learned LPT in the previous shifting iteration (e.g. $y^{\\ast}$ can be the maximum value) and $\\delta_{y}$ is a small increment representing extrapolation. The hyper-parameter $\\delta_{y}$ quantifies the shifting speed. Specifically, since the latent prompts decouple the molecule generation and property prediction, we first sample the latent prompts $z_{0}\\sim p_{\\theta}(\\bar{z}_{0}|y=y^{*}+\\delta_{y})$ before the molecule generation and then use $x\\stackrel{\\bar{}}{\\sim}p_{\\beta}(x|z=U_{\\alpha}^{\\bar{}}(z_{0}))$ to generate the novel molecules. To sample molecules satisfying binary constraints, we use $z_{0}\\,\\sim\\,p_{\\theta}(z_{0}|y\\,=\\,1)$ . This latent space sampling scheme relieves the expensive autoregressive sampling in the data space, and allows efficient gradient-based sampling in low-dimensional latent space, such as Langevin dynamics. In step (b), after generating the novel molecules in step (a), we use the oracle function $o(x)$ to relabel them, obtaining a synthetic dataset of molecule-property pairs with ground-truth values. This synthetic dataset serves as a replay buffer, storing generated molecules along the optimization trajectory, and is used for MLE training of the LPT in step (c). The illustration of the online learning of LPT is depicted in Fig. 2. ", "page_idx": 4}, {"type": "text", "text": "Compared to population-based methods such as genetic algorithms (Nigam et al., 2020) and particleswarm algorithms (Winter et al., 2019), our method maintains both a synthetic dataset (which can be considered a small population) and a generative model to fti the dataset, enabling the improvement of generated molecules from the model. The model itself can be seen as an infinite population, as it can generate an unlimited number of new samples. ", "page_idx": 4}, {"type": "text", "text": "3.5 Efficiency Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In molecule design, computational efficiency and sample efficiency are crucial, with varying priorities depending on the objectives. Computational efficiency measures the ability to learn quickly with limited resources, while sample efficiency assesses the capacity to learn a good model with minimal oracle function interactions. Generative models are applied to molecular design for two purposes: obtaining starting points and optimizing them to meet specific objectives. For objectives accessible via existing software, computational efficiency is prioritized, aiming to train the model and query software as quickly as possible until convergence. For properties that are time-consuming and expensive to simulate, such as wet-lab experiments, sample efficiency is more important, minimizing the number of oracle queries for practical usage. In real-world scenarios, a combination of both may be necessary, depending on the specific requirements. We propose the following techniques for LPT to address efficiency issues accordingly. ", "page_idx": 4}, {"type": "image", "img_path": "dg3tI3c2B1/tmp/7ba23875d14f47817b3f05ee8146408699dde2228f2d9551fc26245afd1ff04f.jpg", "img_caption": ["Figure 2: Illustration of online learning LPT. For each shift iteration, we plot the densities of docking scores $E$ using AutoDock-GPU. The increase of the docking scores indicates better binding affinity. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Computational efficiency The computational overhead for online learning of LPT primarily arises from the iterative MCMC sampling procedure. In the posterior sampling stage of $p_{\\theta}(z_{0}|y)$ and $p_{\\theta}(z_{0}|x,y)$ in step (a) and (c), rather than using a Gaussian noise-initialized MCMC chain with a fixed number of MCMC steps $N=15$ ) for each learning iteration, we employ the Persistent Markov Chain (PMC) method (Tieleman, 2008; Xie et al., 2016; Han et al., 2017; Xie et al., 2019), which amortizes sampling across shifting iterations. Specifically, for the starting point $z_{0}^{\\tau=0}$ in Eq. (6) of each MCMC chain, its initialization is drawn from a Gaussian distribution at the first learning iteration, then subsequently from the previous iteration\u2019s sampling output. With this approach, the number of MCMC updates can be reduced to $N=2$ steps per sampling stage, achieving an approximate $5\\times$ speedup in posterior sampling. ", "page_idx": 5}, {"type": "text", "text": "Sample efficiency We aim to improve sample efficiency in both steps (a) and (c) in Sec. 3.4. In step (a), property-conditioned generation, we refine the exploration strategy during test-time computation. This approach demonstrates a clear advantage of posterior inference over direct use of VAE or GAN. By guiding the learned LPT towards exploitation during generation, we encourage the latent prompts $z$ to converge on the modes of the posterior distribution in Langevin dynamics. This concept is analogous to the intuition behind classifier guidance in conditional diffusion models (Dhariwal and Nichol, 2021; Ho and Salimans, 2022). In Eq. (8), we can adjust $\\sigma^{2}$ to balance the trade-off between exploitation and exploration: $\\begin{array}{r}{\\nabla_{z_{0}}\\log p(z_{0}|\\bar{y})=\\nabla_{z_{0}}(\\log p(\\bar{z}_{0})+\\log p(y|z_{0}))=-z_{0}+\\frac{1}{\\sigma^{2}}(y-s_{\\gamma}(z))\\bar{\\nabla}_{z_{0}}s_{\\gamma}(z)}\\end{array}$ . When $\\mathbf{\\bar{\\rho}}^{1}\\!/\\!\\sigma^{2}\\,=\\,1$ , the sampled latent prompts $z$ represent the density of the posterior distribution, resulting in an efficient exploration scheme. As $1/\\sigma^{2}$ increases, the sampled posterior $z$ concentrates around the modes of the posterior distribution, indicating increased confidence and a stronger bias towards exploitation. In step (c), we leverage the synthetic dataset to train the LPT on the most informative samples in terms of property values. By training the LPT on a distribution that assigns higher probability mass to high-value points and lower mass to low-value points, the training objective encourages a larger fraction of the feasible region\u2019s volume to model high-value points while simultaneously using other data points to learn useful representations and avoid overfitting. We modify the standard objective $\\textstyle\\sum_{i=1}^{n}{\\mathbf{1}}/{n}\\log p_{\\theta}(x_{i},y_{i})$ by assigning an importance weight $w_{i}$ to each molecule-property pair $\\left({x_{i},y_{i}}\\right)$ in the synthetic dataset, resulting in a biased objective function, $\\begin{array}{r}{\\sum_{i=1}^{n}w_{i}\\log p_{\\theta}^{\\bar{\\ }}(x_{i}^{\\bar{}},y_{i})}\\end{array}$ , where $\\textstyle\\sum_{i}w_{i}\\,=\\,1$ . In our experiments, we set $w_{i}\\,=\\,1/N$ if $y_{i}$ is in the top- $N$ property scores, and $w_{i}=0$ otherwise. This approach is inspired by prioritized experience replay (Schaul et al., 2015) in online reinforcement learning and weighted retraining (Tripp et al., 2020) in black-box optimization, both of which prioritize learning from the most informative samples to improve sample efficiency. ", "page_idx": 5}, {"type": "text", "text": "4 Related Work ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Latent space optimization Latent space optimization has been widely applied in generative models and high-dimensional data manipulation (Jin et al., 2018; Kusner et al., 2017; Kajino, 2019; Dai et al., 2018). This method involves representing data in a lower-dimensional space while retaining its essential characteristics. Latent space optimization improves the fidelity and quality of generated data and facilitates more efficient and effective exploration of the latent space. This leads to better generalization and robustness in various applications, such as image synthesis (Karras et al., 2019; Razavi et al., 2019; Song et al., 2020; Dhariwal and Nichol, 2021; Rombach et al., 2022), data compression (Ball\u00e9 et al., 2016; Mentzer et al., 2018), molecular optimization (Kong et al., 2023; Jain et al., 2023; Zhu et al., 2024), and automatic machine learning (Liu et al., 2018; Zhang et al., 2019). When the search space is significantly simplified in the latent space, familiar Bayesian optimization (BO) tools can be readily applied (Maus et al., 2022; Tripp et al., 2020). Unlike BO-based methods, our LPT is based on the explicit probabilistic form of $p(z|y)$ , which allows us to perform optimization as conditional generation using Bayes\u2019 rule. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Generative molecule design This research follows two main approaches. The first uses latent space generative models to translate discrete molecule graphs into continuous latent vectors, enabling optimization of molecular properties within the latent space (G\u00f3mez-Bombarelli et al., 2018; Kusner et al., 2017; Jin et al., 2018; Maziarz et al., 2021; Eckmann et al., 2022; Kong et al., 2023). The second directly employs combinatorial optimization methods, such as reinforcement learning, to fine-tune molecular attributes within the graph data space (You et al., 2018; De Cao and Kipf, 2018; Zhou et al., 2019; Shi et al., 2020; Luo et al., 2021; Du et al., 2022). Alternative data space methodologies, like genetic algorithms (Nigam et al., 2020), particle-swarm strategies (Winter et al., 2019), Monte Carlo tree search (Yang et al., 2020), and scaffolding trees (Fu et al., 2021), have also gained traction. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We demonstrate the effectiveness of our approach across a wide range of optimization tasks. In the context of molecule design, this includes binding affinity maximization, constrained optimization, and multi-objective optimization (see Sec. 5.2). Additionally, we optimize protein sequences for high fluorescence and DNA sequences for enhanced binding affinity (see Sec. 5.3). Finally, we validate the sample efficiency of LPT by performing optimization with a limited number of oracle function queries (see Sec. 5.4). Additional experiments, including robustness to noisy oracles, online learning from scratch, ablation studies and a detailed discussion of related works and baselines, are provided in Apps. A.2 and A.4. ", "page_idx": 6}, {"type": "text", "text": "5.1 Overview ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Molecule Sequence Design For molecule design tasks, we use SELFIES representations of the ZINC (Irwin et al., 2012) dataset, which comprises 250K drug-like molecules as our offline dataset $\\mathcal{D}$ . We utilize RDKit (Landrum et al.) to compute several key metrics, including penalized logP, drug-likeness (QED), and the synthetic accessibility score (SA). Additionally, we use AutoDockGPU (Santos-Martins et al., 2021) to derive docking scores $E$ , which serve as proxies for estimating the binding affinity of compounds to three protein targets: the human estrogen receptor (ESR1), human peroxisomal acetyl-CoA acyltransferase 1 (ACAA1), and Phosphoglycerate dehydrogenase (PHGDH). The binding affinity is expressed as the dissociation constant $K_{D}(\\mathrm{nM})$ , which is approximated by the formula $K_{D}\\approx e^{-E/c}$ , where $E$ is the docking score and $c$ is a constant. A lower $K_{D}$ value indicates stronger binding affinity. ESR1 is a well-characterized protein with numerous known binders, making it a suitable reference point for evaluating molecules generated by our model. In contrast, ACAA1 has no known binders, providing an opportunity to test the model\u2019s capability for de novo design (Eckmann et al., 2022). Additionally, we propose to design molecules that bind to PHGDH, an enzyme pivotal in the early stages of L-serine synthesis. Recently, PHGDH has gained attention as a potential therapeutic target in cancer treatment due to to its involvement in various human cancers (Zhao et al., 2020). The crystal structure of PHGDH (PDB: 2G76) is well-established, and there exists a comprehensive case study on the development of PHGDH inhibitors, showcasing a structure-based progression from simpler to more complex molecules targeting its NAD binding site (Spillier and Fr\u00e9d\u00e9rick, 2021), as illustrated in Fig. 3. Furthermore, we observe that the $K_{D}$ values estimated from wet lab experiments align closely with the trends predicted by AutoDock-GPU. This congruence makes PHGDH-NAD an excellent case study for testing LPT on single-objective, structure-constrained, and multi-objective optimization tasks using AutoDock-GPU. More details can be found in App. A.5. ", "page_idx": 6}, {"type": "text", "text": "Protein and DNA Sequence Design We further apply our method to biological sequence design via two tasks in Design-Bench (Trabucco et al., 2022): TF Bind 8 and GFP. To be specific, the TF Bind 8 task focuses on identifying DNA sequences that are 8 bases long, aiming for maximum binding affinity. This task contains a training set of 32,898 samples and includes an exact oracle function. The GFP task involves generating protein sequences of 237 amino acids that exhibit high fluorescence. ", "page_idx": 6}, {"type": "text", "text": "C1 C2 C3 HO SPR Kd = 1.6 \u03bcM SPR Kd = 0.18 \u03bcM Autodock $\\mathrm{Kd}=36.1~\\upmu\\mathrm{M}$ Autodock Kd = 2.5 \u03bcM Autodock Kd = 0.97 \u03bcM ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "For this task, we use a subset of 5,000 samples as the training set by following the methodology outlined in Trabucco et al. (2022). Due to the unavailability of an exact oracle function for the GFP task, we follow the same oracle function preparation as in Design-Bench, and train a Transformer regression model on the full dataset with a total of 56,086 samples as the oracle function. We evaluate the design performance and diversity of the generated samples. ", "page_idx": 7}, {"type": "text", "text": "Training Setup The prior model, $p_{\\alpha}(z)$ , of LPT is a one-dimensional UNet (Ronneberger et al., 2015) where $z$ contains 4 tokens, each of size 256. The sequence generation model, $p_{\\beta}(x|z)$ , is implemented as a 3-layer causal Transformer, while a 3-layer MLP serves as the predictor model, $p_{\\gamma}\\bar{(}y|z)$ . As described in Sec. 3.2, we pre-train LPT on molecules for 30 epochs and then fine-tune it with target properties for an additional 10 epochs, following the procedure outlined in Alg. 1 in App. A.3. We perform up to 25 iterations of online learning, generating 2,500 samples per iteration, which totals a maximum of 62.5K oracle function queries. We use the AdamW optimizer (Loshchilov and Hutter, 2019; Kingma and Ba, 2014) with a weight decay of 0.1. Training was conducted on an NVIDIA A6000 GPU, requiring 20 hours for pre-training, 10 hours for fine-tuning, and 12 hours for online learning. Additional details can be found in App. A.3. ", "page_idx": 7}, {"type": "text", "text": "5.2 Binding Affinity Maximization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Single-Objective Optimization For the single-objective binding affinity optimization task, we aim to design ligands with optimal binding affinities to ESR1, ACAA1, and PHGDH as de novo design, without any constraints. LPT does not use any prior knowledge of existing binders, and exclusively uses the crystal structures of the aforementioned proteins. The predictor model $p_{\\gamma}(y|z)$ for this task is a regression model that estimates docking scores. We compare our model with several baseline methods, which are introduced in App. A.4. As shown in Tabs. 1 and 2, our model significantly surpasses other methods across all three binding affinity maximization tasks in terms of $K_{D}$ , often achieving substantial improvement. Lower $K_{D}$ values indicate better performance. Furthermore, in Tab. 2, we report the average performance of the top 50 and top 100 molecules to demonstrate that our model can effectively generate a diverse pool of candidate molecules with the desired properties. Visualizations of generated molecules are provided in App. A.6.1. ", "page_idx": 7}, {"type": "text", "text": "Table 1: Single-objective binding affinity optimization results for ESR1 and ACAA1. Top 3 performance in terms of $K_{D}(\\mathrm{nM})$ achieved by each model are reported. The best scores are in bold. ", "page_idx": 7}, {"type": "table", "img_path": "dg3tI3c2B1/tmp/fa272cc8afa6a57664a6a148d324c098785b98fa948b235d693a064cf663264b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 2: Single-objective binding affinity maximization results for PHGDH, reporting the 1st, 2nd, and 3rd performance, along with average performance of top 50 and top 100 molecules for each model. Performance is measured by $K_{D}(10^{-2}\\mathrm{nM})$ . The highest scores are in bold. ", "page_idx": 7}, {"type": "table", "img_path": "dg3tI3c2B1/tmp/134772e46b11ad57ce63168ef185ff2b5b6d8c1cf4c4619ecd3b9b58377544b3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Structure-constrained Optimization The structure-constrained optimization task mimics lead optimization in drug discovery, aiming to decorate a fixed core substructure to optimize activity and pharmacological properties. Our model\u2019s factorization, $p(z,x,y)=p(z)p(x|z)p(y|z)$ , enables the decoupling of molecule generation and property prediction, simplifying conditional generation. ", "page_idx": 7}, {"type": "image", "img_path": "dg3tI3c2B1/tmp/1e311f166a0ea8f25a361380c88e3694cc5dab9efc4a0d117b286676c7538478.jpg", "img_caption": ["Figure 4: (a) Structure-constrained Optimization. Conditionally generated compounds C2 and C3 closely resemble the human-designed compounds C2 and C3 shown in Fig. 3. Additionally, the right column also presents further optimized compounds that achieve improved $K_{D}$ scores. (b) Illustration of generated molecules binding to PHGDH with docking poses generated by AutoDock-GPU. The left panel visualizes the molecule generated through multi-objective optimization, while the right panel displays the molecule generated via structure-constrained optimization. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Given a substructure ${\\hat{x}}=(x^{(1)},\\ldots,x^{(k)})$ , we aim to sample from $p_{\\theta}(x,y|\\hat{x})$ . This is accomplished by sampling $z\\sim p(z|y)$ and then $x\\sim p(x|\\hat{x},z)$ , which only requires rearranging $\\hat{x}$ \u2019s sequence to start from the desired atom. In Fig. 3, Compound 2 (C2) is designed by humans as an extension of Compound 1 (C1), and Compound 3 (C3) is similarly an extension of C2. In Fig. 4, we show that: (1) given C1 or C2, our model is able to design compounds similar to the human-designed C2 or C3, and (2) our model can identify molecules that outperform those designed by humans. Meanwhile, we confirm that C2, C3, and LPT-generated molecules are novel compared to the ZINC training set, with an average Tanimoto similarity less than 0.5. More generated molecules are shown in App. A.6.2. ", "page_idx": 8}, {"type": "text", "text": "Our model explores high-affinity PHGDH inhibitors by adding functional groups to an indole backbone, a common scaffold for such inhibitors (Spillier and Fr\u00e9d\u00e9rick, 2021). The model identifies aromatic or heteroaromatic groups, such as benzene or pyridine, at the second position as frequently occurring and exhibiting higher binding scores compared to the indole backbone itself. This finding aligns with reported data: a published molecule with a benzothiophene backbone similar to C1 exhibits a binding affinity of $470\\;\\mu\\mathrm{M}$ , while C2, which includes an aromatic group at the second position, shows a significantly improved binding affinity of $1.6\\;\\mu\\mathrm{M}$ (Spillier and Fr\u00e9d\u00e9rick, 2021; Fuller et al., 2016). Furthermore, the model introduces a second functional group at the sixth position of the indole in C2, generating inhibitors closely resembling the structure of C3. The observed trend of increasing binding affinities $1.6\\,\\mu\\mathrm{M}$ for C2 and $0.18~\\mu\\mathrm{M}$ for C3) aligns with the literature values, providing validation for our proposed method in identifying potential high-affinity inhibitors. ", "page_idx": 8}, {"type": "text", "text": "Multi-Objective Optimization For multi-objective optimization tasks, we aim to simultaneously maximize binding affinity and QED, while minimizing SA. These objectives are balanced as a weighted combination, with constraints of $\\mathrm{QED}>0.4$ and $\\mathrm{SA}<5.5$ . We evaluate our method on three protein targets: ESR1, ACAA1, and PHGDH, comparing the results against two baseline methods, LIMO (Eckmann et al., 2022) and SGDS (Kong et al., 2023). As shown in Tab. 3, our method, LPT, achieves QED and SA scores comparable to those of SGDS while significantly improving binding affinity across all three protein targets, demonstrating its superior modeling capability. Examples of the generated molecules are provided in App. A.6.1. ", "page_idx": 8}, {"type": "text", "text": "Table 3: Multi-objective optimization Results. Top 2 performance, measured by $K_{D}(\\mathrm{nM})$ , QED and SA, are reported for each method. Baseline methods include LIMO (Eckmann et al., 2022) and SGDS (Kong et al., 2023). Best results are marked in bold, and the second best results are underlined. ", "page_idx": 8}, {"type": "table", "img_path": "dg3tI3c2B1/tmp/05e410bccb9069f8da1bdb8696fae9d2dbc6689c29dd660455bed2037f1fdcad.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.3 Biological Sequence Design ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our model excels in biological sequence design, a single-objective optimization application, as demonstrated by two benchmarks in Design-Bench (Trabucco et al., 2022): TF Bind 8 and GFP. Tab. 4 shows that LPT significantly outperforms other methods in these tasks. In the TF Bind 8 task, our approach surpasses the strong competitor GFlowNet-AL (Jain et al., 2022), while maintaining comparable diversity. For the GFP task, we achieve superior performance with reasonable diversity. ", "page_idx": 9}, {"type": "table", "img_path": "dg3tI3c2B1/tmp/07ea2068964d3600aeddf196725634ba26c1abefe658abfd16e56dcb9bdcf1e7.jpg", "table_caption": ["Table 4: Results of biological sequence design on TF Bind 8 and GFP benchmarks. Performance and diversity are evaluated on 128 samples. Results of other baselines are obtained from Jain et al. (2022). Bold highlighting indicates top scores. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5.4 Sample Efficiency ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We validate LPT\u2019s sample efficiency on the Practical Molecular Optimization (PMO) benchmark (Gao et al., 2022), where multi-property objectives (MPO) are optimized within a limited oracle budget of 10K queries. Tab. 5 shows that our method, LPT, surpasses previous approaches, such as MARS (Xie et al., 2021), GFlowNet (Jain et al., 2022) and SMILES/SELFIES-VAE (G\u00f3mez-Bombarelli et al., 2018; Maus et al., 2022) with Bayesian Optimization (BO). Also, LPT achieves comparable performance to LSTM HC (Brown et al., 2019), the best generative molecule design method in PMO, and demonstrates performance on par with GP-BO (Tripp et al., 2021), the best BO-based method in PMO, under the limited budge of oracle function queries. We acknowledge that there remains a performance gap between generative model-based optimization and methods like genetic algorithms when working with a small budget. This is primarily due to the data-intensive requirements of training generative models. To ensure a fair assessment, our comparison focuses on representative generative molecule design methods within PMO. It\u2019s worth noting that generative models offer distinct advantages when maintaining relatively large budgets, as the learned model itself can be viewed as infinite populations for further exploration. ", "page_idx": 9}, {"type": "table", "img_path": "dg3tI3c2B1/tmp/a0a17dd441873ea0c5dbc617f6d4d5bc37d9b0f5f70d80cedb4938f553eb8195.jpg", "table_caption": ["Table 5: Comparison of sample efficiency on the PMO benchmark. The mean and standard deviation of AUC Top-10 from 5 independent runs are reported. Best results are marked in bold. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Limitation and Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we presented LPT, a novel generative model for molecule design that achieves strong performance through its offilne and online learning algorithms. Contemporary work has extended the similar model to offilne reinforcement learning (Kong et al., 2024). While the model demonstrates significant potential, there are opportunities to better understand how LPT handles the inherent trade-offs in multi-objective optimization scenarios, particularly in characterizing the Pareto front nature of optimal solutions. Future work could also explore alternative architectures to extend LPT\u2019s applicability beyond sequence-based optimization problems in science and engineering. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors would like to thank the anonymous reviewers for providing constructive comments and suggestions to improve the work. The work was partially supported by NSF DMS-2015577, NSF DMS-2415226, a gift fund from Amazon, and XSEDE grant CIS210052. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Christof Angermueller, David Dohan, David Belanger, Ramya Deshpande, Kevin Murphy, and Lucy Colwell. Model-based reinforcement learning for biological sequence design. In International conference on learning representations (ICLR), 2019.   \nJohannes Ball\u00e9, Valero Laparra, and Eero P Simoncelli. End-to-end optimized image compression. arXiv preprint arXiv:1611.01704, 2016.   \nDavid Brookes, Hahnbeom Park, and Jennifer Listgarten. Conditioning by adaptive sampling for robust design. In International conference on machine learning (ICML), pages 773\u2013782, 2019.   \nNathan Brown, Marco Fiscato, Marwin HS Segler, and Alain C Vaucher. Guacamol: benchmarking models for de novo molecular design. Journal of Chemical Information and Modeling, 59(3): 1096\u20131108, 2019.   \nAustin H Cheng, Andy Cai, Santiago Miret, Gustavo Malkomes, Mariano Phielipp, and Al\u00e1n AspuruGuzik. Group selfies: a robust fragment-based molecular string representation. Digital Discovery, 2023.   \nHanjun Dai, Yingtao Tian, Bo Dai, Steven Skiena, and Le Song. Syntax-directed variational autoencoder for structured data. In International Conference on Learning Representations (ICLR), 2018.   \nNicola De Cao and Thomas Kipf. Molgan: An implicit generative model for small molecular graphs. arXiv preprint arXiv:1805.11973, 2018.   \nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In Advances in Neural Information Processing Systems (NeurIPS), volume 34, pages 8780\u20138794, 2021.   \nYuanqi Du, Tianfan Fu, Jimeng Sun, and Shengchao Liu. Molgensurvey: A systematic survey in machine learning models for molecule design. arXiv preprint arXiv:2203.14500, 2022.   \nPeter Eckmann, Kunyang Sun, Bo Zhao, Mudong Feng, Michael K Gilson, and Rose Yu. Limo: Latent inceptionism for targeted molecule generation. In International Conference on Machine Learning (ICML), 2022.   \nClara Fannjiang and Jennifer Listgarten. Autofocused oracles for model-based design. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pages 12945\u201312956, 2020.   \nTianfan Fu, Wenhao Gao, Cao Xiao, Jacob Yasonik, Connor W Coley, and Jimeng Sun. Differentiable scaffolding tree for molecular optimization. arXiv preprint arXiv:2109.10469, 2021.   \nNathan Fuller, Loredana Spadola, Scott Cowen, Joe Patel, Heike Sch\u00f6nherr, Qing Cao, Andrew McKenzie, Fredrik Edfeldt, Al Rabow, and Robert Goodnow. An improved model for fragmentbased lead generation at astrazeneca. Drug Discovery Today, 21(8):1272\u20131283, 2016.   \nWenhao Gao, Tianfan Fu, Jimeng Sun, and Connor Coley. Sample efficiency matters: a benchmark for practical molecular optimization. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 21342\u201321357, 2022.   \nRafael G\u00f3mez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Benjam\u00edn S\u00e1nchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Al\u00e1n Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS Central Science, 4(2):268\u2013276, 2018.   \nTian Han, Yang Lu, Song-Chun Zhu, and Ying Nian Wu. Alternating back-propagation for generator network. In AAAI Conference on Artificial Intelligence (AAAI), pages 1976\u20131984, 2017. ", "page_idx": 10}, {"type": "text", "text": "Nikolaus Hansen. The cma evolution strategy: a comparing review. Towards a new evolutionary computation: Advances in the estimation of distribution algorithms, pages 75\u2013102, 2006. ", "page_idx": 11}, {"type": "text", "text": "Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. ", "page_idx": 11}, {"type": "text", "text": "John J Irwin, Teague Sterling, Michael M Mysinger, Erin S Bolstad, and Ryan G Coleman. Zinc: a free tool to discover chemistry for biology. Journal of Chemical Information and Modeling, 52(7): 1757\u20131768, 2012. ", "page_idx": 11}, {"type": "text", "text": "Moksh Jain, Emmanuel Bengio, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Bonaventure FP Dossou, Chanakya Ajit Ekbote, Jie Fu, Tianyu Zhang, Michael Kilgour, Dinghuai Zhang, et al. Biological sequence design with gflownets. In International Conference on Machine Learning (ICML), pages 9786\u20139801. PMLR, 2022. ", "page_idx": 11}, {"type": "text", "text": "Moksh Jain, Sharath Chandra Raparthy, Alex Hern\u00e1ndez-Garc\u0131a, Jarrid Rector-Brooks, Yoshua Bengio, Santiago Miret, and Emmanuel Bengio. Multi-objective gflownets. In International Conference on Machine Learning (ICML), pages 14631\u201314653, 2023. ", "page_idx": 11}, {"type": "text", "text": "Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph generation. In International Conference on Machine Learning (ICML), pages 2323\u20132332, 2018. ", "page_idx": 11}, {"type": "text", "text": "Hiroshi Kajino. Molecular hypergraph grammar with its application to molecular optimization. In International Conference on Machine Learning (ICML), pages 3183\u20133191, 2019. ", "page_idx": 11}, {"type": "text", "text": "Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4401\u20134410, 2019. ", "page_idx": 11}, {"type": "text", "text": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. ", "page_idx": 11}, {"type": "text", "text": "Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In International Conference on Learning Representations (ICLR), 2014. ", "page_idx": 11}, {"type": "text", "text": "Deqian Kong, Bo Pang, Tian Han, and Ying Nian Wu. Molecule design by latent space energy-based modeling and gradual distribution shifting. In Conference on Uncertainty in Artificial Intelligence (UAI), volume 216, pages 1109\u20131120, 2023. ", "page_idx": 11}, {"type": "text", "text": "Deqian Kong, Dehong Xu, Minglu Zhao, Bo Pang, Jianwen Xie, Andrew Lizarraga, Yuhao Huang, Sirui Xie, and Ying Nian Wu. Latent plan transformer for trajectory abstraction: Planning as latent space inference. In Advances in Neural Information Processing Systems (NeurIPS), 2024. ", "page_idx": 11}, {"type": "text", "text": "Mario Krenn, Florian H\u00e4se, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. Selfreferencing embedded strings (selfies): A $100\\%$ robust molecular string representation. Machine Learning: Science and Technology, 1(4):045024, 2020. ", "page_idx": 11}, {"type": "text", "text": "Matt J Kusner, Brooks Paige, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Grammar variational autoencoder. In International Conference on Machine Learning (ICML), pages 1945\u20131954, 2017. ", "page_idx": 11}, {"type": "text", "text": "Greg Landrum et al. RDKit: Open-source cheminformatics. URL https://www.rdkit.org. ", "page_idx": 11}, {"type": "text", "text": "Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018. ", "page_idx": 11}, {"type": "text", "text": "Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations (ICLR), 2019. ", "page_idx": 11}, {"type": "text", "text": "Youzhi Luo, Keqiang Yan, and Shuiwang Ji. Graphdf: A discrete flow model for molecular graph generation. In International Conference on Machine Learning (ICML), pages 7192\u20137203, 2021. ", "page_idx": 11}, {"type": "text", "text": "Natalie Maus, Haydn Jones, Juston Moore, Matt J Kusner, John Bradshaw, and Jacob Gardner. Local latent space bayesian optimization over structured inputs. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 34505\u201334518, 2022. ", "page_idx": 11}, {"type": "text", "text": "Krzysztof Maziarz, Henry Jackson-Flux, Pashmina Cameron, Finton Sirockin, Nadine Schneider, Nikolaus Stief,l Marwin Segler, and Marc Brockschmidt. Learning to extend molecular scaffolds with structural motifs. arXiv preprint arXiv:2103.03864, 2021.   \nFabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte, and Luc Van Gool. Conditional probability models for deep image compression. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4394\u20134402, 2018.   \nEdouard Mullarky, Jiayi Xu, Anita D Robin, David J Huggins, Andy Jennings, Naoyoshi Noguchi, Andrea Olland, Damodharan Lakshminarasimhan, Michael Miller, Daisuke Tomita, et al. Inhibition of 3-phosphoglycerate dehydrogenase (phgdh) by indole amides abrogates de novo serine synthesis in cancer cells. Bioorganic & Medicinal Chemistry Letters, 29(17):2503\u20132510, 2019.   \nRadford M Neal. MCMC using hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, 2, 2011.   \nAkshatKumar Nigam, Pascal Friederich, Mario Krenn, and Al\u00e1n Aspuru-Guzik. Augmenting genetic algorithms with deep neural networks for exploring the chemical space. In International Conference on Learning Representations (ICLR), 2020.   \nErik Nijkamp, Bo Pang, Tian Han, Linqi Zhou, Song-Chun Zhu, and Ying Nian Wu. Learning multi-layer latent variable model via variational optimization of short run mcmc for approximate inference. In European Conference on Computer Vision (ECCV), pages 361\u2013378, 2020.   \nBo Pang, Tian Han, Erik Nijkamp, Song-Chun Zhu, and Ying Nian Wu. Learning latent space energy-based prior model. In Advances in Neural Information Processing Systems (NeurIPS), 2020.   \nAli Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. In Advances in Neural Information Processing Systems (NeurIPS), volume 32, 2019.   \nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 10684\u201310695, 2022.   \nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical Image Computing and ComputerAssisted Intervention (MICCAI), pages 234\u2013241, 2015.   \nDebangshu Samanta and Gregg L Semenza. Serine synthesis helps hypoxic cancer stem cells regulate redox. Cancer Research, 76(22):6458\u20136462, 2016.   \nDiogo Santos-Martins, Leonardo Solis-Vasquez, Andreas F Tillack, Michel F Sanner, Andreas Koch, and Stefano Forli. Accelerating AutoDock4 with GPUs and gradient-based local search. Journal of Chemical Theory and Computation, 17(2):1060\u20131073, 2021.   \nTom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015.   \nChence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. Graphaf: a flow-based autoregressive model for molecular graph generation. arXiv preprint arXiv:2001.09382, 2020.   \nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.   \nQuentin Spillier and Rapha\u00ebl Fr\u00e9d\u00e9rick. Phosphoglycerate dehydrogenase (phgdh) inhibitors: A comprehensive review 2015\u20132020. Expert Opinion on Therapeutic Patents, 31(7):597\u2013608, 2021.   \nKevin Swersky, Yulia Rubanova, David Dohan, and Kevin Murphy. Amortized bayesian optimization over discrete spaces. In Conference on Uncertainty in Artificial Intelligence (UAI), pages 769\u2013778, 2020.   \nTijmen Tieleman. Training restricted boltzmann machines using approximations to the likelihood gradient. In International Conference on Machine Learning (ICML), pages 1064\u20131071, 2008.   \nBrandon Trabucco, Aviral Kumar, Xinyang Geng, and Sergey Levine. Conservative objective models for effective offilne model-based optimization. In International Conference on Machine Learning (ICML), pages 10358\u201310368, 2021.   \nBrandon Trabucco, Xinyang Geng, Aviral Kumar, and Sergey Levine. Design-bench: Benchmarks for data-driven offline model-based optimization. In International Conference on Machine Learning (ICML), pages 21658\u201321676, 2022.   \nAustin Tripp, Erik Daxberger, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Sample-efficient optimization in the latent space of deep generative models via weighted retraining. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pages 11259\u201311272, 2020.   \nAustin Tripp, Gregor NC Simm, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. A fresh look at de novo molecular design benchmarks. In NeurIPS 2021 AI for Science Workshop, 2021.   \nJudith E Unterlass, Robert J Wood, Arnaud Basl\u00e9, Julie Tucker, C\u00e9line Cano, Martin ME Noble, and Nicola J Curtin. Structural insights into the enzymatic activity and potential substrate promiscuity of human 3-phosphoglycerate dehydrogenase (phgdh). Oncotarget, 8(61):104478, 2017.   \nDavid Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of Chemical Information and Computer Sciences, 28(1):31\u201336, 1988.   \nJames T Wilson, Riccardo Moriconi, Frank Hutter, and Marc Peter Deisenroth. The reparameterization trick for acquisition functions. arXiv preprint arXiv:1712.00424, 2017.   \nRobin Winter, Floriane Montanari, Andreas Steffen, Hans Briem, Frank No\u00e9, and Djork-Arn\u00e9 Clevert. Efficient multi-objective molecular optimization in a continuous latent space. Chemical Science, 10(34):8016\u20138024, 2019.   \nJianwen Xie, Yang Lu, Song-Chun Zhu, and Ying Nian Wu. A theory of generative convnet. In International Conference on Machine Learning (ICML), pages 2635\u20132644, 2016.   \nJianwen Xie, Ruiqi Gao, Zilong Zheng, Song-Chun Zhu, and Ying Nian Wu. Learning dynamic generator model by alternating back-propagation through time. In AAAI Conference on Artificial Intelligence, (AAAI), pages 5498\u20135507, 2019.   \nJianwen Xie, Yaxuan Zhu, Yifei Xu, Dingcheng Li, and Ping Li. A tale of two latent flows: Learning latent space normalizing flow with short-run langevin flow for approximate inference. In AAAI Conference on Artificial Intelligence (AAAI), 2023.   \nYutong Xie, Chence Shi, Hao Zhou, Yuwei Yang, Weinan Zhang, Yong Yu, and Lei Li. Mars: Markov molecular sampling for multi-objective drug discovery. In International Conference on Learning Representations (ICLR), 2021.   \nXiufeng Yang, Tanuj Kr Aasawat, and Kazuki Yoshizoe. Practical massively parallel monte-carlo tree search applied to molecular design. arXiv preprint arXiv:2006.10504, 2020.   \nJiaxuan You, Bowen Liu, Zhitao Ying, Vijay Pande, and Jure Leskovec. Graph convolutional policy network for goal-directed molecular graph generation. In Advances in Neural Information Processing Systems (NeurIPS), pages 6410\u20136421, 2018.   \nMuhan Zhang, Shali Jiang, Zhicheng Cui, Roman Garnett, and Yixin Chen. D-vae: A variational autoencoder for directed acyclic graphs. In Advances in Neural Information Processing Systems (NeurIPS), pages 1588\u20131600, 2019.   \nXiaoya Zhao, Jianfei Fu, Jinlin Du, and Wenxia Xu. The role of d-3-phosphoglycerate dehydrogenase in cancer. International Journal of Biological Sciences, 16(9):1495, 2020.   \nZhenpeng Zhou, Steven Kearnes, Li Li, Richard N Zare, and Patrick Riley. Optimization of molecules via deep reinforcement learning. Scientific Reports, 9(1):1\u201310, 2019.   \nYiheng Zhu, Jialu Wu, Chaowen Hu, Jiahuan Yan, Tingjun Hou, Jian Wu, et al. Sample-efficient multiobjective molecular optimization with gflownets. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, 2024. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Model Learning ", "text_level": 1, "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}\\log p_{\\theta}(x,y)=\\frac{\\nabla_{\\theta}p_{\\theta}(x,y)}{p_{\\theta}(x,y)}}\\\\ &{\\quad\\quad\\quad\\quad=\\frac{1}{p_{\\theta}(x,y)}\\int\\nabla_{\\theta}p_{\\theta}(x,y,z=U_{\\alpha}(z_{0}))d z_{0}}\\\\ &{\\quad\\quad\\quad=\\int\\frac{p_{\\theta}(x,y,z=U_{\\alpha}(z_{0}))}{p_{\\theta}(x,y)}\\nabla_{\\theta}\\log p_{\\theta}(x,y,z=U_{\\alpha}(z_{0}))d z_{0}}\\\\ &{\\quad\\quad\\quad=\\int p_{\\theta}(z_{0}|x,y)\\nabla_{\\theta}\\log p_{\\theta}(x,y,z=U_{\\alpha}(z_{0}))d z_{0}}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}_{p_{\\theta}(z_{0}|x,y)}\\left[\\nabla_{\\theta}\\log p_{\\theta}(x,y,z=U_{\\alpha}(z_{0}))\\right]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}_{p_{\\theta}(z_{0}|x,y)}\\left[\\nabla_{\\theta}\\log p_{\\theta}(x|U_{\\alpha}(z_{0}))+\\nabla_{\\theta}\\log p_{\\gamma}(y|U_{\\alpha}(z_{0}))+\\nabla_{\\theta}\\log p_{0}(z_{0})\\right]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}_{p_{\\theta}(z_{0}|x,y)}\\left[\\nabla_{\\theta}\\log p_{\\beta}(x|U_{\\alpha}(z_{0}))+\\nabla_{\\theta}\\log p_{\\gamma}(y|U_{\\alpha}(z_{0}))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.2 Additional Experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.2.1 Model Sanity Check: Penalized logP and QED Maximization ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The experiments focus on optimizing the Penalized logP (P-logP) and QED properties, both of which can be calculated using RDKit. Since P-logP scores are positively correlated with the length of a molecule, we maximize P-logP while limiting molecule length to the maximum length of molecules in ZINC using the SELFIES (Krenn et al., 2020; Cheng et al., 2023) representation, following Eckmann et al. (2022). We compare our model with several baseline methods, including JT-VAE (Jin et al., 2018), MolDQN (Zhou et al., 2019), LIMO (Eckmann et al., 2022), GCPN (You et al., 2018), GraphDF (You et al., 2018), MARS (Xie et al., 2021), SGDS (Kong et al., 2023). Tab. 6 presents the results, demonstrating that LPT outperforms other methods and achieves the highest QED score among methods that limits molecule length. ", "page_idx": 14}, {"type": "table", "img_path": "dg3tI3c2B1/tmp/317e82eefdca5c1c6410ab9ad47e6f8321ccf01f6f2f5d9c8acdf8d0ec6d37e6.jpg", "table_caption": ["Table 6: Results of P-logP and QED maximization. The top 3 highest scores achieved by each model are reported. \u201cLength Limit\u201d indicates the application of a maximum molecule length limit. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.2.2 Robustness to Noisy Oracles ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To evaluate the robustness of our model in single-objective QED optimization tasks under the oracle query budget of 25K, we conduct experiments with varying levels of oracle noise. We define noised oracles as $y_{\\mathrm{noise}}=y_{\\mathrm{true}}+e$ , where $\\bar{e^{\\star}}\\!\\sim\\!\\mathcal N(0,\\sigma^{2})$ and $\\sigma$ varies as a percentage of the property range. The minimal degradation in performance seen in Tab. 7 demonstrates that our model is resilient to the noised oracles. ", "page_idx": 14}, {"type": "text", "text": "A.2.3 Online Learning from Scratch ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We investigate LPT\u2019s performance without offilne pre-training data, relying solely on online learning with a budget limit of 300K (comparable to the size of the ZINC dataset plus our previous online learning budget). Results in Tab. 8 show that LPT can effectively discover high-binding molecules for ESR1 and ACAA1, even without pre-training. However, performance on PHGDH remained suboptimal compared to the version with pre-training, indicating that this target may require additional oracle queries due to its inherent complexity. These findings highlight the potential of pure online learning approaches for future exploration. ", "page_idx": 14}, {"type": "table", "img_path": "dg3tI3c2B1/tmp/f300a096e041df420e9597b12d8c5334f60bf0e099e5d6721f1d2a82781963bd.jpg", "table_caption": ["Table 7: Performance across different Oracle noise levels "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Table 8: Results of online learning from scratch. Top 3 scores of $K_{D}(\\mathrm{nM})$ are reported. ", "page_idx": 15}, {"type": "table", "img_path": "dg3tI3c2B1/tmp/165d6a6fd25125e5727957b2c41714f81f8dc33ca314b916dd7d24cac51c10f1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.2.4 Ablation Studies ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We conduct ablation studies to investigate the contributions of key components in LPT using a challenging PHGDH single-objective optimization task. The variations in our experiments included: ", "page_idx": 15}, {"type": "text", "text": "1. Using samples from $z\\sim p_{\\alpha}(z)$ instead of $p_{\\theta}(z|y)$ to generate the proposals $\\mathcal{P}^{t}$ .   \n2. Removing weighted retraining and applying the standard objective $\\textstyle\\sum_{i=1}^{n}{\\mathrm{1/}}{\\log{p_{\\theta}(x_{i},y_{i})}}$   \n3. Setting the total number of shifting iterations as 1.   \n4. Replacing the Unet prior with a Gaussian $\\mathcal{N}(0,I_{d})$ . ", "page_idx": 15}, {"type": "text", "text": "As shown in Tab. 9, each component was essential, and removing or altering any of them might lead to performance degradation, underscoring their importance in achieving the model\u2019s high efficacy. ", "page_idx": 15}, {"type": "table", "img_path": "dg3tI3c2B1/tmp/1739feae98f31747017de7d80bb9cf95455096937b4af828067b2ad45a8771c8.jpg", "table_caption": ["Table 9: Ablation of Key Components. We report the mean and standard deviation of $K_{D}(\\mathrm{nM})$ over the top 100 unique molecules generated in the last shifting iteration. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "In addition, we investigate the effect of the exploitation scheme $1/\\sigma^{2}$ in Eq. (8). Experiments are conducted on the practical molecular optimization (PMO) benchmark, focusing on optimizing the multi-property objective (MPO) for amlodipine under a 10K-query oracle budget. As shown in Tab. 10, increasing $1/\\sigma^{2}$ enhances LPT\u2019s performance, indicating a stronger bias toward exploiting the sampled posterior $z$ and leading to improved optimization efficiency. ", "page_idx": 15}, {"type": "text", "text": "Furthermore, we study the effect of oracle budget size on performance, conducting experiments on the single-objective ESR1 binding affinity optimization task. As shown in Tab. 11, LPT demonstrates robust performance even under a limited oracle query budget of $10\\mathbf{k}$ , outperforming most existing methods. As the budget increases, LPT continues to exhibit significant performance enhancements, further highlighting its efficiency and scalability. ", "page_idx": 15}, {"type": "text", "text": "Table 10: Effects of the exploration schemes. We report the AUC Top-10 for the multi-property objective (MPO) on amlodipine in the PMO benchmark. ", "page_idx": 16}, {"type": "table", "img_path": "dg3tI3c2B1/tmp/18ccce6193bd04d6a1fa4277946aed00031e7f554dbc751036d0976f3bcf0ca6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 11: Single-objective ESR1 binding affinity $K_{D}(\\mathrm{nM})$ optimization with different budgets. ", "page_idx": 16}, {"type": "table", "img_path": "dg3tI3c2B1/tmp/dae7cd1bf501adff75eeca26cb4f7f965207374f4a4e087b8611d70739fdbdc4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.3 Model Architecture and Training Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As illustrated in Fig. 1, the prior model in LPT is parameterized by a 1D Unet, with $z$ sequence length of 4, where each of them is size of 256. The molecule generation model, $p_{\\beta}(x|z)$ , employs a 3-layer causal Transformer with an embedding size of 256 and maximum input token length of 73. The predictor model is a 3-layer MLP, which takes $z$ as input and outputs predicted property values or classification results. The total number of parameters for LPT is 4.33M. ", "page_idx": 16}, {"type": "text", "text": "LPT is trained in a two-step process. Initially, it undergoes pre-training solely on molecules for 30 epochs, using cross-entropy loss with a learning rate that varies between $7.5e{-4}$ and $7.5e{-5}$ , following a cosine scheduling approach. Subsequently, LPT is fine-tuned for 10 epochs on both molecules and their properties, as outlined in Alg. 1, with the learning rate adjusted between $3e\\!-\\!4$ and $7.5e{-5}$ . For multi-objective optimization, i.e., simultaneously maximizing binding affinity, QED, and minimizing SA, the predictors for binding affinity and QED/SA are selected as a regression model and a classifier, which are supervised by mean squared error (MSE) and binary cross-entropy (BCE) loss functions, respectively. ", "page_idx": 16}, {"type": "text", "text": "Algorithm 1 MLE learning of Latent Prompt Transformer (LPT) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Input: Number of learning iterations $T$ , initial parameters $\\theta_{0}\\,=\\,(\\alpha_{0},\\beta_{0},\\gamma_{0})$ , observed sam  \nples $D\\,=\\,\\{x_{i},y_{i}\\}_{i=1}^{n}$ , posterior sampling step size $s$ , the number of MCMC steps $N$ , and the   \nlearning rate \u03b70, \u03b71, \u03b72.   \nOutput: $\\theta_{T}$   \nfor $t=1$ to $T$ do 1.Posterior sampling: For each $(x_{i},y_{i})$ , sample $z_{0}\\sim p_{\\theta_{t}}(z_{0}|x_{i},y_{i})$ using Eq. (6), where the target distribution $\\pi$ is $p_{\\theta_{t}}(z_{0}|x_{i},y_{i})$ with $N$ steps and step size $s$ . 2.Learn prior model $p_{\\alpha}(z)$ , generation model $p_{\\beta}(x|z)$ and predictor model $p_{\\gamma}(y|z)$ : $\\begin{array}{r}{\\alpha_{t+1}=\\alpha_{t}+\\eta_{0}\\frac{1}{n}\\sum_{i}\\delta_{\\alpha}(x_{i},y_{i});}\\end{array}$ $\\begin{array}{r}{\\beta_{t+1}=\\beta_{t}+\\eta_{1}\\frac{1}{n}\\sum_{i}\\delta_{\\beta}(x_{i},y_{i});}\\end{array}$ ; $\\begin{array}{r}{\\gamma_{t+1}=\\gamma_{t}+\\eta_{2}\\frac{1}{n}\\sum_{i}\\delta_{\\gamma}(x_{i},y_{i})}\\end{array}$ as in Sec. 3.2.   \nend for ", "page_idx": 16}, {"type": "text", "text": "For LPT\u2019s online learning, as detailed in Alg. 2, we establish a maximum number of shifting iterations to be 25, generating 2,500 new samples in each iteration. This results in a total of up to $62.5\\mathbf{k}$ oracle function queries. Throughout the training processes, we utilize the AdamW optimizer (Loshchilov and Hutter, 2019) with a weight decay of 0.1. The pre-training, fine-tuning, and online learning phases of LPT require approximately 20, 10, and 12 hours, respectively, on a single NVIDIA A6000 GPU. ", "page_idx": 16}, {"type": "text", "text": "Input: Number of proposals $m$ , fixed increment $\\delta_{y}$ , number of PMC steps $N$ , initial parameters $\\theta_{t}=(\\alpha_{t},\\beta_{t},\\gamma_{t})$ , initial dataset $\\mathcal{D}^{0}=\\{x_{i}^{0},y_{i}^{0}\\}_{i=1}^{\\bar{n}}$ , oracle function $o(x)$ , maximum number of shifting iterations $T$ . ", "page_idx": 17}, {"type": "text", "text": "Output: $\\bar{\\theta}^{T}$ , $\\mathcal{D}^{T}$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "while $t<T$ do ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "step (a): Sample molecules and properties from the learned model $\\{z_{i}^{t},x_{i}^{t},y_{i}^{t}\\}_{i=1}^{m}$ , where $z^{t}\\stackrel{*}{\\sim}p_{\\theta}(z|y=\\stackrel{\\cdot}{y}^{t-1}+\\delta_{y})$ ; $x^{t}\\sim\\bar{p_{\\beta}}(\\bar{x|}z=z^{t})$ .   \nstep (b): Relabel the proposal property values by oracle function query: $y_{i}^{t}\\gets o(x_{i}^{t})$ , and update the dataset by $\\mathcal{D}^{t}=\\dot{\\{z_{i}^{t},x_{i}^{t},y_{i}^{t}\\}}_{i=1}^{\\dot{m}}\\stackrel{.}{\\cup}\\mathcal{D}^{t-1}$ .   \nstep (c): Update LPT using maximum likelihood on synthetic dataset $\\mathcal{D}^{t}$ by following Alg. 1 ", "page_idx": 17}, {"type": "text", "text": "A.4 Baselines ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Our model is based on Kong et al. (2023). The differences are as follows. (1) While Kong et al. (2023) used an LSTM model for molecule generation, we adopt a more expressive causal Transformer model for generation, with the latent vector serving as latent prompt. (2) While Kong et al. (2023) used a latent space energy-based model for the prior distribution of the latent vector, we assume that the latent $z$ is generated by a UNet transformation of a Gaussian white noise vector. This approach allows us to eliminate the need for Langevin dynamics in prior sampling during training, thus simplifying the learning algorithm. (3) Our experimental results are significantly stronger, surpassing those of Kong et al. (2023) and achieving new state-of-the-art performance. ", "page_idx": 17}, {"type": "text", "text": "For molecule generation, JT-VAE (Jin et al., 2018) utilizes a variational autoencoder (VAE). GCPN (You et al., 2018) and GraphDF (Luo et al., 2021) employ the deep graph model to discover novel molecules. A reinforcement learning framework that fuses chemical domain knowledge with double Q-learning and randomized value functions for molecule optimization was presented by MolDQN (Zhou et al., 2019). MARS (Xie et al., 2021) develops a Markov molecular Sampling framework targeting multi-objective drug discovery, while LIMO (Eckmann et al., 2022) uses a VAE-generated latent space along with neural networks for property prediction, enabling efficient gradient-based optimization of molecular properties. ", "page_idx": 17}, {"type": "text", "text": "In contrast to existing latent space generative models (G\u00f3mez-Bombarelli et al., 2018; Kusner et al., 2017; Jin et al., 2018; Eckmann et al., 2022), our approach incorporates a learnable prior model, enabling our model to effectively catch up with the evolving dataset in the optimization process. ", "page_idx": 17}, {"type": "text", "text": "The landscape of biological sequence design is shaped by a diverse range of computational strategies. DyNAPPO (Angermueller et al., 2019) harnesses active learning with reinforcement learning to facilitate iterative sequence generation. GFlowNet-AL (Jain et al., 2022) capitalizes on GFlowNets for generative active learning within sequence contexts. Model-based optimization techniques like COMs (Trabucco et al., 2021) and AmortizedBO (Swersky et al., 2020) integrate Bayesian Optimization with reinforcement learning, enhancing search efficiency. BO-QEI (Wilson et al., 2017), a variant of Bayesian optimization, refines the search process using quantile Expected Improvement. Deep generative models, e.g., CBAs (Fannjiang and Listgarten, 2020) and MINs (Brookes et al., 2019), leverage deep learning for complex pattern discovery. Meanwhile, CMA-ES (Hansen, 2006) excels in high-dimensional optimization, adapting search strategies over generations. ", "page_idx": 17}, {"type": "text", "text": "A.5 Background of PHGDH and its NAD binding site ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Phosphoglycerate dehydrogenase (PHGDH) is an enzyme that plays a crucial role in the early stages of L-serine synthesis. Recently, PHGDH has been identified as an attractive therapeutic target in cancer therapy due to its involvement in various human cancers, including breast cancer, melanoma, lung cancer, pancreatic cancer, and kidney cancer. Several studies have been devoted to the exploration of small molecule inhibitors targeting PHGDH including CBR-5884 $\\operatorname{IC}50=33{\\pm}12$ $(\\mu\\mathrm{M})$ ), NCT-503 $\\operatorname{IC50}=2.5{\\pm}0.6\\;(\\mu\\mathrm{M})$ ), AZ PHGDH inhibitor $\\mathrm{IC}50=180\\$ (nM)), RAZE PHGDH inhibitor $(\\mathrm{IC50}=0.01\\sim1.5\\;(\\mu\\mathrm{M}))$ , PKUMDL-WQ-2201 ${\\mathrm{IC}}50=35.7$ $(\\mu\\mathrm{M})$ ), BI-4924 $\\mathrm{IC}50=2$ (nM)), and others. Additionally, the crystal structure of PHGDH (PDB: 2G76) has been elucidated. ", "page_idx": 17}, {"type": "text", "text": "This makes PHGDH an excellent protein model for conducting docking calculations, aiding in the precise localization of binding sites in order to optimize our algorithms. ", "page_idx": 18}, {"type": "text", "text": "PHGDH utilizes nicotinamide adenine dinucleotide (oxidized form, $\\mathrm{NAD+}$ ; reduced form, NADH) as a co-factor for enzymatic activity, producing NADH during the synthesis of 3- phosphohydroxypyruvate (3PHP) from 3-phosphoglycerate (3PG) (Samanta and Semenza, 2016). ", "page_idx": 18}, {"type": "text", "text": "The co-crystallization of PHGDH with NAD has been documented (PDB: 5N6C). The $\\mathrm{NAD+}$ pocket is surrounded by hydrophobic residues of P176, Y174, L151, L193, L216, T213, T207 and L210 (Mullarky et al., 2019). Specifically, nicotinamide moiety exhibited interactions with the protein backbone (specifically A285 and C233) as well as the side chain of D259. Additionally, the hydroxyl groups of the sugar moieties were also involved in hydrogen bonds with both the protein backbone (T206) and the side chain of D174. The phosphate linker demonstrated interactions with the main chain of R154 and I155, along with the side chain of R154 (Fig. 5) (Unterlass et al., 2017). ", "page_idx": 18}, {"type": "image", "img_path": "dg3tI3c2B1/tmp/40f5b86d2ee99ac524f78f907c23ea648ce464a5d3710722d80d4411212799e3.jpg", "img_caption": ["Figure 5: PHGDH with NAD binding site. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "A.6 Visualization of Generated Molecules ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We present visualizations of a subset of generated molecules from both single-objective and multiobjective optimization for ESR1, ACAA1, and PHGDH. Additionally, several conditionally generated compounds, C2 and C3, which have similar characteristics to human-designed ones, are also presented in Fig. 12 and Fig. 13. ", "page_idx": 18}, {"type": "text", "text": "A.6.1 Multi-Objective and Single-Objective Optimization ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Figs. 6 to 8 visualize some molecules produced during the multi-objective optimization for ESR1, ACAA1, and PHGDH, respectively. Meanwhile, Figs. 9 to 11 depict some examples of molecules produced during the single-objective optimization for ESR1, ACAA1, and PHGDH, respectively. ", "page_idx": 18}, {"type": "image", "img_path": "dg3tI3c2B1/tmp/00ee836dedae0d57ebe0759e317c2672cfbe80b997b6cb4e7826d8ca246b62fe.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 6: Molecules produced during the multi-objective optimization for ESR1. The legends denote $\\bar{K_{D}}(\\mathrm{nM})\\downarrow,\\mathrm{SA}\\downarrow$ and $\\mathrm{QED}\\uparrow$ . ", "page_idx": 19}, {"type": "image", "img_path": "dg3tI3c2B1/tmp/ae40776bf4ad6c9143fe4e000bfaae5f5cd6faaf7dbfb80f45cf3a6356141d06.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 7: Molecules produced during the multi-objective optimization for ACAA1. The legends denote $K_{D}(\\mathrm{nM})\\downarrow,\\mathrm{SA}\\downarrow$ and $\\mathrm{QED}\\uparrow$ . ", "page_idx": 19}, {"type": "image", "img_path": "dg3tI3c2B1/tmp/f22fc1f2c9b296baa763b7fdee11679abd73ee25646c08a2076ffcfdfa086d2a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 8: Molecules produced during the multi-objective optimization for PHGDH. The legends denote $K_{D}(\\mathrm{nM})\\downarrow,\\mathrm{S\\bar{A}\\downarrow}$ and $\\mathrm{QED}\\uparrow$ . ", "page_idx": 20}, {"type": "image", "img_path": "dg3tI3c2B1/tmp/c7dbcb52d45387566786dbe9f1109761f82f5caa96869c01af4ed8c280c8ef81.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 9: Molecules produced during the single-objective optimization for ESR1. The legends denote $\\bar{\\mathrm{K_{D}}}(n M)\\downarrow$ . ", "page_idx": 20}, {"type": "image", "img_path": "dg3tI3c2B1/tmp/cb191c7ff9aad87f9d5d9a21530e5944dd2238b9a4ef57507db10025bc5e0eea.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 10: Molecules produced during the single-objective optimization for ACAA1. The legends denote $\\mathrm{K}_{\\mathrm{D}}(n M)\\downarrow$ . ", "page_idx": 21}, {"type": "image", "img_path": "dg3tI3c2B1/tmp/7881c57056314247ce468a479136c9b203a24947b97a93018fce6a2dcaf62407.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 11: Molecules produced during the single-objective optimization for PHGDH. The legends denote $\\mathrm{K}_{\\mathrm{D}}(n M)\\downarrow$ . ", "page_idx": 21}, {"type": "image", "img_path": "dg3tI3c2B1/tmp/08c9dcd85ef2a54bbe7ee92e0141020c9dcc68a33107d1e107448b2c91492eff.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 12: Molecules produced during the structure-constrained optimization from C1 to C2 for PHGDH. The legends denote $K_{D}(\\mu\\mathrm{M})\\downarrow,\\mathrm{SA}\\downarrow$ and $\\mathrm{QED}\\uparrow$ . ", "page_idx": 22}, {"type": "image", "img_path": "dg3tI3c2B1/tmp/dd057b75351ca514a00493be21571b27f5127610bed8c60e70deeda14239df3e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 13: Molecules produced during the structure-constrained optimization from C2 to C3 for PHGDH. The legends denote $K_{D}(\\mu\\mathrm{M})\\downarrow,\\mathrm{SA}\\downarrow$ and $\\mathrm{QED}\\uparrow$ . ", "page_idx": 22}, {"type": "text", "text": "A.7 Broader Impact ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We introduce a novel generative model for jointly modeling molecule sequences and their target properties, which potentially leads to more capable and efficient molecule design algorithm. One potential negative impact could come from the misuse of the generative modeling in designing molecules or biological sequences for harmful purposes. Therefore, developing suitable safeguards and regulations will be crucial to mitigate potential negative impacts. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See Sec. 3 and Appendix. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: See Sec. 6. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See Sec. 3.2. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: See App. A.3. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Code is released in our project page. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: See App. A.3 and Sec. 5.1. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: See Sec. 5 and App. A.2. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: See App. A.3. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: None. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: See App. A.7. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 26}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: None. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: None. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "page_idx": 28}, {"type": "text", "text": "Justification: None. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [No] ", "page_idx": 28}, {"type": "text", "text": "Justification: None. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [No] Justification: None. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]