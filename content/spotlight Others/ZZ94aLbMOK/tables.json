[{"figure_path": "ZZ94aLbMOK/tables/tables_5_1.jpg", "caption": "Table 1: Test accuracies of CordsNets obtained using our initialization method and after fine-tuning, compared to their equivalent feedforward CNN counterparts. For controls, we trained CordsNets directly for the same amount of time taken by the initialization method (C). Fully-connected RNNs were also trained with matched parameter counts (R).", "description": "This table presents the test accuracies achieved by CordsNets (trained using the proposed initialization method and after fine-tuning) on various datasets (MNIST, F-MNIST, CIFAR-10, CIFAR-100, ImageNet).  The results are compared against those of equivalent feedforward CNNs, as well as control groups using CordsNets trained directly (for the same amount of time as the initialization method) and fully-connected RNNs (with matched parameter counts). The table shows how the initialization method improves performance compared to directly training the model and that performance is comparable to CNNs after fine-tuning.", "section": "3 Training and results"}, {"figure_path": "ZZ94aLbMOK/tables/tables_16_1.jpg", "caption": "Table S1: Number of trainable parameters in the recurrent weights of different network architectures across three different sizes. To match parameter counts, we vary the kernel size of CordsNets, weight matrix rank of low-rank RNNs and weight matrix sparsity of sparsely-connected RNNs.", "description": "This table shows the number of trainable parameters in the recurrent weight matrices of different recurrent neural network architectures used in the paper's experiments.  Three different network sizes (125, 216, and 512 neurons) are compared. To keep the total number of parameters roughly consistent across the models, the kernel size of CordsNets, the rank of low-rank RNNs, and the sparsity of sparse RNNs were adjusted accordingly.  This allows for a fair comparison of the different architectures' performance in the subsequent cognitive tasks.", "section": "B.3 Comparison with other architectures"}, {"figure_path": "ZZ94aLbMOK/tables/tables_17_1.jpg", "caption": "Table S2: Mean distance compared to fully-connected RNN of each architecture-task pair.", "description": "This table presents the mean Procrustes distances between the neural trajectories of different recurrent neural network architectures (Sparse RNN, CordsNet, Low-Rank RNN) and a fully-connected RNN across five cognitive tasks.  Lower distances indicate greater similarity in the dynamical solutions found by the different architectures.", "section": "B.3 Comparison with other architectures"}, {"figure_path": "ZZ94aLbMOK/tables/tables_19_1.jpg", "caption": "Table S3: CordsNet architectures for image classification. In each block, the top convolution represents a feedforward transformation, while the bottom convolution represents the recurrent weights in the recurrent dynamical system.", "description": "This table details the architecture of four different CordsNet models (CordsNet-R2, CordsNet-R4, CordsNet-R6, and CordsNet-R8) used for image classification. Each model consists of multiple blocks, each containing a convolutional layer (feedforward transformation) followed by a convolutional recurrent layer (recurrent weights).  The table shows the output size of each block and the kernel sizes and number of channels for the convolutional and recurrent layers within each block. The final layer of each model consists of an average pooling layer, a linear layer, and a softmax layer for classification.", "section": "C Model architecture"}, {"figure_path": "ZZ94aLbMOK/tables/tables_19_2.jpg", "caption": "Table 1: Test accuracies of CordsNets obtained using our initialization method and after fine-tuning, compared to their equivalent feedforward CNN counterparts. For controls, we trained CordsNets directly for the same amount of time taken by the initialization method (C). Fully-connected RNNs were also trained with matched parameter counts (R).", "description": "This table presents the test accuracies achieved by CordsNets using the proposed initialization method and after fine-tuning.  It compares these results against their equivalent feedforward CNN counterparts. Control experiments involved training CordsNets directly (without the initialization method) and training fully-connected RNNs, both matched to the same training time as the initialization method. Results are shown for various datasets (MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, and ImageNet).", "section": "3 Training and results"}, {"figure_path": "ZZ94aLbMOK/tables/tables_20_1.jpg", "caption": "Table 1: Test accuracies of CordsNets obtained using our initialization method and after fine-tuning, compared to their equivalent feedforward CNN counterparts. For controls, we trained CordsNets directly for the same amount of time taken by the initialization method (C). Fully-connected RNNs were also trained with matched parameter counts (R).", "description": "This table compares the test accuracies of CordsNets (trained using the proposed initialization method and fine-tuned) against their corresponding feedforward CNN counterparts across various datasets (MNIST, F-MNIST, CIFAR-10, CIFAR-100, and ImageNet).  It also includes control experiments where CordsNets were trained directly without initialization and fully-connected RNNs were trained with matching parameter counts, providing a comprehensive comparison of the models' performance.", "section": "Training and results"}, {"figure_path": "ZZ94aLbMOK/tables/tables_22_1.jpg", "caption": "Table S6: Ablation studies for the logscale range used to weigh the cross-entropy loss terms across time (top) and the coefficient of the spontaneous penalty term (bottom).", "description": "This ablation study investigates the impact of different ranges for the logarithmic scaling of cross-entropy loss across time steps and different coefficients for the spontaneous activity penalty term on the types of solutions obtained during training.  The results indicate the number of models that converged to a steady-state solution versus a transient solution for varying logarithmic ranges and the number of models that converged to a mono-stable solution versus other solutions for varying spontaneous penalty coefficients.  The goal was to identify parameter settings that reliably produce networks with mono-stable, consistent behavior.", "section": "D Loss function ablation study"}]