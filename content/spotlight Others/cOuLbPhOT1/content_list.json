[{"type": "text", "text": "PACE: marrying generalization in PArameter-efficient fine-tuning with Consistency rEgularization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yao Ni\u2020 Shan Zhang\u2021,\u2020 Piotr Koniusz\u2217,\u00a7,\u2020 ", "page_idx": 0}, {"type": "text", "text": "\u2020The Australian National University \u00a7Data61 CSIRO \u2021Australian Institute for Machine Learning, University of Adelaide \u2020yao.ni@anu.edu.au \u2021shan.zhang@adelaide.edu.au piotr.koniusz@data61.csiro.au ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Parameter-Efficient Fine-Tuning (PEFT) effectively adapts pre-trained transformers to downstream tasks. However, the optimization for tasks performance often comes at the cost of generalizability in fine-tuned models. To address this issue, we theoretically connect smaller weight gradient norms during training and larger datasets to the improved model generalization. Motivated by this connection, we propose reducing gradient norms for enhanced generalization and aligning finetuned model with the pre-trained counterpart to retain knowledge from large-scale pre-training data. Yet, naive alignment does not guarantee gradient reduction and can potentially cause gradient explosion, complicating efforts to manage gradients. To address such issues, we propose PACE, marrying generalization of PArameterefficient fine-tuning with Consistency rEgularization. We perturb features learned from the adapter with the multiplicative noise and ensure the fine-tuned model remains consistent for same sample under different perturbations. Theoretical analysis shows that PACE not only implicitly regularizes gradients for enhanced generalization, but also implicitly aligns the fine-tuned and pre-trained models to retain knowledge. Experimental evidence supports our theories. PACE surpasses existing PEFT methods in visual adaptation tasks (VTAB-1k, FGVC, few-shot learning, domain adaptation) and showing potential for resource-efficient finetuning. It also improves LoRA in text classification (GLUE) and mathematical reasoning (GSM-8K). Code will be available at MaxwellYaoNi/PACE. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformers [67], with the self-attention mechanism [3] capturing long-range dependencies in data, have been successful in various deep learning tasks, including image classification (ViT [15]), multimodal learning (CLIP [54]), image synthesis (StableDiffusion [56]), semantic segmentation (SAM [32]) and text generation (LLaMA [64]). The success of transformers can be largely attributed to the availability of abundant data, such as ImageNet [11] and Laion5B [59], which has enabled researchers to scale up these models by training them with an enormous number of parameters. ", "page_idx": 0}, {"type": "text", "text": "Such huge models, with knowledge from large-scale pre-training [62], have become foundation models that can be easily adapted to various downstream tasks through full fine-tuning or linear probing [19], eliminating the need for task-specific model design [8]. However, full fine-tuning is storage-intensive and infeasible for maintaining separate model weights as the number of tasks grows, while linear probing, which only trains the last head layer, yields inferior adaptation performance. ", "page_idx": 0}, {"type": "text", "text": "To overcome these limitations, Parameter-Efficient Fine-Tuning (PEFT) [23] fine-tunes only a small subset of parameters, thereby reducing storage requirements while surpassing the performance of full fine-tuning and linear probing. These advantages have popularized PEFT and inspired the development of various PEFT methods for deep learning tasks, which can be categorized into two groups: those increasing inference cost and cost-efficient ones. The first group introduces additional learning branches, such as non-linear adapters [24, 8], or concatenates learnable parameters with input tokens, e.g., visual prompts [27, 81, 51], increasing inference cost. The second group, focuses on cost-efficiency involving lower-rank adaptation in linear layers [7, 25], or affine transformations such as SSF [40] and RepAdapters [44], which can be reparameterized during inference for efficiency. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Despite the superiority and efficiency of PEFT, prioritizing optimization for downstream tasks compromises the generalizability of fine-tuned models, yielding suboptimal performance. Although some analyses have been conducted on PEFT [62, 26, 17, 71, 38], they fail to fully explain the generalization of PEFT, leading to ineffective strategies for improving generalization. ", "page_idx": 1}, {"type": "text", "text": "To address this gap in understanding generalization in PEFT, we establish a theoretical connection from generalization theory: smaller weight gradient norms and larger data volumes contribute to better generalization. Motivated by this, we propose reducing weight gradient norms and aligning output space of the fine-tuned model with the pre-trained one to retain knowledge captured from large pre-training data. Yet, theoretical analyses reveal this naive alignment dose not guarantee gradient regularization and can even cause gradient explosion, complicating efforts for gradient management. To address this issue, we propose perturbing features learned from the adapter with multiplicative noise and constraining the network output to be consistent across different perturbations. ", "page_idx": 1}, {"type": "text", "text": "We call our method PACE. It marries generalization of PArameter-efficient fine-tuning with Consistency rEgularization. The name reflects our goal of keeping the output behavior of the fine-tuned model in pace with the pre-trained one. Despite its simplicity, theoretical analysis confirms that PACE not only implicitly regularizes weight gradients for better generalization but also implicitly aligns the fine-tuned model with the pre-trained counterpart to retain knowledge from large-scale pre-training data. Experimental evidence supports our theories. PACE improves existing PEFT methods, achieving superior results across six adaptation benchmarks. Our key contributions are: ", "page_idx": 1}, {"type": "text", "text": "i. We establish a theory connecting smaller weight gradient norms and larger datasets with enhanced generalization, motivating gradient reduction and model alignment for fine-tuning.   \nii. We propose PACE, a simple yet effective method perturbing features from adapters with multiplicative noise and constraining output of fine-tuned model to be consistent across perturbations.   \niii. Our theoretical and empirical evidence confirms that PACE implicitly regularizes gradients and aligns the fine-tuned model with the pre-trained one. PACE excels on 4 visual adaptation tasks.   \niv. We provide novel theoretical explanations for how gradient penalization and consistency regularization benefti generalization, offering fundamental insights applicable across deep learning. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Parameter-Efficient Fine-Tuning (PEFT). LoRA [25] uses low-rank decomposition to reduce parameters and treats adapters as side paths. SSF [40] proposes affine transformations on latent features. FacT [29] decomposes and reassembles parameter matrices in ViT. Surgical fine-tuning [35] different network parts results in different performance for different datasets. FLoRA [73] aims at real-time global service. GLoRA [7] unifies cost-efficient PEFT methods. NOAH [81] uses parameter search on neural prompts. ARC [13] leverages cross-layer ViT similarity, parameter-sharing adapter and scaling factors for lower fine-tuning cost. RLRR [14] incorporates a residual term for flexibility while preserving pre-trained representation. RepAdapter [44] reparameterizes adapters for efficient inference. Res-tuning [28] unbinds tuners from the backbone for memory efficiency. Zhao et al. [82] show impressive fine-tuning results by tuning only the attention layer normalization. OFT [53] and BOFT [41] propose orthogonal fine-tuning to preserve hypersphere energy between neurons. ", "page_idx": 1}, {"type": "text", "text": "Consistency Regularization. Fixmatch [60] applies consistency regularization over augmented images for semi-supervised learning. Openmatch [58] utilizes it on outlier predictions for open-set semi-supervised learning. R-Drop [75] applies it to transformers [67] with dropout for NLP tasks. CR [78] applies it over augmented real and fake images for GAN training. CAGAN [49] enforces consistency on discriminators with dropout for GAN training. Despite the empirical success of consistency regularization demonstrated by previous works, theoretical analysis is lacking. While NICE [47] demonstrates that consistency regularization lowers latent feature gradients for stable GAN training, it fails to reveal reduced weight gradient for enhanced generalization. Our study goes beyond prior works by providing a theoretical link between smaller weight gradients and improved generalization, effectively marrying generalization of PEFT with consistency regularization. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Generalization of Fine-Tuning. Li et al. [37] constrain the fine-tuned model\u2019s closeness to the pre-trained model in weight space. Fu et al. [17] induce sparsity on PEFT methods for enhanced generalization. Wang et al. [71] finds PEFT methods improve generalization on fine-tuning graph neural network. Recent works, including VioLET [72], PromptSRC [30], CoPrompt [57], propose aligning the fine-tuned model with the pre-trained one for enhanced generalization or avoiding forgetting, which can be seen as our naive alignment. Additionally, L2SP [76], DELTA [39], and FTP [63] aim to retain pre-trained knowledge by aligning finetuned models with pre-trained ones, reducing distance in weight space, feature space and using projected gradient descent, respectively. However, they fail to provide a theoretical analysis for this alignment. Our study goes beyond understanding generalization of PEFT by discovering the benefits of gradient regularization and model alignment. We propose PACE to match both requirements, paving a comprehensive understanding for PEFT. ", "page_idx": 2}, {"type": "text", "text": "Gradient regularization. Previous studies have empirically shown that gradient regularization improves neural network performance [66, 83, 46, 48]. However, they failed to theoretically establish the connection between smaller gradient norms and better generalization [16, 80, 6]. Our work bridges this gap by establishing a fundamental theory between reduced gradient norms and improved generalization, providing a solid foundation for future research on enhancing generalization. ", "page_idx": 2}, {"type": "text", "text": "3 Approach ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We begin with a unified perspective on cost-efficient PEFT based on GLoRA [7], linking generalization with gradients and large-scale data and motivating the alignment of the fine-tuned model with the pre-trained model to leverage its knowledge. We identify limitations of naive alignment in gradient regularization and introduce PACE, which implicitly enhances gradient regularization and model alignment. We conclude with theoretical justification and efficient implementations. ", "page_idx": 2}, {"type": "text", "text": "3.1 A unified perspective on cost-efficient PEFT methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The transformer architectures [67, 15] have excelled in natural language processing and computer vision tasks through their powerful sequential modeling capabilities. This success stems from their ability to process text/image tokens through $L$ transformer blocks, where each block contains selfattention and MLP modules primarily composed of linear layers. These linear layers enable the self-attention mechanism to capture long-range dependencies, allowing transformers to achieve superior performance when scaled to huge parameters and trained on extensive datasets. ", "page_idx": 2}, {"type": "text", "text": "With massive parameters pretrained on large-scale data, transformers serve as foundation models that can be fine-tuned for downstream tasks using limited data. However, fully fine-tuning all parameters for various downstream tasks requires substantial memory and can lead the forgetting of pretrained knowledge. To alleviate this without increasing inference cost, adapters with lightweight parameters are often preferred for fine-tuning. Let $\\bar{h}_{0}(\\cdot)$ be a transformation within the pre-trained transformer. Current adapters can be unified as introducing a residual branch \u2206\u00afh to form a new transformation $\\bar{h}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\bar{h}(\\pmb{a})=\\bar{h}_{0}(\\pmb{a})+\\Delta\\bar{h}(\\pmb{a}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $\\textbf{\\em a}$ is the input and $\\bar{h}_{0}$ can represent MLP modules, as in Adapter [24] and AdaptFormer [8], or linear layers in self-attention and MLP modules, as in [25, 7, 12, 33]. In SSF [40], $\\bar{h}_{0}$ is the identity mapping and $\\Delta\\bar{h}(a)=a\\odot(\\gamma-1)+\\beta$ with $\\gamma$ and $\\beta$ as affine transformation parameters. ", "page_idx": 2}, {"type": "text", "text": "Given that linear layers are key components in transformer, tuning them offers a flexible and effective way to adapt models to downstream tasks. This work focuses on methods that tune the linear layer without increasing inference cost. Let $(W_{0},b_{0})$ , $(\\Delta W,\\Delta b)$ , and $(W,b)$ be the parameters of pretrained model, adapter and finetuned model, respectively, where $W_{0},\\Delta W$ , $W\\in\\mathbb{R}^{d_{\\mathrm{out}}\\times d_{\\mathrm{in}}}$ and $\\dot{\\pmb{b}}_{0},\\Delta\\pmb{b},\\pmb{b}\\in\\mathbb{R}^{d_{\\mathrm{out}}}$ , finetuning a linear layer in self-attention or MLP module can be formed as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}&{h(\\pmb{a})=W\\pmb{a}+b=(W_{0}+\\Delta W)\\pmb{a}+(b_{0}+\\Delta b)}\\\\ &{\\qquad=h_{0}(\\pmb{a})+\\Delta h(\\pmb{a})=(W_{0}\\pmb{a}+b_{0})+(\\Delta W\\pmb{a}+\\Delta b).}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Based on GLoRA [7], cost-efficient PEFT methods for linear layers vary in the form of $\\Delta W,\\Delta b$ : ", "page_idx": 2}, {"type": "text", "text": "$\\mathbf{LoRA_{add}}\\colon\\Delta W=W_{\\mathrm{d}}W_{\\mathrm{u}},\\Delta b=b_{\\mathrm{lora}}$ where $W_{\\mathrm{d}}\\in\\mathbb{R}^{d_{\\mathrm{out}}\\times r}$ , $W_{\\mathrm{u}}\\in\\mathbb{R}^{r\\times d_{\\mathrm{in}}}$ , and $r$ is the rank. ", "page_idx": 2}, {"type": "text", "text": "$\\mathbf{LoRA_{mul}}$ : $\\Delta W\\!=\\!W_{0}{\\odot}(W_{\\mathrm{d}}W_{\\mathrm{u}})$ , $\\Delta b\\!=\\!b_{0}\\odot b_{\\mathrm{lora}}$ , including RepAdapter [44] via reparameterization. ", "page_idx": 3}, {"type": "text", "text": "$\\mathbf{VPT_{add}}$ : $\\Delta W$ is zero, $\\Delta b=W_{0}P$ , with learnable $P\\in\\mathbb{R}^{d_{\\mathrm{in}}\\times1}$ as layer-wise visual prompt. We use $\\mathrm{VPT_{add}}$ to differentiate from VPT [27], which concatenates $_{P}$ with tokens, increasing inference cost. ", "page_idx": 3}, {"type": "text", "text": "3.2 Generalization of deep neural networks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Having established a unified perspective on cost-efficient PEFT, we now motivate our method from a perspective on improving generalization of neural networks to enhance performance on unseen data. Consider a network $f:=\\bar{\\phi}(g(x))$ with $l$ layers, where $g$ is feature extractor and $\\phi$ is the classification head. Let $\\pmb{\\theta}:=\\{(\\pmb{W}^{(i)},\\pmb{b}^{(i)})\\}_{i=1}^{l}$ be the parameter set with dimension $d$ and $\\mathbf{\\mathcal{D}}^{n}:=\\{(\\pmb{x}_{i},\\pmb{y}_{i})\\}_{i=1}^{n}$ be the training set of size $n$ drawn i.i.d. from distribution $\\mathcal{D}$ , which contains infinite data. The following lemma from [16] builds a relationship between the empirical and population loss. ", "page_idx": 3}, {"type": "text", "text": "Lemma 1 (Theorem 1 from [16]) Let $\\mathcal{L}_{\\mathcal{D}^{n}}(\\pmb{\\theta})$ be the empirical loss function over $f$ on training set $\\textstyle{\\mathcal{D}}^{n}$ and $\\mathcal{L}_{\\mathcal{D}}(\\pmb{\\theta})$ be the population loss. For any $\\rho>0$ , with high probability over $\\mathscr{D}^{n}\\sim\\mathscr{D}$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal L_{\\mathcal D}(\\pmb\\theta)\\leq\\operatorname*{max}_{\\|\\epsilon\\|_{2}\\leq\\rho}\\mathcal L_{\\mathcal D^{n}}(\\pmb\\theta+\\epsilon)+R\\Big(\\frac{\\|\\pmb\\theta\\|_{2}^{2}}{\\rho^{2}},\\frac{1}{n}\\Big),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $R:(\\mathbb{R}_{+},\\mathbb{R}_{+})\\rightarrow\\mathbb{R}_{+}$ is a increasing function (under some conditions on $\\mathcal{L}_{\\mathcal{D}}(\\pmb{\\theta})$ and $n$ ). ", "page_idx": 3}, {"type": "text", "text": "Lemma 1 bounds the population loss by the empirical loss with perturbed weights, indicating that minimal empirical loss increase from small weight perturbations implies low population loss. By observing that the maximum of $\\mathcal{L}_{\\mathcal{D}^{n}}$ is achieved at $\\begin{array}{r}{\\epsilon=\\frac{\\rho\\pmb{\\nabla}_{\\pmb{\\theta}}}{\\|\\pmb{\\nabla}_{\\pmb{\\theta}}\\|_{2}}}\\end{array}$ , where $\\nabla_{\\theta}$ is the gradient of $\\mathcal{L}_{\\mathcal{D}^{n}}$ at $\\pmb{\\theta}$ , and performing a Taylor expansion of ${\\mathcal{L}}_{\\mathcal{D}^{n}}$ around $\\pmb{\\theta}$ , we formulate the following theorem: ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 Denote $\\nabla_{\\theta}$ as the gradient and $\\lambda_{m a x}^{H}$ as the largest eigenvalues of the Hessian matrix $H_{\\theta}$ of $\\mathcal{L}_{\\mathcal{D}^{n}}$ at . For any $\\rho>0$ , with high probability over training set $\\mathcal{D}^{n}\\sim\\mathcal{D}$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathcal{D}}(\\pmb{\\theta})\\leq\\mathcal{L}_{\\mathcal{D}^{n}}(\\pmb{\\theta})+\\rho\\|\\pmb{\\nabla}_{\\pmb{\\theta}}\\|_{2}+\\frac{\\rho^{2}}{2}\\lambda_{m a x}^{H}+R\\Big(\\frac{\\|\\pmb{\\theta}\\|_{2}^{2}}{\\rho^{2}},\\frac{1}{n}\\Big).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, higher-order terms from the Taylor expansion are incorporated into $\\begin{array}{r}{R\\Big(\\frac{||\\pmb{\\theta}||_{2}^{2}}{\\rho^{2}},\\frac{1}{n}\\Big)}\\end{array}$ , which is related to weights norm and inversely related to the training data size $n$ . ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 (proof in $\\S B.1$ ) outlines strategies for enhancing generalization. These involve regularizing weight norms and the largest Hessian eigenvalues, and crucially, increasing data size $n$ and reducing the weight gradient norms (illustrated in Figure 1). However, excessive reduction should be avoided as it could impair network\u2019s representation capacity, yielding higher empirical and population loss. ", "page_idx": 3}, {"type": "text", "text": "3.3 Motivation and limitation of aligning the fine-tuned model with the pre-trained model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Theorem 1 emphasizes that large-scale data and smaller gradient magnitudes are essential for better generalization in neural network training. Therefore, aligning the fine-tuned model with the pretrained one is crucial, as it ensures retention of knowledge developed from large-scale data, preserving generalizability. PEFT methods achieve this alignment by limiting the number of trainable parameters, restricting model\u2019s capacity to deviate from the pre-trained one and often outperforming full finetuning. However, the training objective prioritizes downstream task performance, compromising alignment with pre-trained knowledge. While sparsity regularization [17] and weight decay on adapter weights help, they do not ensure alignment, as even small weight changes can lead to significant divergence in output space [74, 20, 16]. Therefore, we propose to achieve the alignment by reducing the FP-distance (output distance between fine-tuned and pre-trained models on training samples): ", "page_idx": 3}, {"type": "equation", "text": "$$\nD^{\\mathrm{fp}}(\\pmb\\theta)=\\frac{1}{n}\\sum_{i=1}^{n}\\lVert f(\\pmb x_{i};\\pmb\\theta)-f(\\pmb x_{i};\\pmb\\theta_{0})\\rVert_{2}^{2},\\quad\\pmb\\theta=\\pmb\\theta_{0}+\\Delta\\pmb\\theta,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\theta,\\pmb{\\theta}_{0},\\Delta\\pmb{\\theta}\\in\\mathbb{R}^{d}$ are parameters for the fine-tuned model, pre-trained model and the adapter. ", "page_idx": 3}, {"type": "text", "text": "While reducing FP-distance keeps the fine-tuned model close to the pre-trained model, thus preserving its knowledge, it does not ensure reduced gradient magnitudes, leading to suboptimal generalization. To understand the gradient-related limitations in this alignment, we assume $\\Delta\\pmb{\\theta}$ is small enough for a ", "page_idx": 3}, {"type": "image", "img_path": "cOuLbPhOT1/tmp/239d75f9de2b5093110d1345358ccd0ef0d0783f9aed063a5a4eb41af97b024d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 1: Thm. 1: A flatter minimum has smaller gradient and Hessian norms, yielding better generalization. Thm. 2: Large gradient norms indicate large differences among perturbations. PACE minimizes these differences, reducing gradient norms. Thm. 3: Minimizing all pairs of distances between $f(\\pmb{\\theta}_{0}\\!+\\!z_{1}\\odot\\Delta\\pmb{\\theta})$ and $f(\\pmb{\\theta}_{0}\\!+\\!z_{2}\\odot\\!\\Delta\\pmb{\\theta})$ where $z_{1}$ , $z_{2}\\!\\sim\\!\\mathcal{N}(\\mathbf{1},\\sigma^{2}I)$ also reduces FP-distance (between fine-tuned $f(\\pmb{\\theta}_{0}+\\Delta\\pmb{\\theta})$ and pre-trained $f(\\pmb\\theta_{0}))$ ), especially when ${z_{1}}\\mathrm{{=}}1$ , ${z_{2}\\!=\\!0}$ or vice versa. ", "page_idx": 4}, {"type": "text", "text": "Taylor expansion approximation. Following standard practices [16, 79, 2], we perform the expansion up to the second-order terms. Given the independence between elements in squared $L_{2}$ distances $(\\S B.4)$ and to simplify our theories, we analyze a one-dimensional output for a single i.i.d. sample, which leads us to the following proposition. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1 Assuming $\\Delta\\pmb{\\theta}$ is small, denote $f(\\pmb\\theta)\\in\\mathbb{R}$ as the one-dimensional output for $\\textbf{\\em x}$ , with $\\mathbf{v}$ and $\\pmb{H}$ as its gradient and Hessian at $\\pmb{\\theta}$ . FP-distance over $\\textbf{\\em x}$ can be decomposed as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{[f(\\theta)-f(\\theta_{0})]^{2}=[f(\\theta)-f(\\theta-\\Delta\\theta)]^{2}\\approx\\left[f(\\theta)-[f(\\theta)-\\Delta\\theta^{T}\\nabla+\\frac{1}{2}\\Delta\\theta^{T}\\!H\\Delta\\theta]\\right]^{2}}}\\\\ {{\\approx[\\Delta\\theta^{T}\\nabla-\\frac{1}{2}\\Delta\\theta^{T}\\!H\\Delta\\theta]^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Prop. 1 establishes the relationship between weight gradients, adapter weights, and FP-distance. However, it remains unclear if it regulates gradients. Our experiments show that minimizing FPdistance can sometimes increase gradient magnitude, complicating efforts for managing gradient. ", "page_idx": 4}, {"type": "text", "text": "3.4 Consistency regularization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To achieve better generalization by both regularizing gradients and aligning the fine-tuned model with the pre-trined model, we propose a consistency regularization loss for $f$ , encouraging invariance of $f$ to the same input under varying multiplicative noise perturbations on the adapter weights, as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nD^{\\mathrm{pace}}(\\theta)=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{z_{1},z_{2}}\\|f(x_{i};\\theta_{0}+z_{1}\\odot\\Delta\\theta)-f(x_{i};\\theta_{0}+z_{2}\\odot\\Delta\\theta)\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $z_{1},z_{2}\\sim\\mathcal{N}({\\bf1},\\sigma^{2}I)$ is the multiplicative noise applied on adapter weight. To understand the generalization benefits in this consistency regularization, we simplify the analysis by focusing on one-dimensional output for a single sample, resulting in the following theorem. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2 Using notations from Prop. $I$ , let $f(\\pmb\\theta_{0}+\\pmb z\\odot\\Delta\\pmb\\theta)\\in\\mathbb{R}$ be the one-dimensional output for $\\textbf{\\em x}$ . Define $\\Delta\\theta_{j}$ as $j$ -th element in $\\Delta\\pmb{\\theta},\\nabla_{j}$ as the $j$ -th element in $\\triangledown$ and $H_{j k}$ as the $(j,k)$ -entry in $\\pmb{H}$ . With $z_{1},z_{2}\\sim\\mathcal{N}(\\mathbf{1},\\sigma^{2}I)$ , the consistency loss over $\\textbf{\\em x}$ can be approximated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad{\\mathbb E}_{z_{1},z_{2}}[f(\\theta_{0}+z_{1}\\odot\\Delta\\theta)-f(\\theta_{0}+z_{2}\\odot\\Delta\\theta)]^{2}}\\\\ &{\\approx2\\sigma^{2}\\!\\sum_{j}\\Delta\\theta_{j}^{2}\\nabla_{j}^{2}+\\sigma^{4}\\!\\sum_{j,k}\\!\\Delta\\theta_{k}^{2}\\Delta\\theta_{j}^{2}H_{j k}^{2}\\ =2\\sigma^{2}\\|\\Delta\\theta\\odot\\nabla\\|_{2}^{2}\\!+\\!\\sigma^{4}\\|(\\Delta\\theta\\Delta\\theta^{T})\\odot H\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Theorem 2 (Proof in $\\S B.2)$ shows that consistency regularization essentially penalizes the first- and second-order gradients of $f$ at $\\pmb{\\theta}$ (illustrated in Figure 1), with the regularization strength controlled by the noise variance $\\sigma^{2}$ and adaptively influenced by the magnitude of elements in adapter weight $\\Delta\\pmb{\\theta}$ . Thus, minimizing the consistency loss implicitly regularizes the gradients, improving generalization. ", "page_idx": 4}, {"type": "text", "text": "With the FP-distance in Prop. 1 and consistency loss in Theorem 2, we establish their relationship as: ", "page_idx": 4}, {"type": "text", "text": "Theorem 3 With d as the dimension of $\\pmb{\\theta}$ , Eq. 6 can be upper bounded as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n[\\Delta\\theta^{T}\\nabla-\\frac{1}{2}\\Delta\\theta^{T}H\\Delta\\theta]^{2}\\leq2d\\|\\Delta\\theta\\odot\\nabla\\|_{2}^{2}+d^{2}\\|(\\Delta\\theta\\Delta\\theta^{T})\\odot H\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Transformer block with adapter perturbed by noise Consistency regularization between two outputs of $_x$ ", "page_idx": 5}, {"type": "image", "img_path": "cOuLbPhOT1/tmp/c95e84f347d6a34ae6b1a47ab270ffed60fa4795ba857742e11f56077b58df76.jpg", "img_caption": ["Figure 2: Our pipeline. Adapter $\\Delta h$ and $h_{0}$ from pre-trained model form the linear layer $h$ of Multi-Head Attention and MLP in fine-tuned model. We perturb $\\Delta h$ with multiplicative noise and ensure the network remains consistent to same inputs under varying perturbations. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Theorem 3 (proof in B.3) establishes the relationship between Eq. 6 and Eq. 8, showing Eq. 6 is upperbounded by terms involving $\\|\\Delta\\pmb{\\theta}\\odot\\pmb{\\nabla}\\|_{2}^{2}$ and $\\|(\\Delta\\bar{\\pmb{\\theta}}\\Delta\\pmb{\\theta}^{T})\\odot\\bar{\\pmb{H}}\\|_{F}^{2}$ which appear in Eq. 8. Reducing these terms results in a decrease in Eq. 6. Thus minimizing the consistency loss implicitly aligns the fine-tuned with pre-trained models (illustrated in Figure 1), preserving pre-trained knowledge. ", "page_idx": 5}, {"type": "text", "text": "3.5 Efficient implementation of PACE ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Providing different weight perturbations for each input in a mini-batch increases memory and computational demands. To avoid this inefficiency, we perturb feature outputs from the adapter $\\Delta h$ , effectively simulating perturbation that shares noise across each row in the weight $\\Delta W$ . Our simple pipeline is illustrated in Figure 2. Consider $\\pmb{X}\\in\\mathbb{R}^{B\\times T\\times d_{\\mathrm{in}}}$ as a batch of data where $B,T$ be the batch and token sizes. The calculation for the linear layer of the fine-tuned model, which utilizes pre-trained weights $W_{0},b_{0}$ and adapter weights $\\Delta W,\\Delta b$ , processes an output size of $d_{\\mathrm{out}}$ as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r}{h_{0}({\\pmb X})={\\pmb W}_{0}{\\pmb X}+b_{0};\\quad\\Delta h({\\pmb X})=\\Delta{\\pmb W}{\\pmb X}+\\Delta b,}\\\\ {h({\\pmb X})=h_{0}({\\pmb X})+{\\pmb Z}\\odot\\Delta h({\\pmb X}).\\qquad\\qquad}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here $\\odot$ is the element-wise multiplication after expanding the left matrix $Z\\in\\mathbb{R}^{B\\times d_{\\mathrm{out}}}\\sim\\mathcal{N}({\\bf1},\\sigma^{2}I)$ into $B\\times T\\times d_{\\mathrm{out}}$ where tokens within the same example share same noise. Motivated by [36], the $\\sigma$ decreases linearly as block depth increases. Let $f_{1}$ and $f_{2}$ be two networks share same weights but non-share noises. The loss function for PACE is: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}^{\\mathrm{PACE}}=\\frac{1}{n}\\sum_{i=1}^{n}\\ell(f_{1}(\\pmb{x}_{i}),\\pmb{y}_{i})+\\lambda\\|f_{1}(\\pmb{x}_{i})-f_{2}(\\pmb{x}_{i})\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\ell$ is the classification loss and $\\lambda$ is a hyperparameter controlling regularization strength. During inference, noise and regularization are ommitted, $\\Delta W,\\Delta b$ are integrated with $W_{0},b_{0}$ for efficiency: ", "page_idx": 5}, {"type": "equation", "text": "$$\nW=W_{0}+\\Delta W;\\quad b=b_{0}+\\Delta b;\\quad h(X)=W X+b.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Efficient PACE variants. In $\\S C$ , We present two variants that match the computational/memory costs of the baseline while achieving superior performance with substantially reduced resources. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We combine $\\mathrm{LoRA}_{\\mathrm{mul}}$ and $\\mathrm{VPT_{add}}$ to form a strong baseline $\\mathrm{LoRA_{mul}+V P T_{a d d}}$ , outperforming other combinations in most cases. We evaluate our method across four visual classification adaptation tasks: VTAB-1K [77], few-shot learning [29], FGVC [27] and domain adaptation [81]. We demonstrate PACE improves LoRA on GLUE [69] for text classification and GSM-8K [9] for text generation. ", "page_idx": 5}, {"type": "text", "text": "Datasets and evluations. VTAB-1K comprises 19 datasets clustered into (i) Natural images, (ii) Specialized datasets (remote sensing, medical) and (iii) Structured datasets (scene structure) domains. Each dataset has 1K training examples. Following [77, 27], we use the provided 800-200 train split for hyperparameter selection, evaluate using the full training set and report average accuracy across three trails. Few-shot learning involves 5 fine-grained datasets: FGVC-Aircraft [45], Food101 [4], OxfordFlowers102 [50], OxfordPets [52] and StanfordCars [34]. Following [29], we evaluate 1, ", "page_idx": 5}, {"type": "table", "img_path": "cOuLbPhOT1/tmp/4ee5e835a25b19858edc7a77a4b3be33aae9ce1a5925f46972471f3934e1d827.jpg", "table_caption": ["Table 1: Results on VTAB-1K with ViT-B/16. Mean Acc. is the average of group mean values. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "cOuLbPhOT1/tmp/36ebf0ca22931c9fc9cbec6f4a0a14b81411d58eb879df4e9eeca50011397560.jpg", "table_caption": ["Table 2: Classification accuracy on Few-shot learning with ViT-B/16 pretrained on ImageNet-21K. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "2, 4, 8 and 16 shots, train on the provided training set, tune hyperparameters using validation and report average test accuracy over three random seeds. FGVC includes 5 fine-grained datasets: CUB200-2011 [68], NABirds [65], OxfordFlowers [50], StanfordDogs [10] and StanfordCars [34]. We follow [27] to use validation set for hyperparameter and report test results. For domain adaptation, following [81, 7], we train on ImageNet [11] with a 16-shot setting, use the validation split by [81] for hyperparameter selection and report the results on the official validation set and 4 out-of-domain datasets: ImageNet-Sketch [70], ImageNet-V2 [55], ImageNet-A [22] and ImageNet-R [21]. We evaluate on GLUE [69] for text classification and GSM-8K [9] for mathematical reasoning. ", "page_idx": 6}, {"type": "text", "text": "Pre-trained backbones. We experiment with two vision transformers, Vision Transforms (ViT-B/16) [15] and Swin Transformer (Swin-B) [43]. These two are pre-trained on ImageNet-21K [11]. We test a ViT-B-Laion-IN12K model, pre-trained on Laion-2B [59] and fine-tuned on ImageNet-12K [11]. We use $\\mathbf{RoBERTa_{\\mathrm{base}}}$ [42] and Phi-3-mini- $.4\\mathbf{k}$ -instruct [1] for text classification and generation. ", "page_idx": 6}, {"type": "text", "text": "Implementation details. We follow [27] for image processing. $224\\times224$ resizing for VTAB-1K; random flips and crops to $224\\times224$ for FGVC and few-shot learning; stronger augmentation for domain adaptation task, following [15, 81, 40]. We use the Adam optimizer [31] with cosine learning rate decay and linear warm-up (first 10 epochs). Models are fine-tuned for 300 epochs on VTAB-1K and 100 epochs on other vision adaptation tasks, with batch size 64. For text classification we follow [25]. See $\\S\\mathrm{G}$ for mathematical reasoning details. All experiments used an NVIDIA H100 GPU. ", "page_idx": 6}, {"type": "text", "text": "Baseline. For each dataset, we identified the better method $\\mathrm{(LoRA_{mul}+V P T_{a d d}}$ or $\\mathrm{LoRA}_{\\mathrm{add}}$ ) and tuned the rank, learning rate, and weight decay to form a strong baseline. The detailed baseline settings for each task and the number of trainable parameters, are provided in $\\S\\mathrm{F}$ , where $\\mathrm{LoRA_{mul}+V P T_{a d d}}$ generally outperformed other variants. Building on the strong $\\mathrm{LoRA_{mul}+V P T_{a d d}}$ , we use grid search for our $\\lambda$ and $\\sigma$ , following strategies from previous studies [27, 40, 25]. Beyond $\\mathrm{LoRA_{mul}+V P T_{a d d}}$ , PACE also enhances PEFT methods like AdaptFormer, GLoRA, COFT, and BOFT (\u00a7D.4). ", "page_idx": 6}, {"type": "table", "img_path": "cOuLbPhOT1/tmp/447499f610a87989aaac52f892bf105182f81a0a84f10400b9d58edb3b3ecf21.jpg", "table_caption": ["Table 3: Results on FGVC with ViT-B/16. \\* denotes using augmented ViT by AugReg [61] "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "cOuLbPhOT1/tmp/58cba5fa2f6549c0270428929efcdbe7f4351982c005062de35ac9746f1b585d.jpg", "table_caption": ["Table 4: Results on domain adaptation with ViTB/16 pretrained on ImageNet-21K. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "cOuLbPhOT1/tmp/50274261d3dec9f63bbac26550b4d68cc0d0695b655b85953fafffbbdc0f25af.jpg", "table_caption": ["Table 5: Results for GLUE w/ RoBERTabase. Matthew\u2019s correlation for COLA, Pearson correlation for STSB, and accuracy for others. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "cOuLbPhOT1/tmp/986965aca2d53af14aeaf86e8ed30ecc00e4845f395940de6a01a3ade66557d1.jpg", "table_caption": ["Table 6: Results for GSM-8K using Phi-3-mini-4k-instruct. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "cOuLbPhOT1/tmp/1ca41de801995286cf46a18262c78e42f64ded7349f1dc906a863f88ab8fc096.jpg", "table_caption": ["Table 7: Classification results on domain adaptation and CIFAR-100 in VTAB-1K based different pretrained models. Src. is short for \u2018source\u2019 in Table 4. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.1 Comparison with the State of the Arts ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Results on VTAB-1K. Table 1 presents the results comparing PACE with recent state-of-the-art PEFT methods. PACE improves the strong baseline by $2.6\\%$ accuracy, surpassing the previous SOTA GLoRA [7] by $1\\%$ , which uses two stages for parameter search. In $\\S D.1$ , we show that reducing training epochs to 50 or 100 has minimal impact on PACE performance. ", "page_idx": 7}, {"type": "text", "text": "Results on Few-shot Learning. Table 2 compares performance w/ and w/o our PACE. PACE improves $\\mathrm{LoRA}_{\\mathrm{add}}$ , $\\mathrm{VPT_{add}}$ , $\\mathrm{LoRA_{mul}+V P T_{a d d}}$ , with $\\mathrm{LoRA_{mul}+V P T_{a d d}+P A C E}$ performing best in most cases. PACE yields notable improvement, especially when the number of shot is small. ", "page_idx": 7}, {"type": "text", "text": "Results on FGVC. Table 3 shows that PACE improves the strong $\\mathrm{LoRA_{mul}+V P T_{a d d}}$ by $0.7\\%$ , outperforming SSF [40], ARC [13] and RLRR [14] that use strongly pre-trained ViT with augmentations. In $\\S D.2$ , PACE achieves larger improvements on smaller datasets. ", "page_idx": 7}, {"type": "text", "text": "Results on domain adaptation. Table 4 compares PACE with others. $\\mathrm{LoRA_{mul}+V P T_{a d d}}$ outperforms GLoRA [7] which relies on parameter search. Meanwhile, PACE improves $\\mathrm{LoRA_{mul}+V P T_{a d d}}$ by $1.5\\%$ , outperforming other PEFT methods, demonstrating superior performance on domain adaptation. ", "page_idx": 7}, {"type": "text", "text": "Results on text classification and mathematical reasoning. Table 5 shows that PACE outperforms LoRA by $1\\%$ on GLUE text classification and by $3.11\\%$ on GSM-8K mathematical reasoning. ", "page_idx": 7}, {"type": "text", "text": "Generalize to other backbones. We evaluate PACE on CIFAR-100 (VTAB-1K) and domain adaptation using Swin-B [43] pretrained on ImageNet-21K and ViT-B (pretrained on Laion 2B, then fine-tuned on ImageNet-12K). Table 7 shows PACE outperforms baseline $\\mathrm{LoRA_{mul}+V P T_{a d d}}$ and other PEFT methods across all backbones, demonstrating its effective generalizability. Further experiments in $\\S\\mathrm{D}.3$ show PACE works effectively with self-supervised models like MAE [18] and DINO [5]. ", "page_idx": 7}, {"type": "text", "text": "4.2 Analyses ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To verify our theories, we conduct experiments on CIFAR-100 (VTAB-1K) using ViT-B/16 and Camelyon (VTAB-1K) on Swin-B. Figure 3 & 4 plot the gradient norm (summed across all layers) and FP-distance (Eq. 5) and the train & validation accuracy during training for baseline $\\mathrm{LoRA_{mul}+V P T_{a d d}}$ and PACE on validation set. Figures 3a & 4a show that PACE has a smaller gradient norm than baseline, verifying Theorem 2 that PACE can implicitly lower the weight gradient norm for better generalization. Figures 3b & 4b demonstrate that PACE maintains a lower FP-distance than the baseline, verifying Theorem 3 that PACE can implicitly align the fine-tuned model with pre-trained model, retaining knowledge developed from large-scale pre-training. Owing to the advantages of the gradient regularization and model alignment, PACE shortens the performance gap between seen and unseen data, yielding higher accuracy on the unseen validation set, as shown in Figures 3c & 4c. ", "page_idx": 8}, {"type": "image", "img_path": "cOuLbPhOT1/tmp/610da839de322af52ff72b8751647f0c432dd197a8964dd7535a6206565de5d2.jpg", "img_caption": ["(a) Gradient Norm. (b) FP-Distance (c) Train and validation accuracy. Figure 3: Analysis for PACE. (a) gradient norm, (b) FP-Distance and (c) train & val accuracy, are evaluated on validation set of CIFAR-100 (VTAB-1K) with baseline $\\mathrm{LoRA_{mul}+V P T_{a d d}}$ on ViT-B/16. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "cOuLbPhOT1/tmp/848c42bb37ab928b7c64cdc7ac3f1673b280d3bd4a246461e137c4dfd76aac3c.jpg", "img_caption": ["Figure 4: Analysis for PACE. (a) gradient norm, (b) FP-Distance and (c) train & val accuracy, are evaluated on validation set of Camelyon (VTAB-1K) with baseline $\\mathrm{LoRA_{mul}+V P T_{a d d}}$ on Swin-B. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "To clarify why naive alignment is problematic, we vary the regularization strength $\\lambda$ over a wide range (1e-3 to 5e4) for both Fine-tuned Pre-trained model Alignment (FPA) by minimizing $D^{\\mathrm{fp}}$ in Eq. 5) and PACE. Figure 5 plots the averaged gradient norm over training (see also Figures 8 & 9 for more visualizations). PACE robustly lowers gradient norms with larger $\\lambda$ , while FPA exhibits unpredictable behavior, even causing gradient explosion. This verifies Prop. 1 that minimizing $D^{\\mathrm{fp}}$ is problematic for gradient regularization, complicating gradient management. ", "page_idx": 8}, {"type": "image", "img_path": "cOuLbPhOT1/tmp/6d3b83166ef61dd500f9a93000a650ad595f90191a9b4a371c412a7e6567664c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 5: Gradient norms of models across wide range of regu- Figure 6: Ablation results for aplarization strengths $\\lambda$ on CIFAR-100 (VTAB-1K) w/ ViT-B/16. plying PACE among $M$ nets and Line and shadow represent mean and std across training epochs. lazily at every $N$ steps. ", "page_idx": 8}, {"type": "text", "text": "4.3 Ablation studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We ablate PACE based on the baseline $\\mathrm{LoRA_{mul}+V P T_{a d d}}$ on CIFAR-100 (VTAB-1K) and ImageNet1K in domain adaption as shown in Table 8. The ablations include Noise (baseline w/ noise perturbing adapter), $\\mathrm{PACE_{\\mathrm{add}}}$ (replacing multiplicative noise with additive noise), ${\\mathrm{PACE}_{h}}$ (perturbing $h$ instead of $\\Delta h$ in Eq. 11), $\\mathrm{PACE_{\\mathrm{drop}}}$ (replacing Gaussian noise with dropout noise), $\\mathrm{PACE}_{\\sigma=}$ (all transformer blocks share the same $\\sigma$ ), $\\mathrm{PACE}_{\\sigma\\uparrow}$ ( $\\bar{\\sigma}$ increases linearly with depth), FPA (fine-tuned and pretrined alignment by minimizing Eq. 5), SAM (sharpness-aware minimization [16]), GP (gradient penalization), $\\ell_{1}$ (sparsity regularization), and transfer learning methods L2SP [76], DELTA [39] and FTP [63]. We grid-search hyperparameters and report the best results. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Table 8 presents the results for all variants. PACE improves over Noise, which itself is better than baseline, justifying our adapter perturbation and consistency regularization. $\\mathrm{PACE_{\\mathrm{add}}}$ performs worse than PACE, showing the superiority of multiplicative noise. Although ${\\mathrm{PACE}_{h}}$ can implicitly regularize gradients, it underperforms PACE, verifying the advantages of perturbing adapter to implicitly align models. $\\mathrm{PACE_{\\mathrm{drop}}}$ is worse than PACE, indicating dropout noise is suboptimal. $\\mathrm{PACE}_{\\sigma=}$ and $\\mathrm{PACE}_{\\sigma\\uparrow}$ performs worse, justifying our design of linearly decreasing $\\sigma$ . FPA, SAM and GP, which either only align models or only regularize gradients, are outperformed by PACE. Despite combining $\\mathrm{FPA+GP}_{\\mathrm{s}}$ it still underperforms ours, suggesting ineffective combination. $\\ell_{1}$ , L2SP, DELTA, and FTP obtain worse results than PACE, showing their limitations in improving generalization. PACE regularizes gradients for better generalization and align models to retain knowledge, surpassing all other variants. ", "page_idx": 9}, {"type": "table", "img_path": "cOuLbPhOT1/tmp/85f37d9c6b5f92eec55f128fa2092224cab1294acdb977b42a1d4cf10aa530bf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "We further evaluate applying PACE across multiple $M$ networks during training or applying it lazily with half batch size at every $N$ steps $(\\mathrm{PACE_{\\mathrm{lazy}}^{\\mathrm{half}}}$ in $\\S C_{\\l,\\l}$ ). Figure 6 presents the results, showing that applying PACE among two networks at every training step performs best. However, lazy regularization applied every few steps can still provide reasonable results while saving computational/memory costs. ", "page_idx": 9}, {"type": "text", "text": "We test the sensitivity of hyperparameter $\\lambda$ and $\\sigma$ introduced in our PACE on OxfordPets for few-shot learning accross 1, 2, 4, 8 shots. The results presented in Figure 7 demonstrate that with less data, larger $\\lambda$ and $\\sigma$ are favoured, verifying that the effectiveness of PACE in improving generalization. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have introduced PACE, a novel and effective method that combines generalization of PArameterefficient fine-tuning with Consistency rEgularization. Through rigorous theoretical analyses, we have shown PACE reduces weight gradient for improved generalization and aligns the fine-tuned model with the pre-trained model for retaining pre-training knowledge. Our experimental results support the theoretical analyses, justifying the generalization advantages of PACE over other PEFT methods. With its dual advantages, PACE consistently outperforms other variants across different backbones, firmly establishing PACE as a powerful solution for enhancing generalization for PEFT methods. Limitations and border impacts are discussed in $\\S\\mathrm{A}$ . ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements. We thank Moyang Liu, Melody Ip, Chenyi Du, and Yinuo Xu for their valuable discussions and support. PK is funded by CSIRO\u2019s Science Digital. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 7, 22   \n[2] Guillaume Alain and Yoshua Bengio. What regularized auto-encoders learn from the data-generating distribution. JMLR, 15(110):3743\u20133773, 2014. 5   \n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. 1   \n[4] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative components with random forests. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13, pages 446\u2013461. Springer, 2014. 6   \n[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9650\u20139660, 2021. 8   \n[6] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and Sungrae Park. Swad: Domain generalization by seeking flat minima. Advances in Neural Information Processing Systems, 34:22405\u201322418, 2021. 3   \n[7] Arnav Chavan, Zhuang Liu, Deepak Gupta, Eric Xing, and Zhiqiang Shen. One-for-all: Generalized lora for parameter-efficient fine-tuning. arXiv preprint arXiv:2306.07967, 2023. 2, 3, 7, 8, 21, 22   \n[8] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo. Adaptformer: Adapting vision transformers for scalable visual recognition. Advances in Neural Information Processing Systems, 35:16664\u201316678, 2022. 1, 2, 3, 21   \n[9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 6, 7, 22   \n[10] E Dataset. Novel datasets for fine-grained image categorization. In First Workshop on Fine Grained Visual Categorization, CVPR. Citeseer. Citeseer. Citeseer, 2011. 7   \n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009. 1, 7, 20   \n[12] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024. 3   \n[13] Wei Dong, Dawei Yan, Zhijun Lin, and Peng Wang. Efficient adaptation of large vision transformer via adapter re-composing. Advances in Neural Information Processing Systems, 36, 2024. 2, 8   \n[14] Wei Dong, Xing Zhang, Bihui Chen, Dawei Yan, Zhijun Lin, Qingsen Yan, Peng Wang, and Yang Yang. Low-rank rescaled vision transformer fine-tuning: A residual design approach. arXiv preprint arXiv:2403.19067, 2024. 2, 8   \n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. 1, 3, 7   \n[16] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In International Conference on Learning Representations, 2021. 3, 4, 5, 10   \n[17] Zihao Fu, Haoran Yang, Anthony Man-Cho So, Wai Lam, Lidong Bing, and Nigel Collier. On the effectiveness of parameter-efficient fine-tuning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 12799\u201312807, 2023. 2, 3, 4   \n[18] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000\u201316009, 2022. 8, 20   \n[19] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729\u20139738, 2020. 1   \n[20] Zhezhi He, Adnan Siraj Rakin, Jingtao Li, Chaitali Chakrabarti, and Deliang Fan. Defending and harnessing the bit-flip based adversarial weight attack. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14095\u201314103, 2020. 4   \n[21] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF international conference on computer vision, pages 8340\u20138349, 2021. 7   \n[22] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15262\u201315271, 2021. 7   \n[23] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International conference on machine learning, pages 2790\u20132799. PMLR, 2019. 1   \n[24] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International conference on machine learning, pages 2790\u20132799. PMLR, 2019. 2, 3   \n[25] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. 2, 3, 7   \n[26] Shengding Hu, Zhen Zhang, Ning Ding, Yadao Wang, Yasheng Wang, Zhiyuan Liu, and Maosong Sun. Sparse structure search for delta tuning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. 2   \n[27] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pages 709\u2013727. Springer, 2022. 2, 4, 6, 7, 22   \n[28] Zeyinzi Jiang, Chaojie Mao, Ziyuan Huang, Ao Ma, Yiliang Lv, Yujun Shen, Deli Zhao, and Jingren Zhou. Res-tuning: A flexible and efficient tuning paradigm via unbinding tuner from backbone. Advances in Neural Information Processing Systems, 36, 2024. 2   \n[29] Shibo Jie and Zhi-Hong Deng. Fact: Factor-tuning for lightweight adaptation on vision transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 1060\u20131068, 2023. 2, 6   \n[30] Muhammad Uzair Khattak, Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, and Fahad Shahbaz Khan. Self-regulating prompts: Foundational model adaptation without forgetting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15190\u201315200, 2023. 3   \n[31] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 7   \n[32] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4015\u20134026, 2023. 1   \n[33] Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki M Asano. VeRA: Vector-based random matrix adaptation. In The Twelfth International Conference on Learning Representations, 2024. 3   \n[34] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on computer vision workshops, pages 554\u2013561, 2013. 6, 7   \n[35] Yoonho Lee, Annie S Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, and Chelsea Finn. Surgical fine-tuning improves adaptation to distribution shifts. In The Eleventh International Conference on Learning Representations, 2023. 2   \n[36] Bonan Li, Yinhan Hu, Xuecheng Nie, Congying Han, Xiangjian Jiang, Tiande Guo, and Luoqi Liu. Dropkey for vision transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22700\u201322709, 2023. 6   \n[37] Dongyue Li and Hongyang Zhang. Improved regularization and robustness for fine-tuning in neural networks. Advances in Neural Information Processing Systems, 34:27249\u201327262, 2021. 3   \n[38] Shengrui Li, Xueting Han, and Jing Bai. Adaptergnn: Parameter-efficient fine-tuning improves generalization in gnns. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 13600\u201313608, 2024. 2   \n[39] Xingjian Li, Haoyi Xiong, Hanchao Wang, Yuxuan Rao, Liping Liu, Zeyu Chen, and Jun Huan. Delta: Deep learning transfer using feature map with attention for convolutional networks. arXiv preprint arXiv:1901.09229, 2019. 3, 10   \n[40] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Scaling & shifting your features: A new baseline for efficient model tuning. In Advances in Neural Information Processing Systems (NeurIPS), 2022. 2, 3, 7, 8, 22   \n[41] Weiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan Xue, Longhui Yu, Haiwen Feng, Zhen Liu, Juyeon Heo, Songyou Peng, Yandong Wen, Michael J. Black, Adrian Weller, and Bernhard Sch\u00f6lkopf. Parameter-efficient orthogonal finetuning via butterfly factorization. In ICLR, 2024. 2, 21   \n[42] Yinhan Liu. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. 7   \n[43] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 7, 8   \n[44] Gen Luo, Minglang Huang, Yiyi Zhou, Xiaoshuai Sun, Guannan Jiang, Zhiyu Wang, and Rongrong Ji. Towards efficient visual adaption via structural re-parameterization. arXiv preprint arXiv:2302.08106, 2023. 2, 4, 22   \n[45] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013. 6 [46] Yao Ni and Piotr Koniusz. Chain: Enhancing generalization in data-efficient gans via lipschitz continuity constrained normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6763\u20136774, June 2024. 3 [47] Yao Ni and Piotr Koniusz. Nice: Noise-modulated consistency regularization for data-efficient gans. Advances in Neural Information Processing Systems, 36, 2024. 2, 16 [48] Yao Ni, Piotr Koniusz, Richard Hartley, and Richard Nock. Manifold learning beneftis gans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11265\u201311274, 2022. 3 [49] Yao Ni, Dandan Song, Xi Zhang, Hao Wu, and Lejian Liao. Cagan: Consistent adversarial training enhanced gans. In IJCAI, pages 2588\u20132594, 2018. 2 [50] Maria-Elena Nilsback and Andrew Zisserman. A visual vocabulary for flower classification. In IEEE Conference on Computer Vision and Pattern Recognition, volume 2, pages 1447\u20131454, 2006. 6, 7 [51] Changdae Oh, Hyeji Hwang, Hee-young Lee, YongTaek Lim, Geunyoung Jung, Jiyoung Jung, Hosik Choi, and Kyungwoo Song. Blackvip: Black-box visual prompting for robust transfer learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24224\u201324235, 2023. 2 [52] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In IEEE Conference on Computer Vision and Pattern Recognition, 2012. 6 [53] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard Sch\u00f6lkopf. Controlling text-to-image diffusion by orthogonal finetuning. In NeurIPS, 2023. 2,   \n21 [54] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR,   \n2021. 1 [55] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In International conference on machine learning, pages 5389\u20135400. PMLR, 2019.   \n7 [56] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022. 1 [57] Shuvendu Roy and Ali Etemad. Consistency-guided prompt learning for vision-language models. In The Twelfth International Conference on Learning Representations, 2024. 3 [58] Kuniaki Saito, Donghyun Kim, and Kate Saenko. Openmatch: Open-set semi-supervised learning with open-set consistency regularization. Advances in Neural Information Processing Systems, 34:25956\u201325967,   \n2021. 2 [59] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. 1, 7 [60] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural information processing systems, 33:596\u2013608, 2020. 2 [61] Andreas Peter Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train your vit? data, augmentation, and regularization in vision transformers. Transactions on Machine Learning Research, 2022. 8 [62] Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai Lin, Huadong Wang, Kaiyue Wen, Zhiyuan Liu, Peng Li, Juanzi Li, et al. On transferability of prompt tuning for natural language processing. arXiv preprint arXiv:2111.06719, 2021. 1, 2 [63] Junjiao Tian, Yen-Cheng Liu, James S Smith, and Zsolt Kira. Fast trainable projection for robust fine-tuning. Advances in Neural Information Processing Systems, 36, 2024. 3, 10 [64] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1 [65] Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber, Jessie Barry, Panos Ipeirotis, Pietro Perona, and Serge Belongie. Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 595\u2013604, 2015. 7 [66] D\u00e1niel Varga, Adri\u00e1n Csisz\u00e1rik, and Zsolt Zombori. Gradient regularization improves accuracy of discriminative models. arXiv preprint arXiv:1712.09936, 2017. 3 [67] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. 1, 2, 3   \n[68] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. Thecaltech-ucsdbirds-200-2011 dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011. 7   \n[69] Alex Wang. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018. 6, 7   \n[70] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. Advances in Neural Information Processing Systems, 32, 2019. 7   \n[71] Yihan Wang, Jatin Chauhan, Wei Wang, and Cho-Jui Hsieh. Universality and limitations of prompt tuning. Advances in Neural Information Processing Systems, 36, 2024. 2, 3   \n[72] Yaoming Wang, Yuchen Liu, Xiaopeng Zhang, Jin Li, Bowen Shi, Chenglin Li, Wenrui Dai, Hongkai Xiong, and Qi Tian. Violet: Vision-language efficient tuning with collaborative multi-modal gradients. In Proceedings of the 31st ACM International Conference on Multimedia, pages 4595\u20134605, 2023. 3   \n[73] Yeming Wen and Swarat Chaudhuri. Batched low-rank adaptation of foundation models. arXiv preprint arXiv:2312.05677, 2023. 2   \n[74] Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization. Advances in neural information processing systems, 33:2958\u20132969, 2020. 4   \n[75] Lijun Wu, Juntao Li, Yue Wang, Qi Meng, Tao Qin, Wei Chen, Min Zhang, Tie-Yan Liu, et al. Rdrop: Regularized dropout for neural networks. Advances in Neural Information Processing Systems, 34:10890\u201310905, 2021. 2   \n[76] LI Xuhong, Yves Grandvalet, and Franck Davoine. Explicit inductive bias for transfer learning with convolutional networks. In International Conference on Machine Learning, pages 2825\u20132834. PMLR, 2018. 3, 10   \n[77] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867, 2019. 6   \n[78] Han Zhang, Zizhao Zhang, Augustus Odena, and Honglak Lee. Consistency regularization for generative adversarial networks. arXiv preprint arXiv:1910.12027, 2019. 2   \n[79] Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou. How does mixup help with robustness and generalization? In ICLR, 2021. 5   \n[80] Shan Zhang, Yao Ni, Jinhao Du, Yanxia Liu, and Piotr Koniusz. Semantic transfer from head to tail: Enlarging tail margin for long-tailed visual recognition. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1350\u20131360, 2024. 3   \n[81] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural prompt search. arXiv preprint arXiv:2206.04673, 2022. 2, 6, 7   \n[82] Bingchen Zhao, Haoqin Tu, Chen Wei, Jieru Mei, and Cihang Xie. Tuning layernorm in attention: Towards efficient multi-modal LLM finetuning. In The Twelfth International Conference on Learning Representations, 2024. 2   \n[83] Yang Zhao, Hao Zhang, and Xiuyuan Hu. Penalizing gradient norm for efficiently improving generalization in deep learning. In International Conference on Machine Learning, pages 26982\u201326992. PMLR, 2022. 3 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "PACE: marrying generalization of PArameter-efficient fine-tuning with Consistency rEgularization (Supplementary Material) ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Yao Ni\u2020 Shan Zhang\u2021,\u2020 Piotr Koniusz $^{*,\\S,\\dag}$ \u2020The Australian National University \u00a7Data61 CSIRO \u2021Australian Institute for Machine Learning, University of Adelaide \u2020yao.ni@anu.edu.au \u2021shan.zhang@adelaide.edu.au piotr.koniusz@data61.csiro.au ", "page_idx": 14}, {"type": "text", "text": "A Broader impacts and limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Broader impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our work provides a powerful solution for improving generalization in Parameter Efficient FineTuning (PEFT), allowing for effective fine-tuning of pre-trained models while reduce the heavily reliance on pretraining from scratch using massive data. Our advancement in PEFT, supported by Theorems 1, 2 and 3, offer novel insights into gradient regularization and model alignment. These insights extend beyond PEFT and can be applied to other areas such as continual learning and transfer learning, potentially enhancing the performance and efficiency of models in various domains. By leveraging our findings, practitioners can develop more robust and adaptable models that generalize well to new tasks and environments, leading to more intelligent and versatile AI systems. In terms of negative impacts, the robustness of our fine-tuning method could potentially be misused to create more convincing deepfakes, raising concerns about the spread of misinformation, manipulation of public opinion, and malicious activities such as fraud, blackmail, or harassment. ", "page_idx": 14}, {"type": "text", "text": "A.2 Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "While our work effectively improves generalization ability, it introduces additional computational costs by requiring input samples to be passed through the network twice for regularization. However, this can be mitigated by using two efficient variants, $\\mathrm{PACE_{\\mathrm{fast}}}$ and $\\mathrm{{PACE_{\\mathrm{{lazy}}}^{h a l f}}}$ Elhaazlfy, proposed in \u00a7C, where we demonstrate the potential for resource-efficient finetuning. Additionally, our method introduces extra hyperparameters $\\lambda$ and $\\sigma$ , which require caution during hyperparameter search. Nonetheless, Figure 7 suggests that fewer training data requires larger $\\lambda$ and $\\sigma$ values, providing insight for hyperparameter tuning. ", "page_idx": 14}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Settting $\\begin{array}{r}{\\epsilon=\\frac{\\rho\\nabla_{\\theta}}{||\\nabla_{\\theta}||_{2}}}\\end{array}$ , we perform a second-order Taylor expansion of $\\mathcal{L}_{\\mathcal{D}^{n}}$ around $\\pmb{\\theta}$ . By incorporating the higher-order terms from the Taylor expansion into $R\\!\\left(\\frac{||\\pmb\\theta||_{2}^{2}}{\\rho^{2}},\\frac{1}{n}\\right)$ , we derive: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathcal{D}}(\\pmb{\\theta})\\leq\\mathcal{L}_{\\mathcal{D}^{n}}\\Big(\\pmb{\\theta}+\\frac{\\rho\\pmb{\\nabla}_{\\pmb{\\theta}}}{\\|\\pmb{\\nabla}_{\\pmb{\\theta}}\\|_{2}}\\Big)+R\\Big(\\frac{\\|\\pmb{\\theta}\\|_{2}^{2}}{\\rho^{2}},\\frac{1}{n}\\Big)}\\\\ &{\\qquad\\approx\\mathcal{L}_{\\mathcal{D}^{n}}(\\pmb{\\theta})+\\rho\\|\\pmb{\\nabla}_{\\pmb{\\theta}}\\|_{2}+\\frac{\\rho^{2}}{2\\|\\pmb{\\nabla}_{\\pmb{\\theta}}\\|_{2}^{2}}\\pmb{\\nabla}_{\\pmb{\\theta}}^{T}\\pmb{H}_{\\theta}\\pmb{\\nabla}_{\\pmb{\\theta}}+R\\Big(\\frac{\\|\\pmb{\\theta}\\|_{2}^{2}}{\\rho^{2}},\\frac{1}{n}\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Assuming that the approximation does not alter the inequality relationship, i.e., it preserves the $\\leq$ relation on both sides and considering the largest eigenvalue of $H_{\\theta}$ as $\\lambda_{\\operatorname*{max}}^{\\dot{H}}$ , implying $\\pmb{v}^{T}\\pmb{H_{\\theta}v}\\leq$ $\\lambda_{\\mathrm{max}}^{H}||\\pmb{v}||_{2}^{2}$ for any $\\pmb{v}$ , we further bound Eq. 14 as follows and arrive at: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathcal{D}}(\\pmb{\\theta})\\leq\\mathcal{L}_{\\mathcal{D}^{n}}(\\pmb{\\theta})+\\rho\\|\\pmb{\\nabla}_{\\pmb{\\theta}}\\|_{2}+\\frac{\\rho^{2}}{2}\\lambda_{\\operatorname*{max}}^{H}+R\\Big(\\frac{\\|\\pmb{\\theta}\\|_{2}^{2}}{\\rho^{2}},\\frac{1}{n}\\Big)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "B.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The proof is motivated from [47]. We include the proof process for completeness. Denote $_{m_{1}}=$ $z_{1}-1,m_{2}=z_{2}-1$ thus $m_{1},m_{2}\\sim\\mathcal{N}(\\mathbf{0},\\sigma^{2})$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\l\"}\\ltimes\\lnot\\textcircled{\\l=\\l}\\ltimes\\lnot\\textcircled{\\l=\\l}\\ltimes\\lnot\\textcircled{\\l=\\l}\\ltimes\\lnot(\\lnot)}\\\\ &{d^{\\mathrm{pace}}=\\mathbb{E}_{z_{1},z_{2}}[f(\\theta_{0}+z_{1}\\odot\\Delta\\theta)-f(\\theta_{0}+z_{2}\\odot\\Delta\\theta)]^{2}}\\\\ &{\\qquad=\\mathbb{E}_{z_{1},z_{2}}[f(\\theta_{0}+\\Delta\\theta+(z_{1}-1)\\odot\\Delta\\theta)-f(\\theta_{0}+\\Delta\\theta+(z_{2}-1)\\odot\\Delta\\theta)]^{2}}\\\\ &{\\qquad=\\mathbb{E}_{m_{1},m_{2}}[f(\\theta+m_{1}\\odot\\Delta\\theta)-f(\\theta+m_{2}\\odot\\Delta\\theta)]^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Defining $\\pmb{v}:=\\pmb{m}_{1}\\odot\\Delta\\pmb{\\theta}$ and $\\pmb{u}:=\\pmb{m}_{2}\\odot\\pmb{\\Delta\\theta}$ , where $v,u\\sim\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\mathrm{diag}(\\Delta\\pmb{\\theta}\\odot\\Delta\\pmb{\\theta}))$ , we can rewrite Eq. 15 as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{\\mathbf{v},\\mathbf{u}}[f(\\theta+\\mathbf{v})-f(\\theta+\\mathbf{u})]^{2}}\\\\ &{\\sim\\mathbb{E}_{\\mathbf{v},\\mathbf{u}}\\Big[f(\\theta)+v^{T}\\nabla+\\frac{1}{2}v^{T}H v-f(\\theta)-u^{T}\\nabla-\\frac{1}{2}u^{T}H u\\Big]^{2}}\\\\ &{-\\mathbb{E}_{\\mathbf{v},\\mathbf{u}}\\Big[v^{T}\\nabla+\\frac{1}{2}v^{T}H v-u^{T}\\nabla-\\frac{1}{2}u^{T}H u\\Big]^{2}}\\\\ &{-\\mathbb{E}_{\\mathbf{v},\\mathbf{u}}\\Big[(v-u)^{T}\\nabla+\\frac{1}{2}v^{T}H v-\\frac{1}{2}u^{T}H u\\Big]^{2}}\\\\ &{=\\!\\mathbb{E}_{\\mathbf{v},\\mathbf{u}}\\Big[(v-u)^{T}\\nabla\\Big]^{2}}\\\\ &{\\quad+\\mathbb{E}_{\\mathbf{v},\\mathbf{u}}\\Big[(v-u)^{T}\\nabla\\Big](v^{T}H v-u^{T}H u)\\Big]}\\\\ &{\\quad+\\frac{1}{4}\\mathbb{E}_{\\mathbf{v}}\\Big[v^{T}H v\\Big]^{2}+\\frac{1}{4}\\mathbb{E}_{\\mathbf{u}}\\Big[u^{T}H u^{T}\\Big]^{2}}\\\\ &{\\quad-\\frac{1}{2}\\mathbb{E}_{\\mathbf{v},\\mathbf{u}}\\Big[(v^{T}H v)(u^{T}H u)\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Next, we derive the four terms, Eq. 16, 17, 18, and 19, respectively as follows: ", "page_idx": 15}, {"type": "text", "text": "Eq. 16. Using $\\mathbb{E}_{z_{1},z_{2}}[(z_{1}-z_{2})^{2}]=2\\sigma^{2}$ for $z_{1},z_{2}\\sim\\mathcal{N}(0,\\sigma^{2})$ , we can simplify (Eq. 16) as follows, noting that terms related to different dimensions are canceled due to zero-mean independent Gaussian noise: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{v,u}\\big[(v-u)^{T}\\boldsymbol{\\nabla}\\big]^{2}=\\mathbb{E}_{v,u}\\big[\\sum_{j}(v_{j}-u_{j})^{2}\\boldsymbol{\\nabla}_{j}^{2}\\big]=2\\sigma^{2}\\sum_{j}\\Delta\\theta_{j}^{2}\\boldsymbol{\\nabla}_{k}^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Eq. 17. Utilizing $E[z^{3}]=\\mu^{3}+3\\mu\\sigma^{2}$ for $z\\sim\\mathcal{N}(\\mu,\\sigma^{2})$ , and noting that $E[z^{3}]=0$ for $\\mu=0$ , Eq. 17 is derived as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\quad\\mathbb{E}_{v,u}\\!\\left[\\left((v-u)^{T}\\nabla\\right)\\left(v^{T}\\!H\\boldsymbol{v}-u^{T}\\boldsymbol{H}\\boldsymbol{u}\\right)\\right]}&\\\\ &{=\\!\\!\\mathbb{E}_{v}\\!\\left[(v^{T}\\nabla)(v^{T}\\!H\\boldsymbol{v})\\!\\right]\\!+\\!\\mathbb{E}_{u}\\!\\left[(u^{T}\\nabla)(u^{T}\\!H\\boldsymbol{u})\\right]\\!-\\!\\mathbb{E}_{v,u}\\!\\left[(v^{T}\\nabla)(u^{T}\\!H\\boldsymbol{u})\\right]\\!-\\!\\mathbb{E}_{v,u}\\!\\left[(u^{T}\\nabla)(v^{T}\\!H\\boldsymbol{v})\\right]\\!\\!}&\\\\ &{=\\!\\!2\\mathbb{E}_{v}\\!\\left[(v^{T}\\nabla)(v^{T}\\!H\\boldsymbol{v})\\right]=0.}&{(2}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Eq. 18. We first decompose Eq. 18, then discuss each case and obtain the final result. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{4}\\mathbb{E}_{v}[v^{T}H v]^{2}+\\frac{1}{4}\\mathbb{E}_{u}[u^{T}H u]^{2}=\\frac{1}{2}\\mathbb{E}_{v}[v^{T}H v]^{2}=\\frac{1}{2}\\mathbb{E}_{v}\\big[\\sum_{j,k,p,q}v_{j}H_{j k}v_{k}v_{p}H_{p q}v_{q}\\big].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Given the independence of elements in $\\pmb{v}$ , only terms with an element repeated two or four times contribute non-zero results, leading to four distinct, non-overlapping cases. Using $\\mathbb{E}[z^{2}]=\\sigma^{2}+\\mu^{2}$ and $\\mathbb{E}[z^{4}]=\\mu^{4}+6\\mu^{2}\\sigma^{2}+3\\sigma^{4}$ for $z\\sim\\mathcal{N}(\\mu,\\sigma^{2})$ , and simplifying to $\\mathbb{E}[z^{2}]=\\sigma^{\\overline{{2}}}$ and $\\bar{\\mathbb{E}}[z^{4}]=3\\sigma^{4}$ when $\\mu=0$ , we have: ", "page_idx": 16}, {"type": "text", "text": "Case $^{\\,I}$ : $j=k\\neq p=q$ , given the independence of $v_{j}$ and $v_{p}$ , we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{v}\\big[\\sum_{j}\\sum_{p\\neq j}v_{j}^{2}H_{j j}v_{p}^{2}H_{p p}\\big]=\\sum_{j,p\\neq j}H_{j j}H_{p p}\\mathbb{E}[v_{j}^{2}]\\mathbb{E}[v_{p}^{2}]=\\sigma^{4}\\!\\sum_{j,k\\neq j}\\!H_{j j}H_{k k}\\Delta\\theta_{j}^{2}\\Delta\\theta_{k}^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Case 2: For $j=p\\neq k=q$ , the independence of $v_{j}$ and $v_{k}$ simplifies our calculation, leading to: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{v}\\big[\\sum_{j}\\sum_{k\\neq j}v_{j}H_{j k}v_{k}v_{j}H_{j k}v_{k}\\big]=\\sum_{j,k\\neq j}H_{j k}^{2}\\mathbb{E}[v_{j}^{2}]\\mathbb{E}[v_{k}^{2}]=\\sigma^{4}\\sum_{j,k\\neq j}H_{j k}^{2}\\Delta\\theta_{j}^{2}\\Delta\\theta_{k}^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Case 3: For $j\\,=\\,q\\,\\neq\\,k\\,=\\,p$ , utilizing the independence of $v_{j}$ and $v_{k}$ as well as the symmetry $H_{j k}=H_{k j}$ , we obtain: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{v}\\big[\\sum_{j}\\sum_{k\\neq j}v_{j}H_{j k}v_{k}v_{k}H_{k j}v_{j}\\big]=\\sum_{j,k\\neq j}H_{j k}^{2}\\mathbb{E}[v_{j}^{2}]\\mathbb{E}[v_{k}^{2}]=\\sigma^{4}\\sum_{j,k\\neq j}H_{j k}^{2}\\Delta\\theta_{j}^{2}\\Delta\\theta_{k}^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Case 4: For $j=q=k=p$ , using $\\mathbb{E}[z^{4}]=3\\sigma^{4}$ where $z\\sim\\mathcal{N}(0,\\sigma^{2})$ , we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{v}\\Big[\\sum_{j}v_{j}H_{j j}v_{j}v_{j}H_{j j}v_{j}\\Big]=\\sum_{j}H_{j j}^{2}\\mathbb{E}[v_{j}^{4}]=3\\sigma^{4}\\sum_{j}H_{j j}^{2}\\Delta\\theta_{j}^{4}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combining above four cases together, we have the result for Eq. 18: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\sigma^{4}}{2}\\Big(\\sum_{j}3H_{j j}^{2}\\Delta\\theta_{j}^{4}+\\sum_{j,k\\neq j}(H_{j j}H_{k k}+2H_{j k}^{2})\\Delta\\theta_{j}^{2}\\Delta\\theta_{k}^{2}\\Big).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Eq. 19: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\frac{1}{2}\\mathbb{E}_{s,u}\\big[(\\boldsymbol{v}^{T}\\boldsymbol{H}\\boldsymbol{v})(\\boldsymbol{u}^{T}\\boldsymbol{H}\\boldsymbol{u})\\big]}\\\\ &{=-\\frac{1}{2}\\mathbb{E}_{s}\\big[(\\boldsymbol{v}^{T}\\boldsymbol{H}\\boldsymbol{v})\\big]\\mathbb{E}_{u}\\big[(\\boldsymbol{u}^{T}\\boldsymbol{H}\\boldsymbol{u})\\big]}\\\\ &{=-\\frac{1}{2}\\mathbb{E}_{v}\\big[\\displaystyle\\sum_{j}{H}_{j j}\\boldsymbol{v}_{j}^{2}\\big]\\mathbb{E}_{u}\\big[\\displaystyle\\sum_{k}{H}_{k k}\\boldsymbol{v}_{k}^{2}\\big]}\\\\ &{=-\\frac{1}{2}\\Big(\\displaystyle\\sum_{j}{H}_{j j}\\mathbb{E}[\\boldsymbol{v}_{j}^{2}]\\Big)\\Big(\\sum_{k}{H}_{k k}\\mathbb{E}[\\boldsymbol{v}_{k}^{2}]\\Big)}\\\\ &{=-\\frac{\\sigma^{4}}{2}\\Big(\\displaystyle\\sum_{j}{H}_{j j}^{2}\\Delta\\theta_{j}^{4}+\\displaystyle\\sum_{j,k\\neq j}{H}_{j j}{H}_{k k}\\Delta\\theta_{j}^{2}\\Delta\\theta_{k}^{2}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "With results of Eq. 20, 21, 27, 28, we have the final results: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{d^{\\mathrm{pace}}\\approx2\\sigma^{2}\\sum_{j}\\Delta\\theta_{j}^{2}\\nabla_{j}^{2}+0}}\\ ~}\\\\ {{\\displaystyle\\qquad+\\frac{\\sigma^{4}}{2}\\biggl(\\sum_{j}3H_{j j}^{2}\\Delta\\theta_{j}^{4}+\\sum_{j,k\\neq j}(H_{j j}H_{k k}+2H_{j k}^{2})\\Delta\\theta_{j}^{2}\\Delta\\theta_{k}^{2}-\\sum_{j}H_{j j}^{2}\\Delta\\theta_{j}^{4}-\\sum_{j,k\\neq j}H_{j j}H_{k k}\\Delta\\theta_{j}^{2}\\Delta\\theta_{k}^{2}\\biggr)}}\\\\ {{\\displaystyle\\qquad=2\\sigma^{2}\\sum_{j}\\Delta\\theta_{j}^{2}\\nabla_{j}^{2}+\\sigma^{4}\\biggl(\\sum_{j}H_{j j}^{2}\\Delta\\theta_{j}^{4}+\\sum_{j,k\\neq j}H_{j k}^{2}\\Delta\\theta_{j}^{2}\\Delta\\theta_{k}^{2}\\biggr)}}\\\\ {{\\displaystyle\\qquad=2\\sigma^{2}\\sum_{j}\\Delta\\theta_{j}^{2}\\nabla_{k}^{2}+\\sigma^{4}\\sum_{j,k}H_{j k}^{2}\\Delta\\theta_{j}^{2}\\Delta\\theta_{k}^{2}=2\\sigma^{2}\\lVert\\Delta\\theta\\odot\\nabla\\rVert_{2}^{2}+\\sigma^{4}\\lVert(\\Delta\\theta\\Delta\\theta^{T})\\odot H\\rVert_{F}^{2}\\ \\ (29)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.3 Proof of Theorem 3 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The Cauchy-Schwarz inequality states that for $\\pmb{u},\\pmb{v}\\in\\mathbb{R}^{d}$ , we have $\\begin{array}{r}{(\\sum_{j}u_{j}v_{j})^{2}\\le(\\sum_{j}u_{j}^{2})(\\sum_{j}v_{j}^{2})}\\end{array}$ . Let ${\\boldsymbol{u}}={\\bf1}$ , it follows that $\\begin{array}{r}{(\\sum_{j}v_{j})^{2}\\leq d\\|\\pmb{v}\\|_{2}^{2}}\\end{array}$ . Using this inequality, we then prove the following: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~\\displaystyle[\\Delta\\theta^{T}\\nabla-\\frac{1}{2}\\Delta\\theta^{T}\\boldsymbol{H}\\Delta\\theta]^{2}\\leq2[\\Delta\\theta^{T}\\nabla]^{2}+[\\Delta\\theta^{T}\\boldsymbol{H}\\Delta\\theta]^{2}}\\\\ &{\\quad\\quad\\quad[\\Delta\\theta^{T}\\nabla]^{2}=\\Big(\\displaystyle\\sum_{j}\\Delta\\theta_{j}\\nabla_{j}\\Big)^{2}\\leq d\\|\\Delta\\theta\\odot\\nabla\\|_{2}^{2}}\\\\ &{\\displaystyle[\\Delta\\theta^{T}\\boldsymbol{H}\\Delta\\theta]^{2}=\\Big(\\displaystyle\\sum_{j,k}\\Delta\\theta_{j}\\Delta\\theta_{k}\\boldsymbol{H}_{j k}\\Big)^{2}\\leq d^{2}\\big\\|(\\Delta\\theta\\Delta\\theta^{T})\\odot\\boldsymbol{H}\\big\\|_{F}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here, the inequality is obtained by treating $\\Delta\\theta_{j}\\Delta\\theta_{k}H_{j k}$ as an element of a vector with size of $d^{2}$ .   \nThis leads to the final results. ", "page_idx": 17}, {"type": "text", "text": "B.4 Rationale for one-dimensional output analysis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "we use the squared $L_{2}$ distance for multi-dimensional outputs for $D^{\\mathrm{fp}}$ and $D^{\\mathrm{pace}}$ , which allows our one-dimensional analysis to naturally generalize to multiple dimensions. For example, for a vector-valued function in the naive alignment, $f(\\pmb\\theta)=[f_{1}(\\pmb\\theta),...,f_{m}(\\pmb\\theta)]$ , where $m$ is the output dimension, we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|f(\\pmb\\theta_{0})-f(\\pmb\\theta_{0}+\\Delta\\pmb\\theta)\\|_{2}^{2}=\\sum_{i=1}^{m}[f_{i}(\\pmb\\theta_{0})-f_{i}(\\pmb\\theta_{0}+\\Delta\\pmb\\theta)]^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This equality shows that the squared $L_{2}$ distance in multiple dimensions is simply the sum of nonnegative squared differences in each dimension. Consequently, this additive nature enables our one-dimensional analysis to extend seamlessly to multiple dimensions in practice, aligning with our empirical observations. ", "page_idx": 17}, {"type": "text", "text": "C Efficient PACE variants ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Building upon strong theoretical foundation of PACE for generalization, we demonstrate that simple modifications can reduce its memory and training time requirements. In this section, we explore two efficient variants: $\\mathrm{PACE_{\\mathrm{fast}}}$ and $\\bar{\\bf P}\\mathrm{ACE_{\\mathrm{lazy}}^{\\mathrm{half}}}$ , both maintaining similar computational and memory requirements as the baseline while improving performance. We then provide empirical results show that $\\mathrm{PACE_{\\mathrm{fast}}}$ slightly outperforms $\\mathrm{PACE_{lazy}^{h a l f}}$ while requiring no additional hyperparameters and fewer computational resources. Given its superior efficiency, we further explore the potential of $\\mathrm{PACE_{\\mathrm{fast}}}$ for resource-efficient fine-tuning. By simply reducing batch size and epochs, $\\mathrm{PACE_{\\mathrm{fast}}}$ outperforms the baseline while using significantly less GPU memory and training time. ", "page_idx": 17}, {"type": "text", "text": "$\\mathbf{P\\mathbf{ACE_{fast}}}$ : Building on the observation that only small datasets are typically available for fine-tuning, we assume model behavior changes gradually across epochs. Under this assumption, we store the model outputs from the previous epoch $(f_{e-1}(x))$ , which contain inherent noise due to adapter perturbation, and compute the consistency regularization loss between these stored outputs and the current epoch\u2019s noised outputs: ", "page_idx": 17}, {"type": "equation", "text": "$$\nd_{\\mathrm{fast}}^{\\mathrm{pace}}(\\pmb{x})=\\|\\b{f}(\\pmb{x})-\\pmb{o_{e-1}}\\|_{2}^{2};\\quad\\mathrm{where}\\quad\\pmb{o_{e-1}}=f_{e-1}(\\pmb{x})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here the output vector $\\pmb{o}\\in\\mathbb{R}^{C}$ where $C$ is the number of classes. Since $f$ applies noise perturbation to the adapter and changes gradually between epochs, $f_{e-1}(x)$ and $f(x)$ can be seen as applying different i.i.d. noises to similar model states. This approach preserves the theoretical foundation of PACE while incurring minimal storage and computation costs. With typically few classes $C$ and limited samples in fine-tuning, storing $o_{e-1}$ is manageable within GPU or CPU memory. ", "page_idx": 17}, {"type": "text", "text": "$\\mathbf{PACE_{lazy}^{h a l f}}$ : During training, the network always applies noise perturbations. Every $N$ -th iteration uses a half batch size and consistency regularization, while all other iterations use the full batch size. ", "page_idx": 17}, {"type": "text", "text": "Memory and computational efficiency of two variants. Both variants maintain similar computational and memory requirements as the baseline. To demonstrate this, we conduct experiments on CIFAR-100 (VTAB-1K) using ViT-16/B, Camelyon (VTAB-1K) with Swin-B, and ImageNet (domain adaptation) with ViT-16/B. Table 9 compares maximum GPU memory usage, total training time, and accuracy for each task, showing that $\\mathrm{PACE_{\\mathrm{fast}}}$ and $\\mathrm{{PACE_{\\mathrm{{lazy}}}^{\\mathrm{{half}}}}}$ significantly improve upon the baseline while maintaining similar computational demands. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "We find that $\\mathrm{PACE_{\\mathrm{fast}}}$ slightly outperforms $\\mathrm{{PACE_{\\mathrm{{lazy}}}^{\\mathrm{{half}}}}}$ without requiring additional hyperparameters, yet it needs to store outputs from the previous epoch. We therefore analyze its memory requirements. ", "page_idx": 18}, {"type": "table", "img_path": "cOuLbPhOT1/tmp/8372bede9c95312e3d78519e129bf07bb46e446ab5e68b09db9c89120cf23fe9.jpg", "table_caption": ["Table 9: GPU memory usage, training time, and accuracy for $\\mathrm{PACE_{\\mathrm{fast}}}$ and $\\mathrm{{PACE_{\\mathrm{{lazy}}}^{\\mathrm{{half}}}}}$ . \u2018m\u2019 denotes minutes, Both variants outperform the baseline while maintaining similar computational demands. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Memory efficiency of $\\mathbf{P}\\mathbf{ACE_{fast}}$ . We compare the additional memory requirement of $\\mathrm{PACE_{\\mathrm{fast}}}$ with baseline GPU memory consumption. Table 10 shows that the memory overhead of $\\mathrm{PACE_{\\mathrm{fast}}}$ is negligible compared to baseline GPU memory requirements and can be easily stored in GPU. Moreover, even in the rare scenario of fine-tuning on the full ImageNet 1K dataset (1.2 million samples), $\\mathrm{PACE_{\\mathrm{fast}}}$ requires only 4.8GB of additional memory for storing the output of the model\u2019s classification head. This is significantly smaller than the dataset itself $(>\\!100\\mathrm{GB})$ and can be easily accommodated in CPU/GPU memory. ", "page_idx": 18}, {"type": "table", "img_path": "cOuLbPhOT1/tmp/e26b466b8bc789428d8bd4f853119102cd2e0e72eb8b3ddd08d831a4d2ad450e.jpg", "table_caption": ["Table 10: Comparison of $\\mathrm{PACE}_{\\mathrm{fast}}$ memory overhead and baseline GPU memory requirements. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Resource-Efficient training with $\\mathbf{P}\\mathbf{ACE_{fast}}$ . Given the superior performance, minimal memory overhead, and no need for additional hyperparameters of $\\mathrm{PACE_{\\mathrm{fast}}}$ , we explore its potential for resource-efficient training by maintaining the same number of updates with reduced batch size and proportionally reduced epochs. Table 11 shows that even with 1/8 batch size and epochs, $\\mathrm{PACE_{\\mathrm{fast}}}$ still outperforms the baseline by $1.7\\%$ while only using ${\\sim}1/3$ GPU memory and ${\\sim}1/4$ training time. This demonstrates the robustness and generalization beneftis that $\\mathrm{PACE_{\\mathrm{fast}}}$ brings to models, enabling them to excel under constrained training configurations. Such efficiency is particularly valuable for fine-tuning large foundation models, where resource constraints necessitate small batch sizes and typically lead to sharp loss landscapes, yet the theoretical guarantee of PACE for smooth loss landscapes provides a promising solution for these challenges. ", "page_idx": 18}, {"type": "table", "img_path": "cOuLbPhOT1/tmp/c8ddf81437ab4ca4cd4702b3a33f4e224a1093d2f7dc1e79fe8afa1f12362976.jpg", "table_caption": ["Table 11: Results of $\\mathrm{PACE_{\\mathrm{fast}}}$ with reduced batch size and epochs on CIFAR-100 (VTAB-1K w/ ViT-16/B), Camelyon (VTAB-1K w/ Swin-B), ImageNet (Domain adaptaion w/ ViT-16/B). $\\mathrm{PACE_{\\mathrm{fast}}}$ outperforms baseline while using less GPU memory and training time. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "cOuLbPhOT1/tmp/8f6911f25bca98accdac64e01af713ef45679c7754d2f885530d3827d819cd8c.jpg", "table_caption": ["Table 12: Classification results for different methods on VTAB-1K with different training epochs. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "D Additional Experiments ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we provide additional experiments of PACE on VTAB-1K with different epochs, varying training data sizes on FGVC benchmarks, self-supervised pretrained backbones and combinations with other PEFT methods. ", "page_idx": 19}, {"type": "text", "text": "D.1 Experiments of VTAB-1K with different epochs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Table 1, We use 300 epochs for VTAB-1K tasks as we observed slight improvements over 100 epochs. However, this does not mean PACE requires longer training to converge. Since the optimizer uses cosine learning rate decay, reducing the number of training epochs to 100 has minimal impact on performance, as shown in Table 12. ", "page_idx": 19}, {"type": "text", "text": "To ensure fair memory and computational budgets, we also tested PACE with half the batch size and 50 epochs. Table 12 shows that under these conditions, PACE still improves baseline accuracy by $2.10\\%$ , and outperforms the previous SOTA GLoRA, which uses 500 epochs for training and 30 for parameter search. These results demonstrate PACE\u2019s efficiency and effectiveness across various training configurations. ", "page_idx": 19}, {"type": "text", "text": "D.2 Experiments on FGVC with limited training data ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To validate generalization benefits of PACE on limited data settings, we conduct experiments on FGVC using $50\\%$ , $20\\%$ , and $10\\%$ of the original training samples. Table 13 shows that PACE achieves larger improvements with smaller data sizes, aligning with our theoretical analyses. ", "page_idx": 19}, {"type": "table", "img_path": "cOuLbPhOT1/tmp/55d43999c2a3c51f5f1fd46307ba853edeb7b0e351b555d2e5057bcaa5ec0a32.jpg", "table_caption": ["Table 13: Classification results on FGVC using varying percentages of data based on ViT-16/B. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "D.3 Experiments on self-supervised pre-trained backbones ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To further verify the effectiveness of PACE on a self-supervised pre-trained backbone, we conduct VTAB-1K experiments on SVHN, Camelyon, and Clevr-Count using MAE [18] and DINO [18], with ViT-16/B pretrained on ImageNet-1K [11]. Table 14 shows that PACE improves the baseline on these self-supervised backbones, confirming its applicability to fine-tuning self-supervised models. ", "page_idx": 19}, {"type": "table", "img_path": "cOuLbPhOT1/tmp/66f0a8847f44812db8d57c591324e780b6732440a24e06280de3974de4fed47f.jpg", "table_caption": ["Table 14: Classification results on VTAB-1K using self-supervised DINO and MAE, with ViT-16/B pretrained on the ImageNet-1K dataset. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "D.4 Experiments of Combining PACE with Other PEFT ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We conducted experiments combining PACE with several PEFT methods, including AdaptFormer [8], GLoRA [7], COFT [53], and BOFT [41], on CIFAR-100 (VTAB-1K) and ImageNet (domain adaptation) using ViT-16/B. Table 15 shows that integrating PACE improves the baseline performance. ", "page_idx": 20}, {"type": "table", "img_path": "cOuLbPhOT1/tmp/d081101117b7d3e74347300e872c057a7939ae8b3c0be85d397a20cb81583fe3.jpg", "table_caption": ["Table 15: Classification results of different PEFT methods based on ViT-16/B "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "E Additional Plots ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Figures 8 and 9 show the gradient issues in FPA and the gradient regularization effects of PACE. ", "page_idx": 20}, {"type": "image", "img_path": "cOuLbPhOT1/tmp/d96aa95820cc02deae8ccba116f5b191e4e2232991e14ae46e92956658f9343e.jpg", "img_caption": ["Figure 8: Gradient norms of (a) FPA and (b) PACE with different regularization strengths $\\lambda$ during training on CIFAR-100 (VTAB-1K) w/ ViT-B/16. Figure 5 illustrates the average gradient norm over training epochs. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "cOuLbPhOT1/tmp/7e5c920cd817604b6c9438ebfbbc92c108bc15fa2507632478e518d8dbb6930e.jpg", "img_caption": ["Figure 9: Gradient norms of models across wide range of regularization strengths $\\lambda$ on Camelyon (VTAB-1K) w/ Swin-B. Line and shadow represent mean and std over training epochs. While gradient explosion is less frequent for FPA in this setting, it exhibits unpredictable gradient norm with varied regularization strengths. In contrast, PACE reliably lowers gradient norms as regularization strength $\\lambda$ increases, demonstrating its robustness for effective gradient control. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "F Hyperparameter settings ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For each dataset, we follow strategies from previous works [40, 27, 7, 44] to apply grid search on the rank, learning rate and weight decay to establish strong baselines. Table 16, 17, 18 and 19 present the hyperparameters and number of trainable parameters used in our strong baseline for VTAB-1K, few-shot learning, FGVC and domain adaptation tasks. ", "page_idx": 21}, {"type": "text", "text": "With these strong baselines, we apply grid search on $\\lambda\\,\\in\\,\\{0.02,0.05,0.1,0.2,0.5,1\\}$ and $\\sigma\\ \\in$ $\\{0.1,0.5,1,1.5,2\\}$ for PACE to optimize its performance. ", "page_idx": 21}, {"type": "table", "img_path": "cOuLbPhOT1/tmp/64c484dcce643d658d2b38008539e1843f9c05c6897d89c62494335e852931ff.jpg", "table_caption": ["Table 16: Hyperparameters for baseline on VTAB-1K with ViT-B/16. A: $\\mathrm{LoRA_{mul}+V P T_{a d d}}$ , B: $\\mathrm{LoRA}_{\\mathrm{add}}$ . lr: learning rate. WD: weight decay. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "cOuLbPhOT1/tmp/b7ed01ab5b893f08dc5240f44b2701a0846c47655ce6a849cd9479a8f532e836.jpg", "table_caption": ["Table 17: Ranks for baselines in Few-shot learning. Weight decay is fixed at 1e-4. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "cOuLbPhOT1/tmp/7108dba220f6b425b86c32ad2018481d7c3028c7735853060bfaf6afbd9e87b8.jpg", "table_caption": ["Table 18: Hyperparameters for the baseline $\\mathrm{LoRA_{mul}+V P T_{a d d}}$ in FGVC "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "cOuLbPhOT1/tmp/28dd79818974919d763eb9e4bfc521ca7a4c45ff4da59e8dc30d660884bdab51.jpg", "table_caption": ["Table 19: Hyperparameters for baseline $\\mathrm{LoRA_{mul}+V P T_{a d d}}$ in domain adaptation. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "G Experiment details for GSM-8K ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We conduct experiments on text generation tasks by fine-tuning Phi-3-mini- $.4\\mathbf{k}$ -instruct [1] on the GSM-8K [9] dataset using causal language modeling. We use learning rate of 2e-6, batch size of 4, LoRA rank of 16, prompt \"Answer below question. First think step-by-step and then answer the final number: $\\mathsf{u n d e Q}$ uestion>\" as instruction and fine-tune models on the training set and evaluated the performance on the test set. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We theoretically and empirically verify the claims and contributions made in the abstract and introduction. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The limitations of our work are discussed in \u00a7A ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Complete proofs for each theorem are provided in $\\S B$ . Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: Training details and hyperparameter selection are presented in Sec. 4 and $\\S\\mathrm{F}$ , respectively. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We will release our code. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Experimental settings and details are presented in Sec. 4. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: All reported results are averaged over three random seeds. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All experiments were conducted on a single NVIDIA H100 GPU with 96 GB memory, with each experiment completing within 8 hours. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] , ", "page_idx": 25}, {"type": "text", "text": "Justification: We have carefully reviewed and adhered to the code of ethics throughout our research and writing process. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Potential impacts are discussed in $\\S\\mathrm{A}$ . ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 26}, {"type": "text", "text": "Justification: Our work poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: All publicly available assets (models, code, and data) used in this work have been properly credited, and their respective licenses and terms of use have been explicitly mentioned and adhered to. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: We do not release new assets in the submission. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]