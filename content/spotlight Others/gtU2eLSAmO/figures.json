[{"figure_path": "gtU2eLSAmO/figures/figures_2_1.jpg", "caption": "Figure 1: Brain-JEPA. With a Vision Transformer (ViT) as the observation encoder fe, Brain-JEPA employs a single observation block to predict the representations of target blocks. (1) The input fMRI data is initially segmented into patches for subsequent processing. (2) Through Spatiotemporal Masking, the input data\u2014excluding the observation block\u2014is divided into three distinct regions: Cross-ROI (\u03b1), Cross-Time (\u03b2), and Double-Cross (\u03b3). The target blocks are sampled from different regions separately. (3) A narrower ViT, serving as the predictor g\u03b8, takes the output s x from f\u03b8. It predicts the representations of target blocks conditioned on positional embedding (brain gradient positioning for ROI locations and sine and cosine functions for temporal positioning). (4) These predicted representations align with those s y from the target encoder f\u03b8, whose parameters are incrementally updated through an Exponential Moving Average (EMA) of the observation encoder\u2019s parameters.", "description": "Brain-JEPA uses a Vision Transformer as its encoder.  The input fMRI data is divided into patches, and spatiotemporal masking divides the data into three regions (cross-ROI, cross-time, double-cross).  A smaller ViT predictor network uses the observation block to predict the representations of target blocks from these regions.  The target encoder's parameters are updated using an exponential moving average of the observation encoder's parameters.", "section": "3 Method"}, {"figure_path": "gtU2eLSAmO/figures/figures_3_1.jpg", "caption": "Figure 2: Brain gradient positioning. Brain cortical regions are situated in the top 3 gradient axes and colored based on their positions. These colors are then projected back into the brain surface.", "description": "This figure shows how Brain Gradient Positioning works.  Panel (A) displays the brain surface, where different cortical regions are colored according to their positions in a three-dimensional gradient space. Panel (B) shows the three-dimensional gradient space itself, with each point representing a brain region and its position defined by three gradient axes. The color coding in (A) and (B) is consistent, illustrating the mapping between the brain's functional organization and the gradient space representation. The gradient axes are derived from the functional connectivity between brain regions, capturing their relationships and forming a functional coordinate system for brain activity analysis.", "section": "3.1 Brain Gradient Positioning"}, {"figure_path": "gtU2eLSAmO/figures/figures_7_1.jpg", "caption": "Figure 3: Performance scaling of the model sizes.", "description": "This figure shows the performance of Brain-JEPA with different model sizes (ViT-S, ViT-B, and ViT-L) on three downstream tasks: age prediction, sex prediction, and NC/MCI classification.  The results demonstrate that larger model configurations consistently achieve better performance.  The x-axis represents the model size, and the y-axis represents the performance metric (Pearson correlation for age prediction and accuracy for sex prediction and NC/MCI classification).", "section": "4.4 Performance scaling"}, {"figure_path": "gtU2eLSAmO/figures/figures_7_2.jpg", "caption": "Figure 3: Performance scaling of the model sizes.", "description": "The figure shows the performance of Brain-JEPA with different model sizes (ViT-S, ViT-B, and ViT-L) across three downstream tasks: age prediction, sex prediction, and amyloid classification.  It demonstrates that larger models generally achieve better performance, indicating a positive scaling property with model size. The x-axis represents the model size while the y-axis represents the performance metrics.  Specific metrics shown are Pearson correlation for age prediction, accuracy for sex prediction, and accuracy for amyloid classification. ", "section": "4.4 Performance scaling"}, {"figure_path": "gtU2eLSAmO/figures/figures_7_3.jpg", "caption": "Figure 5: Comparisons of spatial positional embedding (For the first task, refer to the left axis for the Pearson's Correlation, with the right y axis accuracy for the last two tasks).", "description": "This figure compares three different methods for spatial positional embedding in the Brain-JEPA model: sine and cosine functions, anatomical locations, and brain gradient positioning. The results show that brain gradient positioning achieves significantly better performance across three downstream tasks: age prediction, sex prediction, and NC/MCI classification. This highlights the effectiveness of brain gradient positioning in capturing functional relationships between brain regions.", "section": "4.6 Ablation study"}, {"figure_path": "gtU2eLSAmO/figures/figures_8_1.jpg", "caption": "Figure 3: Performance scaling of the model sizes.", "description": "This figure shows the performance of Brain-JEPA across various model sizes (ViT-S, ViT-B, and ViT-L).  The results demonstrate that larger model configurations consistently achieve better performance, with a clear trend of increasing accuracy/correlation with larger models.  The largest model (ViT-L) consistently achieves the best performance across age prediction, sex prediction, and NC/MCI classification tasks.", "section": "4.4 Performance scaling"}, {"figure_path": "gtU2eLSAmO/figures/figures_8_2.jpg", "caption": "Figure 7: Attention across different brain networks for NC/MCI classification.", "description": "This figure displays the attention weights across seven different brain networks (CN, DMN, DAN, LN, SAN, SMN, VN) for NC/MCI classification in both Caucasian and Asian populations.  The bar graphs show the average attention weights for each network in each group, while the brain image displays the spatial distribution of attention weights across the ROIs, color-coded according to the network they belong to.  The results highlight the consistent patterns across different ethnic groups, emphasizing the critical roles of several networks (DMN, CN, SAN, and LN) in cognitive impairment. ", "section": "4.7 Interpretation"}]