[{"figure_path": "5G7MRfPngt/tables/tables_7_1.jpg", "caption": "Table 1: Evaluation on TEACh unseen validation set. All evaluations are done using GPT3.5-turbo-1106 unless otherwise noted. Visual Demos = demonstrations labeled with inverse dynamics model. Kinesthetic Demos = demos labeled with GT actions. GC = goal-condition success", "description": "This table presents the results of the evaluation of the ICAL method and several baseline methods on the unseen validation set of the TEACh benchmark.  The table shows the success rate (SR) and goal condition success rate (GC) for various methods, including the ICAL method with and without certain components (such as the abstraction and human-in-the-loop phases). Baselines include HELPER (using hand-written examples), zero-shot chain of thought (CoT), and raw visual and kinesthetic demonstrations.  The results are broken down into those using ground truth segmentation, depth and attributes and those using estimated perception.  The results demonstrate the ICAL method's superior performance compared to the baselines.", "section": "4.2 ICAL beats written & unchanged demonstrations in household instruction following"}, {"figure_path": "5G7MRfPngt/tables/tables_7_2.jpg", "caption": "Table 2: Results in VisualWebArena. ICAL outperforms the prior best, GPT40/V + Set of Marks. Ablation studies were conducted with GPT4V on a subset of 257 episodes.", "description": "This table presents the results of the VisualWebArena experiment.  The ICAL approach is compared against the state-of-the-art method, GPT40/V + Set of Marks.  The table also includes ablation studies using GPT4V to examine the impact of removing image input and using full text trajectory on the performance of ICAL.", "section": "4.3 ICAL obtains state-of-the-art performance on visual web tasks"}, {"figure_path": "5G7MRfPngt/tables/tables_7_3.jpg", "caption": "Table 3: Evaluation on the Ego4D unseen validation subset. ICAL outperforms few-shot GPT4V and matches supervised baselines using 639x less in-domain data.", "description": "This table presents the results of the Ego4D experiment, comparing ICAL's performance against few-shot and zero-shot GPT4V and a supervised baseline.  The key metric is Edit Distance (ED) at Z=20, measuring the difference between predicted and ground truth action sequences.  ICAL shows improvement over few-shot GPT4V, and is competitive with a supervised model trained on substantially more data.", "section": "4.4 ICAL outperforms few-shot VLMs on egocentric video action forecasting"}, {"figure_path": "5G7MRfPngt/tables/tables_18_1.jpg", "caption": "Table 2: Results in VisualWebArena. ICAL outperforms the prior best, GPT4V + Set of Marks. Ablation studies were conducted with GPT4V on a subset of 257 episodes.", "description": "This table presents the results of the VisualWebArena experiment.  The ICAL method is compared to the previous state-of-the-art (GPT4V + Set of Marks) and ablation studies using GPT4V are performed on a smaller dataset to analyze the effect of each component.  The table shows a significant improvement in performance by the ICAL method compared to the baseline.", "section": "4.3 ICAL obtains state-of-the-art performance on visual web tasks"}, {"figure_path": "5G7MRfPngt/tables/tables_18_2.jpg", "caption": "Table S2: Tasks successfully completed after applying the ICAL method (out of 250). We compare ICAL using either visual demonstrations or kinesthetic demonstrations. Kinesth. = Kinesthetic; demos with GT actions. Visual = action labeled from RGB frames with inverse dynamics model.", "description": "This table compares the number of tasks successfully completed by ICAL using either visual demonstrations (actions labeled using an inverse dynamics model) or kinesthetic demonstrations (actions with ground truth labels).  The results are broken down by task type and show how the use of accurate action labels improves ICAL's performance.", "section": "S4.3 TEACh results on ICAL learning using trajectories with ground truth action labels and GPT3.5"}, {"figure_path": "5G7MRfPngt/tables/tables_19_1.jpg", "caption": "Table 1: Evaluation on TEACh unseen validation set. All evaluations are done using GPT3.5-turbo-1106 unless otherwise noted. Visual Demos = demonstrations labeled with inverse dynamics model. Kinesthetic Demos = demos labeled with GT actions. GC = goal-condition success", "description": "This table presents the results of evaluating different methods on the unseen validation set of the TEACh benchmark.  The methods compared include using hand-written examples from the state-of-the-art HELPER model, zero-shot chain of thought prompting, raw visual demonstrations with predicted actions, raw kinesthetic demonstrations with ground truth actions, and the proposed ICAL method.  The evaluation metrics are task success rate (SR) and goal-condition success rate (GC).  The table highlights the improvements achieved by ICAL compared to other methods, particularly in goal-condition success rate.", "section": "4.2 ICAL beats written & unchanged demonstrations in household instruction following"}, {"figure_path": "5G7MRfPngt/tables/tables_19_2.jpg", "caption": "Table 1: Evaluation on TEACh unseen validation set. All evaluations are done using GPT3.5-turbo-1106 unless otherwise noted. Visual Demos = demonstrations labeled with inverse dynamics model. Kinesthetic Demos = demos labeled with GT actions. GC = goal-condition success", "description": "This table presents the results of an experiment evaluating the performance of different methods on the unseen validation set of the TEACh dataset.  It compares the success rate (SR) and goal condition success rate (GC) of several approaches:  HELPER (hand-written examples), zero-shot chain of thought, raw visual demonstrations (with predicted and true actions), and the ICAL method. The table helps to show the improvement achieved by ICAL over baseline methods.", "section": "4.2 ICAL beats written & unchanged demonstrations in household instruction following"}, {"figure_path": "5G7MRfPngt/tables/tables_20_1.jpg", "caption": "Table 1: Evaluation on TEACh unseen validation set. All evaluations are done using GPT3.5-turbo-1106 unless otherwise noted. Visual Demos = demonstrations labeled with inverse dynamics model. Kinesthetic Demos = demos labeled with GT actions. GC = goal-condition success", "description": "This table presents the results of evaluating different methods on the unseen validation set of the TEACh benchmark for household instruction following.  It compares the performance of ICAL against several baselines, including hand-written examples from HELPER (a state-of-the-art method), zero-shot chain-of-thought prompting, and methods using raw visual or kinesthetic demonstrations. The metrics used are task success rate (SR) and goal-condition success rate (GC), showing the percentage of tasks completed successfully and the percentage of tasks that partially fulfilled the instructions, respectively.", "section": "4.2 ICAL beats written & unchanged demonstrations in household instruction following"}, {"figure_path": "5G7MRfPngt/tables/tables_20_2.jpg", "caption": "Table S6: Attribute detection accuracy in TEACh for different open-source VLMs. We find CogVLM currently outperforms the other open-source VLMs at posed and unposed attribute detection for object crops.", "description": "This table presents a comparison of the accuracy of different open-source Vision-Language Models (VLMs) in detecting attributes of objects in images from the TEACh dataset. The models were evaluated on both \"clean\" (posed, unoccluded) and \"random\" (various angles, potentially occluded) viewpoints.  CogVLM demonstrates superior performance across both viewpoints.", "section": "S4.7 Benchmarking open-source VLMs for attribute detection in TEACH"}]