[{"figure_path": "W8rFsaKr4m/tables/tables_6_1.jpg", "caption": "Table 1: Image classification performance on the ImageNet-1K validation set. T, C and S indicate the model type of Transformer, CNN and SSM, respectively. All models take a scale of 2242 as input.", "description": "This table presents a comparison of different model architectures (Transformer, CNN, and State Space Model) on the ImageNet-1K image classification task.  The table shows the Top-1 accuracy achieved by each model, along with the number of parameters and FLOPs (floating-point operations). This allows for a comparison of model performance relative to their computational cost and architecture.", "section": "4.1 Image Classification"}, {"figure_path": "W8rFsaKr4m/tables/tables_7_1.jpg", "caption": "Table 2: Semantic segmentation performance on ADE20K val set. The crop size is all set to 5122. SS and MS denote single-scale and multi-scale testing, respectively.", "description": "This table presents the results of semantic segmentation experiments on the ADE20K validation set.  The models were evaluated using two different testing strategies: single-scale (SS) and multi-scale (MS).  The results show the mean Intersection over Union (mIoU) for each model and testing strategy, allowing for comparison of performance across different models and testing approaches.  The crop size for all models was 512x512 pixels.", "section": "4.3 Semantic Segmentation"}, {"figure_path": "W8rFsaKr4m/tables/tables_8_1.jpg", "caption": "Table 3: Evaluation on language model benchmarks. Arc-E, WG, L-ppl and BQA indicate Arc-easy [8], WinoGrande, LAMBADA [49] and Openbookqa [47] benchmark, respectively.", "description": "This table presents the performance comparison of three different language models on several benchmark datasets.  The first model is the baseline Mamba model. The second adds LoRA fine-tuning. The third is the proposed MambaTreeL model. The benchmarks cover various aspects of language understanding, including commonsense reasoning, knowledge-based question answering, and reading comprehension.  The results show that the proposed MambaTreeL model achieves the best average accuracy across all benchmarks, indicating improvements over both the baseline and the LoRA-tuned Mamba model. ", "section": "4.4 Language Understanding"}, {"figure_path": "W8rFsaKr4m/tables/tables_8_2.jpg", "caption": "Table 1: Image classification performance on the ImageNet-1K validation set. T, C and S indicate the model type of Transformer, CNN and SSM, respectively. All models take a scale of 2242 as input.", "description": "This table compares the performance of MambaTreeV with other state-of-the-art image classification models on the ImageNet-1K dataset.  The models are categorized by their type (Transformer, CNN, or State Space Model) and size, allowing for a performance comparison across different architectures and scales. The Top-1 accuracy and number of parameters/FLOPs are provided for each model.", "section": "4.1 Image Classification"}, {"figure_path": "W8rFsaKr4m/tables/tables_9_1.jpg", "caption": "Table 7: Runtime comparison on an Nvidia V100 GPU during inference.", "description": "This table compares the inference throughput, GPU memory usage, FLOPS, number of parameters, and top-1 accuracy of different state space models, including PlainMamba-L2, VMamba-T, LocalVMamba-T, and three variants of MambaTreeV-T on an Nvidia V100 GPU.  The variants of MambaTreeV-T represent different optimization strategies, showing the impact of architectural choices on performance.  The table highlights that MambaTreeV-T*, a variant with shared tree topology structures across stages, achieves the highest throughput while maintaining high accuracy.", "section": "4 Experiments"}, {"figure_path": "W8rFsaKr4m/tables/tables_15_1.jpg", "caption": "Table 8: Object detection and instance segmentation performance on COCO val2017. APb and APm indicate the mAP of detection and segmentation, respectively. MS indicates the multi-scale training strategy.", "description": "This table presents a comparison of different object detection and instance segmentation methods on the COCO 2017 validation set.  The results are broken down by various metrics including Average Precision (AP), AP at different Intersection over Union (IoU) thresholds (AP50, AP75), and average precision for masks (APm).  The table also differentiates between results obtained using a single-scale training schedule (1x) and a multi-scale training schedule (3x MS).  The performance of MambaTreeV is highlighted in comparison to other state-of-the-art methods.", "section": "4.2 Object Detection"}, {"figure_path": "W8rFsaKr4m/tables/tables_18_1.jpg", "caption": "Table 9: Standard error on language model benchmarks. LAM-ppl indicates LAMBADA [49].", "description": "This table presents the standard error for the MambaTreeL model on various language model benchmark datasets.  The benchmarks include PIQA, Arc-Easy, SST, WinoGrande, LAMBADA (indicated as LAM-ppl), Race, and Openbookqa. The standard error values represent the variability in the model's performance across different runs or datasets.", "section": "4.5 Ablation Study & Qualitative Results"}]