[{"figure_path": "105ZuvpdyW/tables/tables_2_1.jpg", "caption": "Table 1: The different settings and functions of SAM-like interactive segmentation methods.", "description": "This table compares several state-of-the-art interactive segmentation methods similar to Segment Anything (SAM).  It contrasts their image domain (natural vs. medical), dimensionality (2D vs. 3D), training approach (full parameter training, decoder only, adapter), and the types of prompts they support (point, bounding box, text). Finally, it shows the input resolution used during inference.", "section": "2 Methodology"}, {"figure_path": "105ZuvpdyW/tables/tables_5_1.jpg", "caption": "Table 2: Quantitative comparative experiment results for SegVol and other 5 SAM-like interactive segmentation methods settings in terms of the median value of Dice score.", "description": "This table presents a quantitative comparison of SegVol's performance against five other similar interactive segmentation methods across various anatomical structures and datasets.  The comparison is based on the median Dice score, a common metric for evaluating segmentation accuracy.  Lower Dice scores indicate less accurate segmentations.  The table highlights SegVol's superior performance compared to the other methods across multiple datasets and anatomical categories.", "section": "3 Experiments"}, {"figure_path": "105ZuvpdyW/tables/tables_6_1.jpg", "caption": "Table 3: Ablation experiment on the zoom-out-zoom-in mechanism.", "description": "This table presents the ablation study results on the zoom-out-zoom-in mechanism. Three different mechanisms are compared: Resize, Sliding window, and Zoom-out-zoom-in.  The results are shown in terms of average Dice score and average time per case. The zoom-out-zoom-in mechanism is shown to achieve the best performance with a significant improvement in the Dice score compared to the other methods. ", "section": "3.3 Ablation Studies"}, {"figure_path": "105ZuvpdyW/tables/tables_14_1.jpg", "caption": "Table 4: Information of datasets involved in supervised fine-tuning and experiments.", "description": "This table lists 25 datasets used for training and evaluating the SegVol model.  Each dataset is identified, along with the anatomical targets it contains (organs or tissues), the number of categories present, and the number of training volumes available for each dataset.  The datasets cover various body regions and include both organs and lesions.", "section": "2.1 Dataset Construction"}, {"figure_path": "105ZuvpdyW/tables/tables_16_1.jpg", "caption": "Table 5: Availability of datasets involved in supervised fine-tuning and experiments.", "description": "This table provides links to the 25 open-source datasets used in the paper for supervised fine-tuning and external datasets used in comparative experiments.  The table cross-references dataset names with their respective URLs, facilitating access to the data used in the study.  This is essential for reproducibility of results.", "section": "2 Methodology"}, {"figure_path": "105ZuvpdyW/tables/tables_16_2.jpg", "caption": "Table 6: Complexity comparison of popular methods.", "description": "This table compares the model complexity of SegVol and other SAM-like interactive medical image segmentation methods in terms of the total number of parameters, average MACs (Multiply-Accumulates) per case, average inference time per case, and average Dice score.  It highlights the trade-off between model size and performance, showing that SegVol achieves superior performance with a reasonable increase in complexity compared to other methods.", "section": "3.2 Compared with SAM-like Interactive Methods"}, {"figure_path": "105ZuvpdyW/tables/tables_19_1.jpg", "caption": "Table 7: The comparison of the average Dice score of SegVol and nnU-Net[22] across 3 lesion segmentation tasks.", "description": "This table compares the average Dice scores achieved by SegVol and nnU-Net on three lesion segmentation tasks: Lung Tumor, Colon Cancer, and Liver Tumor.  The Dice score is a common metric for evaluating the accuracy of image segmentation. Higher Dice scores indicate better segmentation performance. The table shows that SegVol significantly outperforms nnU-Net on all three tasks, indicating its superior performance in lesion segmentation.", "section": "3 Experiments"}, {"figure_path": "105ZuvpdyW/tables/tables_19_2.jpg", "caption": "Table 7: The comparison of the average Dice score of SegVol and nnU-Net[22] across 3 lesion segmentation tasks.", "description": "This table compares the average Dice scores achieved by SegVol and nnU-Net across three lesion segmentation tasks: lung tumor, colon cancer, and liver tumor.  The Dice score is a common metric for evaluating image segmentation performance. Higher Dice scores indicate better agreement between the model's predictions and the ground truth.", "section": "3.2 Compared with SAM-like Interactive Methods"}, {"figure_path": "105ZuvpdyW/tables/tables_26_1.jpg", "caption": "Table 9: Comparative experiment results of 3DUX-NET, SwinUNETR, nnU-Net, and SegVol on the test set of supervised fine-tuning datasets in terms of Dice score. Dice scores are displayed as 'Median values (First quartile, Third quartile)'", "description": "This table presents a comparison of the performance of four different medical image segmentation models (3DUX-NET, SwinUNETR, nnU-Net, and SegVol) on a test set of datasets.  The models were all fine-tuned on supervised datasets.  The Dice score, a common metric for evaluating segmentation accuracy, is reported for each model and each anatomical structure.  The results are presented as median values along with the first and third quartiles to show the distribution of the results.", "section": "C Additional Experimental Analysis"}, {"figure_path": "105ZuvpdyW/tables/tables_27_1.jpg", "caption": "Table 10: Comparative experiment results of SAM(Point), SAM(Bbox), SAM-MED2D, SAM-MED3D, MedSAM, and SegVol in terms of Dice score. Dice scores are displayed as 'Median values (First quartile, Third quartile). The three columns in the table are, in order, the AMOS22[44], ULS23[74], and SegTHOR[75] datasets.", "description": "This table presents a comparison of several interactive segmentation methods, including SegVol, across three different datasets (AMOS22, ULS23, and SegTHOR).  The performance is measured using the median Dice score, with first and third quartiles provided to show variability.  Each dataset focuses on different anatomical structures, allowing for a comprehensive evaluation across diverse segmentation tasks and datasets.", "section": "3.2 Compared with SAM-like Interactive Methods"}, {"figure_path": "105ZuvpdyW/tables/tables_28_1.jpg", "caption": "Table 1: The different settings and functions of SAM-like interactive segmentation methods.", "description": "This table compares several state-of-the-art interactive segmentation methods similar to Segment Anything (SAM).  It highlights key differences in their input image domain (2D or 3D), training methods, prompt types supported (points, bounding boxes, or text), and the resolution of the input used during inference.  This helps to contextualize the proposed SegVol method within the existing landscape of interactive segmentation techniques.", "section": "2 Methodology"}]