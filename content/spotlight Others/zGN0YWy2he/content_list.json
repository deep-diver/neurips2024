[{"type": "text", "text": "Scene Graph Disentanglement and Composition for Generalizable Complex Image Generation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yunnan Wang1,2 Ziqiang $\\mathbf{Li}^{1,2}$ Wenyao Zhang1,2 Zequn Zhang2,3 Baao $\\mathbf{X}\\mathbf{ie}^{2}$ Xihui Liu4 Wenjun Zeng2 Xin Jin2\u2217 ", "page_idx": 0}, {"type": "text", "text": "1Shanghai Jiao Tong University, Shanghai, China 2 Ningbo Institute of Digital Twin, Eastern Institute of Technology, Ningbo, China 3University of Science and Technology of China, Hefei, China 4The University of Hong Kong, Hong Kong, China {wangyunnan, ziqiangli}@sjtu.edu.cn jinxin@eitech.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "There has been exciting progress in generating images from natural language or layout conditions. However, these methods struggle to faithfully reproduce complex scenes due to the insufficient modeling of multiple objects and their relationships. To address this issue, we leverage the scene graph, a powerful structured representation, for complex image generation. Different from the previous works that directly use scene graphs for generation, we employ the generative capabilities of variational autoencoders and diffusion models in a generalizable manner, compositing diverse disentangled visual clues from scene graphs. Specifically, we first propose a Semantics-Layout Variational AutoEncoder (SL-VAE) to jointly derive (layouts, semantics) from the input scene graph, which allows a more diverse and reasonable generation in a one-to-many mapping. We then develop a Compositional Masked Attention (CMA) integrated with a diffusion model, incorporating (layouts, semantics) with fine-grained attributes as generation guidance. To further achieve graph manipulation while keeping the visual content consistent, we introduce a Multi-Layered Sampler (MLS) for an \u201cisolated\u201d image editing effect. Extensive experiments demonstrate that our method outperforms recent competitors based on text, layout, or scene graph, in terms of generation rationality and controllability. Code is available at https://github.com/wangyunnan/DisCo. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Text-to-image (T2I) generation with diffusion models (DMs) [1, 2] has yielded remarkable advancements [3, 4, 5] in recent years, benefiting from the developments of vision-language foundation models [6, 7, 8, 9]. However, textual conditions with linear structure struggle to delineate the intricacies of complex scenes precisely. For example, as shown in the failure cases of DALL\u00b7E 3 [3] in Figure 1 (a), given the intricate text prompt \u201cA sheep by another sheep ... a boat on the grass.\u201d, the T2I model may have difficulty accurately generating object relationships or quantities. Consequently, some studies [10, 11, 12, 13] strive to improve spatial relationship (e.g., \u201cby\u201d and \u201con\u201d) control by incorporating additional layout conditions. Nevertheless, as illustrated in the failure cases of LayoutDiffusion [10] in Figure 1 (b), layout-to-image (L2I) methods inevitably encounter challenges in representing certain non-spatial interactions, such as depicting \u201cplaying\u201d within spatial topology. ", "page_idx": 0}, {"type": "text", "text": "To efficiently depict complex scenes for guiding generative models, recent methods [14, 15, 16] utilize structured scene graphs as conditions instead of text or layout prompts. Scene graphs [17] ", "page_idx": 0}, {"type": "image", "img_path": "zGN0YWy2he/tmp/9982d9ee2eeb7fb75a35a09ca7d79dd38b84338784ffb20ed5843d0dac0adaca.jpg", "img_caption": ["Relationship and Quantity Confusion of T2I ", "(c) R3CD (middle) and Ours (right). ", "Non-Spatial Interaction Dilemma of L2I ", "(d) Results w/o (middle) and w/ (right) AC. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Failure cases generated by (a) text-to-image (T2I) (DALL\u00b7E 3 [3]), (b) layout-to-image (L2I) (LayoutDiffusion [10]), and (c) semantics-based scene-graph-to-image (SG2I) (R3CD [14]) methods . (d) Generalizable object Attribute Control (AC) under consistency achieved by our DisCo. ", "page_idx": 1}, {"type": "text", "text": "represent scenes with a structured graph format, where objects within the scene are denoted as nodes and the relationships between objects are represented as edges. Scene-Graph-to-Image (SG2I) generation is a challenging task due to the frequent ambiguous alignment between graph edges and relationships/interactions among visual objects. To address this issue, layout-based SG2I methods [15, 17, 18, 19] explicitly predict the spatial arrangements of objects in scenes by additional layout predictors, followed by L2I synthesis according to the layout (as demonstrated in Figure 2 (a)). These methods commonly employ one-to-one mapping, i.e., a single scene graph only corresponds to one layout, which severely limits the generation diversity. Besides, they also inherit the limitation of the L2I approach in modeling non-spatial interactions, whereby each object is typically generated independently. In contrast, as shown in Figure 2 (b), semantic-based SG2I methods implicitly encode graph edges into node embeddings by graph convolutional networks (GCNs), which effectively aligns object semantics with non-spatial interactions. Nonetheless, these methods are weak in logically determining the spatial positions of independent nodes, which might cause the absence of independent nodes (e.g., the \u201clamp\u201d and \u201cstone\u201d shown in Figure 1 (c)) in the generated image. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose DisCo, a Compositional image generation framework that integrates the Disentangled layout and semantics derived from scene graph representations (as depicted in Figure 2 (c)). To boost the representational capacity of scene graphs for complex scenes, we augment the node and edge representations with CLIP [6] text embeddings, and incorporate extra spatial information (i.e., bounding box embeddings) for nodes during training. Once the textual scene graph is constructed, we propose a Semantics-Layout Variational AutoEncoder (SL-VAE) based on tripletGCN [18] to jointly model the spatial relationships and non-spatial interactions in the scene. SL-VAE allows the one-to-many disentanglement for spatial layout and interactive semantics that match the input scene graph. This is achieved by sampling from a Gaussian distribution, offering object-level (layouts, semantics) conditions for the diffusion process [20]. Given that the layout and semantics encapsulate global and local relational information, we further introduce the Compositional Masked Attention (CMA) mechanism to inject object-level graph information with fine-grained attributes into the diffusion model, thereby preventing relational confusion and attribute leakage.Finally, we present a Multi-Layered Sampler (MLS) technique that leverages the diverse conditions generated by SL-VAE, achieving generalizable generation for object-level graph manipulation (i.e., node addition and attribute control) in the SG2I task, as depicted by the color change of two \u201csheep\u201d in Figure 1 (d). ", "page_idx": 1}, {"type": "image", "img_path": "zGN0YWy2he/tmp/59c6507d4469f4d4f6fe4c550f1441a173cf61622641e5d2c4d1b21dc8c4f7f5.jpg", "img_caption": ["Figure 2: Comparison between the previous SG2I architectures and ours. (a) Layout-based SG2I model [15] generate a spatial arrangement with an object layout; (b) Semantic-based SG2I models [14, 16] build interactive semantic embedding between objects; (c) Our method leverages scene graph representation by jointly deriving the disentangled layout and semantics with the proposed SL-VAE. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "In summary, our key contributions are as follows: (i) We apply the textual scene graph as a structured scene representation and introduce the Semantics-Layout Variational AutoEncoder (SL-VAE) to disentangle diverse spatial layouts and interactive semantics from the scene graph; (ii) We present the Compositional Masked Attention (CMA) to inject extracted object-level graph information with finegrained attributes into the diffusion model, which avoids relational confusion and attribute leakage; (iii) We introduce the Multi-Layered Sampler (MLS), a technique that leverages the diverse conditions produced by SL-VAE to implement object-level graph manipulation while keeping the visual content consistent; (iv) Our method outperforms current text/layout-based methods in relationship generation and achieves significantly superior generation performance compared to state-of-the-art SG2I models, thus showcasing the generalization of textual scene graphs in depicting complex scenes. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Text-to-Image Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion models (DMs) [1] are generative models that learn the data distribution $p(x)$ by gradually performing $T$ -step noise reduction from the variables $x_{T}$ sampled from the Gaussian distribution $\\bar{\\mathcal{N}}(0,1)$ . Thus the training process of DMs can be regarded as the reverse process of a Markov chain with a fixed length $T$ . To generate high-resolution images with less computational resources, Latent Diffusion Models (LDMs) [20] encode the image $\\textbf{\\em x}$ into the latent space $_{z}$ with the pre-trained Vector Quantized Variational AutoEncoder (VQ-VAE). Subsequently, the LDMs aim to predict the distribution $p(z)$ rather than $p(x)$ . For the text-to-image LDMs, the text is encoded with the CLIP [6] text encoder $E_{\\mathrm{CLIP}}$ . Then the objective function of a text-guided LDM can be formulated as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{L D M}=\\mathbb{E}_{z,\\epsilon\\sim\\mathcal{N}(0,1),t}[\\|\\epsilon-\\epsilon_{\\theta}(z_{t},E_{\\mathrm{CLIP}}(c),t)\\|_{2}^{2}],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $E_{\\mathrm{CLIP}}(c)$ is the text embedding of the text condition $c,t$ is the diffusion step, $\\epsilon_{\\theta}$ is a model for estimating the noise $\\epsilon$ , and $z_{t}=\\alpha_{t}z_{t-1}+\\sigma_{t}\\epsilon$ is the $t$ -step noised latent code from the ground-truth $z_{0}$ . During inference, the model $\\epsilon_{\\theta}$ with various samplers [1, 21] gradually denoises the initial noise $z_{T}\\sim\\mathcal{N}(\\bar{0},1)$ . Finally, the predicted latent code is decoded into the image space. ", "page_idx": 2}, {"type": "image", "img_path": "zGN0YWy2he/tmp/203a3dd299e9d099fa4475ab9d9220bc5d896fa04d722eea4920a9d63c0d6d27.jpg", "img_caption": ["Figure 3: Framework overview. (I) We parameterize the node embeddings into the Gaussian distribution with the Graph Union Encoder, which jointly models the spatial relationships and nonspatial interactions in scene graphs; (II) The Semantic and Layout Decoders generate spatial layouts and interactive semantics sampled from Gaussian distribution, respectively; (III) A diffusion model with the proposed Compositional Masked Attention (CMA) incorporates object-level conditions to generate visual images following the scene graph description; (IV) Detailed structure of CMA Layer. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2.2 Scene Graph Representation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The scene graph $G=(O,E)$ [17] presents a structured scene representation. Nodes $O=\\{o_{i}\\}_{i=1}^{N_{o}}$ denotes $N_{o}$ objects within the scene, while edges $E\\,=\\,\\{e_{i j}\\}_{1\\le i,j\\le N_{o},i\\neq j}$ denotes relationships between objects. All nodes and edges come with a semantic label, denoted as $c_{i}^{o}\\in\\mathcal{C}^{o}$ and $c_{i j}^{e}\\in\\mathcal{C}^{e}$ , where $\\mathcal{C^{o}}$ and $\\mathcal{C^{\\epsilon}}$ are category vocabularies of nodes and edges, respectively. In practice, the nodes $O=\\{o_{i}\\}_{i=1}^{N_{o}}$ and the triples $T=\\{t_{i j}=(o_{i},e_{i j},o_{j})\\}_{1\\leq i,j\\leq N_{o},i\\neq j}$ representing connections from $o_{i}$ to $o_{j}$ serve as inputs for graph convolutional networks (GCNs). Moreover, nodes and edges are typically converted into learnable embeddings using embedding layers denoted as $E_{e m b}^{o}$ and $E_{e m b}^{e}$ . ", "page_idx": 3}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As illustrated in Figure 3, we present a novel SG2I synthesis framework known as DisCo. The DisCo comprises three primary components: (1) Semantics-Layout Variational AutoEncoder (SL-VAE) that disentangle diverse spatial layouts and interactive semantics from the scene graph (Section 3.1); (2) Compositional Masked Attention (CMA) that injects object-level (layouts, semantics) with fine-grained attributes into the diffusion model (Section 3.2); and (3) Multi-Layered Sampler (MLS) that implements generalizable generation for object-level graph manipulation (Section 3.3). ", "page_idx": 3}, {"type": "text", "text": "3.1 Semantics-Layout Variational AutoEncoder ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Textual Scene Graph Construction. For constructing the scene graph representation, we employ the visual-language model to fully leverage the inherent disentangled semantics of the language, while simultaneously facilitating the alignment between images and scene graphs. Specifically, we augment the node and edge embeddings of the scene graph with CLIP [6] text embeddings. During training, we also incorporate spatial information for node embeddings by including bounding box coordinates (i.e., top-left corner and box size denoted as $b_{i}=(x_{i},y_{i},w_{i},h_{i}))$ . Then the node embeddings $\\scriptscriptstyle\\mathcal{O}$ and edge embeddings $\\mathcal{E}$ can be formulated as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{O}=\\{E_{e m b}^{o}(c_{i}^{o})\\otimes E_{\\mathrm{CLIP}}(o_{i})\\otimes E_{b o x}(b_{i})\\}_{i=1}^{N_{o}},\\ \\mathcal{E}=\\{E_{e m b}^{e}(c_{i j}^{e})\\otimes E_{\\mathrm{CLIP}}(t_{i j})\\}_{1\\leq i,j\\leq N_{o},i\\neq j}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $E_{\\mathrm{CLIP}}$ denotes the frozen pre-trained text encoder, $E_{b o x}$ is the spatial encoder for bounding box coordinates using Multi-Layer Perceptions (MLPs), and $\\otimes$ denotes concatenate operation. ", "page_idx": 3}, {"type": "text", "text": "Graph Union Encoding. Although layout-based SG2I methods are superior in modeling spatial topology compared to the semantics-based method, they fall short in capturing object interactions (i.e., non-spatial relationships) within the scene. Accordingly, after obtaining the node and edge embeddings mentioned above, we apply a Conditional Variational Autoencoder (CVAE) [22] based on triplet-GCN [18] to jointly model the layout and semantics information. As shown in Figure 3.I, the $L$ -layer Graph Union Encoder $E_{u}$ takes node and edge embeddings as inputs: ", "page_idx": 3}, {"type": "equation", "text": "$$\n(\\phi_{i}^{l+1},\\phi_{i j}^{l+1},\\phi_{j}^{l+1})=\\mathrm{GCN}_{l}(\\phi_{i}^{l},\\phi_{i j}^{l},\\phi_{j}^{l}),l\\in\\{0,\\ldots,L-1\\}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $l$ denotes the layer index of Graph Union Encoder, and $\\phi$ denotes intermediate features. Here we initialize $(\\phi_{i}^{0},\\phi_{i j}^{0},\\phi_{j}^{0}\\bar{)}=(\\mathcal{O}_{i},\\mathcal{E}_{i j},\\mathcal{O}_{j})$ . Please refer to the Appendix for more details about the tripletGCN. Given that the last node embedding $\\phi_{i}^{L}$ integrates both topology and interaction information, we conduct layout-semantic modeling by parameterizing it into Gaussian spaces $Z\\sim{\\mathcal{N}}(\\mu,\\sigma)$ . In this context, the means $\\boldsymbol{\\mu}\\in\\mathbb{R}^{D_{z}}$ and variances $\\sigma\\in\\mathbb{R}^{D_{z}}$ are estimated individually by two supplementary MLPs, where $D_{z}$ denotes the dimensional of latent space for node embedding. Hence, we jointly model the layout and semantics through the following minimization: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{u n i o n}=\\mathrm{KL}\\left(E_{u}(u|y,\\mathcal{O},\\mathcal{E})\\parallel p(u|y)\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where KL denotes the Kullback-Liebler divergence, $y$ denotes condition and the prior $p(u|y)$ is the standard Gaussian distribution $\\mathcal{N}(u\\mid0,1)$ . Specifically, we condition the latent space of the graph structure using the edge embedding following Equation 2 alongside the updated node embedding $\\{E_{e m b}^{o}(c_{i}^{o})\\otimes E_{\\mathrm{CLIP}}(o_{i})\\otimes u_{i}\\}_{i=1}^{N_{o}}$ , where $u_{i}$ is a random vector sampled from $Z$ . This architecture ensures that layout is solely necessary for training, with no need for hand-crafted layout in inference. ", "page_idx": 3}, {"type": "text", "text": "Disentangled Semantics-Layout Decoding. As illustrated in Figure 3.II, we disentangle the explicit spatial layout and implicit interactive semantics from the latent space using two separate triplet-GCNbased decoders, i.e., layout decoder $D_{l}$ and semantic decoder $D_{s}$ . The proposed Semantics-Layout ", "page_idx": 3}, {"type": "text", "text": "Variational AutoEncode (SL-VAE) comprises these two decoders and the graph union encoder mentioned above, which derives the spatial topology and object interactions from the scene graph representation. During training, the layout decoder is optimized by the following objective function: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{l a y o u t}=\\frac{1}{N_{o}}\\sum_{i=1}^{N_{o}}|b_{i}-\\hat{b}_{i}|_{1},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\hat{b}_{i}$ denotes the predicted coordinates. We only incorporate the ground truth layout $\\boldsymbol{B}=\\{b_{i}\\}_{i=1}^{N_{o}}$ during training, while generating $N_{l}$ diverse layouts $\\{\\hat{B}_{n}=\\{\\hat{b}_{n,i}\\}_{i=1}^{N_{o}}\\}_{n=1}^{N_{l}}$ by sampling Gaussian noise at inference time. For simplicity, we omit the superscript\u02c6in the following description. The semantic decoder $D_{s}$ generates semantics embeddings $\\bar{S}=\\{s_{i}\\bar{\\}_{i=1}^{N_{o}}$ to facilitate subsequent diffusion processes, and its parameters are iteratively updated with the diffusion loss in the next Section 3.2. ", "page_idx": 4}, {"type": "text", "text": "3.2 Diffusion with Compositional Masked Attention ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Object-level Fusion Tokenizer. We integrate the spatial layout $\\boldsymbol{B}\\:=\\:\\{b_{i}\\}_{i=1}^{N_{o}}$ and interactive semantics $\\boldsymbol{S}=\\{s_{i}\\}_{i=1}^{N_{o}}$ at object level, as illustrated in Figure 3.III. The single-object embeddings $\\mathcal{C}\\,=\\,\\{c_{i}\\}_{i=1}^{N_{o}}\\,=\\,\\{s_{i}\\,\\overset{\\cdot}{\\otimes}\\mathcal{F}(b_{i})\\}_{i=1}^{N_{o}}$ are acquired by directly applying semantic embeddings, while encoding box information using a Fourier mapping [23]. We define a learnable null embedding to pad the embedding length to $N_{m a x}$ , thereby accommodating varying numbers of objects: ", "page_idx": 4}, {"type": "equation", "text": "$$\nc_{i}=\\left\\{\\!\\!\\begin{array}{l l}{s_{i}\\otimes\\mathcal{F}(b_{i}),}&{i\\leq N_{o}}\\\\ {c_{n u l l},}&{\\mathrm{otherwise}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $c_{n u l l}$ denotes the learnable null embedding for padding. We optionally add attribute embedding $\\mathcal{A}=\\{a_{i}\\}_{i=1}^{N_{m a x}}$ i}iN=m1axto construct updated C = {ci \u2297ai }iN=m1ax, where ci and ai are separately processed by two MLPs before concatenation. Note that $a_{i}$ is obtained similarly to edge embedding in Equation 2. We also define a learnable null embedding $a_{n u l l}$ for cases where no attribute is specified. ", "page_idx": 4}, {"type": "text", "text": "Compositional Masked Attention. The cross-attention mechanism in diffusion bridges the visual and textual information, while selfattention captures self-related information within visual tokens [24]. Therefore, we insert our proposed Compositional Masked Attention (CMA) between self-attention and cross-attention layers. This technique effectively injects graph information into the diffusion process at the object level, preventing semantic confusion and attribute leakage through the attention mask. Specifically, we denote the visual token output by the vanilla self-attention as $\\boldsymbol{\\dot{\\nu}}\\in\\mathbb{R}^{N_{v}\\times D_{v}}$ , where $N_{v}$ and $D_{v}$ represent the number and dimensions of tokens, respectively. Then the CMA layer can be expressed as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathcal{V}}=S A_{m a s k}(\\mathcal{V}\\otimes\\hat{\\mathcal{C}},\\mathbf{M})[:N_{v}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "awrhe earlei $\\hat{\\mathcal{C}}=\\{\\hat{c}_{i}\\}_{i=1}^{N_{m a x}}$ vdiseunaolt etso koebnj uesminbge dMdiLngPss . wThhoes e mdaitmriexn $\\mathcal{V}$ $\\mathbf{M}\\in$ R(Nv+Nmax)\u00d7(Nv+Nmax) denotes the attention mask that depends on layout $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , which can be constructed as follows: ", "page_idx": 4}, {"type": "image", "img_path": "zGN0YWy2he/tmp/a7a293bfbb146806fe2e3caf6fb25cf72ea01b077b7e000394cb1aba0f661b15.jpg", "img_caption": ["Figure 4: Toy example of (a) compositional masked attention, and (b) its corresponding attention mask. We use visual tokens and object embeddings of objects A and B for demonstration. A and B have 1 and 2 visual tokens, respectively, whose attribution is determined by bounding boxes. "], "img_footnote": [], "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{M}_{i,j}={\\left\\{\\begin{array}{l l}{1,{\\mathrm{~if~}}i,j{\\mathrm{~fall~into~the~same~object}}}\\\\ {-\\,i n f,{\\mathrm{~otherwise}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\quad{\\dot{\\iota}},\\,j$ fall into the same object\u201d means that $i$ and $j$ index the visual tokens or object embeddings of the same object. Figure 4 illustrates the mechanism of CMA through a toy example. In contrast to vanilla self-attention, the proposed CMA prevents relational confusion and attribute leakage between different objects through the well-designed object-level masks mentioned above. As shown in Figure 3.IV, we forward the output of the CMA layer into the subsequent layers, serving as the updated visual token. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Diffusion Loss. Based on the object semantics output from the proposed SL-VAE, we optimize the evidence lower bound between sampling noise and prediction noise conditioned on object-level information (i.e., ground truth spatial layout and generated interactive semantics). Then the training loss of the diffusion model equipped with CMA can be summarized as follows: ", "page_idx": 4}, {"type": "table", "img_path": "zGN0YWy2he/tmp/6b4c3ef88e53cc15f333931d4cf4f0f4a65d4b59de962e19cbd1d3727b7202b5.jpg", "table_caption": ["Table 1: Performance comparison on COCO-Stuff and Visual Genome datasets using Inception Score (IS) and Fr\u00e9chet Inception Distance (FID) metrics. We report the results of methods with two generator structures, namely GAN- and Diffusion-based. The architecture of these methods is based on the layout (L) or semantics (S), while our approach includes both. The best results are bolded. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{L D M}=\\mathbb{E}_{z,\\epsilon\\sim\\mathcal{N}(0,1),t}[\\|\\epsilon-\\epsilon_{\\theta}(z_{t},E_{\\mathrm{CLIP}}(O),\\mathcal{B},S,\\mathcal{A},t)\\|_{2}^{2}].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Finally, we employ an end-to-end joint training pipeline for the whole proposed DisCo framework. The total objective function is presented as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t o t a l}=\\lambda_{1}\\mathcal{L}_{L D M}+\\lambda_{2}\\mathcal{L}_{u n i o n}+\\lambda_{3}\\mathcal{L}_{l a y o u t},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\lambda_{1},\\lambda_{2}$ , and $\\lambda_{3}$ are hyperparameters, which are typically set to 1.0, 0.1 and 1.0, respectively. ", "page_idx": 5}, {"type": "text", "text": "3.3 Multi-Layered Sampler ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Manipulations in the input scene graph, such as node addition and attribute adjustment, pose challenges for maintaining visual consistency in the generated images, ultimately compromising generalizability. To achieve an \u201cisolated\u201d image editing effect, we also provide the Multi-Layered Sampler (MLS) motivated by SceneDiffusion [25]. The scheme defines each object as a layer, thus allowing independent object-level Gaussian sampling. In contrast to SceneDiffusion which scrambles the reference layouts randomly, we sample additional $N_{l}$ (layouts, semantics) by the SL-VAE. Note that $N_{l}$ fixed seeds exist for the same scene. Then we aggregate latent codes from various layers into $z_{n}$ and utilize layout-converted non-overlapping masks $\\bar{\\{\\mathcal{M}}_{n}}=\\{m_{n,i}\\}_{i=1}^{N_{o}}\\}_{n=1}^{N_{l}}$ n,i}iN=o1}nNl=1 for locally conditioned diffusion. During the inference, the noise estimation for $N_{l}$ scenes is calculated as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\epsilon}_{n}^{(t)}=\\sum_{i=1}^{N_{o}}m_{n,i}\\;\\odot\\epsilon_{\\theta}\\bigl(z_{n}^{(t)},E_{\\mathrm{CLIP}}(o_{i}),b_{n,i},s_{n,i},a_{i},t\\bigr),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Subsequently, the latent code for each object is computed as the weighted average of the $N_{l}$ cropped denoised views. Please refer to the Appendix for more details about MLS. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Dataset. We conduct scene-graph-to-image (SG2I) generation experiments on the Visual Genome (VG) [27] and COCO-Stuff (COCO) [26] datasets. The VG dataset comprises 108, 077 image-scene graph pairs, accompanied by the bounding box coordinates and object attributes. Following previous work [17], we select objects and relationships that appear at least 2, 000 and 500 times respectively in VG, resulting in 178 objects and 45 unique relationship types. Also, we ignore small objects and use images containing 5 to 30 objects along with a minimum of 3 relationships. Based on the above filtering, we have 62, 565 images available for training, each containing an average of 10 objects and 5 relationships. While the original COCO-Stuff dataset [26] lacks scene graph annotations, it consists of 40, 000 images annotated with bounding box coordinates and captions, essential for synthesizing geometric scene graphs [15, 16]. All images in the COCO-Stuff dataset are labeled as 80 item categories and 91 stuff categories. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Implementation Details. We fine-tune the pre-trained Stable-Diffusion $1.5^{1}$ with the modified Attention module on 4 NVIDIA A100 GPUs, each with 80GB of memory. We apply the CLIP text encoder (vit-large-patch14 ) to construct the textual scene graph. We train the model with a batch size of 64 using the AdamW optimizer [30] with an initial learning rate of $1.0\\times10^{-4}$ , which is adjusted linearly over 50, 000 steps. During inference, we use the 50-step PNDMScheduler [21] with a classifiers-free scale [31] of 7.5. The sample number $N_{l}$ in the multi-layered sampler is set to 5. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics. Following previous works [14, 15, 16], we evaluate the performance of our method with the Inception Score (IS) [32] and the Fr\u00e9chet Inception Distance (FID) [33]. The IS score is derived from a pre-trained Inception Net [34], assessing both the quality and diversity of synthesized images. The FID score quantifies the dissimilarity between the generated image and the real image distribution, which evaluates the fidelity of the generated images. To measure the effectiveness of compositional generation, we further evaluate our method on the T2I-CompBench [35]. Besides, we apply CLIP for zero-shot attribute classification of the controlled object cropped by the bounding box, and subsequently evaluate the attribute control performance by the classification accuracy $\\mathsf{A C C}_{a t t r}$ . ", "page_idx": 6}, {"type": "text", "text": "Quantitative Comparisons. To demonstrate the effectiveness of the proposed DisCo, we compare it with current state-of-the-art SG2I methods on the COCO-Stuff and Visual Genome datasets, which are summarized in Table 1. Our DisCo outperforms other methods in both IS and FID scores, revealing its superior performance in both fidelity and diversity of image generation. Compared with previous methods, the primary architectural advantage of DisCo is ", "page_idx": 6}, {"type": "table", "img_path": "zGN0YWy2he/tmp/d7cdfbd7e7cbb670cd4df587ce4c06dbf2884a022d9805e8f11e25ce6329473a.jpg", "table_caption": ["Table 2: Relationship and attribute generation compared with text-to-image methods on T2I-CompBench [35]. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "its innovative approach of simultaneously integrating disentangled layout and semantics extracted from scene graph representations. Moreover, the proposed SL-VAE achieves the diverse generation of layouts and semantics from a single scene graph through Gaussian distribution sampling. Therefore, our DisCo integrates the beneftis of both layout-based and semantic-based methods, which is further ablated in detail in Table 4 of the ablation study. We proceed to assess the compositional generation on the T2I-CompBench [35], as shown in Table 2. The benchmark evaluates the competency of the text-to-image model in responding to compositional prompts. We report UniDet, CLIP, B-VQA, and 3-in-1 scores for measuring the generation of spatial/non-spatial relationships, attributes, and complex scenes, respectively. Following [35], we use the UniDet [39], CLIP [6], and BLIP [7] to evaluate these results. Our DisCo surpasses all compared T2I methods, confirming the efficacy of scene graphs in depicting complex scenes. ", "page_idx": 6}, {"type": "table", "img_path": "zGN0YWy2he/tmp/b705923ba6d6c6ad1f8b9905b69a006de4c931e94c723c68ddb6e1986abc9ee1.jpg", "table_caption": ["Table 3: User study. The score quantifies the user evaluation (i.e., relationships, quantities, and generation quality) of the alignment between the given prompt and the generated image. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "User Study. We conduct a user study by recruiting 50 participants from Amazon Mechanical Turk. We randomly select 8 prompts for each method, resulting in 80 generated images. We ask participants to score each generated image independently based on the image-prompt alignment. The worker can choose a score from $\\{1,\\bar{2},3,4,\\bar{5}\\}$ and we normalize the scores by dividing them by 5. We then compute the average score across all images and all workers. The results are presented in the Table 3. Our method is favored by most participants in terms of generation rationality and controllability. ", "page_idx": 6}, {"type": "image", "img_path": "zGN0YWy2he/tmp/e1fcf9fc02df8cd27fe0e9f494bc34c134e74529d5e93dc7b521a87ffa49f493.jpg", "img_caption": ["(c) Comparison with SG2I methods in independent node inference and generation quality. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 5: Qualitative Comparisons with (a) text-to-image (T2I) (SableDiffusion-XL [40], DALL\u00b7E 3 [3], and Imagen 2 [41]), (b) layout-to-image (L2I) (GLIGEN [12], LayoutDiffusion [10], and MIGC [13]), and (c) scene-graph-to-image (SG2I) (SG2Im [17], SGDiff [16], and R3CD [14]) methods. ", "page_idx": 7}, {"type": "table", "img_path": "zGN0YWy2he/tmp/6520c239f9e47b8652cfe3ebce70c9d1a211a51fd984bc8b54a1770edf17f9ff.jpg", "table_caption": ["Table 4: Ablation study for overall architecture.Table 5: Ablation study for attention mechanism. SL-VAE w/o $D_{s}$ means independent use of $\\scriptscriptstyle\\mathcal{O}$ . Vanilla attention means off-the-shelf T2I attention. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "zGN0YWy2he/tmp/41885c9a4155ffc1cad37733db11f270547b570ce65e1366a5366d83a40161e1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Qualitative Comparisons. Figure 5 visualizes the results of the methods conditioned by text, layout, or scene graph, showcasing our advantages in generating rationality and controllability: (i) Comparison with the text-to-image (T2I) methods. In Figure 5 (a), we present the superiority of ", "page_idx": 7}, {"type": "text", "text": "the disentangled structured scene graph over linear text for representing complex scenes. Firstly, we resolve ambiguity in textual relationships and semantics by employing layout and semantic disentanglement within the scene graph. For example, our DisCo clarifies the relationship between \u201cboat\u201d and \u201cgrass\u201d in the first line, as well as the semantics of \u201cbus\u201d and \u201cbuilding\u201d in the following ", "page_idx": 8}, {"type": "image", "img_path": "zGN0YWy2he/tmp/dbe1cc434bdbb179d3457b899663d676c96eae032c4ded3f3f30bd3f22c75291.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 6: Illustration of object-level Node Addition (NA) and Attribute Control (AC) in the scene. From left to right: (a) the image generated by the unmodified scene graph; (b) the chair addition; (c) the blue-colored wall; and (d) the red-colored wall. ", "page_idx": 8}, {"type": "text", "text": "line. Additionally, the samples in the first and last lines also showcase our capacity to generate the specified quantities of objects precisely. (ii) Comparison with the layout-to-image (L2I) methods. We also demonstrate that our DisCo outperforms the diffusion methods relying on manually crafted scene layout representations, as illustrated in Figure 5 (b). While the L2I method struggles to model non-spatial interactions (such as \u201cplaying\u201d in the first line and \u201cchasing\u201d in the second line), our DisCo addresses this challenge using disentangled object interactive semantics. Furthermore, by establishing semantics among objects derived from their relationships, we prevent independent generation instances that rely solely on the layout, exemplified by the \u201cbus\u201d and \u201ctire\u201d in the last line. (iii) Comparison with the scene-graph-to-image (SG2I) methods. The SG2I visualization results of different methods are showcased in Figure 5 (c). Our DisCo significantly improves the quality of SG2I generation, particularly for independent nodes. The proposed layout and semantics disentanglement technique effectively capture both the spatial and interactive information of independent nodes. Taking the \u201clamp\u201d and \u201cstone\u201d in the first image and the \u201cshadow\u201d in the second image as illustrations, these entities are neglected by previous methods, whereas our DisCo not only retains their semantic relevance but also infers their appropriate spatial placement within the scene. We also demonstrate the generalizable generation under consistency for graph manipulation (i.e., node addition and attribute control) in SG2I tasks, as shown in Figure 6. ", "page_idx": 8}, {"type": "text", "text": "Ablation Study. Table 4 explores the overall architecture by evaluating the alignment of the generated image with the objects and relationships depicted in the input scene graph. Following SGDiff [16], we conduct this analysis by graph-to-image (G2I) and imageto-graph (I2G) retrieval experiments. ", "page_idx": 8}, {"type": "table", "img_path": "zGN0YWy2he/tmp/44f4ff3e296bb218b1e3e3c28039c5b3d021c52984ed655f2458250e641359a6.jpg", "table_caption": ["Table 6: Ablation study for Multi-Layer Sampler (MLS). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Note that \u201cw/o $D_{s}{}^{,}$ means processing each node embedding by MLP independently,instead of obtaining object interactive semantics through $D_{s}$ . We observe that the spatial layout and interactive semantics collaborate to boost both retrieval tasks. These results demonstrate the effectiveness of integrating explicit spatial relations with implicit interactive semantics. In Table 5, we study the impact of different attention mechanisms. We inject graph conditions using different mechanisms: (a) Vanilla attention mechanism in the T2I diffusion model without our CMA; (b) CMA without attention mask M; (c) CMA with a union MLP after concatenating object and attribute embeddings; and (d) CMA with two separate MLPs before concatenating object and attribute embeddings. We found that CMA, which fuses separate encoding of object and attribute embeddings, significantly enhances the overall generation performance. Table 6 presents the Multi-Layer Sampler (MLS) ablation results, confirming its enhancement over the baseline and LSD [29]. In contrast to LSD, which randomly scrambles layouts, the proposed MLS naturally leverages a variety of coherent layouts and semantics produced by SL-VAE. Moreover, the increase in $\\mathrm{ACC}_{a t t r}$ scores also indicates that MLS facilitates controllability, especially in attribute control, while ensuring generation quality. ", "page_idx": 8}, {"type": "text", "text": "5 Related Works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Diffusion Models. Diffusion models (DMs) [2, 20, 31, 42] have achieved great success in high-quality image generation. The essence of DMs lies in estimating image distributions by iterative denoising noise-corrupted image, showcasing the superiority over VAEs [43, 44] and GANs [45] in training stability and likelihood estimation. To further explore the controllability of DMs, considerable efforts have been devoted to conditional generation based on DMs. Benefiting from the naturalness of language [6] and the advancements of vision-language foundation models [6, 7, 8, 9], numerous textto-image DMs [40, 41, 46] are beginning to emerge, facilitating explicit control of the corresponding semantics and style. However, the expressive capacity of linear text is limited. Therefore, many studies also endeavor to bolster global control through supplementary conditions, such as depth [46, 47], layout [10, 11, 12, 13], segmentation map [46, 48], and scene graph [14, 15]. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Image Generation from Scene Graphs. Scene graphs are structured scene representations, where nodes represent objects and edges represent relationships between objects [17, 27]. Given the superiority of scene graphs over linear text in delineating multiple objects and their intricate relationships [27, 49, 50], many studies investigate image generation from scene graphs. These approaches typically fall into two categories: layout-based and semantics-based methods. Layout-based methods [17, 18, 19, 28, 36, 51] initially map scene graphs to coarse scene layouts comprising multiple bounding boxes and further refine these layouts to images with a layout-to-image model (e.g., LayoutDiffusion [10]). While the layout depicts spatial relationships, it fails to capture abstract relationships within the scene, leading to a lack of object interaction. Another branch is semantics-based methods [14, 16, 29, 52, 53], which focus on graph understanding by directly encoding semantic information from the scene graph. Nevertheless, these methods have limitations in addressing independent scene nodes, leading to issues like entity loss and unreasonable placement. In this paper, we propose a compositional image generation that leverages the layout and semantics derived from the scene graph representation. We complement explicit layout and implicit semantics to enhance the understanding of the diffusion model for scene graphs. Additionally, to improve the controllability in the scenegraph-to-image task, we also attain generalizable generation for object-level graph manipulation (i.e., node addition and attribute control). ", "page_idx": 9}, {"type": "text", "text": "6 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The proposed CMA injects object-level information into the diffusion model via masks from the layout, effectively mitigating semantic ambiguity and limiting attribute leakage. In scenarios involving object overlap, the proposed CMA inhibits direct interaction between the visual token and the object embedding along with its attributes. Nonetheless, the attribute information from the visual token inadvertently leaks into the overlapping region in subsequent layers. Hence, there may be attribute leakage among the objects, as shown in Figure 7. ", "page_idx": 9}, {"type": "image", "img_path": "zGN0YWy2he/tmp/f56fe19bc3d879cb5030da99527f6452b156714fccfc1e71f8bae19a0980b181.jpg", "img_caption": ["Figure 7: Qualitative limitations on attribute leakage of overlapping. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this study, we leverage the disentangled textual scene graph representation to condition the diffusion process for generating complex scene images. The innovation of our framework lies in utilizing the VAE for scene relationship modeling and diffusion model (DM) for the composite visual generation. To comprehensively capture spatial relationships and non-spatial interactions within scenes, we introduce the Semantics-Layout Variational AutoEncoder (SL-VAE) for deriving diverse layouts and semantics from a single scene graph. Building upon them, we propose the Compositional Masked Attention (CMA) integrated with DM, which guides the de-noising trajectory by compositing extracted object-level graph information with fine-grained attributes. We also introduce a Multi-Layer Sampler (MLS) to preserve the main visual content while modifying the input scene graph. Extensive experiments demonstrate that our framework outperforms current methods conditioned by text, layout, or scene graph in relationship modeling and controllability. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research is supported by the National Natural Science Foundation of China [Grant 62302246] and the Zhejiang Provincial Natural Science Foundation of China [Grant LQ23F010008]. We also express our sincere gratitude to the AI Computing Center at the Eastern Institute of Technology for their valuable support and assistance. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems (NeurIPS), 33:6840\u20136851, 2020. 1, 3 [2] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems (NeurIPS), 34:8780\u20138794, 2021. 1, 9 [3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2:8, 2023. 1, 2, 7, 8 [4] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 1 [5] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 1 [6] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning (ICML), pages 8748\u20138763, 2021. 1, 2, 3, 4, 7, 10 [7] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In Proceedings of the International Conference on Machine Learning (ICML), pages 12888\u201312900, 2022. 1, 7, 10 [8] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Proceedings of the International Conference on Machine Learning (ICML), pages 19730\u201319742, 2023. 1, 10 [9] Kecheng Zheng, Yifei Zhang, Wei Wu, Fan Lu, Shuailei Ma, Xin Jin, Wei Chen, and Yujun Shen. Dreamlip: Language-image pre-training with long captions. arXiv preprint arXiv:2403.17007,   \n2024. 1, 10 [10] Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi, Ying Shan, and Xi Li. Layoutdiffusion: Controllable diffusion model for layout-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages   \n22490\u201322499, 2023. 1, 2, 7, 8, 10 [11] Jiaxin Cheng, Xiao Liang, Xingjian Shi, Tong He, Tianjun Xiao, and Mu Li. Layoutdiffuse: Adapting foundational diffusion models for layout-to-image generation. arXiv preprint arXiv:2302.08908, 2023. 1, 10 [12] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages   \n22511\u201322521, 2023. 1, 7, 8, 10 [13] Dewei Zhou, You Li, Fan Ma, Zongxin Yang, and Yi Yang. Migc: Multi-instance generation controller for text-to-image synthesis. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 1, 7, 8, 10 [14] Jinxiu Liu and Qi Liu. R3cd: Scene graph to image generation with relation-aware compositional contrastive control diffusion. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 38, pages 3657\u20133665, 2024. 1, 2, 6, 7, 8, 10 [15] Azade Farshad, Yousef Yeganeh, Yu Chi, Chengzhi Shen, B\u00f6jrn Ommer, and Nassir Navab. Scenegenie: Scene graph guided diffusion models for image synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 88\u201398, 2023. 1, 2, 6, 7,   \n10 [16] Ling Yang, Zhilin Huang, Yang Song, Shenda Hong, Guohao Li, Wentao Zhang, Bin Cui, Bernard Ghanem, and Ming-Hsuan Yang. Diffusion-based scene graph to image generation with masked contrastive pre-training. arXiv preprint arXiv:2211.11138, 2022. 1, 2, 6, 7, 8, 9, 10 [17] Justin Johnson, Agrim Gupta, and Li Fei-Fei. Image generation from scene graphs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1219\u20131228, 2018. 1, 2, 4, 6, 7, 8, 10   \n[18] Oron Ashual and Lior Wolf. Specifying object attributes and relations in interactive scene generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 4561\u20134569, 2019. 2, 4, 6, 10   \n[19] Roei Herzig, Amir Bar, Huijuan Xu, Gal Chechik, Trevor Darrell, and Amir Globerson. Learning canonical representations for scene graph to image generation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 210\u2013227. Springer, 2020. 2, 6, 10   \n[20] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10684\u201310695, 2022. 3, 6, 7, 9, 14, 16   \n[21] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. arXiv preprint arXiv:2202.09778, 2022. 3, 7   \n[22] Andrew Luo, Zhoutong Zhang, Jiajun Wu, and Joshua B Tenenbaum. End-to-end optimization of scene layout. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3754\u20133763, 2020. 4   \n[23] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in Neural Information Processing Systems (NeurIPS), 33:7537\u20137547, 2020. 5   \n[24] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. $^{p+}$ : Extended textual conditioning in text-to-image generation. arXiv preprint arXiv:2303.09522, 2023. 5   \n[25] Jiawei Ren, Mengmeng Xu, Jui-Chieh Wu, Ziwei Liu, Tao Xiang, and Antoine Toisoul. Move anything with layered scene diffusion. arXiv preprint arXiv:2404.07178, 2024. 6, 9, 14, 15   \n[26] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1209\u20131218, 2018. 6, 7   \n[27] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision (IJCV), 123:32\u201373, 2017. 6, 10   \n[28] Yikang Li, Tao Ma, Yeqi Bai, Nan Duan, Sining Wei, and Xiaogang Wang. Pastegan: A semi-parametric method to generate image from scene graph. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019. 6, 10   \n[29] Yang Wu, Pengxu Wei, and Liang Lin. Scene graph to image synthesis via knowledge consensus. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 37, pages 2856\u20132865, 2023. 6, 9, 10   \n[30] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 7   \n[31] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 7, 9   \n[32] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in Neural Information Processing Systems (NeurIPS), 29, 2016. 7   \n[33] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in Neural Information Processing Systems (NeurIPS), 30, 2017. 7   \n[34] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2818\u20132826, 2016. 7   \n[35] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems (NeurIPS), 36:78723\u201378747, 2023. 7 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[36] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional visual generation with composable diffusion models. In Proceedings of the European Conference on Computer Vision (ECCV), pages 423\u2013439. Springer, 2022. 7, 10 ", "page_idx": 12}, {"type": "text", "text": "[37] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional text-to-image synthesis. arXiv preprint arXiv:2212.05032, 2022. 7 ", "page_idx": 12}, {"type": "text", "text": "[38] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics (TOG), 42(4):1\u201310, 2023. 7 ", "page_idx": 12}, {"type": "text", "text": "[39] Xingyi Zhou, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Simple multi-dataset detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7571\u20137580, 2022. 7 ", "page_idx": 12}, {"type": "text", "text": "[40] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 7, 8, 10 ", "page_idx": 12}, {"type": "text", "text": "[41] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems (NeurIPS), 35:36479\u201336494, 2022. 7, 8, 10 ", "page_idx": 12}, {"type": "text", "text": "[42] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 9 ", "page_idx": 12}, {"type": "text", "text": "[43] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 9 ", "page_idx": 12}, {"type": "text", "text": "[44] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. Advances in Neural Information Processing Systems (NeurIPS), 28, 2015. 9 ", "page_idx": 12}, {"type": "text", "text": "[45] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139\u2013144, 2020. 9 ", "page_idx": 12}, {"type": "text", "text": "[46] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 3836\u20133847, 2023. 10 ", "page_idx": 12}, {"type": "text", "text": "[47] Lezhong Wang, Jeppe Revall Frisvad, Mark Bo Jensen, and Siavash Arjomand Bigdeli. Stereodiffusion: Training-free stereo image generation using latent diffusion models. arXiv preprint arXiv:2403.04965, 2024. 10 ", "page_idx": 12}, {"type": "text", "text": "[48] Ziqi Huang, Kelvin CK Chan, Yuming Jiang, and Ziwei Liu. Collaborative diffusion for multimodal face generation and editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6080\u20136090, 2023. 10 ", "page_idx": 12}, {"type": "text", "text": "[49] Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David Shamma, Michael Bernstein, and Li Fei-Fei. Image retrieval using scene graphs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3668\u20133678, 2015. 10 ", "page_idx": 12}, {"type": "text", "text": "[50] Subarna Tripathi, Anahita Bhiwandiwalla, Alexei Bastidas, and Hanlin Tang. Using scene graph context to improve image generation. arXiv preprint arXiv:1901.03762, 2019. 10 ", "page_idx": 12}, {"type": "text", "text": "[51] Yilun Du, Conor Durkan, Robin Strudel, Joshua B Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, and Will Sussman Grathwohl. Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and mcmc. In Proceedings of the International Conference on Machine Learning (ICML), pages 8489\u20138510, 2023. 10 ", "page_idx": 12}, {"type": "text", "text": "[52] Ruijun Li, Weihua Li, Yi Yang, Hanyu Wei, Jianhua Jiang, and Quan Bai. Swinv2-imagen: Hierarchical vision transformer diffusion models for text-to-image generation. Neural Computing and Applications, pages 1\u201316, 2023. 10 ", "page_idx": 12}, {"type": "text", "text": "[53] Ruichen Wang, Zekang Chen, Chen Chen, Jian Ma, Haonan Lu, and Xiaodong Lin. Compositional text-to-image synthesis with attention map control of diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 38, pages 5544\u20135552, 2024. 10 ", "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This appendix is organized as follows: Section A.1 provides more details about Semantics-Layout Variation AutoEncoder; Section A.2 introduces the Stable Diffusion and its attention mechanism; Section A.3 describes the implementation of the Multi-Layer Sampler in detail; Section A.4 covers more ablation studies; Section A.5 presents more qualitative results, including comparison visualization and graph manipulation; Section A.6 delves into the broader societal impacts of this work. The core script is zipped and attached to the supplementary material. ", "page_idx": 13}, {"type": "text", "text": "A.1 Semantics-Layout Variation AutoEncoder ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Recall that we apply the triplet-GCN-based CVAE architecture in Section 3.1. Each triplet-GCN layer in the encoder and decoder takes the node and edge embeddings. Specifically, the $G C N_{l}$ mentioned in the paper uses two cascading MLPs $\\{\\mathrm{mlp_{1},m l p_{2}}\\}$ to deal with node and edge embeddings: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{(\\psi_{i}^{l},\\phi_{i j}^{l+1},\\psi_{j}^{l})=\\mathrm{mlp}_{1}(\\phi_{i}^{l},\\phi_{i j}^{l},\\phi_{j}^{l}),l\\in\\{0,\\ldots,L-1\\},}\\\\ &{}&{\\phi_{i}^{l+1}=\\psi_{i}^{l}+\\mathrm{mlp}_{2}(\\mathrm{avg}(\\psi_{j}^{l}\\mid j\\in\\mathcal{N}_{\\mathcal{E}}(o_{i}))),\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $l$ denotes the layer index of encoder or decoder, $\\mathcal{N}_{\\mathcal{E}}$ denotes the neighbor index set for each node, avg denotes the average pooling operation, $\\phi$ and $\\psi$ denote intermediate features. Hence, $\\mathrm{\\mlp_{1}}$ conducts message passing among interconnected nodes and updates the edge features, while $\\mathrm{{mlp}_{2}}$ aggregates features from all neighboring nodes and updates its features. For graph union encoder, we let $(\\phi_{i}^{0},\\phi_{i j}^{0},\\phi_{j}^{0})\\:=\\:(\\mathcal{O}_{i},\\mathcal{E}_{i j},\\mathcal{O}_{j})$ . The last embedding $\\phi_{i}^{L}$ is parameterized to the Gaussian distribution $Z\\sim{\\mathcal{N}}(\\mu,\\sigma)$ , where $\\boldsymbol{\\mu},\\boldsymbol{\\sigma}\\in\\mathbb{R}^{D_{z}}$ output by two additional MLPs and $D_{z}$ denotes the dimensional of latent space for node embedding. ", "page_idx": 13}, {"type": "text", "text": "A.2 Diffusion with Compositional Masked Attention ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Stable Diffusion [20] is one of most popular text-to-image model. As described in Section 2.1, Stable Diffusion uses a U-Net $\\epsilon_{\\theta}$ composed of convolution and transformer to estimate noise. The transformer includes two attention mechanisms, namely Cross-Attention, and Self-Attention. ", "page_idx": 13}, {"type": "text", "text": "Cross-Attention Layer. Text prompts are mapped to sequence embeddings by CLIP text encoder and integrated into UNet via Cross-Attention to guide the de-noising trajectory: ", "page_idx": 13}, {"type": "equation", "text": "$$\nA t t e n t i o n(Q_{v i s u a l},K_{t e x t},V_{t e x t})=s o f t m a x(\\frac{Q_{v i s u a l}K_{t e x t}^{T}}{\\sqrt{d}})\\cdot V_{t e x t}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $Q_{v i s u a l}$ denotes the Query from the visual token of the UNet, $K_{t e x t}$ and $V_{t e x t}$ denotes Key and Value from text embeddings, all of which are projected by linear layers, $d$ denotes the dimension of $Q_{v i s u a l}$ , $K_{t e x t}$ , and $V_{t e x t}$ . ", "page_idx": 13}, {"type": "text", "text": "Self-Attention Layer. Self-Attention captures self-related information within visual tokens: ", "page_idx": 13}, {"type": "equation", "text": "$$\nA t t e n t i o n(Q_{v i s u a l},K_{v i s u a l},V_{v i s u a l})=s o f t m a x(\\frac{Q_{v i s u a l}K_{v i s u a l}^{T}}{\\sqrt{d}})\\cdot V_{v i s u a l}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $Q_{v i s u a l}$ , $K_{v i s u a l}$ , and $V_{v i s u a l}$ separately represent the Query, Key, and Value in self-attention flow between specific tokens by multiplying a mask M to the QvisualKvTisual. Since M is applied layers, which are projected by linear layers. The self-attention mechanism isolates the information before softmax, the value of the isolated position is set to negative infinity $-i n f$ . ", "page_idx": 13}, {"type": "text", "text": "Compositional Masked Attention Layer. Based on the attention mask $\\mathbf{M}$ that depends on layout $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , the Compositional Masked Attention can be expressed as: ", "page_idx": 13}, {"type": "equation", "text": "$$\nA t t e n t i o n(Q_{C M A},K_{C M A},V_{C M A})=s o f t m a x(\\frac{Q_{C M A}K_{C M A}^{T}\\odot\\mathbf{M}}{\\sqrt{d}})\\cdot V_{C M A}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $Q_{C M A}$ , $K_{C M A}$ , and $V_{C M A}$ individually represent the Query, Key, and Value derived from $\\nu\\otimes{\\hat{\\mathcal{C}}}$ , achieved through linear layer projections. We insert our proposed Compositional Masked Attention (CMA) between self-attention and cross-attention layers. ", "page_idx": 13}, {"type": "text", "text": "A.3 Multi-Layer Sampler ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Layered Scene Representation. We decompose a controllable scene containing $N_{o}$ objects into $N_{o}$ layers. Different from SceneDiffusion [25], our approach involves each layer incorporating not only separate latent code $z_{i}$ and spatial layout $b_{i}$ , but also integrating the interactive semantics $s_{i}$ produced by the SL-VAE. Here we convert the layout parameter $b_{i}$ to two parts: (1) a fixed object-centric binary mask $m_{i}\\in\\{0,1\\}^{c\\times w\\times h}$ to solely show the geometric property of the object, and (2) a two-element offset $p_{i}=\\{\\mu_{i},\\upsilon_{i}\\}$ to solely indicate its spatial locations, with $\\mu_{i}$ and $\\upsilon_{i}$ defining the horizontal and vertical movement range. We sample Gaussian noise individually for the initial latent code of each layer, i.e., $\\mathcal{Z}=\\{z_{i}^{(T)}\\stackrel{\\bar{\\sim}}{\\sim}\\mathcal{N}(0,1)\\}_{i=1}^{\\bar{N}_{o}}$ . Then we utilize the layout-converted non-overlapping masks $\\{l_{i}\\}_{i=1}^{N_{o}}$ to derive the aggregated latent code $_{\\textit{z}}$ from various layers: ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle z^{(t)}=\\sum_{i=1}^{N_{o}}l_{i}\\odot\\overline{{s h i f t}}(z_{i}^{(t)},p_{i})}}\\\\ {{\\displaystyle l_{i}=\\overline{{s h i f t}}(m_{i},p_{i})\\prod_{j=1}^{N_{i}-1}(1-\\overline{{s h i f t}}(m_{j},p_{j})),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\odot$ denotes element-wise multiplication, and $\\overline{{s h i f t}}(x,p)$ denotes spatially shifting the values of $x$ in the direction of $p$ . ", "page_idx": 14}, {"type": "text", "text": "Multi-Layer Generation. We introduce the Multi-Layer Sampler that matches our diverse layout and semantic simulation. In contrast to SceneDiffusion [25] which scrambles the reference layouts randomly, we sample additional $N_{l}$ layouts and semantics by the proposed SL-VAE. On the one hand, the SL-VAE ensures that the generated scene layout is reasonable. On the other hand, we take full advantage of the paired object-level (layouts, semantics). Specifically, the denoising scheme consists of four steps: ", "page_idx": 14}, {"type": "text", "text": "(a) Sampling additional $N_{l}$ layouts $\\{B_{n}=\\{b_{n,i}\\}_{i=1}^{N_{o}}\\}_{n=1}^{N_{l}}$ and semantics $\\{S_{n}=\\{s_{n,i}\\}_{i=1}^{N_{o}}\\}_{n=1}^{N_{l}}$ by the proposed SL-VAE. Note that $N_{l}$ fixed seeds exist for the same scene graph. According to the description of the layered representation, we convert the layout to get offset $\\{\\bar{\\mathcal{P}}_{n}=\\{p_{n,i}\\}_{i=1}^{N_{o}^{-}}\\}_{n=1}^{N_{l}}$ . ", "page_idx": 14}, {"type": "text", "text": "(b) Aggregating latent codes from various layers in each scene: ", "page_idx": 14}, {"type": "equation", "text": "$$\nz_{n}^{(t)}=\\sum_{i=1}^{N_{o}}l_{i}\\odot\\overline{{s h i f t}}(z_{i}^{(t)},p_{n,i})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "(c) Estimating the noise $\\hat{\\epsilon}_{n}^{(t)}$ from each aggregated latent code $z_{n}^{(t)}$ and gets denoised aggregated latent code $\\hat{z}_{n}^{(\\mathrm{\\boldsymbol{t}}-1)}\\in\\{\\hat{z}_{1}^{(t-1)},\\dots,\\hat{z}_{N_{l}}^{(t-1)}\\}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{\\epsilon}_{n}^{(t)}=\\sum_{i=1}^{N_{o}}m_{n,i}\\;\\odot\\epsilon_{\\theta}\\bigl(z_{n}^{(t)},E_{\\mathrm{CLIP}}(o_{i}),b_{n,i},s_{n,i},a_{i},t\\bigr),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $m_{n,i}$ is the non-overlapping mask converted by the layout $b_{n,i}$ ", "page_idx": 14}, {"type": "text", "text": "(d) Updating the latent code of each layer by computing the weighted average of the $N_{l}$ aggregated latent code ", "page_idx": 14}, {"type": "equation", "text": "$$\nz_{i}^{(t-1)}=\\frac{\\sum_{n=1}^{N_{l}}\\overline{{s h i f t}}(l_{i}\\odot\\hat{z}_{n}^{(t-1)},-p_{n,i})}{\\sum_{n=1}^{N_{l}}\\overline{{s h i f t}}(l_{i},-p_{n,i})}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where ${\\overline{{s h i f t}}}(x,-p)$ denotes spatially shifting the values of $x$ in the reverse direction of $p$ . ", "page_idx": 14}, {"type": "text", "text": "A.4 More Ablation Studies ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Graph Construction. We conduct ablation for graph construction in Table 7. We investigate the impact of different graph components (i.e., CLIP, Box, and Learnable Embeddings) by turning off each independently. We observe that each component improves the performance, all of which are crucial components presented in our DisCo. ", "page_idx": 14}, {"type": "table", "img_path": "zGN0YWy2he/tmp/41608a5cc90b69c1c6c2f650d692605498033f5de8002ac78df9c5ccaa35d75f.jpg", "table_caption": ["Table 7: Ablation study for graph construction. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Computing Consumption. We demonstrate the impact of our proposed CMA on the computational complexity of the U-Net within the Stable Diffusion, as presented in Table 8. We use Floating Point ", "page_idx": 14}, {"type": "table", "img_path": "zGN0YWy2he/tmp/ffa226088de875ae0338c2ef7767d2a064da49898cf668abca4dfa64581fb4a5.jpg", "table_caption": ["Table 8: Ablation study for computing consumption. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Operations (FLOPs), the number of parameters (Params), and inference time (Time) to measure computing consumption. The FLOPS and Time metrics are conducted by processing the tensor with a resolution of $2\\times4\\times64\\times64$ on an NVIDIA A100 GPU. Our proposed DisCo significantly improves the controllability of the Stable Diffusion with a tolerable increase in computational cost. ", "page_idx": 15}, {"type": "text", "text": "A.5 More Visualization Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Figure 8 showcases more generalizable generation results under consistency for graph manipulation (i.e., node addition and attribute control) in SG2I task. In Figure 9, 10, and 11, we present more visualization comparisons with the methods conditioned by text, layout, or scene graph, which demonstrates the superiority of our DisCo in terms of generation rationality and controllability. ", "page_idx": 15}, {"type": "text", "text": "A.6 Broader Impacts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We demonstrate the superiority of our DisCo over existing generation methods based on text, layout, and scene graphs, suggesting a potential beneficial influence on the realms of art creation and data synthesis. Nevertheless, there remains a concern regarding the possibility of generating malicious images or infringing copyright. ", "page_idx": 15}, {"type": "image", "img_path": "zGN0YWy2he/tmp/40e51e8edd5b092ec7e6a2209c080e23ae883bcfbc60fafbb1088c36697f17cd.jpg", "img_caption": ["Figure 8: Generalizable Generation Samples under Consistency for Graph Manipulation. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "zGN0YWy2he/tmp/c659d45a068d56155ed8eade09c5ad64676803a9be62e15edb91c3c16ecc295f.jpg", "img_caption": ["Figure 9: Qualitative Comparison with Text-to-Image methods. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "zGN0YWy2he/tmp/c8154b691beef1b297e2e6039189d251ec69ca1385e3293ae29786eaecc66f6a.jpg", "img_caption": ["Figure 10: Qualitative Comparison with Layout-to-Image methods. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "zGN0YWy2he/tmp/6b0b39d93dbb0af0e8ae72019a78bb0c0f9bb667d03f1bedc2854a746ce6b5d0.jpg", "img_caption": ["Figure 11: Qualitative Comparison with Scene-Graph-to-Image methods. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We claim in the abstract and introduction that this paper studies generating complex images with structured scene graphs. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: The limitations are addressed in the Appendix, which will be included in the camera-ready version. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: All the theorems and formulas are clearly stated in Section 2 and 3 of the main paper, and complemented in Section A.1, A.2 and A.3 of the Supplemental Material. Besides, all theorems and assumptions related to this work are properly referenced. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide the implementation details for reproducibility in Section 4, and the project will be open-sourced. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We attach the core script and data preparation to the Supplemental Material. The complete code will be released in the camera-ready version, accompanied by detailed instructions for reproducibility. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide the training and test details in Section 4. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We report error bars suitably and correctly to achieve statistical significance in the paper. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide sufficient information on the computer resources in Section 4. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our work conforms with the NeurIPS Code of Ethics in every respect. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We discuss both potential positive and negative societal impacts in Section A.6 of the Appendix. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 23}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our model is heavily based on existing StableDiffusion, so our safeguards are the same as theirs. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: In Section 4, we properly credit all the public baselines and datasets utilized in this paper. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The assets introduced in the paper are well documented. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]