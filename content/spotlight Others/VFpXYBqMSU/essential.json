{"importance": "This paper is **crucial** for researchers in diffusion models and generative AI. It challenges the conventional wisdom about data quality, offering a novel approach to improve model performance and opens **new avenues** for exploring the data-model relationship in deep learning.  Its findings have **significant implications** for training more effective and efficient generative models using large-scale datasets.", "summary": "Slightly corrupting pre-training data significantly improves diffusion models' image generation quality, diversity, and fidelity.", "takeaways": ["Introducing slight corruption into the pre-training data of diffusion models significantly improves the quality, diversity, and fidelity of generated images.", "Theoretically, slight corruption increases entropy and reduces the Wasserstein distance to the true data distribution, explaining the improved performance.", "A simple method called Condition Embedding Perturbation (CEP) effectively boosts model performance, both during pre-training and downstream tasks."], "tldr": "Many high-quality images, audios, and videos are generated by diffusion models, which benefit greatly from extensive pre-training on large-scale datasets. However, these datasets often contain corrupted data where conditions don't accurately describe the data. This study examines this problem by exploring the impact of such condition corruption during pre-training. It found that introducing slight corruption can improve the models' quality, diversity, and fidelity. \nThis research systematically investigates the impact of synthetically introducing various types of slight condition corruption into pre-training data. The study uses over 50 conditional diffusion models trained on ImageNet-1K and CC3M.  Experiments show that slight corruption enhances the models' performance.  A new method, Condition Embedding Perturbation (CEP), is proposed to improve model training by adding controlled perturbations, significantly enhancing performance in both pre-training and downstream tasks. This work provides valuable insights into optimizing data and pre-training processes for diffusion models.", "affiliation": "Carnegie Mellon University", "categories": {"main_category": "Computer Vision", "sub_category": "Image Generation"}, "podcast_path": "VFpXYBqMSU/podcast.wav"}