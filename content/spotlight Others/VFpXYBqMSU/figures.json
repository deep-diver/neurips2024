[{"figure_path": "VFpXYBqMSU/figures/figures_1_1.jpg", "caption": "Figure 1: Visualization from class and text-conditional DMs pre-trained with clean, slight, and severe condition corruption. Slight corruption in pre-training improves the quality and diversity of images.", "description": "This figure displays image samples generated by four different diffusion models (LDM-4 IN-1K, DiT-XL/2 IN-1K, LDM-4 CC3M, and LCM-v1.5 CC3M) trained under three different conditions: clean, slight corruption, and severe corruption. Each row represents a different level of corruption in the pre-training data, and each column represents a different model.  The results show that a slight amount of corruption in the pre-training data leads to significantly better image quality and diversity compared to models trained on clean data, while severe corruption negatively impacts the results.", "section": "3 Understanding the Pre-training Corruption in Diffusion Models"}, {"figure_path": "VFpXYBqMSU/figures/figures_1_2.jpg", "caption": "Figure 2: (a) FID and (b) IS of DMs pre-trained on IN-1K and CC3M with various corruption. Slight corruption of various types helps DMs achieve better performance, compared to the clean ones.", "description": "This figure shows the Fr\u00e9chet Inception Distance (FID) and Inception Score (IS) for diffusion models trained on ImageNet-1K (IN-1K) and Conceptual Captions 3M (CC3M) datasets with varying levels of synthetic condition corruption.  The x-axis represents the percentage of corrupted conditions, while the y-axis displays FID (lower is better) and IS (higher is better). The results indicate that introducing a small amount of corruption during pre-training improves the performance of the diffusion models, as measured by both FID and IS, compared to models trained on clean data.  Different types of corruption were tested, showing that slight corruption is beneficial regardless of the specific corruption type.  This suggests that a degree of noise or imperfection in the training data might be beneficial to model generalization and performance.", "section": "3 Understanding the Pre-training Corruption in Diffusion Models"}, {"figure_path": "VFpXYBqMSU/figures/figures_3_1.jpg", "caption": "Figure 2: (a) FID and (b) IS of DMs pre-trained on IN-1K and CC3M with various corruption. Slight corruption of various types helps DMs achieve better performance, compared to the clean ones.", "description": "This figure shows the FID (Fr\u00e9chet Inception Distance) and IS (Inception Score) for diffusion models trained on ImageNet-1K (IN-1K) and Conceptual Captions 3 Million (CC3M) datasets with varying levels of synthetic condition corruption.  The x-axis represents the percentage of corruption, and the y-axis represents FID and IS. Lower FID and higher IS indicate better image quality and diversity.  The results demonstrate that a small amount of corruption improves model performance compared to training with clean data.", "section": "3 Understanding the Pre-training Corruption in Diffusion Models"}, {"figure_path": "VFpXYBqMSU/figures/figures_3_2.jpg", "caption": "Figure 4: Quantitative evaluation of complexity and diversity of class and text-conditional LDMs. We plot the top-1% RMD score ((a) and (c)) which measures the complexity and diversity of samples (with s = 2.0 and s = 3.0 for IN-1K and CC3M LDMs), and the sample entropy ((b) and (d)) as a proxy measure of diversity, where each point indicates the result of a guidance scale. Models pre-trained with slight condition corruption generate samples of higher complexity and diversity.", "description": "This figure quantitatively evaluates the complexity and diversity of generated images from class and text-conditional diffusion models.  It uses two metrics: Top-1% Relative Mahalanobis Distance (RMD) and sample entropy. RMD assesses complexity and diversity, while sample entropy directly measures diversity.  The results are shown for different guidance scales, revealing that models pre-trained with slight condition corruption produce samples with significantly higher complexity and diversity compared to those trained on clean data.", "section": "3.1 Pre-training Evaluation"}, {"figure_path": "VFpXYBqMSU/figures/figures_4_1.jpg", "caption": "Figure 5: Qualitative evaluation of images generated from circular walk around the learned latent space using (a) class-conditional IN-1K LDMs and (b) text-conditional CC3M LDMs. Models pre-trained with slight condition corruption present more diversity in the learned distribution.", "description": "This figure shows a qualitative comparison of image generation results from circular walks in the latent space of trained diffusion models. The images are generated using class-conditional IN-1K LDMs (left) and text-conditional CC3M LDMs (right).  Each row represents a different corruption ratio in the pre-training data. The results show that pre-training with a slight amount of corruption leads to significantly more diverse image generations in both IN-1K and CC3M model comparisons.", "section": "3.1 Pre-training Evaluation"}, {"figure_path": "VFpXYBqMSU/figures/figures_5_1.jpg", "caption": "Figure 11: Qualitative evaluation results of 50K images generated by class-conditional LDMs pre-trained on ImageNet-1K with synthetic corruptions. The images are generated with various guidance scales using 1K class conditions and compared with 50K validation images of ImageNet-1K.", "description": "This figure presents a comprehensive quantitative evaluation of 50,000 images generated by class-conditional Latent Diffusion Models (LDMs) pre-trained on ImageNet-1K with various levels of synthetic condition corruption.  The evaluation uses multiple metrics (FID, IS, Precision, Recall, sFID, TopPR F1, Top-1% RMD, Memorization Ratio, Avg L2 Dist, CLIP Score) across different guidance scales, comparing the results against 50,000 validation images from ImageNet-1K.  The purpose is to demonstrate the impact of different corruption levels on the quality and diversity of the generated images.", "section": "3.1 Pre-training Evaluation"}, {"figure_path": "VFpXYBqMSU/figures/figures_5_2.jpg", "caption": "Figure 7: Qualitative evaluation of ControlNet and T2I-Adapter (a) IN-1K and (b) CC3M LDMs.", "description": "This figure shows a qualitative comparison of images generated using ControlNet and T2I-Adapter for both ImageNet-1K and Conceptual Captions 3M datasets.  It visualizes the impact of slight condition corruption during pre-training on the diversity and quality of generated images for different conditional image generation tasks.", "section": "3.2 Downstream Personalization Evaluation"}, {"figure_path": "VFpXYBqMSU/figures/figures_7_1.jpg", "caption": "Figure 8: Ablation with LDM-4 IN-1K. (a) FID and average L2 distance of conditional embeddings against clean ones with \u03b3 \u2208 {0.1, 0.5, 1.0, 5.0, 10.0}, indicated by square points (left to right). We compare with fixed synthetic corruption \u03b7 \u2208 {2.5, 5, 10, 15}%, shown by circle points. (b) CEP on corrupted IN-1K.", "description": "This figure shows the ablation study of the conditional embedding perturbation (CEP) method.  It compares the FID (Fr\u00e9chet Inception Distance) performance with different levels of perturbation added to the conditional embeddings in the training process.  Part (a) shows an ablation study of the perturbation magnitude (\u03b3), comparing it to the performance when training using a fixed corruption ratio (\u03b7).  Part (b) shows the performance of CEP-U and CEP-G on the corrupted ImageNet-1K dataset (IN-1K) with varying corruption levels.", "section": "5 Improving Diffusion Models with Conditional Embedding Perturbation"}, {"figure_path": "VFpXYBqMSU/figures/figures_8_1.jpg", "caption": "Figure 9: Comparison of DMs pre-trained with CEP against IP and without perturbation.", "description": "This figure shows a qualitative comparison of images generated by diffusion models (DMs) pre-trained with three different methods: clean (no corruption), input perturbation (IP), and conditional embedding perturbation (CEP).  The comparison is made across different model families (LDM-4 and DiT-XL/2 on ImageNet-1K, LDM-4 and LCM-v1.5 on CC3M) and across pre-training and downstream personalization tasks (using ControlNet).  The image samples illustrate the impact of each pre-training method on the quality and diversity of generated images, highlighting the advantages of CEP in improving image fidelity and diversity.", "section": "3.2 Downstream Personalization Evaluation"}, {"figure_path": "VFpXYBqMSU/figures/figures_33_1.jpg", "caption": "Figure 10: Annotation examples of IN-100.", "description": "This figure shows example images from the ImageNet-100 dataset used for downstream personalization.  Each row shows the same image with different annotations: the original image, the canny edge detection result, a segmentation mask generated by SegmentAnything, and the BLIP caption. This illustrates the types of annotations used to personalize the models for downstream tasks.", "section": "3.1 Pre-training Evaluation"}, {"figure_path": "VFpXYBqMSU/figures/figures_36_1.jpg", "caption": "Figure 11: Qualitative evaluation results of 50K images generated by class-conditional LDMs pre-trained on ImageNet-1K with synthetic corruptions. The images are generated with various guidance scales using 1K class conditions and compared with 50K validation images of ImageNet-1K.", "description": "This figure displays the quantitative results of image generation using class-conditional diffusion models pre-trained on ImageNet-1K with various levels of synthetic condition corruption.  Multiple metrics (FID, IS, Precision, Recall, sFID, TopPR F1, Top-1% RMD, Memorization Ratio, Avg L2 Dist, CLIP score) are plotted against different guidance scales, showing how slight corruption affects the quality and diversity of generated images compared to clean and heavily corrupted training data.", "section": "3.1 Pre-training Evaluation"}, {"figure_path": "VFpXYBqMSU/figures/figures_37_1.jpg", "caption": "Figure 11: Qualitative evaluation results of 50K images generated by class-conditional LDMs pre-trained on ImageNet-1K with synthetic corruptions. The images are generated with various guidance scales using 1K class conditions and compared with 50K validation images of ImageNet-1K.", "description": "This figure shows the quantitative evaluation results of 50,000 images generated by class-conditional Latent Diffusion Models (LDMs) pre-trained on ImageNet-1K with different levels of synthetic condition corruptions.  The evaluation metrics used include FID, IS, Precision, Recall, sFID, TopPR F1, Top-1% RMD, Memorization Ratio, Average L2 Distance, and CLIP Score. The x-axis represents the guidance scale used during image generation, and each line represents a different level of corruption in the pre-training data. The results indicate that slight corruption can improve the quality and diversity of the generated images as measured by these metrics. ", "section": "3.1 Pre-training Evaluation"}, {"figure_path": "VFpXYBqMSU/figures/figures_37_2.jpg", "caption": "Figure 11: Qualitative evaluation results of 50K images generated by class-conditional LDMs pre-trained on ImageNet-1K with synthetic corruptions. The images are generated with various guidance scales using 1K class conditions and compared with 50K validation images of ImageNet-1K.", "description": "This figure shows the quantitative evaluation results for 50,000 images generated by class-conditional Latent Diffusion Models (LDMs) pre-trained on ImageNet-1K with different levels of synthetic condition corruption.  The metrics used (FID, IS, Precision, Recall, sFID, TopPR F1, Top-1% RMD, and Memorization Ratio) are plotted against various guidance scales.  The results demonstrate the impact of different levels of corruption on the quality and diversity of the generated images compared to clean ImageNet-1K data.", "section": "3.1 Pre-training Evaluation"}, {"figure_path": "VFpXYBqMSU/figures/figures_38_1.jpg", "caption": "Figure 11: Qualitative evaluation results of 50K images generated by class-conditional LDMs pre-trained on ImageNet-1K with synthetic corruptions. The images are generated with various guidance scales using 1K class conditions and compared with 50K validation images of ImageNet-1K.", "description": "This figure presents a quantitative evaluation of images generated by class-conditional Latent Diffusion Models (LDMs) pre-trained on ImageNet-1K with varying degrees of synthetic condition corruption.  The evaluation metrics include FID (Fr\u00e9chet Inception Distance), IS (Inception Score), Precision, Recall, sFID, TopPR F1, Top-1% RMD (Relative Mahalanobis Distance), and Memorization Ratio.  The x-axis represents different guidance scales used during image generation. The results show how different levels of corruption in the pre-training data affect the quality and diversity of the generated images.", "section": "3.1 Pre-training Evaluation"}, {"figure_path": "VFpXYBqMSU/figures/figures_38_2.jpg", "caption": "Figure 11: Qualitative evaluation results of 50K images generated by class-conditional LDMs pre-trained on ImageNet-1K with synthetic corruptions. The images are generated with various guidance scales using 1K class conditions and compared with 50K validation images of ImageNet-1K.", "description": "This figure shows the quantitative evaluation results of 50,000 images generated by class-conditional Latent Diffusion Models (LDMs) pre-trained on ImageNet-1K with synthetically introduced corruptions at various levels.  The metrics used to evaluate the generated images include Fr\u00e9chet Inception Distance (FID), Inception Score (IS), Precision, Recall, sFID, Top-1% Relative Mahalanobis Distance (RMD), and average L2 distance.  The results show that slight corruption helps achieve a better trade-off between image quality (FID, IS, sFID) and diversity (RMD, L2). The different lines correspond to different corruption levels.", "section": "3.1 Pre-training Evaluation"}, {"figure_path": "VFpXYBqMSU/figures/figures_39_1.jpg", "caption": "Figure 11: Qualitative evaluation results of 50K images generated by class-conditional LDMs pre-trained on ImageNet-1K with synthetic corruptions. The images are generated with various guidance scales using 1K class conditions and compared with 50K validation images of ImageNet-1K.", "description": "This figure presents a comprehensive quantitative evaluation of 50,000 images generated by class-conditional Latent Diffusion Models (LDMs) trained on ImageNet-1K with varying levels of synthetic condition corruption.  The evaluation metrics include Fr\u00e9chet Inception Distance (FID), Inception Score (IS), Precision, Recall, sFID, TopPR F1, Top-1% Relative Mahalanobis Distance (RMD), Memorization Ratio, and Average L2 Distance.  The x-axis represents different guidance scales used during image generation, showcasing the impact of corruption on image quality and diversity at various guidance levels. The results demonstrate that slight condition corruption improves most metrics compared to clean pre-training, suggesting a benefit to introducing carefully controlled noise into the training data.", "section": "3.1 Pre-training Evaluation"}, {"figure_path": "VFpXYBqMSU/figures/figures_40_1.jpg", "caption": "Figure 11: Qualitative evaluation results of 50K images generated by class-conditional LDMs pre-trained on ImageNet-1K with synthetic corruptions. The images are generated with various guidance scales using 1K class conditions and compared with 50K validation images of ImageNet-1K.", "description": "This figure presents a comprehensive quantitative analysis of images generated by class-conditional Latent Diffusion Models (LDMs) pre-trained on ImageNet-1K with varying levels of synthetic condition corruption.  The evaluation metrics used include Fr\u00e9chet Inception Distance (FID), Inception Score (IS), Precision, Recall, sFID, TopPR F1, Top-1% Relative Mahalanobis Distance (RMD), and Memorization Ratio.  Each metric is plotted against different guidance scales, revealing the impact of corruption levels on image quality, fidelity, diversity, and memorization. The results show that slight corruption enhances model performance across several metrics.", "section": "3.1 Pre-training Evaluation"}, {"figure_path": "VFpXYBqMSU/figures/figures_41_1.jpg", "caption": "Figure 18: Visualization of LDMs IN-1K pre-training results.", "description": "This figure visualizes the results of pre-training Latent Diffusion Models (LDMs) on the ImageNet-1K dataset.  It shows example images generated by the LDMs with varying degrees of synthetic condition corruption.  The rows represent different classes of images (e.g., Ptarmigan, Carbonara, etc.), and the columns represent different levels of corruption, ranging from clean data to 20% corruption. The figure aims to visually demonstrate the impact of condition corruption on the generated images' quality and diversity.", "section": "3.1 Pre-training Evaluation"}, {"figure_path": "VFpXYBqMSU/figures/figures_42_1.jpg", "caption": "Figure 19: Visualization of DiT-XL/2 IN-1K pre-training results.", "description": "This figure visualizes the IN-1K pre-training results for the DiT-XL/2 model.  It shows a grid of images for several classes, with each row representing a different class (e.g., Tiger Shark, Ostrich, Junco, etc.).  Within each row, the images show the model's output at different levels of synthetic condition corruption (\u03b7), ranging from clean data (0%) to highly corrupted data (20%). This illustrates how the introduction of slight corruption affected image generation quality and diversity across various corruption levels.", "section": "3.1 Pre-training Evaluation"}, {"figure_path": "VFpXYBqMSU/figures/figures_43_1.jpg", "caption": "Figure 1: Visualization from class and text-conditional DMs pre-trained with clean, slight, and severe condition corruption. Slight corruption in pre-training improves the quality and diversity of images.", "description": "This figure displays image samples generated by different diffusion models trained with varying levels of synthetic condition corruption in their pre-training data.  The three columns represent models trained on 'clean' data (no corruption), data with 'slight' corruption, and data with 'severe' corruption. Each row shows examples of images generated based on a specific class or text prompt. The caption highlights that introducing a slight level of corruption during pre-training leads to higher image quality and diversity compared to using clean data or severely corrupted data.", "section": "3 Understanding the Pre-training Corruption in Diffusion Models"}, {"figure_path": "VFpXYBqMSU/figures/figures_44_1.jpg", "caption": "Figure 1: Visualization from class and text-conditional DMs pre-trained with clean, slight, and severe condition corruption. Slight corruption in pre-training improves the quality and diversity of images.", "description": "This figure shows sample images generated by different diffusion models (DMs).  The models were pre-trained using three different conditions: clean data, data with slight corruption, and data with severe corruption. The \"clean\" images show the results when the model is trained on accurate data.  The \"slight corruption\" images demonstrate that introducing minor errors during training enhances the overall quality and variety of the generated images.  Conversely, \"severe corruption\" shows a drop in quality and diversity, illustrating the optimal level of data corruption for enhancing DM performance.", "section": "3 Understanding the Pre-training Corruption in Diffusion Models"}, {"figure_path": "VFpXYBqMSU/figures/figures_45_1.jpg", "caption": "Figure 11: Qualitative evaluation results of 50K images generated by class-conditional LDMs pre-trained on ImageNet-1K with synthetic corruptions. The images are generated with various guidance scales using 1K class conditions and compared with 50K validation images of ImageNet-1K.", "description": "This figure presents a quantitative analysis of images generated by class-conditional diffusion models trained on ImageNet-1K with varying levels of synthetic condition corruption.  Multiple metrics are plotted against different guidance scales to assess the impact of corruption on image quality, fidelity, and diversity.  The metrics shown include FID, IS, Precision, Recall, sFID, TopPR F1, Top-1% RMD, and Memorization Ratio.  The results demonstrate that a small amount of corruption can improve several key metrics, while excessive corruption leads to diminished performance.", "section": "3.1 Pre-training Evaluation"}, {"figure_path": "VFpXYBqMSU/figures/figures_45_2.jpg", "caption": "Figure 11: Qualitative evaluation results of 50K images generated by class-conditional LDMs pre-trained on ImageNet-1K with synthetic corruptions. The images are generated with various guidance scales using 1K class conditions and compared with 50K validation images of ImageNet-1K.", "description": "This figure presents a comprehensive quantitative evaluation of images generated by class-conditional diffusion models pre-trained on ImageNet-1K with varying levels of synthetic condition corruption.  It uses multiple metrics (FID, IS, Precision, Recall, SFID, TopPR F1, Top-1% RMD, Memorization Ratio, Average L2 Distances, CLIP Score) to assess image quality, fidelity, diversity, and memorization across different guidance scales.  The results demonstrate the impact of slight corruption on these metrics, indicating its potential benefits for model training.", "section": "3.1 Pre-training Evaluation"}, {"figure_path": "VFpXYBqMSU/figures/figures_46_1.jpg", "caption": "Figure 11: Qualitative evaluation results of 50K images generated by class-conditional LDMs pre-trained on ImageNet-1K with synthetic corruptions. The images are generated with various guidance scales using 1K class conditions and compared with 50K validation images of ImageNet-1K.", "description": "This figure presents a quantitative evaluation of 50K images generated by class-conditional Latent Diffusion Models (LDMs) pre-trained on ImageNet-1K with synthetically introduced condition corruptions.  The evaluation metrics used include FID (Fr\u00e9chet Inception Distance), IS (Inception Score), Precision, Recall, sFID (modified FID), TopPR F1, Top-1% RMD (Relative Mahalanobis Distance), and Memorization Ratio.  Each metric is plotted against different guidance scales, revealing the impact of various corruption ratios (\u03b7) on the quality and diversity of generated images.  The results illustrate that slight corruption improves several image quality metrics, indicating the beneficial impact of this corruption on the pre-training process.", "section": "3.1 Pre-training Evaluation"}, {"figure_path": "VFpXYBqMSU/figures/figures_46_2.jpg", "caption": "Figure 11: Qualitative evaluation results of 50K images generated by class-conditional LDMs pre-trained on ImageNet-1K with synthetic corruptions. The images are generated with various guidance scales using 1K class conditions and compared with 50K validation images of ImageNet-1K.", "description": "This figure presents a quantitative evaluation of images generated by class-conditional Latent Diffusion Models (LDMs) pre-trained on ImageNet-1K with varying levels of synthetic condition corruption.  It shows multiple metrics plotted against guidance scale,  assessing the impact of corruption on image quality, fidelity, and diversity. The metrics include FID (Fr\u00e9chet Inception Distance), IS (Inception Score), Precision, Recall, sFID, TopPR F1, Top-1% RMD, Memorization Ratio, and Average L2 Distance. Each line represents a different corruption ratio, allowing for a comparison of the effects of different corruption levels on image generation.", "section": "3.1 Pre-training Evaluation"}, {"figure_path": "VFpXYBqMSU/figures/figures_47_1.jpg", "caption": "Figure 9: Comparison of DMs pre-trained with CEP against IP and without perturbation.", "description": "This figure shows a qualitative comparison of images generated by diffusion models (DMs) pre-trained with three different methods: clean data, input perturbation (IP), and conditional embedding perturbation (CEP).  The figure demonstrates the effect of each method on both pre-training and personalization stages.  For pre-training, diverse examples across various categories are shown, highlighting the effect of each method on image diversity and quality. In the personalization section, the same categories are shown with additional spatial control (Canny edge detection), further illustrating the impact of the pre-training methods on controlled image generation.", "section": "3.2 Downstream Personalization Evaluation"}, {"figure_path": "VFpXYBqMSU/figures/figures_47_2.jpg", "caption": "Figure 27: Visualization of LDMs IN-1K ControlNet SAM personalization results.", "description": "This figure shows the qualitative results of personalization experiments using ControlNet with SAM segmentation masks on LDMs pre-trained on IN-1K. Different corruption levels (\u03b7) are compared with clean and IP (input perturbation) baselines.  The images illustrate the effects of slight condition corruption on the diversity and quality of generated images across various control styles and corruption levels. The results show that using models trained with slight corruption levels produces superior image quality and diversity.", "section": "Downstream Personalization Evaluation"}, {"figure_path": "VFpXYBqMSU/figures/figures_48_1.jpg", "caption": "Figure 28: Visualization of LDMs CC3M ControlNet Canny personalization results.", "description": "This figure shows the qualitative results of personalization experiments on text-conditional LDMs pre-trained on CC3M using ControlNet with Canny edge as spatial control.  Different levels of synthetic condition corruption (\u03b7) are tested, ranging from clean (no corruption) to 20% corruption. The results are shown for four different prompts, demonstrating the effect of corruption level on the quality and diversity of generated images. Each row represents a different prompt, while each column shows the generated images under various corruption ratios. The spatial control (canny edge) is consistent across all conditions, revealing how corruption influences the generated images.", "section": "D Downstream Personalization Evaluation"}, {"figure_path": "VFpXYBqMSU/figures/figures_48_2.jpg", "caption": "Figure 29: Visualization of LDMs CC3M ControlNet SAM personalization results.", "description": "This figure shows the qualitative results of personalization experiments using ControlNet with SAM segmentation masks on text-conditional LDMs pre-trained on CC3M.  The results are presented for various levels of condition corruption (\u03b7).  Each row displays a specific prompt and the corresponding images generated by models trained with different levels of corruption, ranging from clean to highly corrupted.  The goal is to visually compare the image quality and diversity across different levels of corruption.", "section": "D.2 Qualitative Results"}, {"figure_path": "VFpXYBqMSU/figures/figures_49_1.jpg", "caption": "Figure 30: Visualization of CEP on IN-1K pre-trained LDM-4 and DiT-XL/2", "description": "This figure shows a qualitative comparison of images generated by two different diffusion models (LDM-4 and DiT-XL/2) trained on the ImageNet-1K dataset using three different methods: clean training, input perturbation (IP), and conditional embedding perturbation (CEP).  The images are arranged in rows, each row showing different classes from the dataset and the three generation methods, to compare their image quality and diversity. This figure helps demonstrate the impact of conditional embedding perturbation on the quality and variety of images produced by the diffusion models.", "section": "E Full Results of Conditional Embedding Perturbation"}, {"figure_path": "VFpXYBqMSU/figures/figures_50_1.jpg", "caption": "Figure 31: Visualization of CEP on CC3M pre-trained LDM-4 and LCM-v1.5", "description": "This figure visualizes the results of applying Conditional Embedding Perturbation (CEP) to pre-trained diffusion models, specifically Latent Diffusion Models (LDM-4) and Latent Consistency Models (LCM-v1.5), on the CC3M dataset.  It compares the image generation capabilities of models trained with clean data, data with input perturbations, and data with CEP-Uniform and CEP-Gaussian perturbations. Each row shows examples generated from different models under the same text prompt, illustrating the impact of CEP on image generation quality and diversity.  The results show that CEP generally leads to more visually appealing and realistic image generations than the other methods.", "section": "E Full Results of Conditional Embedding Perturbation"}, {"figure_path": "VFpXYBqMSU/figures/figures_51_1.jpg", "caption": "Figure 30: Visualization of CEP on IN-1K pre-trained LDM-4 and DiT-XL/2", "description": "This figure shows a qualitative comparison of images generated by two different diffusion models (LDM-4 and DiT-XL/2) pre-trained on ImageNet-1K using different methods: clean, input perturbation (IP), and conditional embedding perturbation (CEP). The images are grouped by class, and each class shows the images generated using each of the three methods, allowing for a visual comparison of the effect of the different pre-training methods on image quality and diversity. The results suggest that CEP can improve the quality and diversity of images generated by diffusion models.", "section": "E Full Results of Conditional Embedding Perturbation"}, {"figure_path": "VFpXYBqMSU/figures/figures_51_2.jpg", "caption": "Figure 33: Visualization of CEP on ControlNet adapted CC3M pre-trained LDM-4", "description": "This figure visualizes the impact of Conditional Embedding Perturbation (CEP) on the performance of a ControlNet adapted, CC3M pre-trained Latent Diffusion Model (LDM-4). It compares the image generation results from models trained with clean data, input perturbation (IP), uniform CEP (CEP-U), and Gaussian CEP (CEP-G).  The results show images generated in response to various text prompts.", "section": "E.1 Qualitative Results"}]