[{"figure_path": "3kDWoqs2X2/figures/figures_4_1.jpg", "caption": "Figure 1: The effect of step size (a or \u20ac) and number of MC samples (nsamp) on different EP variants in a stochastic version of the clutter problem of Minka [37]. EP (naive) uses maximum likelihood estimation for the updates, and EP (debiased) uses the estimator of Xu et al. [52]. Step size corresponds to a for EP, and \u20ac for EP-\u00b5 and EP-\u03b7. Only EP-n and EP-\u03bc can perform 1-sample updates, hence the other traces are not visible. The left panel shows the expected decrease in L after 100/nsamp steps. Performing e.g. 100\u00d7 1-sample steps, or 10\u00d7 10-sample steps, achieves a much larger decrease in L than a single 100-sample step. The right panel shows the magnitude of the bias in Ai after a single parallel update, averaged over all sites and dimensions. The bias of EP-\u03bc shrinks far faster as the step size decreases than that of EP. EP-n is always unbiased and so is not visible.", "description": "This figure analyzes the effect of step size and the number of Monte Carlo samples on different Expectation Propagation (EP) variants.  It shows that the new EP-\u03b7 and EP-\u03bc variants are more robust to Monte Carlo noise and more sample efficient than previous methods, particularly when using a single sample. The left panel displays the expected decrease in the loss function L, while the right panel shows the bias in the site parameter \u03bb\u1d62 after a single update.", "section": "3 Fearlessly stochastic EP algorithms"}, {"figure_path": "3kDWoqs2X2/figures/figures_7_1.jpg", "caption": "Figure 2: Pareto frontiers showing the number of NUTS steps (x-axis) against the KL divergence from p to an estimate of the optimum (y-axis). Each point on the plot marks the lowest average KL divergence attained by any hyperparameter setting by that step count. Error bars mark the full range of values for the marked hyperparameter setting across 5 random seeds.", "description": "This figure compares the performance of different expectation propagation (EP) variants in terms of the number of NUTS steps required to reach a certain level of accuracy, measured by the KL divergence from the approximate posterior to the true posterior. Each point represents the lowest average KL divergence achieved at a given number of NUTS steps, across different hyperparameter settings. The error bars indicate the range of KL divergences obtained across five random seeds.", "section": "4 Evaluation"}, {"figure_path": "3kDWoqs2X2/figures/figures_9_1.jpg", "caption": "Figure 3: Comparison of EP-\u03b7 with conjugate-computation variational inference (CVI) on a hierarchical logistic regression model. The two leftmost plots show forward (solid) and reverse (dashed) KL divergences between the approximation of each method and a MVN distribution estimated directly from MCMC samples. The left panel shows a comparison with respect to wall-clock time, when NUTS is used as the underlying sampling kernel for EP-\u03b7. The left-middle panel shows a similar comparison, but with respect to the number of samples drawn, and using an \"oracle\" sampling kernel for EP-\u03b7. Hyperparameters were tuned for each method and shaded regions show the range of trajectories across 5 random seeds. The right and right-middle panels show pairwise marginals of the various MVN approximations overlaid on contours of the true posterior. Coloured dots and ellipses correspond to means and 2-standard-deviation contours, respectively. See Section 5 for discussion of these results, and Appendix M for further details.", "description": "This figure compares the performance of EP-\u03b7 against conjugate-computation variational inference (CVI) on a hierarchical logistic regression model.  It shows KL divergence (forward and reverse) plots against both wall-clock time (using NUTS for sampling) and the number of samples drawn (using an \"oracle\" sampling kernel). Pairwise posterior marginals are also displayed to visually compare the accuracy of the different methods.", "section": "Limitations"}, {"figure_path": "3kDWoqs2X2/figures/figures_22_1.jpg", "caption": "Figure 4: Directed graphical model for the experiments of Section 4.", "description": "This figure shows the graphical model used for the experiments in Section 4.  It illustrates the hierarchical structure where global parameters *z* influence local latent variables *w* which in turn influence the observed data *D*. There are *m* repeated instances of the *w* \u2192 *D* portion of the model, one for each data partition.", "section": "Evaluation"}, {"figure_path": "3kDWoqs2X2/figures/figures_24_1.jpg", "caption": "Figure 1: The effect of step size (a or \u20ac) and number of MC samples (nsamp) on different EP variants in a stochastic version of the clutter problem of Minka [37]. EP (naive) uses maximum likelihood estimation for the updates, and EP (debiased) uses the estimator of Xu et al. [52]. Step size corresponds to a for EP, and \u20ac for EP-\u00b5 and EP-\u03b7. Only EP-n and EP-\u03bc can perform 1-sample updates, hence the other traces are not visible. The left panel shows the expected decrease in L after 100/nsamp steps. Performing e.g. 100\u00d7 1-sample steps, or 10\u00d7 10-sample steps, achieves a much larger decrease in L than a single 100-sample step. The right panel shows the magnitude of the bias in Ai after a single parallel update, averaged over all sites and dimensions. The bias of EP-\u03bc shrinks far faster as the step size decreases than that of EP. EP-n is always unbiased and so is not visible.", "description": "This figure compares the performance of different Expectation Propagation (EP) variants when using Monte Carlo (MC) sampling to estimate the updates. It shows that the new variants, EP-\u03b7 and EP-\u03bc, are more robust to MC noise and more sample-efficient than previous methods.  The left panel illustrates the improvement in the variational objective (L) achieved by using multiple 1-sample updates compared to a single large-sample update. The right panel demonstrates how the bias in the updates decreases as the step size reduces for EP-\u03bc, while EP-\u03b7 remains unbiased.", "section": "3 Fearlessly stochastic EP algorithms"}, {"figure_path": "3kDWoqs2X2/figures/figures_24_2.jpg", "caption": "Figure 1: The effect of step size (a or \u20ac) and number of MC samples (nsamp) on different EP variants in a stochastic version of the clutter problem of Minka [37]. EP (naive) uses maximum likelihood estimation for the updates, and EP (debiased) uses the estimator of Xu et al. [52]. Step size corresponds to a for EP, and \u20ac for EP-\u00b5 and EP-\u03b7. Only EP-\u03b7 and EP-\u03bc can perform 1-sample updates, hence the other traces are not visible. The left panel shows the expected decrease in L after 100/nsamp steps. Performing e.g. 100\u00d7 1-sample steps, or 10\u00d7 10-sample steps, achieves a much larger decrease in L than a single 100-sample step. The right panel shows the magnitude of the bias in Ai after a single parallel update, averaged over all sites and dimensions. The bias of EP-\u03bc shrinks far faster as the step size decreases than that of EP. EP-\u03b7 is always unbiased and so is not visible.", "description": "This figure compares the performance of different Expectation Propagation (EP) variants under different step sizes and numbers of Monte Carlo (MC) samples. It shows that the new variants, EP-\u03b7 and EP-\u03bc, are more robust to MC noise and more sample-efficient than previous methods, especially when using only a single sample per update.", "section": "Fearlessly stochastic EP algorithms"}, {"figure_path": "3kDWoqs2X2/figures/figures_25_1.jpg", "caption": "Figure 7: Pareto frontiers showing the number of seconds elapsed (x-axis) against the KL divergence from p to an estimate of the optimum (y-axis). Each point on the plot marks the lowest KL divergence attained by any hyperparameter setting by that time. Error bars mark the full range of values for the marked hyperparameter setting across 5 random seeds.", "description": "This figure presents Pareto frontiers illustrating the trade-off between computation time (in seconds) and the KL divergence from the approximate posterior (p) to the true posterior (obtained via a high-sample estimate).  Each point represents the lowest average KL divergence achieved at a given time point across five different random seeds. The error bars reflect the range of KL divergences observed across these seeds for each hyperparameter configuration.", "section": "4 Evaluation"}, {"figure_path": "3kDWoqs2X2/figures/figures_27_1.jpg", "caption": "Figure 3: Comparison of EP-\u03b7 with conjugate-computation variational inference (CVI) on a hierarchical logistic regression model. The two leftmost plots show forward (solid) and reverse (dashed) KL divergences between the approximation of each method and a MVN distribution estimated directly from MCMC samples. The left panel shows a comparison with respect to wall-clock time, when NUTS is used as the underlying sampling kernel for EP-\u03b7. The left-middle panel shows a similar comparison, but with respect to the number of samples drawn, and using an \u201coracle\u201d sampling kernel for EP-\u03b7. Hyperparameters were tuned for each method and shaded regions show the range of trajectories across 5 random seeds. The right and right-middle panels show pairwise marginals of the various MVN approximations overlaid on contours of the true posterior. Coloured dots and ellipses correspond to means and 2-standard-deviation contours, respectively. See Section 5 for discussion of these results, and Appendix M for further details.", "description": "This figure compares the performance of EP-\u03b7 and conjugate-computation variational inference (CVI) on a hierarchical logistic regression model.  The left two plots illustrate the KL divergence (both forward and reverse) between the approximations and a true posterior (estimated with MCMC).  The left plot shows time comparison, and the middle plot shows sample comparison using an \u2018oracle\u2019 sampling kernel for EP-\u03b7 to remove the effect of the sampler.  The right two plots show pairwise posterior marginals for both methods against the true posterior marginals, showing that EP-\u03b7 is closer to the truth. The discussion of the results and further details can be found in section 5 and appendix M.", "section": "Limitations"}, {"figure_path": "3kDWoqs2X2/figures/figures_28_1.jpg", "caption": "Figure 9: Synthetic data, generated using the cosmic radiation model of Vehtari et al. [48]. Each plot shows galactic far ultraviolet radiation (FUV) versus infrared radiation (i100) for a single sector of the observable universe.", "description": "This figure shows scatter plots of synthetic data generated to mimic the cosmic radiation data used in Vehtari et al. [48]. Each subplot represents a single sector of the observable universe, plotting the galactic far ultraviolet radiation (FUV) against the 100-\u00b5m infrared emission (i100). The data was generated using parameters manually tuned to match the qualitative properties of the original dataset.", "section": "J Cosmic radiation model"}]