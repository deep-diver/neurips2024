{"importance": "This paper is crucial for researchers working with large-scale machine learning models that involve matrix operations.  It provides **efficient and accurate methods for differentiating functions of large matrices**, a common bottleneck in many applications.  This opens up new avenues for developing more sophisticated and scalable models across diverse fields.", "summary": "This research presents novel adjoint methods for efficiently differentiating Lanczos and Arnoldi iterations, unlocking accurate gradients for large-matrix functions in machine learning.", "takeaways": ["Adjoint methods enable efficient differentiation of Lanczos and Arnoldi iterations for large matrices.", "The proposed approach yields exact gradients with linear time and memory complexity.", "Improved performance demonstrated in Gaussian process models, PDE solvers, and Bayesian neural networks."], "tldr": "Many machine learning models rely on evaluating functions of large matrices, often using Lanczos and Arnoldi iterations. However, differentiating these methods efficiently has been a challenge, hindering the development of more complex and powerful models.  Existing approaches often involve approximations, limiting accuracy and scalability.\nThis work introduces novel adjoint systems for Lanczos and Arnoldi iterations, overcoming previous limitations.  The proposed method is implemented in JAX and provides **exact gradients with linear time and memory complexity**. It significantly outperforms existing methods in various applications, demonstrating its efficiency and accuracy.", "affiliation": "Technical University of Denmark", "categories": {"main_category": "Machine Learning", "sub_category": "Deep Learning"}, "podcast_path": "RL4FXrGcTw/podcast.wav"}