[{"type": "text", "text": "Gradients of Functions of Large Matrices ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Nicholas Kra\u00a8mer, Pablo Moreno-Mun\u02dcoz, Hrittik Roy, S\u00f8ren Hauberg Technical University of Denmark Kongens Lyngby, Denmark {pekra, pabmo, hroy, sohau}@dtu.dk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tuning scientific and probabilistic machine learning models \u2013 for example, partial differential equations, Gaussian processes, or Bayesian neural networks \u2013 often relies on evaluating functions of matrices whose size grows with the data set or the number of parameters. While the state-of-the-art for evaluating these quantities is almost always based on Lanczos and Arnoldi iterations, the present work is the first to explain how to differentiate these workhorses of numerical linear algebra efficiently. To get there, we derive previously unknown adjoint systems for Lanczos and Arnoldi iterations, implement them in JAX, and show that the resulting code can compete with Diffrax when it comes to differentiating PDEs, GPyTorch for selecting Gaussian process models and beats standard factorisation methods for calibrating Bayesian neural networks. All this is achieved without any problem-specific code optimisation. Find the code at https://github.com/pnkraemer/experiments-lanczos-adjoints and install the library with pip install matfree. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Automatic differentiation has dramatically altered the development of machine learning models by allowing us to forego laborious, application-dependent gradient derivations. The essence of this automation is to evaluate Jacobian-vector and vector-Jacobian products without ever instantiating the full Jacobian matrix, whose column count would match the number of parameters of the neural network. Nowadays, everyone can build algorithms around matrices of unprecedented sizes by exploiting this matrix-free implementation. However, differentiable linear algebra for Jacobian-vector products and similar operations has remained largely unexplored to this day. We introduce a new matrix-free method for automatically differentiating functions of matrices. Our algorithm yields the exact gradients of the forward pass, all gradients are obtained with the same code, and said code runs in linear time- and memory-complexity. ", "page_idx": 0}, {"type": "text", "text": "For a parametrised matrix $A=A(\\theta)\\in\\mathbb{R}^{N\\times N}$ and an analytic function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ , we call $f(A)$ a function of the matrix (different properties of $A$ imply different definitions of $f(A)$ ; one of them is applying $f$ to each eigenvalue of $A$ if $A$ is diagonalisable; see [1]). However, we assume that $A$ is the Jacobian of a large neural network or a matrix of similar size and never materialise $f(A)$ . Instead, we only care about the values and gradients of the matrix-function-vector product ", "page_idx": 0}, {"type": "equation", "text": "$$\n(\\theta,v)\\mapsto f[A(\\theta)]v\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "assuming that $A$ is only accessed via differentiable matrix-vector products. Table 1 lists examples. ", "page_idx": 0}, {"type": "text", "text": "Evaluating Equation 1 is crucial for building large machine learning models, e.g., Bayesian neural networks: A common hyperparameter-calibration loss of a (Laplace-approximated) Bayesian neural network involves the log-determinant of the generalised Gauss\u2013Newton matrix [13] ", "page_idx": 0}, {"type": "equation", "text": "$$\nA(\\alpha):=\\sum_{(x_{i},y_{i})\\in\\mathrm{data}}[D_{\\theta}g](x_{i})^{\\top}[D_{g}^{2}\\rho](y_{i},g(x_{i}))[D_{\\theta}g](x_{i})+\\alpha^{2}I,\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "Table 1: Some applications for functions of matrices. Log-determinants apply by combining $\\log\\operatorname*{det}(A)=\\;\\operatorname{trace}\\left(\\log(A)\\right)$ with stochastic trace estimation, which is why most vectors in this table are Rademacher samples. \u201cPDE\u201d / \u201cODE\u201d $=$ \u201cPartial/Ordinary differential equation\u201d. ", "page_idx": 1}, {"type": "table", "img_path": "RL4FXrGcTw/tmp/919786f3e3d18c1093673ecc7acb4d878698cd7b7cacc55b23f867bab08d6654.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "where ${\\cal D}_{\\theta}g$ is the parameter-Jacobian of the neural network $g$ , $D_{g}^{2}\\rho$ is the Hessian of the loss function $\\rho$ with respect to $g(x_{i})$ , and $\\alpha$ is a to-be-tuned parameter. The matrix $A(\\alpha)$ in Equation 2 has as many rows and columns as the network has parameters, which makes traditional, cubic-complexity linear algebra routines for log-determinant estimation entirely unfeasible. To compute this log-determinant, one chooses between either (i) simplifying the problem by pretending that the Hessian matrix is more structured than it actually is, e.g., diagonal [14]; or (ii) approximating $\\log\\operatorname*{det}(A)$ by combining stochastic trace estimation [15] ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathrm{trace}\\left(A\\right)=\\mathbb{E}\\left[v^{\\top}A v\\right]\\approx\\frac{1}{L}\\sum_{\\ell=1}^{L}v_{\\ell}^{\\top}A v_{\\ell},\\quad\\mathrm{for}\\quad\\mathbb{E}\\left[v v^{\\top}\\right]=I,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "with a Lanczos iteration $A(\\theta)\\approx Q H Q^{\\top}$ [16], to reduce the log-determinant to [17, 18] ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\log\\operatorname*{det}(A)=\\operatorname{trace}\\left(\\log A\\right)\\approx\\frac{1}{L}\\sum_{\\ell=1}^{L}v_{\\ell}^{\\top}\\log(A)v_{\\ell}\\approx\\frac{1}{L}\\sum_{\\ell=1}^{L}v_{\\ell}^{\\top}Q\\log(H)Q^{\\top}v_{\\ell}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The matrix $H$ in $A\\approx Q H Q^{\\top}$ has as many rows/columns as we are willing to evaluate matrix-vector products with $A$ ; thus, it is small enough to evaluate the matrix-logarithm $\\bar{\\log(H)}$ in cubic complexity. ", "page_idx": 1}, {"type": "text", "text": "Contributions This article explains how to differentiate not just log-determinants but any Lanczos and Arnoldi iteration so we can build loss functions for large models with such matrix-free algorithms (thereby completing the pipeline in Figure 1). This kind of functionality has been sorely missing from the toolbox of differentiable programming until now, even though the demand for functions of matrices is high in all of probabilistic and scientific machine learning [e.g. 2\u201312, 19\u201331]. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "image", "img_path": "RL4FXrGcTw/tmp/58afbb6dcc72c14ff48346ad449ed4f6be2c68fe9795e41e798af24c97aa9102.jpg", "img_caption": ["Figure 1: Values (down) and gradients (up) of functions of large matrices. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Here, we focus on applications in machine learning and illustrate how prior work avoids differentiating matrixfree decomposition methods like the Lanczos and Arnoldi iterations. Golub and Meurant [32] discuss applications outside machine learning. ", "page_idx": 1}, {"type": "text", "text": "Generative models, e.g., normalising flows [27, 33], rely on the change-of-variables formula, which involves the log-determinant of the Jacobian matrix of a neural network. Behrmann et al. [9] and Chen et al. [10] combine stochastic trace estimation with a Taylor-series expansion for the matrix logarithm. Ramesh and LeCun [34] use Chebyshev expansions instead of Taylor expansions. That said, Ubaru et al. [17] demonstrate how both methods converge more slowly than the Lanczos iteration when combined with stochastic trace estimation. ", "page_idx": 1}, {"type": "text", "text": "Gaussian process model selection requires values and gradients of log-probability density functions of Gaussian distributions (which involve log-determinants), where the covariance matrix $A(\\theta)$ has as many rows and columns as there are data points [35]. Recent work [6, 7, 11, 19, 23, 36] all uses some combination of stochastic trace estimation with the Lanczos iteration, and unanimously identifies gradients of log-determinants as (\u201cd\u201d shall be an infinitesimal perturbation; see Section 4) ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mu:=\\log\\operatorname*{det}(K(\\theta)),\\quad\\mathrm{d}\\mu=\\operatorname{trace}\\left(K(\\theta)^{-1}\\mathrm{d}K(\\theta)\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Another round of stochastic trace estimation then estimates $\\mathrm{d}\\mu$ [6, 23, 36]. In contrast, our contribution is more fundamental: not only do we derive the exact gradients of the forward pass, but our formulation also applies to, say, matrix exponentials, whereas Equation 5 only works for log-determinants. Section 5 shows how our black-box gradients match state-of-the-art code for Equation 5 [6]. ", "page_idx": 2}, {"type": "text", "text": "Laplace approximations and neural tangent kernels face the same problem of computing derivatives of log-determinants but with the generalised Gauss\u2013Newton (GGN) matrix from Equation 2. In contrast to the Gaussian process literature, prior work on Laplace approximations prefers structured approximations of the GGN by considering subsets of network weights [37\u201339], or algebraic approximations of the GGN via diagonal, KFAC, or low-rank factors [31, 40\u201344]. All such approximations imply simple expressions for log-determinants, which are straightforward to differentiate automatically. Unfortunately, these approximations discard valuable information about the correlation between weights, so a linear-algebra-based approach leads to superior likelihood calibration (Section 7). ", "page_idx": 2}, {"type": "text", "text": "Linear differential equations, for instance ${\\dot{y}}(t)=A y(t)$ , $y(0)=y_{0}$ are solved by matrix exponentials, $y(t)=\\exp(A t)y_{0}$ . By this relation, matrix exponentials have frequent applications not just for the simulation of differential equations [e.g. 2, 45], but also for the construction of exponential integrators [3, 26, 29], state-space models [5, 46], and in generative modelling [4, 26, 28, 47]. There are many ways of computing matrix exponentials [48, 49], but only Al-Mohy and Higham [50] consider the problem of differentiating it and only in forward mode. In contrast, differential equations have a rich history of adjoint methods [e.g. 51, 52] with high-performance open-source libraries [53\u201356]. Still, the (now differentiable) Arnoldi iteration can compete with state-of-the-art solvers in JAX (Section 6). ", "page_idx": 2}, {"type": "text", "text": "3 Problem statement ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Recall $A=A(\\theta)\\in\\mathbb{R}^{N\\times N}$ from Section 1. The focus of this paper is on matrices that are too large to store in memory, like Jacobians of neural networks or discretised partial differential equations: ", "page_idx": 2}, {"type": "text", "text": "Assumption 3.1. $A(\\theta)$ is only accessed via differentiable matrix-vector products $(\\theta,v)\\mapsto A(\\theta)v$ . ", "page_idx": 2}, {"type": "text", "text": "The de-facto standard for linear algebra under Assumption 3.1 are matrix-free algorithms [e.g. 57, Chapters 10 & 11], like the conjugate gradient method for solving large sparse linear systems [58]. But there is more to matrix-free linear algebra than conjugate gradient solvers: ", "page_idx": 2}, {"type": "text", "text": "Matrix-free implementations of matrix decompositions usually revolve around variations of the Arnoldi iteration [59], which takes an initial vector $\\boldsymbol{v}\\,\\in\\,\\mathbb{R}^{N}$ and a prescribed number of iterations $K\\in\\mathbb N$ and produces a column-orthogonal $Q\\in\\mathbb{R}^{N\\times K}$ , structured $\\dot{H}\\in\\mathbb{R}^{K\\times K}$ , residual vector $r\\in\\mathbb{R}^{N}$ , and length $c\\in\\mathbb{R}$ such that ", "page_idx": 2}, {"type": "image", "img_path": "RL4FXrGcTw/tmp/6df6b10ab8418f0723331598900364b10a6690eca78d245dbd7928bb7d517133.jpg", "img_caption": ["Figure 2: Lanczos/Arnoldi iteration. "], "img_footnote": [], "page_idx": 2}, {"type": "equation", "text": "$$\nA Q=Q H+r(e_{K})^{\\top},\\quad{\\mathrm{and}}\\quad Q e_{1}=c v\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "hold (Figure 2; $e_{1},e_{K}\\in\\mathbb{R}^{K}$ are the first and last unit vectors). If $A$ is symmetric, $H$ is tridiagonal, and the Arnoldi iteration becomes the Lanczos iteration [16]. Both iterations are popular for implementing matrix-function-vector products in a matrix-free fashion [1, 57], because the decomposition in Equation 6 implies $A\\approx Q H\\dot{Q}^{\\top}$ , thus ", "page_idx": 2}, {"type": "equation", "text": "$$\n(\\theta,v)\\mapsto f(A(\\theta))v\\approx Q f(H)Q^{\\top}v=c^{-1}Q f(H)e_{1}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The last step, $Q^{\\top}v=c^{-1}e_{1}$ , is due to the orthogonality of $Q$ . Since the number of matrix-vector products $K$ rarely exceeds a few hundreds or thousands, the following Assumption 3.2 is mild: ", "page_idx": 2}, {"type": "text", "text": "Assumption 3.2. The map $H\\mapsto f(H)e_{1}$ is differentiable, and $Q$ fits into memory. ", "page_idx": 2}, {"type": "text", "text": "In summary, we evaluate functions of large matrices by firstly decomposing a large matrix into a product of small matrices (with Lanczos or Arnoldi) and, secondly, using conventional linear algebra to evaluate functions of small matrices. Functions of small matrices can already be differentiated efficiently [60\u201362]. This work contributes gradients of the Lanczos and Arnoldi iteration under Assumptions 3.1 and 3.2, and thereby makes matrix-free implementations of matrix decompositions and functions of large matrices (reverse-mode) differentiable. ", "page_idx": 2}, {"type": "text", "text": "Automatic differentiation, i.e. \u201cbackpropagating through\u201d the matrix decomposition, is far too inefficient to be a viable option (Figure 3; setup in Appendix A). Our approach via implicit differentiation or the adjoint method, respectively, leads to gradients that inherit the linear runtime and memory-complexity of the forward pass. ", "page_idx": 3}, {"type": "text", "text": "Limitations and future work The landscape of Lanczos  \nand Arnoldi-style matrix decompositions is vast, and some   \nadjacent problems cannot be solved by this single article:   \n(i) Forward-mode derivatives would require a derivation   \nseparate from what comes next. Yet, since functions of   \nmatrices map many to few parameters (matrices to vec  \ntors), reverse-mode is superior to forward-mode anyway   \n[66, p. 153]. (ii) We only consider real-valued matrices joint method on a sparse matrix [63\u201365]. ", "page_idx": 3}, {"type": "image", "img_path": "RL4FXrGcTw/tmp/fd32c6981b11f9fd7ffbbddac6441657dac27066ff4ced250a78dc626fa9b6d1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 3: Backpropagation vs our ad(for their relevance to machine learning), even though the decompositions generalise to complex arithmetic with applications in physics [67]. (iii) We assume $Q$ fits into memory, which relates to combining Arnoldi/Lanczos with full reorthogonalisation [57, 68\u201371]. Relaxing this assumption requires gradients of partial reorthogonalisation (among other things), which we leave to future work. ", "page_idx": 3}, {"type": "text", "text": "4 The method: Adjoints of the Lanczos and Arnoldi iterations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Numerical algorithms are rarely differentiated automatically, and usually, some form of what is known as \u201cimplicit differentiation\u201d [66, 72] applies. The same is true for the Lanczos and Arnoldi iterations. However, and perhaps surprisingly, we differentiate the iterations like a dynamical system using the \u201cadjoint method\u201d [51, 73, 74], a variation of implicit differentiation that uses Lagrange multipliers [66], and not like a linear algebra routine [60, 61, 75]. To clarify this distinction, we briefly review implicit differentiation before the core contributions of this work in Sections 4.1 and 4.2. ", "page_idx": 3}, {"type": "text", "text": "Notation Let ${\\mathrm{d}}x$ be an infinitesimal perturbation of some $x$ . $D$ is the Jacobian operator, and $\\langle\\cdot,\\cdot\\rangle$ the Euclidean inner product between two equally-sized inputs. For a loss $\\rho\\in\\mathbb{R}$ that depends on some $x$ , the linearisation $\\mathrm{d}\\rho=D_{x}\\rho\\,\\mathrm{d}x$ and the gradient identity $\\mathrm{d}\\rho=\\langle\\nabla_{x}\\rho,\\mathrm{d}x\\rangle$ will be important [76, 77]. ", "page_idx": 3}, {"type": "text", "text": "Implicit differentiation Let $\\mathbf{a}:\\theta\\mapsto x$ be a numerical algorithm that computes some $x$ from some $\\theta$ . Assume that the input and output of $\\mathbf{a}(\\cdot)$ satisfy the constraint $\\mathbf{c}(\\theta,\\mathbf{a}(\\theta))=0$ . For instance, if $\\mathbf{a}(\\cdot)$ solves $A x=b$ , the constraint is $\\mathbf{c}(A,b;x)=A x-b$ with $\\theta:=\\{A,b\\}$ . We can use $\\mathbf{c}(\\cdot)$ in combination with the chain rule to find the derivatives of $\\mathbf{a}(\\cdot)$ , $\\mathbf{\\dot{c}}=0$ implies $\\mathrm{d}\\mathbf{c}=0$ ) ", "page_idx": 3}, {"type": "equation", "text": "$$\n0=\\mathrm{d}\\mathbf{c}(\\theta,x)=D_{x}\\mathbf{c}(\\theta,x)\\mathrm{d}x+D_{\\theta}\\mathbf{c}(\\theta,x)\\mathrm{d}\\theta.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In other words, we \u201clinearise\u201d the constraint $\\mathbf{c}(\\theta,x)\\,=\\,0$ . The adjoint method [78] proceeds by \u201ctransposing\u201d this linearisation as follows. Let $\\rho$ be a loss that depends on $y$ with gradient $\\nabla_{y}\\rho$ and recall the gradient identity from the \u201cNotation\u201d paragraph above. Then, for all Lagrange multipliers $\\lambda$ with the same shape as the outputs of $\\mathbf{c}(\\cdot)$ , we know that since $\\mathbf{c}=0$ implies $\\mathrm{d}\\mathbf{c}=0$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}\\rho=\\langle\\nabla_{x}\\rho,\\mathrm{d}x\\rangle=\\langle\\nabla_{x}\\rho,\\mathrm{d}x\\rangle+\\langle\\lambda,\\mathrm{d}\\mathbf{c}\\rangle=\\langle\\nabla_{x}\\rho+(D_{x}\\mathbf{c})^{\\top}\\lambda,\\mathrm{d}x\\rangle+\\langle(D_{\\theta}\\mathbf{c})^{\\top}\\lambda,\\mathrm{d}\\theta\\rangle\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "must hold. By matching Equation 9 to $\\mathrm{d}\\rho=\\langle\\nabla_{\\theta}\\rho,\\mathrm{d}\\theta\\rangle$ (this time, regarding $\\rho$ as a function of $\\theta$ , not of $x$ ; recall the \u201cNotation\u201d paragraph), we conclude that if $\\lambda$ solves the adjoint system ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla_{x}\\rho+\\left(D_{x}\\mathbf{c}\\right)^{\\top}\\lambda=0,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "then $\\nabla_{\\theta}\\rho:=(D_{\\theta}\\mathbf{c})^{\\top}\\lambda$ must be the gradient of $\\rho$ with respect to input $\\theta$ . This is the adjoint method [66, Section 10.4]. In automatic differentiation frameworks like JAX [79], this gradient implements a vector-Jacobian product with the Jacobian of $\\mathbf{a}(\\cdot)$ \u2013 implicitly via the Lagrange multiplier $\\lambda$ , without differentiating \u201cthrough\u201d $\\mathbf{a}(\\cdot)$ explicitly. In comparison to approaches that explicitly target vector-Jacobian products with implicit differentiation [like 66, Proposition 10.1], the adjoint method shines when applied to highly structured, non-vector-valued constraints, such as dynamical systems or the Lanczos and Arnoldi iterations. The reason is that the adjoint method does not change if $\\mathbf{c}(\\cdot)$ becomes matrix- or function-space-valued, as long as we can define inner products and adjoint operators, whereas other approaches (like what Blondel et al. [72] use for numerical optimisers) would become increasingly laborious in these cases. In summary, to reverse-mode differentiate a numerical algorithm with the adjoint method, we need four steps: (i) find a constraint, (ii) linearise it, (iii) introduce Lagrange multipliers, and (iv) solve the resulting adjoint system. Carrying out those four steps for the Lanczos and Arnoldi iterations is the main contribution of the paper: Section 4.1 states both adjoint systems and Section 4.2 covers a matrix-free implementation. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "4.1 Adjoint system of the Arnoldi and Lanczos iterations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Let $e_{j}$ be the $j$ th unit vector. Denote by \u201c\u25e6\u201d the element-wise matrix product, and define the matrices ", "page_idx": 4}, {"type": "equation", "text": "$$\nI_{\\leq}:=[\\delta_{i\\leq j}]_{i,j=1}^{K},\\quad I_{<}:=[\\delta_{i<j}]_{i,j=1}^{K},\\quad I_{\\ll}:=[\\delta_{i+1<j}]_{i,j=1}^{K},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "so that for example, $I_{\\leq}\\circ A$ extracts the lower triangular matrix of $A$ (including the diagonal), and $I_{\\ll}\\circ A\\,=\\,0$ enforces Hessenberg form [57]. The following two theorems do not require Assumptions 3.1 and 3.2, which are only relevant for analysing the computational complexities. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1 (Adjoint system of the Arnoldi iteration). Let $K\\in\\mathbb{N}$ , $v\\in\\mathbb{R},$ , and $A\\in\\mathbb{R}^{N\\times N}$ , and $a$ loss $\\rho(\\cdot)\\in\\mathbb{R}$ be given. If $Q\\in\\mathbb{R}^{N\\times K}$ , $H\\in\\mathbb{R}^{K\\times K}$ , $\\boldsymbol{r}\\in\\mathbb{R}^{N}$ , and $c\\in\\mathbb{R}$ solve the forward constraint ", "page_idx": 4}, {"type": "equation", "text": "$$\nA Q=Q H+r(e_{K})^{\\top},\\quad Q e_{1}=v c,\\quad I_{\\leq}\\circ[Q^{\\top}Q]=I,\\quad I_{\\ll}\\circ H=0,\\quad Q^{\\top}r=0,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0=\\nabla_{Q}\\rho+A^{\\top}\\Lambda-\\Lambda H^{\\top}+\\lambda(e_{1})^{\\top}+Q(I_{\\leq}\\circ\\Gamma)+Q(I_{\\leq}\\circ\\Gamma)^{\\top}+r\\gamma^{\\top}}\\\\ &{0=\\nabla_{H}\\rho-Q^{\\top}\\Lambda+I_{\\ll}\\circ\\Sigma}\\\\ &{0=\\nabla_{r}\\rho-\\Lambda e_{K}+Q\\gamma}\\\\ &{0=\\nabla_{c}\\rho-v^{\\top}\\lambda,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "then the gradients of $\\rho$ with respect to $A$ and v are ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{A}\\rho:=\\Lambda Q^{\\top},\\quad\\nabla_{v}\\rho:=\\lambda c.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Sketch of the proof. To derive the statement, start with Equation 12 as $\\mathbf{c}(\\cdot)$ . Apply the chain- and product rules liberally to get $\\operatorname{d}\\!\\mathbf{c}(\\cdot)$ . Introduce Lagrange multipliers $\\lambda,\\,\\Lambda,\\,\\gamma,\\,\\Gamma$ , and $\\Sigma$ like in the previous section, by adding Lagrange-multiplied constraints to ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{d}\\rho=\\langle\\nabla_{Q}\\rho,\\mathrm{d}Q\\rangle+\\langle\\nabla_{H}\\rho,\\mathrm{d}H\\rangle+\\langle\\nabla_{r}\\rho,\\mathrm{d}r\\rangle+\\langle\\nabla_{c}\\rho,\\mathrm{d}c\\rangle,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and rearrange the terms to see that Equation 13 implies Equation 14. Details are in Appendix B. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.2 (Adjoint system of the Lanczos iteration). Let a symmetric $A\\,\\in\\,\\mathbb{R}^{N\\times N}$ , as well as $\\boldsymbol{v}\\ \\in\\ \\mathbb{R}^{N}$ , $\\bar{K}\\,\\in\\,\\mathbb{N}$ , and a loss $\\rho$ be known. In the following equations, set $b_{0}\\;:=\\;1\\;\\in\\;\\mathbb{R},$ , $x_{0}:=0\\in\\mathbb{R}^{n},\\lambda_{K+1}:=0,\\,\\mu_{0}:=0,\\,\\nu_{0}:=0\\,\\iota$ to simplify the expressions. If $x_{1},...,x_{K+1}\\in\\mathbb{R}^{N}$ , and $a_{1},...,a_{k},b_{1},...,b_{k}\\in\\mathbb{R}^{K}$ , satisfy the forward constraint ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{x_{1}-v/(v^{\\top}v)=0,}&{\\qquad\\qquad k=0,}\\\\ {-b_{k-1}x_{k-1}+(A-a_{k}I)x_{k}-b_{k}x_{k+1}=0,}&{\\qquad\\qquad k=1,...,K}\\\\ {x_{k+1}^{\\top}x_{k+1}-1=0,}&{\\qquad\\qquad k=1,...,K,}\\\\ {x_{k-1}^{\\top}x_{k}=0,}&{\\qquad\\qquad k=2,...,K+1}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and if $\\mathbf{\\Psi}^{\\lambda_{0},\\,...,\\,\\lambda_{K}}\\in\\mathbb{R}^{N}$ , $\\mu_{1},...,\\mu_{K},\\nu_{1},...,\\nu_{K}\\in\\mathbb{R}$ satisfy the adjoint system ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0=-\\lambda_{K}b_{K}+(\\nabla_{x_{K+1}}\\rho+\\mu_{K}x_{K+1}+\\nu_{K}x_{K}),\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(\\omega_{K-1})}\\\\ &{0=-b_{k}\\lambda_{k+1}+(A^{\\top}-a_{k}I)\\lambda_{k}-b_{k-1}\\lambda_{k-1}+(\\nabla_{x_{k}}\\rho+\\mu_{k-1}x_{k}+\\nu_{k}x_{k+1}+\\nu_{k-1}x_{k-1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{0}=\\nabla_{a_{k}}\\rho-\\lambda_{k}^{\\top}x_{k},}\\\\ &{\\mathrm{0}=\\nabla_{b_{k}}\\rho-\\lambda_{k+1}^{\\top}x_{k}-\\lambda_{k}^{\\top}x_{k+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where all expressions involving $k$ hold for all $k=K,...,1$ , then ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{v}\\rho:=\\frac{\\lambda_{0}^{\\top}x_{1}}{v^{\\top}v}x_{1}-\\lambda_{0},\\quad\\nabla_{A}\\rho:=\\sum_{k=1}^{K}\\lambda_{k}x_{k}^{\\top}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "are the gradients of $\\rho$ with respect to v and $A$ . ", "page_idx": 4}, {"type": "text", "text": "Sketch of the proof. This theorem is proven similarly to that of Theorem 4.1, but instead of a few equations involving matrices, we have many equations involving scalars because for symmetric matrices, $H$ must be tridiagonal [57], and we expand $A Q=Q H+r(e_{K+1})^{\\top}$ column-wise. The coefficients $a_{k}$ and $b_{k}$ are the tridiagonal elements in $H$ . We rename $q_{k}$ from Arnoldi to $x_{k}$ for Lanczos to make it easier to distinguish the two different sets of constraints. Details: Appendix C. ", "page_idx": 5}, {"type": "text", "text": "4.2 Matrix-free implementation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Solving the adjoint systems To compute $\\nabla_{A}\\rho$ and $\\nabla_{v}\\rho$ , we need to solve the adjoint systems. When comparing the forward constraints to the adjoint systems, similarities emerge: for instance, the adjoint system of the Arnoldi iteration follows the same $\\dot{A}^{(\\top)}H-\\Lambda H^{(\\top)}+\\mathrm{rest}=0$ structure as the forward constraint. This structure suggests deriving a recursion for the backward pass that mirrors that of the forward pass. Appendix E contains this derivation and contrasts the resulting algorithm with that of the forward pass. The main observation is that the complexity of the adjoint passes for Lanczos and Arnoldi mirrors that of the forward passes. Gradients can be implemented purely with matrix-vector products, which is helpful because it makes our custom backward pass as matrixfree as backpropagation \u201cthrough\u201d the forward pass would be. This matrix-free implementation in combination with the efficient recursions in Theorems 4.1 and 4.2 explains the significant performance gains of our method compared to naive backpropagation, observed in Figure 3. ", "page_idx": 5}, {"type": "text", "text": "Theorems 4.1 and 4.2\u2019s expressions for $\\nabla_{A}\\rho$ are not directly applicable when we only have matrixvector products with $A$ . Fortunately, parameter-gradients emerge from matrix-gradients: ", "page_idx": 5}, {"type": "text", "text": "Corollary 4.3 (Parameter gradients). Under Assumption 3.1 and the assumptions of Theorem 4.1, and if $A$ is parametrised by some $\\theta$ , the gradients of $\\rho$ with respect to $\\theta$ are ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\rho=\\sum_{k=1}^{K}\\nabla\\left[\\theta\\mapsto(e_{k})^{\\top}Q^{\\top}A(\\theta)^{\\top}\\Lambda e_{k}\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which can be assembled online during the backward pass. For the Lanczos iteration, we assume the conditions of Theorem 4.2 instead of Theorem 4.1, replace $Q e_{k}$ and $\\Lambda e_{k}$ with $x_{k}$ and $\\lambda_{k}$ , let the sum run from $k=0$ to $k=K$ , and the rest of this statement remains true. ", "page_idx": 5}, {"type": "text", "text": "Sketch of the proof. The proof of this identity combines the expression(s) for $\\nabla_{A}\\rho$ from Theorems 4.1 and 4.2 with $\\mathrm{d}A=D_{\\theta}A\\mathrm{d}\\theta$ . The derivations are lengthy and therefore relegated to Appendix D. ", "page_idx": 5}, {"type": "text", "text": "Reorthogonalisation It is well known that the Lanczos and Arnoldi iterations suffer from a loss of orthogonality and that reorthogonalisation of the columns in $Q$ is often necessary [68\u201371]. Reorthogonalisation does not affect the forward constraints, so the adjoint systems remain the same with and without reorthogonalisation. But adjoint systems also suffer from a loss of orthogonality: The equivalent of orthogonality for the adjoint system is the projection constraint in Equation 13b, which constrains the Lagrange multipliers $\\Lambda$ to a hyperplane defined by $Q$ and other known quantities. The constraint can \u2013 and should (Table 2) \u2013 be used whenever the forward pass requires reorthogonalisation.1 In the case studies below, we always use full reorthogonalisation on the forward and adjoint pass, also for the Arnoldi iteration [71, Table 7.1], even though this is slightly less common than for the Lanczos iteration. ", "page_idx": 5}, {"type": "table", "img_path": "RL4FXrGcTw/tmp/e670ce8b203fb940ea312a5a7e55ff6c45341ade95f7fc0e908d7b2f48cc1dbc.jpg", "table_caption": ["Table 2: Accuracy loss when differentiating the Arnoldi iteration on a Hilbert matrix in double precision ( $\\mathit{\\Delta}\\left(\\phi\\right)$ : decompose with a full-rank Arnoldi iteration, then reconstruct the original matrix; measure $\\lVert\\partial\\boldsymbol{\\phi}-\\boldsymbol{I}\\rVert$ ; details in Appendix F). "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Summary (before the case studies) The main takeaway from Sections 4.1 and 4.2 is that now, we do not only have closed-form expressions for the gradients of Arnoldi and Lanczos iterations (Theorems 4.1 and 4.2), but that we can compute them in the same complexity as the forward pass, in a numerically stable way, and evaluate parameter-gradients in linear time- and spacecomplexity (Corollary 4.3). While some of the derivations are somewhat technical, the overall approach follows the general template for the adjoint method relatively closely. The resulting algorithm beats backpropagation \u201cthrough\u201d the iterations by a margin in terms of speed (Figure 3) and enjoys the same stability gains from reorthogonalisation as the forward pass (Table 2). Our open-source implementation of reverse-mode differentiable Lanczos and Arnoldi iterations can be installed via \u201cpip install matfree\u201d. Next, we put this code to the test on three challenging machine-learning problems centred around functions of matrices to see how it fares against state-ofthe-art differentiable implementations of exact Gaussian processes (Section 5), differential equation solvers (Section 6), and Bayesian neural networks (Section 7). ", "page_idx": 5}, {"type": "table", "img_path": "RL4FXrGcTw/tmp/b7d5f328969af3cdd9e1fc6bab859c383b460e72644cd24303c42f8e5d4a7661.jpg", "table_caption": ["Table 3: Our method yields the same root-mean-square errors (RMSEs) as GPyTorch. It reaches lower training losses but is $\\approx20\\times$ slower per epoch due to different matrix-vector-product backends (see Appendix G). Three runs, significant improvements in bold. We use an 80/20 train/test split. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5 Case study: Exact Gaussian processes ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Model selection for Gaussian processes has arguably been the strongest proponent of the Lanczos iteration and similar matrix-free algorithms in recent years [6, 7, 11, 19, 20, 23, 80], and most of these efforts have been bundled up in the GPyTorch library [6]. For example, GPyTorch defaults to choosing a Lanczos iteration over a Cholesky decomposition as soon as the dataset exceeds 800 data points.2 Calibrating hyperparameters of Gaussian process models involves optimising logmarginal-likelihoods of the regression targets, which requires computing $x^{\\top}A^{-1}x$ and $\\log\\operatorname*{det}(\\bar{A})$ for a covariance matrix $A$ with as many rows and columns as there are data points. Recent works [6, 7, 11, 19, 20, 23, 80] unanimously suggest to differentiate log-determinants via $\\mu:=\\{\\mathrm{race}\\left(\\log(A)\\right)$ and $\\mathrm{d}\\mu\\,=\\,\\mathrm{trace}\\left(A^{-1}\\mathrm{d}A\\right)$ (Equation 5). Since we seem to be the first to take a different path, benchmarking Gaussian processes in comparison to GPyTorch is a good first testbed for our gradients. ", "page_idx": 6}, {"type": "text", "text": "Setup: Like GPyTorch\u2019s defaults We mimic recent suggestions for scalable Gaussian process models [7, 19]: we implement a pivoted Cholesky preconditioner [81] and combine it with conjugate gradient solvers for $x^{\\top}K^{\\pm}x$ (which can be differentiated efficiently). We estimate the log-determinant stochastically via $\\log\\operatorname*{det}(A)=\\;\\operatorname{trace}\\left(\\log(A)\\right)=E[v^{\\top}\\log(A)v],$ , and compute $\\log(A)v$ via the Lanczos iteration. While all of the above is common for \u201cexact\u201d Gaussian processes [6, 7, 19] (\u201cexact\u201d as opposed to variational approaches, which are not relevant for this comparison), there are three key differences between our code and GPyTorch\u2019s: (i) GPyTorch is in Pytorch and uses KeOps [82] for efficient kernel-matrix-vector products. We use JAX and must build our own low-memory matrix-vector products (Appendix G). (ii) GPyTorch runs all algorithms adaptively (we specify tolerances and maximum iterations as much as possible). We use adaptive conjugate gradient solvers and fixed ranks for everything else. (iii) GPytorch differentiates the log-determinant with a tailored approximation of Equation 5 [6]; we embed our gradients of the Lanczos iteration into automatic differentiation. To keep the benchmark objective, we mimic the parameter suggestions from GPyTorch\u2019s default settings, and optimise hyperparameters of a $\\mathrm{{Mat}\\dot{\\epsilon}r n\\left(\\frac{3}{2}\\right)}$ model on UCI datasets with the Adam optimiser [83]. Appendix H lists parameters and discusses the datasets. ", "page_idx": 6}, {"type": "image", "img_path": "RL4FXrGcTw/tmp/132234d10bdc5fa42bc88eb19c83116a151a934102526f90b50570bb42b51c38.jpg", "img_caption": ["Figure 5: Arnoldi\u2019s superior convergence on the forward pass (A1) is inherited by the gradients (A2; mind the shared $y$ -axis) and ultimately leads to fast training (B). For training, Arnoldi uses ten matrix-vector products, and the other two use 15 (so they have equal error $\\approx10^{-4}$ in A1 and A2.) "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Analysis: Trains like GPyTorch; large scale only limited by matrix-vector-product backends In this benchmark, we are looking for low reconstruction errors, fast runtimes and well-behaved loss functions. Table 3 shows that this is the case for both implementations: the reconstruction errors are essentially the same, and both methods converge well (we achieve lower training losses). This result shows that by taking a numerically exact gradient of the Lanczos iteration, and leaving everything else to automatic differentiation, matches the performance of state-of-the-art solvers. Larger datasets are only limited by the efficiency of our matrix-vector products (in comparison to KeOps); Appendix G discusses this in detail. Overall, this result strengthens the democratisation of exact Gaussian processes because it reveils a simple yet effective alternative to GPyTorch\u2019s domain-specific gradients. ", "page_idx": 7}, {"type": "text", "text": "6 Case study: Physics-informed machine learning with PDEs ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Much of the paper thus far discusses functions of matrices in the context of log-determinants. So, in order to demonstrate performance for (i) a problem that is not a log-determinant and (ii) for a non-symmetric matrix which requires Arnoldi instead of Lanczos, we learn the coefficient field $\\omega$ of ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}}{\\partial t^{2}}u(t;x_{1},x_{2})=\\omega(x_{1},x_{2})^{2}\\left[\\frac{\\partial^{2}}{\\partial x_{1}^{2}}u(t;x_{1},x_{2})+\\frac{\\partial^{2}}{\\partial x_{2}^{2}}u(t;x_{1},x_{2})\\right]\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "subject to Neumann boundary conditions. We discretise this equation on a $128\\times128$ grid in space and transform the resulting $\\mathrm{i28^{2}}$ -dimensional second-order ordinary differential equation into a first-order differential equation, $\\dot{w}=A w$ , $w(0)=w_{0}$ , with solution operator $w(t)\\stackrel{*}{=}\\exp(A t)w_{0}$ . The system matrix $A$ is sparse, asymmetric, and has 32, 768 rows and columns. We sample a true $\\omega$ from a Gaussian process with a square exponential kernel and generate data by sampling 256 initial conditions and solving the equation numerically with high precision. Details are in Appendix I. ", "page_idx": 7}, {"type": "text", "text": "Setup: Arnoldi vs Diffrax\u2019s Runge-Kutta methods for a $\\mathbf{250k}$ parameter MLP We learn $\\omega$ with a multi-layer perceptron (MLP) with approximately 250,000 parameters. We had similar reconstructions with fewer parameters but use 250,000 to display how gradients of the Arnoldi iteration scale to many parameters. We compare an implementation of the solution operator $(\\theta,w_{0})\\mapsto$ $\\exp(A(\\theta))w_{0}$ with the Arnoldi iteration to Diffrax\u2019s [54] implementation of \u201cDopri5\u201d [84, 85] with a differentiate-then-discretise adjoint [86] as well as \u201cTsit5\u201d [87] with a discretise-then-differentiate adjoint (recommended by [53, 54]). All methods receive equal matrix-vector products per simulation. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Analysis: All methods train, but Arnoldi is more accurate for fixed matrix-vector-product budgets We evaluate the approximation errors in computing the values and gradients of a mean-squared error loss for all three solvers and then use the solvers to train the MLP. We are looking for low approximation errors for few matrix-vector products and for a good reconstruction of the truth. Figure 5 shows the results. The Arnoldi iteration has the lowest forward-pass and gradient error, but Table 4 demonstrates how all ap", "page_idx": 7}, {"type": "image", "img_path": "RL4FXrGcTw/tmp/d056e3a7fa66b5de603d831a570f5be4c85c0149e3f69cc8a04e1fcb46d70552.jpg", "img_caption": ["Figure 4: All methods find the truth. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 4: All three methods reconstruct the parameter well (std.-deviations exceed differences for testloss and RMSE), but Arnoldi and Dopri5 are faster than Tsit5. Dopri5 uses the BacksolveAdjoint, and Tsit5 the RecursiveCheckpointAdjoint in Diffrax [54]. We contribute Arnoldi\u2019s adjoints. ", "page_idx": 8}, {"type": "table", "img_path": "RL4FXrGcTw/tmp/15bc93e442535dd18a73c9057de989ed03656457ab9c6bd47c48ee6b582f30fd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "proaches lead to low errors on $\\omega$ as well as on a test set (a held-back percentage of the training data); see also Figure 4. The adjoints of the Arnoldi iteration match the efficiency of the differentiate-thendiscretise adjoint [86], and both outperform the discretise-then-differentiate adjoint by a margin. This shows how linear-algebra solutions to matrix exponentials can compete with highly optimised differential equation solvers. We anticipate ample opportunities of using the now-differentiable Arnoldi iteration for physics-based machine learning. ", "page_idx": 8}, {"type": "text", "text": "7 Case study: Calibrating Bayesian neural networks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Next, we differentiate a function of a matrix on a problem that is native to machine learning: marginal likelihood optimisation of a Bayesian neural network (a high-level introduction is in Appendix J). ", "page_idx": 8}, {"type": "text", "text": "Setup: Laplace-approximation of a VAN pre-trained on ImageNet We consider as $g_{\\theta}(x)$ a \u201cVisual Attention Network\u201d [88] with 4,105,800 parameters, pre-trained on ImageNet [89]. We assume $p(\\theta)=N(0,\\alpha^{-2}I)$ , and Laplace-approximate the log-marginal likelihood of the data as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\log p(y\\mid x)\\approx\\log p(y,\\theta\\mid x)-{\\frac{1}{2}}\\log\\operatorname*{det}(A(\\alpha))+\\operatorname{const}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $A(\\alpha)$ is the generalised Gauss\u2013Newton matrix (GGN) from Section 1 (recall Equation 2). ", "page_idx": 8}, {"type": "text", "text": "We optimise $\\alpha$ via Equation 21, implementing the log-determinant via stochastic trace estimation in combination with a Lanczos iteration (like in Section 5). Contemporary works [31, 37\u201343] rely on sparse approximations of the GGN (such as diagonal or KFAC approximations), so we compare our implementation to a diagonal approximation of the GGN matrix, which yields closed-form logdeterminants. The exact diagonal of the 4-million-column GGN matrix would require 4 million GGN-vector products with unit vectors, and like Deng et al. [90], we find this too expensive and resort to stochastic diagonal approximation (similar to trace estimation; all details are in Appendix J). We give both the stochastic diagonal approximation and our Lanczos-based estimator exactly 150 matrix-vector products to approximate Equation 21. We compare ", "page_idx": 8}, {"type": "image", "img_path": "RL4FXrGcTw/tmp/cbc94151312a8f44a4e315ab28f95d78fc3f41128b915519005ba4d32ca908e8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 6: Lanczos vs diagonal the evolution of the loss function over time and various uncertainty approx. for a Bayesian VAN. calibration metrics. Figure 6 demonstrates training and Table 5 shows results. ", "page_idx": 8}, {"type": "text", "text": "Analysis: Lanczos uses matrix-vector products better (by a margin) The results suggest how, for a fixed matrix-vector-product budget, Lanczos achieves a drastically better likelihood at a similar computational budget and already shows significant improvement with a much smaller budget. Lanczos outperforms the diagonal approximation on all metrics except ECE. The subpar performance of the diagonal approximation matches the observations of Ritter et al. [31]; see also [14]. The main takeaway from this study is that differentiable matrix-free linear algebra unlocks new techniques for Laplace approximations and allows further advances for Bayesian neural networks in general. ", "page_idx": 8}, {"type": "text", "text": "Table 5: Lanczos outperforms the diagonal approximation for calibrating a Bayesian version of an ImageNet pre-trained VAN. One training run; calibration estimated with 30 samples (sampling \u201cLanczos\u201d and \u201cdiagonal\u201d with another Lanczos/diagonal approximation; see Appendix J). places365 [91] is the out-of-distribution data; mean and std.-deviations of 3 runs. \u201cMVs\u201d: matrix-vector products. ", "page_idx": 9}, {"type": "table", "img_path": "RL4FXrGcTw/tmp/88a6c3fc0c8d18c9e06a857633112bef0bb23b035d55868c453c10e46959bfba.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by a research grant (42062) from VILLUM FONDEN. The work was partly funded by the Novo Nordisk Foundation through the Center for Basic Machine Learning Research in Life Science (NNF20OC0062606). This project received funding from the European Research Council (ERC) under the European Union\u2019s Horizon programme (grant agreement 101125993). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Nicholas J Higham. Functions of Matrices: Theory and Computation. SIAM, 2008. ", "page_idx": 9}, {"type": "text", "text": "[2] Efstratios Gallopoulos and Yousef Saad. Efficient solution of parabolic equations by Krylov approximation methods. SIAM Journal on Scientific and Statistical Computing, 13(5):1236\u2013 1264, 1992. [3] Marlis Hochbruck and Alexander Ostermann. Exponential integrators. Acta Numerica, 19: 209\u2013286, 2010.   \n[4] Changyi Xiao and Ligang Liu. Generative flows with matrix exponential. In International Conference on Machine Learning, pages 10452\u201310461. PMLR, 2020.   \n[5] Patrik Axelsson and Fredrik Gustafsson. Discrete-time solutions to the continuous-time differential Lyapunov equation with applications to Kalman filtering. IEEE Transactions on Automatic Control, 60(3):632\u2013643, 2014. [6] Jacob Gardner, Geoff Pleiss, Kilian Q Weinberger, David Bindel, and Andrew G Wilson. GPyTorch: Blackbox matrix-matrix Gaussian process inference with GPU acceleration. In Advances in Neural Information Processing Systems, 2018. [7] Jonathan Wenger, Geoff Pleiss, Philipp Hennig, John Cunningham, and Jacob Gardner. Preconditioning for scalable Gaussian process hyperparameter optimization. In International Conference on Machine Learning. PMLR, 2022. [8] Alexander Immer, Tycho FA Van Der Ouderaa, Mark Van Der Wilk, Gunnar Ratsch, and Bernhard Scho\u00a8lkopf. Stochastic marginal likelihood gradients using neural tangent kernels. In International Conference on Machine Learning, pages 14333\u201314352. PMLR, 2023. [9] Jens Behrmann, Will Grathwohl, Ricky TQ Chen, David Duvenaud, and Jo\u00a8rn-Henrik Jacobsen. Invertible residual networks. In International Conference on Machine Learning, pages 573\u2013 582. PMLR, 2019.   \n[10] Ricky TQ Chen, Jens Behrmann, David K Duvenaud, and J\u00a8orn-Henrik Jacobsen. Residual flows for invertible generative modeling. In Advances in Neural Information Processing Systems, volume 32, 2019.   \n[11] Geoff Pleiss, Jacob Gardner, Kilian Weinberger, and Andrew Gordon Wilson. Constant-time predictive distributions for Gaussian processes. In International Conference on Machine Learning, pages 4114\u20134123. PMLR, 2018.   \n[12] Chris Finlay, Jo\u00a8rn-Henrik Jacobsen, Levon Nurbekyan, and Adam Oberman. How to train your neural ODE: the world of Jacobian and kinetic regularization. In International Conference on Machine Learning, pages 3154\u20133164. PMLR, 2020.   \n[13] Nicol N Schraudolph. Fast curvature matrix-vector products for second-order gradient descent. Neural Computation, 14(7):1723\u20131738, 2002.   \n[14] Erik Daxberger, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen, Matthias Bauer, and Philipp Hennig. Laplace redux \u2013 effortless Bayesian deep learning. Advances in Neural Information Processing Systems, 34:20089\u201320103, 2021.   \n[15] Michael F Hutchinson. A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines. Communications in Statistics-Simulation and Computation, 18(3):1059\u2013 1076, 1989.   \n[16] Cornelius Lanczos. An iteration method for the solution of the eigenvalue problem of linear differential and integral operators. 1950.   \n[17] Shashanka Ubaru, Jie Chen, and Yousef Saad. Fast estimation of $\\operatorname{tr}(f(A))$ via stochastic Lanczos quadrature. SIAM Journal on Matrix Analysis and Applications, 38(4):1075\u20131099, 2017.   \n[18] Tyler Chen and Eric Hallman. Krylov-aware stochastic trace estimation. SIAM Journal on Matrix Analysis and Applications, 44(3):1218\u20131244, 2023.   \n[19] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kilian Q Weinberger, and Andrew Gordon Wilson. Exact Gaussian processes on a million data points. Advances in Neural Information Processing Systems, 32, 2019.   \n[20] Kaiwen Wu, Jonathan Wenger, Haydn Jones, Geoff Pleiss, and Jacob R Gardner. Largescale Gaussian processes via alternating projection. International Conference on Artificial Intelligence and Statistics, 2024.   \n[21] Josie K\u00a8onig, Max Pfeffer, and Martin Stoll. Efficient training of Gaussian processes with tensor product structure. Preprint on ArXiv:2312.15305, 2023.   \n[22] Alexander James Davies. Effective implementation of Gaussian process regression for machine learning. PhD thesis, University of Cambridge, 2015.   \n[23] Kun Dong, David Eriksson, Hannes Nickisch, David Bindel, and Andrew G Wilson. Scalable log determinants for Gaussian process kernel learning. In Advances in Neural Information Processing Systems, 2017.   \n[24] Geoff Pleiss, Martin Jankowiak, David Eriksson, Anil Damle, and Jacob Gardner. Fast matrix square roots with applications to Gaussian processes and Bayesian optimization. Advances in Neural Information Processing Systems, 33:22268\u201322281, 2020.   \n[25] Hao Wang. The Krylov subspace methods for the computation of matrix exponentials. 2015.   \n[26] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. International Conference on Learning Representations, 2023.   \n[27] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International Conference on Machine Learning, pages 1530\u20131538. PMLR, 2015.   \n[28] Emiel Hoogeboom, Victor Garcia Satorras, Jakub Tomczak, and Max Welling. The convolution exponential and generalized Sylvester flows. In Advances in Neural Information Processing Systems, volume 33, pages 18249\u201318260, 2020.   \n[29] Marlis Hochbruck, Christian Lubich, and Hubert Selhofer. Exponential integrators for large systems of differential equations. SIAM Journal on Scientific Computing, 19(5):1552\u20131574, 1998.   \n[30] Alexander Immer, Matthias Bauer, Vincent Fortuin, Gunnar R\u00a8atsch, and Khan Mohammad Emtiyaz. Scalable marginal likelihood estimation for model selection in deep learning. In International Conference on Machine Learning, 2021.   \n[31] Hippolyt Ritter, Aleksandar Botev, and David Barber. A scalable Laplace approximation for neural networks. In International Conference on Learning Representations, volume 6, 2018.   \n[32] Gene H Golub and G\u00b4erard Meurant. Matrices, Moments and Quadrature with Applications, volume 30. Princeton University Press, 2009.   \n[33] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In International Conference on Learning Representations, 2023.   \n[34] Aditya Ramesh and Yann LeCun. Backpropagation for implicit spectral densities. In Advances in Neural Information Processing Systems, 2018.   \n[35] Christopher KI Williams and Carl Edward Rasmussen. Gaussian Processes for Machine Learning, volume 2. MIT Press Cambridge, MA, 2006.   \n[36] Kurt Cutajar, Michael Osborne, John Cunningham, and Maurizio Filippone. Preconditioning kernel matrices. In International Conference on Machine Learning, pages 2529\u20132538. PMLR, 2016.   \n[37] Erik Daxberger, Eric Nalisnick, James U Allingham, Javier Antor\u00b4an, and Jos\u00b4e Miguel Hern\u00b4andez-Lobato. Bayesian deep learning via subnetwork inference. In International Conference on Machine Learning, pages 2510\u20132521. PMLR, 2021.   \n[38] Agustinus Kristiadi, Matthias Hein, and Philipp Hennig. Being Bayesian, even just a bit, fixes overconfidence in ReLu networks. In International Conference on Machine Learning, pages 5436\u20135446. PMLR, 2020.   \n[39] Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Mostofa Patwary, Mr Prabhat, and Ryan Adams. Scalable Bayesian optimization using deep neural networks. In International Conference on Machine Learning, pages 2171\u20132180. PMLR, 2015.   \n[40] John Denker and Yann LeCun. Transforming neural-net output levels to probability distributions. Advances in Neural Information Processing Systems, 3, 1990.   \n[41] Hippolyt Ritter, Aleksandar Botev, and David Barber. Online structured Laplace approximations for overcoming catastrophic forgetting. Advances in Neural Information Processing Systems, 31, 2018.   \n[42] Wesley J Maddox, Gregory Benton, and Andrew Gordon Wilson. Rethinking parameter counting in deep models: Effective dimensionality revisited. arXiv preprint arXiv:2003.02139, 2020.   \n[43] Apoorva Sharma, Navid Azizan, and Marco Pavone. Sketching curvature for efficient out-ofdistribution detection for deep neural networks. In Uncertainty in Artificial Intelligence, pages 1958\u20131967. PMLR, 2021.   \n[44] Marco Miani, Frederik Warburg, Pablo Moreno-Mu\u02dcnoz, Nicki Skafte Detlefsen, and S\u00f8ren Hauberg. Laplacian autoencoders for learning stochastic representations. In Advances in Neural Information Processing Systems, 2022.   \n[45] Marlis Hochbruck and Christian Lubich. On Krylov subspace approximations to the matrix exponential operator. SIAM Journal on Numerical Analysis, 34(5):1911\u20131925, 1997.   \n[46] Simo Sa\u00a8rkka\u00a8 and Arno Solin. Applied Stochastic Differential Equations, volume 10. Cambridge University Press, 2019.   \n[47] Severi Rissanen, Markus Heinonen, and Arno Solin. Generative modelling with inverse heat dissipation. In International Conference on Learning Representations, 2023.   \n[48] Cleve Moler and Charles Van Loan. Nineteen dubious ways to compute the exponential of a matrix. SIAM Review, 20(4):801\u2013836, 1978.   \n[49] Cleve Moler and Charles Van Loan. Nineteen dubious ways to compute the exponential of a matrix, twenty-five years later. SIAM Review, 45(1):3\u201349, 2003.   \n[50] Awad H Al-Mohy and Nicholas J Higham. Computing the Fr\u00b4echet derivative of the matrix exponential, with an application to condition number estimation. SIAM Journal on Matrix Analysis and Applications, 30(4):1639\u20131657, 2009.   \n[51] Yang Cao, Shengtai Li, Linda Petzold, and Radu Serban. Adjoint sensitivity analysis for differential-algebraic equations: The adjoint DAE system and its numerical solution. SIAM Journal on Scientific Computing, 24(3):1076\u20131089, 2003.   \n[52] Moritz Diehl and S\u00b4ebastien Gros. Numerical optimal control. Optimization in Engineering Center (OPTEC), 2011.   \n[53] Christopher Rackauckas and Qing Nie. DifferentialEquations.jl\u2013a performant and feature-rich ecosystem for solving differential equations in Julia. Journal of Open Research Software, 5(1): 15\u201315, 2017.   \n[54] Patrick Kidger. On Neural Differential Equations. PhD thesis, University of Oxford, 2021.   \n[55] Chris Rackauckas, Mike Innes, Yingbo Ma, Jesse Bettencourt, Lyndon White, and Vaibhav Dixit. DiffEqFlux.jl\u2013a julia library for neural differential equations. arXiv preprint arXiv:1902.02376, 2019.   \n[56] Yingbo Ma, Vaibhav Dixit, Michael J Innes, Xingjian Guo, and Chris Rackauckas. A comparison of automatic differentiation and continuous sensitivity analysis for derivatives of differential equation solutions. In 2021 IEEE High Performance Extreme Computing Conference (HPEC), pages 1\u20139. IEEE, 2021.   \n[57] Gene H Golub and Charles F Van Loan. Matrix Computations. JHU press, 2013.   \n[58] Magnus Rudolph Hestenes, Eduard Stiefel, et al. Methods of conjugate gradients for solving linear systems, volume 49. NBS Washington, DC, 1952.   \n[59] Walter Edwin Arnoldi. The principle of minimized iterations in the solution of the matrix eigenvalue problem. Quarterly of Applied Mathematics, 9(1):17\u201329, 1951.   \n[60] Matthias Seeger, Asmus Hetzel, Zhenwen Dai, Eric Meissner, and Neil D Lawrence. Autodifferentiating linear algebra. Preprint on ArXiv:1710.08717, 2017.   \n[61] Sebastian F Walter and Lutz Lehmann. Algorithmic differentiation of linear algebra functions with application in optimum experimental design (extended version). Preprint on ArXiv:1001.1654, 2010.   \n[62] Andreas Griewank and Andrea Walther. Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation. SIAM, 2008.   \n[63] Scott P Kolodziej, Mohsen Aznaveh, Matthew Bullock, Jarrett David, Timothy A Davis, Matthew Henderson, Yifan Hu, and Read Sandstrom. The SuiteSparse matrix collection website interface. Journal of Open Source Software, 4(35):1244, 2019.   \n[64] Timothy A Davis and Yifan Hu. The University of Florida sparse matrix collection. ACM Transactions on Mathematical Software (TOMS), 38(1):1\u201325, 2011.   \n[65] Iain S Duff, Roger G Grimes, and John G Lewis. Sparse matrix test problems. ACM Transactions on Mathematical Software (TOMS), 15(1):1\u201314, 1989.   \n[66] Mathieu Blondel and Vincent Roulet. The elements of differentiable programming. arXiv preprint arXiv:2403.14606, 2024.   \n[67] Gerrit C Groenenboom and Henk M Buck. Solving the discretized time-independent Schr\u00a8odinger equation with the Lanczos procedure. The Journal of Chemical Physics, 92 (7):4374\u20134379, 1990.   \n[68] Christopher Conway Paige. The computation of eigenvalues and eigenvectors of very large sparse matrices. PhD thesis, University of London, 1971.   \n[69] Christopher C Paige. Computational variants of the Lanczos method for the eigenproblem. IMA Journal of Applied Mathematics, 10(3):373\u2013381, 1972.   \n[70] Christopher C Paige. Error analysis of the Lanczos algorithm for tridiagonalizing a symmetric matrix. IMA Journal of Applied Mathematics, 18(3):341\u2013349, 1976.   \n[71] Steffen B\u00a8orm and Christian Mehl. Numerical methods for Eigenvalue Problems. Walter de Gruyter, 2012.   \n[72] Mathieu Blondel, Quentin Berthet, Marco Cuturi, Roy Frostig, Stephan Hoyer, Felipe LlinaresLo\u00b4pez, Fabian Pedregosa, and Jean-Philippe Vert. Efficient and modular implicit differentiation. Advances in Neural Information Processing Systems, 35:5230\u20135242, 2022.   \n[73] Michael Betancourt, Charles C Margossian, and Vianey Leos-Barajas. The discrete adjoint method: Efficient derivatives for functions of discrete sequences. Preprint on ArXiv:2002.00326, 2020.   \n[74] Brian Kha Tran and Melvin Leok. Geometric methods for adjoint systems. Journal of Nonlinear Science, 34(1):25, 2024.   \n[75] Denisa AO Roberts and Lucas R Roberts. QR and LQ decomposition matrix backpropagation algorithms for square, wide, and deep\u2013real or complex\u2013matrices and their software implementation. Preprint on ArXiv:2009.10071, 2020.   \n[76] Thomas P Minka. Old and new matrix algebra useful for statistics. See www.stat.cmu.edu/minka/papers/matrix.html, 4, 2000.   \n[77] Mike Giles. An extended collection of matrix derivative results for forward and reverse mode automatic differentiation. 2008.   \n[78] Jean C\u00b4ea. Conception optimale ou identification de formes, calcul rapide de la d\u00b4eriv\u00b4ee directionnelle de la fonction cou\u02c6t. ESAIM: Mode\u00b4lisation mathe\u00b4matique et analyse nume\u00b4rique, 20(3):371\u2013402, 1986.   \n[79] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.   \n[80] Andrew Wilson and Hannes Nickisch. Kernel interpolation for scalable structured Gaussian processes (KISS-GP). In International Conference on Machine Learning, pages 1775\u20131784. PMLR, 2015.   \n[81] Helmut Harbrecht, Michael Peters, and Reinhold Schneider. On the low-rank approximation by the pivoted Cholesky decomposition. Applied Numerical Mathematics, 62(4):428\u2013440, 2012.   \n[82] Benjamin Charlier, Jean Feydy, Joan Alexis Glaunes, Fran\u00b8cois-David Collin, and Ghislain Durif. Kernel operations on the GPU, with autodiff, without memory overflows. Journal of Machine Learning Research, 22(74):1\u20136, 2021.   \n[83] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. San Diega, CA, USA, 2015.   \n[84] J. R. Dormand and P. J. Prince. A family of embedded Runge\u2013Kutta formulae. J. Comp. Appl. Math, 6:19\u201326, 1980. [85] Lawrence F. Shampine. Some practical Runge-Kutta formulas. Mathematics of Computation, 46(173):135\u2013150, 1986. doi: https://doi.org/10.2307/2008219.   \n[86] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. Advances in Neural Information Processing Systems, 31, 2018. [87] Charalampos Tsitouras. Runge\u2013Kutta pairs of order 5 (4) satisfying only the first column simplifying assumption. Computers & Mathematics with Applications, 62(2):770\u2013775, 2011.   \n[88] Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, and Shi-Min Hu. Visual attention network. Computational Visual Media, 9(4):733\u2013752, 2023. [89] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255. IEEE, 2009. [90] Zhijie Deng, Feng Zhou, and Jun Zhu. Accelerated linearized Laplace approximation for Bayesian deep learning. Advances in Neural Information Processing Systems, 35:2695\u20132708, 2022. [91] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep features for scene recognition using places database. Advances in Neural Information Processing Systems, 27, 2014. [92] Tianlin Liu, Jose Antonio Lara Benitez, Florian Faucher, AmirEhsan Khorashadizadeh, Maarten V de Hoop, and Ivan Dokmani\u00b4c. WaveBench: Benchmarks datasets for modeling linear wave propagation PDEs. Transactions on Machine Learning Research, 2023. [93] Peter Deuflhard, Andreas Hohmann, Peter Deuflhard, and Andreas Hohmann. Three-term recurrence relations. Numerical Analysis in Modern Scientific Computing: An Introduction, pages 151\u2013178, 2003.   \n[94] Peter Deuflhard. On algorithms for the summation of certain special functions. Computing, 17:37\u201348, 1976. [95] Simon Bartels, Kristoffer Stensbo-Smidt, Pablo Moreno-Mu\u02dcnoz, Wouter Boomsma, Jes Frellsen, and Soren Hauberg. Adaptive Cholesky Gaussian processes. In International Conference on Artificial Intelligence and Statistics, pages 408\u2013452. PMLR, 2023.   \n[96] Rui Camachol. Inducing models of human control skills. In Machine Learning: ECML-98: 10th European Conference on Machine Learning Chemnitz, Germany, April 21\u201323, 1998 Proceedings 10, pages 107\u2013118. Springer, 1998. [97] Anton Schwaighofer and Volker Tresp. Transductive and inductive methods for approximate Gaussian process regression. Advances in Neural Information Processing Systems, 15, 2002.   \n[98] Paul Shannon, Andrew Markiel, Owen Ozier, Nitin S Baliga, Jonathan T Wang, Daniel Ramage, Nada Amin, Benno Schwikowski, and Trey Ideker. Cytoscape: a software environment for integrated models of biomolecular interaction networks. Genome research, 13(11):2498\u20132504, 2003. [99] P. J Prince and J. R. Dormand. High order embedded Runge\u2013Kutta formulae. Journal of Computational and Applied Mathematics, 7(1):67\u201375, 1981.   \n[100] Antonio Stanziola, Simon R Arridge, Ben T Cox, and Bradley E Treeby. j-Wave: An opensource differentiable wave simulator. SoftwareX, 22:101338, 2023.   \n[101] David JC MacKay. Bayesian interpolation. Neural computation, 4(3):415\u2013447, 1992.   \n[102] Alexander Immer, Maciej Korzepa, and Matthias Bauer. Improving predictions of Bayesian neural nets via local linearization. In International Conference on Artificial Intelligence and Statistics, pages 703\u2013711. PMLR, 2021.   \n[103] Vardan Papyan. Traces of class/cross-class structure pervade deep learning spectra. Journal of Machine Learning Research, 21(252):1\u201364, 2020.   \n[104] Costas Bekas, Effrosyni Kokiopoulou, and Yousef Saad. An estimator for the diagonal of a matrix. Applied Numerical Mathematics, 57(11-12):1214\u20131229, 2007.   \n[105] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated probabilities using Bayesian binning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 29, 2015. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Appendix: Overview ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Some of the results in the main paper promised detailed information about setups, data, compute, or additional proofs. For example, the case study about partial differential equations involves a data generation process which will receive further explanation in this supplement. ", "page_idx": 16}, {"type": "text", "text": "The appendix provides the following details: Appendices A and F elaborate on Figure 3 and Table 2 respectively; Appendices B to E contain proofs for the main results; and Appendices G to J describe the setup used for the case studies. Notably, Appendix G describes how we implement low-memory matrix-vector products in JAX (to replicate what makes libraries like KeOps [82] so efficient), and Appendix I outlines a PDE data set similar to that by Liu et al. [92]. ", "page_idx": 16}, {"type": "text", "text": "Code Most of the contributions of this paper pertain to differentiable implementations of numerical algorithms \u2013 at the heart of it are our reverse-mode differentiable Lanczos and Arnoldi iterations. We provide JAX code to reproduce all experiments at the URL ", "page_idx": 16}, {"type": "text", "text": "https://github.com/pnkraemer/experiments-lanczos-adjoints and have packaged all numerical methods in a JAX library that can be installed via ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "pip install matfree ", "page_idx": 16}, {"type": "text", "text": "Next to Lanczos and Arnoldi, this includes variants of conjugate gradient methods [58] and pivoted Cholesky preconditioners [81] (which we used for Gaussian processes), low-memory kernel-matrixvector products (also Gaussian processes), efficient GGN-vector products (used for Bayesian neural networks), and matrix-free sampling algorithms to sample from Gaussian processes and Laplaceapproximated Bayesian neural networks (used for Gaussian processes and Bayesian neural networks). These methods are known in some form or another, but until now, they have all lacked a software implementation in the current JAX ecosystem (with some exceptions relating to conjugate gradients). ", "page_idx": 16}, {"type": "text", "text": "Compute All experiments before the case studies were run on CPU. The Gaussian process and differential equation case studies run on a V100 GPU, the Bayesian neural network one on a P100 GPU. The Gaussian process and Bayesian neural network studies run in a few hours, all other code finishes in a few minutes. ", "page_idx": 16}, {"type": "text", "text": "A Additional context for Figure 3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To create Figure 3, we load the \u201cbcsstk18\u201d matrix from the SuiteSparse matrix collection [63\u2013 65]. This matrix is symmetric and has 11,948 rows/columns and 149,090 nonzero entries. We implement matrix-vector products with jax.experimental.sparse, and use a Lanczos iteration without reorthogonalisation. We time the execution of the forward pass, as well as the backward pass with and without implementing a custom vector-Jacobian product (the custom vector-Jacobian product involves the adjoint). The results were shown in Figure 3, and displayed how rapidly the computational complexity of automatic differentiation increases, whereas the computational complexity of our custom gradient mirrors that of the forward pass. This figure showed how without our proposed gradients, differentiating through the Lanczos iteration is unfeasible. ", "page_idx": 16}, {"type": "text", "text": "To add to this benchmark, we repeat the same for the Arnoldi iteration and show both compilation and runtime for Lanczos and Arnoldi in Figure 7 (this includes the curves from Figure 3 again). We see ", "page_idx": 16}, {"type": "text", "text": "Figure 7: Run and compilation times for Lanczos and Arnoldi. The \u201cbackprop\u201d curves were stopped at 100, because for higher values we encountered memory issues. All experiments run on CPU. ", "page_idx": 16}, {"type": "image", "img_path": "RL4FXrGcTw/tmp/808be5288b4577ade1ea94ba196e9569fa1647bdc42949dbf2aef712fac0966d.jpg", "img_caption": ["different behaviour for Lanczos and Arnoldi. Whereas for back-propagation through Lanczos without "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "the custom gradient, the compilation times remain constant for increasing Krylov-space depth $K$ and runtimes increase rapidly, the reverse is true for the Arnoldi iteration. The adjoint method mirrors that of the forward pass in both benchmarks. In either case, the increasing memory requirements for backpropagation through Lanczos and Arnoldi without our proposed adjoints becomes apparent. ", "page_idx": 17}, {"type": "text", "text": "B Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Here is how we derive the adjoints of the Arnoldi system. The structure is the usual: take the forward constraint, differentiate, add Lagrange multipliers (\u201ctranspose\u201d), and identify the adjoint system. ", "page_idx": 17}, {"type": "text", "text": "B.1 Linearisation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The linearisation of Equation 12 is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(\\mathrm{d}A)Q+A\\mathrm{d}Q-(\\mathrm{d}Q)H-Q\\mathrm{d}H-(\\mathrm{d}r)(e_{K})^{\\top}=0}&{{}\\in\\mathbb{R}^{N\\times K},}\\\\ {(\\mathrm{d}Q)e_{1}-(\\mathrm{d}v)c-v\\mathrm{d}c=0}&{{}\\in\\mathbb{R}^{N\\times1},}\\\\ {I_{\\leq}\\circ[\\langle Q e_{i},(\\mathrm{d}Q)e_{j}\\rangle+\\langle Q e_{j},(\\mathrm{d}Q)e_{i}\\rangle]_{i,j=1}^{K}=0}&{{}\\in\\mathbb{R}^{K\\times K},}\\\\ {I_{\\ll}\\circ\\mathrm{d}H=0}&{{}\\in\\mathbb{R}^{K\\times K},}\\\\ {[\\langle r,(\\mathrm{d}Q)e_{j}\\rangle+\\langle Q e_{j},\\mathrm{d}r\\rangle]_{j=1}^{K}=0}&{{}\\in\\mathbb{R}^{K\\times1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To see this, apply the chain- and product rules to the original constraint in Equation 12. ", "page_idx": 17}, {"type": "text", "text": "B.2 Transposition ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Let $\\rho=\\rho(Q,r,H,c)\\in\\mathbb{R}$ be a scalar function of the outputs. In the following, interpret vectors as $N\\times1$ matrices and scalars as $1\\times1$ matrices. The values of inner products and the realisations of $\\rho$ are the only scalars. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\iota=\\langle\\nabla_{\\nabla}\\rho,\\mathrm{d}Q\\rangle+\\langle\\nabla_{\\nabla}\\rho,\\mathrm{d}H\\rangle+\\langle\\nabla_{\\nabla}\\rho,\\mathrm{d}H\\rangle+\\langle\\nabla_{\\nabla}\\rho,\\mathrm{d}\\rangle}\\\\ &{=\\langle\\nabla_{\\nabla}Q,\\mathrm{d}Q\\rangle+\\langle\\nabla\\mu,\\mathrm{d}H\\rangle+\\langle\\nabla_{\\nabla}\\rho,\\mathrm{d}H\\rangle+\\langle\\nabla_{\\nabla}\\rho,\\mathrm{d}H\\rangle}\\\\ &{\\quad+\\langle\\Lambda,(d)\\Delta\\rho+d Q-(d Q)H-Q(H-\\langle\\Phi\\rangle)^{\\prime}}\\\\ &{\\quad+\\langle\\Delta,(d)\\rho\\rangle+\\langle\\nabla_{\\nabla}\\rho,\\mathrm{d}H\\rangle}\\\\ &{\\quad+\\langle\\nabla_{\\mathrm{r}}\\rho,\\mathrm{d}H\\rangle-\\langle\\nabla_{\\mathrm{c}}(\\rho)\\phi\\rangle_{\\mathrm{c}}+\\langle\\nabla\\rho,(d Q)\\kappa_{\\mathrm{d}}\\rangle\\vert_{\\mathrm{d},\\mathrm{j=1}}^{\\mathcal{N}}}\\\\ &{\\quad+\\langle\\nabla_{\\mathrm{r}},L_{\\mathrm{c}}\\circ d L\\rangle}\\\\ &{\\quad+\\langle\\nabla_{\\mathrm{r}}\\rho,\\mathrm{d}H\\rangle}\\\\ &{=\\langle\\nabla_{\\phi}Q\\rangle+\\langle\\nabla\\mu,\\mathrm{d}H\\rangle+\\langle\\nabla_{\\phi}\\rho,\\mathrm{d}H\\rangle+\\langle\\nabla_{\\phi}\\rho,\\mathrm{d}\\rangle}\\\\ &{\\quad+\\langle\\partial Q^{\\prime}\\rangle,\\mathrm{d}A\\rangle+\\langle\\lambda^{\\prime}\\Lambda,\\mathrm{d}Q\\rangle-\\langle\\Lambda H^{\\prime},\\mathrm{d}Q\\rangle-\\langle\\nabla^{\\prime}\\Lambda,\\mathrm{d}H\\rangle-\\langle\\Lambda\\kappa,\\mathrm{d}\\rangle}\\\\ &{\\quad+\\langle\\lambda(c)^{\\prime},\\mathrm{d}Q\\rangle-\\langle\\lambda^{\\prime},\\mathrm{d}\\phi\\rangle-\\langle\\nabla^{\\prime}\\lambda,\\mathrm{d}\\phi\\rangle}\\\\ &{\\quad+\\langle(J_{c}\\leq\\Gamma_{\\mathrm{r}})\\phi\\rangle+Q\\langle(\\mathcal{L}_{\\mathrm{c}}\\leq\\mathrm{p})^{\\top},\\mathrm{d}Q\\rangle}\\\\ &{\\quad+\\langle(J_{c}\\leq\\mathrm{d})\\phi\\rangle+\\langle(\\mathcal{L}_{\\mathrm{c}}\\circ\\mathrm{d})\\rangle+\\langle(\\mathcal{L}_{\\mathrm{c}}\\circ\\mathrm{f})^{\\top},\\mathrm{d}Q\\rangle}\\\\ &{\\quad+\\langle(J_{c}\\leq\\mathrm{d})\\phi\\rangle+\\langle(\\mathcal{L}_{\\mathrm{f}}\\circ\\mathrm{d})\\rangle}\\\\ &{=\\langle2\\phi,\\mathrm{d}Q\\rangle+\\langle(\\mathcal{L \n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with the constraints ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{Z_{Q}:=\\nabla_{Q}\\rho+A^{\\top}\\Lambda-\\Lambda H^{\\top}+\\lambda(e_{1})^{\\top}+Q(I_{\\leq}\\circ\\Gamma)+Q(I_{\\leq}\\circ\\Gamma)^{\\top}+r\\gamma^{\\top}}&{\\in\\mathbb{R}^{N\\times K}}\\\\ &{Z_{H}:=\\nabla_{H}\\rho-Q^{\\top}\\Lambda+I_{\\ll}\\circ\\Sigma}&{\\in\\mathbb{R}^{K\\times K}}\\\\ &{Z_{r}:=\\nabla_{r}\\rho-\\Lambda e_{K}+Q\\gamma}&{\\in\\mathbb{R}^{N\\times1}}\\\\ &{Z_{c}:=\\nabla_{c}\\rho-v^{\\top}\\lambda}&{\\in\\mathbb{R}^{1\\times1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Solving the adjoint system, $Z_{Q}=0,Z_{H}=0,Z_{r}=0,$ and $Z_{c}=0$ as a function of $\\Lambda,\\lambda,\\Gamma,\\Sigma$ , and $\\gamma$ , yields the desired $\\nabla_{A}\\rho=\\Lambda Q^{\\top}$ and $\\nabla_{v}f=\\lambda c^{\\top}$ . Theorem 4.1 is complete. ", "page_idx": 17}, {"type": "text", "text": "C Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Recall that $\\rho\\,=\\,\\rho(x_{1},...,x_{K+1};a_{1},...,a_{K};b_{1},...,b_{K})$ shall be a scalar/loss that depends on the output of the algorithm. Denote by $\\nabla_{x_{k}}\\rho$ the gradient of $\\rho$ with respect to each Lanczos vector $x_{k}$ , and by $\\nabla_{a_{k}}\\rho$ and $\\nabla_{b_{k}}\\rho$ the gradients with respect to $a_{k}$ and $b_{k}$ respectively. ", "page_idx": 18}, {"type": "text", "text": "The differential of normalisation, i.e., the operation $s\\mapsto h=s/(s^{\\top}s)$ is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{d}h=\\frac{1}{s^{\\top}s}\\left(I-h h^{\\top}\\right)\\mathrm{d}s.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The next steps are the usual ones: we start with the forward constraint, linearise, add Lagrange multipliers, and identify the adjoint system. The order of the middle two steps (linearise, multipliers) is interchangeable; while for Arnoldi, we linearise first and then add Lagrange multipliers, for Lanczos, we go the other way. ", "page_idx": 18}, {"type": "text", "text": "Define Lagrange multipliers $\\{\\lambda_{k}\\}_{k=0}^{K}\\subseteq\\mathbb{R}^{n}$ and $\\{\\mu_{k}/2,\\nu_{k}\\}_{j,k=1}^{K}\\subseteq{\\mathbb{R}}_{:}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\rho=\\rho+\\sum_{k=1}^{K}\\lambda_{k}^{\\top}\\left(-b_{k-1}x_{k-1}+(A-a_{k}I)x_{k}-b_{k}x_{k+1}\\right)\\qquad\\qquad\\qquad(b_{0}=1,x_{0}=0)}\\\\ {\\displaystyle-\\ \\lambda_{0}^{\\top}\\left(x_{1}-\\frac{v}{\\sqrt{v^{\\top}v}}\\right)+\\frac{1}{2}\\sum_{k=1}^{K}\\mu_{k}(x_{k+1}^{\\top}x_{k+1}-1)+\\sum_{k=1}^{K}\\nu_{k}\\,x_{k}^{\\top}x_{k+1}.\\qquad\\qquad(26)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Differentiate and use that the forward constraint must be satisfied, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\mathrm{d}\\rho=\\displaystyle\\sum_{k=1}^{K+1}(\\nabla_{x_{k},\\rho})^{\\top}\\mathrm{d}x_{k}+\\displaystyle\\sum_{k=1}^{K}(\\nabla_{x_{k},\\rho})^{\\top}\\mathrm{d}a_{k}+\\displaystyle\\sum_{k=1}^{K}(\\nabla_{x_{k},\\rho})^{\\top}\\mathrm{d}h_{k}}\\\\ &{\\quad+\\displaystyle\\sum_{k=1}^{K}\\lambda_{k}^{\\top}\\left[(\\mathrm{d}A)x_{k}-(\\mathrm{d}a_{k})x_{k}-(\\mathrm{d}b_{k-1})x_{k-1}-(\\mathrm{d}b_{k})x_{k+1}\\right]}&&{\\quad{\\mathrm{(d}b_{0}=1,\\mathrm{d}x_{0}=0)}}\\\\ &{\\quad+\\displaystyle\\sum_{k=1}^{K}\\lambda_{k}^{\\top}\\left[(\\mathrm{d}A)x_{k}-a_{k}\\mathrm{d}x_{k}-b_{k-1}\\mathrm{d}x_{k-1}-b_{k}\\mathrm{d}x_{k+1}\\right]}&&{\\quad{\\mathrm{(d}b_{0}=1,x_{0}=0)}}\\\\ &{\\quad+\\displaystyle\\sum_{k=1}^{K}\\lambda_{k}^{\\top}\\left[4\\mathrm{d}x_{k}-a_{k}\\mathrm{d}x_{k}-b_{k-1}\\mathrm{d}x_{k-1}-b_{k}\\mathrm{d}x_{k+1}\\right]}&&{\\quad{\\mathrm{(b}_{0}=1,x_{0}=0)}}\\\\ &{\\quad-\\lambda_{0}^{\\top}(\\mathrm{d}x_{1}-(I-x_{1}x_{1}^{\\top})/(\\nabla^{\\top}v)\\mathrm{d}v)}\\\\ &{\\quad+\\displaystyle\\sum_{k=1}^{K}\\mu_{k}x_{k+1}^{\\top}\\mathrm{d}x_{k+1}}\\\\ &{\\quad+\\displaystyle\\sum_{k=1}^{K}\\nu_{k}(x_{k+1}^{\\top}\\mathrm{d}x_{k}+x_{k}^{\\top}\\mathrm{d}x_{k+1}).}&&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Sort all terms by differential, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{d}\\rho=\\sum_{k=1}^{K}Z_{a_{k}}\\mathrm{d}a_{k}+\\sum_{k=1}^{K}Z_{b_{k}}\\mathrm{d}b_{k}+\\sum_{k=1}^{K+1}Z_{x_{k}}\\mathrm{d}x_{k}+Z_{v}\\mathrm{d}v+\\,\\mathrm{trace}\\left(Z_{A}\\mathrm{d}A\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "so that enforcing that all $Z_{a_{k}},\\,Z_{b_{k}},\\,Z_{x_{k}}$ terms are zero yields constraints for the multipliers from which we can compute gradients with respect to $v$ and $A$ . ", "page_idx": 18}, {"type": "text", "text": "What are those terms? Let $\\lambda_{K+1}=0$ , $\\mu_{0}=0$ , and $\\nu_{0}=0$ (to simplify notation below); then, ", "page_idx": 18}, {"type": "equation", "text": "$$\nZ_{x_{K+1}}=-\\lambda_{K}b_{K}+(\\nabla_{x_{K+1}}\\rho+\\mu_{K}x_{K+1}+\\nu_{K}x_{K}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and for all $k=K,...1$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{Z_{x_{k}}=-b_{k}\\lambda_{k+1}+(A^{\\top}-a_{k}I)\\lambda_{k}-b_{k-1}\\lambda_{k-1}+(\\nabla_{x_{k}}\\rho+\\mu_{k-1}x_{k}+\\nu_{k}x_{k+1}+\\nu_{k-1}x_{k-1}),}&\\\\ &{\\qquad}&{\\qquad\\qquad(3\\Omega)}\\\\ &{Z_{a_{k}}=\\nabla_{a_{k}}\\rho-\\lambda_{k}^{\\top}x_{k},}&{\\qquad\\qquad}&{\\qquad\\qquad(30\\Omega)}\\\\ &{Z_{b_{k}}=\\nabla_{b_{k}}\\rho-\\lambda_{k+1}^{\\top}x_{k}-\\lambda_{k}^{\\top}x_{k+1}.}&{\\qquad\\qquad}&{\\qquad\\qquad(30\\Omega)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The expressions are like the forward-Lanczos constraints, and the main differences are (i) that the recursions are run backwards in \u201ctime\u201d and (ii) the existence of a nonzero bias term in the adjoints (marked by parentheses). Enforcing $Z_{a_{k}}$ , $Z_{b_{k}}$ , $Z_{x_{k}}$ to be zero identifies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla_{v}\\rho=Z_{v}:=\\left(\\frac{\\lambda_{0}^{\\top}x_{1}}{v^{\\top}v}x_{1}^{\\top}-\\lambda_{0}^{\\top}\\right),\\quad\\nabla_{A}\\rho=Z_{A}:=\\left[\\sum_{k=1}^{K}x_{k}\\lambda_{k}^{\\top}\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Theorem 4.2 is complete. ", "page_idx": 19}, {"type": "text", "text": "D Proof of Corollary 4.3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The following derivation covers only the case for Lanczos, i.e., we use the variables $x_{1},...,x_{K+1}$ instead of Arnoldi\u2019s $q_{1},...,q_{K}$ . But the derivation is the same for both methods. ", "page_idx": 19}, {"type": "text", "text": "The expression we manipulate is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{d}\\rho=\\mathrm{trace}\\left(\\left[\\sum_{k=1}^{K}{x_{k}\\lambda_{k}^{\\top}}\\right]\\mathrm{d}A\\right)+\\mathrm{const}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where all non- $\\mathrm{d}A$ -related quantities are treated as some unimportant constants. ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{\\operatorname{trace}\\left(\\left[{\\displaystyle\\sum_{k=1}^{K}}x_{k}\\lambda_{k}^{\\top}\\right]D_{j}{\\mathcal{A}}\\mathrm{d}\\theta_{j}\\right)=\\operatorname{trace}\\left(\\left[{\\displaystyle\\sum_{k=1}^{K}}x_{k}\\lambda_{k}^{\\top}\\right](\\nabla_{\\theta_{j}}A)^{\\top}\\right)\\mathrm{d}\\theta_{j}\\qquad\\qquad{\\mathrm{(chain~rule)}}}\\\\ {=\\operatorname{trace}\\left({\\displaystyle\\sum_{k=1}^{K}}\\lambda_{k}^{\\top}(\\nabla_{\\theta_{j}}A)^{\\top}x_{k}\\right)\\mathrm{d}\\theta_{j}\\qquad{\\mathrm{(cyclic~property~of~traces)}}}\\\\ {=\\displaystyle\\sum_{k=1}^{K}\\lambda_{k}^{\\top}(\\nabla_{\\theta_{j}}A)^{\\top}x_{k}\\mathrm{d}\\theta_{j}\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{(assdaris~is~own~trace)}}\\\\ {=\\nabla_{\\theta_{j}}\\left[{\\displaystyle\\sum_{k=1}^{K}}\\lambda_{k}^{\\top}A^{\\top}x_{k}\\right]\\mathrm{d}\\theta_{j}\\qquad}&{{\\mathrm{(Iinearity~of~diff.~\\&~summation)}}}\\\\ {=\\nabla_{\\theta_{j}}\\left[{\\displaystyle\\sum_{k=1}^{K}}\\lambda_{k}^{\\top}A x_{k}\\right]\\mathrm{d}\\theta_{j}.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In conclusion, the derivative of $\\rho$ wrt $\\theta_{j}$ is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla_{\\theta_{j}}\\rho=\\nabla_{j}\\left[\\sum_{k=1}^{K}{x_{k}^{\\top}A^{\\top}\\lambda_{k}}\\right]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and stacking all of those partial derivatives on top of each other, we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\rho=[\\nabla_{\\theta_{j}}\\rho]_{j}=\\nabla\\left[\\theta\\mapsto\\sum_{k=1}^{K}x_{k}^{\\top}A(\\theta)^{\\top}\\lambda_{k}\\right]=\\sum_{k=1}^{K}\\nabla\\left[\\theta\\mapsto A(\\theta)^{\\top}\\lambda_{k}\\right]x_{k}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We already compute $A(\\theta)^{\\top}\\lambda_{k}$ during the backward pass, so we are a single vector-Jacobian product with $x_{k}$ away from a matrix-parameter-gradient instead of a matrix-gradient. This requires $O(p+n)$ storage, and is computed online, which makes the memory-complexity independent of $K$ . ", "page_idx": 19}, {"type": "text", "text": "E Solving the adjoint system ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The upcoming section details how to solve the adjoint system for both, the Lanczos and the Arnoldi iterations. It reuses notation from Appendices B and C. ", "page_idx": 19}, {"type": "text", "text": "We begin with Lanczos, because the solution is less technical, and because starting with Lanczos can provide a template for solving Arnoldi\u2019s adjoint system. All results in the present section (except for those that explicitly point to Deuflhard et al. [93], Deuflhard [94], which are marked as such) are new and a contribution of this work. ", "page_idx": 19}, {"type": "text", "text": "E.1 Lanczos ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The inputs to the adjoint system are the Lanczos vectors $\\{x_{k}\\}_{k=1}^{K+1}$ and the coefficients $\\{a_{k}\\}_{k=1}^{K}$ as well as $\\{a_{k}\\}_{k=1}^{K}$ from the forward pass, the corresponding input derivatives $\\{\\nabla_{x_{k}}\\rho\\}_{k=1}^{K+1}$ , $\\{\\nabla_{a_{k}}\\rho\\}_{k=1}^{K}$ and $\\{\\nabla_{a_{k}}\\rho\\}_{k=1}^{K}$ , and matrix $A$ and initial vector $v$ . ", "page_idx": 20}, {"type": "text", "text": "The overall strategy for solving the adjoint system of the Lanczos iteration (Theorem 4.2) is the following: for every $k=K,...,1$ , alternate the two steps: ", "page_idx": 20}, {"type": "text", "text": "1. Combine orthogonality with the $Z_{a_{k}}$ constraints to get $\\nu_{k}$ , and combine it with the $Z_{b_{k}}$ constraints to get $\\mu_{k}$ . 2. Once each $\\mu_{k}$ and $\\nu_{k}$ are available, solve for $\\lambda_{k}$ and repeat with the next lower $k$ . ", "page_idx": 20}, {"type": "text", "text": "This results in the following procedure: To start, set $\\zeta_{K+1}=-(\\nabla_{x_{K+1}}\\rho)$ and $\\lambda_{K+1}=0$ . Then, for all $k=K,...,1$ , compute ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\xi_{k}=\\zeta_{k+1}/b_{k}}}\\\\ {{\\tilde{\\mu}_{k}=\\nabla_{b_{k}}\\rho-\\lambda_{k+1}^{\\top}x_{k}+x_{k+1}^{\\top}\\xi_{k}}}\\\\ {{\\tilde{\\nu}_{k}=\\nabla_{a_{k}}\\rho+x_{k}^{\\top}\\xi_{k}}}\\\\ {{\\lambda_{k}=-\\xi_{k}+\\tilde{\\mu}_{k}\\cdot x_{k+1}+\\tilde{\\nu}_{k}\\cdot x_{k}}}\\\\ {{\\zeta_{k}=-\\nabla_{x_{k}}\\rho-A^{\\top}\\lambda_{k}+a_{k}\\cdot\\lambda_{k}+b_{k}\\cdot\\lambda_{k+1}-b_{k}\\cdot\\tilde{\\nu}_{k}\\cdot x_{k+1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Finally, set $\\lambda_{0}=\\zeta_{1}$ . Only $\\lambda$ and $\\zeta$ affect subsequent steps; $\\mu,\\,\\nu$ , and $\\xi$ are only needed for computing $\\lambda$ and $\\zeta$ . The $k$ -th step depends on $a_{k},b_{k},x_{k},x_{k+1},\\nabla_{a_{k}}\\rho,\\nabla_{b_{k}}\\rho,\\nabla_{x_{k}}\\rho.$ . ", "page_idx": 20}, {"type": "text", "text": "The strategy above yields all $\\{\\lambda_{k}\\}_{k=0}^{K}$ . Finalise the gradients ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla_{v}\\rho=\\frac{\\lambda_{0}^{\\top}x_{1}}{v^{\\top}v}x_{1}-\\lambda_{0},\\quad\\nabla_{A}\\rho=\\sum_{k=1}^{K}\\lambda_{k}x_{k}^{\\top}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which can be embedded into any reverse-mode algorithmic-differentiation-engine. ", "page_idx": 20}, {"type": "text", "text": "E.2 Arnoldi ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The process for Arnoldi is similar to that for Lanczos, but the derivation is more technical. It shares many similarities with deriving the Arnoldi iteration (i.e., the forward pass), so we begin by providing a perspective on recurrence relations (which include the Arnoldi iteration) through the lens of linear system solvers [93, 94] before we use this perspective to solve the adjoint system. ", "page_idx": 20}, {"type": "text", "text": "E.2.1 Solving the original system ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "At its core, finding the Arnoldi vectors amounts to solving ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-A Q+Q H+r(e_{K})^{\\top}=0,}\\\\ {Q e_{1}-c v=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which is possible in closed form as follows: We rewrite the first two constraints as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c c}{(e_{1}\\otimes I)\\mathrm{vec}\\left(Q\\right)=\\mathrm{vec}\\left(c v\\right)}\\\\ {-(I\\otimes A)\\mathrm{vec}\\left(Q\\right)+(H^{\\top}\\otimes I)\\mathrm{vec}\\left(Q\\right)+(e_{K}\\otimes I)\\mathrm{vec}\\left(r\\right)=0.}&\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This expression is equivalent to ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left(\\!\\!\\begin{array}{c c c}{e_{1}\\otimes I}&{0}\\\\ {H^{\\top}\\otimes I-I\\otimes A}&{e_{K}\\otimes I}\\end{array}\\!\\!\\right)\\left(\\!\\!\\begin{array}{c}{\\mathrm{vec}\\left(Q\\right)}\\\\ {\\mathrm{vec}\\left(r\\right)}\\end{array}\\!\\!\\right)=\\left(\\!\\!\\begin{array}{c}{\\mathrm{vec}\\left(c v\\right)}\\\\ {0}\\end{array}\\!\\!\\right)\\in\\mathbb{R}^{N\\left(K+1\\right)\\times1}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This is similar to the work by Deuflhard [94], who explain adjoints of three-term recurrence relations. Since $c^{2}=\\langle v,v\\rangle$ holds (ie, $c$ is known), the first row of $Q$ is known. Then, the first row of $Q$ together with the orthogonality constraints yields the first row of $H^{\\top}$ , which then defines the next row of the linear system thus the second row of $Q$ . Alternating between deriving the next row of $H^{\\top}$ and solving the lower triangular systems is then Arnoldi\u2019s algorithm: ", "page_idx": 20}, {"type": "text", "text": "Algorithm E.1 (Arnoldi\u2019s forward pass; paraphrased). Assume that $v$ and $K$ are known. Compute $c=\\sqrt{\\langle v,v\\rangle}$ . Then, for $k=1,...,K$ , alternate the following tsteps: ", "page_idx": 21}, {"type": "text", "text": "1. Derive the next column of $H$ using the orthogonality constraints. ", "page_idx": 21}, {"type": "text", "text": "2. Forward-substitute (\u201csolve\u201d) the block-lower-triangular system for the next column of $Q$ (respectively $r$ at the last iteration). ", "page_idx": 21}, {"type": "text", "text": "Return all $Q$ and $H$ , as well as $c$ and $r$ . ", "page_idx": 21}, {"type": "text", "text": "The same principle applies to the adjoint system, and the only difference is that the notation is slightly more complicated: ", "page_idx": 21}, {"type": "text", "text": "E.2.2 Solving the adjoint system ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The constraints $Z_{Q}=0$ and $Z_{r}=0$ mirror those of $A Q-Q H-r(e_{K})^{\\top}=0$ and $Q e_{1}-c v=0$ ; the constraints $Z_{H}=0$ and $Z_{c}=0$ mirror the orthogonality constraints $Q^{\\top}Q=I$ and $Q^{\\top}r=0$ . Therefore, we start with $Z_{Q}=0$ and $Z_{r}=0$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{Q}f+\\boldsymbol{A}^{\\top}\\boldsymbol{\\Lambda}-\\boldsymbol{\\Lambda}\\boldsymbol{H}^{\\top}+\\boldsymbol{\\lambda}(e_{1})^{\\top}+\\boldsymbol{Q}(I_{\\leq}\\circ\\Gamma)+\\boldsymbol{Q}(I_{\\leq}\\circ\\Gamma)^{\\top}+\\boldsymbol{r}\\boldsymbol{\\gamma}^{\\top}=0}\\\\ {\\nabla_{r}f-\\boldsymbol{\\Lambda}e_{K}+\\boldsymbol{Q}\\gamma=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Introduce the auxiliary quantities ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\Psi(\\Gamma,\\gamma):=\\nabla_{Q}f+Q(I_{\\leq}\\circ\\Gamma)+Q(I_{\\leq}\\circ\\Gamma)^{\\top}+r\\gamma^{\\top}\\qquad\\qquad}&&{\\in\\mathbb{R}^{N\\times K}}\\\\ &{\\quad\\psi(\\gamma):=\\nabla_{r}f+Q\\gamma\\qquad}&&{\\in\\mathbb{R}^{N\\times1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "$\\Psi$ and $\\psi$ only serve the purpose of simplifying the notation in the coming part; there is no \u201cmeaning\u201d associated with them. Vectorise both expressions, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{(I\\otimes A^{\\top})\\mathrm{vec}\\left(\\Lambda\\right)-[(H^{\\top})^{\\top}\\otimes I]\\mathrm{vec}\\left(\\Lambda\\right)+(e_{1}\\otimes I)\\mathrm{vec}\\left(\\lambda\\right)=-\\mathrm{vec}\\left(\\Psi(\\Gamma,\\gamma)\\right)}\\\\ &{}&{\\quad[(e_{K})^{\\top}\\otimes I]\\mathrm{vec}\\left(\\Lambda\\right)=\\mathrm{vec}\\left(\\psi(\\gamma)\\right)\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and observe that this can be written as a linear system ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left({\\begin{array}{c c}{e_{1}\\otimes I}&{I\\otimes A^{\\top}-(H^{\\top})^{\\top}\\otimes I}\\\\ {0}&{(e_{K})^{\\top}\\otimes I}\\end{array}}\\right)\\left(\\operatorname{vec}\\left(\\lambda\\right)\\right)=\\left(\\operatorname{-vec}\\left(\\Psi(\\Gamma,\\gamma)\\right)\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "with a system matrix that is the transpose of the system matrix of the forward pass. The matrix is upper triangular, and the equation can be solved with backward substitution provided $\\psi(\\gamma)$ and $\\Psi(\\bar{\\Gamma},\\gamma)$ are known. ", "page_idx": 21}, {"type": "text", "text": "The defining quantities $\\Psi$ and $\\psi$ emerge by combining the adjoint recursion with the projection constraints $Z_{H}=0$ (for $\\Gamma$ , which yields $\\Psi$ ) and $Z_{r}=0$ (for $\\gamma$ , which yields $\\psi_{.}$ ). We use $Z_{c}=0$ to get a single element in $\\Gamma$ ; more on this below. Summarise the adjoint pass: ", "page_idx": 21}, {"type": "text", "text": "Algorithm E.2 (Arnoldi\u2019s adjoint pass; paraphrased). Assume $Q,H,c$ , and $r$ as well as the gradients of $f$ with respect to those quantities. Then, compute $\\psi$ via computing $\\gamma$ using $Z_{r}=0$ . Then, for $k=K,...,1$ , alternate the following two steps: ", "page_idx": 21}, {"type": "text", "text": "1. Derive the next row of $\\Psi$ by combining $Z_{Q}=0$ with the projection constraint $Z_{H}=0$ ", "page_idx": 21}, {"type": "text", "text": "2. Backward-substitute (\u201csolve\u201d) for the next row of $\\Lambda$ (recall: we loop backwards) ", "page_idx": 21}, {"type": "text", "text": "Finally, use $Z_{c}\\,=\\,0$ to get the first row of $\\Psi$ and solve for $\\lambda$ . Then, return $\\nabla_{A}f\\,=\\,\\Lambda Q^{\\top}$ and $\\nabla_{v}f=\\lambda c^{\\top}$ . ", "page_idx": 21}, {"type": "text", "text": "The structure of the adjoint pass is similar to the forward pass (Table 6). In the following, we will elaborate on each of those steps. We assume that the reader knows how to solve a lower triangular linear system. We focus on constructing $\\psi$ and $\\Psi$ via $\\Gamma$ and $\\gamma$ . ", "page_idx": 21}, {"type": "table", "img_path": "RL4FXrGcTw/tmp/673f074835bbe3c65540aa7a73fb4baf6d751aae7e531c5fd99d4424cddd37ce.jpg", "table_caption": ["Table 6: Forward versus adjoint (backward) pass "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "E.2.3 Initialisation ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Initialisation of the adjoint pass implies computing $\\psi$ . To get $\\psi$ , we need $\\gamma$ : Consider multiplying $Z_{r}$ with $Q^{\\top}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0=Q^{\\top}Z_{r}=Q^{\\top}\\nabla_{r}f-Q^{\\top}\\Lambda e_{K}+\\gamma}\\\\ &{\\qquad\\qquad=Q^{\\top}\\nabla_{r}f-\\left(\\nabla_{H}f+I_{\\ll}\\circ\\Sigma\\right)e_{K}+\\gamma}\\\\ &{\\qquad\\qquad=(Q^{\\top}\\nabla_{r}f-\\nabla_{H}f e_{K})+(I_{\\ll}\\circ\\Sigma)e_{K}+\\gamma}\\\\ &{\\qquad\\qquad=(Q^{\\top}\\nabla_{r}f-\\nabla_{H}f e_{K})+\\gamma}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n((I_{\\ll}\\circ\\Sigma)e_{K}=0)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we use that the last column in $I_{\\ll}\\circ\\Sigma$ consists entirely of zeros. All other quantities are known. Therefore, $\\gamma$ is isolated and we identify ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\gamma=\\nabla_{H}f e_{K}-Q^{\\top}\\nabla_{r}f.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Next, use this $\\gamma$ to build $\\psi$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\psi(\\gamma)=\\nabla_{r}f+Q\\gamma\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and the initialisation step is complete. ", "page_idx": 22}, {"type": "text", "text": "E.2.4 Recursion ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "With $\\psi$ in place, we get the last column of $\\Lambda$ . To get the next column of $\\Lambda$ , we need to derive the right-hand side $\\Psi$ . To get $\\Psi$ , we need $\\Gamma$ . ", "page_idx": 22}, {"type": "text", "text": "Multiply $Q^{\\top}Z_{Q}$ to obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0=Q^{\\top}Z_{Q}}\\\\ &{\\quad=Q^{\\top}\\nabla_{Q}f+Q^{\\top}A^{\\top}\\Lambda-Q^{\\top}\\Lambda H^{\\top}+Q^{\\top}\\lambda(e_{1})^{\\top}+I_{\\leq}\\circ\\Gamma+(I_{\\leq}\\circ\\Gamma)^{\\top}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{(since~}Q^{\\top}Q=I\\mathrm{~and~}Q}\\\\ &{\\quad=Q^{\\top}\\nabla_{Q}f+Q^{\\top}A^{\\top}\\Lambda-(\\nabla_{H}f+I_{\\ll}\\circ\\Sigma)H^{\\top}+Q^{\\top}\\lambda(e_{1})^{\\top}+I_{\\leq}\\circ\\Gamma+(I_{\\leq}\\circ\\Gamma)^{\\top}}\\\\ &{\\quad=(Q^{\\top}\\nabla_{Q}f-\\nabla_{H}f H^{\\top})+Q^{\\top}A^{\\top}\\Lambda+Q^{\\top}\\lambda(e_{1})^{\\top}+I_{\\leq}\\circ\\Gamma+(I_{\\leq}\\circ\\Gamma)^{\\top}+(I_{\\ll}\\circ\\Sigma)^{\\top}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $H$ is Hessenberg, $(I_{\\ll}\\circ\\Sigma)H^{\\top}$ is strictly lower triangular. Therefore, multiplication with $I_{\\geq}$ removes $\\Sigma$ from the expression, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0=I_{\\geq}\\circ\\left[Q^{\\top}\\nabla_{Q}f-\\nabla_{H}f H^{\\top}+Q^{\\top}A^{\\top}\\Lambda+Q^{\\top}\\lambda(e_{1})^{\\top}+I_{\\leq}\\circ\\Gamma+(I_{\\leq}\\circ\\Gamma)^{\\top}\\right]}\\\\ &{\\quad=I_{\\geq}\\circ\\left[Q^{\\top}\\nabla_{Q}f-\\nabla_{H}f H^{\\top}+Q^{\\top}A^{\\top}\\Lambda\\right]+I_{\\geq}\\circ[Q^{\\top}\\lambda(e_{1})^{\\top}]+I_{=}\\circ\\Gamma+I_{\\geq}\\circ\\Gamma^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The term involving $\\lambda$ can be simplified as follows: Due to the presence of $(e_{1})^{\\top}$ , we know that $Q^{\\top}\\lambda(e_{1})^{\\top}$ is lower triangular (in fact, it has a single nonzero column). Thus, $I_{\\geq}\\circ(Q^{\\top}\\lambda(e_{1})^{\\top})$ is proportional to $e_{1}(e_{1})^{\\top}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{\\geq}\\circ(Q^{\\top}\\lambda(e_{1})^{\\top})=[(Q e_{1})^{\\top}\\lambda]\\,e_{1}(e_{1})^{\\top}}\\\\ &{\\qquad\\qquad\\qquad\\quad=c v^{\\top}\\lambda\\,e_{1}(e_{1})^{\\top}}\\\\ &{\\qquad\\qquad\\quad=c\\,\\nabla_{c}f\\,e_{1}(e_{1})^{\\top}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and all quantities are known; hence, ", "page_idx": 22}, {"type": "equation", "text": "$$\nI_{=}\\circ\\Gamma+I_{\\geq}\\circ\\Gamma^{\\top}=-I_{\\geq}\\circ[Q^{\\top}\\nabla_{Q}f-\\nabla_{H}f H^{\\top}+Q^{\\top}A^{\\top}\\Lambda]-c\\nabla_{c}f\\,e_{1}(e_{1})^{\\top}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Algorithm E.3 (Forward pass). Initialise $k=$ $1,Q e_{1}$ . Then, for $k=1,...,K$ : ", "page_idx": 23}, {"type": "text", "text": "Algorithm E.4 (Backward pass). Initialise $k=K$ , $\\Lambda e_{1}$ . Then, for $k=K,...,1$ : ", "page_idx": 23}, {"type": "text", "text": "1. Use orthogonality for a new row in the system matrix in Equation 39. 2. Solve for the next column of $Q$ 3. Optional: re-enforce $Q^{\\top}Q=I$ . ", "page_idx": 23}, {"type": "text", "text": "1. Use projection for a new column of the right-hand side $\\Psi$ 2. Solve for the next column of $\\Lambda$ 3. Optional: re-enforce $Z_{H}=0$ . ", "page_idx": 23}, {"type": "text", "text": "Solve for $r$ and return $Q,H,r,c.$ . ", "page_idx": 23}, {"type": "text", "text": "Solve for $\\lambda$ and return $\\nabla_{\\boldsymbol{\\theta}}\\rho$ and $\\nabla_{v}\\rho$ . ", "page_idx": 23}, {"type": "text", "text": "Figure 8: Forward and backward pass of the Arnoldi iteration (paraphrased) ", "page_idx": 23}, {"type": "text", "text": "must hold. ", "page_idx": 23}, {"type": "text", "text": "Now, the most important observation is the following: the last column of $I_{=}\\circ\\Gamma+I_{\\geq}\\circ\\Gamma^{\\top}$ depends on the last column of $Q^{\\top}A^{\\top}\\Lambda$ and known quantities; the penultimate column depends on the penultimate column of $\\Gamma$ , and so on. But at the time of assembling the last column of $\\Gamma$ , the last column of $\\Lambda$ is known! More generally, we always know one more column of $\\Lambda$ than of $\\Gamma$ , so we can recursively assemble $I_{=}\\circ\\bar{\\Gamma}+I_{\\geq}\\circ\\Gamma^{\\top}$ : ", "page_idx": 23}, {"type": "text", "text": "Let ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname{sym}\\left(M\\right):=I_{\\geq}\\circ M+\\left(I_{>}\\circ M\\right)^{\\top}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "be a symmetrisation operator. We define it for the sole purpose of reconstructing ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname{sym}\\left(I_{=}\\circ\\Gamma+I_{\\geq}\\circ\\Gamma^{\\top}\\right)=I_{\\leq}\\circ\\Gamma+(I_{\\leq}\\circ\\Gamma)^{\\top}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let us use it: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{I_{\\leq}\\circ\\Gamma+(I_{\\leq}\\circ\\Gamma)^{\\top}=\\mathrm{sym}\\left(-I_{\\geq}\\circ[Q^{\\top}\\nabla_{Q}f-\\nabla_{H}f H^{\\top}+Q^{\\top}A^{\\top}\\Lambda]-c\\nabla_{c}f\\,e_{1}(e_{1})^{\\top}\\right)}\\\\ {=\\mathrm{sym}\\left(-I_{\\geq}\\circ[Q^{\\top}\\nabla_{Q}f-\\nabla_{H}f H^{\\top}+Q^{\\top}A^{\\top}\\Lambda]\\right)-c\\nabla_{c}f\\,e_{1}(e_{1})^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This yields the next row/column of $\\Gamma+\\Gamma^{\\top}$ , and therefore the next row of $\\Psi$ . From there, we can assemble the next column of $\\Lambda$ and iterate. Figure 8 (respectively Algorithms E.3 and E.4) compare pseudocode for forward and adjoint passes. Altogether, the implementation of the adjoint pass is very similar to that of the forward pass. ", "page_idx": 23}, {"type": "text", "text": "At the final step, we obtain not the last column of $\\Lambda$ but $\\lambda$ , though this is a byproduct of solving the triangular linear system. It does not need further explanation. ", "page_idx": 23}, {"type": "text", "text": "Remark E.5 $(\\Sigma)$ . Like for gradients of QR decompositions [61, 75], we never solve for $\\Sigma$ . ", "page_idx": 23}, {"type": "text", "text": "F Setup for Table 2 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To create Table 2, we implement an operator ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{Z}:A\\mapsto(H,Q,r,c)\\mapsto Q H Q^{\\top}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $(H,Q,r,c)$ are the result of a full-rank Arnoldi iteration (i.e. $K=N$ ). For $K=N$ , $Q H Q^{\\top}=A$ and $\\mathcal{T}$ must have an identity Jacobian; thus, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\varepsilon:=\\|I_{N^{2}}-\\mathcal{T}\\|_{\\mathrm{RMSE}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "image", "img_path": "RL4FXrGcTw/tmp/594af241621d026f80a513221622ff951fe74673093313acfcda45ab55f0315a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "measures the loss of accuracy when differentiating the Arnoldi iteration. A small $\\varepsilon$ is desirable. ", "page_idx": 23}, {"type": "text", "text": "Then, using double-precision, we construct a Hilbert matrix $A=[1/(i+j+1)]_{i,j=1}^{N}\\in\\mathbb{R}^{N\\times N}$ which is a famously ill-conditioned matrix and a common test-bed for ", "page_idx": 23}, {"type": "text", "text": "Figure 9: Accuracy loss $\\varepsilon$ when differentiating $\\mathcal{T}$ for a Hilbert matrix of increasing size $N$ . Uses double precision. ", "page_idx": 23}, {"type": "text", "text": "the loss of orthogonality in methods like the Lanczos and Arnoldi iteration [e.g. 71, Table 7.1]. We evaluate three algorithms, all of which rely on the Arnoldi iteration with full reorthogonalisation on the forward-pass: One algorithm does not re-project on the adjoint constraints, another one does, and ", "page_idx": 23}, {"type": "image", "img_path": "RL4FXrGcTw/tmp/72c8ee91ddced526ada7adb5fdd6a97c82ee38960d033640b3ac4e86c7c9a732.jpg", "img_caption": ["Figure 10: For matrices with at least 10,000 rows/columns, KeOps remains the state of the art. This experiment uses a square-exponential kernel, on an artificial dataset with $d=3$ dimensions. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "for reference we compute $\\varepsilon$ when \u201cbackpropagating through\u201d the re-orthogonalised Arnoldi iteration as a third option. Figure 3 has demonstrated that the first two options beat the third one in terms of speed, but we consider numerical accuracy here. ", "page_idx": 24}, {"type": "text", "text": "We evaluate $\\varepsilon$ for $N\\,=\\,1,...,8$ (see Figure 9), and show the values for $N\\,=\\,8$ in Table 2. The numerical accuracy of the re-projected adjoint method matches that of differentiating \u201cthrough\u201d re-orthogonalisation, and outperforms not re-projecting by a margin. ", "page_idx": 24}, {"type": "text", "text": "G Memory-efficient kernel-matrix-vector products in JAX ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Matrix-free linear algebra requires efficient matrix-vector products. For kernel function $k=k(x,x^{\\prime})$ , and input data $x_{1},...,x_{N}$ , Gaussian process covariance matrices are of the form $A=[k(x_{i},x_{j})]_{i,j=1}^{N}$ . Matrix-vector products with $A$ thus look like ", "page_idx": 24}, {"type": "equation", "text": "$$\nv\\mapsto A v=\\left[\\sum_{j=1}^{N}k(x_{i},x_{j})v_{j}\\right]_{i=1}^{N}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and can be assembled row-wise, either sequentially or parallely. ", "page_idx": 24}, {"type": "text", "text": "The more rows we assemble in parallel, the faster the runtime but also the higher the memory requirements, so we follow Gardner et al. [6] and choose the largest number of rows of $A$ that still fit into memory, say $r$ such rows, and assemble $A v$ in blocks of $r$ . In practice, we implement this in JAX by combining jax.lax.map and $\\mathtt{j a x.v m a p}$ , but care has to be taken with reverse-mode automatic differentiation through $(v,\\theta)\\mapsto A(\\theta)v$ because by default, reverse-mode differentiation stores all intermediate results. To solve this problem, we place checkpoints around each such batch of rows, which reduces the memory requirements but roughly doubles the runtime. (We place another checkpoint around each stochastic trace-estimation sample, which roughly doubles the runtime again.) ", "page_idx": 24}, {"type": "text", "text": "An alternative to doing this manually is the KeOps library [82], which GPyTorch [6] builds on. However, there currently exists no JAX-compatible interface to KeOps which is why we have to implement the above solution. ", "page_idx": 24}, {"type": "text", "text": "Figure 10 compares the runtime of our approach to that of KeOps custom CUDA code. We see that we are competitive, but roughly $5\\times$ slower for medium to large datasets. Multiplying this with the $4\\times$ increase due to the checkpoints discussed above explains the $20\\times$ increase in runtime compared to GPyTorch. Being $20\\times$ slower than GPyTorch per epoch is only due to the matrix-vector products, and has nothing to do with the algorithm contribution. Future work should explore closing this gap with a KeOps-to-JAX interface. ", "page_idx": 24}, {"type": "text", "text": "H Experiment configurations for the Gaussian process study ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Data For the experiments we use the \u201cProtein\u201d, \u201cKEGG (undirected\u201d, \u201cKEGG (directed)\u201d, \u201cElevators\u201d, and $\\mathrm{^{\\bullet\\bullet}K i n40k^{\\circ}}$ datasets (Table 7, adapted from Bartels et al. [95]). All are part of the UCI data ", "page_idx": 24}, {"type": "table", "img_path": "RL4FXrGcTw/tmp/0abf0d1eb9fc6d2f048940d70ea61483ff887a40e72f4d14cbc6390720e9e6cf.jpg", "table_caption": ["Table 7: Datasets used in this study. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "repository, and accessible through there. ", "page_idx": 25}, {"type": "text", "text": "The data is subsampled to admit the train/test split of $80/20\\%$ , and to admit an even division into the number of row partitions. More specifically, we use 10 partitions for the kernel-matrix vector products. This way, we have to discard less than $1\\%$ of the data; e.g., on KEGG (undir), we use 63,600 instead of the original 63,608 points. ", "page_idx": 25}, {"type": "text", "text": "We calibrate a Mate\u00b4rn prior with smoothness $\\nu=1.5$ , using 10 matrix-vector products per Lanczos iteration, conjugate gradients tolerance of $\\epsilon=1$ , a rank-15 pivoted Cholesky preconditioner, and 10 Rademacher samples. We evaluate all samples sequentially (rematerialising on the backward pass to save memory, as discussed in Appendix G). The conjugate-gradients tolerances are taken to be absolute (instead of relative), and the parametrisations of the Gaussian process models and loss functions match that of GPyTorch. ", "page_idx": 25}, {"type": "text", "text": "For every model, we calibrate an independent lengthscale for each input dimension, as well as an scalar observation noise, scalar output-scale, and the value of a constant prior mean. All parameters are initialised randomly. We use the Adam optimiser with learning rate 0.05 for 75 epochs. All experiments are repeated for three different seeds. ", "page_idx": 25}, {"type": "text", "text": "I Partial differential equation data ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We generate data for the differential equations as follows: Recall the problem setup of a partial differential equation ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}}{\\partial^{2}t}u(t;x_{1},x_{2})=\\omega(x_{1},x_{2})^{2}\\left(\\frac{\\partial^{2}}{\\partial x_{1}^{2}}u(t;x_{1},x_{2})+\\frac{\\partial^{2}}{\\partial x_{2}^{2}}u(t;x_{1},x_{2})\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "with Neumann boundary conditions. The coefficient field $\\omega$ is space- but not time-dependent. ", "page_idx": 25}, {"type": "text", "text": "First, we discretise the Laplacian operator with central differences on an equidistant, tensor-product mesh that consists of 128 points per dimension, which yields $128^{2}$ grid points. The resulting second-order ordinary differential equation ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}}w=\\omega^{2}M w,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $M$ is the discretised Laplacian, is then transformed into a first-order differential equation ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left(\\!\\!\\begin{array}{c}{{w}}\\\\ {{\\dot{w}}}\\end{array}\\!\\!\\right)=\\left(\\!\\!\\begin{array}{c c}{{0}}&{{I}}\\\\ {{\\omega^{2}M}}&{{0}}\\end{array}\\!\\!\\right)w=:A w.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This equation is solved by the matrix exponential, and the system matrix $A$ is asymmtric (by construction), and highly sparse because $M$ is. Matrix-vector products with $A$ are cheap, because we can implement them with jax.scipy.signal.convolve2d. ", "page_idx": 25}, {"type": "text", "text": "Then, we sample a true $\\omega$ from a Gaussian process with a square-exponential covariance kernel, using lengthscale softplus $(-0.75)$ and output-scale softplus $(-10)$ . We sample from this process with the Lanczos algorithm [11] using Krylov-depth $K=32$ . ", "page_idx": 25}, {"type": "text", "text": "Then, we use another Gaussian process with the same kernel, but lengthscale softplus(0) and output scale softplus(0), to sample 256 initial distributions \u2013 again with the Lanczos algorithm ", "page_idx": 25}, {"type": "image", "img_path": "RL4FXrGcTw/tmp/1d7a9350cd8f9271c89d6cdc95e2665898c8208730f7cce2ee43c04e181255cb.jpg", "img_caption": ["Figure 11: Three exemplary input/output pairs from the PDE dataset. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "[11]. These 256 initial conditions are solved with Diffrax\u2019s implementation of Dopri8 [99] using 128 timesteps. Some example input/output pairs are in Figure 11. ", "page_idx": 26}, {"type": "text", "text": "This setup is similar to that of the WaveBench dataset [92], with the main difference being that the WaveBench dataset uses a slightly different formulation of the wave equation.4 We use the one above because it lends itself more naturally to matrix exponentials, which are at the heart of this experiment. ", "page_idx": 26}, {"type": "text", "text": "J Implementation details for the Bayesian neural network study ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "J.1 Bayesian neural networks with Laplace approximations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Another possible application of the gradients of matrix functions is marginal-likelihood-based optimisation of Bayesian Neural Networks. Suppose $g_{\\theta}(x)$ is the output of a neural network with parameters $\\boldsymbol{\\theta}\\in\\mathbb{R}^{P}$ . The choice of the model shall be denoted by $\\mathcal{M}$ and consist of both continuous and discrete hyperparameters (such as network architecture, likelihood precision, prior precision, etc.). For some choice of prior given by ", "page_idx": 26}, {"type": "equation", "text": "$$\np(\\theta\\mid\\mathcal{M})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and likelihood ", "page_idx": 26}, {"type": "equation", "text": "$$\np(y|x,\\theta,,\\mathcal{M})=p(y\\mid x,g_{\\theta}(x),\\mathcal{M})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "we can specify a Bayesian model. The posterior distribution is then given by: ", "page_idx": 26}, {"type": "equation", "text": "$$\np(\\theta,y\\mid x,\\mathcal{M})\\propto p(y\\mid x,g_{\\theta}(x),\\mathcal{M})p(\\theta\\mid\\mathcal{M})d\\theta.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The marginal likelihood is given by normalizing constant of this posterior, i.e. ", "page_idx": 26}, {"type": "equation", "text": "$$\np(y\\mid x,\\mathcal{M})=\\int p(y\\mid x,g_{\\theta}(x),\\mathcal{M})p(\\theta\\mid\\mathcal{M})d\\theta.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "As suggested by MacKay [101], this marginal likelihood can be used for model selection in Bayesian neural networks. Immer et al. [30] use the Laplace approximation of the posterior to obtain access to the marginal likelihood of the Bayesian neural network and its stochastic gradients. ", "page_idx": 26}, {"type": "text", "text": "The Laplace approximation of the marginal likelihood is given by: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\log p(\\boldsymbol{y}\\mid\\boldsymbol{x},\\mathcal{M})\\approx\\log p(\\boldsymbol{y},\\theta_{\\mathrm{MAP}}\\mid\\boldsymbol{x},\\mathcal{M})-\\frac{1}{2}\\log\\operatorname*{det}\\left(\\frac{1}{2\\pi}H_{\\theta_{\\mathrm{MAP}}}\\right)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\begin{array}{r}{H_{\\theta_{\\mathrm{MAP}}}\\,=\\,-\\nabla_{\\theta}^{2}\\log p(y,\\theta_{\\mathrm{MAP}}\\mid x,\\mathcal{M})}\\end{array}$ . Usual choices of the prior are $N(0,\\alpha^{-1}\\mathbb{I})$ . Usually this Hessian is approximated with the generalized Gauss-Newton (GGN) matrix [102] ", "page_idx": 26}, {"type": "equation", "text": "$$\nH_{\\theta\\mathrm{{Mar}}}\\approx A(\\alpha):=\\sum_{j=1}^{J}[D_{\\theta}g_{\\theta\\mathrm{{Mar}}}])(x_{j})^{\\top}[D_{g}^{2}\\rho](y_{j},g_{\\theta\\mathrm{{Mar}}}(x_{j}))[D_{\\theta}g_{\\theta\\mathrm{{Mar}}}])(x_{j})^{\\top}+\\alpha^{2}I\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $D^{2}\\rho$ is the Hessian of the loss, and ${\\cal D}_{\\theta}g$ the Jacobian of $g$ (recall Equation 2). This objective is used to optimize the prior precision of the model or any continuous model hyperparameters. Matrix-vector products with the GGN matrix can be accessed through automatic differentiation using Jacobian-vector and vector-Jacobian products. With these efficient matrix-vector products, one can estimate the log-determinant of GGN using matrix-free techniques like the Lanczos iteration. ", "page_idx": 27}, {"type": "text", "text": "To make predictions using the Laplace approximation of the posterior, we also need to sample from the normal distribution $N(\\theta_{\\mathrm{MAP}},A^{-1})$ . Samples from this distribution can be written as: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\theta=\\theta_{\\mathrm{MAP}}+A^{-1/2}\\epsilon\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\epsilon\\sim N(0,I)$ . The main bottleneck in this computation is the inversion and matrix square root of the GGN matrix, and we implement it with a Lanczos iteration using $f(x)=x^{-1/2}$ . Since the GGN is empirically known to have low-rank [103], doing a few Lanczos iterations can get us close to an accurate estimation. ", "page_idx": 27}, {"type": "text", "text": "J.2 Experiment setup ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We estimate the diagonal of the GGN stochastically via (\u201c\u25e6\u201d is the element-wise product) [104] ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname{diagonal}\\left(A\\right)=\\mathbb{E}[v\\circ A v]\\approx\\frac{1}{L}\\sum_{\\ell=1}^{L}v_{\\ell}\\circ A v,\\quad\\mathbb{E}[v v^{\\top}]=I.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We use 150 matrix-vector products for both diagonal calibration and our Lanczos-based estimation. We use 30 Monte-Carlo samples to estimate the log-likelihoods for evaluating the test metrics, and we use places365 [91] as an out-of-distribution dataset to compute OOD-AUROC. We also compute the expected calibration error (ECE) [105] of the model. ", "page_idx": 27}, {"type": "text", "text": "Data: We show scalability by doing Laplace approximation on Imagenet1k image classification [89].   \nThe training set consists of approximately 1.2 million images, each belonging to one of 1000 classes.   \nWe find that we can take small subsets of this dataset and still converge to the same prior precision.   \nOur computational budget allows us to use 10 percent of the samples for each class. However, even for very small subsamples of the data, we converge to a very similar prior precision. ", "page_idx": 27}, {"type": "text", "text": "Method: To optimize the prior precision we use the marginal likelihood as the objective. We use the RMSprop optimizer with a learning rate of 0.01 for 100 epochs for optimizing both the diagonal and Lanczos approximations of the GGN. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The main contributions, adjoint systems for the Lanczos and Arnoldi iterations, are explained in Section 4. The three case studies are in Sections 5 to 7. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The limitations are discussed in the paragraph titled \u201cLimitations and future work\u201d on page 4. All assumptions (i.e., Assumptions 3.1 and 3.2) are contextualised in the sentences before and after they are have been introduced. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The main contributions, Theorems 4.1 and 4.2 and Corollary 4.3, are proven in Appendices B to D. Appendix E discusses solving the adjoint system in full detail. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The most important information about the experiment setup is a part of the main paper in Sections 5 to 7; further information can be found in Appendices $\\mathrm{G}$ to J. Appendices A and F discuss the setup for Figure 3 and Table 2. Code will be published upon acceptance. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Code has been submitted as a part of the supplementary material, and will be published upon acceptance. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: See the answer to \u201c4. Experimental Result Reproducibility\u201d above. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: All case studies report mean and standard deviations of multiple runs. The only exception is the Bayesian neural network example, which uses a single training run (but evaluates test metrics on multiple seeds). ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The introductory part of the appendix contains a paragraph titled \u201cCompute\u201d. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We use datasets that are either self-created (Section 6 and Appendix I), or common test-cases for machine learning methods (UCI datasets, ImageNet) or numerical algorithms (SuiteSparse matrix collection). ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "page_idx": 31}, {"type": "text", "text": "Justification: This research provides a foundational algorithm for computational sciences, and societal impact is difficult if not impossible to predict. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper does not contribute data or models that would require safeguards. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All assets used in the experiments have been cited. See also the answer to \u201c9.   \nCode of Ethics\u201d. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The code (attached to this submission) is documented. The appendices contain all other information. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This study does not involve crowdsourcing nor human subjects. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 33}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]