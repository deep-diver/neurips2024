{"references": [{"fullname_first_author": "Alex Graves", "paper_title": "Sequence transduction with recurrent neural networks", "publication_date": "2012-11-21", "reason": "This paper introduced the RNN-Transducer, a foundational model for sequence transduction that heavily influenced the development of the Aligner-Encoder."}, {"fullname_first_author": "Alex Graves", "paper_title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "publication_date": "2006-01-01", "reason": "This paper introduced Connectionist Temporal Classification (CTC), a crucial technique for training sequence-to-sequence models without explicit alignments, which is contrasted with the proposed method."}, {"fullname_first_author": "Jan K Chorowski", "paper_title": "Attention-based models for speech recognition", "publication_date": "2015-01-01", "reason": "This paper introduced attention-based encoder-decoder models for speech recognition, providing a key comparison point for the new Aligner-Encoder architecture."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This paper introduced the Transformer architecture, a fundamental building block of the Aligner-Encoder, and its use in sequence transduction is a key aspect of the paper's contribution."}, {"fullname_first_author": "Anmol Gulati", "paper_title": "Conformer: Convolution-augmented transformer for speech recognition", "publication_date": "2020-05-01", "reason": "This paper introduced the Conformer encoder, a specific neural network architecture used in the experiments, demonstrating state-of-the-art performance and providing a strong baseline for comparison."}]}