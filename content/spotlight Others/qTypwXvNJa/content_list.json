[{"type": "text", "text": "Geodesic Optimization for Predictive Shift Adaptation on EEG data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Apolline Mellot,\u2217 Antoine Collas ", "page_idx": 0}, {"type": "text", "text": "Sylvain Chevallier ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Inria, CEA, Universit\u00e9 Paris-Saclay Palaiseau, France apolline.mellot@inria.fr antoine.collas@inria.fr ", "page_idx": 0}, {"type": "text", "text": "TAU Inria, LISN-CNRS,   \nUniversity Paris-Saclay, France. sylvain.chevallier@   \nuniversite-paris-saclay.fr ", "page_idx": 0}, {"type": "text", "text": "Alexandre Gramfort ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Denis A. Engemann ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Inria, CEA, Universit\u00e9 Paris-Saclay Palaiseau, France alexandre.gramfort@inria.fr ", "page_idx": 0}, {"type": "text", "text": "Roche Pharma Research and Early Development, Neuroscience and Rare Diseases, Roche Innovation Center Basel,   \nF. Hoffmann\u2013La Roche Ltd., Basel, Switzerland. denis.engemann@roche.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Electroencephalography (EEG) data is often collected from diverse contexts involving different populations and EEG devices. This variability can induce distribution shifts in the data $X$ and in the biomedical variables of interest $y$ , thus limiting the application of supervised machine learning (ML) algorithms. While domain adaptation (DA) methods have been developed to mitigate the impact of these shifts, such methods struggle when distribution shifts occur simultaneously in $X$ and $y$ . As state-of-the-art ML models for EEG represent the data by spatial covariance matrices, which lie on the Riemannian manifold of Symmetric Positive Definite (SPD) matrices, it is appealing to study DA techniques operating on the SPD manifold. This paper proposes a novel method termed Geodesic Optimization for Predictive Shift Adaptation (GOPSA) to address test-time multi-source DA for situations in which source domains have distinct $y$ distributions. GOPSA exploits the geodesic structure of the Riemannian manifold to jointly learn a domain-specific re-centering operator representing site-specific intercepts and the regression model. We performed empirical benchmarks on the cross-site generalization of age-prediction models with resting-state EEG data from a large multi-national dataset (HarMNqEEG), which included 14 recording sites and more than 1500 human participants. Compared to state-of-the-art methods, our results showed that GOPSA achieved significantly higher performance on three regression metrics ( $R^{2}$ , MAE, and Spearman\u2019s $\\rho$ ) for several source-target site combinations, highlighting its effectiveness in tackling multi-source DA with predictive shifts in EEG data analysis. Our method has the potential to combine the advantages of mixed-effects modeling with machine learning for biomedical applications of EEG, such as multicenter clinical trials. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine learning (ML) has enabled advances in the analysis of complex biological signals, such as magneto- and electroencephalography (M/EEG), in diverse applications including biomarker exploration [ 58 , 20 , 60 ] or developing Brain-Computer Interface (BCI) [ 57 , 15 , 1 , 2 ]. However, a major challenge in applying ML to these signals arises from their inherent variability, a problem commonly referred to as dataset shift [ 12 ]. In the case of M/EEG data this variability can be caused by differences in recording devices (electrode positions and amplifier configurations), recording protocols, population demographics, and inter-subject variability [ 36 , 14 , 23 , 29 ]. Notably, shifts can occur not only in the data $X$ but also in the biomedical variable $y$ we aim to predict, further complicating the use of ML algorithms. ", "page_idx": 0}, {"type": "image", "img_path": "qTypwXvNJa/tmp/3f00a460b1b7f869a128f73f79c0f59fe4ffe3f7f0039cbeeb87795f92352f51.jpg", "img_caption": ["Figure 1: Joint shift in $X$ and $y$ distributions on the HarMNqEEG dataset [ 33 ]. Subset of mean PSDs (A) and age distributions $({\\bf{B}})$ from three recording sites used for the empirical benchmarks. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Riemannian geometry has significantly advanced EEG data analysis by enabling the use of spatial covariance matrices as EEG descriptors [ 5 , 4 , 6 , 38 , 31 , 35 , 33 , 48 , 17 , 56 ]. In [ 4 , 6 ], the authors introduced a classification framework for BCI based on the Riemannian geometry of covariance matrices. These methods classify EEG signals directly on the tangent space using the Riemannian manifold of symmetric positive definite (SPD) matrices $(\\mathbb{S}_{d}^{++})$ , effectively capturing spatial information. More recently, [ 47 , 48 , 7 ] extended this framework to regression problems from M/EEG data in the context of biomarker exploration. Furthermore, [ 47 , 48 ] proved that Riemannian metrics lead to regression models with statistical guarantees in line with log-linear brain dynamics [ 10 ] and are, therefore, well-suited for neuroscience applications. Across various biomarker-exploration tasks and datasets, recent work has shown that Riemannian M/EEG representations offer parameter-sparse alternatives to non-Riemannian deep learning architectures [ 13 , 40 , 17 ]. ", "page_idx": 1}, {"type": "text", "text": "Domain adaptation addresses the challenges posed by differences in data distributions between source and target domains, e.g., when data are recorded with different cameras in computer vision [  55 ], different writing styles in natural language processing [ 32 ], or varying sensor setups in time series analysis [ 22 ]. In particular, DA considers target shift where the shift is in the outcome variable $y$ . For classification it means source and target data share the same labels but in different proportions [ 34 ]. Target shift is also frequent in the context of multicenter neuroscience studies, as the studied population of one site may vary significantly from the studied population of another site (cf. Figure 1 ). To tackle various sources of variability in neurophysiological data like EEG, there is a need for a DA approach that can deal with a joint shift in $X$ and $y$ . ", "page_idx": 1}, {"type": "text", "text": "Related work In [ 62 ], the authors addressed DA for EEG-based BCI using re-centering affine transformation of covariance matrices ( Section 2  ) to align data from different sessions or human participants, improving classification accuracies. Yair et al. [  59 ] extended this with parallel transport showing its effectiveness in EEG analysis, whereas, Peng et al [ 43 ] introduced a domain-specific regularizer based on the Riemannian mean. Notably, this parallel transport approach reduces to [ 62 ] when the common reference point is the identity. In a deep learning context, Kobler et al. [ 31 ] proposed to do a per-domain online re-centering which can be seen as a domain specific Riemannian batch norm. Going beyond re-centering, Riemannian Procrustes Analysis (RPA) [ 45 ] was proposed for EEG transfer learning, using three steps: mean alignment, dispersion matching, and rotation correction. However, the rotation step is unsuitable for regression problems and RPA adapts only a single source to a target domain. Recently, [ 36 ] demonstrated the benefits of re-centering for regression problems, showing improvements in handling task variations in MEG and enhancing across-dataset inference in EEG. ", "page_idx": 1}, {"type": "text", "text": "On the other hand, mixed-effects models (or multilevel models) have been successfully used to tackle data shifts in $X$ and $y$ [ 16 , 25 ]. In biomedical data, mixed-effects models are crucial due to the presence of common effects, such as disease status and age. Riemannian mixed-effects models have been used to analyze observations on Riemannian manifolds, accommodating individual trajectories with mixed effects at both group and individual levels [ 30 , 49 , 50 ]. These models adapt a base point on the manifold for each data point and utilize parallel transport for this adaptation, which is necessary for accurate trajectory modeling. However, they differ significantly from the problem we address in this work. Notably, the input data $X$ are covariates (e.g., age or disease status) which belong to a Euclidean space and the variables $y$ to predict belong to the manifold (e.g., MRI diffusion tensors on $\\mathbb{S}_{d}^{++}$ ) which is the opposite of the paper\u2019s studied problem. This distinction is critical as it highlights that while both methods use the geometry of Riemannian manifolds, the nature of the predicted variables and the type of data used differ from existing Riemannian mixed-effects models. ", "page_idx": 2}, {"type": "text", "text": "Contributions In this work, we address the challenging problem of multi-source domain adaptation with predictive shifts on the SPD manifold, focusing on distribution shifts in both the input data $X$ and the variable to predict $y$ . We propose a novel method called Geodesic Optimization for Predictive Shift Adaptation (GOPSA). It enables mixed-effects modeling by jointly learning parallel transport along a geodesic for each domain and a global regression model common to all domains, with the assumption that the mean $\\bar{y}\\tau$ of the target domain is known. GOPSA aims to advance the state of the art by: $(i)$ addressing shifts in both covariance matrices and the outcome variable $y$ , (ii) being tailored for regression problems, and (iii) being a multi-source test-time domain adaptation method, meaning that once trained on source data, it can generalize to any target domain without requiring access to source data or retraining a new model. ", "page_idx": 2}, {"type": "text", "text": "We first introduce in Section 2 how to do regression from covariance matrices on the Riemannian manifold of $\\mathbb{S}_{d}^{++}$ . We also interpret classical learning methods on $\\mathbb{S}_{d}^{++}$ from heterogeneous domains as parallel transports combined with Riemannian logarithmic mappings. This leads us to GOPSA in Section 3 , which learns to parallel transport each domain, with algorithms at train and test times. Finally, in Section 4 , we apply GOPSA as well as different baselines on simulated data and the HarMNqEEG dataset. ", "page_idx": 2}, {"type": "text", "text": "Notations Vectors and matrices are represented by small and large cap boldface letters respectively (e.g., $x,\\,X)$ . The set $\\{1,...,K\\}$ is denoted by $[1,K]$ . $\\mathbb{S}_{d}^{++}$ and $\\mathbb{S}_{d}$ represent the sets of $d\\times d$ symmetric positive definite and symmetric matr i ces.  uvec : $\\mathbb{S}_{d}\\rightarrow\\mathbb{R}^{d(d+1)/2}$ vectorizes the upper triangular part of a symmetric matrix. Frobenius and 2-norms are denoted by $\\|.\\|_{F}$ and $\\lVert.\\rVert_{2}$ , respectively. $\\triangleq$ defines the left part of the equation as the right part. The transpose and Euclidean inner product operations are represented by \u00b7\u22a4. $\\mathbf{1}_{n}\\triangleq[1,\\ldots,1]^{\\intercal}$ denotes an $n$ -dimensional vector of ones. For a loss function $\\mathcal{L}$ , $\\nabla\\mathcal{L}$ and ${\\mathcal{L}}^{\\prime}$ denote its gradient and derivative. We consider $K$ labeled source domains, each consisting of $N_{k}$ covariance matrices, along with their corresponding outcome values, denoted by $\\{(\\Sigma_{k,i},y_{k,i})\\}_{i=1}^{N_{k}}$ . The target domain is $(\\Sigma_{\\mathcal{T},i})_{i=1}^{N_{\\mathcal{T}}}$ with the average outcome $\\bar{y}\\tau$ ", "page_idx": 2}, {"type": "text", "text": "2 Regression modeling from covariance matrices using Riemannian geometry ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Riemannian geometry of $\\mathbb{S}_{d}^{++}$ The covariance matrices belong to the set of $d{\\times}d$ symmetric positive definite matrices denoted $\\mathbb{S}_{d}^{++}$ [ 51 , 44 ]. The latter is open in the set of $d\\times d$ symmetric matrices denoted $\\mathbb{S}_{d}$ , and thus $\\mathbb{S}_{d}^{++}$ is a smooth manifold [ 9 ]. A vector space is defined at each $\\Sigma\\in\\mathbb{S}_{d}^{++}$ , called the tangent space, denoted $T_{\\Sigma}\\mathbb{S}_{d}^{++}$ , and is equal to $\\mathbb{S}_{d}$ , the ambient space. Equipped with a To do so, we make use of the affine invariant Riemannian metric [ 51 , 44 ]. Given smooth inner product at every tangent space, a smooth manifold b ecomes a Riemannian manifold. $\\Gamma,\\Gamma^{\\prime}\\in T_{\\Sigma}\\mathbb{S}_{d}^{++}$ , this metric is $\\langle\\Gamma,\\Gamma^{\\prime}\\rangle_{\\Sigma}=\\mathrm{tr}\\left(\\Sigma^{-1}\\Gamma\\Sigma^{-1}\\Gamma^{\\prime}\\right)$ . ", "page_idx": 2}, {"type": "text", "text": "Riemannian mean The Riemannian distance (or geodesic distance) associated with the affine invariant metric is $\\delta_{R}(\\Sigma,\\Sigma^{\\prime})\\triangleq\\left\\lVert\\log\\left(\\Sigma^{-1/2}\\Sigma^{\\prime}\\Sigma^{-1/2}\\right)\\right\\rVert_{F}$ with $\\log:\\mathbb{S}_{d}^{++}\\to\\mathbb{S}_{d}$ being the matrix logarithm (see Appendix A.1 for a definition). This distance is used to compute the Riemannian mean $\\bar{\\Sigma}$ defined for a set $\\{\\Sigma_{i}\\}_{i=1}^{N}\\subset\\mathbb{S}_{d}^{++}$ as $\\begin{array}{r}{\\overline{{\\Sigma}}\\triangleq\\arg\\operatorname*{min}_{\\Sigma\\in\\mathbb{S}_{d}^{++}}\\sum_{i=1}^{N}\\delta_{R}\\big(\\Sigma,\\mathbf{\\dot{\\Sigma}}\\Sigma_{i}\\big)^{2}}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "This mean is efficiently computed with a Riemannian gradient descent [ 44 , 63 ]. ", "page_idx": 2}, {"type": "text", "text": "Riemannian logarithmic mapping The idea of the covariance-based approach is to define nonlinear feature transformations into vectors that can be used as input for classical linear machine learning models. To do so, the Riemannian logarithmic mapping of $\\Sigma^{\\prime}$ at $\\Sigma$ is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\log_{\\Sigma}(\\Sigma^{\\prime})\\triangleq\\Sigma^{1/2}\\log\\left(\\Sigma^{-1/2}\\Sigma^{\\prime}\\Sigma^{-1/2}\\right)\\Sigma^{1/2}\\in T_{\\Sigma}\\mathbb{S}_{d}^{++}\\ .\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Thus, matrices in the Riemannian manifold $\\mathbb{S}_{d}^{++}$ are transformed into tangent vectors. ", "page_idx": 3}, {"type": "text", "text": "Parallel transport A classical practice to align distributions is parallel transport of covariance matrices from their mean to the identity and then apply the logarithmic mapping ( 1 ). Parallel transport along a curve allows to move SPD matrices from one point on the curve to another point on the curve while keeping the inner product between the logarithmic mappings with any other vector transported along the same curve constant. The following lemma gives the parallel transport of $\\Sigma^{\\prime}$ from $\\Sigma$ to $\\pmb{I}_{d}$ along the geodesic between these two points (See proof in Appendix A.2 ). ", "page_idx": 3}, {"type": "text", "text": "Lemma 2.1 (Parallel transport to the identity). Given $\\Sigma,\\Sigma^{\\prime}\\in\\mathbb{S}_{d}^{++}$ , the parallel transport of $\\Sigma^{\\prime}$ along the geodesic from $\\Sigma$ to the identity $\\pmb{I}_{d}$ at $\\alpha\\in[0,1]$ is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{PT}\\left(\\Sigma^{\\prime},\\Sigma,\\alpha\\right)\\triangleq\\Sigma^{-\\alpha/2}\\Sigma^{\\prime}\\Sigma^{-\\alpha/2}\\ .\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Learning on $\\mathbb{S}_{d}^{++}$ The logarithmic mapping ( 1 ) at the identity is simply the matrix logarithm. Thus, a classical non-linear feature extraction [ 6 , 36 , 8 ] of a dataset $\\{\\pmb{\\Sigma}_{i}\\}_{i=1}^{N}$ of Riemannian mean $\\overline{\\Sigma}$ combines parallel transport and logarithmic mapping at the identity, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\phi\\left(\\Sigma_{i},\\overline{{\\Sigma}}\\right)\\triangleq\\operatorname{uvec}\\left(\\log_{I_{d}}\\left(\\operatorname{PT}\\left(\\Sigma_{i},\\overline{{\\Sigma}},1\\right)\\right)\\right)=\\operatorname{uvec}\\left(\\log\\left(\\overline{{\\Sigma}}^{-1/2}\\Sigma_{i}\\overline{{\\Sigma}}^{-1/2}\\right)\\right)\\in\\mathbb{R}^{d(d+1)/2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where uvec vectorizes the upper triangular part with off-diagonal elements multiplied by $\\sqrt{2}$ to preserve the norm. Correcting dataset shifts by re-centering all source datasets [ 62 ], corresponds to parallel transporting data $\\{\\bar{\\Sigma_{k,i}}\\}_{i=1}^{N_{k}}$ of each domain $k\\in[1,K]$ from its Riemannian mean $\\overline{{\\Sigma}}_{k}$ to the identity, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\phi(\\Sigma_{k,i},\\overline{{\\Sigma}}_{k})=\\operatorname{uvec}\\left(\\log\\left(\\overline{{\\Sigma}}_{k}^{-1/2}\\Sigma_{k,i}\\overline{{\\Sigma}}_{k}^{-1/2}\\right)\\right)\\ .\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This method is the go-to approach for reducing shifts of the covariance matrix distributions coming from different domains and has been applied successfully for brain-computer interfaces [  45 , 59 ] and age prediction from M/EEG data [ 36 ]. ", "page_idx": 3}, {"type": "text", "text": "3 Learning to recenter from highly shifted $y$ distributions with GOPSA ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we develop a novel multi-source domain adaptation method, called Geodesic Optimization for Predictive Shift Adaptation (GOPSA), that operates on the $\\mathbb{S}_{d}^{++}$ manifold and is capable of handling vastly different distributions of $y$ . Our approach implements a Riemannian mixed-effects model, which consists of two components: a single parameter estimating a geodesic intercept specific to each domain and a set of parameters shared across domains. ", "page_idx": 3}, {"type": "text", "text": "At train-time, GOPSA jointly learns the parallel transport of each of the $K$ source domains and the regression model shared across domains. At test-time, we assume having access to the target mean response value $\\bar{y}\\tau$ and predict on the unlabeled target domain of covariance matrices $(\\bar{\\Sigma}_{\\mathcal{T},i})_{i=1}^{N_{\\mathcal{T}}}$ . GOPSA focuses solely on learning the parallel transport of the target domain so that the mean prediction, using the regression model learned at train-time, matches $\\bar{y}\\tau$ . ", "page_idx": 3}, {"type": "text", "text": "Parallel transport along the geodesic In Section 2 , we presented how domain adaptation is performed on $\\bar{\\mathrm{S}_{d}^{++}}$ . In particular, ( 3 ) presents how to account for data shifts of each domain. However, this operator can only work if the variability between domains is considered as noise. As explained earlier, we are interested in shifts in both features and the response variable. Thus, ( 3 ) discards shift coming from the response variable and hence harms the performance of the predictive model. Based on the Lemma 2.1 , we propose to parallel transport features to any point on the geodesic between a domain-specific Riemannian mean $\\overline{{\\Sigma}}_{k}$ and the identity. Indeed, GOPSA parallel transports $\\pmb{\\Sigma}_{k,i}$ on this geodesic with $\\alpha\\in[0,1]$ and then applies the Riemannian logarithmic mapping ( 1 ) at the identity, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\phi(\\Sigma_{k,i},\\overline{{\\Sigma}}_{k},\\alpha)\\triangleq\\operatorname{uvec}\\left(\\log_{I_{d}}\\left(\\operatorname{PT}\\left(\\Sigma_{k,i},\\overline{{\\Sigma}}_{k},\\alpha\\right)\\right)\\right)=\\operatorname{uvec}\\left(\\log\\left(\\overline{{\\Sigma}}_{k}^{-\\alpha/2}\\Sigma_{k,i}\\overline{{\\Sigma}}_{k}^{-\\alpha/2}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This allows each domain to undergo parallel transport to a certain degree, effectively moving it toward the identity. ", "page_idx": 3}, {"type": "table", "img_path": "qTypwXvNJa/tmp/c08c31264e5638776cbaf42a87af5d6a57e0d7a1cd20d72c0bbf82daa1cdbc33.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.1 Train-time ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "GOPSA aims to learn simultaneously features from ( 4 ) and a regression model. To do so, we solve the following optimization problem ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{minimize}_{\\boldsymbol{\\beta}_{S}\\in\\mathbb{R}^{d(d+1)}/2}\\sum_{k=1}^{K}\\sum_{i=1}^{N_{k}}\\left(\\boldsymbol{y}_{k,i}-\\boldsymbol{\\beta}_{S}^{\\top}\\boldsymbol{\\phi}\\left(\\Sigma_{k,i},\\overline{{\\Sigma}}_{k},\\alpha_{k}\\right)\\right)^{2}}\\\\ &{\\displaystyle\\alpha_{S}\\!\\in\\![0,1]^{K}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with $\\pmb{\\alpha}_{S}=[\\alpha_{1},\\dots,\\alpha_{K}]^{\\top}$ . This cost function is decomposed into three key aspects. First, covariance matrices undergo parallel transported using Lemma 2.1 to account for shifts between domains. Second, they are vectorized, and a linear regression predicts the output variable from these vectorized features. Third, the coefficients of the linear regression $\\beta_{S}$ and the $\\alpha_{S}$ are learned jointly so that the predictor is adapted to the parallel transport and reciprocally. Besides, to enforce the constraint on $\\alpha_{S}$ , we re-parameterize it using the sigmoid function, which defines a bijection between $\\mathbb{R}$ and $(0,1)$ , thereby ensuring that the resulting $\\alpha_{S}$ values lie within the desired range: $\\alpha_{k}=\\sigma(\\gamma_{k})\\triangleq(1+\\exp(-\\gamma_{k}))^{-1}$ . Thus, the constrained problem ( 5 ) can be formulated as the following unconstrained optimization problem ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\underset{\\beta_{S}\\in\\mathbb{R}^{d(d+1)/2}}{\\mathrm{minimize}}\\sum_{k=1}^{K}\\sum_{i=1}^{N_{k}}\\left(y_{k,i}-\\beta_{S}^{\\top}\\phi\\left(\\Sigma_{k,i},\\overline{{\\Sigma}}_{k},\\sigma(\\gamma_{k})\\right)\\right)^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with $\\gamma_{S}=\\left[\\gamma_{1},\\ldots,\\gamma_{K}\\right]^{\\top}$ . Let us define the matrix $Z_{S}(\\gamma)\\in\\mathbb{R}^{N_{S}\\times d(d+1)/2}$ , with $\\begin{array}{r}{N_{S}=\\sum_{k=1}^{K}N_{k}}\\end{array}$ as the concatenation of the source data, where each row corresponds to a feature vector: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{Z}_{\\mathcal{S}}(\\gamma)=\\left[\\phi\\left(\\pmb{\\Sigma}_{1,1},\\overline{{\\Sigma}}_{1},\\sigma(\\gamma_{1})\\right),\\dots,\\phi\\left(\\pmb{\\Sigma}_{K,N_{K}},\\overline{{\\Sigma}}_{K},\\sigma(\\gamma_{K})\\right)\\right]^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In the same manner, the source labels are concatenated to $\\pmb{{y}}_{S}=[y_{1,1},\\dots,y_{K,N_{K}}]^{\\top}\\in\\mathbb{R}^{N_{S}}$ . Given a fixed $\\gamma_{S}$ , the problem ( 6 ) is solved with the ordinary least squares estimator. In practice, we choose to regularize the estimation of the linear regression with a Ridge penalty. Thus, ( 6 ) is rewritten as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{S}^{\\star}\\triangleq\\underset{\\gamma\\in\\mathbb{R}^{K}}{\\arg\\operatorname*{min}}\\left\\lbrace\\mathcal{L}_{S}(\\gamma)\\triangleq\\|y_{S}-Z_{S}(\\gamma)\\beta_{S}^{\\star}(\\gamma)\\|_{2}^{2}\\right\\rbrace}\\\\ &{\\qquad\\quad\\mathrm{subject}\\,\\,\\omega\\quad\\beta_{S}^{\\star}(\\gamma)\\triangleq Z_{S}(\\gamma)^{\\top}(\\lambda I_{N}+Z_{S}(\\gamma)Z_{S}(\\gamma)^{\\top})^{-1}y_{S}\\,\\,\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\beta_{S}^{\\star}(\\gamma)\\in\\mathbb{R}^{d(d+1)/2}$ are the Ridge estimated coefficients given a fixed $\\gamma$ and $\\lambda>0$ is the regularization hyperparameter. The problem ( 8 ) is efficiently solved with any gradient-based solver [ 39 ]. ", "page_idx": 4}, {"type": "text", "text": "The train-time of GOPSA is summarized in Algorithm 1 . The proposed training algorithm begins by calculating the Riemannian mean of covariance matrices for each domain $k$ . It then iteratively optimizes the parameters $\\gamma_{S}$ by computing the feature matrix ( 7 ), determining Ridge regression coefficients ( 8 ), and updating $\\gamma_{S}$ using a gradient descent step on the loss function ( 8 ) until convergence. The output result is the optimized Ridge regression coefficients. For clarity of presentation, ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 employs a gradient descent. In practice, we use L-BFGS and obtain the gradient using automatic differentiation through the Ridge solution that is plugged into the loss in ( 8 ). ", "page_idx": 5}, {"type": "text", "text": "3.2 Test-time ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "At test-time, we now have a ftited linear model on source data with coefficients $\\beta_{S}^{\\star}(\\gamma_{S}^{\\star})$ . The goal is to adapt a new target domain $(\\Sigma_{\\mathcal{T},i})_{i=1}^{N_{\\mathcal{T}}}$ for which the average outcome $\\bar{y}\\tau$ is assumed to be known. First, let us define the matrix $Z_{\\mathcal{T}}(\\gamma)\\in\\mathbb{R}^{N_{\\mathcal{T}}\\times d(d+1)/2}$ as the concatenation of the target data ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{Z}_{T}(\\gamma)=\\left[\\phi\\left(\\pmb{\\Sigma}_{T,1},\\overline{{\\pmb{\\Sigma}}}_{T},\\sigma(\\gamma)\\right),\\pmb{\\Sigma}\\cdot\\pmb{\\Sigma}\\cdot\\mathbf{\\Sigma},\\phi\\left(\\pmb{\\Sigma}_{T,N_{T}},\\overline{{\\pmb{\\Sigma}}}_{T},\\sigma(\\gamma)\\right)\\right]^{\\top}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then, GOPSA adapts to this new target domain by minimizing the error between $\\bar{y}\\tau$ and its estimation computed with the fitted linear model. This minimization is performed with respect to $\\gamma_{T}\\in\\mathbb{R}$ that parametrizes the parallel transport of the target domain, i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\gamma_{\\mathcal{T}}^{\\star}=\\underset{\\gamma\\in\\mathbb{R}}{\\arg\\operatorname*{min}}\\,\\left\\lbrace\\mathcal{L}_{\\mathcal{T}}(\\gamma)\\triangleq\\left(\\bar{y}_{\\mathcal{T}}-\\frac{1}{N_{\\mathcal{T}}}\\mathbf{1}_{N_{\\mathcal{T}}}^{\\top}\\pmb{Z}_{\\mathcal{T}}(\\gamma)\\beta_{S}^{\\star}(\\gamma_{S}^{\\star})\\right)^{2}\\right\\rbrace\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Finally, the predictions on the target domain are ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{\\pmb{y}}_{T}=\\pmb{Z}_{T}(\\gamma_{T}^{\\star})\\beta_{S}^{\\star}(\\gamma_{S}^{\\star})\\in\\mathbb{R}^{N_{T}}\\ .\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The test-time procedure of GOPSA is summarized in Algorithm 2 . The algorithm begins by calculating the Riemannian mean of the target covariance matrices {\u03a3T ,i}iN=T1 . It then iteratively optimizes the parameter $\\gamma_{T}$ by computing the feature matrix ( 9 ), the derivative of the loss function ( 10 ), and updating $\\gamma_{T}$ using a gradient descent step until convergence. The algorithm determines the estimated target outcomes, ${\\widehat{\\pmb{y}}}\\tau$ , by using the optimized $\\gamma_{T}^{\\star}$ on the feature matrix, combined with the pre-trained regression coefficients $\\beta_{S}^{\\star}(\\gamma_{S}^{\\star})$ . The output result is the predicted target outcomes ${\\widehat{\\pmb{y}}}\\tau$ . It should be noted that, once again, for clarity of presentation, Algorithm 2 employs a gradient descent, but other derivative-based optimization methods can be used. ", "page_idx": 5}, {"type": "text", "text": "4 Empirical benchmarks ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we built empirical benchmark to evaluate the performance of GOPSA. We first present the simulated data that we used to illustrate the relevance of our method when there is a joint distribution shift of the data and the labels. Then, we present the EEG dataset that we used to evaluate the performance of GOPSA with real data from different recording sites. Finally, we present the baseline methods that are compared with GOPSA. ", "page_idx": 5}, {"type": "text", "text": "Simulated data To generate simulated data, we used the generative model described in [ 47 , 48 , 36 ]. The data follow the classical instantaneous mixing model: ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\pmb x}_{i}(t)={\\pmb A}{\\pmb s}_{i}(t)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{x}_{i}(t)\\in\\mathbb{R}^{d}$ are the observed time-series, $\\pmb{s}_{i}(t)\\in\\mathbb{R}^{d}$ are the underlying signal of the neural generators and $\\pmb{A}$ is the mixing matrix whose columns are the observed spatial patterns of the neural generators. Furthermore, we assume that $y$ follow a log-linear model: ", "page_idx": 5}, {"type": "equation", "text": "$$\ny_{i}=\\beta_{0}+\\sum_{\\ell=1}^{d}\\beta_{\\ell}\\log(p_{\\ell i})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $p_{\\ell i}~>~0$ is the variance of the $\\ell$ -th element of the underlying signal ${\\pmb s}_{i}(t)$ as introduced in [ 47 , 48 , 36 ]. From this, we generate domains (source and target) by applying shifts on $X$ and $y$ . To do so, we introduced a per-domain shift in the data distribution by applying an affine transformation to the covariance matrices $\\pmb{\\Sigma}_{i}\\triangleq\\mathbb{E}[\\pmb{x}_{i}(t)\\pmb{x}_{i}(t)^{\\top}]$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{\\Sigma}_{i}\\mapsto\\pmb{B}_{k}^{\\xi}\\pmb{\\Sigma}_{i}\\pmb{B}_{k}^{\\xi}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with $B_{k}\\,\\in\\,\\mathbb{S}_{d}^{++}$ and $\\xi\\:\\geq\\:0$ controlling the amplitude of the shift. Then, we shifted the label distribution by modifying the variance of the underlying signal $p_{\\ell i}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\np_{\\ell i}\\mapsto p_{\\ell i}^{1+k\\xi}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with $\\xi\\:\\geq\\:0$ still controlling the amplitude of the shift. Thus, the distribution of $y$ is shifted per domain because of the log-linear relationship of ( 13 ). It should be noted that $\\beta$ is kept constant across domains. ", "page_idx": 6}, {"type": "text", "text": "HarMNqEEG dataset The HarMNqEEG dataset [  33 ] was used for our numerical experiments. This dataset includes EEG recordings collected from 1564 participants across 14 different study sites, distributed across 9 countries. In our analysis, we consider each study site as a distinct domain. Appendix A.5 provides detailed demographic information. The EEG data were recorded with the same montage of 19 channels of the 10/20 International Electrodes Positioning System. The dataset provides pre-computed cross-spectral tensors for each participant rather than raw data, and anonymized metadata including the age and the sex of the participants. More precisely, the shared data consists of cross-spectral matrices with a frequency range of $1.17\\,\\mathrm{Hz}$ to $19.14\\,\\mathrm{Hz}$ , sampled at a resolution of $0.39\\,\\mathrm{Hz}$ . A standardized recording protocol was enforced to ensure the consistency across EEG recording of the dataset. In addition to recording constraints, this protocol included artifact cleaning procedures. The cross-spectrum were computed using Bartlett\u2019s method (See Appendix A.3 ). Our pre-processing steps were guided by the pre-processing pipeline outlined in [ 33 ]. First, we performed a common average reference (CAR) on all cross-spectrum (See Appendix A.3  ) as different EEG references were used across domains. Subsequently, we extracted the real part of the cross-spectral tensor to obtain co-spectrum tensors containing frequency-specific covariance estimates along the frequency spectrum. Due to the linear dependence between channels introduced by the CAR, the covariance matrices are rank deficient. To address this, we applied a shrinkage regularization with a coefficient of $10^{-5}$ to the data. Additionally, we implemented a global-scale factor (GSF) correction, which compensates for amplitude variations between EEG recordings by scaling the covariance matrices with a subject-specific factor [ 24 , 33 ] (See Appendix A.3  ). Following these pre-processing steps, we obtained a set of 49 covariance matrices for each EEG recording, with each matrix corresponding to a specific frequency bin of the EEG signal. This pre-processed co-spectrum served as the input data for our domain adaptation study. ", "page_idx": 6}, {"type": "text", "text": "Performance evaluation and hyperparameter selection To evaluate the performance of the compared methods, we conducted experiments across several combinations of source and target sites. We selected source domains such that the union distribution of their predictive variable $y$ encompasses a broad age range. All remaining sites were assigned as target domains. For each source-target combination we performed a stratified shuffle split approach with 100 repetitions on the target data. Stratification was based on the recording sites to ensure that each split contained a balanced proportion of participants from each site. The regularization parameter $\\lambda$ in Ridge regression was selected with a nested cross-validation (grid search) over a logarithmic grid of values from $10^{-1}$ to $10^{5}$ . To evaluate the benefit of GOPSA, we compared it against four baselines. Detailed mathematical formulations of these baselines can be found in Appendix A.4 . For each baseline method, the regression task was performed with Ridge regression. ", "page_idx": 6}, {"type": "text", "text": "Domain-aware dummy model (DO Dummy) As GOPSA requires access to the mean $\\bar{y}_{k}$ of each domain, we used a domain-aware dummy model predicting always the mean $\\bar{y}_{k}$ of each domain. ", "page_idx": 6}, {"type": "text", "text": "No re-center / No domain adaptation (No DA) This second baseline method involves applying the regression pipeline outlined in [ 47 , 48 ] without any re-centering. In this setup, all covariance matrices are projected to the tangent space at the source geometric mean \u03a3 computed from all source points, no matter their recording sites. ", "page_idx": 6}, {"type": "text", "text": "Re-center to a common reference point (Re-center and Re-scale) As introduced in Section 2 , a common transfer learning approach is a Riemannian re-centering of all domains to a common point on the manifold [ 62 , 36 ]. This baseline thus correspond to re-centering each domain $k$ , source and target, independently by whitening them by their respective geometric mean $\\overline{{\\Sigma}}_{k}$ . An extension of this approach is to perform a Riemannian re-scaling of all domains to a common dispersion, as presented in [ 45 , 36 ]. ", "page_idx": 6}, {"type": "text", "text": "Domain-aware intercept (DO Intercept) This method consists in fitting one intercept $\\beta_{0}$ per domain. In practice since we assume to know $\\bar{y}\\tau$ , we correct the predicted values so that their mean is equal to ${\\bar{y}}\\tau$ . This approach is in line with defining mixed-effects models on the Riemannian manifold [ 33 ]. ", "page_idx": 6}, {"type": "text", "text": "Deep learning (GREEN) The GREEN model [  40 ] is a deep-learning architecture tailored for EEG applications like age prediction. Since the HarMNqEEG dataset consists of covariance matrices, we used the $\\mathbf{\\omega}^{\\bullet}\\mathbf{G}2^{\\bullet}$ variant of GREEN, which starts at the covariance matrices level and includes pooling layers. This variant is designed for SPD matrices, making it an SPD network [ 26 ]. Although GREEN has been evaluated on multiple datasets for various predictive tasks, it has not yet been applied in a domain adaptation context and does not include an adaptation layer. ", "page_idx": 6}, {"type": "image", "img_path": "qTypwXvNJa/tmp/c69a13904b15e36f2cfa63a2772c2724cd1e1bd20ce0a7fcf7af8d09bf8bfaca.jpg", "img_caption": ["Figure 2: $R^{2}$ scores $\\uparrow$ for different methods on simulated data. Performance is measured across 5 source domains and 1 target domain, with shifts controlled by $\\xi$ (0 to maximum). Data are generated 100 times, with 5 sensors and 300 covariance matrices per domain. The target domain is randomly selected between the 6 domains generated as presented in Section 4 , with the remaining domains used as sources. (A) A shift is applied on the covariance matrices following ( 14 ). (B) A shift is applied on the variances following ( 15 ). (C) Both shifts from ( 14 ) and ( 15 ) are applied simultaneously. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We applied the domain-adaptation methods independently to each of the 49 frequency bins, resulting in 49 geometric means per domain, except for GREEN, which processes all frequency bands simultaneously. $\\bar{y}_{T}$ of each domain was estimated on target splits ( $50\\%$ of the data) that do not overlap with the evaluation target splits ( $50\\%$ remaining). Statistical inference for model comparisons was implemented with a corrected t-test following [ 37 ]. Experiments with 100 repetitions and all site combinations have been run on a standard Slurm cluster for 12 hours with 250 CPU cores. ", "page_idx": 7}, {"type": "text", "text": "5 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Simulated data  Figure 2 presents the results of simulated experiments where shifts are applied on either $X$ , $y$ , or both $(X,y)$ as presented in Section 4 . All methods were evaluated in three simulation scenarios: shift in $X$ only, shift in $y$ only, and joint shift in $X$ and $y$ . The intensity of the shift was controlled by $\\xi$ in all scenarios. If there is no shift in $X$ , we observe that No DA perfectly estimates the $y$ because the log-linear model is easily estimated across domains even when the $y$ distribution changes (  Figure $^{2\\textrm{B}}$ ). The performance of No DA however drops when a shift in $X$ is introduced ( Figure $2\\,\\mathrm A$ and C). Re-center and Re-scale led to the same results as no scaling shift was applied in the simulation. Both were able to correct the shift in $X$ , but performed poorly when a shift in $y$ was added ( Figure $^{2\\textrm{B}}$ and C). GREEN notably showed consistant performance across all scenarios, and was relatively resistant to both types of shifts given it is not designed for domain adaptation. DO Intercept and GOPSA showed the best performance across all scenarios, with an advantage for GOPSA. The interest of GOPSA is to estimate this log-linear model with shifts in $(X,y)$ per domain ( Figure $2\\,\\mathrm{C})$ which other methods were not able to do. These experiments demonstrate the efficiency of the proposed method in estimating shifts in $X$ between domains even in the presence of a shift in $y$ , contrary to the baseline methods. Theoretically, based on the generative model of the simulated data, the data $X$ and outcome $y$ are linked by a log-linear relationship. This implies that, knowing the shift in $X$ for the target domain, predictions can be made even when $y$ distributions do not overlap between the source and target. Since GOPSA estimates the target shift in $X$ by minimizing $(\\bar{y}-\\mathrm{mean}(\\hat{y}_{i}))^{2}$ , it is capable of handling such scenarios. ", "page_idx": 7}, {"type": "text", "text": "HarMNqEEG data We computed benchmarks for five combinations of source sites and we displayed the results for the three metrics selected for performance evaluation, each colored box representing one method ( Figure 3 ). A min-max normalization was applied to each site combinations separately across methods. We first conducted model comparisons in terms of absolute performance across all baselines (A). No DA, without domain specific re-centering, performed worse than DO Dummy in terms of $R^{2}$ score and MAE. Re-center and Re-scale led to lower performances across all metrics, which can be expected as the Riemannian mean is correlated with age in our problem setting Figure 1  . Eventhough its architecture does not include an adaptation layer, GREEN reached better performance than the previous methods mentionned, but lacked consistency across site combinations and metrics with large variance especially for the $R^{2}$ score and MAE. For all scores, DO Intercept and GOPSA reached the best average performance with lower variance. A version of Figure $3\\textbf{A}$ without normalization is presented in Appendix A.7  . As DO intercept and GOPSA showed overlapping performance distributions, we investigated their paired split-wise (nonrescaled) score differences $({\\bf{B}})$ . The site-specific differences of GOPSA scores minus DO Intercept are displayed with their associated p-values. For one site combination $(\\mathrm{Ba},\\mathrm{Be},\\mathrm{Cho},\\mathrm{Co},\\mathrm{Cu}90,\\mathrm{G},\\mathrm{R})$ , DO Intercept yielded higher $R^{2}$ scores, and no significant difference was found between the two methods for Ba,Co,G. Similarly, no significant difference was observed on Spearman\u2019s $\\rho$ results for Cu03,M,R,S. Overall, GOPSA significantly outperformed DO Intercept in five site combinations for MAE, four for Spearman\u2019s $\\rho$ and three for $R^{2}$ score. Detailed results for each source-target combination are presented in Appendix A.6 for Spearman\u2019s $\\rho$ , $R^{2}$ score, and MAE. The bottom rows correspond to the mean performance of each method of all site combinations, and their average standard deviation (see Appendix A.8 for associated boxplots). We expected GOPSA to outperform the baseline methods (e.g. DO Intercept) whenever joint $(X,y)$ shifts occur. In our experimental benchmark, GOPSA significantly outperformed the baseline methods in some site combinations, but not all. This allows us to assume that not all site combinations show joint shifts. ", "page_idx": 7}, {"type": "image", "img_path": "qTypwXvNJa/tmp/2da767d8bf68baa8aacf601ad85d98d09872f165cdde3eeff7509836761af1b2.jpg", "img_caption": ["Figure 3: Normalized performance of the different methods on several source-target combinations for three metrics: Spearman\u2019s $\\rho\\uparrow$ (left), $R^{2}$ score $\\uparrow$ (middle) and Mean Absolute Error $\\downarrow$ (right). As a large variability in the score values was present between the site combinations, we applied a min-max normalization per combination to set the minimum score across all methods to 0 and the maximum score to 1. (A) Boxplot of the concatenated results for the three normalized scores. One point corresponds to one split of one site combination. $({\\bf{B}})$ Boxplots of the difference between the normalized scores of GOPSA and DO Intercept. A row corresponds to one site combination, one point corresponds to one split. For each plot, the associated results of Nadeau\u2019s & Bengio\u2019s corrected t-test [ 37 ] are displayed. A p-value lower than 0.05 indicates a significant difference between the two methods. Ba: Barbados, Be: Bern, Chb: CHBMP (Cuba), Co: Columbia, Cho: Chongqing, $\\mathrm{Cu}03$ : Cuba2003, Cu90: Cuba90, G: Germany, M: Malaysia, R: Russia, S: Switzerland "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Model inspection Next, we investigated the impact of the different re-centering approaches on the data Figure 4 . Power spectrum densities (PSDs) were computed as the mean across sensors of the diagonals of the covariance matrices Riemannian mean for each site combination after No DA, Re-center and GOPSA (A). PSDs for No DA display the initial variability between sites without recentering (cf. Figure 1 ). Re-center resulted in flat PSDs because all data were re-centered to the identity. PSDs produced by GOPSA are flattened and more similar across sites compared to No DA without removing too much information, unlike the un-effective Re-center method (cf. Figure 3 ). The alpha values are inspected as a function of the site mean age (B). Re-center leads to alpha values all equal to one as all sites are re-centered to the identity. For GOPSA, we observed a linear relationship between alpha and the sites\u2019 mean age $/R^{2}=0.99)$ . This is a direct consequence of the optimization process in GOPSA, which thus can be regarded a geodesic intercept in a mixed-effects model. Overall, GOPSA effectively re-centered sites with younger participants closer to the identity matrix. Re-centering sites around a common point helped reduce the shift in $X$ , while not placing all sites at the exact same reference point helped manage the shift in $y$ , hence preserving the statistical associations between $X$ and $y$ . ", "page_idx": 8}, {"type": "image", "img_path": "qTypwXvNJa/tmp/66072eddc73b4ff340c6f57c63427d2ee0c5d011b32b84243c5b137f50ef9548.jpg", "img_caption": ["Figure 4: Model inspection of GOPSA versus No DA and Re-center. Power Spectral Densities (PSDs) and $\\alpha$ values were computed on the source sites Barbados, Chongqing, Germany, and Switzerland. The remaining sites were used as target domains. (A) Mean PSDs computed across sensors for No DA, Recenter and GOPSA on two source (Barbados and Switzerland) and two target (New York and Columbia) sites. (B) $\\alpha$ values versus site\u2019s mean age for Re-center and GOPSA. One point corresponds to one site. The coefficient of determination is reported for the GOPSA method. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We proposed a novel multi-source domain adaptation approach that adapts shifts in $X$ and $y$ simultaneously by learning jointly a domain specific re-centering operator and the regression model. GOPSA was specifically developed to handle joint shifts in the data distribution and the outcome distribution, as illustrated by the simulations in Figure 2 . GOPSA is a test-time method that does not require to retrain a model when a new domain is presented. GOPSA achieved state-of-the-art performance on the HarMNqEEG [ 33 ] dataset with EEG from 14 recording sites and over 1500 participants. Our benchmarks showed a significant gain in performance for three different metrics in a majority of site combinations compared to baseline methods. GOPSA can thus be used by researchers as a decision rule to infer the presence of joint shifts and, hence, serve as a tool for data exploration and model interpretation. While we focused on shallow regression models, the implementation of GOPSA using PyTorch readily supports its inclusion in more complex Riemannian deep learning models [ 26 , 56 , 11 , 40 , 31 ]. This direction seems promising given our observation that GREEN \u2013 a simple deep net combining Riemannian computation with a fully connected layer - already possessed some intrinsic robustness to data shifts. This may point at the capacity of the fully-connected layer to provide additional non-linear transformations that can accommodate the data-generating scenario in which continuous log-linear generators are modified in a discrete manner by site factors. More generally, it emphasizes the potential of complex nonlinear methods for domain adaptation, in line with a recent study on the same dataset reporting positive generalization results using a kernel method [ 28 ]. Furthermore, although this work specifically addresses age prediction, the methodology is applicable to a broader range of regression analyses. While GOPSA necessitates knowledge or estimability of the average $\\bar{y}$ per domain, this requirement aligns with that of mixed-effects models [ 16 , 25 , 61 ], which are extensively employed in biomedical statistics. By combining mixed-effects modeling with Riemannian geometry for EEG, GOPSA opens up various applications at the interface between machine learning and biostatistics, such as, biomarker exploration in large multicenter clinical trials [ 46 , 52 , 53 ]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by grants ANR-20-CHIA-0016 and ANR-20-IADJ-0002 to AG while at Inria, ANR-20-THIA-0013 to AM, ANR-22-CE33-0015-01 and ANR-17-CONV-0003 operated by LISN to SC and ANR-22-PESN-0012 to AC under the France 2030 program, all managed by the Agence Nationale de la Recherche (ANR) Competing interests: DE is a full-time employee of F. Hoffmann-La Roche Ltd. ", "page_idx": 10}, {"type": "text", "text": "Numerical computation was enabled by the scientific Python ecosystem: Matplotlib [ 27 ], Scikitlearn [ 42 ], Numpy [ 21 ], Scipy [ 54 ], PyTorch [ 41 ] PyRiemann [ 3 ], MNE [ 19 ] and SKADA [ 18 ]. ", "page_idx": 10}, {"type": "text", "text": "This work was conducted at Inria, AG is presently employed by Meta Platforms. All the datasets used for this work were accessed and processed on the Inria compute infrastructure. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Benyamin Allahgholizadeh Haghi, Spencer Kellis, Sahil Shah, Maitreyi Ashok, Luke Bashford, Daniel Kramer, Brian Lee, Charles Liu, Richard Andersen, and Azita Emami. Deep multi-state dynamic recurrent neural networks operating on wavelet based neural features for robust brain machine interfaces. Advances in Neural Information Processing Systems, 32, 2019. [2] Gopala K. Anumanchipalli, Josh Chartier, and Edward F. Chang. Speech synthesis from neural decoding of spoken sentences. Nature, 568(7753):493\u2013498, 2019. [3] Alexandre Barachant, Quentin Barth\u00e9lemy, Jean-R\u00e9mi King, Alexandre Gramfort, Sylvain Chevallier, Pedro L. C. Rodrigues, Emanuele Olivetti, Vladislav Goncharenko, Gabriel Wagner vom Berg, Ghiles Reguig, Arthur Lebeurrier, Erik Bj\u00e4reholt, Maria Sayu Yamamoto, Pierre Clisson, and Marie-Constance Corsi. pyriemann/pyriemann: v0.3, July 2022.   \n[4] Alexandre Barachant, St\u00e9phane Bonnet, Marco Congedo, and Christian Jutten. Classification of covariance matrices using a riemannian-based kernel for BCI applications. Neurocomputing, 112:172\u2013178, 2013. [5] Alexandre Barachant, Stphane Bonnet, Marco Congedo, and Christian Jutten. Common spatial pattern revisited by riemannian geometry. In 2010 IEEE international workshop on multimedia signal processing, pages 472\u2013476. IEEE, 2010.   \n[6] Alexandre Barachant, St\u00e9phane Bonnet, Marco Congedo, and Christian Jutten. Multiclass Brain\u2013Computer Interface Classification by Riemannian Geometry. IEEE Transactions on Biomedical Engineering, 59(4):920\u2013928, 2012. [7] Philipp Bomatter, Joseph Paillard, Pilar Garces, J\u00f6rg Hipp, and Denis-Alexander Engemann. Machine learning of brain-specific biomarkers from EEG. Ebiomedicine, 106, 2024. [8] Cl\u00e9ment Bonet, Beno\u00eet Mal\u00e9zieux, Alain Rakotomamonjy, Lucas Drumetz, Thomas Moreau, Matthieu Kowalski, and Nicolas Courty. Sliced-Wasserstein on Symmetric Positive Definite Matrices for M/EEG Signals. In Proceedings of the 40th International Conference on Machine Learning, pages 2777\u20132805. PMLR, July 2023. [9] Nicolas Boumal. An introduction to optimization on smooth manifolds. Cambridge University Press, 2023.   \n[10] Gy\u00f6rgy Buzs\u00e1ki and Kenji Mizuseki. The log-dynamic brain: how skewed distributions affect network operations. Nature Reviews Neuroscience, 15(4):264\u2013278, 2014.   \n[11] Igor Carrara, Bruno Aristimunha, Marie-Constance Corsi, Raphael Y de Camargo, Sylvain Chevallier, and Th\u00e9odore Papadopoulo. Geometric neural network based on phase space for BCI decoding. arXiv preprint arXiv:2403.05645, 2024.   \n[12] J\u00e9r\u00f4me Dock\u00e8s, Ga\u00ebl Varoquaux, and Jean-Baptiste Poline. Preventing dataset shift from breaking machine-learning biomarkers. GigaScience, 10(9):giab055, 2021.   \n[13] Denis A. Engemann, Apolline Mellot, Richard H\u00f6chenberger, Hubert Banville, David Sabbagh, Lukas Gemein, Tonio Ball, and Alexandre Gramfort. A reusable benchmark of brain-age prediction from M/EEG resting-state signals. NeuroImage, 262:119521, November 2022.   \n[14] Denis A Engemann, Federico Raimondo, Jean-R\u00e9mi King, Benjamin Rohaut, Gilles Louppe, Fr\u00e9d\u00e9ric Faugeras, Jitka Annen, Helena Cassol, Olivia Gosseries, Diego Fernandez-Slezak, et al. Robust EEG-based cross-site and cross-protocol classification of states of consciousness. Brain, 141(11):3179\u20133192, 2018.   \n[15] Dylan Forenzo, Hao Zhu, Jenn Shanahan, Jaehyun Lim, and Bin He. Continuous tracking using deep learning-based decoding for noninvasive brain-computer interface. PNAS nexus, 3(4):pgae145, 2024.   \n[16] Andrew Gelman. Multilevel (hierarchical) modeling: what it can and cannot do. Technometrics, 48(3):432\u2013435, 2006.   \n[17] Lukas AW Gemein, Robin T Schirrmeister, Patryk Chrab a\u02dbszcz, Daniel Wilson, Joschka Boedecker, Andreas Schulze-Bonhage, Frank Hutter, and Tonio Ball. Machine-learning-based diagnostics of EEG pathology. NeuroImage, 220:117021, 2020.   \n[18] Th\u00e9o Gnassounou, Oleksii Kachaiev, R\u00e9mi Flamary, Antoine Collas, Yanis Lalou, Antoine de Mathelin, Alexandre Gramfort, Ruben Bueno, Florent Michel, Apolline Mellot, Virginie Loison, Ambroise Odonnat, and Thomas Moreau. Skada : Scikit adaptation, 7 2024.   \n[19] Alexandre Gramfort, Martin Luessi, Eric Larson, Denis A. Engemann, Daniel Strohmeier, Christian Brodbeck, Roman Goj, Mainak Jas, Teon Brooks, Lauri Parkkonen, and Matti S. H\u00e4m\u00e4l\u00e4inen. MEG and EEG data analysis with MNE-Python. Frontiers in Neuroscience, 7(267):1\u201313, 2013.   \n[20] Haris Hakeem, Wei Feng, Zhibin Chen, Jiun Choong, Martin J Brodie, Si-Lei Fong, KhengSeang Lim, Junhong Wu, Xuefeng Wang, Nicholas Lawn, et al. Development and validation of a deep learning model for predicting treatment response in patients with newly diagnosed epilepsy. JAMA neurology, 79(10):986\u2013996, 2022.   \n[21] C.R. Harris, K.J. Millman, S.J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor, S. Berg, N.J. Smith, R. Kern, M. Picus, S. Hoyer, M.H. van Kerkwijk, M. Brett, A. Haldane, J. Fern\u00e1ndez del R\u00edo, M. Wiebe, P. Peterson, P. G\u2019erard-Marchant, K. Sheppard, T. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, and T.E. Oliphant. Array programming with NumPy. Nature, 585(7825):357\u2013362, 2020.   \n[22] Huan He, Owen Queen, Teddy Koker, Consuelo Cuevas, Theodoros Tsiligkaridis, and Marinka Zitnik. Domain adaptation for time series under feature and label shifts. In International Conference on Machine Learning, pages 12746\u201312774. PMLR, 2023.   \n[23] Elisabeth R M Heremans, Huy Phan, Pascal Borz\u00e9e, Bertien Buyse, Dries Testelmans, and Maarten De Vos. From unsupervised to semi-supervised adversarial domain adaptation in electroencephalography-based sleep staging. Journal of Neural Engineering, 19(3):036044, jun 2022.   \n[24] JL Hern\u00e1ndez, P Vald\u00e9s, R Biscay, T Virues, S Szava, J Bosch, A Riquenes, and I Clark. A global scale factor in brain topography. International journal of neuroscience, 76(3-4):267\u2013278, 1994.   \n[25] Joop Hox. Multilevel modeling: When and why. In Classification, data analysis, and data highways: proceedings of the 21st Annual Conference of the Gesellschaft f\u00fcr Klassifikation eV, University of Potsdam, March 12\u201314, 1997, pages 147\u2013154. Springer, 1998.   \n[26] Zhiwu Huang and Luc Van Gool. A riemannian network for SPD matrix learning. In Proceedings of the AAAI conference on artificial intelligence, volume 31, 2017.   \n[27] J.D. Hunter. Matplotlib: A 2D graphics environment. Computing in science & engineering, 9(3):90\u201395, 2007.   \n[28] Cecilia Jarne, Ben Griffin, and Diego Vidaurre. Predicting subject traits from brain spectral signatures: an application to brain ageing. bioRxiv, pages 2023\u201311, 2023.   \n[29] Haiteng Jiang, Peiyin Chen, Zhaohong Sun, Chengqian Liang, Rui Xue, Liansheng Zhao, Qiang Wang, Xiaojing Li, Wei Deng, Zhongke Gao, et al. Assisting schizophrenia diagnosis using clinical electroencephalography and interpretable graph neural networks: a real-world and cross-site study. Neuropsychopharmacology, 48(13):1920\u20131930, 2023.   \n[30] Hyunwoo J. Kim, Nagesh Adluru, Heemanshu Suri, Baba C. Vemuri, Sterling C. Johnson, and Vikas Singh. Riemannian nonlinear mixed effects models: Analyzing longitudinal deformations in neuroimaging. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5777\u20135786, 2017.   \n[31] Reinmar Kobler, Jun-ichiro Hirayama, Qibin Zhao, and Motoaki Kawanabe. SPD domainspecific batch normalization to crack interpretable unsupervised domain adaptation in EEG. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 6219\u20136235. Curran Associates, Inc., 2022.   \n[32] Dianqi Li, Yizhe Zhang, Zhe Gan, Yu Cheng, Chris Brockett, Bill Dolan, and Ming-Ting Sun. Domain adaptive text style transfer. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3304\u20133313, Hong Kong, China, November 2019. Association for Computational Linguistics.   \n[33] Min Li, Ying Wang, Carlos Lopez-Naranjo, Shiang Hu, Ronaldo C\u00e9sar Garc\u00eda Reyes, Deirel PazLinares, Ariosky Areces-Gonzalez, Aini Ismafairus Abd Hamid, Alan C Evans, Alexander N Savostyanov, et al. Harmonized-Multinational qEEG norms (HarMNqEEG). NeuroImage, 256:119190, 2022.   \n[34] Yitong Li, Michael Murias, Samantha Major, Geraldine Dawson, and David Carlson. On target shift in adversarial domain adaptation. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 616\u2013625. PMLR, 2019.   \n[35] Carlos Lopez Naranjo, Fuleah Abdul Razzaq, Min Li, Ying Wang, Jorge F Bosch-Bayard, Martin A Lindquist, Anisleidy Gonzalez Mitjans, Ronaldo Garcia, Arielle G Rabinowitz, Simon G Anderson, et al. EEG functional connectivity as a riemannian mediator: An application to malnutrition and cognition. Human Brain Mapping, 45(7):e26698, 2024.   \n[36] Apolline Mellot, Antoine Collas, Pedro LC Rodrigues, Denis Engemann, and Alexandre Gramfort. Harmonizing and aligning M/EEG datasets with covariance-based techniques to enhance predictive regression modeling. Imaging Neuroscience, 1:1\u201323, 2023.   \n[37] Claude Nadeau and Yoshua Bengio. Inference for the generalization error. Advances in neural information processing systems, 12, 1999.   \n[38] Chuong H Nguyen, George K Karavas, and Panagiotis Artemiadis. Inferring imagined speech using EEG signals: a new approach using riemannian manifold features. Journal of neural engineering, 15(1):016002, 2017.   \n[39] Jorge Nocedal and Stephen J Wright. Numerical optimization. Springer, 1999.   \n[40] Joseph Paillard, Joerg F Hipp, and Denis A Engemann. GREEN: a lightweight architecture using learnable wavelets and riemannian geometry for biomarker exploration. bioRxiv, pages 2024\u201305, 2024.   \n[41] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Advances in Neural Information Processing Systems (NeurIPS), pages 8024\u20138035. Curran Associates, Inc., 2019.   \n[42] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine Learning in Python . Journal of Machine Learning Research, 12:2825\u20132830, 2011.   \n[43] Peizhen Peng, Liping Xie, Kanjian Zhang, Jinxia Zhang, Lu Yang, and Haikun Wei. Domain adaptation for epileptic EEG classification using adversarial learning and riemannian manifold. Biomedical Signal Processing and Control, 75:103555, 2022.   \n[44] Xavier Pennec, Pierre Fillard, and Nicholas Ayache. A Riemannian Framework for Tensor Computing. International Journal of Computer Vision, 66(1):41\u201366, January 2006.   \n[45] Pedro Luiz Coelho Rodrigues, Christian Jutten, and Marco Congedo. Riemannian Procrustes Analysis: Transfer Learning for Brain\u2013Computer Interfaces. IEEE Transactions on Biomedical Engineering, 66(8):2390\u20132401, August 2019.   \n[46] Andrea O Rossetti, Kaspar Schindler, Raoul Sutter, Stephan R\u00fcegg, Fr\u00e9d\u00e9ric Zubler, Jan Novy, Mauro Oddo, Loane Warpelin-Decrausaz, and Vincent Alvarez. Continuous vs routine electroencephalogram in critically ill adults with altered consciousness and no recent seizure: a multicenter randomized clinical trial. JAMA neurology, 77(10):1225\u20131232, 2020.   \n[47] David Sabbagh, Pierre Ablin, Ga\u00ebl Varoquaux, Alexandre Gramfort, and Denis A Engemann. Manifold-regression to predict from MEG/EEG brain signals without source modeling. Advances in Neural Information Processing Systems, 32, 2019.   \n[48] David Sabbagh, Pierre Ablin, Ga\u00ebl Varoquaux, Alexandre Gramfort, and Denis A Engemann. Predictive regression modeling with MEG/EEG: from source power to signals and cognitive states. NeuroImage, 222:116893, 2020.   \n[49] Jean-Baptiste Schiratti, St\u00e9phanie Allassonniere, Olivier Colliot, and Stanley Durrleman. Learning spatiotemporal trajectories from manifold-valued longitudinal data. Advances in neural information processing systems, 28, 2015.   \n[50] Jean-Baptiste Schiratti, St\u00e9phanie Allassonni\u00e8re, Olivier Colliot, and Stanley Durrleman. A bayesian mixed-effects model to learn trajectories of changes from repeated manifold-valued observations. Journal of Machine Learning Research, 18(133):1\u201333, 2017.   \n[51] Lene Theil Skovgaard. A Riemannian Geometry of the Multivariate Normal Model. Scandinavian Journal of Statistics, 11(4):211\u2013223, 1984. Publisher: [Board of the Foundation of the Scandinavian Journal of Statistics, Wiley].   \n[52] Paola Vassallo, Jan Novy, Fr\u00e9d\u00e9ric Zubler, Kaspar Schindler, Vincent Alvarez, Stephan R\u00fcegg, and Andrea O Rossetti. EEG spindles integrity in critical care adults. analysis of a randomized trial. Acta Neurologica Scandinavica, 144(6):655\u2013662, 2021.   \n[53] Paul M Vespa, DaiWai M Olson, Sayona John, Kyle S Hobbs, Kapil Gururangan, Kun Nie, Masoom J Desai, Matthew Markert, Josef Parvizi, Thomas $\\mathbf{P}$ Bleck, et al. Evaluating the clinical impact of rapid response electroencephalography: the DECIDE multicenter prospective observational clinical study. Critical care medicine, 48(9):1249\u20131257, 2020.   \n[54] P. Virtanen, R. Gommers, T.E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, S.J. van der Walt, M. Brett, J. Wilson, J.K. Millman, N. Mayorov, A.R.J. Nelson, E. Jones, R. Kern, E. Larson, C.J. Carey, I. Polat, Y. Feng, E.W. Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E.A. Quintero, C.R. Harris, A.M. Archibald, A.H. Ribeiro, F. Pedregosa, P. van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261\u2013272, 2020.   \n[55] Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing, 312:135\u2013153, 2018.   \n[56] Daniel Wilson, Robin Tibor Schirrmeister, Lukas Alexander Wilhelm Gemein, and Tonio Ball. Deep riemannian networks for EEG decoding. arXiv preprint arXiv:2212.10426, 2022.   \n[57] Jonathan R Wolpaw, Dennis J McFarland, Gregory W Neat, and Catherine A Forneris. An EEG-based brain-computer interface for cursor control. Electroencephalography and clinical neurophysiology, 78(3):252\u2013259, 1991.   \n[58] Wei Wu, Yu Zhang, Jing Jiang, Molly V Lucas, Gregory A Fonzo, Camarin E Rolle, Crystal Cooper, Cherise Chin-Fatt, Noralie Krepel, Carena A Cornelssen, et al. An electroencephalographic signature predicts antidepressant response in major depression. Nature biotechnology, 38(4):439\u2013447, 2020.   \n[59] Or Yair, Felix Dietrich, Ronen Talmon, and Ioannis G. Kevrekidis. Domain Adaptation with Optimal Transport on the Manifold of SPD matrices, July 2020. arXiv:1906.00616 [cs, stat].   \n[60] Yuzhe Yang, Yuan Yuan, Guo Zhang, Hao Wang, Ying-Cong Chen, Yingcheng Liu, Christopher G Tarolli, Daniel Crepeau, Jan Bukartyk, Mithri R Junna, et al. Artificial intelligenceenabled detection and assessment of parkinson\u2019s disease using nocturnal breathing signals. Nature medicine, 28(10):2207\u20132215, 2022.   \n[61] Tal Yarkoni. The generalizability crisis. Behavioral and Brain Sciences, 45:e1, 2022.   \n[62] Paolo Zanini, Marco Congedo, Christian Jutten, Salem Said, and Yannick Berthoumieu. Transfer Learning: A Riemannian Geometry Framework With Applications to Brain\u2013Computer Interfaces. IEEE Transactions on Biomedical Engineering, 65(5):1107\u20131116, May 2018.   \n[63] Hongyi Zhang and Suvrit Sra. First-order Methods for Geodesically Convex Optimization. In Conference on Learning Theory, pages 1617\u20131638. PMLR, June 2016. ISSN: 1938-7228. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Matrix operations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Given $\\Sigma\\in\\mathbb{S}_{d}^{++}$ and its Singular Value Decomposition (SVD) $\\begin{array}{r}{\\Sigma={U\\,\\mathrm{diag}(\\lambda_{1},\\ldots,\\lambda_{d})}{U^{\\top}}}\\end{array}$ , the matrix logarithm of $\\Sigma$ is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\log(\\Sigma)=U\\operatorname{diag}(\\log(\\lambda_{1}),\\dots,\\log(\\lambda_{d}))U^{\\top}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "$\\Sigma$ to the power $\\alpha\\in\\mathbb{R}$ is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma^{\\alpha}={U}\\,\\mathrm{diag}(\\lambda_{1}^{\\alpha},\\dots,\\lambda_{d}^{\\alpha}){\\cal U}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.2 Proof ofLemma 2.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "First, we recall that the geodesic associated with the affine invariant metric from $\\Sigma$ to $\\Sigma^{\\prime}$ is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Sigma\\sharp_{\\alpha}\\Sigma^{\\prime}\\triangleq\\Sigma^{1/2}\\left(\\Sigma^{-1/2}\\Sigma^{\\prime}\\Sigma^{-1/2}\\right)^{\\alpha}\\Sigma^{1/2}\\quad{\\mathrm{for~}}\\alpha\\in[0,1].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence, $\\Sigma\\sharp_{\\alpha}I_{d}=\\Sigma^{1-\\alpha}$ . From [ 59 ], the parallel transport of $\\Sigma^{\\prime}$ from $\\pmb{\\Sigma}_{1}$ to $\\pmb{\\Sigma}_{2}$ is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{E\\Sigma^{\\prime}E^{\\top}\\quad\\mathrm{with}\\quad E\\triangleq\\Sigma_{1}^{1/2}\\left(\\Sigma_{1}^{-1/2}\\Sigma_{2}\\Sigma_{1}^{-1/2}\\right)^{1/2}\\Sigma_{1}^{-1/2}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence, the parallel transport of $\\Sigma^{\\prime}$ from $\\Sigma$ to $\\Sigma\\sharp_{\\alpha}I_{d}$ is $E\\Sigma^{\\prime}E^{\\top}$ with ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{E\\triangleq\\sum^{1/2}\\left(\\sum^{-1/2}\\Sigma^{1-\\alpha}\\Sigma^{-1/2}\\right)^{1/2}\\Sigma^{-1/2}}}\\\\ {{=\\sum^{1/2}\\Sigma^{-\\alpha/2}\\Sigma^{-1/2}=\\Sigma^{-\\alpha/2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 14}, {"type": "text", "text": "A.3 Cross-spectrum computation and preprocessing ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Bartlett estimator From [ 33 ], the features provided in the HarMNqEEG dataset have been computed using the Bartlett\u2019s estimator by averaging more than 20 consecutive and non-overlapping segments. Thus, data consist of cross-spectral matrices with a frequency range of $f_{\\mathrm{min}}=1.17\\,\\mathrm{Hz}$ to $f_{\\mathrm{max}}=19.14\\,\\mathrm{Hz}$ , sampled at a resolution of $\\Delta\\omega=0.39\\,\\mathrm{Hz}$ . These cross-spectral matrices are denoted $\\mathbf{S}_{k,i}(\\omega)\\in\\mathcal{H}_{d}^{++}$ where $k$ is the site, $i$ the participant and $\\omega\\in\\{f_{\\operatorname*{min}},f_{\\operatorname*{min}}+\\Delta\\omega,\\ldots,f_{\\operatorname*{max}}\\}$ . ", "page_idx": 14}, {"type": "text", "text": "Common average reference (CAR) The cross-spectrum matrices $\\mathbf{S}_{i}(\\omega)$ were re-referenced from their original montages with a CAR: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{S}}_{k,i}(\\omega)\\triangleq\\mathbf{H}\\mathbf{S}_{k,i}(\\omega)\\mathbf{H}^{\\top}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\mathbf{H}\\triangleq I_{d}-\\mathbf{1}_{d}\\mathbf{1}_{d}^{\\top}/d$ . ", "page_idx": 14}, {"type": "text", "text": "Global Scale Factor (GSF) Co-spectrum matrices were re-scaled with an individual scalar $\\widehat{\\zeta}_{k,i}$ that is calculated as the geometric mean of their power spectrum across sensors and frequencies : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\widehat{\\zeta}_{k,i}\\triangleq\\exp\\left(\\frac{1}{N_{\\omega}d}\\sum_{\\ell=0}^{N_{\\omega}-1}\\sum_{c=1}^{d}\\log\\left(\\left(\\widehat{\\mathbf{S}}_{k,i}(f_{\\operatorname*{min}}+\\ell\\Delta\\omega)\\right)_{c,c}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\begin{array}{r}{N_{\\omega}\\triangleq\\frac{f_{\\mathrm{max}}-f_{\\mathrm{min}}}{\\Delta\\omega}+1}\\end{array}$ . The GSF correction is then applied to the co-spectrum (the real part of the cross-spectrum) for all frequencies $\\omega$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma_{k,i}(\\omega)\\triangleq\\Re\\left(\\widetilde{\\mathbf{S}}_{k,i}(\\omega)\\right)/\\widehat{\\zeta}_{k,i}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The $\\Sigma_{k,i}(\\omega)\\in\\mathbb{S}_{d}^{++}$ are the features used the Section 4 . ", "page_idx": 14}, {"type": "text", "text": "A.4 Baselines ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The four baseline methods used in this work are detailed in the following. For every methods we have access to $K$ labeled source domains, each including $N_{k}$ covariance matrices and their corresponding variables of interest (\u03a3k,i, yk,i)iN=k1 . The method DO Dummy and the mixed-effects model baseline DO Intercept both access the mean value $\\bar{y}\\tau$ of the target domain variable to predict. We remind that as the dataset used in the empirical benchmarks is constituted of several frequency bands, each method is applied to each frequency band independently and then computed vectors are concatenated. ", "page_idx": 15}, {"type": "text", "text": "Domain-aware dummy model (DO Dummy) The DO Dummy simply returns the mean value $\\bar{y}_{7}$ for every predictions of a given target domain. ", "page_idx": 15}, {"type": "text", "text": "No re-center / No domain adaptation (No DA) The covariance matrices are used as input of the regression pipeline without any re-centering. First, the geometric mean of the source matrices is computed: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\overline{{\\Sigma}}_{\\mathcal{S}}\\triangleq\\underset{\\Sigma\\in\\mathbb{S}_{d}^{++}}{\\arg\\operatorname*{min}}\\,\\sum_{k=1}^{K}\\sum_{i=1}^{N_{k}}\\delta_{R}(\\Sigma,\\Sigma_{k,i})^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, source feature vectors are extracted with: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\phi(\\Sigma_{k,i},\\overline{{\\Sigma}}_{\\cal S})=\\mathrm{uvec}\\left(\\mathrm{log}\\left(\\overline{{\\Sigma}}_{\\cal S}^{-1/2}\\Sigma_{k,i}\\overline{{\\Sigma}}_{\\cal S}^{-1/2}\\right)\\right)\\in\\mathbb{R}^{d(d+1)/2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Finally, the target feature vectors are extracted from the target data $(\\Sigma_{\\mathcal{T},i})_{i=1}^{N_{\\mathcal{T}}}$ with: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\phi(\\Sigma_{\\mathcal{T},i},\\overline{{\\Sigma}}_{\\mathcal{S}})=\\mathrm{uvec}\\left(\\log\\left(\\overline{{\\Sigma}}_{\\mathcal{S}}^{-1/2}\\Sigma_{\\mathcal{T},i}\\overline{{\\Sigma}}_{\\mathcal{S}}^{-1/2}\\right)\\right)\\in\\mathbb{R}^{d(d+1)/2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Re-center to a common reference point (Re-center) Domains are re-centered to a common reference point, here we decided to use the identity. First, the geometric mean of each domain is computed: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\overline{{\\Sigma}}_{k}\\triangleq\\arg\\operatorname*{min}_{\\mathbf{\\Sigma}\\mathbf{\\epsilon}\\in\\mathbb{S}_{d}^{++}}\\sum_{i=1}^{N_{k}}\\delta_{R}(\\mathbf{\\Sigma},\\mathbf{\\Sigma}\\mathbf{\\Sigma}_{\\mathbf{\\Sigma}}\\mathbf{,}\\mathbf{\\Sigma}_{k,i})^{2}\\,\\mathrm{~.~}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, feature vectors are extracted using the specific geometric mean of each domain: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\phi(\\Sigma_{k,i},\\overline{{\\Sigma}}_{k})=\\operatorname{uvec}\\left(\\log\\left(\\overline{{\\Sigma}}_{k}^{-1/2}\\Sigma_{k,i}\\overline{{\\Sigma}}_{k}^{-1/2}\\right)\\right)\\in\\mathbb{R}^{d(d+1)/2}\\ \\ .\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Covariance matrices of the target domain $(\\Sigma_{\\mathcal{T},i})_{i=1}^{N_{\\mathcal{T}}}$ are also re-centered to the identity using their geometric mean : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\overline{{\\Sigma}}_{\\mathcal{T}}\\triangleq\\arg\\operatorname*{min}_{\\pmb{\\Sigma}\\in\\mathbb{S}_{d}^{++}}\\sum_{i=1}^{M}\\delta_{R}(\\Sigma,\\Sigma_{\\mathcal{T},i})^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\phi(\\Sigma_{\\mathcal{T},i},\\overline{{\\Sigma}}_{\\mathcal{T}})=\\mathrm{uvec}\\left(\\log\\left(\\overline{{\\Sigma}}_{\\mathcal{T}}^{-1/2}\\Sigma_{i}\\overline{{\\Sigma}}_{\\mathcal{T}}^{-1/2}\\right)\\right)\\in\\mathbb{R}^{d(d+1)/2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For both No DA and Re-center, the regression task was performed using a Ridge regression, which included an intercept: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\beta_{\\mathcal{S}}^{\\star},\\beta_{0,S}^{\\star}=\\underset{\\beta\\in\\mathbb{R}^{d(d+1)/2}}{\\arg\\operatorname*{min}}\\sum_{k=1}^{K}\\sum_{i=1}^{N_{k}}\\left(y_{k,i}-\\beta^{\\top}z_{k,i}-\\beta_{0}\\right)^{2}+\\lambda\\left\\|\\beta\\right\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $z_{k,i}$ is computed with ( 25 ) or ( 28 ). The predicted values were computed as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widehat{y}_{\\mathcal{T},i}=(\\beta_{S}^{\\star})^{\\top}z_{\\mathcal{T},i}+\\beta_{0,S}^{\\star}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $z_{T,i}$ is computed with ( 26 ) or ( 30 ). ", "page_idx": 15}, {"type": "text", "text": "Domain-aware intercept (DO Intercept) In addition to the $K$ labeled source domains, we assume to have access to the mean of the variable to predict of the target domain $\\bar{y}\\tau$ . At train-time, we fit a Ridge regression with a specific intercept for each domain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\beta_{S}^{\\star}=\\underset{\\beta\\in\\mathbb{R}^{d(d+1)/2}}{\\arg\\operatorname*{min}}\\sum_{i=1}^{K}\\sum_{i=1}^{N_{k}}\\left(y_{k,i}-\\beta^{\\top}\\phi(\\Sigma_{k,i},\\overline{{\\Sigma}}_{S})-\\bar{y}_{k}\\right)^{2}+\\lambda\\left\\|\\beta\\right\\|_{2}^{2}\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, at test-time, we fit a new intercept $\\beta_{0,{\\mathcal T}}$ using the target features: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\phi\\left(\\Sigma_{\\mathcal{T},i},\\overline{{\\Sigma}}_{\\mathcal{S}}\\right)=\\operatorname{uvec}\\left(\\log\\left(\\overline{{\\Sigma}}_{\\mathcal{S}}^{-1/2}\\Sigma_{\\mathcal{T},i}\\overline{{\\Sigma}}_{\\mathcal{S}}^{-1/2}\\right)\\right)\\in\\mathbb{R}^{d(d+1)/2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The fitted intercept is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\beta_{0,T}=\\bar{y}_{\\mathcal{T}}-\\frac{1}{N_{\\mathcal{T}}}\\sum_{i=1}^{N_{\\mathcal{T}}}(\\beta_{S}^{\\star})^{\\top}\\phi\\left(\\Sigma_{\\mathcal{T},i},\\overline{{\\Sigma}}_{S}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and the predictions are ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\widehat{y}_{\\mathcal{T},i}=(\\beta_{S}^{\\star})^{\\top}\\phi\\left(\\Sigma_{\\mathcal{T},i},\\overline{{\\Sigma}}_{S}\\right)+\\beta_{0,\\mathcal{T}}\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.5 HarMNqEEG dataset ", "page_idx": 17}, {"type": "image", "img_path": "qTypwXvNJa/tmp/3b93ef15d4c5c6756eca41f081b83d876bc45d3e2cd9b64b701100eb4dab7a46.jpg", "img_caption": ["Figure 5: Age distribution of the 14 sites of the HarMNqEEG dataset [ 33 ]. The distributions are represented with a kernel density estimate. The y-scales are not shared for visualization purpose. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "A.6 Table of performance scores. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Table 1: Performance scores for different combinations of source sites. The remaining sites were used as target domains. ", "page_idx": 18}, {"type": "table", "img_path": "qTypwXvNJa/tmp/f1df8d6e0b3d917b3422d5411ab3aa19eba9ec898f1059e5f2eddc0e165b7a04.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "A.7 Figure 3 without normalization ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Without Re-center and Re-scale: ", "page_idx": 19}, {"type": "image", "img_path": "qTypwXvNJa/tmp/fdb73f8b958bed8e043e1215b075a69d7edc789b733dcf9eed376005b3de82a4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 6: Performance of four methods on several source-target combinations for three metrics: Spearman\u2019s $\\rho\\uparrow$ (left), $R^{2}$ score $\\uparrow$ (middle) and Mean Absolute Error $\\downarrow$ (right). Re-center was removed from the plot to better visualize the other methods. A box represents the concatenated results across all site combinations. One point corresponds to one split of one site combination. ", "page_idx": 19}, {"type": "text", "text": "With Re-center and Re-scale: ", "page_idx": 19}, {"type": "image", "img_path": "qTypwXvNJa/tmp/cd6abe48538d9059b7b9420343ab225c35a6f1879187d293cd2a8c1413e05546.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 7: Performance of all methods on several source-target combinations for three metrics: Spearman\u2019s $\\rho\\uparrow$ (left), $R^{2}$ score $\\uparrow$ (middle) and Mean Absolute Error \u2193(right). A box represents the concatenated results across all site combinations. One point corresponds to one split of one site combination. ", "page_idx": 19}, {"type": "text", "text": "A.8 Boxplots of each source-target sites for the three metrics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The following figures represent the performance scores that are displayed in subsection A.6 . ", "page_idx": 19}, {"type": "image", "img_path": "qTypwXvNJa/tmp/b5b3d84965eda2a1ced36f9c8cfaa77fb8c76ac64a64fc67eb3b4ec07318e25e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 8: Spearman\u2019s $\\rho\\uparrow$ for every site combination. One panel corresponds to the results of one site combination. One point corresponds to one split. ", "page_idx": 20}, {"type": "image", "img_path": "qTypwXvNJa/tmp/bad50adbdfb13071a712a8db8a8d28eefb41deddd45db7f0a9f6f118988b6ca8.jpg", "img_caption": ["Figure 9: $R^{2}$ score $\\uparrow$ for every site combination. One panel corresponds to the results of one site combination. One point corresponds to one split. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "qTypwXvNJa/tmp/9c0f58de21ebfb2c0994ce4df84e07dc87a1eeb6b331b30b9ae8918709d00342.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 10: Mean Absolute Error $\\downarrow$ for every site combination. One panel corresponds to the results of one site combination. One point correspond to one split. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We claim we propose a new test-time multi-source domain adaption method. We also claim this method is state of the art on the HarMNqEEG dataset. These two claims are asserted in Section 3 and in the Results paragraph of Section 4 . ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: A limitation of the proposed method is to require the mean outcome value $\\bar{y}\\tau$ of the target set. This limitation is stated in the Contributions paragraph of Section 1 and discussed in Section 6 . ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The proposed method uses the parallel transport from a given SPD matrix to the identity. The formula is presented in Lemma 2.1 , and the proof is provided in Appendix A.2 . Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The empirical benchmarks ( Section 4 ) use the HarMNqEEG dataset which is available in open access. All the preprocessing steps, as well as the baselines, are extensively detailed in Appendix A.3 and Appendix A.4 , respectively. The technical details (such as gradient computation and optimization algorithm) to implement the proposed method are provided in Section 3 . ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The dataset HarMNqEEG [  33 ] is in open access. We provide the code to reproduce the experiments from the raw data. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ public/guides/CodeSubmissionPolicy ) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ( https: //nips.cc/public/guides/CodeSubmissionPolicy ) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 23}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification:  Section 3 and Section 4 provide all the necessary details to reproduce the results: optimizer, splitting strategy, hyperparameter selection, and model evaluation. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Error bars and box plots computed from different splits of the data were reported. p-values following [ 37 ] between the best baseline and the proposed method were computed and presented in Figure 3 . ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provided in Section 4 the computational resources used for running all the benchmarks. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The authors acknowledged having read the NeurIPS Code of Ethics, and the paper conforms to it. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The proposed approach opens up various applications at the interface between machine learning and biostatistics, such as biomarker exploration in large multicenter clinical trials. References are provided in Section 6 . ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Anonymized data are already available in open access, and the proposed benchmark is limited to biomarker regression. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The dataset presented by [  33 ] is an open-source dataset accessible on https: //synapse.org , https://10.0.28.135/syn26712693 . We cited the work beyond the data reference and gave credit to scientific ideas and concepts based on [ 33 ]. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: No new assets are provided in the paper. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets. \u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: ", "page_idx": 27}, {"type": "text", "text": "The human research data used in this study was curated by [ 33 ] and conducted by academic researchers. We do not reproduce their data acquisition protocol here and refer to the original work [ 33 ]. We argue that this is appropriate as our work propose a new statistical method and does not present novel biomedical results. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: ", "page_idx": 27}, {"type": "text", "text": "The data acquisition was approved by institutional review boards as stated in [ 33 ]: \u201cThe studies involving human participants were reviewed and approved by the Ethics Committees of all involved institutions. In all cases, the participants and/or their legal guardians/next of kin provided written informed consent to participate in this study. All data were de-identified, and participants gave permission for their data to be shared as part of the informed consent process.\u201c. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]