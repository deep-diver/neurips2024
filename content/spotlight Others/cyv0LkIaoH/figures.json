[{"figure_path": "cyv0LkIaoH/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of the curation phenomenon: 1. User proposes prompts such as \"butterfly going to the bathroom\", 2. Four images are generated with Midjourney, 3. User only upscale one (e.g. the top left image) image, 4. Solely upscaled images are incorporated into the JourneyDB dataset (Pan et al., 2023). Samples from other diffusion models can be found in Figures 12a and 12b.", "description": "This figure illustrates the process of data curation in the context of generative models.  It shows a user providing a prompt to an AI image generator (Midjourney). The generator produces four images in response. The user selects one of these images to upscale (increase resolution), effectively curating the generated data. Only the upscaled image is then included in the final dataset, which is used for training future generative models.  This example demonstrates how human preferences influence the selection of data that is ultimately used for model training.", "section": "2 Iterative retraining with curated synthetic data"}, {"figure_path": "cyv0LkIaoH/figures/figures_8_1.jpg", "caption": "Figure 2: CIFAR-10. Evolution of the proportion of the class 'Airplane' and of the 9 other classes when filtering on curated synthetic samples with reward r(x) = \u03b3\u00b7q0(x) (as suggested by Theorem 2.3). On the other hand, the average reward increases before stabilizing as predicted by Theorem 2.3.", "description": "This figure shows the results of an experiment on the CIFAR-10 dataset.  The left panel displays the proportion of \"Airplane\" class images and the proportion of all other classes over 10 retraining iterations. The right panel shows the average reward over the same iterations.  The experiment used curated synthetic data, where samples were filtered based on a reward function related to the probability of the classifier for the airplane class. As predicted by Theorem 2.3, the proportion of airplane images increases while the proportion of other classes decreases.  The average reward also increases over iterations, as expected.", "section": "4.1 Natural images on CIFAR10"}, {"figure_path": "cyv0LkIaoH/figures/figures_8_2.jpg", "caption": "Figure 3: CIFAR-10. Evolution of the proportion of each class and the average reward r(x) when filtering based on the confidence of a classifier. On the left, retraining is done solely on the curated synthetic samples which results in the emergence of proportion biases. On the right, retraining is performed on a mixture of real and curated synthetic samples which results in both increased stability and still reward augmentation.", "description": "This figure shows the results of experiments on the CIFAR-10 dataset.  Two scenarios are compared: one where only curated synthetic data is used for retraining, and another where a mixture of real and curated synthetic data is used. The left-hand side plots show how the proportion of each image class changes over multiple retraining steps when using only curated synthetic data.  There is a clear bias amplification where some classes dominate the model's output. The right-hand side plots show the same experiment but using a mixture of real and curated synthetic data. In this setting, the class proportions remain more stable and the increase of the average reward shows the benefits of this approach. ", "section": "4 Experiments"}, {"figure_path": "cyv0LkIaoH/figures/figures_26_1.jpg", "caption": "Figure 4: Mixture of Gaussians. Iterative retraining on the two moons dataset for 8 iterations. On the top row, we display the fully filtered synthetic loop, and below we use a mixture of real and filtered data.", "description": "This figure shows the results of an experiment on a mixture of Gaussian dataset using iterative retraining with and without real data. The top row shows the results when only curated synthetic data is used at each retraining step, while the bottom row shows the results when a mixture of real and curated synthetic data is used. The images show the model's learned distribution at different iterations.", "section": "4 Experiments"}, {"figure_path": "cyv0LkIaoH/figures/figures_26_2.jpg", "caption": "Figure 5: Two moons. Iterative retraining on the two moons dataset for 5 iterations. On the top row, we display the fully filtered synthetic loop, and below we use a mixture of real and filtered data.", "description": "This figure shows the results of an experiment on a \"two moons\" dataset using iterative retraining with and without curated synthetic data.  The top row displays the results when only curated synthetic data is used for retraining, demonstrating the model's tendency to collapse towards high-reward regions.  The bottom row shows the results when a mixture of real and curated synthetic data is used. This demonstrates that adding real data helps maintain the diversity and stability of the model, preventing collapse.", "section": "4 Experiments"}, {"figure_path": "cyv0LkIaoH/figures/figures_27_1.jpg", "caption": "Figure 6: Plots of respectively a) the level sets of the reward b) the density of the mixture of Gaussians c) The limit density of the fully synthetic retraining loop with curation as predicted by theory", "description": "This figure shows three heatmaps. The first heatmap (a) visualizes the level sets of the reward function used in the experiments, which is defined as the negative distance to the closest center of a mixture of Gaussians, clipped to be non-negative. The second heatmap (b) shows the density of the initial mixture of Gaussians distribution used for training. The third heatmap (c) displays the theoretical limit density, as predicted by the theorems in the paper, which is obtained by re-weighting the initial density to maximize the expected reward, focusing on the level sets of the reward function.  The figure illustrates the convergence of the iterative retraining process to a distribution concentrated on the highest reward regions.", "section": "4 Experiments"}, {"figure_path": "cyv0LkIaoH/figures/figures_27_2.jpg", "caption": "Figure 5: Two moons. Iterative retraining on the two moons dataset for 5 iterations. On the top row, we display the fully filtered synthetic loop, and below we use a mixture of real and filtered data.", "description": "This figure shows the results of an experiment using the \"two moons\" dataset.  The top row illustrates the iterative retraining process using only curated synthetic data, showing a progressive collapse towards a single mode. The bottom row displays the same process, but with real data re-injected at each step, demonstrating increased stability and the preservation of both modes.", "section": "4.1 Natural images on CIFAR10"}, {"figure_path": "cyv0LkIaoH/figures/figures_28_1.jpg", "caption": "Figure 8: FID, precision and recall when retraining with filtering and r(x) = \u03b3 \u00b7 q0(x), \u03b3 = 5", "description": "This figure shows the Fr\u00e9chet Inception Distance (FID), precision, and recall scores across multiple retraining steps when a filtering process is applied to the synthetic data. The reward function used in the filtering process is defined as r(x) = \u03b3 \u00b7 q0(x), where \u03b3 = 5, and q0(x) is the probability assigned to class 0 by a classifier.  The figure illustrates the impact of the retraining process on the generated data\u2019s quality. High FID indicates poor image quality and low FID indicates higher similarity to real images. Precision and recall show how effectively the filtering selects samples corresponding to the chosen reward function.", "section": "4.1 Natural images on CIFAR10"}, {"figure_path": "cyv0LkIaoH/figures/figures_28_2.jpg", "caption": "Figure 3: CIFAR-10. Evolution of the proportion of each class and the average reward r(x) when filtering based on the confidence of a classifier. On the left, retraining is done solely on the curated synthetic samples which results in the emergence of proportion biases. On the right, retraining is performed on a mixture of real and curated synthetic samples which results in both increased stability and still reward augmentation.", "description": "This figure shows the results of experiments on the CIFAR-10 dataset. The left panel shows the effect of retraining a model solely on curated synthetic data, where the reward is based on classifier confidence. This leads to an increase in the average reward but also to a skewed distribution of classes, indicating bias amplification.  The right panel demonstrates the effect of retraining on a mixture of real and curated synthetic data.  Here, increased stability is observed alongside continued reward augmentation, suggesting that the inclusion of real data mitigates the negative effects of bias amplification observed when using only synthetic data.", "section": "4 Experiments"}, {"figure_path": "cyv0LkIaoH/figures/figures_28_3.jpg", "caption": "Figure 10: CIFAR-10. FID, precision and recall when retraining with filtering and r(x) = \u03b3 \u00b7 arg max0<i<9 qi(x), \u03b3 = 15 and reusing real data at each step", "description": "This figure shows the FID, precision, and recall scores over 20 retraining steps on the CIFAR-10 dataset.  The reward function used for filtering is based on the confidence of a classifier, and real data is re-injected at each step. The results show that the FID remains stable, precision increases and recall remains relatively stable, demonstrating the positive impact of re-injecting real data on stability, in contrast to the instability that can occur when only using curated synthetic data.", "section": "4.1 Natural images on CIFAR10"}, {"figure_path": "cyv0LkIaoH/figures/figures_29_1.jpg", "caption": "Figure 11: CIFAR-10. Evolution of the proportion of the classes and the average reward when filtering based on the confidence of a classifier for three independent runs. The curves have small variance which supports our results when only one run was reported due to the high compute costs of retraining a generative models multiple times.", "description": "This figure shows the results of three independent runs of an experiment on the CIFAR-10 dataset.  The experiment involved iteratively retraining a generative model using a filtering process based on classifier confidence as a reward. The top row of plots displays the proportion of each class over multiple retraining iterations for each run. The bottom row shows the average confidence (reward) for the model over the same iterations for each run. The results across the three runs demonstrate high consistency, supporting the study's claims despite only presenting one run in the main body due to high computational cost.", "section": "4.1 Natural images on CIFAR10"}, {"figure_path": "cyv0LkIaoH/figures/figures_30_1.jpg", "caption": "Figure 1: Illustration of the curation phenomenon: 1. User proposes prompts such as \"butterfly going to the bathroom\", 2. Four images are generated with Midjourney, 3. User only upscale one (e.g. the top left image) image, 4. Solely upscaled images are incorporated into the JourneyDB dataset (Pan et al., 2023). Samples from other diffusion models can be found in Figures 12a and 12b.", "description": "This figure illustrates the process of data curation in generative models.  A user provides a prompt (e.g., a whimsical image description). The model generates four different images based on the prompt. The user then selects only one of these images, often upscaling it to a higher resolution, before it is added to a training dataset. This curation process implicitly reflects user preferences and influences the model's future generations.", "section": "1 Introduction"}]