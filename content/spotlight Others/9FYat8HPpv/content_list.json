[{"type": "text", "text": "SpikeReveal: Unlocking Temporal Sequences from Real Blurry Inputs with Spike Streams ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kang Chen1,2\u2217 Shiyan Chen1,2\u2217 Jiyuan Zhang1,2 Baoyue Zhang1,2 Yajing Zheng1,2 Tiejun Huang1,2,3 Zhaofei Yu1,2,3 ", "page_idx": 0}, {"type": "text", "text": "1 School of Computer Science, Peking University 2 National Key Laboratory for Multimedia Information Processing, Peking University 3 Institute for Artificial Intelligence, Peking University {mrchenkang,strerichia002p,jyzhang,byzhang}@stu.pku.edu.cn {yj.zheng,tjhuang,yuzf12}@pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reconstructing a sequence of sharp images from the blurry input is crucial for enhancing our insights into the captured scene and poses a significant challenge due to the limited temporal features embedded in the blurry image. Spike cameras, sampling at rates up to $40{,}000\\ \\mathrm{Hz}$ , have proven effective in capturing motion features and beneficial for solving this ill-posed problem. Nonetheless, existing methods fall into the supervised learning paradigm, which suffers from notable performance degradation when applied to real-world scenarios that diverge from the synthetic training data domain. To address this challenge, we propose the first self-supervised framework for the task of spike-guided motion deblurring. Our approach begins with the formulation of a spike-guided deblurring model that explores the theoretical relationships among spike streams, blurry images, and their corresponding sharp sequences. We subsequently develop a self-supervised cascaded framework to alleviate the issues of spike noise and spatial-resolution mismatching encountered in the deblurring model. With knowledge distillation and reblur loss, we further design a lightweight deblur network to restore high-quality sequences with brightness and texture consistency with the original input. Quantitative and qualitative experiments conducted on our real-world and synthetic datasets with spikes validate the superior generalization of the proposed framework. Our code, data and trained models are available at https://github.com/chenkang455/S-SDM. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Traditional cameras, constrained by their exposure-based imaging mechanism, often produce blurry images when capturing fast-moving objects or during camera movement throughout the exposure process [19, 35]. While these blurry images lose significant details, the ability to recover dynamic motion trajectories from the static blurry input becomes critically important. However, the inherent challenge lies in the limited motion features available within blurry frames, leading to potential ambiguities such as multiple motion trajectories corresponding to the same blurry input. This is exemplified by scenarios where two objects move along the same trajectory but in opposite directions [23, 24, 38], rendering the task of motion deblurring ill-posed. Recent advancements in learningbased approaches [40, 12, 46] seek to address this challenge by establishing direct mappings from ", "page_idx": 0}, {"type": "image", "img_path": "9FYat8HPpv/tmp/b5bee94779558a99526b3657066406f937a4eb8893a8bf02cb741d24e49b43e5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Illustration of the superiority of our self-supervised framework (S-SDM) over supervised methods. Supervised methods, while effective on synthetic datasets, suffer from a significant performance decline when applied to real-world datasets, primarily due to data distribution discrepancies. In contrast, our self-supervised framework, necessitating no Ground Truth (GT) for training, seamlessly bridges this dataset gap through fine-tuning on real-world datasets. ", "page_idx": 1}, {"type": "text", "text": "blurry inputs to sharp sequences in a supervised learning manner. Despite these efforts, traditional cameras struggle to capture fine details in high-speed motion due to their exposure constraints, thus limiting the effectiveness of these methods in scenarios not covered by the training datasets. ", "page_idx": 1}, {"type": "text", "text": "In recent years, neuromorphic cameras [9, 10], leveraging their ultra-high temporal resolution and high dynamic range, have found widespread use in many fields, including computer vision and robotics. These cameras, including event and spike cameras, are distinguished by their ability to produce high temporal resolution outputs directly tied to changes in light intensity. Specifically, event cameras generate events in areas where light intensity changes [4], while spike cameras capture the absolute brightness of the scene at each pixel, offering a stream of spikes as output [10]. This distinctive feature endows spike cameras with a significant advantage [47, 45, 48, 5, 41, 6] in capturing and recovering sharp texture from scenes with rapid motion. ", "page_idx": 1}, {"type": "text", "text": "Recent studies [2, 8] have explored the potential of RGB-Spike fusion, i.e., harnessing the strengths of both traditional and spike cameras to reconstruct sharp sequences from blurry inputs. However, their frameworks are constrained within the supervised learning paradigm, which necessitates extensive datasets comprising pairs of blurry and sharp images, as well as spike sequences. While synthetically acquiring such paired data, as demonstrated in previous studies [8, 4, 24], is feasible, collecting them in real-world scenarios presents the following challenges: (1) high-speed cameras are prohibitively expensive and not readily deployable in many settings; (2) spatial-temporal calibration between spike cameras and high-speed RGB cameras complicates the data collection process. These problems render the fine-tuning of supervised methods on real-world datasets challenging, further leading to their performance deterioration in such environments as shown in Fig. 1. The resulting degradation in image quality, manifesting as color distortion, brightness inconsistency, and inaccurate texture restoration, is mainly caused by the disparity between synthetic and real-world datasets, especially in terms of the density of spike stream, spike generation mechanism, and blurry image generation. Moreover, the effectiveness of supervised methods is inherently limited by the ground truth sequences created through motion analysis interpolation algorithms [22, 11], which inherently differs from the real-world scene and thus affects the model\u2019s generalization ability. ", "page_idx": 1}, {"type": "text", "text": "To overcome these issues, we propose the first-of-its-kind Self-supervised Spike-guided Deblurring Model (S-SDM), capable of recovering the continuous sharp sequence from a single blurry input with the assistance of low-resolution spike streams. We begin with a theoretical analysis of the relationship between spike streams, blurry images, and sharp sequences, leading to the development of our Spike-guided Deblurring Model (SDM). We further construct a self-supervised processing pipeline by cascading the denoising network and the super-resolution network to reduce the sensitivity of the SDM to spike noise and its reliance on spatial-resolution matching between the two modalities. To reduce the computational cost and enhance the utilization of spatial-temporal spike information within this pipeline, we further design a Lightweight Deblurring Network (LDN) and train it based on pseudo-labels from the teacher model, i.e., the established self-supervised processing pipeline. Further introducing reblur loss during LDN training, we achieve better restoration performance and faster processing speed than the processing-lengthy and structure-complicated teacher model. To validate the performance of our S-SDM across various scenarios, we build an RGB-Spike binocular system and propose the first spatially-temporally calibrated Real-world Spike Blur (RSB) dataset in this community. Quantitative and qualitative experiments conducted on the real-world and synthetic datasets validate the superiority of our method. In summary, our key contributions are: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 We develop a self-supervised spike-guided image deblurring framework, addressing the performance degradation due to the synthetic-real domain gap in supervised methods.   \n\u2022 We perform an in-depth theoretical analysis of the fusion between the spike stream and blurry image, leading to the development of the SDM.   \n\u2022 We propose a real-world dataset RSB and experiments on GOPRO and RSB datasets validate the superior generalization of our S-SDM. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Spike Camera. The spike camera, inspired by the primate retina, stands apart from conventional cameras with its ability to generate synchronous spike streams for each pixel at extremely low latency. This distinct feature provides significant advantages in various applications such as highspeed imaging [47, 45, 48, 5, 41, 6, 33], optical flow estimation [43], object detection [44], 3D reconstruction [30], depth estimation [34], motion deblurring [8, 32], and occlusion removal [31]. ", "page_idx": 2}, {"type": "text", "text": "Spike-guided Motion Deblurring. While the spike camera boasts an ultra-high temporal resolution, its development is currently impeded by the low spatial resolution. Additionally, the single-channel output from the spike camera restricts previous methods from recovering the image color information. To address these issues, a promising approach is establishing an RGB-Spike hybrid imaging system [2]. The binocular system achieves the multi-modality fusion of High-spatial/Low-temporal RGB blurry input and High-temporal/Low-spatial spike stream, thereby also serving as a spike-guided motion deblurring method [8]. However, to the best of our knowledge, existing spike-guided deblurring methods [2, 8] predominantly rely on supervised training on synthetic datasets. This reliance results in significant performance degradation when these methods are evaluated in real-world scenarios due to the domain discrepancies between synthetic and real datasets as illustrated in Fig. 1. ", "page_idx": 2}, {"type": "text", "text": "Event-based Motion Deblurring. Event camera [21] can asynchronously generate events that record log-intensity changes at the pixel level with minimal latency, which contains a rich set of motion features beneficial for motion deblurring tasks. Numerous supervised methods [16, 4, 3, 16, 23, 24] have been proposed to learn the mapping from the blurry input, events to the sharp outcome. Despite these advancements, a major hurdle remains in obtaining real blurry-sharp image pairs for training. To overcome the domain gap between synthetic and real-world datasets, recent methods [29, 38, 39] explored the mutual constraint between the blurry image and event stream, enabling the training of networks on real-world blur datasets. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Spike Camera Mechanism. Consider $\\mathbf L(t)$ to represent the latent sharp frame at time $t$ . Each pixel $p$ in the spike camera [10] has an integrator that accumulates the incoming photons at a high frequency. Once the cumulative intensity exceeds a predefined threshold $C$ at time $t_{e}$ , pixel $p$ emits a spike, and the accumulation of photons is reset to zero. This process can be mathematically described as ", "page_idx": 2}, {"type": "text", "text": "follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\int_{t_{s}}^{t_{e}}\\mathbf{L}(t)d t\\geq C,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $t_{s}$ denotes the firing time of the previous spike. While the spike camera is capable of generating asynchronous spike streams akin to that of event cameras, its effectiveness is constrained by the inherent limitations of its physical circuitry, which necessitates reading spikes at a predetermined sampling rate. We denote the generated spike stream as $S\\,\\in\\,\\{0,1\\}^{K\\times1\\dot{\\times}\\,H\\times W}$ , where $H$ and $W$ signify the height and width of the image, and $K$ represents the length of the spike sequence. ", "page_idx": 3}, {"type": "text", "text": "Problem Formulation. In traditional photography, motion blur occurs when there is relative movement between the camera and the scene during the exposure period. According to the motion blur physical model [9], the blurry image $\\mathbf{B}$ can be represented as the average of the latent frame $\\mathbf L(t)$ over the exposure $\\tau$ , i.e.: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{B}={\\frac{1}{T}}\\int_{t\\in T}\\mathbf{L}(t)d t,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $T$ represents the exposure period. Despite the spike camera\u2019s superior temporal resolution, its spatial resolution remains comparatively low. This limitation is primarily attributed to the constraints in data transmission bandwidth and the challenges inherent in the manufacturing process. Here, we postulate that the spatial resolution of the spike camera is approximately one-quarter that of a conventional RGB camera. ", "page_idx": 3}, {"type": "text", "text": "In this paper, we aim to enhance the High-spatial/Low-temporal resolution blurry input $\\textbf{B}\\in$ $\\mathbb{R}^{1\\times3\\times\\dot{H}\\times\\dot{W}}$ into a sequence of High-Quality images $\\{\\mathbf{L}(t_{i})\\}_{i=1}^{K}\\,\\in\\,\\mathbb{R}^{K\\times3\\times H\\times W}$ with the a id\u2208- ing of High-temporal/Low-spatial resolution spike stream $\\begin{array}{r}{S_{\\mathcal{T}}\\in\\{0,1\\}^{K\\times1\\times\\frac{H}{4}\\times\\frac{W}{4}}}\\end{array}$ , which can be mathematically formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\{\\mathbf{L}(t_{i})\\}_{i=1}^{K}=\\mathrm{Deblur}(t_{i};\\mathbf{B},S_{T}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In Eq. (3), ${\\mathrm{Deblur}}(\\cdot)$ represents the Spike-guided Deblur-Net as shown in Fig. 1, $i$ refers to the $i$ -th frame in the spike stream $S_{T}$ , and $t_{i}$ is the timestamp associated with this frame. ", "page_idx": 3}, {"type": "text", "text": "3.2 Theoretical Analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The spike camera, with its photodetector tailored to capture the single-channel light intensity, faces difficulties in obtaining the color information that the multi-channel RGB camera can effortlessly capture. Therefore, we modify the color intensity $\\mathbf{L}(t)$ in Eq. (1) to the grayscale value $\\mathbf{L}_{g}(t)$ for further analysis: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\int_{t_{s}}^{t_{e}}\\mathbf{L}_{g}(t)d t\\geq C.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In this formulation, $\\mathbf{L}_{g}(t)=w_{r}\\cdot\\mathbf{L}_{r}(t)+w_{g r e}\\cdot\\mathbf{L}_{g r e}(t)+w_{b}\\cdot\\mathbf{L}_{b}(t)$ , where $\\mathbf{L}_{c}(t),w_{c}$ denote the intensity and weight of channel $c\\in\\{r,g r e,b\\}$ respectively. ", "page_idx": 3}, {"type": "text", "text": "Given the blurry input $\\mathbf{B}$ and its corresponding spike stream $s_{\\tau}$ , we incorporate Eq. (4) into the motion blur model presented in Eq. (2). This integration formulates a link between the two modalities as outlined below: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{B}_{g}=\\frac{C\\cdot N_{T}}{T},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{B}_{g}$ denotes the grayscale version of the blurry input, and $N_{T}$ denotes the total number of spikes accumulated over the exposure period. The accumulation $N_{T}$ is calculated as $\\begin{array}{r}{N_{\\mathcal{T}}=\\sum_{i=1}^{K}S[i]}\\end{array}$ , with $S[i]$ indicating the $i$ -th frame of the spike stream. ", "page_idx": 3}, {"type": "text", "text": "Within the exposure $\\tau$ , we consider a shorter spike sequence centered around the $t$ moment $S_{T^{\\prime}}\\in$ $\\{0,1\\}^{K^{\\prime}\\times1\\times H\\times W}$ , satisfying $K^{\\prime}\\ll K$ , $t\\in\\mathcal T$ , and ${\\mathcal{T}}^{\\prime}\\subset{\\mathcal{T}}$ . Similar to Eq. (5), we can derive the relationship between the short-exposure gray image $\\mathbf{E}_{g}(t,\\mathcal{T}^{\\prime})$ and the short spike stream $S_{T^{\\prime}}$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{E}_{g}(t,\\mathcal{T}^{\\prime})=\\frac{1}{T^{\\prime}}\\int_{s\\in\\mathcal{T}^{\\prime}}\\mathbf{L}_{g}(s)d s=\\frac{C\\cdot N_{T^{\\prime}}}{T^{\\prime}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $T^{\\prime}\\ll T$ represents the short exposure period. ", "page_idx": 3}, {"type": "image", "img_path": "9FYat8HPpv/tmp/a07808bc84504630a09ef388fc7f60973917ba8ea269e7aa46db175c84e8c90c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: The schematic diagram of our proposed distillation self-supervised framework. The \u201c $\\sun$ \" indicates that certain computations are executed in a non-network manner. ", "page_idx": 4}, {"type": "text", "text": "Given the observation that the color information of adjacent pixels often exhibits similarity under the premise of minor motion amplitude, we postulate that the colors of the blurry input $\\mathbf{B}$ and the short-exposure image $\\mathbf{E}(t,\\mathcal{T}^{\\prime})$ are identical. This assumption implies that the intensity proportion among RGB channels in the blurry input $\\alpha_{c}^{\\mathbf{B}}$ and the short-exposure image $\\alpha_{c}^{\\bf E}(t,\\tau^{\\prime})$ is approximately equivalent, satisfying $\\alpha_{c}^{\\bf B}={\\bf B}_{g}^{\\bf\\^{\\prime}}/{\\bf B}_{c}^{\\bf^{\\prime}}$ and $\\alpha_{c}^{\\mathbf{E}}(t,\\mathcal{T}^{\\prime})=\\mathbf{E}_{g}\\dot{(t,\\mathcal{T}^{\\prime})}/\\mathbf{E}_{c}\\ddot{(t,\\mathcal{T}^{\\prime})}$ . More details can be found in the supplementary materials. ", "page_idx": 4}, {"type": "text", "text": "Upon establishing it, we move forward to build a mathematical relation between the blurry image and the spike stream. By substituting the gray channel $g$ with color channel $c$ and dividing Eq. (5) by Eq. (6), we efficiently eliminate the unknown threshold $C$ and weights $\\alpha_{c}^{\\bf B}/\\alpha_{c}^{\\bf E}(t,T^{\\prime})$ , leading to the following equation: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{E}_{c}(t,{\\mathcal{T}}^{\\prime})=\\mathbf{B}_{c}\\cdot\\frac{N_{\\mathcal{T}^{\\prime}}}{N_{\\mathcal{T}}}\\cdot\\frac{\\mathcal{T}}{T^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "By applying Eq. (7) to RGB three channels, we explicitly establish the relationship between the color blurry input B, the color short-exposure image $\\mathbf{E}(t,\\mathcal{T}^{\\prime})$ and the spike stream $S_{T}$ as shown in Fig. 18. Since the exposure period $\\mathcal{T}^{\\prime}$ is relatively short, it is reasonable to assume that the scene remains static. In this context, we interpret the short-exposure image $\\mathbf{E}(t,\\mathcal{T}^{\\prime})$ as the latent sharp frame $\\mathbf L(t)$ , allowing us to modify Eq. (7) as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{L}(t)=\\mathbf{B}\\cdot\\frac{N_{T^{\\prime}}}{N_{T}}\\cdot\\frac{T}{T^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To this end, we have conducted a comprehensive theoretical analysis of the spike-guided motion deblurring task, which is neglected in prior learning-based motion deblur methodologies [2, 8]. For further discussion readability, we refer to Eq. (8) as the Spike-guided Deblurring Model (SDM), which is analogous to the baseline motion deblur model EDI [18] in event camera. ", "page_idx": 4}, {"type": "text", "text": "3.3 Self-supervised Spike-guided Deblurring Model ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.3.1 Processing Pipeline ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "While SDM theoretically allows for the fusion of the blurry image and the spike stream, its practical deployment faces the following obstacles: ", "page_idx": 4}, {"type": "text", "text": "\u2022 The deblurred image $\\mathbf{E}(t,\\mathcal{T}^{\\prime})$ suffers from noise-related degradation due to the lack of adequate spike information during the short exposure $\\mathcal{T}^{\\prime}$ . ", "page_idx": 4}, {"type": "text", "text": "\u2022 The spatial resolution of the spike camera is approximately one-quarter of the RGB camera, rendering the SDM implementation impractical. ", "page_idx": 5}, {"type": "text", "text": "To overcome these limitations, we further cascade the self-supervised denoising network to eliminate the spike noise in $N_{T^{\\prime}}$ and super-resolution network to match spatial resolutions of the blurry image and spike stream, with the processing pipeline illustrated in the bottom of Fig. 2. ", "page_idx": 5}, {"type": "text", "text": "Denoising Network. We leverage the Blind Spot Network (BSN) [13, 1, 27, 5, 14, 7] to predict the clean spike accumulation $N_{T^{\\prime}}$ from the input short-exposure spike stream $S_{T^{\\prime}}$ . The core idea of BSN is to design the blind-spot strategy that compels the convolutional layer to estimate the clean value of each pixel solely based on its surrounding pixels. ", "page_idx": 5}, {"type": "text", "text": "Under the premise that the spike stochastic thermal noise is independent identically distributed [42], the BSN is trained to deduce sharp spike frames from the input, with the loss function formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{BSN}}=||\\mathrm{BSN}(S_{\\mathcal{T}^{\\prime}};\\Theta_{1})-N_{\\mathcal{T}^{\\prime}})||_{2}^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the denoised spike frame ${\\tt B S N}(S_{T^{\\prime}};\\Theta_{1})$ is denoted by $\\mathbf{L}_{\\mathrm{bsn}}(t)$ for further analysis. ", "page_idx": 5}, {"type": "text", "text": "Super-Resolution Network. In this task, we observe that the blurry input $\\mathbf{B}_{g}$ and the long-exposure spike frame $N_{T}$ exhibit the same texture features as shown in Eq. (5). This observation motivates us to train the Super-Resolution (SR) network based on pairs of the blurry images and the long-exposure spike frames. ", "page_idx": 5}, {"type": "text", "text": "We leverage the well-explored Enhanced Deep Super-Resolution network (EDSR) [15] as the backbone of our SR network, with the loss function formulated as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{EDSR}}=||\\mathrm{EDSR}(N_{T};\\Theta_{2})-\\mathbf{B}_{g}||_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "With the training of the SR network completed, we freeze its parameters and apply it to the denoised spike frame $\\mathbf{L}_{\\mathrm{bsn}}(t)$ , yielding the resolution-enhanced spike frame $\\mathbf{L}_{\\mathrm{bsn}}^{\\uparrow}(t)$ . ", "page_idx": 5}, {"type": "text", "text": "3.3.2 Knowledge Distillation Framework ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "While the aforementioned processing pipeline achieves the multi-modality fusion of the blurry input and the spike stream, several aspects still need refinement: ", "page_idx": 5}, {"type": "text", "text": "\u2022 The framework is lengthy and computationally demanding, which hinders its suitability for real-time system deployment.   \n\u2022 The blind-spot strategy of the BSN limits the full utilization of the spatial information inherent in the spike stream.   \n\u2022 The representation of the short-exposure image does not fully reflect the advantages of the high temporal resolution inherent in the spike stream. ", "page_idx": 5}, {"type": "text", "text": "To improve them, we further build a knowledge distillation framework building upon the existing processing pipeline. This pipeline serves as the teacher model, providing the reconstructed sequence as pseudo-labels for the training of the student model LDN, as illustrated in Fig. 2. ", "page_idx": 5}, {"type": "text", "text": "Lightweight Deblur Network. LDN adheres to a similar input and output pattern as previous research [8], i.e., taking the blurry input $\\mathbf{B}$ and the short spike stream $S_{T^{\\prime}}$ centered around moment $t$ as inputs, with the output being the reconstructed sharp image $\\mathbf{L}_{\\mathrm{ldn}}(t)$ , mathematically formulated as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\bf L}_{\\mathrm{ldn}}(t)=\\mathrm{LDN}({\\bf B},S_{T^{\\prime}};\\Theta_{3}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Full details regarding the LDN structure are available in the supplementary materials. ", "page_idx": 5}, {"type": "text", "text": "To avoid the scenario where the LDN exactly replicates the mapping of the teacher model, we design the teacher loss $\\mathcal{L}_{\\mathrm{tea}}$ based on the LPIPS [36] loss and further introduce the blur reconstruction loss. The reblur loss ${\\mathcal{L}}_{\\mathrm{reblur}}$ measures the difference between the blurry input $\\mathbf{B}$ and the re-synthesized blurry image $\\widetilde{\\bf B}$ , satisfying: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{B}}=\\frac{1}{M}\\sum_{m=1}^{M}\\mathbf{L}_{\\mathrm{ldn}}(t_{m}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{L}_{\\mathrm{ldn}}(t_{m})$ represents the $m$ -th recovered image within the exposure period $\\tau$ and $M$ is the total number of reconstructed images. Finally, we sum up two loss functions with the weighting parameter ", "page_idx": 5}, {"type": "image", "img_path": "9FYat8HPpv/tmp/183779c84a304edc7f8f6e288e0d6c685ca507187dd38531c13e3f7a5509a0a0.jpg", "img_caption": ["Figure 3: Qualitative comparison for the single frame restoration on the RSB dataset. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "9FYat8HPpv/tmp/4513a9354bbf752bab7d39ddfb1c9683d08b68f835b060284a3b5aa3d66e91f9.jpg", "img_caption": ["Figure 4: Qualitative comparison for the sequence reconstruction on the RSB dataset. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "$\\lambda$ , and the final loss function is formulated as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}=\\mathcal{L}_{\\mathrm{tea}}+\\lambda\\cdot\\mathcal{L}_{\\mathrm{reblur}}}\\\\ &{\\quad=\\displaystyle\\sum_{m=1}^{M}\\mathcal{L}_{\\mathrm{LPIPS}}(\\mathbf{L}_{\\mathrm{bsn}}^{\\uparrow}(t_{m}),\\mathbf{L}_{\\mathrm{ldn}}(t_{m}))+\\lambda\\cdot\\mathcal{L}_{\\mathrm{MSE}}(\\widetilde{\\mathbf{B}},\\mathbf{B}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Dataset ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Synthetic Data. For quantitative analysis of our spike-guided motion deblurring task, we construct the synthetic dataset based on the widely employed GOPRO [17] dataset. We initially utilize the interpolation algorithm XVFI [22] to augment the video frame by interpolating additional 7 frames between each pair of consecutive sharp images. To generate the spike stream that mimics reality closely, we downsample the interpolated video to the resolution of $320\\times180$ and simulate the spike stream based on the spike simulator [42]. To replicate real-world motion blur, we synthesize each blurry input by averaging 97 frames from interpolated video sequences. ", "page_idx": 6}, {"type": "table", "img_path": "9FYat8HPpv/tmp/ea6e63ec604d1038bb1729c1fdb68af5cfaa2fae1cf5bc927efaac9850f554b5.jpg", "table_caption": ["Table 1: Quantitative comparison of the sequence reconstruction task on the GOPRO dataset. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "9FYat8HPpv/tmp/2b3edd22583a740e8dee4b65a9edfaa14cb44e028d180ef5969315fe83313f41.jpg", "img_caption": ["Figure 5: Visual comparison of our S-SDM against other methods on the GOPRO dataset. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Real-world Data. We construct an RGB-Spike binocular system and propose the first Real-world Spike-guided Blur dataset (RSB) in this community. This system consists of a fixed-exposure RGB camera (Basler acA1920-150uc) and a spike camera [10], enabling us to capture the blurry image and the corresponding spike stream simultaneously. Further details about our RSB and the spatial-temporal calibration for our binocular system are provided in the supplementary materials. ", "page_idx": 7}, {"type": "text", "text": "4.2 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct both quantitative and qualitative comparisons of our S-SDM against state-of-the-art (SOTA) motion deblurring methods, including frame-based Motion-ETR [40], LEVS [12], videobased BiT [46], event-based TRMD [4], REFID [25], RED [29] and spike-based SpkDeblurNet [8] on the GOPRO and RSB datasets. For event-based methods, we replace the event stream with the spike stream and adopt the same input representation in these methods [29, 25, 4]. We further cascade the image super-resolution technique DASR [26] as in [39] for the deblurred sequence to overcome the modality resolution inconsistency which is not considered in these methods. We reconstruct 7 images from one blurry input for sequence restoration evaluation [39] as listed in Tab. 1. ", "page_idx": 7}, {"type": "table", "img_path": "9FYat8HPpv/tmp/0276bb557a91148fcf2119f226e1c39c235232ea2b246fc795e740c132a92819.jpg", "table_caption": ["Table 2: Performance comparison between the SAN in GEM [39] and our designed LDN. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Results on GOPRO. To simulate the spike density domain gap as depicted in Fig. 1, we train all supervised spike-based deblurring methods on the GOPRO dataset under spike threshold [42] $V_{t h}=1$ and evaluate them on datasets with spike thresholds $V_{t h}=1,2,4$ . Quantitative comparison results are listed in Tab. 1 and the visual comparison is demonstrated in Fig. 5. Given that the principal contribution of our S-SDM is self-supervised learning, it is foreseeable that our method might be slightly inferior to the supervised methods SpkdDeblurNet on the dataset with $V_{t h}=1$ . While these supervised methods deteriorate on datasets with $V_{t h}\\neq1$ , our method achieves great generalization beneftiing from the self-supervision design. Specifically, SpkDeblurNet tends to produce darker and blurrier reconstructions on images with high thresholds as shown in Fig. 5. Besides, our method achieves better restoration performance than the self-supervised method RED due to our consideration of the spatial-resolution mismatch between two modalities and the designed teacher loss, which imposes a stronger constraint than the optical loss in RED. ", "page_idx": 8}, {"type": "text", "text": "Results on RSB. We further present visualizations of single frame and sequence reconstruction comparisons on the real-world RSB dataset as depicted in Fig. 3 and 4. Both frame-based and videobased approaches fail to replicate fine textures and detailed elements present in the blurry input. While SpkDeblurNet is capable of recovering structural details and the motion trajectory, it is deteriorated by significant noise and color distortion under conditions with lower spike density than those simulated in the GOPRO dataset. Similarly, in scenarios of higher spike stream densities, the restoration of SpkDeblurNet tends to exhibit over-exposure, resulting in brightness inconsistency compared to the blurry input. This over-exposure affects the dynamic range of the reconstructed image, ultimately compromising the overall perceptual quality and uniformity of the restored sequence. Our method addresses these challenges by finetuning on the RSB dataset, ensuring that the restored sequence aligns with the real-captured blurry input and spike stream. More comparative experiments and analyses are accessible in the supplementary material. ", "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We perform ablation experiments on the GOPRO and RSB datasets to evaluate the performance of each module within S-SDM, the validity of our designed network architecture, as well as the overall effectiveness of our distillation learning framework. In this subsection, we evaluate the performance based on the single middle frame for simplicity. ", "page_idx": 8}, {"type": "text", "text": "Modules Cascading. Building upon the SDM, we sequentially cascade the BSN, the SR, and the LDN to evaluate their respective effectiveness, with quantitative results on GOPRO presented in Tab. 3. We employ bilinear interpolation as a substitute for the SR network in experiments I-1 and I-2 to align the spatial resolution of two modalities. ", "page_idx": 8}, {"type": "text", "text": "Qualitative ablation experiments are illustrated in Fig. 6. These comparisons reveal that while the SDM effectively removes motion blur, it struggles with significant noise and detail loss due to the spike noise and the low resolution of the spike stream. While the BSN mitigates noise and the SR network improves spatial resolution explicitly, the LDN trained via distillation learning further refines these enhancements, enabling the recognition of intricate textural features in the images, such as the license plate and the door number shown in Fig. 6. ", "page_idx": 8}, {"type": "text", "text": "Network Architecture. While our designed LDN mirrors the Scale-aware Network (SAN) proposed in the GEM [39], we replace the SAN with the LDN to compare the performance difference between the two architectures as depicted in Tab. 2. In the self-supervised learning framework discussed in this paper, enhancing restoration performance primarily hinges on improving the quality of pseudolabels rather than the network architecture itself. Despite its simple design, which consists only of convolutional layers, ResBlocks, and basic modules such as CBAM [28], LDN outperforms SAN in both PSNR and SSIM while requiring fewer parameters and less computation, demonstrating that LDN is both sufficient and efficient for the self-supervised spike-guided motion deblurring task. ", "page_idx": 8}, {"type": "image", "img_path": "9FYat8HPpv/tmp/a88840611edae80116a2fbb224589ccb515ba1f4769533cae027089ba8d59826.jpg", "img_caption": ["Figure 6: Modules cascading comparisons on GO PRO. Experiments ID can be viewed on Tab. 3. ", "Figure 7: Distillation learning comparisons on RSB. Experiments ID can be viewed on Tab. 4 "], "img_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "9FYat8HPpv/tmp/c3bae313fa3731dc0b91a058d67cd11c1d1c4912cae429fb20a1c3ac51b3f701.jpg", "table_caption": ["Table 3: Modules cascading ablation on GOPRO. ", "Table 4: Distillation learning ablation on GOPRO. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Distillation Learning. We focus on analyzing the contribution of the teacher loss $\\mathcal{L}_{\\mathrm{tea}}$ and the reblur loss ${\\mathcal{L}}_{\\mathrm{reblur}}$ within the distillation framework, with quantitative results listed in Tab. 4. Without the teacher loss $\\mathcal{L}_{\\mathrm{tea}}$ , the LDN tends toward learning identity mapping from the blurry input. While under the guidance of the teacher model, the reblur loss ${\\mathcal{L}}_{\\mathrm{reblur}}$ not only enforces motion consistency in the reconstructed sequence but also enriches the LDN with high-resolution details from the non-blurry regions of the input, thus improving the performance on GOPRO as listed in Tab. 4. ", "page_idx": 9}, {"type": "text", "text": "We further apply the LDN trained on GOPRO to the real-world dataset RSB, with qualitative visualization illustrated in Fig. 7. As observed in the figure, the absence of the reblur loss ${\\mathcal{L}}_{\\mathrm{reblur}}$ leads to significant noise in the recovered image, which predominantly arises from the disparity in spike density and generation mechanism between the simulated and real-captured spike stream. This discrepancy causes the LDN to overestimate the spike number, resulting in black holes in regions with lower spike density than simulated, which reflects the drawback inherent in the supervised learning strategies as discussed in Sec. 1. Increasing the weight of the reblur loss ${\\mathcal{L}}_{\\mathrm{reblur}}$ allows the LDN to incorporate more information from the blurry input, thereby mitigating this issue. However, this adjustment also leads to the presence of blurry edges. We follow the parameters set in Experiment II-6 and retrain the LDN on the RSB dataset (referred to as Experiment II-7). The retrained LDN effectively recovers the sharp edge of the calibration board and suppresses the spike noise in the background, validating the feasibility of our self-supervised framework in real-world scenarios. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduce a novel self-supervised spike-guided motion deblurring framework S-SDM, which reconstructs sequences of sharp images from real-world blurry inputs with the spike stream. Additionally, we construct an RGB-Spike binocular system and propose the first spatially-temporally calibrated real-world dataset RSB in this community. Quantitative and qualitative experiments validate the superior generalization capabilities of our proposed S-SDM. ", "page_idx": 9}, {"type": "text", "text": "Limitation. The limitation of our S-SDM lies in its dependence on strict spatial-temporal calibration.   \nMisalignment will lead to color shifts and quality degradation in the deblurred sequence. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We sincerely appreciate Yuyan Chen (HUST) for her valuable suggestions and for polishing the figures. This work was supported by the National Natural Science Foundation of China (62176003, 62088102, 62306015), the China Postdoctoral Science Foundation (2023T160015), the Young Elite Scientists Sponsorship Program by CAST (2023QNRC001), and the Beijing Nova Program (20230484362). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jaeseok Byun, Sungmin Cha, and Taesup Moon. Fbi-denoiser: Fast blind image denoiser for poissongaussian noise. In CVPR, pages 5768\u20135777, 2021.   \n[2] Yakun Chang, Chu Zhou, Yuchen Hong, Liwen Hu, Chao Xu, Tiejun Huang, and Boxin Shi. 1000 fps hdr video with a spike-rgb hybrid camera. In CVPR, pages 22180\u201322190, 2023.   \n[3] Haoyu Chen, Minggui Teng, Boxin Shi, Yizhou Wang, and Tiejun Huang. A residual learning approach to deblur and generate high frame rate video with an event camera. IEEE TMM, 2022.   \n[4] Kang Chen and Lei Yu. Motion deblur by learning residual from events. IEEE TMM, 2024.   \n[5] Shiyan Chen, Chaoteng Duan, Zhaofei Yu, Ruiqin Xiong, and Tiejun Huang. Self-supervised mutual learning for dynamic scene reconstruction of spiking camera. IJCAI, 2022.   \n[6] Shiyan Chen, Zhaofei Yu, and Tiejun Huang. Self-supervised joint dynamic scene reconstruction and optical flow estimation for spiking camera. In AAAI, volume 37, pages 350\u2013358, 2023. [7] Shiyan Chen, Jiyuan Zhang, Zhaofei Yu, and Tiejun Huang. Exploring asymmetric tunable blind-spots for self-supervised denoising in real-world scenarios. arXiv preprint arXiv:2303.16783, 2023.   \n[8] Shiyan Chen, Jiyuan Zhang, Yajing Zheng, Tiejun Huang, and Zhaofei Yu. Enhancing motion deblurring in high-speed scenes with spike streams. In NeurIPS, 2023.   \n[9] Guillermo Gallego, Tobi Delbr\u00fcck, Garrick Orchard, Chiara Bartolozzi, Brian Taba, Andrea Censi, Stefan Leutenegger, Andrew J Davison, J\u00f6rg Conradt, Kostas Daniilidis, et al. Event-based vision: A survey. IEEE TPAMI, 44(1):154\u2013180, 2020.   \n[10] Tiejun Huang, Yajing Zheng, Zhaofei Yu, Rui Chen, Yuan Li, Ruiqin Xiong, Lei Ma, Junwei Zhao, Siwei Dong, Lin Zhu, et al. $1000\\times$ faster camera and machine vision with ordinary devices. Engineering, 25:110\u2013119, 2023.   \n[11] Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, and Shuchang Zhou. Real-time intermediate flow estimation for video frame interpolation. In ECCV, pages 624\u2013642. Springer, 2022.   \n[12] Meiguang Jin, Givi Meishvili, and Paolo Favaro. Learning to extract a video sequence from a single motion-blurred image. In CVPR, pages 6334\u20136342, 2018.   \n[13] Samuli Laine, Tero Karras, Jaakko Lehtinen, and Timo Aila. High-quality self-supervised deep image denoising. NeurIPS, 32, 2019.   \n[14] Wooseok Lee, Sanghyun Son, and Kyoung Mu Lee. Ap-bsn: Self-supervised denoising for real-world images via asymmetric pd and blind-spot network. In CVPR, pages 17725\u201317734, 2022.   \n[15] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual networks for single image super-resolution. In CVPR, pages 136\u2013144, 2017.   \n[16] Songnan Lin, Jiawei Zhang, Jinshan Pan, Zhe Jiang, Dongqing Zou, Yongtian Wang, Jing Chen, and Jimmy Ren. Learning event-driven video deblurring and interpolation. In ECCV, pages 695\u2013710. Springer, 2020.   \n[17] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep multi-scale convolutional neural network for dynamic scene deblurring. In CVPR, July 2017.   \n[18] Liyuan Pan, Cedric Scheerlinck, Xin Yu, Richard Hartley, Miaomiao Liu, and Yuchao Dai. Bringing a blurry frame alive at high frame-rate with an event camera. In CVPR, pages 6820\u20136829, 2019.   \n[19] Bryan Peterson. Understanding exposure: how to shoot great photographs with any camera. AmPhoto books, 2016.   \n[20] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234\u2013241. Springer, 2015.   \n[21] Teresa Serrano-Gotarredona and Bernab\u00e9 Linares-Barranco. A $128\\times128\\;1.5\\%$ contrast sensitivity $0.9\\%$ fpn 3 \u00b5s latency 4 mw asynchronous frame-free dynamic vision sensor using transimpedance preamplifiers. IEEE Journal of Solid-State Circuits, 48(3):827\u2013838, 2013.   \n[22] Hyeonjun Sim, Jihyong Oh, and Munchurl Kim. Xvf:i extreme video frame interpolation. In ICCV, pages 14489\u201314498, 2021.   \n[23] Chen Song, Qixing Huang, and Chandrajit Bajaj. E-cir: Event-enhanced continuous intensity recovery. In CVPR, pages 7803\u20137812, 2022.   \n[24] Lei Sun, Christos Sakaridis, Jingyun Liang, Qi Jiang, Kailun Yang, Peng Sun, Yaozu Ye, Kaiwei Wang, and Luc Van Gool. Event-based fusion for motion deblurring with cross-modal attention. In ECCV, pages 412\u2013428. Springer, 2022.   \n[25] Lei Sun, Christos Sakaridis, Jingyun Liang, Peng Sun, Jiezhang Cao, Kai Zhang, Qi Jiang, Kaiwei Wang, and Luc Van Gool. Event-based frame interpolation with ad-hoc deblurring. In CVPR, pages 18043\u201318052, 2023.   \n[26] Longguang Wang, Yingqian Wang, Xiaoyu Dong, Qingyu Xu, Jungang Yang, Wei An, and Yulan Guo. Unsupervised degradation representation learning for blind super-resolution. In CVPR, pages 10581\u201310590, 2021.   \n[27] Zejin Wang, Jiazheng Liu, Guoqing Li, and Hua Han. Blind2unblind: Self-supervised image denoising with visible blind spots. In CVPR, pages 2027\u20132036, 2022.   \n[28] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module. In ECCV, pages 3\u201319, 2018.   \n[29] Fang Xu, Lei Yu, Bishan Wang, Wen Yang, Gui-Song Xia, Xu Jia, Zhendong Qiao, and Jianzhuang Liu. Motion deblurring with real events. In ICCV, pages 2583\u20132592, 2021.   \n[30] Jiyuan Zhang, Kang Chen, Shiyan Chen, Yajing Zheng, Tiejun Huang, and Zhaofei Yu. Spikegs: 3d gaussian splatting from spike streams with high-speed camera motion. arXiv preprint arXiv:2407.10062, 2024.   \n[31] Jiyuan Zhang, Shiyan Chen, Yajing Zheng, Zhaofei Yu, and Tiejun Huang. Unveiling the potential of spike streams for foreground occlusion removal from densely continuous views. arXiv preprint arXiv:2307.00821, 2023.   \n[32] Jiyuan Zhang, Shiyan Chen, Yajing Zheng, Zhaofei Yu, and Tiejun Huang. Spike-guided motion deblurring with unknown modal spatiotemporal alignment. In CVPR, pages 25047\u201325057, 2024.   \n[33] Jiyuan Zhang, Shanshan Jia, Zhaofei Yu, and Tiejun Huang. Learning temporal-ordered representation for spike streams based on discrete wavelet transforms. In AAAI, volume 37, pages 137\u2013147, 2023.   \n[34] Jiyuan Zhang, Lulu Tang, Zhaofei Yu, Jiwen Lu, and Tiejun Huang. Spike transformer: Monocular depth estimation for spiking camera. In ECCV, pages 34\u201352. Springer, 2022.   \n[35] Kaihao Zhang, Wenqi Ren, Wenhan Luo, Wei-Sheng Lai, Bj\u00f6rn Stenger, Ming-Hsuan Yang, and Hongdong Li. Deep image deblurring: A survey. IJCV, 130(9):2103\u20132130, 2022.   \n[36] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, pages 586\u2013595, 2018.   \n[37] Weixia Zhang, Guangtao Zhai, Ying Wei, Xiaokang Yang, and Kede Ma. Blind image quality assessment via vision-language correspondence: A multitask learning perspective. In CVPR, pages 14071\u201314081, 2023.   \n[38] Xiang Zhang and Lei Yu. Unifying motion deblurring and frame interpolation with events. In CVPR, pages 17765\u201317774, 2022.   \n[39] Xiang Zhang, Lei Yu, Wen Yang, Jianzhuang Liu, and Gui-Song Xia. Generalizing event-based motion deblurring in real-world scenarios. In ICCV, pages 10734\u201310744, 2023.   \n[40] Youjian Zhang, Chaoyue Wang, Stephen J Maybank, and Dacheng Tao. Exposure trajectory recovery from motion blur. PAMI, 44(11):7490\u20137504, 2021.   \n[41] Jing Zhao, Ruiqin Xiong, Hangfan Liu, Jian Zhang, and Tiejun Huang. Spk2imgnet: Learning to reconstruct dynamic scene from continuous spike stream. In CVPR, pages 11996\u201312005, 2021.   \n[42] Junwei Zhao, Shiliang Zhang, Lei Ma, Zhaofei Yu, and Tiejun Huang. Spikingsim: A bio-inspired spiking simulator. In ISCAS, pages 3003\u20133007. IEEE, 2022.   \n[43] Rui Zhao, Ruiqin Xiong, Jing Zhao, Zhaofei Yu, Xiaopeng Fan, and Tiejun Huang. Learning optical flow from continuous spike streams. NeurIPS, 35:7905\u20137920, 2022.   \n[44] Yajing Zheng, Zhaofei Yu, Song Wang, and Tiejun Huang. Spike-based motion estimation for object tracking through bio-inspired unsupervised learning. IEEE TIP, 32:335\u2013349, 2022.   \n[45] Yajing Zheng, Lingxiao Zheng, Zhaofei Yu, Boxin Shi, Yonghong Tian, and Tiejun Huang. High-speed image reconstruction through short-term plasticity for spiking cameras. In CVPR, pages 6358\u20136367, 2021.   \n[46] Zhihang Zhong, Mingdeng Cao, Xiang Ji, Yinqiang Zheng, and Imari Sato. Blur interpolation transformer for real-world motion from blur. In CVPR, pages 5713\u20135723, 2023.   \n[47] Lin Zhu, Siwei Dong, Tiejun Huang, and Yonghong Tian. A retina-inspired sampling method for visual texture reconstruction. In ICME, pages 1432\u20131437. IEEE, 2019.   \n[48] Lin Zhu, Siwei Dong, Jianing Li, Tiejun Huang, and Yonghong Tian. Retina-like visual image reconstruction via spiking neural model. In CVPR, pages 1438\u20131446, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "image", "img_path": "9FYat8HPpv/tmp/48963935a8c03bc2ab064c350e23de893987c715ae9851ff5d564ebd006a2683.jpg", "img_caption": ["Figure 8: Video comparison of our method against the BiT and SpkDeblurNet on the RSB dataset under different luminance conditions, with the static visualization displayed in Fig. 13. It is recommended to view the pdf using Acrobat PDF reader and the gif demonstration with a higher frame rate is available in the supplementary zip file. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "This supplementary material provides a comprehensive elaboration on the methodologies and experiments in this paper. It is organized into four distinct sections: Theory Analysis in appendix A.1, Network Settings in appendix A.2, RSB Dataset in appendix A.3, Experimental Details in appendix A.4 and Additional Figure in appendix A.5. ", "page_idx": 12}, {"type": "text", "text": "A.1 Theory Analysis ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We define $k_{1}^{\\mathbf{B}}$ and $k_{2}^{\\mathbf{B}}$ as the ratios of the red channel to the green and blue channels in the blurry input $\\mathbf{B}$ respectively, i.e., ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{k_{1}^{\\mathbf{B}}=\\mathbf{B}_{r}/\\mathbf{B}_{g r e},}\\\\ &{k_{2}^{\\mathbf{B}}=\\mathbf{B}_{r}/\\mathbf{B}_{b}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Since the gray image is the weighted sum of RGB channels, the ratio of the gray to the red image $\\alpha_{r}^{\\mathbf{B}}$ is formulated as: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{r}^{\\mathbf{B}}=\\mathbf{B}_{g}/\\mathbf{B}_{r}}\\\\ &{\\qquad=(w_{r}\\cdot\\mathbf{B}_{r}+w_{g r e}\\cdot\\mathbf{B}_{g r e}+w_{b}\\cdot\\mathbf{B}_{b})/\\mathbf{B}_{r}}\\\\ &{\\qquad=w_{r}+w_{g r e}/k_{1}^{\\mathbf{B}}+w_{b}/k_{2}^{\\mathbf{B}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $w_{c}$ denote the weight of channel $c\\in\\{r,g r e,b\\}$ respectively. ", "page_idx": 12}, {"type": "text", "text": "Similarly, we define $k_{1}^{\\mathbf{E}}(t,\\mathcal{T}^{\\prime})$ and $k_{2}^{\\mathbf{E}}(t,\\mathcal{T}^{\\prime})$ as the fractions of the red channel relative to the green and blue channels in the short-exposure image $\\mathbf{E}(t,\\mathcal{T}^{\\prime})$ , resulting in: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\alpha_{r}^{\\bf E}(t,T^{\\prime})=w_{r}+w_{g r e}/k_{1}^{\\bf E}(t,T^{\\prime})+w_{b}/k_{2}^{\\bf E}(t,T^{\\prime}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Given the observation that the color information of adjacent pixels often exhibits similarity under the premise of minor motion amplitude, we postulate that the colors of the blurry input $\\mathbf{B}$ and the shortexposure image $\\mathbf{E}(t,\\mathcal{T}^{\\prime})$ are identical. This assumption implies that the intensity proportion among RGB channels in two images is approximately equivalent, i.e., $k_{1}^{\\mathbf{B}}\\approx k_{1}^{\\mathbf{E}}(t,T^{\\prime})$ and $k_{2}^{\\dot{\\mathbf{B}}}\\approx k_{2}^{\\mathbf{E}}(t,T^{\\prime})$ Drawing from Eq. (19) and Eq. (20), We further deduce the following relation: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\alpha_{r}^{\\bf B}\\approx\\alpha_{r}^{\\bf E}(t,{\\mathcal T}^{\\prime}),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "which can be readily generalized across channels, leading to $\\alpha_{c}^{\\bf B}\\approx\\alpha_{c}^{\\bf E}(t,\\mathcal{T}^{\\prime})$ . ", "page_idx": 12}, {"type": "image", "img_path": "9FYat8HPpv/tmp/fc3381c7fb34be85c6e7a3722b9a59b28f0ae4c24e04347092f7e46b9c32d99e.jpg", "img_caption": ["Figure 9: Network diagrams of the BSN (a) and our designed LDN (b). "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.2 Network Settings ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.2.1 Blind Spot Network ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We construct our BSN based on the blind-spot strategy outlined in [13]. We first rotate the input image four times, then concatenate them into the U-Net [20] structure like denoising network. To prevent direct mapping from the input pixel to the output pixel, a single-pixel offset strategy is employed in the convolutional kernel to separate its receptive field from the central pixel. Finally, the denoising output is obtained by merging the results of four branches via a $1\\times1$ convolution, as shown in Fig. 9(a). ", "page_idx": 13}, {"type": "text", "text": "A.2.2 Lightweight Deblur Network ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The network structure is depicted in Fig. 9(b), where the encoder for blurry images consists of two layers of down-sampling convolutions to align the spatial resolution of two modalities. In contrast to the intricate cross-attention mechanism outlined in [4, 8], we implement the fusion of two modalities by the simple operation of Concat(\u00b7), with the cascaded CBAM [28] and residual blocks for further feature fusion. ", "page_idx": 13}, {"type": "text", "text": "A.3 RSB Dataset ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We detail the construction of our RGB-Spike hybrid system as illustrated in Fig. 10. The system comprises a Spike Camera-001T-Gen2 with a resolution of $400\\times250$ pixels, paired with a Basler acA1920-150uc RGB Camera, offering a higher resolution of $1920\\times1200$ pixels. ", "page_idx": 13}, {"type": "image", "img_path": "9FYat8HPpv/tmp/a02caf1a4b6d3e139ad0ccae3f322c539e200c7493f9b4de7511b6b21171bccb.jpg", "img_caption": ["Figure 10: RGB-Spike camera system. ", "Figure 11: Calibration result. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Table 5: Quantitative comparison of the single frame task on the RSB dataset, where non-reference metric LIQE ranging from 1 to 5 is employed. LIQE is a positive metric denoted as $\\uparrow$ where higher scores reflect better performance. ", "page_idx": 14}, {"type": "table", "img_path": "9FYat8HPpv/tmp/e16ecb62cd6cfa4754316b8c33f62f685e5bcee551b76fa35e80e480663ad7dd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "9FYat8HPpv/tmp/facb32c677306ff9f0645b8d2c0e653be020f1e9273cea432afcecb29a43cfa9.jpg", "img_caption": ["Figure 12: Qualitative comparisons for single-frame restoration on the RSB dataset are illustrated, where \u201cBoard-L\u201d, \u201cBoard-M\u201d, and \u201cBoard-H\u201d represent the board captured under low, middle and high lighting conditions. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "To achieve the spatial calibration of two cameras, we perform simultaneous captures of the calibration board using the RGB-Spike system as shown in Fig. 10. The RGB images are cropped to a resolution of $1600\\,\\times\\,1000$ , aligning them to be fourfold the resolution of the spike camera. We convert the cropped RGB image to grayscale and take the TFP [47] image reconstruction result as the reference from the spike camera. We utilize the MATLAB calibration toolbox to implement the calibration process, with the results detailed in Fig. 11. ", "page_idx": 14}, {"type": "text", "text": "Our RSB dataset contains 10 video sequences under different conditions, captured under varied conditions including scene brightness levels (e.g., Low, Middle, and High light) and motion patterns (e.g., camera shake and object motion), which introduce different types of motion blur. Besides, the RSB dataset comprises a large amount of blur-spike pairs with each blurry input corresponding to 400 spike frames. ", "page_idx": 14}, {"type": "text", "text": "A.4 Experimental Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.4.1 Comparison ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To assess the performance of our method on the GOPRO dataset, we utilize the Peak Signal-toNoise Ratio (PSNR) and the Structural Similarity Index (SSIM) as the quantitative metrics, which are commonly used in motion deblurring tasks. In real-world datasets, where obtaining ground truth sharp sequence is challenging, we opt for the non-reference image quality assessment method Language-Image Quality Evaluator (LIQE) [37] as a reference. By assessing visual quality through the computation of joint probabilities from visual-textual embeddings, LIQE adeptly identifies the clarity and blurriness of images independently of the ground truth, making it ideally suited for our task. ", "page_idx": 14}, {"type": "image", "img_path": "9FYat8HPpv/tmp/c0d5a0350e67906b2cd5f78a42b6e39b085f83aac9511060f5f59bcc2ba29dc6.jpg", "img_caption": ["Figure 13: Qualitative comparison for the sequence reconstruction on the RSB dataset. (a),(b),(c),(d) denote results of \u201cLEVS\", \u201cMoiton-ETR\" , \u201cBiT\" and \u201cSpkDeblurNet\" respectively. The upper panel depicts a real-world scene with a lower spike density than the simulation, whereas the lower image exhibits a higher spike density. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Comparison on the RSB dataset. Qualitative and quantitative experiments of the single frame restoration task on the RSB dataset are shown in Fig. 12 and Tab. 5 respectively. The visual results coupled with the LIQE metrics demonstrate that our method outperforms other methods in handling the real-world RSB dataset. While the supervised SpkDeblurNet encounters substantial noise and overexposure challenges in both low-light and high-light environments, our approach demonstrates superior restoration performance, which is attributed to the designed self-supervised framework. ", "page_idx": 15}, {"type": "image", "img_path": "9FYat8HPpv/tmp/d824fec2e7ffb8b1992730cd8b4fc9d11a75a4d88e9861077b1b865338c0932c.jpg", "img_caption": ["Figure 14: Comparison of our S-SDM against other methods on GOPRO with $V_{t h}=1,2,4$ . "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "9FYat8HPpv/tmp/e1e63660c9331b7ddb0031aaa83145b651f56ba64895d99415fefae6647e309a.jpg", "img_caption": ["Figure 15: Comparison of our S-SDM against other methods on GOPRO with $V_{t h}=1,2,4$ . "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Moreover, as depicted in Fig. 13 and Fig. 8, S-SDM exhibits outstanding performance in precisely retrieving luminance information and texture details while ensuring the motion consistency of reconstructed sequences. In contrast, the video reconstruction of the BiT suffers from poor image quality and shows inadequate sequence continuity in the restored sequence. Besides, SpkDeblurNet encounters issues with color distortion, brightness inconsistency, and inaccurate texture restoration owing to the domain gap between synthetic and real-world datasets. These observations further highlight the superior performance of our S-SDM in real-world scenarios. ", "page_idx": 16}, {"type": "text", "text": "Comparison on the GOPRO dataset. We provide additional visual comparison of our method against other SOTA methods on the GOPRO dataset as shown in Fig. 14 and 15. ", "page_idx": 16}, {"type": "text", "text": "A.4.2 Implementation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To augment the dataset and accelerate the training process, we randomly crop $512\\times512$ image from each blurry frame, along with the $128\\times128$ spike stream. We use PyTorch to build and train our S-SDM using an NVIDIA GeForce GTX 4090 GPU and AMD EPYC 7742 64-Core Processor. The training of our LDN consumes about 4 hours on the GOPRO. During the testing phase, we feed the entire image and the spike stream into the network to assess performance. ", "page_idx": 16}, {"type": "text", "text": "We complete the training of BSN on the GOPRO dataset, employing an initial learning rate of $3e^{-4}$ and spanning 1000 epochs. The training uses the Adam optimizer with a cosine scheduler and sets the batch size to 8 for each epoch. Adopting the same settings as BSN, EDSR is trained on the blur-spike ", "page_idx": 16}, {"type": "image", "img_path": "9FYat8HPpv/tmp/47e813623f2e9a6013b7257eaad942e3575a45843cc9c76ca8a42769b7f7022c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 16: Ablation study for evaluating modules of S-SDM on the GOPRO dataset. Experiments corresponding to the ID can be viewed through Tab. 3. ", "page_idx": 17}, {"type": "image", "img_path": "9FYat8HPpv/tmp/dd38827e36b0083ead9e42714f83d7b1ac5a7e2b0a6c2b90284d35162e92e178.jpg", "img_caption": ["Figure 17: Ablation study for evaluating the effectiveness of our distillation learning framework on the RSB dataset. The details corresponding to the experiment ID can be viewed through the Tab. 4. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "paired data for 70 epochs. Subsequently, LDN undergoes the training of 100 epochs with the learning rate adjusted to $1e^{\\bar{-}3}$ . ", "page_idx": 17}, {"type": "text", "text": "A.4.3 Ablation study ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We provide additional ablation visualizations to demonstrate the effectiveness of our designed modules and the distillation framework as shown in Fig. 16 and 17. ", "page_idx": 17}, {"type": "text", "text": "A.5 Additional Figure ", "page_idx": 17}, {"type": "image", "img_path": "9FYat8HPpv/tmp/a185d6afa8e68314d325012ade0ee280695db48362fa5e5ba10cfe1bbfbc922f.jpg", "img_caption": ["Figure 18: The schematic diagram of our designed SDM. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See lines 72-77. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: See lines 298-299. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: This paper provided the theoretical analysis on the spike-guided motion deblurring task. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Experiments are reproduceable. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We have released the code. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Yes, they are included in the supplementary materials. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: We don\u2019t report it. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, they are included in the supplementary materials. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, we follow the NeurIPS Code of Ethics. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 21}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Yes, they are correctly used. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]