[{"figure_path": "h1iMVi2iEM/tables/tables_3_1.jpg", "caption": "Table 2: Test accuracy on the CIFAR-10 / 100 dataset. We fix the total client C = 100 and P = 10 under training local 50 iterations. We test 3 setups of IID, Dir-1.0, and Dir-0.1 on each dataset. Each group is tested on LeNet (upper portion) and ResNet-18 (lower portion) models. Each results are tested with 4 different random seeds. \u201c-\u201d means can not stably converge. \"Family\" distinguishes whether the algorithm is a primal method (P) or a primal dual method (PD) and \"Local Opt\" distinguishes whether the algorithm adopts SGD-based or SAM-based local optimizer.", "description": "This table presents the test accuracy results on CIFAR-10 and CIFAR-100 datasets using different federated learning algorithms under various data distributions (IID, Dir-1.0, Dir-0.1). The table compares primal and primal-dual methods, along with SGD and SAM optimizers, showcasing their performance under different settings. The results are averaged across four random seeds.", "section": "Experiments"}, {"figure_path": "h1iMVi2iEM/tables/tables_7_1.jpg", "caption": "Table 2: Test accuracy on the CIFAR-10 / 100 dataset. We fix the total client C = 100 and P = 10 under training local 50 iterations. We test 3 setups of IID, Dir-1.0, and Dir-0.1 on each dataset. Each group is tested on LeNet (upper portion) and ResNet-18 (lower portion) models. Each results are tested with 4 different random seeds. \u201c-\u201d means can not stably converge. \"Family\" distinguishes whether the algorithm is a primal method (P) or a primal dual method (PD) and \"Local Opt\" distinguishes whether the algorithm adopts SGD-based or SAM-based local optimizer.", "description": "This table presents the test accuracy results on CIFAR-10 and CIFAR-100 datasets for various federated learning algorithms.  The experiments use 100 clients, with 10 active clients per round, and 50 local training iterations.  Three data distribution scenarios (IID, Dir-1.0, and Dir-0.1) representing different levels of data heterogeneity are evaluated.  Results are shown for LeNet and ResNet-18 models, differentiating between primal and primal-dual methods, and using either SGD or SAM as local optimizers.  The table highlights the performance variations under different settings.", "section": "Experiments"}, {"figure_path": "h1iMVi2iEM/tables/tables_14_1.jpg", "caption": "Table 2: Test accuracy on the CIFAR-10 / 100 dataset. We fix the total client C = 100 and P = 10 under training local 50 iterations. We test 3 setups of IID, Dir-1.0, and Dir-0.1 on each dataset. Each group is tested on LeNet (upper portion) and ResNet-18 (lower portion) models. Each results are tested with 4 different random seeds. \u201c-\u201d means can not stably converge. \"Family\" distinguishes whether the algorithm is a primal method (P) or a primal dual method (PD) and \"Local Opt\" distinguishes whether the algorithm adopts SGD-based or SAM-based local optimizer.", "description": "This table presents the test accuracy results on CIFAR-10 and CIFAR-100 datasets using different federated learning algorithms.  The experiments were conducted with 100 total clients, 10 active clients per round, and 50 local training iterations. Three data distributions are compared: IID, Dir-1.0 (low heterogeneity), and Dir-0.1 (high heterogeneity).  Both LeNet and ResNet-18 model architectures are used. The table shows the average accuracy and standard deviation (over 4 random seeds) for each algorithm and dataset. The \"Family\" column indicates whether the algorithm is a primal or primal-dual method, and the \"Local Opt\" column shows whether the algorithm uses SGD or SAM for local optimization.", "section": "5 Experiments"}, {"figure_path": "h1iMVi2iEM/tables/tables_18_1.jpg", "caption": "Table 2: Test accuracy on the CIFAR-10 / 100 dataset. We fix the total client C = 100 and P = 10 under training local 50 iterations. We test 3 setups of IID, Dir-1.0, and Dir-0.1 on each dataset. Each group is tested on LeNet (upper portion) and ResNet-18 (lower portion) models. Each results are tested with 4 different random seeds. \u201c-\u201d means can not stably converge. \"Family\" distinguishes whether the algorithm is a primal method (P) or a primal dual method (PD) and \"Local Opt\" distinguishes whether the algorithm adopts SGD-based or SAM-based local optimizer.", "description": "This table presents the test accuracy results on CIFAR-10 and CIFAR-100 datasets for various federated learning algorithms.  The experiments used 100 clients total, with 10 active clients per round, and 50 local training iterations. Three data distributions (IID, Dir-1.0, Dir-0.1) representing different levels of data heterogeneity were tested with LeNet and ResNet-18 models.  The table compares primal and primal-dual methods, and algorithms using SGD or SAM optimizers.  A '-' indicates that the algorithm did not converge stably.", "section": "5 Experiments"}, {"figure_path": "h1iMVi2iEM/tables/tables_19_1.jpg", "caption": "Table 2: Test accuracy on the CIFAR-10 / 100 dataset. We fix the total client C = 100 and P = 10 under training local 50 iterations. We test 3 setups of IID, Dir-1.0, and Dir-0.1 on each dataset. Each group is tested on LeNet (upper portion) and ResNet-18 (lower portion) models. Each results are tested with 4 different random seeds. \u201c-\u201d means can not stably converge. \"Family\" distinguishes whether the algorithm is a primal method (P) or a primal dual method (PD) and \"Local Opt\" distinguishes whether the algorithm adopts SGD-based or SAM-based local optimizer.", "description": "This table presents the test accuracy results for different federated learning algorithms on the CIFAR-10 and CIFAR-100 datasets.  The experiments used 100 clients, with 10 active per round, and 50 local training iterations. Three data distributions (IID, Dir-1.0, Dir-0.1) were tested. The algorithms are categorized by whether they are primal or primal-dual, and whether they utilize SGD or SAM optimizers.  Results are averaged across four random seeds, and a '-' indicates that the algorithm failed to converge.", "section": "5 Experiments"}, {"figure_path": "h1iMVi2iEM/tables/tables_19_2.jpg", "caption": "Table 6: Wall clock time required to train 1 round (100 iterations) on LeNet.", "description": "This table presents the wall-clock time in seconds required to train one round (100 iterations) using the LeNet model.  It compares the time taken by FedAvg, SCAFFOLD, FedSAM, FedDyn, FedSpeed, A-FedPD, and A-FedPDSAM.  The time for FedAvg is used as the baseline (1x), and the times for other methods are expressed relative to that baseline.", "section": "5.1 Experiments"}, {"figure_path": "h1iMVi2iEM/tables/tables_23_1.jpg", "caption": "Table 2: Test accuracy on the CIFAR-10 / 100 dataset. We fix the total client C = 100 and P = 10 under training local 50 iterations. We test 3 setups of IID, Dir-1.0, and Dir-0.1 on each dataset. Each group is tested on LeNet (upper portion) and ResNet-18 (lower portion) models. Each results are tested with 4 different random seeds. \u201c-\u201d means can not stably converge. \"Family\" distinguishes whether the algorithm is a primal method (P) or a primal dual method (PD) and \"Local Opt\" distinguishes whether the algorithm adopts SGD-based or SAM-based local optimizer.", "description": "This table presents the test accuracy results on CIFAR-10 and CIFAR-100 datasets using various federated learning algorithms.  The experiments used 100 clients total, with 10 active in each round and 50 local training iterations.  Three data distributions (IID, Dir-1.0, and Dir-0.1) were tested to assess performance under different levels of data heterogeneity.  The algorithms are categorized as either primal (P) or primal-dual (PD) methods and whether they use SGD or SAM for local optimization.  Results are averaged across four random seeds.", "section": "Experiments"}, {"figure_path": "h1iMVi2iEM/tables/tables_25_1.jpg", "caption": "Table 2: Test accuracy on the CIFAR-10 / 100 dataset. We fix the total client C = 100 and P = 10 under training local 50 iterations. We test 3 setups of IID, Dir-1.0, and Dir-0.1 on each dataset. Each group is tested on LeNet (upper portion) and ResNet-18 (lower portion) models. Each results are tested with 4 different random seeds. \u201c-\u201d means can not stably converge. \"Family\" distinguishes whether the algorithm is a primal method (P) or a primal dual method (PD) and \"Local Opt\" distinguishes whether the algorithm adopts SGD-based or SAM-based local optimizer.", "description": "This table presents the test accuracy results on CIFAR-10 and CIFAR-100 datasets for various federated learning algorithms under different data distributions (IID, Dir-1.0, Dir-0.1) and model architectures (LeNet, ResNet-18).  The results are categorized by algorithm family (primal or primal-dual) and local optimizer (SGD or SAM).  Each result is averaged over four random seeds, and a '-' indicates that the algorithm did not stably converge for that specific setting.", "section": "5 Experiments"}, {"figure_path": "h1iMVi2iEM/tables/tables_28_1.jpg", "caption": "Table 2: Test accuracy on the CIFAR-10 / 100 dataset. We fix the total client C = 100 and P = 10 under training local 50 iterations. We test 3 setups of IID, Dir-1.0, and Dir-0.1 on each dataset. Each group is tested on LeNet (upper portion) and ResNet-18 (lower portion) models. Each results are tested with 4 different random seeds. \u201c-\u201d means can not stably converge. \"Family\" distinguishes whether the algorithm is a primal method (P) or a primal dual method (PD) and \"Local Opt\" distinguishes whether the algorithm adopts SGD-based or SAM-based local optimizer.", "description": "This table presents the test accuracy results on CIFAR-10 and CIFAR-100 datasets using different federated learning algorithms.  The experiments used 100 clients in total with 10 active clients per round. Three data distributions are compared (IID, Dir-1.0, Dir-0.1), reflecting varying levels of data heterogeneity.  Both LeNet and ResNet-18 are used as model backbones. The table also categorizes methods as primal (P) or primal-dual (PD) and specifies the local optimizer used (SGD or SAM).", "section": "5 Experiments"}]