[{"figure_path": "t7euV5dl5M/figures/figures_2_1.jpg", "caption": "Figure 1: (Left.) Fitting an SVGP model with only m = 4 inducing points sacrifices modeling areas of high EI (few data points at right) because the ELBO focuses only on global data approximation (left data) and is ignorant of the downstream decision making task. (Middle.) Because of this, (normalized) EI with the SVGP model peaks in an incorrect location relative to the exact posterior. (Right.) Updating the GP fit and selecting a candidate jointly using the EULBO (our method) results in candidate selection much closer to the exact model.", "description": "This figure compares three different approaches to Bayesian Optimization using Sparse Variational Gaussian Processes (SVGPs). The left panel shows an SVGP fit with only 4 inducing points, illustrating how the standard ELBO objective focuses only on global data approximation and neglects the downstream decision-making task of selecting the next point with high expected improvement (EI). The middle panel demonstrates that the resulting approximate EI peaks at an incorrect location compared to the exact EI. The right panel illustrates how the proposed utility-calibrated variational inference framework (EULBO) addresses this issue by jointly optimizing the GP approximation and data acquisition, resulting in a candidate selection closer to the exact model.", "section": "Approximation-Aware Bayesian Optimization"}, {"figure_path": "t7euV5dl5M/figures/figures_7_1.jpg", "caption": "Figure 2: Optimization results on the 8 considered tasks. We compare all methods for both standard BO and TuRBO-based BO (on all tasks except Hartmann). Each line/shaded region represents the mean/standard error over 20 runs. See subsection B.1 for additional molecule results.", "description": "This figure compares the performance of different Bayesian Optimization (BO) algorithms across eight benchmark tasks.  The algorithms include standard BO and TuRBO (Trust Region Bayesian Optimization) using various acquisition functions (EI and KG) and different methods for approximating the posterior distribution (ELBO, Moss et al., exact). The plot shows the mean reward achieved versus the number of function evaluations.  Shaded areas represent the standard error over 20 independent runs.  Additional results for the molecule tasks are provided in the supplementary material.", "section": "4 Experiments"}, {"figure_path": "t7euV5dl5M/figures/figures_8_1.jpg", "caption": "Figure 2: Optimization results on the 8 considered tasks. We compare all methods for both standard BO and TuRBO-based BO (on all tasks except Hartmann). Each line/shaded region represents the mean/standard error over 20 runs. See subsection B.1 for additional molecule results.", "description": "The figure presents optimization results for eight benchmark tasks using different Bayesian Optimization methods.  It compares the performance of standard Bayesian optimization (BO) with Trust Region Bayesian Optimization (TuRBO) across four different acquisition functions: EULBO EI (all parameters), EULBO EI (variational parameters + inducing points), EULBO EI (variational parameters only), and ELBO EI.  The results illustrate the mean and standard error across 20 runs for each method, highlighting the relative performance of the different approaches on high and low-dimensional problems. Additional results for molecular design tasks are available in subsection B.1.", "section": "4 Experiments"}, {"figure_path": "t7euV5dl5M/figures/figures_16_1.jpg", "caption": "Figure 4: Additional optimization results on three molecule tasks using 10,000 random molecules from the GuacaMol dataset as initialization. Each line/shaded region represents the mean/standard error over 20 runs. We count oracle calls starting after these initialization evaluations for all methods.", "description": "This figure shows the optimization results for three molecule tasks (Osimertinib MPO, Fexofenadine MPO, Median Molecules 1).  Unlike Figure 2, these experiments used 10,000 random molecules from the GuacaMol dataset for initialization before starting the optimization process.  The plot compares different optimization methods (EULBO EI, EULBO KG, ELBO EI, and Moss et al. 2023 EI) using TuRBO, showing mean reward versus the number of oracle calls (function evaluations). Error bars represent the standard error across 20 runs.", "section": "4.2 Optimization Results"}, {"figure_path": "t7euV5dl5M/figures/figures_16_2.jpg", "caption": "Figure 2: Optimization results on the 8 considered tasks. We compare all methods for both standard BO and TuRBO-based BO (on all tasks except Hartmann). Each line/shaded region represents the mean/standard error over 20 runs See subsection B.1 for additional molecule results.", "description": "The figure shows optimization results for eight benchmark tasks using various Bayesian optimization methods.  The methods compared include EULBO with EI and KG acquisition functions, ELBO with EI, the method from Moss et al. 2023 with EI, and exact EI.  Both standard Bayesian optimization and TURBO (Trust Region Bayesian Optimization) are compared.  Each line represents the mean reward across 20 runs, with shaded regions showing the standard error.  The x-axis represents the number of function evaluations, and the y-axis represents the mean reward.  The results show that EULBO generally outperforms the other methods.", "section": "4 Experiments"}, {"figure_path": "t7euV5dl5M/figures/figures_17_1.jpg", "caption": "Figure 2: Optimization results on the 8 considered tasks. We compare all methods for both standard BO and TuRBO-based BO (on all tasks except Hartmann). Each line/shaded region represents the mean/standard error over 20 runs. See subsection B.1 for additional molecule results.", "description": "This figure compares the performance of eight different Bayesian Optimization methods across eight benchmark tasks.  The methods include variations using standard Bayesian Optimization (BO) and Trust Region Bayesian Optimization (TuRBO).  Different acquisition functions (EI and KG) and sparse Gaussian process models (SVGP) with varying approximations are also included.  The shaded regions represent the standard error over 20 runs for each method, showing the mean performance of each optimization algorithm.", "section": "4 Experiments"}, {"figure_path": "t7euV5dl5M/figures/figures_17_2.jpg", "caption": "Figure 7: Ablating the number of inducing points used by EULBO-SVGP and ELBO-SVGP. As in Fig. 2, we compare running TuRBO with EULBO-SVGP and with ELBO-SVGP using m = 100 inducing points used for both methods. We add two additional curves for TURBO with EULBO-SVGP and TuRBO with ELBO-SVGP using m = 1024 inducing points. Each line/shaded region represents the mean/standard error over 20 runs.", "description": "This figure shows the ablation study of the number of inducing points used in the EULBO-SVGP and ELBO-SVGP methods. It compares the performance of TuRBO with both methods using 100 and 1024 inducing points on the Lasso DNA task. The results show that the number of inducing points has a limited impact on the overall performance, and EULBO-SVGP consistently outperforms ELBO-SVGP regardless of the number of inducing points.", "section": "4.3 Ablation Study"}, {"figure_path": "t7euV5dl5M/figures/figures_18_1.jpg", "caption": "Figure 3: Ablation study measuring the impact of EULBO optimization on various SVGP parameters. At each BO iteration, we use the standard ELBO objective to optimize the SVGP hyperparameters, variational parameters, and inducing point locations. We then refine some subset of these parameters by further optimizing them with respect to the EULBO objective.", "description": "This ablation study investigates the effect of using EULBO optimization on different subsets of SVGP parameters (variational parameters only, inducing points only, or all parameters).  It compares the performance of these variations against the standard ELBO approach on four tasks to show the impact of joint optimization on various aspects of the model.", "section": "4.3 Ablation Study"}]