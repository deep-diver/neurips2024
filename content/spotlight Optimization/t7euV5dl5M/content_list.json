[{"type": "text", "text": "Approximation-Aware Bayesian Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Natalie Maus Kyurae Kim Geof fPleiss University of Pennsylvania University of Pennsylvania University of British Columbia nmaus $@$ seas.upenn.edu Vector Institute ", "page_idx": 0}, {"type": "text", "text": "David Eriksson John P. Cunningham Jacob R. Gardner Meta Columbia University University of Pennsylvania ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "High-dimensional Bayesian optimization (BO) tasks such as molecular design often require ${>}10{,}000$ function evaluations before obtaining meaningful results. While methods like sparse variational Gaussian processes (SVGPs) reduce computational requirements in these settings, the underlying approximations result in suboptimal data acquisitions that slow the progress of optimization. In this paper we modify SVGPs to better align with the goals of BO: targeting informed data acquisition rather than global posterior fidelity. Using the framework of utility-calibrated variational inference, we unify GP approximation and data acquisition into a joint optimization problem, thereby ensuring optimal decisions under a limited computational budget. Our approach can be used with any decision-theoretic acquisition function and is readily compatible with trust region methods like TuRBO. We derive efifcient joint objectives for the expected improvement and knowledge gradient acquisition functions for standard and batch BO. Our approach outperforms standard SVGPs on high-dimensional benchmark tasks in control and molecular design. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bayesian optimization (BO; Frazier, 2018; Garnett, 2023; Jones et al., 1998; Mockus, 1982; Shahriari et al., 2015) casts optimization as a sequential decision-making problem. Many recent successes of BO have involved complex and high-dimensional problems. In contrast to \u201cclassic\u201d low-dimensional BO problems\u2014where expensive black-box function evaluations far exceeded computational costs\u2014these modern problems necessitate tens of thousands of function evaluations, and it is often the complexity and dimensionality of the search space that makes optimization challenging, rather than a limited evaluation budget (Eriksson et al., 2019; Grifitfhs and Hern\u00e1ndez-Lobato, 2020; Maus et al., 2022, 2023; Stanton et al., 2022). Because of these scenarios, BO is entering a regime where computational costs are becoming a primary bottleneck (Maddox et al., 2021; Maus et al., 2023; Moss et al., 2023; Vakili et al., 2021), as the Gaussian process (GP; Rasmussen and Williams, 2005) surrogate models that underpin most of Bayesian optimization scale cubically with the number of observations. ", "page_idx": 0}, {"type": "text", "text": "In this new regime, we require scalable GP approximations, an area that has made tremendous progress over the last decade. In particular, sparse variational Gaussian processes (SVGP; Hensman et al., 2013; Qui\u00f1onero-Candela and Rasmussen, 2005; Titsias, 2009) have seen an increase in use (Grifitfhs and Hern\u00e1ndez-Lobato, 2020; Maddox et al., 2021; Maus et al., 2022, 2023; Stanton et al., 2022; Tripp et al., 2020; Vakili et al., 2021), but many challenges remain to effectively deploy SVGPs for large-budget BO. In particular, the standard SVGP training objective is not aligned with the goals of black-box optimization. SVGPs construct an inducing point approximation that maximizes the standard variational evidence lower bound (ELBO; Jordan et al., 1999), yielding a posterior approximation $q^{*}(f)$ that models all observed data (Matthews et al., 2016; Moss et al., 2023). However, the optimal posterior approximation $q^{*}$ is suboptimal for the decision-making tasks involved in BO (Lacoste\u2013Julien et al., 2011). In BO, we do not care about posterior fidelity at the majority of prior observations; rather, we only care about the fidelity of downstream functions involving the posterior, such as the expected utility. To illustrate this point intuitively, consider using the common expected improvement (EI; Jones et al., 1998) acquisition function for selecting new observations. Maximizing the ELBO might result in a posterior approximation that maintains fidelity for training examples in regions of virtually zero EI, thus wasting \u201capproximation budget.\u201d ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To solve this problem, we focus on the deep connections between statistical decision theory (Robert, 2001; Wasserman, 2013, $\\S12_{.}$ ) and Bayesian optimization (Garnett, 2023, $\\S6{-}7\\rangle$ ), where acquisition maximization can be viewed as maximizing posterior-expected utility. Following this perspective, we leverage the utility-calibrated approximate inference framework (Jaiswal et al., 2020, 2023; Lacoste\u2013Julien et al., 2011), and solve the aforementioned problem through a variational bound (Blei et al., 2017; Jordan et al., 1999)\u2013the (log) expected utility lower bound (EULBO)\u2014a joint function of the decision (the BO query) and the posterior approximation (the SVGP). When optimized jointly, the EULBO automatically yields the approximately optimal decision through the minorizemaximize principle (Lange, 2016). The EULBO is reminiscent of the standard variational ELBO (Jordan et al., 1999), and can indeed be viewed as a standard ELBO for a generalized Bayesian inference problem (Bissiri et al., 2016; Knoblauch et al., 2022), where we seek to approximate the utility-weighted posterior. This work represents the first application of utility-calibrated approximate inference towards BO despite its inherent connection with utility maximization. ", "page_idx": 1}, {"type": "text", "text": "The benefits of our proposed approach are visualized in Fig. 1. Furthermore, it can be applied to acquisition function that admits a decision-theoretic interpretation, which includes the popular expected improvement (EI; Jones et al., 1998) and knowledge gradient (KG; Wu et al., 2017) acquisition functions, and is trivially compatible with local optimization techniques like TuRBO (Eriksson et al., 2019) for high-dimensional problems. We demonstrate that our joint SVGP/acquisition optimization approach yields significant improvements across numerous Bayesian optimization benchmarks. As an added benefti, our approach can simplify the implementation and reduce the computational burden of complex (decision-theoretic) acquisition functions like KG. We demonstrate a novel algorithm derived from our joint optimization approach for computing and optimizing the KG that expands recent work on one-shot KG (Balandat et al., 2020) and variational GP posterior refinement (Maddox et al., 2021). ", "page_idx": 1}, {"type": "text", "text": "Overall, our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2219 We propose utility-calibrated variational inference of SVGPs in the context of large-budget BO. \u2219 We study this framework in two special cases using the utility functions of two common acquisition functions: EI and KG. For each, we derive tractable EULBO expressions that can be optimized. \u2219 For KG, we demonstrate that the computation of the EULBO takes only negligible additional work over computing the standard ELBO by leveraging an online variational update. Thus, as a byproduct of optimizing the EULBO, optimizing KG becomes comparable to the cost of the EI. \u2219 We extend this framework to be capable of running in batch mode, by introducing q-EULBO analogs of q-KG and $\\mathbf{q}_{\\mathbf{\\lambda}}$ -EI as commonly used in practice (Wilson et al., 2018). \u2219 We demonstrate the effectiveness of our proposed method against standard SVGPs trained with ELBO maximization on high-dimensional benchmark tasks in control and molecular design, where the dimensionality and evaluation budget go up to 256 and $80\\mathbf{k}$ , respectively. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Noisy Black-Box Optimization. Noisy black-box optimization refers to problems of the form: maximize $\\mathbf{\\hat{x}}{\\in}\\mathcal{X}$ $F\\left(x\\right)$ , where $\\mathcal{X}\\subset\\mathbb{R}^{d}$ is some compact domain, $F:\\mathcal{X}\\to\\mathcal{Y}$ is some objective function, and we assume that only zeroth-order information of $F$ is available. More formally, for some $i\\in\\mathbb{N}_{>0}$ , we assume that observations of the objective function $(\\pmb{x}_{i},y_{i}=\\widehat{F}(\\pmb{x}_{i}))$ have been corrupted by independently and identically distributed (i.i.d.) Gaussian noise $\\widehat{F}\\left(\\pmb{x}_{i}\\right)\\triangleq F(\\pmb{x}_{i})+\\epsilon$ , where $\\epsilon\\sim\\dot{\\mathcal{N}}(0,\\sigma_{\\mathrm{n}}^{\\dot{2}})$ . The noise variance $\\sigma_{\\mathrm{n}}^{\\ddag}$ is also unknown. ", "page_idx": 1}, {"type": "text", "text": "Bayesian optimization. Bayesian Optimization (BO) is and iterative approach to noisy black-box $\\mathcal{D}_{t}=\\{\\left(\\pmb{x}_{i},y_{i}=\\widehat{F}(\\pmb{x}_{i})\\right)\\}_{i=1}^{n_{t}}$ $\\widehat F$ to fit a surrog $\\pmb{\\mathrm{\\Sigma}}$ supervised $t\\geq0$ $f\\in\\mathcal F$ . Typically, $\\mathcal{F}$ is taken ", "page_idx": 1}, {"type": "image", "img_path": "t7euV5dl5M/tmp/e37a431cb621d3ade4542c8277424c288a4bcbe7da3fe72d0f4b93e9042393f8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: (Left.) Fitting an SVGP model with only $m=4$ inducing points sacrifices modeling areas of high EI (few data points at right) because the ELBO focuses only on global data approximation (left data) and is ignorant of the downstream decision making task. (Middle.) Because of this, (normalized) EI with the SVGP model peaks in an incorrect location relative to the exact posterior. (Right.) Updating the GP fti and selecting a candidate jointly using the EULBO (our method) results in candidate selection much closer to the exact model. ", "page_idx": 2}, {"type": "text", "text": "$\\pi(f\\mid\\mathcal{D})$ forms a distribution over surrogate models at step \ud835\udc61. $\\pmb{\\varphi}$ The posterior is then used to form a decision problem where we choose which point we should evaluate next, $\\pmb{x}_{t+1}=\\delta_{\\alpha}(\\mathcal{D}_{t})$ , by maximizing an acquisition function $\\alpha:\\mathcal{X}\\rightarrow\\mathbb{R}$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\delta_{\\alpha}\\left(\\mathcal{D}_{t}\\right)\\triangleq\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\arg\\operatorname*{max}}\\ \\alpha\\left(\\pmb{x};\\mathcal{D}_{t}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "$\\pmb{\\otimes}$ After selecting ${\\pmb x}_{t+1},\\widehat F$ is evaluated to obtain the new datapoint $(\\pmb{x}_{t+1},y_{t+1}=\\widehat{F}(\\pmb{x}_{t+1}))$ . This is then added to the dataset, forming $\\mathcal{D}_{t+1}=\\mathcal{D}_{t}\\cup(\\pmb{x}_{t+1},\\pmb{y}_{t+1})$ to be used in the next iteration. ", "page_idx": 2}, {"type": "text", "text": "Utility-Based Acquisition Functions. Many commonly used acquisition functions, including EI and KG, can be expressed as posterior-expected utility functions ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\alpha\\left(\\boldsymbol{x};\\mathcal{D}\\right)\\triangleq\\int u\\left(\\boldsymbol{x},f;\\mathcal{D}\\right)\\pi\\left(f\\mid\\mathcal{D}\\right)\\mathrm{d}f,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\boldsymbol{u}\\left(\\boldsymbol{x},f;\\mathcal{D}\\right):\\mathcal{X}\\times\\mathcal{F}\\rightarrow\\mathbb{R}$ is some utility function associated with $\\alpha$ (Garnett, 2023, $\\S6{-}7\\$ ). In statistical decision theory, posterior-expected utility maximization policies such as $\\delta_{\\alpha}$ are known as Bayes policies. These are important because, for a given utility function, they attain certain notions of statistical optimality such as Bayes optimality and admissibility (Robert, 2001, $\\S2.4$ ; Wasserman, 2013, $\\S12_{.}$ . However, this only holds true if we can exactly compute Eq. (2) over the posterior. Once approximate inference is involved, making optimal Bayes decisions becomes challenging. ", "page_idx": 2}, {"type": "text", "text": "Sparse Variational Gaussian Processes. While the $\\mathcal{O}(n^{3})$ complexity of exact Gaussian process model selection and inference is not necessarily a roadblock in the traditional regression setting with 10,000-50,000 training examples, BO amplifies the scalability challenge by requiring us to sequentially train or update many large scale GPs as we iteratively acquire more data. ", "page_idx": 2}, {"type": "text", "text": "To address this, sparse variational GPs (SVGP; Hensman et al., 2013; Titsias, 2009) have become commonly used in high-throughput Bayesian optimization. SVGPs modify the original GP prior from $p(f)$ to $p(f\\mid\\pmb{u})\\bar{p}(\\pmb{u})$ , where we assume the latent function $f$ is \u201cinduced\u201d by a finite set of inducing values $\\pmb{u}=(u_{1},\\dots,u_{m})\\in\\mathbb{R}^{m}$ located at inducing points $z_{i}\\in\\mathcal X$ for $i=1,\\ldots,m$ . Inference is done through variational inference (Blei et al., 2017; Jordan et al., 1999), where the posterior of the inducing points is approximated using $q_{\\lambda}\\left(u\\right)=\\mathcal{N}\\left(u;\\lambda=\\left(m,S\\right)\\right)$ and that of the latent functions with $q\\left(f\\mid u\\right)=p\\left(f\\mid u\\right)$ . Here, the variational parameters $\\pmb{m}$ and $\\boldsymbol{s}$ are defined as the learned mean and covariance of the variational distribution $q_{\\lambda}(\\pmb{u})$ . It is standard practice to define $\\pmb{\\lambda}=(\\pmb{m},\\pmb{S})$ so that $\\lambda$ can be used as shorthand to represent all of the trainable variational parameters. As is typical in the BO literature, we use the subscript $\\lambda\\in\\Lambda$ to denote that the distribution denoted as $q$ contains trainable parameters in $\\lambda$ . ", "page_idx": 2}, {"type": "text", "text": "For a positive definite kernel function $k:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}_{>0}.$ , the resulting ELBO objective, which can be computed in a closed form (Hensman et al., 2013), is then ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{ELBO}}\\left(\\lambda;\\mathcal{D}_{t}\\right)\\triangleq\\mathbb{E}_{q_{\\lambda}\\left(f\\right)}\\left[\\sum_{i=1}^{n_{t}}\\log\\ell(y_{i}\\mid f\\left(x_{i}\\right))\\right]-\\mathrm{D}_{\\mathrm{KL}}\\left(q_{\\lambda}\\left(\\pmb{u}\\right),p\\left(\\pmb{u}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\ell(y_{i}\\mid f\\left(\\mathbf{x}_{i}\\right))\\;=\\;\\mathcal{N}\\left(y_{i}\\mid f\\left(\\mathbf{x}_{i}\\right),\\sigma_{\\epsilon}\\right)$ is a Gaussian likelihood. The marginal variational approximation can be computed as ", "page_idx": 2}, {"type": "equation", "text": "$$\nq_{\\lambda}(f)=\\int q_{\\lambda}(f,u)\\,\\mathrm{d}u=\\int p\\left(f\\mid u\\right)\\,q_{\\lambda}(u)\\,\\mathrm{d}u\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "such that the point-wise function evaluation on some $\\pmb{x}\\in\\mathcal{X}$ is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q_{\\lambda}(f(\\pmb{x}))=\\mathcal{N}\\left(f(\\pmb{x});\\quad\\mu_{f}(\\pmb{x})\\triangleq K_{x Z}K_{z Z}^{-1}m,\\quad\\sigma_{f}^{2}\\left(\\pmb{x}\\right)\\triangleq\\widetilde{k}_{x x}+k_{x Z}^{\\top}K_{z Z}^{-1}S K_{z Z}^{-1}k_{Z x}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with $\\widetilde{k}_{x x}\\triangleq k\\left(\\mathbf{x},\\mathbf{x}\\right)-k_{x Z}K_{Z Z}^{-1}k_{Z x}^{\\top}$ , the vector $\\pmb{k}_{\\pmb{Z}\\pmb{x}}\\in\\mathbb{R}^{m}$ is formed as $[{k_{Z x}}]_{i}=k\\left({{z_{i}},x}\\right)$ , and the matrix $\\pmb{K}_{Z Z}\\in\\mathbb{R}^{m\\times m}$ is formed as $\\left[K_{Z Z}\\right]_{i j}=k(z_{i},z_{j})$ . Additionally, the GP likelihood and kernel contain hyperparameters, which we denote as $\\theta\\in\\Theta$ , and we collectively denote the set of inducing point locations as $Z=(z_{1},\\dots,z_{m})\\in\\mathcal{X}^{m}$ . We therefore denote the ELBO as $\\mathcal{L}_{\\mathrm{ELBO}}\\left(\\lambda,Z,\\theta;\\mathcal{D}_{t}\\right)$ . ", "page_idx": 3}, {"type": "text", "text": "3 Approximation-Aware Bayesian Optimization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "When SVGPs are used in conjunction with BO (Maddox et al., 2021; Moss et al., 2023) at iteration $t\\geq0$ , acquisition functions of the form of Eq. (2) are na\u00efvely approximated as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\alpha\\left(\\mathbf{\\boldsymbol{x}};\\mathcal{D}\\right)\\approx\\int u\\left(\\mathbf{\\boldsymbol{x}},f;\\mathcal{D}_{t}\\right)q_{\\lambda}\\left(f\\right)\\mathrm{d}f,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $q_{\\lambda}(f)$ is the approximate SVGP posterior given by Eq. (4). The acquisition policy implied by this approximation contains two separate optimization problems: ", "page_idx": 3}, {"type": "equation", "text": "$$\nx_{t+1}=\\operatorname*{arg\\,max}_{x\\in\\mathcal{X}}\\int u\\left(x,f;\\mathcal{D}_{t}\\right)q_{\\mathrm{\\textsc{XLBO}}}^{*}(f)\\,\\mathrm{d}f\\quad\\mathrm{and}\\quad\\lambda_{\\mathrm{ELBO}}^{*}=\\operatorname*{arg\\,max}_{\\lambda\\in\\Lambda}\\mathcal{L}_{\\mathrm{ELBO}}\\left(\\lambda;\\mathcal{D}_{t}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Treating these optimization problems separately creates an artificial bottleneck that results in sduatbao (ptMiamttahl edwast ae ta calq.,u i2s0it1i6o;n  Mdeocsis seito nals..,  I2n0t2u3it)i, vwelityh, $\\lambda_{\\mathrm{ELBO}}^{*}$ airsd  cfhoor sheon wt ot hfea irtehsfuulltliyn gm modoedle la lpl eorfbosremrvs eadt selecting the next function evaluation in the BO loop. For an illustration of this, see Figure 1. Instead, we propose a modification to SVGPs that couples the posterior approximation and data acquisition through a joint problem of the form: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\,\\pmb{x}_{t+1},\\lambda^{*}\\,)=\\arg\\operatorname*{max}_{\\lambda\\in\\Lambda,x\\in\\mathcal{X}}\\mathcal{L}_{\\mathrm{EULBO}}\\left(\\lambda,\\pmb{x};\\mathcal{D}_{t}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This results in $x_{t+1}$ directly approximating a solution to Eq. (2), where the expected utility lowerbound (EULBO) is an ELBO-like objective function derived below. ", "page_idx": 3}, {"type": "text", "text": "3.1 Expected Utility Lower-Bound ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Consider an acquisition function of the form of Eq. (2), where the utility $u:\\mathcal{X}\\times\\mathcal{F}\\to\\mathbb{R}_{>0}$ is strictly positive. We can derive a similar variational formulation of the acquisition function maximization problem following Lacoste\u2013Julien et al. (2011). That is, given any distribution $q_{\\lambda}$ indexed by $\\lambda\\in\\Lambda$ and considering the SVGP prior augmentation $p(f)\\rightarrow p(f\\mid\\pmb{u})p(\\pmb{u})$ , the acquisition function can be lower-bounded through Jensen\u2019s inequality as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log\\alpha\\left(x;\\mathcal{D}_{t}\\right)=\\log\\displaystyle\\int u\\left(x,f;\\mathcal{D}_{t}\\right)\\pi\\left(f\\mid\\mathcal{D}_{t}\\right)\\mathrm{d}f}\\\\ &{\\qquad\\qquad\\qquad=\\log\\displaystyle\\int u\\left(x,f;\\mathcal{D}_{t}\\right)\\pi\\left(f,u\\mid\\mathcal{D}_{t}\\right)\\frac{q_{\\lambda}\\left(f,u\\right)}{q_{\\lambda}\\left(f,u\\right)}\\mathrm{d}f\\,\\mathrm{d}u}\\\\ &{\\qquad\\qquad\\qquad=\\log\\displaystyle\\int u\\left(x,f;\\mathcal{D}_{t}\\right)\\ell\\left(\\mathcal{D}_{t}\\mid f\\right)p\\left(f\\mid u\\right)p\\left(u\\right)\\frac{q_{\\lambda}\\left(u\\right)p\\left(f\\mid u\\right)}{q_{\\lambda}\\left(u\\right)p\\left(f\\mid u\\right)}\\mathrm{d}f\\,\\mathrm{d}u-\\log Z}\\\\ &{\\qquad\\qquad\\qquad\\geq\\displaystyle\\int\\log\\left(\\frac{u\\left(x,f;\\mathcal{D}_{t}\\right)\\ell\\left(\\mathcal{D}_{t}\\mid f\\right)p\\left(u\\right)}{q_{\\lambda}\\left(u\\right)}\\right)p\\left(f\\mid u\\right)q_{\\lambda}\\left(u\\right)\\,\\mathrm{d}f\\,\\mathrm{d}u-\\log Z,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $Z$ is a normalizing constant. A restriction on $u$ comes from the inequality in Eq. (7), where the utility needs to be strictly positive. This means that non-strictly positive utilities need to be modified to be incorporated into this framework. (See the examples by Ku\u015bmierczyk et al., 2019.) Also, notice that the derivation is reminiscent of expectation-maximization (Dempster et al., 1977) and variational lower bounds (Jordan et al., 1999). That is, through the minorize-maximize principle (Lange, 2016), maximizing the lower bound with respect to $_x$ and $\\lambda$ approximately solves the original problem of maximizing the posterior-expected utility. ", "page_idx": 3}, {"type": "text", "text": "Expected Utility Lower-Bound. Up to a constant and rearranging terms, maximizing Eq. (7) is equivalent to maximizing ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathcal{L}_{\\mathrm{EULBO}}\\left(\\lambda,x;\\mathcal{D}_{t}\\right)\\triangleq\\mathbb{E}_{p(f\\mid u)q_{\\lambda}\\left(u\\right)}\\left[\\log\\ell(\\mathcal{D}_{t}\\mid f)+\\log p\\left(u\\right)-\\log q_{\\lambda}(u)+\\log u\\left(x,f;\\mathcal{D}_{t}\\right)\\right]}&{{}}&{}\\\\ {=\\mathbb{E}_{q_{\\lambda}(f)}\\left[\\sum_{i=1}^{n_{t}}\\log\\ell(y_{i}\\mid f)\\right]-\\operatorname{D}_{\\mathrm{KL}}\\left(q_{\\lambda}(u),p(u)\\right)+\\mathbb{E}_{q_{\\lambda}(f)}\\log u\\left(x,f;\\mathcal{D}_{t}\\right)}&{{}}&{}\\\\ {=\\mathcal{L}_{\\mathrm{ELBO}}\\left(\\lambda;\\mathcal{D}_{t}\\right)+\\mathbb{E}_{q_{\\lambda}(f)}\\log u\\left(x,f;\\mathcal{D}_{t}\\right),}&{{}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which is the joint objective function alluded to in Eq. (6). We maximize EULBO to obtain $(\\mathbf{x}_{t+1},\\lambda^{*})=$ arg $\\begin{array}{r}{\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X},\\lambda\\in\\Lambda}\\;\\mathcal{L}_{\\mathrm{EULBO}}\\left(\\mathbf{x},\\lambda\\right)}\\end{array}$ , where $x_{t+1}$ corresponds our next BO \u201cquery\u201d. ", "page_idx": 4}, {"type": "text", "text": "From Eq. (8), the connection between the EULBO and ELBO is obvious: the EULBO is now \u201cnudging\u201d the ELBO solution toward high utility regions. An alternative perspective is that we are approximating a generalized posterior weighted by the utility (Table. 1 by Knoblauch et al., 2022; Bissiri et al., 2016). Furthermore, Jaiswal et al. (2020, 2023) prove that the resulting actions satisfy consistency guarantees under assumptions typical in such results for variational inference (Wang and Blei, 2019). ", "page_idx": 4}, {"type": "text", "text": "Hyperparameters and Inducing Point Locations. For the hyperparameters $\\boldsymbol{\\theta}$ and inducing point locations $z$ , we use the marginal likelihood to perform model selection, which is common practice in BO (Shahriari et al., 2015, $\\S\\mathrm{V.A}_{\\mathrm{,}}$ ). (Optimizing over $z$ was popularized by Snelson and Ghahramani, 2005.) Following suit, we also optimize the EULBO as a function of $\\boldsymbol{\\theta}$ and $z$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{maximize}_{\\lambda,\\boldsymbol{x},\\boldsymbol{\\theta},\\boldsymbol{Z}}\\,\\left\\{\\,\\mathcal{L}_{\\mathrm{EULBO}}\\left(\\lambda,\\boldsymbol{x},\\boldsymbol{\\theta},\\boldsymbol{Z};\\mathcal{D}_{t}\\right)\\triangleq\\mathcal{L}_{\\mathrm{ELBO}}\\left(\\lambda,\\boldsymbol{Z},\\boldsymbol{\\theta};\\mathcal{D}_{t}\\right)+\\mathbb{E}_{\\boldsymbol{q}_{\\lambda}(f)}\\log u\\left(\\boldsymbol{x},f;\\mathcal{D}_{t}\\right)\\,\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We emphasize here that the SVGP-associated parameters $\\lambda,\\theta,Z$ have gradients that are determined by both terms above. Thus, the expected log-utility term $\\mathbb{E}_{f\\sim q_{\\lambda}(f)}\\log u\\left(\\pmb{x},f;\\mathcal{D}_{t}\\right)$ simultaneously results in acquisition of $x_{t+1}$ and directly influences the underlying SVGP regression model. ", "page_idx": 4}, {"type": "text", "text": "3.2 EULBO for Expected Improvement (EI) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The EI acquisition function can be expressed as a posterior-expected utility, where the underlying \u201cimprovement\u201d utility function is given by the difference between the objective value of the query, $f({\\pmb x})$ , and the current best objective value $\\begin{array}{r}{\\boldsymbol{y}_{t}^{*}=\\operatorname*{max}_{i=1,\\dots,t}\\left\\{\\,\\boldsymbol{y}_{i}\\mid\\boldsymbol{y}_{i}\\in\\mathcal{D}_{t}\\,\\right\\}}\\end{array}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nu_{\\mathrm{EI}}\\left(\\pmb{x},f;\\mathcal{D}_{t}\\right)\\triangleq\\mathbf{ReLU}\\left(f\\left(\\pmb{x}\\right)-y_{t}^{*}\\right),\\qquad\\mathrm{(EI;~Jones~et~al.,~1998)}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathrm{ReLU}\\left(x\\right)\\triangleq\\operatorname*{max}\\left(x,0\\right)$ . Unfortunately, this utility is not strictly positive whenever $f\\left(\\pmb{x}\\right)\\leq y^{*}$ . Thus, we cannot immediately plug $u_{\\mathrm{EI}}$ into the EULBO. While it is possible to add a small positive constant to $u_{\\mathrm{EI}}$ and make it strictly positive as done by Ku\u015bmierczyk et al. (2019), this results in a looser Jensen gap in Eq. (7), which could be detrimental. This also introduces the need for tuning the constant, which is not straightforward. Instead, we define the following \u201csoft\u201d EI utility: ", "page_idx": 4}, {"type": "equation", "text": "$$\nu_{\\mathrm{SEI}}\\left(\\pmb{x},f;\\mathcal{D}_{t}\\right)\\triangleq\\mathrm{softplus}\\left(f\\left(\\pmb{x}\\right)-y_{t}^{\\ast}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the ReLU in Eq. (9) is replaced with softplus $(x)\\triangleq\\log\\left(1+\\exp(x)\\right)$ . softplus $(x)$ converges to the ReLU in both extremes of $x\\to\\pm\\infty$ . Thus, $u_{\\mathrm{SEI}}$ will behave closely to $u_{\\mathrm{EI}}$ , while being slightly more explorative due to positivity. ", "page_idx": 4}, {"type": "text", "text": "Computing the EULBO and its derivatives now requires the computation of $\\mathbb{E}_{f\\sim q_{\\lambda}(f)}\\log u_{\\mathrm{SEI}}(\\pmb{x},f;\\mathcal{D}_{t})$ , which, unlike EI, does not have a closed-form. However, since the utility function only depends on the function values of $f$ , the expectation can be efifciently computed to high precision through one-dimensional Gauss-Hermite quadrature. Crucially, the expensive $K_{z z}^{-1}m$ and $K_{z z}^{-1}S K_{z z}^{-1}$ solves that dominate both the asymptotic and practical running time of both the ELBO and the EULBO are fixed across the log utility evaluations needed by quadrature. Because quadrature only depends on these precomputed moments, the additional work necessary due to lacking a closed form solution is negligible: Gauss-Hermite quadrature converges extremely quickly in the number of quadrature sites, and only requires on the order of 10 or so of these post-solve evaluations to achieve near machine precision. ", "page_idx": 4}, {"type": "text", "text": "3.3 EULBO for Knowledge Gradient (KG) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Although non-trivial, the KG acquisition is also a posterior-expected utility, where the underlying utility function is given by the maximum predictive mean value anywhere in the input domain after ", "page_idx": 4}, {"type": "text", "text": "conditioning on a new observation $({\\pmb x},y)\\in\\mathcal{X}\\times\\mathcal{Y}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nu_{\\mathrm{KG}}\\left(\\pmb{x},y;\\mathcal{D}_{t}\\right)\\triangleq\\operatorname*{max}_{\\pmb{x}^{\\prime}\\in\\mathcal{X}}\\,\\mathbb{E}\\left[f(\\pmb{x}^{\\prime})\\mid\\mathcal{D}_{t}\\cup\\{(\\pmb{x},y)\\}\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that the utility function as defined above is not non-negative: the maximum predictive mean of a Gaussian process can be negative. For this reason, the utility function is commonly (and originally, e.g. Frazier, 2009, Eq. 4.11) written in the literature as the difference between the new maximum mean after conditioning on $(x,y)$ and the maximum mean beforehand: ", "page_idx": 5}, {"type": "equation", "text": "$$\nu_{\\mathrm{KG}}\\left(\\pmb{x},y;\\mathcal{D}_{t}\\right)\\triangleq\\operatorname*{max}_{\\pmb{x}^{\\prime}\\in\\mathcal{X}}\\,\\mathbb{E}\\left[f(\\pmb{x}^{\\prime})\\mid\\mathcal{D}_{t}\\cup\\{(\\pmb{x},y)\\}\\right]-\\mu_{t}^{+},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mu_{t}^{+}\\triangleq\\operatorname*{max}_{\\pmb{x}^{\\prime\\prime}\\in\\mathcal{X}}E\\left[f(\\pmb{x}^{\\prime\\prime})\\mid\\mathcal{D}_{t}\\right]$ . Note that $\\mu_{t}^{+}$ plays the role of a simple constant as it depends on neither $_x$ nor $y$ . Similarly to the EI acquisition, this utility is still not strictly positive, and we thus define its \u201csoftplus-ed\u201d variant: ", "page_idx": 5}, {"type": "equation", "text": "$$\nu_{\\mathrm{SKG}}\\left(\\pmb{x},y;\\mathcal{D}_{t}\\right)\\triangleq\\operatorname{softplus}\\left(u_{\\mathrm{KG}}\\left(\\pmb{x},y;\\mathcal{D}_{t}\\right)-c^{+}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, $c^{+}$ acts as $\\mu_{t}^{+}$ by making $u_{\\mathrm{KG}}$ positive as often as possible. This is particularly important when the GP predictive mean is negative as a consequence of the objective values being negative. One natural choice of constant is using $\\mu_{t}^{+}$ ; however, we find that simply choosing $c^{+}=y_{t}^{+}$ works well and is more computationally efifcient. Here, $y_{t}^{+}$ is the highest value of $y_{t}$ (the highest objective value observed so far). ", "page_idx": 5}, {"type": "text", "text": "One-Shot KG EULBO. The EULBO using $u_{\\mathrm{SKG}}$ results in an expensive nested optimization problem. To address this, we use an approach similar to the one-shot knowledge gradient method of Balandat et al. (2020). For clarity, we will define the reparameterization function ", "page_idx": 5}, {"type": "equation", "text": "$$\ny_{\\lambda}\\left(\\pmb{x};\\epsilon_{i}\\right)\\triangleq\\mu_{q_{\\lambda}}(\\pmb{x})+\\sigma_{q_{\\lambda}}(\\pmb{x})\\,\\epsilon_{i},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where, f(or an i.i.d. sa)mple $\\epsilon_{i}\\,\\sim\\,{\\mathcal{N}}\\left(0,1\\right)$ , computing $y_{i}\\,=\\,y_{\\lambda}\\left(\\pmb{x},\\pmb{\\epsilon}_{i}\\right)$ is equivalent to sampling $y_{i}\\sim\\mathcal N\\left(\\mu_{q_{\\lambda}}(\\pmb{x}),\\sigma_{q_{\\lambda}}(\\pmb{x})\\right)$ . This enables the use of the reparameterization gradient estimator (Kingma and Welling, 2014; Rezende et al., 2014; Titsias and L\u00e1zaro-Gredilla, 2014). Now, notice that the KG acquisition function can be approximated through Monte Carlo as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\alpha_{\\mathrm{KG}}(\\pmb{x};\\mathcal{D})\\approx\\frac{1}{S}\\sum_{i=1}^{S}u_{\\mathrm{KG}}(\\pmb{x},y_{\\lambda}(\\pmb{x};\\pmb{\\epsilon}_{i});\\mathcal{D}_{t})=\\frac{1}{S}\\sum_{i=1}^{S}\\operatorname*{max}_{\\pmb{x}^{\\prime}}\\mathbb{E}\\left[f(\\pmb{x}^{\\prime})\\mid\\mathcal{D}_{t}\\cup\\{\\pmb{x},y_{\\lambda}(\\pmb{x};\\pmb{\\epsilon}_{i})\\}\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where, for $i\\,=\\,1,\\ldots,S,\\;\\epsilon_{i}\\;\\sim\\;{\\mathcal{N}}(0,1)$ are i.i.d. The one-shot KG approach absorbs the nested optimization over $x^{\\prime}$ into a simultaneous joint optimization over $\\pmb{x}$ and a mean maximizer for each of the S samples, $\\pmb{x}_{1}^{\\prime},...,\\pmb{x}_{S}^{\\prime}$ such that $\\begin{array}{r}{\\operatorname*{max}_{x}\\alpha_{\\mathrm{KG}}(x;\\mathcal{D}_{t})\\approx\\operatorname*{max}_{x,x_{1}^{\\prime},\\ldots,x_{S}^{\\prime}}\\alpha_{\\mathrm{1-KG}}(x;\\mathcal{D}),}\\end{array}$ , where ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\alpha_{1\\cdot\\mathrm{KG}}({\\boldsymbol{x}};{\\mathcal{D}}_{t})\\triangleq\\frac{1}{S}\\sum_{i=1}^{S}u_{1\\cdot\\mathrm{KG}}({\\boldsymbol{x}},{\\boldsymbol{x}}_{i}^{\\prime},y_{\\lambda}\\,({\\boldsymbol{x}};{\\boldsymbol{\\varepsilon}}_{i})\\,;{\\mathcal{D}}_{t})=\\frac{1}{S}\\sum_{i=1}^{S}\\mathbb{E}\\left[f({\\boldsymbol{x}}_{i}^{\\prime})\\mid{\\mathcal{D}}_{t}\\cup\\{{\\boldsymbol{x}},y_{\\lambda}\\,({\\boldsymbol{x}};{\\boldsymbol{\\varepsilon}}_{i})\\}\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Evidently, there is no longer an inner optimization problem over $\\pmb{x}^{\\prime}$ . To estimate the \ud835\udc56th term of this sum, we draw a sample of the objective value of $\\pmb{x}$ , $y_{\\lambda}(\\pmb{x};\\pmb{\\epsilon}_{i})$ , and condition the model on this sample. We then compute the new posterior predictive mean at $\\pmb{x}_{i}^{\\prime}$ . After summing, we compute gradients with respect to both the candidate $_x$ and the mean maximizers $\\pmb{x}_{1}^{\\prime},...,\\pmb{x}_{S}^{\\prime}$ . Again, we use the \u201csoft\u201d version of one-shot KG in our EULBO optimization problem: ", "page_idx": 5}, {"type": "equation", "text": "$$\nu_{1\\mathrm{-SKG}}\\left(\\pmb{x},\\pmb{x}^{\\prime},y;\\mathcal{D}_{t}\\right)=\\mathrm{softplus}\\left(\\mathbb{E}\\left[f(\\pmb{x}^{\\prime})\\mid\\mathcal{D}_{t}\\cup\\{(\\pmb{x},y)\\}\\right]-c^{+}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where this utility function is crucially a function of both $_x$ and a free parameter $x^{\\prime}$ . As with $\\alpha_{\\mathrm{1-KG}}$ , maximizing the EULBO can be set up as a joint optimization problem: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{maximize}_{\\boldsymbol{x},\\boldsymbol{x}_{1}^{\\prime},...,\\boldsymbol{x}_{s}^{\\prime},\\lambda,\\boldsymbol{Z},\\theta}\\ \\mathcal{L}_{\\mathrm{ELBO}}(\\lambda,\\boldsymbol{Z},\\theta)+\\frac{\\stackrel{\\cdot}{1}}{S}\\sum_{i=1}^{S}\\log u_{1\\cdot\\mathrm{SKG}}\\left(\\boldsymbol{x},\\boldsymbol{x}_{i}^{\\prime},\\boldsymbol{y}_{\\lambda}\\left(\\boldsymbol{x};\\boldsymbol{\\epsilon}_{i}\\right);\\mathcal{D}_{t}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Efifcient KG-EULBO Computation[. The computation time of]  the non-ELBO term in Eq. (10) is dominated by having to compute $\\mathbb{E}\\left[f(\\pmb{x}_{i}^{\\prime})\\mid\\mathcal{D}_{t}\\cup\\Bar{\\cup}\\left\\{(\\pmb{x},y_{\\lambda}\\left(\\pmb{x};\\in_{i}\\right))\\right\\}\\right]$ \ud835\udc46-times. Notice that we only need to compute an updated posterior predictive mean, and can ignore predictive variances. For this, we can leverage the online updating strategy of Maddox et al. (2021). In particular, the predictive mean can be updated in $\\mathcal{O}(m^{\\tilde{2}})$ time using a simple Cholesky update. The additional $\\mathcal{O}(S\\bar{m}^{2})$ cost of computing the EULBO is therefore amortized by the original $\\mathcal{O}(m^{3})$ cost of computing the ELBO. ", "page_idx": 5}, {"type": "text", "text": "3.4 Extension to q-EULBO for Batch Bayesian Optimization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The EULBO can be extended to support batch Bayesian optimization by using the Monte Carlo batch mode analogs of utility functio(ns as disc)u ssed e.g. by Balandat et al. (2020); Wilson et al. (2018). Given a set of candidates $\\pmb{X}=\\left(\\pmb{x}_{1},...,\\pmb{x}_{q}\\right)\\in\\mathcal{X}^{q}$ , the $q$ -EI utility function is given by: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{u_{q\\mathrm{{-}E I}}\\left(X,f;\\mathcal{D}_{t}\\right)\\triangleq\\underset{j=1\\ldots q}{\\operatorname*{max}}\\ \\mathrm{ReLU}\\left(f\\left(x_{j}\\right)-y_{t}^{*}\\right)\\quad(\\mathrm{q\\mathrm{-}E I};\\mathrm{Balandat~et~al},\\ 2020;\\mathrm{Wilson}\\ \\mathrm{et~al},\\ 2018)}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This utility can again be softened as: ", "page_idx": 6}, {"type": "equation", "text": "$$\nu_{q\\mathrm{-SEI}}\\left(\\boldsymbol{X},\\boldsymbol{f};\\mathcal{D}_{t}\\right)\\triangleq\\operatorname*{max}_{j=1\\ldots q}\\;\\mathrm{softplus}\\left(f\\left(\\boldsymbol{x}_{j}\\right)-\\boldsymbol{y}_{t}^{\\ast}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Because this is now a $q$ -dimensional integral, Gauss-Hermite quadrature is no longer applicable. However, we can apply Monte Carlo as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\stackrel{\\cdot\\cdot\\cdot}{\\mathbb{E}_{q_{\\lambda}(f)}}\\log u_{q\\cdot\\mathrm{SEI}}\\left(\\pmb{X},\\pmb{f};\\mathcal{D}_{t}\\right)\\approx\\frac{1}{S}\\sum_{i=1}^{S}\\operatorname*{max}_{j=1\\dots q}\\mathrm{softplus}\\left(y_{\\lambda}\\left(\\pmb{x};\\epsilon_{i}\\right)-y_{t}^{*}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "As done in the BoTorch software package (Balandat et al., 2020), we observe that fixing the set of base samples $\\epsilon_{1},...,\\epsilon_{S}$ during each BO iteration results in better optimization performance at the cost (of negligib)l e $\\mathfrak{q}$ -EULBO bias. Now, optimizing the q-EULBO is done over the full set of $q$ candidates $(x_{1},...,\\bar{x}_{q})$ jointly, as well as the GP hyperparameters, inducing points, and variational parameters. ", "page_idx": 6}, {"type": "text", "text": "Knowledge Gradient. The KG version of the EULBO can be similarly extended. The expected log utility term in the maximization problem Eq. (10) becomes: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{maximize}_{\\substack{x_{1},\\ldots,x_{q},x_{1}^{\\prime},\\ldots,x_{s}^{\\prime},\\lambda,Z,\\theta}}\\mathcal{L}_{\\mathrm{ELBO}}(\\lambda,Z,\\theta)+\\frac{1}{S}\\sum_{i=1}^{S}\\operatorname*{max}_{j=1\\ldots q}\\log u_{1\\cdot\\mathrm{SKG}}(x_{j},x_{i}^{\\prime},y_{\\lambda}(x;\\epsilon_{i})\\,;\\mathcal{D}_{t}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "resulting in a similar analog to $\\mathbf{q}_{\\mathbf{\\lambda}}$ -KG as described by Balandat et al. (2020). ", "page_idx": 6}, {"type": "text", "text": "3.5 Optimizing the EULBO ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Optimizing the ELBO for SVGPs is known to be challenging (Galy-Fajou and Opper, 2021; Terenin et al., 2024) as the optimization landscape for the inducing points is non-convex, multi-modal, and non-smooth. Naturally, these are also challenges for EULBO; we found that care must be taken when implementing and initializing the EULBO maximization problem. In this subsection, we outline some key ideas, while a detailed description with pseudocode is presented in Appendix A. ", "page_idx": 6}, {"type": "text", "text": "Initialization and Warm-Starting. We warm-start the EULBO maximization procedure by solving the conventional two-step scheme in Eq. (5): At each BO iteration, we obtain the \u201cwarm\u201d initial values for $(\\lambda,\\ Z,\\theta)$ by optimizing the standard ELBO. Then, we use this to maximize the conventional acquisition function corresponding to the chosen utility function $u$ (the expectation of $u$ over $q_{\\lambda}(f))$ , which provides the warm-start initialization for $_x$ . ", "page_idx": 6}, {"type": "text", "text": "Alternating Maximization Scheme. To optimize $\\mathcal{L}_{\\mathrm{EULBO}}\\left(\\pmb{x},\\lambda,Z,\\theta\\right)$ , we alternate between optimizing over the query $\\pmb{x}$ and the SVGP parameters $\\lambda,Z,\\theta$ . We find this block-coordinate descent scheme to be more stable and robust than jointly updating all parameters, though the reason why this is more stable than jointly optimizing all parameters requires further investigation. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate EULBO-based SVGPs on a number of benchmark BO tasks, described in detail in Section 4.1. These tasks include standard low-dimensional BO problems, e.g., the 6D Hartmann function, as well as 7 high-dimensional and high-throughput optimization tasks. ", "page_idx": 6}, {"type": "text", "text": "Baselines. We compare EULBO to several baselines with the main goal of achieving a high reward using as few function evaluations as possible. Our primary point of comparison is ELBO-based SVGPs. We consider two approaches for inducing point locations: 1. optimizing inducing point locations via the ELBO (denoted as ELBO), 2. placing the inducing points using the strategy proposed by Moss et al. (2023) at each stage of ELBO optimization (denoted as Moss et al.). The latter offers improved BO performance over standard ELBO-SVGP in BO settings, yet\u2014unlike our method\u2014it exclusively targets inducing point placement and does not affect variational parameters or hyperparameters of the model. In addition, we compare to BO using exact GPs using 2, 000 function evaluations as the use of exact GP is intractable beyond this point due to the need to repeatedly fit models. ", "page_idx": 6}, {"type": "image", "img_path": "t7euV5dl5M/tmp/08ee84091a235372b5f7c850fdc40f744b00953014fcf85a423b8aa160d8b89f.jpg", "img_caption": ["Figure 2: Optimization results on the 8 considered tasks. We compare all methods for both standard BO and TuRBO-based BO (on all tasks except Hartmann). Each line/shaded region represents the mean/standard error over 20 runs See subsection B.1 for additional molecule results. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Acquisition Functions and BO algorithms. For EULBO, we test the versions based on both the Expected Improvement (EI) and Knowledge Gradient (KG) acquisition functions as well as the batch variant. We test the baseline methods using EI only. On high-dimensional tasks (tasks with dimensionality above 10), we run EULBO and baseline methods with standard BO and with trust region Bayesian optimization (TuRBO) (Eriksson et al., 2019). For the largest tasks (Lasso, Molecules) we use acquisition batch size of 20 $\\mathit{\\Delta}\\mathit{q}=20\\mathit{\\Delta}$ ), and batch size 1 $\\mathit{\\Pi}_{q}=1$ ) for all others. ", "page_idx": 7}, {"type": "text", "text": "Implementation Details and Hyperparameters. Code to reproduce all results in the paper is available at https://github.com/nataliemaus/aabo. We implement EULBO and baseline methods using the GPyTorch (Gardner et al., 2018) and BoTorch (Balandat et al., 2020) packages. For all methods, we initialize using a set of 100 data points sampled uniformly at random in the search space. We use the same trust region hyperparameters as in (Eriksson et al., 2019). In Appendix B.1, we also evaluate an additional initialization strategy for the molecular design tasks. This alternative initialization matches prior work in using 10, 000 molecules from the GuacaMol dataset Brown et al. (2019) rather than the details we used above for consistency across tasks, but does achieve higher overall performance. ", "page_idx": 7}, {"type": "text", "text": "4.1 Tasks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Hartmann 6D. The widely used Hartmann benchmark function (Surjanovic and Bingham, 2013). ", "page_idx": 7}, {"type": "text", "text": "Lunar Lander. The goal of this task is to find an optimal 12-dimensional control policy that allows an autonomous lunar lander to consistently land without crashing. The final objective value we optimize is the reward obtained by the policy averaged over a set of 50 random landing terrains. For this task, we use the same controller setup used by Eriksson et al. (2019). ", "page_idx": 7}, {"type": "text", "text": "Rover. The rover trajectory optimization task introduced by Wang et al. (2018) consists of finding a 60-dimensional policy that allows a rover to move along some trajectory while avoiding a set of obstacles. We use the same obstacle set up as in Maus et al. (2023). ", "page_idx": 7}, {"type": "image", "img_path": "t7euV5dl5M/tmp/7f6be77910f89e2e7c34e2b925f599ee66950b902948a681de9bb8eec3284d65.jpg", "img_caption": ["Figure 3: Ablation study measuring the impact of EULBO optimization on various SVGP parameters. At each BO iteration, we use the standard ELBO objective to optimize the SVGP hyperparameters, variational parameters, and inducing point locations. We then refine some subset of these parameters by further optimizing them with respect to the EULBO objective. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Lasso DNA. We optimize the 180\u2212dimensional DNA task from the LassoBench library (\u0160ehi\u0107 et al., 2022) of benchmarks based on weighted LASSO regression (Gasso et al., 2009). ", "page_idx": 8}, {"type": "text", "text": "Molecular design tasks $\\bf{(x4)}$ . We select four challenging tasks from the Guacamol benchmark suite of molecular design tasks (Brown et al., 2019): Osimertinib MPO, Fexofenadine MPO, Median Molecules 1, and Median Molecules 2. We use the SELFIES-VAE introduced by Maus et al. (2022) to enable continuous 256 dimensional optimization. ", "page_idx": 8}, {"type": "text", "text": "4.2 Optimization Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In Figure 2, we plot the reward of the best point found by the optimizer after a given number of function evaluations. Error bars show the standard error of the mean over 20 replicate runs. EULBO with TuRBO outperforms the other baselines with TuRBO. Similarly, EULBO with standard BO outperforms the other standard BO baselines. One noteworthy observation is that neither acquisition function appears to consistently outperform the other. However, EULBO-SVGP almost always dominates ELBO-SVGP and often requires a small fraction of the number of oracle calls to achieve comparable performance. These results suggest that coupling data acquisition with approximate inference/model selection results in significantly more sample-efifcient optimization. ", "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "While the results in Fig. 2 demonstrate that EULBO-SVGP improves the BO performance it is not immediately clear to what extent joint optimization modifies the posterior approximation beyond what is obtained by standard ELBO optimization. To that end, in Fig. 3 we refine an ELBO-SVGP model with varying degrees of additional EULBO optimization. At every BO iteration we begin by obtaining a SVGP model (where the variational parameters, inducing point locations, and GP hyperparameters are all obtained by optimizing the standard ELBO objective). We then refine some subset of parameters (either the inducing points, the variational parameters, the GP hyperparameters, or all of the above) through additional optimization with respect to the EULBO objective. Interestingly, we find that tasks respond differently to the varying levels of EULBO refinement. In the case of Lasso DNA, there is not much of a difference between EULBO refinement on all parameters versus refinement on the variational parameters alone. On the other hand, the performance on Median Molecules 2 is clearly dominated by refinement on all parameters. Nevertheless, we see that EULBO is always beneficial, whether applied to all parameters or some subset. ", "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Scaling Bayesian Optimization to the Large-Budget Regime. BO has traditionally been confined to the small-budget optimization regime with a few hundred objective evaluations at most. However, recent interest in high-dimensional optimization problems has demonstrated the need to scale BO to large data acquisition budgets. For problems with ${\\sim}10^{3}$ data acquisitions, Hern\u00e1ndez-Lobato et al. (2017); Snoek et al. (2015); Springenberg et al. (2016) consider Bayesian neural networks (BNN; Neal, 1996), McIntire et al. (2016) use SVGP, and Wang et al. (2018) turn to ensembles of subsampled GPs. For problems with $\\gg10^{3}$ acquisitions, SVGP has become the de facto approach to alleviate computational complexity (Grifitfhs and Hern\u00e1ndez-Lobato, 2020; Maus et al., 2022, 2023; Stanton et al., 2022; Tripp et al., 2020; Vakili et al., 2021). As in this paper, many works have proposed modifications to SVGP to improve its performance in BO applications. Moss et al. (2023) proposed an inducing point placement based on a heuristic modification of determinantal point processes (Kulesza and Taskar, 2012), which we used for initialization, while Maddox et al. (2021) proposed a method for a fast online update strategy for SVGPs, which we utilize for the KG acquisition strategy. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Utility-Calibrated Approximate Inference. The utility-calibrated VI objective was first proposed by Lacoste\u2013Julien et al. (2011), where they used a coordinate ascent algorithm to maximize it. Since then, various extensions have been proposed: Ku\u015bmierczyk et al. (2019) leverage black-box variational inference (Ranganath et al., 2014; Titsias and L\u00e1zaro-Gredilla, 2014); Morais and Pillow (2022) use expectation-propagation (EP; Minka, 2001); Abbasnejad et al. (2015) employ importance sampling; Cobb et al. (2018) and Li and Zhang (2023) derive a specific variant for BNNs; and (Wei et al., 2021) derive a specific variant for GP classification. Closest to our work is the GP-based recommendation model learning algorithm by Abbasnejad et al. (2013), which sparsifies an EP-based GP approximation by maximizing a utility similar to those used in BO. ", "page_idx": 9}, {"type": "text", "text": "6 Limitations and Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The main limitation of our proposed approach is increased computational cost. While EULBO-SVGP still retains the $O(m^{3})$ computational complexity of standard SVGP, our practical implementation requires a warm-start: first fitting SVGP with the ELBO loss and then maximizing the acquisition function before jointly optimizing with the EULBO loss. Furthermore, EULBO optimization currently requires multiple tricks such as clipping and block-coordinate updates. In future work, we aim to develop a better understanding of the EULBO geometry in order to develop developing more stable, efifcient, and easy-to-use EULBO optimization schemes. Nevertheless, our results in Section 4 demonstrate that the additional computation of EULBO yields substantial improvements in BO dataefifciency, a desirable trade-off in many applications. Moreover, EULBO-SVGP is modular, and our experiments capture a fraction of its potential use. It can be applied to any decision-theoretic acquisition function, and it is likely compatible with non-standard Bayesian optimization problems such as cost-constrained BO (Snoek et al., 2012), causal BO (Aglietti et al., 2020), and many more. ", "page_idx": 9}, {"type": "text", "text": "More importantly, our paper highlights a new avenue for research in BO, where surrogate modeling, approximate inference, and data selection are jointly determined from a unified objective. Extending this idea to GP approximations beyond SVGP and acquisition functions beyond EI/KG may yield further improvements, especially in the increasingly popular high-throughput BO setting. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors thank the anonymous reviewers for suggestions that improved the quality of the work. ", "page_idx": 10}, {"type": "text", "text": "N. Maus was supported by the National Science Foundation Graduate Research Fellowship; K. Kim was supported by a gift from AWS AI to Penn Engineering\u2019s ASSET Center for Trustworthy AI; G. Pleiss was supported by NSERC and the Canada CIFAR AI Chair program; J. P. Cunningham was supported by the Gatsby Charitable Foundation (GAT3708), the Simons Foundation (542963), the NSF AI Institute for Artificial and Natural Intelligence (ARNI: NSF DBI 2229929), and the Kavli Foundation; J. R. Gardner was supported by NSF awards IIS-2145644 and DBI-2400135. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Ehsan Abbasnejad, Justin Domke, and Scott Sanner. Loss-calibrated Monte Carlo action selection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 29 of AAAI. AAAI Press, March 2015. (page 10)   \nM. Ehsan Abbasnejad, Edwin V. Bonilla, and Scott Sanner. Decision-theoretic sparsification for Gaussian process preference learning. In Machine Learning and Knowledge Discovery in Databases, volume 13717 of LNCS, pages 515\u2013530, Berlin, Heidelberg, 2013. Springer. (page 10)   \nVirginia Aglietti, Xiaoyu Lu, Andrei Paleyes, and Javier Gonz\u00e1lez. Causal Bayesian optimization. In Proceedings of the International Conference on Artificial Intelligence and Statistics, volume 108 of PMLR, pages 3155\u20133164. JMLR, June 2020. (page 10)   \nMaximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham, Andrew Gordon Wilson, and Eytan Bakshy. BoTorch: A framework for efifcient Monte-Carlo Bayesian optimization. In Advances in Neural Information Processing Systems, volume 33, pages 21524\u201321538. Curran Associates, Inc., 2020. (pages 2, 6, 7, 8, 16)   \nP. G. Bissiri, C. C. Holmes, and S. G. Walker. A general framework for updating belief distributions. Journal of the Royal Statistical Society Series B: Statistical Methodology, 78(5):1103\u20131130, 2016. (pages 2, 5)   \nDavid M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859\u2013877, April 2017. (pages 2, 3)   \nNathan Brown, Marco Fiscato, Marwin H.S. Segler, and Alain C. Vaucher. Guacamol: Benchmarking models for de novo molecular design. Journal of Chemical Information and Modeling, 59(3): 1096\u20131108, Mar 2019. (pages 8, 9)   \nAdam D. Cobb, Stephen J. Roberts, and Yarin Gal. Loss-Calibrated Approximate Inference in Bayesian Neural Networks. arXiv Preprint arXiv:1805.03901, arXiv, May 2018. (page 10)   \nA. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society: Series B (Methodological), 39(1):1\u201322, September 1977. (page 4)   \nDavid Eriksson, Michael Pearce, Jacob Gardner, Ryan D Turner, and Matthias Poloczek. Scalable global optimization via local Bayesian optimization. In Advances in Neural Information Processing Systems, volume 32, pages 5496\u20135507. Curran Associates, Inc., 2019. (pages 1, 2, 8)   \nPeter I Frazier. Knowledge-gradient methods for statistical learning. PhD thesis, Princeton University Princeton, 2009. (page 6)   \nPeter I Frazier. A tutorial on Bayesian optimization. arXiv Preprint arXiv:1807.02811, ArXiv, 2018. (page 1)   \nTh\u00e9o Galy-Fajou and Manfred Opper. Adaptive inducing points selection for Gaussian processes. arXiv Preprint arXiv:2107.10066, arXiv, 2021. (page 7)   \nJacob Gardner, Geof fPleiss, Kilian Q. Weinberger, David Bindel, and Andrew G. Wilson. GPyTorch: Blackbox matrix-matrix Gaussian process inference with GPU acceleration. In Advances in Neural Information Processing Systems, volume 31, pages 7576\u20137586. Curran Associates, Inc., 2018. (pages 8, 16)   \nRoman Garnett. Bayesian Optimization. Cambridge University Press, Cambridge, United Kingdom ; New York, NY, 2023. (pages 1, 2, 3, 6) Gilles Gasso, Alain Rakotomamonjy, and St\u00e9phane Canu. Recovering sparse signals with a certain family of nonconvex penalties and DC programming. IEEE Transactions on Signal Processing, 57 (12):4686\u20134698, 2009. (page 9) Ryan-Rhys Grifitfhs and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Constrained Bayesian optimization for automatic chemical design using variational autoencoders. Chemical Science, 11(2):577\u2013586,   \n2020. (pages 1, 10) James Hensman, Nicolo Fusi, and Neil D. Lawrence. Gaussian processes for big data. In Proceedings of the Conference on Uncertainty in Artificial Intelligence, pages 282\u2013290. AUAI Press, 2013. (pages 1, 3) Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, James Requeima, Edward O. Pyzer-Knapp, and Al\u00e1n Aspuru-Guzik. Parallel and distributed Thompson sampling for large-scale accelerated exploration of chemical space. In Proceedings of the International Conference on Machine Learning, volume 70 of PMLR, pages 1470\u20131479. JMLR, July 2017. (page 9) Prateek Jaiswal, Harsha Honnappa, and Vinayak A. Rao. Asymptotic consistency of loss-calibrated variational Bayes. Stat, 9(1):e258, 2020. (pages 2, 5) Prateek Jaiswal, Harsha Honnappa, and Vinayak Rao. On the statistical consistency of risk-sensitive bayesian decision-making. In Advances in Neural Information Processing Systems, volume 36, pages 53158\u201353200. Curran Associates, Inc., December 2023. (pages 2, 5) Martin Jankowiak, Geof fPleiss, and Jacob R. Gardner. Parametric gaussian process regressors. In Proceedings of the 37th International Conference on Machine Learning, ICML\u201920. JMLR.org,   \n2020. (page 19) Donald R. Jones, Matthias Schonlau, and William J. Welch. Efifcient global optimization of expensive black-box functions. Journal of Global Optimization, 13(4):455\u2013492, 1998. (pages 1, 2, 5) Michael I. Jordan, Zoubin Ghahramani, Tommi S. Jaakkola, and Lawrence K. Saul. An introduction to variational methods for graphical models. Machine Learning, 37(2):183\u2013233, 1999. (pages 1, 2,   \n3, 4) Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Proceedings of the International Conference on Learning Representations, San Diego, California, USA, 2015. (pages 15, 16) Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In Proceedings of the International Conference on Learning Representations, Banf,f AB, Canada, April 2014. (page 6) Jeremias Knoblauch, Jack Jewson, and Theodoros Damoulas. An optimization-centric view on Bayes\u2019 rule: Reviewing and generalizing variational inference. Journal of Machine Learning Research, 23 (132):1\u2013109, 2022. (pages 2, 5) Alex Kulesza and Ben Taskar. Determinantal point processes for machine learning. Foundations and Trends $\\circled R$ in Machine Learning, 5(2\u20133):123\u2013286, 2012. (page 10) Tomasz Ku\u015bmierczyk, Joseph Sakaya, and Arto Klami. Variational Bayesian decision-making for continuous utilities. In Advances in Neural Information Processing Systems, volume 32, pages   \n6395\u20136405. Curran Associates, Inc., 2019. (pages 4, 5, 10) Simon Lacoste\u2013Julien, Ferenc Husz\u00e1r, and Zoubin Ghahramani. Approximate inference for the loss-calibrated Bayesian. In Proceedings of the International Conference on Artificial Intelligence and Statistics, volume 15 of PMLR, pages 416\u2013424. JMLR, June 2011. (pages 2, 4, 10) Kenneth Lange. MM Optimization Algorithms. Society for Industrial and Applied Mathematics, Philadelphia, 2016. (pages 2, 4) Bolian Li and Ruqi Zhang. Long-tailed Classification from a Bayesian-decision-theory Perspective. arXiv Preprint arXiv:2303.06075, arXiv, 2023. (page 10) Wesley J Maddox, Samuel Stanton, and Andrew G Wilson. Conditioning sparse variational Gaussian processes for online decision-making. In Advances in Neural Information Processing Systems, volume 34, pages 6365\u20136379. Curran Associates, Inc., 2021. (pages 1, 2, 4, 6, 10) Alexander G. de G. Matthews, James Hensman, Richard Turner, and Zoubin Ghahramani. On sparse variational methods and the Kullback-Leibler divergence between stochastic processes. In Proceedings of the International Conference on Artificial Intelligence and Statistics, volume 51 of PMLR, pages 231\u2013239. JMLR, May 2016. (pages 1, 4)   \nNatalie Maus, Haydn Jones, Juston Moore, Matt J. Kusner, John Bradshaw, and Jacob Gardner. Local latent space Bayesian optimization over structured inputs. In Advances in Neural Information Processing Systems, volume 35, pages 34505\u201334518, December 2022. (pages 1, 9, 10)   \nNatalie Maus, Kaiwen Wu, David Eriksson, and Jacob Gardner. Discovering many diverse solutions with Bayesian optimization. In Proceedings of the International Conference on Artificial Intelligence and Statistics, volume 206, pages 1779\u20131798. PMLR, April 2023. (pages 1, 8, 10)   \nMitchell McIntire, Daniel Ratner, and Stefano Ermon. Sparse Gaussian Processes for Bayesian Optimization. In Proceedings of the Conference on Uncertainty in Artificial Intelligence, Jersey City, New Jersey, USA, 2016. AUAI Press. (page 9)   \nThomas P. Minka. Expectation propagation for approximate bayesian inference. In Proceedings of the Conference on Uncertainty in Artificial Intelligence, pages 362\u2013369, San Francisco, CA, USA, 2001. Morgan Kaufmann Publishers Inc. (page 10)   \nJonas Mockus. The Bayesian approach to global optimization. In System Modeling and Optimization, pages 473\u2013481. Springer, 1982. (page 1)   \nMichael J. Morais and Jonathan W. Pillow. Loss-calibrated expectation propagation for approximate Bayesian decision-making. Technical Report arXiv:2201.03128, arXiv, January 2022. (page 10)   \nHenry B. Moss, Sebastian W. Ober, and Victor Picheny. Inducing point allocation for sparse Gaussian processes in high-throughput Bayesian optimisation. In Proceedings of the International Conference on Artificial Intelligence and Statistics, volume 206 of PMLR, pages 5213\u20135230. JMLR, April 2023. (pages 1, 4, 7, 10, 16, 17, 18)   \nRadford M. Neal. Bayesian Learning for Neural Networks, volume 118 of Lecture Notes in Statistics. Springer New York, New York, NY, 1996. (page 9)   \nJoaquin Qui\u00f1onero-Candela and Carl Edward Rasmussen. A unifying view of sparse approximate Gaussian process regression. Journal of Machine Learning Research, 6(65):1939\u20131959, 2005. (page 1)   \nRajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In Proceedings of the International Conference on Artificial Intelligence and Statistics, volume 33 of PMLR, pages 814\u2013822. JMLR, April 2014. (page 10)   \nCarl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning. The MIT Press, November 2005. (page 1)   \nDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the International Conference on Machine Learning, volume 32 of PMLR, pages 1278\u20131286. JMLR, June 2014. (page 6)   \nChristian P. Robert. The Bayesian Choice: From Decision-Theoretic Foundations to Computational Implementation. Springer Texts in Statistics. Springer, New York Berlin Heidelberg, 2. ed edition, 2001. (pages 2, 3)   \nBobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE, 104(1): 148\u2013175, 2015. (pages 1, 5)   \nEdward Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs. In Advances in Neural Information Processing Systems, volume 18, pages 1257\u20131264. MIT Press, 2005. (page 5)   \nJasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of machine learning algorithms. Advances in neural information processing systems, 25:2951\u20132959, 2012. (page 10)   \nJasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Mostofa Patwary, Mr Prabhat, and Ryan Adams. Scalable Bayesian optimization using deep neural networks. In Proceedings of the International Conference on Machine Learning, volume 37 of PMLR, pages 2171\u20132180. JMLR, June 2015. (page 9)   \nJost Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter. Bayesian Optimization with Robust Bayesian Neural Networks. In Advances in Neural Information Processing Systems, volume 29, pages 4134\u20134142. Curran Associates, Inc., 2016. (page 9)   \nSamuel Stanton, Wesley Maddox, Nate Gruver, Phillip Maffettone, Emily Delaney, Peyton Greenside, and Andrew Gordon Wilson. Accelerating Bayesian optimization for biological sequence design with denoising autoencoders. In Proceedings of the International Conference on Machine Learning, volume 162 of PMLR, pages 20459\u201320478. JMLR, June 2022. (pages 1, 10)   \nSonja Surjanovic and Derek Bingham. Virtual library of simulation experiments: Test functions and datasets, 2013. (page 8)   \nAlexander Terenin, David R. Burt, Artem Artemev, Seth Flaxman, Mark van der Wilk, Carl Edward Rasmussen, and Hong Ge. Numerically stable sparse Gaussian processes via minimum separation using cover trees. Journal of Machine Learning Research, 25(26):1\u201336, 2024. (page 7)   \nMichalis Titsias. Variational learning of inducing variables in sparse gaussian processes. In Proceedings of the International Conference on Artificial Intelligence and Statistics, volume 5 of PMLR, pages 567\u2013574. JMLR, April 2009. (pages 1, 3)   \nMichalis Titsias and Miguel L\u00e1zaro-Gredilla. Doubly stochastic variational Bayes for non-conjugate inference. In Proceedings of the International Conference on Machine Learning, volume 32 of PMLR, pages 1971\u20131979. JMLR, June 2014. (pages 6, 10)   \nAustin Tripp, Erik Daxberger, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Sample-efifcient optimization in the latent space of deep generative models via weighted retraining. In Advances in Neural Information Processing Systems, volume 33, pages 11259\u201311272. Curran Associates, Inc., 2020. (pages 1, 10)   \nSattar Vakili, Henry Moss, Artem Artemev, Vincent Dutordoir, and Victor Picheny. Scalable Thompson sampling using sparse Gaussian process models. In Advances in Neural Information Processing Systems, volume 34, pages 5631\u20135643, 2021. (pages 1, 10)   \nKenan \u0160ehi\u0107, Alexandre Gramfort, Joseph Salmon, and Luigi Nardi. Lassobench: A high-dimensional hyperparameter optimization benchmark suite for LASSO. In Proceedings of the International Conference on Automated Machine Learning, volume 188 of PMLR, pages 2/1\u201324. JMLR, 25\u201327 Jul 2022. (page 9)   \nYixin Wang and David M. Blei. Frequentist consistency of variational Bayes. Journal of the American Statistical Association, 114(527):1147\u20131161, July 2019. (page 5)   \nZi Wang, Clement Gehring, Pushmeet Kohli, and Stefanie Jegelka. Batched large-scale bayesian optimization in high-dimensional spaces. In Proceedings of the International Conference on Artificial Intelligence and Statistics, volume 84 of PMLR, pages 745\u2013754. JMLR, March 2018. (pages 8, 9)   \nLarry Wasserman. All of statistics: a concise course in statistical inference. Springer Science & Business Media, 2013. (pages 2, 3)   \nYadi Wei, Rishit Sheth, and Roni Khardon. Direct loss minimization for sparse Gaussian processes. In Proceedings of the International Conference on Artificial Intelligence and Statistics, volume 130 of PMLR, pages 2566\u20132574. JMLR, March 2021. (page 10)   \nJames Wilson, Frank Hutter, and Marc Deisenroth. Maximizing acquisition functions for Bayesian optimization. In Advances in Neural Information Processing Systems, pages 9884\u20139895. Curran Associates, Inc., 2018. (pages 2, 7)   \nJian Wu, Matthias Poloczek, Andrew G Wilson, and Peter Frazier. Bayesian optimization with gradients. In Advances in Neural Information Processing Systems, volume 30, pages 5267\u20135278. Curran Associates, Inc., 2017. (page 2) ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Implementation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We will now provide additional details on the implementation. For the implementation, we treat the SVGP parameters, such as the variational parameters $\\lambda$ , inducing point locations $z$ , and hyperparameters $\\boldsymbol{\\theta}$ , equally. Therefore, for clarity, we will collectively denote them as ${\\pmb w}=(\\lambda,Z,\\theta)$ such that $\\pmb{w}\\in\\mathcal{W}\\triangleq\\bar{\\Lambda}\\times\\dot{\\mathcal{X}}^{m}\\times\\Theta$ , and the resulting SVGP variational approximation as $q_{w}$ . Then, the ELBO and EULBO are equivalently denoted as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{ELBO}}\\left(\\pmb{w};\\mathcal{D}\\right)\\triangleq\\mathcal{L}_{\\mathrm{ELBO}}\\left(\\lambda,\\pmb{Z},\\pmb{\\theta};\\mathcal{D}\\right)\\qquad\\qquad}\\\\ {\\mathcal{L}_{\\mathrm{EULBO}}\\left(\\pmb{x},\\pmb{w};\\mathcal{D}_{\\pmb{x}},\\mathcal{D}_{\\pmb{w}}\\right)\\triangleq\\mathbb{E}_{f\\sim q_{w}\\left(f\\right)}\\log u\\left(\\pmb{x},f;\\mathcal{D}_{\\pmb{x}}\\right)+\\mathcal{L}_{\\mathrm{ELBO}}\\left(\\pmb{w};\\mathcal{D}_{\\pmb{w}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Also, notice that the $\\mathcal{L}_{\\mathrm{EULBO}}$ separately denote the dataset to be passed to the utility and the ELBO. (Setting $\\mathcal{D}_{t}=\\mathcal{D}_{\\pmb{w}}=\\mathcal{D}_{\\pmb{x}}$ retrieves the original formulation in Eq. (8).) ", "page_idx": 14}, {"type": "text", "text": "Alternating Updates We perform block-coordinate ascent on the EULBO by alternating between maximizing over $\\pmb{x}$ as $\\pmb{w}$ . Using vanilla gradient descent, the $\\pmb{x}_{\\mathbf{\\alpha}}$ -update is equivalent to ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{x}\\leftarrow\\pmb{x}+\\gamma_{x}\\nabla_{x}\\mathcal{L}_{\\mathrm{EULBO}}\\left(\\pmb{x},\\pmb{w};\\mathcal{D}\\right)=\\pmb{x}+\\gamma_{x}\\nabla_{x}\\mathbb{E}_{f\\sim q_{w}\\left(f\\right)}\\log{u\\left(\\pmb{x},f;\\mathcal{D}\\right)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\gamma_{x}$ is the stepsize. On the other hand, for the $\\pmb{w}$ -update, we subsample the data such that we optimize the ELBO over a minibatch $S\\subset{\\mathcal{D}}$ of size $B=|\\boldsymbol{S}|$ as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{w\\gets w+\\gamma_{w}\\nabla_{w}\\mathcal{L}_{\\mathrm{EULBO}}\\left(x,w;S,\\mathcal{D}\\right)=w+\\gamma_{w}\\nabla_{w}\\left(\\mathbb{E}_{f\\sim q_{w}\\left(f\\right)}\\log u\\left(x,f;\\mathcal{D}\\right)+\\mathcal{L}_{\\mathrm{ELBO}}\\left(w;S\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\gamma_{w}$ is the stepsize. Naturally, the $\\pmb{w}$ -update is stochastic due to minibatching, while the $\\pmb{x}_{\\mathbf{\\alpha}}$ -update is deterministic. In practice, we leverage the Adam update rule (Kingma and Ba, 2015) instead of simple gradient descent. Together with gradient clipping, this alternating update scheme is much more robust than jointly updating $(x,w)$ . ", "page_idx": 14}, {"type": "text", "text": "Algorithm 1: EULBO Maximization Policy ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Input: SVGP parameters $\\pmb{w}_{0}=(\\lambda_{0},Z_{0},\\pmb{\\theta}_{0})$ , Dataset $\\mathcal{D}_{t}$ , BO utility function $u$ , Output: BO query \ud835\udc99\ud835\udc61+1 ", "page_idx": 14}, {"type": "text", "text": "1 \u22b3Compute Warm-Start Initializations 2 $\\pmb{w}\\leftarrow\\arg\\operatorname*{max}_{\\pmb{w}\\in\\mathcal{W}}\\mathcal{L}_{\\mathrm{ELBO}}\\left(\\pmb{w};\\mathcal{D}_{t}\\right)$ with $\\pmb{w}_{0}$ as initialization. 3 $\\pmb{x}\\leftarrow\\arg\\operatorname*{max}_{\\pmb{x}\\in\\mathcal{X}}\\int u\\left(\\pmb{x},f;\\mathcal{D}_{t}\\right)q_{\\pmb{w}}\\left(f\\right)\\mathsf{d}f$ 4 \u22b3Maximize EULBO 5 repeat \u22b3Update posterior approximation $q_{w}$ 6 Fetch minibatch $S$ from $\\mathcal{D}_{t}$ 7 Compute $g_{w}\\gets\\nabla_{w}\\mathcal{L}_{\\mathrm{EULBO}}\\left(\\pmb{x},\\pmb{w};S,\\mathcal{D}_{t}\\right)$ 8 Clip $\\pmb{g}_{w}$ with threshold $G_{\\mathrm{clip}}$ 9 $\\pmb{w}\\leftarrow\\mathrm{AdamStep}_{\\gamma_{w}}\\left(\\pmb{w},\\pmb{g}_{w}\\right)$ 10 \u22b3Update BO query $_x$ 11 Compute $g_{x}\\gets\\nabla_{x}\\mathcal{L}_{\\mathrm{EULBO}}\\left(x,\\pmb{w};S,\\mathcal{D}_{t}\\right)$ 12 Clip $\\pmb{g}_{\\pmb{x}}$ with threshold $G_{\\mathrm{clip}}$ 13 \ud835\udc99\u2190AdamStep\ud835\udefe (\ud835\udc99, \ud835\udc88\ud835\udc99) 14 $\\pmb{x}\\leftarrow\\mathrm{proj}_{\\mathcal{X}}\\left(\\pmb{x}\\right)$ 15 until until converged 16 $\\boldsymbol{x}_{t+1}\\gets\\boldsymbol{x}$ 17 ", "page_idx": 14}, {"type": "text", "text": "Overview of Pseudocode. The complete high-level view of the algorithm is presented in Algorithm 1, except for the acquisition-specific details. AdamStep\ud835\udefe $(x,g)$ applies the Adam stepsize rule (Kingma and Ba, 2015) to the current location $\\pmb{x}$ with the gradient estimate $\\pmb{g}$ and the stepsize \ud835\udefe. In practice, Adam is a \u201cstateful\u201d optimizer, which maintains two scalar-valued states for each scalar parameter. For this, we re-initialize the Adam states at the beginning of each BO step. ", "page_idx": 14}, {"type": "text", "text": "Initialization. In the initial BO step $t=0$ , we initialize $Z_{0}$ with the DPP-based inducing point selection strategy of Moss et al. (2023). For the remaining SVGP parameters $\\lambda_{0}$ and $\\theta_{0}$ , we used the default initialization of GPyTorch (Gardner et al., 2018). For the remaining BO steps $t>0$ , we use $\\pmb{w}$ from the previous BO step as the initialization $w_{0}$ of the current BO step. ", "page_idx": 15}, {"type": "text", "text": "Warm-Starting. Due to the non-convexity and multi-modality of both the ELBO and the acquisition function, it is critical to appropriately initialize the EULBO maximization procedure. As mentioned in Section 3.5, to warm-start the EULBO maximization procedure, we use the conventional 2-step scheme Eq. (5), where we maximize the ELBO and then maximize the acquisition function. For ELBO maximization, we apply Adam (Kingma and Ba, 2015) with the stepsize set as $\\gamma_{w}$ until the convergence criteria (described below) are met. For acquisition function maximization, we invoke the highly optimized BoTorch.optimize.optimize_acqf function (Balandat et al., 2020). ", "page_idx": 15}, {"type": "text", "text": "Minibatch Subsampling Strategy. As commonly done, we use the reshufilfng subsampling strategy where the dataset $\\mathcal{D}_{t}$ is shuflfed and partitioned into minibatches of size $B$ . The number of minibatches constitutes an \u201cepoch.\u201d The dataset is reshuflfed/repartitioned after going through a full epoch. ", "page_idx": 15}, {"type": "text", "text": "Convergence Determination. For both maximizing the ELBO during warm-starting and maximizing the EULBO, we continue optimization until we stop making progress or exceed $k_{\\mathrm{epochs}}$ number of epochs. That is if the ELBO/EULBO function value fails to make progress for $n_{\\mathrm{fail}}$ number of steps. ", "page_idx": 15}, {"type": "table", "img_path": "t7euV5dl5M/tmp/4e38a94223c85b9d07d42beb3757be7805e9c175a5627e4583c4d376279e854f.jpg", "table_caption": ["Table 1: Configurations of Hyperparameters used for the Experiments "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Hyperparameters. The hyperparameters used in our experiments are organized in Table 1. For the full-extent of the implementation details and experimental configuration, please refer to the supplementary code. ", "page_idx": 15}, {"type": "text", "text": "B Additional Plots ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We provide additional results and plots that were omitted from the main text. ", "page_idx": 16}, {"type": "text", "text": "B.1 Additional Results on Molecule Tasks ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Fig. 4, we provide plots on additional results that are similar to those in Fig. 2. On three of the molecule tasks, we use 10,000 random molecules from the GuacaMol dataset as initialization. This is more consistent with what has been done in previous works and achieves better overall optimization performance. ", "page_idx": 16}, {"type": "image", "img_path": "t7euV5dl5M/tmp/83abfebdb9f9c76b30c76da539967a97bb4712b9eeeec2c9126187fda93024fc.jpg", "img_caption": ["Figure 4: Additional optimization results on three molecule tasks using 10,000 random molecules from the GuacaMol dataset as initialization. Each line/shaded region represents the mean/standard error over 20 runs. We count oracle calls starting after these initialization evaluations for all methods. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.2 Separate Plots for BO and TuRBO Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we provide additional plots separating out BO and TuRBO results to make visualization easier. ", "page_idx": 16}, {"type": "image", "img_path": "t7euV5dl5M/tmp/c668e69b7603b4eb942b91c3c3db4a2c308a8b6327502b0031782529c922bcb0.jpg", "img_caption": ["Figure 5: BO-only optimization results of Fig. 2. We compare EULBO-SVGP, ELBO-SVGP, ELBO-SVGP with DPP inducing point placement (Moss et al., 2023), and exact GPs. These are a subset of the same results shown in Fig. 2. Each line/shaded region represents the mean/standard error over 20 runs. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "t7euV5dl5M/tmp/d91e0ada7b56b3f9935335693df0eaa91ab824ac160afd5c0a3a08c62042cde8.jpg", "img_caption": ["Figure 6: TuRBO-only optimization results of Fig. 2. We compare EULBO-SVGP, ELBO-SVGP, ELBO-SVGP with DPP inducing point placement (Moss et al., 2023), and exact GPs. These are a subset of the same results shown in Fig. 2. Each line/shaded region represents the mean/standard error over 20 runs. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.3 Effect of Number of Inducing Points ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For the results with approximate-GPs in Section 4, we used $m=100$ inducing points. In Fig. 7, we evaluate the effect of using a larger number of inducing points $m=1024)$ for EULBO-SVGP and ELBO-SVGP. ", "page_idx": 17}, {"type": "image", "img_path": "t7euV5dl5M/tmp/1a7f37699f3ee6e6037f45ab10eca33d759fa1c6bbdea6070c456cfe26e2a5a1.jpg", "img_caption": ["Figure 7: Ablating the number of inducing points used by EULBO-SVGP and ELBO-SVGP. As in Fig. 2, we compare running TuRBO with EULBO-SVGP and with ELBO-SVGP using $m=100$ inducing points used for both methods. We add two additional curves for TuRBO with EULBO-SVGP and TuRBO with ELBO-SVGP using $m=1024$ inducing points. Each line/shaded region represents the mean/standard error over 20 runs. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Fig. 7 shows that the number of inducing points has limited impact on the overall performance of TuRBO, and EULBO-SVGP outperforms ELBO-SVGP regardless of the number of inducing points used. ", "page_idx": 17}, {"type": "text", "text": "B.4 Effect of GP Objective ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The results in Section 4 used a standard SVGP objective. In this section, we evaluate the effect of using an alternative objective: the parametric Gaussian process regressor (PPGPR; Jankowiak et al., 2020) objective. PPGPR differs from the standard SVGP objective in that the variational approximation is optimized to maximize the predictive accuracy instead of matching the posterior. ", "page_idx": 18}, {"type": "image", "img_path": "t7euV5dl5M/tmp/86721e39a87145266e42dbb77e31ec68bf9e50fd8ba76f4b9c23c0d16567022a.jpg", "img_caption": ["Figure 8: Effect of using the PPGPR objective instead of the SVGP objective for EULBO-EI and ELBO-EI. As in Fig. 2, we compare running TuRBO with EULBO-EI and with ELBO-EI using an SVGP model for both methods. We add two additional curves for TuRBO with EULBO-EI with a PPGPR model, and TuRBO with ELBO-EI using a PPGPR model. Each line/shaded region represents the mean/standard error over 20 runs. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "We compare the choice of objective (PPGPR vs SVGP) in Fig. 8 and observe that the objective has limited impact on the overall performance of TuRBO. In particular, EULBO-EI outperforms ELBO-EI regardless of the GP objective. ", "page_idx": 18}, {"type": "text", "text": "C Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "table", "img_path": "t7euV5dl5M/tmp/495aae811dd9ccf8a97e6f89edc5b01d9bc63e9ceb863a2681ed283a688acfaf.jpg", "table_caption": ["Table 2: Internal Cluster Setup "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Type of Compute and Memory. All results in the paper required the use of GPU workers (one GPU per run of each method on each task). The majority of runs were executed on an internal cluster, where details are shown in Table 2, where each node was equipped with an NVIDIA RTX A5000 GPU. In addition, we used cloud compute resources for a short period leading up to the subsmission of the paper. We used 40 RTX 4090 GPU workers from runpod.io, where each GPU had approximately $24\\:\\mathrm{GB}$ of GPU memory. While we used 24 GB GPUs for our experiments, each run of our experiments only requires approximately 15 GB of GPU memory. ", "page_idx": 19}, {"type": "text", "text": "Execution Time. Each optimization run for non-molecule tasks takes approximately one day to finish. Since we run the molecule tasks out to a much larger number of function evaluations than other tasks (80000 total function evaluations for each molecule optimization task), each molecule optimization task run takes approximately 2 days of execution time. With all eight tasks, ten methods run, and 20 runs completed per method, results in Fig. 2 include 1600 total optimization runs (800 for molecule tasks and 800 for non-molecule tasks). Additionally, the two added curves in each plot in Fig. 3 required 160 additional runs (120 for molecule tasks and 40 for non-molecule task). Completing all of the runs needed to produce all of the results in this paper therefore required roughly 2680 total GPU hours. ", "page_idx": 19}, {"type": "text", "text": "Compute Resources Used During Preliminary Investigations. In addition to the computational resources required to produce experimental results in the paper discussed above, we spent approximately 500 hours of GPU time on preliminary investigations. This was done on the aforementioned internal cluster shown in Table 2. ", "page_idx": 19}, {"type": "text", "text": "D Wall-clock Run Times ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Table 3, we provide average wall-clock run times of different methods on the Lasso DNA optimization task. ", "page_idx": 19}, {"type": "text", "text": "Table 3: Average wall-clock run times for one full run of TuRBO on the Lasso DNA task. We compare the average wall-clock run time of TuRBO on all TuRBO methods from Figure 2. Note that we do not include the wall clock run time for TuRBO with Exact EI here because we only ran this method out to 2k oracle calls (rather than the full budget of $20\\mathbf{k}$ oracle calls). ", "page_idx": 19}, {"type": "table", "img_path": "t7euV5dl5M/tmp/d772262a0bdb43f48dfab1ddb49f9e7b2224a18b0977ebc620e8b6bc311792c9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: All stated claims are backed-up with results in Section 4 and the stated focus/scope of the paper accurately reflects what is discussed throughout the rest of the paper. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: See Section 6. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efifciency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 20}, {"type": "text", "text": "Justification: This work does not contain a formal theoretical analysis. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide detailed explanation of how our method works in Section 3 and all additional required details to reproduce results in Section 4 and Appendix A. Additionally, we have included a link to a public GitHub repository containing all of the source code used in the work in Section 4. This source code allows any reader to run our code to reproduce all results in the paper. Additionally, the README in the repository provides detailed instructions to make setting up the proper environment and running the code easy for users. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might sufifce, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufifcient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] Replace by [Yes] , [No] , or [NA] . ", "page_idx": 22}, {"type": "text", "text": "Justification: We have included a link to a public GitHub repository containing all of the source code used in the work in Section 4. This source code allows any reader to run our code to reproduce all results in the paper. Additionally, the README in the repository provides detailed instructions to make setting up the proper environment and running the code easy for users. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: All chosen hyper-parameters and implementation details are stated in section 4 and Appendix A. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: On all plots, we plot the mean taken over multiple random runs and include error bars to show the standard error over the runs. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufifcient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See Appendix C. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We have read the NeurIPS Code of Ethics and made sure to adhere to them in all aspects. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [No] . ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper is methodological, where the considered algorithm does not immediately pose societal risks. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efifciency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not use data with potential societal concerns. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: All creators of assets used to produce our results are cited in Section 4. All assets used are open source software or models. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not introduce new assets. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve human participants. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve live participants. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]