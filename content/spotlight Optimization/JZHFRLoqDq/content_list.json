[{"type": "text", "text": "Energy-Guided Continuous Entropic Barycenter Estimation for General Costs ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alexander Kolesov, Petr Mokrov, Igor Udovichenko & Milena Gazdieva Skolkovo Institute of Science and Technology Moscow, Russia   \n{a.kolesov, p.mokrov, i.udovichenko, m.gazdieva}@skoltech.ru ", "page_idx": 0}, {"type": "text", "text": "Gudmund Pammer ETH Z\u00fcrich Z\u00fcrich, Switzerland gudmund.pammer@math.ethz.ch ", "page_idx": 0}, {"type": "text", "text": "Anastasis Kratsios Vector Institute, McMaster University Ontario, Canada kratsioa@mcmaster.ca ", "page_idx": 0}, {"type": "text", "text": "Evgeny Burnaev & Alexander Korotin   \nSkolkovo Institute of Science and Technology Artificial Intelligence Research Institute Moscow, Russia   \n{e.burnaev, a.korotin}@skoltech.ru ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Optimal transport (OT) barycenters are a mathematically grounded way of averaging probability distributions while capturing their geometric properties. In short, the barycenter task is to take the average of a collection of probability distributions w.r.t. given OT discrepancies. We propose a novel algorithm for approximating the continuous Entropic OT (EOT) barycenter for arbitrary OT cost functions. Our approach is built upon the dual reformulation of the EOT problem based on weak OT, which has recently gained the attention of the ML community. Beyond its novelty, our method enjoys several advantageous properties: (i) we establish quality bounds for the recovered solution; (ii) this approach seamlessly interconnects with the Energy-Based Models (EBMs) learning procedure enabling the use of well-tuned algorithms for the problem of interest; (iii) it provides an intuitive optimization scheme avoiding min-max, reinforce and other intricate technical tricks. For validation, we consider several low-dimensional scenarios and image-space setups, including non-Euclidean cost functions. Furthermore, we investigate the practical task of learning the barycenter on an image manifold generated by a pretrained generative model, opening up new directions for real-world applications. Our code is available at https://github.com/justkolesov/EnergyGuidedBarycenters. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Averaging is a fundamental concept in mathematics and plays a central role in numerous applications. While it is a straightforward operation when applied to scalars or vectors in a linear space, the situation complicates when working in the space of probability distributions. Here, simple convex combinations can be inadequate or even compromise essential geometric features, which necessitates a different way of taking averages. To address this issue, one may carefully select a measure of distance that properly captures similarity in the space of probabilities. Then, the task is to find a procedure which identifies a \u2018center\u2019 that, on average, is closest to the reference distributions. One good choice for comparing and averaging probability distributions is provided by the family of Optimal Transport (OT) discrepancies [110]. They have clear geometrical meaning and practical ", "page_idx": 0}, {"type": "image", "img_path": "JZHFRLoqDq/tmp/3306bce38f389524e60119d590e567bd79ba20fcc8ba533183fb454f65db1704.jpg", "img_caption": ["(a) The unfolded sphere. ", "(b) The sphere viewed from different viewpoints. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Entropic barycenter $\\mathbb{Q}^{*}$ (5) of $N=4$ von Mises distributions $\\mathbb{P}_{n}$ on the sphere (see 5.1) estimated with our barycenter solver (Algorithm 1). The used transport costs are $c_{k}(x_{k},y)=\\textstyle{\\frac{1}{2}}\\big(\\operatorname{arccos}\\left\\langle x_{k},y\\right\\rangle\\big)^{2}$ . ", "page_idx": 1}, {"type": "text", "text": "interpretation [89, 97]. The corresponding problem of averaging probability distributions using OT discrepancies is known as the OT barycenter problem [1]. OT-based barycenters find application in various practical domains: domain adaptation [79], shape interpolation [98], Bayesian inference [101, 102], text scoring [18], style transfer [80], reinforcement learning [77]. ", "page_idx": 1}, {"type": "text", "text": "Over the past decade, the substantial demand from practitioners sparked the development of various methods tackling the barycenter problem. The research community\u2019s initial efforts were focused on the discrete OT barycenter setting, see Appendix B.1 for more details. The continuous setting turns out to be even more challenging, with only a handful of recent works devoted to this setup [72, 17, 59, 55, 32, 82, 14]. Most of these works are devoted to specific OT cost functions, e.g., deal with $\\ell_{2}^{2}$ barycenters [59, 55, 32, 82]; while others require non-trivial a priori selections [72] and have limiting expressivity and generative ability [17, 14], see $\\S3$ for a detailed discussion. ", "page_idx": 1}, {"type": "text", "text": "Contributions. We propose a novel approach for solving Entropy-regularized OT (EOT) barycenter problems, which alleviates the aforementioned limitations of existing continuous OT solvers. ", "page_idx": 1}, {"type": "text", "text": "1. We reveal an elegant reformulation of the EOT barycenter problem by combining weak dual form of EOT with the congruence condition (\u00a74.1); we derive a simple optimization procedure which closely relates to the standard training algorithm of Energy-Based models $(\\S4.2)$ .   \n2. We establish the generalization bounds as well as the universal approximation guarantees for our recovered EOT plans, which push the reference distributions to the barycenter $(\\S4.3)$ .   \n3. We validate the applicability of our approach on various toy and large-scale setups, including the RGB image domain (\u00a75). In contrast to previous works, we also pay attention to non-Euclidean OT costs. Specifically, we conduct a series of experiments looking for a barycenter on an image manifold of a pretrained GAN. In principle, the image manifold support may contribute to the interpretability and plausibility of the resulting barycenter distribution in downstream tasks. ", "page_idx": 1}, {"type": "text", "text": "Notations. We write $\\overline{{K}}=\\{1,2,\\ldots,K\\}$ . Throughout the paper $\\mathcal{X}\\subset\\mathbb{R}^{D^{\\prime}},\\mathcal{Y}\\subset\\mathbb{R}^{D}$ and $\\mathcal{X}_{k}\\subset\\mathbb{R}^{D_{k}}$ $(k\\in\\overline{{K}})$ are compact subsets of Euclidean spaces. Continuous functions on $\\mathcal{X}$ are denoted as $\\mathcal C(\\mathcal X)$ . Probability distributions on $\\mathcal{X}$ are $\\mathcal{P}(\\boldsymbol{\\mathcal{X}})$ . Absolutely continuous probability distributions on $\\mathcal{X}$ are denoted by $P_{\\mathrm{ac}}(\\mathcal{X})\\subset\\mathcal{P}(\\mathcal{X})$ . Given $\\dot{\\mathbb{P}}\\in\\mathcal{P}(\\mathcal{X}),\\mathbb{Q}\\in\\mathcal{P}(\\mathcal{Y})$ , we use $\\Pi(\\mathbb{P},\\mathbb{Q})$ to designate the set of transport plans, i.e., probability distributions on $\\mathcal X\\times\\mathcal X$ with the first and second marginals given by $\\mathbb{P}$ and $\\mathbb{Q}$ , respectively. The density of $\\mathbb{P}\\in\\mathcal{P}_{\\mathrm{ac}}(\\mathcal{X})$ w.r.t. the Lebesgue measure is denoted by $\\textstyle{\\frac{\\mathrm{d}\\mathbb{P}(x)}{\\mathrm{d}x}}$ . ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "First, we recall the formulations of EOT (\u00a72.1) and the barycenter problem (\u00a72.2). Next, we clarify the computational setup of the considered EOT barycenter task $(\\S2.3)$ . ", "page_idx": 1}, {"type": "text", "text": "2.1 Entropic Optimal Transport ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Consider distributions $\\mathbb{P}\\in\\mathcal{P}_{\\mathrm{ac}}(\\mathcal{X})$ , $\\mathbb{Q}\\in\\mathcal{P}_{\\mathrm{ac}}(\\mathcal{Y})$ , a continuous cost function $c:\\mathcal{X}\\times\\mathcal{Y}\\to\\mathbb{R}$ and a regularization parameter $\\epsilon>0$ . The entropic optimal transportation (EOT) problem between $\\mathbb{P}$ and $\\mathbb{Q}$ [15, 78] consists of finding a minimizer of ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathrm{EOT}_{c,\\epsilon}(\\mathbb{P},\\mathbb{Q})\\overset{\\mathrm{def}}{\\underset{\\pi\\in\\Pi(\\mathbb{P},\\mathbb{Q})}{=}}\\textstyle\\operatorname*{min}_{\\stackrel{\\mathrm{\\scriptsize(~\\it{K}~c(\\pi,y)~-~\\epsilon~})}{(x,y)\\sim\\pi}}-\\underset{x\\sim\\mathbb{P}}{\\mathbb{E}}H(\\pi(\\cdot|x))\\overset{\\mathrm{\\scriptsize(~\\it{K}~c(\\pi,y)~-~\\epsilon~})}{\\sim}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Note that (1) is not the only way to formulate EOT. One more popular and equivalent formulation [19, 83, 36] substitutes the conditional entropy term $\\mathbb{E}_{x\\sim\\mathbb{P}}H(\\pi(\\cdot|x))$ in (1) with full entropy $H(\\pi)$ . ", "page_idx": 1}, {"type": "text", "text": "A minimizer $\\pi^{*}\\in\\Pi(\\mathbb{P},\\mathbb{Q})$ of (1) is called the EOT plan; its existence and uniqueness are guaranteed, see, e.g., [16, Th. 3.3]. In practice, we usually do not require the EOT plan $\\pi^{*}$ but its conditional distributions $\\pi^{*}(\\cdot|x)\\in\\mathcal{P}(\\mathcal{P})$ as they prescribe how points $x\\in\\mathscr{X}$ are stochastically mapped to $\\boldsymbol{\\wp}$ [42, \u00a72]. We refer to $\\pi^{*}(\\cdot|x)$ as the conditional plans. ", "page_idx": 2}, {"type": "text", "text": "Weak OT dual formulation of the EOT problem. The EOT problem permits several dual formulations. In our paper, we use the one derived from the weak OT theory [39, Theorem 9.5]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{EOT}_{c,\\epsilon}(\\mathbb{P},\\mathbb{Q})=\\operatorname*{sup}_{f\\in{\\mathcal{C}}(y)}\\Big\\{\\underset{x\\sim\\mathbb{P}}{\\mathbb{E}}f^{C}(x)+\\underset{y\\sim\\mathbb{Q}}{\\mathbb{E}}f(y)\\Big\\},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $f^{C}:\\mathcal{X}\\to\\mathbb{R}$ is the so-called weak entropic $c$ -transform [9, Eq. 1.2] of the function (potential) $f$ . The transform is defined by ", "page_idx": 2}, {"type": "equation", "text": "$$\nf^{C}(x)\\stackrel{\\mathrm{def}}{=}\\operatorname*{min}_{\\mu\\in{\\mathcal{P}}(y)}\\Bigl\\{\\underset{y\\sim\\mu}{\\mathbb{E}}c(x,y)-\\epsilon H(\\mu)-\\underset{y\\sim\\mu}{\\mathbb{E}}f(y)\\Bigr\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We use the capital $C$ in $f^{C}$ to distinguish the weak transform from the classic $c$ -transform [89, $\\S1.6]$ or $(c,\\epsilon)$ -transform [76, \u00a72]. In particular, formulation (2) is not to be confused with the conventional EOT dual, see [78, Appendix A]. ", "page_idx": 2}, {"type": "text", "text": "For each $x\\in\\mathscr{X}$ , the minimizer $\\mu_{x}^{f}\\in\\mathcal{P}(\\mathcal{y})$ of the weak $c$ -transform (3) exists and is unique. Its density has particular form [78, Theorem 1]. Let $\\begin{array}{r}{Z_{c}(f,x)\\overset{\\mathrm{def}}{=}\\int_{\\mathcal{Y}}\\exp\\big(\\frac{f(y)-c(x,y)}{\\epsilon}\\big)\\mathrm{d}y}\\end{array}$ , then ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mu_{x}^{f}(y)}{\\mathrm{d}y}\\stackrel{\\mathrm{def}}{=}\\frac{1}{Z_{c}(f,x)}\\exp\\left(\\frac{f(y)-c(x,y)}{\\epsilon}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "By substituting (4) into (3) and carrying out straightforward manipulations, we arrive at an explicit formula $f^{C}(x)=-\\epsilon\\log Z_{c}(f,x)$ , see [78, Equation (14)]. ", "page_idx": 2}, {"type": "text", "text": "2.2 Entropic OT Barycenter ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider distributions $\\mathbb{P}_{k}\\in\\mathcal{P}_{\\mathrm{ac}}(\\mathcal{X}_{k})$ and continuous cost functions $c_{k}(\\cdot,\\cdot):\\mathcal{X}_{k}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}$ for $k\\in\\overline{{K}}$ . Given weights $\\lambda_{k}>0$ with $\\textstyle\\sum_{k=1}^{K}\\lambda_{k}=1$ , the EOT Barycenter problem is: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\overbrace{\\mathcal{L}^{*}\\overset{\\mathrm{def}}{=}\\operatorname*{inf}_{\\mathbb{Q}\\in\\mathcal{P}(\\mathcal{Y})}\\sum_{k=1}^{K}\\lambda_{k}\\mathrm{EOT}_{c_{k},\\epsilon}(\\mathbb{P}_{k},\\mathbb{Q})}^{\\bullet},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The case where $\\epsilon=0$ corresponds to the classical OT barycenter [1] and falls out of the scope of this paper. Note that the majority of previous research [20, 22, 30, 25, 68, 67] consider a bit different but equivalent EOT barycenter formulation, i.e., which has the same minimizers. The objective (5) is known as Schr\u00f6dinger barycenter problem [15, Table 1], see the extended discussion in Appendix B.3. It is worth noting that under mild assumptions the barycenter $\\mathbb{Q}^{*}$ which delivers optimal value of (5) exists and is unique, see Appendix A.7. ", "page_idx": 2}, {"type": "text", "text": "2.3 Computational aspects of the EOT barycenter task ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Barycenter problems, such as (5), are known to be challenging in practice [2]. To our knowledge, even when $\\mathbb{P}_{1},\\ldots,\\mathbb{P}_{K}$ are Gaussian distributions, there is no direct analytical solution neither for our entropic case $\\left(\\epsilon>0\\right)$ ), see the additional discussion in App. C.4, nor for the unregularized case [3, $\\epsilon=0$ ]. Moreover, in real-world scenarios, the distributions $\\mathbb{P}_{k}$ $k\\in\\overline{{K}})$ are typically not available explicitly but only through empirical samples (datasets). This aspect leads to the next learning setup. ", "page_idx": 2}, {"type": "text", "text": "We assume that each $\\mathbb{P}_{k}$ is accessible only by a limited number of i.i.d. empirical samples $X_{k}=\\{x_{k}^{1},x_{k}^{2},\\ldots x_{k}^{N_{k}}\\}\\sim{\\mathbb{P}}_{k}$ . Our aim is to approximate the optimal conditional plans $\\pi_{k}^{*}(\\cdot|x_{k})$ between the entire source distributions $\\mathbb{P}_{k}$ and the entire (unknown) barycenter $\\mathbb{Q}^{*}$ solving (5). The recovered plans should provide the out-of-sample estimation, i.e., allow generating samples from $\\pi_{k}^{*}(\\cdot|x_{k}^{\\mathrm{new}})$ , where $x_{k}^{\\mathrm{new}}$ is a new sample from $\\mathbb{P}_{k}$ which is not necessarily present in the train sample. ", "page_idx": 2}, {"type": "text", "text": "This setup corresponds to continuous OT [72, 59]. It differs from the discrete OT setup [19, 20] which aims to solve the barycenter task for discrete empirical distributions. Discrete OT are not well-suited for the out-of-sample estimation required in the continuous OT setup. ", "page_idx": 2}, {"type": "text", "text": "3 Related works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The taxonomy of OT solvers is large. Due to space constraints, we discuss here only methods within the continuous OT learning setup that solve the (E-)OT barycenter problem. These methods approximate OT maps or plans between the distributions $\\mathbb{P}_{k}$ and the barycenter $\\mathbb{Q}^{*}$ rather than just their empirical counterparts that are available from the training samples. A broader discussion of general-purpose discrete/continuous (E-)OT solvers is in Appendix B.1. ", "page_idx": 2}, {"type": "text", "text": "Continuous OT barycenter solvers are based on the unregularized or regularized OT barycenter problem within the continuous OT learning setup. The works [59, 32, 82, 55] are designed exclusively for the quadratic Euclidean cost $\\ell^{2}(x,y)\\ {\\stackrel{\\mathrm{def}}{=}}\\ {\\textstyle{\\frac{1}{2}}}\\|x\\--\\ y\\|_{2}^{2}$ . The OT problem with this particular cost exhibits several advantageous theoretical properties $[4,\\,\\S2]$ which are exploited by the aforementioned articles to build efficient pro", "page_idx": 3}, {"type": "table", "img_path": "JZHFRLoqDq/tmp/4ae2669861d3096f14275b6798a7ce9314dbd4f456c4ee7bd329ce31ec241a23.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparison of continuous OT bary solvers "], "page_idx": 3}, {"type": "text", "text": "cedures for barycenter estimation algorithms. In particular, [59, 32] utilize ICNNs [6] which parameterize convex functions, and [82] relies on a specific tree-based Schr\u00f6dinger Bridge reformulation. In contrast, our proposed approach is designed to handle the EOT problem with arbitrary cost functions $c_{1},\\ldots,c_{K}$ . In [72], they also consider regularized OT with non-Euclidean costs. Similar to our method, they take advantage of the dual formulation and exploit the so-called congruence condition $(\\S4)$ . However, their optimization procedure substantially differs. It necessitates selecting a fixed prior for the barycenter, which can be non-trivial. The work [14] takes a step further by directly optimizing the barycenter distribution in a variational manner, eliminating the need for a fixed prior. This modification increases the complexity of optimization and requires specific parametrization of the variational barycenter. In [17], the authors also parameterize the barycenter as a generative model. Their approach does not recover the OT plans, which differs from our learning setup $(\\S2.3)$ . A summary of the key properties is provided in Table 1, highlighting the fact that our approach overcomes many imperfections of competing methods. We are also aware of the novel continuous OT barycenter solver [52]. This approach is more recent than ours and is significantly based on the ideas from our article. Because of this, we exclude it from our comparisons. ", "page_idx": 3}, {"type": "text", "text": "4 Proposed Barycenter Solver ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the first two subsections, we work out our optimization objective $(\\S4.1)$ and its practical implementation $(\\S4.2)$ . In $\\S4.3$ , we alleviate the gap between the theory and practice by offering finite sample approximation guarantees and universality of NNs to approximate the solution. ", "page_idx": 3}, {"type": "text", "text": "4.1 Deriving the optimization objective ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In what follows, we analyze (5) from the dual perspectives. We introduce $\\mathcal{L}:\\mathcal{C}(\\mathcal{V})^{K}\\to\\mathbb{R}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}(f_{1},\\ldots,f_{K})\\stackrel{\\mathrm{def}}{=}\\sum_{k=1}^{K}\\lambda_{k}\\Big\\{\\underset{x_{k}\\sim\\mathbb{P}_{k}}{\\mathbb{E}}f_{k}^{C_{k}}(x_{k})\\Big\\}\\qquad\\Big[\\quad=-\\epsilon\\sum_{k=1}^{K}\\lambda_{k}\\Big\\{\\underset{x_{k}\\sim\\mathbb{P}_{k}}{\\mathbb{E}}\\log Z_{c_{k}}(f_{k},x_{k})\\Big\\}\\Big].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here $f_{k}^{C_{k}}$ denotes the weak entropic $c_{k}$ -transform (3) of $f_{k}$ . Following $\\S2.1$ , we see that it coincides with $-\\epsilon\\log Z_{c_{k}}(f_{k},x_{k})$ . Below we formulate our main theoretical result, which will allow us to solve the EOT barycenter task without optimization over all distributions on $\\boldsymbol{\\wp}$ . ", "page_idx": 3}, {"type": "text", "text": "Theorem 4.1 (Dual formulation of the EOT barycenter problem [proof ref.]). Problem (5) permits the following dual formulation: \u2217 (6) ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}^{*}=\\underset{f_{1},\\ldots,\\;f_{K}\\;\\in\\;c(\\mathcal{Y});\\atop\\sum_{k=1}^{K}\\lambda_{k}f_{k}\\;=0}{\\operatorname*{sup}}\\mathcal{L}\\big(f_{1},\\ldots,f_{K}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We refer to the constraint $\\textstyle\\sum_{k=1}^{K}\\lambda_{k}f_{k}=0$ as the congruence condition w.r.t. weights $\\{\\lambda_{k}\\}_{k=1}^{K}$ . The potentials $f_{k}$ appearing i n (6) play the same role as in (2). Notably, when $\\mathcal{L}(f_{1},\\cdot\\cdot\\cdot,f_{K})$ is close to $\\mathcal{L}^{\\ast}$ , the conditional optimal transport plans $\\pi_{k}^{*}(\\cdot|x_{k}),x_{k}\\in\\mathcal{X}_{k}$ , between $\\mathbb{P}_{k}$ and the barycenter distribution $\\mathbb{Q}^{*}$ can be approximately recovered through the potentials $f_{k}$ . This intuition is formalized in Theorem 4.2 below. First, for $f_{k}\\in\\mathcal{C}(\\mathcal{Y})$ , we define ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}\\pi^{f_{k}}(x_{k},y){\\overset{\\mathrm{def}}{=}}\\mathrm{d}\\mu_{x_{k}}^{f_{k}}(y)\\mathrm{d}\\mathbb{P}_{k}(x_{k})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and set $\\mathbb{Q}^{f_{k}}\\in\\mathcal{P}(\\mathcal{Y})$ to be the second marginal of $\\pi^{f_{k}}$ . ", "page_idx": 3}, {"type": "text", "text": "Theorem 4.2 (Quality bound of plans recovered from dual potentials [proof ref.]). Let $\\{f_{k}\\}_{k=1}^{K},f_{k}\\in$ $\\mathscr{C}(\\mathscr{y})$ be congruent potentials. Then we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}^{*}-\\mathcal{L}(f_{1},\\dots,\\underline{{\\mathcal{I}}}_{K})=\\epsilon\\sum_{k=1}^{K}\\lambda_{k}K L\\left(\\pi_{k}^{*}\\|\\pi^{f_{k}}\\right)\\geq\\epsilon\\sum_{k=1}^{K}\\lambda_{k}K L\\left(\\mathbb{Q}^{*}\\|\\mathbb{Q}^{f_{k}}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\pi_{k}^{*}\\in\\Pi(\\mathbb{P}_{k},\\mathbb{Q}^{*}),k\\in\\overline{{K}}$ are the EOT plans between $\\mathbb{P}_{k}$ and the barycenter distribution $\\mathbb{Q}^{*}$ . ", "page_idx": 3}, {"type": "text", "text": "Input: Distributions $\\mathbb{P}_{k}$ , $k\\in\\overline{{K}}$ accessible by samples; cost functions $c_{k}(x_{k},y):\\mathcal{X}_{k}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}$ ; the regularization coeff. $\\epsilon>0$ ; barycenter averaging coeff. $\\begin{array}{r}{\\lambda_{k}>0:\\sum_{k=1}^{K}\\lambda_{k}=1}\\end{array}$ ; MCMC procedure MCMC_proc; batch size $S>0$ ; NNs $f_{\\theta,k}:\\mathcal{V}\\rightarrow\\mathbb{R}$ , s.t. $\\textstyle\\sum_{k=1}^{K}\\lambda_{k}f_{\\theta,k}\\equiv0$ (see $\\S4.2)$ . ", "page_idx": 4}, {"type": "text", "text": "Output: Trained NNs $f_{\\theta^{*},k}$ recovering the conditional OT plans between $\\mathbb{P}_{k}$ and barycenter $\\mathbb{Q}^{*}$ ", "page_idx": 4}, {"type": "text", "text": "According to Theorem 4.2, an approximate solution $\\{f_{k}\\}_{k=1}^{K}$ of (6) yields distributions $\\pi^{f_{k}}$ which are close to the optimal plans $\\pi_{k}^{*}$ . Each $\\pi^{f_{k}}$ is formed by conditional distributions $\\mu_{x_{k}}^{f_{k}}$ , c.f. (4), with closed-form energy function, i.e., the unnormalized log-likelihood. Consequently, one can generate samples from $\\bar{\\mu}_{x_{k}}^{f_{k}}$ using standard MCMC techniques [7]. In the next subsection, we stick to the practical aspects of optimization of (6), which bears certain similarities to the training of Energy-Based models [69, 100, EBM]. ", "page_idx": 4}, {"type": "text", "text": "Relation to prior works. Works [72, 59] also aim to first get the dual potentials and then recover the barycenter, see the discussion in $\\S3$ for more details. ", "page_idx": 4}, {"type": "text", "text": "4.2 Practical Optimization Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To maximize the dual EOT barycenter objective (6), we replace the potentials $f_{k}\\in\\mathcal{C}(\\mathcal{Y})$ for $k\\in\\overline{{K}}$ with neural networks $f_{\\theta,k}\\,,\\,\\theta\\,\\in\\,\\Theta$ . In order to eliminate the constraint in (6), we parametrize $f_{\\theta,k}$ as $\\begin{array}{r}{g_{\\theta_{k}}-\\sum_{k^{\\prime}=1}^{K}\\lambda_{k^{\\prime}}g_{\\theta_{k^{\\prime}}}}\\end{array}$ taonmd , where $\\{g_{\\theta_{k}}\\,:\\,\\mathbb{R}^{D}\\,\\rightarrow\\,\\mathbb{R},\\theta_{k}\\,\\in\\,\\Theta_{k}\\}_{k=1}^{K}$ o. nOgurru eonbcjee ctciovne dfiutinocnti $\\textstyle\\sum_{k=1}^{K}\\lambda_{k}f_{\\theta,k}\\,\\equiv\\,0$ are neural networks. This . Note that $\\Theta\\!=\\!\\Theta_{1}\\times\\cdot\\cdot\\cdot\\times\\Theta_{K}$ $\\theta\\!=\\!(\\dot{\\theta_{1}},\\dots,\\theta_{K})\\in\\Theta$ ", "page_idx": 4}, {"type": "equation", "text": "$$\nL(\\theta)\\stackrel{\\mathrm{def}}{=}-\\epsilon\\sum_{k=1}^{K}\\lambda_{k}\\Big\\{{\\underset{x_{k}\\sim\\mathbb{P}_{k}}{\\mathbb{E}}}\\log Z_{c_{k}}(f_{\\theta,k},x_{k})\\Big\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The direct computation of the normalizing constant $Z_{c_{k}}$ may be infeasible. Still, the gradient of $L$ with respect to $\\theta$ can be derived similarly to [78, Theorem 3]: ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.3 (Gradient of the dual EOT barycenter objective [proof ref]). The gradient of $L$ satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial\\theta}L(\\theta)=-\\sum_{k=1}^{K}\\lambda_{\\stackrel{k}{x}_{k}\\sim\\mathbb{P}_{k}}\\^{}\\Bigg\\{{\\underset{y\\sim\\mu_{x_{k}}^{f_{\\theta,k}}}{\\mathbb{E}}}\\Bigg[\\frac{\\partial}{\\partial\\theta}f_{\\theta,k}(y)\\Bigg]\\Bigg\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "With this result, we can describe our proposed algorithm which maximizes $L$ using (9). ", "page_idx": 4}, {"type": "text", "text": "TRAINING. To perform stochastic gradient ascent step w.r.t. $\\theta$ , we approximate (9) with Monte-Carlo by drawing samples from $\\mathrm{d}\\pi^{f_{\\theta,k}}(\\bar{x_{k}},y)=\\mathrm{d}\\mu_{x_{k}}^{f_{\\theta,k}}(y)\\bar{\\mathrm{d}}\\mathbb{P}_{k}(x_{k})$ . Analogously to [78, $\\S3.2]$ , this can be achieved by a simple two-stage procedure. At first, we draw a random vector from . This is done by picking a random empirical sample from the available dataset $X_{k}$ . Then, we need to draw a sample from the distribution \u00b5fx\u03b8k, k. Since we know the negative energy (unnormalized log density) of $\\mu_{x_{k}}^{f_{\\theta,k}}$ by (4), we can sample from this distribution by applying an MCMC procedure which uses the negative energy function $\\epsilon^{-1}(f_{\\theta,k}(y)-c_{k}(x_{k},y))$ as the input. Our findings are summarized in Algorithm 1. Note that typical MCMC needs the energy functions, in particular, costs $c_{k}$ , to be differentiable. Otherwise, one can consider gradient-free procedures, e.g., [92, 104]. ", "page_idx": 4}, {"type": "text", "text": "In all our experiments, we use ULA [86, $\\S1.4.1]$ as a MCMC_proc. It is a conventional MCMC algorithm. Specifically, in order to draw a sample $y_{k}\\sim\\mu_{x_{k}}^{f_{\\theta,k}}$ , where $x_{k}\\in\\mathcal{X}_{k}$ , we initialize $y_{k}^{(0)}$ from the $D-$ dimensional distribution $\\mathcal{N}(0,I_{D})$ and then iterate the discretized Langevin dynamics: ", "page_idx": 4}, {"type": "equation", "text": "$$\ny_{k}^{(l+1)}\\!\\left\\vert\\left\\{y_{k}^{(l)}\\!+\\!\\frac{\\eta}{2\\epsilon}\\nabla_{y}\\left(f_{\\theta,k}(y)\\!-\\!c(x_{k},y)\\right)\\right\\vert_{y=y_{k}^{(l)}}\\!+\\!\\sqrt{\\eta}\\xi_{l}\\,,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\xi_{l}\\sim\\mathcal{N}(0,I_{D})$ , $l\\in\\{0,1,2,\\ldots,L\\}$ , $L$ is a number of steps, and $\\eta>0$ is a step size. Note that the iteration procedure above could be straightforwardly adapted to a batch scenario, i.e., we can simultaneously simulate the whole batch of samples $Y_{k}^{(l)}$ conditioned on $X_{k}^{(l)}$ . The particular values of number of steps $L$ and step size $\\eta$ are reported in the details of the experiments, see Appendix C. An alternative importance sampling-based approach for optimizing (9) is presented in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "INFERENCE. We use the same ULA procedure for sampling from the recovered optimal conditional plans $\\pi^{f_{\\theta^{*}}},\\boldsymbol{k}\\left(\\cdot\\middle|x_{k}\\right)$ , see the details on the hyperparameters $L,\\eta$ in $\\S5$ . ", "page_idx": 5}, {"type": "text", "text": "Relation to prior works. Learning a distribution of interest via its energy function (EBMs) is a well-established direction in generative modelling research [69, 111, 28, 100]. Similar to ours, the key step in most energy-based approaches is the MCMC procedure which recovers samples from a distribution accessible only by an unnormalized log density. Typically, various techniques are employed to improve the stability and convergence speed of MCMC, see, e.g., [27, 34, 113]. The majority of these techniques can be readily adapted to complement our approach. At the same time, the primary goal of this study is to introduce and validate the methodology for computing EOT barycenters in an energy-guided manner. Therefore, we opt for the simplest MCMC algorithm, even without the replay buffer [46], as it serves our current objectives. ", "page_idx": 5}, {"type": "text", "text": "4.3 Generalization Bounds and Universal Approximation with Neural Nets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this subsection, we answer the question of how far the recovered plans are from the EOT plan $\\pi_{k}^{*}$ between $\\mathbb{P}_{k}$ and $\\mathbb{Q}$ . In practice, for each distribution $\\mathbb{P}_{k}$ we know only the empirical samples $X_{k}=\\{x_{k}^{1},x_{k}^{2},\\ldots x_{k}^{N_{k}}\\}\\sim{\\mathbb{P}}_{k}$ , i.e., finite datasets. Besides, the available potentials $f_{k}$ , $k\\in\\overline{{K}}$ come from restricted classes of functions and satisfy the congruence condition. More precisely, we have $\\begin{array}{r}{f_{k}\\,=\\,g_{k}\\,-\\,\\sum_{k=1}^{K}\\lambda_{k}g_{k}}\\end{array}$ (\u00a74.2), where each $g_{k}$ is picked from some class $\\mathcal{G}_{k}$ of neural networks. Formally, we write $\\left(f_{1},\\ldots,f_{K}\\right)\\in\\overline{{\\mathcal{F}}}$ to denote the congruent potentials constructed this way from the functional classes $\\mathcal{G}_{1},\\ldots,\\mathcal{G}_{K}$ . Hence, in practice, we optimize the empirical version of (8): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{(f_{1},\\ldots,f_{K})\\in\\frac{\\widehat{\\mathcal{L}}}{\\mathcal{F}}}(f_{1},\\ldots,f_{K})\\stackrel{\\mathrm{def}}{=}\\operatorname*{max}_{(f_{1},\\ldots,f_{K})\\in\\overline{{\\mathcal{F}}}}\\sum_{k=1}^{K}\\frac{\\lambda_{k}}{N_{k}}\\sum_{n=1}^{N_{k}}f_{k}^{C_{k}}(x_{k}^{n});}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\quad(\\widehat{f}_{1},\\ldots,\\widehat{f_{K}})\\stackrel{\\mathrm{def}}{=}\\operatorname*{arg\\,max}_{(f_{1},\\ldots,f_{K})\\in\\overline{{\\mathcal{F}}}}\\widehat{\\mathcal{L}}(f_{1},\\ldots,f_{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "A natural question arises: How close are the recovered plans \u03c0f k to the EOT plans $\\pi_{k}^{*}$ between $\\mathbb{P}_{k}$ and $\\mathbb{Q}^{*}?$ Since our objective (8) is a sum of integrals over distributions $\\mathbb{P}_{k}$ , the generalization error can be straightforwardly decomposed into the estimation and approximation parts. ", "page_idx": 5}, {"type": "text", "text": "Proposition 4.4 (Decomposition of the generalization error [proof ref.]). The following bound holds: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\epsilon\\mathbb E\\displaystyle\\sum_{k=1}^{K}\\lambda_{k}K L\\left(\\pi_{k}^{*}\\|\\pi^{\\widehat f_{k}}\\right)\\leq\\overset{\\displaystyle\\widehat\\mathrm{~\\sum~}_{k=1}^{K}\\lambda_{k}\\mathbb E R e p_{X_{k}}(\\mathcal F_{k}^{C_{k}},\\mathbb{P}_{k})}{\\mathop{\\sum~}_{k=1}^{K}\\lambda_{k}\\mathbb E R e p_{X_{k}}(\\mathcal F_{k}^{C_{k}},\\mathbb{P}_{k})}\\+\\overbrace{\\left[\\mathcal L_{\\mathbf{\\phi}(f_{1},\\ldots,f_{K})\\in\\mathcal F}^{*}\\mathcal E_{k}\\right]}^{\\sum},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{F}_{k}^{C_{k}}\\overset{d e f}{=}\\{f_{k}^{C_{k}}\\mid(f_{1},\\ldots,f_{K})\\in\\overline{{\\mathcal{F}}}\\}$ , and the expectations are taken w.r.t. the random realization of the datasets $X_{1}{\\sim}\\mathbb{P}_{1},\\ldots,X_{K}{\\sim}\\mathbb{P}_{K}$ . Here $R e p_{X_{k}}(\\mathcal{F}_{k}^{C_{k}},\\mathbb{P}_{k})$ is the standard notion of the representativeness of the sample $X_{k}\\ w.r.t.$ functional class $\\mathcal{F}_{k}^{C_{k}}$ and distribution $\\mathbb{P}_{k}$ , see App. A.5. To bound the estimation error, we need to further bound the expected representativeness $\\mathbb{E}\\mathrm{Rep}_{X_{k}}(\\mathcal{F}_{k}^{C_{k}},\\mathbb{P}_{k})$ . Doing preliminary analysis, we found that it does not depend that much on the complexity of the functional class $\\mathcal{F}_{k}$ . However, it seems to heavily depend on the properties of the cost $c_{k}$ . We derive two bounds: a general one for Lipschitz (in $x$ ) costs and a better one for the feature-based quadratic costs. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.5 (Bound on ERep w.r.t. $C_{k}$ -transform classes [proof ref.]). (a) Let $\\mathcal{F}_{k}\\subset\\mathcal{C}(\\mathcal{Y})$ . Assume that $c_{k}(x,y)$ is Lipschitz in $x$ with the same Lipschitz constant for all $y\\!\\in\\!\\mathcal{Y}$ . Then ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}R e p_{X_{k}}(\\mathcal{F}_{k}^{C_{k}},\\mathbb{P}_{k})\\le O\\Big(N_{k}^{-1/(D_{k}+1)}\\Big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$(b)$ Let $\\begin{array}{r}{c_{k}(x_{k},y)=\\frac{1}{2}\\|u_{k}(x_{k})-v(y)\\|^{2}}\\end{array}$ , $\\mathcal{F}_{k}$ be a bounded (w.r.t. the supremum norm) subset of $\\mathscr{C}(\\mathscr{y})$ , $u_{k}:\\mathcal{X}_{k}\\rightarrow\\mathbb{R}^{D^{\\prime\\prime}}$ and $v:\\mathcal{V}\\rightarrow\\mathbb{R}^{D^{\\prime\\prime}}$ be continuous functions. Then ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}R e p_{X_{k}}(\\mathcal{F}_{k}^{C_{k}},\\mathbb{P}_{k})\\le O\\big(N_{k}^{-1/2}\\big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Substituting (11) or (12) to (10) immediately provides the statistical consistency when $N_{1},\\ldots,N_{K}\\to\\infty$ , i.e., vanishing of the estimation error when the sample size grows. ", "page_idx": 6}, {"type": "text", "text": "The case (a) here is not very practically useful as the rate suffers from the curse of dimensionality. Still, this result points to one intriguing property of our solver. Namely, we may take arbitrarily large set $\\mathcal{F}_{k}$ (even $\\begin{array}{r}{\\bar{\\mathcal{F}_{k}}=\\mathcal{C}(\\mathcal{D})!}\\end{array}$ ) and still have the guarantees of learning the barycenter. This happens because of $C_{k}$ -transforms: informally, they make functions $f_{k}\\in\\mathcal{F}_{k}$ smoother and \"simplify\" the set $\\mathcal{F}_{k}$ . In our experiments, we always work with the costs as in (b). As a result, our estimation error is $O(\\sum_{k=1}^{K}N_{k}^{-\\Bar{1}/2})$ ; this is a standard fast and dimension-free convergence rate. In practice, $\\mathcal{F}_{k}$ are usually neural nets. They are indeed bounded, as required in ${\\bf(b)}$ , if their weights are constrained. ", "page_idx": 6}, {"type": "text", "text": "While the estimation error usually decreases when the sample sizes tend to infinity, it is natural to wonder whether the approximation error can be also made arbitrarily small. We positively answer this question when the standard fully-connected neural nets (multi-layer perceptrons) are used. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.6 (Vanishing Approximation Error [proof ref.]). Let $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ be an activation function. Assume that it is non-affine and there is an $\\widetilde x\\in\\mathbb R$ at which $\\sigma$ is differentiable and $\\sigma^{\\prime}(\\widetilde{x})\\neq0$ . Then for every $\\delta>0$ there exist $K$ multi-layer pe r ceptrons $g_{k}:\\mathbb{R}^{D}\\rightarrow\\ddot{\\mathbb{R}}$ with activations $\\sigma$ f or which the congruent functions $\\begin{array}{r}{f_{k}=g_{k}-\\sum_{k=1}^{K}\\lambda_{k}g_{k}}\\end{array}$ satisfy ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\lambda_{k}K L\\left(\\pi_{k}^{*}\\|\\pi^{f_{k}}\\right)=({\\mathcal{L}}^{*}-{\\mathcal{L}}(f_{1},\\ldots,f_{K}))/\\epsilon<\\delta/\\epsilon.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Furthermore, each $g_{k}$ has width at-most $D+4$ . ", "page_idx": 6}, {"type": "text", "text": "Importantly, our Theorem 4.6 is more than just result on universal approximation since it deals with (i) congruent potentials and (ii) entropic $C_{k}$ -transforms. In particular, only specific properties of the entropic $C_{k}$ -transforms allow deriving the desired universal approximation statement, see the proof. ", "page_idx": 6}, {"type": "text", "text": "Summary. Our results of this section show that both the estimation and approximation errors can be made arbitrarily small given a sufficient amount of data and large neural nets, allowing to perfectly recover the EOT plans $\\pi_{k}^{*}$ . ", "page_idx": 6}, {"type": "text", "text": "Relation to prior works. To our knowledge, the generalization and the universal approximation are novel results with no analogues established for any other continuous barycenter solver. Our analysis shows that the EOT barycenter objective (8) is well-suited for statistical learning and approximation theory tools. This aspect distinguishes our work from the predecessors, where complex optimization objectives may not be as amenable to rigorous study. ", "page_idx": 6}, {"type": "text", "text": "4.4 Learning EOT barycenter on data manifold ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Averaging complex data distributions by means of EOT barycenter directly in the data space may be undesirable. In particular, for image data domain: ", "page_idx": 6}, {"type": "text", "text": "\u2022 the entropic barycenter contains noisy images, see, e.g., our MNIST 0/1 experiment, $\\S5.2$ . This is due to the \u201cblurring bias\u201d bias [15, 49] of our entropic barycenter setup and reliance on MCMC. \u2022 searching for (entropic) barycenter is not very practical for standard OT cost functions like $\\ell^{2}$ . It is known that the true unregularized $\\epsilon=0$ ) $\\ell^{2}$ -barycenter of several image domains consists of just some pixel-wise averages of images from these source domains, which is not practically useful. ", "page_idx": 6}, {"type": "text", "text": "To alleviate the problem, we propose solving the (entropic) barycenter problem on some a priori known data manifold $\\mathcal{M}$ , where we want the barycenter to be concentrated on. In our experiments $(\\S5.2,\\S5.3)$ these manifolds are given by pre-trained StyleGAN [50] generator models $G:\\mathcal{Z}\\to\\mathcal{Y}$ ; $\\mathcal{Z}$ is the latent space, $\\mathcal{M}=G(\\mathcal{Z})$ . Technically speaking, to adapt our Alogithm 1 for manifoldconstrained setup, we propose solving the barycenter problem in latent space $\\mathcal{Z}$ with modified cost functions $c_{k,G}(x_{k},z)\\,:=\\,c_{k}(x_{k},G(z))$ . We emphasize that such costs are general (not $\\ell^{2}$ cost!) because $G$ is a non-trivial StyleGAN generator. Hence, while our proposed manifold-constrained barycenter learning setup could be used on par with other OT barycenter solvers, these barycenter solvers should support general costs. In particular, the majority of competitive methods from Table 1 are not adjustable to the manifold setup as they work exclusively with $\\,\\!\\,\\ell^{2}$ . ", "page_idx": 6}, {"type": "text", "text": "Relation to prior works. While the utilization of data manifolds given by pre-trained (foundational) models is ubiquitous in generative modeling, the adaptation of this technique for Optimal Transport barycenter is a novel idea. Apart from our work, this idea is exploited in follow-up paper [52]. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We assess the performance of our barycenter solver on small-dimensional illustrative setups (\u00a75.1) and in image spaces $(\\S5.2,\\ \\S5.3)$ . The source code for our solver is written in the PyTorch framework and available at https://github.com/justkolesov/EnergyGuidedBarycenters. The experiments are issued in the form of convenient $^*$ .ipynb notebooks. Reproducing the most challenging experiments $(\\S5.2,\\ \\S5.3)$ requires less than 12 hours on a single TeslaV100 GPU. The details of the experiments, extended experimental results are in Appendix C, additional experiments with single-cell data are given in Appendix C.5. ", "page_idx": 7}, {"type": "text", "text": "Disclaimer. Evaluating how well our solver recovers the EOT barycenter is challenging because the ground truth barycenter is typically unknown. In some cases, the true unregularized barycenter ( $\\epsilon=0$ ) can be derived (see below). The EOT barycenter for sufficiently small $\\epsilon>0$ is expected to be close to the unregularized one. Therefore, in most cases, our evaluation strategy is to compare the computed EOT barycenter (for small $\\epsilon$ ) with the unregularized one. In particular, we use this strategy to quantitatively evaluate our solver in the Gaussian case, see Appendix C.4. ", "page_idx": 7}, {"type": "text", "text": "5.1 Barycenters of Toy Distributions ", "text_level": 1, "page_idx": 7}, {"type": "image", "img_path": "JZHFRLoqDq/tmp/0e100e22fe5dea0967b68f3f71d6eebbfbf1d651621f66f67bb179efae325734.jpg", "img_caption": ["Figure 2: $2D$ twister example: The true barycenter of 3 comets vs. the one computed by our solver with $\\epsilon=10^{-2}$ . Two costs $c_{k}$ are considered: the twisted cost (2a, 2b) and $\\ell^{2}$ (2c, 2d). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "2D Twister. Consider the map $u:\\mathbb{R}^{2}\\to\\mathbb{R}^{2}$ which, in the polar coordinate system, is represented by $\\mathbb{R}_{+}\\times[0,2\\pi)\\ni(r,\\theta)\\mapsto\\left(r,\\stackrel{\\cdot}{(}\\theta-r)\\bmod2\\pi\\right)$ . The cartesian version of $u$ is presented in Appendix C.1. Let $\\mathbb{P}_{1},\\mathbb{P}_{2},\\mathbb{P}_{3}$ be 2-dimensional distributions as shown in Fig. 2a. For these distributions and uniform weights $\\begin{array}{r}{\\lambda_{k}\\,=\\,\\frac{1}{3}}\\end{array}$ , the unregularized barycenter $\\mathbf{\\epsilon}_{\\epsilon}=0$ ) for the twisted cost $c_{k}(x_{k},y)=$ $\\begin{array}{r}{\\frac{1}{2}\\|u(x_{k})-u(y)\\|^{2}.}\\end{array}$ can be derived analytically, see Appendix C.1. The barycenter is the centered Gaussian distribution which is also shown in Fig. 2a. We run the experiment for this cost with $\\epsilon=10^{-2}$ , and the results are recorded in Fig. 2b. We see that it qualitatively coincides with the true barycenter. For completeness, we also show the EOT barycenter computed with our solver for $\\begin{array}{r}{\\ell^{2}(\\dot{x_{,}}y)=\\frac{1}{2}\\|x-y\\|^{2}}\\end{array}$ costs (Fig. 2c) and the same regularization \u03f5. The true $\\ell^{2}$ barycenter is estimated by using the free_support_barycenter solver from POT package [33]. We stress that the twisted cost barycenter and $\\bar{\\ell^{2}}$ barycenter differ, and so do the learned conditional plans: the $\\ell^{2}$ EOT plan (Fig. 2d) expectedly looks more well-structured while for the twisted cost (Fig. 2b) it becomes more chaotic due to non-trivial structure of this cost. ", "page_idx": 7}, {"type": "text", "text": "Sphere. In this experiment, we look for the barycenter of four von Mises distributions $\\mathbb{P}_{n}$ supported on 3D sphere, see Figure 1. The cost functions are $c_{k}(x_{k},y)\\ {\\stackrel{\\cdot\\,\\cdot\\,}{=}}\\ {\\frac{1}{2}}\\operatorname{arccos}^{2}\\langle x_{k},{\\stackrel{\\cdot\\,}{y}}\\rangle$ , the regularization is $\\epsilon=$ $10^{-2}$ . The learned potentials $f_{\\theta,k}$ operate with ambient $\\mathbb{R}^{3}$ vectors. When performing MCMC, we project each Langevin step to the sphere. Our qualitative results are shown on Figure 1. While the ground truth solution to the considered problem is unknown, the learned barycenter looks reasonable. This showcases the applicability of our approach to nonstandard non-quadratic experimental setups. ", "page_idx": 7}, {"type": "image", "img_path": "JZHFRLoqDq/tmp/ae05863e4903b103c88af237b19c0af778fc48a6e94952f70e36ea512fb094b3.jpg", "img_caption": ["Figure 3: Samples from the StyleGAN $G$ defining the polluted manifold $\\mathcal{M}$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.2 Barycenters of MNIST Classes 0 and 1 ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "A classic experiment considered in the continuous barycenter literature [32, 55, 82, 17] is averaging of distributions of MNIST 0/1 digits with weights $\\left({\\frac{1}{2}},{\\frac{1}{2}}\\right)$ in the grayscale image space $\\mathcal{X}_{1}=\\mathcal{X}_{2}=\\mathcal{Y}=[\\stackrel{\\circ}{-1},1]^{32\\times32}$ ", "page_idx": 7}, {"type": "text", "text": ". The true unregularized $\\epsilon=0$ ) $\\ell^{2}$ -barycenter images $y$ are direct pixel-wise averages $\\textstyle{\\frac{x_{1}+x_{2}}{2}}$ of pairs of images $x_{1}$ and $x_{2}$ ", "page_idx": 7}, {"type": "image", "img_path": "JZHFRLoqDq/tmp/85a62ba5bced5587d3601c7232bbb5d22abe7100bd69f3b37ebf35fe518c1b8c.jpg", "img_caption": ["(a) Maps from $\\mathbb{P}_{1}$ to the barycenter. (b) Maps from $\\mathbb{P}_{2}$ to the barycenter. (c) Maps from $\\mathbb{P}_{3}$ to the barycenter. ", "Figure 4: Experiment on the Ave, celeba! barycenter dataset. The plots compare the transported inputs $x_{k}\\sim\\mathbb{P}_{k}$ to the barycenter learned by various solvers. The true unregularized $\\ell^{2}$ barycenter of $\\mathbb{P}_{1},\\mathbb{P}_{2},\\mathbb{P}_{3}$ are the clean celebrity faces, see [55, \u00a75]. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "JZHFRLoqDq/tmp/4fc7ded991b87eb182871cef59b6ae36bd826742a88436e309d2e98447a9472c.jpg", "img_caption": ["(a) Learned plans from $\\mathbb{P}_{1}$ (zeros) to the barycenter. (b) Learned plans from $\\mathbb{P}_{2}$ (ones) to the barycenter. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 5: Qualitative comparison of barycenters of MNIST 0/1 digit classes computed with barycenter solvers in the image space w.r.t. the pixel-wise $\\dot{\\ell}^{2}$ . Solvers SCWB and WIN only learn the unregularized barycenter $\\epsilon=0$ ) directly in the data space. In turn, our solver learns the EOT barycenter in data space as well as it can learn EOT barycenter restricted to the StyleGAN manifold $\\langle\\epsilon=10^{-2}$ ). ", "page_idx": 8}, {"type": "text", "text": "coming from the $\\ell^{2}$ OT plan between 0\u2019s $(\\mathbb{P}_{1})$ and 1\u2019s $\\left(\\mathbb{P}_{2}\\right)$ ). In Fig. 5, we show the unregularized $\\ell^{2}$ barycenter computed by [32, SCWB], [55, WIN]. ", "page_idx": 8}, {"type": "text", "text": "Data space EOT barycenter. To begin with, we employ our solver to compute the $\\epsilon$ -regularized EOT $\\ell^{\\bar{2}}$ -barycenter directly in the image space $\\boldsymbol{\\wp}$ for $\\epsilon=\\mathrm{\\dot{1}0^{-2}}$ . We emphasize that the true entropic barycenter slightly differs from the unregularized one. To be precise, it is expected that regularized barycenter images are close to the unregularized barycenter images but with additional noise. In Fig. 5, we see that our solver (data space) recovers the noisy barycenter images exactly as expected. ", "page_idx": 8}, {"type": "text", "text": "Manifold-constrained EOT barycenter. Following the reasoning from $\\S4.4$ , we propose to restrict the search space for our algorithm to some pre-defined manifold $\\mathcal{M}$ . As discussed earlier, the support of the image-space unregularized $\\ell^{2}$ -barycenter is a certain subset of ${\\mathcal{M}}^{\\prime}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\left\\{{\\frac{x_{1}+x_{2}}{2}}\\ \\right|\\,x_{1}\\in$ $\\bar{\\mathrm{Supp}}(\\mathbb{P}_{1}),x_{2}\\,\\in\\,\\bar{\\mathrm{Supp}}(\\mathbb{P}_{2})\\}$ . To achieve this, we train a StyleGAN [50] model $G:\\mathcal{Z}\\stackrel{=}{\\rightarrow}\\mathcal{V}$ with $\\mathcal{Z}=\\mathbb{R}^{512}$ to generate some even larger manifold $\\mathcal{M}=G(\\mathcal{Z})$ which is expected to contain $\\mathcal{M}^{\\prime}$ . Namely, we use all possible pixel-wise half-sums $\\textstyle{\\frac{x_{1}+x_{2}}{2}}$ of digits 0 as $x_{1}$ and $\\{1,4,7\\}$ as $x_{2}$ , see Figure 3 with the trained StyleGAN samples. That is, our final constructed manifold $\\mathcal{M}$ is polluted with additional samples (e.g., averages of digits 0 and 7) which should not to lie in the support of the barycenter. Then, we use our solver with $\\epsilon=10^{-2}$ to search for the barycenter of $0/1$ digit distributions on $\\chi_{1},\\chi_{2}$ which lies in the latent space $\\mathcal{Z}$ w.r.t. costs $c_{k,G}(x,z)\\ {\\stackrel{\\mathrm{def}}{=}}\\ {\\textstyle{\\frac{1}{2}}}\\|x-G(z)\\|^{2}$ . This can be interpreted as learning the EOT $\\ell^{2}$ -barycenter in the ambient space but constrained to the StyleGAN-parameterized manifold $G({\\mathcal{Z}})$ . The barycenter $\\mathbb{Q}^{*}$ is some distribution of the latent variables $z$ , which can be pushed to the manifold $G(\\mathcal{Z})\\subset\\mathcal{Y}$ via $G(z)$ . ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "The results are in Fig. 5. There is (a) no noise compared to the data-space EOT barycenter because of the manifold constraint, and ${\\bf(b)}$ our solvers correctly ignores polluted samples from $\\mathcal{M}$ . ", "page_idx": 9}, {"type": "text", "text": "5.3 Evaluation on the Ave, celeba! Dataset ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In [55], the authors developed a theoretically grounded methodology for finding probability distributions whose unregularized $\\dot{\\ell}^{2}$ barycenter is known by construction. Based on the CelebA faces dataset [73], they constructed an Ave, celeba! dataset containing 3 degraded subsets of faces. The true $\\ell^{2}$ barycenter w.r.t. the weights $\\textstyle\\left({\\frac{1}{4}},{\\frac{1}{2}},{\\frac{1}{4}}\\right)$ is the distribution of Celeba faces itself. This dataset is used to test how well our approach recovers the barycenter. ", "page_idx": 9}, {"type": "table", "img_path": "JZHFRLoqDq/tmp/3b433d6a3b408071ac2ddaf259da057dbaf4eae5ec430c09999e88446b4be7ff.jpg", "table_caption": ["Table 2:FID scores of images mapped from inputs $\\mathbb{P}_{k}$ to the barycenter. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "We follow the EOT manifold-constrained setup $(\\S4.4)$ and train the StyleGAN on unperturbed celeba faces. This might sound a little bit unfair, but our goal is to demonstrate the learned transport plan to the constrained barycenter rather than unconditional barycenter samples (recall the setup in $\\S2.3)$ . Hence, we learn the constrained EOT barycenter with $\\epsilon=10^{-4}$ . In Fig. 4, we present the results, depicting samples from the learned plans from each $\\mathbb{P}_{k}$ to the barycenter. Overall, the map is qualitatively good, although sometimes failures in preserving the image content may occur. This is presumably due to MCMC inference getting stuck in local minima of the energy landscape. For comparison, we also show the results of the solvers by [32, SCWB], [55, WIN]. Additionally, we report the FID score [45] for images mapped to the barycenter in Table 2 (std. deviations for our method correspond to running the inference with different random seeds). Owing to the manifold-constrained setup, the FID score of our solver is significantly smaller. ", "page_idx": 9}, {"type": "text", "text": "6 Potential Impact, Limitations and Broader Impact ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Potential impact. In our work, we propose a novel approach for solving EOT barycenter problems which is applicable to general OT costs. From the practical viewpoint, we demonstrate the ability to restrict the sought-for barycenter to the image manifold by utilizing a pretrained generative model. Our findings may be applicable to a list of important real-world applications, see Appendix B.2. We believe that our large-scale barycenter solver will leverage industrial & socially-important problems. ", "page_idx": 9}, {"type": "text", "text": "Methodological limitations. The methodological limitations of our approach are mostly the same as those of EBMs. It is worth mentioning the usage of MCMC during the training/inference. The basic ULA algorithm which we use in $\\S4.2$ may poorly converge to the desired distribution $\\mu_{x}^{f}$ . In addition, MCMC sampling is time-consuming. We leave the search for more efficient sampling procedures for our solver, e.g., [71, 99, 43, 81, 47, 108, 66, 26], for future research. We also note that our theoretical analysis in $\\S4.3$ does not take into the account the optimization errors appearing due to the gradient descent and MCMC. The analysis of these quantities is a completely different domain in machine learning and out of the scope of our work. As the most generative modelling research, we do not attempt to analyse these errors. ", "page_idx": 9}, {"type": "text", "text": "Problem setup limitations. Our paper aims at solving Entropic OT barycenter problem. In the image data space, due to utilization of the Entropy, the learned barycenter distribution may contain noisy images. However, the utilization of our proposed StyleGAN-inspired manifold technique entirely alleviates the problem with the noise. This is demonstrated by our latent-space experiments with MNIST 0/1 (manifold space) and Ave Celeba! dataset. ", "page_idx": 9}, {"type": "text", "text": "Broader impact. This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Skoltech was supported by the Analytical center under the RF Government (subsidy agreement 000000D730321P5Q0002, Grant No. 70-2021-00145 02.11.2021). AK acknowledges financial support from the NSERC Discovery Grant No. RGPIN-2023-04482. We would like to express special thanks to Vladimir Vanovskiy from Skoltech for the insightful discussions and details on geological modelling (Appendix B.2). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Martial Agueh and Guillaume Carlier. Barycenters in the wasserstein space. SIAM Journal on Mathematical Analysis, 43(2):904\u2013924, 2011.   \n[2] Jason M Altschuler and Enric Boix-Adsera. Wasserstein barycenters are np-hard to compute. SIAM Journal on Mathematics of Data Science, 4(1):179\u2013203, 2022.   \n[3] Pedro C \u00c1lvarez-Esteban, E Del Barrio, JA Cuesta-Albertos, and C Matr\u00e1n. A fixed-point approach to barycenters in wasserstein space. Journal of Mathematical Analysis and Applications, 441(2):744\u2013762, 2016.   \n[4] Luigi Ambrosio and Nicola Gigli. A User\u2019s Guide to Optimal Transport, pages 1\u2013155. Springer Berlin Heidelberg, Berlin, Heidelberg, 2013.   \n[5] Brandon Amos. On amortizing convex conjugates for optimal transport. In The Eleventh International Conference on Learning Representations, 2022.   \n[6] Brandon Amos, Lei Xu, and J Zico Kolter. Input convex neural networks. In International Conference on Machine Learning, pages 146\u2013155. PMLR, 2017. [7] Christophe Andrieu, Nando De Freitas, Arnaud Doucet, and Michael I Jordan. An introduction to mcmc for machine learning. Machine learning, 50:5\u201343, 2003.   \n[8] Arip Asadulaev, Alexander Korotin, Vage Egiazarian, and Evgeny Burnaev. Neural optimal transport with general cost functionals. In The Twelfth International Conference on Learning Representations, 2024. [9] Julio Backhoff-Veraguas, Mathias Beiglb\u00f6ck, and Gudmund Pammer. Existence, duality, and cyclical monotonicity for weak transport costs. Calculus of Variations and Partial Differential Equations, 58(6):203, 2019.   \n[10] Peter L. Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: risk bounds and structural results. J. Mach. Learn. Res., 3:463\u2013482, 2002.   \n[11] Jean-David Benamou, Guillaume Carlier, Marco Cuturi, Luca Nenna, and Gabriel Peyr\u00e9. Iterative bregman projections for regularized transportation problems. SIAM Journal on Scientific Computing, 37(2):A1111\u2013A1138, 2015.   \n[12] Elsa Cazelles, Felipe Tobar, and Joaquin Fontbona. A novel notion of barycenter for probability distributions based on optimal weak mass transport. Advances in Neural Information Processing Systems, 34:13575\u201313586, 2021.   \n[13] Tianrong Chen, Guan-Horng Liu, and Evangelos Theodorou. Likelihood training of schr\u00f6dinger bridge using forward-backward SDEs theory. In International Conference on Learning Representations, 2022.   \n[14] Jinjin Chi, Zhiyao Yang, Ximing Li, Jihong Ouyang, and Renchu Guan. Variational wasserstein barycenters with c-cyclical monotonicity regularization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 7157\u20137165, 2023.   \n[15] L\u00e9na\u00efc Chizat. Doubly regularized entropic wasserstein barycenters. arXiv preprint arXiv:2303.11844, 2023.   \n[16] Christian Clason, Dirk A Lorenz, Hinrich Mahler, and Benedikt Wirth. Entropic regularization of continuous optimal transport problems. Journal of Mathematical Analysis and Applications, 494(1):124432, 2021.   \n[17] Samuel Cohen, Michael Arbel, and Marc Peter Deisenroth. Estimating barycenters of measures in high dimensions. arXiv preprint arXiv:2007.07105, 2020.   \n[18] Pierre Colombo, Guillaume Staerman, Chlo\u00e9 Clavel, and Pablo Piantanida. Automatic text evaluation through the lens of Wasserstein barycenters. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10450\u201310466, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.   \n[19] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013.   \n[20] Marco Cuturi and Arnaud Doucet. Fast computation of wasserstein barycenters. In International conference on machine learning, pages 685\u2013693. PMLR, 2014.   \n[21] Marco Cuturi and Gabriel Peyr\u00e9. A smoothed dual approach for variational wasserstein problems. SIAM Journal on Imaging Sciences, 9(1):320\u2013343, 2016.   \n[22] Marco Cuturi and Gabriel Peyr\u00e9. Semidual regularized optimal transport. SIAM Review, 60(4):941\u2013965, 2018.   \n[23] Max Daniels, Tyler Maunu, and Paul Hand. Score-based generative neural networks for largescale optimal transport. Advances in neural information processing systems, 34:12955\u201312965, 2021.   \n[24] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schr\u00f6dinger bridge with applications to score-based generative modeling. Advances in Neural Information Processing Systems, 34:17695\u201317709, 2021.   \n[25] Eustasio del Barrio and Jean-Michel Loubes. The statistical effect of entropic regularization in optimal transportation, 2020.   \n[26] Yilun Du, Conor Durkan, Robin Strudel, Joshua B Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, and Will Sussman Grathwohl. Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and mcmc. In International Conference on Machine Learning, pages 8489\u20138510. PMLR, 2023.   \n[27] Yilun Du, Shuang Li, B. Joshua Tenenbaum, and Igor Mordatch. Improved contrastive divergence training of energy based models. In Proceedings of the 38th International Conference on Machine Learning (ICML-21), 2021.   \n[28] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. Advances in Neural Information Processing Systems, 32, 2019.   \n[29] J. Dugundji. An extension of Tietze\u2019s theorem. Pacific J. Math., 1:353\u2013367, 1951.   \n[30] Pavel Dvurechenskii, Darina Dvinskikh, Alexander Gasnikov, Cesar Uribe, and Angelia Nedich. Decentralize and randomize: Faster algorithm for wasserstein barycenters. Advances in Neural Information Processing Systems, 31, 2018.   \n[31] Jiaojiao Fan, Shu Liu, Shaojun Ma, Hao-Min Zhou, and Yongxin Chen. Neural monge map estimation and its applications. Transactions on Machine Learning Research, 2023. Featured Certification.   \n[32] Jiaojiao Fan, Amirhossein Taghvaei, and Yongxin Chen. Scalable computations of wasserstein barycenter via input convex neural networks. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 1571\u20131581. PMLR, 18\u201324 Jul 2021.   \n[33] R\u00e9mi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aur\u00e9lie Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, L\u00e9o Gautheron, Nathalie T.H. Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony Schutz, Vivien Seguy, Danica J. Sutherland, Romain Tavenard, Alexander Tong, and Titouan Vayer. Pot: Python optimal transport. Journal of Machine Learning Research, 22(78):1\u20138, 2021.   \n[34] Ruiqi Gao, Yang Song, Ben Poole, Ying Nian Wu, and Diederik P Kingma. Learning energybased models by diffusion recovery likelihood. In International Conference on Learning Representations, 2021.   \n[35] Milena Gazdieva, Alexander Korotin, Daniil Selikhanovych, and Evgeny Burnaev. Extremal domain translation with neural optimal transport. In Advances in Neural Information Processing Systems, 2023.   \n[36] Aude Genevay. Entropy-regularized optimal transport for machine learning. PhD thesis, Paris Sciences et Lettres (ComUE), 2019.   \n[37] Aude Genevay, Marco Cuturi, Gabriel Peyr\u00e9, and Francis Bach. Stochastic optimization for large-scale optimal transport. Advances in neural information processing systems, 29, 2016.   \n[38] Lee-Ad Gottlieb, Aryeh Kontorovich, and Robert Krauthgamer. Adaptive metric dimensionality reduction. volume 620, pages 105\u2013118. 2016.   \n[39] Nathael Gozlan, Cyril Roberto, Paul-Marie Samson, and Prasad Tetali. Kantorovich duality for general transport costs and applications. Journal of Functional Analysis, 273(11):3327\u20133405, 2017.   \n[40] Hao Guan and Mingxia Liu. Domain adaptation for medical image analysis: a survey. IEEE Transactions on Biomedical Engineering, 69(3):1173\u20131185, 2021.   \n[41] Nikita Gushchin, Alexander Kolesov, Alexander Korotin, Dmitry Vetrov, and Evgeny Burnaev. Entropic neural optimal transport via diffusion processes. In Advances in Neural Information Processing Systems, 2023.   \n[42] Nikita Gushchin, Alexander Kolesov, Petr Mokrov, Polina Karpikova, Andrey Spiridonov, Evgeny Burnaev, and Alexander Korotin. Building the bridge of schr\\\" odinger: A continuous entropic optimal transport benchmark. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.   \n[43] Raza Habib and David Barber. Auxiliary variational mcmc. In International Conference on Learning Representations, 2018.   \n[44] Pierre Henry-Labordere. (martingale) optimal transport and anomaly detection with neural networks: A primal-dual algorithm. arXiv preprint arXiv:1904.04546, 2019.   \n[45] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.   \n[46] Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural computation, 14(8):1771\u20131800, 2002.   \n[47] Matthew Hoffman, Pavel Sountsov, Joshua V Dillon, Ian Langmore, Dustin Tran, and Srinivas Vasudevan. Neutra-lizing bad geometry in hamiltonian monte carlo using neural transport. arXiv preprint arXiv:1903.03704, 2019.   \n[48] Syunsuke Ikeda, Gary Parker, and Kenji Sawai. Bend theory of river meanders. part 1. linear development. Journal of Fluid Mechanics, 112:363\u2013377, 1981.   \n[49] Hicham Janati, Marco Cuturi, and Alexandre Gramfort. Debiased sinkhorn barycenters. In International Conference on Machine Learning, pages 4692\u20134701. PMLR, 2020.   \n[50] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401\u20134410, 2019.   \n[51] Dominik Klein, Th\u00e9o Uscidda, Fabian Theis, and Marco Cuturi. Generative entropic neural optimal transport to map within and across spaces. arXiv preprint arXiv:2310.09254, 2023.   \n[52] Alexander Kolesov, Petr Mokrov, Igor Udovichenko, Milena Gazdieva, Gudmund Pammer, Evgeny Burnaev, and Alexander Korotin. Estimating barycenters of distributions with neural optimal transport. In Forty-first International Conference on Machine Learning, 2024.   \n[53] Ekaterina Kondrateva, Marina Pominova, Elena Popova, Maxim Sharaev, Alexander Bernstein, and Evgeny Burnaev. Domain shift in computer vision models for mri data analysis: an overview. In Thirteenth International Conference on Machine Vision, volume 11605, pages 126\u2013133. SPIE, 2021.   \n[54] Alexander Korotin, Vage Egiazarian, Arip Asadulaev, Alexander Safin, and Evgeny Burnaev. Wasserstein-2 generative networks. In International Conference on Learning Representations, 2021.   \n[55] Alexander Korotin, Vage Egiazarian, Lingxiao Li, and Evgeny Burnaev. Wasserstein iterative networks for barycenter estimation. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[56] Alexander Korotin, Nikita Gushchin, and Evgeny Burnaev. Light schr\u00f6dinger bridge. In The Twelfth International Conference on Learning Representations.   \n[57] Alexander Korotin, Alexander Kolesov, and Evgeny Burnaev. Kantorovich strikes back! wasserstein gans are not optimal transport? Advances in Neural Information Processing Systems, 35:13933\u201313946, 2022.   \n[58] Alexander Korotin, Lingxiao Li, Aude Genevay, Justin M Solomon, Alexander Filippov, and Evgeny Burnaev. Do neural optimal transport solvers work? a continuous wasserstein-2 benchmark. Advances in Neural Information Processing Systems, 34:14593\u201314605, 2021.   \n[59] Alexander Korotin, Lingxiao Li, Justin Solomon, and Evgeny Burnaev. Continuous wasserstein-2 barycenter estimation without minimax optimization. In International Conference on Learning Representations, 2021.   \n[60] Alexander Korotin, Daniil Selikhanovych, and Evgeny Burnaev. Kernel neural optimal transport. In The Eleventh International Conference on Learning Representations, 2023.   \n[61] Alexander Korotin, Daniil Selikhanovych, and Evgeny Burnaev. Neural optimal transport. In The Eleventh International Conference on Learning Representations, 2023.   \n[62] Anastasis Kratsios and L\u00e9onie Papon. Universal approximation theorems for differentiable geometric deep learning. J. Mach. Learn. Res., 23:Paper No. [196], 73, 2022.   \n[63] Roman Krawtschenko, C\u00e9sar A Uribe, Alexander Gasnikov, and Pavel Dvurechensky. Distributed optimization with quantization for computing wasserstein barycenters. arXiv preprint arXiv:2010.14325, 2020.   \n[64] Alexey Kroshnin, Nazarii Tupitsa, Darina Dvinskikh, Pavel Dvurechensky, Alexander Gasnikov, and Cesar Uribe. On the complexity of approximating wasserstein barycenters. In International conference on machine learning, pages 3530\u20133540. PMLR, 2019.   \n[65] Rafsanjany Kushol, Alan H Wilman, Sanjay Kalra, and Yee-Hong Yang. Dsmri: Domain shift analyzer for multi-center mri datasets. Diagnostics, 13(18):2947, 2023.   \n[66] John Lawson, George Tucker, Bo Dai, and Rajesh Ranganath. Energy-inspired models: Learning with sampler-induced distributions. Advances in Neural Information Processing Systems, 32, 2019.   \n[67] Khang Le, Dung Q Le, Huy Nguyen, Dat Do, Tung Pham, and Nhat Ho. Entropic gromovwasserstein between gaussian distributions. In International Conference on Machine Learning, pages 12164\u201312203. PMLR, 2022.   \n[68] Khang Le, Huy Nguyen, Quang M Nguyen, Tung Pham, Hung Bui, and Nhat Ho. On robust optimal transport: Computational complexity and barycenter computation. Advances in Neural Information Processing Systems, 34:21947\u201321959, 2021.   \n[69] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and Fujie Huang. A tutorial on energy-based learning. Predicting structured data, 1(0), 2006.   \n[70] Christian L\u00e9onard. A survey of the schr\u00f6dinger problem and some of its connections with optimal transport. Discrete and Continuous Dynamical Systems, 34(4):1533\u20131574, 2013.   \n[71] Daniel Levy, Matt D. Hoffman, and Jascha Sohl-Dickstein. Generalizing hamiltonian monte carlo with neural networks. In International Conference on Learning Representations, 2018.   \n[72] Lingxiao Li, Aude Genevay, Mikhail Yurochkin, and Justin M Solomon. Continuous regularized wasserstein barycenters. Advances in Neural Information Processing Systems, 33:17755\u2013 17765, 2020.   \n[73] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.   \n[74] Ashok Makkuva, Amirhossein Taghvaei, Sewoong Oh, and Jason Lee. Optimal transport mapping via input convex neural networks. In International Conference on Machine Learning, pages 6672\u20136681. PMLR, 2020.   \n[75] Anton Mallasto, Augusto Gerolin, and H\u00e0 Quang Minh. Entropy-regularized 2-wasserstein distance between gaussian measures. Information Geometry, 5(1):289\u2013323, 2022.   \n[76] Simone Di Marino and Augusto Gerolin. An optimal transport approach for the schr\u00f6dinger bridge problem and convergence of sinkhorn algorithm. Journal of Scientific Computing, 85(2):27, 2020.   \n[77] Alberto Maria Metelli, Amarildo Likmeta, and Marcello Restelli. Propagating uncertainty in reinforcement learning via wasserstein barycenters. Advances in Neural Information Processing Systems, 32, 2019.   \n[78] Petr Mokrov, Alexander Korotin, Alexander Kolesov, Nikita Gushchin, and Evgeny Burnaev. Energy-guided entropic neural optimal transport. In The Twelfth International Conference on Learning Representations, 2024.   \n[79] Eduardo Fernandes Montesuma and Fred Maurice Ngole Mboula. Wasserstein barycenter for multi-source domain adaptation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16785\u201316793, 2021.   \n[80] Youssef Mroueh. Wasserstein style transfer. In Silvia Chiappa and Roberto Calandra, editors, Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pages 842\u2013852. PMLR, 26\u201328 Aug 2020.   \n[81] Kirill Neklyudov, Max Welling, Evgenii Egorov, and Dmitry Vetrov. Involutive mcmc: a unifying framework. In International Conference on Machine Learning, pages 7273\u20137282. PMLR, 2020.   \n[82] Maxence Noble, Valentin De Bortoli, Arnaud Doucet, and Alain Durmus. Tree-based diffusion schr\u00f6dinger bridge with applications to wasserstein barycenters. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[83] Gabriel Peyr\u00e9, Marco Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends\u00ae in Machine Learning, 11(5-6):355\u2013607, 2019.   \n[84] Aram-Alexandre Pooladian, Carles Domingo-Enrich, Ricky T. Q. Chen, and Brandon Amos. Neural optimal transport with lagrangian costs. In ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems, 2023.   \n[85] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.   \n[86] Gareth O Roberts and Richard L Tweedie. Exponential convergence of langevin distributions and their discrete approximations. Bernoulli, pages 341\u2013363, 1996.   \n[87] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[88] Litu Rout, Alexander Korotin, and Evgeny Burnaev. Generative modeling with optimal transport maps. In International Conference on Learning Representations, 2021.   \n[89] Filippo Santambrogio. Optimal transport for applied mathematicians. Birk\u00e4user, NY, 55(58- 63):94, 2015.   \n[90] Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis. In International conference on machine learning, pages 30105\u201330118. PMLR, 2023.   \n[91] Vivien Seguy, Bharath Bhushan Damodaran, Remi Flamary, Nicolas Courty, Antoine Rolet, and Mathieu Blondel. Large scale optimal transport and mapping estimation. In International Conference on Learning Representations, 2018.   \n[92] Dino Sejdinovic, Heiko Strathmann, Maria Lomeli Garcia, Christophe Andrieu, and Arthur Gretton. Kernel adaptive metropolis-hastings. In International conference on machine learning, pages 1665\u20131673. PMLR, 2014.   \n[93] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.   \n[94] Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion schr\u00f6dinger bridge matching. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[95] Dror Simon and Aviad Aberdam. Barycenters of natural images constrained wasserstein barycenters for image morphing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7910\u20137919, 2020.   \n[96] Maurice Sion. On general minimax theorems. Pacific Journal of Mathematics, 8(1):171\u2013176, 1958.   \n[97] Justin Solomon. Optimal transport on discrete domains. AMS Short Course on Discrete Differential Geometry, 2018.   \n[98] Justin Solomon, Fernando De Goes, Gabriel Peyr\u00e9, Marco Cuturi, Adrian Butscher, Andy Nguyen, Tao Du, and Leonidas Guibas. Convolutional wasserstein distances: Efficient optimal transportation on geometric domains. ACM Transactions on Graphics (ToG), 34(4):1\u201311, 2015.   \n[99] Jiaming Song, Shengjia Zhao, and Stefano Ermon. A-nice-mc: Adversarial training for mcmc. Advances in Neural Information Processing Systems, 30, 2017.   \n[100] Yang Song and Diederik P Kingma. How to train your energy-based models. arXiv preprint arXiv:2101.03288, 2021.   \n[101] Sanvesh Srivastava, Volkan Cevher, Quoc Dinh, and David Dunson. Wasp: Scalable bayes via barycenters of subset posteriors. In Artificial Intelligence and Statistics, pages 912\u2013920. PMLR, 2015.   \n[102] Sanvesh Srivastava, Cheng Li, and David B Dunson. Scalable bayes via barycenter in wasserstein space. The Journal of Machine Learning Research, 19(1):312\u2013346, 2018.   \n[103] Karin Stacke, Gabriel Eilertsen, Jonas Unger, and Claes Lundstr\u00f6m. Measuring domain shift for deep learning in histopathology. IEEE journal of biomedical and health informatics, 25(2):325\u2013336, 2020.   \n[104] Heiko Strathmann, Dino Sejdinovic, Samuel Livingstone, Zoltan Szabo, and Arthur Gretton. Gradient-free hamiltonian monte carlo with efficient kernel exponential families. Advances in Neural Information Processing Systems, 28, 2015.   \n[105] Surya T Tokdar and Robert E Kass. Importance sampling: a review. Wiley Interdisciplinary Reviews: Computational Statistics, 2(1):54\u201360, 2010.   \n[106] Alexander Tong, Kilian FATRAS, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. Transactions on Machine Learning Research, 2024. Expert Certification.   \n[107] Alexander Y Tong, Nikolay Malkin, Kilian Fatras, Lazar Atanackovic, Yanlei Zhang, Guillaume Huguet, Guy Wolf, and Yoshua Bengio. Simulation-free schr\u00f6dinger bridges via score and flow matching. In International Conference on Artificial Intelligence and Statistics, pages 1279\u20131287. PMLR, 2024.   \n[108] Konstantin S Turitsyn, Michael Chertkov, and Marija Vucelja. Irreversible monte carlo algorithms for efficient sampling. Physica D: Nonlinear Phenomena, 240(4-5):410\u2013414, 2011.   \n[109] Francisco Vargas, Pierre Thodoroff, Austen Lamacraft, and Neil Lawrence. Solving schr\u00f6dinger bridges via maximum likelihood. Entropy, 23(9):1134, 2021.   \n[110] C\u00e9dric Villani et al. Optimal transport: old and new, volume 338. Springer, 2009.   \n[111] Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu. A theory of generative convnet. In International Conference on Machine Learning, pages 2635\u20132644. PMLR, 2016.   \n[112] Wenjun Yan, Yuanyuan Wang, Shengjia Gu, Lu Huang, Fuhua Yan, Liming Xia, and Qian Tao. The domain shift problem of medical image segmentation and vendor-adaptation by unet-gan. In Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13\u201317, 2019, Proceedings, Part II 22, pages 623\u2013631. Springer, 2019.   \n[113] Yang Zhao, Jianwen Xie, and Ping Li. Learning energy-based generative models via coarseto-fine expanding and sampling. In International Conference on Learning Representations, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.1 Auxiliary Statements ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We start by showing some basic properties of the $C$ -transform which will be used in the main proofs. Proposition A.1 (Properties of the $C$ -transform). Let $f_{1},f_{2}\\colon\\mathcal{V}\\rightarrow\\mathbb{R}$ be two measurable functions which are bounded from below. It holds that ", "page_idx": 16}, {"type": "text", "text": "(i) Monotonicity: $f_{1}\\leq f_{2}$ implies $f_{1}^{C}\\ge f_{2}^{C}$ ;   \n(ii) Constant additivity: $(f_{1}+a)^{C}=f_{1}^{C}-a$ for all $a\\in\\mathbb R$ ;   \n(iii) Concavity: $(\\lambda f_{1}+(1-\\lambda)f_{2})^{C}\\ge\\lambda f_{1}^{C}+(1-\\lambda)f_{2}^{C}$ for all $\\lambda\\in[0,1]$ ;   \n$(i\\nu)$ Continuity: $f_{1},f_{2}$ bounded implies $\\begin{array}{r}{\\operatorname*{sup}_{x\\in\\mathcal{X}}|f_{1}^{C}(x)-f_{2}^{C}(x)|\\leq\\operatorname*{sup}_{y\\in\\mathcal{Y}}|f_{1}(y)-f_{2}(y)|.}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "Proof of Proposition A.1. We recall the definition of the $C$ -transform ", "page_idx": 16}, {"type": "equation", "text": "$$\nf_{1}^{C}(x)=\\operatorname*{inf}_{\\mu\\in\\mathcal{P}(\\mathcal{Y})}\\left\\{C(x,\\mu)-\\int_{\\mathcal{Y}}f_{1}(y)\\mathrm{d}\\mu(y)\\right\\},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $C(x,\\mu)\\,\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\int_{\\mathcal{Y}}c(x,y)\\mathrm{d}\\mu(y)\\,-\\,\\epsilon H(\\mu)$ . Monotonicity (i) and constant additivity (ii) are immediate from the definition. ", "page_idx": 16}, {"type": "text", "text": "To see (iii), observe that the dependence of $\\textstyle\\int_{\\mathcal{V}}f_{1}(y)\\mathrm{d}\\mu(y)$ on $f_{1}$ is linear. Thus, $f_{1}^{C}$ is the pointwise infimum of a family of linear functionals and thus concave. ", "page_idx": 16}, {"type": "text", "text": "Finally, to show (iv) we have by monotonicity of the integral that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\int_{\\mathcal{Y}}f_{1}(y)\\mathrm{d}\\mu(y)-\\int_{\\mathcal{Y}}f_{2}(y)\\mathrm{d}\\mu(y)\\right|\\leq\\operatorname*{sup}_{y\\in\\mathcal{Y}}|f_{1}(y)-f_{2}(y)|\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for any $\\mu\\in\\mathcal{P}(\\mathcal{Y})$ . For fixed $x\\in\\mathscr{X}$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{1}^{C}(x)-f_{2}^{C}(x)=\\displaystyle\\operatorname*{inf}_{\\mu\\in\\mathcal{P}(y)}\\left[C(x,\\Tilde{\\mu})-\\displaystyle\\int_{y}f_{1}(y)\\mathrm{d}\\Tilde{\\mu}(y)\\right]-\\displaystyle\\operatorname*{inf}_{\\mu\\in\\mathcal{P}(y)}\\left[C(x,\\mu)-\\displaystyle\\int_{y}f_{2}(y)\\mathrm{d}\\mu(y)\\right]}\\\\ {=\\displaystyle\\operatorname*{sup}_{\\mu\\in\\mathcal{P}(y)}\\operatorname*{inf}_{\\Tilde{\\mu}\\in\\mathcal{P}(y)}\\left[C(x,\\Tilde{\\mu})-C(x,\\mu)-\\displaystyle\\int_{y}f_{1}(y)\\mathrm{d}\\Tilde{\\mu}(y)+\\displaystyle\\int_{y}f_{2}(y)\\mathrm{d}\\mu(y)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By setting ${\\tilde{\\mu}}=\\mu$ we increase the value and obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\nf_{1}^{C}(x)-f_{2}^{C}(x)\\leq\\operatorname*{sup}_{\\mu\\in\\mathcal{P}(\\mathcal{Y})}\\int_{\\mathcal{Y}}\\left[f_{2}(y)-f_{1}(y)\\right]\\mathrm{d}\\mu(y)\\leq\\operatorname*{sup}_{y\\in\\mathcal{Y}}|f_{1}(y)-f_{2}(y)|,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last inequality follows from (13). For symmetry reasons, we can swap the roles of $f_{1}$ and $f_{2}$ in (14), which yields the claim. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "A.2 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. By substituting in (5) the primal EOT problems (1) with their dual counterparts (2), we obtain a dual formulation, which is the starting point of our analysis: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{L}^{*}=\\operatorname*{min}_{\\mathbb{Q}\\in\\mathcal{P}(y)}\\operatorname*{sup}_{f_{1},\\ldots,f_{K}\\in\\mathcal{C}(y)}\\sum_{\\underset{i=1}{k=1}}^{K}\\lambda_{k}\\Bigg\\{\\int_{\\mathcal{X}_{k}}f_{k}^{C_{k}}(x_{k})\\mathrm{d}\\mathbb{P}_{k}(x_{k})+\\int_{y}f_{k}(y)\\mathrm{d}\\mathbb{Q}(y)\\Bigg\\}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here, we replaced inf with min because of the existence of the barycenter (\u00a72.2). Moreover, we refer to the entire expression under the min sup as a functional $\\widetilde{\\mathcal{L}}\\colon\\mathcal{P}(\\mathcal{P})\\times\\mathcal{C}(\\mathcal{P})^{K}\\rightarrow\\mathbb{R}$ . For brevity, we introduce, for $\\dot{(f_{1},\\dots,f_{K})}\\in\\mathcal{C}(\\mathcal{V})^{K}$ , the notation ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\bar{f}\\overset{\\mathrm{def}}{=}\\sum_{k=1}^{K}\\lambda_{k}f_{k}\\quad\\mathrm{and}\\quad M\\overset{\\mathrm{def}}{=}\\operatorname*{inf}_{y\\in\\mathcal{Y}}\\bar{f}(y)=\\operatorname*{inf}_{\\mathbb{Q}\\in\\mathcal{P}(\\mathcal{Y})}\\int\\bar{f}(y)\\mathrm{d}\\mathbb{Q}(y),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the equality follows from two elementary observations that (a) $\\begin{array}{r}{M\\le\\int\\bar{f}(y)\\mathrm{d}\\mathbb{Q}(y)}\\end{array}$ for any $\\mathbb{Q}\\in\\mathcal{P}(\\mathcal{Y})$ and (b) $\\begin{array}{r}{\\bar{f}(y)=\\int\\bar{f}(y^{\\prime})\\mathrm{d}\\delta_{y}(y^{\\prime})}\\end{array}$ where $\\delta_{y}$ denotes a Dirac mass at $y\\in\\mathcal{V}$ . ", "page_idx": 17}, {"type": "text", "text": "On the one hand, $\\mathcal{P}(\\mathcal{Y})$ is compact w.r.t. the weak topology because $\\boldsymbol{\\wp}$ is compact, and for fixed potentials $(f_{1},\\dots,f_{K})\\in\\mathcal{P}(\\mathcal{V})^{K}$ we have that $\\widetilde{\\mathcal{L}}(\\cdot,(f_{k})_{k=1}^{K})$ is continuous and linear. In particular, $\\widetilde{\\mathcal{L}}(\\cdot,(f_{k})_{k=1}^{K})$ is convex and l.s.c. On the other hand, for a fixed $\\mathbb{Q}$ , the functional $\\widetilde{\\mathcal{L}}(\\mathbb{Q},\\cdot)$ is concave by (iii) in Proposition A.1. These observations allow us to apply Sion\u2019s minimax theorem [96, Theorem 3.4] to swap min and inf in (15) and obtain using (16) ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal L^{*}=\\underset{f_{1},\\ldots,f_{K}\\in\\mathcal C(\\mathcal I)}{\\operatorname*{sup}}\\underset{\\mathbb Q\\in\\mathcal P(\\mathcal P)}{\\operatorname*{min}}\\sum_{k=1}^{K}\\lambda_{k}\\Bigg\\lbrace\\int_{\\mathcal X_{k}}f_{k}^{C_{k}}(x_{k})\\mathrm d\\mathbb P_{k}(x_{k})+\\int_{\\mathcal X}f_{k}(y)\\mathrm d\\mathbb Q(y)\\Bigg\\rbrace}\\\\ &{\\quad=\\underset{f_{1},\\ldots,f_{K}\\in\\mathcal C(\\mathcal I)}{\\operatorname*{sup}}\\left\\lbrace\\sum_{k=1}^{K}\\lambda_{k}\\int_{\\mathcal X_{k}}f_{k}^{C_{k}}(x_{k})\\mathrm d\\mathbb P_{k}(x_{k})+\\underset{\\mathbb Q\\in\\mathcal P(\\mathcal P)}{\\operatorname*{min}}\\int_{\\mathcal X}\\bar{f}(y)\\mathrm d\\mathbb Q(y)\\right\\rbrace}\\\\ &{\\quad=\\underset{f_{1},\\ldots,f_{K}\\in\\mathcal C(\\mathcal I)}{\\operatorname*{sup}}\\left\\lbrace\\sum_{k=1}^{K}\\lambda_{k}\\int_{\\mathcal X_{k}}f_{k}^{C_{k}}(x_{k})\\mathrm d\\mathbb P_{k}(x_{k})+\\underset{y\\in\\mathcal P}{\\operatorname*{min}}\\bar{f}(y)\\right\\rbrace.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Next, we show that the sup in (17) can be restricted to tuplets satisfying the congruence condition $\\begin{array}{r}{\\sum_{k=1}^{K}\\lambda_{k}f_{k}\\,=\\,0}\\end{array}$ . It remains to show that for every tuplet $(f_{1},\\dots,f_{K})\\in\\mathcal{C}(\\mathcal{V})^{K}$ there exists a congruent tuplet $(\\tilde{f}_{1},\\dots,\\tilde{f}_{K})\\in\\mathcal{C}(\\mathcal{X})^{K}$ such that $\\widetilde{\\mathcal{L}}(\\tilde{f}_{1},\\ldots,\\tilde{f}_{K})\\ge\\widetilde{\\mathcal{L}}(f_{1},\\ldots,f_{K})$ . ", "page_idx": 17}, {"type": "text", "text": "To this end, fix $\\left(f_{1},\\ldots,f_{K}\\right)$ and define the congruent tuplet ", "page_idx": 17}, {"type": "equation", "text": "$$\n(\\tilde{f}_{1},\\dots,\\tilde{f}_{K})\\stackrel{\\mathrm{def}}{=}\\left(f_{1},\\dots,f_{K-1},f_{K}-\\frac{\\bar{f}}{\\lambda_{K}}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We find $\\begin{array}{r}{\\tilde{M}\\stackrel{\\mathrm{def}}{=}\\operatorname*{inf}_{y\\in{\\mathcal y}}\\sum_{k=1}^{K}\\lambda_{k}\\tilde{f}_{k}=0}\\end{array}$ by the congruence and derive ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\widetilde{\\mathcal{L}}(\\tilde{f}_{1},\\dots,\\tilde{f}_{K})-\\widetilde{\\mathcal{L}}(f_{1},\\dots,f_{K})=\\lambda_{K}\\int_{X_{K}}\\left[\\tilde{f}_{K}^{C_{K}}(x_{K})-f_{K}^{C_{K}}(x_{K})\\right]\\mathrm{d}\\mathbb{P}_{K}(x_{K})-M}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\ge\\lambda_{K}\\int_{X_{K}}\\left[\\left(f_{K}-\\frac{M}{\\lambda_{K}}\\right)^{C_{K}}(x_{K})-f_{K}^{C_{K}}(x_{K})\\right]\\mathrm{d}\\mathbb{P}(x_{K})-M}\\\\ {\\displaystyle\\qquad\\qquad=\\lambda_{K}\\int_{X_{K}}\\frac{M}{\\lambda_{K}}\\mathrm{d}\\mathbb{P}(x_{K})-M=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the first inequality follows from $\\begin{array}{r}{\\tilde{f}_{K}=f_{K}-\\frac{\\bar{f}}{\\lambda_{K}}\\leq f_{K}-\\frac{M}{\\lambda_{K}}}\\end{array}$ combined with monotonicity of the -transform, see (i) in Proposition A.1. The second to last equality follow from constant additivity, see (ii) in Proposition A.1. ", "page_idx": 17}, {"type": "text", "text": "In summary, we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}^{*}=\\underset{f_{1},\\ldots,f_{k}\\in\\mathcal{C}(\\mathcal{Y})}{\\operatorname*{sup}}\\,\\widetilde{\\mathcal{L}}(f_{1},\\ldots,f_{K}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Finally, observe that for congruent $\\left(f_{1},\\ldots,f_{K}\\right)$ we have $\\widetilde{\\mathcal{L}}(f_{1},\\ldots,f_{K})={\\mathcal{L}}(f_{1},\\ldots,f_{K})$ . Hence, we can replace $\\widetilde{\\mathcal{L}}$ by $\\mathcal{L}$ in (19), which yields (6). \u53e3 ", "page_idx": 17}, {"type": "text", "text": "A.3 Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. Write $\\mathbb{Q}^{*}$ for the barycenter and $\\pi_{k}^{*}$ for the optimizer of $\\mathrm{EOT}_{c_{k},\\epsilon}(\\mathbb{P}_{k},\\mathbb{Q}^{*})$ . Consider congruent potentials $f_{1},\\ldots,f_{K}\\in\\mathcal{C}(\\mathcal{Y})$ and define the probability distribution ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{d}\\pi^{f_{k}}(x_{k},y)\\stackrel{\\mathrm{def}}{=}\\mathrm{d}\\mu_{x_{k}}^{f_{k}}(y)\\,\\mathrm{d}\\mathbb{P}_{k}(x_{k}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mu_{x_{k}}^{f_{k}}(y)}{\\mathrm{d}y}\\stackrel{\\mathrm{def}}{=}\\frac{1}{Z_{c_{k}}(f_{k},x_{k})}\\exp\\left(\\frac{f_{k}(y)-c_{k}(x_{k},y)}{\\epsilon}\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\nZ_{c_{k}}(f_{k},x_{k})\\stackrel{\\mathrm{def}}{=}\\log\\left(\\int_{\\mathcal{V}}e^{\\frac{f_{k}(y)-c_{k}(x_{k},y)}{\\epsilon}}\\mathrm{d}y\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then we have by [78, Thm. 2]: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname{EOT}_{c_{k},\\epsilon}(\\mathbb{P}_{k},\\mathbb{Q}^{*})-\\left(\\int_{\\mathcal{X}_{k}}f^{C_{k}}(x_{k})\\mathrm{d}\\mathbb{P}(x_{k})+\\int_{\\mathcal{Y}}f(y)\\mathrm{d}\\mathbb{Q}^{*}(y)\\right)=\\epsilon\\mathrm{KL}\\left(\\pi_{k}^{*}\\|\\pi^{f_{k}}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Multiplying (22) by $\\lambda_{k}$ and summing over $k$ yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\lambda_{k}\\mathrm{KL}\\left(\\pi_{k}^{*}\\|\\pi^{f_{k}}\\right)=\\sum_{k=1}\\lambda_{k}\\left\\{\\mathrm{EOT}_{c_{k},\\epsilon}(\\mathbb{P}_{k},\\mathbb{Q}^{*})-\\int_{\\mathcal{X}_{k}}f_{k}^{C_{k}}(x_{k})\\mathrm{d}\\mathbb{P}_{k}(x_{k})\\right\\}-\\int_{\\mathcal{Y}}\\underset{=0}{\\overset{K}{\\sum_{k=1}^{K}}}\\lambda_{k}f(y)\\,\\mathrm{d}\\mathbb{Q}^{*}(x_{k}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last equality follows by congruence, i.e., $\\textstyle\\sum_{k=1}^{K}\\lambda_{k}f_{k}\\equiv0$ . ", "page_idx": 18}, {"type": "text", "text": "The remaining inequality in (7) is a consequence of the data processing inequality for $f$ -divergences which we invoke here to get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{KL}\\left(\\pi_{k}^{*}\\|\\pi^{f_{k}}\\right)\\geq\\mathrm{KL}\\left(\\mathbb{Q}^{*}\\|\\mathbb{Q}^{f_{k}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathbb{Q}^{*}$ and $\\mathbb{Q}^{f_{k}}$ are the second marginals of $\\pi_{k}^{*}$ and $\\pi^{f_{k}}$ , respectively. ", "page_idx": 18}, {"type": "text", "text": "A.4 Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. The desired equation (9) could be derived exactly the same way as in [78, Theorem 3]. ", "page_idx": 18}, {"type": "text", "text": "A.5 Proof of Proposition 4.4 and Theorem 4.5 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The derivations of the quantitative bound for Proposition 4.4 and Theorem 4.5 relies on the following standard definitions from learning theory, which we now recall for convenience (see, e.g. [93, $\\S26],$ ). Consider some class $\\boldsymbol{S}$ of functions $s:\\mathcal{X}\\to\\mathbb{R}$ and a distribution $\\mu$ on $\\mathcal{X}$ . Let $X=\\{\\bar{x}^{1},\\ldots,x^{N}\\}$ be a sample of $N$ points in $\\mathcal{X}$ . ", "page_idx": 18}, {"type": "text", "text": "The representativeness of the sample $X$ w.r.t. the class $\\boldsymbol{S}$ and the distribution $\\mu$ is defined by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname{Rep}_{X}(S,\\mu)\\stackrel{\\mathrm{def}}{=}\\operatorname*{sup}_{s\\in S}\\big[\\int_{X}s(x)\\mathrm{d}\\mu(x)-\\frac{1}{N}\\sum_{n=1}^{N}s(x^{n})\\big].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The Rademacher complexity of the class $\\boldsymbol{S}$ w.r.t. the distribution $\\mu$ and sample size $N$ is given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{R}_{N}(\\ensuremath{\\boldsymbol{S}},\\mu)\\stackrel{\\mathrm{def}}{=}\\frac{1}{N}\\mathbb{E}\\bigg\\{\\operatorname*{sup}_{s\\in\\ensuremath{\\boldsymbol{S}}}\\sum_{n=1}^{N}s(\\ensuremath{\\boldsymbol{x}}^{n})\\sigma_{n}\\bigg\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\{x^{n}\\}_{n=1}^{N}\\sim\\mu$ are mutually independent, $\\{\\sigma^{n}\\}_{n=1}^{N}$ are mutually independent Rademacher random variables, i.e., $\\operatorname{Prob}\\!\\left(\\sigma^{n}=1\\right)=\\operatorname{Prob}\\!\\left(\\sigma^{n}=-1\\right)=0.5$ , and the expectation is taken with respect to both $\\{x_{n}\\}_{n=1}^{N}$ , $\\{\\sigma_{n}\\}_{n=1}^{N}$ . The well-celebrated relation between (25) and (24), as shown in [93, Lemma 26.2], is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E R e p}_{X}(S,\\mu)\\leq2\\cdot\\mathcal{R}_{N}(S,\\mu),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the expectation is taken w.r.t. random i.i.d. sample $X\\sim\\mu$ of size $N$ . ", "page_idx": 18}, {"type": "text", "text": "Proposition 4.4. Observe that by (23) we may write ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\epsilon\\sum_{k=1}^{K}\\lambda_{k}\\mathrm{KL}\\left(\\boldsymbol{\\pi}_{k}^{*}\\Vert\\boldsymbol{\\pi}^{\\widehat{f_{k}}}\\right)=\\mathcal{L}^{*}-\\mathcal{L}(\\widehat{\\mathbf{f}})=\\underbrace{\\left[\\mathcal{L}^{*}-\\operatorname*{max}_{\\mathrm{f\\in\\mathcal{F}}}\\mathcal{L}(\\mathbf{f})\\right]}_{\\mathrm{Approximation~error}}+\\underbrace{\\left[\\operatorname*{max}_{\\mathrm{f\\in\\mathcal{F}}}\\mathcal{L}(\\mathbf{f})-\\mathcal{L}(\\widehat{\\mathbf{f}})\\right]}_{\\mathrm{Estimation~error}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let f be a maximizer of ${\\mathcal{L}}(\\mathbf{f})$ within $\\overline{{\\mathcal{F}}}$ . Analysing the estimation error in (27) yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{f}\\in\\overline{{\\mathcal{F}}}}\\mathcal{L}(\\mathbf{f})-\\mathcal{L}(\\widehat{\\mathbf{f}})=\\mathcal{L}(\\overline{{\\mathbf{f}}})-\\mathcal{L}(\\widehat{\\mathbf{f}})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\big[\\mathcal{L}(\\bar{\\mathbf{f}})-\\widehat{\\mathcal{L}}(\\bar{\\mathbf{f}})\\big]+\\underbrace{\\big[\\widehat{\\mathcal{L}}(\\bar{\\mathbf{f}})-\\widehat{\\mathcal{L}}(\\widehat{\\mathbf{f}})\\big]}_{\\leq0}+\\big[\\widehat{\\mathcal{L}}(\\widehat{\\mathbf{f}})-\\mathcal{L}(\\widehat{\\mathbf{f}})\\big]}\\\\ &{\\leq\\,2\\operatorname*{sup}_{\\mathbf{f}\\in\\overline{{\\mathcal{F}}}}\\big|\\mathcal{L}(\\mathbf{f})-\\widehat{\\mathcal{L}}(\\mathbf{f})\\big|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where central term in line (28) is bounded above by 0 due the maximality of ${\\widehat{\\mathbf{f}}},$ , that is, $\\widehat{\\cal{L}}(\\widehat{\\bf{f}})\\;=\\;$ $\\mathrm{max}_{\\mathbf{f}\\in\\overline{{\\mathcal{F}}}}\\,\\widehat{\\mathcal{L}}(\\mathbf{f})\\,\\geq\\,\\widehat{\\mathcal{L}}(\\bar{\\mathbf{f}})$ . Due to (29), we can bound the estimation error using the Rademacher complex ity ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{up}_{\\in\\overline{{\\mathcal{F}}}}\\left|\\mathcal{L}(\\mathbf{f})-\\widehat{\\mathcal{L}}(\\mathbf{f})\\right|\\leq\\sum_{k=1}^{K}\\lambda_{k}\\operatorname*{sup}_{f_{k}\\in\\mathcal{F}_{k}}\\left[\\int_{\\mathcal{X}_{k}}f_{k}^{C_{k}}(x_{k})\\mathrm{d}\\mathbb{P}_{k}(x_{k})-\\frac{1}{N_{k}}\\sum_{n=1}^{N_{k}}f_{k}^{C_{k}}(x_{k}^{n})\\right]=\\sum_{k=1}^{K}\\lambda_{k}\\mathrm{Rep}_{X_{k}}(\\mathcal{F}_{k}^{C_{k}}(x_{k})).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof of Theorem 4.5. Case (a) - Lipschitz costs: Assume that, for $k\\in\\overline{{K}}$ , $x\\mapsto c_{k}(x,y)$ is Lipschitz with constant $L_{k}\\,\\geq\\,0$ for every $y\\in\\mathcal{V}$ . Recall that $f_{k}^{C_{k}}$ is defined as the pointwise supremum of $L_{k}$ -Lipschitz functions and, therefore, Lipschitz continuous with the same constant. Since the value of the representativeness of a sample w.r.t. a function class is invariant under translating individual elements of said class, we have that $\\mathrm{Rep}_{X}(\\mathcal{F}_{k}^{C_{k}},\\mathbb{P}_{k})$ coincides with $\\mathrm{Rep}_{X}(\\mathcal{G}_{k},\\mathbb{P}_{k})$ where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{G}_{k}\\overset{\\mathrm{def}}{=}\\{f^{C_{k}}-f^{C_{k}}(\\tilde{x}_{k}):f\\in C(\\mathcal{Y})\\},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for some fixed $\\tilde{x}_{k}\\in\\mathcal{X}_{k}$ . All the functions in this class are $L_{k}$ -Lipschitz and, therefore, bounded by $L_{k}\\cdot\\mathrm{diam}(\\mathcal{X}_{k})$ . We may therefore apply [38, Theorem 4.3] to the class $\\mathcal{G}_{k}$ and obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X\\sim\\mathbb{P}_{k}}\\mathrm{Rep}_{X}(\\mathcal F_{k}^{C_{k}},\\mathbb{P}_{k})=\\mathbb{E}_{X\\sim\\mathbb{P}_{k}}\\mathrm{Rep}_{X}(\\mathcal G_{k},\\mathbb{P}_{k})\\le2\\mathcal{R}_{N}(\\mathcal G_{k},\\mathbb{P}_{k})\\le O(N_{k}^{-\\frac{1}{D_{k}+1}}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Case (b) - Feature-based quadratic costs $\\frac{1}{2}\\|u_{k}(\\cdot)-v(\\cdot)\\|_{2}^{2}$ : Alternatively, consider the case where $\\begin{array}{r}{c_{k}(x_{k},y)=\\frac{1}{2}\\|u_{k}(x_{k})-v(y)\\|_{2}^{2}}\\end{array}$ and $\\mathcal{F}_{k}\\subseteq\\mathcal{C}(\\mathcal{Y})$ is bounded (w.r.t. the supremum norm). ", "page_idx": 19}, {"type": "text", "text": "To this end, recall that for a measurable and bounded function $f:y\\to\\mathbb{R}$ , the weak entropic $c_{k}$ -transform satisfies ", "page_idx": 19}, {"type": "equation", "text": "$$\nf^{C_{k}}(x_{k})=-\\epsilon\\log(Z_{c_{k}}(f,x_{k})),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{Z_{c_{k}}(f,x_{k})=\\int_{\\mathcal{Y}}\\exp\\left(\\frac{f(y)-c_{k}(x_{k},y)}{\\epsilon}\\right)d y}\\end{array}$ y. Recall that $\\begin{array}{r}{\\mathbb{R}^{D^{\\prime\\prime}}\\times\\mathbb{R}^{D^{\\prime\\prime}}\\ni(a,b)\\mapsto\\exp\\!\\left(-\\frac{\\|a-b\\|^{2}}{2\\epsilon}\\right)}\\end{array}$ is a positive definite kernel which is widely known as the Gaussian kernel. This means that there exists a Hilbert space $\\mathcal{H}$ and a feature map $\\phi:\\mathbb{R}^{D^{\\prime\\prime}}\\rightarrow\\mathcal{H}$ such that $\\begin{array}{r}{\\exp\\Bigl(-\\frac{\\|a-b\\|^{2}}{2\\epsilon}\\Bigr)=\\langle\\phi(a),\\phi(b)\\rangle_{\\mathcal{H}}}\\end{array}$ . Due to the particular form of $c_{k}$ , we may write ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\exp\\Big(-\\frac{c_{k}(x_{k},y)}{\\epsilon}\\Big)=\\exp\\Big(-\\frac{\\|u_{k}(x)-v(y)\\|^{2}}{2\\epsilon}\\Big)=\\langle\\phi(u_{k}(x_{k})),\\phi(v(y))\\rangle_{\\mathcal{H}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Notice that $\\{\\phi(u_{k}(x_{k})):x_{k}\\in\\mathcal{X}_{k}\\}\\subseteq\\{v\\in\\mathcal{H}:\\|v\\|_{\\mathcal{H}}=1\\}$ since $\\|\\phi(a)\\|_{\\mathcal{H}}^{2}=\\langle\\phi(a),\\phi(a)\\rangle_{\\mathcal{H}}=1$ for every $a\\in\\mathbb{R}^{D^{\\prime\\prime}}$ . ", "page_idx": 19}, {"type": "text", "text": "Using the identity in (30), we can express $Z_{c_{k}}(f,x_{k})$ by ", "page_idx": 19}, {"type": "equation", "text": "$$\nZ_{c_{k}}(f,x_{k})=\\int_{\\mathcal{Y}}\\langle\\phi(u_{k}(x_{k})),\\phi(v(y))\\rangle_{\\mathcal{H}}\\,e^{\\frac{f(y)}{\\epsilon}}\\,d y=\\Big\\langle\\phi\\big(u_{k}(x_{k})\\big),\\int_{\\mathcal{Y}}\\phi(v(y))e^{\\frac{f(y)}{\\epsilon}}\\,d y\\Big\\rangle_{\\mathcal{H}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last equality is justified as $\\boldsymbol{\\wp}$ is compact; furthermore, we note the integrals are well-defined by the measurability of $\\phi$ , $u_{k}$ and $v$ . Moreover, using the boundedness of each $\\mathcal{F}_{k}$ and compactness of $\\boldsymbol{\\wp}$ , we get ", "page_idx": 19}, {"type": "equation", "text": "$$\nR\\stackrel{\\mathrm{def}}{=}\\operatorname*{max}_{k=1,\\ldots,K}\\operatorname*{sup}_{f\\in\\mathcal{F}_{k}}\\Big\\|\\int_{\\mathcal{V}}\\phi(v(y))e^{\\frac{f(y)}{\\epsilon}}\\,d y\\Big\\|_{\\mathcal{H}}<\\infty.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Define ${\\mathcal{G}}_{k}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\{Z_{c_{k}}(f,\\cdot):f\\in{\\mathcal{F}}_{k}\\}$ and observe that $\\mathcal{G}_{k}\\subseteq\\mathcal{G}_{k}^{\\prime}\\stackrel{\\mathrm{def}}{=}\\{\\langle\\phi\\big(u_{k}(\\cdot)\\big),w\\rangle_{\\mathcal{H}}:\\|w\\|_{\\mathcal{H}}\\leq R\\}$ .   \nThis implies that $\\begin{array}{r}{\\mathcal{R}_{N_{k}}(\\mathcal{G}_{k},\\mathbb{P}_{k})\\le\\mathcal{R}_{N_{k}}(\\mathcal{G}_{k}^{\\prime},\\mathbb{P}_{k})}\\end{array}$ by the properties of the Rademacher complexity. ", "page_idx": 19}, {"type": "text", "text": "In turn, the latter Rademacher complexity can be bounded by [10, Lemma 22]. Indeed, write $\\mathbb{P}_{k}^{\\prime}\\overset{\\mathrm{def}}{=}(u_{k})_{\\#}\\mathbb{P}_{k}$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{R}_{N_{k}}(\\mathcal{G}_{k}^{\\prime},\\mathbb{P}_{k})=\\mathcal{R}_{N_{k}}(\\{\\langle\\phi(\\cdot),w\\rangle_{\\mathcal{H}}:\\|w\\|_{\\mathcal{H}}\\leq R\\},\\mathbb{P}_{k}^{\\prime}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which can be directly seen from the definition of the Rademacher complexity. Thus, we can apply [10, Lemma 22] to the right-hand side, and summarizing obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{R}_{N_{k}}({\\mathcal{G}}_{k},\\mathbb{P}_{k})\\leq\\frac{R}{\\sqrt{N_{k}}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since the functions in $\\mathcal{G}_{k}$ are bounded uniformly away from zero by some constant $\\kappa>0$ (depending on the bound of $\\mathcal{F}_{k}$ and $\\epsilon$ ), and since the logarithm restricted to $[\\kappa,\\infty)$ is $1/\\kappa$ -Lipschitz, we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{R}_{N_{k}}(\\mathcal{F}_{k}^{C_{k}},\\mathbb{P}_{k})\\le\\frac{\\epsilon}{\\kappa}\\mathcal{R}_{N_{k}}(\\mathcal{G}_{k},\\mathbb{P}_{k})\\le\\frac{\\epsilon}{\\kappa}\\,\\frac{R}{\\sqrt{N_{k}}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We can now bound the expected representativeness of $\\mathcal{F}_{k}^{C_{k}}$ with the Rademacher complexity by (26), yielding the claim. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "A.6 Proof of Theorem 4.6 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof of Theorem 4.6. Let $\\sigma\\in C(\\mathbb{R})$ be a non-affine activation function which is differentiable at some point $x_{0}\\in\\mathbb{R}$ and for which $\\sigma^{\\prime}(x_{0})\\neq0$ . Let $\\delta>0$ , $\\lambda_{1},\\dots,\\lambda_{K}>0$ , and $K\\in\\mathbb N$ be given. By Theorem 4.1, there exist $K$ congruent continuous functions $f_{1}^{\\prime},\\ldots,f_{K}^{\\prime}$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{L}^{*}-\\mathcal{L}(\\tilde{f}_{1},\\ldots,\\tilde{f_{K}})<\\frac{\\delta}{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Applying [29, Theorem 4.1] we deduce that for each $k=1,\\ldots,K$ , there exist a continuous extension $f_{k}^{\\prime}:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}$ for $\\tilde{f}_{k}$ to all of $\\mathbb{R}^{D}$ ; i.e. $f_{k}^{\\prime}(y)\\,=\\,\\tilde{f}_{k}(y)$ for each $y\\in\\mathcal{V}$ . In particular, (32) can be rewritten as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{L}^{*}-\\mathcal{L}(f_{1}^{\\prime},\\ldots,f_{K}^{\\prime})<\\frac\\delta2.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Set $\\delta_{k}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\delta/(4\\lambda_{k})$ for each $k=1,\\ldots,K$ . Since $\\boldsymbol{\\wp}$ is a compact subset of $\\mathbb{R}^{D}$ , and since we have assumed that $\\sigma\\in C(\\mathbb{R})$ is non-affine activation function which is differentiable at some point $\\widetilde x\\in\\mathbb R$ and for which $\\sigma^{\\prime}(\\widetilde{x})\\neq0$ then the special case of [62, Theorem 9] given in [62, Proposition 53] , implies that for any there exist feedforward neural networks $g_{k}:\\mathbb{R}^{D_{k}}\\rightarrow\\mathbb{R}\\left(k\\in\\overline{{K}}\\right)$ with activation function $\\sigma$ , such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|g_{k}-f_{k}^{\\prime}\\|_{\\infty}\\stackrel{\\mathrm{def}}{=}\\operatorname*{sup}_{y\\in\\mathcal{Y}}|g_{k}(y)-f_{k}^{\\prime}(y)|=\\operatorname*{sup}_{y\\in\\mathcal{Y}}|g_{k}(y)-f_{k}^{\\prime}(y)|<\\delta_{k},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "each $g_{k}$ width at-most $D+4$ . Pick $\\begin{array}{r}{\\delta_{k}=\\frac{\\delta}{4}}\\end{array}$ for all $k\\in\\overline{{K}}$ and suitable neural networks $g_{1},\\ldots,g_{K}$ . Next, we define the congruent sums of neural networks $\\begin{array}{r}{f_{k}\\ {\\stackrel{\\mathrm{def}}{=}}\\ g_{k}-\\sum_{k=1}^{K}\\lambda_{k}g_{k}}\\end{array}$ . We derive ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Big\\|\\sum_{k=1}^{K}\\lambda_{k}g_{k}\\Big\\|_{\\infty}=\\Big\\|\\sum_{k=1}^{K}\\lambda_{k}g_{k}-\\underbrace{\\sum_{k=1}^{K}\\lambda_{k}f_{k}^{\\prime}}_{=0}\\Big\\|_{\\infty}\\leq\\sum_{k=1}^{K}\\lambda_{k}\\Big\\|g_{k}-f_{k}^{\\prime}\\Big\\|_{\\infty}<\\sum_{k=1}^{K}\\lambda_{k}\\frac{\\delta}{4\\lambda_{k}}=\\frac{\\delta}{4}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using (34) we obtain for fixed $k\\in\\overline{{K}}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|f_{k}^{\\prime}-f_{k}\\|_{\\infty}=\\|f_{k}^{\\prime}-g_{k}+\\sum_{k^{\\prime}=1}^{K}\\lambda_{k^{\\prime}}g_{k^{\\prime}}\\|_{\\infty}\\leq\\underbrace{\\|f_{k}^{\\prime}-g_{k}\\|_{\\infty}}_{<\\frac{\\delta}{4}}+\\underbrace{\\|\\sum_{k^{\\prime}=1}^{K}\\lambda_{k^{\\prime}}g_{k^{\\prime}}\\|_{\\infty}}_{<\\frac{\\delta}{4}}<\\frac{\\delta}{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By (iv) in Proposition A.1 together with (35) we find ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|f_{k}^{C_{k}}-(f_{k}^{\\prime})^{C_{k}}\\|_{\\infty}\\leq\\|f_{k}-f_{k}^{\\prime}\\|_{\\infty}<\\frac{\\delta}{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now we use (36) to derive ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\vert\\mathcal{L}(f_{1},\\dots,f_{K})-\\mathcal{L}(f_{1}^{\\prime},\\dots,f_{K}^{\\prime})\\vert\\leq\\displaystyle\\sum_{k=1}^{K}\\lambda_{k}\\left\\vert\\int_{\\mathcal{X}_{k}}f_{k}^{C_{k}}(x_{k})\\mathrm{d}\\mathbb{P}_{k}(x_{k})-\\int_{\\mathcal{X}_{k}}(f_{k}^{\\prime})^{C_{k}}(x_{k})\\mathrm{d}\\mathbb{P}_{k}(x_{k})\\right\\vert}&{{}}\\\\ {\\leq\\displaystyle\\sum_{k=1}^{K}\\lambda_{k}\\int_{\\mathcal{X}_{k}}\\vert f_{k}^{C_{k}}(x_{k})-(f_{k}^{\\prime})^{C_{k}}(x_{k})\\vert\\mathrm{d}\\mathbb{P}_{k}(x_{k})}&{{}}\\\\ {\\leq\\displaystyle\\sum_{k=1}^{K}\\lambda_{k}\\Vert f_{k}^{C_{k}}-(f_{k}^{\\prime})^{C_{k}}\\Vert_{\\infty}<\\underbrace{(\\displaystyle\\sum_{k=1}^{K}\\lambda_{k})}_{=1}\\frac{\\delta}{2}=\\frac{\\delta}{2}.}&{{}\\qquad(3)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Next we combine (33) with (37) to get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{L}^{*}-\\mathcal{L}(f_{1},\\ldots,f_{K})\\le\\underbrace{[\\mathcal{L}^{*}-\\mathcal{L}(f_{1}^{\\prime},\\ldots,f_{K}^{\\prime})]}_{<\\delta/2}+\\underbrace{|\\mathcal{L}(f_{1},\\ldots,f_{K})-\\mathcal{L}(f_{1}^{\\prime},\\ldots,f_{K}^{\\prime})|}_{<\\delta/2}<\\delta.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By using (38) together with Theorem 4.2 we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\epsilon\\sum_{k=1}^{K}\\lambda_{k}\\mathrm{KL}\\left(\\pi_{k}^{*}\\|\\pi^{f_{k}}\\right)={\\mathcal{L}}^{*}-{\\mathcal{L}}(f_{1},\\dots,f_{K})<\\delta\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which completes the proof. ", "page_idx": 21}, {"type": "text", "text": "A.7 Existence and uniqueness of the barycenter distribution which solves (5) ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We introduce an auxiliary functional which is the argument of minimization problem (5): ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{B}(\\mathbb{Q})\\overset{\\mathrm{def}}{=}\\sum_{k=1}^{K}\\lambda_{k}\\mathrm{EOT}_{c_{k},\\epsilon}(\\mathbb{P}_{k},\\mathbb{Q}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "i.e. the optimal value of (5) could be defined as $\\mathcal{L}^{*}=\\operatorname*{inf}_{\\mathbb{Q}\\in\\mathcal{P}(\\mathcal{Y})}\\mathcal{B}(\\mathbb{Q})$ . ", "page_idx": 21}, {"type": "text", "text": "Note that the functional $\\mathbb{Q}\\mapsto B(\\mathbb{Q})$ is strictly convex and lower semicontinuous (w.r.t. the weak topology) as each component $\\mathbb{Q}\\mapsto\\dot{\\mathrm{EOT}}_{c_{k},\\epsilon}(\\mathbb{P}_{k},\\mathbb{Q})$ is strictly convex and l.s.c. (lower semi-continuous) itself. The latter follows from [9, Th. 2.9] by noting that on $\\mathcal{P}(\\mathcal{Y})$ the map $\\mu\\mapsto\\mathbb{E}_{y\\sim\\mu}c_{k}(x,y)\\!-\\!H(\\mu)$ is l.s.c, bounded from below and strictly convex thanks to the entropy term. Since $\\mathscr{P}(\\mathscr{y})$ is weakly compact (as $\\boldsymbol{\\wp}$ is compact due to Prokhorov\u2019s theorem, see, e.g., [89, Box 1.4]), it holds that $\\mathcal{B}(\\mathbb{Q})$ admits at least one minimizer due to the Weierstrass theorem [89, Box 1.1], i.e., a barycenter $\\mathbb{Q}^{*}$ exists. In the paper, we work under the reasonable assumption that there exists at least one $\\mathbb{Q}$ for which $B(\\mathbb{Q}){<}\\infty$ . In this case, the barycenter $\\mathbb{Q}^{*}$ is unique due to the strict convexity of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ . ", "page_idx": 21}, {"type": "text", "text": "B Extended discussions ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "B.1 Extended discussion of related works ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Discrete OT-based solvers provide solutions to OT-related problems between discrete distributions. A comprehensive overview can be found in [83]. The discrete OT methods for EOT barycenter estimation are [20, 98, 11, 21, 22, 30, 63, 49, 68]. An alternative formulation of the barycenter problem based on weak mass transport and corresponding discrete solver could be found in [12]. In spite of sound theoretical foundations and established convergence guarantees [64], these approaches can not be directly adapted to our learning setup, see $\\S2.3$ . ", "page_idx": 21}, {"type": "text", "text": "Continuous OT solvers. Beside the continuous EOT solvers discussed in $\\S3$ , there exist a variety of neural OT solver for the non-entropic (unregularized, $\\epsilon=0$ ) case. For example, solvers such as [44, 74, 54, 58, 57, 32, 35, 88, 5, 31], are based on optimizing the dual form, similar to our (2), with neural networks. We mention these methods because they serve as the basis for certain continuous unregularized barycenter solvers. For example, ideas of [55] are employed in the barycenter method presented in [59]; the solver from [74] is applied in [32]; max-min solver introduced in [58] is used in [55]. It is also worth noting that there exist several neural solvers that cater to more general OT problem formulations [61, 60, 31, 8, 84]. These can even be adapted to the EOT case [42] but require substantial technical effort and the usage of restrictive neural architectures. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Continuous EOT solvers aim to recover the optimal EOT plan $\\pi^{*}$ in EOT problems like (1) between unknown distributions $\\mathbb{P}$ and $\\mathbb{Q}$ which are only accessible through a limited number of samples. One group of methods [37, 91, 23] is based on the dual formulation of OT problem regularized with KL divergences [37, Eq. $(\\mathcal{P}_{\\epsilon})]$ which is equivalent to (1). Another group of methods [109, 24, 13, 41, 106, 94] takes advantage of the dynamic reformulation of EOT via Schr\u00f6dinger bridges [70, 76]. In turn, [51] solve EOT with conditional flow matching [106]. ", "page_idx": 22}, {"type": "text", "text": "In [78], the authors propose an approach to tackle (1) by means of Energy-Based models [69, 100, EBM]. They develop an optimization procedure resembling standard EBM training which retrieves the optimal dual potential $f^{*}$ appearing in (2). As a byproduct, they recover the optimal conditional plans $\\mu_{x}^{f^{\\ast}}=\\pi^{\\ast}(\\bar{\\cdot}|x)$ . Our approach for solving the EOT barycenter (5) is primarily inspired by this work. In fact, we manage to overcome the theoretical and practical difficulties that arise when moving from the EOT problem guided with EBMs to the EOT barycenter problem (multiple marginals, optimization with respect to an unfixed marginal distribution $\\mathbb{Q}$ ), see $\\S4$ for details of our method. ", "page_idx": 22}, {"type": "text", "text": "Other related works. Another relevant work is [95], where the authors study the barycenter problem and restrict the search space to a manifold produced by a GAN. This idea is also utilized in $\\S5.2$ and $\\S5.3$ , but their overall setting drastically differs from our setup and actually is not applicable. We search for a barycenter of $K$ high-dimensional image distributions represented by their random samples (datasets). In contrast, they consider $K$ images, represent each image as a $2D$ distribution via its intensity histogram and search for a single image on the GAN manifold whose density is the barycenter of the input images. To compute the barycenter, they use discrete OT solver. In summary, neither our barycenter solver is intended to be used in their setup, nor their method is targeted to solve the problems considered in our paper. ", "page_idx": 22}, {"type": "text", "text": "B.2 Extended discussion of potential applications ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "It is not a secret that despite considerable efforts in developing continuous barycenter solvers [72, 59, 55, 32, 82, 14], these solvers have not found yet a real working practical application. The reasons for this are two-fold: ", "page_idx": 22}, {"type": "text", "text": "1. Existing continuous barycenter solvers (Table 1) are yet not scalable enough and/or work exclusively with the quadratic cost $(\\ell^{2})$ , which might be not sufficient for the practical needs.   \n2. Potential applications of barycenter solvers are too technical and, unfortunately, require substantial efforts (challenging and costly data collection, non-trivial design of task-specific cost functions, unclear performance metrics, etc.) to be implemented in practice. ", "page_idx": 22}, {"type": "text", "text": "Despite these challenges, there exist rather inspiring practical problem formulations where the continuous barycenter solvers may potentially shine and we name a few below. These potential applications motivate the research in the area. More generally, we hope that our developed solver could be a step towards applying continuous barycenters to practical tasks that benefit humanity. ", "page_idx": 22}, {"type": "text", "text": "1. Solving domain shift problems in medicine (Fig. 6a). In medicine, it is common that the data is collected from multiple sources (laboratories, clinics) and using different equipment from various vendors, each with varying technical characteristics [40, 65, 53, 103, 112]. Moreover, the data coming from each source may be of limited size. These issues complicate building robust and reliable machine learning models by using such datasets, e.g., learning MRI segmentation models to assist doctors. ", "page_idx": 22}, {"type": "text", "text": "A potential way to overcome the above-mentioned limitations is to find a common representation of the data coming from multiple sources. This representation would require translation maps that can transform the new (previously unseen) data from each of the sources to this shared representation. This formulation closely aligns with the continuous barycenter learning setup $(\\S2.3)$ studied in our paper. In this context, the barycenter could play the role of the shared representation. ", "page_idx": 22}, {"type": "text", "text": "To apply barycenters effectively to such domain averaging problems, two crucial ingredients are likely required: appropriate cost functions $c_{k}$ and a suitable data manifold $\\mathcal{M}$ in which to search for the barycenters. The design of the cost itself may be a challenge requiring certain domain-specific knowledge that necessitates involving experts in the field. Meanwhile, the manifold constraint is required to avoid practically meaningless barycenters such as those considered in $\\S5.2$ . Nowadays, with the rapid growth of the field of generative models, it is reasonable to expect that soon the new large models targeted for medical data may appear, analogously to DALL-E [85], StableDiffusion [87] or StyleGAN-T [90] for general image synthesis. These future models could potentially parameterize the medical data manifolds of interest, opening new possibilities for medical data analysis. ", "page_idx": 22}, {"type": "image", "img_path": "JZHFRLoqDq/tmp/df9ad1f396545fd5682a85846005ba8817526448bda602152de80757773eb108.jpg", "img_caption": ["Figure 6: A schematical presentation of potential applications of barycenter solvers. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "2. Mixing geological simulators (Fig. 6b). In geological modeling, variuos simulators exist to model different aspects of underground deposits. Sometimes one needs to build a generic tool which can take into account several desired geological factors which are successfully modeled by separate simulators. ", "page_idx": 23}, {"type": "text", "text": "$\\mathbf{Flumy}^{1}$ is a process-based simulator that uses hydraulic theory [48] to model specific channel depositional processes returning a detailed three-dimensional geomodel informed with deposit lithotype, age and grain size. However, its result is a 3D segmentation field of facies (rock types) and it does not produce the real valued porosity field needed for hydrodynamical modeling. ", "page_idx": 23}, {"type": "text", "text": "Petrel2 software is the other popular simulator in the oil and gas industry. It is able to model complex real-valued geological maps such as the distribution of porosity. The produced porosity fields may not be realistic enough due to paying limited attention to the geological formation physics. ", "page_idx": 23}, {"type": "text", "text": "Both Flumy and Petrel simulators contain some level of stochasticity and are hard to use in conjunction. Even when conditioned on common prior information about the deposit, they may produce maps of facies and permeability which do not meaningfully correspond to each other. This limitation provides potential prospects for barycenter solvers which could be used to get the best from both simulators by mixing the distributions produced by each of them. ", "page_idx": 23}, {"type": "text", "text": "From our personal discussions with the experts in the field of geology, such task formulations are of considerable interest both for scientific community as well as industry. Applying our barycenter solver in this context is a challenge for future research. We acknowledge that this would also require overcoming considerable technical and domain-specific issues, including the data collection and the choice of costs ck. ", "page_idx": 23}, {"type": "text", "text": "B.3 Extended discussion on doubly-regularized OT barycenters ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The objective (5) is not the only way to formulate Entropic OT barycenter problem. Recent studies [15, 82] consider the so-called doubly-regularized Entropic OT barycenters. Following the notations from [15], for $\\lambda\\geq0$ and $\\tau\\geq0$ we define: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{EOT}_{c,\\lambda,\\tau}^{\\mathrm{dr}}(\\mathbb{P},\\mathbb{Q})\\overset{\\mathrm{def}}{\\underset{\\pi\\in\\Pi(\\mathbb{P},\\mathbb{Q})}{\\cong}}\\Big\\{\\underset{(x,y)\\sim\\pi}{\\mathbb{E}}{\\subset}(x,y)+\\lambda K L(\\pi\\|\\mathbb{P}\\otimes\\mathbb{Q})+\\tau H(\\mathbb{Q})\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The corresponding doubly-regularized EOT barycenter problem is as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{L}^{*,\\mathrm{dr}}\\overset{\\mathrm{def}}{=}\\operatorname*{inf}_{\\mathbb{Q}\\in\\mathcal{P}(\\mathcal{Y})}B^{\\mathrm{dr}}(\\mathbb{Q})\\overset{\\mathrm{def}}{=}\\operatorname*{inf}_{\\mathbb{Q}\\in\\mathcal{P}(\\mathcal{Y})}\\sum_{k=1}^{K}\\lambda_{k}\\mathrm{EOT}_{c,\\lambda,\\tau}^{\\mathrm{dr}}\\bigl(\\mathbb{P}_{k},\\mathbb{Q}\\bigr).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "It turns out that our considered EOT barycenter (5) with regularization strength $\\epsilon$ is the particular case of (40) with $\\lambda\\,=\\,\\tau\\,=\\,\\epsilon$ . According to the classification [15, Table 1], this case is known as Schr\u00f6dinger barycenter. The natural question arizes: Is it possible to adapt our energy-guided methodology for the case $\\lambda\\neq\\tau$ ? ", "page_idx": 24}, {"type": "text", "text": "To answer this question, in what follows, we have a closer look at the differences between general doubly-regularized formulations and our particular Schr\u00f6dinger specification. The important property of our case is that the entropy term $H(\\mathbb{Q})$ completely disappears from the barycenter objective. In all the other regularized cases, when $\\lambda\\neq\\tau$ , this entropy term immediately reappears (either with the plus or minus sign). The presence of $H(\\mathbb{Q})$ term notably differs from ours and seems like to be not suitable for our solver. In what follows, we explain the reasons. ", "page_idx": 24}, {"type": "text", "text": "In the Schr\u00f6dinger case, the barycenter problem can be solved via optimizing conditional distributions $\\pi_{k}(y|x_{k})$ . Namely, we use potentials $f_{k}$ combined with costs $c_{k}$ to approximate the energy functions (unnormalized log-densities) of these conditionals. We employ EBM-based techniques to compute the gradient of the learning objective which avoids the direct computation of the entropy terms $\\bar{H}(\\bar{\\pi_{k}}(y|x_{k}))$ appearing in the $C_{k}$ -transforms. ", "page_idx": 24}, {"type": "text", "text": "If we further add the non-zero term $H(\\mathbb{Q})$ to the barycenter objective, this will presumably require (in the dual objective) a separate computation of the entropy terms $H(\\pi_{k}(y))$ of second marginals $\\pi_{k}(y)$ of each $\\pi_{k}$ . This is highly non-trivial and it seems like our solver does not easily generalize to this case. In our framework, we can get samples of $\\pi_{k}(y)$ by MCMC (via sampling $x_{k}\\sim\\mathbb{P}_{k}$ and then running MCMC for $\\pi(y|\\bar{{\\boldsymbol x}}_{k}))$ ). However, estimation of entropy of $\\pi_{k}(y)$ from raw samples still remains infeasible. In particular, EBM-like techniques (which we employ) can not be used here to derive the gradient of the objective. This is because the required unnormalized density of $\\pi_{k}(y)$ is itself unknown (we only know it for conditional distributions $\\pi_{k}(y|x))$ ). ", "page_idx": 24}, {"type": "table", "img_path": "JZHFRLoqDq/tmp/bf7367b90d31fe30808aded067912d9f76dc69af442b4fcf357e869982761810.jpg", "table_caption": ["Table 3: Computational complexity for Ours (all experiments) and baselines (Ave Celeba). "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "C Experimental Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The hyperparameters of our solver are summarized in Table 4. Some hyperparameters, e.g., $L,S,$ , iter, are chosen primarily from time complexity reasons. Typically, the increase in these numbers positively affects the quality of the recovered solutions, see, e.g., [42, Appendix E, Table 16]. However, to reduce the computational burden, we report the figures which we found to be reasonable. Working with the manifold-constraint setup, we parameterize each $g_{\\theta_{k}}(z)$ in our solver as $h_{\\theta_{k}}\\circ G(z)$ , where $G$ is a pre-trained (frozen) StyleGAN and $h_{\\theta_{k}}$ is a neural network with the ResNet architecture. We empirically found that this strategy provides better results than a direct MLP parameterization for the function $g_{\\theta_{k}}(z)$ . ", "page_idx": 24}, {"type": "table", "img_path": "JZHFRLoqDq/tmp/581d38d37e111d4878cada831f7d424fc1374d42473479d97fd1f751c1fdb4fc.jpg", "table_caption": ["Table 4: Hyperparameters that we use in the experiments with our Algorithm 1. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Computational complexity. We report the (approximate) time for training and inference (batch size $S=128)$ ) of our method on different experimental setups, see Table 3 (the hardware is a single $\\mathrm{{V}100\\ g p u)}$ . For Ave Celeba experiment, we additionally report the computational complexity of the competitors. As we can see, all the methods in this experiment (Ave Celeba) require a comparable amount of time for training. The inference with our approach is expectedly costlier than competitors due to the reliance on MCMC. ", "page_idx": 25}, {"type": "text", "text": "C.1 Barycenters of 2D/3D Distributions ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Cartesian representation of twister map. In $\\S5.1$ we define twister map $u$ using polar coordinate system. For clearness, we give its form on cartesian coordinates. Let $x\\in\\mathbb{R}^{2}$ , $x=(x_{(1)},x_{(2)})$ . Note that $\\left\\|{\\boldsymbol{x}}\\right\\|={\\sqrt{x_{(1)}^{2}+x_{(2)}^{2}}}$ Then, ", "page_idx": 25}, {"type": "equation", "text": "$$\nu(x)=u(x_{(1)},x_{(2)})=\\left(\\!\\!\\operatorname*{cos}\\left(\\left\\|x\\right\\|\\!\\right)\\!\\right)\\quad-\\sin\\left(\\left\\|x\\right\\|\\!\\right)\\right)\\left({x_{(1)}}\\right),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "i.e., the twister map rotates input points $x$ by angles equal to $\\|x\\|$ . ", "page_idx": 25}, {"type": "text", "text": "Analytical barycenter distribution for 2D Twister experiment. Below, we provide a mathematical derivation that the true unregularized barycenter of the distributions $\\mathbb{P}_{1},\\mathbb{P}_{2},\\mathbb{P}_{3}$ in Fig. 2a coincides with a Gaussian. ", "page_idx": 25}, {"type": "text", "text": "We begin with a rather general observation. Consider Xk = Y = RD (k \u2208K) and let OTc d=ef $\\mathrm{EOT}_{c,0}$ denote the unregularized OT problem $\\mathbf{\\boldsymbol{\\epsilon}}=\\mathbf{\\boldsymbol{0}}$ ) for a given continuous cost function $c$ . Let $u:\\mathbb{R}^{D}\\to\\mathbb{R}^{D}$ be a measurable bijection and consider $\\mathbb{P}_{k}^{\\prime}\\in\\mathcal{P}(\\mathbb{R}^{D})$ for $k\\in\\overline{{K}}$ . By using the change of variables formula, we have for all $\\mathbb{Q}^{\\prime}\\in\\mathcal{P}(\\mathbb{R}^{D})$ that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{OT}_{c\\circ(u\\times u)}\\big(u_{\\#}^{-1}(\\mathbb{P}^{\\prime}),u_{\\#}^{-1}(\\mathbb{Q}^{\\prime})\\big)=\\mathrm{OT}_{c}(\\mathbb{P}^{\\prime},\\mathbb{Q}^{\\prime}),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\#$ denotes the pushforward operator of distributions and $[c\\circ(u\\times u)](x,y)=c(u(x),u(y))$ . Note that by (41) the barycenter of $\\mathbb{P}_{1}^{\\prime},\\ldots,\\mathbb{P}_{K}^{\\prime}$ for the unregularized problem with cost $c$ coincides with the result of applying the pushforward operator u\u2212#1 to the barycenter of the very same problem but with cost $c\\circ(u\\times u)$ . ", "page_idx": 25}, {"type": "text", "text": "Next, we fix $u$ to be the twister map (\u00a75.1). In Fig. 2a we plot the distributions $\\mathbb{P}_{1}\\ \\stackrel{\\mathrm{def}}{=}$ $u_{\\#}^{-1}\\mathbb{P}_{1}^{\\prime},u_{\\#}^{-1}\\mathbb{P}_{1}^{\\prime},u_{\\#}^{-1}\\mathbb{P}_{3}^{\\prime}$ which are obtained from Gaussian distributions $\\mathbb{P}_{1}^{\\prime}=\\mathcal{N}\\big((0,4),I_{2}\\big),\\mathbb{P}_{2}^{\\prime}=$ ${\\mathcal{N}}((-2,2{\\sqrt{3}}),I_{2}),\\mathbb{P}_{3}^{\\prime}={\\mathcal{N}}{\\big(}(2,2{\\sqrt{3}}),I_{2}{\\big)}$ by the pushforward. Here $I_{2}$ is the 2-dimensional identity matrix. For the unregularized $\\ell^{2}$ barycenter problem, the barycenter of such shifted Gaussians can be derived analytically [3]. The solution coincides with a zero-centered standard Gaussian $\\mathbb{Q}^{\\prime}\\,=\\,\\mathcal{N}\\big(0,I_{2}\\big)$ . Hence, the barycenter of $\\mathbb{P}_{1},\\ldots,\\mathbb{P}_{K}$ w.r.t. the cost $\\ell^{2}\\circ(u\\times u)$ is given by $\\mathbb{Q}^{*}=u_{\\#}^{-1}\\mathbb{Q}^{\\prime}$ . From the particular choice of $u$ it is not hard to see that $\\mathbb{Q}^{*}=\\mathbb{Q}^{\\prime}={\\mathcal{N}}{\\big(}0,I_{2}{\\big)}$ as well. ", "page_idx": 25}, {"type": "text", "text": "C.2 Barycenters of MNIST Classes ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Additional qualitative examples of our solver\u2019s results are given in Figure 7. ", "page_idx": 25}, {"type": "text", "text": "Details of the baseline solvers. For the solver by [32, SCWB], we run their publicly available code from the official repository ", "page_idx": 25}, {"type": "text", "text": "https://github.com/sbyebss/Scalable-Wasserstein-Barycenter ", "page_idx": 25}, {"type": "text", "text": "The authors do no provide checkpoints, and we train their barycenter model from scratch. In turn, for the solver by [55, WIN], we also employ the publicly available code ", "page_idx": 25}, {"type": "text", "text": "Here we do not train their models but just use the checkpoint available in their repo. ", "page_idx": 25}, {"type": "image", "img_path": "JZHFRLoqDq/tmp/59e11cb9c304a0cf60c8c4fed52503da0df73d8f13a2e361e25349f56071a4cd.jpg", "img_caption": ["(b) Learned plans from $\\mathbb{P}_{2}$ (ones, first column) to the barycenter. Figure 7: Experiment with averaging MNIST 0/1 digit classes. The plot shows additional examples of samples transported with our solver to the barycenter. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "C.3 Barycenters of the Ave, Celeba! Dataset ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Additional qualitative examples of our solver\u2019s results are given in Figure 8. ", "page_idx": 26}, {"type": "text", "text": "Extra experiment in Data Space. Our method in the manifold constrained setup on MNIST dataset performs better than in the data space setup (see Figure 5). It generates less noised images and demonstrates better perceptual quality. For this reason, we provide only Manifold-constrained setting for experiment with Ave, Celeba! dataset in $\\S5.3$ . ", "page_idx": 26}, {"type": "text", "text": "For completeness, here we also test our method directly in the data space setup for Ave, Celeba! dataset. Hyperparameters of our solver are listed in Table 4. In Figure 10, we show images obtained in Data space setup. As expected, the FID scores in Data space are not that good as the images are more noised since we solve the entropy-regularized problem. But we stress one more time that our method permits StyleGAN manifold trick, which greatly improves the performance, see the images in Figure 4 for the manifold-constrained setup. ", "page_idx": 26}, {"type": "text", "text": "Convergence at training. We provide the behaviour of ${\\mathcal{L}}_{2}$ -UVP metric (between the unregularized ground truth barycenter mapping and the entropic barycenter mapping for our learned $\\pi^{\\breve{f}_{k}}(y|x_{k}))$ for Ave Celeba experiment $\\S5.3$ , see Figure 9 . We emphasize that ${\\mathcal{L}}_{2}$ -UVP directly measures (by computing pointwise MSE) how the learned mapping differs from the true mapping. ", "page_idx": 26}, {"type": "image", "img_path": "JZHFRLoqDq/tmp/9983e5af14a57ab738439fd545ee5566add121b922f79bb5ebfcb0db54447645.jpg", "img_caption": ["(a) Maps from $\\mathbb{P}_{1}$ to the barycenter. (b) Maps from $\\mathbb{P}_{2}$ to the barycenter. (c) Maps from $\\mathbb{P}_{3}$ to the barycenter. Figure 8: Experiment on the Ave, celeba! barycenter dataset. The plots show additional examples of samples transported with our solver to the barycenter. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "JZHFRLoqDq/tmp/f634770ca1dcf4a3256aa712598f717e8af2b05b190249603ade3e90859fc8de.jpg", "img_caption": ["Figure 9: Training curves of $\\mathcal{L}2$ -UVP vs. time for OUR proposed method in Manifold-constrained setup with Style-GAN on Ave, Celeba! dataset. The duration of the training is $100\\,\\mathrm{h}$ (1 GPU V100). "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Convergence at inference. Since Langevin Dynamics is at the heart of our method for sampling from a conditional plan, it is important to demonstrate the dependence of the inference times/quality on the number of $L$ Langevin steps in Manifold constrained as well as in the Data space setup. We demonstrate Tables 5 and 6, where we show the trade-off between number of Langevin steps $L$ and the obtained quality. To provide the comprehensive analysis, we report FID scores as well as sampling time for different number of steps $L$ . Expectedly, inference time linearly depends on $L$ both setups. ", "page_idx": 27}, {"type": "text", "text": "Our results testify that (in Manifold constrained setting) our method achieves good quality even with rather small number of Langevin steps, i.e., the computational burden of our approach could be considerably reduced with rather little trade-off in quality. ", "page_idx": 27}, {"type": "text", "text": "Details of the baseline solvers. For the [55, WIN] solver, we use their pre-trained checkpoints provided by the authors in the above-mentioned repository. Note that the authors of [32, SCWB] do not consider such a high-dimensional setup with RGB images. Hence, to implement their approach in this setting, we follow [55, Appendix B.4]. ", "page_idx": 27}, {"type": "table", "img_path": "JZHFRLoqDq/tmp/2fa2baa8a4634e806ff619c7d004798cdc19296d1a49a17dc1f35a9a49922f2b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 5: The dependence of running time of ULA on number of Langevin steps during inference mode and the trade-off between the steps and the quality of obtained images in Ave, Celeba! dataset in Manifold constrained setting. ", "page_idx": 28}, {"type": "table", "img_path": "JZHFRLoqDq/tmp/69457c081aae5350ed45b0999f1918648e42040145995149f211a3b50f9e23b0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 6: The dependence of running time of ULA on number of Langevin steps during inference mode and the trade-off between the steps and the quality of obtained images in Ave, Celeba! dataset in Data space setup. ", "page_idx": 28}, {"type": "image", "img_path": "JZHFRLoqDq/tmp/02da87fd40fb8ad95befb3abc3339e0fe3fb0ab29af607fcba1826135229f606.jpg", "img_caption": ["(a) Maps from $\\mathbb{P}_{1}$ to the barycenter. (b) Maps from $\\mathbb{P}_{2}$ to the barycenter. (c) Maps from $\\mathbb{P}_{3}$ to the barycenter. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 10: Experiment on the Ave, celeba! barycenter dataset. The plots represent transported inputs $x_{k}\\sim\\mathbb{P}_{k}$ to the barycenter learned by our algorithm in Data space . The true unregularized $\\ell^{2}$ barycenter of $\\mathbb{P}_{1},\\mathbb{P}_{2},\\mathbb{P}_{3}$ are situated in figure $^{4\\mathrm{a},4\\mathrm{b}}$ and $4c$ correspondingly. ", "page_idx": 28}, {"type": "text", "text": "C.4 Barycenters of Gaussian Distributions ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We note that there exist many ways to incorporate the entropic regularization for barycenters [15, Table 1]; these problems do not coincide and yield different solutions. For some of them, the ground-truth solutions are known for specific cases, such as the Gaussian case. For example, [75, Theorem 3] examines barycenters for OT regularized with KL divergence. They consider the task ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\mathbb{Q}\\in\\mathcal{P}(y)}{\\operatorname*{inf}}\\underset{k=1}{\\overset{K}{\\sum}}\\lambda_{k}(\\displaystyle\\int_{\\mathcal{X}\\times y}\\frac{\\|x-y\\|^{2}}{2}d\\pi_{k}(x,y)+\\epsilon\\mathrm{KL}(\\pi_{k}\\|\\mathbb{P}_{k}\\times\\mathbb{Q}))=}\\\\ {\\underset{\\mathbb{Q}\\in\\mathcal{P}(y)}{\\operatorname*{inf}}\\underset{k=1}{\\overset{K}{\\sum}}\\lambda_{k}(\\displaystyle\\int_{\\mathcal{X}\\times y}\\frac{\\|x-y\\|^{2}}{2}d\\pi_{k}(x,y)-\\epsilon\\displaystyle\\int_{\\mathcal{X}}H(\\pi_{k}(\\cdot|x))d\\mathbb{P}_{k}(x)+\\epsilon H(\\mathbb{Q}))=}\\\\ {\\underset{\\mathbb{Q}\\in\\mathcal{P}(y)}{\\operatorname*{inf}}\\left\\lbrace\\underset{k=1}{\\overset{K}{\\sum}}\\lambda_{k}\\mathrm{EOT}_{\\ell^{2},\\epsilon}(\\mathbb{P}_{k},\\mathbb{Q})+\\epsilon H(\\mathbb{Q})\\right\\rbrace}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This problem differs from our objective (5) with $\\begin{array}{r}{c(x,y)\\,=\\,\\frac{1}{2}\\|x\\,-\\,y\\|^{2}}\\end{array}$ by the non-constant $\\mathbb{Q}$ - dependent term $\\epsilon H(\\mathbb{Q})$ ; this problem yields a different solution. The difference with the majority of other mentioned approaches can be shown in the same way. In particular, [67] tackles the barycenter for inner product Gromov-Wasserstein problem with entropic regularization, which is not relevant for us. In turn, the authors of [49] demonstrate significant progress towards GT results for barycenter problem with our considered regularization, but only for univariate (1D) case. To our knowledge, the general Gaussian ground-truth solution for our problem setup (5) is not yet known, although some of its properties seem to be established [25]. ", "page_idx": 28}, {"type": "text", "text": "Still, when $\\epsilon\\approx0$ , our entropy-regularized barycenter is expected to be close to the unregularized one $\\left.\\varepsilon=0\\right.$ ). In the Gaussian case, it is known that the unregularized OT barycenter for $c_{k}(x,y)=$ ${\\frac{1}{2}}\\|x-y\\|^{2}$ is itself Gaussian and can be computed using the well-celebrated fixed point iterations of [3, Eq. (19)]. This gives us an opportunity to compare our results with the ground-truth unregularized barycenter in the Gaussian case. As the baseline, we include the results of [55, WIN] solver which learns the unregularized barycenter $\\epsilon=0$ ). ", "page_idx": 28}, {"type": "text", "text": "We consider 3 Gaussian distributions $\\mathbb{P}_{1},\\mathbb{P}_{2},\\mathbb{P}_{3}$ in dimensions $D=2,4,8,16,64$ and compute the approximate EOT barycenters $\\pi_{k}^{\\widehat{f}_{k}}$ for $\\epsilon=0.01,1$ w.r.t. weights $\\begin{array}{r}{(\\lambda_{1},\\lambda_{2},\\lambda_{3})=(\\frac{1}{4},\\frac{1}{4},\\frac{1}{2})}\\end{array}$ with our solver. To initialize these distributions, we follow the strategy of [55, Appendix C.2]. The ground truth unregularized barycenter $\\mathbb{Q}^{*}$ is estimated via the above-mentioned iterative procedure. We use the code from WIN repository available via the link mentioned in Appendix C.2. To assess the WIN solver, we use the unexplained variance percentage metrics defined as $\\mathcal{L}_{2^{-}}\\mathrm{UVP}(\\hat{T})=100\\cdot[||\\hat{T}\\!-\\!T^{*}||]_{\\mathbb{P}}^{2}$ where $T^{*}$ denotes the optimal transport map $T^{*}$ , see [54, $\\S5.1]$ . Since our solver computes EOT plans but not maps, we evaluate the barycentric projections of the learned plans, i.e., $\\begin{array}{r}{\\widehat{\\overline{{T}}}_{k}(x)=\\int_{\\mathcal{V}}y\\pi_{k}^{\\widehat{f}_{k}}(y|x)}\\end{array}$ , and calculate $\\mathcal{L}_{2^{-}}\\mathrm{UVP}(\\widehat{\\overline{{T}}}_{k},T_{k}^{*})$ . We evaluate this metric using $10^{4}$ samples. To estimate the barycentric projection in our solver, we use $10^{3}$ samples $y\\sim\\pi_{k}^{\\hat{f}_{k}}(y|x_{k})$ for each $x_{k}$ . To keep the table with the results simple, in each case we report the average of this metric for $k=1,2,3$ w.r.t. the weights $\\lambda_{k}$ . ", "page_idx": 29}, {"type": "table", "img_path": "JZHFRLoqDq/tmp/07275d8dd82c57784bee7adb83dda9caa8b2d0fbfa288eb0f23b727dfe6fe29f.jpg", "table_caption": ["Table 7: $\\mathcal{L}_{2}$ -UVP for our method with $\\epsilon=0.01,1$ and WIN, $D=2,4,8,16,64$ . "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "We see that for small $\\epsilon=0.01$ and dimension up to $D=16$ , our algorithm gives the results even better than WIN solver designed specifically for the unregularized case ( $\\mathbf{\\boldsymbol{\\epsilon}}=0$ ). As was expected, larger $\\epsilon=1$ leads to the increased bias in the solutions of our algorithm and $\\mathcal{L}_{2}$ -UVP metric increases. ", "page_idx": 29}, {"type": "text", "text": "Effect of the batch size. In order to test how our method is affected by batch size, we conduct an additional empirical study with varying batch size. We follow our Gaussian experimental setup from above and pick $(D,\\epsilon)\\;=\\;(2,0.\\dot{1})$ , $(D,\\epsilon)\\,=\\,(16,0.01)$ . As the batch sizes, we consider 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024. As we can see in Table 9, the quality of the recovered plans $\\pi^{f_{k}}$ between reference and barycenter distributions naturally grows with the batch size. In all our other experiments, we typically choose the batch size to be a reasonable number which, on the one hand, allows achieving sufficient quality and, on the other hand, provides reasonable computational burden. In the image experiments, we use batch size $\\leq128$ as it already provides reasonable performance. Going beyond that is challenging due to the computational restrictions. ", "page_idx": 29}, {"type": "text", "text": "Effect of sampling steps number at training. We conducted an ablation study testing how our method is affected by chosen number of discretized Langevin dynamic steps ( $L$ from $\\S4.2\\rangle$ at training, the results are presented in Figure 11. In this experiment, we try to learn the barycenter of Gaussian distributions; we pick the dimensionality $D=64$ (which is the highestdimensional case among the considered); $\\epsilon=\\overline{{10^{-1}}}$ . As we can see, performance drops when the number of steps is insufficient. Overall, in all our experiments, the number of Langevin steps is chosen to achieve reasonable qualitative/quantitative results. ", "page_idx": 29}, {"type": "text", "text": "C.5 Single-cell experiment ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We consider the problem of predicting the interpolation between single cell populations at multiple timepoints from [107]. The objective is to interpolate the distribution of cell population at any intermediate point in time, call it $t_{i}$ , given the cell population distributions at past and future time-points $t_{i+1}$ and $t_{i-1}$ . Since this is an interpolation problem, it is natural to expect that the intermediate population is a (entropy-regularized) barycentric average (with $\\ell^{2}$ cost) of both the population distributions available at the nearest preceding and future times. We leverage the data pre-processing and metrics from paper [56], wherein the authors provide a complete ", "page_idx": 29}, {"type": "image", "img_path": "JZHFRLoqDq/tmp/7311a157da070429390c533067c3e501575f1d1d1a10db91257bc885c00f560f.jpg", "img_caption": ["Figure 11: The training curves of $\\mathcal{L}_{2}$ -UVP vs. iterations for OUR proposed method for the barycenter of Gaussian distributions depending on number of Langevin steps $L$ . "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "table", "img_path": "JZHFRLoqDq/tmp/b79ea1b136cb599a54c35ec98980528526dccd2e285bdff7ee907cd504ff6660.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Table 8: Energy distance (averaged for two setups and 5 random seeds) on the MSCI dataset along with $95\\%$ -confidence interval ( $\\pm$ -intervals). The best solver according to the mean value is bolded. ", "page_idx": 30}, {"type": "table", "img_path": "JZHFRLoqDq/tmp/ba88a3c37e2ee53bf1dfd5cd7afc771baa6bd0c545cd5794c724d09050d1a1ba.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Table 9: Influence of the batch size on the performance of our approach in the case of computing the barycenters of Gaussian distributions. ", "page_idx": 30}, {"type": "text", "text": "notebook with the code to launch the setup similar to [107]. There are 3 experimental settings with dimensions $D=50$ , 100, 1000, each setting contains two setups: predicting day 3 given days 2 and 4 and predicting day 4 given days 3 and 7. The metric is MMD; see [107] or Section 5.3 from [56] for additional experimental details. We report the result in Table 8 where we find that in most cases, our general-purpose entropic barycenter approach nearly matches the performance of leading baselines. This underscores the scope of problems in which our barycentric optimal transport technology can act as a viable foundation model, directly out-of-the-box. ", "page_idx": 30}, {"type": "text", "text": "D Alternative EBM training procedure ", "text_level": 1, "page_idx": 30}, {"type": "image", "img_path": "JZHFRLoqDq/tmp/ecac98941ed691dd975646669c9c38f30d479c6e8a6ca8dab0800ed296c0f7d5.jpg", "img_caption": ["Figure 12: $2D$ twister example. Trained with importance sampling: The true barycenter of 3 comets vs. the one computed by our solver with $\\epsilon=10^{-2}$ . Two costs $c_{k}$ are considered: the twisted cost (12a, 12b) and $\\bar{\\ell}^{2}$ (12c, 12d). We employ the simulation-free importance sampling procedure for training. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "In this section, we describe an alternative simulation-free training procedure for learning EOT barycenter distribution via our proposed methodology. The key challenge behind our approach is to estimate the gradient of the dual objective (4.3). To overcome the difficulty, in the main part of our wmiathn uMscorinptet,- Cwaer luot.i liHzeer eM wCe MdiCs csuasms pa lipnogt efnrtoiaml  aclotenrdniatitiovnea la pdipsrtoraibcuht iboansse $\\mu_{x_{k}}^{\\bar{f}_{\\theta,k}}$ manpdo retsatinmcea tsea tmhep llionsgs (IS) [105]. That is, we evaluate the internal integral over $\\boldsymbol{\\wp}$ in (4.3): ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{Z}(x_{k})\\stackrel{\\mathrm{def}}{=}\\int_{\\mathcal{Y}}\\left[{\\frac{\\partial}{\\partial\\theta}}f_{\\theta,k}(y)\\right]\\mathrm{d}\\mu_{x_{k}}^{f_{\\theta,k}}(y)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "with help of an auxiliary proposal (continuous) distribution accessible by samples with the known density $q(y)$ . Let $Y^{q}={\\bar{\\{y_{1}^{q},\\ldots,y_{P}^{q}\\}}}$ be a sample from $q(y)$ . Define the weights: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\omega_{k}(x_{k},y_{p}^{q})\\overset{\\mathrm{def}}{=}\\exp{\\left(\\frac{f_{\\theta,k}(y_{p}^{q})-c(x_{k},y_{p}^{q})}{\\epsilon}\\right)}q(y_{p}^{q}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then (42) permits the following stochastic estimate: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{Z}(x_{k})\\approx\\frac{\\sum_{p=1}^{P}\\left[\\frac{\\partial}{\\partial\\theta}f_{\\theta,k}(y)\\right]\\omega_{k}(x_{k},y_{p}^{q})}{\\sum_{p=1}^{P}\\omega_{k}(x_{k},y_{p}^{q})}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Experimental illustration. To demonstrate the applicability of IS-based training procedure to our barycenter setting, we conduct the experiment following our 2D Twister setup, see $\\S5.1$ . We employ zero-mean $16I$ -covariance Gaussian distribution as $q$ and pick the batch size $P=1024$ . Our results are shown in Figure 12. As we can see, the alternative training procedure yields similar results as Figure2 but converges faster ( $\\approx1$ min. $\\mathrm{VS}\\approx18~\\mathrm{min}$ . of the original MCMC-based training). ", "page_idx": 31}, {"type": "text", "text": "Concluding remarks. We note that IS-based methods requires accurate selection of the proposal distribution $q$ to reduce the variance of the estimator [105]. It may be challenging in real-world scenarios. We leave the detailed study of more advanced IS approaches in the context of energy-based models and their applicability to our EOT barycentric setup to follow-up research. ", "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: In the Introduction section, we completely describe our contributions. For every contribution, we provide a link to the section about it. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We discuss limitations in Section 6. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Proofs are provided in Appendix A. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Experimental details are discussed in Appendix C. Code for the experiments is available at publicly accessible github. All the datasets are available in public. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification:Code is publicly available at a github repository. Experimental details are provided in Appendix C. All the datasets are publicly available. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Experimental details are discussed in Appendix C; the hyperparameters and peculiarities are hard-coded in our publicly available source code. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [No] ", "page_idx": 32}, {"type": "text", "text": "Justification: Not all of the experiments are conducted following the statistical significance protocol due to computational restrictions. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: In Appendix C, we mention time and resources. ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] Justification: Research conforms with NeurIPS Code of Ethics. ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We discuss broader impact in Appendix 6. ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] Justification: The research does not need special safeguards. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We cite each used assets. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: New code is available at public github repository, the code is self-documented and follows our experimental protocol from the manuscript. The license is MIT. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The research does not engage with Crowdsourcing or Human subjects. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] Justification: The research does not engage with Crowdsourcing or Human subjects. ", "page_idx": 33}]