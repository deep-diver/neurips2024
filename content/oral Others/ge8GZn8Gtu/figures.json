[{"figure_path": "ge8GZn8Gtu/figures/figures_3_1.jpg", "caption": "Figure 1: A geometric interpretation of SNR.", "description": "The figure provides a geometric interpretation of the signal-to-noise ratio (SNR) in the context of clustering under Gaussian Mixture Models (GMMs) with anisotropic covariance structures. The left panel shows two Gaussian distributions with different means and the same covariance matrix. The black line represents the optimal testing procedure that divides the space into two half-spaces. The right panel shows the same two distributions but with a transformation that makes them isotropic. The distance between the centers of the two distributions is shown, which is related to the SNR. This transformation helps visualize how the SNR captures the difficulty of clustering due to the separation of the cluster centers and the covariance structure.", "section": "2 GMM with Unknown but Homogeneous Covariance Matrices"}, {"figure_path": "ge8GZn8Gtu/figures/figures_6_1.jpg", "caption": "Figure 1: A geometric interpretation of SNR.", "description": "The figure provides a geometric interpretation of the signal-to-noise ratio (SNR) used in the paper.  The left panel shows two Gaussian distributions with different means but the same covariance matrix. The black curve represents the optimal testing procedure that separates the two distributions.  The right panel shows the same distributions after a linear transformation that makes them isotropic (variance is equal in all directions).  The distance between the transformed means in the right panel represents SNR.  The closer the transformed means are, the harder it is to distinguish them (lower SNR, more difficult clustering).", "section": "2 GMM with Unknown but Homogeneous Covariance Matrices"}, {"figure_path": "ge8GZn8Gtu/figures/figures_8_1.jpg", "caption": "Figure 3: Left: Performance of Algorithm 1 compared with other methods under Model 1. Right: Performance of Algorithm 2 compared with other methods under Model 2.", "description": "This figure compares the performance of the proposed algorithms (Algorithm 1 and Algorithm 2) with other baseline methods (spectral clustering and vanilla Lloyd's algorithm) under two different anisotropic Gaussian Mixture Models (Model 1 and Model 2).  The x-axis represents the number of iterations, while the y-axis shows the logarithm of the misclustering error rate. The plots visualize how the error rate decreases with increasing iterations for each method.  The dashed black line represents the theoretical minimax lower bound for the error rate.  The results illustrate that the proposed algorithms significantly outperform the baseline methods and achieve the optimal rate predicted by the minimax bounds.", "section": "Numerical Studies"}, {"figure_path": "ge8GZn8Gtu/figures/figures_9_1.jpg", "caption": "Figure 4: Visualization of the Fashion-MNIST dataset using the first two principal components. The data points are color-coded to indicate class membership: Red represents the T-shirt/top class, green denotes the Trouser class, and blue signifies the Ankle boot class. This illustration shows the existence of anisotropic and heterogeneous covariance structures.", "description": "This figure visualizes the Fashion-MNIST dataset using principal component analysis (PCA) to reduce dimensionality.  The left panel shows two classes (T-shirt/top and Trouser), while the right panel includes a third (Ankle boot).  The data points are color-coded by class, revealing the anisotropic and heterogeneous covariance structures (meaning the data's spread and orientation vary across classes). This visualization supports the paper's claim that the proposed clustering methods are suitable for handling such data characteristics.", "section": "Real Data"}]