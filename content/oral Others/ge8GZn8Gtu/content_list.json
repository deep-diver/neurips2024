[{"type": "text", "text": "Achieving Optimal Clustering in Gaussian Mixture Models with Anisotropic Covariance Structures ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xin Chen Princeton University xc5557@princeton.edu ", "page_idx": 0}, {"type": "text", "text": "Anderson Ye Zhang University of Pennsylvania ayz@wharton.upenn.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study clustering under anisotropic Gaussian Mixture Models (GMMs), where covariance matrices from different clusters are unknown and are not necessarily the identity matrix. We analyze two anisotropic scenarios: homogeneous, with identical covariance matrices, and heterogeneous, with distinct matrices per cluster. For these models, we derive minimax lower bounds that illustrate the critical infuence of covariance structures on clustering accuracy. To solve the clustering problem, we consider a variant of Lloyd's algorithm, adapted to estimate and utilize covariance information iteratively. We prove that the adjusted algorithm not only achieves the minimax optimality but also converges within a logarithmic number of iterations, thus bridging the gap between theoretical guarantees and practical efficiency. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Clustering is a fundamentally important task in statistics and machine learning [7, 2]. The most widely recognized and extensively studied model for clustering is the Gaussian Mixture Model (GMM) [17, 19], which is formulated as ", "page_idx": 0}, {"type": "equation", "text": "$$\nY_{j}=\\theta_{z_{j}^{*}}^{*}+\\epsilon_{j},~\\mathrm{where}~\\epsilon_{j}\\overset{i n d}{\\sim}\\mathcal{N}(0,\\Sigma_{z_{j}^{*}}^{*}),~\\forall j\\in[n].\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "Here $Y\\;=\\;(Y_{1},\\ldots,Y_{n})$ are the observations with $n$ being the sample size. We define the set $[n]\\;=\\;\\{1,2,...,n\\}$ .Assume $k$ is the known number of clusters. Let $\\{\\theta_{a}^{*}\\}_{a\\in[k]}$ represent the unknown centers, and $\\Sigma_{a}^{*}$ denote the corresponding unknown covariance matrices. Define $z^{*}\\in[k]^{n}$ as the cluster assignment vector, where for each index $j\\in[n]$ , the value of $z_{j}^{*}$ specifies which cluster the $j$ -th data point is assigned to. The goal is to recover $z^{*}$ from $Y$ . For any estimator $\\hat{z}$ , its clustering performance is measured by the misclustering error rate $h(\\hat{z},z^{*})$ , which will be introduced later in (4). ", "page_idx": 0}, {"type": "text", "text": "There has been increasing interest in theoretical and algorithmic analysis of clustering under GMMs. In a scenario where a GMM is isotropic, meaning that all covariance matrices $\\{\\Sigma_{a}^{*}\\}_{a\\in[k]}$ are equal to the identity matrix, [15] obtained the minimax rate for clustering, which takes the form of $\\mathrm{exp}(-(1+o(1))(\\mathrm{min}_{a\\neq b}\\,\\|\\theta_{a}^{*}-\\theta_{b}^{*}\\|)^{2}/8)$ , with respect to the misclustering error rate. A diverse range of methods has been explored in the context of the isotropic setting. Among these, Lloyd's algorithm [13] stands out as a particularly effective clustering algorithm, renowned for its extensive success in a myriad of disciplines. [15, 8] establish computational and statistical guarantees for the Lloyd's algorithm. Specifically, they showed it achieves the minimax optimal rates after a few iterations provided with some decent initialization. Another popular approach to clustering especially for high dimensional data is the spectral clustering [21, 18, 20], which is an umbrella term for clustering after a dimension reduction through a spectral decomposition. [14] proves the spectral clustering also achieves the optimality under the isotropic GMM. Semidefinite programming (SDP) ", "page_idx": 0}, {"type": "text", "text": "is also used for clustering by exploiting its low-rank structure, and its statistical properties have been studied in literature, for example, [5]. ", "page_idx": 1}, {"type": "text", "text": "Despite the numerous compelling findings, most existing research primarily focuses on isotropic GMMs. The understanding of clustering in an anisotropic context, where the covariance matrices are not constrained to be identity matrices, remains relatively limited. Some studies, including [15, 5, 16, 1, 9, 24], present results for sub-Gaussian mixture models, wherein the errors $\\epsilon_{j}$ are assumed to follow some sub-Gaussian distributions with the variance proxy $\\sigma^{2}$ .At first glance, it might appear that these results encompass the anisotropic case, as distributions of the form $\\{\\mathcal{N}(\\bar{0},\\Sigma_{a}^{*})\\}_{a\\in[k]}$ are indeed sub-Gaussian distributions. However, from a minimax perspective, the least favorable scenario among all sub-Gaussian distributions with variance proxy $\\sigma^{2}$ and thus the most challenging for clustering\u2014is when the errors are distributed as $\\mathcal{N}(0,\\sigma^{2}I)$ .Therefore, the minimax rate for clustering under the sub-Gaussian mixture model essentially equals the one under the isotropic GMM, and methods like Lloyd's algorithm, which require no covariance matrix information, can be rate-optimal. As a result, the aforementioned findings primarily pertain to isotropic GMMs. ", "page_idx": 1}, {"type": "text", "text": "A few studies have explored the direction of clustering under anisotropic GMMs. [3] presents a polynomial-time clustering algorithm that provably performs well when Gaussian distributions are well-separated by hyperplanes. This idea is further developed in [11], which extends the approach to allow overlapping Gaussians, albeit only in two-cluster scenarios. [22] proposes a novel method for clustering under a balanced mixture of two elliptical distributions. They establish a provable upper bound on their clustering performance. Nevertheless, the fundamental limit of clustering under anisotropic GMMs, and whether a polynomial-time procedure can achieve it, remains unknown. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we investigate the clustering task under two anisotropic GMMs. In Model 1, all covariance matrices are equal (i.e., homogeneous) to some unknown matrix $\\Sigma^{*}$ .Model2offers more fexibility, with covariance matrices that are unknown and not necessarily identical (i.e., heterogeneous). The contribution of this paper is two-fold, summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u00b7 Our first contribution is on the minimax rates. We obtain minimax lower bounds for clustering under anisotropic GMMs with respect to the misclustering error rate. We show they take the form of ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{z}}\\operatorname*{sup}_{z^{*}}\\mathbb{E}h(\\hat{z},z^{*})\\geq\\exp\\!\\left(-(1+o(1))\\frac{(\\mathrm{signal-to-noise\\;ratio})^{2}}{8}\\right),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where the signal-to-noise ratio under Model 1 is equal to $\\begin{array}{r l}{\\operatorname*{min}_{a,b\\in[k]:a\\neq b}\\|(\\theta_{a}^{*}-\\theta_{b}^{*})^{T}\\Sigma^{*-\\frac{1}{2}}\\|}\\end{array}$ The signal-to-noise ratio for Model 2 is more intricate and will be introduced in Section 3. For both models, we can see the minimax rates depend not only on the centers but also on the covariance matrices. This is different from the isotropic case, whose signal-to-noise ratio $\\operatorname*{min}_{a\\neq b}\\|\\theta_{a}^{*}-\\theta_{b}^{*}\\|$ . Our results precisely capture the role that covariance matrices play in the clustering problem. This shows that covariance matrices impact the fundamental limits of the clustering problem through complex interactions with the centers, especially in Model 2. We obtain the minimax lower bounds by drawing connections with Linear Discriminant Analysis (LDA) [6] and Quadratic Discriminant Analysis (QDA). ", "page_idx": 1}, {"type": "text", "text": "\u00b7 Our second and more important contribution is on the computational side. We give a computationally feasible procedure and rate-optimal algorithm for the anisotropic GMM. Lloyd's algorithm, developed for the isotropic case, is no longer optimal as it only considers distances among centers [3]. We study an adjusted Lloyd's algorithm which estimates the covariance matrices in each iteration and adjusts the clusters accordingly. It can also be seen as a hard EM algorithm [4]. Here, we modify the E-step of the soft EM by implementing a maximization step that directly assigns data points to clusters, rather than calculating probabilities. As an iterative algorithm, we demonstrate that it achieves the minimax lower bound within $\\log n$ iterations. This offers both statistical and computational guarantees, serving as valuable guidance for practitioners. Specifically, if we let $\\bar{z}^{(t)}$ denote the output of the algorithm after $t$ iterations, it holds with high probability that ", "page_idx": 1}, {"type": "equation", "text": "$$\nh(z^{(t)},z^{\\ast})\\leq\\exp\\!\\left(-(1+o(1))\\frac{\\mathrm{(signal\\mathrm{-}t o\\mathrm{-}n o i s e\\ r a t i o)^{2}}}{8}\\right),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "for all $t\\,\\geq\\,\\log n$ . The algorithm can be initialized using popular methods like spectral clustering or Lloyd's algorithm. In our numerical studies, we demonstrate that our algorithm significantly improves over the two aforementioned methods under anisotropic GMMs, and matches the optimal exponent specified in the minimax lower bound. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Paper Organization. The remaining paper is organized as follows. In Section 2, we study Model 1 where the covariance matrices are unknown but homogeneous. In Section 3, we consider Model 2 where covariance matrices are unknown and heterogeneous. For both cases, we establish the minimax lower bound for the clustering and give a computationally feasible and rate-optimal procedure. In Section 4, we provide a numerical comparison with other popular methods. Proofs are included in the supplement. ", "page_idx": 2}, {"type": "text", "text": "Notation.  For any matrix $X\\in\\mathbb{R}^{d\\times d}$ , we denote $\\lambda_{1}(X)$ as its smallest eigenvalue and $\\lambda_{d}(X)$ as its largest eigenvalue. In addition, we denote $\\|X\\|$ as its operator norm. For any two vectors $u,v$ of the same dimension, we denote $\\langle u,v\\rangle=u^{T}v$ as their inner product. For any positive integer $d$ ,we denote $I_{d}$ as the $d\\times d$ identity matrix. We denote $\\mathcal{N}(\\mu,\\Sigma)$ as the normal distribution with mean $\\mu$ and covariance matrix $\\Sigma$ .We denote $\\mathbb{I}\\left\\{\\cdot\\right\\}$ as the indicator function. For two positive sequences $\\left\\{a_{n}\\right\\}$ and $\\left\\{b_{n}\\right\\}$ \uff0c $a_{n}\\preceq b_{n}$ and $a_{n}=O(b_{n})$ both mean $a_{n}\\leq C b_{n}$ for some constant $C>0$ independent of $n$ We also write $a_{n}=o(b_{n})$ ${\\frac{b_{n}}{a_{n}}}\\,\\rightarrow\\infty$ when $\\begin{array}{r}{\\operatorname*{lim}\\operatorname*{sup}_{n}\\frac{a_{n}}{b_{n}}=0}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "2   GMM with Unknown but Homogeneous Covariance Matrices ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first consider the GMM where the covariance matrices of different clusters are unknown but are assumed to be equal to each other. Then the data-generating process can be displayed as follows: ", "page_idx": 2}, {"type": "text", "text": "Model 1: ", "page_idx": 2}, {"type": "equation", "text": "$$\nY_{j}=\\theta_{z_{j}^{*}}^{*}+\\epsilon_{j},{\\mathrm{~where~}}\\epsilon_{j}\\overset{i n d}{\\sim}\\mathcal{N}(0,\\Sigma^{*}),\\forall j\\in[n].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Throughout the paper, we call it Model $^{\\,l}$ for simplicity and to distinguish it from a different and more complicated one that will be introduced in Section 3. The goal is to recover the underlying cluster assignment vector $z^{*}$ .If $\\Sigma^{*}$ were known, then (1) can be converted into an isotropic GMM by a linear transformation $(\\Sigma^{*})^{-\\frac{1}{2}}Y_{j}$ . However, the unknown nature of $\\Sigma^{*}$ makes clustering under this model more challenging than under isotropic GMMs. ", "page_idx": 2}, {"type": "text", "text": "Signal-to-noise Ratio. Define the signal-to-noise ratio ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{SNR}=\\operatorname*{min}_{a,b\\in[k]:a\\neq b}\\|(\\theta_{a}^{*}-\\theta_{b}^{*})^{T}\\Sigma^{*-\\frac{1}{2}}\\|,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which is a function of all the centers $\\{\\theta_{a}^{*}\\}_{a\\in[k]}$ and the covariance matrix $\\Sigma^{*}$ .As wewill showlater in Theorem 2.1, SNR captures the difficulty of the clustering problem and determines the minimax rate. We defer the geometric interpretation of SNR until after presenting Theorem 2.2. ", "page_idx": 2}, {"type": "text", "text": "A quantity closely related to SNR is the minimum distance among the centers. Define $\\Delta$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Delta=\\operatorname*{min}_{a,b\\in[k]:a\\neq b}\\left\\|\\theta_{a}^{*}-\\theta_{b}^{*}\\right\\|.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Then we can see SNR and $\\Delta$ are of the same order if all eigenvalues of the covariance matrix $\\Sigma^{*}$ are assumed to be constants. If $\\Sigma^{*}$ is further assumed to be $\\sigma^{2}\\bar{I}_{d}$ , then SNR equals $\\Delta/\\sigma$ . As a result, in [15, 8, 14] where the isotropic GMMs are studied, $\\Delta/\\sigma$ plays the role of signal-to-noise ratio and appears in their rates. Since (2) represents a direct generalization, we refer to it as the signal-to-noise ratio for Model 1. ", "page_idx": 2}, {"type": "text", "text": "Loss Function.  To measure the clustering performance, we consider the following loss function. For any $z,z^{*}\\in[k]^{n}$ ,wedefine ", "page_idx": 2}, {"type": "equation", "text": "$$\nh(z,z^{*})=\\operatorname*{min}_{\\psi\\in\\Psi}\\frac{1}{n}\\sum_{j=1}^{n}\\mathbb{I}\\left\\{\\psi(z_{j})\\neq z_{j}^{*}\\right\\},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\Psi=\\{\\psi:\\psi$ is a bijection from $[k]$ to $[k]\\}$ . Here, the minimum is taken over all permutations of $[k]$ to address the identifiability issues of the labels $1,2,\\ldots,k$ . The loss function measures the ", "page_idx": 2}, {"type": "text", "text": "proportion of coordinates where $z$ and $z^{*}$ differ, modulo any permutation of label symbols. Thus, it is referred to as the misclustering error rate in this paper. Another loss that will be used is $\\ell(z,z^{*})$ defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\ell(z,z^{*})=\\sum_{j=1}^{n}\\left\\|\\theta_{z_{j}}^{*}-\\theta_{z_{j}^{*}}^{*}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "It measures the clustering performance of $z$ considering the distances among the true centers. It is related to $h(z,z^{*})$ as $h(\\bar{z},\\bar{z}^{*})\\leq\\ell(z,z^{*})/(n\\Delta^{2})$ and provides more information than $h(z,z^{*})$ .We will mainly use $\\ell(z,z^{*})$ in the technical analysis but will present results using $h(z,z^{*})$ which is more interpretable. ", "page_idx": 3}, {"type": "text", "text": "2.2 Minimax Lower Bound ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We first establish the minimax lower bound for the clustering problem under Model 1. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2.1. Under the assumption $\\frac{S N R}{\\sqrt{\\log k}}\\rightarrow\\infty$ wehave ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{z}}\\operatorname*{sup}_{z^{*}\\in[k]^{n}}\\mathbb{E}h(\\hat{z},z^{*})\\geq\\exp\\biggl(-(1+o(1))\\frac{S N R^{2}}{8}\\biggr)\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "f $S N R=O(1)$ instead, we have $\\begin{array}{r}{\\operatorname*{inf}_{\\hat{z}}\\operatorname*{sup}_{z^{*}\\in[k]^{n}}\\mathbb{E}h(\\hat{z},z^{*})\\geq c}\\end{array}$ for some constant $c>0$ ", "page_idx": 3}, {"type": "text", "text": "Theorem 2.1 allows the cluster numbers $k$ to grow with $n$ and shows that $\\mathrm{SNR}\\rightarrow\\infty$ is a necessary condition to have a consistent clustering. If $k$ is a constant, then $\\mathrm{SNR}\\,\\rightarrow\\,\\infty$ is also a sufficient condition. Theorem 2.1 holds for any arbitrary configurations of $\\{\\theta_{a}^{*}\\}_{a\\in[k]}$ and $\\Sigma^{*}$ , with the minimax lower bound depending on tese throughSNR.Theparameter spaceis onlyfor $z^{*}$ While $\\{\\theta_{a}^{*}\\}_{a\\in[k]}$ and $\\Sigma^{*}$ are held fixed. Hence, (6) can be interpreted as a case-specific result, precisely capturing the explicit dependence of the minimax rates on $\\{\\theta_{a}^{*}\\}_{a\\in[k]}$ and $\\Sigma^{*}$ ", "page_idx": 3}, {"type": "text", "text": "Theorem 2.1 is closely related to the LDA. If there are only two clusters with known centers and a covariance matrix, then estimating each $z_{j}^{*}$ becomes exactly the task of the LDA: we aim to determine from which of two normal distributions, each with a different mean but the same covariance matrix, the observation $Y_{j}$ is generated. In fact, this approach is also how Theorem 2.1 is proved: We first reduce the estimation problem of $z^{*}$ to two-point hypothesis testing for each individual $\\boldsymbol{z}_{j}^{*}$ . The error of these tests is analyzed in Lemma A.1 using the LDA, and we then aggregate all these testing errors together. ", "page_idx": 3}, {"type": "image", "img_path": "ge8GZn8Gtu/tmp/c04bab8869d48fabdc20de789eac0ebd60680ec25c1526b3c6805cdd9067bbe4.jpg", "img_caption": ["Figure 1: A geometric interpretation of SNR. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "With the help of Lemma A.1, we have a geometric interpretation of SNR. In the left panel of Figure 1, we have two normal distributions $\\mathcal{N}(\\theta_{1}^{*},\\bar{\\Sigma}^{*})$ and $\\bar{\\mathcal{N}}({\\theta_{2}^{*}},{\\bar{\\Sigma}^{*}})$ that $X$ follows. The black line represents the optimal testing procedure $\\phi$ displayed in Lemma A.1, dividing the space into two half-spaces. To calculate the testing error, we can make the transformation $X^{\\prime}=(\\Sigma^{\\ast})^{-\\frac{1}{2}}(X-\\theta_{1}^{\\ast})$ so that the two normal distributions become isotropic: $\\mathcal{N}(0,I_{d})$ and $\\mathcal{N}((\\Sigma^{*})^{-\\frac{1}{2}}(\\theta_{2}^{*}-\\theta_{1}^{*}),I_{d})$ as displayed in the right panel. Then the distance between the two centers is $\\lVert\\left(\\boldsymbol{\\Sigma}^{*}\\right)^{-\\frac{1}{2}}\\left(\\theta_{2}^{*}-\\theta_{1}^{*}\\right)\\rVert$ , and the distance from a center to the black curve is half of that. Then, the probability that $\\mathcal{N}(0,I_{d})$ falls within the grayed area equals $\\exp(-(1+o(1))\\|(\\Sigma^{*})^{-\\frac{1}{2}}(\\theta_{2}^{*}-\\theta_{1}^{*})\\|^{2}/8)$ , according to Gaussian tail probability. As a result, $\\lVert\\left(\\boldsymbol{\\Sigma}^{*}\\right)^{-\\frac{1}{2}}\\left(\\theta_{2}^{*}-\\theta_{1}^{*}\\right)\\rVert$ is the effective distance between the two centers of $\\mathcal{N}(\\theta_{1}^{*},\\Sigma^{*})$ and $\\mathcal{N}(\\theta_{2}^{*},\\Sigma^{*})$ for the clustering problem, taking into account the geometry of the covariance matrix. Since we have multiple clusters, SNR defined in (2) can be interpreted as the minimum effective distance among the centers $\\{\\theta_{a}^{*}\\}_{a\\in[k]}$ , considering the anisotropic structure of $\\Sigma^{*}$ . This measure captures the intrinsic difficulty of the clustering problem. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "2.3  Rate-Optimal Adaptive Procedure ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we give a computationally feasible and rate-optimal procedure for clustering under Model 1. Summarized in Algorithm 1, it is a variant of Lloyd's algorithm. Starting with an initial setup, it iteratively updates the estimates of the centers $\\{\\theta_{a}^{*}\\}_{a\\in[k]}$ (in (7), the covariance matrix $\\Sigma^{*}$ (in (8)), and the cluster assignment vector $z^{*}$ (in (9)). This algorithm differs from Lloyd's algorithm in that the latter is designed for isotropic GMMs and does not incorporate the covariance matrix update outlined in (8). Furthermore, (9) updates the estimation of $z_{j}^{*}$ using $;\\operatorname{argmin}_{a\\in[k]}(Y_{j}\\!-\\!\\theta_{a}^{(t)})^{T}(Y_{j}\\!-\\!\\theta_{a}^{(t)})$ instead. To differentiate clearly, we refer to the classic form as the vanilla Lloyd's algorithm and our modified version, which accommodates the unknown and anisotropic covariance matrix, as the adjusted Lloyd's algorithm. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 can also be interpreted as a hard EM algorithm. When applying Expectation Maximization (EM) to Model 1, the M step estimates the parameters $\\{\\theta_{a}^{*}\\}_{a\\in[k]}$ and $\\Sigma^{*}$ , while the E step estimates $z^{*}$ . It turns out the updates on the parameters (7) - (8) are identical to those in the EM's M step. However, the update of $z^{*}$ in Algorithm 1 differs from that in the EM. Instead of computing a conditional expectation typical of the E step, the algorithm performs maximization in (9). As a result, Algorithm 1 effectively consists solely of M steps for both parameters and $z^{*}$ , characterizing it as a hard EM algorithm. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1: Adjusted Lloyd's Algorithm for Model 1. ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Input: Data $Y$ , number of clusters $k$ , an initialization $z^{(0)}$ , number of iterations $T$ Output: z(T) ", "page_idx": 4}, {"type": "text", "text": "1 for $t=1,\\dots,T$ do   \n2 |  Update the centers: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\theta_{a}^{(t)}=\\frac{\\sum_{j\\in[n]}Y_{j}\\mathbb{I}\\left\\{z_{j}^{(t-1)}=a\\right\\}}{\\sum_{j\\in[n]}\\mathbb{I}\\left\\{z_{j}^{(t-1)}=a\\right\\}},\\quad\\forall a\\in[k].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Update the covariance matrix: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Sigma^{(t)}=\\frac{\\sum_{a\\in[k]}\\sum_{j\\in[n]}(Y_{j}-\\theta_{a}^{(t)})(Y_{j}-\\theta_{a}^{(t)})^{T}\\mathbb{I}\\left\\{z_{j}^{(t-1)}=a\\right\\}}{n}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Update the cluster assignment vector: ", "page_idx": 4}, {"type": "equation", "text": "$$\nz_{j}^{(t)}=\\underset{a\\in[k]}{\\mathrm{argmin}}(Y_{j}-\\theta_{a}^{(t)})^{T}(\\Sigma^{(t)})^{-1}(Y_{j}-\\theta_{a}^{(t)}),\\quad\\forall j\\in[n].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In Theorem 2.2, we give a computational and statistical guarantee of Algorithm 1. We show that starting from a decent initialization, within $\\log n$ iterations, Algorithm 1 achieves the error rate $\\exp\\bigl(-(1+o(1))\\mathrm{SNR}^{2}/8\\bigr)$ which matches the minimax lower bound given in Theorem 2.1. As a result, Algorithm 1 is a rate-optimal procedure. In addition, the algorithm is fully adaptive to the unknown $\\{\\theta_{a}^{*}\\}_{a\\in[k]}$ and $\\Sigma^{*}$ . The sole piece of information presumed to be known is $k$ , the number of clusters, as commonly assumed in clustering literature [15, 8, 14]. The theorem also shows that the number of iterations needed to achieve the optimal rate is at most $\\log n$ , providing implementation guidance to practitioners. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2.2. Assume $k\\,=\\,O(1)$ $d=O({\\sqrt{n}})$ and $\\begin{array}{r}{\\operatorname*{min}_{a\\in[k]}\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}\\,=\\,a\\}\\,\\ge\\,\\frac{\\alpha n}{k}}\\end{array}$ for some constant $\\alpha\\,>\\,0$ Assume $S N R\\,\\rightarrow\\,\\infty$ and $\\lambda_{d}(\\Sigma^{*})/\\lambda_{1}(\\Sigma^{*})\\,=\\,\\dot{O(1)}$ .For Algorithm $^{\\,l}$ .suppose $z^{(0)}$ satisfies $\\ell(z^{(0)},z^{*})\\,=\\,o(n)$ with probability at least $1\\textrm{--}\\eta$ Then with probability at least $1-\\eta-\\dot{n}^{-1}-\\mathrm{exp}(-\\dot{S}N\\!R)$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\nh(z^{(t)},z^{*})\\leq\\exp\\!\\left(-(1+o(1))\\frac{S N R^{2}}{8}\\right),\\ \\ \\,f o r\\,a l l\\,t\\geq\\log n.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We make the following remarks on the assumptions of Theorem 2.2: When $k$ is constant, the assumption that $\\mathrm{SNR}\\rightarrow\\infty$ is a necessary condition for consistent recovery of $z^{*}$ , as outlined in the minimax lower bound presented in Theorem 2.1. The assumption on $\\Sigma^{*}$ ensures that the covariance matrix is well-conditioned. The dimensionality $d$ is assumed to be $O({\\sqrt{n}})$ , a stronger assumption than in [15, 8, 14], where $d=O(n)$ is sufficient. This is because, unlike these studies, our work requires estimating the covariance matrix $\\Sigma^{*}$ and controlling the estimation error $\\lVert\\boldsymbol{\\Sigma}^{(t)}-\\boldsymbol{\\Sigma}^{*}\\rVert$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 2.2 needs a decent initialization $z^{(0)}$ in the sense that it is sufficiently close to the ground truth such that $\\ell(z^{(0)},z^{*})=o(n)$ . This is because our theoretical analysis requires the initialization being within a specific proximity to the true parameters. The requirement can be fulfilled by simple procedures. An example is the vanilla Lloyd's algorithm whose performance is studied in [15, 8]. Though [15, 8] are for isotropic GMMs, their results can be extended to sub-Gaussian mixture models with nearly identical proof. Since $\\epsilon_{j}$ are sub-Gaussian random variables with proxy variance $\\lambda_{d}(\\Sigma^{*})$ , [8] implies the vanilla Lloyd's algorithm output $\\hat{z}$ satisfies $\\ell(\\hat{z},z^{*})\\leq$ $n\\exp(-(1+o(1))\\Delta^{2}/(8\\lambda_{d}(\\Sigma^{*})))$ with probability at least $1-\\exp(-\\Delta/\\sqrt{\\lambda_{d}(\\Sigma^{*})})-n^{-1}$ , under the assumption that $\\Delta^{2}/(k^{2}(k d/n+1)\\lambda_{d}(\\Sigma^{*}))\\to\\infty$ . Then we have $\\ell(\\hat{z},z^{*})=o(n)$ with high probability under the assumptions of Theorem 2.2, and hence it can be used as an initialization for the algorithm. ", "page_idx": 5}, {"type": "text", "text": "3  GMM with Unknown and Heterogeneous Covariance Matrices ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "3.1 Model ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we study the GMM where the covariance matrices of each cluster are unknown and not necessarily equal to each other. The data-generation process can be displayed as follows, ", "page_idx": 5}, {"type": "text", "text": "Model 2: ", "page_idx": 5}, {"type": "equation", "text": "$$\nY_{j}=\\theta_{z_{j}^{*}}^{*}+\\epsilon_{j},\\mathrm{~where~}\\epsilon_{j}\\overset{i n d}{\\sim}\\mathcal{N}(0,\\Sigma_{z_{j}^{*}}^{*}),\\forall j\\in[n].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We refer to this as Model 2 throughout the paper to distinguish it from Model 1, as discussed in Section 2. The key difference between (10) and (1) is that here we have distinct covariance matrices $\\{\\Sigma_{a}^{*}\\}_{a\\in[k]}$ for each cluster, instead of a single shared $\\Sigma^{*}$ . We use the same loss function as defined in (4). ", "page_idx": 5}, {"type": "text", "text": "Signal-to-noise Ratio. The signal-to-noise ratio for Model 2 is defined as follows. We use the notation $\\mathrm{SNR^{\\prime}}$ to distinguish it from the SNR used for Model 1. Compared to SNR, $\\mathrm{SNR^{\\prime}}$ is much more complicated and does not have an explicit formula. We first define a set $B_{a,b}\\subset\\mathbb{R}^{d}$ forany $a,b\\in[k]$ such that $a\\neq b$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{a,b}=\\Bigg\\{x\\in\\mathbb{R}^{d}:x^{T}\\Sigma_{a}^{*\\frac{1}{2}}\\Sigma_{b}^{*-1}(\\theta_{a}^{*}-\\theta_{b}^{*})+\\frac{1}{2}x^{T}\\Big(\\Sigma_{a}^{*\\frac{1}{2}}\\Sigma_{b}^{*-1}\\Sigma_{a}^{*\\frac{1}{2}}-I_{d}\\Big)\\,x}\\\\ &{\\qquad\\qquad\\qquad\\leq-\\frac{1}{2}(\\theta_{a}^{*}-\\theta_{b}^{*})^{T}\\Sigma_{b}^{*-1}(\\theta_{a}^{*}-\\theta_{b}^{*})+\\frac{1}{2}\\log|\\Sigma_{a}^{*}|-\\frac{1}{2}\\log|\\Sigma_{b}^{*}|\\Bigg\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We then define $\\begin{array}{r}{\\mathrm{SNR}_{a,b}^{\\prime}=2\\operatorname*{min}_{x\\in B_{a,b}}\\left\\|x\\right\\|}\\end{array}$ and ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{SNR}^{\\prime}=\\operatorname*{min}_{a,b\\in[k]:a\\neq b}\\mathrm{SNR}_{a,b}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The form of $\\mathsf{S N R^{\\prime}}$ is closely connected to the testing error of the QDA, which we will give in Lemma 3.1. The interpretation of the $\\mathrm{SNR^{\\prime}}$ , particularly from a geometric perspective, will be deferred until after the presentation of Lemma 3.1. Here let us consider a few special cases where we are able to simplify $\\mathsf{S N R^{\\prime}}$ : (1) When $\\Sigma_{a}^{*}\\,=\\,\\Sigma^{*}$ for all $a\\in[k]$ , by simple algebra, we have $\\mathbf{SNR}_{a,b}^{\\prime}=\\|(\\theta_{a}^{*}-\\theta_{b}^{*})^{T}\\Sigma^{*-\\frac{1}{2}}\\|$ for any $a,b\\in[k]$ such that $a\\neq b$ . Hence, $\\mathrm{SNR^{\\prime}}=\\mathrm{SNR}$ and Model 2 effectively reduces to Model 1. (2) When $\\Sigma_{a}^{*}=\\sigma_{a}^{2}I_{d}$ for any $a\\in[k]$ where $\\sigma_{1},\\ldots,\\sigma_{k}>0$ are large constants, we have $\\mathrm{SNR}_{a,b}^{\\prime}$ \uff0c $\\mathrm{SNR}_{b,a}^{\\prime}$ both close to $2\\lVert{\\boldsymbol{\\theta}}_{a}^{*}-{\\boldsymbol{\\theta}}_{b}^{*}\\rVert/(\\sigma_{a}^{\\enspace}+\\sigma_{b})$ . From these examples, we can see $\\mathrm{SNR^{\\prime}}$ is determined by both the centers $\\{\\theta_{a}^{*}\\}_{a\\in[k]}$ and the covariance matrices $\\{\\Sigma_{a}^{*}\\}_{a\\in[k]}$ ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "3.2Minimax Lower Bound ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first establish the minimax lower bound for the clustering problem under Model 2. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.1. Assume $d=O(1)$ and $\\mathrm{max}_{a,b\\in[k]}\\,\\lambda_{d}(\\Sigma_{a}^{*})/\\lambda_{1}(\\Sigma_{b}^{*})=O(1)$ Under the assumption $\\frac{S N R^{\\prime}}{\\sqrt{\\log k}}\\rightarrow\\infty$ wehave ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{z}}\\operatorname*{sup}_{z^{*}\\in[k]^{n}}\\mathbb{E}h(\\hat{z},z^{*})\\geq\\exp\\left(-(1+o(1))\\frac{S N R^{'2}}{8}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "f $S N R^{\\prime}=O(1)$ instead, we have $\\begin{array}{r}{\\operatorname*{inf}_{\\hat{z}}\\operatorname*{sup}_{z^{*}\\in[k]^{n}}\\mathbb{E}h(\\hat{z},z^{*})\\geq c}\\end{array}$ for some constant $c>0$ ", "page_idx": 6}, {"type": "text", "text": "Although the statement of Theorem 3.1 appears similar to that of Theorem 2.1, the two minimax lower bounds differ due to the varying dependencies of the centers and covariance matrices on $\\mathrm{SNR^{\\prime}}$ versus SNR. Using the same argument as in Section 2.2, the minimax lower bound established in Theorem 3.1 closely relates to the QDA between two normal distributions with different means and different covariance matrices. ", "page_idx": 6}, {"type": "text", "text": "Lemma 3.1 (Testing Error for the QDA). Consider two hypotheses $\\mathbb{H}_{0}:X\\sim\\mathcal{N}(\\theta_{1}^{*},\\Sigma_{1}^{*})$ and $\\mathbb{H}_{1}:X\\sim{\\mathcal{N}}(\\theta_{2}^{*},\\Sigma_{2}^{*})$ . Define a testing procedure ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\phi=\\mathbb{I}\\left\\{\\log|\\Sigma_{1}^{*}|+(x-\\theta_{1}^{*})^{T}(\\Sigma_{1}^{*})^{-1}(x-\\theta_{1}^{*})\\geq\\log|\\Sigma_{2}^{*}|+(x-\\theta_{2}^{*})^{T}(\\Sigma_{2}^{*})^{-1}(x-\\theta_{2}^{*})\\right\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then we have inf $\\cdot_{\\hat{\\phi}}({\\mathbb{P}}_{{\\mathbb{H}}_{0}}(\\hat{\\phi}=1)+{\\mathbb{P}}_{{\\mathbb{H}}_{1}}(\\hat{\\phi}=0))={\\mathbb{P}}_{{\\mathbb{H}}_{0}}(\\phi=1)+{\\mathbb{P}}_{{\\mathbb{H}}_{1}}(\\phi=0)$ Assume $d=O(1)$ and $\\operatorname*{max}_{a,b\\in\\{1,2\\}}$ $_{a,b\\in\\{1,2\\}}\\lambda_{d}(\\Sigma_{a}^{*})/\\lambda_{1}(\\Sigma_{b}^{*})=O(1)$ $H r\\operatorname*{min}\\left\\{S N R_{1,2}^{\\prime},S N R_{2,1}^{\\prime}\\right\\}\\rightarrow\\infty_{*}$ wehave ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{\\phi}}(\\mathbb{P}_{\\mathbb{H}_{0}}(\\hat{\\phi}=1)+\\mathbb{P}_{\\mathbb{H}_{1}}(\\hat{\\phi}=0))\\ge\\exp\\left(-(1+o(1))\\frac{\\operatorname*{min}\\left\\{S N R_{1,2}^{\\prime},S N R_{2,1}^{\\prime}\\right\\}^{2}}{8}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Otherwise, $\\operatorname*{inf}_{\\hat{\\phi}}(\\mathbb{P}_{\\mathbb{H}_{0}}(\\hat{\\phi}=1)+\\mathbb{P}_{\\mathbb{H}_{1}}(\\hat{\\phi}=0))\\ge c$ for some constant $c>0$ ", "page_idx": 6}, {"type": "image", "img_path": "ge8GZn8Gtu/tmp/6117702857eab8c867a2c2ff70c0a644d17d9d949a502e59a1da4c768093f4c7.jpg", "img_caption": ["Figure 2: A geometric interpretation of $\\mathbf{SNR^{\\prime}}$ "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Lemma 3.1 provides a geometric interpretation of $\\mathrm{SNR^{\\prime}}$ . In the left panel of Figure 2, we have two normal distributions $\\mathcal{N}(\\theta_{1}^{*},\\Sigma_{1}^{*})$ and $\\bar{\\cal N}(\\theta_{2}^{*},\\Sigma_{2}^{*})$ from which $X$ can be generated, and the black curve represents the optimal testing procedure $\\phi$ , as detailed in Lemma 3.1. Since $\\Sigma_{1}^{*}$ is not necessarily equal to $\\Sigma_{2}^{*}$ , the black curve is not necessarily a straight line. If $\\mathbb{H}_{0}$ is true, the probability that $X$ is incorrectly classified occurs when $X$ falls into the gray area, represented by $\\mathbb{P}_{\\mathbb{H}_{0}}(\\phi=1)$ . To calculate this, we transform $X$ to $X^{\\prime}=(\\Sigma_{1}^{*})^{-\\frac{1}{2}}(X\\!-\\!\\theta_{1}^{*})$ , standardizing the first distribution. Then, as displayed in the right panel of Figure 2, the two distributions become $\\mathcal{N}(0,I_{d})$ and $\\mathcal{N}((\\Sigma_{1}^{*})^{-\\frac{1}{2}}(\\theta_{2}^{*}-$ $\\theta_{1}^{*})$ \uff0c\uff0c $\\!\\,(\\bar{\\Sigma_{1}^{*}})^{-\\frac{1}{2}}\\Sigma_{2}^{*}(\\bar{\\Sigma_{1}^{*}})^{-\\frac{1}{2}})$ , and the optimal testing procedure $\\phi$ becomes $\\mathbb{I}\\left\\{X^{\\prime}\\in B_{1,2}\\right\\}$ . As a result, in the right panel of Figure 2, $B_{1,2}$ represents the space colored by gray, and the black curve is its boundary. Then $\\mathbb{P}_{\\mathbb{H}_{0}}\\bar{(\\phi}=1)$ is equal to $\\mathbb{P}(\\mathcal{N}(0,\\bar{I}_{d})\\in B_{1,2})$ . Under the assumption $d=O(1)$ and maxa,be(1,2) $\\lambda_{d}(\\Sigma_{a}^{*})/\\lambda_{1}(\\Sigma_{b}^{*})={\\cal O}(1)$ , in Lemma C.10, we can show $\\mathbb{P}(\\mathcal{N}(0,I_{d})\\in B_{1,2})\\stackrel{\\cdot\\cdot}{=}$ $\\exp(-(1+o(1))\\mathrm{SNR_{1,2}^{'2}/8})$ . As a result, $\\mathrm{SNR^{\\prime}}$ can be interpreted as the minimum effective distance among the centers $\\{\\theta_{a}^{*}\\}_{a\\in[k]}$ . considering the anisotropic and heterogeneous structure of $\\{\\Sigma_{a}^{*}\\}_{a\\in[k]}$ and it captures the intrinsic difficulty of the clustering problem under Model 2. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "3.3 Optimal Adaptive Procedure ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we give a computationally feasible and rate-optimal procedure for clustering under Model 2. Similar to Algorithm 1, Algorithm 2 is a variant of Lloyd's algorithm, adjusted to accommodate unknown and heterogeneous covariance matrices. It can also be interpreted as a hard EM algorithm under Model 2. Algorithm 2 differs from Algorithm 1 in (13) and (14), as now there are $k$ covariance matrices instead of a common one. ", "page_idx": 7}, {"type": "text", "text": "Algorithm 2: Adjusted Lloyd's Algorithm for Model 2. ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Input: Data $Y$ , number of clusters $k$ , an initialization $z^{(0)}$ , number of iterations $T$ Output: $z^{(T)}$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\theta_{a}^{(t)}=\\frac{\\sum_{j\\in[n]}Y_{j}\\mathbb{I}\\left\\{z_{j}^{(t-1)}=a\\right\\}}{\\sum_{j\\in[n]}\\mathbb{I}\\left\\{z_{j}^{(t-1)}=a\\right\\}},\\quad\\forall a\\in[k].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Update the covariance matrices: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\Sigma_{a}^{(t)}=\\frac{\\sum_{j\\in[n]}(Y_{j}-\\theta_{a}^{(t)})(Y_{j}-\\theta_{a}^{(t)})^{T}\\mathbb{I}\\left\\{z_{j}^{(t-1)}=a\\right\\}}{\\sum_{j\\in[n]}\\mathbb{I}\\left\\{z_{j}^{(t-1)}=a\\right\\}},\\quad\\forall a\\in[k].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Update the cluster assignment vector: ", "page_idx": 7}, {"type": "equation", "text": "$$\nz_{j}^{(t)}=\\underset{a\\in[k]}{\\mathrm{argmin}}(Y_{j}-\\theta_{a}^{(t)})^{T}(\\Sigma_{a}^{(t)})^{-1}(Y_{j}-\\theta_{a}^{(t)})+\\log|\\Sigma_{a}^{(t)}|,\\quad\\forall j\\in[n].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In Theorem 3.2, we give a computational and statistical guarantee for Algorithm 2. We demonstrate that, with proper initialization, Algorithm 2 achieves the minimax lower bound within $\\log n$ iterations. The assumptions needed in Theorem 3.2 are similar to those in Theorem 2.2, except that we require stronger assumptions on the dimensionality $d$ since now we have $k$ (instead of one) covariance matrices to be estimated. In addition, by assuming $\\mathrm{max}_{a,b\\in[k]}\\,\\lambda_{d}(\\Sigma_{a}^{*})/\\lambda_{1}(\\Sigma_{b}^{*})=O(1)$ , we ensure not only that each of the $k$ covariance matrices is well-conditioned but also that they are comparable to one another. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3.2. Assume $k\\,=\\,O(1)$ \uff0c $d\\,=\\,O(1)$ ,and $\\begin{array}{r}{\\operatorname*{min}_{a\\in[k]}\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}\\,=\\,a\\}\\,\\ge\\,\\frac{\\alpha n}{k}}\\end{array}$ for some constant $\\alpha\\,>\\,0$ .Assume $S N R^{\\prime}\\rightarrow\\infty$ and $\\mathrm{max}_{a,b\\in[k]}\\,\\lambda_{d}(\\Sigma_{a}^{*})/\\lambda_{1}(\\Sigma_{b}^{*})=O(1)$ . For Algorithm 2, suppose $z^{(0)}$ satisfies $\\ell(z^{(0)},z^{*})=o(n)$ With probability at least $1-\\eta$ Then with probability at least $1-\\eta-5n^{-1}-\\exp(-S N\\!R^{\\prime})$ , we have ", "page_idx": 7}, {"type": "equation", "text": "$$\nh(z^{(t)},z^{*})\\leq\\exp\\left(-(1+o(1))\\frac{{\\cal S}\\!N{\\cal R}^{^{\\prime}2}}{8}\\right),\\ \\ \\,f o r\\,a l l\\,t\\geq\\log n.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The vanilla Lloyd's algorithm can be used as the initialization for Algorithm 2. This is because Model 2 is also a sub-Gaussian mixture model. By the same argument as in Section 2.3, the output of the vanilla Lloyd's algorithm $\\hat{z}$ satisfies $\\ell(\\hat{z},z^{*})=o(n)$ with high probability under the assumptions of Theorem 3.2. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We conclude this section with a time complexity analysis of Algorithm 2. Compared to the vanilla Lloyd's algorithm, our method introduces additional computational overhead due to the need for computing the inverse and determinant of covariance matrices. Specifically, the time complexity of Algorithm 2 is $O(n k d^{3}T)$ . In contrast, the vanilla Lloyd's algorithm has a lower time complexity of $O(n k d T)$ . The increase in complexity stems from matrix operations in $d$ dimensions, as both matrix inversion and determinant computation scale as $O(d^{3})$ ", "page_idx": 8}, {"type": "text", "text": "4    Numerical Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we compare the performance of our methods with other popular clustering methods on synthetic and real datasets under different settings. ", "page_idx": 8}, {"type": "text", "text": "Model 1.  The first simulation is designed for the GMM with unknown but homogeneous covariance matrices (i.e., Model 1). We independently generate $n=1200$ samples with dimension $d=50$ from $k=30$ clusters. Each cluster has 40 samples. We set $\\Sigma^{*}=U^{T}\\Lambda\\dot{U}$ , where $\\Lambda$ is a $50\\times50$ diagonal matrix with diagonal elements selected from 0.5 to 8 with equal space and $U$ is a randomly generated orthogonal matrix. The centers $\\{\\theta_{a}^{*}\\}_{a\\in[n]}$ are orthogonal to each other with $\\|\\theta_{1}^{*}\\|=...=\\|\\theta_{30}^{*}\\|=9$ We consider four popular clustering methods: (1) the spectral clustering method in [14] (denoted as \"spectral\"), (2) the vanilla Lloyd's algorithm in [15] (denoted as \u201cvanilla Lloyd\"), (3) Algorithm 1 initialized by the spectral clustering (denoted as \u201cspectral $.+\\,\\mathrm{Alg}\\ 1^{\\circ},$ 0, and (4) Algorithm 1 initialized by the vanilla Lloyd (denoted as \u201cvanilla Lloyd $+\\,{\\mathrm{Alg~}}1^{\\circ}.$ 0. The comparison is presented in the left panel of Figure 3. ", "page_idx": 8}, {"type": "text", "text": "Model 2. We also compare the performances of four methods (spectral, vanilla Lloyd, spectral $^+$ Alg 2, and vanilla Lloy $\\mathrm{1+Alg\\}2.$ 0 for the GMM with unknown and heterogeneous covariance matrices (i.e., Model 2). In this case, we take $n=1200$ $k=2$ , and $d=9$ .We set $\\Sigma_{1}^{*}=I_{d}$ and $\\Sigma_{2}^{*}=\\Lambda_{2}$ a diagonal matrix where the first diagonal entry is 0.5 and the remaining entries are 5. We set the cluster sizes to be 900 and 300, respectively. To simplify the calculation of $\\mathrm{SNR^{\\prime}}$ , we set $\\theta_{1}^{*}=0$ and $\\theta_{2}^{*}=5e_{1}$ , with $e_{1}$ being the vector that has a 1 in its first entry and Os elsewhere. The comparison is presented in the right panel of Figure 3. ", "page_idx": 8}, {"type": "image", "img_path": "ge8GZn8Gtu/tmp/05fe9e9e176b6064100d8377cbbc3128677dc003fafc7556b5f222791be448b9.jpg", "img_caption": ["Figure 3: Left: Performance of Algorithm 1 compared with other methods under Model 1. Right: Performance of Algorithm 2 compared with other methods under Model 2. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "In Figure 3, the $x$ -axis is the number of iterations and the $y$ -axis is the logarithm of the misclustering error rate, i.e., $\\log(h)$ . Each of the curves plotted is an average of 100 independent trials. We can see both Algorithm 1 and Algorithm 2 outperform the spectral clustering and the vanilla Lloyd's algorithm significantly. Additionally, the dashed lines in the left and right panels represent the optimal exponents $-\\dot{\\mathrm{S}}\\mathrm{NR}^{2}/8$ and $-\\bar{\\bf{S}}\\mathrm{{NR}}^{\\prime2}/8$ of the minimax bounds, respectively. It is observed that both Algorithm 1 and Algorithm 2 meet these benchmarks after three iterations. This justifies the conclusion that both algorithms are rate-optimal. ", "page_idx": 8}, {"type": "text", "text": "Real Data. To further demonstrate the effectiveness of our methods, we conduct experiments using the Fashion-MNIST dataset [23]. In the first analysis, we use a total of $12{,}000\\ 2\\bar{8}{\\times}28$ grayscale images, consisting of 6,0o0 images each from the T-shirt/top class and the Trouser class. The left panel of Figure 4 gives a visualization of the data points using their first two principal components, showing the anisotropic and heterogeneous covariance structures. Since a large number of pixels have zero across most images, we apply PCA to reduce dimensionality from 784 to 50 by retaining the top 50 principal components. Our Algorithm 2 achieves a misclustering error of $5.71\\%$ , outperforming the vanilla Lloyd's algorithm, which has an error of $8.24\\%$ . In the second analysis, we incorporate an additional class, the Ankle boot class, increasing the total to 18,0o0 images across three classes. Following the same preprocessing steps, the visualization of the dataset's structure in the right panel of Figure 4 again confirms the presence of anisotropic and heterogeneous covariances. Here, Algorithm 2 achieves an error of $3.97\\%$ , an improvement over the $5.64\\%$ error rate observed with the vanilla Lloyd's algorithm. ", "page_idx": 9}, {"type": "image", "img_path": "ge8GZn8Gtu/tmp/20feb8057f72bdc02ef27d2810659ddc6edd58161b6fad0a125a978c95fa449f.jpg", "img_caption": ["Figure 4: Visualization of the Fashion-MNIST dataset using the first two principal components. The data points are color-coded to indicate class membership: Red represents the T-shirt/top class, green denotes the Trouser class, and blue signifies the Ankle boot class. This illustration shows the existence of anisotropic and heterogeneous covariance structures. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper focuses on clustering methods and theory for GMMs, with anisotropic covariance structures, presenting new minimax bounds and an adjusted Lloyd's algorithm tailored for varying covariance structures. Our theoretical and empirical analyses demonstrate the algorithm's ability to achieve optimality within a logarithmic number of iterations. Despite these advances, our results have some limitations that are worth addressing in future work: ", "page_idx": 9}, {"type": "text", "text": "1. High-Dimensional Settings: Current results are restricted to dimensions $d$ growing at a rate slower than $n$ , specifically ${\\hat{d}}=O({\\sqrt{n}})$ as stated in Theorem 2.2. Section 3 further requires a stronger assumption $d\\,=\\,O(1)$ . These constraints stem from technical challenges in estimating covariance matrices accurately and in controlling matrix determinant. Adopting more sophisticated analytical tools could potentially relax these bounds to $d=O(n)$ . In scenarios where $d$ exceeds $n$ , the misclustering error deviates from the simpler exponential decay observed under isotropic GMMs, as shown in [16]. This suggests that our model might also exhibit similar complexities, warranting further exploration into the technique used in [16] for potential extensions. ", "page_idx": 9}, {"type": "text", "text": "2. Ill-Conditioned Covariance Structures: Our analysis relies on the assumption of wellconditioned covariance matrices, where $\\mathrm{max}_{a,b\\in[k]}\\;\\lambda_{d}(\\Sigma_{a}^{*})/\\lambda_{1}(\\Sigma_{b}^{*})\\,=\\,O(1)$ .This condition is crucial for the current analytical framework, as it helps manage the estimation errors of covariance matrices and their inverses. While more advanced techniques may allow for a relaxation of this assumption, handling ill-conditioned or degenerate covariance matrices remains challenging, particularly due to the difficulty of working with matrix inverses in such cases. While minimax lower bounds suggest that clustering is still possible even when the covariance matrix is degenerate, it raises computational challenges for our current algorithms. This highlights the need for developing new algorithms that can function effectively under less restrictive conditions. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Emmanuel Abbe, Jianqing Fan, and Kaizheng Wang. An $\\ell_{p}$ theory of PCA and spectral clustering. The Annals of Statistics, 50(4):2359-2385, 2022.   \n[2]  Christopher M Bishop. Pattern recognition and machine learning. springer, 2006.   \n[3] S Charles Brubaker and Santosh S Vempala. Isotropic PCA and affne-invariant clustering. In Building Bridges, pages 241-281. Springer, 2008.   \n[4]  Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society: Series B (Methodological), 39(1):1-22, 1977.   \n[5]  Yingjie Fei and Yudong Chen. Hidden integrality of SDP relaxations for sub-Gaussian mixture models. In Conference On Learning Theory, pages 1931-1965. PMLR, 2018.   \n[6] Ronald A Fisher. The use of multiple measurements in taxonomic problems. Annals of eugenics, 7(2):179-188, 1936.   \n[7] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning, volume 1. Springer series in statistics New York, 2001.   \n[8]  Chao Gao and Anderson Y Zhang. Iterative algorithm for discrete structure recovery. The Annals of Statistics, 50(2):1066-1094, 2022.   \n[9]  Christophe Giraud and Nicolas Verzelen. Partial recovery bounds for clustering with the relaxed $k$ -means. Mathematical Statistics and Learning, 1(3):317-374, 2019.   \n[10]  Daniel Hsu, Sham Kakade, Tong Zhang, et al. A tail inequality for quadratic forms of subgaussian random vectors. Electronic Communications in Probability, 17, 2012.   \n[11]  Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Efficiently learning mixtures of two GaussanIProceedins of theforyseond MsyposuonThoryof compuingag 553-562, 2010.   \n[12]  Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection. The Annals of Statistics, pages 1302-1338, 2000.   \n[13] Stuart Lloyd. Least squares quantization in PCM. IEEE transactions on information theory, 28(2):129-137, 1982.   \n[14]  Mathias Loffer, Anderson Y Zhang, and Harrson H Zhou. Optimality of spectral clustering in the Gaussian mixture model. The Annals of Statistics, 49(5):2506-2530, 2021.   \n[15]  Yu Lu and Harrison H Zhou. Statistical and computational guarantees of Lloyd's algorithm and its variants. arXiv preprint arXiv: 1612.02099, 2016.   \n[16]  Mohamed Ndaoud. Sharp optimal recovery in the two component Gaussian mixture model. The Annals of Statistics, 50(4):2096-2126, 2022.   \n[17]  Karl Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions of the Royal Society of London. A, 185:71-110, 1894.   \n[18] Daniel A Spielman and Shang-Hua Teng. Spectral partitioning works: Planar graphs and finite element meshes. In Proceedings of 37th Conference on Foundations of Computer Science, pages 96-105. IEEE, 1996.   \n[19] D Michael Titterington, Adrian FM Smith, and Udi E Makov. Statistical analysis of finite mixture distributions. Wiley, 1985.   \n[20] S. Vempala and G. Wang. A spectral algorithm for learning mixture models. J. Comput. Syst. Sci., 68(4):841-860, 2004.   \n[21] Urike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395-416, 2007.   \n[22] Kaizheng Wang, Yuling Yan, and Mateo Diaz. Efficient clustering for stretched mixtures: Landscape and optimality. Advances in Neural Information Processing Systems, 33:21309- 21320,2020.   \n[23] Han Xiao, Kashif Rasul, and Roland Vollgraf._ Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.   \n[24]  Anderson Y Zhang and Harrison H Zhou. Leave-one-out singular subspace perturbation analysis for spectral clustering. arXiv preprint arXiv:2205.14855, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "The appendices are organized as follows. Appendix A is dedicated to proving the results in Section 2. To be more specific, we prove the lower bound, Theorem 2.1, in Appendix A.1 and the upper bound, Theorem 2.2, in Appendix A.2. For the upper bound proof, we first give a high-level idea in Appendix A.2.1, followed by a detailed proof in Appendix A.2.2. Appendix B includes proofs of the results in Section 3: the proof of the lower bound, Theorem 3.1, is in Appendix B.1, and the proof of the upper bound, Theorem 3.2, is in Appendix B.2. We include all technical lemmas and their proofs in Appendix C. ", "page_idx": 12}, {"type": "text", "text": "A Proofs in Section 2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Proofs for the Lower Bound ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In the following lemma, we give a sharp and explicit formula for the testing error of the LDA. Here we have two normal distributions $\\bar{\\cal N}(\\theta_{1}^{*},\\Sigma^{*})$ and $\\mathcal{N}(\\theta_{2}^{*},\\Sigma^{*})$ and an observation $X$ that is generated from one of them. We are interested in estimating from which distribution the observation is drawn. By the Neyman-Pearson lemma, it is known that the likelihood ratio test $\\begin{array}{r}{\\mathbb{I}\\left\\{2(\\theta_{2}^{*}-\\theta_{1}^{*})^{T}(\\Sigma^{*})^{-1}\\bar{X}\\geq\\theta_{2}^{*T}\\tilde{(}\\Sigma^{*})^{-1}\\theta_{2}^{*}-\\theta_{1}^{*T}(\\Sigma^{*})^{-1}\\theta_{1}^{*}\\right\\}}\\end{array}$ is the optimal testing procedure. Then by using the Gaussian tail probability, we are able to obtain the optimal testing error, with its lower bound given in Lemma A.1. ", "page_idx": 12}, {"type": "text", "text": "Lemma A.1 (Testing Error for the LDA). Consider two hypotheses $\\mathbb{H}_{0}:X\\sim\\mathcal{N}(\\theta_{1}^{*},\\Sigma^{*})$ and $\\mathbb{H}_{1}:X\\sim{\\mathcal{N}}(\\theta_{2}^{*},\\Sigma^{*})$ Define a testing procedure ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad\\phi=\\mathbb{I}\\left\\{2(\\theta_{2}^{*}-\\theta_{1}^{*})^{T}(\\Sigma^{*})^{-1}X\\geq\\theta_{2}^{*T}(\\Sigma^{*})^{-1}\\theta_{2}^{*}-\\theta_{1}^{*T}(\\Sigma^{*})^{-1}\\theta_{1}^{*}\\right\\}.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\operatorname*{inf}_{\\hat{\\phi}}(\\mathbb{P}_{\\mathbb{H}_{0}}(\\hat{\\phi}=1)+\\mathbb{P}_{\\mathbb{H}_{1}}(\\hat{\\phi}=0))=\\mathbb{P}_{\\mathbb{H}_{0}}(\\phi=1)+\\mathbb{P}_{\\mathbb{H}_{1}}(\\phi=0)\\cdot J f\\lVert(\\theta_{2}^{*}-\\theta_{1}^{*})^{T}(\\Sigma^{*})^{-\\frac{1}{2}}\\rVert\\rightarrow\\infty,}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Then we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{\\phi}}(\\mathbb{P}_{\\mathbb{H}_{0}}(\\hat{\\phi}=1)+\\mathbb{P}_{\\mathbb{H}_{1}}(\\hat{\\phi}=0))\\ge\\exp\\left(-(1+o(1))\\frac{\\|(\\theta_{2}^{*}-\\theta_{1}^{*})^{T}(\\Sigma^{*})^{-\\frac{1}{2}}\\|^{2}}{8}\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Otherwise, $\\operatorname*{inf}_{\\hat{\\phi}}(\\mathbb{P}_{\\mathbb{H}_{0}}(\\hat{\\phi}=1)+\\mathbb{P}_{\\mathbb{H}_{1}}(\\hat{\\phi}=0))\\geq c$ for some constant $c>0$ ", "page_idx": 12}, {"type": "text", "text": "Proof. Note that $\\phi$ is the likelihood ratio test. By the Neyman-Pearson lemma, it is the optimal procedure. That is, $\\begin{array}{r}{\\operatorname*{inf}_{\\hat{\\phi}}(\\mathbb{P}_{\\mathbb{H}_{0}}(\\hat{\\phi}=1)+\\mathbb{P}_{\\mathbb{H}_{1}}(\\hat{\\phi}=0))=\\mathbb{P}_{\\mathbb{H}_{0}}(\\phi=1)+\\mathbb{P}_{\\mathbb{H}_{1}}(\\phi=0)\\,.\\,\\mathrm{L}\\alpha}\\end{array}$ $\\epsilon\\sim\\mathcal{N}(0,I_{d})$ By Gaussian tail probability, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{\\mathbb{P}}_{\\mathrm{H}_{0}}(\\phi=1)+{\\mathbb{P}}_{\\mathrm{H}_{1}}(\\phi=0)={\\mathbb{P}}\\big(2(\\theta_{2}^{*}-\\theta_{1}^{*})^{T}(\\Sigma^{*})^{-1}(\\theta_{1}^{*}+\\epsilon)\\geq\\theta_{2}^{*T}(\\Sigma^{*})^{-1}\\theta_{2}^{*}-\\theta_{1}^{*T}(\\Sigma^{*})^{-1}\\theta_{1}^{*}\\big)}\\\\ {\\quad+{\\mathbb{P}}\\big(2(\\theta_{2}^{*}-\\theta_{1}^{*})^{T}(\\Sigma^{*})^{-1}(\\theta_{2}^{*}+\\epsilon)<\\theta_{2}^{*T}(\\Sigma^{*})^{-1}\\theta_{2}^{*}-\\theta_{1}^{*T}(\\Sigma^{*})^{-1}\\theta_{1}^{*}\\big)}\\\\ {=2{\\mathbb{P}}\\big(2(\\theta_{2}^{*}-\\theta_{1}^{*})^{T}(\\Sigma^{*})^{-1}(\\theta_{1}^{*}+\\epsilon)\\geq\\theta_{2}^{*T}(\\Sigma^{*})^{-1}\\theta_{2}^{*}-\\theta_{1}^{*T}(\\Sigma^{*})^{-1}\\theta_{1}^{*}\\big)}\\\\ {=2{\\mathbb{P}}\\bigg(\\epsilon>\\frac{1}{2}\\|(\\theta_{2}^{*}-\\theta_{1}^{*})^{T}(\\Sigma^{*})^{-\\frac{1}{2}}\\|\\bigg)}\\\\ {\\geq C\\operatorname*{min}\\Bigg\\{1,\\frac{1}{\\|(\\theta_{2}^{*}-\\theta_{1}^{*})^{T}(\\Sigma^{*})^{-\\frac{1}{2}}\\|}\\exp\\left(-\\frac{\\|(\\theta_{2}^{*}-\\theta_{1}^{*})^{T}(\\Sigma^{*})^{-\\frac{1}{2}}\\|^{2}}{8}\\right)\\Bigg\\}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "for some constant $C>0$ . The proof is complete. ", "page_idx": 12}, {"type": "text", "text": "Proof of Theorem 2.1. We adopt the idea from [15]. Without loss of generality, assume the minimum in (2) is achieved at $a=1,b=2$ so that $\\mathrm{SNR}=(\\bar{\\theta}_{1}^{*}-\\theta_{2}^{*})^{T}(\\Sigma^{*})^{-1}\\bar{(}\\theta_{1}^{*}-\\theta_{2}^{*}\\bar{)}$ . Consider an arbitrary $\\bar{z}\\in[k]^{n}$ such that $\\begin{array}{r}{|\\{i\\in[n]:\\bar{z}_{i}=a\\}|\\geq\\lceil\\frac{n}{k}-\\frac{\\dot{n}}{8k^{2}}\\rceil}\\end{array}$ for any $a\\in[k]$ . Then for each $a\\in[k]$ , we can choose a subset of $\\{i\\in[n]:\\bar{z}_{i}=a\\}$ with cardinality $\\textstyle\\left\\lceil{\\frac{n}{k}}\\right\\rceil-{\\frac{n}{8k^{2}}}\\rceil$ , denoted by $T_{a}$ . Let $T=\\cup_{a\\in[k]}T_{a}$ Then we can define a parameter space ", "page_idx": 12}, {"type": "text", "text": "Notice that for any $z\\neq\\tilde{z}\\in\\mathcal{Z}$ , we have $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}{\\mathbb{I}\\{z_{i}\\neq\\tilde{z}_{i}\\}}\\leq\\frac{k}{n}\\frac{n}{8k^{2}}=\\frac{1}{8k}}\\end{array}$ $\\textstyle{\\frac{1}{n}}\\sum_{i=1}^{n}\\mathbb{I}\\{\\psi(z_{i})\\neq$ $\\begin{array}{r}{\\tilde{z}_{i}\\}\\geq\\frac{1}{n}\\big(\\frac{n}{2k}-\\frac{n}{8k^{2}}\\big)\\geq\\frac{1}{4k}}\\end{array}$ for any permutation $\\psi$ on $[k]$ . Thus we can conclude ", "page_idx": 12}, {"type": "equation", "text": "$$\nh(z,\\tilde{z})=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{I}\\{z_{i}\\neq\\tilde{z}_{i}\\},\\quad\\mathrm{for\\,all}\\;z,\\tilde{z}\\in\\mathcal{Z}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We notice that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\hat{z}}{\\operatorname*{inf}}\\ \\underset{z^{*}\\in[k]^{n}}{\\operatorname*{sup}}\\mathbb{E}h(\\hat{z},z^{*})\\geq\\ \\underset{\\hat{z}}{\\operatorname*{inf}}\\ \\underset{z^{*}\\in\\mathcal{Z}}{\\operatorname*{sup}}\\ \\mathbb{E}h(\\hat{z},z^{*})}\\\\ {\\geq\\ \\underset{\\hat{z}}{\\operatorname*{inf}}\\ \\frac{1}{|\\mathcal{Z}|}\\sum_{z^{*}\\in\\mathcal{Z}}\\mathbb{E}h(\\hat{z},z^{*})}\\\\ {\\geq\\ \\frac{1}{n}\\sum_{i\\in T^{c}}\\underset{\\hat{z}_{i}}{\\operatorname*{inf}}\\ \\frac{1}{|\\mathcal{Z}|}\\sum_{z^{*}\\in\\mathcal{Z}}\\mathbb{P}_{z^{*}}(\\hat{z}_{i}\\neq z_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Now consider a fixed $i\\in T^{c}$ . Define $\\mathcal{Z}_{a}\\,=\\,\\{z\\,\\in\\,\\mathcal{Z}\\,:\\,z_{i}\\,=\\,a\\}$ for $a\\,=\\,1,2$ .Then we can see $\\mathcal{Z}=\\mathcal{Z}_{1}\\cup\\mathcal{Z}_{2}$ and $\\mathcal{Z}_{1}\\cap\\mathcal{Z}_{2}=\\emptyset$ What is more, there exists a one-to-one mapping $f(\\cdot)$ between ${\\mathcal{Z}}_{1}$ and ${\\mathcal{Z}}_{2}$ , such that for any $z\\in\\mathcal{Z}_{1}$ , we have $f(z)\\in{\\mathcal{Z}}_{2}$ with $[f(z)]_{j}\\,=\\,z_{j}$ for any $j\\neq i$ and $[f(z)]_{i}=2$ . Hence, we can reduce the problem to a two-point testing probe and then apply Lemma A.1. We first consider the case that $\\mathrm{SNR}\\rightarrow\\infty$ . We have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\Tilde{z}_{i}}{\\operatorname*{inf}}\\ \\frac{1}{|\\mathcal{Z}|}\\sum_{z^{*}\\in\\mathcal{Z}}\\mathbb{P}_{z^{*}}(\\hat{z}_{i}\\neq z_{i})=\\underset{\\Tilde{z}_{i}}{\\operatorname*{inf}}\\ \\frac{1}{|\\mathcal{Z}|}\\sum_{z^{*}\\in\\mathcal{Z}_{1}}\\left(\\mathbb{P}_{z^{*}}(\\hat{z}_{i}\\neq1)+\\mathbb{P}_{f(z^{*})}(\\hat{z}_{i}\\neq2)\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\geq\\frac{1}{|\\mathcal{Z}|}\\sum_{z^{*}\\in\\mathcal{Z}_{1}}\\underset{\\Tilde{z}_{i}}{\\operatorname*{inf}}\\big(\\mathbb{P}_{z^{*}}(\\hat{z}_{i}\\neq1)+\\mathbb{P}_{f(z^{*})}(\\hat{z}_{i}\\neq2)\\big)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\geq\\frac{|\\mathcal{Z}_{1}|}{Z}\\exp\\!\\left(-(1+\\eta)\\frac{\\mathrm{SNR}^{2}}{8}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\geq\\frac{1}{2}\\exp\\!\\left(-(1+\\eta)\\frac{\\mathrm{SNR}^{2}}{8}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "forsome $\\eta=o(1)$ . Here the second inequality is due to Lemma A.1. Then, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\tilde{z}}{\\operatorname*{inf}}\\ \\underset{z^{*}\\in[k]^{n}}{\\operatorname*{sup}}\\mathbb{E}h(\\hat{z},z^{*})\\geq\\frac{|T^{c}|}{2n}\\exp\\biggl(-(1+\\eta)\\frac{\\mathrm{SNR}^{2}}{8}\\biggr)=\\frac{1}{16k}\\exp\\biggl(-(1+\\eta)\\frac{\\mathrm{SNR}^{2}}{8}\\biggr)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\exp\\biggl(-(1+\\eta^{\\prime})\\frac{\\mathrm{SNR}^{2}}{8}\\biggr)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for some other $\\eta^{\\prime}=o(1)$ ,where we use $\\mathrm{SNR}^{2}/\\log k\\to\\infty$ ", "page_idx": 13}, {"type": "text", "text": "The proof for the case $\\mathrm{SNR}=O(1)$ is similar and hence is omitted here. ", "page_idx": 13}, {"type": "text", "text": "A.2  Proofs for the Upper Bound ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.2.1 High-level Idea ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we provide a high-level idea for the proof of Theorem 2.2. The detailed proof is technical and is given later in Appendix A.2.2. ", "page_idx": 13}, {"type": "text", "text": "The key idea for establishing the statistical guarantees of Algorithm 1, an iterative algorithm, is to perform a \u201cone-step\u201d analysis [8]. That is, assume we have an estimation $z$ for $z^{*}$ . Then we can apply (7), (8), and (9) on $z$ to obtain $\\{\\hat{\\theta}_{a}(z)\\}_{a\\in[k]}$ \uff0c $\\hat{\\Sigma}(z)$ , and $\\hat{z}(z)$ sequentially, which all depend on $z$ . Thus, $\\hat{z}(z)$ can be seen as a refined estimate of $z^{*}$ . We will first build the connection between $\\ell(z,z^{*})$ with $\\ell(\\hat{z}(z),z^{*})$ as in Lemma A.2, which informally states that under certain conditions, with high probability, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\ell(\\hat{z}(z),z^{*})\\leq\\xi_{\\mathrm{ideal}}(\\delta)+\\frac{1}{2}\\ell(z,z^{*})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "holds for any $z\\;\\in\\;[k]^{n}$ such that $\\ell(z,z^{*})$ is small. Here $\\xi_{\\mathrm{ideal}}(\\delta)$ refers to the ideal error, which eventually leads to the upper bound in Theorem 2.2. Lemma A.2 tells us $\\hat{z}(\\cdot)$ has a \u201ccontraction' property. That is, after one iteration of (7), (8), and (9), $\\ell(\\hat{z}(z),z^{*})$ is at most a half of $\\ell(z,z^{*})$ , up to an additive term $\\xi_{\\mathrm{ideal}}(\\delta)$ ", "page_idx": 13}, {"type": "text", "text": "To establish Lemma A.2, we decompose the loss $\\ell(\\hat{z}(z),z^{*})$ into several errors according to the difference in their behaviors. Next, we will introduce several conditions (Conditions $1-3$ ),under which we demonstrate that these errors are either negligible, well-controlled by $\\ell(z,z^{*})$ ,orconnected to $\\xi_{\\mathrm{ideal}}(\\delta)$ . Once Lemma A.2 is established, we will show in Lemma A.3 that the connection can be extended to multiple iterations, under two more conditions (Conditions 4 - 5). Lemma A.3 states informally that, under certain conditions and with high probability, we have ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n\\ell(z^{(t)},z^{*})\\leq\\xi_{\\mathrm{ideal}}(\\delta)+{\\frac{1}{2}}\\ell(z^{(t-1)},z^{*})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for all $t\\geq1$ . This implies $\\ell(z^{(t)},z^{*})$ is eventually at most $\\xi_{\\mathrm{ideal}}(\\delta)$ , up to some constant factor. Last, we will show all these conditions hold with high probability. Although the algorithmic guarantees in Lemma A.2 and Lemma A.3 are established with respect to the $\\ell(\\cdot,\\cdot)$ loss, we will use the relationship between $h(\\cdot,\\cdot)$ and $\\ell(\\cdot,\\cdot)$ to convert this result to one involving $h(\\cdot,\\cdot)$ . Hence, we prove Theorem 2.2. ", "page_idx": 14}, {"type": "text", "text": "A.2.2 Detailed Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In the statement of Theorem 2.2, the covariance matrix $\\Sigma^{*}$ is assumed to satisfy $\\lambda_{d}(\\Sigma^{*})/\\lambda_{1}(\\Sigma^{*})=$ $O(1)$ . Without loss of generality, we can replace it by assuming $\\Sigma^{*}$ satisfies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}\\leq\\lambda_{1}(\\Sigma^{*})\\leq\\lambda_{d}(\\Sigma^{*})\\leq\\lambda_{\\operatorname*{max}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\lambda_{\\operatorname*{min}},\\lambda_{\\operatorname*{max}}>0$ are two constants. This is due to the following simple argument using the scaling properties of normal distributions. Let $\\{Y_{j}\\}$ be some dataset generated according to Model 1 with parameters $\\{\\theta_{a}^{*}\\}_{a\\in[k]}$ \uff0c $\\Sigma^{*}$ , and $z^{*}$ . The assumption $\\lambda_{d}(\\Sigma^{*})/\\lambda_{1}^{-}(\\Sigma^{*})={\\cal O}(1)$ is equivalent to assuming there exist some constants $\\lambda_{\\operatorname*{min}},\\lambda_{\\operatorname*{max}}>0$ and some quantity $\\sigma>0$ that may depend on $n$ such that $\\lambda_{\\operatorname*{min}}\\sigma^{2}\\leq\\lambda_{1}(\\Sigma^{*})\\leq\\lambda_{d}(\\Sigma^{*})\\leq\\lambda_{\\operatorname*{max}}\\sigma^{2}$ . By performing a scaling transformation, we obtain another dataset $Y_{j}^{\\prime}=Y_{j}/\\sigma$ . Note that: 1) $\\{Y_{j}^{\\prime}\\}$ can be seen as generated from Model 1 with parameters $\\{\\theta_{a}^{*}/\\sigma\\}_{a\\in[k]},\\Sigma^{*}/\\sigma^{2}$ , and $z^{*}$ . 2) Clustering on $\\{Y_{j}\\}$ is equivalent to clustering on $\\{Y_{j}^{\\prime}\\}$ 3) By the definition in (2), the SNRs that are associated with the data-generating processes of $\\{Y_{j}^{\\prime}\\}$ and $\\{Y_{j}\\}$ are exactly equal to each other. 4) We have $\\lambda_{\\mathrm{min}}\\le\\lambda_{1}(\\Sigma^{*}/\\sigma^{2})\\le\\lambda_{d}(\\Sigma^{*}/\\sigma^{2})\\le\\lambda_{\\mathrm{max}}$ Thus, for the remainder of this section, we assume that (15) holds without any loss of generality. ", "page_idx": 14}, {"type": "text", "text": "In the proof, we will mainly use the loss $\\ell(\\cdot,\\cdot)$ for convenience. Recall $\\Delta$ is defined as the minimum distance among centers in (3). We have ", "page_idx": 14}, {"type": "equation", "text": "$$\nh(z,z^{*})\\leq\\frac{\\ell(z,z^{*})}{n\\Delta^{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The algorithmic guarantees Lemma A.2 and Lemma A.3 are established with respect to the $\\ell(\\cdot,\\cdot)$ loss. Eventually, we will use (16) to convert it into a result with respect to $h(\\cdot,\\cdot)$ in theproof of Theorem 2.2. ", "page_idx": 14}, {"type": "text", "text": "Error Decomposition for the One-step Analysis: Consider an arbitrary $z\\in[k]^{n}$ .Apply (7), (8), and (9) on $z$ to obtain $\\{\\hat{\\theta}_{a}(z)\\}_{a\\in[k]},$ $\\hat{\\Sigma}(z)$ , and $\\hat{z}(z)$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\theta}_{a}(z)=\\frac{\\sum_{j\\in[n]}Y_{j}\\mathbb{I}\\left\\{z_{j}=a\\right\\}}{\\sum_{j\\in[n]}\\mathbb{I}\\left\\{z_{j}=a\\right\\}},\\quad\\forall a\\in[k]}\\\\ &{\\hat{\\Sigma}(z)=\\frac{\\sum_{a\\in[k]}\\sum_{j\\in[n]}(Y_{j}-\\hat{\\theta}_{a}(z))(Y_{j}-\\hat{\\theta}_{a}(z))^{T}\\mathbb{I}\\left\\{z_{j}=a\\right\\}}{n},}\\\\ &{\\hat{z}_{j}(z)=\\underset{a\\in[k]}{\\mathrm{argmin}}(Y_{j}-\\hat{\\theta}_{a}(z))^{T}(\\hat{\\Sigma}(z))^{-1}(Y_{j}-\\hat{\\theta}_{a}),\\quad\\forall j\\in[n].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For simplicity, we denote $\\hat{z}$ as shorthand for $\\hat{z}(z)$ . Let $j\\in[n]$ be an arbitrary index with $z_{j}^{*}=a$ According to (9), $z_{j}^{*}$ will be incorrectly estimated after one iteration in $\\hat{z}$ $a\\ne\\mathrm{argmin}_{b\\in[k]}(Y_{j}-$ $\\hat{\\theta}_{b}(z))^{T}(\\hat{\\Sigma}(z))^{-1}(\\bar{Y}_{j}-\\hat{\\theta}_{b}(z))$ . Therefore, it is important to analyze the event ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\langle Y_{j}-\\hat{\\theta}_{b}(z),(\\hat{\\Sigma}(z))^{-1}(Y_{j}-\\hat{\\theta}_{b}(z))\\rangle\\leq\\langle Y_{j}-\\hat{\\theta}_{a}(z),(\\hat{\\Sigma}(z))^{-1}(Y_{j}-\\hat{\\theta}_{a}(z))\\rangle,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for any $b\\,\\in\\,[k]\\,\\setminus\\{a\\}$ . Note that $Y_{j}\\,=\\,\\theta_{a}^{*}+\\epsilon_{j}$ . After some rearrangements, we can see (17) is equivalent to ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\langle\\epsilon_{j},(\\hat{\\Sigma}(z^{*}))^{-1}(\\hat{\\theta}_{a}(z^{*})-\\hat{\\theta}_{b}(z^{*}))\\rangle}\\\\ &{\\leq-\\cfrac{1}{2}\\langle\\theta_{a}^{*}-\\theta_{b}^{*},(\\Sigma^{*})^{-1}(\\theta_{a}^{*}-\\theta_{b}^{*})\\rangle+F_{j}(a,b,z)+G_{j}(a,b,z)+H_{j}(a,b,z),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{j}(a,b,z)=\\langle\\epsilon_{j},(\\hat{\\Sigma}(z))^{-1}(\\hat{\\theta}_{b}(z)-\\hat{\\theta}_{b}(z^{*}))\\rangle-\\langle\\epsilon_{j},(\\hat{\\Sigma}(z))^{-1}(\\hat{\\theta}_{a}(z)-\\hat{\\theta}_{a}(z^{*}))\\rangle}\\\\ &{\\quad\\quad\\quad\\quad+\\langle\\epsilon_{j},((\\hat{\\Sigma}(z))^{-1}-(\\hat{\\Sigma}(z^{*}))^{-1})(\\hat{\\theta}_{b}(z^{*})-\\hat{\\theta}_{a}(z^{*}))\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{T}_{j}(a,b,z)=\\frac{1}{2}\\langle\\theta_{a}^{*}-\\hat{\\theta}_{a}(z),(\\hat{\\Sigma}(z))^{-1}(\\theta_{a}^{*}-\\hat{\\theta}_{a}(z))\\rangle-\\frac{1}{2}\\langle\\theta_{a}^{*}-\\hat{\\theta}_{a}(z^{*}),(\\hat{\\Sigma}(z))^{-1}(\\theta_{a}^{*}-\\hat{\\theta}_{a}(z^{*}))\\rangle}\\\\ &{\\quad\\quad\\quad\\quad+\\frac{1}{2}\\langle\\theta_{a}^{*}-\\hat{\\theta}_{a}(z^{*}),(\\hat{\\Sigma}(z))^{-1}(\\theta_{a}^{*}-\\hat{\\theta}_{a}(z^{*}))\\rangle-\\frac{1}{2}\\langle\\theta_{a}^{*}-\\hat{\\theta}_{a}(z^{*}),(\\hat{\\Sigma}(z^{*}))^{-1}(\\theta_{a}^{*}-\\hat{\\theta}_{a}(z^{*}))\\rangle}\\\\ &{\\quad\\quad\\quad\\quad-\\frac{1}{2}\\langle\\theta_{a}^{*}-\\hat{\\theta}_{b}(z),(\\hat{\\Sigma}(z))^{-1}(\\theta_{a}^{*}-\\hat{\\theta}_{b}(z))\\rangle+\\frac{1}{2}\\langle\\theta_{a}^{*}-\\hat{\\theta}_{b}(z^{*}),(\\hat{\\Sigma}(z))^{-1}(\\theta_{a}^{*}-\\hat{\\theta}_{b}(z^{*}))\\rangle}\\\\ &{\\quad\\quad\\quad\\quad-\\frac{1}{2}\\langle\\theta_{a}^{*}-\\hat{\\theta}_{b}(z^{*}),(\\hat{\\Sigma}(z))^{-1}(\\theta_{a}^{*}-\\hat{\\theta}_{b}(z^{*}))\\rangle+\\frac{1}{2}\\langle\\theta_{a}^{*}-\\hat{\\theta}_{b}(z^{*}),(\\hat{\\Sigma}(z^{*}))^{-1}(\\theta_{a}^{*}-\\hat{\\theta}_{b}(z^{*}))\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{{H_{j}(a,b,z)=-\\displaystyle\\frac{1}{2}\\langle\\theta_{a}^{*}-\\hat{\\theta}_{b}(z^{*}),(\\hat{\\Sigma}(z^{*}))^{-1}(\\theta_{a}^{*}-\\hat{\\theta}_{b}(z^{*}))\\rangle+\\displaystyle\\frac{1}{2}\\langle\\theta_{a}^{*}-\\theta_{b}^{*},(\\hat{\\Sigma}(z^{*}))^{-1}(\\theta_{a}^{*}-\\theta_{b}^{*})\\rangle}}\\\\ {{-\\displaystyle\\frac{1}{2}\\langle\\theta_{a}^{*}-\\theta_{b}^{*},(\\hat{\\Sigma}(z^{*}))^{-1}(\\theta_{a}^{*}-\\theta_{b}^{*})\\rangle+\\displaystyle\\frac{1}{2}\\langle\\theta_{a}^{*}-\\theta_{b}^{*},(\\Sigma^{*})^{-1}(\\theta_{a}^{*}-\\theta_{b}^{*})\\rangle}}\\\\ {{+\\displaystyle\\frac{1}{2}\\langle\\theta_{a}^{*}-\\hat{\\theta}_{a}(z^{*}),(\\hat{\\Sigma}(z^{*}))^{-1}(\\theta_{a}^{*}-\\hat{\\theta}_{a}(z^{*}))\\rangle.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In the above decomposition, the expression $\\begin{array}{r l r}{\\langle\\epsilon_{j},(\\hat{\\Sigma}(z^{*}))^{-1}(\\hat{\\theta}_{a}(z^{*})\\;-\\;\\hat{\\theta}_{b}(z^{*}))\\rangle}&{{}\\leq}&{-\\frac{1}{2}\\langle\\theta_{a}^{*}\\;-\\;\\frac{{\\cal0}}{2}\\rangle_{\\rho}^{}}\\end{array}$ $\\theta_{b}^{*},(\\Sigma^{*})^{-1}(\\theta_{a}^{*}-\\theta_{b}^{*})\\rangle$ does not involve $z$ . Roughly speaking, it corresponds to the event that $z_{j}^{*}$ will be incorrectly estimated in $\\hat{z}\\big(z^{*}\\big)$ . This is considered the main part of (17) and will contribute to $\\xi_{\\mathrm{ideal}}$ . The difference between (17) and the main term is expressed through the terms $F_{j},G_{j},H_{j}\\colon F_{j}$ includes terms related to noise $\\epsilon_{j}$ , illustrating the impact of measurement noise; $G_{j}$ covers estimation errors for cluster centers $(\\hat{\\theta}_{a}(z)-\\hat{\\theta}_{a}(z^{*}))$ and covariance matrices $(\\hat{\\Sigma}(z)-\\hat{\\Sigma}(z^{*}))$ , showing the effect of the parameter estimation inaccuracies; $H_{j}$ contains all other terms from additional error sources. Readers can refer to [8] for more information about the decomposition. ", "page_idx": 15}, {"type": "text", "text": "Conditions and Guarantees for One-step Analysis. We continue to analyze the event (17). We first define a quantity independent of $z$ , which we refer to as the ideal error: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\xi_{\\mathrm{ideal}}(\\delta)=\\displaystyle\\sum_{j=1}^{n}\\sum_{b\\in[k]\\backslash\\{z_{j}^{*}\\}}\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|^{2}\\mathbb{I}\\Bigg\\{\\langle\\epsilon_{j},(\\hat{\\Sigma}(z^{*}))^{-1}(\\hat{\\theta}_{a}(z^{*})-\\hat{\\theta}_{b}(z^{*}))\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le-\\displaystyle\\frac{1-\\delta}{2}\\langle\\theta_{a}^{*}-\\theta_{b}^{*},(\\Sigma^{*})^{-1}(\\theta_{a}^{*}-\\theta_{b}^{*})\\rangle\\Bigg\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "When $\\delta=0$ , it is determined by the main term in (17), namely $\\langle\\epsilon_{j},(\\hat{\\Sigma}(z^{*}))^{-1}(\\hat{\\theta}_{a}(z^{*})-\\hat{\\theta}_{b}(z^{*}))\\rangle\\leq$ $-{\\textstyle\\frac{1}{2}}\\langle\\theta_{a}^{*}-\\theta_{b}^{*},(\\Sigma^{*})^{-1}(\\theta_{a}^{*}-\\theta_{b}^{*})\\rangle$ . Roughly speaking, $\\xi_{\\mathrm{ideal}}(0)$ relates to the performance of $\\hat{z}(z^{*})$ . Due to the presence of the terms $F_{j},G_{j},H_{j}$ in the decomposition of (17), what appears in the analysis of (17) is $\\xi_{\\mathrm{ideal}}(\\delta)$ instead of $\\xi_{\\mathrm{ideal}}(0)$ where hopefully $\\delta>0$ is some small number. ", "page_idx": 15}, {"type": "text", "text": "To establish the guarantee for one-step analysis, we next give several conditions on the error terms $F_{j}(a,b;z),G_{j}(a,b;z)$ and $H_{j}(a,b;z)$ ", "page_idx": 15}, {"type": "text", "text": "Condition 1. Assume that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\substack{\\{z:l(z,z^{*})\\leq\\tau\\}\\,j\\in[n]\\,b\\in[k]\\backslash\\{z_{j}^{*}\\}}}\\frac{|H_{j}(z_{j}^{*},b,z)|}{\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle}\\leq\\frac{\\delta}{4}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "holds with probability at least $1-\\eta_{1}$ for some $\\tau,\\delta,\\eta_{1}>0$ ", "page_idx": 15}, {"type": "text", "text": "Condition 2. Assume that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\{z:l(z,z^{*})\\leq\\tau\\}}\\sum_{j=1}^{n}\\operatorname*{max}_{b\\in[k]\\setminus\\{z_{j}^{*}\\}}\\frac{F_{j}(z_{j}^{*},b,z)^{2}\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|^{2}}{\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle^{2}\\ell(z,z^{*})}\\leq\\frac{\\delta^{2}}{128}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "holds with probability at least $1-\\eta_{2}$ for some $\\tau,\\delta,\\eta_{2}>0$ ", "page_idx": 15}, {"type": "text", "text": "Condition 3. Assume that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\substack{\\{z:l(z,z^{*})\\leq\\tau\\}\\,j\\in[n]\\,b\\in[k]\\backslash\\{z_{j}^{*}\\}}}\\frac{|G_{j}(z_{j}^{*},b,z)|}{\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle}\\leq\\frac{\\delta}{8}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "holds with probability at least $1-\\eta_{3}$ for some $\\tau,\\delta,\\eta_{3}>0$ ", "page_idx": 16}, {"type": "text", "text": "Lemma A.2. Assumes Conditions $I\\cdot3$ hold for some $\\tau,\\delta,\\eta_{1},\\eta_{2},\\eta_{3},>0$ We then have ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\mathbb P}\\bigg(\\ell(\\hat{z},z^{*})\\leq\\xi_{i d e a l}(\\delta)+\\frac{1}{2}\\ell(z,z^{*})f o r\\,a n y\\,z\\in[k]^{n}\\;s u c h\\,t h a t\\,\\ell(z,z^{*})\\leq\\tau\\bigg)\\geq1-\\eta,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\textstyle\\eta=\\sum_{i=1}^{3}\\eta_{i}$ ", "page_idx": 16}, {"type": "text", "text": "Proof. Consider any $j\\in[n]$ such that $z_{j}^{*}=a$ . We notice that for any $b\\in[k]$ such that $b\\neq a$ \uff0c ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{(\\xi_{j}-b)\\leq\\mathbb{E}\\left\\{(\\zeta_{j},\\zeta_{j}(z),(\\xi_{j}(z))^{-1}(V_{j}-\\theta_{k}(z)))\\leq(\\zeta_{j}-\\bar{\\theta}_{k}(z),(\\zeta_{j}(z))^{-1}(V_{j}-\\theta_{k}(z)))\\right\\}}}\\\\ &{=\\mathbb{E}\\left\\{\\left\\{\\zeta_{j},(\\zeta_{j}(z)^{-1})^{-1}(\\theta_{j}(z^{*})-\\delta_{j}(z^{*}))\\right\\}}\\\\ &{\\leq-\\frac{1}{2}(\\theta_{j}^{*}-\\theta_{k}^{*},(\\zeta_{j}(z)^{-1}(\\theta_{j}^{*})-\\delta_{j}(z^{*}))+F_{j}(z^{*},b_{z})+G_{j}(z_{j}^{*},b_{z})+H_{j}(z_{j}^{*},b_{z}))\\right\\}}\\\\ &{\\leq\\mathbb{E}\\left\\{\\left\\{\\zeta_{j},(\\zeta_{j}(z)^{-1})^{-1}(\\bar{\\theta}_{j}(z^{*})-\\delta_{j}(z^{*}))\\right\\}\\leq-\\frac{1-\\delta}{2}(\\theta_{j}^{*}-\\theta_{k}^{*},(\\zeta_{j}(z)^{-1}-\\theta_{j}^{*}))\\right\\}}\\\\ &{\\quad+\\mathbb{E}\\left\\{\\frac{\\bar{\\theta}_{j}}{2}(\\theta_{j}^{*},\\theta_{k}^{*},(\\zeta_{j}^{-1})^{-1}(\\theta_{j}^{*}-\\theta_{k}^{*}))\\leq F_{j}(z_{j}^{*},b_{z})+G_{j}(z_{j}^{*},b_{z})+H_{j}(z_{j}^{*},b_{z})\\right\\}}\\\\ &{\\leq\\mathbb{E}\\left\\{\\left\\{\\zeta_{j},(\\zeta_{j}(z)^{-1})^{-1}(\\bar{\\theta}_{j}(z^{*})-\\delta_{j}(z^{*}))\\right\\}\\leq-\\frac{1-\\delta}{2}(\\theta_{j}^{*}-\\theta_{k}^{*},(\\zeta_{j}^{-1})^{-1}(\\theta_{j}^{*}-\\theta_{k}^{*}))\\right\\}}\\\\ &{\\quad+\\mathbb\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the second inequality comes from Conditions 1 and 3. Note that we can multiply $\\mathbb{I}\\left\\{\\hat{z}_{j}=b\\right\\}$ on both sides of the above display and the inequality still holds. Hence, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{I}\\left\\{\\hat{z}_{j}=b\\right\\}\\leq\\mathbb{I}\\left\\{\\langle\\epsilon_{j},(\\hat{\\Sigma}(z^{*}))^{-1}(\\hat{\\theta}_{z_{j}^{*}}(z^{*})-\\hat{\\theta}_{b}(z^{*}))\\rangle\\leq-\\displaystyle\\frac{1-\\delta}{2}\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle\\right\\}}\\\\ &{\\quad\\quad\\quad\\quad+\\displaystyle\\frac{64F_{j}(z_{j}^{*},b,z)^{2}}{\\delta^{2}\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle^{2}}\\mathbb{I}\\left\\{\\hat{z}_{j}=b\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell(\\hat{z},z^{*})}\\\\ {=}&{\\displaystyle\\sum_{j=1}^{n}\\sum_{b\\in[k]\\setminus\\{\\pm\\}}\\left\\|\\theta_{b}^{*}-\\theta_{\\hat{z}_{j}^{*}}^{*}\\right\\|^{2}\\mathbb{I}\\left\\{\\hat{z}_{j}=b\\right\\}}\\\\ {\\le}&{\\displaystyle\\xi_{\\mathrm{ideal}}(\\delta)+\\sum_{j=1}^{n}\\sum_{b\\in[k]\\setminus\\{\\pm\\}}\\left\\|\\theta_{b}^{*}-\\theta_{z_{j}^{*}}^{*}\\right\\|^{2}\\mathbb{I}\\left\\{\\hat{z}_{j}=b\\right\\}\\frac{64F_{j}(z_{j}^{*},b,z)^{2}}{\\delta^{2}\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\sum_{s}^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle^{2}}}\\\\ &{\\le\\xi_{\\mathrm{ideal}}(\\delta)+\\displaystyle\\sum_{j=1}^{n}\\operatorname*{max}_{b\\in[k]\\setminus\\{\\pm,z_{j}^{*}\\}}\\left\\|\\theta_{b}^{*}-\\theta_{z_{j}^{*}}^{*}\\right\\|^{2}\\frac{64F_{j}(z_{j}^{*},b,z)^{2}}{\\delta^{2}\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\sum_{s}^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle^{2}}}\\\\ {\\le}&{\\displaystyle\\xi_{\\mathrm{ideal}}(\\delta)+\\frac{\\ell(z,z^{*})}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which implies Lemma A.2. Here the last inequality uses Condition 2. ", "page_idx": 16}, {"type": "text", "text": "Conditions and Guarantees for Multiple Iterations. In the above, we establish a statistical guarantee for the one-step analysis. Now we will extend the result to multiple iterations. That is, starting from some initialization $z^{(0)}$ , we will characterize how the losses $\\bar{\\ell}(z^{(0)},z^{*}),\\,\\ell(z^{(1)},z^{*})$ $\\ell(z^{(2)},z^{*})$ , ..., decay. We impose conditions on $\\xi_{\\mathrm{ideal}}(\\delta)$ and the initialization $z^{(0)}$ ", "page_idx": 17}, {"type": "text", "text": "Condition 4. Assume that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\xi_{i d e a l}(\\delta)\\leq{\\frac{3\\tau}{8}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "holds with probability at least $1-\\eta_{4}$ for some $\\tau,\\delta,\\eta_{4}>0$ ", "page_idx": 17}, {"type": "text", "text": "Finally, we need a condition on the initialization. ", "page_idx": 17}, {"type": "text", "text": "Condition 5. Assume that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\ell(z^{(0)},z^{*})\\leq\\tau\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "holds with probability at least $1-\\eta_{5}$ for some $\\tau,\\eta_{5}>0$ ", "page_idx": 17}, {"type": "text", "text": "With these conditions satisfied, we can give a lemma that shows the convergence of our algorithm. ", "page_idx": 17}, {"type": "text", "text": "Lemma A.3. Assume Conditions $I\\textsuperscript{-5}$ hold for some $\\tau,\\delta,\\eta_{1},\\eta_{2},\\eta_{3},\\eta_{4},\\eta_{5}>0$ We then have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\ell(z^{(t)},z^{*})\\leq\\xi_{i d e a l}(\\delta)+{\\frac{1}{2}}\\ell(z^{(t-1)},z^{*})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for all $t\\geq1$ , with probability at least $1-\\eta,$ where $\\textstyle\\eta=\\sum_{i=1}^{5}\\eta_{i}$ ", "page_idx": 17}, {"type": "text", "text": "Proof. By Conditions 4, 5 and a mathematical induction argument, we can easily conclude $\\ell(z^{(t)},z^{*})\\leq\\tau$ for any $t\\geq0$ . Thus, Lemma A.3 is a direct extension of Lemma A.2. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "With-high-probability Results for the Conditions and Proof of Theorem 2.2. Recall the definition of $\\Delta$ in (3). Recall that in (15) we assume $\\lambda_{\\operatorname*{min}}\\leq\\lambda_{1}(\\Sigma^{*})\\leq\\lambda_{d}(\\Sigma^{*})\\leq\\lambda_{\\operatorname*{max}}$ fortwoconstants $\\lambda_{\\operatorname*{min}},\\lambda_{\\operatorname*{max}}>0$ .Hencewehave $\\Delta$ is of the same order as SNR. Specifically, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{\\lambda_{\\operatorname*{max}}}}\\Delta\\leq\\mathrm{SNR}\\leq\\frac{1}{\\sqrt{\\lambda_{\\operatorname*{min}}}}\\Delta.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence the assumption $\\mathrm{SNR}\\rightarrow\\infty$ in the statement of Theorem 2.2 is equivalently $\\Delta\\rightarrow\\infty$ . Next, we give two with-high-probability lemmas. The frst lemma is for Conditions 1-3, providing upper bounds for the quantities involved in these conditions, showing that $\\delta$ can be taken as some $o(1)$ term. The second lemma shows that for any $\\delta=o(1)$ $\\xi_{\\mathrm{ideal}}(\\delta)$ is upper bounded by the desired minimax rate multiplied by the sample size $n$ \uff1a ", "page_idx": 17}, {"type": "text", "text": "Lemma A.4. Under the same conditions as in Theorem 2.2, for any constant $C^{\\prime}>0$ thereexists someconstant $C>0$ onlydependingon $\\alpha$ and $C^{\\prime}$ suchthat ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\{z:\\ell(z,z^{*})\\leq r\\}}{\\operatorname*{max}}\\underset{j\\in[n]}{\\operatorname*{max}}\\underset{b\\in[k]\\backslash\\{z_{j}^{*}\\}}{\\operatorname*{max}}\\frac{|H_{j}(z_{j}^{*},b,z)|}{\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle}\\leq C\\sqrt{\\frac{k(d+\\log n)}{n}}\\quad\\quad(1\\leq j\\leq n)}\\\\ &{\\underset{\\{z:\\ell(z,z^{*})\\leq r\\}}{\\operatorname*{max}}\\underset{j=1}{\\operatorname*{max}}\\underset{b\\in[k]\\backslash\\{z_{j}^{*}\\}}{\\operatorname*{max}}\\frac{F_{j}(z_{j}^{*},b,z)^{2}\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|^{2}}{\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle^{2}\\ell(z,z^{*})}\\leq C k^{3}\\bigg(\\frac{\\tau}{n}+\\frac{1}{\\Delta^{2}}+\\frac{d^{2}}{n\\Delta^{2}}\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\substack{\\{z:\\ell(z,z^{*})\\leq\\tau\\}\\,j\\in[n]\\,b\\in[k]\\backslash\\{z_{j}^{*}\\}}}\\frac{|G_{j}(z_{j}^{*},b,z)|}{\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle}\\leq C k\\bigg(\\frac{\\tau}{n}+\\frac{1}{\\Delta}\\sqrt{\\frac{\\tau}{n}}+\\frac{d\\sqrt{\\tau}}{n\\Delta}\\bigg)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with probability at least $1-n^{-C^{\\prime}}$ . As a result, Conditions 1-3 hold for some $\\delta=o(1)$ ", "page_idx": 17}, {"type": "text", "text": "Proof. Under the conditions of Theorem 2.2, the inequalities (33)-(38) hold with probability at least $1-n^{-C^{\\prime}}$ . In the remaining proof, we will work on the event these inequalities hold. Denote ", "page_idx": 17}, {"type": "text", "text": "$\\begin{array}{r}{\\hat{\\Sigma}_{a}(z)\\,=\\,\\frac{\\sum_{j\\in[n]}(Y_{j}-\\hat{\\theta}_{a}(z))(Y_{j}-\\hat{\\theta}_{a}(z))^{T}\\mathbb{I}\\{z_{j}=a\\}}{\\sum_{j\\in[n]}\\mathbb{I}\\{z_{j}=a\\}}}\\end{array}$ and $\\Sigma_{a}^{*}\\,=\\,\\Sigma^{*}$ for any $a\\in[k]$ . Then we have the equivalence ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{\\Sigma}(z^{*})-\\Sigma^{*}=\\sum_{a=1}^{k}\\frac{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}=a\\}}{n}(\\hat{\\Sigma}_{a}(z^{*})-\\Sigma_{a}^{*}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence, we can use the results from Lemma C.7 and Lemma C.8. ", "page_idx": 18}, {"type": "text", "text": "By (43) and (44), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|{\\hat{\\Sigma}}(z^{*})-\\Sigma^{*}\\|\\preceq\\,{\\sqrt{\\frac{k(d+\\log n)}{n}}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\hat{\\Sigma}(z)-\\hat{\\Sigma}(z^{*})\\|=}&{\\displaystyle\\left\\|\\displaystyle\\sum_{a=1}^{k}\\frac{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a\\}}{n}\\hat{\\Sigma}_{a}(z)-\\displaystyle\\sum_{a=1}^{k}\\frac{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}=a\\}}{n}\\hat{\\Sigma}_{a}(z^{*})\\right\\|}\\\\ {\\preceq}&{\\displaystyle\\left\\|\\displaystyle\\sum_{a=1}^{k}\\frac{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a\\}}{n}(\\hat{\\Sigma}_{a}(z)-\\hat{\\Sigma}(z^{*}))\\right\\|+\\left\\|\\displaystyle\\sum_{a=1}^{k}\\frac{\\sum_{j=1}^{n}(\\mathbb{I}\\{z_{j}=a\\}-\\mathbb{I}\\{z_{j}^{*}=a\\}-\\mathbb{I}\\{z_{j}^{*}=a\\})}{n}\\hat{\\Sigma}_{a}(z)\\right\\|}\\\\ {\\preceq}&{\\displaystyle\\frac{k\\sqrt{n\\ell(z,z^{*})}}{n\\Delta}+\\displaystyle\\frac{k}{n}\\ell(z,z^{*})+\\displaystyle\\frac{k d}{n\\Delta}\\sqrt{\\ell(z,z^{*})}+\\displaystyle\\frac{k}{n\\Delta^{2}}\\ell(z,z^{*})}\\\\ {\\preceq}&{\\displaystyle\\frac{k\\sqrt{n\\ell(z,z^{*})}}{n\\Delta}+\\displaystyle\\frac{k}{n}\\ell(z,z^{*})+\\displaystyle\\frac{k d}{n\\Delta}\\sqrt{\\ell(z,z^{*})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By the assumption that $k d=O({\\sqrt{n}})$ $\\begin{array}{r}{\\frac{\\Delta}{k}\\rightarrow\\infty}\\end{array}$ and $\\tau=o(n/k)$ , we have $\\|\\hat{\\Sigma}(z^{*})-\\Sigma^{*}\\|,\\|\\hat{\\Sigma}(z)-$ ${\\hat{\\Sigma}}(z^{*})\\|=o(1)$ , which implies $\\|(\\hat{\\Sigma}(z^{*}))^{-1}\\|,\\|(\\hat{\\Sigma}(z))^{-1}\\|\\preceq1$ . Thus, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|(\\hat{\\Sigma}(z^{*}))^{-1}-(\\Sigma^{*})^{-1}\\|\\leq\\|(\\hat{\\Sigma}(z^{*}))^{-1}\\|\\|\\hat{\\Sigma}(z^{*})-\\Sigma^{*}\\|\\|(\\Sigma^{*})^{-1}\\|\\preceq\\sqrt{\\frac{k(d+\\log n)}{n}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and similarly ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|(\\hat{\\Sigma}(z))^{-1}-(\\hat{\\Sigma}(z^{*}))^{-1}\\|\\preceq\\frac{k}{n}\\ell(z,z^{*})+\\frac{k\\sqrt{n\\ell(z,z^{*})}}{n\\Delta}+\\frac{k d}{n\\Delta}\\sqrt{\\ell(z,z^{*})}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now we start to prove (19)-(21). Let $F_{j}(a,b,z)=F_{j}^{(1)}(a,b,z)+F_{j}^{(2)}(a,b,z)+F_{j}^{(3)}(a,b,z)$ where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{j}^{(1)}(a,b,z):=\\langle\\epsilon_{j},(\\hat{\\Sigma}(z))^{-1}(\\hat{\\theta}_{b}(z)-\\hat{\\theta}_{b}(z^{*}))\\rangle-\\langle\\epsilon_{j},(\\hat{\\Sigma}(z))^{-1}(\\hat{\\theta}_{a}(z)-\\hat{\\theta}_{a}(z^{*}))\\rangle,}\\\\ &{F_{j}^{(2)}(a,b,z):=-\\langle\\epsilon_{j},((\\hat{\\Sigma}(z))^{-1}-(\\hat{\\Sigma}(z^{*}))^{-1})(\\theta_{a}^{*}-\\theta_{b}^{*})\\rangle,}\\\\ &{F_{j}^{(3)}(a,b,z):=-\\langle\\epsilon_{j},((\\hat{\\Sigma}(z))^{-1}-(\\hat{\\Sigma}(z^{*}))^{-1})(\\theta_{b}^{*}-\\hat{\\theta}_{b}(z^{*}))\\rangle+\\langle\\epsilon_{j},((\\hat{\\Sigma}(z))^{-1}-(\\hat{\\Sigma}(z^{*}))^{-1})(\\theta_{a}^{*}-\\hat{\\theta}_{b}(z^{*}))\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Notice that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j=1}^{n}\\operatorname*{max}_{1\\leq i\\leq1/1}(\\varepsilon_{j}^{x},\\,\\frac{F_{j}^{(2)}(z^{*},b_{j},z)^{2}\\|\\theta_{x}^{2}-\\theta_{y}^{*}\\|^{2}}{(\\varepsilon_{j}^{x}-\\theta_{x}^{*})(\\varepsilon_{x}^{*}-\\theta_{y}^{*})})^{2/2}\\;}\\\\ &{\\le\\displaystyle\\sum_{j=1}^{n}\\sum_{h=1}^{k}\\Bigg|\\frac{\\langle\\epsilon_{j}(\\varepsilon_{j}(\\hat{x}))^{-1}-(\\hat{x}(z^{*})^{-1})(\\theta_{x}^{*}-\\theta_{y}^{*})\\rangle}{\\|\\theta_{x}^{*}-\\theta_{y}^{*}\\|^{2}\\langle\\theta_{x},z^{*}\\rangle}\\Bigg|^{2}}\\\\ &{\\le\\displaystyle\\sum_{b=1}^{k}\\sum_{\\alpha=i\\in[k]\\backslash\\{h\\}}^{k}\\mathbb{E}[\\varepsilon_{j}^{x}=a]\\frac{\\Big|\\langle\\epsilon_{j}(\\hat{x}(z))^{-1}-(\\hat{x}(z^{*}))^{-1}\\rangle(\\theta_{x}^{*}-\\theta_{y}^{*})\\Big|^{2}}{\\|\\theta_{x}^{*}-\\theta_{y}^{*}\\|^{2}\\langle\\xi_{x},z^{*}\\rangle}}\\\\ &{\\le\\displaystyle\\sum_{b=1}^{k}\\sum_{\\alpha=i\\in[k]\\backslash\\{h\\}}^{k}\\frac{\\overline{{{\\gamma}}}}{2}\\mathbb{I}\\{z_{j}^{*}=a\\}\\frac{\\Big|\\langle\\epsilon_{j}(\\hat{x})\\rangle^{-1}(\\hat{x}(z^{*}))^{-1}(\\hat{x}^{*})\\rangle^{2}(\\varepsilon_{x}^{*}-\\theta_{y}^{*})\\rangle\\Big|^{2}}{\\|\\theta_{x}^{*}-\\theta_{y}^{*}\\|^{2}\\langle\\xi_{x},z^{*}\\rangle}}\\\\ &{\\lesssim\\displaystyle\\sum_{b=1}^{k}\\sum_{\\alpha\\in[k]\\backslash\\{h\\}}\\frac{\\|(\\hat{x}(z))^{-1}-(\\hat{x}(z^{*}))^{-1}\\|\\theta_{x}^{*}-\\theta_{y}^{*}\\|^{2}}{ \n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we use (34), (23), and the fact that $\\ell(z,z^{*})\\,\\leq\\,\\tau$ and $k d\\,=\\,O({\\sqrt{n}})$ for the last inequality. Here the second to last inequality is due to the following argument: for any $w\\,\\in\\,\\mathbb{R}^{d}$ \uff0cwe have $\\begin{array}{r}{\\sum_{j}|\\langle\\epsilon_{j},w\\rangle|^{2}=\\sum_{j}w^{T}\\epsilon_{j}\\epsilon_{j}^{\\dot{T}}w\\,\\stackrel{\\cdot}{=}w^{T}(\\sum_{j}\\epsilon_{j}\\epsilon_{j}^{T})w\\,\\leq\\,\\|w\\|^{2}\\|\\breve{\\sum}_{j}\\,\\epsilon_{j}\\epsilon_{j}^{T}\\|}\\end{array}$ . From (41) we have $\\begin{array}{r}{\\operatorname*{max}_{a\\in[k]}\\|\\theta_{a}^{*}-\\hat{\\theta}_{a}(z^{*})\\|=o(1)}\\end{array}$ under the assumption $k d=O({\\sqrt{n}})$ . By the similar analysis as in $F_{j}^{(2)}(a,b,z)$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{n}\\operatorname*{max}_{b\\in[k]\\setminus\\{z_{j}^{*}\\}}\\frac{F_{j}^{(3)}(z_{j}^{*},b,z)^{2}\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|^{2}}{\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle^{2}\\ell(z,z^{*})}\\preceq k^{3}(\\frac{\\tau}{n}+\\frac{1}{\\Delta^{2}}+\\frac{d^{2}}{n\\Delta^{2}}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Similarly, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j=1}^{n}\\operatorname*{max}_{b\\in[k]\\backslash\\{z_{j}^{*}\\}}\\frac{F_{j}^{(1)}(z_{j}^{*},b,z)^{2}\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|^{2}}{\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle^{2}\\ell(z,z^{*})}}\\\\ {\\preceq}&{\\displaystyle\\sum_{b=1}^{k}\\sum_{a\\in[k]\\backslash\\{b\\}}\\frac{\\|(\\hat{\\Sigma}(z))^{-1}(\\hat{\\theta}_{a}(z)-\\hat{\\theta}_{a}(z^{*}))\\|^{2}}{\\|\\theta_{a}^{*}-\\theta_{b}^{*}\\|^{2}\\ell(z,z^{*})}\\bigg\\|\\displaystyle\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}=a\\}\\epsilon_{j}\\epsilon_{j}^{T}\\bigg\\|}\\\\ &{\\preceq\\ \\frac{k^{3}}{\\Delta^{4}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we use (42) and the fact that $(\\hat{\\Sigma}(z))^{-1}$ has bounded operator norm. Combining these terms together, we obtain (20). ", "page_idx": 19}, {"type": "text", "text": "Next, for (19), by (41) we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\vert-\\langle\\theta_{a}^{*}-\\hat{\\theta}_{b}(z^{*}),(\\hat{\\Sigma}(z^{*}))^{-1}(\\theta_{a}^{*}-\\hat{\\theta}_{b}(z^{*}))\\rangle+\\langle\\theta_{a}^{*}-\\theta_{b}^{*},(\\hat{\\Sigma}(z^{*}))^{-1}(\\theta_{a}^{*}-\\theta_{b}^{*})\\rangle\\vert}\\\\ &{\\le\\vert\\langle\\theta_{b}^{*}-\\hat{\\theta}_{b}(z^{*}),(\\hat{\\Sigma}(z^{*}))^{-1}(\\theta_{b}^{*}-\\hat{\\theta}_{b}(z^{*}))\\rangle\\vert+2\\vert\\langle\\theta_{b}^{*}-\\hat{\\theta}_{b}(z^{*}),(\\hat{\\Sigma}(z^{*}))^{-1}(\\theta_{a}^{*}-\\theta_{b}^{*})\\rangle\\vert}\\\\ &{\\preceq\\frac{k(d+\\log n)}{n}+\\sqrt{\\frac{k(d+\\log n)}{n}}\\Vert\\theta_{a}^{*}-\\theta_{b}^{*}\\Vert,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n|\\langle\\theta_{a}^{*}-\\hat{\\theta}_{a}(z^{*}),(\\hat{\\Sigma}(z^{*}))^{-1}(\\theta_{a}^{*}-\\hat{\\theta}_{a}(z^{*}))\\rangle|\\preceq\\frac{k(d+\\log n)}{n}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By (22) we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n-\\left\\langle\\theta_{a}^{*}-\\theta_{b}^{*},(\\hat{\\Sigma}(z^{*}))^{-1}(\\theta_{a}^{*}-\\theta_{b}^{*})\\right\\rangle+\\left\\langle\\theta_{a}^{*}-\\theta_{b}^{*},(\\Sigma^{*})^{-1}(\\theta_{a}^{*}-\\theta_{b}^{*})\\right\\rangle|\\preceq\\sqrt{\\frac{k(d+\\log n)}{n}}\\lVert\\theta_{a}^{*}-\\theta_{b}^{*}\\rVert^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using the results above we can get (19). ", "page_idx": 19}, {"type": "text", "text": "Finally we are going to establish (21). Recall the definition of $G_{j}(a,b,z)$ which has four terms. For the third and fourth terms, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~|-\\langle\\theta_{a}^{*}-\\hat{\\theta}_{b}(z),(\\hat{\\Sigma}(z))^{-1}(\\theta_{a}^{*}-\\hat{\\theta}_{b}(z))\\rangle+\\langle\\theta_{a}^{*}-\\hat{\\theta}_{b}(z^{*}),(\\hat{\\Sigma}(z))^{-1}(\\theta_{a}^{*}-\\hat{\\theta}_{b}(z^{*}))\\rangle|}\\\\ &{\\preceq\\|\\hat{\\theta}_{b}(z)-\\hat{\\theta}_{b}(z^{*})\\|^{2}+\\|\\hat{\\theta}_{b}(z)-\\hat{\\theta}_{b}(z^{*})\\|\\|\\theta_{a}^{*}-\\theta_{b}^{*}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|-\\langle\\theta_{a}^{*}-\\hat{\\theta}_{b}(z^{*}),(\\hat{\\Sigma}(z))^{-1}(\\theta_{a}^{*}-\\hat{\\theta}_{b}(z^{*}))\\rangle+\\langle\\theta_{a}^{*}-\\hat{\\theta}_{b}(z^{*}),(\\hat{\\Sigma}(z^{*}))^{-1}(\\theta_{a}^{*}-\\hat{\\theta}_{b}(z^{*}))\\rangle|}\\\\ &{\\preceq\\|\\theta_{a}^{*}-\\theta_{b}^{*}\\|^{2}\\|(\\hat{\\Sigma}(z))^{-1}-(\\hat{\\Sigma}(z^{*}))^{-1}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We can easily verify that the other two terms are smaller than the above two terms. Then, by using (42) and (23), we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{|G_{j}(z_{j}^{*},b,z)|}{\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle}}\\\\ &{\\preceq\\frac{\\|\\hat{\\theta}_{b}(z)-\\hat{\\theta}_{b}(z^{*})\\|^{2}+\\|\\hat{\\theta}_{b}(z)-\\hat{\\theta}_{b}(z^{*})\\|\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|+\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|^{2}\\|(\\hat{\\Sigma}(z))^{-1}-(\\hat{\\Sigma}(z^{*}))^{-1}\\|}{\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|^{2}}}\\\\ &{\\preceq\\frac{k\\tau}{n}+\\frac{k}{\\Delta}\\sqrt{\\frac{\\tau}{n}}+\\frac{k d\\sqrt{\\tau}}{n\\Delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma A.5. With the same conditions as in Theorem 2.2, for any $\\delta=o(1)$ ,wehave ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\xi_{i d e a l}(\\delta)\\leq n\\exp\\biggl(-(1+o(1))\\frac{S N R^{2}}{8}\\biggr)\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with probability at least $1-n^{-C^{\\prime}}-\\exp(-S N R)$ ", "page_idx": 20}, {"type": "text", "text": "Proof. Under the conditions of Theorem 2.2, the inequalities (33)-(38) hold with probability at least $1-n^{-C^{\\prime}}$ . In the remaining proof, we will work on the event these inequalities hold. Recall the definitionof $\\xi_{\\mathrm{ideal}}$ .Wecanwrite ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{loat}(\\delta)=\\frac{\\displaystyle\\sum_{j=1}^{N}\\sum_{k\\in[\\delta]\\backslash\\{\\zeta_{j}^{k}\\}}\\|\\theta_{z_{j}^{k}}^{k}-\\theta_{\\theta_{k}}^{k}\\|^{2}\\mathbb{I}\\bigg\\{\\epsilon_{\\theta},(\\hat{\\Sigma}(z^{*}))^{-1}(\\hat{\\theta}_{z_{j}^{k}}(z^{*})-\\hat{\\theta}_{b}(z^{*}))\\bigg\\}\\leq-\\frac{1-\\delta}{2}(\\theta_{z_{j}^{k}}^{*}-\\theta_{\\theta_{k}}^{*},(\\Sigma)^{-1}(\\theta_{z_{j}^{k}}^{*}-\\theta_{\\theta_{k}}^{*})\\mathbb{I}(z^{k})\\bigg)}}\\\\ &{\\leq\\displaystyle\\sum_{j=1}^{N}\\sum_{k\\in[\\delta]\\backslash\\{\\zeta_{j}^{k}\\}}\\|\\theta_{z_{j}^{k}}^{*}-\\theta_{\\theta_{k}}^{*}\\|^{2}\\mathbb{I}\\bigg\\{\\epsilon_{\\theta},(\\Sigma)^{-1}(\\theta_{z_{j}^{k}}^{*}-\\theta_{\\theta_{k}}^{*})\\leq-\\frac{1-\\delta}{2}(\\theta_{z_{j}^{k}}^{*}-\\theta_{\\theta_{k}}^{*},(\\Sigma)^{-1}(\\theta_{z_{j}^{k}}^{*}-\\theta_{\\theta_{k}}^{*})\\mathbb{I}(z^{k})\\bigg)}\\\\ &{+\\displaystyle\\sum_{j=1}^{N}\\sum_{k\\in[\\delta]\\backslash\\{\\zeta_{j}^{k}\\}}\\|\\theta_{z_{j}^{k}}^{*}-\\theta_{\\theta_{k}}^{*}\\|^{2}\\mathbb{I}\\bigg\\{\\epsilon_{\\theta},(\\hat{\\Sigma}(z^{*}))^{-1}(\\hat{\\theta}_{z_{j}^{k}}^{*}-\\theta_{\\theta_{k}}^{*})\\bigg\\}\\leq-\\frac{\\displaystyle\\tilde{\\delta}}{6}(\\theta_{z_{j}^{k}}^{*}-\\theta_{\\theta_{k}}^{*},(\\Sigma^{*})^{-1}(\\theta_{z_{j}^{k}}^{*}-\\theta_{\\theta_{k}}^{*})\\mathbb{I}(z^{k})\\bigg)}\\\\ &{+\\displaystyle\\sum_{j=\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\bar{\\delta}=\\bar{\\delta}_{n}$ is some sequence to be chosen later. We bound the four terms sequentially. Suppose $\\epsilon_{j}=(\\Sigma^{*})^{1/2}w_{j}$ where $w_{j}\\overset{i n d}{\\sim}\\mathcal{N}(0,I_{d})$ .By (22), we know ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M_{2}\\leq\\displaystyle\\sum_{j=1}^{n}\\displaystyle\\sum_{b\\in[k]\\setminus\\{z_{j}^{*}\\}}\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|^{2}\\mathbb{I}\\left\\{\\frac{\\bar{\\delta}}{6\\lambda_{\\operatorname*{max}}}\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|^{2}\\leq\\lambda_{\\operatorname*{max}}\\|w_{j}\\|\\|(\\hat{\\Sigma}(z^{*}))^{-1}-(\\Sigma^{*})^{-1}\\|\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|^{2}\\right\\}}\\\\ &{\\quad\\leq\\displaystyle\\sum_{j=1}^{n}\\displaystyle\\sum_{b\\in[k]\\setminus\\{z_{j}^{*}\\}}\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|^{2}\\mathbb{I}\\left\\{C\\bar{\\delta}\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|\\sqrt{\\frac{n}{d+\\log n}}\\leq\\|w_{j}\\|\\right\\}}\\\\ &{\\quad\\leq\\displaystyle\\sum_{j=1}^{n}\\displaystyle\\sum_{b\\in[k]\\setminus\\{z_{j}^{*}\\}}\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|^{2}\\mathbb{I}\\left\\{C\\bar{\\delta}^{2}\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|^{2}\\frac{n}{d+\\log n}-2d\\leq\\|w_{j}\\|^{2}-2d\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $C$ is a constant which may vary from line by line. Recall that $\\begin{array}{r}{k d=O(\\sqrt{n}),\\operatorname*{min}_{a\\neq b}\\|\\theta_{a}^{*}-}\\end{array}$ $\\theta_{b}^{*}\\|\\rightarrow\\infty$ , and $\\Delta/k\\rightarrow\\infty$ by assumption. Let $n^{-\\frac{1}{4}}=o(\\bar{\\delta})$ . Using the $\\chi^{2}$ tail probability in Lemma C.1, we have for any $a\\neq b\\in[k]$ \uff0c ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}M_{2}\\leq\\sum_{j=1}^{n}\\sum_{b\\in[k]\\setminus\\{z_{j}^{*}\\}}\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|^{2}\\exp\\Bigl(-C\\bar{\\delta}^{2}\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|^{2}\\sqrt{n}\\Bigr)\\leq n\\exp\\biggl(-(1+o(1))\\frac{S\\mathrm{NR}^{2}}{8}\\biggr)\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We can obtain similar bounds on $M_{3}$ and $M_{4}$ by using (41). For $M_{1}$ , the Gaussian tail bound leads to the inequality ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{P}\\bigg\\{\\langle\\epsilon_{j},(\\Sigma^{*})^{-1}(\\theta_{a}^{*}-\\theta_{b}^{*})\\rangle\\leq-\\frac{1-\\delta-\\bar{\\delta}}{2}\\langle\\theta_{a}^{*}-\\theta_{b}^{*},(\\Sigma^{*})^{-1}(\\theta_{a}^{*}-\\theta_{b}^{*})\\rangle\\bigg\\}}\\\\ &{=\\,\\mathbb{P}\\bigg\\{\\langle w_{j},(\\Sigma^{*})^{-1/2}(\\theta_{a}^{*}-\\theta_{b}^{*})\\rangle\\leq-\\frac{1-\\delta-\\bar{\\delta}}{2}\\langle\\theta_{a}^{*}-\\theta_{b}^{*},(\\Sigma^{*})^{-1}(\\theta_{a}^{*}-\\theta_{b}^{*})\\rangle\\bigg\\}}\\\\ &{\\leq\\,\\exp\\!\\left(\\!-\\frac{(1-\\delta-\\bar{\\delta})^{2}}{8}\\langle\\theta_{a}^{*}-\\theta_{b}^{*},(\\Sigma^{*})^{-1}(\\theta_{a}^{*}-\\theta_{b}^{*})\\rangle\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}M_{1}\\leq\\displaystyle\\sum_{j=1}^{n}\\sum_{b\\in[k]\\setminus\\lbrace z_{j}^{*}\\rbrace}\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|^{2}\\exp\\left(-\\frac{(1-\\delta-\\bar{\\delta})^{2}}{8}\\langle\\theta_{a}^{*}-\\theta_{b}^{*},(\\Sigma^{*})^{-1}(\\theta_{a}^{*}-\\theta_{b}^{*})\\rangle\\right)}\\\\ &{\\qquad\\leq n\\exp\\left(-(1+o(1))\\frac{\\mathrm{SNR}^{2}}{8}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Overall, we have $\\begin{array}{r}{\\mathbb{E}\\xi_{\\mathrm{ideal}}\\preceq n\\exp\\Bigl(-(1+o(1))\\frac{\\mathrm{SNR}^{2}}{8}\\Bigr)}\\end{array}$ . By the Markov's inequality, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\xi_{\\mathrm{ideal}}(\\delta_{n})\\geq\\mathbb{E}\\xi_{\\mathrm{ideal}}\\exp(\\mathrm{SNR}))\\leq\\exp(-\\mathrm{SNR}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In other words, with probability at least $1-\\exp(-\\mathrm{SNR})$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\xi_{\\mathrm{ideal}}(\\delta_{n})\\leq\\mathbb{E}\\xi_{\\mathrm{ideal}}(\\delta_{n})\\exp(\\mathrm{SNR})\\leq n\\exp\\biggl(-(1+o(1))\\frac{\\mathrm{SNR}^{2}}{8}\\biggr)\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof of Theorem 2.2. By Lemmas ${\\mathrm{A.3~-~A.5}}$ , we have that Conditions 1 - 5 are satisfied with probability at least $1-\\bar{\\eta^{}}-n^{-1}-\\exp(-\\mathrm{SNR})$ . Then applying Lemma A.3, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\ell(z^{(t)},z^{*})\\leq n\\exp\\biggl(-(1+o(1))\\frac{\\mathrm{SNR}^{2}}{8}\\biggr)+\\frac{1}{2}\\ell(z^{(t-1)},z^{*}),\\quad\\mathrm{for~all}\\;t\\geq1.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By (16), and since there exists a constant $C$ such that $\\Delta\\leq C{\\bf S}{\\bf N}{\\bf R}$ , we can conclude ", "page_idx": 21}, {"type": "equation", "text": "$$\nh(z^{(t)},z^{*})\\leq\\exp\\left(-(1+o(1))\\frac{\\mathrm{SNR}^{2}}{8}\\right)+2^{-t},\\quad\\mathrm{for~all~}t\\geq1.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Notice that $h(\\cdot,\\cdot)$ takes value in the set $\\{j/n:j\\in[n]\\cup\\{0\\}\\}$ , the term $2^{-t}$ in the above inequality should be negligible as long as $2^{-t}=o(\\bar{n}^{-1})$ . Thus, we can claim ", "page_idx": 21}, {"type": "equation", "text": "$$\nh(z^{(t)},z^{*})\\leq\\exp\\!\\left(-(1+o(1))\\frac{\\mathsf{S N R}^{2}}{8}\\right),\\quad\\mathrm{for~all~}t\\geq\\log n.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "B Proofs in Section 3 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "B.1Proofs for the Lower Bound ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof of Lemma 3.1. The Neyman-Pearson lemma tells us the likelihood ratio test $\\phi$ is the optimal procedure. Following the proof of Lemma A.1, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{\\mathbb{H}_{0}}(\\phi=1)+\\mathbb{P}_{\\mathbb{H}_{1}}(\\phi=0)=\\mathbb{P}(\\epsilon\\in B_{1,2})+\\mathbb{P}(\\epsilon\\in B_{2,1})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\exp\\biggl(-\\frac{1+o(1)}{8}\\mathrm{SNR}_{1,2}^{'2}\\biggr)+\\exp\\biggl(-\\frac{1+o(1)}{8}\\mathrm{SNR}_{2,1}^{'2}\\biggr)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last inequality is by Lemma C.10. ", "page_idx": 21}, {"type": "text", "text": "Proof of Theorem 3.1. The proof is identical to the proof of Theorem 2.1 and is omitted here. ", "page_idx": 21}, {"type": "text", "text": "B.2 Proofs for the Upper Bound ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We adopt a similar proof idea as in Section 2 for Model 1. We first present an error decomposition for the one-step analysis for Algorithm 2. In Lemma B.1, we show the loss decays after a one-step iteration under Conditions 6 - 11. Then in Lemma B.2 we extend the result to multiple iterations, under two extra Conditions 12 - 13. Finally, we show that all the conditions are satisfied with high probability and thus prove Theorem 3.2. ", "page_idx": 21}, {"type": "text", "text": "In the statement of Theorem 3.2, we assume $\\mathrm{max}_{a,b\\in[k]}\\,\\lambda_{d}(\\Sigma_{a}^{*})/\\lambda_{1}(\\Sigma_{b}^{*})=O(1)$ for the covariance matrices $\\{\\Sigma_{a}^{*}\\}_{a\\in[k]}$ .Without loss of generality, we can replace it by assuming $\\{\\Sigma_{a}^{*}\\}_{a\\in[k]}$ satisfy ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}\\leq\\operatorname*{min}_{a\\in[k]}\\lambda_{1}(\\Sigma_{a}^{*})\\leq\\operatorname*{max}_{a\\in[k]}\\lambda_{d}(\\Sigma_{a}^{*})\\leq\\lambda_{\\operatorname*{max}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\lambda_{\\operatorname*{min}},\\lambda_{\\operatorname*{max}}~>~0$ are two constants. This is due to the scaling properties of the normal distributions. The reasoning is the same as that in (15) for Model 1 and is omitted here. For the remainder of this section, we will assume that (24) holds for the covariance matrices. ", "page_idx": 22}, {"type": "text", "text": "Error Decomposition for the One-step Analysis: Consider an arbitrary $z\\,\\in\\,[k]^{n}$ .Apply (12), (13), and (14) on $z$ to obtain $\\{\\hat{\\theta}_{a}(z)\\}_{a\\in[k]}$ \uff0c $\\{\\hat{\\Sigma}_{a}(z)\\}_{a\\in[k]}$ , and $\\hat{z}(z)$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\theta}_{a}(z)=\\displaystyle\\frac{\\sum_{j\\in[n]}Y_{j}\\mathbb{I}\\left\\{z_{j}=a\\right\\}}{\\sum_{j\\in[n]}\\mathbb{I}\\left\\{z_{j}=a\\right\\}},}\\\\ &{\\hat{\\Sigma}_{a}(z)=\\displaystyle\\frac{\\sum_{j\\in[n]}(Y_{j}-\\hat{\\theta}_{a}(z))(Y_{j}-\\hat{\\theta}_{a}(z))^{T}\\mathbb{I}\\left\\{z_{j}=a\\right\\}}{\\sum_{j\\in[n]}\\mathbb{I}\\left\\{z_{j}=a\\right\\}},\\quad\\forall a\\in[k],}\\\\ &{\\hat{z}_{j}(t)=\\displaystyle\\operatorname*{argmin}_{a\\in[k]}(Y_{j}-\\hat{\\theta}_{a}(z))^{T}(\\hat{\\Sigma}_{a}(a))^{-1}(Y_{j}-\\hat{\\theta}_{a}(z))+\\log|\\hat{\\Sigma}_{a}(z)|,\\quad\\forall j\\in[n].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For simplicity, we denote $\\hat{z}$ as shorthand for $\\hat{z}(z)$ . Let $j\\in[n]$ be an arbitrary index with $z_{j}^{*}=a$ According to (14), $z_{j}^{*}$ will be incorrectly estimated after one iteration in $\\hat{z}$ $a\\ne\\mathrm{argmin}_{b\\in[k]}(Y_{j}-$ $\\hat{\\theta}_{b}(z))^{T}(\\hat{\\Sigma}_{b}(z))^{-1}(\\bar{Y}_{j}-\\hat{\\theta}_{b}(z))+\\log\\vert\\hat{\\Sigma}_{b}(z)\\vert$ . That is, t is important to analyze the event ", "page_idx": 22}, {"type": "equation", "text": "$$\nY_{j}-\\hat{\\theta}_{b}(z),(\\hat{\\Sigma}_{b}(z))^{-1}(Y_{j}-\\hat{\\theta}_{b}(z))\\rangle+\\log|\\hat{\\Sigma}_{b}(z)|\\le\\langle Y_{j}-\\hat{\\theta}_{a}(z),(\\hat{\\Sigma}_{a}(z))^{-1}(Y_{j}-\\hat{\\theta}_{a}(z))\\rangle+\\log|\\hat{\\Sigma}_{b}(z)|\\le0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for any $b\\in[k]\\setminus\\{a\\}$ . After some rearrangements, we can see (25) is equivalent to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad\\langle\\epsilon_{j},(\\hat{\\Sigma}_{b}(z^{*}))^{-1}(\\theta_{a}^{*}-\\hat{\\theta}_{b}(z^{*}))\\rangle-\\langle\\epsilon_{j},(\\hat{\\Sigma}_{a}(z^{*}))^{-1}(\\theta_{a}^{*}-\\hat{\\theta}_{a}(z^{*}))\\rangle}\\\\ &{\\quad+\\displaystyle\\frac{1}{2}\\langle\\epsilon_{j},((\\hat{\\Sigma}_{b}(z^{*}))^{-1}-(\\hat{\\Sigma}_{a}(z^{*}))^{-1})\\epsilon_{j}\\rangle-\\displaystyle\\frac{1}{2}\\log|\\Sigma_{a}^{*}|+\\displaystyle\\frac{1}{2}\\log|\\Sigma_{b}^{*}|}\\\\ &{\\leq-\\displaystyle\\frac{1}{2}\\langle\\theta_{a}^{*}-\\theta_{b}^{*},(\\Sigma_{b}^{*})^{-1}(\\theta_{a}^{*}-\\theta_{b}^{*})\\rangle}\\\\ &{\\quad+F_{j}(a,b,z)+Q_{j}(a,b,z)+G_{j}(a,b,z)+H_{j}(a,b,z)+K_{j}(a,b,z)+L_{j}(a,b,z),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{j}(a,b,z)=\\langle\\epsilon_{j},(\\hat{\\Sigma}_{b}(z))^{-1}(\\hat{\\theta}_{b}(z)-\\hat{\\theta}_{b}(z^{*}))\\rangle-\\langle\\epsilon_{j},(\\hat{\\Sigma}_{a}(z))^{-1}(\\hat{\\theta}_{a}(z)-\\hat{\\theta}_{a}(z^{*}))\\rangle}\\\\ &{\\quad\\quad\\quad\\quad-\\langle\\epsilon_{j},((\\hat{\\Sigma}_{b}(z))^{-1}-(\\hat{\\Sigma}_{b}(z^{*}))^{-1})({\\theta}_{a}^{*}-\\hat{\\theta}_{b}(z^{*}))\\rangle}\\\\ &{\\quad\\quad\\quad\\quad+\\langle\\epsilon_{j},((\\hat{\\Sigma}_{a}(z))^{-1}-(\\hat{\\Sigma}_{a}(z^{*}))^{-1})({\\theta}_{a}^{*}-\\hat{\\theta}_{a}(z^{*}))\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\nQ_{j}(a,b,z)=-\\frac{1}{2}\\langle\\epsilon_{j},((\\hat{\\Sigma}_{b}(z))^{-1}-(\\hat{\\Sigma}_{b}(z^{*}))^{-1})\\epsilon_{j}\\rangle+\\frac{1}{2}\\langle\\epsilon_{j},((\\hat{\\Sigma}_{a}(z))^{-1}-(\\hat{\\Sigma}_{a}(z^{*}))^{-1})\\epsilon_{j}\\rangle,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\gamma}_{j}(a,b,z)=\\frac{1}{2}\\langle\\theta_{a}^{*}-\\hat{\\theta}_{a}(z),(\\hat{\\Sigma}_{a}(z))^{-1}(\\theta_{a}^{*}-\\hat{\\theta}_{a}(z))\\rangle-\\frac{1}{2}\\langle\\theta_{a}^{*}-\\hat{\\theta}_{a}(z^{*}),(\\hat{\\Sigma}_{a}(z))^{-1}(\\theta_{a}^{*}-\\hat{\\theta}_{a}(z^{*}))\\rangle}\\\\ &{\\hphantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\\\ &{\\hphantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\\\ &{\\hphantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\\\ &{\\hphantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\\\ &{\\hphantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\\\ &{\\hphantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}-\\frac{1}{2}\\langle\\theta_{a}^{x}-\\hat{\\theta}_{b}(z),(\\hat{\\Sigma}_{b}(z))^{-1}(\\theta_{a}^{*}-\\hat{\\theta}_{b}(z))\\rangle+\\frac{1}{2}\\langle\\theta_{a}^{*}-\\hat{\\theta}_{b}(z^{*}),(\\hat{\\Sigma}_{b}(z))^{-1}(\\theta_{a}^{*}-\\hat{\\theta}_{b}(z^{*}))\\rangle}\\\\ &{\\hphantom{x x x x x x x x x x x x}}\\\\ &{\\hphantom{x x x x x x x x x x x x x x x x}}\\\\ &{\\hphantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\\\ &{\\hphantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\\\ &{\\hphantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\\\ &{\\hphantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\\\ &{\\hphantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\\\ &{\\hphantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\nK_{j}(a,b,z)=\\frac{1}{2}(\\log|\\hat{\\Sigma}_{a}(z)|-\\log|\\hat{\\Sigma}_{a}(z^{*})|)-\\frac{1}{2}(\\log|\\hat{\\Sigma}_{b}(z)|-\\log|\\hat{\\Sigma}_{b}(z^{*})|),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\nL_{j}(a,b,z)=\\frac{1}{2}(\\log|\\hat{\\Sigma}_{a}(z^{*})|-\\log|\\Sigma_{a}^{*}|)-\\frac{1}{2}(\\log|\\hat{\\Sigma}_{b}(z^{*})|-\\log|\\Sigma_{b}^{*}|).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Among these terms, $F_{j},G_{j},H_{j}$ are nearly identical to their counterparts in Section A.2.2 with $\\hat{\\Sigma}(z)$ replaced by $\\hat{\\Sigma}_{a}(z)$ or $\\hat{\\Sigma}_{b}(z)$ . There are three extra terms not appearing in Section A.2.2: $Q_{j}$ is a quadratic term of $\\epsilon_{j}$ and $K_{j},L_{j}$ are terms involving matrix determinants. ", "page_idx": 23}, {"type": "text", "text": "Conditions and Guarantees for One-step Analysis.  To establish the guarantee for the one-step analysis, we first give several conditions on the error terms. ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Condition 6. Assume that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\substack{\\{z:\\ell(z,z^{*})\\leq\\tau\\}\\,j\\in[n]\\,b\\in[k]\\backslash\\{z_{j}^{*}\\}}}\\frac{|H_{j}(z_{j}^{*},b,z)|}{\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma_{b}^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle}\\leq\\frac{\\delta}{12}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "holds with probability at least $1-\\eta_{1}$ for some $\\tau,\\delta,\\eta_{1}>0$ ", "page_idx": 23}, {"type": "text", "text": "Condition 7. Assume that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\{z:\\ell(z,z^{*})\\leq\\tau\\}}\\sum_{j=1}^{n}\\operatorname*{max}_{b\\in[k]\\setminus\\{z_{j}^{*}\\}}\\frac{F_{j}(z_{j}^{*},b,z)^{2}\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|^{2}}{\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma_{b}^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle^{2}\\ell(z,z^{*})}\\leq\\frac{\\delta^{2}}{288}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "holds with probability at least $1-\\eta_{2}$ for some $\\tau,\\delta,\\eta_{2}>0$ ", "page_idx": 23}, {"type": "text", "text": "Condition 8. Assume that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\substack{\\{z:\\ell(z,z^{*})\\leq\\tau\\}\\,j\\in[n]\\,b\\in[k]\\backslash\\{z_{j}^{*}\\}}}\\frac{|G_{j}(z_{j}^{*},b,z)|}{\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma_{b}^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle}\\leq\\frac{\\delta}{12}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "holds with probability at least $1-\\eta_{3}$ for some $\\tau,\\delta,\\eta_{3}>0$ ", "page_idx": 23}, {"type": "text", "text": "Condition 9. Assume that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\{z:\\ell(z,z^{*})\\leq\\tau\\}}\\sum_{j=1}^{n}\\operatorname*{max}_{b\\in[k]\\setminus\\{z_{j}^{*}\\}}\\frac{Q_{j}(z_{j}^{*},b,z)^{2}\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|^{2}}{\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma_{b}^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle^{2}\\ell(z,z^{*})}\\leq\\frac{\\delta^{2}}{288}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "holds with probability at least $1-\\eta_{4}$ for some $\\tau,\\delta,\\eta_{4}>0$ ", "page_idx": 23}, {"type": "text", "text": "Condition 10. Assume that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\{z:\\ell(z,z^{*})\\leq\\tau\\}}\\sum_{j=1}^{n}\\operatorname*{max}_{b\\in[k]\\setminus\\{z_{j}^{*}\\}}\\frac{K_{j}(z_{j}^{*},b,z)^{2}\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|^{2}}{\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma_{b}^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle^{2}\\ell(z,z^{*})}\\leq\\frac{\\delta^{2}}{288}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "holds with probability at least $1-\\eta_{5}$ for some $\\tau,\\delta,\\eta_{5}>0$ ", "page_idx": 23}, {"type": "text", "text": "Condition 11. Assume that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\substack{\\{z:\\ell(z,z^{*})\\leq\\tau\\}\\,j\\in[n]\\,b\\in[k]\\backslash\\{z_{j}^{*}\\}}}\\frac{|L_{j}(z_{j}^{*},b,z)|}{\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma_{b}^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle}\\leq\\frac{\\delta}{12}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "holds with probability at least $1-\\eta_{6}$ for some $\\tau,\\delta,\\eta_{6}>0$ ", "page_idx": 23}, {"type": "text", "text": "We next define a quantity referred to as the ideal error, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\operatorname{ideal}(\\delta)=\\sum_{j=1}^{n}\\sum_{b\\in[k]\\backslash\\{z_{j}^{*}\\}}\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|^{2}\\mathbb{I}\\{\\langle\\epsilon_{j},(\\hat{\\Sigma}_{b}(z^{*}))^{-1}(\\theta_{a}^{*}-\\hat{\\theta}_{b}(z^{*}))\\rangle-\\langle\\epsilon_{j},(\\hat{\\Sigma}_{a}(z^{*}))^{-1}(\\theta_{a}^{*}-\\hat{\\theta}_{a}(z^{*}))\\rangle\\}}}\\\\ {{\\displaystyle+\\frac{1}{2}\\langle\\epsilon_{j},((\\hat{\\Sigma}_{b}(z^{*}))^{-1}-(\\hat{\\Sigma}_{a}(z^{*}))^{-1})\\epsilon_{j}\\rangle-\\frac{1}{2}\\log|\\Sigma_{a}^{*}|+\\frac{1}{2}\\log|\\Sigma_{b}^{*}|\\leq-\\frac{1-\\delta}{2}\\langle\\theta_{a}^{*}-\\theta_{b}^{*},(\\Sigma_{b}^{*})^{-1}(\\theta_{a}^{*}-\\hat{\\theta}_{b}^{*})\\rangle}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma B.1. Assumes Conditions $6\\cdot1l$ hold for some $\\tau,\\delta,\\eta_{1},\\dots,\\eta_{6}>0$ . We then have ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\mathbb P}\\bigg(\\ell(\\hat{z},z^{*})\\leq\\xi_{i d e a l}(\\delta)+\\frac{1}{2}\\ell(z,z^{*})f o r\\,a n y\\,z\\in[k]^{n}\\;s u c h\\,t h a t\\,\\ell(z,z^{*})\\leq\\tau\\bigg)\\geq1-\\eta,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\textstyle\\eta=\\sum_{i=1}^{6}\\eta_{i}$ ", "page_idx": 23}, {"type": "text", "text": "Proof. The proof of this lemma is quite similar to the proof of Lemma A.2. The additional terms $Q_{j}$ and $K_{j}$ can be handled in the same way as $F_{j}$ while $L_{j}$ can be handled similarly to $H_{j}$ . We omit the details here. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Conditions and Guarantees for Multiple Iterations. In the above, we establish a statistical guarantee for the one-step analysis. Now we will extend the result to multiple iterations. That is, starting from some initialization $z^{(0)}$ , we will characterize how the losses $\\bar{\\ell}(z^{(0)},z^{*}),\\,\\ell(z^{(1)},z^{*})$ $\\ell(z^{(2)},z^{*})$ ,..., decay. We impose conditions on $\\xi_{\\mathrm{ideal}}(\\delta)$ and the initialization $z^{(0)}$ ", "page_idx": 24}, {"type": "text", "text": "Condition 12. Assume that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\xi_{i d e a l}(\\delta)\\le\\frac{\\tau}{2}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "holds with probability at least $1-\\eta_{7}$ for some T, o, n7 > 0. ", "page_idx": 24}, {"type": "text", "text": "Finally, we need a condition on the initialization. ", "page_idx": 24}, {"type": "text", "text": "Condition 13. Assume that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\ell(z^{(0)},z^{*})\\leq\\tau\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "holds with probability at least $1-\\eta_{8}$ for some T, n8 > 0. ", "page_idx": 24}, {"type": "text", "text": "With these conditions satisfied, we can give a lemma that shows the convergence of our algorithm. Lemma B.2. Assumes Conditions $6\\cdot13$ holdforsome $\\tau,\\delta,\\eta_{1},\\dots,\\eta_{8}>0$ Wethenhave ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\ell(z^{(t)},z^{*})\\leq\\xi_{i d e a l}(\\delta)+{\\frac{1}{2}}\\ell(z^{(t-1)},z^{*})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for all $t\\geq1$ , with probability at least $1-\\eta,$ where $\\textstyle\\eta=\\sum_{i=1}^{8}\\eta_{i}$ ", "page_idx": 24}, {"type": "text", "text": "Proof. The proof of this lemma is the same as the proof of Lemma A.3. ", "page_idx": 24}, {"type": "text", "text": "With-high-probability Results for the Conditions and Proof of Theorem 3.2.  Lemma B.3 and Lemma B.4 are the counterparts of Lemmas A.4 and A.5 in Appendix A.2.2. Recall that (24) is assumed. By Lemma C.10, we have $\\Delta$ is of the same order as $\\mathrm{SN}\\bar{\\mathbf{R^{\\prime}}}$ , which will play a similar role as (18) in Section A.2.2. ", "page_idx": 24}, {"type": "text", "text": "Lemma B.3 and Lemma B.4 are counterparts of Lemmas A.4 and A.5 in Section A.2.2. The first lemma is for Conditions 6-11, providing upper bounds for the quantities involved in these conditions, showingthat $\\delta$ can be taken as some $o(1)$ term. The second lemma shows that for any $\\delta=o(1)$ $\\xi_{\\mathrm{ideal}}(\\delta)$ is upper bounded by the desired minimax rate multiplied by the sample size $n$ ", "page_idx": 24}, {"type": "text", "text": "Lemma B.3. Under the same conditions as in Theorem 3.2, for any constant $C^{\\prime}>0,$ thereexists someconstant $C>0$ onlydependingon $\\alpha,C^{\\prime},\\lambda_{\\operatorname*{min}},\\lambda_{\\operatorname*{max}}$ suchthat ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\left\\{z:\\ell(z,z^{*})\\leq r\\right\\}}{\\operatorname*{max}}\\underset{j\\in[n]}{\\operatorname*{max}}\\underset{b\\in[k]\\setminus\\{z_{j}^{*}\\}}{\\operatorname*{max}}\\frac{|H_{j}(z_{j}^{*},b,z)|}{\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma_{b}^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle}\\leq C\\sqrt{\\frac{k(d+\\log n)}{n}}\\quad\\quad(2\\ell\\neq d),}\\\\ &{\\underset{\\left\\{z:\\ell(z,z^{*})\\leq r\\right\\}}{\\operatorname*{max}}\\underset{b\\in[k]\\setminus\\{z_{j}^{*}\\}}{\\operatorname*{max}}\\frac{F_{j}(z_{j}^{*},b,z)^{2}\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|^{2}}{\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma_{b}^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle^{2}\\ell(z,z^{*})}\\leq C k^{3}\\bigg(\\frac{\\tau}{n}+\\frac{1}{\\Delta^{2}}+\\frac{d^{2}}{n\\Delta^{2}}\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\substack{\\{z:\\ell(z,z^{*})\\leq\\tau\\}\\,j\\in[n]\\,b\\in[k]\\backslash\\{z_{j}^{*}\\}}}\\frac{|G_{j}(z_{j}^{*},b,z)|}{\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma_{b}^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle}\\leq C k\\bigg(\\frac{\\tau}{n}+\\frac{1}{\\Delta}\\sqrt{\\frac{\\tau}{n}}+\\frac{d\\sqrt{\\tau}}{n\\Delta}\\bigg)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\{z:\\ell(z,z^{*})\\leq\\tau\\}}\\sum_{j=1}^{n}\\operatorname*{max}_{b\\in[k]\\setminus\\{z_{j}^{*}\\}}\\frac{Q_{j}(z_{j}^{*},b,z)^{2}\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|^{2}}{\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma_{b}^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle^{2}\\ell(z,z^{*})}\\leq C\\frac{k^{3}d^{2}}{\\Delta^{2}}\\bigg(\\frac{\\tau}{n}+\\frac{1}{\\Delta^{2}}+\\frac{d^{2}}{n\\Delta^{2}}\\bigg)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\{z:\\ell(z,z^{*})\\leq\\tau\\}}\\sum_{j=1}^{n}\\operatorname*{max}_{b\\in[k]\\backslash\\{z_{j}^{*}\\}}\\frac{K_{j}(z_{j}^{*},b,z)^{2}\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|^{2}}{\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma_{b}^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle^{2}\\ell(z,z^{*})}\\leq C\\frac{k^{3}d^{2}}{\\Delta^{2}}\\bigg(\\frac{\\tau}{n}+\\frac{1}{\\Delta^{2}}+\\frac{d^{2}}{n\\Delta^{2}}\\bigg)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\{z:\\ell(z,z^{*})\\leq\\tau\\}}\\operatorname*{max}_{j\\in[n]}\\operatorname*{max}_{b\\in[k]\\backslash\\{z_{j}^{*}\\}}\\frac{|L_{j}(z_{j}^{*},b,z)|}{\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma_{b}^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle}\\leq C\\frac{d}{\\Delta^{2}}\\sqrt{\\frac{k(d+\\log n)}{n}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with probability at least $\\begin{array}{r}{1-n^{-C^{\\prime}}-\\frac{4}{n d}}\\end{array}$ .As a result, Conditions 6-11 hold for some $\\delta=o(1)$ ", "page_idx": 24}, {"type": "text", "text": "Proof. Under the conditions of Theorem 3.2, the inequalities (33)-(38) hold with probability at least $1-\\stackrel{-C^{\\prime}}{n}^{-C^{\\prime}}$ . In the remaining proof, we will work on the event these inequalities hold. Hence, we can use the results from Lemma C.7 and C.8. Using the same arguments as in the proof of Lemma A.4, we can get (26), (27) and (28). ", "page_idx": 25}, {"type": "text", "text": "As for (29), we first use Lemma C.2 to have $\\begin{array}{r}{\\sum_{j=1}^{n}\\|\\epsilon_{j}\\|^{4}\\leq3n d}\\end{array}$ with probability at last $1-4/(n d)$ Then, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}{\\displaystyle\\sum_{i=1}^{n}\\operatorname*{max}_{b\\in[k]\\setminus\\{z_{j}^{*}\\}}\\frac{Q_{j}(z_{j}^{*},b,z)^{2}\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|^{2}}{\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma_{b}^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle^{2}\\ell(z,z^{*})}\\preceq}&{\\displaystyle\\sum_{j=1}^{n}\\sum_{b=1}^{k}\\frac{Q_{j}(z_{j}^{*},b,z)^{2}}{\\Delta^{2}\\ell(z,z^{*})}}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq}&{k\\displaystyle\\sum_{j=1}^{n}\\|\\epsilon_{j}\\|^{4}\\frac{\\operatorname*{max}_{a\\in[k]}\\|(\\hat{\\Sigma}_{a}(z))^{-1}-(\\hat{\\Sigma}_{a}(z^{*}))\\|}{\\Delta^{2}\\ell(z,z^{*})}}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\preceq}&{\\displaystyle\\frac{k^{3}d^{2}}{\\Delta^{2}}\\bigg(\\frac{\\tau}{n}+\\frac{1}{\\Delta^{2}}+\\frac{d^{2}}{n\\Delta^{2}}\\bigg)\\,,}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last inequality is due to (53) and the fact that $\\ell(z,z^{*})\\leq\\tau$ ", "page_idx": 25}, {"type": "text", "text": "Next for (30), notice that by (43), (44), and $\\mathrm{SNR^{\\prime}}\\to\\infty$ , we have for any $\\begin{array}{r}{1\\,\\leq\\,i\\,\\leq\\,d,\\;\\frac{\\lambda_{\\operatorname*{min}}}{2}\\;\\leq\\,}\\end{array}$ $\\lambda_{i}(\\hat{\\Sigma}_{a}(z^{*}))\\leq2\\lambda_{\\operatorname*{max}}$ and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|\\log(1+\\operatorname*{max}_{a\\in[k]}\\frac{\\|\\hat{\\Sigma}_{a}(z)-\\hat{\\Sigma}_{a}(z^{*})\\|_{2}}{\\lambda_{i}(\\hat{\\Sigma}_{a}(z^{*}))})\\right|\\leq\\left|\\log(1-\\operatorname*{max}_{a\\in[k]}\\frac{\\|\\hat{\\Sigma}_{a}(z)-\\hat{\\Sigma}_{a}(z^{*})\\|_{2}}{\\lambda_{i}(\\hat{\\Sigma}_{a}(z^{*}))})\\right|.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus by Weyl's inequality, we know ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{a\\in[k]}{\\operatorname*{max}}\\left|\\log|\\hat{\\Sigma}_{a}(z)|-\\log|\\hat{\\Sigma}_{a}(z^{*})|\\right|}\\\\ &{=\\underset{a\\in[k]}{\\operatorname*{max}}\\left|\\log\\frac{|\\hat{\\Sigma}_{a}(z)|}{|\\hat{\\Sigma}_{a}(z^{*})|}\\right|}\\\\ &{\\leq\\left|\\frac{\\hat{d}}{|\\sum_{i=1}^{d}}\\mathrm{log}(1-\\frac{\\operatorname*{max}_{a\\in[k]}\\,|\\hat{\\Sigma}_{a}(z)-\\hat{\\Sigma}_{a}(z^{*})||_{2}}{\\lambda_{i}(\\hat{\\Sigma}_{a}(z^{*}))})\\right|}\\\\ &{\\leq\\underset{i=1}{\\overset{d}{\\leq}}\\log\\left(1+\\underset{a\\in[k]}{\\operatorname*{max}}\\frac{|\\hat{\\Sigma}_{a}(z)-\\hat{\\Sigma}_{a}(z^{*})||_{2}}{\\lambda_{i}(\\hat{\\Sigma}_{a}(z^{*}))}+\\frac{\\operatorname*{max}_{a\\in[k]}\\,\\frac{|\\hat{\\Sigma}_{a}(z)-\\hat{\\Sigma}_{a}(z^{*})||_{2}^{2}}{\\lambda_{i}^{2}(\\hat{\\Sigma}_{a}(z^{*}))}}{1-\\operatorname*{max}_{a\\in[k]}\\frac{|\\hat{\\Sigma}_{a}(z)-\\hat{\\Sigma}_{a}(z^{*})||_{2}}{\\lambda_{i}(\\hat{\\Sigma}_{a}(z^{*}))}}\\right)}\\\\ &{\\leq\\underset{d\\in[k]}{\\operatorname*{max}}\\left(1-\\underset{a\\in[k]}{\\operatorname*{max}}\\frac{|\\hat{\\Sigma}_{a}(z)-\\hat{\\Sigma}_{a}(z^{*})||_{2}}{\\lambda_{i}(\\hat{\\Sigma}_{a}(z^{*}))}+\\frac{\\operatorname*{max}_{a\\in[k]}\\,\\frac{|\\hat{\\Sigma}_{a}(z)-\\hat{\\Sigma}_{a}(z^{*})||_{2}}{\\lambda_{i}(\\hat{\\Sigma}_{a}(z^{*}))}}{1-\\operatorname*{max}_{a\\in[k]}\\frac{|\\hat{\\Sigma}_{a}(z)-\\hat{\\Sigma}_{a}(z^{*})||_{2}}{\\lambda_{i}(\\hat{\\Sigma}_{a}(z^{* \n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last inequality is due to the fact that $\\lambda_{i}(\\hat{\\Sigma}_{a}(z^{*}))$ is at the constant rate, $\\|\\hat{\\Sigma}_{a}(z)\\textrm{--}$ $\\hat{\\Sigma}_{a}\\bigl(z^{*}\\bigr)\\|_{2}=o(1)$ and the inequality $\\log(1+x)\\leq x$ for any $x>0$ (32) yields to the inequality ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{j=1}^{n}\\operatorname*{max}_{b\\in[k]\\setminus\\{z_{j}^{*}\\}}\\frac{K_{j}(z_{j}^{*},b,z)^{2}\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\|^{2}}{\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma_{b}^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle^{2}\\ell(z,z^{*})}\\preceq}&{\\displaystyle\\sum_{j=1}^{n}\\frac{d^{2}\\operatorname*{max}_{a\\in[k]}\\|\\hat{\\Sigma}_{a}(z)-\\hat{\\Sigma}_{a}(z^{*})\\|^{2}}{\\Delta^{2}\\ell(z,z^{*})}}\\\\ &{\\displaystyle\\preceq\\ \\frac{k^{2}d^{2}}{\\Delta^{2}}\\bigg(\\frac{\\tau}{n}+\\frac{1}{\\Delta^{2}}+\\frac{d^{2}}{n\\Delta^{2}}\\bigg)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Finally for (31), by (43) and the similar argument as (32), we can get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a\\in[k]}\\left|\\log|\\hat{\\Sigma}_{a}(z^{*})|-\\log|\\Sigma_{a}^{*}|\\right|\\preceq d\\sqrt{\\frac{k(d+\\log n)}{n}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which implies (31). We complete the proof. ", "page_idx": 25}, {"type": "text", "text": "Lemma B.4. With the same conditions as Theorem 3.2, for any sequence $\\delta_{n}=o(1)$ wehave ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\xi_{i d e a l}(\\delta_{n})\\leq n\\exp\\biggl(-(1+o(1))\\frac{S N R^{\\prime2}}{8}\\biggr)\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "with probability at least $1-n^{-C^{\\prime}}-\\exp(-S N R^{\\prime})$ ", "page_idx": 25}, {"type": "text", "text": "Proof. Under the conditions of Theorem 3.2, the inequalities (33)-(38) hold with probability at least $1-\\stackrel{-C^{\\prime}}{n}^{-C^{\\prime}}$ . In the remaining proof, we will work on the event these inequalities hold. Similar to the proof of Lemma A.5, we have a decomposition $\\begin{array}{r}{\\xi_{\\mathrm{ideal}}\\leq\\sum_{i=1}^{6}M_{i}}\\end{array}$ where ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle M_{1}=\\sum_{j=1}^{n}\\sum_{b\\in[k]\\backslash\\{z_{j}^{*}\\}}\\left\\|{\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}}\\right\\|^{2}\\mathbb{I}\\bigg\\{\\langle\\epsilon_{j},(\\Sigma_{b}^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle+\\frac{1}{2}\\langle\\epsilon_{j},((\\Sigma_{b}^{*})^{-1}-(\\Sigma_{z_{j}^{*}}^{*})^{-1})\\epsilon_{j}\\rangle}}\\\\ {{\\displaystyle-\\,\\frac{1}{2}\\log|\\Sigma_{z_{j}^{*}}^{*}|+\\frac{1}{2}\\log|\\Sigma_{b}^{*}|\\leq-\\frac{1-\\delta-\\bar{\\delta}}{2}\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma_{b}^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "is the main term and ", "page_idx": 26}, {"type": "text", "text": "${\\cal M}_{2}=\\sum_{j=1}^{n}\\sum_{b\\in[k]\\setminus\\{z_{j}^{*}\\}}\\left\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\right\\|^{2}\\mathbb{I}\\bigg\\{\\langle\\epsilon_{j},((\\hat{\\Sigma}_{b}(z^{*}))^{-1}-(\\Sigma_{b}^{*})^{-1})(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle\\leq-\\frac{{\\bar{\\delta}}}{10}\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma_{b}^{*})^{-1}\\rangle\\bigg\\}.$ $U_{3}=\\sum_{j=1}^{n}\\sum_{b\\in[k]\\setminus\\{z_{j}^{*}\\}}\\left\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\right\\|^{2}\\mathbb{I}\\left\\{-\\langle\\epsilon_{j},(\\hat{\\Sigma}_{z_{j}^{*}}(z^{*}))^{-1}(\\theta_{z_{j}^{*}}^{*}-\\hat{\\theta}_{z_{j}^{*}}(z^{*}))\\rangle\\leq-\\frac{\\bar{\\delta}}{10}\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma_{b}^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\hat{\\theta}_{z_{j}^{*}}(z^{*}))\\rangle\\right\\}.$ ${\\cal U}_{4}=\\sum_{j=1}^{n}\\sum_{b\\in[k]\\setminus\\{z_{j}^{*}\\}}\\left\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\right\\|^{2}\\mathbb{I}\\left\\{-\\langle\\epsilon_{j},(\\hat{\\Sigma}_{b}(z^{*}))^{-1}(\\hat{\\theta}_{b}(z^{*})-\\theta_{b}^{*})\\rangle\\leq-\\frac{{\\bar{\\delta}}}{10}\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma_{b}^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle\\right\\}.$ ${\\cal U}_{5}=\\sum_{j=1}^{n}\\sum_{b\\in[k]\\setminus\\{z_{j}^{*}\\}}\\left\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\right\\|^{2}\\mathbb{I}\\left\\{\\frac{1}{2}\\langle\\epsilon_{j},((\\hat{\\Sigma}_{b}(z^{*}))^{-1}-(\\Sigma_{b}^{*})^{-1})\\epsilon_{j}\\rangle\\leq-\\frac{{\\bar{\\delta}}}{10}\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma_{b}^{*})^{-1}(\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*})\\rangle\\right\\}.$ ${\\cal M}_{6}=\\sum_{j=1}^{n}\\sum_{b\\in[k]\\setminus\\{z_{j}^{*}\\}}\\left\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\right\\|^{2}\\mathbb{I}\\left\\{-\\frac{1}{2}\\langle\\epsilon_{j},((\\hat{\\Sigma}_{z_{j}^{*}}(z^{*}))^{-1}-(\\Sigma_{z_{j}^{*}}^{*})^{-1})\\epsilon_{j}\\rangle\\leq-\\frac{\\bar{\\delta}}{10}\\langle\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*},(\\Sigma_{b}^{*})^{-1}-(\\Sigma_{z_{j}^{*}}^{*}-\\theta_{b}^{*})^{-1}\\rangle\\right\\}.$ ", "page_idx": 26}, {"type": "text", "text": "Using the same arguments as the proof of Lemma A.5, we can choose some $\\bar{\\delta}=\\bar{\\delta}_{n}=o(1)$ which is slowly diverging to zero satisfying ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}M_{i}\\leq n\\exp\\left(-(1+o(1))\\frac{\\mathrm{{SNR}}^{\\prime2}}{2}\\right)\\quad\\mathrm{for}\\;i=2,3,4.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "As for $M_{5}$ , by (43) we have ", "page_idx": 26}, {"type": "equation", "text": "$$\nM_{5}\\leq\\sum_{j=1}^{n}\\sum_{b\\in[k]\\setminus\\{z_{j}^{*}\\}}\\left\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\right\\|^{2}\\mathbb{I}\\left\\{C\\bar{\\delta}\\left\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\right\\|^{2}\\leq\\|w_{j}\\|^{2}\\sqrt{\\frac{\\log n}{n}}\\right\\},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $C$ is a constan and $w_{j}\\overset{i i d}{\\sim}\\mathcal{N}(0,I_{d})$ Since there exists some onstat $C^{\\prime}$ such hat $\\mathrm{SNR^{\\prime}\\leq}$ $C^{\\prime}\\Delta$ , we can choose appropriate $\\bar{\\delta}=o(1)$ such that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}M_{5}\\leq\\displaystyle\\sum_{j=1}^{n}\\displaystyle\\sum_{b\\in[k]\\setminus\\{z_{j}^{*}\\}}\\left\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\right\\|^{2}\\mathbb{P}\\left\\{C\\bar{\\delta}\\left\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{b}^{*}\\right\\|^{2}\\sqrt{\\frac{n}{\\log n}}\\leq\\|w_{j}\\|^{2}\\right\\}}\\\\ &{\\quad\\leq n\\exp\\left(-(1+o(1))\\frac{\\mathrm{SNR}^{'2}}{8}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "$M_{6}$ is essentially the same with $M_{5}$ and can be proved similarly. Finally for $M_{1}$ , using Lemma C.10, wehave ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\bigg(\\langle\\varepsilon_{j},(\\Sigma_{\\hat{z}_{j}^{\\star}}^{*})^{-1}(\\theta_{z_{j}^{\\star}}^{*}-\\theta_{b}^{*})\\rangle+\\frac12\\langle\\epsilon_{j},((\\Sigma_{b}^{*})^{-1}-(\\Sigma_{\\hat{z}_{j}^{\\star}}^{*})^{-1})\\epsilon_{j}\\rangle}\\\\ &{\\qquad\\quad-\\frac12\\log|\\Sigma_{z_{j}^{\\star}}^{*}|+\\frac12\\log|\\Sigma_{b}^{*}|\\leq-\\frac{1-\\delta-\\bar{\\delta}}{2}\\langle\\theta_{z_{j}^{\\star}}^{*}-\\theta_{b}^{*},(\\Sigma_{b}^{*})^{-1}(\\theta_{z_{j}^{\\star}}^{*}-\\theta_{b}^{*})\\rangle\\bigg)}\\\\ &{=\\!\\mathbb{P}\\bigg(\\langle w_{j},(\\Sigma_{z_{j}^{\\star}}^{*})^{\\frac{1}{2}}(\\Sigma_{b}^{*})^{-1}(\\theta_{z_{j}^{\\star}}^{*}-\\theta_{b}^{*})\\rangle+\\frac12\\langle w_{j},((\\Sigma_{z_{j}^{\\star}}^{*})^{\\frac{1}{2}}(\\Sigma_{b}^{*})^{-1}(\\Sigma_{z_{j}^{\\star}}^{*})^{\\frac{1}{2}}-I_{d})w_{j}\\rangle}\\\\ &{\\qquad\\quad-\\frac12\\log|\\Sigma_{z_{j}^{\\star}}^{*}|+\\frac12\\log|\\Sigma_{b}^{*}|\\leq-\\frac{1-\\delta-\\bar{\\delta}}{2}\\langle\\theta_{z_{j}^{\\star}}^{*}-\\theta_{b}^{*},(\\Sigma_{b}^{*})^{-1}(\\theta_{z_{j}^{\\star}}^{*}-\\theta_{b}^{*})\\rangle\\bigg)}\\\\ &{\\leq\\exp\\!\\Bigg(\\!\\!-(1-o(1))\\frac{\\mathrm{SNR}_{z_{j}^{\\star},b}^{\\prime}}{8}\\Bigg)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}M_{1}\\leq n\\exp\\left(-(1+o(1))\\frac{\\mathbf{SNR}^{'2}}{8}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Using the Markov's inequality we complete the proof of Lemma B.4. ", "page_idx": 27}, {"type": "text", "text": "Proof of Theorem 3.2. By Lemmas B.2-B.4, we can obtain the result by arguments used in the proof of Theorem 2.2 and hence the proof is omitted here. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "C Technical Lemmas ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we present and prove technical lemmas used in this paper. Lemmas C.1 and C.2 are about $\\chi^{2}$ distributions. Appendix C.1 gives various upper bounds needed in the proofs of Appendix B. Appendix C.2 is devoted to the calculation related to $\\mathrm{SNR^{\\prime}}$ ", "page_idx": 27}, {"type": "text", "text": "Lemma C.1. For any $x>0$ we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(\\chi_{d}^{2}\\geq d+2\\sqrt{d x}+2x)\\leq e^{-x},}\\\\ {\\mathbb{P}(\\chi_{d}^{2}\\leq d-2\\sqrt{d x})\\leq e^{-x}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. These results are Lemma 1 of [12]. ", "page_idx": 27}, {"type": "text", "text": "Lemma C.2. Let $W_{i}\\stackrel{i i d}{\\sim}\\chi_{d}^{2}$ for any $i\\in[n]$ where $n,d$ are positive integers. Then we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\sum_{i=1}^{n}W_{i}^{2}\\geq3n d^{2}\\right)\\leq\\frac{4}{n d}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. We have $\\begin{array}{r}{\\mathbb{E}\\sum_{i=1}^{n}W_{i}^{2}\\,=\\,n d(d+2)}\\end{array}$ and $\\mathbb{E}\\sum_{i=1}^{n}W_{i}^{4}\\,=\\,n d(d+2)(d+4)(d+6)$ . Then we have $\\begin{array}{r}{\\operatorname{Var}\\bigl(\\sum_{i=1}^{n}W_{i}^{2}\\bigr)=8n d(d+2)(d+3)}\\end{array}$ .Then we btain the desired result by Chebyshev's inequality. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "C.1 With-High-Probability Bounds ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Lemma C.3. For any $z^{*}\\in[k]^{n}$ and $k\\in[n]$ consider independent vectors $\\epsilon_{j}\\sim\\mathcal{N}(0,\\Sigma_{z_{j}^{*}}^{*})$ for any $j\\in[n]$ .Assume there exists a constant $\\lambda_{\\operatorname*{max}}>0$ such that $\\|\\Sigma_{a}^{*}\\|\\leq\\lambda_{\\operatorname*{max}}$ for any $a\\in[k]$ Then, for ", "page_idx": 27}, {"type": "text", "text": "any constant $C^{\\prime}>0$ there exists some constant $C>0$ only depending on $C^{\\prime},\\lambda_{\\mathrm{max}}$ such that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\operatorname*{max}_{a\\in[k]}\\left\\|\\frac{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}=a\\}\\epsilon_{j}}{\\sqrt{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}=a\\}}}\\right\\|\\le C\\sqrt{d+\\log n},}\\\\ &{}&{\\displaystyle\\operatorname*{max}_{a\\in[k]}\\frac{1}{d+\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}=a\\}}\\,\\left\\|\\frac{n}{j!}\\mathbb{I}\\{z_{j}^{*}=a\\}\\epsilon_{j}\\epsilon_{j}^{T}\\right\\|\\le C,\\ ~~}\\\\ &{}&{\\displaystyle\\operatorname*{max}_{T\\in[n]}\\left\\|\\frac{1}{\\sqrt{|T|}}\\sum_{j\\in T}\\epsilon_{j}\\right\\|\\le C\\sqrt{d+n},~~~}\\\\ &{}&{\\displaystyle\\operatorname*{max}_{a\\in[k]}\\sum_{T\\in[j+\\frac{2}{3}]}\\left\\|\\frac{1}{\\sqrt{|T|(d+\\sum_{j}^{*}=a)}}\\sum_{j\\in T}\\epsilon_{j}\\right\\|\\le C,~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "with probability at least $1-n^{-C^{\\prime}}$ . We have used the convention that $0/0=0$ ", "page_idx": 28}, {"type": "text", "text": "Proof.Note that $\\epsilon_{j}$ is sub-Gaussian with parameter $\\lambda_{\\mathrm{max}}$ which is a constant. The inequalities (33) and (35) are respectively Lemmas A.4, A.1 in [15]. The inequality (34) is a slight extension of Lemma A.2 in [15]. This extension follows from a standard union bound argument. The proof of (36) is identical to that of (35). \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Lemma C.4. Consider the same assumptions as in Lemma C.3. Assume additionally $\\begin{array}{r}{\\operatorname*{min}_{a\\in[k]}\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}\\,=\\,a\\}\\,\\ge\\,\\frac{\\alpha n}{k}}\\end{array}$ for some constant $\\alpha\\;>\\;0$ and $\\begin{array}{r}{\\frac{k(d+\\log n)}{n}\\,=\\,o(1)}\\end{array}$ Then, for any constant $C^{\\prime}>0$ thereexistssomeconstant $C>0$ onlydependingon $\\alpha,C^{\\prime},\\lambda_{\\mathrm{{max}}}$ such that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a\\in[k]}\\left\\|\\frac{1}{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}=a\\}}\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}=a\\}\\epsilon_{j}\\epsilon_{j}^{T}-\\Sigma_{a}^{*}\\right\\|\\leq C\\sqrt{\\frac{k(d+\\log n)}{n}},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "with probability at least $1-n^{-C^{\\prime}}$ ", "page_idx": 28}, {"type": "text", "text": "Proof. Note that we have $\\epsilon_{j}=\\Sigma_{z_{j}^{*}}^{*\\frac{1}{2}}\\eta_{j}$ where $\\eta_{j}\\overset{i i d}{\\sim}\\mathcal{N}(0,I_{d})$ for any $j\\in[n]$ Since $\\operatorname*{max}_{a}\\|\\Sigma_{a}^{*}\\|\\leq$ $\\lambda_{\\mathrm{max}}$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{ax}_{\\in[k]}\\left\\|\\frac{1}{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}=a\\}}\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}=a\\}\\epsilon_{j}\\epsilon_{j}^{T}-\\Sigma_{a}^{*}\\right\\|\\leq\\lambda_{\\operatorname*{max}}\\operatorname*{max}_{a\\in[k]}\\left\\|\\frac{1}{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}=a\\}}\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}=a\\}\\eta_{j}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Define ", "page_idx": 28}, {"type": "equation", "text": "$$\nQ_{a}=\\frac{1}{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}=a\\}}\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}=a\\}\\eta_{j}\\eta_{j}^{T}-I_{d}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Take $S^{d-1}=\\{y\\in\\mathbb{R}^{d}:\\|y\\|=1\\}$ and $N_{\\epsilon}=\\{v_{1},\\cdots,v_{|N_{\\epsilon}|}\\}$ is an $\\epsilon$ covering of $S^{d-1}$ In particular, we pick $\\epsilon<\\textstyle{\\frac{1}{4}}$ , then $|N_{\\epsilon}|\\leq9^{d}$ . By the definition of the $\\epsilon$ -covering, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|Q_{a}\\|\\leq\\frac{1}{1-2\\epsilon}\\operatorname*{max}_{i=1,\\cdots,|N_{\\epsilon}|}|v_{i}^{T}Q_{a}v_{i}|\\leq2\\operatorname*{max}_{i=1,\\cdots,|N_{\\epsilon}|}|v_{i}^{T}Q_{a}v_{i}|.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For any $v\\in N_{\\epsilon}$ \uff0c", "page_idx": 28}, {"type": "equation", "text": "$$\n\\boldsymbol{v}^{T}\\boldsymbol{Q}_{a}\\boldsymbol{v}=\\frac{1}{\\sum_{j=1}^{n}\\mathbb{I}\\{\\boldsymbol{z}_{j}^{*}=\\boldsymbol{a}\\}}\\sum_{j=1}^{n}\\mathbb{I}\\{\\boldsymbol{z}_{j}^{*}=\\boldsymbol{a}\\}(\\boldsymbol{v}^{T}\\eta_{j}\\eta_{j}^{T}\\boldsymbol{v}-1).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Denote $\\begin{array}{r}{n_{a}=\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}=a\\}}\\end{array}$ . Then $\\begin{array}{r}{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}=a\\}v^{T}\\eta_{j}\\eta_{j}^{T}v\\sim\\chi_{n_{a}}^{2}}\\end{array}$ . Using Lemma C.1, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{P}(\\operatorname*{max}_{a\\in[k]}\\|Q_{a}\\|\\ge t)\\le\\sum_{a=1}^{k}\\mathbb{P}(\\|Q_{a}\\|\\ge t)}}\\\\ &{}&{\\le\\sum_{a=1}^{k}\\sum_{i=1}^{|N_{t}|}\\mathbb{P}(|v_{i}^{T}Q_{a}v_{i}|\\ge t/2)}\\\\ &{}&{\\le\\sum_{a=1}^{k}2\\exp\\Biggl\\{-\\frac{n_{a}}{8}\\operatorname*{min}\\{t,t^{2}\\}+d\\log9\\Biggr\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Since $\\begin{array}{r}{\\frac{k(d+\\log n)}{n}=o(1)}\\end{array}$ and $n_{a}\\geq\\alpha n/k$ where $\\alpha$ is a constant, we can take $\\begin{array}{r}{t=C^{\\prime\\prime}\\sqrt{\\frac{k(d+\\log n)}{n}}}\\end{array}$ some large constant $C^{\\prime\\prime}$ and the proof is complete. \u53e3 ", "page_idx": 29}, {"type": "text", "text": "Lemma C.5. Consider the same assumptions as in Lemma C.3. Then, for any $s=o(n)$ andforany constant $C^{\\prime}>0$ there existssome constant $C>0$ onlydependingon $C^{\\prime},\\lambda_{\\mathrm{max}}$ suchthat ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\substack{T\\subset[n]:|T|\\le s}}\\frac{1}{|T|\\log\\frac{n}{|T|}+\\operatorname*{min}\\{1,\\sqrt{|T|}\\}d}\\left\\|\\sum_{j\\in T}\\epsilon_{j}\\epsilon_{j}^{T}\\right\\|\\le C,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "with probability at least $1-n^{-C^{\\prime}}$ . We have used the convention that $0/0=0$ ", "page_idx": 29}, {"type": "text", "text": "Proof. Consider any $a\\in[s]$ and a fixed $T\\subset[n]$ such that $|T|=a$ . Similar to the proof of Lemma C.4, we can take $S^{d-1}=\\{y\\in\\mathbb{R}^{d}:\\|y\\|=1\\}$ and its $\\epsilon$ -covering $N_{\\epsilon}$ with $\\epsilon\\,<\\,\\textstyle{\\frac{1}{4}}$ and $|N_{\\epsilon}|\\leq9^{d}$ Then we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|\\sum_{j\\in T}\\epsilon_{j}\\epsilon_{j}^{T}\\|=\\operatorname*{sup}_{\\|w\\|=1}\\sum_{j\\in T}(w^{T}\\epsilon_{j})^{2}\\leq2\\operatorname*{max}_{w\\in N_{\\epsilon}}\\sum_{j\\in T}(w^{T}\\epsilon_{j})^{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that $w^{T}\\epsilon_{j}/\\sqrt{\\lambda_{\\mathrm{max}}}$ is a sub-Gaussian random variable with parameter 1. By the tail probability result for quadratic forms of sub-Gaussian random vectors [10], for any fixed $w\\in N_{\\epsilon}$ ,wehave ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\sum_{j\\in T}(w^{T}\\epsilon_{j})^{2}\\geq\\lambda_{\\operatorname*{max}}\\Big(a+2\\sqrt{a t}+2t\\Big)\\right)\\leq\\exp(-t)\\,.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Since $a=o(n)$ , there exists a constant $C_{0}$ such that $\\begin{array}{r}{2a\\le C_{0}a\\log\\frac{n}{a}}\\end{array}$ .We can take $\\begin{array}{r}{t=\\tilde{C}(a\\log\\frac{n}{a}+d)}\\end{array}$ With $\\begin{array}{r}{\\tilde{C}=\\frac{C}{16}-\\frac{C_{0}}{4}}\\end{array}$ ,then $\\begin{array}{r}{a+2\\sqrt{a t}+2t\\leq\\frac{C}{4}(a\\log\\frac{n}{a}+d)}\\end{array}$ Thus, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\sum_{j\\in T}(w^{T}\\epsilon_{j})^{2}\\geq\\frac{C}{4}(a\\log\\frac{n}{a}+d)\\right)\\leq\\exp\\bigg(-\\tilde{C}(a\\log\\frac{n}{a}+d)\\bigg).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Hence, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\|\\sum_{j\\in T}\\epsilon_{j}\\epsilon_{j}^{T}\\|\\ge\\frac{C}{2}(a\\log\\frac{n}{a}+d)\\right)\\le9^{d}\\exp\\bigg(-\\tilde{C}(a\\log\\frac{n}{a}+d)\\bigg).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "As a result, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{r\\subset[n],1\\leq|T|\\leq s}{\\operatorname*{max}}\\frac{1}{|T|\\log\\frac{n}{|T|}+d}\\|\\underset{j\\in T}{\\sum}\\epsilon_{j}\\epsilon_{j}^{T}\\|\\geq C\\biggr\\}\\leq C\\biggr\\}\\leq\\underset{a=1}{\\sum}^{s}\\mathbb{P}\\biggl\\{\\underset{|T|=a}{\\operatorname*{max}}\\|\\sum_{j\\in T}\\epsilon_{j}\\epsilon_{j}^{T}\\|\\geq C(a\\log\\frac{n}{a}+d)\\biggr\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\underset{a=1}{\\sum}\\binom{n}{a}\\underset{|T|=a}{\\operatorname*{max}}\\mathbb{P}\\biggl\\{\\|\\sum_{j\\in T}\\epsilon_{j}\\epsilon_{j}^{T}\\|\\geq C(a\\log\\frac{n}{a}+d)\\biggr\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\underset{a=1}{\\sum}\\binom{n}{a}9^{d}\\exp\\bigg(-\\tilde{C}(a\\log\\frac{n}{a}+d)\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Since $a\\log{\\frac{n}{a}}$ is an increasing function when $a\\in[1,s]$ and $\\textstyle a\\log{\\frac{n}{a}}\\geq\\log n\\geq\\log s.$ a choice of ${\\tilde{C}}=3+C^{\\prime}$ , that is $C=16C^{\\prime}+4C_{0}+48$ can yield the desired result. ", "page_idx": 30}, {"type": "text", "text": "Finally, to allow $|T|=0$ , we note that $d\\leq\\operatorname*{min}\\lbrace1,\\sqrt{|T|}\\rbrace d$ . The proof is complete. ", "page_idx": 30}, {"type": "text", "text": "Lemma C.6. For any $z^{*}\\;\\in\\;[k]^{n}$ and $k\\ \\in\\ [n]$ asume $\\begin{array}{r}{\\operatorname*{min}_{a\\in[k]}\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}\\,=\\,a\\}\\,\\ge\\,\\frac{\\alpha n}{k}}\\end{array}$ and $\\begin{array}{r}{\\ell(z,z^{*})=o(\\frac{n\\Delta^{2}}{k})}\\end{array}$ then ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a\\in[k]}\\frac{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}=a\\}}{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a\\}}\\leq2.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. For any $z\\in[k]^{n}$ such that $\\ell(z,z^{*})=o(n)$ and any $a\\in[k]$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a\\}\\ge\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}=a\\}-\\displaystyle\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}\\ne z_{j}^{*}\\}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\ge\\displaystyle\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}=a\\}-\\displaystyle\\frac{\\ell(z,z^{*})}{\\Delta^{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\ge\\displaystyle\\frac{\\alpha n}{2k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which implies ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}=a\\}}{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a\\}}\\leq\\frac{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a\\}+\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}\\neq z_{j}^{*}\\}}{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a\\}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq1+\\frac{\\alpha n/2k}{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a\\}}}\\\\ &{\\qquad\\qquad\\qquad\\leq2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Thus, we obtain (39). ", "page_idx": 30}, {"type": "text", "text": "In the following lemma, we are going to analyze estimation errors of the centers and covariance matrices under the anisotropic GMMs. For any $z\\in[k]^{n}$ and for any $z\\in[k]$ , recall the definitions ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\theta}_{a}(z)=\\frac{\\displaystyle\\sum_{j\\in[n]}Y_{j}\\mathbb{I}\\left\\{z_{j}=a\\right\\}}{\\displaystyle\\sum_{j\\in[n]}\\mathbb{I}\\left\\{z_{j}=a\\right\\}},\\forall a\\in[k]}\\\\ &{\\hat{\\Sigma}_{a}(z)=\\frac{\\displaystyle\\sum_{j\\in[n]}(Y_{j}-\\hat{\\theta}_{a}(z))(Y_{j}-\\hat{\\theta}_{a}(z))^{T}\\mathbb{I}\\left\\{z_{j}=a\\right\\}}{\\displaystyle\\sum_{j\\in[n]}\\mathbb{I}\\left\\{z_{j}=a\\right\\}},\\forall a\\in[k].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Lemma C.7. For any $z^{*}\\in[k]^{n}$ and $k\\in[n]$ consider independent vectors $Y_{j}=\\theta_{z_{j}^{*}}^{*}+\\epsilon_{j}$ where $\\epsilon_{j}\\sim\\mathcal{N}(0,\\Sigma_{z_{j}^{*}}^{*})$ for any $j\\in[n]$ .Assume there exist constants $\\lambda_{\\operatorname*{min}},\\lambda_{\\operatorname*{max}}>0$ such that $\\lambda_{\\operatorname*{min}}\\leq$ $\\lambda_{1}(\\Sigma_{a}^{*})\\leq\\lambda_{d}(\\Sigma_{a}^{*})\\leq\\lambda_{\\operatorname*{max}}$ for any $a\\in[k]$ and a constant $\\alpha>0$ such that $\\begin{array}{r l}{\\operatorname*{min}_{a\\in[k]}\\textstyle\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}=}\\end{array}$ $\\textstyle a\\}\\geq{\\frac{\\alpha n}{k}}$ .Assume k(d+tlogn) = o(1) and  \u21920. Assume (33-(38) hold. Thenfor any T = o(n) and for any constant $C^{\\prime}>0$ there exists some constant $C>0$ only depending on $\\alpha,\\lambda_{\\mathrm{max}},C^{\\prime}$ such that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{a\\in\\mathbb{K}}\\left\\|\\hat{\\theta}_{a}(z^{*})-\\theta_{a}^{*}\\right\\|\\leq C\\sqrt{\\frac{k(d+\\log n)}{n}},}\\\\ &{\\displaystyle\\operatorname*{max}_{a\\in\\mathbb{K}}\\left\\|\\hat{\\theta}_{a}(z)-\\hat{\\theta}_{a}(z^{*})\\right\\|\\leq C\\left(\\frac{k}{n\\Delta}\\ell(z,z^{*})+\\frac{k\\sqrt{d+n}}{n\\Delta}\\sqrt{\\ell(z,z^{*})}\\right),}\\\\ &{\\displaystyle\\operatorname*{max}_{a\\in\\mathbb{K}}\\left\\|\\hat{\\Sigma}_{a}(z^{*})-\\Sigma_{a}^{*}\\right\\|\\leq C\\sqrt{\\frac{k(d+\\log n)}{n}},}\\\\ &{\\displaystyle\\operatorname*{max}_{a\\in\\mathbb{K}}\\left\\|\\hat{\\Sigma}_{a}(z)-\\hat{\\Sigma}_{a}(z^{*})\\right\\|\\leq C\\left(\\frac{k}{n}\\ell(z,z^{*})+\\frac{k\\sqrt{n\\ell(z,z^{*})}}{n\\Delta}+\\frac{k d}{n\\Delta}\\sqrt{\\ell(z,z^{*})}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for all $z$ such that $\\ell(z,z^{*})\\leq\\tau$ ", "page_idx": 30}, {"type": "text", "text": "Proof. Using (33) we obtain (41). By the same argument of (118) in [8], we can obtain (42). By (33) and (37) and (41), we can obtain (43). In the remaining proof, we will establish (53). ", "page_idx": 31}, {"type": "text", "text": "Since $\\begin{array}{r}{\\frac{k(d+\\log n)}{n}=o(1)}\\end{array}$ wehave $\\|\\hat{\\Sigma}_{a}(z^{*})\\|\\leq1$ for any $a\\in[k]$ The ifference $\\hat{\\Sigma}_{a}(z)-\\hat{\\Sigma}_{a}(z^{*})$ will be decomposed into several terms. We notice that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\hat{\\Sigma}_{a}(z)-\\hat{\\Sigma}_{a}(z^{*})\\right\\|\\leq S_{1}+S_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{\\hat{y}_{1}}=\\left\\|\\frac{1}{\\sum\\mathbb{I}\\{z_{j}=a\\}}\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a\\}\\left((Y_{j}-\\hat{\\theta}_{a}(z))(Y_{j}-\\hat{\\theta}_{a}(z))^{T}-(Y_{j}-\\hat{\\theta}_{a}(z^{*}))(Y_{j}-\\hat{\\theta}_{a}(z^{*}))^{T}\\right)\\right\\|\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and ", "page_idx": 31}, {"type": "equation", "text": "$$\nS_{2}=\\Big\\|\\left(\\frac{1}{\\sum\\mathbb{I}\\{z_{j}=a\\}}-\\frac{1}{\\sum\\mathbb{I}\\{z_{j}^{*}=a\\}}\\right)\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}=a\\}(Y_{j}-\\hat{\\theta}_{a}(z^{*}))(Y_{j}-\\hat{\\theta}_{a}(z^{*}))^{T}\\Big\\|.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Also, we notice that ", "page_idx": 31}, {"type": "equation", "text": "$$\n{\\cal S}_{1}\\leq{\\cal L}_{1}+{\\cal L}_{2}+{\\cal L}_{3},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where ", "text_level": 1, "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle{\\dot{\\bf\\Xi}}_{1}=\\Big\\|\\frac{1}{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a\\}}\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=z_{j}^{*}=a\\}\\Big((Y_{j}-\\hat{\\theta}_{a}(z))(Y_{j}-\\hat{\\theta}_{a}(z))^{T}-(Y_{j}-\\hat{\\theta}_{a}(z^{*}))(Y_{j}-\\hat{\\theta}_{a}(z))^{T}\\Big)}\\\\ {\\displaystyle{\\dot{\\bf\\Xi}}_{2}=\\Big\\|\\frac{1}{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a\\}}\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a,z_{j}^{*}\\neq a\\}(Y_{j}-\\hat{\\theta}_{a}(z))(Y_{j}-\\hat{\\theta}_{a}(z))^{T}\\Big\\|,}\\\\ {\\displaystyle{\\dot{\\bf\\Xi}}_{3}=\\Big\\|\\frac{1}{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a\\}}\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}\\neq a,z_{j}^{*}=a\\}(Y_{j}-\\hat{\\theta}_{a}(z^{*}))(Y_{j}-\\hat{\\theta}_{a}(z^{*}))^{T}\\Big\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For $L_{1}$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{1}\\leq\\displaystyle\\bigg\\|\\frac{1}{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a\\}}\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=z_{j}^{*}=a\\}(\\hat{\\theta}_{a}(z)-\\hat{\\theta}_{a}(z^{*}))(\\hat{\\theta}_{a}(z)-\\hat{\\theta}_{a}(z^{*}))^{T}\\bigg\\|}\\\\ &{\\qquad+2\\displaystyle\\bigg\\|\\frac{1}{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a\\}}\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=z_{j}^{*}=a\\}(Y_{j}-\\hat{\\theta}_{a}(z^{*}))(\\hat{\\theta}_{a}(z)-\\hat{\\theta}_{a}(z^{*}))^{T}\\bigg\\|}\\\\ &{\\leq\\displaystyle\\bigg\\|\\hat{\\theta}_{a}(z)-\\hat{\\theta}_{a}(z^{*})\\bigg\\|^{2}\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}=a\\}+\\Big\\|\\theta_{a}^{*}-\\hat{\\theta}_{a}(z^{*})\\Big\\|\\left\\|\\hat{\\theta}_{a}(z)-\\hat{\\theta}_{a}(z^{*})\\right\\|\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}=a\\}}\\\\ &{\\qquad+\\left\\|\\hat{\\theta}_{a}(z)-\\hat{\\theta}_{a}(z^{*})\\right\\|\\left\\|\\frac{1}{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a\\}}\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=z_{j}^{*}=a\\}\\epsilon_{j}\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By (36), (39), (40), we have uniformly for any $a\\in[k]$ \uff0c ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\left\\|\\frac{1}{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a\\}}\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=z_{j}^{*}=a\\}\\epsilon_{j}\\right\\|\\preceq\\frac{\\sqrt{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=z_{j}^{*}=a\\}}}{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a\\}}\\sqrt{d+\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}=a\\}}}\\\\ &{}&{\\preceq1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Since $\\begin{array}{r}{\\operatorname*{max}_{a\\in[k]}\\left\\|\\hat{\\theta}_{a}(z^{*})-\\theta_{a}^{*}\\right\\|=o(1)}\\end{array}$ by (39),(42), (41), (47), and (48), we have uniformly for any $a\\in[k]$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n{L}_{1}\\preceq\\left\\|\\hat{\\theta}_{a}(z)-\\hat{\\theta}_{a}(z^{*})\\right\\|\\preceq\\frac{k}{n\\Delta}\\ell(z,z^{*})+\\frac{k\\sqrt{d+n}}{n\\Delta}\\sqrt{\\ell(z,z^{*})}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "To bound $L_{2}$ , we first give the following simple fact. For any positive integer $m$ and_any $\\{u_{j}\\}_{j\\in[m]},\\{v_{j}\\}_{j\\in[m]}\\;\\in\\;\\mathbf{\\bar{R}}^{d}$ wehave $\\begin{array}{r}{\\|\\check{\\sum_{j\\in[m]}}\\dot{(u_{j}\\,+\\,v_{j})}(u_{j}\\,+\\,\\dot{v_{j}}\\dot{)}^{T}\\|\\;\\leq\\;2\\|\\check{\\sum_{j\\in[m]}u_{j}u_{j}^{T}}\\|\\;+}\\end{array}$ $2\\|\\sum_{j\\in[m]}v_{j}v_{j}^{T}\\|$ .Hence, for $L_{2}$ , we have the following decomposition ", "page_idx": 32}, {"type": "equation", "text": "$$\nL_{2}\\leq2R_{1}+2R_{2},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{1}=\\displaystyle\\bigg\\|\\frac{1}{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a\\}}\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a,z_{j}^{*}\\neq a\\}(Y_{j}-\\theta_{a}^{*})(Y_{j}-\\theta_{a}^{*})^{T}\\bigg\\|,}\\\\ &{R_{2}=\\displaystyle\\bigg\\|\\frac{1}{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a\\}}\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a,z_{j}^{*}\\neq a\\}(\\theta_{a}^{*}-\\hat{\\theta}_{a}(z))(\\theta_{a}^{*}-\\hat{\\theta}_{a}(z))^{T}\\bigg\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Since $\\begin{array}{r}{\\operatorname*{max}_{a\\in[k]}\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a,z_{j}^{*}\\neq a\\}\\leq\\frac{\\ell(z,z^{*})}{\\Delta^{2}},}\\end{array}$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{R_{2}\\leq\\left\\|\\theta_{a}^{*}-\\hat{\\theta}_{a}(z)\\right\\|^{2}\\frac{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a,z_{j}^{*}\\neq a\\}}{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a\\}}}}\\\\ {{\\preceq\\left(\\left\\|\\hat{\\theta}_{a}(z)-\\hat{\\theta}_{a}(z^{*})\\right\\|^{2}+\\left\\|\\hat{\\theta}_{a}(z^{*})-\\theta_{a}^{*}\\right\\|^{2}\\right)\\frac{k\\ell(z,z^{*})}{n\\Delta^{2}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By (38) and the fact that $\\begin{array}{r}{\\operatorname*{max}_{a\\in[k]}\\sum_{j=1}^{n}\\mathbb{I}\\{z_{i}=a,z_{i}^{*}\\neq a\\}\\leq\\frac{\\ell(z,z^{*})}{\\Delta^{2}}}\\end{array}$ , we also have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{R_{1}\\leq2\\biggr\\|\\frac{1}{\\sum_{j=1}^{n}\\mathbb{I}\\left\\{z_{j}=a\\right\\}}\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a,z_{j}^{*}\\neq a\\}(\\theta_{z_{j}^{*}}^{*}-\\theta_{z_{j}}^{*})(\\theta_{z_{j}^{*}}^{*}-\\theta_{z_{j}}^{*})^{T}\\biggr\\|}\\\\ &{\\qquad+2\\biggr\\|\\frac{1}{\\sum_{j=1}^{n}\\mathbb{I}\\left\\{z_{j}=a\\right\\}}\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a,z_{j}^{*}\\neq a\\}\\epsilon_{j}\\epsilon_{j}^{T}\\biggr\\|}\\\\ &{\\qquad\\leq2\\frac{\\sum_{j=1}^{n}\\mathbb{I}\\left\\{z_{j}=a,z_{j}^{*}\\neq a\\right\\}\\left\\|\\theta_{z_{j}^{*}}^{*}-\\theta_{z_{j}}^{*}\\right\\|^{2}}{\\sum_{j=1}^{n}\\mathbb{I}\\left\\{z_{j}=a\\right\\}}+2\\biggr\\|\\frac{1}{\\sum_{j=1}^{n}\\mathbb{I}\\left\\{z_{j}=a\\right\\}}\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a,z_{j}^{*}\\neq a\\}\\epsilon_{j}\\epsilon_{j}^{T}\\biggr\\|}\\\\ &{\\qquad\\leq\\frac{k\\ell(z,z^{*})}{n}+\\frac{\\ell(z,z^{*})}{\\Delta^{2}}\\log\\frac{n\\Delta^{2}}{\\ell(z,z^{*})}+d\\sqrt{\\frac{\\ell(z,z^{*})}{\\Delta^{2}}}.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We are going to simplify the above bounds for $R_{1},R_{2}$ . Under the assumption that $\\textstyle{\\frac{k(d+\\log n)}{n}}\\,=$ $o(1),\\;\\Delta/k\\;\\to\\;\\infty$ and $\\ell(z,z^{*})~\\leq~\\tau~=~o(n)$ ,we have $\\mathrm{max}_{a\\in[k]}\\;\\|\\hat{\\theta}_{a}(z)\\,-\\,\\hat{\\theta}_{a}(z^{*})\\|\\;\\,=\\;o(1)$ $\\begin{array}{r}{\\operatorname*{max}_{a\\in[k]}\\|\\hat{\\theta}_{a}(z^{*})-\\theta_{a}^{*}\\|=o(1)}\\end{array}$ and $\\begin{array}{r}{\\frac{k\\ell(z,z^{*})}{n\\Delta^{2}}=o(1)}\\end{array}$ Hence $\\begin{array}{r}{R_{2}\\preceq\\frac{k\\ell(z,z^{*})}{n\\Delta^{2}}}\\end{array}$ ke(2). Also we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{k\\ell(z,z^{*})}{n\\Delta^{2}}\\log\\frac{n\\Delta^{2}}{\\ell(z,z^{*})}=\\frac{k\\sqrt{\\ell(z,z^{*})}}{n\\Delta}\\sqrt{\\frac{\\ell(z,z^{*})}{\\Delta^{2}}\\bigg(\\log\\frac{n\\Delta^{2}}{\\ell(z,z^{*})}\\bigg)^{2}}\\leq\\frac{k\\sqrt{n\\ell(z,z^{*})}}{n\\Delta}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where in the last inequality, we use the fact that $x(\\log(n/x))^{2}$ is an increasing function of $x$ Wwhen $0<x=o(n)$ .Then, ", "page_idx": 32}, {"type": "equation", "text": "$$\nL_{2}\\preceq\\frac{k\\sqrt{n\\ell(z,z^{*})}}{n\\Delta}+\\frac{k}{n}\\ell(z,z^{*})+\\frac{k d}{n\\Delta}\\sqrt{\\ell(z,z^{*})}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Since $L_{3}$ is similar to $L_{2}$ , by (46) we have uniformly for any $a\\in[k]$ ", "page_idx": 32}, {"type": "equation", "text": "$$\nS_{1}\\preceq\\frac{k\\sqrt{n\\ell(z,z^{*})}}{n\\Delta}+\\frac{k}{n}\\ell(z,z^{*})+\\frac{k d}{n\\Delta}\\sqrt{\\ell(z,z^{*})}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "To bound $S_{2}$ , by (70) in [8], we have uniformly for any $a\\in[k]$ ", "page_idx": 32}, {"type": "equation", "text": "$$\nS_{2}=\\frac{\\left|\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}^{*}=a\\}-\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a\\}\\right|}{\\sum_{j=1}^{n}\\mathbb{I}\\{z_{j}=a\\}}\\left\\|\\hat{\\Sigma}_{a}(z^{*})\\right\\|^{2}\\preceq\\frac{k}{n}\\frac{\\ell(z,z^{*})}{\\Delta^{2}},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where we use (43). Since  2) $\\begin{array}{r}{\\frac{k}{n}\\frac{\\ell(z,z^{*})}{\\Delta^{2}}\\preceq\\frac{k\\sqrt{n\\ell(z,z^{*})}}{n\\Delta}}\\end{array}$ , by(45) and the facts that $\\ell(z,z^{*})\\leq\\tau=o(n)$ we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a\\in[k]}\\left\\|\\hat{\\Sigma}_{a}(z)-\\hat{\\Sigma}_{a}(z^{*})\\right\\|\\preceq\\frac{k\\sqrt{n\\ell(z,z^{*})}}{n\\Delta}+\\frac{k}{n}\\ell(z,z^{*})+\\frac{k d}{n\\Delta}\\sqrt{\\ell(z,z^{*})}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Lemma C.8. Under the same assumption as in Lemma C.7, if additionally we assume $k d=O({\\sqrt{n}})$ and $\\tau=o(n/k)$ ,thereexistssomeconstant $C>0$ onlydependingon $\\alpha,\\lambda_{\\mathrm{min}},\\lambda_{\\mathrm{max}},C^{\\prime}$ suchthat ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a\\in[k]}\\left\\|(\\hat{\\Sigma}_{a}(z))^{-1}-(\\hat{\\Sigma}_{a}(z^{*}))^{-1}\\right\\|\\leq C\\biggr(\\frac{k}{n}\\ell(z,z^{*})+\\frac{k\\sqrt{n\\ell(z,z^{*})}}{n\\Delta}+\\frac{k d}{n\\Delta}\\sqrt{\\ell(z,z^{*})}\\biggr)\\,.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. By (43) we have $\\begin{array}{r}{\\operatorname*{max}_{a\\in[k]}\\|\\hat{\\Sigma}_{a}(z^{*})\\|,\\operatorname*{max}_{a\\in[k]}\\|(\\hat{\\Sigma}_{a}(z^{*}))^{-1}\\|\\preceq1}\\end{array}$ . By (44) we also have $\\begin{array}{r}{\\operatorname*{max}_{a\\in[k]}\\|\\hat{\\Sigma}_{a}(z)\\|,\\operatorname*{max}_{a\\in[k]}\\|(\\hat{\\Sigma}_{a}(z))^{-1}\\|\\preceq1}\\end{array}$ .Hence, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{a\\in[k]}{\\operatorname*{max}}\\left\\|(\\hat{\\Sigma}_{a}(z))^{-1}-(\\hat{\\Sigma}_{a}(z^{*}))^{-1}\\right\\|\\leq\\underset{a\\in[k]}{\\operatorname*{max}}\\left\\|(\\hat{\\Sigma}_{a}(z^{*}))^{-1}\\right\\|\\left\\|\\hat{\\Sigma}_{a}(z)-\\hat{\\Sigma}_{a}(z^{*})\\right\\|\\left\\|(\\hat{\\Sigma}_{a}(z))^{-1}\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\preceq\\frac{k\\sqrt{n\\ell(z,z^{*})}}{n\\Delta}+\\frac{k}{n}\\ell(z,z^{*})+\\frac{k d}{n\\Delta}\\sqrt{\\ell(z,z^{*})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "C.2 Calculation Related to SNR' ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In the following lemmas, we study properties of $\\{\\mathrm{SNR}_{a,b}^{\\prime}\\}_{a\\neq b}$ . Consider any pair $a\\neq b\\in[k]$ . Let $\\eta\\sim\\mathcal{N}(0,I_{d})$ and $\\Xi_{a,b}=\\theta_{a}^{*}-\\theta_{b}^{*}$ . Define ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{a,b}(\\delta)=\\left\\{x\\in\\mathbb{R}^{d}:x^{T}\\Sigma_{a}^{\\ast\\frac{1}{2}}(\\Sigma_{b}^{\\ast})^{-1}\\Xi_{a,b}+\\frac{1}{2}x^{T}\\Bigl(\\Sigma_{a}^{\\ast\\frac{1}{2}}(\\Sigma_{b}^{\\ast})^{-1}\\Sigma_{a}^{\\ast\\frac{1}{2}}-I_{d}\\Bigr)\\ x}\\\\ &{\\phantom{A_{a,b}(\\delta)=\\;\\;}\\leq-\\frac{1-\\delta}{2}\\Xi_{a,b}^{T}(\\Sigma_{b}^{\\ast})^{-1}\\Xi_{a,b}+\\frac{1}{2}\\log|\\Sigma_{a}^{\\ast}|-\\frac{1}{2}\\log|\\Sigma_{b}^{\\ast}|\\;\\right\\}}\\\\ &{\\phantom{A_{a,b}(\\delta)=\\;\\;}=\\left\\{x\\in\\mathbb{R}^{d}:\\|x\\|^{2}\\geq\\Bigl(x-(\\Sigma_{a}^{\\ast})^{-\\frac{1}{2}}\\Xi_{b,a}\\Bigr)^{T}\\Bigl((\\Sigma_{a}^{\\ast})^{-\\frac{1}{2}}\\Sigma_{b}^{\\ast}(\\Sigma_{a}^{\\ast})^{-\\frac{1}{2}}\\Bigr)^{-1}\\Bigl(x-(\\Sigma_{a}^{\\ast})^{-\\frac{1}{2}}\\Xi_{b,a}\\Bigr)}\\\\ &{\\phantom{A_{a,b}(\\delta)=\\;\\;}+\\log|(\\Sigma_{a}^{\\ast})^{-\\frac{1}{2}}\\Sigma_{b}^{\\ast}(\\Sigma_{a}^{\\ast})^{-\\frac{1}{2}}|-\\delta\\Xi_{b,a}^{T}(\\Sigma_{b}^{\\ast})^{-1}\\Xi_{b,a}\\Biggr\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "for any $\\delta\\in\\mathbb{R}$ . Then $B_{a,b}(\\delta)\\subset B_{a,b}(\\delta^{\\prime})$ for any $\\delta^{\\prime}\\leq\\delta$ . In addition, we define ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathrm{SNR}_{a,b}^{\\prime}(\\delta)=\\operatorname*{min}_{x\\in B_{a,b}(\\delta)}2\\left\\|x\\right\\|,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mid P_{a,b}(\\delta)={\\mathbb P}(\\eta\\in B_{a,b}(\\delta))\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Recall the definitions of $B_{a,b}$ and $\\mathrm{SNR}_{a,b}^{\\prime}$ in Section 3. Then it is a special case of $B_{a,b}(\\delta)$ and $\\operatorname{SNR}_{a,b}^{\\prime}(\\delta)$ with $\\delta=0$ . That is, we have $B_{a,b}=B_{a,b}(0)$ and $\\mathrm{SNR}_{a,b}^{\\prime}=\\mathrm{SNR}_{a,b}^{\\prime}(0)$ ", "page_idx": 33}, {"type": "text", "text": "To understand these quantities, we first study a canonical setting that can be later applied to establish Lemma C.10. ", "page_idx": 33}, {"type": "text", "text": "Lemma C.9. Consider any $\\theta\\in\\mathbb{R}^{d}\\setminus\\{0\\}$ and any $\\Sigma\\,\\in\\,\\mathbb{R}^{d\\times d}$ that is positive semi-definite. Let $\\lambda_{\\mathrm{max}}$ \uff0c $\\lambda_{\\operatorname*{min}}>0$ be the largest and smallest eigenvalue of $\\Sigma_{i}$ . respectively. For any $t\\in\\mathbb{R}_{}$ define ", "page_idx": 33}, {"type": "equation", "text": "$$\nD(t)=\\{x\\in\\mathbb{R}^{d}:\\|x\\|^{2}\\geq(x-\\theta)^{T}\\Sigma^{-1}(x-\\theta)+t\\},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and $s(t)=\\operatorname*{min}_{x\\in D(t)}\\|x\\|$ Then the following hold: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left\\|\\theta\\right\\|/(\\operatorname*{max}\\{2,2\\sqrt{2\\lambda_{\\operatorname*{max}}}\\})<s(t)<(1-\\operatorname*{min}\\{\\sqrt{\\lambda_{\\operatorname*{min}}/8},1/2\\})\\left\\|\\theta\\right\\|.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "\u00b7f $t^{\\prime}$ also satisfies $-\\left\\|\\theta\\right\\|^{2}/\\lambda_{\\mathrm{max}}<t^{\\prime}<\\left\\|\\theta\\right\\|^{2}/8$ we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n|s(t^{\\prime})-s(t)|\\leq\\lambda_{\\operatorname*{max}}\\frac{t^{\\prime}-t}{2\\operatorname*{min}\\{\\sqrt{\\lambda_{\\operatorname*{min}}/8},1/2\\}\\,\\|\\theta\\|}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "\u00b7If $\\theta$ is further assumed to satisfy $\\operatorname*{min}\\{\\sqrt{\\lambda_{\\operatorname*{min}}/8},1/2\\}\\left\\|\\theta\\right\\|\\ \\ \\geq\\ \\ 2\\lambda_{\\operatorname*{max}}$ and $\\lVert\\theta\\rVert\\ \\geq$ $\\textstyle\\frac{\\lambda_{\\operatorname*{min}}}{32}\\operatorname*{min}\\{\\sqrt{\\lambda_{\\operatorname*{min}}/8},1/2\\}$ there exis a $d$ dimensional ball $\\begin{array}{r l r}{H(t)}&{{}\\in}&{\\mathbb{R}^{d}}\\end{array}$ with radius $(\\lambda_{\\operatorname*{min}}/8)\\operatorname*{min}\\{\\sqrt{\\lambda_{\\operatorname*{min}}/8},1/2\\}$ such that $\\begin{array}{r l r}{H(t)}&{{}\\subset}&{D(t)}\\end{array}$ and $\\begin{array}{r l}{\\|x\\|\\quad}&{{}<}\\end{array}$ $(\\lambda_{\\operatorname*{min}}/8)\\operatorname*{min}\\{\\sqrt{\\lambda_{\\operatorname*{min}}/8},1/2\\}+\\lambda_{\\operatorname*{max}}+s(t).$ for all $x\\in H(t)$ ", "page_idx": 34}, {"type": "text", "text": "Proof. First, we check whether each of the following two points is contained in $D(t)$ or not, under the assumption $-\\left\\|\\theta\\right\\|^{2}/\\lambda_{\\operatorname*{max}}<t<\\left\\|\\theta\\right\\|^{2}$ ", "page_idx": 34}, {"type": "text", "text": "\u00b7 When $x=0$ , we have $(x-\\theta)^{T}\\Sigma^{-1}(x-\\theta)+t\\geq\\left\\|\\theta\\right\\|^{2}/\\lambda_{\\operatorname*{max}}+t>0$ Hence, $0\\not\\in D(t)$ ", "page_idx": 34}, {"type": "text", "text": "\u00b7 When $x=\\theta$ , we have $\\lVert\\theta\\rVert^{2}>t$ Hence, $\\theta\\in D(t)$ ", "page_idx": 34}, {"type": "text", "text": "As a result, $D(t)$ is non-empty, $s(t)$ is well defined, and $0<s(t)<\\|\\theta\\|^{2}$ . Next, we consider a few more points to sharpen upper and lower bounds on $s(t)$ under the assumption $-\\left\\lVert\\theta\\right\\rVert^{2}/(8\\lambda_{\\operatorname*{max}})<$ $t<\\left\\|\\theta\\right\\|^{2}/8$ ", "page_idx": 34}, {"type": "text", "text": "\u00b7 For any $x$ that satisfies $\\left\\Vert x\\right\\Vert\\leq\\left\\Vert\\theta\\right\\Vert/(\\operatorname*{max}\\{2,2\\sqrt{2\\lambda_{\\operatorname*{max}}}\\})$ , we have $\\lVert x-\\theta\\rVert\\geq\\lVert\\theta\\rVert/2$ and consequently, $(x-\\theta)^{T}\\Sigma^{-1}(x-\\theta)+t\\ge(\\|\\theta\\|/2)^{2}/\\lambda_{\\operatorname*{max}}+t\\,=\\,\\|\\theta\\|^{2}/(4\\lambda_{\\operatorname*{max}})+t$ Under the assumption that $t>-\\left\\lVert\\theta\\right\\rVert^{2}/(8\\lambda_{\\operatorname*{max}})$ , we can verify that $\\left\\lvert\\lvert\\theta\\right\\rvert^{2}/(4\\lambda_{\\operatorname*{max}})+t>$ $\\left\\|\\theta\\right\\|^{2}/(8\\lambda_{\\operatorname*{max}})\\geq(\\left\\|\\theta\\right\\|/\\operatorname*{max}\\{2,2\\sqrt{2\\lambda_{\\operatorname*{max}}}\\})^{2}\\geq\\left\\|x\\right\\|^{2}\\!.$ Hence, such $x\\notin D(t)$ \u00b7 When $x=(1-\\operatorname*{min}\\{\\sqrt{\\lambda_{\\operatorname*{min}}/8},1/2\\})\\theta$ , we have $\\left\\|x\\right\\|\\geq\\left\\|\\theta\\right\\|/2$ and $(x-\\theta)^{T}\\Sigma^{-1}(x-$ $\\theta)+t\\,\\leq\\,\\|\\theta\\|^{2}\\,(\\operatorname*{min}\\lbrace\\sqrt{\\lambda_{\\operatorname*{min}}/8},1/2\\rbrace)^{2}/\\lambda_{\\operatorname*{min}}+t\\,=\\,\\|\\theta\\|^{2}\\,/\\operatorname*{max}\\lbrace8,4\\lambda_{\\operatorname*{min}}\\rbrace+t$ Under the assumption that $t\\,<\\,\\left\\|\\theta\\right\\|^{2}/8$ , we have $\\left\\|\\theta\\right\\|^{2}/\\operatorname*{max}\\{8,4\\lambda_{\\operatorname*{min}}\\}+t<\\left\\|\\theta\\right\\|^{2}/8+t\\leq$ $\\left\\Vert\\theta\\right\\Vert^{2}/4\\leq\\left\\Vert x\\right\\Vert^{2}$ . Hence, such $x\\in D(t)$ ", "page_idx": 34}, {"type": "text", "text": "As a result, we have $\\|\\theta\\|\\,/(\\operatorname*{max}\\{2,2\\sqrt{2\\lambda_{\\operatorname*{max}}}\\})<s(t)<(1-\\operatorname*{min}\\{\\sqrt{\\lambda_{\\operatorname*{min}}/8},1/2\\})\\,\\|\\theta\\|.$ ", "page_idx": 34}, {"type": "text", "text": "Define a ball $S(r)=\\{x\\in\\mathbb{R}^{d}:\\|x\\|^{2}\\leq r^{2}\\}$ and define $S_{2}(r;t)=\\{x\\in\\mathbb{R}^{d}:(x\\!-\\!\\theta)^{T}\\Sigma^{-1}(x\\!-\\!\\theta)\\leq$ $r^{2}-t\\}$ to be the part of $\\mathbb{R}^{d}$ that is inside the corresponding ellipsoid. Then we have $s(t)=\\operatorname*{min}\\{r\\geq$ $0:S_{1}\\dot{(}r)\\cap S_{2}(r;t)\\neq\\emptyset\\}$ . By the definition and bounds of $s(t)$ and the convexity of $S_{1}(s(t))$ and $S_{2}(s(t);t)$ , we must have $|S_{1}(s(t))\\cap S_{2}(s(t);t)|=1$ , meaning that $S_{1}(s(t))$ and $S_{2}(s(t);t)$ touch each other externally at one point. This implies $s(t)$ can be obtained by the following process: We let $S_{1}(r)$ and $S_{2}(r)$ grow by increasing $r$ , starting from O. The first time they touch each other, we stop and the value of $r$ is exactly $s(t)$ ", "page_idx": 34}, {"type": "text", "text": "Denote $y(t)\\in\\mathbb{R}$ such that $\\{y(t)\\}=S_{1}(s(t))\\cap S_{2}(s(t);t)$ . Then we must have $y(t)\\in\\bar{S}_{2}(s(t);t)$ By the first conclusion, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|y(t)-\\theta\\|\\geq\\|\\theta\\|-\\|y(t)\\|=\\|\\theta\\|-s(t)\\geq\\operatorname*{min}\\{\\sqrt{\\lambda_{\\operatorname*{min}}/8},1/2\\}\\,\\|\\theta\\|}\\\\ &{\\|y(t)-\\theta\\|\\leq\\|\\theta\\|+\\|y(t)\\|=\\|\\theta\\|+s(t)\\leq2\\,\\|\\theta\\|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "In addition, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Z}(t)-t=(y(t)-\\theta)^{T}\\Sigma^{-1}(y(t)-\\theta)\\geq\\left\\|y(t)-\\theta\\right\\|^{2}/\\lambda_{\\operatorname*{max}}\\geq(\\operatorname*{min}\\{\\sqrt{\\lambda_{\\operatorname*{min}}/8},1/2\\}\\left\\|\\theta\\right\\|)^{2}/\\lambda_{\\operatorname*{max}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Now we are going to prove the second conclusion of the lemma. Without loss of generality, assume $t\\le t^{\\prime}$ .Thenwehave $D(t)\\,\\supset\\,D(t^{\\prime})$ and $s(t)\\,\\leq\\,s(t^{\\prime})$ .We are going to establish a lower bound for $s(t)$ .First by definition of $s(t^{\\prime})$ ,wehave $|S_{1}(s(t^{\\prime}))\\cap S_{2}(s(t^{\\prime});t^{\\prime})|\\,=\\,1$ . Since $t\\le t^{\\prime}$ \uff0cwe have $\\dot{S_{2}}(s(t^{\\prime});t)\\,\\dot{\\supset}\\;S_{2}(s(t^{\\prime});t^{\\prime})$ . We have $S_{1}(s(t^{\\prime}))\\cap S_{2}(s(t^{\\prime});t)\\neq\\emptyset$ . From here we can also see $s(t)\\leq s(t^{\\prime})$ . Now consider any $x\\in\\bar{S}_{2}(s(t^{\\prime});t)$ . It satisfies $(x-\\theta)^{T}\\Sigma^{-1}(x-\\theta)=s^{2}(t^{\\prime})-t$ Then we have ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left(\\sqrt{\\frac{s^{2}(t^{\\prime})-t^{\\prime}}{s^{2}(t^{\\prime})-t}}(x-\\theta)\\right)\\Sigma^{-1}\\left(\\sqrt{\\frac{s^{2}(t^{\\prime})-t^{\\prime}}{s^{2}(t^{\\prime})-t}}(x-\\theta)\\right)=s^{2}(t^{\\prime})-t^{\\prime},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "meaning that 0 + $\\begin{array}{r}{\\theta+\\sqrt{\\frac{s^{2}(t^{\\prime})-t^{\\prime}}{s^{2}(t^{\\prime})-t}}(x-\\theta)\\in\\bar{S}_{2}(s(t^{\\prime});t^{\\prime})}\\end{array}$ Hence, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|x\\|\\ge\\bigg\\|\\theta+\\sqrt{\\frac{s^{2}(t^{\\prime})-t^{\\prime}}{s^{2}(t^{\\prime})-t}}(x-\\theta)\\bigg\\|-\\bigg\\|x-\\bigg(\\theta+\\sqrt{\\frac{s^{2}(t^{\\prime})-t^{\\prime}}{s^{2}(t^{\\prime})-t}}(x-\\theta)\\bigg)\\bigg\\|}\\\\ &{\\qquad=\\bigg\\|\\theta+\\sqrt{\\frac{s^{2}(t^{\\prime})-t^{\\prime}}{s^{2}(t^{\\prime})-t}}(x-\\theta)\\bigg\\|-\\bigg(1-\\sqrt{\\frac{s^{2}(t^{\\prime})-t^{\\prime}}{s^{2}(t^{\\prime})-t}}\\bigg)\\,\\|x-\\theta\\|}\\\\ &{\\qquad\\ge\\operatorname*{min}_{y\\in\\mathcal{S}_{2}(s(t^{\\prime});t^{\\prime})}\\|y\\|-\\bigg(1-\\sqrt{\\frac{s^{2}(t^{\\prime})-t^{\\prime}}{s^{2}(t^{\\prime})-t}}\\bigg)_{y\\in\\mathcal{S}_{2}(s(t^{\\prime});t)}\\,\\|x-\\theta\\|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Since $|S_{1}(s(t^{\\prime}))\\cap\\bar{S}_{2}(s(t^{\\prime});t^{\\prime})|=1$ we have $\\begin{array}{r}{\\operatorname*{min}_{y\\in\\bar{S}_{2}(s(t^{\\prime});t^{\\prime})}\\|y\\|=s(t^{\\prime})}\\end{array}$ Since $(x-\\theta)^{T}\\Sigma^{-1}(x-$$\\theta)\\geq\\lambda_{\\operatorname*{max}}^{-1}\\left\\Vert x-\\theta\\right\\Vert^{2}$ ,wehave $\\|x-\\theta\\|^{2}\\leq\\lambda_{\\operatorname*{max}}(s^{2}(t^{\\prime})-t)$ .Hence,", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|x\\|\\ge s(t^{\\prime})-\\left(1-\\sqrt{\\frac{s^{2}(t^{\\prime})-t^{\\prime}}{s^{2}(t^{\\prime})-t}}\\right)\\sqrt{\\lambda_{\\operatorname*{max}}(s^{2}(t^{\\prime})-t)}}\\\\ &{\\quad\\ge s(t^{\\prime})-\\sqrt{\\lambda_{\\operatorname*{max}}}\\bigg(\\sqrt{s^{2}(t^{\\prime})-t}-\\sqrt{s^{2}(t^{\\prime})-t^{\\prime}}\\bigg)}\\\\ &{\\quad=s(t^{\\prime})-\\sqrt{\\lambda_{\\operatorname*{max}}}\\frac{t^{\\prime}-t}{\\sqrt{s^{2}(t^{\\prime})-t}+\\sqrt{s^{2}(t^{\\prime})-t^{\\prime}}}}\\\\ &{\\quad\\ge s(t^{\\prime})-\\sqrt{\\lambda_{\\operatorname*{max}}}\\frac{t^{\\prime}-t}{2\\sqrt{s^{2}(t^{\\prime})-t^{\\prime}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "$\\begin{array}{r}{r<s(t^{\\prime})\\!-\\!\\sqrt{\\lambda_{\\operatorname*{max}}}\\frac{t^{\\prime}-t}{2\\sqrt{s^{2}(t^{\\prime})-t^{\\prime}}}}\\end{array}$ w have $S_{1}(r)\\cap S_{2}(s(t^{\\prime});t)=\\emptyset$ and conseguendly $S_{1}(r)\\cap S_{2}(r;t)=\\emptyset$ $\\begin{array}{r}{s(t)\\geq s(t^{\\prime})-\\sqrt{\\lambda_{\\operatorname*{max}}}\\frac{t^{\\prime}-t}{2\\sqrt{s^{2}(t^{\\prime})-t^{\\prime}}}}\\end{array}$ $s^{2}(t^{\\prime})-$ $t^{\\prime}\\geq(\\operatorname*{min}\\{\\sqrt{\\lambda_{\\operatorname*{min}}/8},1/2\\}\\,\\|\\theta\\|)^{2}/\\lambda_{\\operatorname*{max}}.$ $\\begin{array}{r}{s(t)\\geq\\dot{s(t^{\\prime})}-\\lambda_{\\operatorname*{max}}\\frac{t^{\\prime}-t}{2\\operatorname*{min}\\{\\sqrt{\\lambda_{\\operatorname*{min}}/8},1/2\\}\\|\\theta\\|}.}\\end{array}$ ", "page_idx": 35}, {"type": "text", "text": "For the third conclusion of the lemma, recall the definition of $y(t)$ . Under the assumption that $\\operatorname*{min}\\{\\sqrt{\\lambda_{\\operatorname*{min}}/8},1/2\\}\\,\\|\\theta\\|\\geq2\\lambda_{\\operatorname*{max}}$ , we have $\\|y(t)-\\theta\\|-\\lambda_{\\operatorname*{max}}>0$ and $\\lambda_{\\operatorname*{max}}/\\left\\lvert\\lvert y(t)-\\theta\\right\\rvert\\rvert\\le1/2$ Denote (=0(Il(t)-0I- \u5165max). Then ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|y^{\\prime}(t)-\\theta\\|\\leq\\|y(t)-\\theta\\|\\leq2\\,\\|\\theta\\|\\,,}\\\\ &{\\|y^{\\prime}(t)-\\theta\\|=(1-\\lambda_{\\operatorname*{max}}/\\,\\|y(t)-\\theta\\|)\\,\\|y(t)-\\theta\\|\\,,}\\\\ &{\\|y(t)-y^{\\prime}(t)\\|=\\left\\|\\frac{y(t)}{\\|y(t)-\\theta\\|}\\lambda_{\\operatorname*{max}}\\right\\|=\\lambda.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Consequently, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(y^{\\prime}(t)-\\theta)^{T}\\Sigma^{-1}(y^{\\prime}(t)-\\theta)-(y(t)-\\theta)^{T}\\Sigma^{-1}(y(t)-\\theta)}\\\\ &{=\\left(1-\\frac{\\lambda_{\\operatorname*{max}}}{\\|y(t)-\\theta\\|}\\right)^{2}(y(t)-\\theta)^{T}\\Sigma^{-1}(y(t)-\\theta)-(y(t)-\\theta)^{T}\\Sigma^{-1}(y(t)-\\theta)}\\\\ &{=-\\frac{\\lambda_{\\operatorname*{max}}}{\\|y(t)-\\theta\\|}\\left(2-\\frac{\\lambda_{\\operatorname*{max}}}{\\|y(t)-\\theta\\|}\\right)(y(t)-\\theta)^{T}\\Sigma^{-1}(y(t)-\\theta)}\\\\ &{\\leq-\\frac{\\lambda_{\\operatorname*{max}}}{\\|y(t)-\\theta\\|}(y(t)-\\theta)^{T}\\Sigma^{-1}(y(t)-\\theta)}\\\\ &{\\leq-\\frac{\\lambda_{\\operatorname*{max}}}{\\|y(t)-\\theta\\|}\\|y(t)-\\theta\\|^{2}\\lambda_{\\operatorname*{max}}^{-1}}\\\\ &{\\leq-\\|y(t)-\\theta\\|}\\\\ &{\\leq-\\operatorname*{min}\\left\\{\\sqrt{\\lambda_{\\operatorname*{max}}/8},1/2\\right\\}\\|\\theta\\|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Denote $H(t)$ to be the ball centered at $y^{\\prime}(t)$ with radius $(\\lambda_{\\operatorname*{min}}/8)\\operatorname*{min}\\{\\sqrt{\\lambda_{\\operatorname*{min}}/8},1/2\\}$ . Then for any $x\\in H(t)$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(x-\\theta)^{T}\\Sigma^{-1}(x-\\theta)-(y(t)-\\theta)^{T}\\Sigma^{-1}(y(t)-\\theta)}\\\\ &{=(y^{\\prime}(t)-\\theta)^{T}\\Sigma^{-1}(y^{\\prime}(t)-\\theta)+2(x-y^{\\prime}(t))^{T}\\Sigma^{-1}(y^{\\prime}(t)-\\theta)+(x-y^{\\prime}(t))^{T}\\Sigma^{-1}(x-y^{\\prime}(t))}\\\\ &{\\quad-\\left(y(t)-\\theta\\right)^{T}\\Sigma^{-1}(y(t)-\\theta)}\\\\ &{\\leq\\left(y^{\\prime}(t)-\\theta\\right)^{T}\\Sigma^{-1}(y^{\\prime}(t)-\\theta)+2\\lambda_{\\operatorname*{min}}^{-1}\\left\\Vert x-y^{\\prime}(t)\\right\\Vert\\left\\Vert y^{\\prime}(t)-\\theta\\right\\Vert+\\lambda_{\\operatorname*{min}}^{-1}\\left\\Vert x-y^{\\prime}(t)\\right\\Vert^{2}}\\\\ &{\\quad-\\left(y(t)-\\theta\\right)^{T}\\Sigma^{-1}(y(t)-\\theta)}\\\\ &{\\leq-\\operatorname*{min}\\left\\{\\sqrt{\\lambda_{\\operatorname*{min}}/8},1/2\\right\\}\\left\\Vert\\theta\\right\\Vert+2\\lambda_{\\operatorname*{min}}^{-1}\\left\\Vert x-y^{\\prime}(t)\\right\\Vert\\left\\Vert y^{\\prime}(t)-\\theta\\right\\Vert+\\lambda_{\\operatorname*{min}}^{-1}\\left\\Vert x-y^{\\prime}(t)\\right\\Vert^{2}}\\\\ &{\\leq-\\operatorname*{min}\\left\\{\\sqrt{\\lambda_{\\operatorname*{min}}/8},1/2\\right\\}\\left\\Vert\\theta\\right\\Vert+4\\lambda_{\\operatorname*{min}}^{-1}\\left\\Vert x-y^{\\prime}(t)\\right\\Vert\\left\\Vert\\theta\\right\\Vert+\\lambda_{\\operatorname*{min}}^{-1}\\left\\Vert x-y^{\\prime}(t)\\right\\Vert^{2}}\\\\ &{\\leq-\\frac{1}{2}\\operatorname*{min}\\{\\sqrt{\\lambda_{\\operatorname*{min}}/8},1/2\\}\\left\\Vert\\theta\\right\\Vert+\\frac{\\lambda_{\\operatorname*{min}}}{64}\\operatorname*{min}\\left\\{\\sqrt{\\lambda_{\\operatorname*{min}}/8},1/2\\right\\}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the last inequality holds under the assumption $\\begin{array}{r}{\\|\\theta\\|\\ge\\frac{\\lambda_{\\operatorname*{min}}}{32}\\operatorname*{min}\\lbrace\\sqrt{\\lambda_{\\operatorname*{min}}/8},1/2\\rbrace}\\end{array}$ . Hence, $H(t)\\subset S_{2}(s(t);t)$ and consequently $H(t)\\subset D(\\bar{t})$ . On the other hand, for any $x\\in H(t)$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|x\\|\\leq\\|x-y^{\\prime}(t)\\|+\\|y^{\\prime}(t)-y(t)\\|+\\|y(t)\\|}\\\\ &{\\qquad\\leq(\\lambda_{\\operatorname*{min}}/8)\\operatorname*{min}\\{\\sqrt{\\lambda_{\\operatorname*{min}}/8},1/2\\}+\\lambda_{\\operatorname*{max}}+s(t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The proof is complete. ", "page_idx": 36}, {"type": "text", "text": "Lemma C.10. Assume $d\\,=\\,O(1)$ .Consider any $a\\ \\neq\\ b\\ \\in\\ [k]$ .Assume there exist constants $\\lambda_{\\operatorname*{min}},\\lambda_{\\operatorname*{max}}$ such that $0<\\lambda_{\\operatorname*{min}}\\leq\\lambda_{1}(\\Sigma_{j}^{*})\\leq\\lambda_{d}(\\Sigma_{j}^{*})\\leq\\lambda_{\\operatorname*{max}}.$ for any $j\\in\\{a,b\\}$ Then $S N R_{a,b}^{\\prime}$ and $\\|\\Xi_{b,a}\\|$ are of the same order. When $S N R_{a,b}^{\\prime}=O(1)$ we have $P_{a,b}(0)\\ge c$ for some constant $c>0$ When $S N R_{a,b}^{\\prime}\\rightarrow\\infty$ we have ", "page_idx": 36}, {"type": "equation", "text": "$$\nP_{a,b}(0)\\geq\\exp\\biggl(-\\frac{1+o(1)}{8}S N R_{a,b}^{'2}\\biggr)\\,,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and for any $\\delta=o(1)$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\nP_{a,b}(\\delta)\\leq\\exp\\biggl(-\\frac{1-o(1)}{8}S N R_{a,b}^{'2}\\biggr)\\,.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. Recall the setting stated in Lemma C.9. By the definition of $B_{a,b}$ , we can take $\\theta\\,=$ $(\\Sigma_{a}^{*})^{-\\frac{1}{2}}\\Xi_{b,a}$ \uff0c $\\Sigma\\ =\\ (\\Sigma_{a}^{*})^{-\\frac{1}{2}}\\Sigma_{b}^{*}(\\Sigma_{a}^{*})^{-\\frac{1}{2}}$ ,and $t~=~\\log|(\\Sigma_{a}^{*})^{-\\frac{1}{2}}\\Sigma_{b}^{*}(\\Sigma_{a}^{*})^{-\\frac{1}{2}}|$ such that $\\boldsymbol{B}_{a,b}\\;\\;=\\;$ $D(t)$ . Due to $d\\ =\\ O(1)$ and (24), we have that all eigenvalues of $\\Sigma$ are constants and that $\\log\\big|(\\Sigma_{a}^{*})^{-\\frac{1}{2}}\\Sigma_{b}^{*}(\\Sigma_{a}^{*})^{-\\frac{1}{2}}\\big|$ is also a constant. Consequently, $t$ is constant. In adition, $\\lVert\\theta\\rVert$ is of the same order as $\\left\\|\\Xi_{b,a}\\right\\|$ ", "page_idx": 36}, {"type": "text", "text": "When $\\|\\Xi_{b,a}\\|$ is a constant, $\\lVert\\theta\\rVert$ is a constant, then $\\mathrm{SNR}_{a,b}^{\\prime}$ must be a constant as well. This is because $D(t)$ is the set of points where one density is greater or equal to another. Then $D(t)$ is non-empty as both densities have integral 1. Hence, $s(t)$ must be finite, meaning $\\mathrm{SNR}_{a,b}^{\\prime}$ is a constant. In this case, we have $P_{a,b}(0)=\\mathbb{P}(\\eta\\in D(t))$ being a constant as well. ", "page_idx": 37}, {"type": "text", "text": "When $\\|\\Xi_{b,a}\\|\\,\\rightarrow\\,\\infty$ , we have $\\|\\theta\\|\\,\\rightarrow\\,\\infty$ . Since all the assumptions needed in Lemma C.9 are satisfied, by its first conclusion, we have $s(t)$ being of the same order as $\\lVert\\theta\\rVert$ , and consequently being of the same order as $\\|\\Xi_{b,a}\\|$ . As a result, $\\bar{\\mathrm{SNR}}_{a,b}^{\\prime}$ and $\\|\\Xi_{b,a}\\|$ are of the same order. In addition, $H(t)$ exists and its radius is some constant $c_{1}>0$ . Hence, its volume is $c_{1}^{d}V_{d}$ where $V_{d}$ is denoted as the volume of a $d$ -dimensional unit ball. In addition, for any $x\\,\\in\\,H(t)$ , we have $\\|\\boldsymbol{x}\\|\\leq s(t)+c_{2}=\\mathrm{SNR}_{a,b}^{\\prime}/2+c_{2}$ for some constant $c_{2}>0$ . Recall $\\eta\\sim N(0,I_{d})$ . Then we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{a,b}(0)\\geq\\mathbb{P}(\\eta\\in H(t))\\geq c_{1}^{d}V_{d}\\displaystyle\\operatorname*{min}_{x\\in H(t)}\\displaystyle\\frac{1}{\\sqrt{(2\\pi)^{d}}}\\exp\\!\\left(\\!-\\frac{1}{2}\\left\\|x\\right\\|^{2}\\right)}\\\\ &{\\qquad\\geq c_{1}^{d}V_{d}\\displaystyle\\frac{1}{\\sqrt{(2\\pi)^{d}}}\\exp\\!\\left(\\!-\\frac{1}{2}\\left\\|\\mathbf{S}\\mathbf{N}_{a,b}^{\\prime}/2+c_{2}\\right\\|^{2}\\!\\right)}\\\\ &{\\qquad\\geq\\exp\\!\\left(-\\frac{1+o(1)}{8}\\mathbf{S}\\mathbf{N}_{a,b}^{\\prime2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Now let us consider $B_{a,b}(\\delta)$ where $\\delta\\;=\\;o(1)$ .We can take $\\theta,\\Sigma$ same as before, but let $t^{\\prime}\\,=$ $\\log|(\\Sigma_{a}^{*})^{-\\frac{1}{2}}\\Sigma_{b}^{*}(\\Sigma_{a}^{*})^{-\\frac{1}{2}}|-\\delta\\Xi_{b,a}^{T}(\\Sigma_{b}^{*})^{-1}\\Xi_{b,a}$ Then we have $t^{\\prime}=o(1)\\left\\|\\theta\\right\\|^{2}$ . Hence, by Lemma C.9, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left|\\operatorname{SNR}_{a,b}^{\\prime}-\\operatorname{SNR}_{a,b}^{\\prime}(\\delta)\\right|=2\\left|s(t)-s(t^{\\prime})\\right|\\preceq\\frac{\\left|t-t^{\\prime}\\right|}{\\left\\|\\theta\\right\\|}=o(1)\\left\\|\\theta\\right\\|=o(1)\\operatorname{SNR}_{a,b}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Hence, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{a,b}(\\delta)=\\mathbb{P}(\\eta\\in B_{a,b}(\\delta))\\leq\\underset{x\\in B_{a,b}(\\delta)}{\\operatorname*{max}}\\frac{1}{\\sqrt{(2\\pi)^{d}}}\\exp\\biggl(-\\frac{1}{2}\\left\\|x\\right\\|^{2}\\biggr)}\\\\ &{\\phantom{=}\\frac{1}{\\sqrt{(2\\pi)^{d}}}\\exp\\biggl(-\\frac{1}{2}\\underset{x\\in B_{a,b}(\\delta)}{\\operatorname*{min}}\\left\\|x\\right\\|^{2}\\biggr)}\\\\ &{\\phantom{=}\\frac{1}{\\sqrt{(2\\pi)^{d}}}\\exp\\biggl(-\\frac{1}{8}\\mathrm{SN}_{a,b}^{\\prime}(\\delta)\\biggr)}\\\\ &{\\phantom{=}=\\exp\\biggl(-\\frac{1-o(1)}{8}\\mathrm{SN}_{a,b}^{\\prime2}\\biggr)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 38}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 38}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 38}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 39}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 I'he answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 39}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 39}, {"type": "text", "text": "Answer: [No] ", "page_idx": 40}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 40}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 40}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [No] ", "page_idx": 40}, {"type": "text", "text": "Justification: Experiment results were averaged. Error bars were inappropriate from a visual standpoint. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 41}, {"type": "text", "text": "Answer:[No] ", "page_idx": 41}, {"type": "text", "text": "Justification: ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 41}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] Justification: ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 41}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper carried out foundational and theoretical analysis on clustering which has no direct social impacts. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks. ", "page_idx": 42}, {"type": "text", "text": "\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 42}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7. The answer NA means that the paper does not use existing assets. ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 42}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 43}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 'I'he answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 43}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 43}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 43}]