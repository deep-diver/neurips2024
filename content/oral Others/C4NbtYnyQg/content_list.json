[{"type": "text", "text": "Flipped Classroom: Aligning Teacher Attention with Student in Generalized Category Discovery ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Haonan Lin1\u2020 Wenbin $\\mathbf{A}\\mathbf{n}^{2\\dagger}$ Jiahao Wang1 Yan Chen1\u2217 Feng Tian1\u2217 ", "page_idx": 0}, {"type": "text", "text": "Mengmeng Wang3,4 Guang Dai4 Qianying Wang5 Jingdong Wang6 ", "page_idx": 0}, {"type": "text", "text": "1 School of Comp. Science & Technology, MOEKLINNS Lab, Xi\u2019an Jiaotong University   \n2 School of Auto. Science & Engineering, MOEKLINNS Lab, Xi\u2019an Jiaotong University 3 College of Comp. Science & Technology, Zhejiang University of Technology 4 SGIT AI Lab, State Grid Corporation of China 5 Lenovo Research 6 Baidu Inc ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advancements have shown promise in applying traditional Semi-Supervised Learning strategies to the task of Generalized Category Discovery (GCD). Typically, this involves a teacher-student framework in which the teacher imparts knowledge to the student to classify categories, even in the absence of explicit labels. Nevertheless, GCD presents unique challenges, particularly the absence of priors for new classes, which can lead to the teacher\u2019s misguidance and unsynchronized learning with the student, culminating in suboptimal outcomes. In our work, we delve into why traditional teacher-student designs falter in open-world generalized category discovery as compared to their success in closed-world semi-supervised learning. We identify inconsistent pattern learning across attention layers as the crux of this issue and introduce FlipClass\u2014a method that dynamically updates the teacher to align with the student\u2019s attention, instead of maintaining a static teacher reference. Our teacher-student attention alignment strategy refines the teacher\u2019s focus based on student feedback from an energy perspective, promoting consistent pattern recognition and synchronized learning across old and new classes. Extensive experiments on a spectrum of benchmarks affirm that FlipClass significantly surpasses contemporary GCD methods, establishing new standards for the field. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Teacher-Student architecture has proved its effectiveness in Semi-Supervised Learning (SSL) [66, 32, 53], which aims to take advantage of a large collection of unlabeled data, reducing the expensive costs of annotation [57, 90, 89] . Previous approaches tend to model $p$ (student|teacher), where the teacher typically acts as a fixed point of reference, providing a form of \u201csupervision prior\" to guide the student [25, 42, 60]. This supervision comes from labeled data and is asymmetrical: while the teacher has robust prior knowledge and provides a stable learning signal, the student\u2019s knowledge is incomplete and evolving. The student learns from both the teacher\u2019s supervision and the data it is exposed to, trying to emulate the teacher by aligning its predictions with those of the teacher. ", "page_idx": 0}, {"type": "image", "img_path": "C4NbtYnyQg/tmp/6aec21d785454b098aaa0f628524355364ceb6dfe52de1cea859e9c5b626ba72.jpg", "img_caption": ["Figure 1: Left: Learning effects of traditional Teacher-Student Consistency Model (TSCM, e.g., SimGCD [75]) and our Flipped Classroom Consistency Model (FlipClass) on Stanford Cars [33]. Middle: Model comparison between TSCM and our FlipClass, where $\\mathbb{D}^{\\mathrm{new}}$ refers to data belonging to new classes. Right: Illustration of the inner feedback mechanism in FlipClass, where teacher attention is adapted to the student, leading to the alignment of attention. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Teacher-Student designs traditionally rely on a closed-world assumption, where it is expected that the teachers have supervision prior to all classes they will face while instructing students [61, 3]. However, real-world applications often involve dynamic and open environments, where instances belonging to new classes may appear [84, 30, 31, 41]. In such cases, discovering novelties could enable models to adapt new information and evolve continually as biological systems do [14, 49]. Recently, Generalized Category Discovery (GCD) [69] stands out by challenging models to categorize unlabeled data containing both old and new classes using only partial labels for training. Although recent GCD methods have adapted closed-world Teacher-Student strategies with notable success [75, 50], the transition is not seamless and presents several challenges. ", "page_idx": 1}, {"type": "text", "text": "Challenge I: Learning gap. Fig. 1 top left illustrates the learning evolution of student and teacher. The previous teacher-student models result in unsynchronized learning and a significant learning gap in new classes. The ideal learning dynamic between the teacher and student should be cohesive, which requires teaching students in accordance with their aptitude. ", "page_idx": 1}, {"type": "text", "text": "Challenge II: Discrepancies in features. The learning gap arises from the teacher\u2019s fast pace, leading to large discrepancies in representations between weakly-augmented data (teacher) and strongly-augmented data (student), especially for new classes (Fig. 1 middle). This causes significant prediction differences, making consistency loss optimization difficult and hindering effective student learning. Over time, the iterative learning process exacerbates this misalignment. ", "page_idx": 1}, {"type": "text", "text": "Challenge III: Attention inconsistency. Inadequate supervision for new classes leads to inaccurate instructions from the teacher, causing the student to focus on different parts than the teacher (Fig. 1 right). Imagine a classroom transitioning to a new subject. Without proper guidance, students\u2019 attention diverges from the teacher\u2019s, resulting in confusion and ineffective learning. ", "page_idx": 1}, {"type": "text", "text": "To sum up, the challenges of previous teacher-student models arise from inadequate supervision on new classes and the gap between weakly and strongly augmented data. This results in attention inconsistency (Chall. III), which leads to discrepancies in predictions and representations (Chall. II), ultimately causing a significant performance gap (Chall. I). Addressing these challenges requires developing teacher-student dynamics that align the evolving knowledge of both teacher and student (Fig. 1, bottom left). These findings lay the foundation for our approach, FlipClass, which models the teacher\u2019s posterior $p$ (teacher|student) from the energy perspective of attention, building an adaptive teacher to bridge the learning gap. FlipClass offers a plug-and-play solution to foster an interactive learning environment where the student can influence the teacher\u2019s guidance in real-time, allowing teachers to tailor their instructions based on students\u2019 current attention [7, 1]. By aligning attention, FlipClass ensures that the learning pace of the teacher and student is in sync, leading to improvement on both old and new classes. Our contributions are summarized as: ", "page_idx": 1}, {"type": "image", "img_path": "C4NbtYnyQg/tmp/ec7e0b3d75e935bfcf742295587217df2617e5dab118904339140c6be59a5b42.jpg", "img_caption": ["Figure 2: Exploring prior gaps between SSL and GCD on SCars and CUB datasets. Left: Accuracy of sorted pseudo labels for old and new classes. Middle: Consistency loss trends over epochs, illustrating challenges in optimization and slower convergence for new classes. Right: Categorize errors [75], where \u201cTrue Old\" refers to predicting an \u2018Old\u2019 class sample to another \u2018Old\u2019 class, while \u2018False Old\" indicates predicting an \u2018Old\u2019 class sample as some \u2018New\u2019 class. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "(1) Empirical analysis: We highlight the challenge of applying the closed-world Teacher-Student paradigm to the open-world scenario of GCD. Our in-depth analysis identifies attention misalignment between teacher and student as the key issue hindering synchronized learning between them. ", "page_idx": 2}, {"type": "text", "text": "(2) Methodology: Based on these analyses, we propose a flexible and effective method, FlipClass, which enables teachers to adapt and respond to student feedback to synchronize their learning progress, thereby leading to an overall improvement in teaching outcomes. ", "page_idx": 2}, {"type": "text", "text": "(3) Superiority: FlipClass consistently outperforms state-of-the-art generalized category discovery methods on both coarse-grained and fine-grained datasets. ", "page_idx": 2}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first provide some background on semi-supervised learning and generalized category discovery to better contextualize our analysis. Let us consider a data set $\\mathbb{D}$ consisting of labeled data $\\mathbb{D}_{L}=$ $\\{(\\mathbf{x}_{i}^{\\ell},{y}_{i})\\}_{i=1}^{N_{L}}$ from $|\\mathbb{C}_{K}|$ old classes and unlabeled data $\\mathbb{D}_{U}=\\{\\mathbf{x}_{j}^{u}\\}_{j=1}^{N_{U}}$ that may contain instances from both old classes $\\mathbb{C}_{K}$ and new classes $\\mathbb{C}_{N}$ , with $\\mathbb{C}=\\mathbb{C}_{K}\\cup\\mathbb{C}_{N}$ . For a data instance $\\mathbf{X}$ , let $p_{\\mathrm{m}}(\\boldsymbol{\\mathrm{y}}\\mid f_{\\boldsymbol{\\theta}}(\\mathbf{x}))$ denote the predicted class distribution produced by the model $f$ . ", "page_idx": 2}, {"type": "text", "text": "2.1 Integrating SSL Techniques into a Consistency Loss Framework ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In semi-supervised learning (SSL), the goal is to enhance model performance by leveraging unlabeled data, traditionally drawn from the same class spectrum as the labeled data [90]. The goal of SSL can be formalized by integrating three fundamental techniques: $(I)$ Consistency regularization ensures that the model outputs consistent predictions for augmented versions of the same instance. This technique utilizes different transformations to test the robustness of the model\u2019s predictions, promoting stability across variations in the input data [8, 59, 36]. (2) Pseudo-labeling utilizes the model to generate artificial labels for unlabeled data by adopting \u201chard\" labels (that is, the argmax of the model output) and keeping only the labels where the highest class probability exceeds a predefined threshold [62, 47, 57, 37, 23]. (3) Teacher-Student model incorporates a structured learning relationship where the teacher model, typically trained on weakly-augmented instances, generates high-quality pseudo labels. These pseudo labels are then used to guide the training of the student model, which processes strongly-augmented instances. This approach helps improve the generalization capabilities of the student model by learning from the refined knowledge and stable supervision signals provided by the teacher [66, 43, 77, 12, 53]. Several methods integrate some of these techniques and achieve advanced performance in SSL [63, 83, 78, 87]. We unify these SSL techniques into one consistency loss: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{cons}}=\\frac{1}{|\\mathbb{D}_{U}|}\\sum_{\\mathbf{x}\\in\\mathbb{D}_{U}}H\\Big(p_{\\mathrm{m}}\\big(y\\mid f_{\\theta}(\\alpha(\\mathbf{x}))\\big),p_{\\mathrm{m}}\\big(y\\mid f_{\\theta}(\\mathcal{R}(\\mathbf{x}))\\big)\\Big),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where cross-entropy $H(\\cdot,\\cdot)$ measures consistency for regularization, while the prediction $p_{\\mathrm{m}}(y\\ |$ $f_{\\pmb{\\theta}}(\\mathbf{x}))$ serves as a pseudo label. This setup captures a teacher-student dynamic, where $\\alpha(\\mathbf{x})$ and ${\\mathcal{A}}(\\mathbf{x})$ represent the teacher (weakly-augmented) and student (strongly-augmented) instances, respectively. ", "page_idx": 3}, {"type": "text", "text": "2.2 Class Prior Gap between SSL and GCD ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The GCD task pushes the boundaries of SSL by questioning the closed-world assumption that all classes in the unlabeled dataset $\\mathbb{D}_{U}$ are previously known [69]. Instead, GCD incorporates new classes $\\mathbb{C}_{N}$ into the unlabeled dataset, demanding that the model learn to recognize and then correctly classify them [4, 21, 80, 5]. In this open-world setting, SSL methods face obstacles with new classes due to the lack of supervision [24, 56], resulting in significantly lower quality of pseudo-labels for these new classes than for the old ones (Fig. 2 left). This gap exacerbates the complexity of optimizing the consistency loss Eq. 1 for new classes, leading to learning instability and slow convergence (Fig. 2, middle). Such optimization issues lead to severe prediction bias, resulting in new classes\u2019 performance lagging behind that of old classes (Fig. 2, right), underlining the limitations of existing SSL techniques in GCD scenarios. More empirical analysis can be found in Appendix B.2. ", "page_idx": 3}, {"type": "text", "text": "3 How Consistency Loss Goes Awry: Unraveling the Pitfalls ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Acknowledging challenges presented by the absence of prior knowledge for new classes in traditional semi-supervised learning (SSL) methods is the first step toward addressing the complexities of openworld tasks. We further identify that the \u2018prior gap\u2019 manifests as issues in learning synchronization and representation discrepancy (Sec. 3.1). Our analysis targets the minimization of the energy function between teacher and student representations to bridge the \u2018prior gap\u2019. We find that aligning their attention on similar patterns reduces energy, indicating effective alignment and learning (Sec. 3.2). This key understanding paves the way for the development of our proposed methods, aiming to synchronize teacher-student attentions for improved model learning dynamics (Sec. 4). ", "page_idx": 3}, {"type": "text", "text": "3.1 What to Bridge the Class Prior ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The challenge of optimizing consistency loss leads to a learning gap between the student and the teacher, particularly evident when dealing with new classes (Fig. 1 top left). This gap causes the student to plateau, as it cannot keep pace with the teacher\u2019s more advanced understanding, which in turn restricts the teacher\u2019s progress in new classes Moreover, this learning gap also manifests itself in the divergent representations between teacher and student (Fig. 1 middle), specifically for new classes. Based on these observations, we revisit consistency loss (Eq. 1) in the closed-world setting. ", "page_idx": 3}, {"type": "text", "text": "Insight 3.1. The large discrepancy between $f_{\\theta}(\\alpha(x))$ and $f_{\\theta}(\\mathcal{A}(x))$ complicates maintaining consistency across the model predictions. To narrow this divide, an intuitive idea is to align $f_{\\theta}(\\alpha(\\mathbf{x}))$ more closely with $f_{\\theta}(\\mathcal{A}(\\mathbf{x}))$ , simplifying the optimization of $\\mathcal{L}_{c o n s}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{cons}}=\\frac{1}{|\\mathbb{D}_{U}|}\\sum_{\\mathbf{x}\\in\\mathbb{D}_{U}}d\\Big(p_{\\mathrm{m}}\\!\\left(y\\mid f_{\\theta}(\\alpha(\\mathbf{x}))-\\Delta\\mathfrak{R}\\right),p_{\\mathrm{m}}\\!\\left(y\\mid f_{\\theta}(\\mathcal{R}(\\mathbf{x}))\\right)\\!\\Big),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Delta\\Re$ aims to pull $f_{\\theta}(\\alpha(\\mathbf{x}))$ closer to $f_{\\theta}(\\mathcal{A}(\\mathbf{x}))$ . Ideally, $\\Delta\\Re$ would be adaptive, scaling with the discrepancy between $f_{\\theta}({\\mathcal{A}}(x))$ and $f_{\\theta}(\\alpha(x))$ , while avoiding make them too similar, which enables model to find a shortcut of $\\mathcal{L}_{c o n s}$ . ", "page_idx": 3}, {"type": "text", "text": "To design it, we delve into the vision transformer, a representation encoder that has significantly advanced the performance of the GCD task. We found that the self-attention mechanism excels at capturing critical image patterns: as depicted in Fig. 3 left, deeper features (after the 8th layer) reveal semantic, high-level commonalities (e.g., car shell) across all images; and the shallow features are more attuned to high-frequency, low-level details (e.g., color and texture). ", "page_idx": 3}, {"type": "text", "text": "3.2 Inconsistent Patterns Spoil the Whole Barrel ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Inconsistent patterns can disrupt learning, making it crucial to align stored and queried patterns effectively. To address this, we draw inspiration from the Hopfield Network [2] \u2014 an associative memory model known for its energy-based mechanism that naturally pulls similar patterns together ", "page_idx": 3}, {"type": "image", "img_path": "C4NbtYnyQg/tmp/49c39163f4e92ce56479a9f3ae0e648a303efbdfe1f8d1178e746db96712ceb9.jpg", "img_caption": ["Figure 3: Left: Attention heatmaps for teacher and student across attention layers. Right: Energy trend over epochs, with lower energy indicating less discrepancy in pattern recognition between teacher and student. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "(see Appendix A.1 for details). We follow Ramsauer et al. to define the energy function for a state pattern (query) $\\pmb{\\xi}\\in\\mathbb{R}^{d}$ , parameterized by $N$ stored (key) patterns $\\mathbf{X}=[\\mathbf{x}_{1},\\cdots,\\mathbf{x}_{N}]\\in\\mathbb{R}^{d\\times N}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nE(\\xi;\\mathbf{X})={\\frac{1}{2}}\\xi^{\\top}\\xi-\\operatorname{lse}(\\mathbf{X}^{\\top}\\xi,\\beta)+c,\\quad{\\mathrm{with~lse}}(\\mathbf{v},\\beta):=\\beta^{-1}\\log\\left(\\sum_{i=1}^{N}\\exp(\\mathbf{v}_{i})\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Minimizing $E(\\pmb{\\xi};\\mathbf{X})$ resembles retrieving stored pattern $\\mathbf{X}_{i}$ that is most similar to the query $\\xi$ , and log-sum-exp (lse) function is parameterized by $\\beta>0$ and $c$ is a preset constant. Particularly, the first term ensures the finiteness of the query, while the second term measures the alignment of the query with each stored pattern. The update rule for a state pattern $\\xi$ is equivalent to a gradient descent update of minimizing the energy $E$ with step size $\\eta=1$ [54], ensures that the query moves closer to the most similar stored pattern: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{\\xi}\\leftarrow\\pmb{\\xi}-\\eta\\nabla_{\\pmb{\\xi}}E(\\pmb{\\xi};\\mathbf{X})=\\pmb{\\xi}-\\mathrm{sm}(\\beta\\pmb{\\xi}^{\\top}\\mathbf{X})\\mathbf{X}^{\\top}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Moreover, the energy function is closely related to the Transformer\u2019s self-attention mechanism [67] (Appendix A.1.2). By extending the energy model from self-attention to cross-attention, we model the dynamics between student and teacher learning patterns. Taking the student representations $\\mathbf{f}_{s}=f(\\mathcal{A}(\\mathbf{x}))$ as examples, we have $\\mathbf{Q}_{s}=\\mathbf{f}_{s}\\mathbf{W}_{Q}$ and ${\\bf K}_{s}={\\bf f}_{s}{\\bf W}_{K}$ . By applying Eq. 3 to key and query matrices, we set energy functions to track the teacher-student relationship: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{E(\\mathbf{Q}_{s};\\mathbf{K}_{t})=\\displaystyle\\frac{\\alpha}{2}\\mathrm{diag}(\\mathbf{K}_{{\\mathbf{t}}}\\mathbf{K}_{{\\mathbf{t}}}^{T})-\\displaystyle\\sum_{i=1}^{N}\\mathrm{lse}(\\mathbf{Q}_{s}\\mathbf{k}_{t,i}^{T},\\boldsymbol{\\beta})+c,}\\\\ {E(\\mathbf{K}_{t})=\\mathrm{lse}\\left(\\displaystyle\\frac{1}{2}\\mathrm{diag}(\\mathbf{K}_{{\\mathbf{t}}}\\mathbf{K}_{{\\mathbf{t}}}^{T}),1\\right)=\\log\\displaystyle\\sum_{i=1}^{N}\\mathrm{exp}\\left(\\displaystyle\\frac{1}{2}\\mathbf{k}_{t,i}\\mathbf{k}_{t,i}^{T}\\right)+c,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $E(\\mathbf{Q}_{s};\\mathbf{K}_{t})$ indicates the alignment in learning patterns of the student and teacher; $\\mathbf{k}_{t,i}$ denotes the $i$ -th row vector of $\\mathbf{K}_{t}$ and $\\alpha\\,\\geq\\,0$ . Intuitively, $\\mathrm{lse}(\\mathbf{Q}_{s}\\mathbf{k}_{t,i}^{T},\\beta)$ captures the smooth maximum alignment between student queries $\\mathbf{q}_{s,i}$ and teacher keys $\\mathbf{k}_{t,i}$ . Specifically, it nudges each teacher key $\\mathbf{k}_{t,j}$ towards a semantic alignment with its most corresponding student query ${\\bf q}_{s,j}$ . The regularization term diag $(\\mathbf{K}_{\\mathbf{t}}\\mathbf{K}_{\\mathbf{t}}^{T})$ acts as a constraint on the energy levels of teacher\u2019s representation $\\mathbf{k}_{t,i}$ , guarding against any disproportionate increase during the maximization of $\\mathrm{lse}(\\mathbf{Q}_{\\mathbf{s}}\\mathbf{k}_{t,i}^{T},\\beta)$ . This ensures that no individual teacher representation becomes too closely mirrored in the student\u2019s representation, maintaining a diverse learning trajectory. ", "page_idx": 4}, {"type": "text", "text": "Insight 3.2. When applying closed-world consistency regularization to the GCD task, it becomes difficult to gradually reduce the energy $E(\\mathbf{Q}_{s};\\mathbf{K}_{t})$ as training progresses (Fig. 3 right). The sustained high energy demonstrated a flaw in the previous methods: teachers and students focused on identifying patterns that were inconsistent, leading to divergent learning paths. Specifically, when teachers and students focus on similar patterns (e.g., taillights), energy is reduced, indicating better prediction consistency and effective learning. In contrast, when their attention is distracted, the energy rises, leading to severe inconsistencies in predictions and making the optimization of $\\mathcal{L}_{c o n s}$ more difficult. ", "page_idx": 4}, {"type": "text", "text": "4 FlipClass: Teacher-Student Attention Alignment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Teacher Attention Update Rule ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Based on Insights 3.1 and 3.2, our objective is to minimize the energy function $E(\\mathbf{Q}_{s};\\mathbf{K}_{t})$ between teacher and student representations, thereby easing the optimization of ${\\mathcal{L}}_{\\mathrm{cons}}$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1. The minimization can be formulated as obtaining a maximum a posteriori probability $(M A P)$ estimate of teacher keys $\\mathbf{K}_{t}$ given a set of observed student queries $\\mathbf{Q}_{s}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\np(\\mathbf{K}_{t}|\\mathbf{Q}_{s})=\\frac{p(\\mathbf{Q}_{s}|\\mathbf{K}_{t})p(\\mathbf{K}_{t})}{p(\\mathbf{Q}_{s})},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $p(\\mathbf{Q}_{s}|\\mathbf{K}_{t})$ and $p(\\mathbf{K}_{t})$ are modeled by energy functions Eq. 5a and $5b$ , respectively. We approximate the posterior inference by the gradient of the log posterior, estimated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\mathbf{K}_{t}}\\log p(\\mathbf{K}_{t}|\\mathbf{Q}_{s})=-\\left(\\nabla_{\\mathbf{K}_{t}}E(\\mathbf{Q}_{s};\\mathbf{K}_{t})+\\nabla_{\\mathbf{K}_{t}}E(\\mathbf{K}_{t})\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=s m\\left(\\beta\\mathbf{Q}_{s}\\mathbf{K}_{\\mathrm{t}}^{T}\\right)\\mathbf{Q}_{s}-\\left(\\alpha\\mathbf{I}+\\mathcal{D}\\left(s m\\left(\\frac{1}{2}d i a g(\\mathbf{K}_{\\mathrm{t}}\\mathbf{K}_{\\mathrm{t}}^{T})\\right)\\right)\\right)\\mathbf{K}_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $s m(\\mathbf{v}):=\\exp\\left(\\mathbf{v}-l s e(\\mathbf{v},1)\\right)$ and $\\mathcal{D}(\\cdot)$ is a vector-to-diagonal-matrix operator. Incorporating Eq. 4, the update rule of teacher keys $\\mathbf{K}_{t}$ is derived as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\bf K}_{t}^{u p d a t e}={\\bf K}_{t}+\\gamma_{u p d a t e}\\left[\\left(s m\\left(\\beta K Q^{T}\\right)Q W_{K}^{T}\\right)-\\gamma_{\\mathrm{reg}}\\left(\\alpha{\\bf I}+\\mathcal{D}\\left(s m\\left(\\frac12\\operatorname{diag}\\left(K K^{T}\\right)\\right)\\right)K W_{K}^{T}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\alpha$ , \ud835\udefeupdate and $\\gamma_{r e g}$ are hyper-parameters. ", "page_idx": 5}, {"type": "text", "text": "For a proof, refer to Appendix A.2. The teacher-attention update rule in Theorem 4.1 minimizes an implicit energy function determined by student queries and teacher keys. It serves as using the student queries to search for the most similar teacher patterns in the stored set. As illustrated in Fig. 4, the update rule adjusts the teacher\u2019s attention in the direction of student attention, facilitating the retrieval of related patterns and improving semantic alignment. This design establishes a bidirectional information flow: the teacher not only imparts advanced knowledge to the student, but also adjusts guidance based on the student\u2019s learning effects, achieving a more cohesive learning dynamic. ", "page_idx": 5}, {"type": "image", "img_path": "C4NbtYnyQg/tmp/248e9a45436d4794a1576e1563a0f1f85b74400826f31c256145eb587309ee40.jpg", "img_caption": ["Figure 4: Framework of FlipClass demonstrating teacher-student interaction, where teacher\u2019s and student\u2019s attention is aligned by teacher\u2019s updating (Eq. 8). Then $\\mathcal{L}_{\\mathrm{rep}}$ and ${\\mathcal{L}}_{\\mathrm{cons}}$ are combined for optimization. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.2 Representation Learning and Parametric Classification ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Contrastive learning plus consistency regularization under the parametric paradigm has been demonstrated effective in GCD task [75]. Formally, given two views (random augmentations $\\mathbf{X}_{i}$ and $\\mathbf{x}_{i}^{\\prime}$ ) of the same image in a mini-batch $\\mathbb{B}$ , the supervised and self-supervised contrastive loss is written as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{rep}}^{s}=\\displaystyle\\frac{1}{|\\mathbb{B}^{\\prime}|}\\sum_{i\\in\\mathbb{B}^{\\prime}}\\frac{1}{|\\mathbb{N}_{i}|}\\sum_{q\\in\\mathbb{N}_{i}}-\\log\\displaystyle\\frac{\\exp(\\mathbf{z}_{i}^{T}\\boldsymbol{z}_{q}^{\\prime}/\\tau_{c})}{\\sum_{i^{\\prime}\\neq i}\\exp(\\mathbf{z}_{i}^{T}\\boldsymbol{z}_{i^{\\prime}}^{\\prime}/\\tau_{c})}},}\\\\ {\\mathcal{L}_{\\mathrm{rep}}^{u}=\\displaystyle\\frac{1}{|\\mathbb{B}|}\\sum_{i\\in\\mathbb{B}}-\\log\\displaystyle\\frac{\\exp(\\mathbf{z}_{i}^{T}\\mathbf{z}_{i^{\\prime}}^{\\prime}/\\tau_{u})}{\\sum_{i^{\\prime}\\neq i}\\exp(\\mathbf{z}_{i}^{T}\\boldsymbol{z}_{i^{\\prime}}^{\\prime}/\\tau_{u})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the feature $\\mathbf{z}_{i}=f(\\mathbf{x}_{i})$ and is $\\ell_{2}$ -normalised, and $\\tau_{u},\\,\\tau_{c}$ are temperature values. For $\\mathcal{L}_{\\mathrm{rep}}^{s}$ , $\\mathbb{N}_{i}$ indexes all other images in the same batch that hold the same label as $\\mathbf{X}_{i}$ . The representation learning loss is balanced with $\\lambda{\\mathrm{:~}}L_{\\mathrm{rep}}=(1-\\lambda)L_{\\mathrm{rep}}^{u}+\\lambda L_{\\mathrm{rep}}^{s}$ , where $\\mathbb{B^{\\prime}}$ corresponds to the labeled subset of $\\mathbb{B}$ . ", "page_idx": 5}, {"type": "table", "img_path": "C4NbtYnyQg/tmp/95e921fd61e17ffd2903a8341851c71be4910bc8f08478338482f5fc6b8ca683.jpg", "table_caption": ["Table 1: Evaluation on the Semantic Shift Benchmark (SSB). Bold values represent the best results, while underlined values represent the second-best results. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "The consistency regularization objectives (Eq. 1) are then simply cross-entropy loss $\\ell(q^{\\prime},p)\\;=\\;$ $\\textstyle-\\sum_{k}q^{\\prime}(k)\\log{\\dot{p}(k)}$ between the predictions and pseudo-labels or ground-truth labels: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{cons}}=\\left\\{\\frac{1}{|\\mathbb{B}|}\\sum_{i\\in\\mathbb{B}}\\ell(q_{i}^{\\prime},p_{i})-\\varepsilon H(\\bar{p})\\right.\\quad\\mathrm{for~unlabeled,}}\\\\ {\\frac{1}{|\\mathbb{B}^{\\prime}|}\\sum_{i\\in\\mathbb{B}^{\\prime}}\\ell(y_{i},p_{i})\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\mathrm{for~labeled.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The one-hot labels $y_{i}$ correspond to $\\mathbf{X}_{i}$ , and the soft pseudo-label $q_{i}^{\\prime}$ is produced by the teacher instance $\\alpha(\\mathbf{x})_{i}$ . Moreover, a mean-entropy regularizer [6], $\\begin{array}{r}{H(\\bar{p})=-\\sum_{k}\\bar{p}(k)\\log\\bar{p}(k)}\\end{array}$ , is included to encourage diverse predictions. The combined classification loss, ${\\mathcal{L}}_{\\mathrm{cons}}$ , balances unsupervised and supervised terms with a parameter $\\lambda$ . And the overall training objective is $\\mathcal{L}_{\\mathrm{rep}}+\\mathcal{L}_{\\mathrm{cons}}$ . ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Experimental Settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. We evaluate the effectiveness of FlipClass on three generic image recognition datasets (i.e., CIFAR-10/100 [34] and ImageNet-100 [20]), three fine-grained datasets [68] (i.e., CUB [71], Stanford Cars [33], and FGVC-Aircraft [46]) contained in Semantic Shift Benchmark (SSB) [68], and the challenging datasets Herbarium-19 [65], ImageNet-1k [20]. For each dataset, we first subsample $|\\mathbb{C}_{l}|$ seen (labeled) classes from all classes. Following GCD [69], we subsample $80\\%$ samples in CIFAR-100 and $50\\%$ samples in all other datasets from the seen classes to construct $\\mathbb{D}_{l}$ , while the remaining images are treated as $\\mathbb{D}_{u}$ (refer to Table 9). ", "page_idx": 6}, {"type": "text", "text": "Evaluation Protocols. The performance was evaluated by measuring accuracy between the model\u2019s cluster assignments and ground-truth labels on the test set, with three aspects: all instances (All), instances from old categories (Old), and instances from new categories (New). The number of categories in the unlabeled dataset $(|\\mathbb{C}_{u}|)$ is often unknown. Following previous studies [64, 85], we set $K$ (cluster number) equal to $\\left|\\mathbb{C}_{u}\\right|$ , as approximate cluster estimation is usually feasible in the real world. The estimation of the number of categories in unlabeled datasets can be found in the Appendix C.4. Further implementation details can be found in Appendix D.1. ", "page_idx": 6}, {"type": "image", "img_path": "C4NbtYnyQg/tmp/d76c6ee847ed7f4e53cea30725b71381b345b2e763991156bfc48a5e7eb84899.jpg", "img_caption": ["Figure 5: Ablation study results for FlipClass, indicate the critical role of strong augmentations, attention alignment, and regularization in model performance across multiple datasets. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "C4NbtYnyQg/tmp/a518a44a4698ea5242bb288914e3e40a83a1cf81ce7bcb00c5629c69bb3582ea.jpg", "table_caption": ["Table 2: Evaluation on three generic image recognition datasets. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.2 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare SOTA methods with ours in GCD using features from both DINO [15] and DINOv2 [51]. Our approach shows significant performance improvement, particularly in the recognition of \u2018New\u2019 classes across both the SSB fine-grained benchmark (Tab. 1) and generic image recognition datasets (Tab. 2), consistently surpassing existing SOTA methods. Moreover, in fine-grained image classification (Tab. 1), recognizing subtle differences between closely related categories is crucial, which is in contrast to coarse-grained datasets where the visual differences between classes are more obvious. In fine-grained settings, the risk of the model generating incorrect pseudo labels is higher, which makes consistency regularization counterproductive (Sec. 2.2). However, the results across these datasets demonstrate our model\u2019s capability to effectively adapt consistency regularization strategies from closed-world settings to more complex open-world scenarios. Moreover, the balanced accuracy between new and old classes on the CUB dataset (Tab. 1), also observed with methods like PCAL, CiPR, and AdaptGCD, can be attributed to the dataset\u2019s small size (6,000 images) and large class split (200). This limited data reduces the likelihood of overftiting to old classes, promoting more uniform performance across both categories. Additional results on Herbarium 19 and ImageNet-1k are detailed in the Appendix C.2. ", "page_idx": 7}, {"type": "text", "text": "5.3 Analysis and Discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Ablation Studies. Our ablation study, shown in Fig. 5, underscores the significance of our design choices. First, we replace the student\u2019s augmentations with the teacher\u2019s, i.e., using only weak augmentations, the performance on \u2018New\u2019 classes significantly declines (2nd set of bars). This underscores the importance of strong augmentations for the student, which are essential to bolster generalizability. Then we validate the importance of attention alignment in Eq. 8 (3rd set of bars), we see performance drop across both \u2018Old\u2019 and \u2018New\u2019 classes, affirming that our attention alignment strategy is crucial for maintaining a consistent learning pace between the teacher and student, leading to sustained performance gains. Finally, the 4th set of bars verifies the role of regularization during (a) Comparison of attention alignment methods, show- (b) Categorize errors for FlipClass on CIFAR100 and casing the effectiveness of our teacher-attention update CUB, showing the reduction in prediction bias for strategy over alternatives in improving classification \u2018False Old\u2019 and \u2018False New\u2019 classes with different upaccuracy for \u2018Old\u2019 (solid) and \u2018New\u2019 (dotted) classes. date rates (\ud835\udefeupdate), highlighting model robustness. ", "page_idx": 7}, {"type": "image", "img_path": "C4NbtYnyQg/tmp/2d696a86c96794e24d84862980aacd5c71632c5f528ff9ac6c18f797e6a1ef92.jpg", "img_caption": ["Figure 6: Accuracy and representation alignment with different strategies: (1) initial state, (2) distribution alignment, (3) FixMatch, and (4) our teacher-attention update. Performance on \u2018New\u2019 and \u2018Old\u2019 classes are shown, alongside alignment of teacher (red) and student (blue) representation. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "C4NbtYnyQg/tmp/3ae1db34f709fd41d7493ea0e2d06a23e7480a0e9f0f3a9f0acd9efd927f3314.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Figure 7: Attention alignment methods comparison and categorize errors with different update rates. ", "page_idx": 8}, {"type": "text", "text": "the attention update, which integrates the prior energy of the teacher, preventing any single student pattern from overly influencing the teacher\u2019s attention. ", "page_idx": 8}, {"type": "text", "text": "Enhanced Consistency through Attention Alignment. To validate our assumption on modeling $\\mathfrak{R}$ in Eq. 2, we showcase how distribution-based strategies (distribution alignment [11], FixMatch [63]) and our representation-based method, attention alignment, achieve consistency on new classes from CUB dataset (Fig. 6). Our method stands out by significantly reducing the discrepancy between the representations of teacher (weakly-augmented) and student (strongly-augmented) data. This representation-based alignment leads to a more consistent learning process and is evidenced by the superior accuracies we achieve for both \u2018Old\u2019 and \u2018New\u2019 classes. ", "page_idx": 8}, {"type": "text", "text": "Design Choice of Attention Alignment. We experimented with various techniques to model $\\mathfrak{R}$ in Eq. 2, including scheduled data augmentation (SDA), increasing similarity between $\\mathbf{Q}_{s}$ and $\\mathbf{K}_{t}$ via $\\ell_{2}$ norm, Kullback-Leibler divergence (KLD) or CORrelation ALignment (CORAL) loss (see details in Appendix D.1). As shown in Fig. 7a, our teacher-attention update strategy outperforms these alternatives on both CUB and SCars datasets. ", "page_idx": 8}, {"type": "text", "text": "FlipClass mitigates prediction bias. We verify the effectiveness and robustness of FlipClass, by diagnosing the model\u2019s classification errors under four different $(\\gamma_{\\mathrm{update}})$ as defined in Eq. 8. As depicted in Fig. 7b, both \u201cFalse New\" and \u201cFalse Old\" errors are consistently mitigated\u2014where \u2018Old\u2019 class samples are mistakenly labeled as \u2018New\u2019 and vice-versa. Moreover, as illustrated in Fig.8b bottom, FlipClass outperforms leading methods [55] by moving closer to the true class distribution, yielding higher and less biased accuracies across all classes. ", "page_idx": 8}, {"type": "text", "text": "Does the improved energy dynamic make for performance gains? Fig. 8a top shows that aligning attention in these deeper (9-10) layers yields the highest performance on SCars dataset. And Fig. 8a bottom displays a greater reduction of energy $E(\\mathbf{Q}_{s};\\mathbf{K}_{t})$ in deeper layers. Moreover, this trend highlights that attention alignment constantly maintains lower energy levels than without, indicating improved alignment of student patterns with teacher updating. ", "page_idx": 8}, {"type": "text", "text": "How does FlipClass change the representations? Fig. 8b showcases representation enhancements with FlipClass against the leading method, InfoSieve [55], using the same t-SNE and PCA components to ensure consistent projection space and scale. FlipClass forms clusters with higher compactness and purity, demonstrating enhanced feature discrimination and less inter-class confusion. The zoom-in comparison on CUB dataset is provided in Fig. 9, which showcases that FlipClass achieves more distinct and well-separated clusters. Further, we assess prediction bias and class", "page_idx": 8}, {"type": "text", "text": "layers. Bottom: Energy tracking across attention layers and epochs. (Both on SCars dataset) ", "page_idx": 9}, {"type": "image", "img_path": "C4NbtYnyQg/tmp/ef67496f707af2c225228fd90fdebcc62595645051a93b194b96d496efd44171.jpg", "img_caption": ["Figure 8: Attention alignment improves energy dynamic and brings performance gains. ", "(b) Comparison of representations and classwise accuracy between InfoSieve [55] and FlipClass on CUB. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "specific accuracies. Unlike InfoSieve\u2019s skewed predictions, FlipClass aligns better with true class distributions, and significantly improves over the tail classes in CUB dataset (More experiments in Appendix C.3). ", "page_idx": 9}, {"type": "image", "img_path": "C4NbtYnyQg/tmp/3f7f13fcaf370a60e925792507f5364cd60d6740cbdabaa526f1ba57514b1c51.jpg", "img_caption": ["Figure 9: Zoom-in comparison of InfoSieve and FlipClass on the CUB dataset using t-SNE and PCA. FlipClass shows improved cluster separation and compactness. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper introduces FlipClass, a dynamic teacher-student attention alignment strategy for improving learning synchronization, providing a new view on applying closed-world models to open-world task of GCD. By aligning the attention of teacher and student, FlipClass bridges the learning gap between them, resulting in performance improvement on both old and new classes. Extensive experiments and analysis demonstrate that FlipClass outperforms existing state-of-the-art methods across diverse datasets by a large margin. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by National Science and Technology Major Project (2022ZD0117102), National Natural Science Foundation of China (62293551, 62377038,62177038,62277042). Project of China Knowledge Centre for Engineering Science and Technology, Project of Chinese academy of engineering \u201cThe Online and Offline Mixed Educational Service System for \u2018The Belt and Road\u2019 Training in MOOC China\". \u201cLENOVO-XJTU\" Intelligent Industry Joint Laboratory Project. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] G\u00f6k\u00e7e Ak\u00e7ay\u0131r and Murat Ak\u00e7ay\u0131r. The flipped classroom: A review of its advantages and challenges. Computers & Education, 126:334\u2013345, 2018.   \n[2] S-I Amari. Learning patterns and pattern sequences by self-organizing nets of threshold elements. IEEE Transactions on computers, 100(11):1197\u20131206, 1972.   \n[3] Wenbin An, Wenkai Shi, Feng Tian, Haonan Lin, QianYing Wang, Yaqiang Wu, Mingxiang Cai, Luyan Wang, Yan Chen, Haiping Zhu, et al. Generalized category discovery with large language models in the loop. arXiv preprint arXiv:2312.10897, 2023.   \n[4] Wenbin An, Feng Tian, Qinghua Zheng, Wei Ding, QianYing Wang, and Ping Chen. Generalized category discovery with decoupled prototypical network. In Proceedings of the AAAI Conference on Artificial Intelligence, number 11, pages 12527\u201312535, 2023.   \n[5] Wenbin An, Feng Tian, Wenkai Shi, Yan Chen, Yaqiang Wu, Qianying Wang, and Ping Chen. Transfer and alignment network for generalized category discovery. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 10856\u201310864, 2024. [6] Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Mike Rabbat, and Nicolas Ballas. Masked siamese networks for label-efficient learning. In European Conference on Computer Vision, pages 456\u2013473. Springer, 2022.   \n[7] Isaiah T Awidi and Mark Paynter. The impact of a flipped classroom approach on student learning experience. Computers & education, 128:269\u2013283, 2019.   \n[8] Philip Bachman, Ouais Alsharif, and Doina Precup. Learning with pseudo-ensembles. Advances in neural information processing systems, 27, 2014. [9] Jianhong Bai, Zuozhu Liu, Hualiang Wang, Ruizhe Chen, Lianrui Mu, Xiaomeng Li, Joey Tianyi Zhou, Yang Feng, Jian Wu, and Haoji Hu. Towards distribution-agnostic generalized category discovery. Advances in Neural Information Processing Systems, 36, 2024.   \n[10] Anwesha Banerjee, Liyana Sahir Kallooriyakath, and Soma Biswas. Amend: Adaptive margin and expanded neighborhood for efficient generalized category discovery. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2101\u20132110, 2024.   \n[11] David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring. In International conference on learning representations, 2020.   \n[12] Zhaowei Cai, Avinash Ravichandran, Subhransu Maji, Charless Fowlkes, Zhuowen Tu, and Stefano Soatto. Exponential moving average normalization for self-supervised and semisupervised learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 194\u2013203, 2021.   \n[13] Kaidi Cao, Maria Brbic, and Jure Leskovec. Open-world semi-supervised learning. arXiv preprint arXiv:2102.03526, 2021.   \n[14] Susan Carey. The origin of concepts. Journal of Cognition and Development, 1(1):37\u201341, 2000.   \n[15] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9650\u20139660, 2021.   \n[16] Florent Chiaroni, Jose Dolz, Ziko Imtiaz Masud, Amar Mitiche, and Ismail Ben Ayed. Mutual information-based generalized category discovery. arXiv preprint arXiv:2212.00334, 2, 2022.   \n[17] Sua Choi, Dahyun Kang, and Minsu Cho. Contrastive mean-shift learning for generalized category discovery. arXiv preprint arXiv:2404.09451, 2024.   \n[18] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 702\u2013703, 2020.   \n[19] Mete Demircigil, Judith Heusel, Matthias L\u00f6we, Sven Upgang, and Franck Vermet. On a model of associative memory with huge storage capacity. Journal of Statistical Physics, 168:288 \u2013 299, 2017. URL https://api.semanticscholar.org/CorpusID:119317128.   \n[20] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[21] Yixin Fei, Zhongkai Zhao, Siwei Yang, and Bingchen Zhao. Xcon: Learning with experts for fine-grained category discovery. arXiv preprint arXiv:2208.01898, 2022.   \n[22] Enrico Fini, Enver Sangineto, St\u00e9phane Lathuili\u00e8re, Zhun Zhong, Moin Nabi, and Elisa Ricci. A unified objective for novel class discovery. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9284\u20139292, 2021.   \n[23] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. Advances in neural information processing systems, 17, 2004.   \n[24] Lan-Zhe Guo, Yi-Ge Zhang, Zhi-Fan Wu, Jie-Jing Shao, and Yu-Feng Li. Robust semisupervised learning when not all classes have labels. Advances in Neural Information Processing Systems, 35:3305\u20133317, 2022.   \n[25] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. Advances in neural information processing systems, 31, 2018.   \n[26] Kai Han, Andrea Vedaldi, and Andrew Zisserman. Learning to discover novel visual categories via deep transfer clustering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8401\u20138409, 2019.   \n[27] Kai Han, Sylvestre-Alvise Rebuff,i Sebastien Ehrhardt, Andrea Vedaldi, and Andrew Zisserman. Automatically discovering and learning new visual categories with ranking statistics. In International conference on learning representations, 2020.   \n[28] Shaozhe Hao, Kai Han, and Kwan-Yee K Wong. Cipr: An efficient framework with crossinstance positive relations for generalized category discovery. arXiv preprint arXiv:2304.06928, 2023.   \n[29] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data processing method to improve robustness and uncertainty. arXiv preprint arXiv:1912.02781, 2019.   \n[30] Yen-Chang Hsu, Zhaoyang Lv, and Zsolt Kira. Learning to cluster in order to transfer across domains and tasks. In International conference on learning representations, 2018.   \n[31] Yen-Chang Hsu, Zhaoyang Lv, Joel Schlosser, Phillip Odom, and Zsolt Kira. Multi-class classification without multi-class labels. In International conference on learning representations, 2019.   \n[32] Zhanghan Ke, Daoye Wang, Qiong Yan, Jimmy Ren, and Rynson WH Lau. Dual student: Breaking the limits of the teacher in semi-supervised learning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6728\u20136736, 2019.   \n[33] Jonathan Krause, Michael Stark, and Li Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on computer vision workshops, pages 554\u2013561, 2013.   \n[34] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical Report 0, University of Toronto, Toronto, Ontario, 2009. URL https://www.cs. toronto.edu/\\~kriz/learning-features-2009-TR.pdf.   \n[35] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012.   \n[36] Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. In International conference on learning representations, 2017.   \n[37] Dong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. Workshop on challenges in representation learning, ICML, 3(2):896, 2013.   \n[38] Wenbin Li, Zhichen Fan, Jing Huo, and Yang Gao. Modeling inter-class and intra-class constraints in novel class discovery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3449\u20133458, 2023.   \n[39] Haonan Lin, Wenbin An, Yan Chen, Feng Tian, Wei Ding, QianYing Wang, Ping Chen, Yaqiang Wu, mingxiang cai, and Guang Dai. Semantic-enhanced prototypical network for universal novel category discovery, 2024. URL https://openreview.net/forum?id $\\fallingdotseq$ UoaHvbjpbG.   \n[40] Haonan Lin, Wenbin An, Yan Chen, Feng Tian, Yuzhe Yao, Wei Ding, Qianying Wang, and Ping Chen. A tri-branch network with prototype-aware matching for universal category discovery. In 2024 IEEE International Conference on Multimedia and Expo (ICME), pages 1\u20136. IEEE, 2024.   \n[41] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. Advances in neural information processing systems, 33:21464\u201321475, 2020.   \n[42] Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mezard, and Lenka Zdeborov\u00e1. Learning curves of generic features maps for realistic datasets with a teacher-student model. Advances in Neural Information Processing Systems, 34:18137\u201318151, 2021.   \n[43] Yucen Luo, Jun Zhu, Mengxi Li, Yong Ren, and Bo Zhang. Smooth neighbors on teacher graphs for semi-supervised learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8896\u20138905, 2018.   \n[44] Shijie Ma, Fei Zhu, Zhun Zhong, Xu-Yao Zhang, and Cheng-Lin Liu. Active generalized category discovery. arXiv preprint arXiv:2403.04272, 2024.   \n[45] James MacQueen et al. Some methods for classification and analysis of multivariate observations. Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, 1(14): 281\u2013297, 1967.   \n[46] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Finegrained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.   \n[47] Geoffrey J McLachlan. Iterative reclassification procedure for constructing an asymptotically optimal rule of allocation in discriminant analysis. Journal of the American Statistical Association, 70(350):365\u2013369, 1975.   \n[48] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 41(8):1979\u20131993, 2018.   \n[49] Gregory Murphy. The big book of concepts. MIT press, 2004.   \n[50] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In European conference on computer vision, pages 69\u201384. Springer, 2016.   \n[51] Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.   \n[52] Jona Otholt, Christoph Meinel, and Haojin Yang. Guided cluster aggregation: A hierarchical approach to generalized category discovery. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2618\u20132627, 2024.   \n[53] Hieu Pham, Zihang Dai, Qizhe Xie, and Quoc V Le. Meta pseudo labels. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11557\u201311568, 2021.   \n[54] Hubert Ramsauer, Bernhard Sch\u00e4f,l Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlovic\u00b4, Geir Kjetil Sandve, et al. Hopfield networks is all you need. In International conference on learning representations, 2021.   \n[55] Sarah Rastegar, Hazel Doughty, and Cees Snoek. Learn to categorize or categorize to learn? self-coding for generalized category discovery. Advances in Neural Information Processing Systems, 36, 2024.   \n[56] Mamshad Nayeem Rizve, Navid Kardan, and Mubarak Shah. Towards realistic semi-supervised learning. In European Conference on Computer Vision, pages 437\u2013455. Springer, 2022.   \n[57] Chuck Rosenberg, Martial Hebert, and Henry Schneiderman. Semi-supervised self-training of object detection models. Carnegie Mellon University, 2005.   \n[58] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013252, 2015. doi: 10.1007/s11263-015-0816-y.   \n[59] Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. Advances in neural information processing systems, 29, 2016.   \n[60] Felix Sarnthein, Gregor Bachmann, Sotiris Anagnostidis, and Thomas Hofmann. Random teachers are good teachers. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 30022\u201330041. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/ v202/sarnthein23a.html.   \n[61] Walter J Scheirer, Anderson de Rezende Rocha, Archana Sapkota, and Terrance E Boult. Toward open set recognition. IEEE transactions on pattern analysis and machine intelligence, 35(7): 1757\u20131772, 2012.   \n[62] Henry Scudder. Probability of error of some adaptive pattern-recognition machines. IEEE Transactions on Information Theory, 11(3):363\u2013371, 1965.   \n[63] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semisupervised learning with consistency and confidence. Advances in neural information processing systems, 33:596\u2013608, 2020.   \n[64] Yiyou Sun and Yixuan Li. Opencon: Open-world contrastive learning. Transactions on Machine Learning Research, 2022.   \n[65] Kiat Chuan Tan, Yulong Liu, Barbara Ambrose, Melissa Tulig, and Serge Belongie. The herbarium challenge 2019 dataset. arXiv preprint arXiv:1906.05372, 2019.   \n[66] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems, 30, 2017.   \n[67] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[68] Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Open-set recognition: A good closed-set classifier is all you need? In International conference on learning representations, 2022.   \n[69] Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Generalized category discovery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7492\u20137501, 2022.   \n[70] Sagar Vaze, Andrea Vedaldi, and Andrew Zisserman. No representation rules them all in category discovery. Advances in Neural Information Processing Systems, 36, 2024.   \n[71] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. Technical report, California Institute of Technology, 2011.   \n[72] Hongjun Wang, Sagar Vaze, and Kai Han. SPTNet: An efficient alternative framework for generalized category discovery with spatial prompt tuning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=3QLkwU40EE.   \n[73] Ye Wang, Yaxiong Wang, Yujiao Wu, Bingchen Zhao, and Xueming Qian. Beyond known clusters: Probe new prototypes for efficient generalized class discovery. arXiv preprint arXiv:2404.08995, 2024.   \n[74] Yu Wang, Zhun Zhong, Pengchong Qiao, Xuxin Cheng, Xiawu Zheng, Chang Liu, Nicu Sebe, Rongrong Ji, and Jie Chen. Discover and align taxonomic context priors for open-world semi-supervised learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[75] Xin Wen, Bingchen Zhao, and Xiaojuan Qi. Parametric classification for generalized category discovery: A baseline study. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16590\u201316600, 2023.   \n[76] Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. Unsupervised data augmentation for consistency training. Advances in neural information processing systems, 33: 6256\u20136268, 2020.   \n[77] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10687\u201310698, 2020.   \n[78] Yi Xu, Lei Shang, Jinxing Ye, Qi Qian, Yu-Feng Li, Baigui Sun, Hao Li, and Rong Jin. Dash: Semi-supervised learning with dynamic thresholding. In International Conference on Machine Learning, pages 11525\u201311536. PMLR, 2021.   \n[79] Muli Yang, Yuehua Zhu, Jiaping Yu, Aming Wu, and Cheng Deng. Divide and conquer: Compositional experts for generalized novel class discovery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14268\u201314277, 2022.   \n[80] Qing Yu, Daiki Ikami, Go Irie, and Kiyoharu Aizawa. Self-labeling framework for novel category discovery over domains. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 3161\u20133169, 2022.   \n[81] Alan L Yuille and Anand Rangarajan. The concave-convex procedure (cccp). Advances in neural information processing systems, 14, 2001.   \n[82] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6023\u20136032, 2019.   \n[83] Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, and Takahiro Shinozaki. Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling. Advances in Neural Information Processing Systems, 34:18408\u201318419, 2021.   \n[84] He Zhang and Vishal M Patel. Sparse representation-based open set recognition. IEEE transactions on pattern analysis and machine intelligence, 39(8):1690\u20131696, 2016.   \n[85] Sheng Zhang, Salman Khan, Zhiqiang Shen, Muzammal Naseer, Guangyi Chen, and Fahad Shahbaz Khan. Promptcal: Contrastive affinity learning via auxiliary prompts for generalized novel category discovery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3479\u20133488, 2023.   \n[86] Bingchen Zhao and Kai Han. Novel visual category discovery with dual ranking statistics and mutual knowledge distillation. Advances in Neural Information Processing Systems, 34: 22982\u201322994, 2021.   \n[87] Mingkai Zheng, Shan You, Lang Huang, Fei Wang, Chen Qian, and Chang Xu. Simmatch: Semi-supervised learning with similarity matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14471\u201314481, 2022.   \n[88] Zhun Zhong, Enrico Fini, Subhankar Roy, Zhiming Luo, Elisa Ricci, and Nicu Sebe. Neighborhood contrastive learning for novel class discovery. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10867\u201310875, 2021.   \n[89] Zhi-Hua Zhou. A brief introduction to weakly supervised learning. National science review, 5 (1):44\u201353, 2018.   \n[90] Xiaojin Jerry Zhu. Semi-supervised learning literature survey. University of Wisconsin-Madison Department of Computer Sciences, 2005. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A Theory Assumptions and Proofs 18 ", "page_idx": 16}, {"type": "text", "text": "A.1 Preliminaries: Hopfield Network Energy Function . . . 18   \nA.2 Derivation of The Teacher Attention Update Rule 19 ", "page_idx": 16}, {"type": "text", "text": "B Extended Experimental Analysis of Attention Alignment 21 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Representation Discrepancy of Old and New Classes 21   \nB.2 Enhanced Consistency Loss Optimization . . . 21   \nB.3 Attention Alignment Bridges Prior Gap and Benefits Synchronized Learning . . . 22   \nB.4 Attention Specialization in Deep Network Layers . . . . . . 23   \nB.5 Attention Alignment in Layer Selection . . . 23   \nB.6 Time Efficiency of Attention Alignment . . . 24 ", "page_idx": 16}, {"type": "text", "text": "C More Experimental Results 25 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Main Results with Error Bars . . 25   \nC.2 Results on Complex Datasets . . 25   \nC.3 Clustering and Per-class Prediction Distribution 25   \nC.4 Robustness to Number of Classes . . 26   \nC.5 Results with Varying Proportion of Old Classes 27 ", "page_idx": 16}, {"type": "text", "text": "D Experimental Settings 28 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Implementation Details . 28   \nD.2 Design of Data Augmentation 28   \nD.3 Datasets . . . 29   \nD.4 Other Alignment Strategies . . 30 ", "page_idx": 16}, {"type": "text", "text": "E Related Works 30 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "E.1 Consistency Regularization . . . 30   \nE.2 Novel Category Discovery . . . 31   \nE.3 Generalized Category Discovery . . . 31 ", "page_idx": 16}, {"type": "text", "text": "F Limitations and Future Work 31 ", "page_idx": 16}, {"type": "text", "text": "G Broader Impacts 31 ", "page_idx": 16}, {"type": "text", "text": "A Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A.1 Preliminaries: Hopfield Network Energy Function ", "page_idx": 17}, {"type": "text", "text": "A.1.1 Global Convergence of Hopfield Network Energy Function ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We first provide the formulation of Hopfield network energy function, and present its convergence, which build the fundamental of our one-step Teacher-Attention Update Strategy. Ramsauer et al. proposed a new energy function that is a modification of the energy of modern Hopfield networks [19], and a new update rule which can be proven to converge to stationary points of the energy. ", "page_idx": 17}, {"type": "text", "text": "Given $N$ stored (key) patterns $\\mathbf{x}_{i}\\,\\in\\,\\mathbb{R}^{d}$ represented by the matrix $\\mathbf{X}=(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{N})$ with the state (query) pattern $\\pmb{\\xi}\\in\\mathbb{R}^{d}$ , the energy function $E$ of the modern Hopfield networks can be expressed: ", "page_idx": 17}, {"type": "equation", "text": "$$\nE=\\exp(\\mathrm{lse}(1,{\\bf X}^{T}\\pmb{\\xi})),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathrm{lse}(\\beta,x)=\\beta^{-1}\\log\\left(\\sum_{i=1}^{N}\\exp(\\beta\\mathbf{x}_{i})\\right)}\\end{array}$ is the log-sum-exp function (lse) for $0<\\beta$ . Ramsauer et al. then proposed to take the logarithm of the negative energy of modern Hopfield networks and add a quadratic term of the current state to ensure that the norm of the state vector $\\pmb{\\xi}$ remains finite and the energy is bounded, reads: ", "page_idx": 17}, {"type": "equation", "text": "$$\nE=-\\mathrm{lse}(\\boldsymbol{\\beta},\\mathbf{X}^{T}\\boldsymbol{\\xi})+\\frac{1}{2}\\boldsymbol{\\xi}^{T}\\boldsymbol{\\xi}+\\boldsymbol{\\beta}^{-1}\\log\\ensuremath{N}+\\frac{1}{2}\\ensuremath{M^{2}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using $p=\\operatorname{softmax}(\\beta\\mathbf{X}^{T}\\pmb{\\xi})$ , the update rule is defined as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\xi}^{\\mathrm{new}}=f(\\pmb{\\xi})=\\mathbf{X}\\boldsymbol{p}=\\mathbf{X}\\mathrm{softmax}(\\beta\\mathbf{X}^{T}\\pmb{\\xi}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is the Concave-Convex Procedure (CCCP) for minimizing the energy $E$ and can be proven as converging globally. ", "page_idx": 17}, {"type": "text", "text": "Theorem A.1 (Global Convergence (Zangwill): Energy). The update rule Eq. 10 converges globally: For $\\pmb{\\xi}^{t+1}=f(\\pmb{\\xi}^{t})$ , the energy $E(\\pmb{\\xi}^{t})\\rightarrow E(\\pmb{\\xi}^{*})$ for $t\\to\\infty$ and a fixed point $\\boldsymbol{\\xi}^{*}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. The Concave-Convex Procedure (CCCP) [81] minimizes a function that is the sum of a concave function and a convex function. And since \ud835\udc59\ud835\udc60\ud835\udc52is a convex, \u2212lse a concave function. Therefore, the energy function $E(\\pmb\\xi)$ is the sum of the convex function $\\begin{array}{r}{\\dot{E}_{1}(\\pmb{\\xi})=\\frac{1}{2}\\pmb{\\xi}^{T}\\pmb{\\xi}+C_{1}}\\end{array}$ and the concave function $E_{2}(\\pmb{\\xi})=-\\mathrm{lse}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{E(\\pmb{\\xi})=E_{1}(\\pmb{\\xi})+E_{2}(\\pmb{\\xi}),}}\\\\ {{E_{1}(\\pmb{\\xi})=\\displaystyle\\frac{1}{2}\\pmb{\\xi}^{T}\\pmb{\\xi}+\\beta^{-1}\\ln N+\\frac{1}{2}\\pmb{M}^{2}=\\frac{1}{2}\\pmb{\\xi}^{T}\\pmb{\\xi}+C_{1},}}\\\\ {{E_{2}(\\pmb{\\xi})=-\\mathrm{lse}(\\beta,\\mathbf{X}^{T}\\pmb{\\xi}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $C_{1}$ does not depend on $\\pmb{\\xi}$ . ", "page_idx": 17}, {"type": "text", "text": "The Concave-Convex Procedure (CCCP) applied to $E$ is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\nabla_{\\xi}E_{1}(\\pmb{\\xi}^{t+1})=-\\nabla_{\\xi}E_{2}(\\pmb{\\xi}^{t}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which results in the update rule: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\pmb{\\xi}^{t+1}=\\mathbf{X}p^{t}=X\\mathrm{softmax}(\\beta\\mathbf{X}^{T}\\pmb{\\xi}^{t})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $p^{t}=\\mathrm{softmax}(\\beta\\mathbf{X}^{T}\\pmb{\\xi}^{t})$ . This is the update rule in Eq. 10. ", "page_idx": 17}, {"type": "text", "text": "A.1.2 Hopfield Update Rule is Attention of The Transformer ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The Hopfield network update rule is the attention mechanism used in transformer. Assume $N$ stored (key) patterns $\\mathbf{X}_{i}$ and $S$ state (query) patterns $\\mathbf{r}_{i}$ with dimension $d_{k}$ , we can have $\\mathbf{X}=(\\mathbf{x}_{1},\\hdots,\\mathbf{x}_{N})^{T}$ and $\\mathbf{R}\\;=\\;(\\mathbf{r}_{1},\\ldots,\\mathbf{r}_{S})^{T}$ combine the $\\mathbf{X}_{i}$ and $\\mathbf{r}_{i}$ as row vectors. Define the key as $\\mathbf{k}_{i}\\ =\\ \\mathbf{W}_{K}^{T}\\mathbf{x}_{i}$ $\\mathbf{q}_{i}=\\mathbf{W}_{Q}^{T}\\mathbf{r}_{i}$ , and multiply the result of the update rule (Eq. 10) with $\\mathbf{W}_{V}$ . By defining the matrices ${\\bf K}=({\\bf X}{\\bf W}_{K})^{T}$ , ${\\bf Q}=({\\bf R}{\\bf W}_{Q})^{T}$ , and $\\mathbf{V}=\\mathbf{X}\\mathbf{W}_{K}\\mathbf{W}_{V}=\\mathbf{K}^{T}\\mathbf{W}_{V}$ , where $\\mathbf{W}_{K}\\in\\mathbb{R}^{d_{x}\\times d_{k}}$ , $\\mathbf{W}_{Q}\\in\\mathbb{R}^{d_{r}\\times d_{k}}$ , $\\mathbf{W}_{V}\\in\\mathbb{R}^{d_{k}\\times d_{o}}$ . If $\\beta=1/\\sqrt{d_{k}}$ and softmax $\\in\\mathbb{R}^{N}$ is changed to a row vector, there is: ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname{softmax}\\left(\\frac{1}{\\sqrt{d_{k}}}\\mathbf{Q}\\mathbf{K}^{T}\\right)\\mathbf{V}=\\operatorname{softmax}\\left(\\beta\\mathbf{R}\\mathbf{W}_{Q}\\mathbf{W}_{K}^{T}\\mathbf{X}^{T}\\right)\\mathbf{X}\\mathbf{W}_{K}\\mathbf{W}_{V},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the left part is the transformer attention, while the right part is the update rule Eq. 4 multiplied by $\\mathbf{W}_{V}$ . ", "page_idx": 18}, {"type": "text", "text": "A.2 Derivation of The Teacher Attention Update Rule ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We first provide several lemmas for the derivation of the teacher-attention update rule (Eq. 8). Lemma A.2. For a given column vector $\\mathbf{x}\\in\\mathbb{R}^{N}$ , we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\frac{\\partial\\log\\sum\\exp(\\beta\\mathbf{x})}{\\partial\\mathbf{x}}}=\\operatorname{softmax}(\\beta\\mathbf{x})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Consider $\\begin{array}{r}{S=\\sum_{i=1}^{N}\\exp(\\beta\\mathbf{x}_{i})}\\end{array}$ and $\\begin{array}{r}{f(\\mathbf{x})=\\log S,\\,\\frac{\\partial f(\\mathbf{x})}{\\partial\\mathbf{x}}}\\end{array}$ can be computed as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\frac{\\partial f(\\mathbf{x})}{\\partial\\mathbf{x}_{j}}}={\\frac{\\partial}{\\partial\\mathbf{x}_{j}}}\\log S={\\frac{1}{S}}\\cdot{\\frac{\\partial S}{\\partial\\mathbf{x}_{j}}}\\,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The partial derivative $\\frac{\\partial S}{\\partial\\mathbf{x}_{j}}$ is: ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\frac{\\partial S}{\\partial\\mathbf{x}_{j}}}={\\frac{\\partial}{\\partial\\mathbf{x}_{j}}}\\sum_{i=1}^{N}\\exp(\\beta\\mathbf{x}_{i})=\\sum_{i=1}^{N}{\\frac{\\partial}{\\partial\\mathbf{x}_{j}}}\\exp(\\beta\\mathbf{x}_{i})=\\sum_{i=1}^{N}\\beta\\exp(\\beta\\mathbf{x}_{i})\\delta_{i j}=\\beta\\exp(\\beta\\mathbf{x}_{j})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\delta_{i j}$ is the Kronecker delta. ", "page_idx": 18}, {"type": "text", "text": "Substitute $\\frac{\\partial S}{\\partial\\mathbf{x}_{j}}$ back into the expression for $\\frac{\\partial f(\\mathbf{x})}{\\partial\\mathbf{x}_{j}}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\frac{\\partial f(\\mathbf{x})}{\\partial\\mathbf{x}_{j}}}={\\frac{1}{S}}\\cdot\\beta\\exp(\\beta\\mathbf{x}_{j})=\\beta\\cdot{\\frac{\\exp(\\beta\\mathbf{x}_{j})}{S}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Recognize that $\\frac{\\exp(\\beta\\mathbf{x}_{j})}{S}$ is the $j$ -th component of the softmax function applied to $\\beta\\mathbf{x}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{softmax}(\\beta\\mathbf{x})_{j}=\\frac{\\exp(\\beta\\mathbf{x}_{j})}{\\sum_{i=1}^{N}\\exp(\\beta\\mathbf{x}_{i})}=\\frac{\\exp(\\beta\\mathbf{x}_{j})}{S}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\frac{\\partial\\log\\sum_{i=1}^{N}\\exp(\\beta\\mathbf{x}_{i})}{\\partial\\mathbf{x}_{j}}}={\\boldsymbol{\\beta}}\\cdot\\operatorname{softmax}({\\boldsymbol{\\beta}}\\mathbf{x})_{j}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Putting it back into vector notation: ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\frac{\\partial\\log\\sum_{i=1}^{N}\\exp(\\beta\\mathbf{x}_{i})}{\\partial\\mathbf{x}}}={\\boldsymbol{\\beta}}\\cdot{\\mathrm{softmax}}({\\boldsymbol{\\beta}}\\mathbf{x})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $\\frac{\\partial\\log\\sum\\exp(\\beta\\mathbf{x})}{\\partial\\mathbf{x}}=\\beta^{-1}\\frac{\\partial\\log\\sum_{i=1}^{N}\\exp(\\beta\\mathbf{x}_{i})}{\\partial\\mathbf{x}},$ , we can have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\frac{\\partial\\log\\sum\\exp(\\beta\\mathbf{x}_{i})}{\\partial\\mathbf{x}}}={\\mathrm{softmax}}(\\beta\\mathbf{x})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This confirms the lemma. ", "page_idx": 18}, {"type": "text", "text": "Lemma A.3. Let $\\mathbf{k}_{i}$ denote the \ud835\udc56-th row vector of $\\mathbf{K}\\in\\mathbb{R}^{N\\times d}$ . Then, we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathbf{k}_{i}\\mathbf{k}_{i}^{T}}{\\partial\\mathbf{K}}=2\\mathbf{e}_{i}^{N}(\\mathbf{e}_{i}^{N})^{T}\\mathbf{K},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathbf{e}_{i}^{N}$ represents an $N$ -dimensional column vector where only the \ud835\udc56-th entry is 1, with all other entries set to zero. ", "page_idx": 18}, {"type": "text", "text": "Proof. First, note that the expression $\\mathbf{k}_{i}\\mathbf{k}_{i}^{T}$ can be equivalently rewritten as $\\mathbf{e}_{i}^{N}\\mathbf{k}_{i}\\mathbf{k}_{i}^{T}(\\mathbf{e}_{i}^{N})^{T}$ . ", "page_idx": 19}, {"type": "text", "text": "To find the derivative of $\\mathbf{k}_{i}\\mathbf{k}_{i}^{T}$ with respect to $\\mathbf{K}$ , we need to consider the individual elements of $\\mathbf{K}$ . Let $\\mathbf{K}=[\\mathbf{k}_{1}^{T};\\mathbf{k}_{2}^{T};\\cdot\\cdot\\cdot:\\mathbf{k}_{N}^{T}]$ . Therefore, $\\mathbf{k}_{i}^{T}$ is the $i$ -th row of $\\mathbf{K}$ , and we denote this as $\\mathbf{k}_{i}^{T}=\\mathbf{K}_{i,:}$ . The differential of $\\mathbf{k}_{i}\\mathbf{k}_{i}^{T}$ is given by: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d(\\mathbf{k}_{i}\\mathbf{k}_{i}^{T})=d(\\mathbf{K}_{i,:}^{T}\\mathbf{K}_{i,:})=d(\\mathbf{K}_{i,:}^{T})\\mathbf{K}_{i,:}+\\mathbf{K}_{i,:}^{T}d(\\mathbf{K}_{i,:}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\mathbf{K}_{i,:}^{T}d(\\mathbf{K}_{i,:})=\\mathbf{e}_{i}^{N}(\\mathbf{e}_{i}^{N})^{T}d\\mathbf{K}\\mathbf{K}_{i,:}=(\\mathbf{e}_{i}^{N}(\\mathbf{e}_{i}^{N})^{T}d\\mathbf{K})\\mathbf{K}$ and similarly for the transpose term, we get: ", "page_idx": 19}, {"type": "equation", "text": "$$\nd(\\mathbf{k}_{i}\\mathbf{k}_{i}^{T})=\\mathbf{e}_{i}^{N}(\\mathbf{e}_{i}^{N})^{T}d\\mathbf{K}\\mathbf{K}_{i,:}+\\mathbf{K}_{i,:}^{T}(\\mathbf{e}_{i}^{N}(\\mathbf{e}_{i}^{N})^{T}d\\mathbf{K}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, we can summarize the derivative as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathbf{k}_{i}\\mathbf{k}_{i}^{T}}{\\partial\\mathbf{K}}=2\\mathbf{e}_{i}^{N}(\\mathbf{e}_{i}^{N})^{T}\\mathbf{K},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which matches Eq. 16 ", "page_idx": 19}, {"type": "text", "text": "Lemma A.4. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{K}}\\mathrm{lse}(\\mathbf{Q}\\mathbf{k}_{i}^{T},\\beta)=\\mathrm{softmax}(\\beta\\mathbf{Q}\\mathbf{k}_{i}^{T})\\cdot\\mathbf{Q}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Define $\\mathbf{z}=\\mathbf{Q}\\mathbf{k}_{i}^{T}$ , we can rewrite $\\mathrm{lse}(\\mathbf{Q}\\mathbf{k}_{i}^{T},\\beta)$ as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname{lse}(\\mathbf{z},\\beta)=\\beta^{-1}\\log\\left(\\sum_{j=1}^{N}\\exp(\\beta z_{j})\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using Lemma A.3, we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{z}}\\mathrm{lse}(\\mathbf{z},\\beta)=\\mathrm{softmax}(\\beta\\mathbf{z})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Substitute $\\mathbf{z}=\\mathbf{Q}\\mathbf{k}_{i}^{T}$ back to the expression, we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{K}}\\mathrm{lse}(\\mathbf{Q}\\mathbf{k}_{i}^{T},\\beta)=\\nabla_{\\mathbf{z}}\\mathrm{lse}(\\mathbf{z},\\beta)\\cdot\\frac{\\partial\\mathbf{z}}{\\partial\\mathbf{K}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "With $\\begin{array}{r}{\\frac{\\partial z_{j}}{\\partial\\mathbf{k}_{i}}=\\mathbf{Q}_{j},}\\end{array}$ : and Eq. 18, we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{K}}\\mathrm{lse}(\\mathbf{Q}\\mathbf{k}_{i}^{T},\\beta)=\\mathrm{softmax}(\\beta\\mathbf{Q}\\mathbf{k}_{i}^{T})\\cdot\\mathbf{Q}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, the lemma is proved. ", "page_idx": 19}, {"type": "text", "text": "Lemma A.5. ", "text_level": 1, "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathrm{diag}(\\mathbf{K}\\mathbf{K}^{T})}{\\partial\\mathbf{K}}=2\\mathbf{K}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where diag(A) denotes the trace of A. ", "page_idx": 19}, {"type": "text", "text": "Proof. Let us construct a column vector $\\mathbf{X}$ whose $i$ -th element is given by $\\mathbf{x}_{i}:=\\mathbf{k}_{i}\\mathbf{k}_{i}^{T}/2$ . Then, using Lemmas A.2 and A.3 and the chain rule, we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{{\\displaystyle{\\frac{\\partial\\mathrm{diag}({\\bf K}{\\bf K}^{T})}{\\partial{\\bf K}}}={\\frac{\\partial\\log\\sum_{i=1}^{N}\\exp\\left({\\frac{1}{2}}{\\bf k}_{i}{\\bf k}_{i}^{T}\\right)}{\\partial{\\bf K}}}}\\\\ {\\quad\\quad\\quad\\quad=\\sum_{i}{\\frac{\\partial{\\bf x}_{i}}{\\partial{\\bf K}}}{\\frac{\\partial\\mathrm{lse}({\\bf x},{\\bf1})}{\\partial{\\bf x}_{i}}}}\\\\ {\\quad\\quad\\quad=\\sum_{i}{\\bf e}_{i}^{N}({\\bf e}_{i}^{N})^{T}{\\bf K}[\\mathrm{softmax}(x)]_{i}}\\\\ {\\quad\\quad\\quad=\\sum_{i}{\\bf e}_{i}^{N}{\\bf k}_{i}[\\mathrm{softmax}({\\bf x})]_{i}=2{\\bf K}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This proves the lemma. ", "page_idx": 19}, {"type": "text", "text": "Derivation of Eq. 7. Now we proof that we can approximate the posterior inference of $p(\\mathbf{K}_{t}\\mid\\mathbf{Q}_{s})$ by the gradient of the log posterior, estimated as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\mathbf{K}_{t}}\\log p(\\mathbf{K}_{t}|\\mathbf{Q}_{s})=-\\left(\\nabla_{\\mathbf{K}_{t}}E(\\mathbf{Q}_{s};\\mathbf{K}_{t})+\\nabla_{\\mathbf{K}_{t}}E(\\mathbf{K}_{t})\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\,\\mathrm{sm}\\left(\\beta\\mathbf{Q}_{s}\\mathbf{K}_{\\mathrm{t}}^{\\mathrm{\\tiny~T}}\\right)\\mathbf{Q}_{s}-\\left(\\alpha\\mathbf{I}+\\mathcal{D}\\left(\\mathrm{sm}\\left(\\frac{1}{2}\\mathrm{diag}(\\mathbf{K}_{\\mathrm{t}}\\mathbf{K}_{\\mathrm{t}}^{T})\\right)\\right)\\right)\\mathbf{K}_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\operatorname{sm}(\\mathbf{v})=\\operatorname{softmax}(\\mathbf{v}):=\\exp\\left(\\mathbf{v}-\\operatorname{lse}(\\mathbf{v},1)\\right)$ and $\\mathcal{D}(\\cdot)$ is a vector-to-diagonal-matrix operator. Moreover, recall Eq. 5a and 5b, the energy $E(\\mathbf{Q}_{s};\\mathbf{K}_{t})$ and $E({\\bf K}_{t})$ are denoted as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{E(\\mathbf{Q}_{s};\\mathbf{K}_{t})=\\displaystyle\\frac{\\alpha}{2}\\mathrm{diag}(\\mathbf{K}_{{\\mathbf{t}}}\\mathbf{K}_{t}^{T})-\\displaystyle\\sum_{i=1}^{N}\\mathrm{lse}(\\mathbf{Q}_{s}\\mathbf{k}_{t,i}^{T},\\boldsymbol{\\beta})+c,}\\\\ {E(\\mathbf{K}_{t})=\\mathrm{lse}\\left(\\displaystyle\\frac{1}{2}\\mathrm{diag}(\\mathbf{K}_{{\\mathbf{t}}}\\mathbf{K}_{t}^{T}),1\\right)=\\log\\displaystyle\\sum_{i=1}^{N}\\mathrm{exp}\\left(\\displaystyle\\frac{1}{2}\\mathbf{k}_{t,i}\\mathbf{k}_{t,i}^{T}\\right)+c,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Using Lemmas A.4 and A.5, we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{K}_{t}}E(\\mathbf{Q}_{s};\\mathbf{K}_{t})=\\alpha\\mathbf{K}_{t}-\\mathrm{sm}(\\beta\\mathbf{Q}_{s}\\mathbf{K}_{t}^{T})\\cdot\\mathbf{Q}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $\\mathbf{K}_{t}$ can be expressed as $\\scriptstyle\\mathrm{lse}\\left({\\frac{1}{2}}\\mathbf{k}_{i}\\mathbf{k}_{i}^{T}\\right)$ , incorporating Lemma A.5, we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{K}_{t}}E(\\mathbf{K}_{t})=\\mathcal{D}\\left(\\operatorname{sm}\\left(\\frac{1}{2}\\mathrm{diag}(\\mathbf{K}_{{\\mathrm{t}}}\\mathbf{K}_{{\\mathrm{t}}}{}^{T})\\right)\\right)\\mathbf{K}_{t}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Put them together, we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{K}_{t}}\\log p(\\mathbf{K}_{t}|\\mathbf{Q}_{s})=\\sin\\left(\\beta\\mathbf{Q}_{s}\\mathbf{K}_{t}^{T}\\right)\\mathbf{Q}_{s}-\\left(\\alpha\\mathbf{I}+\\mathcal{D}\\left(\\mathbf{sm}\\left(\\frac{1}{2}\\mathrm{diag}(\\mathbf{K}_{t}\\mathbf{K}_{t}^{T})\\right)\\right)\\right)\\mathbf{K}_{t},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This matches Eq. 7. ", "page_idx": 20}, {"type": "text", "text": "B Extended Experimental Analysis of Attention Alignment ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we begin by examining the representation discrepancy between old and new classes, highlighting alignment issues (B.1). Enhanced consistency loss optimization is then detailed, showing improvements in learning stability (B.2). We discuss how attention alignment bridges the prior gap and benefits synchronized learning, enhancing overall performance (B.3). The focus then shifts to attention specialization in deep network layers (B.4), and the performance impact of layer selection for attention alignment on different dataset (B.5). Finally, we represent the negligible impact on computational cost of the Attention Alignment strategy (B.6), thereby proving its practical viability. ", "page_idx": 20}, {"type": "text", "text": "B.1 Representation Discrepancy of Old and New Classes ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In GCD, directly applying consistency regularization leads to challenges, especially for new classes. Due to the lack of prior knowledge, the teacher struggles to guide the student, resulting in representation discrepancies between the teacher (weakly-augmented) and student (strongly-augmented). This is evident in Fig. 10 (left), where new classes show poor alignment between teacher and student representations compared to known classes. ", "page_idx": 20}, {"type": "text", "text": "This discrepancy causes unsynchronized learning, as shown in Fig. 10 (right). The two main phenomena observed are the learning gap and learning regression. The learning gap indicates that the student struggles to reach the teacher\u2019s level of understanding, particularly for new classes, leading to stagnation. Learning regression affects the teacher, hampering improvement for new classes and causing regression in known classes due to the alignment efforts with the student. ", "page_idx": 20}, {"type": "text", "text": "B.2 Enhanced Consistency Loss Optimization ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Building on the previous discussion on the representation discrepancy of old and new classes, we address the challenges in optimizing consistency loss (Sec. 2.2) that contribute to the learning gap. To ", "page_idx": 20}, {"type": "image", "img_path": "C4NbtYnyQg/tmp/43b127211260f7d6ecceba37ce8952b3de73afd3ea5ba2278ff265d3e5eac822.jpg", "img_caption": ["Figure 10: Left: Comparison of representation discrepancy with respect to old and new classes before and after training, showing the misalignment of student (blue) and teacher (red). Right: Learning unsynchronization between teacher and student with trends of learning regression and learning gap for old and new classes. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "C4NbtYnyQg/tmp/19574cc3216224212e921f054a132c0c738cb6ab47541ce3e8e8d1a8413dfad3.jpg", "img_caption": ["(a) Consistency loss optimization on SCars and CUB,(b) Categorize errors of FlipClass. Compared to comparing FlipClass with various update rates (\ud835\udefeupdate)those of SimGCD (Fig. 2 right), the reduced errors to the SimGCD baseline, demonstrating more rapid and on \u2018False New\u2019 represent that FlipClass mitigates the stable convergence. overfitting of old classes brought by the prior gap. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 11: Attention alignment bridges the prior gap with better-converged consistency loss and leads to less biased predictions. ", "page_idx": 21}, {"type": "text", "text": "verify how FlipClass improves this process, we track the consistency loss ${\\mathcal{L}}_{\\mathrm{cons}}$ on new classes. As shown in Fig. 11a, FlipClass with various update rates $(\\gamma_{\\mathrm{update}})$ demonstrates faster and more stable convergence compared to the SimGCD baseline. Specifically, the experiments on SCars and CUB datasets reveal that FlipClass streamlines the optimization process of consistency loss, leading to more rapid and stable convergence. ", "page_idx": 21}, {"type": "text", "text": "B.3 Attention Alignment Bridges Prior Gap and Benefits Synchronized Learning ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The enhanced consistency loss optimization further aids in achieving consistency between the student and the teacher, reflecting in two main aspects. Firstly, it mitigates the effects of prior gap, as shown in Fig. 11b. Compared to the categorize errors of SimGCD (Fig. 2), FlipClass reduces the \u2018False New\u2019 error (where the model incorrectly predicts new classes as old classes). This indicates that FlipClass can mitigate overftiting on old classes due to the lack of prior knowledge about new classes. Secondly, as shown in Fig. 12, compared to the traditional teacher-student model used in generalized category discovery (e.g., SimGCD), which suffers from a learning gap, FlipClass successfully bridges the learning gap. It achieves better teacher-student learning effects, ensuring more synchronized and stable learning outcomes. ", "page_idx": 21}, {"type": "image", "img_path": "C4NbtYnyQg/tmp/3fc01d9599c399b5c1c5121e0e7be70b1bc78a744a57eeb4dd0f551968cc0b6a.jpg", "img_caption": ["Figure 12: Learning curves for SCars and CUB datasets. FlipClass achieves better synchronized and stable learning effects compared to the traditional teacher-student model. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "C4NbtYnyQg/tmp/5ae616de3211b097cc4b4d65cafa1b8d3483197ba01cedb395027095325c674a.jpg", "img_caption": ["Figure 13: Attention heatmap accuracy per layer on SCars dataset, with deeper layers focused on local semantic features and earlier layers on general features, indicating better transfer learning for old and new classes with attention alignment in deeper network layers. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "B.4 Attention Specialization in Deep Network Layers ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Moreover, on the SCars dataset, the model shows a greater energy reduction in deeper layers, suggesting a focus on local semantics over general ones. This is illustrated in Fig. 13, where deeper layers emphasize local semantic parts, and earlier ones are associated with general semantics (e.g., texture, color). This local semantic concentration enhances transferability across \u2018Old\u2019 and \u2018New\u2019 classes, with attention alignment in deeper layers (9-10) yielding the highest performance. We observe that different heads attend to disjoint regions of the image, focusing on important parts. After training with our method, attention heads become more specialized to semantic parts, displaying more concentrated and local attention. Our model learns to specialize attention heads (shown as columns) to different semantically meaningful parts, improving transferability between labeled and unlabeled categories. ", "page_idx": 22}, {"type": "text", "text": "B.5 Attention Alignment in Layer Selection ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Furthermore, we provide the performance of layer selection for attention alignment on CUB and Cifar10. In this context, \u201c2 layers\" means performing attention alignment in the two layers depicted in the Fig. 14. As shown, Cifar-10 (coarse-grained) tends to achieve higher performance when alignment is performed in the middle layers (4-5), while CUB (fine-grained) achieves higher performance with alignment in deeper layers, which aligns with the trend of SCars (Fig. 13). The difference stems from the dataset nature. CIFAR-10 benefits from middle-layer alignment, capturing general features like shapes and textures, which suffice for its simpler categories. Conversely, CUB requires deeper layer alignment for detailed features needed to distinguish similar bird species. Deeper layers provide the refined features critical for complex tasks. ", "page_idx": 22}, {"type": "image", "img_path": "C4NbtYnyQg/tmp/b1d24f33716c2acc1159ac4f4979953136213d333b13c98bd12d02d86be89f03.jpg", "img_caption": ["Figure 14: Layer selection performance for attention alignment on Cifar-10 and CUB datasets. Higher performance is observed in middle layers for Cifar-10 and deeper layers for CUB, indicating the need to tailor attention alignment to the nature of the data. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "B.6 Time Efficiency of Attention Alignment ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "A potential concern is whether attention alignment increases the time cost. Since we perform the alignment in a one-loop manner, the additional time overhead is negligible. This is demonstrated in Table 3, showing that the training and inference times for FlipClass are comparable to those of SimGCD. ", "page_idx": 23}, {"type": "table", "img_path": "C4NbtYnyQg/tmp/61ae539526877df065956d43293e830839fb1c1469a88e6aa5acb10b4dc28420.jpg", "table_caption": ["Table 3: Comparison of training and inference times for SImGCD on ImageNet-100 and AirCraft datasets, with 200 epochs "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "We further analyze the potential extra overhead of our attention update strategy compared to a conventional self-attention block: ", "page_idx": 23}, {"type": "text", "text": "\u2022 Overall Training Cost: For a Vision Transformer (ViT) with $L$ layers, the training cost includes (1) self-attention cost, $O(L\\cdot N\\cdot d_{k}^{2})$ ; (2) feed-forward network (FFN) cost, ${\\cal O}(L\\,\\cdot\\,$ $N\\cdot d_{k}\\cdot d_{m})$ . Here, $N$ is the sequence length, $d_{k}$ is the feature dimension, and $d_{m}$ is the hidden dimension in the FFN. \u2022 Attention Update Cost: Our update strategy applies only to 2-3 layers (see Appendix B.5), with each update having the same cost as one FFN operation per layer. ", "page_idx": 23}, {"type": "text", "text": "In sum, we add 2-3 extra FFN-like operations during the forward pass. And there is no impact on the backward pass, as only the last block is updated without introducing extra parameters for optimization. Therefore, the additional overhead is minimal, with only a slight increase in training time, as shown in Table 4. ", "page_idx": 23}, {"type": "table", "img_path": "C4NbtYnyQg/tmp/eb7bc3a48645f3720b1b852d838719dd89ce704970791978c5281716f3f88c40.jpg", "table_caption": ["Table 4: Time cost (seconds) per forward pass on CUB and Stanford Cars. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "C4NbtYnyQg/tmp/684df7150868db994f4b57b85216c3947f925ca1305f44f9e599fcbc39273cb5.jpg", "table_caption": ["Table 5: Complete results of FlipClass in have five independent runs with random seeds. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "In summary, the attention update operation introduces a small extra overhead equivalent to 2- 3 additional FFN operations, which remains insignificant relative to the overall training cost, especially given the performance improvements achieved by FlipClass. ", "page_idx": 24}, {"type": "text", "text": "C More Experimental Results ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We detail our experimental results, including main outcomes with error bars for statistical significance (C.1). We evaluate performance on complex datasets (C.2), analyze clustering and per-class prediction distributions (C.3). Additionally, we examine robustness to varying numbers of old and new classes (C.4), and investigate how different proportions of old classes affect performance (C.5), demonstrating our method\u2019s stability and adaptability. ", "page_idx": 24}, {"type": "text", "text": "C.1 Main Results with Error Bars ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Table 5 reports error bars to provide a clear understanding of the statistical significance and variability of the main results in our experiments. Specifically, we include both the mean and standard deviation (std) values for the performance of our FlipClass across different datasets and class types (All, Old, New). The standard deviations are calculated from five independent runs with random seeds, offering insight into the consistency of our method\u2019s performance. ", "page_idx": 24}, {"type": "text", "text": "C.2 Results on Complex Datasets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Here, we discuss the performance of various methods on challenging datasets, specifically Herbarium19 [65] and ImageNet-1K [35]. Herbarium19, a long-tailed dataset, poses significant challenges due to the varying frequencies of different categories, leading to unbalanced cluster sizes. Table 6 demonstrates the robustness of our proposed FlipClass, in handling such frequency imbalances and its ability to accurately distinguish categories even with few examples. Table 7 showcases the performance on ImageNet-1K, a large-scale generic classification dataset, in evaluating the model\u2019s capability in real-world applications of generalized category discovery. The results demonstrate the robustness of FlipClass for tasks involving both familiar and unfamiliar data in complex, real-world scenarios. ", "page_idx": 24}, {"type": "text", "text": "C.3 Clustering and Per-class Prediction Distribution ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Clustering Analysis. Fig. 15 presents a visual comparison of the clustering results obtained with FlipClass against the existing state-of-the-art method, InfoSieve [55], on Cifar-10 and Cifar-100 datasets. On Cifar-10, FlipClass forms clusters that exhibit higher compactness and purity, indicating enhanced feature discrimination and reduced interclass confusion. In contrast, on Cifar-100, although InfoSieve forms visually more compact clusters, these clusters show less purity, with a higher incidence of false class predictions. ", "page_idx": 24}, {"type": "table", "img_path": "C4NbtYnyQg/tmp/8b4bfe3b562acfeaf6beb7d2244244b0a1b0386aaa0077186ac2b17062cb27a1.jpg", "table_caption": ["Table 6: Performance comparison of different methods on the Herbarium19 dataset. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "C4NbtYnyQg/tmp/bd184f518b30a88b339852425eaab6b78eeb9dd7fbf521645787e95c84f624f3.jpg", "table_caption": ["Table 7: Performance comparison of different methods on the ImageNet-1K dataset. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Prediction Distribution and Class-specific Accuracies. Fig. 16 evaluates the prediction distribution and class-specific accuracies of FlipClass compared to InfoSieve [55]. FlipClass demonstrates a better fti to the true distribution, whereas InfoSieve shows skewed predictions. Moreover, FlipClass significantly outperforms InfoSieve in recognizing tail classes on the CUB and Stanford Cars datasets, improving accuracy and reducing prediction bias. ", "page_idx": 25}, {"type": "text", "text": "C.4 Robustness to Number of Classes ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Varying Number of Classes during Clustering. In the main experiments (Section 5), the class number, $K$ , is assumed as a known prior following prior works [69, 64, 75, 70], however, this setting has been questioned as impractical [9, 74, 73]. In Fig. 17, we conduct experiments when this assumption is removed, evaluating results with different numbers of classes, where the ratio changes from $80\\%$ to $200\\%$ compared to the ground truth number of classes. During clustering (e.g., KMeans [45]), a predefined class number lower than the ground truth significantly limits the ability to discover new classes, causing the model to focus more on old classes. Conversely, increasing the class number results in less harm to the generic image recognition datasets (e.g., Cifar-100) and can even be beneficial for some fine-grained, long-tailed datasets (e.g., CUB). This phenomenon occurs because overestimating the number of classes allows the model to maintain higher flexibility and adaptability in recognizing new classes in these fine-grained, challenging datasets. For fine-grained, class-distribution biased datasets like CUB, overestimating the class number helps capture subtle differences between closely related categories, thereby improving class separation and reducing prediction bias. However, for generic datasets like Cifar-100, the visual differences between classes are more pronounced, and overestimating the class number can introduce unnecessary complexity, leading to overfitting and decreased performance. ", "page_idx": 25}, {"type": "text", "text": "Estimation of Number of Classes. Additionally, to further validate the robustness of our model, we trained FlipClass using an estimated number of classes in the dataset, where the number of classes is predicted using the over-clustering method from GCD [69]. We obtained a similar predicted number of classes as SimGCD. As expected, our method performs worse on Cifar-100 when using an estimated number of classes due to the mismatch between the estimated and actual class distribution. Interestingly, the performance of SimGCD (traditional teacher-student model) improves on CUB with both new and old classes, while our FlipClass makes improvements on old classes but sees a decrease in new classes. ", "page_idx": 25}, {"type": "image", "img_path": "C4NbtYnyQg/tmp/9c4a045bb71226507c1c57965e8b2425525914111a0c867b84e2810594fa0705.jpg", "img_caption": ["Figure 15: Comparison of clustering results on Cifar-10 and Cifar-100 datasets using GCD, InfoSieve, and our FlipClass. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "C4NbtYnyQg/tmp/6ed117298d27f5bb19315ab8d86859eb910a19148cc83150b84bebbb7111c97f.jpg", "img_caption": ["Figure 16: Prediction distribution and class-specific accuracies of InfoSieve and FlipClass on Cifar-10, CUB, and Stanford Cars datasets. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "C.5 Results with Varying Proportion of Old Classes ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In the primary experiments, we fix the number of the old classes $|C_{\\ell}|$ (details in Appendix D.3). Here, we experiment with our method by changing the class split setting. Specifically, on Cifar-100 $(|C_{\\ell}|\\,=\\,80)$ and CUB $\\left\\langle\\boldsymbol{C}_{\\ell}\\right|\\,=\\,100)$ datasets, we test with fewer old classes, as shown in Fig. 18. For Cifar-100 and CUB, as the number of old classes decreases, the accuracy for both old and new classes slightly declines but remains stable. This demonstrates FlipClass\u2019s effectiveness in leveraging additional old class information and robustness in handling varying numbers of known classes. ", "page_idx": 26}, {"type": "image", "img_path": "C4NbtYnyQg/tmp/db8f68d664e07834f13829eb2055f3aefbcb3421d2ad398c3870a5dfda0867f9.jpg", "img_caption": ["Figure 17: Results with varying numbers of classes during clustering, where the ratio changes from $80\\%$ to $200\\%$ of the ground truth number of classes. "], "img_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "C4NbtYnyQg/tmp/ca985e094fae93a8f3566e9ed67ff4e6a9e898eb7d7e49955f5b1ec3889e3cce.jpg", "table_caption": ["Table 8: Performance of FlipClass and the baseline method SimGCD with an estimated number of categories on CUB and Cifar100. Bold values represent the best results. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "D Experimental Settings ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "D.1 Implementation Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We develop our FlipClass upon the SimGCD [75] baseline on the pre-trained ViT-B/16 DINO2 [15]. Specifically, we take the final feature corresponding to the CLS token from the backbone as the image feature, which has a dimension of 768. For the feature extractor $\\mathbb{F}$ , we only fine-tune the last block. We set the balancing factor $\\lambda$ to 0.35 and the temperature values $\\tau_{u}$ and $\\tau_{c}$ to 0.07 and 1.0, respectively, following SimGCD. For the temperature values $\\tau_{t}$ and $\\tau_{s}$ in the classification losses, we also set them to 0.07 and 0.1. For update rule (Eq. 8), we set $\\alpha=0$ , $\\beta=1$ , $\\gamma_{\\mathrm{update}}=0.1$ and $\\gamma_{\\mathrm{reg}}=0.5$ . All experiments are conducted using a single NVIDIA A100 GPU with 200 epochs, which we find sufficient for the losses to plateau. ", "page_idx": 27}, {"type": "text", "text": "D.2 Design of Data Augmentation ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this experimental setup, we design both weak and strong augmentations for the teacher and student networks. For weak augmentation, we use common techniques such as RandomHorizontalFlip and RandomCrop for all datasets, aiming to pass less perturbed versions of the input images to the teacher network. For the strong augmentation that is applied to the images fed to the student, we incorporate more aggressive transformations to expose the student to a wider range of variations. Specifically, we add RandomResizedCrop with a scale range of 0.3 to 1.0, which allows for more aggressive cropping and resizing. Additionally, we include Gaussian blurring to simulate different levels of image blurriness. For datasets that are used for generic recognition tasks, we further enhance the strong augmentation by including ColorJitter with probability 0.8 and RandomGrayscale with probability 0.2. Solarization inverts pixel values above a threshold, simulating the effect of solarizing an image, while Grayscale converts the image to black and white, reducing color information. These additional augmentations help expose the student network to even more diverse image variations, improving its robustness and generalization capabilities. ", "page_idx": 27}, {"type": "image", "img_path": "C4NbtYnyQg/tmp/d42ee69f0854aa6421ffa1d631c39adeff31c273f0732314363f77008e9900e7.jpg", "img_caption": ["Figure 18: Results with varying the number of old classes $|C_{\\ell}|$ . "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "D.3 Datasets ", "text_level": 1, "page_idx": 28}, {"type": "table", "img_path": "C4NbtYnyQg/tmp/8a6e8970d95b499464462fbf869b9bd432802437c27eaaacb4f0949a07c276d1.jpg", "table_caption": ["Table 9: Statistics of the datasets used in our experiments. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "We follow the dataset settings from earlier works [75, 70] to subsample the training dataset. Specifically, $50\\%$ of known categories and all samples of unknown categories are used for training. For all datasets except Cifar-100, $50\\%$ of the categories are considered known during training, whereas for Cifar-100, $80\\%$ of the categories are known during training. Detailed statistics are displayed in Table 9. Below is a summary of the datasets and how their combination supports experiments in generalized category discovery: ", "page_idx": 28}, {"type": "text", "text": "Cifar-10/1 $\\mathit{O O}^{3}$ [34] are coarse-grained datasets consisting of general categories with low-resolution images and even class distribution. ", "page_idx": 28}, {"type": "text", "text": "ImageNet- $I O O/I K^{4}$ is a subset of 100/1K categories from the coarse-grained ImageNet5 [35, 58] dataset. It includes a large scale of high-resolution real-world images with evenly distributed classes. ", "page_idx": 28}, {"type": "text", "text": "CUB (Caltech-UCSD Birds-200-2011)6 [71] is widely used for fine-grained image recognition, containing different bird species distinguished by subtle details. ", "page_idx": 28}, {"type": "text", "text": "Stanford Cars7 [33] is a fine-grained dataset of various car brands, providing multi-view objects for class detection and scene understanding, challenging real-world applications in distinguishing subtle appearance differences. ", "page_idx": 28}, {"type": "text", "text": "FGVC-Aircraft (Fine-Grained Visual Classification of Aircraft)8 [46] is a fine-grained dataset organized in a three-level hierarchy. At the finer level, differences between models are subtle but visually measurable. Unlike animals, aircraft are rigid and less deformable, presenting variations in purpose, size, designation, structure, historical style, and branding. ", "page_idx": 28}, {"type": "text", "text": "Herbarium 19 (FGVC 2019 Herbarium Challenge)9 [65] provides a curated dataset of over 46,000 herbarium specimens across 680 species, presenting a long-tailed distribution and challenges for species recognition. ", "page_idx": 29}, {"type": "text", "text": "D.4 Other Alignment Strategies ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In the ablation studies (Section 5.3), we apply different strategies such as distribution alignment [11], FixMatch [63] and CORAL. These strategies are employed to encourage the consistency between the teacher and student, therefore modeling $\\mathfrak{R}$ in Eq. 2. We briefly provide the main idea of these alignment strategies below. ", "page_idx": 29}, {"type": "text", "text": "Distribution alignment is designed by maintaining a running average of the model\u2019s predictions on unlabeled data $\\tilde{p}(y)$ . Given the model\u2019s prediction $q=p_{\\mathrm{model}}(y\\mid\\mathbf{x}_{u})$ on an unlabeled example $\\mathbf{X}_{u},\\,q$ is scaled by the ratio $p(y)/\\tilde{p}(y)$ and then renormalize the result to form a valid probability distribution: $\\tilde{q}=$ Normalize $(q\\times p(y)/\\tilde{p}(y))$ . ", "page_idx": 29}, {"type": "text", "text": "FixMatch is a semi-supervised learning method that combines consistency regularization and pseudolabeling. It works by first generating pseudo-labels for unlabeled images using the model\u2019s predictions on weakly enhanced versions of those images, retaining only high-confidence predictions as following: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\ell_{u}=\\frac{1}{\\mu B}\\sum_{b=1}^{\\mu B}\\mathbb{1}\\left(\\operatorname*{max}\\left(q_{b}\\right)\\geq\\tau\\right)\\mathsf{H}\\left(\\hat{q}_{b},p_{\\mathsf{m}}\\left(y\\mid\\mathcal{R}\\left(u_{b}\\right)\\right)\\right),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\tau$ is a scalar hyperparameter denoting the threshold above which a pseudo-label should be retained. Then, the model is trained to predict these pseudo-labels using strongly enhanced versions of the same images. The loss function consists of two cross-entropy terms: a supervised loss for labeled data and an unsupervised loss for unlabeled data, where the unsupervised loss utilizes the pseudo-labels calculated from weakly enhanced images and the model\u2019s predictions on strongly enhanced images. ", "page_idx": 29}, {"type": "text", "text": "CORAL (CORelation ALignment) aligns the second-order statistics (covariances) of two spaces. Specifically, CORAL minimizes the difference in covariance matrices between the student and teacher representations (S and $\\mathbf{T}$ ). The goal of CORAL is to find a transformation for S that minimizes the Frobenius norm of the difference between the covariance matrices of S and $\\mathbf{T}$ , denoting $\\mathbf{C}_{S}$ and ${\\bf C}_{T}$ , respectively. CORAL minimizes the following objective: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{S^{\\prime}}\\|\\mathbf{C}_{S^{\\prime}}-\\mathbf{C}_{T}\\|_{F}^{2},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where ${\\bf C}_{S^{\\prime}}$ is the covariance matrix of the transformed student representations $\\mathbf{S^{\\prime}}$ , and $\\|.\\|_{F}$ denotes the Frobenius norm. ", "page_idx": 29}, {"type": "text", "text": "E Related Works ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "E.1 Consistency Regularization ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In semi-supervised learning (SSL), the goal is to enhance model performance by leveraging unlabeled data, traditionally drawn from the same class spectrum as the labeled data [90]. A key strategy in SSL, consistency regularization [36], in recent years, centers on promoting model stability by ensuring that the teacher instance (weakly-augmented instance) and the student instance (strongly-augmented instance) yield coherent predictions[11, 63, 78, 87, 83]. Building on the \u03a0-Model\u2019s teacher-student framework, several approaches have advanced its capabilities [66, 43, 77, 12, 53]. MeanTeacher [66] deploys an exponential moving average of the model parameters to stablize the teacher\u2019s output. NoisyStudent [77] employs a self-training strategy that incorporates noise into the student model\u2019s training, cycling the improved student back into the teacher role. Previous methods in SSL have largely concentrated on promoting the teacher\u2019s performance, often overlooking whether the student can keep pace, and neglecting the harmony of interaction. Our approach pivots to synchronizing the teacher\u2019s and student\u2019s attention, a shift that\u2019s especially pivotal in GCD, where consistency is challenged by the introduction of new classes. This strategy ensures a balanced teacher-student dynamic, crucial for effective consistency regularization in the open-world setting. ", "page_idx": 29}, {"type": "text", "text": "E.2 Novel Category Discovery ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Novel category discovery (NCD) is first formalized as cross-task transfer in [30], which aims to discover unseen categories from unlabeled data that have nonoverlapped classes with the labeled ones. Earlier works [31, 26, 86, 79, 38] mostly maintain two networks for learning from labeled and unlabeled data respectively. AutoNovel [27] introduces a three-stage framework. Specifically, the model is firstly trained with the whole dataset in a self-supervised manner and then fine-tuned only with the fully-supervised labeled set to capture the semantic knowledge for the final joint-learning stage. UNO [22] addresses the problem by jointly modeling the labeled and unlabeled sets to prevent the model from overfitting to labeled categories. Similarly, NCL [88] generates pairwise pseudo labels for unlabeled data and mixes samples in the feature space to construct hard negative pairs. ", "page_idx": 30}, {"type": "text", "text": "E.3 Generalized Category Discovery ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Generalized Category Discovery (GCD) extends NCD by categorizing unlabeled images from both seen and unseen categories [69], which tackles this issue by tuning the representation of the pretrained ViT model with DINO ([15], [51]) with contrastive learning, followed by semi-supervised $\\boldsymbol{\\mathrm{k}}$ -means clustering. ORCA [13] considers the problem from a semi-supervised learning perspective and introduces an adaptive margin loss for better intra-class separability for both seen and unseen classes. CiPR [28] introduces a method for more effective contrastive learning and a hierarchical clustering method for GCD without requiring the category number in the unlabeled data to be known a priori. SimGCD [75] proposes a parametric method with entropy regularization to improve performance. TIDA [74] discovers multi-granularity semantic concepts and then leverages them to enhance representation learning and improve the quality of pseudo labels. Moreover, $\\mu\\mathrm{GCD}$ [70] take a leap forward by extending the MeanTeacher paradigm to the GCD task. Instead of managing dual models as in $\\mu\\mathrm{GCD}$ , our approach achieves teacher-student consistency more effectively within a single-model structure, streamlining computational demands. Crucially, we found that the learning discrepancy between teachers and students in the open-world context is the reason why consistency is difficult to achieve, and solved this problem by synchronizing the attention of teachers and students. ", "page_idx": 30}, {"type": "text", "text": "F Limitations and Future Work ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Catastrophic Forgetting. While our method achieves significant improvement on new classes, the performance on old classes, particularly on CUB, lacks compared to the state-of-the-art methods. We attribute this to a phenomenon akin to catastrophic forgetting, where the model forgets previously learned concepts. Addressing these issues is essential for enhancing the robustness and effectiveness of the proposed methods. ", "page_idx": 30}, {"type": "text", "text": "Sub-optimal $K$ Estimation. As shown in Appendix C.4, for fine-grained, class-distribution biased datasets like CUB, overestimating the class number helps capture subtle differences between closely related categories, thereby improving class separation and reducing prediction bias. However, for coarse-grained datasets like CIFAR-100, overestimating the class number can introduce unnecessary complexity, leading to overfitting and decreased performance. Some works have delved into this path and show promising performance [73, 74, 9], highlighting the potential of tailored $K$ estimation strategies to balance complexity and performance across different types of datasets. ", "page_idx": 30}, {"type": "text", "text": "Data Augmentation to Enhance Teacher-Student Consistency. Effective data augmentation techniques has been investigated a lot in semi-supervised learning [18, 82, 48, 76, 29], the techniques for generalized category discovery are still lacking, which affects the consistency between the teacher and student models. The strength of data augmentation for new classes needs careful control to avoid ineffective learning due to excessive noise or insufficient variability. Additionally, preventing data leakage during augmentation is critical, as pretrained diffusion models can compromise evaluation integrity by leaking training data. Addressing these issues is essential for enhancing the robustness and effectiveness of the proposed methods. ", "page_idx": 30}, {"type": "text", "text": "G Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Our study extends the capability of AI systems from the closed world to the open world, fostering AI systems capable of categorizing and organizing open-world data automatically. While Generalized ", "page_idx": 30}, {"type": "text", "text": "Category Discovery (GCD) has many real-world applications, it can be unreliable and must be applied with caution. Currently, supervised learning with extensive fine annotations is the mainstream solution for many computer vision tasks, but the cost and difficulty of obtaining these annotations can be prohibitive. Our work addresses this issue by advancing an open-set semi-supervised learning paradigm, significantly reducing the need for precise annotations and promoting the application of AI models in areas where annotations are difficult to obtain. ", "page_idx": 31}, {"type": "text", "text": "This work provides a new idea for open-set semi-supervised learning. Specifically, while conventional approaches apply closed-world semi-supervised learning techniques to generalized category discovery, they rarely consider the attention alignment gap between teacher and student models. We point out that bridging this gap can significantly improve learning efficiency and accuracy. We hope that this methodology can be generalized to more relevant label-efficient tasks, promoting broader applications of AI in scenarios with limited labeled data. ", "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction (Section 1) accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: The limitations of the work are discussed in Appendix F. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Concerning the proposition 4.1, Appendix A.1 and A provide its corresponding proofs, with a short proof sketch to provide intuition. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Refer to Appendix D.1 for implementation details, the provided code in the supplementary materials ensure the reproducibility as well. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Refer to provided code in the supplementary materials. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Table 9 specifies all the training and test detail. Appendix D.1 provides the employed hyperparameters. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Appendix C.1 provides the statistical significance of the experiments. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Appendix D.1 and B.6 provide sufficient information on the computer resources and the related costs. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The authors have reviewed and understand the NeurIPS Code of Ethics, and confirm that their research conforms to it in every respect. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Potential societal impacts of the work are discussed in Appendix G. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 35}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The original papers of assets, and the corresponding versions are provided in Section 5, References and Appendix D.1. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We will well document the assets when we officially release our code. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not describe potential risks incurred by study participants. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}]