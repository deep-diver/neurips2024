[{"figure_path": "C4NbtYnyQg/tables/tables_6_1.jpg", "caption": "Table 1: Evaluation on the Semantic Shift Benchmark (SSB). Bold values represent the best results, while underlined values represent the second-best results.", "description": "This table presents a comparison of various methods' performance on the Semantic Shift Benchmark (SSB) dataset.  The results are broken down by dataset (CUB, Stanford Cars, Aircraft), and further categorized by overall accuracy, accuracy on 'Old' (previously seen) classes, and accuracy on 'New' (unseen) classes.  Bold values highlight the best performance for each category and underlined values show the second best.  This allows for easy comparison of different models in terms of their generalization ability to new categories and their overall performance.", "section": "5 Experiments"}, {"figure_path": "C4NbtYnyQg/tables/tables_7_1.jpg", "caption": "Table 1: Evaluation on the Semantic Shift Benchmark (SSB). Bold values represent the best results, while underlined values represent the second-best results.", "description": "This table presents the performance of various methods on the Semantic Shift Benchmark (SSB), a dataset designed to evaluate the ability of models to generalize to new categories.  The results are broken down by dataset (CUB, Stanford Cars, Aircraft), and performance is measured across all classes, old classes (seen during training), and new classes (unseen during training). Bold values indicate the best performance for each metric, and underlined values indicate the second-best. The table allows for a comparison of different methods' ability to handle both previously seen and novel categories.", "section": "5 Experiments"}, {"figure_path": "C4NbtYnyQg/tables/tables_23_1.jpg", "caption": "Table 1: Evaluation on the Semantic Shift Benchmark (SSB). Bold values represent the best results, while underlined values represent the second-best results.", "description": "This table presents the performance comparison of various methods on the Semantic Shift Benchmark (SSB) dataset for generalized category discovery.  The results are broken down by dataset (CUB, Stanford Cars, Aircraft), and further categorized into overall accuracy ('All'), accuracy on known classes ('Old'), and accuracy on novel classes ('New').  Bold values highlight the best performance for each category, while underlined values show the second-best performance.  The results show the effectiveness of the proposed method, FlipClass, compared to several state-of-the-art approaches.", "section": "5 Experiments"}, {"figure_path": "C4NbtYnyQg/tables/tables_24_1.jpg", "caption": "Table 1: Evaluation on the Semantic Shift Benchmark (SSB). Bold values represent the best results, while underlined values represent the second-best results.", "description": "This table presents the performance comparison of various methods on the Semantic Shift Benchmark (SSB) dataset for generalized category discovery.  The results are categorized by dataset (CUB, Stanford Cars, Aircraft), and further broken down into overall accuracy, and accuracy for old and new categories. Bold values indicate the best result for each category and underlined values indicate the second-best result. This table shows that FlipClass significantly outperforms other state-of-the-art methods across various datasets and metrics.", "section": "5 Experiments"}, {"figure_path": "C4NbtYnyQg/tables/tables_24_2.jpg", "caption": "Table 1: Evaluation on the Semantic Shift Benchmark (SSB). Bold values represent the best results, while underlined values represent the second-best results.", "description": "This table presents the performance evaluation results of various methods on the Semantic Shift Benchmark (SSB) dataset. The results are categorized into overall accuracy (All), accuracy on old classes (Old), and accuracy on new classes (New). The best and second-best results are highlighted in bold and underlined, respectively.  Different backbones are used for different models. The table shows the effectiveness of different GCD methods on three fine-grained image recognition datasets: CUB, Stanford Cars, and Aircraft.", "section": "5 Experiments"}, {"figure_path": "C4NbtYnyQg/tables/tables_25_1.jpg", "caption": "Table 1: Evaluation on the Semantic Shift Benchmark (SSB). Bold values represent the best results, while underlined values represent the second-best results.", "description": "This table presents the performance comparison of various methods on the Semantic Shift Benchmark (SSB) dataset, across three fine-grained image recognition datasets: CUB, Stanford Cars, and Aircraft. The performance is measured by accuracy (All, Old, and New classes) and averaged across all three datasets. Bold values indicate the best performance, while underlined values show the second best performance for each category.", "section": "5 Experiments"}, {"figure_path": "C4NbtYnyQg/tables/tables_25_2.jpg", "caption": "Table 1: Evaluation on the Semantic Shift Benchmark (SSB). Bold values represent the best results, while underlined values represent the second-best results.", "description": "This table presents the results of various methods on the Semantic Shift Benchmark (SSB), a dataset designed for evaluating generalized category discovery (GCD) methods.  The table shows the accuracy achieved by each method on three different datasets (CUB, Stanford Cars, and Aircraft) for all images, images from known classes, and images from novel classes. The results are categorized by the backbone used (DINO or DINOv2).  Bold values highlight the best performance for each category, while underlined values indicate the second-best performance. This allows for a comparison of the proposed FlipClass method against state-of-the-art GCD approaches.", "section": "5 Experiments"}, {"figure_path": "C4NbtYnyQg/tables/tables_27_1.jpg", "caption": "Table 1: Evaluation on the Semantic Shift Benchmark (SSB). Bold values represent the best results, while underlined values represent the second-best results.", "description": "This table presents the performance comparison of various methods on the Semantic Shift Benchmark (SSB) dataset.  The methods are evaluated across three different fine-grained image recognition datasets: CUB, Stanford Cars, and Aircraft.  Performance is measured by accuracy, broken down into overall accuracy, accuracy on old classes, and accuracy on new classes. Bold values highlight the best performance for each category, and underlined values show the second-best performance. The table provides an overall comparison of the different methods for generalized category discovery.", "section": "5 Experiments"}, {"figure_path": "C4NbtYnyQg/tables/tables_28_1.jpg", "caption": "Table 1: Evaluation on the Semantic Shift Benchmark (SSB). Bold values represent the best results, while underlined values represent the second-best results.", "description": "This table presents the results of the proposed FlipClass model and other state-of-the-art methods on the Semantic Shift Benchmark (SSB) dataset.  The SSB dataset consists of three fine-grained image recognition datasets: CUB, Stanford Cars, and Aircraft. The table shows the accuracy of each method on each dataset, broken down by all classes ('All'), old classes ('Old'), and new classes ('New'). Bold values indicate the best performance for each category, while underlined values show the second-best performance.", "section": "5 Experiments"}]