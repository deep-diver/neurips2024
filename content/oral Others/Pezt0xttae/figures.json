[{"figure_path": "Pezt0xttae/figures/figures_1_1.jpg", "caption": "Figure 1: A motivational example of DapperFL with heterogeneous devices and multiple domains.", "description": "This figure illustrates a scenario in federated learning where three edge devices with varying capabilities and data distributions participate in training a global model. Device 1, due to resource constraints, fails to complete its local model update within the allotted time.  Devices 2 and 3 collect data from different domains, resulting in non-IID (independent and identically distributed) data.  This heterogeneity and the failure of Device 1 lead to performance degradation of the global model, highlighting the challenges addressed by the DapperFL framework.", "section": "Introduction"}, {"figure_path": "Pezt0xttae/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of DapperFL framework with two clients for each communication round.", "description": "This figure illustrates the workflow of the DapperFL framework during a single communication round. It shows how two clients (Client i and Client j) process their local data, and how the central server aggregates their updated models. The process involves several steps: 1) Model Fusion Pruning (MFP) is used to generate personalized, compact local models. 2) Domain Adaptive Regularization (DAR) is used to further improve model performance. 3) A specific aggregation algorithm is used to aggregate heterogeneous local models. The steps are shown in a diagrammatic format, where each step is represented with its own box. The figure provides a high-level overview of the process and helps to understand how the framework works.", "section": "3.1 Overview of DapperFL"}, {"figure_path": "Pezt0xttae/figures/figures_8_1.jpg", "caption": "Figure 3: Comparison of model accuracy of FedMP, NeFL, and DapperFL with different pruning ratios on the Digits and Office Caltech.", "description": "This figure compares the performance of three federated learning frameworks (DapperFL, FedMP, and NeFL) across different pruning ratios on two benchmark datasets (Digits and Office Caltech).  It shows how model accuracy varies as the pruning ratio (the amount of model compression) increases.  This helps to understand the trade-off between model efficiency and accuracy for each method. DapperFL consistently outperforms others.", "section": "4.3 Ablation Study"}, {"figure_path": "Pezt0xttae/figures/figures_9_1.jpg", "caption": "Figure 4: The effect of hyper-parameters in the MFP and DAR modules on model accuracy. \"Office\" in the legend represents the Office Caltech benchmark, and the average accuracy calculated for both benchmarks is labeled as \"AVG\".", "description": "This figure shows the impact of four hyperparameters on model accuracy in the proposed DapperFL framework.  The hyperparameters, \u03b1\u2080, \u03b1\u2098\u1d62\u2099, \u03b5, and \u03b3, are part of the Model Fusion Pruning (MFP) and Domain Adaptive Regularization (DAR) modules.  The plots display the model accuracy on the Digits and Office-Caltech benchmark datasets, as well as the average accuracy across both datasets.  The goal is to demonstrate the optimal values for each hyperparameter to maximize model performance.", "section": "4.3 Ablation Study"}, {"figure_path": "Pezt0xttae/figures/figures_15_1.jpg", "caption": "Figure 5: Learning curves of global accuracies for FedAvg, FedDrop, FedProx, MOON, FedMP, FedSR, NeFL, FPL, and DapperFL on the Digits benchmark and the Office Caltech benchmark.", "description": "This figure shows the learning curves for global accuracy across different communication rounds for nine Federated Learning (FL) frameworks, including DapperFL, on two benchmark datasets (Digits and Office Caltech).  The curves illustrate how the global model accuracy improves as the FL training progresses.  The comparison allows for evaluating the performance of DapperFL against state-of-the-art approaches.", "section": "4.2 Performance Comparison"}, {"figure_path": "Pezt0xttae/figures/figures_15_2.jpg", "caption": "Figure 2: Overview of DapperFL framework with two clients for each communication round.", "description": "This figure illustrates the workflow of the DapperFL framework during a single communication round. It involves two clients, each performing Model Fusion Pruning (MFP) and Domain Adaptive Regularization (DAR) on their local models.  The MFP module generates personalized compact local models using both local and global knowledge.  The DAR module improves the performance of pruned models. The figure visually shows the steps of MFP, DAR, and aggregation on the server. The central server then aggregates the updated local models to produce a new global model for the next round. The figure details the process of model fusion, pruning, aggregation and the interaction between the clients and the server.", "section": "3.1 Overview of DapperFL"}, {"figure_path": "Pezt0xttae/figures/figures_16_1.jpg", "caption": "Figure 3: Comparison of model accuracy of FedMP, NeFL, and DapperFL with different pruning ratios on the Digits and Office Caltech.", "description": "This figure compares the performance of three federated learning (FL) frameworks \u2013 FedMP, NeFL, and DapperFL \u2013 across different pruning ratios on two benchmark datasets (Digits and Office Caltech).  The x-axis represents the pruning ratio (the proportion of model parameters removed). The y-axis shows the model accuracy.  The figure visually demonstrates how the model accuracy changes as model size is reduced, showing DapperFL's superior performance and robustness to model compression.", "section": "4.3 Ablation Study"}]