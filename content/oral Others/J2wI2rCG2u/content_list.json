[{"type": "text", "text": "Stochastic Taylor Derivative Estimator: Efficient amortization for arbitrary differential operators ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zekun Shi Zheyuan Hu Min Lin National University of Singapore National University of Singapore Sea AI Lab Sea AI Lab e0792494@u.nus.edu, linmin@sea.com, shizk@sea.com, ", "page_idx": 0}, {"type": "text", "text": "Kenji Kawaguchi National University of Singapore kenji $@$ nus.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Optimizing neural networks with loss that contain high-dimensional and high-order differential operators is expensive to evaluate with back-propagation due to $O(d^{k})$ scaling of the derivative tensor size and the ${\\mathcal{O}}(2^{k-1}L)$ scaling in the computation graph, where $d$ is the dimension of the domain, $L$ is the number of ops in the forward computation graph, and $k$ is the derivative order. In previous works, the polynomial scaling in $d$ was addressed by amortizing the computation over the optimization process via randomization. Separately, the exponential scaling in $k$ for univariate functions $[d=1]$ ) was addressed with high-order auto-differentiation (AD). In this work, we show how to efficiently perform arbitrary contraction of the derivative tensor of arbitrary order for multivariate functions, by properly constructing the input tangents to univariate high-order AD, which can be used to efficiently randomize any differential operator. When applied to Physics-Informed Neural Networks (PINNs), our method provides $>\\!1000\\times$ speed-up and ${>}30\\times$ memory reduction over randomization with first-order AD, and we can now solve $^{\\,l}$ -million-dimensional PDEs in 8 minutes on a single $N V/D I A\\,A\\,l O O\\,G P U^{1}$ . This work opens the possibility of using high-order differential operators in large-scale problems. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In many problems, especially in Physics-informed machine learning [19, 32], one needs to solve optimization problems where the loss contains differential operators: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{arg\\,min}_{\\theta}f(\\mathbf{x},u_{\\theta}(\\mathbf{x}),\\mathcal{D}^{\\alpha^{(1)}}u_{\\theta}(\\mathbf{x}),\\dots,\\mathcal{D}^{\\alpha^{(n)}}u_{\\theta}(\\mathbf{x})),\\quad u_{\\theta}:\\mathbb{R}^{d}\\to\\mathbb{R}^{d^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "In this above, \u2202x1\u03b1 1\u2202,.|.\u03b1.,|\u2202xd\u03b1d , \u03b1 = (\u03b11, \u03b12, . . . , \u03b1d) is a multi-index, u\u03b8 is some neural network parameterized by $\\theta$ , and $f$ is some cost function. When either the differentiation order $k$ or the dimensionality $d$ is high, the objective function above is expensive to evaluate with back-propagation (backward mode AD) in both memory and computation: the size of the derivative tensor has scaling ${\\mathcal{O}}(d^{k})$ , and the size of the computation graph has scaling $\\mathcal{O}\\big(2^{k-1}L\\big)$ where $L$ is the number of ops in the forward computation graph. ", "page_idx": 0}, {"type": "text", "text": "There have been several efforts to tackle this curse of dimensionality. One line of work uses randomization to amortize the cost of computing differential operators with AD over the optimization process so that the $d$ in the above scaling becomes a constant for the case of $k\\,=\\,2$ . Stochastic Dimension Gradient Descent (SDGD) [13] randomizes over the input dimensions where in each iteration, the partial derivatives are only calculated for a minibatch of sampled dimensions with back-propagation. In [12, 21, 15], the classical technique of Hutchinson Trace Estimator (HTE) [16] is used to estimate the trace of Hessian or Jacobian to inputs. Others choose to bypass AD completely to reduce the complexity of computation. In [30], the finite difference method is used for estimating the Hessian trace. Randomized smoothing [11, 14] uses the expectation over Gaussian random variable as ansatz, so that its derivatives can be expressed as another expectation Gaussian random variable via Stein\u2019s identity [38]. However, compared to AD, the accuracy of these methods is highly dependent on the choice of discretization. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we address the scaling issue in both $d$ and $k$ for the optimization problem in Eq. 1 at the same time, by proposing an amortization scheme that can be efficiently evaluated via high-order AD, which we call Stochastic Taylor Derivative Estimator (STDE). Our main contributions are: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We demonstrate how Taylor mode AD [6], a high-order AD method, can be used to amortize the optimization problem in Eq. 1. Specifically, we show that, with properly constructed input tangents, the univariate Taylor mode can be used to contract multivariate functions\u2019 derivative tensor of arbitrary order;   \n\u2022 We provide a comprehensive procedure for randomizing arbitrary differential operators with STDE, while previous works mainly focus on the Laplacian operator, and we provide abundant examples of STDE constructed for operators in common PDEs;   \n\u2022 STDE encompass and generalizes previous methods like SDGD [13] and HTE [16, 12]. We also prove that HTE-type estimator cannot be generalized beyond fourth order differential operator;   \n\u2022 We determine the efficacy of STDE experimentally. When applied to PINN, our method provides a significant speed-up compared to the baseline method SDGD [13] and the backward-free method like random smoothing [11]. Due to STDE\u2019s low memory requirements and reduced computation complexity, PINNs with STDE can solve 1-million-dimensional PDEs on a single NVIDIA A100 40GB GPU within 8 minutes, which shows that PINNs have the potential to solve complex real-world problems that can be modeled as high-dimensional PDEs. We also provide a detailed ablation study on the source of performance gain of our method. ", "page_idx": 1}, {"type": "text", "text": "2 Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "High-order and forward mode AD The idea of generalizing forward mode AD to high-order derivatives has existed in the AD community for a long time [5, 18, 39, 22]. However, accessible implementation for machine learning was not available until the recent implementation in JAX [6, 7], which implemented the Taylor mode AD for accelerating ODE solver. There are also efforts in creating the forward rule for a specific operator like the Laplacian [23]. Randomization over the linearized part of the AD computation graph was considered in [29]. Forward mode AD can also be used to compute neural network parameter gradient as shown in [2]. ", "page_idx": 1}, {"type": "text", "text": "Randomized Gradient Estimation Randomization [27, 28, 8] is a common technique for tackling the curse of dimensionality for numerical linear algebra computation, which can be applied naturally in amortized optimization [1]. Hutchinson trace estimator [16] is a well-known technique, which has been applied to diffusion model [36] and PINNs [12]. Another case that requires gradient estimation is when the analytical form of the target function is not available (black box), which means AD cannot be applied. The method of zeroth-order optimization [24] can be used in this case, as it only requires evaluating the function at arbitrary input. It is also useful when the function is very complicated like in the case of a large language model [26]. ", "page_idx": 1}, {"type": "text", "text": "3 Preliminaries and discussions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "3.1 First-order auto-differentiation (AD) ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "AD is a technique for evaluating the gradient of composition of known analytical functions commonly called primitives. In an AD framework, a neural network $F_{\\theta}:\\mathbb{R}^{d}\\rightarrow\\mathbf{\\dot{R}}^{d^{\\prime}}$ is constructed as the composition of primitives $F_{i}$ that are parameterized by some parameters $\\theta_{i}$ . In this section, we will consider the neural networks with linear computation graphs like $F=F_{L}\\circ F_{L-1}\\circ\\cdot\\cdot\\cdot\\circ F_{1}$ , but the results generalize to arbitrary directed acyclic graphs (DAGs). We will assume that all hidden dimensions are $h$ . See Appendix $\\mathbf{B}$ for more details on first-order AD. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Forward mode AD Each primitives $F_{i}$ is linearized as the Fr\u00e9chet (directional) derivative $\\partial F_{i}:$ $\\mathbb{R}^{h}\\to\\mathrm{L}(\\mathbb{R}^{h},\\mathbb{R}^{h})$ , which computes the Jacobian-vector-product (JVP): $\\begin{array}{r}{\\partial F_{i}(\\mathbf{a})(\\mathbf{v})=\\left.\\frac{\\partial F}{\\partial\\mathbf{x}_{.}}\\right|_{\\mathbf{a}}\\mathbf{v}}\\end{array}$ , where a is referred to as the primal and $\\mathbf{v}$ the tangent. $\\partial{\\cal F}_{i}$ form a linearized computation graph (third row in Fig. 3), that computes the JVP of the composition \u2202x v: ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\frac{\\partial F}{\\partial\\mathbf{x}}}\\mathbf{v}=\\partial F(\\mathbf{x})(\\mathbf{v})=[\\partial F_{L}\\circ\\partial F_{L-1}\\circ\\cdot\\cdot\\cdot\\circ\\partial F_{1}](\\mathbf{x})(\\mathbf{v}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "By setting the tangent to $\\mathbf{v}$ one of the standard basis of $\\mathbb{R}^{d}$ , JVP computes one column of the Jacobian $D_{F}$ , so the full Jacobian can be computed with $d$ JVPs. Each JVP call requires $O(\\operatorname*{max}(d,h))$ memory as only the current activation $\\mathbf{y}_{i}$ and tangent $\\mathbf{v}_{i}$ are needed to carry out the computation, and the computation complexity is usually in the same order as the forward computation graph. In the case of MLP, both the forward and the linearized graph have a complexity of $\\mathcal{O}\\big(d h+(\\bar{L}-\\mathrm{\\dot{1}{1}})h^{2}\\big)$ . ", "page_idx": 2}, {"type": "text", "text": "Backward mode AD Each primitives $F_{i}$ is linearized as the adjoint of the Fr\u00e9chet derivative $\\partial^{\\top}{\\boldsymbol{F}}_{i}$ instead, which computes the vector-Jacobian-product (VJP): $\\begin{array}{r}{\\partial^{\\top}\\bar{F}_{i}(\\mathbf{a})(\\mathbf{v}^{\\top})=\\mathbf{v}^{\\top}\\ \\frac{\\partial F}{\\partial\\mathbf{x}}\\big\\vert_{\\mathbf{a}}}\\end{array}$ where $\\mathbf{v}^{\\top}$ is the cotangent. The linearized computation graph now runs in the reverse order: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{v}^{\\top}{\\frac{\\partial F}{\\partial\\mathbf{x}}}=\\partial^{\\top}F(\\mathbf{x})(\\mathbf{v}^{\\top})=[\\partial^{\\top}F_{1}(\\mathbf{x})\\circ\\cdots\\circ\\partial^{\\top}F_{L-1}(\\mathbf{y}_{L-2})\\circ\\partial^{\\top}F_{L}(\\mathbf{y}_{L-1})](\\mathbf{v}^{\\top}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which is also clear from Fig. 3. Furthermore, due to this reversion, we first need to do a forward pass to obtain the evaluation trace $\\{\\mathbf{y}_{i}\\}_{i=1}^{L}$ before we can invoke the VJPs $\\partial^{\\top}{\\boldsymbol{F}}_{i}$ , which apparent as shown in Eq. 3. Hence the number of sequential computations is twice as much compared to forward mode. The memory requirement becomes $\\mathcal{O}(d+(L^{-}1)h)$ as we need to store the entire evaluation trace. Similar to JVP, VJP computes one row of $J_{F}$ at a time, so the full Jacobian \u2202\u2202Fx can be computed using $d^{\\prime}$ VJPs. When optimizing scalar cost functions $\\ell(\\theta):\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ of the network parameters $\\theta$ , backward mode efficiently trades off memory with computation complexity as $d^{\\prime}=1$ and only 1 VJP is needed to get the full Jacobian. Furthermore, all parameter $\\theta_{i}$ can use the same cotangent $\\mathbf{v}^{\\top}$ , whereas with forward mode, separate tangent for each parameter $\\theta_{i}$ is needed. ", "page_idx": 2}, {"type": "text", "text": "3.2 Inefficiency of the first-order AD for high-order derivative on inputs ", "text_level": 1, "page_idx": 2}, {"type": "image", "img_path": "J2wI2rCG2u/tmp/bf224d7404ae0f99af3f39624356e1b1016b53e17e7a0d1930dc503537cdd210.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: The computation graph of computing second order gradient by repeated application of backward mode AD, for a function $F(\\cdot)$ with 4 primitives $\\mathcal{L}=4)$ , which computes the Hessianvector-product. Red nodes represent the cotangent nodes in the second backward pass. With each repeated application of VJP the length of sequential computation doubles. ", "page_idx": 2}, {"type": "text", "text": "High-order input derivatives \u2202\u2202kxuk\u03b8 for scalar u\u03b8 can be implemented as repeated applications of first-order AD, but this approach will exhibit fundamental inefficiency that cannot be remedied by randomization. ", "page_idx": 2}, {"type": "text", "text": "Repeating backward mode AD With each repeated application of backward mode AD, the new evaluation trace will include the cotangents from the previous application of backward AD, so the length of sequential computation doubles. Furthermore, the size of the cotangent also grows by $d$ times. Therefore applying backward mode AD has additional memory cost of $O(d+(L-1)h)$ and additional computation cost of $\\mathcal{O}\\big(2d h+2(L-1)h^{2}\\big)$ , which is clear from Fig. 1. In general, with $k$ repeated applications of backward mode AD will incur $\\mathcal{O}\\big(2^{k-1}(d+(L-1)h)\\big)$ memory cost and $\\mathcal{O}\\big(2^{k}(d h+(L-1)h^{2})\\big)$ computation cost. And $O\\big(d^{k-1}\\big)$ calls are needed to evaluate the entire derivative tensor. So both memory and compute scale exponentially in derivative order $k$ ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Repeating forward mode AD Consider $u_{\\theta}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ . The input tangent dimension is $d$ on the first application of forward mode AD, but on the second application, it will become $d\\times d$ since we are now computing the forward mode AD for $\\nabla u_{\\theta}:\\mathbb{R}^{d}\\rightarrow\\mathbf{\\dot{R}}^{d}$ . So the size of the input tangent with $k$ repeated application is $O\\big(d^{k}\\big)$ , so it grows exponentially. This is also inefficient. ", "page_idx": 3}, {"type": "text", "text": "Mixed mode AD schemes are also likely inefficient See more detail in Appendix C. ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.3 Stochastic Dimension Gradient Descent ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "SDGD [13] amortizes high-dimensional differential operators by computing only a minibatch of derivatives in each iteration. It replaces a differential operator $\\mathcal{D}$ with a randomly sampled subset of additive terms, where each term only depends on a few input dimensions ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{D}:=\\sum_{j=1}^{N_{\\mathcal{D}}}\\mathcal{D}_{j}\\approx\\frac{N_{\\mathcal{D}}}{|J|}\\sum_{j\\in J}\\mathcal{D}_{j}:=\\tilde{\\mathcal{D}_{J}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\tilde{\\mathcal{D}}_{J}$ denotes the SDGD operator that approximates the true operator $\\mathcal{D},\\,J$ is the sampled index set, and $|J|$ is the batch size. For example, in $d$ -dimensional Poisson equation, $N_{D}\\,=\\,d$ , $\\begin{array}{r}{D=\\sum_{j=1}^{d}\\frac{\\partial^{2}}{\\partial x_{j}^{2}}}\\end{array}$ , and the additive terms are $\\begin{array}{r}{\\mathcal{D}_{j}=\\frac{\\partial^{2}}{\\partial x_{j}^{2}}}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "$\\tilde{\\mathcal{D}}_{J}$ are cheaper to compute than $\\mathcal{D}$ due to reduced dimensionality: for each sampled index, by treating all other input as constant we get a function with scalar input and output. For a given index set $J$ , the memory requirements are reduced from $\\mathcal{O}\\big(2^{k-1}(d+(L^{'}-1)h)\\big)$ to $\\mathcal{O}\\big(|J|(2^{\\check{k}-1}(1+(L-1)h))\\big)$ , and the computation complexity reduces to $\\mathcal{O}\\big(|J|2^{k}(h+(L-1)h^{2})\\big)$ . This reduction is significant when $d\\gg h$ as in the experimental setting of SDGD [13], but the exponential scaling in $k$ persists. ", "page_idx": 3}, {"type": "text", "text": "3.4 Univariate Taylor mode AD ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "One way to define high-order AD is by determining how the high-order Taylor expansion of a univariate function changes when mapped by primitives. Firstly, the Fr\u00e9chet derivative $\\partial F$ can be rewritten to operate on a space curve $g:\\mathbb{R}\\to\\bar{\\mathbb{R}}^{d}$ that passes through the primal a, i.e. $g(t)=\\mathbf{a}$ , and has tangent $g^{\\bar{\\prime}}(t)=\\mathbf{v}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\partial F(g(t))(g^{\\prime}(t))=\\left.{\\frac{\\partial F}{\\partial\\mathbf{x}}}\\right|_{\\mathbf{x}=g(t)}g^{\\prime}(t)={\\frac{\\mathrm{d}}{\\mathrm{d}t}}[F\\circ g](t).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This shows that the $\\partial$ (JVP) is the same as the univariate chain rule. The tuple $J_{g}(t):=(g(t),g^{\\prime}(t))$ can be thought of as the first-order expansion of $g$ which lives in the tangent bundle of $F$ . Treating $F$ as the smooth map between manifolds, we can define the pushforward $\\mathrm{d}F$ which pushes the first order expansion of $g$ (i.e. $J_{g}(t))$ forward to the first order expansion of $F\\circ g$ (i.e. $J_{F o g}(t))$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}F(J_{g}(t))=J_{F o g}(t)=\\bigg([F\\circ g](t),\\frac{\\mathrm{d}}{\\mathrm{d}t}[F\\circ g](t)\\bigg)=(F(\\mathbf{a}),\\partial F(\\mathbf{a})(\\mathbf{v})).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Naturally, to extend this to higher orders, one can consider the $k$ th order expansion of the input curve $g$ , which is equivalent to the tuple $J_{g}^{k}(t):=(g(t),g^{\\prime}(t),g^{\\prime\\prime}(t),\\ldots,g^{(k)}(t))\\overset{\\cdot}{=}(\\mathbf{a},\\mathbf{v}^{(1)},\\mathbf{v}^{(2)},\\ldots,\\mathbf{v}^{k})$ known as the $k$ -jet of $g$ where $\\mathbf{v}^{j}$ is called the $j$ th order tangent of $g$ . $J_{g}^{k}$ lives in the $k$ th order tangent bundle of $F$ , and we can define the $k$ th-order pushforward $\\mathrm{d}^{k}F$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{d}^{k}F(J_{g}^{k}(t))=J_{F o g}^{k}(t)=\\bigg([F\\circ g](t),\\frac{\\partial}{\\partial t}[F\\circ g](t),\\frac{\\partial^{2}}{\\partial t^{2}}[F\\circ g](t),\\dots,\\frac{\\partial^{k}}{\\partial t^{k}}[F\\circ g](t)\\bigg)}\\\\ &{}&{=\\!(F(\\mathbf{a}),\\partial F(\\mathbf{a})(\\mathbf{v}^{(1)}),\\partial^{2}F(\\mathbf{a})(\\mathbf{v}^{(1)},\\mathbf{v}^{(2)}),\\dots,\\partial^{k}F(\\mathbf{a})(\\mathbf{v}^{(1)},\\dots,\\mathbf{v}^{(k)})),\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which pushes the $k$ th order expansion of $g$ (i.e. $J_{g}^{k}$ ) forward to the $k$ th order expansion of $F\\circ g$ (i.e. $J_{F\\circ g}^{k})$ . $\\begin{array}{r}{\\partial^{k}F=\\frac{\\partial^{k}}{\\partial t^{k}}[F\\circ g](t)}\\end{array}$ is the $k$ -th order Fr\u00e9chet derivative, whose analytical formula is given by the high-order univariate chain rule known as the Faa di Bruno\u2019s formula (Eq. 43). ", "page_idx": 3}, {"type": "text", "text": "Since $J_{g}^{k}$ contains all information needed to evaluate $\\frac{\\partial^{j}}{\\partial t^{j}}[F\\circ g](t)$ for any $j\\leq k$ , the map $\\mathrm{d}^{k}F$ is well-defined. $\\mathrm{d}^{k}$ defines a high-order AD: we can compute $\\mathrm{d}^{k}F$ of arbitrary composition $F$ from the $k$ th-order pushforward of the primitives $\\mathrm{d}^{k}F_{i}$ , since $\\mathrm{d}^{k}$ is an homomorphism of the group $(\\{F_{i}\\},\\circ)$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{d}^{k}[F_{2}\\circ F_{1}](J_{g}^{k}(t))=J_{F_{2}\\circ F_{1}\\circ g}^{k}(t)=\\mathrm{d}^{k}F_{2}(J_{F_{1}\\circ g}^{k}(t))=[\\mathrm{d}^{k}F_{2}\\circ\\mathrm{d}^{k}F_{1}](J_{g}^{k}(t)).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This approach of composing $\\mathrm{d}^{k}$ of primitives is also known as the Taylor mode AD. For more details on Taylor mode AD, see Appendix D. ", "page_idx": 4}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "From the previous discussion, it is clear that the exponential scaling in $k$ for the problem described in Eq. 1 cannot be mitigated by amortization alone. Although high-order AD methods like Taylor mode AD [6] can address this scaling issue, it is only defined for univariate functions. In this section, we describe a method that addresses the scaling issue in $k$ and $d$ simultaneously when amortizing Eq. 1 by seeing univariate Taylor mode AD as contractions of multivariate derivative tensor. ", "page_idx": 4}, {"type": "text", "text": "4.1 Univariate Taylor mode AD as contractions of multivariate derivative tensor ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "$\\mathrm{d}F$ projects the Jacobian of $F$ to $\\mathbb{R}^{d^{\\prime}}$ with a 1-jet $J_{g}(t)$ . Similarly, $\\mathrm{d}^{k}F$ contracts a set of derivative tensors to $\\mathbb{R}^{d^{\\prime}}$ with a $k$ -jet $J_{g}^{k}$ . We can expand ${\\frac{\\partial^{k}}{\\partial t^{k}}}F\\circ g$ with Eq. 43 to see the form of the contractions. For example, $\\partial F$ is JVP, and $\\partial^{2}F$ contains a quadratic form of the Hessian $D_{F}^{2}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\partial^{2}F(\\mathbf{a})(\\mathbf{v}^{(1)},\\mathbf{v}^{(2)})=\\frac{\\partial^{2}}{\\partial t^{2}}[F\\circ g](t)=D_{F}(\\mathbf{a})\\mathbf{v}^{(2)}+D_{F}^{2}(\\mathbf{a})_{d^{\\prime},d_{1},d_{2}}v_{d_{1}}^{(1)}v_{d_{2}}^{(1)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "From Eq. 43, one can always find a $J_{g}^{l}$ with large enough $l\\geq k$ such that there exists $k\\leq l^{\\prime}\\leq l$ with $\\underline{{\\partial}}^{l^{\\prime}}F(J_{g}^{l^{\\prime}})=D_{F}^{k}(\\mathbf{a})\\cdot\\otimes_{i=1}^{k}\\mathbf{v}^{(v_{i})}$ where $v_{i}\\in[1,k]$ , by setting some tangents $\\mathbf{v}^{(v_{i})}$ to the zero vector. That is, arbitrary derivative tensor contraction is contained within a Fr\u00e9chet derivative of high-order, which can be efficiently evaluated through Taylor mode AD. ", "page_idx": 4}, {"type": "text", "text": "How large $l$ should be depends on how off-diagonal the operator is. If the operator is diagonal (i.e. contains no mixed partial derivatives), $l=k$ is enough. If the operator is maximally non-diagonal, i.e. it is a partial derivative where all dimensions to be differentiated are distinct, then the minimum $l$ needed is $(1+k)k/2$ . For more details, please refer to Appendix F where a general procedure for determining the jet structure is discussed. ", "page_idx": 4}, {"type": "image", "img_path": "J2wI2rCG2u/tmp/fccb284a2fa5840dcb4f9b26c3927a00ccc4871e213595ae3be08b3182e814ae.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: The computation graph of $\\mathrm{d}^{2}F$ for $F$ with 4 primitives. Parameters $\\theta_{i}$ are omitted. The first column from the left represents the input 2-jet $J_{g}^{2}(t)=(\\mathbf{x},\\mathbf{v}^{(1)},\\mathbf{v}^{(2)})$ , and $\\mathrm{d}^{2}F_{1}$ pushes it forward JF21\u25e6g(t) = (y1, v(11 ), tion  tphaer a2l-ljeel,t two hbice hc iasc thheed .subsequent column. Each row can be computed ", "page_idx": 4}, {"type": "text", "text": "4.2 Estimating arbitrary differential operator by pushing forward random jets ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Next, we show how to use the above facts to construct a stochastic estimator derivative operator. Differential operators can be evaluated through derivative tensor contraction. The action of the derivative $\\begin{array}{r}{\\Bar{D^{\\alpha}}=\\frac{\\partial^{|\\alpha|}}{\\partial x_{1}^{\\alpha_{1}},\\dots,\\partial x_{d}^{\\alpha_{d}}}}\\end{array}$ on function $u$ can be identified with the derivative tensor slice $D_{u}^{|\\alpha|}(\\mathbf{a})_{\\alpha}$ Differential operator $\\mathcal{L}$ can be written as a linear combination of derivatives: $\\begin{array}{r}{\\mathcal{L}=\\sum_{\\alpha\\in\\mathcal{Z}(\\mathcal{L})}C_{\\alpha}\\mathcal{D}^{\\alpha}}\\end{array}$ , where $\\mathcal{T}(\\mathcal{L})$ is the set of tensor indices representing terms included in the operator $\\mathcal{L}$ . For simplicity we only consider $k$ th order differential operator, i.e. $|\\alpha|=k\\in\\mathbb{N}$ for all $\\alpha$ . For scalar $u:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ , we can identify a $k$ th order differential operator $\\mathcal{L}$ with the following tensor dot product ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}u({\\mathbf a})=\\sum_{\\alpha\\in\\mathbb{Z}(\\mathcal{L})}C_{\\alpha}\\mathcal{D}^{\\alpha}u({\\mathbf a})=\\sum_{d_{1},\\ldots,d_{k}}D_{u}^{k}({\\mathbf a})_{d_{1},\\ldots,d_{k}}C_{d_{1},\\ldots,d_{k}}(\\mathcal{L})=D_{u}^{k}({\\mathbf a})\\cdot{\\mathbf C}(\\mathcal{L}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $d_{i}\\in[1,d],i\\in[1,k]$ is the tensor index on the ith axis, , and $\\mathbf{C}({\\mathcal{L}})$ is a tensor of the same shape as $D_{u}^{k}(\\mathbf{a})$ that equals $C_{\\alpha}$ when $d_{1},\\ldots,d_{k}$ matches the multi-index $\\alpha\\in{\\mathcal{Z}}({\\mathcal{L}})$ and 0 otherwise. We call $\\mathbf{C}({\\mathcal{L}})$ the coefficient tensor of $\\mathcal{L}$ . For example, the coefficient tensor of the Laplacian $\\nabla^{2}$ is the $d$ -dimensional identity matrix I. More complicated operators can be built as $f(\\mathbf{x},u,\\mathcal{D}_{k_{1}}u,\\ldots,\\mathcal{D}_{k_{n}}u)$ where $f$ is arbitrary function. ", "page_idx": 5}, {"type": "text", "text": "Any derivative tensor contractions $D_{u}^{k}(\\mathbf{a})\\!\\cdot\\!\\mathbf{C}(\\mathcal{L})$ can be estimated through random contraction, which can be implemented efficiently as pushing forward random jets from an appropriate distribution. With random $(\\mathbf{v}^{(1)},\\ldots,\\mathbf{v}^{(k)})$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}[D_{u}^{k}(\\mathbf{a})_{d_{1},\\ldots,d_{k}}v_{d_{1}}^{(v_{1})}\\cdot\\cdot\\cdot v_{d_{k}}^{(v_{k})}]=D_{u}^{k}(\\mathbf{a})_{d_{1},\\ldots,d_{k}}\\mathbb{E}[v_{d_{1}}^{(v_{1})}\\cdot\\cdot\\cdot v_{d_{k}}^{(v_{k})}]=D_{u}^{k}(\\mathbf{a})\\cdot\\mathbb{E}\\left[\\bigotimes_{i=1}^{k}\\mathbf{v}^{(v_{i})}\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\otimes$ denotes Kronecker product, $v_{d_{i}}^{(v_{i})}\\in[1,k]$ is the $d_{i}$ dimension of the $v_{i}\\mathrm{th}$ order tangent in the input $k$ -jet. Eq. 11 is an unbiased estimator of the $k$ th order operator $\\mathcal{L}u=D_{u}^{k}(\\mathbf{a})\\cdot\\mathbf{C}(\\mathcal{L})$ when ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}[v_{d_{1}}^{(v_{1})}\\dots v_{d_{k}}^{(v_{k})}]=C_{d_{1},\\dots,d k}(\\mathcal{L}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For example, the condition for unbiasedness for the Laplacian $\\nabla^{2}$ is $\\mathbb{E}[\\mathbf{v}^{(a)}\\mathbf{v}^{(b)\\top}]=\\mathbf{I}$ . As discussed, one can always find a $J_{g}^{l}$ with large enough $l\\geq k$ such that $\\partial^{l}F(J_{g}^{l})=D_{F}^{k}(\\mathbf{a})\\cdot\\otimes_{i=1}^{k}\\mathbf{v}^{(v_{i})}$ , so with a distribution $p$ over the input $l$ -jet $J_{g}^{l}$ that satisfies the unbiasedness condition (Eq. 12), we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{J_{g}^{l}\\sim p}[\\partial^{l}u(J_{g}^{l})]=\\mathbb{E}[v_{d_{1}}^{(v_{1})}\\dots v_{d_{k}}^{(v_{k})}]=D_{u}^{k}(\\mathbf{a})\\cdot{\\bf C}(\\mathcal{L})=\\mathcal{L}u(\\mathbf{a}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which means $\\mathcal{L}u(\\mathbf{a})$ can be approximated by the sample mean of the pushforwards of random $l_{\\cdot}$ -jet drawn from $p$ , which can be computed efficiently via Taylor mode AD. We call this method Stochastic Taylor Derivative Estimator (STDE). The advantages of STDE are: ", "page_idx": 5}, {"type": "text", "text": "1. General: STDE can be applied to differential operators of arbitrary order and dimensionality. ", "page_idx": 5}, {"type": "text", "text": "2. Scalable: The scaling issue in the dimensionality $d$ and the derivative order $k$ are addressed at the same time. From the example computation graph (Fig. 2) we see that, for one call to $\\mathrm{d}^{k}F$ , the memory requirement has scaling of ${\\mathcal{O}}(k d)$ and the computation complexity has scaling $O\\big(k^{2}d L\\big)$ . Like first-order forward mode AD, the derivative tensor $D_{u}^{k}$ is never fully computed and stored. Combined with randomization, the polynomial scaling in $d$ will be removed. ", "page_idx": 5}, {"type": "text", "text": "3. Parallelizable: The number of sequential computations does not grow with the order as can be seen in Fig. 2, and the computation can be trivially vectorized and parallelized since the pushforward of sample jets can be computed independently, and it uses the same computation graph $(\\mathrm{d}^{k}u)$ ; ", "page_idx": 5}, {"type": "text", "text": "4.3 Constructing STDE for high-order differential operators with sparse random jets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Note that all coefficient tensor has the following additive decomposition: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{C}(\\mathcal{L})=\\sum_{d_{1},...,d_{k}\\in\\mathcal{Z}(\\mathcal{D})}C_{d_{1},...,d_{k}}\\mathbf{e}_{d_{1}}\\otimes\\cdot\\cdot\\cdot\\otimes\\mathbf{e}_{d_{k}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{e}_{i}$ is the ith standard basis. For example, if the input dimension $d$ is 3, then $\\mathbf{e}_{2}=\\left[0,1,0\\right]^{\\top}$ . As discussed before, there exists a $J_{g}^{k}$ whose pushforward under $\\partial^{l}u$ is equivalent to contracting $D_{u}^{k}$ with $\\otimes_{i=1}^{k}\\mathbf{e}_{d_{i}}$ . We call $k$ -jet consisting of only standard basis and the zero vector 0 sparse. Therefore the discrete distribution $p$ over the sparse $k$ -jets in Eq. 14 satisfies the unbiasedness condition 12 ", "page_idx": 5}, {"type": "equation", "text": "$$\np\\big(\\otimes_{i=1}^{k}\\mathbf{e}_{d_{i}}\\big)=C_{d_{1},\\ldots,d_{k}}/Z,\\quad d_{1},\\ldots,d_{k}\\in\\mathbb{Z}(\\mathcal{L}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $Z$ is the normalization factor and we identify $\\otimes_{i=1}^{k}\\mathbf{e}_{d_{i}}$ with the corresponding $k$ -jet $J_{u}^{k}$ . ", "page_idx": 5}, {"type": "text", "text": "4.3.1 Differential operator with easy to remove mixed partial derivatives ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Next, we show some concrete examples for constructing STDE with sparse random jets. ", "page_idx": 5}, {"type": "text", "text": "Laplacian From Eq. 9 we know that the quadratic form of Hessian can be computed through $\\partial^{2}$ by setting $\\mathbf{v}^{(2)}=\\mathbf{0}$ and $\\bar{\\mathbf{v}}^{(1)}=\\mathbf{e}_{j}$ . Therefore, the STDE of the Laplacian operator is given by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tilde{\\nabla}^{2}\\j u_{\\theta}(\\mathbf{a})=\\frac{d}{|J|}\\sum_{j\\in J}\\frac{\\partial^{2}}{\\partial x_{j}^{2}}u_{\\theta}(\\mathbf{a})=\\frac{d}{|J|}\\sum_{j\\in J}\\partial^{2}u_{\\theta}(\\mathbf{a})(\\mathbf{e}_{j},\\mathbf{0})=\\frac{d}{|J|}\\sum_{j\\in J}\\mathrm{d}^{2}u_{\\theta}(\\mathbf{a},\\mathbf{e}_{j},\\mathbf{0})_{[2]},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $J$ is the sampled index set, and the subscript [2] means taking the second-order tangent from the output jet. See example implementation in JAX in Appendix A.4. ", "page_idx": 6}, {"type": "text", "text": "High-order diagonal differential operators We call a differential operator diagonal if it is a linear $\\begin{array}{r}{\\mathcal{L}=\\sum_{j=1}^{d}\\frac{\\partial^{k}}{\\partial x_{j}^{k}}}\\end{array}$ that setting the first-order tangent $\\mathbf{v}^{(1)}$ to ${\\bf{e}}_{j}$ and all other tangents $\\mathbf{v}^{(i)}$ to the zero vector gives the desired high-order diagonal element: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{L}}_{J}u_{\\theta}(\\mathbf{a})=\\frac{d}{|J|}\\sum_{j\\in J}\\frac{\\partial^{k}}{\\partial\\mathbf{x}_{j}^{k}}u_{\\theta}(\\mathbf{a})=\\frac{d}{|J|}\\sum_{j\\in J}\\partial^{k}u_{\\theta}(\\mathbf{a})(\\mathbf{e}_{j},\\mathbf{0},\\dots).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "General nonlinear second-order PDEs Second-order parabolic PDEs are a large class of PDEs. It includes the Fokker-Planck equation in statistical mechanics to describe the evolution of the state variables in stochastic differential equations (SDEs), which can be used for generative modeling [37]. It also includes the Black-Scholes equation in mathematical finance for option pricing, the Hamilton-Jacobi-Bellman equation in optimal control, and the Schr\u00f6dinger equation in quantum physics and chemistry. Its form is given by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\partial}{\\partial t}u({\\bf x},t)+\\frac{1}{2}\\operatorname{tr}\\left(\\sigma\\sigma^{\\top}({\\bf x},t)\\frac{\\partial^{2}}{\\partial{\\bf x}^{2}}u({\\bf x},t)\\right)+\\nabla u({\\bf x},t)\\cdot\\mu({\\bf x},t)+f(t,{\\bf x},u({\\bf x},t),\\sigma^{\\top}({\\bf x},t)\\nabla u({\\bf x},t))=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We have a second order derivative term $\\begin{array}{r}{\\frac{1}{2}\\operatorname{tr}\\left(\\sigma(\\mathbf{x},t)\\sigma(\\mathbf{x},t)^{\\top}\\frac{\\partial^{2}}{\\partial\\mathbf{x}^{2}}u(\\mathbf{x},t)\\right)}\\end{array}$ with off-diagonal term. The off-diagonals can be easily removed via a change of variable: ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\frac{1}{2}}\\operatorname{tr}\\left(\\sigma(\\mathbf{x},t)\\sigma(\\mathbf{x},t)^{\\top}{\\frac{\\partial^{2}}{\\partial\\mathbf{x}^{2}}}u(\\mathbf{x},t)\\right)={\\frac{1}{2}}\\sum_{i=1}^{d}\\partial^{2}u(\\mathbf{x},t)(\\sigma(\\mathbf{x},t)\\mathbf{e}_{i},\\mathbf{0}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "See derivation in Appendix E. Its STDE samples over the $d$ terms in the expression above. ", "page_idx": 6}, {"type": "text", "text": "4.3.2 Differential operators with arbitrary mixed partial derivative ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "It is not always possible to remove the mixed partial derivatives but discussed in section 4.2, for an arbitrary $k$ th order derivative tensor element $\\mathbf{\\dot{D}}_{u}^{k}(\\mathbf{a})_{n_{1},\\dots,n_{k}}$ , we can find an appropriate $l$ -jet $J_{g}^{l}(t)$ with $g(t)=\\mathbf{a}$ such that $\\partial^{l}u(J_{g}^{l})=D_{u}^{k}(\\mathbf{a})_{n_{1},\\dots,n_{k}}$ . Here we show a concrete example. ", "page_idx": 6}, {"type": "text", "text": "2D Korteweg-de Vries $(\\mathbf{KdV})$ equation Consider the following 2D KdV equation ", "page_idx": 6}, {"type": "equation", "text": "$$\nu_{t y}+u_{x x x y}+3(u_{y}u_{x})_{x}-u_{x x}+2u_{y y}=0.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "All the derivative terms can be found in the pushforward of the following jet: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\Im=\\mathrm{d}^{13}u(\\mathbf{x},\\mathbf{v}^{(1)},\\allowbreak\\dots,\\mathbf{v}^{(13)}),\\;\\mathbf{v}^{(3)}=\\mathbf{e}_{x},\\mathbf{v}^{(4)}=\\mathbf{e}_{y},\\mathbf{v}^{(7)}=\\mathbf{e}_{t},\\mathbf{v}^{(i)}=\\mathbf{0},\\forall i\\notin\\{3,4,7\\},}\\\\ &{}&{u_{x}=\\Im_{[1]},\\;\\boldsymbol{u}_{y}=\\Im_{[2]},\\;\\boldsymbol{u}_{x x}=\\Im_{[4]},\\;\\boldsymbol{u}_{x y}=\\Im_{[5]}/35,}\\\\ &{}&{u_{y y}=\\Im_{[6]}/35,\\;\\boldsymbol{u}_{t y}=\\Im_{[9]}/330,\\;u_{x x x y}=\\Im_{[11]}/200200.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the subscript $[i]$ means selecting the ith order tangent from the jet, and the prefactors are determined through Faa di Bruno\u2019s formula (Eq. 43). In this case, no randomization is needed since all the terms can be computed with just one pushforward. Alternatively, these terms can be computed with pushforwards of different jets of lower order (Appendix I.4). When input dimension $d$ is high, randomization via STDE will provide significant speed up. We tested a few more high-order PDEs with irremovable mixed partial derivatives (see Appendix I.4), and the experimental results will be provided later. ", "page_idx": 6}, {"type": "text", "text": "4.4 Dense random jet and connection to HTE ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In section 4.3 we show how to construct STDE with the pushforward of sparse random jets. It is also possible to construct STDE with dense random jets, i.e. jets with tangents that are not the standard basis. For example, the classical method of Hutchinson trace estimator (HTE) [16] can be implemented in the STDE framework as the pushforward of isotropic dense random jets, i.e. $(\\mathbf{a},\\mathbf{v},\\mathbf{\\bar{0}})\\sim\\delta_{\\mathbf{a}}\\times p\\times\\delta$ with $\\mathbb{E}_{p}[\\mathbf{v}\\mathbf{v}^{\\top}]=\\mathbf{I}$ . ", "page_idx": 7}, {"type": "text", "text": "We generalize the dense construction to arbitrary second-order differential operators using a multivariate Gaussian distribution with the eigenvalues of the corresponding coefficient tensor as its covariance. Suppose $\\mathcal{D}$ is a second-order differential operator with coefficient tensor $\\mathbf{C}$ . With the eigendecomposition $\\mathbf{C}^{\\prime\\prime}=\\frac{1}{2}(\\mathbf{C}+\\mathbf{C}^{\\top})+\\lambda\\mathbf{I}=\\mathbf{U}\\Sigma\\mathbf{U}^{\\top}$ where $-\\lambda$ is smaller than the smallest eigenvalue of $\\mathbf{C}$ , we can construct a STDE for $\\mathcal{D}$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathbf{v}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{Z})}[\\partial^{2}u(\\mathbf{a})(\\mathbf{U}\\mathbf{v},\\mathbf{0})]-\\lambda\\mathbb{E}_{\\mathbf{v}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})}[\\partial^{2}u(\\mathbf{a})(\\mathbf{v},\\mathbf{0})]=D_{u}^{2}(\\mathbf{a})\\cdot[\\mathbf{C}^{\\prime\\prime}-\\lambda\\mathbf{I}]=D_{u}^{2}(\\mathbf{a})\\cdot\\mathbf{C}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "However, it is not always possible to construct dense STDE beyond the second order, even if we consider $p$ with non-diagonal covariance. We prove this by providing a counterexample: one cannot construct an STDE for the fourth order operator $\\textstyle\\sum_{i=1}^{d}{\\frac{\\partial^{\\bar{4}}}{\\partial x^{4}}}$ with dense jets. For more details on dense jets, see Appendix K. For specific high-order operators like the Biharmonic operator, it is still possible to construct STDE with dense jets which we show in Appendix J. ", "page_idx": 7}, {"type": "text", "text": "The main differences between the sparse and the dense version of STDE are: ", "page_idx": 7}, {"type": "text", "text": "1. sparse STDE is universally application whereas the dense STDE can only be applied to certain operators;   \n2. the source of variance is different (see Appendix K.3). ", "page_idx": 7}, {"type": "text", "text": "It is also worth noting that both the sparse and the dense versions of STDE would have similar computation costs if the batch size of random jets were the same. In general, we would suggest to use sparse STDE unless it is known a priori that the sparse version would suffer from excess variance and the dense STDE is applicable. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We applied STDE to amortize the training of PINNs on a set of real-world PDEs. For the case of $k=2$ and large $d$ , we tested two types of PDEs: inseparable and effectively high-dimensional PDEs (Appendix I.1) and semilinear parabolic PDEs (Appendix I.2). We also tested high-order PDEs (Appendix I.4) that cover the case of $k=3,4$ , which includes PDEs describing 1D and 2D nonlinear dynamics, and high-dimensional PDE with gradient regularization [41]. Furthermore, we tested a weight-sharing technique (Appendix G), which further reduces memory requirements (Appendix I.3). In all our experiments, STDE drastically reduces computation and memory costs in training PINNs, compared to the baseline method of SDGD with stacked backward-mode AD. Due to the page limit, the most important results are reported here, and the full details including the experiment setup and hyperparameters (Appendix $\\mathrm{H}$ ) can be found in the Appendix. ", "page_idx": 7}, {"type": "text", "text": "5.1 Physics-informed neural networks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "PINN [32] is a class of neural PDE solver where the ansatz $u_{\\theta}(\\mathbf{x})$ is parameterized by a neural network with parameter $\\theta$ . It is a prototypical case of the optimization problem in Eq. 1. We consider PDEs defined on a domain $\\Omega\\subset\\mathring{\\mathbb{R}}^{d}$ and boundary/initial $\\partial\\Omega$ as follows ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal L u(\\mathbf x)=f(\\mathbf x),\\quad\\mathbf x\\in\\Omega,\\quad\\mathcal B u(\\mathbf x)=g(\\mathbf x),\\quad\\mathbf x\\in\\partial\\Omega,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\mathcal{L}$ and $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ are known operators, $f(\\mathbf{x})$ and $g\\mathbf{(x)}$ are known functions for the residual and boundary/initial conditions, and $u:\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}$ is a scalar-valued function, which is the unknown solution to the PDE. The approximated solution $u_{\\theta}(\\mathbf{x})\\approx u(\\mathbf{x})$ is obtained by minimizing the mean squared error (MSE) of the PDE residual $R(\\mathbf{x};\\theta)=\\mathcal{L}u_{\\theta}(\\mathbf{x})-f(\\mathbf{x})$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{residual}}(\\theta;\\{\\mathbf{x}^{(i)}\\}_{i=1}^{N_{r}})=\\frac{1}{N_{r}}\\sum_{i=1}^{N_{r}}\\left|\\mathcal{L}u_{\\theta}(\\mathbf{x}^{(i)})-f(\\mathbf{x}^{(i)})\\right|^{2}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the residual points $\\{\\mathbf{x}^{(i)}\\}_{i=1}^{N_{r}}$ are sampled from the domain $\\Omega$ . We use the technique from [25] that reparameterizes $u_{\\theta}$ such that the boundary/initial condition $B u(\\mathbf{x})=g(\\mathbf{x})$ are satisfied exactly for all $\\mathbf{x}\\in\\partial\\Omega$ , so boundary loss is not needed. ", "page_idx": 7}, {"type": "text", "text": "Amortized PINNs PINN training can be amortized by replacing the differential part of the operator $\\mathcal{L}$ with a stochastic estimator like SDGD and STDE. For example, for the Allen-Cahn equation, $\\mathcal{L}u=\\nabla^{2}u+u-u^{3}$ , the differential part of $\\mathcal{L}$ is the Laplacian $\\nabla^{\\hat{2}}$ . With amortization, we minimize the following loss ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\tilde{\\ell}_{\\mathrm{residual}}(\\theta;\\{\\mathbf{x}^{(i)}\\}_{i=1}^{N_{r}},J,K)=\\frac{1}{N_{r}}\\sum_{i=1}^{N_{r}}\\left[\\tilde{\\mathcal{L}}_{J}u_{\\theta}(\\mathbf{x}^{(i)})-f(\\mathbf{x}^{(i)})\\right]\\cdot\\left[\\tilde{\\mathcal{L}}_{K}u_{\\theta}(\\mathbf{x}^{(i)})-f(\\mathbf{x}^{(i)})\\right],\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "which is a modification of Eq. 24. Its gradient \u2202\u2113\u02dcr\u2202es\u03b8idual is then an unbiased estimator to the gradient of the original PINN residual loss, i.e. $\\begin{array}{r}{\\mathbb{E}[\\frac{\\partial\\tilde{\\ell}_{\\mathrm{residual}}}{\\partial\\theta}]=\\frac{\\partial\\ell_{\\mathrm{residual}}}{\\partial\\theta}}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "5.2 Ablation study on the performance gain ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To ascertain the source performance gain of our method, we conduct a detailed ablation study on the inseparable Allen-Cahn equation with a two-body exact solution described in Appendix I.1. The results are in Table 1 and 2, where the best results for each dimensionality are marked in bold. All methods were implemented using JAX unless stated. OOM indicates that the memory requirement exceeds 40GBs. Since the only change is how the derivatives are computed, the relative L2 error is expected to be of the same order among different randomization methods, as seen in Table 3 in the Appendix. We have included Forward Laplacian which is an exact method. It is expected to perform better in terms of L2 error. However, as we can see in Table 3, the L2 error is of the same order, at least in the case where the dimension is more than 1000. ", "page_idx": 8}, {"type": "table", "img_path": "J2wI2rCG2u/tmp/8dbb742e6018f617cea66e433d14697c5a49fdfe50cc9501cdd2fb974023086e.jpg", "table_caption": ["Table 1: Speed ablation for the two-body Allen-Cahn equation. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "J2wI2rCG2u/tmp/d0856e2810b6d6d1d6246e77c815346e463fea183e0c51765ce9889472ba15ce.jpg", "table_caption": ["Table 2: Memory ablation for the two-body Allen-Cahn equation. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "JAX vs PyTorch The original SDGD with stacked backward mode AD was implemented in PyTorch. We reimplement it in JAX (see Appendix A.1). From Table 1 and 2, JAX provides $\\mathord{\\sim}15\\times$ speed-up and up to ${\\sim}4\\times$ memory reduction. ", "page_idx": 8}, {"type": "text", "text": "Parallelization The original SDGD implementation uses a for-loop to iterate through the sampled dimension (Appendix A.1). This can be parallelized (denoted as \u201cParallelized SDGD via HVP\u201d, details in Appendix A.2). Parallelization provides $\\mathord{\\sim}15\\times$ speed up and reduction in peak memory for the JIT compilation phase. We also tested mixed mode AD (dubbed as \u201cForward-over-Backward SDGD\u201d), which gives roughly the same performance as parallelized stacked backward mode, which is expected as explained in Appendix C. ", "page_idx": 8}, {"type": "text", "text": "Forward Laplacian Forward Laplacian [23] provides a constant-level optimization for the calculation of Laplacian operator by removing the redundancy in the AD pipeline, and we can see from ", "page_idx": 8}, {"type": "text", "text": "Table 1 and 2 that it is the best method in both speed and memory when the dimension is 100. But since it is not a randomized method, the scaling is much worse. Its computation complexity is $O(d)$ , whereas a randomized estimator like STDE has a computation complexity of $\\mathcal{O}(|J|)$ . Naturally, with a high enough input dimension $d$ , the difference in the constant prefactor is trumped by scaling. When the dimension is larger than 1000, it becomes worse than even parallelized stacked backward mode SDGD. ", "page_idx": 9}, {"type": "text", "text": "STDE Compared to the best realization of baseline method SDGD, the parallelized stacked backward mode AD, STDE provides up to $10\\times$ speed up and memory reduction of at least $4\\times$ . ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduce STDE, a general method for constructing stochastic estimators for arbitrary differential operators that can be evaluated efficiently via Taylor mode AD. We evaluated STDE on PINNs, an instance of the optimization problem where the loss contains differential operators. Amortization with STDE outperforms the baseline methods, and STDE also applies to a wider class of problems as it can be applied to arbitrary differential operators. ", "page_idx": 9}, {"type": "text", "text": "Applicability Besides PINNs, STDE can be applied to arbitrarily high-order and high-dimensional AD-based PDE solvers. This makes STDE more general than a branch of related methods. STDE is also more applicable than deep ritz method [40], weak adversarial network (WAN) [42], backward SDE-based solvers [3, 33, 10], deep Galerkin method [34], and the recently proposed forward Laplacian [23], which are all restricted to specific forms of second-order PDEs. STDE applies naturally to differential operators in PDEs, but it can also be applied to other problems that require input gradients. For example, adversarial attacks, feature attribution, and meta-learning, to name a few. ", "page_idx": 9}, {"type": "text", "text": "Limitations Being a general method, STDE forgoes the optimization possibilities that apply to specific operators. Furthermore, we did not consider variance reduction techniques that could be applied, which can be explored in future works. Also, we observed that lowering the randomization batch size improves both speed and memory profile, but the trade-off between cheaper computation and larger variance needs further analysis. Furthermore, the method is not suited for computing the high order derivative of neural network parameter as explained in Section 3. ", "page_idx": 9}, {"type": "text", "text": "Future works The key insight of the STDE construction is that the univariate Taylor mode AD contains arbitrary contraction of the derivative tensor and that derivative operators are derivative tensor contractions. This shows the connection between the fields of AD and randomized numerical linear algebra and indicates that further works in the intersection of these two fields might bring significant progress in large-scale scientific modeling with neural networks. One example would be the many-body Schr\u00f6dinger equations, where one needs to compute a high-dimensional Laplacian. Another example is the high-dimensional Black-Scholes equation, which has numerous uses in mathematical finance. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "[1] Brandon Amos. Tutorial on amortized optimization, April 2023. arXiv:2202.00665 [cs, math].   \n[2] At\u0131l\u0131m G\u00fcnes\u00b8 Baydin, Barak A. Pearlmutter, Don Syme, Frank Wood, and Philip Torr. Gradients without Backpropagation, February 2022. arXiv:2202.08587 [cs, stat]. [3] Christian Beck, Sebastian Becker, Patrick Cheridito, Arnulf Jentzen, and Ariel Neufeld. Deep splitting method for parabolic PDEs. SIAM Journal on Scientific Computing, 43(5):A3135\u2013 A3154, January 2021. arXiv:1907.03452 [cs, math, stat].   \n[4] Sebastian Becker, Ramon Braunwarth, Martin Hutzenthaler, Arnulf Jentzen, and Philippe von Wurstemberger. Numerical simulations for full history recursive multilevel Picard approximations for systems of high-dimensional partial differential equations. Communications in Computational Physics, 28(5):2109\u20132138, June 2020. arXiv:2005.10206 [cs, math].   \n[5] Claus Bendtsen and Ole Stauning. Tadiff , a flexible $\\mathrm{c}++$ package for automatic differentiation using taylor series expansion. 1997.   \n[6] Jesse Bettencourt, Matthew J. Johnson, and David Duvenaud. Taylor-mode automatic differentiation for higher-order derivatives in JAX. In Program Transformations for ML Workshop at NeurIPS 2019, 2019.   \n[7] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python $^{+}$ NumPy programs, 2018.   \n[8] Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, and Mark Crowley. Johnson-Lindenstrauss Lemma, Linear and Nonlinear Random Projections, Random Fourier Features, and Random Kitchen Sinks: Tutorial and Survey, August 2021. arXiv:2108.04172 [cs, math, stat].   \n[9] Andreas Griewank and Andrea Walther. Evaluating Derivatives. Society for Industrial and Applied Mathematics, second edition, 2008.   \n[10] Jiequn Han, Arnulf Jentzen, and Weinan E. Solving high-dimensional partial differential equations using deep learning. Proceedings of the National Academy of Sciences, 115(34):8505\u2013 8510, Aug 2018.   \n[11] Di He, Shanda Li, Wenlei Shi, Xiaotian Gao, Jia Zhang, Jiang Bian, Liwei Wang, and Tie-Yan Liu. Learning Physics-Informed Neural Networks without Stacked Back-propagation, February 2023. arXiv:2202.09340 [cs].   \n[12] Zheyuan Hu, Zekun Shi, George Em Karniadakis, and Kenji Kawaguchi. Hutchinson Trace Estimation for High-Dimensional and High-Order Physics-Informed Neural Networks. Computer Methods in Applied Mechanics and Engineering, 424:116883, May 2024. arXiv:2312.14499 [cs, math, stat].   \n[13] Zheyuan Hu, Khemraj Shukla, George Em Karniadakis, and Kenji Kawaguchi. Tackling the Curse of Dimensionality with Physics-Informed Neural Networks, July 2023. arXiv:2307.12306 [cs, math, stat].   \n[14] Zheyuan Hu, Zhouhao Yang, Yezhen Wang, George Em Karniadakis, and Kenji Kawaguchi. Bias-Variance Trade-off in Physics-Informed Neural Networks with Randomized Smoothing for High-Dimensional PDEs, November 2023. arXiv:2311.15283 [cs, math, stat].   \n[15] Zheyuan Hu, Zhongqiang Zhang, George Em Karniadakis, and Kenji Kawaguchi. Score-Based Physics-Informed Neural Networks for High-Dimensional Fokker-Planck Equations, February 2024. arXiv:2402.07465 [cs, math, stat].   \n[16] M.F. Hutchinson. A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines. Communications in Statistics - Simulation and Computation, 18(3):1059\u2013 1076, January 1989.   \n[17] Martin Hutzenthaler, Arnulf Jentzen, Thomas Kruse, Tuan Anh Nguyen, and Philippe von Wurstemberger. Overcoming the curse of dimensionality in the numerical approximation of semilinear parabolic partial differential equations, July 2018.   \n[18] Jerzy Karczmarczuk. Functional differentiation of computer programs. In Proceedings of the Third ACM SIGPLAN International Conference on Functional Programming, ICFP \u201998, pages 195\u2013203, New York, NY, USA, 1998. Association for Computing Machinery.   \n[19] George Em Karniadakis, Ioannis G. Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-informed machine learning. Nature Reviews Physics, 3(6):422\u2013440, Jun 2021.   \n[20] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.   \n[21] Chieh-Hsin Lai, Yuhta Takida, Naoki Murata, Toshimitsu Uesaka, Yuki Mitsufuji, and Stefano Ermon. Regularizing score-based models with score fokker-planck equations. In NeurIPS 2022 Workshop on Score-Based Methods, 2022.   \n[22] Jacob Laurel, Rem Yang, Shubham Ugare, Robert Nagel, Gagandeep Singh, and Sasa Misailovic. A general construction for abstract interpretation of higher-order automatic differentiation. Proc. ACM Program. Lang., 6(OOPSLA2), oct 2022.   \n[23] Ruichen Li, Haotian Ye, Du Jiang, Xuelan Wen, Chuwei Wang, Zhe Li, Xiang Li, Di He, Ji Chen, Weiluo Ren, and Liwei Wang. Forward Laplacian: A New Computational Framework for Neural Network-based Variational Monte Carlo, July 2023. arXiv:2307.08214 [physics].   \n[24] Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred Hero, and Pramod K. Varshney. A Primer on Zeroth-Order Optimization in Signal Processing and Machine Learning, June 2020. arXiv:2006.06224 [cs, eess, stat].   \n[25] Lu Lu, Rapha\u00ebl Pestourie, Wenjie Yao, Zhicheng Wang, Francesc Verdugo, and Steven G. Johnson. Physics-informed neural networks with hard constraints for inverse design. SIAM Journal on Scientific Computing, 43(6):B1105\u2013B1132, 2021.   \n[26] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee, Danqi Chen, and Sanjeev Arora. Fine-Tuning Language Models with Just Forward Passes, January 2024. arXiv:2305.17333 [cs].   \n[27] Per-Gunnar Martinsson and Joel Tropp. Randomized Numerical Linear Algebra: Foundations & Algorithms, March 2021. arXiv:2002.01387 [cs, math].   \n[28] Riley Murray, James Demmel, Michael W. Mahoney, N. Benjamin Erichson, Maksim Melnichenko, Osman Asif Malik, Laura Grigori, Piotr Luszczek, Micha\u0142 Derezin\u00b4ski, Miles E. Lopes, Tianyu Liang, Hengrui Luo, and Jack Dongarra. Randomized Numerical Linear Algebra : A Perspective on the Field With an Eye to Software, April 2023. arXiv:2302.11474 [cs, math].   \n[29] Deniz Oktay, Nick McGreivy, Joshua Aduol, Alex Beatson, and Ryan P. Adams. Randomized Automatic Differentiation, March 2021. arXiv:2007.10412 [cs, stat].   \n[30] Tianyu Pang, Kun Xu, Chongxuan Li, Yang Song, Stefano Ermon, and Jun Zhu. Efficient Learning of Generative Models via Finite-Difference Score Matching, November 2020. arXiv:2007.03317 [cs, stat].   \n[31] Juncai Pu and Yong Chen. Lax pairs informed neural networks solving integrable systems, January 2024. arXiv:2401.04982 [nlin].   \n[32] M. Raissi, P. Perdikaris, and G.E. Karniadakis. Physics-informed neural networks: a deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686\u2013707, February 2019.   \n[33] Maziar Raissi. Forward-Backward Stochastic Neural Networks: Deep Learning of Highdimensional Partial Differential Equations, April 2018. arXiv:1804.07010 [cs, math, stat].   \n[34] Justin Sirignano and Konstantinos Spiliopoulos. Dgm: a deep learning algorithm for solving partial differential equations. Journal of computational physics, 375:1339\u20131364, 2018.   \n[35] Maciej Skorski. Modern analysis of hutchinson\u2019s trace estimator. In 2021 55th Annual Conference on Information Sciences and Systems (CISS). IEEE, March 2021.   \n[36] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced Score Matching: A Scalable Approach to Density and Score Estimation, June 2019. arXiv:1905.07088 [cs, stat].   \n[37] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-Based Generative Modeling through Stochastic Differential Equations, February 2021. arXiv:2011.13456 [cs, stat].   \n[38] Charles M. Stein. Estimation of the Mean of a Multivariate Normal Distribution. The Annals of Statistics, 9(6):1135 \u2013 1151, 1981.   \n[39] Mu Wang. High Order Reverse Mode of Automatic Differentiation. PhD thesis, 2017. Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works; Last updated - 2023-03-04.   \n[40] E Weinan and Ting Yu. The deep ritz method: a deep learning-based numerical algorithm for solving variational problems. Communications in Mathematics and Statistics, 6:1 \u2013 12, 2017.   \n[41] Jeremy Yu, Lu Lu, Xuhui Meng, and George Em Karniadakis. Gradient-enhanced physicsinformed neural networks for forward and inverse PDE problems. Computer Methods in Applied Mechanics and Engineering, 393:114823, April 2022. arXiv:2111.02801 [physics].   \n[42] Yaohua Zang, Gang Bao, Xiaojing Ye, and Haomin Zhou. Weak Adversarial Networks for Highdimensional Partial Differential Equations. Journal of Computational Physics, 411:109409, June 2020. arXiv:1907.08272 [cs, math]. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Example implementations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 PyTorch implementation of SDGD-PINN using backward mode AD ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The original implementation of SDGD-PINN [13] computes the SDGD estimation of derivatives using a for-loop that iterates over the sampled PDE term/dimension. For example, given a function $f$ representing the MLP PINN, the computation of SDGD for the Laplacian operator can be implemented in PyTorch as follows: ", "page_idx": 13}, {"type": "text", "text": "$\\tt f_{\\mathrm{-}x}\\;=$ torch.autograd.grad(f.sum(), x, create_graph $\\overrightharpoon{}$ True)[0]   \nidx_set $=\\,\\mathfrak{n p}$ .random.choice(dim, sdgd_batch_size, replace $=$ False)   \nhess_diag_val $=~0$ .   \nfor i in idx_set: hess_diag_i $=$ torch.autograd.grad( f_x[:, i].sum(), x, create_graph $\\equiv$ True)[0][:, i] hess_diag_val $+=$ hess_diag_i.detach() $^*$ dim / sdgd_batch_size ", "page_idx": 13}, {"type": "text", "text": "After computing the PDE differential operator, it is plugged into the residual loss, and then backwardmode AD is employed to produce the gradient for optimization concerning $\\theta$ . ", "page_idx": 13}, {"type": "text", "text": "A.2 JAX implementation of SDGD Parallelization via HVP ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "def hvp(f, x, v): \"\"\"stacked backward-mode Hessian-vector product\"\"\" return jax.grad(lambda x: jnp.vdot(jax.grad(f)(x), v))(x)   \nf_hess_diag_fn $=$ lambda i: hvp(f_partial, x_i, jnp.eye(dim)[i])[i]   \nidx_set $=$ jax.random.choice( key, dim, shape $=$ (sdgd_batch_size,), replace $=$ False   \nhess_diag_val $=$ jax.vmap(f_hess_diag_fn)(idx_set) ", "page_idx": 13}, {"type": "text", "text": "A.3 JAX implementation of Forward-over-backward AD ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The forward-over-backward AD In JAX mentioned in Appendix C can be implemented as follows: ", "page_idx": 13}, {"type": "table", "img_path": "J2wI2rCG2u/tmp/5b211b1b8fb5269d2b44e0f85e7de5e5eb02d21bffec1f4321def88751cf2fb3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.4 JAX implementation of STDE for the Laplacian operator ", "text_level": 1, "page_idx": 13}, {"type": "table", "img_path": "J2wI2rCG2u/tmp/a09ebaa4a5e0214de33aeae18f37f2712d0e644aabcb65dd5011ca2a448e0eb3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "The jet.jet function from JAX implements the high-order pushforward $\\mathrm{d}^{n}$ of jets in Eq. 7. It decomposes the input function into primitives, which have analytical derivatives derived up to arbitrary order, and uses the generalized chain rule (see section D.2) to compose the primitives into the pushforward of jets. Note that in the API of jet.jet, all the high-order tangents of the input jet are specified via the series argument. ", "page_idx": 13}, {"type": "text", "text": "B Further details on first-order auto-differentiation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Computation graph of first-order AD ", "text_level": 1, "page_idx": 14}, {"type": "image", "img_path": "J2wI2rCG2u/tmp/ea5fa1ab7a6bf27acf0b2c34dc37252019604ed17431138a3137455c5ce60ccf.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 3: The computation graph of forward mode AD (left) and backward mode AD (right) of a function $F(\\cdot)$ with 4 primitives $F_{i}$ each parameterized by $\\theta_{i}$ . Nodes represent (intermediate) values, and arrows represent computation. Input nodes are colored blue; output nodes are colored green, and intermediate nodes are colored yellow. ", "page_idx": 14}, {"type": "text", "text": "B.2 Derivative via composition ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "First-order AD is based on a simple observation: for a set of functions $\\mathcal{L}$ , the set of tuples of functions $f$ and its Jacobian $J_{f}$ is closed under composition: ", "page_idx": 14}, {"type": "equation", "text": "$$\n(f,J_{f})\\circ(g,J_{g})=(f\\circ g,J_{f\\circ g}),\\quad J_{f o g}(t)=J_{f}(g(t))J_{g}(t)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\circ$ denotes both function composition and the composition of the tuple $(f,J_{f})$ . If we have the analytical formula of the Jacobian $J_{f}$ for every $f\\in\\mathcal{L}$ , then we can calculate the Jacobian of any composition of functions from $\\mathcal{L}$ using the above composition rule for the tuple $(f,J_{f})$ . The set $\\mathcal{L}$ of functions are usually called the primitives. ", "page_idx": 14}, {"type": "text", "text": "B.3 Fr\u00e9chet derivative and linearization ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Given normed vector spaces $V,W$ , the Fr\u00e9chet derivative $\\partial f$ of a function $f:V\\rightarrow W$ is a map from $V$ to the space of all bounded linear operators from $V$ to $W$ , denoted as $\\mathrm{L}(V,W)$ , that is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\partial f:V\\to\\operatorname{L}(V,W),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "such that at a point $\\textbf{a}\\in\\ V$ it gives the best linear approximation $\\partial f(\\mathbf{a})(\\cdot)$ of $f$ , in the sense that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\|\\mathbf{h}\\|\\rightarrow0}{\\frac{\\|f(\\mathbf{a}+\\mathbf{h})-f(\\mathbf{a})-\\partial f(\\mathbf{a})(\\mathbf{h})\\|_{W}}{\\|h\\|_{V}}}=0\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, it is also called the linearization of $f$ at point a. Concretely, consider a function in Euclidean spaces $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{m}$ . At any point $\\mathbf{a}\\in\\mathbb{R}^{n}$ , the Fr\u00e9chet derivative $\\partial f$ can be seen as the directional derivative of $f$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\partial f:\\mathbb{R}^{n}\\to\\mathbf{L}(\\mathbb{R}^{n},\\mathbb{R}^{m}),\\quad\\partial f(\\mathbf{a})(\\mathbf{v})=J_{f}(\\mathbf{a})\\mathbf{v}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $J_{f}(\\mathbf{a})\\in\\mathbb{R}^{m\\times n}$ denote the Jacobian of $f$ at point a called the primal, and $\\mathbf{v}\\in\\mathbb{R}^{n}$ , also called the tangent is a vector representing the direction. Therefore the Fr\u00e9chet derivative is also called Jacobian-vector-product $(J V P)$ . And we can write the truncated Taylor expansion as ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(\\mathbf{a}+\\Delta\\mathbf{x})=f(\\mathbf{a})+\\partial f(\\mathbf{a},\\Delta\\mathbf{x})+\\mathcal{O}\\big(\\Delta\\mathbf{x}^{2}\\big).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Many operators have efficient JVP implementation due to sparsity. For example, element-wise application of scalar function (e.g. activation in neural networks) has diagonal Jacobian, and its JVP can be efficiently implemented as a Hadamard product. Another prominent example is discrete convolution, whose JVP has efficient implementation via FFT. ", "page_idx": 14}, {"type": "text", "text": "B.4 Adjoint of the Fr\u00e9chet derivative ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Given two topological vector spaces $X,Y$ , the linear map $u:X\\rightarrow Y$ has an adjoint ${}^{t}u:Y^{\\prime}\\to X^{\\prime}$ where $X^{\\prime},Y^{\\prime}$ are the dual spaces. The adjoint satisfies the following ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\forall y\\in Y^{\\prime},x\\in X,\\quad\\left\\langle{}^{t}u(y),x\\right\\rangle=\\left\\langle y,u(x)\\right\\rangle\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In the finite-dimensional case, the dual space is the space of row vectors, and any linear map can be written as $u(\\mathbf{x})=A\\mathbf{x}$ . One can easily verify that the adjoint is the transpose: ${}^{t}u(\\mathbf{y}^{\\top})=\\mathbf{\\dot{y}}^{\\top}A$ . The adjoint (transpose) of the Fr\u00e9chet derivative of $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{m}$ , denoted as $\\partial^{\\top}f$ , is thus defined as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\partial^{\\top}f:\\mathbb{R}^{n}\\to\\mathrm{L}(\\mathbb{R}^{m},\\mathbb{R}^{n}),\\quad\\partial^{\\top}f(\\mathbf{a})(\\mathbf{v})=\\mathbf{v}^{\\top}J_{f}(\\mathbf{a}),\\quad\\mathbf{v}\\in\\mathbb{R}^{m}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\mathbf{v}^{\\top}$ is the cotangent which lives in the dual space of the codomain. Note that the adjoint is taken to $\\mathbf{v}$ only where $\\mathbf{a}$ is kept fixed. $\\partial^{\\top}f$ is also called vector-Jacobian-product $(V J P)$ . ", "page_idx": 15}, {"type": "text", "text": "C Why mixed mode AD schemes like the forward-over-backward might not be better than stacked backward mode AD in the case of PINN ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In AD literature [9], the second order derivative is recommended to be computed via forward-overbackward AD, i.e., first do a backward mode AD to get the first order derivative, then apply forward mode AD to the first order derivative to obtain the second order derivative. Usually, we will expect that forward-over-backward AD gives better performance in memory usage over stacked backward AD since the outer differential operator has to differentiate a larger computation graph than the inner one, and forward AD has less overhead as explained in section B.2. Essentially, forward-overbackward reverses the arrows in the third row in Fig. 1, therefore reducing the number of sequential computations and also the size of the evaluation trace. However, in the case of PINN, yet another differentiation to the network parameters $\\theta$ needs to be taken. So, computing the second-order differential operator here with forward-over-backward AD might not yield any advantage. ", "page_idx": 15}, {"type": "text", "text": "D Taylor mode AD ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "D.1 High-order Fr\u00e9chet Derivatives ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The $k$ th order Fr\u00e9chet derivative of a function $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{m}$ at a point a is the multi-linear map with $k$ arguments around point a that best approximates $f$ . For example, when $k=2$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\partial^{2}f:\\mathbb{R}^{n}\\rightarrow\\mathbf{L}(\\mathbb{R}^{n}\\times\\mathbb{R}^{n},\\mathbb{R}^{m}),\\quad\\partial^{2}f(\\mathbf{a})(\\mathbf{v},\\mathbf{v}^{\\prime})=\\mathbf{v}^{\\top}H_{f}(\\mathbf{a})\\mathbf{v}^{\\prime}=\\sum_{j,k}H_{f}(\\mathbf{a})_{i,j,k}v_{j}v_{k}^{\\prime}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $H_{f}(\\mathbf{a})\\in\\mathbb{R}^{m\\times n\\times n}$ denote the Hessian of $f$ at point a, and $\\mathbf{v},\\mathbf{v}^{\\prime}\\in\\mathbb{R}^{n}$ . We can now write the second-order truncated Taylor series with it ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(\\mathbf{a}+\\Delta\\mathbf{x})=f(\\mathbf{a})+\\partial f(\\mathbf{a})(\\Delta\\mathbf{x})+\\partial^{2}f(\\mathbf{a})(\\Delta\\mathbf{x},\\Delta\\mathbf{x})+\\mathcal{O}\\big(\\Delta\\mathbf{x}^{3}\\big).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For the more general case, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\partial^{k}f:\\mathbb{R}^{n}\\rightarrow\\mathrm{L}\\left(\\stackrel{k}{\\bigotimes}\\mathbb{R}^{n},\\mathbb{R}^{m}\\right),\\quad\\partial^{k}f(\\mathbf{a})(\\mathbf{v}^{(1)},\\dots,\\mathbf{v}^{(k)})=\\sum_{i_{1},\\dots,i_{k}}D_{f}^{k}(\\mathbf{a})_{i_{0},i_{1},\\dots,i_{k}}v_{i_{1}}^{(1)}\\cdot\\cdot\\cdot v_{i_{k}}^{(k)}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "High-order Fr\u00e9chet derivative can be seen as the best $k$ th order polynomial approximation of $f$ by taking all input tangents to be the same ${\\bf v}\\in\\mathbb{R}^{n}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(\\mathbf{a}+\\Delta\\mathbf{x})=f(\\mathbf{a})+\\partial f(\\mathbf{a})(\\mathbf{v})+{\\frac{1}{2}}\\partial^{2}f(\\mathbf{a})(\\mathbf{v},\\mathbf{v})+\\cdots+{\\frac{1}{k!}}\\partial^{k}f(\\mathbf{a})(\\mathbf{v}^{\\otimes k})+\\mathcal{O}\\big(\\Delta\\mathbf{x}^{k+1}\\big).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "D.2 Composition rule for high-order Fr\u00e9chet derivatives ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Next, we derive the higher-order composition rule by repeatedly applying the usual chain rule. ", "page_idx": 15}, {"type": "text", "text": "For composition $f(g(x))$ of scalar functions, we can generalize the chain rule for high-order derivatives by iteratively applying the chain rule to lower-order chain rules: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial}{\\partial x}f(g(x))=f^{(1)}(g(x))\\cdot g^{(1)}(x)}\\\\ {\\displaystyle\\frac{\\partial^{2}}{\\partial x^{2}}f(g(x))=f^{(1)}(g(x))\\cdot g^{(2)}(x)+f^{(2)}(g(x))\\cdot[g^{(1)}(x)]^{2}}\\\\ {\\displaystyle\\frac{\\partial^{3}}{\\partial x^{3}}f(g(x))=f^{(1)}(g(x))\\cdot g^{(3)}(x)+3f^{(2)}(g(x))\\cdot g^{(1)}(x)\\cdot g^{(2)}(x)+f^{(3)}(g(x))\\cdot[g^{(1)}(x)]_{2}^{3}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we give the example of up to the third order. For arbitrary $k$ , the $k$ th order derivative of the composition is given by the Faa di Bruno\u2019s formula (scalar version) ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial^{k}}{\\partial x^{k}}f(g(x))=\\sum_{\\stackrel{(p_{1},\\ldots,p_{k})}{\\sum_{i=1}^{k}i\\cdot p_{i}=k}}\\frac{k!}{\\prod_{i}^{k}p_{i}!(i!)^{p_{i}}}\\cdot\\big(f^{(\\sum_{i=1}^{n}p_{i})}\\circ g\\big)(x)\\cdot\\prod_{j=1}^{k}\\left(\\frac{1}{j!}g^{(j)}(x)\\right)^{p_{j}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the outermost summation is taken over all partitions of the derivative order $k$ . Here a partition of $k$ is defined as a tuple $(p_{1},\\cdot\\cdot\\cdot,p_{k})\\in\\mathbb{N}^{k}$ that satisfies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{k}i\\cdot p_{i}=k.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For vector-valued functions $g:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{m},f:\\mathbb{R}^{m}\\rightarrow\\mathbb{R}^{l}$ , let ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{a}=g(\\mathbf{x})\\in\\mathbb{R}^{m},\\ \\ \\ \\mathbf{v}^{(1)}=\\frac{\\partial g(\\mathbf{x})}{\\partial\\mathbf{x}}\\in\\mathbb{R}^{m\\times n},}\\\\ {\\mathbf{v}^{(2)}=\\frac{\\partial^{2}g(\\mathbf{x})}{\\partial\\mathbf{x}^{2}}\\in\\mathbb{R}^{m\\times n\\times n},\\ \\ \\ \\mathbf{v}^{(3)}=\\frac{\\partial^{3}g(\\mathbf{x})}{\\partial\\mathbf{x}^{3}}\\in\\mathbb{R}^{m\\times n\\times n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "we can derive the following composition rule similarly ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial}{\\partial\\mathbf{x}}f(g(\\mathbf{x}))=D_{f}(\\mathbf{a})_{l,m}v_{m,n}^{(1)}\\in\\mathbb{R}^{l\\times n}}\\\\ &{\\displaystyle\\frac{\\partial^{2}}{\\partial\\mathbf{x}^{2}}f(g(\\mathbf{x}))=D_{f}(\\mathbf{a})_{l,m}v_{m,n,n^{\\prime}}^{(2)}+D_{f}^{2}(\\mathbf{a})_{l,m,m^{\\prime}}v_{m,n}^{(1)}v_{m^{\\prime},n^{\\prime}}^{(1)}\\in\\mathbb{R}^{l\\times n\\times n}}\\\\ &{\\displaystyle\\frac{\\partial^{3}}{\\partial\\mathbf{x}^{3}}f(g(\\mathbf{x}))=\\!D_{f}(\\mathbf{a})_{l,m}v_{m,n,n^{\\prime},n^{\\prime\\prime}}^{(3)}}\\\\ &{\\qquad\\qquad\\qquad+3\\cdot D_{f}^{2}(\\mathbf{a})_{l,m,m^{\\prime}}v_{m,n}^{(1)}v_{m^{\\prime},n^{\\prime\\prime}}^{(2)}}\\\\ &{\\qquad\\qquad\\qquad+D_{f}^{3}(\\mathbf{a})_{l,m,m^{\\prime},m^{\\prime\\prime}}v_{m,n}^{(1)}v_{m^{\\prime},n^{\\prime\\prime}}^{(1)}v_{m^{\\prime\\prime},n^{\\prime\\prime}}^{(1)}\\in\\mathbb{R}^{l\\times n\\times n\\times n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where again we give the example of up to the third order, and repeated indexes are summed as in Einstein notation. The general formula is again given by the multivariate version of the Faa di Bruno\u2019s formula. Note that in the multivariate version of the Faa di Bruno\u2019s formula, it is possible to take a derivative to distinguishable variables, but here we just present the version with indistinguishable input variables. This gives the composition rule for $k$ th order total derivative. ", "page_idx": 16}, {"type": "text", "text": "The composition of the high-order Fr\u00e9chet derivative $\\partial^{k}$ is the case of $n=1$ , as the contraction with the input tangents $\\mathbf{v}^{(i)}\\in\\bar{\\mathbb{R}}^{d}$ is the same as composing with a scalar input function $\\boldsymbol{g}:\\mathbb{R}\\to\\mathbb{R}^{d}$ with $\\mathbf{v}^{(i)}=D_{g}^{i}$ . All derivative tensors of $f(g(x))$ can be represented using a $\\mathbb{R}^{l}$ vector, and similarly all derivative tensor $\\mathbf{v}^{(i)}$ of $g$ can be represented using a $\\mathbb{R}^{m}$ vector. Then, the above chain rule can be simplified to ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial}{\\partial t}f(g(t))=D_{f}(\\mathbf{a})_{l,m}v_{m}^{(1)}\\in\\mathbb{R}^{l}}\\\\ &{\\displaystyle\\frac{\\partial^{2}}{\\partial t^{2}}f(g(t))=D_{f}(\\mathbf{a})_{l,m}v_{m}^{(2)}+D_{f}^{2}(\\mathbf{a})_{l,m,m^{\\prime}}v_{m}^{(1)}v_{m^{\\prime}}^{(1)}\\in\\mathbb{R}^{l}}\\\\ &{\\displaystyle\\frac{\\partial^{3}}{\\partial t^{3}}f(g(t))=\\!D_{f}(\\mathbf{a})_{l,m}v_{m}^{(3)}+3\\cdot D_{f}^{2}(\\mathbf{a})_{l,m,m^{\\prime}}v_{m}^{(1)}v_{m^{\\prime}}^{(2)}+D_{f}^{3}(\\mathbf{a})_{l,m,m^{\\prime},m^{\\prime\\prime}}v_{m}^{(1)}v_{m^{\\prime}}^{(1)}v_{m^{\\prime\\prime}}^{(1)}\\in\\mathbb{R}^{l}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The Faa di Bruno\u2019s formula again gives the general formula for arbitrary derivative order ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial^{k}}{\\partial t^{k}}f(g(t))=\\sum_{\\substack{(p_{1},\\ldots,p_{k})\\in\\mathbb{N}^{k}}}\\frac{k!}{\\prod_{i}^{k}p_{i}!(i!)^{p_{i}}}\\cdot D_{f}^{\\sum_{i=1}^{k}p_{i}}(\\mathbf{a})_{l,m_{1},\\ldots,m_{\\sum_{i=1}^{k}p_{i}}}\\cdot\\prod_{j=1}^{k}\\left(\\frac{1}{j!}v_{m_{j}}^{(j)}\\right)^{p_{j}}\\in\\mathbb{R}^{l}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which is written in the perspective of input primal a and tangents $\\mathbf{v}^{(i)}$ . ", "page_idx": 16}, {"type": "text", "text": "E Removing the mixed partial derivatives term from second order semilinear parabolic PDE ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\frac{1}{2}\\mathrm{tr}\\left(\\sigma({\\bf x},t)\\sigma({\\bf x},t)^{\\top}(\\mathrm{Hess}_{\\bf x}u)({\\bf x},t)\\right)=\\frac{1}{2}\\mathrm{tr}\\left(\\sigma({\\bf x},t)^{\\top}(\\mathrm{Hess}_{\\bf x}u)({\\bf x},t)\\sigma({\\bf x},t)\\right)}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}=\\frac{1}{2}\\sum_{i=0}^{d}\\left[\\sigma({\\bf x},t)^{\\top}(\\mathrm{Hess}_{\\bf x}u)({\\bf x},t)\\sigma({\\bf x},t)\\right]_{i,i}}}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}=\\frac{1}{2}\\sum_{i=0}^{d}\\mathrm{e}_{i}^{\\top}\\sigma({\\bf x},t)^{\\top}(\\mathrm{Hess}_{\\bf x}u)({\\bf x},t)\\sigma({\\bf x},t)\\mathrm{e}_{i}}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}=\\frac{1}{2}\\sum_{i=0}^{d}\\partial^{2}u(({\\bf x},t),\\sigma({\\bf x},t)\\mathrm{e}_{i},{\\bf0}^{\\top})_{[3]}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "F Evaluating arbitrary mixed partial derivatives ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "F.1 A concrete example ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Let\u2019s first consider a concrete case. Suppose the domain is $D$ -dimensional we want to compute the mixed derivative $\\frac{\\partial}{\\partial x_{i}^{2}\\partial x_{j}}$ . The naive approach would be to compute the entire third order derivative tensor $D_{f}^{3}$ , which is a tensor of shape $D\\times D\\times D$ , then extract the element at index $(j,i,i)$ . However note that from Eq. 43, for any $k>3$ , the pushforward of $k$ -jet under $\\mathrm{d}^{k}f$ contains contractions of $D_{f}^{3}$ . Although in the case of $k=3$ , the only contraction of $\\bar{D}_{f}^{3}$ is in the $\\partial^{3}f$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\nD_{f}^{3}(\\mathbf{a})_{l,m,m^{\\prime},m^{\\prime\\prime}}v_{m}^{(1)}v_{m^{\\prime}}^{(1)}v_{m^{\\prime\\prime}}^{(1)}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which can only be used to compute the diagonal or the block diagonal elements, when $k>3$ , we will have a contraction that computes off-diagonal terms, i.e. the mixed partial derivatives. For example, in $\\mathrm{d}^{4}f$ , if all input tangents are set to zero except for $\\mathbf{v}^{(1)}$ and $\\mathbf{v}^{(2)},\\partial^{4}f$ becomes: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{:D_{f}^{2}(\\mathbf{a})_{l,m_{1},m_{2}}v_{m_{1}}^{(2)}v_{m_{2}}^{(2)}+6\\cdot D_{f}^{3}(\\mathbf{a})_{l,m_{1},m_{2},m_{3}}v_{m_{1}}^{(2)}v_{m_{2}}^{(1)}v_{m_{3}}^{(1)}+D_{f}^{4}(\\mathbf{a})_{l,m_{1},m_{2},m_{3},m_{4}}v_{m_{1}}^{(1)}v_{m_{2}}^{(1)}v_{m_{3}}^{(1)}v_{m_{4}}^{(1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which contains the contraction of $D_{f}^{3}$ that we want: ", "page_idx": 17}, {"type": "equation", "text": "$$\nD_{f}^{3}(\\mathbf{a})_{l,m_{1},m_{2},m_{3}}v_{m_{1}}^{(2)}v_{m_{2}}^{(1)}v_{m_{3}}^{(1)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "However, there are extra terms. We can remove them by doing two extract pushforwards. We can compute the desired mixed partial derivative with the following pushforward of standard basis: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial x_{i}^{2}\\partial x_{j}}u_{\\theta}({\\bf x})=[\\partial^{4}u_{\\theta}({\\bf x})({\\bf e}_{i},{\\bf e}_{j},{\\bf0},{\\bf0})-\\partial^{4}u_{\\theta}({\\bf x})({\\bf e}_{i},{\\bf0},{\\bf0},{\\bf0})-3\\partial^{2}u_{\\theta}({\\bf x})({\\bf e}_{j},{\\bf0})]/6.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "If we go to higher-order jets, we can use more flexible contractions, and we can compute the mixed derivative with fewer terms to correct, hence less pushforwards. For example, the pushforward of the fifth-order tangent is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{10\\cdot D_{f}^{3}(\\mathbf{a})_{l,m_{1},m_{2},m_{3}}v_{m_{1}}^{(3)}v_{m_{2}}^{(1)}v_{m_{3}}^{(1)}+D_{f}^{5}(\\mathbf{a})_{l,m_{1},m_{2},m_{3},m_{4},m_{5}}v_{m_{1}}^{(1)}v_{m_{2}}^{(1)}v_{m_{3}}^{(1)}v_{m_{4}}^{(1)}v_{m_{5}}^{(1)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "if all input tangents are set to zero except for $\\mathbf{v}^{(1)}$ and $\\mathbf{v}^{(3)}$ . With this we only need to remove one term: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial x_{i}^{2}\\partial x_{j}}u_{\\theta}(\\mathbf{x})=[\\partial^{5}u_{\\theta}(\\mathbf{x})(\\mathbf{e}_{i},\\mathbf{0},\\mathbf{e}_{j},\\mathbf{0},\\mathbf{0})-\\partial^{5}u_{\\theta}(\\mathbf{x})(\\mathbf{e}_{i},\\mathbf{0},\\mathbf{0},\\mathbf{0},\\mathbf{0})]/10.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly, by going to the seventh-order tangent, we can compute this mixed derivative with only one pushforward. $\\mathrm{d}^{7}f$ contains $\\partial^{7}$ , and when all input tangents are set to zero except for $\\mathbf{v}^{(2)}$ and $\\mathbf{v}^{(3)}$ , $\\bar{\\partial}^{7}$ equals ", "page_idx": 17}, {"type": "equation", "text": "$$\n105\\cdot D_{f}^{3}(\\mathbf{a})_{l,m_{1},m_{2},m_{3}}v_{m_{1}}^{(3)}v_{m_{2}}^{(2)}v_{m_{3}}^{(2)}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is the exact contraction we want. With this we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial x_{i}^{2}\\partial x_{j}}u_{\\theta}(\\mathbf{x})=\\partial^{7}u_{\\theta}(\\mathbf{x})(\\mathbf{0},\\mathbf{e}_{i},\\mathbf{e}_{j},\\mathbf{0},\\mathbf{0},\\mathbf{0},\\mathbf{0})/105.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "F.2 Procedure for finding the right pushforwards for arbitrary mixed partial derivatives ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "More generally, consider the case where we need to compute arbitrary mixed partial derivative ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\partial^{\\sum_{j}^{T}q_{i_{j}}}}{\\partial{x}_{i_{1}}^{q_{i_{1}}}\\cdot\\cdot\\cdot\\partial{x}_{i_{T}}^{q_{i_{T}}}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $T$ is the number of different input dimensions in the mixed partial derivative, and $q_{i_{t}}$ is the order. To compute it with $k$ -jet pushforward, one needs to find: ", "page_idx": 18}, {"type": "text", "text": "1. a derivative order $k\\in\\mathbb N$ , ", "page_idx": 18}, {"type": "text", "text": "2. a sparsity pattern for the tangents $\\mathbf{v}^{(i)}$ of the input jet, which is defined as the tuple of $T$ integers $\\boldsymbol J=(j_{1},\\dots,j_{T})$ where $\\mathbf{v}^{(j)}=\\mathbf{0}$ when $j\\not\\in J$ and $j_{t}<k$ for all $t\\in[1,T]$ , ", "page_idx": 18}, {"type": "text", "text": "such that when setting ", "page_idx": 18}, {"type": "equation", "text": "$$\np_{j}=\\left\\{\\begin{array}{c c}{0,}&{j\\notin J}\\\\ {q_{i_{t}},}&{j=j_{t}}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "$(p_{1},p_{2},\\dotsc,p_{k})\\in\\mathbb{N}^{k}$ is a partition of $k$ as defined in Eq. 39. ", "page_idx": 18}, {"type": "text", "text": "Let\u2019s use the concrete example \u2202x2\u2202\u2202xj again. In this case $T\\;=\\;2$ , $q_{i_{1}}~=~2$ and $q_{i_{2}}~=~1$ . We demonstrated that this can be computed with one 7-jet pushforward, which is equivalent to setting $J=(2,3)$ , $k=2j_{1}+j_{2}=7$ , and the partition $(0,2,1,0,0,0,0)$ . The Faa di Bruno\u2019s formula (Eq. 43) ensures that the pushforward of the $k$ th order tangent contains a contraction that can be used to compute the desired mixed partial derivative. ", "page_idx": 18}, {"type": "text", "text": "Furthermore, if there are no other partitions with a sparsity pattern that is the subset of the sparsity pattern of the partition in consideration, there are no extra terms to remove. Intuitively, if a partition has a sparsity pattern that is not a subset, it will vanish when we set the input tangents to zero according to the sparsity pattern of the partition in consideration. To understand this point better, let\u2019s look at the concrete example with the 5-jet pushforward demonstrated above. $(2,0,1,0,0)$ and $(5,0,0,0,0)$ are both valid partition of $k=5$ , and the sparsity pattern of $(5,0,0,0,0)$ is the subset of that of $(2,0,1,0,0);\\,p_{1}$ are non-zero in both partition. Therefore the pushforward contains extra terms that can be removed with another pushforward. In the example with 7-jet pushforward, no other partition has the sparsity pattern that is the subset of that of the partition $(0,2,1,0,0,0,0)$ . This is equivalent to say, $2+2+3$ is the only way to sum up to 7 when you can only use 2 and 3, which can be verified easily. ", "page_idx": 18}, {"type": "text", "text": "With this setup, it is clear why the diagonal terms can always be computed with pushforward of the lowest possible order: $(k,0,\\dot{\\cdot}\\dot{\\cdot}\\dot{\\cdot},0)\\bar{\\in}\\mathbb{N}^{k}$ is always a valid partition $k$ , and no other partition has sparsity pattern that is a subset of it. ", "page_idx": 18}, {"type": "text", "text": "Fwohri cmhi xceadn  pbaer tiinatle rdperrietveatdi vaess ,t hthe e ddeigfrfeiceu lotfy  tshcea l\u201ceos ftfh-dei taogtoaln aolrndeesrs \u201do fo tfh teh oe poepraetroart $\\textstyle\\sum_{t=1}^{T}q_{i_{t}}$ ,x aanmdp $T$ consider the case where $T=3$ and $q_{i_{1}}\\,=\\,3,q_{i_{1}}\\,=\\,2,q_{i_{1}}\\,=\\,1$ . This corresponds to the operator $\\frac{\\partial}{\\partial x_{i}^{3}\\partial x_{j}^{2}\\partial x_{k}}$ . To avoid overlapping with the diagonal sparsity pattern $(k,0,\\ldots,0)$ and to keep the order of derivative low, one might try $k=16$ and the partition $(0,3,2,1,0,\\dots)\\in\\mathbb{N}^{16}$ . However, with higher $k$ , there is more chance that other partitions will have a subset sparsity pattern. In this case $(0,8,0,0,0,\\dots)\\in\\mathbb{N}^{16}$ is one such example. One will need to either find all the partitions with subset sparsity pattern and remove them with multiple pushforward, or further increase the derivative order to find a pattern with no extra term. ", "page_idx": 18}, {"type": "text", "text": "G Further memory reduction via weight sharing in the first layer ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "When dealing with high-dimensional data, the parameters of the model\u2019s first layer in a conventional fully connected network would grow proportionally with the input dimension, resulting in a significant increase in memory requirements and forming a memory bottleneck due to massive model parameters. To address this issue, convolutional networks are often employed in deep learning for images to reduce the number of model parameters. Here, we adopt a similar approach to mitigate the memory cost of model parameters in high-dimensional PDEs, called weight sharing in the first layer. ", "page_idx": 18}, {"type": "text", "text": "Denote the input dimension as $d$ , which is potentially excessively high, and the hidden dimension of the MLP as $h$ , and assume that $d\\gg h$ . The first layer weight is an $d\\times h$ dimensional matrix, whereas all subsequent layers have a weight matrix with a size of only $h\\times h$ . ", "page_idx": 19}, {"type": "text", "text": "By introducing a weight-sharing scheme, one can reduce the redundancy in the parameters in the first layer. Specifically, we perform an additional 1D convolution to the input vectors $\\mathbf{x}_{i}$ before passing the input into the MLP PINN, as in Fig. 4. The 1D convolution has filter size $B$ that divides $D$ and stride size $B$ , so the convolution output is non-overlapping, and the number of channels is set to 1. ", "page_idx": 19}, {"type": "text", "text": "This weight-sharing scheme reduces the parameters by approximately $\\textstyle{\\frac{1}{B}}$ . The number of parameters in the filters is $B\\times1$ , and the subsequent fully connected layer will have a weight matrix of size $\\scriptstyle{\\frac{d}{B}}\\ V\\,$ . Therefore, the total number of the first layer is reduced from $d\\times h$ to only $\\begin{array}{r}{\\frac{d}{B}\\times h+B}\\end{array}$ , and we can see that with a larger block size $B$ , we will have fewer parameters, and the reduction factor is approximately $\\textstyle{\\frac{1}{B}}$ . More concretely, suppose $d=10^{6},h=100$ where one million $(10^{6})$ dimensional problems are also tested experimentally, so the number of parameters in the first layer is $d\\times h=100{\\bar{\\times}}\\,10^{6}$ . If we use a block size of $B=100$ , we will reduce the number of parameters to $\\begin{array}{r}{\\frac{d}{B}\\times h+B\\,=\\,10^{6}\\,+100}\\end{array}$ . If the block size is $B\\,=\\,10$ , the number of parameters will be $\\begin{array}{r}{\\frac{d}{B}\\times h+B=10\\times10^{6}+10.}\\end{array}$ .e rIsn. other words, with a larger block size of $B$ , we significantly reduce ", "page_idx": 19}, {"type": "text", "text": "We will demonstrate the memory efficiency and acceleration thanks to weight-sharing in the experimental section. ", "page_idx": 19}, {"type": "table", "img_path": "J2wI2rCG2u/tmp/92c9d07ca5c3ba5f3213f0169c70e9abd944b27019fd21eeee529841a51c17fd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "J2wI2rCG2u/tmp/1c2c44169068fa9031e9dfe7a847571b1368ec04cc4a84ad388b7b8f720934fc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 4: Convolutional weight sharing in the first layer, with input dimension 9 and filter size 3. ", "page_idx": 19}, {"type": "text", "text": "H Experiment setup ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Each experiment is run with five different random seeds, and the average and the standard deviations of these runs are reported. ", "page_idx": 19}, {"type": "text", "text": "To get an accurate reading of memory usage, we use a separate run where GPU memory pre-allocation for JAX is disabled through setting the environment variable XLA_PYTHON_CLIENT_ALLOCATOR $\\left$ platform, and the test data set is stored on the CPU memory. The GPU memory usage was obtained via NVIDIA-smi and peak memory was reported. ", "page_idx": 19}, {"type": "text", "text": "All the experiments were done on a single NVIDIA A100 GPU with 40GB memory and CUDA 12.2.   \nwith driver 535.129.03 and JAX version 0.4.23. ", "page_idx": 19}, {"type": "text", "text": "Network architecture and training hyperparameters For the semilinear parabolic PDEs tested in Appendix I.2 we follow the network architecture of the original SDGD [13]: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The network is a 4-layer multi-layer perceptron (MLP) with 128 hidden units activated by Tanh. \u2022 The network is trained with Adam [20] for 10K steps, with an initial learning rate of 1e-3 that linearly decays to 0 in 10K steps, where at each step we calculate the model parameters gradient with 100 uniformly sampled random residual points. \u2022 The model is evaluated using 20K uniformly sampled random points fixed throughout the training. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The zero boundary condition is satisfied via the following parameterization ", "page_idx": 20}, {"type": "equation", "text": "$$\nu_{\\theta}(\\mathbf{x})=(1-\\|\\mathbf{x}\\|_{2}^{2})u_{\\theta}^{\\mathrm{MLP}}(\\mathbf{x})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where u\u03b8M $u_{\\theta}^{\\mathrm{MLP}}$ is the MLP network, and $u_{\\theta}$ is the PDE ansatz, as described in [25]. ", "page_idx": 20}, {"type": "text", "text": "For the semilinear parabolic PDEs tested in Appendix I.2, we made the following modifications: ", "page_idx": 20}, {"type": "text", "text": "\u2022 Instead of using re-parameterization, the boundary/initial condition is satisfied by adding a regularization loss to the residual loss: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{boundary}}(\\theta;\\{\\mathbf x_{b,i}\\}_{i=1}^{N_{b}})=\\frac{1}{N_{b}}\\sum_{i=1}^{N_{b}}|u_{\\theta}(\\mathbf x_{b,i},0)-g(\\mathbf x_{b,i})|^{2}+C_{g}\\cdot\\frac{1}{N_{b}}\\sum_{i=1}^{N_{b}}|\\nabla u_{\\theta}(\\mathbf x_{b,i},0)-\\nabla g(\\mathbf x_{b,i})|^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $g(\\cdot)$ is the initial data, $N_{b}$ is the batch size for boundary points, $u_{\\theta}$ is the PDE ansatz, $C_{g}$ is the coefficient for the first-order derivative boundary loss term, which we set to 0.05. The total loss is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{residual}}(\\theta;\\{\\mathbf{x}_{r,i}\\}_{i=1}^{N_{r}})+20\\ell_{\\mathrm{boundary}}(\\theta;\\{\\mathbf{x}_{b,i}\\}_{i=1}^{N_{b}}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "\u2022 Instead of discretizing the time and sample residual points using the underlying stochastic process, we uniformly sample the time steps between the initial and the terminal time, i.e. $t\\sim$ uniform $[0,T]$ , and then sample x directly from the distribution of $\\mathbf{X}_{t}$ , i.e. $\\mathbf{x}\\sim\\mathcal{N}(0,(T\\!-\\!t)\\!\\cdot\\!\\mathbf{I}_{d\\times d})$ . To match the original training setting of 100 SDE trajectories with 0.015 step size for time discretization, we use a batch size of 2000 for residual points and 100 for boundary/initial points. \u2022 We use a 4-layer multi-layer perceptron (MLP) with 1024 hidden units activated by Tanh. The network is trained with Adam [20] for 10K steps, with an initial learning rate of 1e-3 that exponentially decays with exponent 0.9995. \u2022 To test the quality of the PINN solution, we measure the relative L1 error at the point $(\\mathbf{x}_{\\mathrm{test}},T)$ against the reference value computed via multilevel Picard\u2019s method [3, 4, 17]. ", "page_idx": 20}, {"type": "text", "text": "In all experiments, we use the biased version of Eq. 25: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{\\ell}_{\\mathrm{residual}}(\\theta;\\{\\mathbf{x}^{(i)}\\}_{i=1}^{N_{r}},J)=\\frac{1}{N_{r}}\\sum_{i=1}^{N_{r}}\\left[\\tilde{\\mathcal{L}}_{J}u_{\\theta}(\\mathbf{x}^{(i)})-f(\\mathbf{x}^{(i)})\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "as the bias in practice is very small and does not affect convergence. ", "page_idx": 20}, {"type": "text", "text": "I Experiments Results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "I.1 Inseparable and effectively high-dimensional PDEs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The first class of PDEs is defined via a nonlinear, inseparable, and effectively high-dimensional exact solution $u_{\\mathrm{{exact}}}(\\mathbf{x})$ defined within the $d$ -dimensional unit ball $\\mathbb{B}^{d}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}u(\\mathbf{x})=f(\\mathbf{x}),~~~\\mathbf{x}\\in\\mathbb{B}^{d}}\\\\ {u(\\mathbf{x})=0,~~~~~~\\mathbf{x}\\in\\partial\\mathbb{B}^{d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\mathcal{L}$ is a linear/nonlinear operator and $g(\\mathbf{x})=\\mathcal{L}u_{\\mathrm{exact}}(\\mathbf{x})$ . The zero boundary condition ensures that no information about the exact solution is leaked through the boundary condition. We will consider the following operators: ", "page_idx": 20}, {"type": "text", "text": "\u2022 Poisson equation: $\\mathcal{L}u({\\mathbf{x}})=\\nabla^{2}u({\\mathbf{x}})$ . \u2022 Allen-Cahn equation: $\\begin{array}{r}{\\mathcal{L}u(\\mathbf{x})=\\nabla^{2}u(\\mathbf{x})+u(\\mathbf{x})-u(\\mathbf{x})^{3}.}\\end{array}$ \u2022 Sine-Gordon equation: $\\mathcal L u(\\mathbf x)=\\nabla^{2}u(\\mathbf x)+\\sin(u(\\mathbf x))$ . ", "page_idx": 20}, {"type": "text", "text": "For the exact solution, we consider the following with all $c_{i}\\sim\\mathcal{N}(0,1)$ : ", "page_idx": 20}, {"type": "text", "text": "\u2022 two-body interaction: $\\begin{array}{r l}&{u_{\\mathrm{exact}}(\\mathbf{x})=(1-\\|\\mathbf{x}\\|_{2}^{2})\\left(\\sum_{i=1}^{d-1}c_{i}\\sin(x_{i}+\\cos(x_{i+1})+x_{i+1}\\cos(x_{i}))\\right).}\\\\ &{:u_{\\mathrm{exact}}(\\mathbf{x})=(1-\\|\\mathbf{x}\\|_{2}^{2})\\left(\\sum_{i=1}^{d-2}c_{i}\\exp(x_{i}x_{i+1}x_{i+2})\\right).}\\end{array}$ \u2022 three-body interaction ", "page_idx": 20}, {"type": "text", "text": "We tested the performance of STDE on these equations, and the results are presented in Table 3, 4, 5, 6. For the Allen-Cahn equation, we performed a detailed ablation study (Table 3), and we expect these results to generalize over these second-order PDEs. ", "page_idx": 21}, {"type": "text", "text": "I.1.1 Further details on ablation study ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The gain by using JAX instead of PyTorch Since the original SDGD was implemented in PyTorch, we implemented the stacked backward mode without parallelization in SDGD dimensions in JAX for fair comparison (dubbed as \u201cStacked Backward mode SDGD in JAX\u201d in Table 3). The for-loop over SDGD dimension is implemented using jax.lax.scan. Table 3 shows that, even with the original stacked backward mode AD, the speed of JAX implementation can be more than $10\\times$ faster when the dimension is high. The memory profile is similar. The difference could come from the fact that JAX uses XLA to perform Just-in-time (JIT) compilation of the Python code into optimized kernels. However, note that for the case of 100,000 dimensions, the JAX implementation of the stacked backward mode AD encountered an out-of-memory (OOM) error. This is because performing JIT compilation requires extra memory, and the peak memory requirement during JIT compilation is higher than that during training. ", "page_idx": 21}, {"type": "text", "text": "Randomization batch size We also tested the case where the STDE randomization batch size is reduced to 16. As seen in Table 3, in the case of Allen-Cahn provides ${\\sim}2{\\times}$ speed up, without hurting performance. However, theoretically lowering the randomization batch size leads to higher variance. The trade-off between computational efficiency and stability in convergence warrants further studies. ", "page_idx": 21}, {"type": "text", "text": "I.2 Semilinear Parabolic PDEs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The second class of PDEs is the semilinear parabolic PDEs, where the initial condition is specified: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial}{\\partial t}u(\\mathbf{x},t)=\\!\\mathcal{L}u(\\mathbf{x},t)\\quad(\\mathbf{x},t)\\in\\mathbb{R}^{d}\\times[0,T]}\\\\ &{\\quad u(\\mathbf{x},t)=\\!g(\\mathbf{x}),\\quad(\\mathbf{x},t)\\in\\mathbb{R}^{d}\\times\\{0\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $g\\mathbf{(x)}$ is a known, analytical, and time-independent function that specifies the initial condition, and $T$ is the terminal time. We aim to approximate the solution\u2019s true value at one test point $\\mathbf{x}_{\\mathrm{test}}\\in\\mathbb{R}^{d}$ , at the terminal time $t=T$ , i.e. at $(\\mathbf{x}_{\\mathrm{test}},T)$ . ", "page_idx": 21}, {"type": "text", "text": "We will consider the following operators ", "page_idx": 21}, {"type": "text", "text": "\u2022 Semilinear Heat Eq. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{L}u(\\mathbf{x},t)=\\nabla^{2}u(\\mathbf{x},t)+\\frac{1-u(\\mathbf{x},t)^{2}}{1+u(\\mathbf{x},t)^{2}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "with initial condition $g(\\mathbf{x})=5/(10+2\\|\\mathbf{x}\\|^{2})$ , ", "page_idx": 21}, {"type": "text", "text": "\u2022 Allen-Cahn equation ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}u(\\mathbf{x},t)=\\nabla^{2}u(\\mathbf{x},t)+u(\\mathbf{x},t)-u(\\mathbf{x},t)^{3}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "with initial condition $g(\\mathbf{x})=\\arctan(\\operatorname*{max}_{i}x_{i})$ , ", "page_idx": 21}, {"type": "text", "text": "\u2022 Sine-Gordon equation ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}u(\\mathbf{x},t)=\\nabla^{2}u(\\mathbf{x},t)+\\sin(u(\\mathbf{x},t)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "with initial condition $g(\\mathbf{x})=5/(10+2\\|\\mathbf{x}\\|^{2})$ , ", "page_idx": 21}, {"type": "text", "text": "All three equation uses the test point $\\mathbf{x}_{\\mathrm{test}}=\\mathbf{0}$ and terminal time $T=0.3$ . ", "page_idx": 21}, {"type": "text", "text": "I.3 Weight sharing ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We tested the weight-sharing technique mentioned in Section G. ", "page_idx": 21}, {"type": "text", "text": "In this section, we evaluate the performance of the weight-sharing scheme described in Appendix G. We tested the best-performing method from Table 3 (STDE with small randomization batch size of 16) with different weight-sharing block sizes, on the inseparable Allen-Cahn equation with the two-body exact solution. ", "page_idx": 21}, {"type": "table", "img_path": "J2wI2rCG2u/tmp/18f0020fd7025386cde7e9365b8e5f2e5f6d126ec8f52743764fe398a8159f41.jpg", "table_caption": ["Table 3: Computational results for the Inseparable Allen-Cahn equation with the two-body exact solution, where the randomization batch size is set to 100 unless stated otherwise. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "J2wI2rCG2u/tmp/e3e24bc54e4fc61b6828a17325f91e36af7f14f7e4084c583af1efdf34426198.jpg", "table_caption": ["Table 4: Computational results for the Inseparable Poisson equation with two-body exact solution. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "J2wI2rCG2u/tmp/2c9ca61e8392b0212f7cc54f5bd0a80e84c9a98d5d7aecac3931d5dabcd8c4ed.jpg", "table_caption": ["Table 5: Computational results for the Inseparable Sine-Gordon equation with two-body exact solution. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "J2wI2rCG2u/tmp/cbab14ed39f4e1570d5521a658049ce878a0df88a1e4349df505ef429fdc460a.jpg", "table_caption": ["Table 6: Computational results for the Inseparable Allen-Cahn, Poisson, and Sine-Gordon equation with the three-body exact solution, computed via STDE with randomization batch size $|J|$ set to 16. \\*STDE with randomization batch size $(|J|)$ of 16 performs poorly on the 1M dimensional Inseparable Poisson equation with three-body exact solution: the L2 relative error is only $9.05\\mathrm{E}{-}02\\pm6.88\\mathrm{E}{-}04.$ To get better convergence, we increase the randomization batch size to 50 for the 1M case. This incurs no extra memory cost and is only slightly slower than the original setting (speed is 46.80it/s when randomization batch size is 16). "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "J2wI2rCG2u/tmp/bd047742552ccf7d31e2f9b5612790f4ea043caa28cf9a9b1c98e4ad7a9f7e6c.jpg", "table_caption": ["Table 7: Computational results for the Time-dependent Semilinear Heat equation, where the number of SDGD sampled dimensions is set to 10. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "J2wI2rCG2u/tmp/5f23be4c907f565d992a3b260257113a1bbeb12e71f78f039027d1c9a7ae73be.jpg", "table_caption": ["Table 8: Computational results for the Time-dependent Allen-Cahn equation, where the number of SDGD sampled dimensions is set to 10. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "J2wI2rCG2u/tmp/5e002ab07b3aa9de88c89f7afdedbaaf5e23af6a84b6e091db4013482f4f9d95.jpg", "table_caption": ["Table 9: Computational results for the Time-dependent Sine-Gordon equation, where the number of SDGD sampled dimensions is set to 10. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "From Table 10, we can see that weight sharing drastically reduces the number of network parameters and memory usage. With $B=50$ , there is a $2.5\\mathrm{x}$ reduction in memory and there is no performance loss in terms of L2 relative error. ", "page_idx": 24}, {"type": "text", "text": "However, from the experiments we can see that, in both the 1M and the 5M case, increasing the block size beyond 50 provides diminishing returns. For the 1M case, increasing $B$ to 1000 affects the convergence quality, as the L2 relative error goes up by $100\\mathbf{x}$ . For 5M, the maximum block size one can use before degrading performance is 500, which is expected as the dimensionality of the problem is higher. ", "page_idx": 24}, {"type": "text", "text": "From Table 10 we can also see that in the 5M-dimensional case, we will have an out-of-memory (OOM) error without weight sharing. With weight sharing enabled, we can effectively solve the 5M-dimensional PDE with good relative L2 error, in around 30 minutes. ", "page_idx": 24}, {"type": "text", "text": "I.4 High-order PDEs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Here we demonstrate how to use STDE to calculate mixed partial derivatives in some actual PDE. We will consider the 2D Korteweg-de Vries (KdV) equation and the 2D Kadomtsev-Petviashvili equation from [31], and the regular 1D KdV equation with gPINN [41]. ", "page_idx": 24}, {"type": "table", "img_path": "J2wI2rCG2u/tmp/33cc86c8b744a08e644ea3960318f53e3bb9fe0597ab7276d324ffa737ad8ce6.jpg", "table_caption": ["Table 10: Effects of different weight sharing block sizes $B$ for the Inseparable Allen-Cahn equation with two-body exact solution solved with STDE with randomization batch size of 16. $B=1$ equals no weight sharing. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "We will demonstrate that STDE increases the speed for computing the mixed partial derivatives, as it avoids computing the entire derivative tensor. Since these equations are low-dimensional we do not need to sample over the space dimension. ", "page_idx": 25}, {"type": "text", "text": "In this section, the equations are all time-dependent and the space is 2D, and we will omit the argument to the solution, i.e. we will write $u(\\mathbf{x},t)=u$ . To test the speed improvement, we run the STDE implementation against repeated backward mode AD on a Nvidia A100 GPU with 40GB memory. The results are reported in Table 11. From the Table we see that STDE provides around ${\\sim}2{\\times}$ speed up compared to repeated application of backward mode AD across different network sizes. ", "page_idx": 25}, {"type": "text", "text": "I.4.1 High-order low-dimensional PDEs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Alternative way to compute the terms in 2D Korteweg-de Vries (KdV) equation The terms in the 2D KdV equation ", "page_idx": 25}, {"type": "equation", "text": "$$\nu_{t y}+u_{x x x y}+3(u_{y}u_{x})_{x}-u_{x x}+2u_{y y}=0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "can alternatively be computed with the pushforward of the following jets ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{I}^{(1)}=\\mathrm{d}^{9}u(\\mathbf{x},\\mathbf{0},\\mathbf{e}_{x},\\mathbf{e}_{y},\\mathbf{0},\\dots),\\ \\ \\mathfrak{I}^{(2)}=\\mathrm{d}^{3}u(\\mathbf{x},\\mathbf{0},\\mathbf{e}_{y},\\mathbf{e}_{t}),\\ \\ \\mathfrak{I}^{(3)}=\\mathrm{d}^{3}u(\\mathbf{x},\\mathbf{0},\\mathbf{e}_{y},\\mathbf{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "All the derivative terms can be found in these output jets $\\{\\mathfrak{J}^{(i)}\\}$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{u_{x}=\\mathfrak{J}_{[2]}^{(1)},\\ u_{y}=\\mathfrak{J}_{[3]}^{(1)},\\ u_{x x}=\\mathfrak{J}_{[4]}^{(1)}/3,\\ u_{x y}=\\mathfrak{J}_{[5]}^{(1)}/10,u_{y y}=\\mathfrak{J}_{[2]}^{(3)},}\\\\ {u_{y y y}=\\mathfrak{J}_{[3]}^{(3)},u_{x x x y}=(\\mathfrak{J}_{[9]}^{(1)}-280u_{y y y})/840,\\ u_{t y}=(\\mathfrak{J}_{[3]}^{(2)}-u_{y y y})/3,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "2D Kadomtsev-Petviashvili (KP) equation Consider the following equation ", "text_level": 1, "page_idx": 25}, {"type": "equation", "text": "$$\n(u_{t}+6u u_{x}+u_{x x x})_{x}+3\\sigma^{2}u_{y y}=0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which can be expanded as ", "page_idx": 25}, {"type": "equation", "text": "$$\nu_{t x}+6u_{x}u_{x}+6u u_{x x}+u_{x x x x}+3\\sigma^{2}u_{y y}=0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "All the derivative terms can be computed with a 5-jet, 4-jet, and a 2-jet pushforward. Let ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mathbf{\\upsigma}}^{(1)}:=\\!\\mathrm{d}^{5}u(\\mathbf{x},\\mathbf{0},\\mathbf{e}_{t},\\mathbf{e}_{x},\\mathbf{0},\\mathbf{0})}\\\\ &{\\tilde{\\mathbf{\\upsigma}}^{(2)}:=\\!\\mathrm{d}^{4}u(\\mathbf{x},\\mathbf{e}_{x},\\mathbf{0},\\mathbf{0},\\mathbf{0})}\\\\ &{\\tilde{\\mathbf{\\upsigma}}^{(3)}:=\\!\\mathrm{d}^{2}u(\\mathbf{x},\\mathbf{e}_{y},\\mathbf{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then all required derivative terms can be evaluated as follows. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{u_{t x}=\\mathfrak{J}_{[5]}^{(1)}/10,}\\\\ {u_{x}=\\mathfrak{J}_{[1]}^{(2)},\\ u_{x x}=\\mathfrak{J}_{[2]}^{(2)},\\ u_{x x x x}=\\mathfrak{J}_{[4]}^{(2)},}\\\\ {u_{y y}=\\mathfrak{J}_{[2]}^{(3)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Gradient-enhanced 1D Korteweg-de Vries $({\\bf g-K d V})$ equation Consider the following equation ", "text_level": 1, "page_idx": 26}, {"type": "equation", "text": "$$\nu_{t}+u u_{x}+\\alpha u_{x x x}=0.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Gradient-enhanced PINN (gPINN) [41] regularizes the learned PINN such that the gradient of the residual is close to the zero vector. This increases the accuracy of the solution. Specifically, the PINN loss (Eq. 24) is augmented with the term ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{gPINN}}\\left(\\{\\mathbf{x}^{(i)}\\}_{i=1}^{N_{r}}\\right)=\\frac{1}{N_{r}}\\sum_{i}\\sum_{j}^{d}\\bigg|\\frac{\\partial}{\\partial x_{j}}R(\\mathbf{x}^{(i)})\\bigg|^{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The total loss becomes ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{residual}}\\,+c_{\\mathrm{gPINN}}\\,\\ell_{\\mathrm{gPINN}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $c_{\\mathrm{gPINN}}$ is the $\\mathrm{g}$ -PINN penalty weight. To perform gradient-enhancement we need to compute the gradient of the residual: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R(x,t):=u_{t}+u u_{x}+\\alpha u_{x x x},}\\\\ {\\nabla R(x,t)=[u_{t t}+u_{t}u_{x}+u u_{t x}+\\alpha u_{t x x x},\\quad u_{t x}+u_{x}u_{x}+u u_{x x}+\\alpha u_{x x x x}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "All the derivative terms can be computed with one 2-jet and two 7-jet pushforward. Let ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mathbf{\\upsigma}}^{(1)}:=\\mathrm{d}^{7}u(\\mathbf{x},\\mathbf{e}_{x},\\mathbf{0},\\mathbf{0},\\mathbf{0},\\mathbf{0},\\mathbf{0},\\mathbf{0},\\mathbf{0})}\\\\ &{\\tilde{\\mathbf{\\upsigma}}^{(2)}:=\\mathrm{d}^{7}u(\\mathbf{x},\\mathbf{e}_{x},\\mathbf{0},\\mathbf{0},\\mathbf{e}_{t},\\mathbf{0},\\mathbf{0},\\mathbf{0})}\\\\ &{\\tilde{\\mathbf{\\upsigma}}^{(3)}:=\\mathrm{d}^{2}u(\\mathbf{x},\\mathbf{e}_{t},\\mathbf{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then all required derivative terms can be evaluated as follows. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{u_{x}=\\mathfrak{J}_{[1]}^{(1)},\\ u_{x x}=\\mathfrak{J}_{[2]}^{(1)},\\ u_{x x x}=\\mathfrak{J}_{[3]}^{(1)},\\ u_{x x x x}=\\mathfrak{J}_{[4]}^{(1)},\\ u_{x x x x x}=\\mathfrak{J}_{[5]}^{(1)},}\\\\ &{}&{u_{t x x x}=(\\mathfrak{J}_{[7]}^{(2)}-\\mathfrak{J}_{[8]}^{(1)})/35,\\ u_{t x}=(\\mathfrak{J}_{[5]}^{(2)}-u_{x x x x x})/5,\\ u_{t}=\\mathfrak{J}_{[4]}^{(2)}-u_{x x x x x},}\\\\ &{}&{u_{t t}=\\mathfrak{J}_{[2]}^{(3)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Table 11: Speed scaling for training low-dimensional high-order PDEs with different network sizes. The base network has depth $L=4$ and width $h=128$ . STDE\\* is the alternative scheme using lower-order pushforwards. ", "page_idx": 26}, {"type": "table", "img_path": "J2wI2rCG2u/tmp/7f3ec2c7a17cac958958120c6df78273a60d1ebdb96d4794eff9c47dd47586d6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "I.4.2 Amortized gradient-enhanced PINN for high-dimensional PDEs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "It is expensive to apply gradient enhancement for high-dimensional PDEs. For example, the gradient of the residual for the inseparable Allen-Cahn equation described in I.1 is given by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\partial}{\\partial x_{j}}R(\\mathbf{x})=\\frac{\\partial}{\\partial x_{j}}\\left[\\sum_{i}\\frac{\\partial^{2}}{\\partial x_{i}^{2}}u(\\mathbf{x})+u(\\mathbf{x})-u^{3}(\\mathbf{x})-f(\\mathbf{x})\\right]}}\\\\ &{}&{\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\sum_{i=1}^{d}\\frac{\\partial^{3}}{\\partial x_{j}\\partial x_{i}^{2}}u(\\mathbf{x})+\\frac{\\partial}{\\partial x_{j}}u(\\mathbf{x})-3u^{2}(\\mathbf{x})\\frac{\\partial}{\\partial x_{j}}u(\\mathbf{x})-\\frac{\\partial}{\\partial x_{j}}f(\\mathbf{x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "With STDE randomization, we randomized the second order term $\\frac{\\partial^{2}}{\\partial{{x}_{i}^{2}}}$ with index $i$ sampled from $[1,d]$ . We can also sample the gPINN penalty terms. As mentioned in Appendix F.1, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Im=\\mathrm{d}^{7}u({\\bf x},{\\bf0},{\\bf e}_{i},{\\bf e}_{j},{\\bf0},{\\bf0},{\\bf0},{\\bf0}),\\quad\\frac{\\partial}{\\partial x_{i}^{2}\\partial x_{j}}u({\\bf x})=\\Im_{[7]}/{105}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We further have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}}{\\partial x_{i}^{2}}u({\\bf x})=\\Im_{[4]}/3,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "so the STDE of the Laplacian operator can be computed together with the above pushforward. With this pushforward, we can efficiently amortize the $\\mathrm{\\gPINN}$ regularization loss by minimizing the following upperbound on the original gPINN loss with randomized Laplacian ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phantom{=}\\displaystyle\\tilde{\\ell}_{\\mathrm{gpINN}}\\left(\\{\\mathbf{x}^{(i)}\\}_{i=1}^{N_{r}}I,I,J\\right)}\\\\ &{=\\displaystyle\\frac{1}{N_{r}}\\sum_{j\\in J}\\sum_{i\\in I}\\left|\\frac{\\partial^{3}}{\\partial x_{j}\\partial x_{i}^{2}}u(\\mathbf{x})+\\frac{\\partial}{\\partial x_{j}}u(\\mathbf{x})-3u^{2}(\\mathbf{x})\\frac{\\partial}{\\partial x_{j}}u(\\mathbf{x})-\\frac{\\partial}{\\partial x_{j}}f(\\mathbf{x})\\right|^{2}}\\\\ &{\\displaystyle\\geq\\frac{1}{N_{r}}\\sum_{j\\in J}\\left|\\sum_{i\\in I}\\frac{\\partial^{3}}{\\partial x_{j}\\partial x_{i}^{2}}u(\\mathbf{x})+\\frac{\\partial}{\\partial x_{j}}u(\\mathbf{x})-3u^{2}(\\mathbf{x})\\frac{\\partial}{\\partial x_{j}}u(\\mathbf{x})-\\frac{\\partial}{\\partial x_{j}}f(\\mathbf{x})\\right|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $J$ is an independently sampled index set for sampling the gPINN terms. The total loss is ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\widetilde{\\ell}_{\\mathrm{residual}}(\\theta;\\{\\mathbf{x}^{(i)}\\}_{i=1}^{N_{r}},I)+\\widetilde{\\ell}_{\\mathrm{gPINN}}\\left(\\{\\mathbf{x}^{(i)}\\}_{i=1}^{N_{r}},I,J\\right)\\!.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We call this technique amortized gPINN. The above formula applies to all PDEs where the derivative operator is the Laplacian. For example, for the Sine-Gordon equation, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\tilde{\\ell}_{\\mathrm{gPINN}}\\left(\\{\\mathbf{x}^{(i)}\\}_{i=1}^{N_{r}},I,J\\right)}\\\\ &{=\\!\\!\\displaystyle\\frac{1}{N_{r}}\\sum_{j\\in J}\\sum_{i\\in I}\\left|\\frac{\\partial^{3}}{\\partial x_{j}\\partial x_{i}^{2}}u(\\mathbf{x})+\\cos u(\\mathbf{x})\\frac{\\partial}{\\partial x_{j}}u(\\mathbf{x})-\\frac{\\partial}{\\partial x_{j}}f(\\mathbf{x})\\right|^{2}\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We use $c_{\\mathrm{gPINN}}=0.1$ , and to get better convergence, we train for $20K$ steps instead of $10K$ steps as in all other experiments in this paper. The results are reported in Table 12. We implement the baseline method based on the best performing first-order AD scheme, the parallelized backward mode SDGD, which we denoted as JVP-HVP in the table. Specifically, to compute the residual gradient we apply one more JVP to the HVP-based implementation of Laplacian (Appendix A.2). From the table, we see that STDE-based amortized gPINN performs better than the JVP-HVP implementation, and both are more efficient than applying backward mode AD in a for-loop. Furthermore, through amortizing we can apply gPINN to high-dimensional PDE which was intractable. ", "page_idx": 27}, {"type": "text", "text": "J Pushing forward dense random jets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section we establish the connection between the classical technique of HTE [16] and STDE by demonstrating that HTE is a pushforward of dense isotropic random 2-jet. ", "page_idx": 27}, {"type": "text", "text": "J.1 Review of HTE ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "HTE provides a random estimation of the trace of a matrix $A\\in\\mathbb{R}^{d\\times d}$ as follows: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname{tr}(A)=\\mathbb{E}_{\\mathbf{v}\\sim p(\\mathbf{v})}\\left[\\mathbf{v}^{\\mathrm{T}}A\\mathbf{v}\\right],\\quad\\mathbf{v}\\in\\mathbb{R}^{d}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $p(\\mathbf{v})$ is isotropic, i.e. $\\mathbb{E}_{\\mathbf{v}\\sim p(\\mathbf{v})}[\\mathbf{v}\\mathbf{v}^{T}]=I$ . Therefore, the trace can be estimated by Monte Carlo: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname{tr}(A)\\approx{\\frac{1}{V}}\\sum_{i=1}^{V}\\mathbf{v}_{i}^{\\mathrm{T}}A\\mathbf{v}_{i},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where each $\\mathbf{v}_{i}\\in\\mathbb{R}^{d}$ are i.i.d. samples from $p(\\mathbf{v})$ . ", "page_idx": 27}, {"type": "text", "text": "There are several viable choices for the distribution $p(\\mathbf{v})$ in HTE, such as the most common standard normal distribution. Among isotropic distributions, the Rademacher distribution minimizes the variance of HTE. The proof for the minimal variance is given in [35]. ", "page_idx": 27}, {"type": "table", "img_path": "J2wI2rCG2u/tmp/fe6d7b2ea1e5a7aaaa539531734d504da742cb7eff7e865457c56bb5d9dcc01f.jpg", "table_caption": ["Table 12: Performance comparison of STDE-gPINN for high-dimensional inseparable PDEs. \u201cNone\u201d in the \u201cgPINN method\u201d column indicates that no gPINN loss was used. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "J.2 HTE as the pushforward of dense isotropic random 2-jets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Note that both HTE and the STDE Hessian trace estimator (Eq. ) are computing the quadratic form of Hessian, a specific contraction that is included in the pushforward of 2-jet. In STDE, the random vectors are the unit vectors whose indexes are sampled from\u221a the index set without replacement. This can be seen as a discrete distribution $p(\\mathbf{v})$ such that $\\mathbf{v}={\\sqrt{d}}\\mathbf{e}_{i}$ for $i=1,2,\\cdots,d$ with probability $1/d$ , which is isotropic. Hence HTE can also be defined as a push forward of random 2-jet that are isotropic. ", "page_idx": 28}, {"type": "text", "text": "We can now write the computation of HTE as follows ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\tilde{\\nabla^{2}}_{p,N}u_{\\theta}=\\frac{d}{N}\\sum_{j=1}^{N}\\partial^{2}u_{\\theta}(\\mathbf{x})(\\mathbf{v}_{j},\\mathbf{0}),\\quad\\mathbf{v}_{j}\\sim p(\\mathbf{v}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\tilde{\\nabla}^{2}{}_{N}$ is the STDE for Laplacian with random jet batch size $N$ . ", "page_idx": 28}, {"type": "text", "text": "J.3 Estimating the Biharmonic operator ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "It was shown in [12] that the Biharmonic operator ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Delta^{2}u(\\mathbf{x})=\\sum_{i=1}^{d}\\sum_{j=1}^{d}\\frac{\\partial^{4}}{\\partial x_{i}^{2}\\partial x_{j}^{2}}u(\\mathbf{x})\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "has the following unbiased estimator: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Delta^{2}u({\\bf x})=\\frac{1}{3}\\mathbb{E}_{{\\bf v}\\sim p({\\bf v})}\\left[\\partial^{4}u({\\bf x})({\\bf v},{\\bf0},{\\bf0},{\\bf0})\\right]\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $p$ is the $d$ -dimensional normal distribution. Therefore its STDE estimator is ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\tilde{\\Delta}^{2}{}_{N}u(\\mathbf{x})=\\frac{d}{3N}\\sum_{j=1}^{N}\\partial^{4}u(\\mathbf{x})(\\mathbf{v}_{j},\\mathbf{0},\\mathbf{0},\\mathbf{0}),\\quad\\mathbf{v}_{j}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "K STDE with dense jets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "K.1 STDE with second order dense jets as generalization of HTE ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Suppose $\\mathcal{D}$ is a second-order differential operator with coefficient tensor $\\mathbf{C}$ . If $\\mathbf{C}$ is not symmetric, we can symmetrize it as $\\mathbf{C}^{\\prime}=\\textstyle{\\frac{1}{2}}(\\mathbf{C}+\\mathbf{C}^{\\top})$ , and $D_{u}^{2}(\\mathbf{a})\\cdot\\mathbf{C}=D_{u}^{2}(\\mathbf{a})\\cdot\\mathbf{C^{\\prime}}$ since $D_{u}^{2}(\\mathbf{a})$ is symmetric. ", "page_idx": 28}, {"type": "text", "text": "Furthermore, we can make $\\mathbf{C}$ positive-definite by adding a constant diagonal $\\lambda\\mathbf{I}$ where $-\\lambda$ is smaller than the smallest eigenvalue of $\\mathbf{C}$ . The matrix $\\mathbf{C}^{\\prime\\prime}=\\frac{1}{2}(\\mathbf{C}+\\mathbf{C}^{\\top})+\\lambda\\mathbf{I}$ then has the eigen decomposition $\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{U}^{\\top}$ where $\\Sigma$ is diagonal and all positive. Now we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbf{\\mathbb{E}_{v\\sim\\mathcal{N}(0,\\Sigma)}}[\\mathbf{Uvv}^{\\top}\\mathbf{U}^{\\top}]=\\mathbf{U\\SigmaU}^{\\top}=\\mathbf{C}^{\\prime\\prime}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "K.2 Why STDE with dense jets is not generalizable ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Specifically, we will prove that it is impossible to construct dense STDE for the fourth-order diagonal operator Lu =  id=1\u2202\u2202xiu4 . ", "page_idx": 29}, {"type": "text", "text": "The mask tensor of $\\mathcal{L}$ is the rank-4 identity tensor $\\mathbf{I}_{4}\\in\\mathbb{R}^{d\\times d\\times d\\times d}$ , so the condition for unbiasedness is ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{v}\\sim p}[v_{i}^{(a)}v_{j}^{(b)}v_{k}^{(c)}v_{l}^{(d)}]=M_{i j k l}=\\delta_{i j k l},\\quad a,b,c,d\\in\\{1,2,3,4\\}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\delta_{i j k l}=1$ when $i=j=k=l$ , and is 0 otherwise. ", "page_idx": 29}, {"type": "text", "text": "IvDnee ctnthooert se.m  tIohns ett  hfgiose uncreatrshae-l  mwcoea smceae nnw tdh teeefrnein $a\\neq b\\neq c\\neq d$ ,a wtnehc eec naa sn $\\begin{array}{r}{\\mathbb{E}_{\\mathbf{v}\\sim p}[\\mathbf{v}^{(a)}\\mathbf{v}^{(b)}]=\\pmb{\\Sigma}^{a b}}\\end{array}$ $\\mathbf{v}\\in\\mathbb{R}^{4d}$ ,b plalointcd i $\\Sigma=\\left[\\Sigma^{a b}\\right]_{a b}$ $\\mathbb{R}^{d}$ $p$ $\\mu_{i j k l}$ $\\pmb{\\mu}^{a b c d}$ fourth moment tensor should match $\\mathbf{C}$ . Fourth moments can always be decomposed into second moments: ", "page_idx": 29}, {"type": "equation", "text": "$$\nM_{i j k l}=\\mu_{i j k l}^{a b c d}=\\Sigma_{i j}^{a b}\\Sigma_{k l}^{c d}+\\Sigma_{i k}^{a c}\\Sigma_{j l}^{b d}+\\Sigma_{i l}^{a d}\\Sigma_{j k}^{b c}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "So finding the $p$ that satisfies Eq. 90 is equivalent to finding a zero-mean distribution $p$ with covariance that satisfies the above equation. In the case of $\\mathcal{L}$ , the mask tensor is block-diagonal: $M_{i j k l}=\\sigma_{i j}\\delta_{i j,k l}$ . So in the case where $a\\neq b$ , set $a=1,b=2$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sigma_{i j}=\\mu_{i j i j}^{1212}=\\Sigma_{i i}^{11}\\Sigma_{j j}^{22}+2(\\Sigma_{i j}^{12})^{2}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and $\\pmb{\\Sigma}=\\left[\\pmb{\\Sigma}^{11}\\quad\\pmb{\\Sigma}^{12}\\right]\\in\\mathbb{R}^{2d\\times2d}$ . Firstly, consider the diagonal entries of $\\sigma$ : ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sigma_{i i}=\\mu_{i i i i}^{a a a a}=3(\\Sigma_{i i}^{a a})^{2},\\quad a\\in\\{1,2\\}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This can always be satisfied by setting the diagonal entries of both $\\pmb{\\Sigma}^{a a}$ and $\\pmb{\\Sigma}^{a a}$ block as follows: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\Sigma_{i i}^{a a}=\\sqrt{\\sigma_{i i}/3},\\quad a\\in\\{1,2\\}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Next, consider the entire $\\sigma$ matrix. We have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sigma_{i j}=\\mu_{i j i j}^{1212}=\\Sigma_{i i}^{11}\\Sigma_{j j}^{22}+2(\\Sigma_{i j}^{12})^{2}=\\frac{1}{3}\\sqrt{\\sigma_{i i}\\sigma_{j j}}+2(\\Sigma_{i j}^{12})^{2}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In the case of $\\mathcal{L}$ , we have $\\sigma_{i j}=\\delta_{i j}$ , so for $i\\neq j$ we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n0=\\frac{1}{3}+2(\\Sigma_{i j}^{12})^{2}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which is impossible to satisfy since entries in a covariance matrix must be real. ", "page_idx": 29}, {"type": "text", "text": "K.3 Sparse vs dense jets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The variance of the sparse STDE estimator comes from the variance of selected derivative tensor elements, whereas the variance of the dense estimator comes from the derivative tensor elements that are not selected. For example, in the case of Laplacian, as also discussed in [12], the variance of the sparse STDE estimator comes from the diagonal element of the Hessian, whereas the variance of the dense STDE estimator comes from all the off-diagonal element of the Hessian. ", "page_idx": 29}, {"type": "text", "text": "L Further ablation study ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Figure 5: Ablation on randomization batch size with Inseparable and effectively high-dimensional PDEs, $\\scriptstyle{\\mathrm{dim}}=100\\mathbf{k}$ , 5 runs with different random seeds. Model converges when the difference of L2 error is below 1e-7. ", "page_idx": 30}, {"type": "image", "img_path": "J2wI2rCG2u/tmp/88e3b0f3fe9fc748d30e146421f1b317bd29af3cab4608c2e04b346307ac1e22.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our claims are backed by both theoretical and experimental evidence. The theoretical evidence is provided in the section 4, and the experimental evidence is provided in section 5. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We discussed the limitation of our work in section 6. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 31}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Most of our theoretical results are asymptotic analyses on the computation complexity, and we have clearly stated the assumption we have made. Our claim on the non-generalizability of HTE construction is proved rigorously in the Appendix K. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have included sample implementations of key steps of our method in the first section in the Appendix A. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We will open-source our code later. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have included the setting for all the hyperparameters in the Appendix H. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 33}, {"type": "text", "text": "\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We use the average of 5 random seeds for all our experiment results. We also reported the standard deviation for the relative error in PINN training in the Appendix I. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We have included the hardware and software specifications we used to conduct our experiments in the Appendix H. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: We have read the NeurIPS Code of Ethics and made sure that the paper conforms to it. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our work is not tied to particular applications, and there are no obvious paths that lead to potential harm. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our work is foundational and not tied to particular applications. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our work does not use existing assets. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our work does not release new assets. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used. ", "page_idx": 36}, {"type": "text", "text": "\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our paper did not involve crowdsourcing and human subjects. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our paper did not involve crowdsourcing and human subjects. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]