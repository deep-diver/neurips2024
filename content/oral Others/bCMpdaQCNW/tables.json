[{"figure_path": "bCMpdaQCNW/tables/tables_4_1.jpg", "caption": "Table 1: Data Statistics. Avg. Len. is the average number of words.", "description": "This table presents the statistics of the YESBUT dataset, showing the number of instances and average length for each component.  The components include the image, literal description of the comic narrative, the explanation of the contradiction, the underlying philosophy, and the comic title.  Average length is measured in words.", "section": "3.2 Data Annotation"}, {"figure_path": "bCMpdaQCNW/tables/tables_5_1.jpg", "caption": "Table 2: Main results. For literal description and contradiction, we report BERT score (recall), BLEURT (BLT), and GPT evaluation score. For philosophy and title, we report accuracy (%). Best scores are bold and the second best ones are marked with underline.", "description": "This table presents the main results of the experiments conducted in the paper. It shows the performance of various large language models (LLMs) and large vision-language models (VLMs) on four different tasks related to understanding comics: Literal Description, Contradiction, Underlying Philosophy Selection, and Title Matching. The results are presented in terms of accuracy (%) for the philosophy and title tasks, and BERT score (recall), BLEURT (BLT), and GPT evaluation score for the literal description and contradiction tasks. The best and second-best scores for each task and model are highlighted in bold and underlined, respectively.  The table allows for a direct comparison of model performance across different tasks and model types.", "section": "4 Experiments"}, {"figure_path": "bCMpdaQCNW/tables/tables_8_1.jpg", "caption": "Table 2: Main results. For literal description and contradiction, we report BERT score (recall), BLEURT (BLT), and GPT evaluation score. For philosophy and title, we report accuracy (%). Best scores are bold and the second best ones are marked with underline.", "description": "This table presents the main results of the experiments conducted in the paper.  It shows the performance of various large language models (LLMs) and large vision-language models (VLMs) on four different tasks related to understanding comics: Literal Description, Contradiction Generation, Underlying Philosophy Selection, and Title Matching. The metrics used to evaluate performance vary depending on the task and include accuracy, BERT score, BLEURT score and GPT evaluation scores. The table highlights the superior performance of commercial models (GPT-4, Claude-3) compared to open-sourced models, especially in the more complex tasks involving deep reasoning.  The inclusion of oracle comic descriptions is also examined, showcasing their positive impact on overall performance.", "section": "4 Experiments"}, {"figure_path": "bCMpdaQCNW/tables/tables_15_1.jpg", "caption": "Table 1: Data Statistics. Avg. Len. is the average number of words.", "description": "This table presents statistics for the YESBUT dataset, showing the number of samples for each component (image, literal description, contradiction, philosophy, and title) and the average length (in words) of the literal descriptions.", "section": "3 The YESBUT Dataset"}]