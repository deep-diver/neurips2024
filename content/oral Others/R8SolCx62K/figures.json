[{"figure_path": "R8SolCx62K/figures/figures_3_1.jpg", "caption": "Figure 1: t-SNE embedding of DGI on Co.CS dataset. The blue points represent the embeddings of the perturbed negative samples, and the red points denote that of positive nodes. As can be seen in Figures (a) and (b) of model random initialization and the layer of the trained encoder, the DGI-like methods essentially maximize the JS divergence between node embedding and embedding mean.", "description": "This figure shows the results of t-SNE dimensionality reduction applied to node embeddings generated by the DGI method on the Co-CS dataset.  The visualizations illustrate the distribution of node embeddings before training (random initialization), after the first GNN layer, and after the second GNN layer. Red points represent the embeddings of positive nodes (original graph), and blue points represent embeddings of negative nodes (corrupted graph).  The plots demonstrate that DGI-like methods maximize the Jensen-Shannon divergence (JS divergence) between the positive and negative node embeddings by pushing them apart in the embedding space, which is indicative of representation scattering.", "section": "3.1 DGI-like methods"}, {"figure_path": "R8SolCx62K/figures/figures_4_1.jpg", "caption": "Figure 2: The impact of Batch Normalization in BGRL.", "description": "This bar chart compares the F1-scores achieved by the BGRL model with and without Batch Normalization (BN) across four benchmark datasets: Photo, Co.CS, Computers, and Physics.  The results demonstrate a significant performance decrease in the BGRL model when BN is removed, highlighting its importance for representation scattering within this framework.  Error bars are included to show the variability of the F1-scores.", "section": "3.3 BGRL-like methods"}, {"figure_path": "R8SolCx62K/figures/figures_5_1.jpg", "caption": "Figure 3: The overview of SGRL. Consider a graph G processed using two distinct encoders (online encoder and target encoder): fo() with parameters \u03b8 and f$(\u00b7) with \u03c6, aimed at generating node representations Honline and Htarget, respectively. For Htarget, the mean representation of all nodes is calculated to serve as the scattered center c. The parameters of f(.) are updated via RSM that encourages node representations to diverge from c. Honline is processed through TCM to incorporate topology information, resulting in Htopology. Subsequently, Htopology is embedded through a predictor qe to predict Htarget, and the parameters in fo(\u00b7) is updated through back-propagation while stopping the gradient of f(.). Both channels are trained simultaneously. At the end of each epoch, we employ an Exponential Moving Average (EMA) to update parameters \u03c6. Finally, the representations generated by fo() are employed across various downstream tasks.", "description": "This figure shows a schematic overview of the Scattering Graph Representation Learning (SGRL) framework. It illustrates the two encoders (online and target), the representation scattering mechanism (RSM), the topology-based constraint mechanism (TCM), and the alignment loss function used for training.  The figure highlights the process of generating node representations, incorporating topological information, and pushing node representations away from a central point (scattering) for improved performance on downstream tasks.", "section": "4 Methodology"}, {"figure_path": "R8SolCx62K/figures/figures_8_1.jpg", "caption": "Figure 4: t-SNE embeddings of nodes in CS dataset.", "description": "This figure shows the visualization of node embeddings using t-SNE for the Coauthor-CS dataset. Each point represents a node, colored by its label.  The figure compares the visualizations generated by GRACE, DGI, BGRL, and SGRL, highlighting the differences in the clustering and separation of nodes based on their labels.  SGRL shows clearer inter-class boundaries and better intra-class clustering, indicating effective representation scattering and semantic aggregation.", "section": "5.2 Model Analysis"}, {"figure_path": "R8SolCx62K/figures/figures_8_2.jpg", "caption": "Figure 4: t-SNE embeddings of nodes in CS dataset.", "description": "This figure visualizes the t-SNE embeddings of nodes in the Computers dataset.  Each point represents a node, colored by its label.  The figure compares the node embeddings generated by four different methods: GRACE, DGI, BGRL, and SGRL (the proposed method). The visualization aims to show how well each method separates different classes (inter-class separation) and groups similar nodes together (intra-class clustering).  SGRL shows clearer inter-class boundaries and better intra-class clustering compared to the other methods.", "section": "5.2 Model Analysis"}]