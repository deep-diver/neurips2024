{"references": [{"fullname_first_author": "Charles Godfrey", "paper_title": "On the symmetries of deep learning models and their internal representations", "publication_date": "2022-12-01", "reason": "This paper provides a unified framework for characterizing symmetries in neural networks, crucial for understanding and leveraging symmetries in metanetworks."}, {"fullname_first_author": "Aviv Navon", "paper_title": "Equivariant architectures for learning in deep weight spaces", "publication_date": "2023-07-01", "reason": "This paper is among the first to propose equivariant architectures for processing neural network weights, a key concept for metanetworks, and it demonstrates significant performance improvements."}, {"fullname_first_author": "Miltiadis Kofinas", "paper_title": "Graph neural networks for learning equivariant representations of neural networks", "publication_date": "2024-01-01", "reason": "This paper proposes a graph-based metanetwork approach, which directly addresses the graph structure of neural networks, providing a strong foundation for the current work."}, {"fullname_first_author": "Allan Zhou", "paper_title": "Permutation equivariant neural functionals", "publication_date": "2023-12-01", "reason": "This paper introduces permutation-equivariant neural functionals, a closely related concept to the current work's scale-equivariant approach, highlighting the importance of symmetries in NN processing."}, {"fullname_first_author": "Derek Lim", "paper_title": "Sign and basis invariant networks for spectral graph representation learning", "publication_date": "2023-05-01", "reason": "This paper explores sign symmetries, which are closely related to scaling symmetries, and introduces a novel framework that is both sign and permutation invariant, relevant to the inductive bias of ScaleGMNs."}]}