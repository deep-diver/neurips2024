[{"figure_path": "Vi8AepAXGy/figures/figures_1_1.jpg", "caption": "Figure 1: We draw parallels between traditional protocols and the use of MLLMs for evaluating visual representations. MLLMs employ visual question answering to address a diverse array of real-world perception tasks. The bottom section highlights the five key pillars studied in Cambrian-1.", "description": "This figure illustrates the Cambrian-1 framework for evaluating visual representations using multimodal large language models (MLLMs). It draws a parallel between traditional evaluation protocols (linear probing or end-to-end tuning on datasets like ImageNet-1k, COCO, and ADE20k) and the use of MLLMs for visual question answering (VQA). The bottom part of the figure highlights the five key components of the Cambrian-1 framework: visual representations, connector design, instruction tuning data, instruction tuning recipes, and evaluation protocol.  The figure shows how different vision models (e.g., CLIP, DINO) can be incorporated into the MLLM pipeline and how the visual information is integrated with the LLM to answer questions.", "section": "Evaluating Visual Representations through MLLMs"}, {"figure_path": "Vi8AepAXGy/figures/figures_2_1.jpg", "caption": "Figure 2: Examples of various vision models, objectives, and architectures studied. Image from [48].", "description": "This figure shows examples of different vision models used in the paper, categorized by their training method and architecture.  It visually represents the variety of visual encoders investigated in the Cambrian-1 project.  The models include both class label supervised models (ImageNet-1k), language supervised models (CLIP), self-supervised models using contrastive (DINOv2) and masking (MAE) approaches, diffusion models (Stable Diffusion), depth-supervised models (MiDaS), and segmentation-supervised models (SAM).  Each category is represented by an example image illustrating the model's output or training process. This illustrates the breadth of vision encoders used to explore and evaluate visual representations for multimodal large language models (MLLMs).", "section": "2 Evaluating Visual Representations through MLLMs"}, {"figure_path": "Vi8AepAXGy/figures/figures_3_1.jpg", "caption": "Figure 3: Left: Performance comparison of MLLMs with visual input enabled and disabled across various benchmarks. Benchmarks are sorted by the difference between the average score with vision enabled and disabled. Right: Principal component analysis displaying clusters of benchmarks based on performance metrics, with bubble size corresponding to benchmark size. We label the clusters as \u201cGeneral\u201d in green, \u201cKnowledge\u201d in yellow, \u201cChart & OCR\u201d in red, and \u201cVision-Centric\u201d in blue.", "description": "This figure presents a comparative analysis of Multimodal Large Language Models (MLLMs) performance with and without visual input across various benchmarks. The left panel shows a bar chart illustrating the performance difference between vision-enabled and vision-disabled MLLMs for each benchmark, sorted by the magnitude of this difference.  The right panel displays a principal component analysis (PCA) plot, visualizing the clustering of benchmarks based on their performance similarity. The clusters are labelled and color-coded, categorizing them into General, Knowledge, Chart & OCR, and Vision-Centric.", "section": "2 Evaluating Visual Representations through MLLMs"}, {"figure_path": "Vi8AepAXGy/figures/figures_4_1.jpg", "caption": "Figure 4: Effect of Training Recipe on Model Performance. Boxplots display the distribution of benchmark scores across benchmark categories for different training recipes and types of visual encoders. The four training recipes include freezing the visual encoder with various amounts of adapter data (OM, 0.5M, 1.2M) as well as unfreezing it with 1.2M adapter data. Amount of Adapter Data: All model types show increased performance on general and vision-centric benchmarks; knowledge benchmarks show mixed results; OCR & chart benchmarks benefit from more data for language-supervised models. Unfreezing: Unfreezing the visual encoder with 1.2M adapter data generally benefits all categories.", "description": "This figure shows the effect of different training recipes on the performance of multimodal large language models (MLLMs). Four training recipes are compared: (1) freezing the visual encoder with no adapter data (OM), (2) freezing with 0.5M adapter data, (3) freezing with 1.2M adapter data, and (4) unfreezing the visual encoder with 1.2M adapter data. The boxplots show the distribution of benchmark scores across four categories of benchmarks: General, Knowledge, OCR & Chart, and Vision-Centric.  The results indicate that increasing the amount of adapter data generally improves performance, especially for general and vision-centric benchmarks. Unfreezing the visual encoder also tends to improve performance across all benchmark categories.", "section": "2.3 Instruction Tuning Recipes"}, {"figure_path": "Vi8AepAXGy/figures/figures_5_1.jpg", "caption": "Figure 5: Evaluating Visual Representations with MLLMs While language-supervised models outperform self-supervised or other models, a well-trained self-supervised model like DINOv2 can also achieve competitive performance on vision-centric tasks.", "description": "This figure shows the average performance of different vision models across four benchmark categories (General, Knowledge, OCR & Chart, Vision-Centric).  Language-supervised models (like CLIP) generally perform best, particularly in the OCR & Chart and Knowledge categories.  However, a well-trained self-supervised model like DINOv2 shows competitive performance in the Vision-Centric category, suggesting potential for improving self-supervised visual representations.", "section": "2.4 MLLMs as a Visual Representation Evaluator"}, {"figure_path": "Vi8AepAXGy/figures/figures_5_2.jpg", "caption": "Figure 6: Continued Fine-Tuning Narrows the Gap Between CLIP and DINOv2. Performance is compared with 0.7M and 5M instruction tuning data in both frozen () and unfrozen (0) settings. DINOv2 shows significant performance improvement with increased data and unfreezing-surpassing the 0.7M CLIP model in several benchmarks and narrowing the gap to the 5M model in knowledge and vision-centric tasks.", "description": "This figure compares the performance of models using CLIP and DINOv2 vision encoders with varying amounts of instruction tuning data (0.7M and 5M).  It shows that DINOv2, initially lagging behind CLIP, significantly improves its performance with more data and when the vision encoder is unfrozen during training.  The performance gap between DINOv2 and CLIP narrows considerably at the 5M data point, particularly in knowledge and vision-centric tasks, demonstrating the potential of self-supervised methods with sufficient training data.", "section": "2.3 Instruction Tuning Recipes"}, {"figure_path": "Vi8AepAXGy/figures/figures_7_1.jpg", "caption": "Figure 7: Spatial Vision Aggregator (SVA). We propose SVA, a dynamic and spatially-aware connector that integrates multiple vision features with LLMs while reducing the number of tokens.", "description": "This figure illustrates the architecture of the Spatial Vision Aggregator (SVA), a novel connector designed to efficiently integrate visual features from multiple vision encoders into an LLM.  The SVA uses learnable latent queries to perform cross-attention with the visual features, resulting in a dynamic and spatially-aware integration that reduces the number of tokens required.  The figure shows the SVA being incorporated multiple times within the LLM's transformer blocks to repeatedly access and integrate visual information.", "section": "3 Spatial Vision Aggregator (SVA): A New Connector Design"}, {"figure_path": "Vi8AepAXGy/figures/figures_8_1.jpg", "caption": "Figure 8: Cambrian-7M: A Large-Scale Curated Instruction Tuning Dataset for MLLM. Left: The inner circle shows the original distribution of Cambrian-10M. The outer circle shows the curated Cambrian-7M. Right: All the data sources in the Cambrian dataset as well as the ones filtered in data curation.", "description": "This figure illustrates the composition of the Cambrian-7M dataset, a curated version of the larger Cambrian-10M dataset.  The left panel shows a donut chart visualizing the distribution of data across different categories in Cambrian-10M. The right panel provides a detailed breakdown of all data sources used in the Cambrian dataset. The outer ring highlights the curated subset (Cambrian-7M) and shows how data from different sources were filtered or included during the curation process to achieve a more balanced and high-quality dataset for training multimodal LLMs.", "section": "4 Instruction Tuning Data for Training MLLMs"}, {"figure_path": "Vi8AepAXGy/figures/figures_8_2.jpg", "caption": "Figure 3: Left: Performance comparison of MLLMs with visual input enabled and disabled across various benchmarks. Benchmarks are sorted by the difference between the average score with vision enabled and disabled. Right: Principal component analysis displaying clusters of benchmarks based on performance metrics, with bubble size corresponding to benchmark size. We label the clusters as \"General\" in green, \"Knowledge\" in yellow, \"Chart & OCR\" in red, and \"Vision-Centric\" in blue.", "description": "This figure presents a comparative analysis of multimodal large language models (MLLMs) performance with and without visual input across various benchmarks.  The left panel displays a bar chart illustrating the difference in performance with and without visual input for each benchmark, revealing the benchmarks' reliance on visual information. Benchmarks are sorted by the magnitude of this performance difference.  The right panel showcases a principal component analysis (PCA) of the benchmark scores, visually clustering the benchmarks into four categories based on their performance characteristics: General, Knowledge, Chart & OCR, and Vision-Centric.  The size of each point in the PCA plot represents the size of the benchmark dataset.", "section": "2 Evaluating Visual Representations through MLLMs"}, {"figure_path": "Vi8AepAXGy/figures/figures_19_1.jpg", "caption": "Figure 3: Left: Performance comparison of MLLMs with visual input enabled and disabled across various benchmarks. Benchmarks are sorted by the difference between the average score with vision enabled and disabled. Right: Principal component analysis displaying clusters of benchmarks based on performance metrics, with bubble size corresponding to benchmark size. We label the clusters as \"General\" in green, \"Knowledge\" in yellow, \"Chart & OCR\" in red, and \"Vision-Centric\" in blue.", "description": "This figure presents a comparative analysis of multimodal large language models (MLLMs) with and without visual input across various benchmarks. The left panel shows a bar chart illustrating the performance difference between vision-enabled and vision-disabled MLLMs for each benchmark, highlighting the benchmarks' reliance on visual input. The right panel displays a principal component analysis (PCA) plot that clusters the benchmarks into four groups based on performance similarities: General, Knowledge, Chart & OCR, and Vision-Centric. These clusters represent different aspects of MLLM capabilities and their reliance on visual information. The size of each bubble in the PCA plot corresponds to the size of the benchmark dataset.", "section": "2 Evaluating Visual Representations through MLLMs"}, {"figure_path": "Vi8AepAXGy/figures/figures_20_1.jpg", "caption": "Figure 11: Cambrian Vision-Centric Benchmark (CV-Bench). We repurpose standard vision benchmarks to evaluate the fundamental 2D and 3D visual understanding of MLLMs. See Section 2.2 for more details.", "description": "This figure shows four example images from the Cambrian Vision-Centric Benchmark (CV-Bench).  CV-Bench repurposes existing vision benchmarks (ADE20K, COCO, Omni3D) to assess various aspects of multimodal large language models (MLLMs) by evaluating their ability to answer questions about images. The four examples represent four different tasks: Spatial Relationship (2D), Object Count (2D), Depth Order (3D), and Relative Distance (3D).  Each image has a question posed to illustrate how the benchmark tests the model's understanding of spatial relationships, object counting, depth perception, and relative object distances.", "section": "2.2 Cambrian Vision-Centric Benchmark (CV-Bench)"}, {"figure_path": "Vi8AepAXGy/figures/figures_21_1.jpg", "caption": "Figure 12: CV-CB Benchmark Filtering. We reformulate classic 2D and 3D CV benchmarks into Q&A questions to evaluate MLLM's visual capabilities.", "description": "This figure shows the workflow of the data curation process for the Cambrian Vision-Centric Benchmark (CV-Bench). It starts with three existing vision datasets: ADE20k, COCO, and Omni3D. These datasets are used to generate question-answer pairs for four different visual tasks: spatial relationship (2D), object count (2D), depth order (3D), and relative distance (3D).  A manual filtering step is then applied to remove inaccurate or ambiguous examples. The resulting dataset is a curated set of question-answer pairs suitable for evaluating the visual understanding capabilities of multimodal large language models.", "section": "2.2 Cambrian Vision-Centric Benchmark (CV-Bench)"}, {"figure_path": "Vi8AepAXGy/figures/figures_26_1.jpg", "caption": "Figure 3: Left: Performance comparison of MLLMs with visual input enabled and disabled across various benchmarks. Benchmarks are sorted by the difference between the average score with vision enabled and disabled. Right: Principal component analysis displaying clusters of benchmarks based on performance metrics, with bubble size corresponding to benchmark size. We label the clusters as \"General\" in green, \"Knowledge\" in yellow, \"Chart & OCR\" in red, and \"Vision-Centric\" in blue.", "description": "This figure shows two graphs. The left graph displays the performance difference between MLLMs with and without visual input across various benchmarks.  The benchmarks are ordered by the difference in performance, highlighting which tasks heavily rely on visual information versus language understanding. The right graph shows the result of a principal component analysis performed on the benchmark scores. This analysis reveals clusters of benchmarks based on their similarity in performance across different MLLMs, and these clusters are labeled as \"General\", \"Knowledge\", \"Chart & OCR\", and \"Vision-Centric\". This helps to categorize the benchmarks based on what aspects of MLLM capabilities they primarily assess.", "section": "2 Evaluating Visual Representations through MLLMs"}, {"figure_path": "Vi8AepAXGy/figures/figures_28_1.jpg", "caption": "Figure 1: We draw parallels between traditional protocols and the use of MLLMs for evaluating visual representations. MLLMs employ visual question answering to address a diverse array of real-world perception tasks. The bottom section highlights the five key pillars studied in Cambrian-1.", "description": "This figure illustrates the Cambrian-1 framework for evaluating visual representations using Multimodal Large Language Models (MLLMs).  It highlights the key components involved in the process, including pretrained vision models, visual instruction tuning with LLMs, connector design, instruction tuning data, and evaluation protocols. The framework draws parallels between traditional methods of evaluating visual representations and the novel use of MLLMs, particularly focusing on visual question answering to tackle real-world perception challenges.  The five pillars of the Cambrian-1 study are also highlighted: visual representations, connector design, instruction tuning data, instruction tuning recipes, and evaluation protocols.", "section": "Evaluating Visual Representations through MLLMs"}, {"figure_path": "Vi8AepAXGy/figures/figures_28_2.jpg", "caption": "Figure 1: We draw parallels between traditional protocols and the use of MLLMs for evaluating visual representations. MLLMs employ visual question answering to address a diverse array of real-world perception tasks. The bottom section highlights the five key pillars studied in Cambrian-1.", "description": "This figure illustrates the Cambrian-1 methodology for evaluating visual representations using Multimodal Large Language Models (MLLMs). It highlights the parallels between traditional evaluation protocols (like linear probing and end-to-end tuning) and the use of MLLMs for assessing various visual encoders. The MLLM framework leverages visual question answering (VQA) to address real-world perception challenges.  The figure's lower section emphasizes the five key pillars of Cambrian-1: Visual Representations, Connector Design, Instruction Tuning Data, Instruction Tuning Recipes, and Evaluation Protocol.", "section": "2 Evaluating Visual Representations through MLLMs"}, {"figure_path": "Vi8AepAXGy/figures/figures_31_1.jpg", "caption": "Figure 12: CV-CB Benchmark Filtering. We reformulate classic 2D and 3D CV benchmarks into Q&A questions to evaluate MLLM's visual capabilities.", "description": "This figure shows the process of filtering the data used for the Cambrian Vision-Centric Benchmark (CV-Bench).  The process starts with classic 2D (ADE20K, COCO) and 3D (Omni3D) computer vision benchmarks and reformulates them into visual question answering (VQA) tasks. The initial data generated through this process is then manually filtered to remove inaccurate or ambiguous questions. The filtering criteria are described for the counting, relative distance and depth order tasks and the final dataset is used for evaluation.", "section": "2.2 Cambrian Vision-Centric Benchmark (CV-Bench)"}, {"figure_path": "Vi8AepAXGy/figures/figures_32_1.jpg", "caption": "Figure 17: Data Balancing via Applying Thresholds on Data Sources. Applying threshold t alleviates the exponential tail of Cambrian-10M.", "description": "The figure shows the cumulative sum of counts for entries sorted by counts from tail to head for different data balancing methods. Data Mix 1 is unfiltered, while Data Mixes 2-5 apply different thresholds (t) to filter data from various sources.  The plot demonstrates that applying a threshold between 150k and 350k is effective in preventing an explosive heavy tail, leading to a more balanced dataset. This helps to mitigate the issue of noisy and unbalanced data that often leads to suboptimal performance in multimodal large language models (MLLMs).", "section": "G.4 Full results on data curation experiment"}, {"figure_path": "Vi8AepAXGy/figures/figures_32_2.jpg", "caption": "Figure 18: Comparison of model average performances on each category. Cambrian-1 outperforms other open-source models across all sizes. The lead is especially large on OCR & Chart and Vision-Centric benchmarks, highlighting the advantage of our vision-centric design.", "description": "This figure compares the average performance of Cambrian-1 and other leading MLLMs across different benchmark categories (General, Knowledge, OCR & Chart, and Vision-Centric).  Cambrian-1 demonstrates superior performance across all categories, particularly in the OCR & Chart and Vision-Centric tasks, which emphasizes its vision-centric design.", "section": "J More results of Cambrian-1 Model"}, {"figure_path": "Vi8AepAXGy/figures/figures_36_1.jpg", "caption": "Figure 3: Left: Performance comparison of MLLMs with visual input enabled and disabled across various benchmarks. Benchmarks are sorted by the difference between the average score with vision enabled and disabled. Right: Principal component analysis displaying clusters of benchmarks based on performance metrics, with bubble size corresponding to benchmark size. We label the clusters as \"General\" in green, \"Knowledge\" in yellow, \"Chart & OCR\" in red, and \"Vision-Centric\" in blue.", "description": "The left panel of the figure shows the performance difference between MLLMs with and without visual input enabled across different benchmarks. The benchmarks are sorted by the difference.  Benchmarks with a small difference indicate a lesser dependence on visual input.  The right panel shows a principal component analysis clustering benchmarks into four groups based on their performance metrics: General, Knowledge, Chart & OCR, and Vision-Centric.", "section": "2 Evaluating Visual Representations through MLLMs"}, {"figure_path": "Vi8AepAXGy/figures/figures_37_1.jpg", "caption": "Figure 3: Left: Performance comparison of MLLMs with visual input enabled and disabled across various benchmarks. Benchmarks are sorted by the difference between the average score with vision enabled and disabled. Right: Principal component analysis displaying clusters of benchmarks based on performance metrics, with bubble size corresponding to benchmark size. We label the clusters as \"General\" in green, \"Knowledge\" in yellow, \"Chart & OCR\" in red, and \"Vision-Centric\" in blue.", "description": "The figure presents two plots analyzing the performance of Multimodal Large Language Models (MLLMs). The left plot compares MLLM performance with and without visual input across several benchmarks. Benchmarks are ranked by the difference in MLLM scores with and without vision.  The right plot shows a principal component analysis (PCA) clustering benchmarks into four groups (general, knowledge, chart & OCR, and vision-centric) based on their performance metrics.", "section": "2.1 Analyzing the Benchmarks"}, {"figure_path": "Vi8AepAXGy/figures/figures_39_1.jpg", "caption": "Figure 18: Comparison of model average performances on each category. Cambrian-1 outperforms other open-source models across all sizes. The lead is especially large on OCR & Chart and Vision-Centric benchmarks, highlighting the advantage of our vision-centric design.", "description": "This figure compares the average performance of Cambrian-1, Mini-Gemini-HD, and LLaVA-NeXT across four benchmark categories (General, Knowledge, OCR & Chart, and Vision-Centric) for three different model sizes (8B, 13B, and 34B parameters).  It shows that Cambrian-1 consistently outperforms the other two open-source models, especially in the OCR & Chart and Vision-Centric categories, demonstrating the effectiveness of its vision-centric design.", "section": "J More results of Cambrian-1 Model"}, {"figure_path": "Vi8AepAXGy/figures/figures_40_1.jpg", "caption": "Figure 1: We draw parallels between traditional protocols and the use of MLLMs for evaluating visual representations. MLLMs employ visual question answering to address a diverse array of real-world perception tasks. The bottom section highlights the five key pillars studied in Cambrian-1.", "description": "This figure illustrates the Cambrian-1 framework, which uses multimodal large language models (MLLMs) to evaluate visual representations.  It highlights the relationship between traditional evaluation protocols (linear probing, end-to-end fine-tuning) and the use of MLLMs for evaluating a wider range of real-world visual perception tasks. The figure also emphasizes the five key research pillars of Cambrian-1: visual representations, connector design, instruction tuning data, instruction tuning recipes, and evaluation protocol.", "section": "Evaluating Visual Representations through MLLMs"}]