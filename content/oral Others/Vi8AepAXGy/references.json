{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "CLIP, introduced in this paper, is the foundational visual encoder used by most multimodal LLMs, significantly influencing Cambrian-1's design and evaluation."}, {"fullname_first_author": "Shengbang Tong", "paper_title": "Eyes wide shut? exploring the visual shortcomings of multimodal LLMs", "publication_date": "2024-06-01", "reason": "This paper, also by the Cambrian-1 authors, directly addresses the limitations of existing MLLM benchmarks and motivates the vision-centric approach of Cambrian-1."}, {"fullname_first_author": "Hoang-An Le", "paper_title": "Visual Instruction Tuning", "publication_date": "2023-12-01", "reason": "This paper introduces the crucial visual instruction tuning paradigm, which forms the core methodology for training Cambrian-1 and comparing various vision encoders."}, {"fullname_first_author": "Matthieu Buccheri", "paper_title": "Self-supervised learning from images with a joint-embedding predictive architecture", "publication_date": "2023-06-01", "reason": "This paper introduces DINOv2, a key self-supervised vision model explored in Cambrian-1, demonstrating the potential of self-supervised methods as a competitive alternative to language-supervised approaches."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-03-01", "reason": "Vision Transformers (ViTs), introduced in this paper, are a dominant architecture for visual encoders in many MLLMs and are extensively investigated for their effectiveness in the Cambrian-1 study."}]}