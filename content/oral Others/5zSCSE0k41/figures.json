[{"figure_path": "5zSCSE0k41/figures/figures_1_1.jpg", "caption": "Figure 1: Given a single portrait image, a speech audio clip, and optionally a set of other control signals, our approach produces a high-quality lifelike talking face video of 512x 512 resolution at up to 40 FPS. The method is generic and robust, and the generated talking faces can faithfully mimic human facial expressions and head movements, reaching a high level of realism and liveliness. (All the photorealistic portrait images in this paper are virtual, non-existing identities generated by [30, 5]. See our project page for the generated video samples with audios.)", "description": "This figure showcases the results of the VASA-1 model.  Given a single portrait image and an audio clip (with optional control signals), the model generates a high-quality, lifelike talking face video at 512x512 resolution and up to 40 frames per second. The generated faces realistically mimic human facial expressions and head movements.", "section": "1 Introduction"}, {"figure_path": "5zSCSE0k41/figures/figures_4_1.jpg", "caption": "Figure 2: Our holistic facial dynamics and head pose generation framework with diffusion transformer.", "description": "This figure illustrates the VASA-1 framework's architecture.  The left side shows the training pipeline for motion latent diffusion, where a video's motion latents are fed into a transformer network and undergo a diffusion process (adding and removing noise) conditioned on audio features and other control signals. The right side depicts the test pipeline, which takes a single image and audio as input, extracts the relevant latent codes (appearance, identity, and motion), applies a denoising process through the transformer network, and finally reconstructs the output video frames using a decoder. The figure visualizes the core idea of generating high-quality talking face videos by modeling and controlling facial dynamics in a latent space.", "section": "3 Method"}, {"figure_path": "5zSCSE0k41/figures/figures_7_1.jpg", "caption": "Figure 3: Generated talking faces under different control signals. Top row: results under different main gaze direction condition (forward-facing, leftwards, rightwards, and upwards, respectively). Middle row: results under different head distances (from far to near). Bottom row: results under different emotion offset (neutral, happy, angry and surprised, respectively).", "description": "This figure shows the results of generating talking faces with different control signals using the VASA-1 model.  The top row demonstrates control over gaze direction, the middle row shows control over head distance from the camera, and the bottom row demonstrates control over the emotional expression of the face.", "section": "Experiments"}, {"figure_path": "5zSCSE0k41/figures/figures_8_1.jpg", "caption": "Figure 4: Ablation study on loss function lconsist for disentangled latent space learning. We generate the results by only transferring the facial dynamics from source to target with head pose unchanged. lconsist is crucial for decoupling subtle yet important facial dynamics from head pose.", "description": "This figure shows the ablation study of the loss function  lconsist, which is designed to disentangle facial dynamics from head pose.  The experiment transfers only facial dynamics from a source image to a target image while keeping the target's head pose unchanged. Comparing the results with and without lconsist, we can see that lconsist is essential for decoupling subtle facial dynamics from head pose, resulting in more natural and realistic facial expressions.", "section": "3 Method"}, {"figure_path": "5zSCSE0k41/figures/figures_15_1.jpg", "caption": "Figure 1: Given a single portrait image, a speech audio clip, and optionally a set of other control signals, our approach produces a high-quality lifelike talking face video of 512x 512 resolution at up to 40 FPS. The method is generic and robust, and the generated talking faces can faithfully mimic human facial expressions and head movements, reaching a high level of realism and liveliness. (All the photorealistic portrait images in this paper are virtual, non-existing identities generated by [30, 5]. See our project page for the generated video samples with audios.)", "description": "This figure shows example results of the VASA-1 model. Given a single image of a person, an audio clip, and optional control signals, the model generates a high-quality, lifelike talking face video at a resolution of 512x512 pixels and a frame rate of up to 40 FPS.  The generated faces exhibit realistic facial expressions and head movements, demonstrating the model's ability to produce highly lifelike results.", "section": "1 Introduction"}, {"figure_path": "5zSCSE0k41/figures/figures_15_2.jpg", "caption": "Figure A.2: Disentanglement between head pose and facial dynamics. From top to bottom: the raw generated sequence, applying generated poses with fixed initial facial dynamics, and applying generated facial dynamics with fixed initial head pose and pre-defined spinning poses, respectively.", "description": "This figure demonstrates the disentanglement between head pose and facial dynamics in the VASA-1 model.  It shows three sets of generated video frames: 1) the original sequence with both natural head pose and facial dynamics, 2) the same sequence but with fixed facial dynamics and only changing head pose, and 3) the same sequence with fixed head pose and only varying facial dynamics. This highlights the model's ability to control these aspects independently.", "section": "More Qualitative Evaluation, Comparison and Ablation Study"}, {"figure_path": "5zSCSE0k41/figures/figures_16_1.jpg", "caption": "Figure A.3: Generation results with out-of-distribution images (non-photorealistic) and audios (singing audios for the first two rows and non-English speech for the last row). Our method can still generate high quality videos well-aligned with the audios, although it was not trained on such data variations. See the supplementary video with audio for a better illustration of these results.", "description": "This figure demonstrates the robustness of the VASA-1 model by showing generation results using various out-of-distribution inputs, including non-photorealistic images and audio containing singing and non-English speech.  Despite not being trained on such data, the model maintains high-quality video output synchronized with the audio.", "section": "More Qualitative Evaluation, Comparison and Ablation Study"}, {"figure_path": "5zSCSE0k41/figures/figures_16_2.jpg", "caption": "Figure A.4: Generation results from different methods with the input audio segment uttering \u201cpush ups\u201d. See our supplementary video for a better illustration and comparison.", "description": "This figure compares the visual results of four different talking face generation methods (MakeItTalk, Audio2Head, SadTalker, and the proposed method) for the same input audio segment saying \"push ups\". It demonstrates the differences in lip synchronization, facial expressions, and head movements produced by each method. The supplementary video provides a more detailed visual comparison of the generated videos.", "section": "More Qualitative Evaluation, Comparison and Ablation Study"}, {"figure_path": "5zSCSE0k41/figures/figures_17_1.jpg", "caption": "Figure A.4: Generation results from different methods with the input audio segment uttering \u201cpush ups\u201d. See our supplementary video for a better illustration and comparison.", "description": "This figure compares the results of four different methods for generating talking head videos. The input for all methods is the same audio segment, which says \"push ups\".  The figure shows a sequence of frames generated by each method, allowing for a visual comparison of the lip synchronization, head pose, and overall realism of the generated videos.  The methods compared are MakeItTalk, Audio2Head, SadTalker, and the authors' proposed method.  The supplementary video provides a more comprehensive comparison because it includes the audio.", "section": "More Qualitative Evaluation, Comparison and Ablation Study"}, {"figure_path": "5zSCSE0k41/figures/figures_17_2.jpg", "caption": "Figure A.4: Generation results from different methods with the input audio segment uttering \u201cpush ups\u201d. See our supplementary video for a better illustration and comparison.", "description": "This figure compares the results of four different methods for generating talking head videos from audio: MakeItTalk, Audio2Head, SadTalker, and the authors' method.  The audio input is the phrase \"push ups.\" Each row represents a different method, showing a sequence of frames generated for that audio clip. The figure highlights the differences in the visual quality, realism, and synchronization between audio and visual movements across the different methods. A supplementary video is suggested for a more comprehensive comparison.", "section": "More Qualitative Evaluation, Comparison and Ablation Study"}, {"figure_path": "5zSCSE0k41/figures/figures_18_1.jpg", "caption": "Figure A.7: Generation results from different methods with the input audio segment uttering \u201clots of questions\u201d. See our supplementary video for a better illustration and comparison.", "description": "This figure compares the results of four different methods for generating talking faces from audio: MakeItTalk, Audio2Head, SadTalker, and the authors' method (Ours).  The input audio segment is the phrase \"lots of questions\".  The figure shows a sequence of frames for each method, allowing for a visual comparison of lip synchronization, facial expressions, and overall realism.  The authors recommend viewing the supplementary video for a more thorough assessment.", "section": "More Qualitative Evaluation, Comparison and Ablation Study"}]