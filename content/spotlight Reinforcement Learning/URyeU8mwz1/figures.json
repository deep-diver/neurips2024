[{"figure_path": "URyeU8mwz1/figures/figures_7_1.jpg", "caption": "Figure 1: Examples: CR for grid and chain environments.", "description": "This figure shows two example Markov Decision Processes (MDPs) used in the paper to analyze the competitive ratio (CR) of reward lookahead.  The first is a chain MDP, where an agent starts at the head of a chain and can either move forward or transition to an absorbing terminal state. The second is a grid MDP, where the agent starts at the bottom-left corner and can move up or right until reaching the top-right corner. These examples are used to illustrate how the CR changes in different MDP structures and to show that the worst-case CR could be achieved in simple environments.", "section": "5 Examples"}, {"figure_path": "URyeU8mwz1/figures/figures_22_1.jpg", "caption": "Figure 2: A near-worst-case environment: tree-like MDP. An agent can decide to stay at the root of the tree, but once it starts to traverse the tree, it must navigate to one of its leaves, from which it moves to a non-rewarding terminal state. All leaves have long-shot rewards, while all other nodes yield no reward.", "description": "This figure shows a tree-like Markov Decision Process (MDP) used to illustrate a near-worst-case scenario for the competitive ratio (CR).  The agent starts at the root and can choose to stay there or traverse the tree.  Once traversal begins, the agent must continue until reaching a leaf node. Leaf nodes have a long-shot reward distribution (very high reward with low probability). All other nodes have zero reward. This structure is designed to highlight the difference in performance between agents with and without reward lookahead.", "section": "C.1 Upper-Bounds for Reward Lookahead \u2013 Delayed Trees"}, {"figure_path": "URyeU8mwz1/figures/figures_25_1.jpg", "caption": "Figure 2: A near-worst-case environment: tree-like MDP. An agent can decide to stay at the root of the tree, but once it starts to traverse the tree, it must navigate to one of its leaves, from which it moves to a non-rewarding terminal state. All leaves have long-shot rewards, while all other nodes yield no reward.", "description": "This figure shows a tree-like Markov Decision Process (MDP) used to illustrate a near worst-case scenario for the competitive ratio. The agent starts at the root node and can choose to either stay at the root or traverse down the tree.  Once traversal starts, it must continue to a leaf node.  Leaf nodes have a long-shot reward (high reward with low probability), while all other nodes have zero reward. The structure forces a trade-off between immediate rewards and long-term rewards, making it a challenging environment to analyze lookahead.", "section": "C.1 Upper-Bounds for Reward Lookahead \u2013 Delayed Trees"}]