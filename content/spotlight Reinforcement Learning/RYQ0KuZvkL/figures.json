[{"figure_path": "RYQ0KuZvkL/figures/figures_2_1.jpg", "caption": "Figure 1: A motivating example for differences. The rewards for all actions other than the ones specified in the figure are 0. Define policy set II = {\u03c01, \u03c02} so that \u03c0\u2081 always plays a\u2081, whereas \u03c02 plays a1 on green states but a2 on red states. The difference of their state-action visitation probabilities is only non-zero in states s3, s4 and are just O(\u20ac) apart.", "description": "This figure shows a simple Markov Decision Process (MDP) with four states and three actions. Two policies, \u03c01 and \u03c02 are defined; \u03c01 always takes action a1, while \u03c02 takes action a2 in the red states (s3 and s4) and a1 otherwise. The key point illustrated is that the difference in state-action visitations between \u03c01 and \u03c02 is only significant in the red states and negligible elsewhere. This motivates the idea that estimating only the difference between policy values can be more efficient than estimating the value of each policy individually.", "section": "Introduction"}, {"figure_path": "RYQ0KuZvkL/figures/figures_16_1.jpg", "caption": "Figure 1: A motivating example for differences. The rewards for all actions other than the ones specified in the figure are 0. Define policy set II = {\u03c01, \u03c02} so that \u03c0\u2081 always plays a\u2081, whereas \u03c02 plays a1 on green states but a2 on red states. The difference of their state-action visitation probabilities is only non-zero in states s3, s4 and are just O(\u20ac) apart.", "description": "This figure is a simple MDP with three actions and four states.  Two policies (\u03c01 and \u03c02) are defined. Policy \u03c01 always selects action a1. Policy \u03c02 selects a1 except in states s3 and s4 (the red states), where it selects a2. The key takeaway is that the difference in the state-action visitations of these two policies is small (O(\u03b5)) and only present in states s3 and s4.  This highlights the core idea of the paper: that focusing on the difference between policies can be more sample-efficient than estimating each policy's value independently.", "section": "1 Introduction"}]